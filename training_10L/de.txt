Sarah Bernhardt [sara bɛrˈnar] (* 22. Oktober 1844 in Paris; † 26. März 1923 ebenda; eigentlich Marie Henriette Rosine Bernardt) war eine französische Schauspielerin. Sie gilt als die berühmteste Darstellerin ihrer Zeit und war einer der ersten Weltstars.
Sarah Bernhardt war die Tochter von Judith Bernardt, einer in Berlin geborenen niederländischen Modistin, die sich in Paris als Kurtisane niederließ und dort auch unter dem Namen Youle (Julie) bekannt war. Als ursprünglicher Familienname der Mutter ist van Hardt belegt. Die Identität von Sarah Bernhardts Vater lag lange Zeit im Dunkeln. Erst 2022 wurde nachgewiesen, dass es sich um den 1819 geborenen Édouard Gustave Viel, einen jungen Rechtsanwalt in Le Havre, handelte, der schließlich 1855 wegen Betrugs eine achtzehnmonatige Gefängnisstrafe verbüßte und, seiner Bürgerrechte beraubt, 1857 unter ungeklärten Umständen in Pisa starb. Er hatte eine Liebesbeziehung zu Judith Bernardt unterhalten, die 1851 möglicherweise eine zweite Tochter von ihm zur Welt brachte, Jeanne, deren Vaterschaft jedoch ungeklärt blieb. Judith Bernardt trat 1860 in einen Rechtsstreit um Sarahs Anteil an Viels Erbe; Sarah erbte schließlich 8.000 Francs und wurde auch im Testament ihrer Großmutter, der Mutter ihres Vaters, mit 3.000 Francs bedacht, ohne dass die Vaterschaft Viels anerkannt wurde.[1]
Bis zu ihrem achten Lebensjahr wurde Sarah Bernhardt von einem Kindermädchen aufgezogen, dann kam sie in ein Mädchenpensionat und mit zehn Jahren in eine Klosterschule in Versailles. Ihr kindlicher Berufswunsch war es, Nonne zu werden. Doch der Duc de Morny, ein Halbbruder Napoleons III. und Gönner ihrer Mutter, war der Ansicht, das leicht erregbare, einfühlsame Wesen des jungen Mädchens prädestiniere es für die Kunst des Theaters, und vermittelte der 14-Jährigen eine Schauspielausbildung am Pariser Konservatorium.[2] Vier Jahre später debütierte sie an der Comédie-Française in der Titelrolle des klassischen Schauspiels Iphigénie von Jean Racine. Nach einem Streit mit einer Kollegin wurde sie entlassen und musste sich eine Zeitlang mit unbedeutenden Rollen an kleineren Theatern begnügen. 1864 wurde in Paris ihr Sohn, Maurice, geboren. Sein Vater war der belgische Fürst Henri de Ligne, den Bernhardt in Brüssel kennen gelernt hatte und der sie heiraten wollte, doch von seiner Familie zurückgehalten wurde.
Bernhardts erster großer beruflicher Erfolg stellte sich im Jahr 1868 ein, als sie am Odéon, einem Theater am Jardin du Luxembourg in Paris, in Kean von Alexandre Dumas d. Ä. auftrat. Im deutsch-französischen Krieg 1870/71, als alle Theater geschlossen wurden und das Odéon als Lazarett diente, betätigte sie sich als Krankenschwester und pflegte verwundete Soldaten. Nach dem Krieg kehrte sie an die Comédie-Française zurück, und es begann ihr schneller steiler Aufstieg. Bald war sie die berühmteste Darstellerin ihrer Zeit, in Frankreich gefeiert als la Voix d’or, „die goldene Stimme“ (so nannte sie zuerst Victor Hugo), und la Divine, „die Göttliche“.
Ein wesentlicher Teil ihrer künstlerischen Arbeit – und Voraussetzung für ihren weltweiten Ruhm – waren ausgedehnte Gastspielreisen. Mit ihrer eigenen Schauspieltruppe trat sie 1879 in London auf. 1880 folgte eine halbjährige Tournee durch 51 Städte der USA. 1881 gab sie Vorstellungen in Russland, Italien, Griechenland, Ungarn, der Schweiz, Dänemark, Belgien und den Niederlanden. Englands Königin Victoria und der russische Zar Alexander III. gehörten zu ihren Bewunderern.
Im Jahr 1882 heiratete Bernhardt einen griechischen Botschaftsattaché, Jacques Damala, der sich für begabt genug hielt, um als ihr Bühnenpartner aufzutreten. Bernhardt eröffnete für ihn ein eigenes Theater, das ihr Sohn, Maurice, leitete. Das Unternehmen scheiterte, da Damalas Akzent den Spott des Publikums hervorrief und er zudem seine Glücksspiel- und Morphinsucht aus der Theaterkasse finanzierte. Noch im Jahr der Hochzeit trennte sich das Paar. Nach vorübergehender Aussöhnung und erneutem Zerwürfnis starb Damala 1889 mit 34 Jahren an den Folgen seiner Drogensucht.
Nachdem ihr Ehemann sie finanziell ruiniert hatte, unternahm Bernhardt eine Europatournee, um die Verluste wieder einzuspielen. Zwischen 1886 und 1889 folgten erneut gefeierte Gastspiele in den USA und von 1891 bis 1893 eine Welttournee. So wurde Sarah Bernhardt ein Weltstar, verehrt in ganz Europa und Amerika. Auftritte in Deutschland lehnte sie wegen des Krieges von 1870/71 zunächst ab. Erst 1902 besuchte sie Berlin und trat an mehreren Abenden in Folge als Fedora, Tosca, Kameliendame, Hamlet und Phädra auf. Am zweiten Abend wohnte auch Kaiser Wilhelm II. ihrer Darbietung bei.[3] 
Zwischen ihren Tourneen engagierte sich Sarah Bernhardt in Paris. Hier leitete sie mehrere Theater, an denen sie auch selbst auftrat: von 1893 bis 1899 das Théâtre de la Renaissance und von 1899 bis zu ihrem Tod das Théâtre Lyrique an der Place du Châtelet (früherer Name auch Théâtre des Nations), das sie in Théâtre Sarah-Bernhardt umbenannte (heute Théâtre de la Ville).[4] In diesen Jahren (1895–1900) entwarf Alfons Mucha ihre Werbeplakate, die große Aufmerksamkeit erregten.
Bernhardt galt als kapriziöse Exzentrikerin, die sich mit zahlreichen Liebhabern umgab, darunter der Lebemann Charles Haas, der Schauspieler Jean Mounet-Sully und der Maler und Illustrator Gustave Doré. Sie verstand es, mit ungewöhnlichen Aktionen für sich und ihre Arbeit zu werben. So stieg sie in einer Montgolfière in den Himmel über Frankreich auf und ließ Fotos verkaufen, die sie schlafend oder beim Rollenstudium in einem Sarg zeigten. Ihre Wohnung beherbergte eine Menagerie heimischer und exotischer Tiere. Auf der Bühne beeindruckte ihre intensive Darstellung; berühmt waren ihre expressiven Sterbeszenen, von denen sie selbst berichtete, dass sie danach von der Bühne getragen werden musste und erst in ihrer Garderobe wieder zur Besinnung kam.[5]
Als einflussreiche, engagierte Frau ergriff sie in der Dreyfus-Affäre Partei für den zeitweilig verfemten Schriftsteller und Fürsprecher Dreyfus', Émile Zola. Sie war eine erklärte Gegnerin der Todesstrafe und setzte sich während einer Amerikatournee für den Eintritt der USA in den Ersten Weltkrieg ein, der Frankreich und ganz Europa zu verheeren drohte.
Neben Romanen und Lustspielen verfasste Bernhardt 1907 ihre Memoiren (Mein Doppelleben). Damit inspirierte sie Marcel Proust, der in seinem Roman Auf der Suche nach der verlorenen Zeit die Figur der Schauspielerin La Berma nach dem Vorbild Bernhardts und die Hauptfigur Swann nach ihrem Geliebten, Charles Haas, entwarf. Außer beim Schreiben zeigte sie auch in der Malerei und Bildhauerei Talent.
Im Jahr 1906 erhielt sie eine Professur am Pariser Konservatorium. 1914 wurde sie Mitglied der französischen Ehrenlegion, eine der höchsten Auszeichnungen ihres Heimatlandes.
1905 musste Bernhardt in Victorien Sardous Stück La Tosca in Rio de Janeiro auf der Bühne von einer Mauer springen und zog sich dabei eine schwere Knieverletzung zu. Nach jahrelangen Schmerzen und wegen der Spätfolgen einer Gangrän wurde 1915 ihr rechtes Bein amputiert. Von da an trat sie mit einer Prothese auf, was ihrer Wirkung und ihrem Erfolg keinen Abbruch tat. In späten Jahren wurde sie auf einem Sessel auf die Bühne getragen, wo sie sitzend ihre Rollen spielte.[6] So wirkte Bernhardt während des Ersten Weltkriegs an der Betreuung französischer Truppen mit und spielte hinter der Front in Zelten, Scheunen und Lazaretten. Auch unternahm sie eine weitere Tournee durch die USA.
1900 übernahm sie in Le Duel d'Hamlet ihre erste Rolle in einem Kinofilm (Stummfilm), erklärte danach aber ihre heftige Abneigung gegen das neue Medium. Trotzdem trat sie später in weiteren Filmen auf, z. B. in La Tosca (1909), La Dame aux camélias (1911) und Königin Elisabeth von England (1912). All ihre Filmauftritte in den 1910er Jahren standen unter der Regie von Louis Mercanton. Ihre Stimme ist auf einigen Grammophonplatten und Phonographenwalzen dokumentiert (siehe unten).
Sarah Bernhardt starb am 26. März 1923 in Paris und wurde auf dem Friedhof Père Lachaise beerdigt. Ihr wurde ein Staatsbegräbnis zuteil. Der Trauerzug soll der bedeutendste seit der Beisetzung des Schriftstellers Victor Hugo gewesen sein;[7] mehr als eine halbe Million Menschen säumte die Straßen von Paris.
Sarah Bernhardt wurde gerühmt wegen ihrer schönen Stimme, der Anmut ihrer Bewegungen und wegen ihres Temperaments.
„[…] Wahrhaftig, sie ist all der Aufregung wert, in die sie die Welt versetzt hat. Der Zauber, der von ihrer Persönlichkeit ausgeht, ist unbeschreiblich. Sie […] ist ein so von Geist durchdrungenes schönes Weib, daß jede Regung ihrer Seele zum Aufleuchten hellster Schönheit wird. Man weiß, daß sie zu allem Talent hat, sie malt, sie ist Bildhauerin. Das glaube ich wohl; denn sie ist nichts anderes als der von einem anmutigen und zugleich energischen Genius beseelte, jedem warmen Impulse dienstbare Leib einer so nicht leicht zum zweiten Male existierenden Frau. […] Ich weiß nur, daß ich etwas Schöneres als die Gesamtheit dieser Erscheinung und dieser Bewegungen niemals gesehen habe, wozu dann noch der Schmelz der Stimme kam, ein Wohllaut, weit über allen Gesang. […]“
Sie vertrat in Vollendung einen Theaterstil, der bald nach ihrer Zeit ebenfalls Vergangenheit war, einen romantischen Stil überschwänglicher Deklamation und großer Gebärden. Sie selbst äußerte sich über ihre Arbeit:
„Man hat mich oft gefragt, wie viele Stunden ich täglich arbeite. Aber ich erarbeite mir eine Rolle nicht vollständig. Ich gehe mechanisch vor, lerne Wort für Wort auswendig, drehe und wende die Textstellen, bis ich sie auch im schnellen Dialog absolut beherrsche. Dann, wenn ich erst einmal meinen Text ganz genau kenne […], mache ich mir keine Gedanken mehr darüber. Alles, was ich an Schmerz, Leidenschaft oder Freude zeigen muss, ergibt sich im Ablauf des Stückes. […] Man sollte nach einer bestimmten Körperhaltung, nach der Art und Weise eines Ausrufs nicht suchen, keinesfalls! Man sollte sie auf der Bühne finden …“
Ihre Begabung für emotionales Schauspiel gab ihr die Möglichkeit, als große Tragödin in klassischen Dramen ebenso wie in modernen Gesellschaftsstücken zu überzeugen. Sie hatte triumphale Auftritte als Phädra in der gleichnamigen Tragödie von Racine, aber auch in den romantischen Dramen Ruy Blas und als Dona Sol in Hernani von Victor Hugo. Bewundert und bestaunt wurde ihre Interpretation von Hosenrollen. So spielte sie 1896 die Titelrolle in dem Drama Lorenzaccio von Alfred de Musset und George Sand, 1899 den Hamlet in Shakespeares gleichnamiger Tragödie und 1901 den Herzog von Reichstadt in L’Aiglon („Der junge Adler“); Edmond Rostand hatte dieses Stück über den Sohn Napoleon Bonapartes eigens für sie geschrieben.
Die prägende Rolle ihres Lebens war die der Kameliendame in dem Stück von Alexandre Dumas dem Jüngeren. Sarah Bernhardt spielte sie von 1880 an wieder und wieder, bis ins hohe Alter. Der Maler und Plakatkünstler Alfons Mucha, mit dem sie seit 1894 zusammenarbeitete, schuf 1896 für Sarah Bernhardt als Kameliendame ein Plakat, das vielfach als einer der frühen Höhepunkte der Jugendstil-Grafik betrachtet wird.
Ihre Filme wurden teils hymnisch kommentiert, aber, vor allem in der Rückschau späterer Generationen, auch kritisch beurteilt, so z. B. Königin Elisabeth von England[9]:
„Sarah Bernhardts berühmtester Film vermittelt nur eine blasse Ahnung von ihrem wirklichen schauspielerischen Vermögen. Die Darstellungskunst der alternden Diva ist stark überzogen, und der nach dem Konzept der Film d‘Art realisierte Film machte keine Anstalten, Bühnentraditionen in den Film zu übersetzen. Sein großer Erfolg beruhte auf der Tatsache, daß er erstmals ein weltberühmtes Phänomen einem breiten Publikum vorstellte …“
Sarah Bernhardt übersetzte Theaterstücke, schrieb Romane und ein Buch über das Schauspiel; ihre Memoiren veröffentlichte sie bereits 1907.
Bernhardt zählt zu den ersten Künstlern, die in dem damals neu aufgekommenem Medium Film aufgetreten sind. Diese sind teilweise heute noch erhalten.
Bernhardts eigenes Leben war Thema des Films Die unglaubliche Sarah (The Incredible Sarah, 1976) mit Glenda Jackson in der Titelrolle.

Frank Lloyd Wright (* 8. Juni 1867 in Richland Center, Wisconsin; † 9. April 1959 in Phoenix, Arizona) war ein US-amerikanischer Architekt, Innenarchitekt, Schriftsteller und Kunsthändler. Er wurde berühmt für seine Gestaltungsphilosophie, die eine Harmonie von Mensch und Natur anstrebt, die er selbst zur organischen Architektur zählte. Seine Vision für Usonia und der von Wright stark geprägte Prairie Style haben Architektur und suburbane Planungen in den USA des 20. Jahrhunderts mitgeprägt. Eine Auswahl der von Wright geplanten Bauten gehört seit 2019 zum UNESCO-Weltkulturerbe.
Wright wuchs im ländlichen Wisconsin auf und studierte ab 1885 an der University of Wisconsin, verließ diese aber ohne Abschluss 1887 – den Ehrendoktortitel der Universität erhielt er 1955. In der Folge trat er in das Architekturbüro von Joseph Lyman Silsbee in Chicago ein, verließ die Firma aber noch im selben Jahr, um im Büro von Dankmar Adler und Louis Sullivan zu arbeiten.[1]
1893 gründete er eine eigene Firma an seinem neuen Wohnort Oak Park, einem Vorort von Chicago. Bis 1901 hatte er etwa 50 Projekte erarbeitet. Bis etwa 1910 schuf er vornehmlich sogenannte Prairie Houses. Zu seinen Angestellten gehörten unter anderem Walter Burley Griffin und dessen spätere Ehefrau Marion Mahony Griffin.[2]
Der enge Kontakt mit der Landschaft seines Heimatstaates Wisconsin war in seiner späteren Arbeit zentral: Die möglichst nahtlose Integration des Bauwerkes in die Landschaft ist eines der Motive seines immensen Schaffens. Diese Gestaltungsphilosophie kommt wohl am besten in Wrights bekanntestem Werk, der für Edgar J. Kaufmann an einem kleinen Wasserfall erbauten Villa Fallingwater, zur Geltung. Weitere bekannte Entwürfe sind das in Verbindung mit Hilla von Rebay verwirklichte Solomon R. Guggenheim Museum in New York und das Verwaltungsgebäude für die Johnson Wax Company.
Wright ging es nach Jahrhunderten der kulturellen Abhängigkeit Amerikas vom alten Kontinent darum, eine unabhängige Architektur des neuen Kontinents zu etablieren. Seine so genannten Prairie Houses sollten Ausdruck des amerikanischen Geistes von Demokratie, Pioniergeist und Zusammenhalt sein. Das wichtigste Element war die Feuerstelle als Treffpunkt der Gemeinschaft, um den herum sich das Gebäude entwickelt. Harmonisch bettet sich das Ganze in die umliegende Landschaft ein.
Wright war einer der ersten Architekten, die den Begriff einer „organischen Bauweise“ benutzten. Es ging ihm dabei um einen organischen Zusammenhang der Architektur mit den verschiedenen Elementen der Kunst, der Natur und der menschlichen Lebensbereiche, also weniger im biologischen Sinne.
Unter dem Namen Taliesin gründete Wright mehrere „Ateliers“ mitten in der amerikanischen Prärie, die als Entwicklungsplattformen für die neue, unabhängige amerikanische Architektur dienen sollten. Die Architektin Cornelia Brierly war 1934 die erste weibliche Stipendiatin von Wright in Taliesin. Wrights Büro war ein Anziehungspunkt für Jungarchitekten aus aller Welt, die bei ihm zeitweise arbeiteten und seinen Stil später in ihren Heimatländern weiterentwickelten, so auch auf dem alten Kontinent wie zum Beispiel Werner Max Moser, auch Bruce Goff war sein Schüler und der österreichische Architekt und Adolf-Loos-Schüler Leopold Fischer, der aufgrund der Nürnberger Rassegesetze 1936 vor den Nationalsozialisten nach Los Angeles auswanderte, arbeitete von 1938 bis 1940 auf Empfehlung von Arnold Schönberg mit Frank Lloyd Wright zusammen. 1937 besuchte Wright für 3 Wochen die UdSSR[3]
In seinem 1945 erschienenen Buch When democracy builds (deutsch 1950) erstellt Frank Lloyd Wright eine Art utopischen Masterplan für die Zivilisation des 20. Jahrhunderts.
„Usonien“ (Usonia) nennt Wright sein visionäres Amerika. Es besteht aus einer von ihm erdachten neuen urbanen Form: Broadacre City (Weite Stadt). Nur hier genössen die Menschen „wahre Individualität“ (und nicht „robuste Individualität“ = Egoismus) in einer echten ganzheitlichen Demokratie. Kennzeichnend für diese Vision sind die Werte der Unabhängigkeitserklärung. Da alle potentiellen Führer (Politiker, Philosophen, Künstler, …) versagt hätten, wird Usonien von einem Architekten geleitet. Die Regierung nimmt nur noch Verwaltungsaufgaben wahr.
Freundschaft und Faszination empfand Wright für den schillernden kaukasischen Esoteriker Georges I. Gurdjieff, einen bereits zu seiner Zeit kontrovers diskutierten charismatischen Weisheitslehrer, den er über seine dritte Frau Olgivanna kennenlernte, die in den 20er Jahren Gurdjieffs Schülerin gewesen war.[4]
Wright war neben seiner Tätigkeit als Architekt und Schriftsteller auch als Kunsthändler und Sammler tätig. Schon bei seiner ersten Reise nach Japan im Jahre 1905 erwarb er einen großen Bestand an Holzschnitten des Ukiyo-e-Genres, den er zu einem Teil für die Einrichtung eigener Anwesen, zum anderen für den gewinnbringenden Weiterverkauf verwendete. Mit Geschick verstand er es nach Errichtung der von ihm projektierten Gebäude, dass die Bauherren dazu auch passende Kunstgegenstände kauften. Zeitweilig waren die Erträge aus dem Kunsthandel höher als aus der Architektentätigkeit.[5]
Im Gegensatz zu allen anderen großen amerikanischen Architekten seiner Zeit hat Frank Lloyd Wright nur wenige offizielle Ehrungen oder Auszeichnungen in Form von Architektur-Preisen oder Ehrendoktoraten erhalten. An Mitgliedschaften in den einschlägigen Organisationen ist seine Wahl zum assoziierten Mitglied (ANA) der National Academy of Design im Jahre 1952 belegt[6] sowie seine Mitgliedschaft seit 1947 in der American Academy of Arts and Letters[7] belegt. 1953 erhielt er die Gold Medal in Architecture dieser Akademie.
Im Februar 2015 wurden zehn Gebäude Wrights für die Aufnahme in das UNESCO-Weltkulturerbe nominiert.[8] Im Juli 2019 wurde eine abgeänderte Liste mit acht Gebäuden zum Weltkulturerbe erklärt.[9][10] Dies sind:
Nach diesen Prinzipien orientierte sich Wrights Architektur:
Seine Prinzipien der organischen Architektur blieben nicht statisch und änderten sich meist allmählich.[11]
Studio und Wohnhaus von Frank Lloyd Wright
William H. Winslow House (1893)
Robie House (1908)
Unity Temple (1908)
Imperial Hotel (1916)
Fallingwater (1937)
V. C. Morris Gift Shop (1948)
Solomon R. Guggenheim Museum (1956)
Price Tower (1956)
Annunciation Greek Orthodox Church (1959)
chronologisch
Der Song So long, Frank Lloyd Wright des amerikanischen Popsänger-Duos Simon and Garfunkel ist keine Hommage an Wright, sondern bezieht sich auf den Wright-Fan Garfunkel, der das Duo verlassen wollte, um verstärkt als Schauspieler zu arbeiten.[22]
Der United States Postal Service gab ihm zu Ehren 1966 eine 2-Cent-Briefmarke heraus, ein Dreiviertelportrait vor dem Guggenheim Museum New York.

Jean-Baptiste Poquelin alias Molière (getauft am 15. Januar 1622[1] in Paris; † 17. Februar 1673 ebenda) war ein französischer Schauspieler, Theaterdirektor und Dramatiker.
Er ist einer der großen Klassiker und machte die Komödie zu einer der Tragödie potenziell gleichwertigen Gattung. Vor allem erhob er das Theater seiner Zeit zum Diskussionsforum über allgemeine menschliche Verhaltensweisen in der Gesellschaft.
Molière ist ein Künstlername, den der Schauspieler und spätere Autor wohl ab 1643, spätestens jedoch seit Juni 1644 benutzte. Die Herkunft des Namens ist unklar, vielleicht stand eine gleichnamige südfranzösische Ortschaft Pate. Geboren wurde Molière als ältester Sohn eines wohlhabenden Pariser Händlers für Heimtextilien (tapissier), der 1631 das Amt eines Tapissier du Roi kaufte, d. h. eines königlichen Dekorateurs und Raumausstatters.
Mit zehn Jahren verlor der Junge seine Mutter, mit knapp 15 dann auch seine Stiefmutter, beide starben im Kindbett. Die Schulzeit absolvierte er auf dem von Jesuiten geführten Pariser Collège de Clermont, wo er eine solide klassische Bildung erhielt und einige Mitschüler hatte, die später für ihn eine besondere Rolle spielten. Sein Großvater mütterlicherseits, ein Theaterliebhaber, nahm ihn immer wieder zu Aufführungen mit, insbesondere zum volkstümlichen Jahrmarktstheater (théâtre de la foire), wo er Einblicke in eine Welt erhielt, die ihn früh faszinierte.
Mit knapp 16 legte er den Amtseid als künftiger Nachfolger seines Vaters im Tapissier-Amt ab und studierte wenig später Jura in Orléans. Zurück in Paris erhielt er die Zulassung als Anwalt. Ob er je als solcher tätig war, ist nicht bekannt. Um dieselbe Zeit frequentierte er die Vorlesungen des Naturforschers und Philosophen Pierre Gassendi, was ihm eine gewisse Distanz zu den Dogmen der Kirche vermittelte. Offenbar verfasste er damals eine Vers-Übertragung von De rerum natura des römischen Philosophen Lukrez, die aber verlorengegangen ist.
1641 oder 1642, also um die 20, lernte er die vier Jahre ältere Schauspielerin Madeleine Béjart kennen, die ihn in seinem Drang zum Theater bestärkte – gegen den Willen seines Vaters, der ihn im Sommer 1642 nötigte, in Ausübung seines Tapissier-Amtes Ludwig XIII. auf einer längeren Reise zu begleiten und ihm die wechselnden Nachtquartiere einzurichten.
1643 übertrug Molière das ungeliebte Amt seinem jüngeren Bruder, ließ sich einen Vorschuss auf das Erbe seiner Mutter auszahlen und gründete, noch unter dem Namen Poquelin, gemeinsam mit Madeleine Béjart, ihren Geschwistern Louis und Geneviève sowie fünf weiteren Komödianten mit Vertrag vom 30. Juni 1643[2] eine Theatergruppe, das L’Illustre Théâtre. Dieses ging 1645 bankrott und Molière wurde vorübergehend in Schuldhaft genommen. In der Folge schloss er sich mit den Béjarts der Wandertruppe des Schauspielers Charles du Fresne an, die vom Duc d’Épernon finanziell unterstützt wurde und hauptsächlich in West- und Südfrankreich auftrat.
Bald stieg er zum Direktor der Truppe auf und gewann 1653 für einige Jahre den Gouverneur des Languedoc als Förderer, den Prince de Conti. Eine Legende ist wohl die von La Grange in die Welt gesetzte Behauptung, jener habe mit Molière zusammen das Collège de Clermont besucht.[3] Das Repertoire der Truppe umfasste neben Tragödien, Tragikomödien und Komödien zeitgenössischer Autoren auch komische Farcen und lustige Theaterstücke im Stil der italienischen Commedia dell’arte. Ab 1655 nahm Molière auch eigene Werke ins Programm, z. B. die in Versen verfasste Komödie L’Étourdi ou Les Contretemps (Der Tollpatsch oder die Querstreiche), in der ein gewitzter und pfiffiger Diener und sein notorisch ungeschickter junger Herr die Hauptrollen spielen.
Nach 13 Wanderjahren, in denen er Menschen aus allen Schichten kennengelernt hatte und sein Handwerk als Schauspieler, Theaterdirektor und schließlich auch Autor von Grund auf gelernt hatte, gastierte Molière 1658 in Rouen, wo er dem berühmten Dramatiker Pierre Corneille begegnete. Vor allem aber kam er hier in Kontakt mit „Monsieur“, d. h. dem jüngeren Bruder von Ludwig XIV., Herzog Philippe I. d’Orléans. Dieser lud die Truppe an den Hof nach Paris ein, wo Molière die Tragödie Nicomède von Corneille und seine eigene Farce Le médecin amoureux (Der verliebte Arzt) aufführte. Letztere gefiel dem jungen, gerade erst 20-jährigen König so sehr, dass er der Truppe erlaubte, im Saal des an den Louvre grenzenden Hôtel du Petit-Bourbon zu spielen. Die Sonntage, Dienstage und Freitage gehörten dort allerdings schon einer italienischen Truppe um den Komödianten Tiberio Fiorilli (1608–1694), der wegen seiner Paraderolle als Scaramouche berühmt war.
Den Durchbruch erzielte Molière im November 1659 mit seiner in Prosa verfassten Komödie Les précieuses ridicules (Die lächerlichen feinen Damen), seinem ersten für ein überwiegend Pariser Publikum konzipierten Stück. Am Beispiel der beiden Protagonistinnen, zweier etwas exaltierter, möchte-gern-adelig und gebildet tuender Bürgermädchen, verspottet er hier die gekünstelte Sprechweise und die wirklichkeitsfremden Denkweisen der Preziösen, wie sie inzwischen auch im Bürgertum zu finden waren. Der Erfolg des Stücks verschaffte ihm erste Neider, das Thema erste Feinde, darunter den Chef der Verwaltung der königlichen Schlösser, der pünktlich zu Beginn der Spielzeit 1660/61 den Abriss des Petit-Bourbon verfügte. Molière blieb drei Monate ohne Spielstätte, bis er vom König den Saal des Palais Royal zugewiesen bekam.
Ein weiterer Schlag war 1661 der komplette Misserfolg der Tragikomödie Dom Garcie de Navarre, mit der Molière sich offenbar dem gehobenen Genus der Tragödie anzunähern gedachte.
Mit dem zentralen Thema des Stücks, der exzessiven Eifersucht, bearbeitete er sicher aber auch ein persönliches Problem, denn der 40-Jährige umwarb zu dieser Zeit die offenbar kokette 18-jährige Armande Béjart, die jüngste Schwester (oder Tochter?) von Madeleine und ebenfalls Schauspielerin in seiner Truppe.
Die Trauung von Armande Béjart und dem 20 Jahre älteren Molière fand am 20. Februar 1662 in der Pfarrkirche Saint-Germain-l’Auxerrois statt. Das Paar bekam drei Kinder: Louis (* Januar 1664), der im Alter von 8 Monaten starb, Esprit-Madeleine (* August 1665, † ohne Nachkommen 1723) und Pierre-Jean-Baptiste-Armand (* 1672), der nur einige Tage alt wurde. Die Ehe Armandes mit Molière war nicht immer glücklich. Ab 1666 lebte sie eine Weile getrennt von ihm.
Der nächste große Erfolg war Ende 1662 L’École des femmes (Die Schule der Frauen), eine Verskomödie, in der Molière (dem soeben Armande ihr Jawort gegeben hatte) für eine gemäßigte Emanzipation der jungen Frauen wirbt und für ihr Recht auf eine Liebesheirat. Die heftige Kontroverse, die er hiermit auslöste, heizte er 1663 weiter mit den Prosastücken La Critique de l’École des femmes (Kritik der Schule der Frauen) und L’Impromptu de Versailles (Das Impromptu von Versailles) an. Dem König scheint dies gefallen zu haben, denn er setzte Molière eine jährliche Pension von 1000 Livres aus. Im Januar 1664 wurde der König sogar Taufpate Molières ersten (allerdings bald danach verstorbenen) Kindes Louis, was er wohl auch deshalb tat, um das Gerücht zu widerlegen, Armande sei ein Kind Madeleine Béjarts und Molières und dieser habe somit seine eigene Tochter geheiratet.
In den Jahren 1663 bis 1665 wurde Molière für kurze Zeit zum Protektor des noch unbekannten Nachwuchsdramatikers Jean Racine. Er beauftragte ihn mit einer Tragödie über den Ödipus-Stoff, die er Anfang 1664 wenig erfolgreich inszenierte unter dem Titel La Thébaïde. Ou les frères ennemis (Die Thebais. Oder die feindlichen Brüder). 1665 spielte er mit immerhin mäßigem Erfolg Racines Tragikomödie Alexandre le Grand.
Molière erlebte allerdings, dass der mit der Inszenierung unzufriedene Jungautor mit seinem Stück zu der Truppe des Hôtel de Bourgogne abwanderte, die auf Tragödien spezialisiert war. Dabei nahm Racine eine von Molières beliebtesten Schauspielerinnen mit, Mademoiselle du Parc, die sich mit Racine liiert hatte und ihm zur Konkurrenz folgte. Das Verhältnis der beiden Männer war hiernach naturgemäß gespannt. Molière rächte sich, indem er in der Folgezeit häufig ältere Stücke von Racines Rivalen Pierre Corneille wieder aufnahm oder neue uraufführte.
Im Mai 1664 – inzwischen war er zum Vergnügungsdirektor Ludwigs XIV. avanciert – organisierte Molière ein mehrtägiges Hoffest im neuangelegten Park von Versailles. Dort spielte er zunächst, mit Balletteinlagen, die sein jüngerer Freund Jean-Baptiste Lully komponiert und choreographiert hatte, die unverfänglichen (eigenen) Komödien La Princesse d’Élide (Die Fürstin von Elis), Le Mariage forcé (Die Zwangsheirat) und Les Fâcheux. Am sechsten Tag führte er eine neue Verskomödie in drei Akten auf, die zum Politikum wurde: Tartuffe.
Schon im Vorfeld hatten etliche fromme Höflinge die Aufführung dieses Stücks um einen scheinbar strenggläubigen, in Wahrheit aber herrschsüchtigen, raffgierigen und lüsternen Schwindler zu verhindern versucht. Nach der Aufführung brach Empörung beim gesamten „alten Hof“ aus, einer Gruppierung meist älterer Höflinge, die sich um die fromme Königinmutter Anna von Österreich scharten und der Zeit vor 1661 nachtrauerten, wo man unter ihr und ihrem Minister Kardinal Mazarin die Macht gehabt hatte. Dem König war Molières Attacke auf die auch ihm lästigen Frömmler zunächst sehr recht gewesen, unter dem Druck des „alten Hofes“ hielt er es aber doch für geraten, das Stück zu verbieten. Die nächsten Jahre Molières waren bestimmt von seinem Kampf für den Tartuffe und gegen die Intrigen des „Klüngels der Frommen“, wie er sie nannte. Diese waren teilweise in einem bigotten Geheimbund organisiert, der Compagnie du Saint-Sacrement, der auch sein ehemaliger Gönner Conti angehörte, der nach einer Syphilisinfektion fromm geworden war.
Molière verfolgte unterdessen das Thema der Heuchelei weiter: Ende 1664, also bald nach dem ersten Verbot des Tartuffe, verfasste er Don Juan, ein Prosastück über einen hochadligen Heiratsschwindler, Betrüger und Libertin, der, um sich den Nachstellungen empörter Geschädigter zu entziehen, eine Bekehrung zu christlicher Moral und Frömmigkeit heuchelt, aber schließlich zur Hölle fährt. Auch dieses Stück wurde nach wenigen Aufführungen verboten, vermutlich wegen der nicht eindeutig negativen Darstellung von Don Juans Freidenkertum.
Immerhin sah sich Molière vom König insofern unterstützt, als er im Sommer 1665 seine Jahrespension von 1000 auf 6000 Livre erhöht bekam und mit seiner Truppe den Titel Troupe du roi annehmen durfte, beides kurz nach der Geburt seiner Tochter Esprit-Madeleine, die als einziges Kind überleben sollte.
Im Juni 1666 brachte Molière die Verskomödie Le Misanthrope (Der Menschenfeind) heraus, eine Satire auf die unehrliche Schmeichelei am Hof und die geheuchelte Nettigkeit in den Pariser Salons. Die ungewöhnlich stark autobiographisch geprägte Figur des Misanthropen Alceste, von Molière selbst gespielt, spiegelt sichtlich dessen eigenes Unvermögen und seine Unlust wider, sich auf dem glatten Parkett der Hofgesellschaft opportunistisch und angepasst zu verhalten. In der enttäuschten Liebe Alcestes zu der koketten jungen Célimène spiegelt sich die Enttäuschung Molières über seine 20 Jahre jüngere Frau Armande wider, die sich gerade (vorübergehend) von ihm getrennt hatte.
Im Sommer 1667 versuchte er eine auf fünf Akte verlängerte, überarbeitete und in L’Imposteur (Der Schwindler) umbetitelte Version des Tartuffe in sein Programm aufzunehmen, wobei er den Protagonisten in „Panulphe“ umbenannte und nicht mehr priesterähnlich, sondern als Adeligen kostümierte. Doch der Präsident des Parlement de Paris, der für den auf einem Feldzug in Flandern befindlichen König die Polizeigewalt ausübte, reagierte sofort mit einem Verbot; der Erzbischof von Paris drohte Molière sogar mit Exkommunikation. Als dieser zwei Schauspieler mit einer Bittschrift zum König schickte, signalisierte der zwar Wohlwollen, tat aber nichts. Immerhin duldete er, dass sein Bruder Philippe und danach der Fürst de Condé (der ältere Bruder Contis) 1668 das Stück in ihren Schlössern privat aufführen ließen.
Nach dem Verbot auch der zweiten Tartuffe-Version übte Molière 1668 in der Verskomödie Amphitryon erstmals leise Kritik an seinem wenig zuverlässigen Gönner Ludwig, den er verschlüsselt in der Rolle Iupiters ganz ungeniert seinem sexuellen Lustgewinn nachgehen lässt. In George Dandin (Prosa, ebenfalls 1668) brandmarkte er die Arroganz, mit der Adlige, selbst wenn sie verarmt sind, glauben, die gesellschaftlich nützliche Bourgeoisie verachten und ausbeuten zu dürfen.
Erst am 5. Februar 1669, nachdem der „alte Hof“ nach Annas Tod 1666 endgültig entmachtet, die Compagnie du Saint-Sacrement verboten und Ludwigs Macht nach innen- und außenpolitischen Erfolgen so gefestigt war, dass er keine Rücksicht mehr auf die frommen Gegner Molières nehmen musste, konnte dieser das nochmals überarbeitete, nun als Tartuffe, ou l’Imposteur betitelte Stück frei aufführen. Die Aufführung war ein triumphaler Erfolg und gilt als eines der großen Ereignisse der französischen Theatergeschichte.
Insgesamt aber hatte sich Molière nach 1667 auf unverfänglichere Themen zu verlegen begonnen. Mit gefälligen Stücken, insbesondere sogenannten Ballettkomödien zu Musik von Lully, versuchte er sein Theater zu füllen und den König bei Laune zu halten. Neben etlichen anderen, heute vergessenen Stücken schrieb er:
Diese letzten Lebensjahre Molières waren gekennzeichnet von einem sich stetig verschlechternden Gesundheitszustand, bedingt durch den beruflichen Stress sowie das lange Hin und Her um den Tartuffe. Häufige Eheschwierigkeiten setzten ihm zusätzlich zu. 1671 kam es bei der Einstudierung der Ballett-Tragödie Psyché (deren letzte zwei Drittel Corneille verfasst hatte) zum Bruch mit Partner Lully. Anfang 1672 erkrankte und verstarb seine langjährige Weggefährtin Madeleine Béjart. Ende desselben Jahres starb ein drittes Kind bald nach der Geburt, und Molière musste erleben, wie Lully zum Rivalen wurde, den der König vorzuziehen begann.
Le Malade imaginaire sollte in bitterer Ironie sein letztes Stück bleiben und die Hauptrolle des eingebildeten Kranken seine letzte Rolle. Während der vierten Aufführung am 17. Februar 1673 erlitt er einen Schwächeanfall inklusive Blutsturz, den die Zuschauer zunächst für eine Einlage innerhalb der Komödie hielten. Wenig später starb er in seiner nahe gelegenen Wohnung. Nur mühsam gelang es seiner Frau Armande, den Widerstand des Gemeindepfarrers zu brechen und über den König beim Erzbischof von Paris zu erreichen, dass eine halbwegs ehrbare Bestattung auf einem kirchlichen Friedhof genehmigt wurde.
Die Truppe Molières blieb unter Armandes Leitung zunächst bestehen. Sie schloss sich aber bald, als Rivale Lully den Saal des Palais Royal zugesprochen bekam, der Truppe des Théâtre du Marais an, wobei Armande einen von deren Schauspielern heiratete. 1680 verschmolz die neue Truppe auf Anweisung von Ludwig XIV. mit der Truppe des Hôtel de Bourgogne: Die noch heute bestehende Comédie-Française war geboren.
Erst 1752 wurden seine Lustspiele ins Deutsche übersetzt. Der Asteroid des äußeren Hauptgürtels (3046) Molière wurde nach ihm benannt.[4]
Die Autorschaft der Werke von Molière ist nicht unumstritten. Im Zentrum dieser Kontroverse steht die Frage, ob Pierre Corneille, Schöpfer der französischen Tragödie, einige Werke, welche traditionellerweise Molière zugesprochen werden, als sein Ghostwriter verfasst hat.[5]
Im 21. Jahrhundert wird versucht, diese Streitfrage mit Hilfe mathematisch-stilometrischer Verfahren der Computerphilologie zu entscheiden.[6]
In alphabetischer Reihenfolge:
Folgende Komödianten waren – in der chronologischen Reihenfolge ihrer Aufnahme – in Molières Truppe:
Der fliegende Doktor (1645) ·
Die Eifersucht des Angeschmierten (1650) ·
Der Unbesonnene oder Zur Unzeit (1655) ·
Der Liebesverdruss (1656) ·
Der verliebte Arzt (1658) ·
Die lächerlichen Preziösen (1659) ·
Sganarell oder Der vermeintliche Hahnrei (1660) ·
Don Garcia de Navarra oder Der eifersüchtige Fürst (1661) ·
Die Schule der Ehemänner (1661) ·
Die Plagegeister (1661) ·
Die Schule der Frauen (1662) ·
Die Eifersucht von Gros-René (1663) ·
Das Impromptu von Versailles (1663) ·
Die Kritik der „Schule der Frauen“ (1663) ·
Die Zwangsheirat (1664) ·
Die Fürstin von Elis (1664) ·
Die Liebe als Arzt (1665) ·
Don Juan oder Der steinerne Gast (1665) ·
Der Arzt wider Willen (1666) ·
Der Menschenfeind (1666) ·
Mélicerte (1666) ·
Das komische Hirtengedicht (1667) ·
Der Sizilier oder Die Liebe als Maler (1667) ·
George Dandin (1668) ·
Amphitryon (1668) ·
Der Geizige (1668) ·
Tartuffe (1669) ·
Monsieur de Pourceaugnac (1669) ·
Die großartigen Liebhaber (1670) ·
Der Bürger als Edelmann (1670) ·
Psyché (1671) ·
Die Gräfin von Escarbagnas (1671) ·
Scapins Streiche (1671) ·
Die gelehrten Frauen (1672) ·
Der eingebildete Kranke (1673)

Wolfgang Amadeus Mozart (* 27. Jänner 1756 in Salzburg, Erzstift Salzburg;[1] † 5. Dezember 1791 in Wien[2]), der überwiegend mit Wolfgang Amadé Mozart unterschrieb, war ein Salzburger[3] Musiker und Komponist der Wiener Klassik. Sein umfangreiches Werk genießt weltweite Popularität und gehört zum Bedeutendsten im Repertoire klassischer Musik.
Wolfgang Amadeus Mozart kam am 27. Jänner 1756 um acht Uhr abends in Salzburg in der Getreidegasse 9 in einer Dreizimmerwohnung eines Mehrfamilienhauses (Hagenauerhaus) auf die Welt und wurde am nächsten Vormittag um zehn Uhr im Salzburger Dom von Stadtkaplan Leopold Lamprecht auf die Namen Joannes Chrysostomus Wolfgangus Theophilus getauft und so im Taufbuch eingetragen (sein Vater Leopold Mozart verwendete die Namensform Joannes Chrisostomus Wolfgang Gottlieb).[4] Er wurde Wolferl, Wolfgang oder auch Woferl gerufen.[5] Das Wolferl war das siebte Kind seiner Eltern, aber erst das zweite, das überlebte. Seine Geschwister hießen Johannes Leopold Joachim (* 1748, starb im sechsten Lebensmonat), Maria Anna Cordula (* 1749, wurde sechs Tage alt), Maria Anna Nepomucena Walburga (* 1750, starb im dritten Lebensmonat), Maria Anna Walburga Ignatia – das Nannerl (* 1751, wurde 78 Jahre alt), Johann Baptist Karl Amadeus (* 1752, wurde nicht ganz drei Monate alt) und Maria Crescentia Franziska de Paula (* 1754, starb im zweiten Lebensmonat). Sein Vater war der aus Augsburg zum Studium[6] an der Benediktineruniversität (1622–1810)[7] nach Salzburg gezogene, fürstbischöfliche Kammermusikus (ab 1757 Hofkomponist und ab 1763 Vizekapellmeister) Leopold Mozart, seine Mutter die in Sankt Gilgen aufgewachsene Anna Maria Pertl.
Bereits im Alter von vier Jahren erhielten er und seine fünf Jahre ältere Schwester Maria Anna Mozart, das Nannerl genannt, vom Vater den ersten Musik- und allgemeinbildenden Unterricht in Klavier, Violine (→ Mozarts Kindergeige)[8] und Komposition. Schon 1761 zeichnete Vater Leopold ein Andante und ein Allegro als des „Wolfgangerl Compositiones“ auf, denen ein Allegro und ein Menuetto folgten, datiert auf den 11. bzw. 16. Dezember 1761. Das fälschlicherweise immer wieder als früheste Komposition genannte Menuett G-Dur mit einem Menuett C-Dur als Trio KV 1 entstand vermutlich erst 1764. Auch Mozarts Begabung im Klavier- und Violinspiel trat schnell hervor. 1762 folgten seine ersten Auftritte.
Erste Konzertreisen Wolfgangs und seiner Schwester Nannerl mit den Eltern wurden Anfang 1762 nach München und Herbst 1762 von Passau nach Wien arrangiert, um dem Adel die talentierten Kinder zu präsentieren. Nach dem Erfolg der Wunderkind-Geschwister in München und Wien startete die Familie am 9. Juni 1763 zu einer ausgedehnten Tournee durch die deutschen Lande und Westeuropa, die bis zur Rückkehr nach Salzburg am 29. November 1766 dreieinhalb Jahre dauerte. Wichtige Stationen waren München, Augsburg, Ludwigsburg, Schwetzingen, Heidelberg, Mainz, Frankfurt am Main, Koblenz, Köln, Aachen, Brüssel, Paris (Ankunft am 18. November 1763), Versailles, London (Ankunft am 23. April 1764), Dover, Belgien, Den Haag (September 1765), Amsterdam, Utrecht, Mechelen, erneut Paris (Ankunft 10. Mai 1766), Dijon, Lyon, Genf, Lausanne, Bern, Zürich, Donaueschingen, Ulm und München, wo die Kinder bei Hofe oder in öffentlichen Akademien musizierten. Während dieser Reisen entstanden die ersten Sonaten für Klavier und Violine sowie die erste Sinfonie Es-Dur (KV 16). Die vier Sonaten für Klavier und Violine KV 6 bis 9 sind 1764 die ersten gedruckten Kompositionen Mozarts.
Im Laufe dieser Reise wurde Mozart in London mit der italienischen Symphonie und Oper vertraut gemacht. Dort lernte er zudem Johann Christian Bach kennen, der sein erstes Vorbild wurde. 1778 schrieb Mozart aus Paris nach dem dortigen Wiedersehen nach Hause: „ich liebe ihn (wie sie wohl wissen) von ganzem herzen – und habe hochachtung vor ihm.“
Nach der Rückkehr folgten erste Uraufführungen in Salzburg, darunter auch die Schuloper Die Schuldigkeit des ersten Gebots, die der elfjährige Mozart zusammen mit den wesentlich älteren Salzburger Hofmusikern Anton Cajetan Adlgasser und Michael Haydn komponiert hatte. Im September folgte eine zweite Reise mit der Familie nach Wien. Um der grassierenden Pockenepidemie zu entgehen, fuhren sie nach Brünn und Olmütz.[9] Die Krankheit erreichte aber Wolfgang und seine Schwester auch dort. Nach der Genesung der Kinder kehrte Mozart am 10. Jänner 1768 nach Wien zurück, wo er das Singspiel Bastien und Bastienne (KV 50), die Waisenhausmesse (KV 139) sowie die Opera buffa La finta semplice (KV 51) fertigstellte. Obwohl vom deutschen Kaiser Franz I. bestellt, konnte die Letztere nicht aufgeführt werden; Grund waren Intrigen der sogenannten „italienischen Partei“ um den Hofintendanten Giuseppe Affligio.
Zwischen 1767 und 1769 hielt sich Mozart wiederholt im Benediktinerkloster Seeon auf. Noch 1771 wurden von ihm dort Offertorien aufgeführt. Mozart schrieb speziell für das Kloster Seeon zwei Offertorien: Scande coeli limina (KV 34; 1769) und Inter natos mulierum (KV 72; 1771). Die sogenannte „Mozarteiche“, unter der er der Überlieferung nach gerne gesessen haben soll, wächst bis heute am Seeoner See.
Nach 15 Monaten in Wien kehrte Mozart mit seiner Familie am 5. Jänner 1769 nach Salzburg zurück. Hier wurde La finta semplice am 1. Mai endlich aufgeführt, und hier erlebte er am 27. Oktober mit der Berufung zum Dritten Konzertmeister der Salzburger Hofkapelle die erste, wenn auch unbesoldete Anstellung.
Knapp drei Wochen später, am 13. Dezember 1769, brach Mozart mit seinem Vater zu seiner ersten von drei außerordentlich erfolgreichen Italienreisen auf, die – mit Unterbrechungen von März bis August 1771 und Dezember 1771 bis Oktober 1772 – fast dreieinhalb Jahre dauerte.
Die erste Reise führte sie nach Verona, Mailand, Bologna, Florenz, Rom, Neapel, Turin, Venedig, Padua, Vicenza, Innsbruck und zurück nach Salzburg. Hier erholte sich Mozart bis zum Herbst, um danach zu einem längeren (dritten) Aufenthalt in Mailand zu starten. Von Papst Clemens XIV. wurde er 1770 in Rom zum Ritter vom Goldenen Sporn ernannt, doch machte er im Gegensatz zu Gluck von dem Privileg, sich „Ritter“ zu nennen, nie Gebrauch. In Rom gelang ihm, nachdem er nur ein- oder zweimal dem neunstimmigen Miserere von Gregorio Allegri zugehört hatte, das Grundgerüst dieser vom Vatikan streng geheim gehaltenen Partitur aus dem Gedächtnis fehlerfrei niederzuschreiben. Nicht klar ist, inwieweit die Sänger Stimmen improvisierend koloriert haben und ob Mozart das aufschreiben konnte. Das Original dieser Transkription ist nicht überliefert und jüngere Untersuchungen geben durchaus nachvollziehbare Erklärungen für diese scheinbar unerklärliche Leistung. Erleichtert wurde die Niederschrift etwa durch die Wiederholungsstruktur des Stücks.[10]
Bei Padre Giovanni Battista Martini in Bologna studierte Mozart Kontrapunkt. Nach einer Klausur wurde er in die Accademia Filarmonica di Bologna aufgenommen. Dort begegnete er so bedeutenden Musikern wie Giovanni Battista Sammartini, Niccolò Piccinni, Pietro Nardini und Giovanni Paisiello. Am 26. Dezember 1770 erlebte er die Uraufführung seiner Opera seria Mitridate, re di Ponto (KV 87) in Mailand, deren Publikumserfolg zu zwei weiteren Aufträgen führte: der Serenata teatrale Ascanio in Alba (KV 111, Uraufführung in Mailand am 17. Oktober 1771) sowie dem Dramma per musica Lucio Silla (KV 135), Uraufführung in Mailand in der Saison 1772/73. Am 15. Dezember 1771 kehrten Vater und Sohn nach Salzburg zurück, nachdem sich Hoffnungen auf eine Anstellung in Italien nicht erfüllt hatten.
Im Jahr 1772 wurde Hieronymus Franz Josef von Colloredo zum Fürsterzbischof von Salzburg gewählt; er folgte dem verstorbenen Sigismund Christoph Graf von Schrattenbach. Vom neuen Fürsten wurde Mozart im August zum besoldeten Konzertmeister der Salzburger Hofkapelle ernannt. Trotzdem führte dies nicht zu einem Ende seiner vielen Reisen mit dem Vater. Wolfgang versuchte weiterhin, dem engen Reglement des Salzburger Dienstes zu entkommen: Vom 24. Oktober 1772 bis zum 13. März 1773 folgte die dritte Italienreise zur Uraufführung des Lucio Silla, während der auch das Exsultate, jubilate entstand, und von Mitte Juli bis Mitte Ende September 1773 die dritte Reise nach Wien.[11] Im selben Jahr entstand sein erstes Klavierkonzert. Ab Oktober 1773 bewohnte die Familie Mozart den ersten Stock des Tanzmeisterhauses, das zuvor dem Salzburger Hoftanzmeister Franz Gottlieb Spöckner (ca. 1705–1767) gehört hatte.
Nach einer längeren Pause folgte am 6. Dezember 1774 eine Reise nach München zur Uraufführung der Opera buffa La finta giardiniera (KV 196). Am 13. Jänner 1775 und nach der Rückkehr am 7. März versuchte Mozart erneut, sich auch in Salzburg als Künstler der Musik zu etablieren. Er ließ zum Beispiel das Dramma per musica Il re pastore am 23. April 1775 in Salzburg uraufführen, das allerdings beim Publikum nicht gut ankam. Nach mehrfachen erfolglosen Bitten um Urlaub reichte er 1777 sein Abschiedsgesuch beim Fürsterzbischof ein und bat um Entlassung aus der Salzburger Hofkapelle.
Nach seiner Entlassung aus den Diensten des Fürsten begab sich Mozart am 23. September 1777 mit seiner Mutter auf eine Städtereise; er versuchte eine neue und bessere Anstellung zu finden. Zuerst sprach er vergeblich am bayerischen Kurfürstenhof in München vor, danach in Augsburg und am Hof des Mannheimer Kurfürsten Karl Theodor, wo er das kurfürstliche Orchester und dessen Kapellmeister, seinen späteren Freund Christian Cannabich, kennenlernte (siehe auch Mannheimer Schule). Aber auch hier bekam er weder eine Anstellung noch irgendwelche musikalischen Aufträge. Er lernte aber die Familie Weber kennen und deren Tochter Aloisia, eine junge Sängerin und spätere Münchner Primadonna, in die er sich verliebte.
Nach fünf Monaten in Mannheim fuhren er und seine Mutter, vom Vater gedrängt, weiter nach Paris, wo sie am 23. März 1778 ankamen. Dort konnte Mozart immerhin seine Ballettmusik Les petits riens aufführen, bekam darüber hinaus aber keine weiteren Engagements. Am 3. Juli 1778 verstarb seine Mutter um 10 Uhr abends. Der junge Mozart wohnte anschließend einige Monate in einer Wohnung des Barons Melchior Grimm, wo auch Joseph Bologne, Chevalier de Saint-Georges schon seit zwei Jahren lebte.[12] In Paris freundete sich Mozart auch mit dem dort ansässigen deutschstämmigen Komponisten und Verleger Jean-Georges Sieber an, wobei die Freundschaft später über Briefwechsel aufrechterhalten wurde.[13]
Die Rückreise nach Salzburg, die er widerwillig knapp drei Monate später am 26. September antrat, um die vakante Stelle eines Hoforganisten anzutreten, führte ihn über Straßburg, Mannheim und Kaisersheim nach München, wo er noch einmal der Familie Weber begegnete. Erst Mitte Jänner 1779 erreichte er seine Heimatstadt und wurde am 17. Jänner zum Hoforganisten ernannt. Hier komponierte er die später so genannte Krönungsmesse (KV 317).
Dieser erneute Versuch mit einem Engagement in Salzburg ging 20 Monate leidlich gut, obwohl das Verhältnis zum Erzbischof angespannt blieb, da dieser ihm die Mitwirkung an einträglichen Konzerten in Wien untersagte. Bei einer erneuten Reise am 5. November 1780 nahm er in München an der sehr erfolgreichen Uraufführung seiner Opera seria Idomeneo (KV 366) am 29. Jänner 1781 teil. Danach nahm Mozart in Wien im Auftrag des Erzbischofs an Akademien der Salzburger Hofmusiker teil. Nach zwei heftigen Auseinandersetzungen mit dem Erzbischof und einem „Fußtritt“ durch dessen gräflichen Abgesandten, den fürsterzbischöflichen Oberstküchenmeister Karl Joseph Maria Graf Arco[14] – über den gräflichen „Fußtritt“ berichtet Mozart selbst in seinen Briefen – kam es zum endgültigen Bruch. Mozart kündigte am 8. Juni 1781 den Salzburger Dienst auf, ließ sich in Wien nieder und bestritt dort in den nächsten Jahren seinen Lebensunterhalt durch Konzerte in privaten und öffentlichen Akademien.
Befreit von den Salzburger „Fesseln“, schuf der nun unabhängige Komponist und Musiklehrer, der ständig auf der Suche nach Auftraggebern und Klavierschülern war und der sich auch nicht scheute, auf „Vorrat“ zu arbeiten, die ganz großen Opern und eine Vielzahl von Klavierkonzerten, die er meist selbst vortrug.
In dieser Phase komponierte Mozart außerdem die Große Messe in c-Moll (KV 427) (1783) und wichtige Instrumentalwerke: die sechs Joseph Haydn gewidmeten Streichquartette (KV 387, 421, 428, 458, 464, 465) (1785), die Linzer Sinfonie (KV 425), die Prager Sinfonie (KV 504) (1786) und die Serenade Eine kleine Nachtmusik (KV 525) (1787) sowie die drei letzten Sinfonien, in Es-Dur (KV 543, Nr. 39), g-Moll (KV 550, Nr. 40) und in C-Dur, genannt Jupiter-Sinfonie (KV 551, Nr. 41).
In Wien lernte Mozart um 1782/83 Gottfried van Swieten kennen, einen ausgewiesenen Musikliebhaber und Präfekten der kaiserlichen Bibliothek, der heutigen Österreichischen Nationalbibliothek. Dieser machte ihn bei den regulären Sonntagskonzerten in van Swietens Räumen in der kaiserlichen Bibliothek mit den Manuskripten Johann Sebastian Bachs und Georg Friedrich Händels bekannt, die er in Berlin gesammelt hatte. Die Begegnung mit diesen Barockkomponisten machte einen tiefen Eindruck auf Mozart und hatte umgehend großen Einfluss auf seine Kompositionen.[15]
Am 4. August 1782 heiratete Mozart Constanze Weber,[16] eine jüngere Schwester Aloisias. Mozart hatte seine Frau drei Jahre zuvor in Mannheim kennengelernt. Sie bekamen in den folgenden Jahren sechs Kinder: Raimund Leopold († 19. August 1783), Carl Thomas (* 1784; † 31. Oktober 1858), Johann Thomas Leopold († 15. November 1786), Theresia Konstantia Adelheid Friderika (* 1787; † 29. Juni 1788), Anna Maria († 16. November 1789) und Franz Xaver Wolfgang (* 1791; † 29. Juli 1844). Lediglich Carl Thomas und Franz Xaver Wolfgang überlebten die Kinderzeit.
Vater Leopold Mozart, den Wolfgang in seiner Wiener Zeit 1783 ein letztes Mal in Salzburg besuchte[17] und der 1785 zu ihm nach Wien gekommen war, starb am 28. Mai 1787.
Als Originalverleger Mozarts trat mit Heinrich Philipp Boßler einer der bedeutendsten Musikverleger seiner Zeit auf. In Boßlers Verlagshaus erschienen unter anderem die Ouvertüren Le nozze di Figaro und Don Giovanni.[18] Weiterhin wirkte Heinrich Philipp Bossler als Impresario der begnadeten Virtuosin Marianne Kirchgeßner für deren Glasharmonikaspiel Mozart das Adagio und Rondo für Glasharmonika, Flöte, Klarinette, Viola und Violoncello (KV 617) und das Adagio (KV 356/617a) komponierte. Schon 1784 hatte H. P. Bossler, der Mozart persönlich kannte, einen Kupferstich mit dem Titel Signor Mozart angefertigt.[19] Impresario Boßler war es auch, der 1792 einen ausführlichen Nachruf auf Wolfgang Amadé Mozart publizierte und darin auf die schlechte Situation der mittellosen Kinder einging.[20]
Durch seine Freundschaft mit Otto Heinrich von Gemmingen-Hornberg trat Mozart am 14. Dezember 1784 in die Wiener Freimaurerloge Zur Wohltätigkeit ein. Mozart besuchte regelmäßig eine zweite Wiener Loge Zur wahren Eintracht, deren Meister der Illuminat Ignaz von Born war. Dort wurde er am 7. Jänner 1785 zum Gesellen befördert. Er konnte aber am 11. Februar nicht bei der Initiation seines Freundes Joseph Haydn anwesend sein, da er am selben Abend, an dem auch sein Vater Leopold Mozart aus Salzburg angekommen war, das erste seiner sechs Subskriptionskonzerte in der Mehlgrube gab und dabei den Solopart seines Klavierkonzertes in d-Moll KV 466 spielte. Auf Mozarts Veranlassung wurde auch sein Vater Leopold Mozart Freimaurer: Dieser wurde am Mittwoch, den 6. April 1785, in der Bauhütte seines Sohnes als Maurerlehrling eingeweiht und am 16. und 22. April 1785, erneut in der Loge Zur wahren Eintracht, in den 2. resp. 3. Grad erhoben.[21][22]
Speziell in seinen Opern Die Zauberflöte und Le nozze di Figaro sind gesellschaftskritische Töne aus dieser Mitgliedschaft zu spüren, die vielleicht mit dazu beigetragen haben, dass es Mozart nach der Uraufführung des Figaro finanziell nicht mehr so gut ging, zumal kurz danach der ungünstig verlaufende 8. Österreichische Türkenkrieg gegen das Osmanische Reich geführt wurde. Am 7. Dezember 1787 ernannte Joseph II. Mozart zum k.k. Kammermusicus und stattete ihn mit einem Jahresgehalt von 800 Gulden aus, am 9. Mai 1791 außerdem zum unbesoldeten Adjunkten des Domkapellmeisters des Stephansdoms, Leopold Hofmann.
Mit der Aufführung von Le nozze di Figaro 1786, die Joseph II. trotz des systemkritischen Inhalts freigab, überforderte er das Wiener Publikum, so dass es sich von ihm zurückzog. So verschlechterte sich seine wirtschaftliche Situation, ohne dass er dieser Tatsache mit seinen Ausgaben Rechnung trug. Trotz des vorherigen Wohlstandes hatte er keine Ersparnisse angesammelt und musste mehrfach von Freunden Geld leihen. Diese Misserfolge führten zu einem Wendepunkt in seinem Leben. Erfolg hatte er in dieser Zeit nur in Prag.
Abseits der Wiener Öffentlichkeit erschuf er die Werke seiner letzten Lebensjahre. Vergeblich versuchte er mit erneuten Reisen die wirtschaftliche Talfahrt aufzuhalten. Diese Reisen führten ihn zu den Aufführungen von 8. Jänner bis Mitte Februar 1787 und Ende August bis Mitte September 1791 nach Prag. Vom 8. April bis 4. Juni 1789 reiste er mit dem Fürsten Karl Lichnowsky über Prag, Dresden und Leipzig nach Potsdam und Berlin zum preußischen König Friedrich Wilhelm II. Vom 23. September bis Anfang November 1790 reiste er zur Krönung des Kaisers Leopold II., der dem verstorbenen Joseph II. nachfolgte, nach Frankfurt am Main. Dort war Mozart zusammen mit seinem Freund, dem Theaterdirektor Johann Heinrich Böhm, im „Backhaus“ in der Kalbächer Gasse 10 einquartiert.[23][24] Auf Heimreisen machte er Station in Mannheim und München.
Aber die Reisen nach Berlin 1789 und Frankfurt 1790 verhalfen ihm nicht zu erneutem Wohlstand. In Berlin erhielt er weder Einnahmen noch eine Anstellung. Die vom Kaiser erbetene Oper Così fan tutte fand nur mäßigen Anklang, ebenso wenig der Auftritt in Frankfurt am Main und die Uraufführung von La clemenza di Tito in Prag. Erst der große Beifall für die Zauberflöte versprach wirtschaftliche Besserung, aber jetzt war es nicht mehr der Adel, sondern die „einfachere“ Bevölkerung, bei der er Resonanz fand.
Nach der Uraufführung von La clemenza di Tito in Prag war Mozart Mitte September 1791 nach Wien zurückgekehrt und hatte sich sofort in die Arbeit für die Uraufführung der Zauberflöte (KV 620) gestürzt, die zwei Wochen später – endlich wieder mit Erfolg – über die Bühne ging. Gleichzeitig hatte er die Motette Ave verum corpus (KV 618) ausgearbeitet und mit der Niederschrift des Requiems (KV 626) begonnen, die er jedoch nicht mehr abschließen konnte. Franz Xaver Süßmayr, laut Constanze Mozart ein ehemaliger Schüler Mozarts, vollendete das Requiem.
Wenige Wochen nach der Uraufführung der Zauberflöte am 30. September 1791 wurde Mozart am 20. November (etwa zwei Tage, nachdem er die Uraufführung seiner Kantate Laut verkünde unsre Freude, KV 623 geleitet hatte)[25] bettlägerig, am 5. Dezember, fünf Minuten vor 1 Uhr früh[26] starb er. Er wurde nicht ganz 36 Jahre alt. Während seines letzten Lebensjahres wohnte er im Kleinen Kayserhaus, das sich bis zur Mitte des 19. Jahrhunderts in der Rauhensteingasse 8[27] auf der Rückseite des heutigen Kaufhaus Steffl (Kärntner Straße 19) befand. Eine Gedenktafel erinnert daran, dass Mozart dort am 5. Dezember 1791 starb.[28]
Als Todesursache wurde durch den Totenbeschauer „hitziges Frieselfieber“ (am ehesten „die Kombination eines hoch fieberhaften Krankheitsverlaufs mit einem sichtbaren Hautausschlag“) genannt.[29] In der Folge wurden diverse weitere Todesursachen in Erwägung gezogen:[30] zum einen verschiedene virale, bakterielle und parasitäre Infektionskrankheiten wie Syphilis sowie eventuell in Verbindung damit eine damals geläufige Anwendung von Quecksilber,[31] Trichinellose, Pharyngitis oder eine Infektion mit Streptokokken, die zu einer Kreuzreaktion von gegen Streptokokken gerichteten Antikörpern gegen Herzinnenhäute und -klappen führte, dem sogenannten Rheumatischen Fieber, woraufhin sich dann möglicherweise eine zum Tode führende Aortenklappeninsuffizienz entwickelte.[32][33] Ferner werden auch Erkrankungen wie Purpura Schönlein-Henoch, Nierenversagen, Herzversagen oder die Folgen eines mehrmals, zuletzt am 3. Dezember durchgeführten Aderlasses genannt.
Mozart selbst war davon überzeugt, vergiftet worden zu sein, und äußerte sich gegenüber Constanze dazu wenige Wochen vor seinem Tod während eines Besuchs im Prater: „Gewiß, man hat mir Gift gegeben.“ Für einen Giftmord gibt es allerdings keine dokumentierten Anhaltspunkte.[34]
Beigesetzt wurde Mozart, nachdem sein Leichnam zunächst verordnungsgemäß in der Wohnung aufgebahrt[35] worden war, in einem allgemeinen Grab am Sankt Marxer Friedhof. Organisiert wurde das Begräbnis durch Gottfried van Swieten.[36] Mozarts Witwe besuchte das Grab zum ersten Mal erst nach 17 Jahren. 1855 wurde der Standort seines Grabes so gut wie möglich bestimmt und 1859 an der vermuteten Stelle ein Grabmal errichtet, das später von der Stadt Wien in die Gruppe der Musiker-Ehrengräber am Zentralfriedhof (32 A-55) übertragen wurde. Auf der alten, frei gewordenen Grabstelle wurde in Eigeninitiative des Friedhofswärters Alexander Kugler abermals eine Mozart-Gedenktafel errichtet, die mit der Zeit aus Spolien anderer Gräber zu einem Grabmal ausgebaut wurde und heute eine viel besuchte Sehenswürdigkeit ist.
Die These vom „verarmten Genius Mozart“ stammt aus der Romantik. Jeder Biograph versuchte Mozart „noch ärmer zu machen“. Besonders in der Öffentlichkeit ist das Klischee vom „armen Mozart“ noch verbreitet, während es die neuere Forschung ablehnt. Mozart war sicher nicht reich im Vergleich zu einem Grafen oder Fürsten, reich war er aber gegenüber den anderen Angehörigen seines Standes, des vierten Standes der Bürger.[37]
Nach heutigen Maßstäben war Mozart ein Großverdiener, dennoch war er, bedingt durch seinen Lebenswandel, oft in finanziellen Nöten. Für ein Engagement als Pianist erhielt er nach eigenen Angaben „wenigstens 1000 Gulden“ (zum Vergleich: Seiner Magd bezahlte er einen Gulden pro Monat). Zusammen mit seinen Klavierstunden, für die er jeweils zwei Gulden berechnete, und seinen Einkünften aus den Konzerten und Auftritten verfügte er über ein Jahreseinkommen von rund 10.000 Gulden, was nach heutiger Kaufkraft etwa 125.000 Euro entspricht. Dennoch reichte das Geld nicht für seinen aufwendigen Lebensstil, so dass er oft genug andere, wie Johann Michael Puchberg, einen Logenfreund, um Geld bat. Er bewohnte große Wohnungen[38] und beschäftigte viel Personal, außerdem hegte er eine gewisse Leidenschaft für Karten- und Billardspiele, wodurch er große Summen verloren haben könnte. Der wertvollste Einzelposten seiner Hinterlassenschaft waren laut Verlassenschaftsverzeichnis nicht die zahlreichen wertvollen Bücher oder Musikinstrumente in seinem Besitz, sondern es war seine teure Kleidung. Mozart starb nicht in Armut, denn er hatte immer noch Kredit und bei Anton Stadler sogar einen Kredit von 500 Gulden ausständig. Sein Billardtisch, der zu jener Zeit ein luxuriöses Statussymbol war, gibt Zeugnis von Mozarts durchaus gehobenen Lebensumständen im Jahr 1791.
Der dänische Neurologe und Psychiater Rasmus Fog spekulierte 1985 über eine mögliche Erkrankung Mozarts am Tourette-Syndrom.[46] 2005 untersuchte der irische Professor für Kinder- und Jugendpsychiatrie Michael Fitzgerald in seiner Veröffentlichung The Genesis of Artistic Creativity die Frage, ob Mozart das Asperger-Syndrom gehabt hätte. Anhand des biographischen Materials hält er es durchaus für möglich. Wegen Mozarts Hyperaktivität und Impulsivität könnte auch eine Diagnose von ADHS zutreffen.[47]
Die ersten Legenden zu den Todesursachen zirkulierten schon kurz nach Mozarts Tod. So werden zum Beispiel eine Quecksilbervergiftung[48] oder Mordmotive seines Konkurrenten Antonio Salieri behauptet.[49]
Am 28. Jänner 1756 – einen Tag nach seiner Geburt – wurde Mozart auf die Namen Joannes Chrysostomus Wolfgangus Theophilus getauft (andere Schreibweise seiner Vornamen: Joannes Chrisostomus Wolfgang Gottlieb.[4]) Der erste und letzte der genannten Vornamen verweisen auf den Taufpaten Joannes Theophilus Pergmayr, Senator et Mercator Civicus, die ersten beiden Namen zugleich auf den damaligen Tagesheiligen des Geburtstages Johannes Chrysostomos,[50] der mittlere Vorname Wolfgang auf Mozarts Großvater Wolfgang Nicolaus Pertl. Das griechische Theophilus („Gottlieb“) hat Mozart später in seine französische Entsprechung Amadé bzw. (selten) latinisierend Amadeus übersetzt.
Sein Rufname war zeitlebens Wolfgang. In der Zeit der Italienreisen nannte er sich oft Wolfgango Amadeo Mozart. Als Erwachsener unterschrieb er zumeist als Wolfgang Amadé Mozart, wenn nicht überhaupt nur als Wolfgang Mozart (so etwa trug er sich in die Anwesenheitsliste der Wiener Freimaurerloge Zur Wohlthätigkeit ein). Amadeus nannte er sich nur im Scherz in drei seiner Briefe. Die Namensform Wolfgang Amadeus erschien zu Mozarts Lebzeiten offiziell nur einmal, und zwar im Frühjahr 1787 in einem amtlichen Schreiben der Niederösterreichischen Statthalterei.[51] Die erste postume amtliche Nennung Mozarts mit dem latinisierten Vornamen ist die Eintragung im Totenbeschauprotokoll des Wiener Magistrats am 5. Dezember 1791.
Mozart schrieb, beginnend im Jugendalter, während seines Lebens zahlreiche Briefe, die ein Kennenlernen seiner Persönlichkeit und seiner musikalischen Ansichten und Arbeitsweisen ermöglichen und so eine wichtige Forschungsbasis zu Mozarts Leben und Werk liefern. Der wichtigste briefliche Korrespondenzpartner war Mozarts Vater Leopold Mozart.
Mozart war insgesamt über zehn Jahre, beinahe ein Drittel seines Lebens auf Reisen, die ihn in zehn Länder des heutigen Europas führten. Allein schon die Fahrten per Kutsche – eine Reise von Salzburg nach Wien dauerte zum Beispiel je nach Jahreszeit und Wetter etwa sechs Tage – waren zur damaligen Zeit eine physische Herausforderung. Zudem reisten die Mozarts oft im Winter.[52] So schreibt Leopold Mozart am 29. Dezember 1762 über die Fahrt von Preßburg nach Wien an Lorenz Hagenauer, den Vermieter und gleichzeitigen Gönner der Mozarts in Salzburg:
„[…] wir reisten diesen Tag nicht sonderlich bequemm, indem der weeg zwar ausgefrohren, allein unbeschreiblich knoppericht und voller tieffer gruben und schläge war; den̄ die Ungarn machen keinen weeg. Hätte ich in Pressburg nicht einen Wagen kauffen müssen, der recht gut gehängt ist, so hätten wir ganz gewiß ein paar Rippen weniger nach Hause gebracht. Den wagen muste ich kauffen, wenn ich anders wollte, daß wir gesund nach Wien̄ kommen sollten. Den̄ in ganz Presburg war kein 4sitziger geschlossner wagen bey allen Landkutschern anzutreffen. Diesen wagen hatte ein Stattkutscher – die Stattkutscher därffen aber nicht über Land fahren, aufgenommen mit 2 Pferd nur auf etliche Stunde.“[53]
Wie unangenehm er eine Fahrt von Salzburg nach München erlebte, schildert Wolfgang Amadeus am 8. November 1780 in einem Brief an seinen Vater:
„Glücklich und vergnügt war meine Ankunft! – glücklich, weil uns auf der Reise nichts widriges zugestossen, und vergnügt, weil wir kaum den Augenblick, an ort und Ende zu kommen, erwarten konnten, wegen der obwohl kurzen doch sehr beschwerlichen Reise; – denn, ich versichere Sie, daß keinem von uns möglich war nur eine Minute die Nacht durch zu schlafen – Dieser Wagen stößt einem doch die Seele heraus! – und die Sitze! – hart wie stein! – Von Wasserburg aus glaubte ich in der that meinen Hintern nicht ganz nach München bringen zu können! – er war ganz schwierig – und vermuthlich feuer Roth – Zwey ganze Posten fuhr ich die Hände auf dem Polster gestützt, und den Hintern in lüften haltend – doch genug davon, das ist nun schon vorbey! – aber zur Regel wird es mir seyn, lieber zu fus zu gehen, als in einem Postwagen zu fahren.“[54]
Obwohl einige von Mozarts frühen Tastenwerken für Cembalo geschrieben wurden, lernte er in seinen frühen Jahren auch Fortepiani des Regensburger Klavierbaumeisters Franz Jakob Späth kennen.[55] Später, als Mozart Augsburg besuchte, haben ihn die Fortepiani von Johann Andreas Stein sehr beeindruckt, wie er seinem Vater Leopold in einem Brief mitteilte.[55] Am 22. Oktober 1777 führte Mozart sein 7. Klavierkonzert (KV 242) auf von Stein zur Verfügung gestellten Instrumenten erstmals auf.[56] Der Augsburger Domorganist Demmler spielte den ersten, Mozart den zweiten und Stein den dritten Teil.[57] 1783 erwarb Mozart in Wien ein Fortepiano von Walter.[58] In einem Brief bestätigt Leopold Mozart die enge Bindung seines Sohnes an sein Fortepiano von Walter: „Es ist unmöglich, die Hektik zu beschreiben. Das Klavier eures Bruders wurde mindestens zwölf Mal von seinem Haus ins Theater oder in das Haus eines anderen gebracht.“[59]
Die Frage der Bürgerschaft bzw. Landsmannschaft des Komponisten wird in der Rezeptionsgeschichte unterschiedlich beantwortet. Salzburg war seit dem späten 14. Jahrhundert Hauptstadt des im Wesentlichen unabhängigen[60] Erzstifts Salzburg,[61] das geistlich dem Heiligen Stuhl in Rom unterstand, weltlich als Teil des bayerischen Reichskreises dem römisch-deutschen Kaiser (zu Mozarts Lebzeiten waren das 1745–1765 Franz I., 1765–1790 Joseph II. und 1790–1792 Leopold II.), nicht aber der „österreichischen“ Habsburgermonarchie.[62][63] Sein Vater Leopold entstammte einer schwäbischen Familie, welche seit Generationen in Augsburg lebte, und die Familie seiner Mutter Anna ist in der Salzburger Umgebung beheimatet, wobei sich hieraus jeweils keine Staatsangehörigkeit im modernen Sinne für Wolfgang ergab.[64] Mozart war im Erzbistum als Untertan der Fürsterzbischöfe geboren und blieb dies sein Leben lang. Die Landeszugehörigkeit Mozarts könnte daher als „(Fürsterzbischöflich-)Salzburg(er)isch“ bezeichnet werden,[65] jedoch ist diese Umschreibung seiner Landsmannschaft weniger verbreitet.[66][67] Das viel verwendete Grove Dictionary of Music and Musicians bezeichnet Mozart als österreichischen Komponisten.[68] Auch das Houghton Mifflin Dictionary of Biography (2003), das Oxford Concise Dictionary of Music (Bourne und Kennedy 2004) und das NPR Listener’s Encyclopedia of Classical Music (Libbey, 2006) bezeichnen ihn als solchen. Die Encyclopædia Britannica liefert zwei unterschiedliche Ergebnisse:[69] Die kurze anonyme Fassung in der Micropedia bezeichnet ihn als österreichischen Komponisten; der Hauptartikel in der Macropedia, geschrieben von H. C. Robbins Landon, befasst sich nicht mit seiner Nationalität. In früheren Quellen wird Mozart oft als Deutscher bezeichnet, vor allem vor der Gründung des heutigen modernen österreichischen Nationalstaates. Eine Londoner Zeitung berichtete im Jahr 1791 vom Tod des Komponisten. Dort wird er als „der gefeierte deutsche Komponist“ (englisch: the celebrated German composer) bezeichnet.[70] In Lieber u. a. (1832, S. 78), wird Mozart als „der große deutsche Komponist“ vorgestellt. Ferris (1891) nahm Mozart – unter anderem neben Frédéric Chopin, Franz Schubert und Joseph Haydn – in sein Buch The Great German Composers („Die großen deutschen Komponisten“) auf. Andere Bezeichnungen als Deutscher findet man bei Kerst (1906, S. 3), Mathews und Liebling (1896), sowie MacKey und Haywood (1909); viel später auch bei Hermand und Steakley (1981).
Manche Quellen änderten ihre Zuordnungen Mozarts zu heutigen Staaten im Laufe der Zeit. Grove bezeichnete Mozart nicht immer als „Österreicher“; dies erschien erstmals in der ersten Auflage des New Grove im Jahr 1980. Ähnlich war es auch beim Baker’s Biographical Dictionary of Musicians. Ursprünglich erwähnten sie die Landsmannschaft Mozarts nicht. Das Wort „Austrian“ wurde erstmals im Anfangssatz in der achten Auflage von 1992 erwähnt und wurde seither beibehalten.[71] Die Encyclopædia Britannica, die ihn heute als „Austrian“ bezeichnet, beschrieb ihn früher als deutschen Komponisten.[72]
Mozart selbst äußerte sich in seinen nachgelassenen Schriften nicht zur Frage seiner „Staatsangehörigkeit“ im modernen Sinne, nennt sich selber aber Teutscher, so in Briefen an seinen Vater, z. B. vom 29. Mai 1778, in dem es heißt: 
„Was mich aber am meisten aufrichtet und guten Muthes erhält, ist, daß ich ein ehrlicher Teutscher bin“
und vom 11. September 1778, in dem er schreibt: 
„mir ist nur leid, daß ich nicht hier bleibe, um ihm zu zeigen, daß ich ihn nicht brauche – und daß ich so viell kann als sein Piccini – obwohl ich nur ein Teutscher bin.“
In einem Brief vom 17. August 1782 schreibt er: 
„Will mich Teutschland, mein geliebtes Vaterland, worauf ich (wie Sie wissen) stolz bin, nicht aufnehmen, so muß in Gottes Namen Frankreich oder England wieder um einen geschickten Teutschen mehr reich werden – und das zur Schande der teutschen Nation.“
Daraus wird ersichtlich, dass für ihn Teutschland als Bezeichnung für die deutschsprachigen Gebiete Mitteleuropas und die teutsche Nation (jeweils in der damals im oberdeutschen Raum üblichen Schreibung) als Kollektivum der dort lebenden deutschsprachigen Menschen begriffliche Realität waren, ohne dass dabei der Nationalstaatsgedanke unserer Zeit darauf Anwendung finden könnte: Zu seiner Zeit existierte ein Rechtssubjekt mit Namen „Deutschland“ ebenso wenig wie eines namens „Italien“, von dem er an anderer Stelle schreibt.
Was jedoch existierte, war das Heilige Römische Reich deutscher Nation, welches das heutige Deutschland und Österreich einschloss. Für dessen Kaiser schrieb er in Wien Musik zur Zeit der Verfassung obiger Aussage, nachdem er im Jahr zuvor aus Salzburg übersiedelt war und geheiratet hatte. Dies bildet somit den Kontext zum Verständnis der Aussage über seine Selbstverortung.
Seine beiden die Kindheit überlebenden Söhne starben kinderlos. Es gibt daher keine direkten Nachkommen von Mozart mehr.
Das Gedenken an Wolfgang A. Mozart und die Beschäftigung mit seinem Werk wird heute weltweit durch Biographien, musikwissenschaftliche Forschung, Radio- und Fernsehsendungen, Symposien und insbesondere durch Aufführungen seiner Kompositionen in den Opernhäusern und Konzertsälen aufrechterhalten. Seit dem 19. Jahrhundert werden – vor allem in Österreich und Deutschland – zu allen runden Gedenkjahren Mozartjahre begangen.
Die Republik Österreich hat Mozart mehrfach auf Münzen oder Banknoten gewürdigt, wie zum Beispiel auf der 5000-Schilling-Banknote von 1989 und der österreichischen 1-Euro-Münze.
Die Bundesrepublik Deutschland hat zu Ehren seines 250. Geburtstages im Jahr 2006 eine 10-Euro-Silbermünze herausgebracht. Damit soll, so die offizielle Begründung, die Persönlichkeit des Komponisten als großes Ereignis „aus der deutschen Kultur und Geschichte“ für die Nachwelt erhalten bleiben. Außerdem hat die Deutsche Post AG zum gleichen Anlass eine Sondermarke herausgegeben. Dies geschah bereits zum 225. Geburtstag seitens der Deutschen Post der DDR in Form einer Blockausgabe.
Daneben gibt es etliche Merchandising-Artikel (z. B. Mozartkugeln).
2-Schilling-Münze (1931)
Briefmarke (1956) der Deutschen Bundespost zum 200. Geburtstag
Österreichische 1-Euro-Münze (2002)
ÖBB-Werbelok zum 250. Geburtstag, 2006
Briefmarke (2006) der Deutschen Post zum 250. Geburtstag
Mozartkugeln, seit 1890
Mozarts Name bedeutet für Orte aus seiner Biographie einen bedeutenden Wirtschaftsfaktor im Bereich internationaler Städtetourismus. Eine besondere Rolle kommt seiner Geburtsstadt Salzburg (Mozartdenkmal am Mozartplatz), Wien als seinem langjährigen Wohnort (Mozart-Statue im Burggarten) und außerdem der Stadt Augsburg als Geburtsstadt seines berühmten Vaters Leopold Mozart zu. Prag war ein von Mozart geschätzter Aufführungsort. Daher genießt er auch hier große Popularität. Ihm zu Ehren wurde eine Büste in der Walhalla bei Regensburg aufgestellt.
In mehreren Städten gibt es Mozart-Gedenkstätten, die sich der Erinnerung an den Komponisten in besonderer Weise annehmen. Gleiches gilt auch für Mozart-Gesellschaften und -Vereine. Das erste Denkmal für den Komponisten, ein Mozarttempel genannter, freskengeschmückter Pavillon, errichtete ein privater Verehrer im Frühjahr 1792 in Graz.
Mozart zu Ehren erhielt ein 1924 entdeckter Asteroid die Bezeichnung (1034) Mozartia und ein 1991 entdecktes Mineral den Namen Mozartit. Darüber hinaus trägt der Mozart-Piedmont-Gletscher vor der Westküste der antarktischen Alexander-I.-Insel seinen Namen. Auch die Pflanzengattung Mozartia Urb. aus der Familie der Myrtengewächse (Myrtaceae) ist nach ihm benannt.[76]
Zahlreiche Festivals befassen sich überwiegend mit Mozarts Werken. Bereits im 19. Jahrhundert fanden in seiner Geburtsstadt eine Reihe von Mozart-Festen statt. Zu den bedeutenden Festivals der Gegenwart gehören (in Klammer jeweils das Gründungsdatum):
Charakteristikum fast aller dieser Festivals ist, dass Mozart zwar deren zentrale Achse darstellt, aber zumeist auch Kompositionen anderer Komponisten aufgeführt werden. Weiters finden regelmäßig Mozart-Festivals in Bath, Texas und Vermont statt.
In der Getreidegasse richtete die einstige Internationale Mozart-Stiftung (die von 1870 bis 1879 existierte) im Geburtshaus Wolfgang Amadeus Mozarts (→ Hagenauerhaus) ein Museum ein. Ein weiteres Mozart-Museum befindet sich in der 1773 von der Familie Mozart bezogenen Wohnung im Tanzmeisterhaus am Makartplatz. 2006 wurden die Räume durch den Regisseur und Designer Robert Wilson neu gestaltet. Das Mozart-Denkmal von Ludwig Schwanthaler auf dem Mozartplatz ist in Blickrichtung Alte Residenz und Dom ausgerichtet und wurde 1842 enthüllt. Wie sehr Mozart damals schon nicht nur lokalpatriotisch österreichisch, sondern als ständeübergreifender Besitz aller Deutschen verstanden wurde, zeigt die Planung und Finanzierung des Projektes: An den Plänen waren vor allem Nicht-Salzburger beteiligt, und unter den finanziellen Förderern findet man neben Kaiser Ferdinand I. die Könige von Preußen und Bayern, den Adel sowie bürgerliche Musikvereine und prominente Musiker.[78]Eine bronzene Mozart-Statue befindet sich auf dem Kapuzinerberg. Diese wurde anlässlich des Ersten Internationalen Mozartfestes im Jahr 1877, einem Vorläufer der Salzburger Festspiele, enthüllt und stammt von Edmund Hellmer.[79] Dabei wurde auch das Zauberflötenhäuschen, in dem Mozart angeblich Die Zauberflöte komponiert hatte, hinter der Statue aufgestellt.[80] Betreiber dieser Aktionen war der Mozart-Enthusiast Johann Evangelist Engl (1835–1925), auf den die Gründung der Stiftung Mozarteum zurückgeht und der auch das „Schaugrab“ der Mozarts errichten ließ. Die am Ursulinenplatz vor der Markuskirche 2005 aufgestellte Mozart-Skulptur „Mozart – Eine Hommage“ von Markus Lüpertz führte einige Zeit zu Kontroversen.
In Salzburg hat die Internationale Stiftung Mozarteum ihren Sitz. Sie wurde 1880 von Salzburger Bürgern gegründet und ist aus dem 1841 entstandenen Dommusikverein und Mozarteum hervorgegangen. Die Autographensammlung der Stiftung enthält rund 190 Originalbriefe Mozarts, die Bibliotheca Mozartiana ist mit rund 35.000 Titeln die umfangreichste einschlägige Bibliothek der Welt. Die Stiftung besitzt zudem reiches Bildmaterial, darunter mehrere authentische Mozart-Porträts. Die Ton- und Filmsammlung verfügt über rund 18.000 Audiotitel (darunter sonst nicht zugängliche Mitschnitte von Mozart-Aufführungen) und etwa 1800 Videoproduktionen (Spielfilme, Fernsehproduktionen, Opernaufzeichnungen, Dokumentarfilme). Die Stiftung verwaltet auch die beiden Salzburger Mozart-Museen. In der Satzung der Stiftung verankert ist das 1931 gegründete Zentralinstitut für Mozart-Forschung, das heute unter dem Namen Akademie für Mozart-Forschung firmiert. Sie veranstaltet in regelmäßigen Abständen wissenschaftliche Tagungen, über die im Mozart-Jahrbuch berichtet wird. Sämtliche Bereiche der Mozart-Forschung werden hierbei berücksichtigt, zentral jedoch ist seit 1954 die Herausgabe der Neuen Mozart-Ausgabe, der historisch-kritischen Edition von Mozarts Werken.
Die Stiftung besitzt auch das Konzertgebäude Mozarteum mit zwei Sälen. Der Große Saal des Mozarteums wird nicht nur für den Salzburger Konzertbetrieb genutzt, sondern regelmäßig auch von den Salzburger Festspielen – mit Matineen, Liederabenden, Solistenkonzerten, aber auch Orchesterkonzerten – bespielt. Alljährlich im Jänner veranstaltet die Stiftung seit 1956 die Mozartwoche, bei der renommierte Orchester (etwa die Wiener Philharmoniker oder das Mahler Chamber Orchestra) und Interpreten (Nikolaus Harnoncourt, Riccardo Muti u. a.) Mozarts Werke aufführen, ebenfalls im Großen Saal des Mozarteums.
Ebenfalls im Jahr 1880 gegründet wurde die Öffentliche Musikschule Mozarteum, aus der sich schließlich die Universität Mozarteum entwickelte. Dort werden Ausbildungen für Streich-, Blas-, Zupf- und Schlaginstrumente und auch eine Ausbildung für Schauspiel angeboten. Die Universität Mozarteum ist heute in der Neustadt im Alten Borromäum nächst dem Mirabellgarten beheimatet. Zunächst aus Studierenden dieser Institution haben sich auch die zwei Mozart-Orchester Salzburgs entwickelt:
Seit 1908 besteht das Mozarteumorchester Salzburg (derzeit mit 91 Musikern), das heute als Orchester von Stadt und Land Salzburg sowohl den Opern- und Operettenbetrieb des Salzburger Landestheaters bestreitet, als auch bei den Salzburger Festspielen wichtige Aufgaben übernimmt: Es spielt seit 1950 alljährlich Mozarts Große Messe in c-Moll (KV 427) in der Stiftskirche St. Peter, wirkt in Opernproduktionen, den Mozart-Matineen am Sonntagvormittag, Serenaden, Orchesterkonzerten und Festveranstaltungen mit. Der Klangkörper hat seine Wurzeln im 1841 gegründeten „Dommusikverein und Mozarteum“ und wurde unter Mithilfe von Constanze Mozart ins Leben gerufen.
Das zweite Salzburger Mozart-Orchester ist die Camerata Salzburg, die 1952 von Bernhard Paumgartner als Camerata Academica des Mozarteums Salzburg aus Lehrern und Schülern der Universität Mozarteum gegründet wurde. Das Ziel der Camerata war und ist vorrangig die Mozart-Pflege. Unter ihrem Chefdirigenten Sándor Végh (1978–1997) übernahm sie für viele Jahre die Mozart-Matineen der Salzburger Festspiele und gastiert seither weltweit unter Leitung namhafter Dirigenten, wie Heinz Holliger, Kent Nagano, Trevor Pinnock oder Franz Welser-Möst.
Aus den Mozart-Festen des späten 19. und frühen 20. Jahrhunderts entwickelten sich schließlich ab 1920 die Salzburger Festspiele, in deren Mittelpunkt unverändert seit der Gründung Mozart steht. Analog zu Bayreuth, das alljährlich die Werke Richard Wagners aufführt, sollte der Salzburger Genius loci jeden Sommer in exemplarischen Aufführungen gewürdigt werden. Rund die Hälfte aller Opernproduktionen der Festspiele sind Mozart-Opern gewidmet, die erste Opernaufführung der Festspiele war der Don Giovanni am 14. August 1922, dirigiert von Richard Strauss und gesungen von den Damen Claire Born, Gertrud Kappel, Lotte Schöne und den Herren Alfred Jerger, Viktor Madin, Franz Markhoff, Richard Mayr, Richard Tauber.
Zu den Spielstätten der Salzburger Festspiele zählt seit 2006 das Haus für Mozart in der Hofstallgasse. Ursprünglich stand an dieser Stelle die Große Winterreitschule, die 1925 für Max Reinhardts Schauspielproduktionen als Festspielhaus adaptiert wurde. Ab 1927 spielte man jeden Sommer auch Opern – überwiegend Mozarts – in diesem Haus, das schließlich noch mehrmals umgebaut wurde. Anlässlich des bevorstehenden 250. Geburtstags Mozarts wurde das Festspielhaus zwischen 2003 und 2006 grundlegend erneuert und erhielt den neuen Namen. Die Eröffnung fand am 26. Juli 2006 mit einer Neuproduktion von Le nozze di Figaro statt. In diesem Mozartjahr wurden erstmals alle anderen Bühnenwerke Mozarts im Rahmen der Festspiele gezeigt (Projekt Mozart 22, siehe Opernchronologie der Salzburger Festspiele).
In Wien ist eine der Wohnungen Mozarts erhalten, allerdings ohne Möbel, die verschollen sind; sie wurde in ein Museum umgewandelt: Domgasse 5, gleich hinter dem Stephansdom. Die ursprüngliche Gedenkstätte wurde vor einiger Zeit um zwei Stockwerke erweitert und im Jänner 2006 als Mozarthaus Vienna wiedereröffnet. Mozarts Leben und seine Zeit werden dem Besucher durch teils aufwendige multimediale Präsentationen erläutert. An zahlreichen anderen Häusern, in denen Mozart lebte oder auftrat, sind Gedenktafeln angebracht.
Das Mozart-Denkmal, von Architekt Karl König und Bildhauer Viktor Tilgner 1896 gestaltet, stand auf dem Albertinaplatz. Nach dem Zweiten Weltkrieg wurde es 1953 in den Burggarten überstellt. Die Skulpturen bestehen aus Laaser Marmor (Vinschgau, Südtirol), die Stufen der Basis aus dunklem Diorit. Die Balustraden sind aus grobem Sterzinger Marmor in Südtirol, zwei bei der Neuaufstellung dazugekommene Pfeiler wurden aus St. Margarethener Kalksandstein gefertigt.[81]
Im Jahr 1862 wurde in Wien-Wieden (4. Bezirk) die Mozartgasse nach dem Komponisten benannt, 1899 der Mozartplatz; 1905 wurde dort der Mozart-Brunnen errichtet. Im Jänner 2006 wurde das Theater an der Wien, das in den Jahrzehnten zuvor überwiegend Musical-Produktionen beherbergte, anlässlich des Mozart-Jubiläumsjahres wiederum zu einem Opernhaus umgewidmet. Mozart stellt nach wie vor einen Schwerpunkt der Programmierung von Wiens Neuem Opernhaus dar.
Im Mozarthaus in der nördlichen Altstadt von Augsburg befindet sich eine Gedenkstätte zur Geschichte der Familie Mozart. In diesem Haus wurde sein Vater Leopold geboren. Eine Gedenktafel am Haus der Augsburger Fuggerei (Mittelgasse 14) erinnert zudem an seinen Urgroßvater, den Maurermeister Franz Mozart (1649–1694), der hier wohnte und starb.
Die Deutsche Mozart-Gesellschaft (DMG) mit Sitz in Augsburg „widmet sich … der praktischen und wissenschaftlichen Pflege des Werkes von Wolfgang Amadé Mozart, der Erforschung des Lebens und Schaffens des Meisters und seiner Familie und der Erhaltung und Förderung der Mozart-Gedenkstätten in der Bundesrepublik Deutschland, insbesondere des Geburtshauses von Leopold Mozart in Augsburg“.[82]
Vielfach gedacht wird Mozarts auch in Mannheim, wo er nicht nur bei vier Aufenthalten 176 Tage seines Lebens verbracht, sondern auch eine Reihe bedeutender Werke komponiert, 1790 eine Aufführung von Le nozze di Figaro dirigiert[83] und sich bei seinem ersten Aufenthalt, 1777, in Aloisia Weber verliebt sowie deren Schwester Constanze, seine spätere Frau, kennengelernt hat. Die damalige „Mannheimer Schule“ war musikhistorisch von europäischem Rang, doch konnte Mozart dort beruflich letztlich nicht reüssieren.[84] Gedenktafeln befinden sich an zahlreichen Wohn- und Wirkungsstätten des Komponisten, so etwa am Schloss, an der Jesuitenkirche und am Palais Bretzenheim.
Im Prager Stadtteil Smíchov wurde 1956 in der sogenannten Vila Bertramka ein Mozartmuseum eingerichtet. Zu Mozarts Lebzeiten lag das Gebäude jenseits der Stadtmauer und diente der Familie des Komponisten Franz Xaver Duschek als Landgut. Es gehörte der Ehefrau Duscheks, der Sängerin Josepha Duschek, der Enkelin Ignatz Anton von Weisers, des Salzburger Bürgermeisters und Textdichters Mozarts. Mozart wohnte hier im Oktober 1787 (Vollendung und Uraufführung des Don Giovanni) und von Ende August bis Anfang September 1791 (Einstudierung und Uraufführung von La clemenza di Tito).[85]
Joseph Haydn würdigte Mozarts Musik, als er 1785 nach dem ersten Hören der ihm von Mozart gewidmeten Streichquartette Leopold Mozart versicherte:
„[…] ich sage ihnen vor gott, als ein ehrlicher Mann, ihr Sohn ist der größte Componist, den ich von Person und den Nahmen nach kenne: er hat geschmack, und über das die größte Compositionswissenschaft.“[86]
Mozart selbst bekannte in einem Brief an seinen Vater vom 7. Februar 1778:
„denn ich kann so ziemlich, wie sie wissen, alle art und styl vom Compostitions annehmen und nachahmen.“[87]
Es ist eine nachweisbare Eigenheit Mozarts, dass er während all seiner Kompositionsperioden Musik der verschiedensten Stile in sich aufgenommen und hieraus mannigfaltige Anregungen geschöpft hat.[88] Wesentlich geprägt ist sein Kompositionsstil von süddeutschen und italienischen Stilelementen der zweiten Hälfte des 18. Jahrhunderts. Die frühesten Einflüsse stammen von seinem Vater und den Salzburger Lokalkomponisten. Wie sehr Mozart zunächst seinem Umfeld verhaftet blieb, zeigt der Streit um die beiden „Lambacher“ Sinfonien, bei denen lange unklar war, welche von Leopold Mozart und welche von Wolfgang Amadeus Mozart stammt.[89]
Bei seinen Reisen nach Italien lernte er den dortigen Opernstil kennen, der ihn stark prägte und ihm auch in London von Johann Christian Bach vermittelt wurde. Großen Einfluss – vor allem auf sein späteres Schaffen – hatte das Studium des Kontrapunktes: zuerst durch den Kompositionsunterricht bei Padre Martini in Italien, später in Wien durch die praktische Auseinandersetzung mit der Musik Johann Sebastian Bachs und Georg Friedrich Händels, die er bei Gottfried van Swieten kennenlernte. Mozart dazu an seinen Vater am 30. März 1783: „denn wir lieben uns mit allen möglichen Meistern zu unterhalten; – mit alten und mit Modernen“.
Als typisch für Mozarts kompositorisches Schaffen lassen sich folgende Punkte nennen:
Alles in allem schuf Mozart aus den von ihm vorgefundenen Stilen und Kompositionstechniken dank seiner herausragenden Fähigkeiten Musik von großer Komplexität und bedeutender Stilhöhe. Daran konnten Beethoven und die Komponisten des 19. Jahrhunderts anknüpfen.
Mozarts Klavierspiel wurde überall gerühmt und geschätzt. Dabei muss bedacht werden, dass er nicht auf dem modernen Klavier, sondern auf dem Hammerklavier und gelegentlich auf dem Cembalo spielte.
Als Grundartikulation pflegte Mozart das zu seiner Zeit übliche Non legato. Dies wird von Ludwig van Beethoven, der ihn mehrfach in Konzerten hörte, bezeugt und von Carl Czerny wiedergegeben. Demnach habe Mozart „ein feines, aber zerhacktes Spiel gehabt, kein ligato.“[94]
Die Werke Mozarts werden meist nach ihrer Sortierung im Köchelverzeichnis (KV) gezählt, das der chronologischen Reihenfolge des Entstehens zu folgen versucht.
Insgesamt 21 Opern.
17 Messen, darunter
Siehe dazu den Artikel: Liste der Kirchenmusikwerke Mozarts
Siehe Liste der Sinfonien Mozarts
Siehe Liste der Klavierkonzerte Mozarts
Siehe auch Violinkonzerte (Mozart)
Insgesamt 12 Werke.
Flötenkonzerte und -sätze
Hornkonzerte und -sätze
Insgesamt 13 Werke.
Serenaden
Notturni
Divertimenti
Märsche
Kassationen
Insgesamt 23 Werke.
Siehe Liste der Klaviermusikwerke Mozarts
Obwohl Mozart in einem Brief an seinen Vater vom 17. Oktober 1777 schrieb, dass die Orgel seine Passion sei und bekannte „Die Orgel ist doch in meinen Augen und Ohren der König aller Instrumente“,[95] hat er nur wenige Orgelwerke komponiert.
Insgesamt 42 Werke.
Mozart hat textierte und untextierte Kanons geschrieben. Unter den textierten finden sich Werke mit kirchlichem Inhalt:
Es gibt aber auch Kanons mit zum Teil recht derbem Inhalt, der an Mozarts Bäsle-Briefe erinnert, die er an seine Cousine Maria Anna Thekla Mozart schrieb. In vielen Liederbüchern ist der originale Text durch einen neuen, „entschärften“ ersetzt.
So zum Beispiel:
Der vierstimmige Kanon KV Anh. 191 (1788; 562c) ist für zwei Violinen, Viola und Bass gesetzt.
Die Figur Wolfgang Amadeus Mozarts wurde in vielen Romanen und Erzählungen verarbeitet, unter anderem in
Werkverzeichnisse, Noten, Dokumente
Hörbeispiele
Historische biografische Texte
Gesellschaften

Marco Polo (* 1254 vermutlich in Venedig; † 8. Januar 1324 ebenda)[1] war ein Asienreisender, der aus einer venezianischen Händlerfamilie stammte und durch die Berichte über seine Reise ins Kaiserreich China bekannt wurde. Motiviert wurde er durch die Berichte seines Vaters und seines Onkels, die bereits vor ihm China bereist hatten. Obwohl einzelne Geschichtswissenschaftler aufgrund von falschen Angaben und vermeintlichen Ungereimtheiten in den Reiseberichten immer wieder Zweifel an der Historizität seiner China-Reise geäußert haben, wird diese von den meisten Historikern als erwiesen angesehen.[2][3]
Die Polos waren angesehene Bürger Venedigs, zählten aber nicht zu den obersten Schichten. Marco Polo selbst wird in den Archiven als nobilis vir (Edelmann) bezeichnet, ein Titel, auf den er stolz war.[4] Der Name Polo stammt vom lateinischen Paulus, und Polos sind in Venedig seit 971 nachweisbar.[5] Nach einer (unbelegten) Überlieferung stammte die Familie ursprünglich aus Dalmatien, unter anderem wurden die Stadt Šibenik und die Insel Korčula als mögliche Herkunftsorte genannt.[6] Die Polos waren Händler,[7] und schon der Großonkel Marco kommandierte 1168 in Konstantinopel ein Handelsschiff. Zur Zeit von Marco Polos Geburt waren sein Vater Niccolò Polo und sein Onkel Maffeo (auch Maffio oder Matteo), über die ansonsten wenig bekannt ist, auf Handelsreise im Osten.
Marco Polos Vater Niccolò und sein Onkel Maffeo brachen 1260 zu einer Reise auf, um am Unterlauf der Wolga Edelsteine zu verkaufen. Über Konstantinopel gingen sie nach Soldaia (heute Sudak) auf der Krim, wo Marco der Ältere, der dritte der Brüder Polo, ein Kontor betrieb. Somit reisten sie nahezu auf derselben Route, die auch Wilhelm von Rubruk 1253 für seine Mission gen Osten gewählt hatte. Bevor die Polos nach Asien zu den Mongolen aufbrachen, hatten schon die Mönche André de Longjumeau und Johannes de Plano Carpini im Auftrag von Papst Innozenz IV. und später auch Wilhelm von Rubruk im Auftrag von König Ludwig IX. jeweils eine solche Reise in offizieller Mission angetreten. Nach ihrer Rückkehr verfassten sie jeweils eigene Reiseberichte.[8] Nach ihrem Zwischenaufenthalt gelangten die Polos in das Gebiet, das damals von der Goldenen Horde beherrscht wurde, und hielten sich etwa ein Jahr in der Nähe des Dschingis-Khan-Enkels Berke Khan an der Wolga auf. Anschließend wurden sie durch die dort noch herrschenden Kriegswirren immer weiter gen Osten über den Fluss Ural und entlang der Seidenstraße (nördlicher Abzweig nach Südrussland) bis nach Buchara verschlagen.
Da sie durch Kriegsfolgen an einer Rückreise gehindert waren, verblieben sie dort drei Jahre und schlossen sich schließlich einer persischen Gesandtschaft an, die auf dem Weg zum Großkhan Kubilai war. In den Wintermonaten 1266 trafen sie nach einjähriger Reisezeit am Hofe des Mongolenherrschers in Peking (damaliger Name: Khanbaliq, bei Marco Polo: Kambaluk) ein, wo sie vom Khan willkommen geheißen wurden. Dieser ließ den Polos bei ihrer Abreise ein sogenanntes Païza in Form eines Goldtäfelchens zukommen, das sicheres Geleit und freie Versorgung im Gebiet des Großkhans garantierte.[9] Außerdem wurden sie vom Großkhan beauftragt, dem Papst eine Botschaft mit der Bitte zu überbringen, ihm geweihtes Öl aus dem Jesusgrab in Jerusalem und etwa einhundert christliche Gelehrte zum Verbreiten des Evangeliums unter seinen Untertanen zu schicken. So traten die Polos die Rückreise nach Venedig an, wo sie um 1269 eintrafen. In der Zwischenzeit hatten mehrere Nachfolger den jeweils verstorbenen Papst abgelöst, aktuell hatte eine Nachfolgewahl begonnen. Auch Marco Polos Mutter war gestorben.
Papst Clemens IV., der sich nie in Rom aufgehalten hatte, war am 29. November 1268 in Viterbo (Italien) gestorben. Wegen anhaltender Uneinigkeit im Kardinalskollegium dauerte die päpstliche Sedisvakanz bis September 1271. Doch die Polos wollten nicht länger auf ein für sie nicht absehbares Ende der Papstwahl warten und beschlossen daher, auch ohne päpstlichen Auftrag oder päpstliche Botschaft erneut die Reise zum Großkhan anzutreten, um diesen nicht länger auf die Erfüllung seiner Wünsche warten zu lassen. Noch vor dem Ende der Sedisvakanz brachen Niccolò und Maffeo Polo 1271 wieder auf und nahmen den siebzehnjährigen Marco mit.
In Akkon betrat dieser zum ersten Mal den Boden Asiens. Hier erläuterten die drei Polos dem dortigen päpstlichen Legaten und Archidiakon von Lüttich, Tebaldo Visconti da Piacenza, den Sinn und Zweck ihrer Reise und baten ihn zunächst darum, nach Jerusalem weiterreisen zu dürfen, da der Mongolenherrscher Niccolò und Maffeo Polo auf ihrer ersten Asienreise gebeten hatte, ihm Öl aus der Lampe des Heiligen Grabes mitzubringen. Mit der gewünschten Erlaubnis reisten die Polos nach Jerusalem, wo sie das erbetene Öl ohne Probleme besorgen konnten, und kamen anschließend nach Akkon zurück.[10] Nunmehr übergab der Legat den Reisenden einen Brief an den Großkhan, in dem bezeugt wurde, dass die Brüder sich aufrichtig bemüht hätten, ihren Auftrag beim Papst zu erfüllen, dieser jedoch verstorben und ein neues Oberhaupt der christlichen Kirche noch immer nicht gewählt sei. Auf ihrer anschließenden Weiterreise waren sie schon an der Küste Kleinasiens in Laias (İskenderun/Alexandretta)[11] angekommen, als sie erfuhren, dass der Legat Tebaldo Visconti da Piacenza nunmehr als Papst Gregor X. gewählt worden war, und es erreichte sie dort auch ein Schreiben des neu gewählten Papstes, in dem sie aufgefordert wurden, unverzüglich nach Akkon zurückzukehren. Gregor X., der sich zum Zeitpunkt seiner Wahl als Kreuzfahrer in Palästina aufhielt, beauftragte dort die Polos nunmehr offiziell als Kirchenoberhaupt, ihre Reise zum Großkhan fortzusetzen, um diesen zum Christentum zu bekehren und als Bündnispartner gegen den Islam zu gewinnen. Dafür wurden ihnen zwei italienische Mönche (Bruder Nicolao von Vicenza und Bruder Wilhelm von Tripolis[12]) mitgegeben, die als gelehrte Männer und kenntnisreiche Theologen galten, auf der wieder aufgenommenen Reise in Richtung Asien jedoch bald umkehrten. Anschließend ging es über die den jungen Polo durch ihre bunten Basare beeindruckende Stadt Täbris weiter nach Saveh. Nach Marco Polo waren hier die heiligen drei Könige begraben. Von dort führte sie ihre Reise in die Oasenstadt Yazd, die mit durch Qanaten aus den Bergen hergeleitetem Wasser gespeist wurde. Marco Polo berichtete aus dieser Stadt, dass die dort hergestellten, Jasdi genannten Seidenstoffe von den ansässigen Kaufleuten mit gutem Gewinn veräußert würden.
Die Reise führte die Polos danach nach Kerman, wo die Juwelenhändler ihre Pferde wahrscheinlich gegen robustere Kamele eintauschten. Nächste Reisestationen waren Rajen, eine Stadt der Schmiede und Herstellungsort kunstvoller Stahlerzeugnisse, und Qamadin, die Endstation einer Route, auf der Pfeffer und andere Gewürze aus Indien herbeigeschafft wurden. Über diese heute zerstörte Stadt schrieb Marco Polo, dass sie oft von den aus Zentralasien eindringenden Tataren verwüstet worden sei. Der anschließende Besuch der Stadt Hormus, des heutigen Minab mit seinem mittlerweile versandeten Hafen, hinterließ bei Marco Polo einen starken Eindruck, denn dort wurden Gewürze, Edelsteine, Perlen, Seidenstoffe und Elfenbein umgeschlagen.
Von hier aus wollten die Handelsreisenden eigentlich über den Seeweg nach China aufbrechen, doch ließ sie der schlechte Zustand der Schiffe in Hormus von ihren Plänen Abstand nehmen. Durch die jetzt notwendigen erheblichen Umwege gelangte Marco Polo 1273 bis vor die Ruinen der Stadt Balch. Die Stadt soll durch die Truppen Dschingis Khans zerstört worden sein. Marco Polo schrieb dazu: „Es standen hier herrliche Paläste und prächtige Marmorvillen, aber heute sind es Ruinen.“ Auch in der Stadt Taluquan machten sie halt – Marco Polo beschreibt die Umgebung der Stadt als „sehr schön“. Ihm gefallen besonders die goldgelben Reisfelder, die Pappelalleen und die Bewässerungskanäle. Die Stadt Faisabad war damals berühmt für ihre blaugrünen Lapislazuli-Edelsteine, angeblich die feinsten Lapislazuli der Welt.
Die weitere Reise führte über die Orte Ischkaschim, Qala Panja, 1274 über die am Westrand der Sandwüste Taklamakan gelegenen Stadt Kaschgar weiter entlang der Südroute der sich dort aufzweigenden Seidenstraße auch zur Oasenstadt Nanhu. Marco Polo berichtet hier von „Geistern, die einen Nachzügler fortlocken konnten, indem sie ihn mit Stimmen riefen, die denen seiner Gefährten täuschend ähnelten. Und nicht selten meinte man, verschiedene Musikinstrumente, besonders Trommeln, zu vernehmen“. Heute wird als Ursache für solche Sinnestäuschungen der durch die Dünen wehende Sand oder pfeifender Wüstenwind angenommen.
Die Stadt Shazhou, heute Dunhuang, war ein bedeutender Knotenpunkt der damaligen Handelsstraßen, da dort auch die Süd- und Nordroute zur Umgehung der Wüste Taklamakan wieder zusammentrafen. Marco Polo, der nun endgültig chinesisches Land erreicht hatte, sah seinem Bericht nach in dieser bedeutenden Oasenstadt erstmals eine große Zahl von Chinesen, die sich in einem der damals größten buddhistischen Zentren Chinas angesiedelt hatten. Die Reisegruppe durchquerte anschließend die Städte Anxi, Yumen und Zhangye und kam 1275 in Shangdu als ihrem eigentlichen Reiseziel an. Dort traf Marco Polo Kublai Khan, den Großkhan der Mongolen und Enkel von Dschingis Khan, in seiner Sommerresidenz. Kublais Reich erstreckte sich damals von China bis in das Gebiet des heutigen Irak und im Norden bis nach Russland. Die drei Handelsreisenden ließen sich hier unter der Obhut des Herrschers bis 1291 nieder.
Der Großkhan fand Gefallen an dem jungen Europäer und ernannte ihn zu seinem Präfekten. Als solcher durchstreifte Marco Polo China über mehrere Jahre nach allen Himmelsrichtungen. Dabei gelangte er über die Städte Daidu und Chang’an (heute: Xi’an) in die Stadt Dali, wo die Leute, damals wie heute, rohes Schweinefleisch mit Knoblauch und Sojasoße essen. Seinem Bericht zufolge kam Marco Polo das offenbar ziemlich „barbarisch“ vor, da er selbst aus einer Kultur stammte, die solche Essgewohnheiten nicht kannte. Über die Stadt Kunming reiste er weiter nach Yangzhou, dem damaligen Sitz der Regionalregierung. In den zahlreichen Handwerksbetrieben dieser Stadt wurden Harnische für die Armee des Khan hergestellt. Anschließend berichtet Marco Polo von der Ankunft in seiner Lieblingsstadt Quinsai, dem heutigen Hangzhou. Er schwärmt von prächtigen Palästen und öffentlichen Warmbädern sowie vom Hafen, in dem Schiffe aus ganz Asien einliefen und Gewürze, Perlen und Edelsteine ausluden. Später wird auch erstmals Japan unter dem Namen Cipangu erwähnt.
Als unruhige Zeiten auszubrechen drohten, wollten die Polos zurück nach Venedig reisen. Trotz ihrer Bittgesuche ließ der Großkhan sie nicht ziehen, da sie ihm inzwischen eine wertvolle Stütze geworden waren. Zu diesem Zeitpunkt erschienen drei persische Diplomaten mit ihrem Gefolge am Hofe Kubilai Khans und baten um eine Braut für den Khan Arghun des persischen Il-Khanats. Der Mongolenherrscher bestimmte die siebzehnjährige Prinzessin Kököchin zur Vermählung, die nach Persien geführt werden sollte. Da der Landweg zu gefährlich war, ergriffen die Kaufleute diese Gelegenheit und schlugen dem Großkhan vor, die Prinzessin zusammen mit den Diplomaten auf dem Seeweg sicher nach Persien zu geleiten. Widerstrebend nahm dieser schließlich das einzig aussichtsreiche Angebot an und erlaubte ihnen damit letztlich die Heimreise.
Die Rückreise nach Venedig auf dem Seeweg begann 1291 im Hafen von Quanzhou, einer kosmopolitischen Stadt mit Niederlassungen aller wichtigen Religionen. Sie erfolgte auf 14 Dschunken mit insgesamt 600 Passagieren, von denen am Ende nur 17 überlebten. Auf den Zwischenstationen in Sumatra und Ceylon (heute Sri Lanka) lernte Marco Polo die dortigen Kulturen kennen und beschrieb sie später in seinem Reisebericht. Nach 18 Monaten der Weiterfahrt erreichte das Schiff den persischen Hafen Hormus. Später am Schwarzen Meer im Kaiserreich Trapezunt, dem heutigen Trabzon, konfiszierten die dortigen Beamten von den Seefahrern etwa 500 Kilogramm Rohseide, die die Polos mit nach Hause bringen wollten.
1295 erreichten die Reisenden schließlich die Republik Venedig und sollen zunächst von ihren Verwandten nicht erkannt worden sein. Angeblich gaben sie sich dadurch zu erkennen, dass sie die Säume ihrer Kleidung aufschnitten und die mitgebrachten Edelsteine hervorholten.
Von seinem anschließenden Aufenthalt in Venedig ist heute nicht viel bekannt. Sicher ist, dass er drei Töchter hatte, zwei Prozesse wegen Kleinigkeiten führte und im Stadtteil Cannaregio ein kleines Haus nahe dem corte del Milion erwarb. Nach dem Zeitgenossen und ersten Biographen Jacopo d’Aqui gab damals die Bevölkerung Marco Polo diesen Namen, da er unaufhörlich von den Millionen des großen Khan und seinem eigenen Reichtum redete.[13]
Nach Angaben des Chronisten und Biographen Giovan Battista Ramusio, der aus ihm einen „Helden der Serenissima“ gemacht hatte[14], nahm Marco Polo einige Zeit später als Flottenkommandant an einem Seekrieg teil, in den Venedig schon seit Jahren mit seinem Erzrivalen Genua verstrickt war.[13] In der Seeschlacht bei Curzola soll er 1298 eine venezianische Galeere geführt und dabei in genuesische Gefangenschaft geraten sein, in der er bis Mai 1299 festgehalten wurde. In dem als Gefängnis genutzten Palazzo San Giorgio[15] wurde er angeblich von dem auch als Autor von Ritterromanen bekannten Mitgefangenen Rustichello da Pisa[16] gedrängt, diesem den Bericht seiner Fernost-Reise zu diktieren. Das Ergebnis ging in die Literaturgeschichte ein als Le divisament dou monde („Die Aufteilung der Welt“), französisch unter dem Titel Le Livre des merveilles du monde („Das Buch von den Wundern der Welt“).
Die Reiseabenteuer des Marco Polo wurden in den nachfolgenden zwei Jahrhunderten sehr viel gelesen, denn rund 150 Handschriften sind erhalten, darunter auch von Übersetzungen in andere Sprachen, zum Beispiel ins Toskanische als Libro delle meravigilie del mondo, später unter dem Titel Il Milione, oder ins Venezianische. Die größte Verbreitung fand die lateinische Übersetzung des Dominikaners Francesco Pipino aus Bologna, die allein in über 50 Handschriften erhalten ist. Darüber hinaus wurde das Buch von Gelehrten aller Art ausgewertet, vor allem Geographen, die Polos sehr exakt wirkende Entfernungsangaben für ihre Karten übernahmen. Noch Christoph Kolumbus benutzte diese Angaben zur Errechnung der Länge einer Seefahrt nach las Indias, womit er die Stadt Quinsay meinte, das heutige Hangzhou auf dem chinesischen Festland.[17] Er kalkulierte dabei aber zu optimistisch. Kolumbus besaß eine reichlich mit eigenen Anmerkungen versehene Abschrift des Reiseberichts Il Milione, die heute in einem Museum in Sevilla aufbewahrt wird.[18] Der Erstdruck erfolgte 1477 in Nürnberg.[19]
Als bereits schwer kranker Mann schrieb Marco Polo kurz vor seinem Tode Anfang Januar 1324 sein Testament, das erhalten geblieben ist.[20] Hieraus geht hervor, dass er bald nach seiner Freilassung im Jahre 1299 und seiner Rückkehr aus Genua in Venedig Donata Badoer, die Tochter des Kaufmanns Vidal Badoer, heiratete und später Vater von drei Töchtern mit Namen Fantina, Bellela und Moreta wurde, von denen die beiden ersten im Jahre 1324 schon verheiratet waren. Er hinterließ eine goldene Tafel und verfügte die Freilassung seines mongolischen Sklaven Piedro Tartarino.[13]
Marco Polos Haus befand sich etwa am rechtwinkligen Zusammentreffen des Rio di San Giovanni Crisòstomo und des Rio di San Lio, vermutlich am Corte seconda del Milion 5845-5847[21] oder dort, wo jetzt das Teatro Malibran ist. Es ist 1596 abgebrannt.
Von seinem Vater Niccolò Polo ist nur bekannt, dass er um 1300 verstarb, und von seinem Onkel Maffeo kennt man nur ein 1310 angefertigtes Testament, das die Schenkung von drei Goldtafeln durch den Großkhan auf ihrer ersten Reise belegt.[13]
Im Jahr 1324 starb Marco Polo. Da Kritiker schon damals seine Erzählungen für unwahr hielten, wurde er zuletzt von Priestern, Freunden und Verwandten aufgefordert, um seines Seelenheiles willen den Lügengeschichten doch endlich abzuschwören. Dem Bericht des Chronisten Jacopo d’Aqui zufolge soll Marco Polo jedoch auf dem Sterbebett erwidert haben: „Ich habe nicht die Hälfte dessen erzählt, was ich gesehen habe!“[22][23]
Angeblich wurde Marco Polo nach seinem Tode in der Benediktinerkirche von San Lorenzo (Venedig) beigesetzt, in der sein Vater ebenfalls begraben war. Diese Grabstätten sollen beim Umbau der Kirche in den Jahren 1580 bis 1616 verlorengegangen sein. Nach anderen Angaben wurde er in der heute nicht mehr existierenden Kirche San Sebastiano begraben.[24] Sein Erbe hatte einen Wert von mehr als 70 Kilogramm Gold.[19]
Seine von ihm im Reisebericht genannte Reise nach China auf Land und zurück auf See, die Rustichello für ihn niedergeschrieben hatte, und die dabei von ihm beschriebenen Entdeckungen trugen entscheidend zu den späteren Entdeckungen im 15. und 16. Jahrhundert bei und damit auch zur Welt, wie wir sie heutzutage erleben. Dafür wird Marco Polo in der heutigen Welt weiterhin gewürdigt.
Der Mondkrater Marco Polo ist nach ihm benannt, ebenso seit 2002 der Asteroid (29457) Marcopolo.[25]
Marco Polo wurde auf der Vorderseite der italienischen 1000-Lire-Banknote abgebildet, die von der Banca d’Italia zwischen 1982 und 1991 ausgegeben wurde.
Über Marco Polo selbst ist nur wenig überliefert, aber immerhin gibt es rund 150 Handschriften seines Reiseberichtes. Der Schotte Henry Yule konnte allein schon 78 Manuskripte nachweisen. Davon sind 41 in Latein, 21 auf Italienisch, zehn auf Französisch und vier in deutscher Sprache verfasst worden.[26] Es wird von vielen Forschern angenommen, dass Polo seine Erlebnisse nicht eigenhändig selbst im Gefängnis aufschrieb, sondern höchstens über Notizen verfügte, die er dem Rustichello da Pisa diktierte. Die vergleichenden Forschungen führten zu dem Ergebnis, dass ein Manuskript in altfranzösischer Sprache eine sehr große Nähe zur Urfassung hat. Damit ist das französische Manuskript gemeint, das die Geographische Gesellschaft zu Paris im Jahre 1824 veröffentlichte und das seitdem als der „Geographische Text“ bezeichnet wird. Von weiterem besonderen Interesse für die Forscher sind in dieser Hinsicht auch ein franko-italienischer Text und das lateinische „Zelada-Manuskript“, die beide ebenfalls als der Urfassung sehr nahestehend betrachtet werden.[27] Darüber, dass von diesen drei frühesten Manuskripten eines der Originalfassung am allernächsten steht, gibt es bislang keine Einigkeit.
Nach Ansicht von Barbara Wehr, Professorin für Französische und Italienische Sprachwissenschaft der Johannes Gutenberg-Universität Mainz, muss die bislang vorherrschende Auffassung, Marco Polo habe seinen Reisebericht dem Rustichello da Pisa diktiert und die Sprache des Urtexts sei Altfranzösisch, möglicherweise korrigiert werden.[28] Ihrer Ansicht nach gibt es mit der lateinischen Fassung von Francesco Pipino da Bologna einen Strang der Textüberlieferung, der keinerlei Spuren der französischen Version von Rustichello da Pisa aufweist. Sie schließt daraus, dass Rustichello da Pisa sich erst nachträglich in die Textüberlieferung eingemischt hat und dass es einen Urtext gab, der direkt aus der Feder Marco Polos stammte und auf Altvenezianisch verfasst war.[28]
Die Frage, ob Marco Polo wirklich in China gewesen ist, beschäftigt seit Jahrhunderten Forscher und Wissenschaftler, denn es gibt nur indirekte Beweise für seinen Aufenthalt in diesem Land; er selbst wird dort nirgendwo namentlich erwähnt. Letzteres kann allerdings damit zusammenhängen, dass sein mongolischer bzw. chinesischer Name unbekannt ist.[29] Marco Polos Darstellung seiner eigenen Bedeutung erscheint manchen Historikern in einzelnen Aspekten auch als übertrieben.[30]
Zunächst stellte John W. Haeger 1978 mit kritischen Anmerkungen lediglich den Aufenthalt Marco Polos in Südchina in Frage, wobei er es allerdings für möglich hielt, dass dieser doch mit Kublai Khan zusammengetroffen war.[31] 1995 entfachte Frances Wood, Historikerin und Kuratorin der Chinesischen Sammlungen in der British Library, die Diskussion in ihrem Buch Marco Polo kam nicht bis China von neuem. Sie vertritt die These, Marco Polo habe in seinem Reisebericht nur Erzählungen von anderen Chinareisenden niedergeschrieben, sei aber selbst nicht dort gewesen.[32] Diese Aussage begründet sie unter anderem damit, dass in Marco Polos Reisebeschreibungen wesentliche Besonderheiten der chinesischen Kultur nicht erwähnt werden. Trotzdem sei es vor allem ihm zu verdanken, dass sich ein reger Verkehr zwischen West und Ost entwickelte.[33] Woods Ausführungen wurden in der Fachwelt unterschiedlich, jedoch im Wesentlichen ablehnend diskutiert.[34]
Ein herausragendes Beispiel für die Begründung ihrer These sieht Wood etwa in der Tatsache, dass Marco Polo die Chinesische Mauer nicht erwähnt. Dies hatte schon 1667 Athanasius Kircher verwundert.[35] Zweifel an Polos Anwesenheit vor Ort äußerte wegen dieser Nichterwähnung dann 1747 der anonyme Herausgeber von Thomas Astleys New general collection of voyages and travels.[36] In der Polo-Forschung wird diesem Argument jedoch keine Bedeutung beigemessen, sondern vielmehr darauf verwiesen, dass nach dem Ergebnis der Forschungen von Arthur Waldron die Große Mauer in ihrer seither weltberühmt gewordenen Anlage erst durch deren Jahrhunderte späteren Ausbau während der Ming-Dynastie entstand,[37] während die älteren Befestigungsanlagen, die auch schon zu Polos Zeit vorhanden waren, selbst in der mongolischen und chinesischen Überlieferung dieser Zeit noch keine herausgehobene Rolle spielten.[38] Die zur Zeit Marco Polos herrschenden Mongolen hatten kurz zuvor China erobert, dabei die damalige Version der chinesischen Mauer überwunden, waren mit ihrer Reiterei ganz auf Bewegungskrieg ausgerichtet und nicht auf statische Befestigungen. Es erscheint daher logisch, dass dieses Bauwerk während der Herrschaft der Mongolen vernachlässigt wurde. Zudem lag die Mauer vor allem im Norden und im Westen am Rande des Reiches, und es gab damals für die wenigsten Besucher einen Anlass, die noch vorhandenen Reste der Mauer zu besuchen.
Marco Polos Bericht über seine Rückreise als Begleitung einer mongolischen Prinzessin, die als Gemahlin für den Khan des persischen Il-Khanats ausersehen war, hält Frances Wood für eine Übernahme aus einem noch unbekannten Text. Der chinesische Historiker Yang Zhi Jiu hat dagegen eine Quelle gefunden und mehrfach beschrieben,[39] die sich stark mit Marco Polos Mitteilungen hinsichtlich der Reise deckt, die drei Polos allerdings nicht erwähnt.[40] Es handelt sich dabei um eine interne Anweisung Kublai Khans, die in der Yongle Dadian niedergeschrieben ist, der größten chinesischen mittelalterlichen Enzyklopädie, die erst zu Beginn des 15. Jahrhunderts vollendet wurde. Diese Anweisung gibt auch die Namen der drei Abgesandten des persischen Khans an, die mit der Prinzessin reisten.[41]
Eine weitere Quelle enthält ebenfalls einen kurzen Hinweis auf die Rückreise der Polos, ohne allerdings diese selbst zu erwähnen: Der persische Historiker Raschīd ad-Dīn spricht in seinem Werk Dschāmiʿ at-tawārīch („Die Universalgeschichte“), das er zu Beginn des 14. Jahrhunderts fertigstellte, kurz über die Ankunft der Gesandtschaft in Abhar in der Nähe von Qazvin im Iran und nennt dabei den Namen des einzigen überlebenden Gesandten.[42]
Beide Quellen sind erst in der Zeit nach Marco Polos China-Aufenthalt entstanden, können ihm also bei der Abfassung seines Buches nicht bekannt gewesen sein, entsprechen aber besonders hinsichtlich der Namen der Gesandten und deren Schicksal völlig seinen Angaben. Diese exakte Übereinstimmung wertet Yang als Beweis für die Anwesenheit Marco Polos im Reich der Mitte und führt die Nichterwähnung der Reisenden auf ihre relative Bedeutungslosigkeit seitens der Schreiber der Quellen zurück.[43]
John H. Pryor von der Universität Sydney sieht im Zusammenhang mit der Rückreise der Polos noch ein anderes Argument für deren Glaubwürdigkeit: Er weist darauf hin, dass die Angaben in Marco Polos Buch hinsichtlich des Aufenthalts an verschiedenen Rückreiseorten den Bedingungen entsprechen, welche die Windzyklen des Monsuns den Reisenden auf einer solchen Seeroute per Segelschiff vorgeben. Marco Polo selbst hat von diesen Windzyklen und -zirkulationen im Südchinesischen Meer, dem Golf von Bengalen und dem Indischen Ozean nichts gewusst, erwähnt sie auch nicht in seinem Buch und kann sie nach Pryor deshalb auch nicht von irgendwoher übernommen haben. Erst mit dem heutigen Kenntnisstand ist im Nachhinein erschließbar, dass die Windverhältnisse im Rückreisegebiet der Polos eben genau diese Zwischenaufenthalte zwangsläufig erforderlich gemacht hatten.[44]
Hans Ulrich Vogel von der Eberhard Karls Universität Tübingen argumentiert, dass die Präzision, mit der Marco Polo das chinesische Salzmonopol, die darauf basierenden Steuereinnahmen sowie das Papiergeldsystem der Yuan-Dynastie beschreibt, so nicht auch nur annähernd in einer anderen außerchinesischen Quelle vorkomme. Somit müsse Marco Polo über weit mehr als das flüchtige Wissen eines Durchreisenden verfügt haben, sondern sich in der Tat auf in langen Jahren erworbene Kenntnisse und Kontakte zur Regierung gestützt haben. Ein reines Zusammenstellen und Abschreiben bestehender Quellen sei mithin als äußerst unwahrscheinlich anzusehen.[45]
Belletristik
Bibliographie

Michael Faraday [ˈmaɪkəl ˈfærədeɪ] (* 22. September 1791 in Newington, Surrey; † 25. August 1867 in Hampton Court Green, Middlesex) war ein englischer Naturforscher, der als einer der bedeutendsten Experimentalphysiker gilt. Faradays Entdeckungen der „elektromagnetischen Rotation“ und der elektromagnetischen Induktion legten den Grundstein zur Herausbildung der Elektroindustrie. Seine anschaulichen Deutungen des magnetooptischen Effekts und des Diamagnetismus mittels Kraftlinien und Feldern führten zur Entwicklung der Theorie des Elektromagnetismus. Bereits um 1820 galt Faraday als führender chemischer Analytiker Großbritanniens. Er entdeckte eine Reihe von neuen Kohlenwasserstoffen, darunter Benzol und Buten, und formulierte die Grundgesetze der Elektrolyse.
Aufgewachsen in einfachen Verhältnissen und ausgebildet als Buchbinder, fand der von der Naturforschung begeisterte Faraday eine Anstellung als Laborgehilfe von Humphry Davy an der Royal Institution, die zu seiner wichtigsten Wirkungsstätte wurde. Im Labor der Royal Institution führte er seine wegbereitenden elektromagnetischen Experimente durch, in ihrem Hörsaal trug er mit seinen Weihnachtsvorlesungen dazu bei, neue wissenschaftliche Erkenntnisse zu verbreiten. 1833 wurde Faraday zum ersten Fuller-Professor für Chemie ernannt. Faraday führte etwa 30.000 Experimente durch und veröffentlichte 450 wissenschaftliche Artikel. Die wichtigsten seiner Publikationen zum Elektromagnetismus fasste er in seinen Experimental Researches in Electricity (Experimental-Untersuchungen über Elektrizität) zusammen. Sein populärstes Werk Chemical History of a Candle (Naturgeschichte einer Kerze) war die Mitschrift einer seiner Weihnachtsvorlesungen.
Im Auftrag des britischen Staates bildete Faraday mehr als zwanzig Jahre lang die Kadetten der Royal Military Academy in Woolwich in Chemie aus. Er arbeitete für eine Vielzahl von Behörden und öffentlichen Einrichtungen, beispielsweise für die Schifffahrtsbehörde Trinity House, das British Museum, das Home Office und das Board of Trade.
Faraday gehörte zu den Anhängern einer kleinen christlichen Minderheit, den Sandemanianern, an deren religiösem Leben er aktiv teilnahm.
Michael Faraday wurde am 22. September 1791 in Newington in der Grafschaft Surrey, das heute zum London Borough of Southwark gehört, geboren. Er war das dritte von vier Kindern des Schmieds James Faraday (1761–1810) und dessen Frau Margaret (geborene Hastwell, 1764–1838), einer Bauerntochter. Bis Anfang 1791[1] lebten seine Eltern mit seinen beiden älteren Geschwistern Elizabeth (1787–1847) und Robert (1788–1846) im kleinen Dorf Outhgill in der damaligen Grafschaft Westmorland im Nordwesten Englands (heute Cumbria). Als die Auswirkungen der Französischen Revolution zu einem Rückgang des Handels führten und die Familie von Armut bedroht war, beschloss sie, in die unmittelbare Nähe von London zu ziehen. Faradays Vater fand Arbeit beim Eisenwarenhändler James Boyd im Londoner Stadtteil West End. Die Familie zog kurz darauf in die Gilbert Street und etwa fünf Jahre später in die Jacob’s Well Mews. Dort wurde Faradays jüngere Schwester Margaret (1802–1862) geboren.
Bis zu seinem zwölften Lebensjahr besuchte Faraday eine einfache Tagesschule, wo ihm die Grundlagen des Lesens, Schreibens und Rechnens beigebracht wurden. 1804 fand er eine Anstellung als Laufbursche beim hugenottischen Auswanderer George Riebau, der in der Blanford Street einen Buchladen betrieb. Eine von Faradays Aufgaben bestand darin, am Morgen die Zeitung zu Riebaus Kunden zu bringen, sie im Laufe des Tages wieder abzuholen und zu weiteren Kunden zu tragen. Nach etwa einem Jahr als Laufbursche unterzeichnete Faraday am 7. Oktober 1805 einen siebenjährigen Lehrvertrag für eine Buchbinderlehre bei Riebau. Entsprechend den Gepflogenheiten der damaligen Zeit zog er zu seinem Lehrmeister und wohnte während seiner Ausbildung bei ihm.
Faraday erwies sich als ein geschickter, aufgeschlossener und wissbegieriger Lehrling. Er erlernte das Buchbinderhandwerk schnell und las aufmerksam viele der zum Binden gebrachten Bücher. Darunter befanden sich Jane Marcets 1806 erschienene Conversations on Chemistry, eine populäre Einführung in die Chemie, und der von James Tytler für die dritte Auflage der Encyclopædia Britannica verfasste Beitrag über Elektrizität, aber auch die Geschichte von Ali Baba sowie Nachschlagewerke und Zeitschriften über Kunst. Riebau gestattete ihm die Durchführung kleinerer chemischer und elektrischer Experimente.
Unter den Werken, die Faraday studierte, befand sich auch Isaac Watts’ Buch The Improvement of the Mind (1741), das sich an Leser richtete, die ihr Wissen und ihre geistigen Fähigkeiten selbständig erweitern wollten. Der Autor legte in seinen Ausführungen Wert darauf, Wissen nicht nur passiv zu vermitteln, sondern seine Leser dazu anzuregen, sich aktiv damit auseinanderzusetzen. Watts empfahl unter anderem, sich Notizen zu Artikeln zu machen, bei Vorträgen Mitschriften anzufertigen und den Gedankenaustausch mit Gleichgesinnten zu suchen.[2]
Unter diesem Eindruck begann Faraday 1809 eine von ihm The Philosophical Miscellany betitelte Sammlung von Notizen über Artikel zu den Themen Kunst und Wissenschaft, die er in verschiedenen Zeitungen und Zeitschriften gelesen hatte.[3] 1810 ermutigte Riebau den 19-jährigen Faraday, die jeden Montag vom Goldschmied John Tatum in seinem Haus abgehaltenen wissenschaftlichen Vorträge zu besuchen. Tatum war der Gründer der 1808 ins Leben gerufenen City Philosophical Society, deren Ziel es war, Handwerkern und Lehrlingen den Zugang zu wissenschaftlichen Kenntnissen zu ermöglichen. Für die Vorträge war jeweils eine Gebühr von einem Schilling zu entrichten, den Faraday von seinem Bruder Robert erhielt. Mit dieser Unterstützung konnte er vom 19. Februar 1810 an bis zum 26. September 1811 etwa ein Dutzend Vorträge besuchen.[4] Während Tatums Vorträgen fertigte Faraday Notizen an, die er in seiner freien Zeit überarbeitete, zusammenfasste und in ein Notizbuch übertrug. Bei Tatum freundete er sich mit den Quäkern Benjamin Abbott (1793–1870) und Edward Magrath (1791?–1861) sowie Richard Phillips (1778–1851) an. Mit Abbott begann er am 12. Juli 1812 einen schriftlichen Gedankenaustausch, der viele Jahre fortdauerte.[5]
Faraday, dessen Lehrzeit bei Riebau dem Ende entgegenging, verspürte wenig Neigung, sein Leben als Buchbinder zu verbringen. Er schrieb einen Brief an Joseph Banks, den Präsidenten der Royal Society, in dem er um eine niedrige Anstellung in den Laboratorien der Royal Society bat. Banks hielt es jedoch nicht für erforderlich, sein Ersuchen zu beantworten.[6] Am 8. Oktober 1812, einen Tag nach Ende seiner Lehrzeit, trat Faraday seine Tätigkeit als Buchbindergeselle bei Henri De La Roche an.[7]
Anfang 1812 zeigte Riebau dem Sohn von William Dance[8] (1755–1840), einem seiner Kunden, Faradays Notizbuch mit den Mitschriften von Tatums Vorträgen. Dance berichtete seinem Vater davon, der daraufhin Faraday zu Humphry Davys letzten vier Vorlesungen mit dem Titel The Elements of Chemical Philosophy als Professor der Chemie im März und April 1812 mitnahm. Davy galt als herausragender Dozent und hatte sich in der Fachwelt durch die Entdeckung der Elemente Kalium, Natrium und Chlor ein hohes Ansehen erworben. Während Davys Vorträge machte sich Faraday zahlreiche Notizen, die er, überarbeitet und mit Zeichnungen versehen, zu einem Buch band und an Davy schickte.
Ende Oktober 1812 befand sich Davy jedoch nicht in London, sondern wiederholte gemeinsam mit John George Children in Tunbridge Wells einen Versuch von Pierre Louis Dulong, der kurz zuvor eine neue Verbindung aus Chlor und Stickstoff entdeckt hatte. Während der Experimente explodierte ein Glasröhrchen mit dem entstandenen Stickstofftrichlorid und verletzte Davys linkes Auge schwer. Davy wurde umgehend zur Behandlung nach London gebracht und fand dort Faradays Sendung vor. Da er aufgrund seiner Augenverletzung zur Ordnung seiner Notizen Hilfe benötigte, lud er Faraday Ende des Jahres 1812 zu sich nach Hause ein.[9]
Am 19. Februar 1813[10] kam es an der Royal Institution zwischen dem Laborgehilfen William Payne und dem Instrumentenbauer John Newmann zu einer handgreiflichen Auseinandersetzung. Drei Tage später wurde Payne von den Managern der Royal Institution entlassen. Davy, der einen neuen Assistenten benötigte, schlug Faraday für den vakanten Posten vor. Am 1. März 1813 begann dieser seine Tätigkeit als Laborgehilfe an der Royal Institution. Seine Pflichten umfassten die Betreuung und Unterstützung der Vortragenden und Professoren bei der Vorbereitung und Durchführung ihrer Vorlesungen, das wöchentliche Reinigen der Modelle im Lager sowie das monatliche Entstauben der Instrumente in den Glaskästen.[11] Er bezog die zwei Räume seines Vorgängers und erhielt die Erlaubnis, das Labor für eigene Experimente zu benutzen.
Napoleon Bonaparte hatte Davy eine Goldmedaille für dessen Beiträge zur Elektrochemie verliehen, die dieser in Paris entgegennehmen wollte. Trotz der andauernden Napoleonischen Kriege erhielt er von der französischen Regierung die Erlaubnis, Kontinentaleuropa zu bereisen. Davy und seine Frau Jane Apreece (1780–1855) planten daher 1813 eine Reise durch Kontinentaleuropa, die auf zwei oder drei Jahre ausgelegt war und bis nach Konstantinopel führen sollte. Er bat Faraday, ihn als sein Amanuensis (Sekretär und wissenschaftlicher Gehilfe) zu begleiten. Das bot diesem, der sich noch nie „weiter als zwölf Meilen“ von London entfernt hatte, die Möglichkeit, von Davy zu lernen und mit einigen der bedeutendsten ausländischen Naturforscher in Kontakt zu kommen.
Am 13. Oktober 1813 verließ die fünfköpfige Reisegesellschaft London. In Plymouth schiffte sie sich nach Morlaix ein, wo sie durchsucht und für etwa eine Woche festgesetzt wurde. Am Abend des 27. Oktober erreichte sie schließlich Paris. Faraday erkundete die Stadt, die ihn sehr beeindruckte,[12] und besuchte gemeinsam mit Davy und dem Geologen Thomas Richard Underwood (1772–1835) das Musée Napoleon. Im Labor des Chemikers Louis-Nicolas Vauquelin beobachteten Davy und Faraday die Herstellung von Kaliumchlorid, die sich von der in England angewandten Methode unterschied. Am Morgen des 23. November suchten André-Marie Ampère, Nicolas Clément und Charles-Bernard Desormes Davy in seinem Hotel auf, präsentierten ihm eine zwei Jahre zuvor durch Bernard Courtois entdeckte Substanz und führten ihm einige Experimente vor, bei denen violette Dämpfe entstanden. Mit Faradays Hilfe führte Davy eigene Experimente durch, unter anderem im Labor von Eugène Chevreul im Jardin des Plantes. Am 11. Dezember wurde ihm klar, dass es sich bei der Substanz um ein neues Element handelte, das er nach dem griechischen Wort iodes für ‚violett‘ Iod nannte. Davys Experimente verzögerten die geplante Weiterreise nach Italien.
Am 29. Dezember 1813 verließen sie Paris in Richtung Mittelmeerküste, wo Davy hoffte, iodhaltige Pflanzen für seine Untersuchungen zu finden. Faraday wurde Anfang Februar in Montpellier Zeuge des Durchzugs von Papst Pius VII., der nach seiner Befreiung durch die Alliierten nach Italien zurückkehrte. Nach einem einmonatigen Aufenthalt setzten sie in Begleitung von Frédéric-Joseph Bérard (1789–1828) ihren Weg nach Italien fort. Über Nîmes und Nizza überquerten sie die Alpen über den Tenda-Pass. Während des beschwerlichen Weges von Stadt zu Stadt erklärte Davy Faraday die geologische Beschaffenheit der Landschaft und machte ihn mit den antiken Kulturstätten vertraut.
In Genua verhinderte schlechtes Wetter die Weiterreise. Davy nutzte die Verzögerung, um bei Domenico Viviani (1772–1840), der einige „Elektrische Fische“ in Gefangenschaft hielt, Experimente durchzuführen, mit denen er überprüfen wollte, ob die Entladung dieser Fische ausreichte, um Wasser zu zersetzen. Die Ergebnisse seiner Experimente waren negativ. Am 13. März überquerten sie mit dem Schiff den Golf von Genua. Einen Tag vor der Landung der britischen Armee in Livorno passierten sie Lucca und gelangten am 16. März nach Florenz, wo sie das Museum der Accademia del Cimento besuchten, in dem sich unter anderem Galileo Galileis Beobachtungsinstrumente befanden. Davy und Faraday setzten ihre Versuche mit Iod fort und bereiteten ein Experiment vor, das beweisen sollte, dass Diamanten aus reinem Kohlenstoff bestanden. Dazu verwendeten sie große Brenngläser[13] aus dem Besitz von Großherzog Ferdinand III. Am 27. März 1814 gelang dieser Nachweis zum ersten Mal. In den folgenden Tagen wiederholten die beiden das Experiment noch mehrere Male.
Die Ankunft in Rom erfolgte inmitten der Karwoche. Wie schon an anderen Orten erkundete Faraday die Stadt auf eigene Faust. Er war besonders vom Petersdom und dem Kolosseum beeindruckt. An der Accademia dei Lincei experimentierten Davy und Faraday mit Kohle, um einigen offenen Fragen aus dem Diamanten-Experiment nachzugehen. Am 5. Mai waren sie im Haus von Domenico Morichini (1773–1836) zu Gast. Dort wiederholte Faraday erfolglos unter der Anleitung des Hausherrn dessen Experiment zur vermeintlichen Magnetisierung einer Nadel durch den violetten Spektralanteil des Sonnenlichts. Zwei Tage später brachen sie zu einem zweiwöchigen Abstecher nach Neapel auf. Dort bestiegen sie mehrmals den Vesuv. Caroline Bonaparte, die Königin von Neapel, machte Davy ein Gefäß mit antiken Farbpigmenten zum Geschenk, die Davy und Faraday später analysierten.
Um der Sommerhitze zu entfliehen, brach die Reisegesellschaft am 2. Juni von Rom aus in Richtung Schweiz auf. Über Terni, Bologna, Mantua und Verona gelangten sie nach Mailand. Hier begegnete Faraday am 17. Juni Alessandro Volta. Sie kamen am 25. Juni 1814 in Genf an und verbrachten den Sommer bei Charles-Gaspard de la Rive in dessen Haus am Genfersee, jagten, fischten, experimentierten weiter mit Iod und arbeiteten mit Marc-Auguste Pictet und Nicolas-Théodore de Saussure zusammen. Am 18. September 1814 reisten sie über Lausanne, Vevey, Payerne, Bern, Zürich und den Rheinfall bei Schaffhausen schließlich nach München, wo sie drei Tage blieben.
Über den Brennerpass kehrten sie nach Italien zurück und besuchten dabei Padua und Venedig. In Florenz untersuchten sie ein brennbares Gas, das in Pietramala dem Erdboden entwich und das sie als Methan identifizierten. In Rom, wo sie am 2. November 1814 ankamen und bis zum März 1815 blieben, erlebte Faraday das Weihnachtsfest und besuchte während des Karnevals mehrere Maskenbälle. Davy und Faraday führten weitere Experimente mit Chlor und Iod durch. Ihre ursprünglichen Pläne, nach Konstantinopel weiterzureisen, zerschlugen sich. Nachdem sie Tirol und Deutschland durchquert hatten, erreichten sie am 23. April 1815 schließlich London.
Nach der Rückkehr war Faraday in London zunächst ohne Anstellung. Auf Wunsch von William Thomas Brande, der 1812 von Davy die Position des Professors für Chemie übernommen hatte, und mit voller Unterstützung durch Davy, der eine Woche zuvor zum Vizepräsidenten der Royal Institution gewählt worden war, erhielt Faraday am 15. Mai seinen alten Posten als Laborgehilfe wieder und war zusätzlich für die mineralogische Sammlung verantwortlich.
Faraday besuchte erneut die Vorträge der City Philosophical Society und wurde Mitglied der Gesellschaft. Am 17. Januar 1816[14] hielt er dort seinen ersten Vortrag über Chemie, dem in den nächsten zweieinhalb Jahren 16 weitere folgten. Um seine Fähigkeiten als Vortragender zu vervollkommnen, besuchte er 1818 die am Donnerstagabend an der Royal Institution abgehaltenen Rhetorikkurse von Benjamin Humphrey Smart (1786–1872). Gemeinsam mit vier Freunden gründete er im Sommer desselben Jahres einen Schreibzirkel. Die Mitglieder der nach den Richtlinien der City Philosophical Society organisierten Gruppe verfassten Aufsätze zu frei wählbaren oder festgelegten Themen, die anonym eingereicht und in der Gruppe gemeinsam bewertet wurden.[15]
Im Labor der Royal Institution führte Faraday häufig in Davys Auftrag Experimente durch und war 1816 maßgeblich an dessen Untersuchungen beteiligt, die zur Entwicklung der im Bergbau eingesetzten „Davy-Lampe“ führten. Für Brande, den Herausgeber des Quarterly Journal of Science, stellte Faraday ab 1816 die Miscellanea betitelten Seiten zusammen und übernahm im August 1816 während Brandes Abwesenheit die volle Verantwortung für das Journal.[16] 1816 erschien im Quarterly Journal of Science auch Faradays erste wissenschaftliche Veröffentlichung über aus der Toskana stammende Kalksteinproben. Bis Ende 1819 hatte er 37 Mitteilungen und Artikel im Quarterly Journal of Science veröffentlicht,[17] darunter eine Untersuchung über das Entweichen von Gasen aus Kapillarrohren und Bemerkungen über „singende Flammen“.
In seinem Labor führte Faraday für William Savage (1770–1843), den Drucker der Royal Institution, Papieranalysen durch, untersuchte Tonerdeproben für den Keramikproduzenten Josiah Wedgwood II (1769–1843) und nahm in gerichtlichem Auftrag kriminaltechnische Untersuchungen vor.[18] Anfang 1819 begann Faraday gemeinsam mit James Stodart (1760–1823), der chirurgische Instrumente herstellte, eine umfangreiche Reihe von Experimenten, die sich mit der Verbesserung von Stahllegierungen beschäftigten. Er untersuchte zunächst Wootz, ein weit verbreitetes Ausgangsprodukt für Stahl, auf dessen chemische Zusammensetzung.[19] Es folgten zahlreiche Versuche zur Veredelung von Stahl, bei denen unter anderem Platin und Rhodium zum Einsatz kamen.[20][21] Die Stahluntersuchungen erstreckten sich über einen Zeitraum von etwa fünf Jahren und wurden nach Stodarts Tod von Faraday alleine fortgeführt.[22]
Am 21. Dezember 1820 wurde Faradays erste für den Abdruck in den Philosophical Transactions bestimmte Abhandlung vor den Mitgliedern der Royal Society verlesen. Darin wurden die beiden neuen von ihm entdeckten Chlorkohlenstoffverbindungen Tetrachlorethen und Hexachlorethan beschrieben.[23] Zu dieser Zeit galt Faraday bereits als Großbritanniens führender chemischer Analytiker.[24] 1821 wurde er zum „Superintendent of the House“ der Royal Institution ernannt. Am 12. Juni 1821 heiratete er Sarah Barnard (1800–1879), die Schwester seines Freundes Eduard Barnard (1796–1867), die er im Herbst 1819 kennengelernt hatte. Ihre Ehe blieb kinderlos.
1821 bat Richard Phillips, mittlerweile Herausgeber der Annals of Philosophy, Faraday um einen Abriss aller bekannten Erkenntnisse über Elektrizität und Magnetismus. Kurz zuvor hatte Hans Christian Ørsted seine Beobachtungen über die Ablenkung einer Kompassnadel durch elektrischen Strom veröffentlicht. Faraday wiederholte in seinem Labor Experimente von Ørsted, André-Marie Ampère und François Arago. Sein zweiteiliger Historical Sketch of Electro-Magnetism erschien, auf seinen Wunsch anonym, im September und Oktober 1821 in den Annals of Philosophy.[25] Am 3. September[26] gelang Faraday zum ersten Mal ein Experiment, bei dem sich ein stromdurchflossener Leiter unter dem Einfluss eines Dauermagneten um seine eigene Achse drehte. Noch im gleichen Monat veröffentlichte er seine Entdeckung im Quarterly Journal of Science.[27] Die sogenannte „elektromagnetische Rotation“ war eine wesentliche Voraussetzung für die Entwicklung des Elektromotors.
Bereits wenige Tage nach Veröffentlichung seiner Entdeckung bezweifelten Freunde von William Hyde Wollaston, darunter Davy, die Eigenständigkeit der Arbeit Faradays. Sie bezichtigten ihn, die Idee „elektromagnetische Rotation“ von Wollaston gestohlen und dessen Autorschaft nicht gewürdigt zu haben. Faradays experimenteller Nachweis unterschied sich jedoch völlig von der von Wollaston vorgeschlagenen Lösung, was dieser auch anerkannte. Da die Gerüchte in der Öffentlichkeit darüber nicht abebbten, war Faraday gezwungen, die Autorschaft seines Historical Sketch of Electro-Magnetism offenzulegen.[28]
Im Jahr 1818 hatte Michael Faraday die einschläfernde Wirkung des „Schwefeläthers“ beschrieben.[29] 1823 begann Faraday die Eigenschaften des von Davy entdeckten Chlorhydrats zu untersuchen.[30] Als er es unter Druck erhitzte, gelang ihm zum ersten Mal die Verflüssigung von Chlor.[31] 1823 und nochmals 1844, als er sich erneut mit dem Thema beschäftigte, gelang es ihm, Ammoniak, Kohlenstoffdioxid, Schwefeldioxid, Distickstoffmonoxid, Chlorwasserstoff, Schwefelwasserstoff, Dicyan und Ethen zu verflüssigen. Faraday erkannte als Erster, dass eine kritische Temperatur existierte, oberhalb derer sich Gase unabhängig vom ausgeübten Druck nicht mehr verflüssigen ließen. Er wies nach, dass die Zustände „fest“, „flüssig“ und „gasförmig“ ineinander überführbar waren und keine festen Kategorien bildeten.[32]
1825 fielen Faraday in Kannen mit Leuchtgas, die sein bei der London Gas Company arbeitender Bruder Robert der Royal Institution lieferte, flüssige Rückstände auf. Er analysierte die Flüssigkeit und entdeckte eine neue Kohlenwasserstoff-Verbindung, die er als „Bicarburet of Hydrogen“ bezeichnete.[33] Von Eilhard Mitscherlich erhielt diese Substanz, ein aromatischer Kohlenwasserstoff, im selben Jahr die Bezeichnung Benzol. Kurz darauf entdeckte er mit Buten eine Verbindung, die die gleiche Verhältnisformel wie Ethen hatte, sich aber in den chemischen Eigenschaften völlig unterschied. 1826 ermittelte Faraday die Zusammensetzung von Naphthalin und stellte zwei verschiedene kristalline Proben von Naphthalinschwefelsäure her.
Im April 1827 erschien Chemical Manipulation. Diese Monografie Faradays war eine Einführung in die praktische Chemie und richtete sich an Anfänger auf dem Gebiet der chemischen Naturforschung. Sie umfasste alle Belange der praktischen Chemie, beginnend mit der zweckmäßigen Einrichtung eines Laboratoriums über die zweckmäßige Durchführung chemischer Experimente bis hin zur Fehleranalyse. Der Erstausgabe folgten 1830 und 1842 zwei weitere Auflagen.[34]
Am 1. April 1824 gründeten die Royal Society und das Board of Longitude eine gemeinsame Kommission (Committee for the Improvement of Glass for Optical Purposes). Sie hatte das Ziel, Rezepturen für die Herstellung hochwertiger optischer Gläser zu finden, die mit den von Joseph von Fraunhofer in Deutschland hergestellten Flintgläsern konkurrieren konnten. Die Untersuchungen fanden anfangs in den von Apsley Pellatt (1763–1826) und James Green betriebenen Falcon Glass Works statt. Um die Durchführung der Experimente direkter überwachen zu können, wurde am 5. Mai 1825 ein Unterkomitee berufen, das aus John Herschel, George Dollond und Faraday bestand. Nach der Errichtung eines neuen Schmelzofens an der Royal Institution wurden die Glasuntersuchungen ab September 1827 an der Royal Institution durchgeführt. Zur Entlastung Faradays wurde am 3. Dezember 1827 Charles Anderson, ein ehemaliger Sergeant der Royal Artillery, eingestellt. Die Glasuntersuchungen waren für über fünf Jahre Faradays Hauptaufgabe und Ende 1829 das Thema seiner ersten Baker-Vorlesung vor der Royal Society. 1830 wurden die Glasexperimente aus finanziellen Gründen gestoppt. Ein 1831 vorgelegter Bericht der Astronomen Henry Kater (1777–1835) und John Pond, die ein Teleskop mit einem Objektiv aus einem von Faraday hergestellten Glas testeten, bescheinigte dem Glas gute achromatische Eigenschaften. Faraday hielt die Ergebnisse seiner fünfjährigen Arbeit jedoch für unzulänglich.[35]
Auf Betreiben seines Freundes Richard Phillips, der kurz zuvor selbst in die Royal Society aufgenommen worden war, wurde am 1. Mai 1823 zum ersten Mal der Antrag zur Aufnahme von Faraday in die Gesellschaft verlesen. Der Antrag trug die Unterschrift von 29 Mitgliedern und musste an zehn aufeinanderfolgenden Sitzungen verlesen werden.[36] Davy, seit 1820 Präsident der Royal Society, wollte die Wahl Faradays verhindern und versuchte, die Rücknahme des Antrages zu erwirken. Mit einer Gegenstimme[37] wurde Faraday am 8. Januar 1824 in die Royal Society aufgenommen.
Von März bis Juni 1824[38] fungierte Faraday aushilfsweise als erster Sekretär des von Davy mitgegründeten Londoner Clubs The Athenaeum. Als ihm im Mai vorgeschlagen wurde, den Posten für ein Jahresgehalt von 100 Pfund dauerhaft zu übernehmen, schlug er das Angebot aus und empfahl seinen Freund Edward Magrath für diese Position.
Am 7. Februar 1825 wurde Faraday zum Labordirektor der Royal Institution ernannt und begann dort die ersten eigenen Vorträge abzuhalten. Im Februar 1826 wurde er von der Verpflichtung befreit, Brande bei dessen Vorlesungen zu assistieren. 1827 hielt Faraday Chemievorlesungen an der London Institution und gab die erste seiner zahlreichen Weihnachtsvorlesungen. Ein Angebot, erster Professor für Chemie an der neu gegründeten University of London zu werden, lehnte er mit einem Hinweis auf seine Verpflichtungen an der Royal Institution ab. 1828 wurde er mit der Fuller-Medaille geehrt. Bis 1831 half er Brande bei der Herausgabe des Quarterly Journal of Science und betreute anschließend die ersten fünf Ausgaben des neuen Journal of the Royal Institution.
Bereits 1822 merkte Faraday in seinem Notizbuch an: „Convert magnetism into electricity“ („Magnetismus in Elektrizität umwandeln“).[24] In dem im September 1820 begonnenen Labortagebuch notierte er am 28. Dezember 1824 erstmals ein Experiment, mit dem er versuchte, mit Hilfe von Magnetismus Elektrizität zu erzeugen. Der erwartete elektrische Strom blieb jedoch aus.[39] Am 28. und 29. November 1825 sowie am 22. April 1826 führte er weitere Versuche durch, ohne jedoch das gewünschte Ergebnis zu erzielen.
Nach einer durch die aufwändigen Glasuntersuchungen bedingten fünfjährigen Pause wandte sich Faraday am 29. August 1831 erstmals wieder elektromagnetischen Experimenten zu. Er hatte von seinem Assistenten Anderson einen Weicheisenring mit einem Innendurchmesser von sechs Zoll (etwa 15 Zentimeter) anfertigen lassen. Auf der einen Seite des Ringes brachte er drei Wicklungen aus Kupferdraht an, die durch Bindfaden und Kattun voneinander isoliert waren. Auf der anderen Seite des Ringes befanden sich zwei solcher Wicklungen. Er verlängerte auf der einen Seite die beiden Enden einer der Wicklungen mit einem langen Kupferdraht, der zu einer etwa drei Fuß (etwa ein Meter) entfernten Magnetnadel führte. Eine der Wicklungen auf der anderen Seite verband er mit den Polen einer Batterie. Jedes Mal, wenn er den Stromkreis schloss, bewegte sich die Magnetnadel aus ihrer Ruhelage. Beim Öffnen des Stromkreises bewegte sich die Nadel erneut, nur diesmal in die entgegengesetzte Richtung. Faraday hatte die elektromagnetische Induktion entdeckt und dabei ein Prinzip angewandt, das den später entwickelten Transformatoren zugrunde liegt. Seine Experimente, die bis zum 4. November andauerten, unterbrach er für einen dreiwöchigen Ferienaufenthalt mit seiner Frau in Hastings und eine vierzehntägige Untersuchung für die Royal Mint. Während seiner an nur elf Tagen[40] durchgeführten Experimente fand er heraus, dass ein zylindrischer Stabmagnet, der durch eine Drahtwendel bewegt wurde, eine elektrische Spannung in dieser induzierte. Nach diesem Grundprinzip arbeiten elektrische Generatoren.[41][42][43][44]
Faradays Bericht über die Entdeckung der elektromagnetischen Induktion[45] wurde von ihm Ende 1831 vor der Royal Society vorgetragen. Die in den Philosophical Transactions abgedruckte Form erschien erst im Mai 1832. Die lange Verzögerung ergab sich aus einer Änderung der Veröffentlichungsbedingungen für neue Artikel. Bis Ende 1831 reichte ein Mehrheitsbeschluss des Committee of Papers zur Veröffentlichung eines Artikels in den Philosophical Transactions. Die geänderten Regeln sahen eine individuelle Begutachtung der Artikel vor. Das Gutachten zu Faradays Artikel schrieben der Mathematiker Samuel Hunter Christie und der Mediziner John Bostock (1773–1846).[46]
Im Dezember 1831 schrieb Faraday an seinen langjährigen französischen Briefpartner Jean Nicolas Pierre Hachette und teilte ihm darin seine jüngsten Entdeckungen mit. Hachette zeigte den Brief dem Sekretär des Institut de France, François Arago, der das Schreiben am 26. Dezember 1831 vor den Mitgliedern des Instituts verlas. In den französischen Zeitungen Le Temps und Le Lycée erschienen am 28. bzw. 29. Dezember 1831 Berichte über Faradays Entdeckung. Der Londoner Morning Advertiser druckte diese am 6. Januar 1832 nach. Die Presseberichte bedrohten die Priorität seiner Entdeckung, da die Italiener Leopoldo Nobili und Vincenzo Antinori (1792–1865) in Florenz einige Versuche Faradays wiederholt hatten und ihre in der Zeitschrift Antologia[47] veröffentlichten Ergebnisse vor Faradays Aufsatz in den Philosophical Transactions erschienen.[48]
Nach seiner Entdeckung, dass Magnetismus Elektrizität zu erzeugen vermag, stellte sich Faraday die Aufgabe nachzuweisen, dass unabhängig davon, wie Elektrizität erzeugt wird, diese immer gleichartig wirkt. Am 25. August 1832 begann er mit den bekannten Elektrizitätsquellen zu arbeiten. Er verglich die Wirkungen von voltaischer Elektrizität, Reibungselektrizität, Thermoelektrizität, tierischer Elektrizität und magnetischer Elektrizität. In seinem am 10. und 17. Januar[49] verlesenen Beitrag gelangte er aufgrund seiner Experimente zum Schluss, „…daß die Elektricität, aus welcher Quelle sie auch entsprungen sey, identisch ist in ihrer Natur“.[50]
Ende Dezember 1832 stellte sich Faraday die Frage, ob ein elektrischer Strom in der Lage wäre, einen festen Körper – beispielsweise Eis – zu zersetzen. Bei seinen Experimenten stellte er fest, dass sich Eis im Gegensatz zu Wasser wie ein Nichtleiter verhielt. Er testete eine Reihe von Substanzen mit niedrigem Schmelzpunkt und beobachtete, dass ein nichtleitender fester Körper nach dem Übergang in die flüssige Phase den Strom leitete und sich unter dem Einfluss des Stromes chemisch zersetzte. Am 23. Mai 1833 sprach er vor der Royal Society Über ein neues Gesetz der Elektrizitätsleitung.[51]
Diese Untersuchungen führten Faraday direkt zu seinen Experimenten über die „elektro-chemische Zersetzung“, die ihn ein Jahr lang beschäftigten. Er sichtete die vorhandenen Ansichten, insbesondere die von Theodor Grotthuß und Davy, und kam zu der Auffassung, dass die Zersetzung im Inneren der Flüssigkeit vor sich ging und die elektrischen Pole nur die Rolle einer Begrenzung der Flüssigkeit spielten.
Unzufrieden mit den ihm für die Beschreibung der chemischen Zersetzung unter dem Einfluss eines elektrischen Stromes zur Verfügung stehenden Begriffen, wandte sich Faraday Anfang 1834 an William Whewell und diskutierte darüber auch mit seinem Arzt Whitlock Nicholl. Letzterer schlug Faraday vor, zur Beschreibung des Vorgangs der elektrochemischen Zersetzung die Begriffe Elektrode für die Ein- und Austrittsflächen des Stromes, Elektrolyse für den Vorgang selbst und Elektrolyt für die betroffene Substanz zu verwenden. Whewell, der die polare Natur des Vorganges kenntlicher machen wollte, prägte für die beiden Elektroden die Termini Anode und Kathode sowie für die betroffenen Teilchen die Begriffe Anion, Kation und Ion.[52] Zu Beginn der siebenten Folge seiner Experimental Researches in Electricity, die er am 9. Januar 1834 der Royal Society vorlegte,[53] schlug Faraday die neuen Begriffe zur Beschreibung des Vorgangs der elektrochemischen Zersetzung (Elektrolyse) vor. In diesem Artikel formulierte er die beiden Grundgesetze der Elektrolyse:
Mit seinen Untersuchungen schloss Faraday den Einfluss von Faktoren, wie beispielsweise der Konzentration der elektrolytischen Lösung oder der Beschaffenheit und Größe der Elektroden, auf den Vorgang der Elektrolyse aus. Nur die Elektrizitätsmenge und die beteiligten chemischen Äquivalente waren von Bedeutung. Es war der Nachweis, dass chemische und elektrische Kräfte eng miteinander verbunden waren und quantitativ zusammenhingen. Diesen Zusammenhang nutzte Faraday bei seinen weiteren Experimenten zur genauen Messung der Elektrizitätsmenge.[55][56][57]
Mitte Januar 1836 baute Faraday im Hörsaal der Royal Institution einen Würfel mit 12 Fuß (etwa 3,65 Meter) Seitenlänge auf, dessen Kanten aus einem leichten Holzrahmen gebildet wurden. Die Seitenflächen waren netzartig mit Kupferdraht bespannt und mit Papier verkleidet. Der Würfel stand auf vier 5,5 Zoll (etwa 14 Zentimeter) hohen Glasfüßen, um ihn vom Untergrund zu isolieren. In den am 15. und 16. Januar 1836[58] durchgeführten Untersuchungen verband er den Würfel mit einer Elektrisiermaschine, um ihn elektrisch zu laden. Anschließend begab er sich mit einem Goldblatt-Elektrometer in das Innere der Anordnung, um die möglicherweise in der Luft induzierte Elektrizität nachzuweisen. Jeder Punkt des Raumes erwies sich jedoch als frei von Elektrizität.[59][60]
Die als faradayscher Käfig bekannte Anordnung, bei der das elektrische Feld im Inneren eines geschlossenen, leitfähigen Körpers verschwindet, dient heute in der Elektrotechnik zur Abschirmung von elektrostatischen Feldern.
1837 dachte Faraday darüber nach, auf welche Weise sich die elektrische Kraftwirkung durch den Raum ausbreitete. Der Gedanke an eine Fernwirkung der elektrischen Kräfte, wie ihn das coulombsche Gesetz implizierte, bereitete ihm Unbehagen. Er vermutete hingegen, dass der Raum bei der Kraftübertragung eine Rolle spielen und eine Abhängigkeit vom Raum füllenden Medium existieren müsse. Faraday begann den Einfluss von Isolatoren systematisch zu untersuchen und entwarf eine Versuchsanordnung aus zwei identischen Kugelkondensatoren. Diese Kugelkondensatoren bestanden ihrerseits aus zwei mit einem Abstand von drei Zentimetern ineinandergestellten Messingkugeln. Die Kugeln waren durch einen mit isolierendem Schellack überzogenen Messinggriff miteinander verbunden und bildeten eine Leidener Flasche. Faraday lud zunächst einen der beiden Kondensatoren auf, brachte ihn anschließend mit dem anderen in elektrischen Kontakt und überzeugte sich mit einer selbstgebauten Coulombschen Drehwaage, dass nach dem Ladungsausgleich beide Kondensatoren die gleiche Ladung trugen. Anschließend füllte er den Luftraum des einen Kondensators mit einem Isolator und wiederholte den Versuch. Seine erneute Messung ergab, dass der Kondensator mit dem Isolator die größere Ladung trug. Er wiederholte das Experiment mit verschiedenen Stoffen. Faraday erhielt ein quantitatives Maß für den Einfluss der Isolatoren auf die Kapazität der Kugeln, das er „specific inductive capacity“ nannte, was heute der Dielektrizitätskonstanten entspricht.[61] Für eine nichtleitende Substanz, die sich zwischen zwei Leitern befindet, hatte Whewell Ende 1836[62] den Begriff Dielektrikum vorgeschlagen, der von Faraday auch genutzt wurde.[63] Faraday erklärte sein experimentelles Ergebnis mit einer Polarisation der Teilchen innerhalb der Isolatoren, bei der die Wirkung von Teilchen zu Teilchen weitergegeben wird, und dehnte diese Idee auch auf den Transport der Elektrizität innerhalb von Leitern aus.[64]
Anfang 1839 fasste Faraday seine zwischen November 1831 und Juni 1838 in den Philosophical Transactions erschienenen Artikel über seine Untersuchungen über Elektrizität unter dem Titel Experimental Researches in Electricity zusammen. Von August bis November 1839 führte Faraday Untersuchungen zur Funktionsweise der Voltaschen Säule durch, die er im Dezember 1839 unter dem Titel Über die Quelle der Kraft in der Volta’schen Säule[65][66] veröffentlichte. Darin trat er mit zahlreichen experimentellen Belegen der voltaischen Kontakttheorie entgegen.
Ende 1839 erlitt Faraday einen schweren gesundheitlichen Zusammenbruch, den er auf Überarbeitung zurückführte, und dessen Symptome Kopfschmerzen, Schwindelgefühl und zeitweiliger Gedächtnisverlust waren. Sein Arzt Peter Mere Latham (1789–1875) riet ihm, sich zeitweilig von seinen zahlreichen Verpflichtungen entbinden zu lassen und sich in Brighton zu erholen.[67] Faraday arbeitete die nächsten Jahre nur noch sporadisch in seinem Labor. Im Januar und Februar 1840 führte er an fünf Tagen seine Untersuchungen an der Voltaschen Säule fort. Im August und September experimentierte er nochmals an fünf Tagen. Nach dem 14. September 1840 schrieb er für etwa zwanzig Monate bis zum 1. Juli 1842 keinen Eintrag in sein Labortagebuch. Ende 1840 erkannten die Manager der Royal Institution die Ernsthaftigkeit von Faradays Erkrankung und beurlaubten ihn bis zu seiner vollständigen Genesung. Fast ein Jahr lang hielt er keine Vorlesungen. Gemeinsam mit seiner Frau, deren Bruder George Barnard (1807–1890) und dessen Frau Emma begab er sich am 30. Juni 1841 auf eine dreimonatige Erholungsreise in die Schweiz, wo er in den Berner Alpen ausgedehnte Wanderungen unternahm.
1840 hatte William George Armstrong entdeckt, dass beim Ausströmen von Wasserdampf unter hohem Druck in die Luft Elektrizität erzeugt wird. Im Sommer 1842 begann Faraday nach der Ursache dieser Elektrizität zu forschen. Er konnte nachweisen, dass es sich um Reibungselektrizität handelte.[68] Nach Abschluss dieser Arbeiten im Januar 1843 schloss sich eine weitere längere Phase an, in der er kaum experimentierte. Erst ab dem 23. Mai 1844 begann Faraday erneut mit Versuchen, Gase in den flüssigen und festen Zustand zu überführen, die über ein Jahr andauerten. Er knüpfte dabei an seine Experimente von 1823 an. Es gelang ihm, sechs Gase in Flüssigkeiten umzuwandeln und sieben, darunter Ammoniak, Distickstoffmonoxid und Schwefelwasserstoff, in den festen Zustand zu überführen.[69]
In dieser Zeit schien Faraday Zweifel daran zu haben, ob er weiterhin wichtige Beiträge als Naturforscher leisten könne. Er stellte die 15. bis 18. Folge seiner Elektrizitätsuntersuchungen gemeinsam mit etwa 30 weiteren Arbeiten zum zweiten Band der Experimental Researches in Electricity zusammen, der Ende 1844 erschien.[70]
Im Juni 1845 nahm Faraday am Jahrestreffen der British Association for the Advancement of Science in Cambridge teil. Dort begegnete er dem jungen William Thomson, dem späteren Lord Kelvin. Anfang August erhielt Faraday von Thomson einen Brief, in dem sich dieser nach dem Einfluss eines lichtdurchlässigen Nichtleiters auf polarisiertes Licht erkundigte.[71] Faraday erwiderte,[72] dass er 1833 ergebnislos derartige Versuche durchgeführt habe, und versprach, sich der Frage nochmals zuzuwenden. Mit einer leuchtstarken Argand-Lampe wiederholte er Ende August bis Anfang September mit verschiedenen Materialien seine Versuche, erzielte jedoch keinen Effekt. Der Effekt, nach dem Faraday gesucht hatte, der elektrooptische Kerr-Effekt, wurde erst dreißig Jahre später durch John Kerr nachgewiesen.
Am 13. September 1845 schickte Faraday polarisiertes Licht durch die zuvor benutzten Materialien, die er dem Einfluss eines starken Magneten aussetzte. Die ersten Versuche mit Luft und Flintglas erbrachten keine Ergebnisse. Als er ein im Rahmen seiner Glasexperimente in den 1820er Jahren hergestelltes Bleiborat-Glas benutzte, fand er beim Durchgang eine schwache, aber erkennbare Drehung der Polarisationsebene, wenn er den Lichtstrahl parallel zu den Magnetfeldlinien ausrichtete. Er setzte seine Experimente fort und wurde zunächst bei einer weiteren seiner alten Glasproben fündig, bevor er den Effekt an weiteren Materialien, darunter Flintglas, Kronglas, Terpentinöl, Halitkristall, Wasser und Ethanol, nachweisen konnte. Faraday hatte den Nachweis erbracht, dass Licht und Magnetismus zwei miteinander verbundene physikalische Phänomene waren. Seine Ergebnisse veröffentlichte er unter dem Titel Über die Magnetisierung des Lichts und die Belichtung der Magnetkraftlinien.[73] Der von Faraday gefundene magnetooptische Effekt wird heute als Faraday-Effekt bezeichnet.[74]
Faraday stellte sich sofort die Frage, ob auch der umgekehrte Effekt existiere und Licht etwas elektrisieren oder magnetisieren könne. Ein Versuch dazu, bei dem er eine Drahtspule dem Sonnenlicht aussetzte, scheiterte jedoch.
Während einer Freitagabendvorlesung Anfang April 1846 äußerte Faraday einige Spekulationen über „Schwingungsstrahlungen“, die er zwei Wochen später in einem Brief an das Philosophical Magazine schriftlich niederlegte.[75] In ihr skizzierte er die Möglichkeit, dass Licht durch transversale Schwingungen von Kraftlinien entstehen könnte. Faradays Spekulation war eine Anregung für James Clerk Maxwell bei der Entwicklung seiner elektromagnetischen Theorie des Lichtes, die er 18 Jahre später formulierte.[76]
Die Experimente mit polarisiertem Licht zeigten Faraday, dass ein nichtmagnetischer Stoff durch Magnetismus beeinflusst werden kann. Für seine weiteren Experimente lieh er sich einen starken Elektromagneten von der Royal Military Academy in Woolwich aus. Er befestigte eine Bleiboratglasprobe an zwei Seidenfäden und hängte sie zwischen die zugespitzten Polschuhe des Elektromagneten. Als er den elektrischen Stromkreis schloss, beobachtete er, dass sich die Glasprobe von den Polschuhen fortbewegte und sich senkrecht zur gedachten Verbindungslinie zwischen den Polschuhen ausrichtete. Sie verhielt sich damit anders als magnetische Materialien, die sich entlang der Verbindungslinie ausrichteten. Faraday fand schnell eine Vielzahl von Materialien, die sich wie seine Glasprobe verhielten, darunter Holz, Olivenöl, Apfel, Rindfleisch und Blut. Die deutlichsten Effekte erzielte er mit einem Bismutbarren. In Analogie zum Begriff „dielektrisch“ bezeichnete Faraday diese Stoffe am 18. September 1845 in seinem Labortagebuch als „dimagnetisch“.[77] Erneut half Whewell Faraday bei der Begriffsbildung. Whewell korrigierte die von Faraday benutzte Vorsilbe in dia für ‚durch‘, da die Wirkung durch die Körper hindurch stattfand („diamagnetisch“), und schlug vor, alle Substanzen, die sich nicht so verhielten, als „paramagnetisch“ zu bezeichnen.[78] In seinem Labortagebuch benutzte Faraday in diesem Zusammenhang am 7. November erstmals den Begriff „Magnetfeld“.[79] Faradays Entdeckung des Diamagnetismus führte zur Herausbildung der Magnetochemie, die sich mit den magnetischen Eigenschaften von Materialien beschäftigt.[80][81]
Nach seiner Entdeckung des Einflusses eines Magnetfeldes auf polarisiertes Licht kam Faraday immer mehr zu der Auffassung, dass Kraftlinien eine reale physikalische Bedeutung haben könnten. Das ungewöhnliche Verhalten diamagnetischer Körper ließ sich nur schwer mit den herkömmlichen Magnetpolen erklären und führte zu einem Disput zwischen Faraday und Wilhelm Eduard Weber, der glaubte, nachweisen zu können, dass der Magnetismus wie die Elektrizität polarer Natur sei. 1848 begann Faraday mit neuen Experimenten das Verhalten von diamagnetischen Körpern unter dem Einfluss eines Magneten zu untersuchen. Dabei entdeckte er, dass Kristalle sich entlang bestimmter Vorzugsachsen orientieren (Magnetische Anisotropie). Dieses Verhalten ließ sich nicht mit den bisher genutzten Begriffen von Anziehung oder Abstoßung deuten. In seinem Untersuchungsbericht sprach Faraday erstmals von einem magnetischen Feld, das zwischen zwei Magnetpolen besteht und dessen Wirkung ortsabhängig ist.[82]
1852 fasste Faraday seine Ansichten über Kraftlinien und Felder im Artikel On the physical character of the lines of magnetic force[83] (Über den physikalischen Charakter der magnetischen Kraftlinien) zusammen. Darin lehnte er eine Fernwirkung der Gravitationskräfte ab und vertrat die Auffassung eines mit der Masse eines Körpers verbundenen Gravitationsfeldes.[84]
Faradays Interesse für Gravitation reichte bis in die Mitte der 1830er Jahre zurück. Ende 1836 las er eine Arbeit des Italieners Ottaviano Fabrizio Mossotti, in der dieser die Gravitation auf elektrische Kräfte zurückführte.[85] Faraday war anfangs von der Arbeit begeistert,[86] ließ sie ins Englische übersetzen und sprach in einer Freitagabendvorlesung über sie. Später verwarf er jedoch Mossottis Erklärung, da er zu der Überzeugung gelangt war, die Unterschiede, wie die Schwerkraft gegenüber anderen Kräften wirkt, seien zu groß. In den nächsten Jahren spekulierte Faraday häufig darüber, auf welche Weise die Schwerkraft mit anderen Kräften in Beziehung stehen könnte. Im März 1849 begann er zu überlegen, wie ein Zusammenhang zwischen Gravitation und Elektrizität experimentell nachzuweisen sei. Er stellte sich die Gravitation als eine Kraft mit zwei komplementären Komponenten vor, bei der ein Körper positiv ist, wenn er sich zur Erde hin und negativ, wenn er sich von ihr wegbewegt. Er stellte die These auf, dass diese beiden Bewegungen mit entgegengesetzten elektrischen Zuständen verbunden seien. Für seine Versuche konstruierte Faraday eine Drahtspule, die er mit einem Galvanometer verband und aus großer Höhe fallen ließ. Er konnte jedoch bei keiner Messung einen Effekt nachweisen. Trotz des negativen Ausganges der Versuche beschrieb er seine Bemühungen in der Baker-Vorlesung vom 28. November 1850.[87]
Im Februar 1859 begann Faraday erneut eine Reihe von Experimenten, mit denen er einen Zusammenhang zwischen Gravitation und Elektrizität nachzuweisen hoffte. Aufgrund des zu erwartenden geringen Effektes benutzte er einige hundert Kilogramm schwere Bleimassen, die er vom 50 Meter hohen Schrotturm in Lambeth fallen ließ. Mit anderen Experimenten hoffte er, eine Temperaturänderung beim Heben und Senken einer Masse nachweisen zu können. Am 9. Juli 1859 brach Faraday die Versuche erfolglos ab. Er verfasste darüber den Aufsatz Note on the Possible Relation of Gravity with Electricity or Heat, den er am 16. April 1860[88] fertigstellte und der wie gewohnt in den Philosophical Transactions erscheinen sollte. George Gabriel Stokes, der befand, dass die Arbeit nicht veröffentlichungswürdig sei, da er nur negative Ergebnisse vorzuweisen habe, empfahl Faraday, seinen Artikel zurückzuziehen,[89] was dieser nach Erhalt von Stokes Brief umgehend tat.[90][91]
Kurz nach seiner Ernennung zum Labordirektor der Royal Institution Anfang 1825 öffnete Faraday die Laboratorien des Instituts für die Treffen der Institutsmitglieder. An drei bis vier Freitagabenden wollte er vor interessierten Mitgliedern von Experimenten begleitete Chemievorträge abhalten. Aus diesen informellen Treffen entwickelte er das Konzept der regelmäßig stattfindenden Freitagabendvorlesungen, bei denen Themen aus Naturforschung und Technik für Laien verständlich dargestellt werden sollten. Bei der ersten Freitagabendvorlesung am 3. Februar 1826 sprach Faraday über Kautschuk. Von den 17 Vorlesungen des ersten Jahres hielt er sechs zu Themen wie Isambard Kingdom Brunels Gasverflüssiger, Lithografie und den Thames Tunnel. Nach Faradays Ansicht sollten die Vorlesungen Spaß machen, unterhalten, bilden und vor allem anregend sein. Seine Vorlesungen wurden aufgrund der schlichten Vortragsweise sehr populär und waren stets gut besucht. Bis 1862 gab Faraday insgesamt 126[92] dieser einstündigen Vorlesungen. Als Sekretär des Komitees für die „Weekly Evening Meetings“ sorgte Faraday dafür, dass die Vorträge in der Literary Gazette und im Philosophical Magazin veröffentlicht wurden und auf diese Weise einem noch breiteren Publikum zugänglich waren.[93][94][95]
Neben den Freitagabendvorlesungen wurde zum Jahreswechsel 1825/26 erstmals eine Weihnachtsvorlesung abgehalten, die sich speziell an jugendliche Hörer richtete. Bis Anfang der 1860er Jahre prägte Faraday die Ausgestaltung der Weihnachtsvorlesungen wesentlich. Von 1827 an war er für insgesamt 19 Folgen verantwortlich, die meist aus sechs Einzelvorlesungen bestanden. 1860/61 nutzte er seine Notizen der bereits 1848/49 abgehaltenen Vorlesung mit dem Titel Chemical History of a Candle (Naturgeschichte einer Kerze). Auf Betreiben von William Crookes wurde Faradays Weihnachtsvorlesung mitgeschrieben und erschien als sechsteilige Artikelfolge in Crookes Chemical News. Die kurze Zeit später erschienene Buchfassung gilt als eines der erfolgreichsten populärwissenschaftlichen Bücher und wurde in zahlreiche Sprachen übersetzt.[96]
Neben seiner Forschungs- und Vorlesungstätigkeit war Faraday in vielfältiger Weise für den britischen Staat tätig. Im Sommer 1829 wandte sich Percy Drummond († 1843), Lieutenant Governor der Royal Military Academy in Woolwich, an Faraday und fragte ihn, ob er bereit sei, als Nachfolger des Geologen John MacCulloch den Posten des Professors für Chemie an der Akademie zu übernehmen. Nach längeren Verhandlungen, bei denen es vorwiegend um seine Pflichten und die Bezahlung ging, sagte Faraday zu. Bis 1852 hielt er in Woolwich jährlich 25 Vorlesungen.[97]
Ab dem 4. Februar 1836 war Faraday als wissenschaftlicher Berater für die Schifffahrtsbehörde Trinity House tätig, die unter anderem die englischen Leuchttürme betreibt. Er war verantwortlich für die chemische Analyse der beim Betrieb der Leuchttürme eingesetzten Materialien und begutachtete neue Beleuchtungssysteme, die Trinity House für den Einsatz vorgeschlagen worden waren. Faraday sorgte für die Modernisierung der englischen Leuchttürme. Vorbild waren ihm dabei die französischen Leuchttürme, bei denen zur Verbesserung der Lichtstärke Fresnel-Linsen eingesetzt wurden. Er begleitete auch die ersten Versuche zu ihrer Elektrifizierung. In Blackwall an der Themse gab es zwei speziell für seine Untersuchungen errichtete Leuchttürme.[98][99]
Im Auftrag der Regierung war Faraday an der Untersuchung zweier heikler Unfälle beteiligt. Am 13. April 1843 zerstörte eine Explosion die vom Ordnance Office geführte Schießpulverfabrik in Waltham Abbey (Essex), woraufhin Faraday mit der Ursachenanalyse betraut wurde. In seinem Bericht an den Labordirektor der Militärakademie von Woolwich James Pattison Cockburn (1779?–1847) zählte er mehrere mögliche Ursachen auf und gab Ratschläge, wie diese Probleme zukünftig vermieden werden könnten.[100] Gemeinsam mit Charles Lyell und Samuel Stutchbury (1798–1859) erhielt er im Oktober 1844 vom Home Office den Auftrag, die Explosion in der Haswell-Grube in Durham zu untersuchen, bei der am 28. September 95 Menschen ums Leben gekommen waren. Lyell und Faraday erkannten, dass der Kohlenstaub eine wesentliche Rolle bei der Explosion gespielt hatte, und empfahlen die Einführung eines besseren Bewetterungssystemes.[101]
Ein erheblicher Teil Faradays beratender Tätigkeit befasste sich mit der Konservierung von Gegenständen und Gebäuden. Ab 1853 beriet er das Select Committee on the National Gallery bei der Konservierung von Gemälden. Beispielsweise untersuchte er den Einfluss der Gasbeleuchtung auf Gemälde. Anfang 1856 wurde Faraday in die Royal Commission berufen, die sich mit der Zukunft des Standortes der National Gallery befasste. Im Auftrag von Thomas Leverton Donaldson (1795–1885) untersuchte er für das British Museum, ob die Elgin Marbles ursprünglich bemalt waren. 1859 beriet er das Metropolitan Board of Works bei der Auswahl eines Mittels zur Behandlung der Kalksteine des kürzlich wiedererbauten Houses of Parliament, die sich unter dem Einfluss der schwefelhaltigen Londoner Luft zersetzten.[102]
Faraday war ein zutiefst religiöser Mensch. Sein Vater gehörte der kleinen christlichen Sekte der Sandemanianer an, die sich Ende der 1720er Jahre von der Church of Scotland losgesagt hatten. Sie gründeten ihren Glauben und dessen Ausübung auf eine wörtliche Auslegung der Bibel. Im Großraum London gab es zur damaligen Zeit etwa einhundert und in ganz Großbritannien etwa eintausend Sandemanianer. Bereits als Kind begleitete Faraday seinen Vater zu den sonntäglichen Predigten. Kurz nach seiner Hochzeit mit Sarah Barnard, die ebenfalls Mitglied der Sandemanianer war und deren Vater der Gemeinde als Ältester („Elder“) diente, legte er am 15. Juli 1821 seinen Eid ab und wurde Mitglied.[103]
Als Zeichen ihrer hohen Wertschätzung wählte die Londoner Gemeinde Faraday am 1. Juli 1832 zum Diakon und am 15. Oktober 1840 zu einem der drei Ältesten.[104] In den folgenden dreieinhalb Jahren gehörte es zu seinen Verpflichtungen, an jedem zweiten Sonntag die Predigt zu halten, auf die er sich genauso sorgfältig wie auf seine Vorlesungen vorbereitete. Am 31. März 1844 wurde Faraday bis zum 5. Mai aus der Gemeinde ausgeschlossen.[105] Die Gründe hierfür sind nicht ganz geklärt, sind aber nicht in einer persönlichen Verfehlung Faradays zu suchen, sondern auf eine Kontroverse innerhalb der Sandemanianer zurückzuführen, da neben Faraday zu dieser Zeit auch zahlreiche weitere Mitglieder ausgeschlossen wurden.[106] In seine Position als Ältester wurde er erst wieder am 21. Oktober 1860 gewählt.[107] Bis 1864 war Faraday wieder regelmäßig für die Predigten zuständig und erhielt den Kontakt zu anderen sandemanianischen Gemeinden, so beispielsweise in Chesterfield, Glasgow und Dundee, aufrecht. Seine Predigten bestanden aus einer Reihe von Zitaten aus dem Alten und Neuen Testament, die er kommentierte.[108] Seine religiösen Ansichten waren für ihn eine sehr private Angelegenheit und er äußerte sich nur selten gegenüber seinen Briefpartnern oder in der Öffentlichkeit darüber.[109]
Der dritte und letzte Band der Experimental Researches in Electricity, den Faraday Anfang 1855 zusammenstellte, umfasste alle seine seit 1846 in den Philosophical Transactions veröffentlichten Arbeiten. Zusätzlich nahm er zwei im Philosophical Magazine publizierte Artikel auf, die an die 29. Folge der Experimental Researches in Electricity anschlossen und seine charakteristische Abschnittsnummerierung fortsetzten. Einige kürzere Artikel ergänzten den Band. Insgesamt publizierte Faraday 450 wissenschaftliche Artikel.[110]
Durch Vermittlung von Prinz Albert bezogen die Faradays
im September 1858 ein Haus in Hampton Court Green, das Königin Victoria gehörte und sich in unmittelbarer Nähe des Hampton Court Palace befand. Im Oktober 1861 bat der siebzigjährige Faraday die Manager der Royal Institution um seine Entlassung aus dem Institutsdienst. Diese lehnten sein Ersuchen jedoch ab und erließen ihm nur die Verantwortung für die Weihnachtsvorlesungen.
Am 25. November 1861 begann Faraday eine letzte Versuchsreihe, bei der er mit einem von Carl August von Steinheil konstruierten Spektroskop die Auswirkungen eines Magnetfeldes auf das Lichtspektrum einer Flamme untersuchte. Seinen letzten Eintrag im Labortagebuch machte er am 12. März 1862.[111] Die Versuche blieben wegen der nicht ausreichend empfindlichen Messanordnung erfolglos; der Zeeman-Effekt wurde erst 1896 entdeckt.
Am 20. Juni 1862 hielt Faraday vor über 800 Zuhörern seinen letzten Freitagabendvortrag On Gas Furnaces (Über Gasöfen) und beendete seine fast vier Jahrzehnte andauernde Vortragstätigkeit für die Royal Institution. Im Frühjahr 1865 wurde er auf einmütigen Beschluss der Manager der Royal Institution von allen seinen Verpflichtungen entbunden. Bis zum Mai 1865 stand er mit seinem Rat noch der Schifffahrtsbehörde zur Verfügung.
Faraday starb am 25. August 1867 in seinem Haus in Hampton Court und wurde fünf Tage später auf dem Highgate Cemetery begraben.
„Faraday ist der Vater der erweiterten Lehre des Elektromagnetismus und wird dies immer bleiben.“
„Faraday sah im Geiste die den ganzen Raum durchdringenden Kraftlinien, wo die Mathematiker fernwirkende Kraftzentren sahen; Faraday sah ein Medium, wo sie nichts als Abstände sahen; Faraday suchte das Wesen der Vorgänge in den reellen Wirkungen, die sich in dem Medium abspielten, jene waren aber damit zufrieden, es in den fernwirkenden Kräften der elektrischen Fluida gefunden zu haben…“
Faradays Konzepte und seine Ansicht von der Einheitlichkeit der Natur, die ohne eine einzige mathematische Formel auskamen, hinterließen beim jungen James Clerk Maxwell einen tiefen Eindruck. Maxwell stellte es sich zur Aufgabe, Faradays experimentelle Befunde und ihre Beschreibung mittels Kraftlinien und Felder in eine mathematische Darstellung zu überführen. Maxwells erster größerer Aufsatz über Elektrizität On Faraday’s Lines of Force (Über Faradays Kraftlinien) erschien 1856. Auf Grundlage einer Analogie zur Hydrodynamik stellte Maxwell darin eine erste Theorie des Elektromagnetismus auf, indem er die Vektorgrößen elektrische Feldstärke, magnetische Feldstärke, elektrische Stromdichte und magnetische Flussdichte einführte und mit Hilfe des Vektorpotentials zueinander in Beziehung setzte. Fünf Jahre später berücksichtigte Maxwell in On Physical Lines of Force (Über physikalische Kraftlinien) auch das Medium, in dem die elektromagnetischen Kräfte wirkten. Er modellierte das Medium durch elastische Eigenschaften. Daraus ergab sich, dass eine zeitliche Änderung eines elektrischen Feldes zu einem zusätzlichen Verschiebungsstrom führt. Außerdem ergab sich, dass Licht eine transversale Wellenbewegung des Mediums ist, womit Faradays Spekulation über die Natur des Lichtes bestätigt wurde. Die weitere Ausarbeitung der Theorie durch Maxwell führte 1864 schließlich zur Formulierung der Maxwellschen Gleichungen, welche die Grundlage der Elektrodynamik bilden und mit denen sich alle von Faraday gefundenen elektromagnetischen Entdeckungen erklären lassen.[114][115] Eine der vier Maxwellschen Gleichungen ist eine mathematische Beschreibung der von Faraday entdeckten elektromagnetischen Induktion.
„Ihn zeichnete eine unbeschreibliche Raschheit und Lebendigkeit aus. Der Widerschein seines Genius umgab ihn mit einer ganz besonderen, strahlenden Aura. Diesen Charme spürte gewiß jeder – ob tiefsinniger Philosoph oder schlichtes Kind –, der den Vorzug genoß, ihn in seinem Zuhause zu erleben – in der Royal Institution.“
Am Ende des 19. Jahrhunderts wurde Faraday als Erfinder des Elektromotors, des Transformators und des Generators sowie als Entdecker von Benzol, des magnetooptischen Effektes, des Diamagnetismus und als Schöpfer der elektromagnetischen Feldtheorie wahrgenommen. 1868 erschien John Tyndalls Biografie Faraday as a Discoverer (Faraday und seine Entdeckungen). Tyndall, der Nachfolger von Brande an der Royal Institution war, beschrieb darin hauptsächlich Faradays wissenschaftliche Entdeckungen. Hermann Helmholtz, der Tyndalls Biografie ins Deutsche übersetzte, ergänzte diese durch zahlreiche biografische Anmerkungen. Kurz darauf publizierte Henry Bence Jones, Sekretär der Royal Institution und Arzt Faradays, eine typische viktorianische „Life-and-Letters“-Biografie, für die er auf Faradays Briefe, seine Labortagebücher und andere unveröffentlichte Manuskripte zurückgriff und Ausschnitte aus Tyndalls Biografie nutzte. Bence Jones zweibändige Biografie ist noch heute eine wichtige Quelle, da einige der darin zitierten Briefe und Tagebücher nicht mehr auffindbar sind. Diese und weitere Darstellungen von Faraday führten zu einem Bild eines Forschers, der allein und in der Abgeschiedenheit seines Labors an der Royal Institution den Naturgeheimnissen auf den Grund ging.[117][118]
Nach dem Ende des Ersten Weltkrieges versuchten die etablierte Gasindustrie und die aufstrebende Elektroindustrie, deren Ziel die umfassende Elektrifizierung Großbritanniens war und die sich damit in unmittelbarer Konkurrenz zur Gasindustrie befand, in den 1920er Jahren die Bekanntheit Faradays für ihre jeweiligen Ziele zu nutzen. Anlässlich des einhundertsten Jahrestages der Entdeckung von Benzol konstituierte sich unter dem Vorsitz des Chemikers Henry Edward Armstrong ein Komitee aus Mitgliedern der Royal Institution, der Chemical Society, der Society of Chemical Industry und der Association of British Chemical Manufacturers. Während der Feierlichkeiten im Juni 1925 wurde hervorgehoben, welche Bedeutung Faraday für die moderne Chemieindustrie habe, und er wurde als „Vater der Chemieindustrie“ zelebriert.[119]
Auf Initiative von Walter Adolph Vignoles (1874–1953), Direktor der Electrical Development Association, und mit Unterstützung von William Henry Bragg, Direktor des Davy-Faraday Research Laboratory an der Royal Institution, wurde im Februar 1928 ein neunköpfiges Komitee berufen, das die Feierlichkeiten aus Anlass des einhundertsten Jahrestages der Entdeckung der elektromagnetischen Induktion 1931 organisieren sollte. Vom 23. September bis 3. Oktober 1931 fand in der Royal Albert Hall eine Ausstellung zu Ehren Faradays und seiner Entdeckung statt. Den Mittelpunkt der Ausstellung bildete eine Kopie der von John Henry Foley (1818–1874) und Thomas Brock (1847–1922) geschaffenen Skulptur, die sich seit 1876 in der Royal Institution befand und die Faraday in akademischer Kleidung mit seinem Induktionsring zeigte. In unmittelbarer Nähe der Skulptur befanden sich die einfachen Dinge, mit denen Faraday seine ersten Experimente durchführte: ein Draht, ein Magnet und ein Quecksilbertropfen. Die Skulptur bildete den Mittelpunkt für die darum kreisförmig angeordneten Ausstellungsstände. Auf den der Skulptur nächstgelegenen Ständen wurden die von Faraday für die einzelnen Experimente benutzten Apparaturen und seine damit verbundenen Aufzeichnungen gezeigt. Die äußeren Stände demonstrierten die daraus hervorgegangenen modernen Technologien der Elektroindustrie. Eine 12-seitige Broschüre, die die Ausstellung begleitete und von der etwa 100.000 Kopien verteilt wurden, trug den Titel Faraday: The Story of an Errand-Boy. Who Changed the World (Faraday: Die Geschichte eines Laufburschen, der die Welt veränderte). Die aufwändige Ausstellung von 1931 und die damit verbundenen Feierlichkeiten waren einerseits dem Bestreben der Elektroindustrie geschuldet, Elektrizität in vermarktbare Produkte zu verwandeln. Andererseits unterstützten sie auch das Bestreben der Naturwissenschaftler, zu zeigen, wie Grundlagenforschung zur Entwicklung neuer Technologien beitragen kann.[120]
Faradays Biograf Henry Bence Jones verzeichnet insgesamt 95 Ehrentitel und Auszeichnungen.[121] Die erste Würdigung durch eine Gelehrtengesellschaft wurde Faraday 1823 durch die Cambridge Philosophical Society zuteil, die ihn als ihr Ehrenmitglied aufnahm. 1832 wurde er in die American Academy of Arts and Sciences, 1835 in die Göttinger Akademie der Wissenschaften[122] und die Royal Society of Edinburgh[123] sowie 1840 in die American Philosophical Society[124] gewählt. Auf Bestreben von Jean-Baptiste André Dumas wurde Faraday 1844 als eines der acht Auslandsmitglieder in die Académie des sciences gewählt.[125] 1847 wurde er als auswärtiges Mitglied in die Bayerische Akademie der Wissenschaften und 1851 in die Königlich Niederländische Akademie der Wissenschaften[126] aufgenommen. Im Jahre 1857 wurde er zum Mitglied der Leopoldina gewählt. 1864 wurde er letztmals durch die Società Reale di Napoli geehrt, die ihn als assoziiertes Auslandsmitglied führte.[127] Ebenfalls 1864 wurde er in die National Academy of Sciences gewählt.
Die Royal Society zeichnete ihn mit der Copley-Medaille (1832 und 1838), der Royal Medal (1835 und 1846) und der Rumford-Medaille (1846) aus. Das Angebot, Präsident der Royal Society zu werden, lehnte Faraday zweimal (1848 und 1858) ab.[128] 1842 erhielt Faraday den preußischen Verdienstorden Pour le Mérite.
Ein speziell für die Verlegung von Seekabeln gebauter Kabelleger, die Faraday, wurde 1874 von seinem Konstrukteur Carl Wilhelm Siemens nach Faraday benannt. Der in Paris tagende Congrès international d’électriciens (Internationaler Elektrikerkongress) beschloss am 22. September 1881, die Einheit für die elektrische Kapazität zu seinen Ehren Farad zu nennen.[129] Ebenso sind nach ihm der Mondkrater Faraday und der Asteroid Faraday benannt. William Whewell ehrte Faraday und Davy mit der Benennung einer seiner „Epochen der Chemie“.[130]
Am 5. Juni 1991[131] emittierte die Bank of England eine neue 20-Pfund-Sterling-Banknote mit dem Bildnis von Faraday, die bis zum 28. Februar 2001[132] gültiges Zahlungsmittel war.
Mehrere Preise sind nach ihm benannt, unter anderem die Faraday-Medaille (IOP), Faraday-Medaille (IEE) und der Michael-Faraday-Preis der Royal Society.
Nach ihm benannt ist die Pflanzengattung Faradaya F.Muell. aus der Familie der Lippenblütler (Lamiaceae).[133]
Faradays schriftlicher Nachlass ist wahrscheinlich der umfangreichste, den ein Naturforscher in der Geschichte der Naturwissenschaften hinterlassen hat. Er umfasst seine Labortagebücher, Tagebücher, Commonplace-Books, Notizen, Manuskripte, Briefe, Bücher und anderes. Im Nachlass finden sich Aufzeichnungen zu etwa 30.000 von Faraday durchgeführten Experimenten.[134]
Anfang 1855[135] gab Faraday erste Anweisungen zur Regelung seines Nachlasses. Er hinterließ der Royal Institution seine Labortagebücher, einige Sonderdrucke und andere persönliche Dinge. Nach Faradays Tod erhielt die Royal Institution weiteres Material von seiner Frau Sarah. Trinity House überließ sie die Akten mit seinen Arbeiten für die Behörde. Diese befinden sich heute in der Guildhall Library. Etliche Stücke gab sie zur Erinnerung an Faraday an Freunde und Verwandte. Ein Teil davon gelangte Ende 1915 in den Besitz der Institution of Electrical Engineers. Die Manuskripte von Faradays Artikeln für die Philosophical Transactions wurden, nachdem er sie zur Veröffentlichung eingereicht hatte, Eigentum der Royal Society. Die Hälfte von ihnen blieb bewahrt.[136] Von Faradays Korrespondenz sind etwa 4800 Briefe erhalten, die sich in 230 Archiven auf der ganzen Welt befinden.[137]
Nach der aus dem Englischen von Salomon Kalischer übersetzten Ausgabe von 1889 bis 1891 mit einer Einleitung von Friedrich Steinle:
Klassische
Moderne

Nikola Tesla (serbisch-kyrillisch Никола Тесла; * 10. Juli 1856 in Smiljan, Kroatische Militärgrenze, Kaisertum Österreich; † 7. Januar 1943 in New York, Vereinigte Staaten) war ein Erfinder, Physiker und Elektroingenieur. Sein Lebenswerk ist geprägt durch zahlreiche Neuerungen auf dem Gebiet der Elektrotechnik, insbesondere der elektrischen Energietechnik, wie die Entwicklung des heute als Zweiphasenwechselstrom bezeichneten Systems zur elektrischen Energieübertragung. Tesla erhielt in 26 Ländern über 280 Patente, davon 112 in den USA.[1]
Tesla wurde als viertes von fünf Kindern serbischstämmiger Eltern in dem Dorf Smiljan in der Lika unweit von Gospić im heutigen Kroatien geboren. Seine Eltern waren der serbisch-orthodoxe Priester Milutin Tesla (1819–1879) und dessen Frau Georgina (Rufname Đuka, geborene Mandić, 1822–1892). Das Geburtshaus war das Pfarrhaus der Sankt-Peter-und-Paul-Kirche von Smiljan.[2] Im Taufregister der orthodoxen Kirche wurde das Geburtsdatum noch nach dem julianischen Kalender eingetragen.[3] Er hatte drei Schwestern und einen älteren Bruder, der aber schon im Alter von zwölf Jahren bei einem Reitunfall starb, als Nikola fünf Jahre alt war.
Tesla besuchte die Grundschule und die Mittelschule in Gospić und ab 1870 das Gymnasium in Karlovac. Während seiner Gymnasialzeit lebte er bei seiner Tante Stanka und deren Mann Dane Branković, einem pensionierten Oberst.
1875 nahm er ein Studium Generale an der Kaiserlich-Königlichen Technischen Hochschule in Graz auf und belegte im ersten Jahr überdurchschnittlich viele Vorlesungen. Am 21. Juli 1876 erhielt er die Zugangsberechtigung zum Hauptstudium in Maschinenbau. Beim Physikprofessor Jakob Pöschl lernte er die Gramme-Maschine kennen, einen damals neuartigen Gleichstromgenerator von Zénobe Gramme. Im zweiten Studienjahr nahm seine Studienaktivität deutlich ab. Im dritten Studienjahr schloss er keine Prüfung ab und wurde schließlich von der Hochschule 1877/78 exmatrikuliert, nachdem er das Unterrichtsgeld nicht bezahlt hatte.[4]
Tesla zog nach Marburg an der Drau, wo er eine Anstellung als Maschinenbauer fand. Seine Freizeit verbrachte er als Karten- und Billardspieler in einschlägigen Lokalen. Am 24. März 1879 wurde er per polizeilicher Anordnung aus Marburg verwiesen und in seine Heimatgemeinde Gospić zurückgeschickt. Einen Monat später, im April 1879, starb sein Vater. Nach dessen Tod blieb Tesla zunächst in Gospić und nahm eine Anstellung als Aushilfslehrer an.
1880 ging Tesla mit finanzieller Unterstützung durch seinen Onkel Dane Branković nach Prag, um an der dortigen, damals deutschsprachigen Karls-Universität sein Studium abzuschließen. Allerdings ist weder der Besuch noch der Abschluss der von ihm besuchten Vorlesungen belegt; auch die notwendigen Studiengebühren wurden nie bezahlt.
Von 1881 bis 1882 lebte Tesla in Budapest, wo er 1882 eine Anstellung als Telegrafenamtstechniker bei Tivadar Puskás fand, der zu jener Zeit Repräsentant der Firmen von Thomas Alva Edison in Europa war.

Mit einer Empfehlung von Puskás zog Tesla Ende 1882 nach Paris zu Charles Batchelor, der eine der führenden Edison-Firmen in Frankreich betrieb. Neben weiteren Tätigkeiten betreute Tesla von November 1883 bis Februar 1884 die neu installierte elektrische Beleuchtung am Gare de l’Est in Paris.[5]Am 6. Juni 1884 zog Tesla praktisch ohne Finanzmittel nach New York. Bereits zwei Tage später, am 8. Juni 1884, begann er für Thomas Alva Edison zu arbeiten. Sein Dienstverhältnis bestand jedoch nur bis zum 7. Dezember 1884, als es wegen Differenzen bei den Gehaltsvorstellungen seitens Tesla aufgelöst wurde.[6]
Im März 1885 gründete Tesla gemeinsam mit zwei Geschäftspartnern die Firma Tesla Electric Light and Manufacturing Company mit Sitz in Rahway. 1885 wurden die ersten Patente wie Teslas Bogenlampe und ein neuartiger Kommutator beantragt und in die Firma eingebracht.[7][8] Die beiden Geschäftspartner hintergingen Tesla jedoch. Ihnen war eine Beteiligung nur wichtig, um Lizenzzahlungen an andere Lampenkonstrukteure umgehen zu können. Ende 1886 meldete die Firma Konkurs an.[9]
Im Frühjahr 1887 war Tesla zeitweise arbeitslos, arbeitete als Tagelöhner im Straßenbau und lernte über zufällige Bekanntschaften den Superintendenten der Western Union Alfred S. Brown und den Anwalt Charles F. Peck kennen. Teslas Ideen zu einem rotierenden magnetischen Feld, einem sogenannten Drehfeld, gebildet aus zwei Wechselströmen, die gegeneinander um 90° phasenversetzt sind und heute unter dem Begriff Zweiphasenwechselstrom bekannt sind, überzeugten Brown und Peck. So konnte Tesla im April 1887 als Teilhaber seine zweite Firma Tesla Electric Company gründen und sich mit den ersten Arbeiten zu Zweiphasenwechselstrom beschäftigen.[10] Bis zum Mai 1888 wurden sieben Patente angemeldet (Peck ist neben Tesla als Mitinhaber eingetragen), die sich mit mehrphasigem Wechselstrom und dessen Übertragung beschäftigten, den sogenannten Polyphase-Patenten.[11] Eines der wichtigsten Patente hiervon, US-Patent Nr. 381.968, beschreibt die erste Zweiphasen-Synchronmaschine, die zu den Drehstrommaschinen zählt. Im April 1888 folgten Publikationen in renommierten Fachzeitschriften wie dem Electrical Review und der Electrical World, woraus eine gewisse Bekanntheit in Fachkreisen resultierte.
Am 16. Mai 1888 wurde Tesla eingeladen, einen Vortrag zum Mehrphasenwechselstrom vor dem American Institute of Electrical Engineers (AIEE, heute IEEE) zu halten.[12] Dieser Vortrag wurde unter dem Titel New York Lecture bekannt, erregte großes Aufsehen und führte dazu, dass der Großindustrielle George Westinghouse auf Tesla aufmerksam wurde. Westinghouse, der sich in einer später Stromkrieg genannten Auseinandersetzung mit Edison befand, sicherte sich Mitte 1888 die Rechte auf Teslas Polyphase-Patente, musste in den folgenden zehn Jahren aber erhebliche finanzielle Mittel in die Verteidigung jener Patente gegen Galileo Ferraris aufwenden, der praktisch gleichzeitig und unabhängig von Tesla das Drehstromsystem erfand.[13] Unabhängig von Tesla erfand Michail Ossipowitsch Doliwo-Dobrowolski im Jahr 1888 das heute in der elektrischen Energietechnik und in Stromnetzen übliche Dreiphasensystem.[14]
Von Juli 1888 bis Juli 1889 arbeitete Tesla gemeinsam mit Technikern von Westinghouse in Pittsburgh an praktischen Realisierungen von Wechselspannungssystemen – aus der Zeit stammen mehrere Patente, unter anderem zur Konvertierung von Gleichstrom in Wechselstrom. Die Zusammenarbeit war nicht konfliktfrei: Tesla war es gewohnt, als „Einzelkämpfer“ zu arbeiten, und konnte sich nur schwer integrieren. Technisch waren die bestehenden Westinghouse-Maschinen einphasige Generatoren, ausgelegt auf eine Netzfrequenz von 133 Hz. Teslas Maschinen waren auf Mehrphasenwechselstrom für 60 Hz ausgelegt.[15] 1890 setzten sich die noch heute in Nordamerika übliche Netzfrequenz von 60 Hz und das Mehrphasensystem bei Westinghouse endgültig durch. Zur gleichen Zeit war im sogenannten Stromkrieg ein Streit entbrannt zwischen Edison, der ein Gleichstromsystem, und Westinghouse, der ein Wechselstromsystem favorisierte.
Im August 1889 zog Tesla nach kurzem europäischen Aufenthalt in Paris und seiner Heimatgemeinde Gospić nach New York, wo er sich in der Grand Street ein Labor einrichtete und sich mit hochfrequenten Wechselströmen und elektromagnetischen Wellen zu befassen begann. Er wohnte zu dieser Zeit, losgelöst von finanziellen Sorgen, in First-Class-Hotels wie dem Astor-House am Broadway.[16] Im Jahr 1890 entstanden mehrere Patente, unter anderem zu Unterbrecherkontakten, vielpoligen Wechselspannungs-Generatoren und Resonanz-Transformatoren, den sogenannten Tesla-Transformatoren, um mit Funkenstrecken die Lichtbogenlampe zu verbessern.[17]
Die Idee, die Tesla damals verfolgte und die ihn zeitlebens nicht mehr loslassen sollte, war, mittels hochfrequenter Wechselströme eine drahtlose Energieübertragung zu ermöglichen. Er experimentierte dazu mit Geißlerröhren, einer frühen Form der Gasentladungsröhre, und veranstaltete regelmäßig Vorführungen, bei denen er die Geißlerröhren ohne Kabelanschluss zwischen im Raum angebrachten Elektroden leuchten ließ. Dieses beeindruckende Schauspiel führte meist zu großem Staunen im Publikum. Infolge dieser Vorführungen lud William Arnold Anthony, damals Präsident der AIEE, ihn ein, am 20. Mai 1891 einen Vortrag am Columbia College in New York zu halten. Diese als Columbia Lecture bekannt gewordene Vorlesung, eine Mischung aus Fachvortrag und Bühnenshow, fand große Beachtung, erntete aber aufgrund ihres okkulten Stils auch Missfallen, zum Beispiel bei Mihajlo Pupin, damals Physikprofessor an der Columbia University.[18]
Infolge der Columbia Lecture und seiner öffentlichen Auftritte wurde Tesla in der New Yorker High Society bekannt. Dies öffnete ihm verschiedene gesellschaftliche Türen. Unter anderem erhielt er am 31. Juli 1891 die amerikanische Staatsbürgerschaft. Von Februar 1892 bis Ende August 1892 weilte Tesla in Europa, unter anderem in London, wo er am 3. Februar 1892 vor der Institution of Electrical Engineers (IEE) und danach bei der Royal Institution of Great Britain eine überarbeitete Version seiner Columbia Lecture vortrug. Es folgten Präsentationen in Paris, danach reiste er in seine Heimatgemeinde Gospić, wo seine Mutter am 16. April 1892 starb.[19]
Trotz der Wirtschaftskrise in den USA im Jahr 1893 wurde auf der World’s Columbian Exposition in Chicago (diese Weltausstellung stand im Zeichen der Entdeckung Amerikas durch Christoph Kolumbus) von Westinghouse Tesla’s Egg of Columbus ausgestellt. Dieser Demonstrationsaufbau sollte die Wirkung des magnetischen Drehfeldes anhand eines metallischen Eies und dessen Rotation veranschaulichen und im Rahmen des Stromkrieges Stimmung für das von Westinghouse vertretene Wechselstromsystem machen. Gleichzeitig zog Tesla 1893 in das New Yorker Hotel Gerlach, das bereits elektrisches Licht und einen Fahrstuhl hatte. Auch richtete er in diesem Jahr ein größeres Labor am West Broadway ein und beschäftigte fünf Mechaniker.[20] In seinem neuen Labor ließ er an der Decke umlaufende Kabel montieren und von einem Tesla-Oszillator speisen.[21] Damit war es möglich, mit Geißlerröhren im Raum umherzugehen, während die Röhren durch die hohen elektrischen Feldstärken drahtlos leuchteten. Im gleichen Jahr folgten Vorträge zu diesen Experimenten am Franklin Institute in Philadelphia und medial beachtete Vorführungen auf der World’s Columbian Exposition in Chicago. Er stellte sich dabei quasi selbst aus und ließ durch die nach ihm benannten[22] hochfrequente Wechselströme, die durch den Skin-Effekt bis zu einer gewissen Stärke gefahrlos sind, Koronaentladungen und deren optische Lichterscheinungen an der Oberfläche seiner Kleidung und seines Haars entstehen.[23]
Tesla war in den 1890er Jahren auch in der New Yorker High Society sehr aktiv und pflegte einen teuren Lebensstil. Er hatte beispielsweise die Angewohnheit, Lederhandschuhe und Krawatten nach einer Woche wegzuwerfen und durch neue zu ersetzen. In dieser Zeit knüpfte er auch Kontakte zu der Theaterdiva Sarah Bernhardt und zu dem Bankier J. P. Morgan.[24] Er hatte auch Rückschläge zu verkraften: Am 13. März 1895 vernichtete ein Brand sein Labor mit allen Geräten, Unterlagen und Aufzeichnungen. Der Brand wurde durch eine Heizungsfirma, die im selben Gebäude im Erdgeschoss untergebracht war, ausgelöst. Dies führte zu einer depressiven Phase. Zwar konnte er im selben Jahr ein neues Labor einrichten, seine finanzielle Lage verschlechterte sich aber dadurch gegen Ende 1895.[25]
Im Jahr 1896 beschäftigte sich Tesla mit der damals neu entdeckten Röntgenstrahlung;[26] 1897 widmete er sich wieder intensiv den Ideen zur drahtlosen Energie- und Nachrichtenübertragung,[27] die bis zu einem gewissen Grad in Konkurrenz zu den Arbeiten des Funkpioniers Guglielmo Marconi standen. Tesla beschäftigte sich zu der Zeit auch mit anderen Themen, wie einem 1,1 Meter langen unbemannten U-Boot, das drahtlos ferngesteuert werden konnte und mit Sprengstoff beladen im Spanisch-Amerikanischen Krieg eingesetzt werden sollte, aber nie zum Einsatz kam.[28][29][30]
Ende 1898 zog Tesla in das New Yorker Luxushotel Waldorf-Astoria und konnte dessen wohlhabenden Besitzer, John Jacob Astor IV, als Investor gewinnen. Teslas Finanzen wurden 1898 für ihn verstärkt zum Problem, da seine Ausgaben die unregelmäßigen Einkünfte aus Lizenzen und gelegentlichen Vorträgen überstiegen. Westinghouse unterstützte Tesla mit einem Kredit, dafür musste dieser auf einen Teil der Lizenzeinnahmen aus seinen Patenten verzichten. Außerdem war sein New Yorker Labor für die immer größeren Aufbauten zur Erprobung seiner drahtlosen Energieübertragung zu klein geworden.[31]
Über seinen Patentanwalt inspiriert, der Anteile an dem Elektrizitätswerk El Paso Electric Company in Colorado Springs besaß, baute Tesla ab Mai 1899 in dem damals nur dünn besiedelten Gebiet um Colorado Springs ein größeres Labor auf. Tesla wollte mit den geplanten Anlagen bis zur Weltausstellung Paris 1900 drahtlos „Nachrichten und Energie“ von der Ostküste der USA zu einer geplanten Empfangsstation nach Frankreich übertragen. Das aus Holz aufgebaute Labor enthielt verschiedene Spulen und Aufbauten und in der Mitte einen bis auf 50 m Höhe ausziehbaren Eisenmast, der dazu dienen sollte, Blitzentladungen einzufangen. Tesla bezeichnete in seinem damals geführten Tagebuch dieses Gebilde als magnifying transmitter, war aber gleichzeitig bemüht, möglichst wenig Information darüber nach außen dringen zu lassen. Sein Labor in Colorado Springs durfte von Außenstehenden nicht betreten werden.[32]
Bei seinen Experimenten mit Blitzentladungen beschrieb er auch die im niederfrequenten Bereich in der Atmosphäre auftretenden stehenden Wellen, die heute als Schumann-Resonanz bezeichnet werden. Er konnte diese Beobachtungen aber nicht systematisch einordnen, auch waren um 1900 der Aufbau der Atmosphäre und die Ionosphäre noch unbekannt. Erst 50 Jahre später konnte Winfried Otto Schumann diese Beobachtungen erklären.
Bei seinen Versuchen in Colorado Springs wurden durch künstliche und natürliche Blitzentladung wiederholt Teile des Labors in Brand gesetzt. Im Oktober 1899 steigerte Tesla die Leistung so weit, dass der Generator der El Paso Electric Company durchbrannte und die Stadt Colorado Springs tagelang keinen Strom hatte. Zu der Zeit war Tesla der Überzeugung, ein funktionierendes „Welt-Energie-System“ gefunden zu haben. Im Dezember 1899 entstanden im Labor und in der Umgebung davon einige Aufnahmen für Werbezwecke, die vom Chef-Fotografen des damals renommierten Century Magazine, Dickenson V. Alley, gemacht wurden. Tesla zog am 7. Januar 1900 nach New York und ließ das Labor, wie es war. Er bezahlte weder die offenen Stromrechnungen noch die Löhne seiner Arbeiter – fünf Jahre später wurde Tesla wegen dieser Schulden verklagt, und die Einrichtungen und Materialien des Labors wurden als Baumaterial verkauft.
Am 20. März 1900 erhielt Tesla sein erstes Patent über die drahtlose Energieübertragung,[33] das heute als erstes Patent der Funktechnik gilt, obwohl er damit Energie zur Beleuchtung übertragen wollte. Einen Monat später, am 26. April 1900, meldete Guglielmo Marconi sein Patent zur drahtlosen Telegraphie an – Marconi sollte es gelingen, die erste drahtlose transatlantische Funkverbindung zwischen Nordamerika und Europa zu realisieren.
Nach 1900 wurden die Arbeiten von Tesla zunehmend skurriler, mit einem Hang zum Transzendentalismus und Bereichen der Metaphysik. Den Menschen deutete Tesla als eine gänzlich von äußeren Antriebskräften, vor allem von der Sonnenenergie, bestimmte Maschine (automáton). Mitte 1900 veröffentlichte er im Century Magazine einen ausschweifenden Artikel zur Steigerung menschlicher Energie unter dem Titel The Problem of Increasing Human Energy, in dem der auch für eine bevorzugt vegetarische Ernährung eintretende Vertreter eines energetischen Vegetarismus[34] eine apokalyptische Prophezeiung über die Gegenwart und Zukunft der Menschheit zeichnet sowie vor allem die Nutzbarmachung der Sonnenenergie als „natürliche Quelle zur Energieversorgung der Menschheit“ vorschlägt. Darin kommt auch seine Vorstellung zu einer kabellosen Energieübertragung für die gesamte Menschheit vor. Unter anderem behauptet er darin, dass „die Energie eines Menschen durch die Hälfte seiner Masse, multipliziert mit dem Quadrat einer noch unbekannten Geschwindigkeit“, bestimmt sei (Formel der Kinetischen Energie).[35] Die Steigerung jener menschlichen Energie sei durch „Essen, Frieden und Arbeit“ möglich. Der Artikel, der auch ein Patent von Tesla zu einem „Welt-Energie-System“[36] referenziert, rief heftige Kritik in Fachkreisen und den Medien hervor, unter anderem in großen US-Zeitungen, in denen er als Träumer und unpraktischer Erfinder bezeichnet wurde.[37]
In der Januar-Ausgabe 1901 des auflagenstarken US-Magazins Collier’s Weekly spekulierte Tesla über die Möglichkeit der Kommunikation mit anderen Planeten.[38] Im März 1901 meldete er sein Patent für einen Apparat zum Gebrauch von Strahlungsenergie mit US-Pat. Nr. 685.957 an, der „Raumenergie“ auffangen und in elektrische Energie umwandeln soll.[39] Diese Arbeiten werden bis heute in Teilen jener Parawissenschaft, in denen eine stets verfügbare und praktisch kostenlose sogenannte „Freie Energie“ oder „Raum-Energie“ propagiert wird, als Quelle angegeben.[40]
Ende 1900 verschlechterte sich die finanzielle Situation Teslas wieder. Sein bisheriger Investor, der Hotelier Astor, hatte wegen Teslas Eskapaden kein Interesse mehr. In der Folge brachte sich Tesla verstärkt in der gehobenen New Yorker Gesellschaft ein und verbrachte Zeit auf verschiedenen gesellschaftlichen Veranstaltungen. Im Februar 1901 gelang es ihm, den Bankier J. P. Morgan als neuen Investor zu gewinnen.[41]
Mit Hilfe der Investition von J. P. Morgan konnte Tesla 1901 beginnen, den Wardenclyffe Tower am Nordufer von Long Island zu bauen. Im Verlauf des Projektes kam es zu Differenzen mit seinem Geldgeber, unter anderem weil Tesla gegenüber J. P. Morgan das Projekt als Hochleistungs-Funksender zur Nachrichtenübertragung darstellte, er in Wirklichkeit aber mit dem Turm seinen Traum von der weltweiten drahtlosen Energieübertragung realisieren wollte. Morgan hatte Interesse an einem Funksystem in Konkurrenz zu Marconi, um damit beispielsweise Börsennachrichten aus Europa schneller verfügbar zu haben. Als Tesla J. P. Morgan über die eigentliche Aufgabe des Wardenclyffe Tower im September 1902 informierte, stieg dieser aus dem Projekt aus. Die geschäftliche Beziehung zu J. P. Morgan zerbrach 1904 endgültig.[42]
Im Jahr 1905 kam das Projekt Wardenclyffe Tower zum Stillstand, und die Finanzprobleme Teslas wurden immer prekärer. Er konnte die Mitarbeiter nicht mehr bezahlen, und es kam zu Gerichtsverfahren aus seiner Zeit in Colorado Springs. Ende 1905 und die folgenden Jahre bis 1908 zog sich Tesla immer mehr zurück, schrieb zwar noch gelegentlich Artikel, experimentierte an sich selbst mit Elektroschock-Therapieverfahren und verfiel zeitweise dem Alkohol.[43] Der Turm wurde nicht vollendet und ging nie in Betrieb.
Seine Lage verbesserte sich 1908 durch die Erfindung der Tesla-Turbine, einer Form der Scheibenläuferpumpe, die das Interesse des im Goldbergbau reich gewordenen John Hays Hammond weckte. Mit dem neuen Investor konnte er die Firma Tesla Propulsion Company in New York gründen.[44] Ab 1909 begann er, einen Teil seiner Schulden zurückzuzahlen. Allerdings konnte die Tesla-Turbine mit der damaligen Werkstofftechnik nicht zur Serienreife gebracht werden und wird auch heute wegen praktischer Fertigungsprobleme kaum benutzt. In den Jahren nach 1910 verschärften sich seine finanziellen Probleme wieder.
Im Jahr 1913 ließ sich Tesla in mehreren Sitzungen von Elisabeth Vilma Lwoff-Parlaghy in Ölfarben porträtieren. Bei den ersten Präsentationen des Porträts installierte er extra Glühlampen mit blauem Lichtfilter. Das Bild wurde deshalb unter der Bezeichnung Blue Portrait bekannt und 1913 und 1916 in New York auf Verkaufsausstellungen präsentiert.[45] Es fanden sich aber keine Käufer, und das Gemälde verblieb im Eigentum der Künstlerin. Nach der Versteigerung des Nachlasses der Malerin galt es als verschollen, bis es Anfang der 2000er Jahre im Fundus des Husumer NordseeMuseum-Nissenhaus wiederentdeckt[46] und 2009 als das Blaue Porträt erkannt wurde.[47]
Ende 1913 versuchte Tesla, Prototypen seiner Tesla-Turbine an die in Deutschland ansässigen Bergmann-Elektrizitätswerke zu verkaufen, scheiterte jedoch an den Transportkosten. Im Spannungsfeld vor dem Ersten Weltkrieg versuchte er im gleichen Jahr, Geldgeber im Deutschen Kaiserreich zu finden, veröffentlichte Zeitschriftenartikel über verschiedene Waffengattungen wie „automatische Lufttorpedos“ und ließ dabei Bewunderung für verschiedene deutsche Waffengattungen erkennen.[48] 1914, nach der Kriegserklärung Deutschlands an Russland, wurden alle deutschen Überseekabel durch die britische Marine gekappt – die verbleibende Nachrichtenverbindung war eine von der amerikanischen Tochterfirma der Telefunken betriebene Funkstation in Sayville. Diese Funkstation hatte allerdings eine zu geringe Sendeleistung, um ohne Relaisschiff im Atlantik Nachrichten nach Europa übertragen zu können. Tesla nahm 1914, auch wegen seiner finanziellen Probleme, den Auftrag zur Verbesserung dieser kriegswichtigen Station an. Dies führte dazu, dass der amerikanische Geheimdienst auf ihn aufmerksam wurde.[49]
1916 ließ er sein Teslaventil patentieren, das darauf basiert, dass der Strömungswiderstand in eine Flussrichtung geringer als in die entgegengesetzte ist, wodurch sich eine Gleichrichterwirkung erzielen lässt.
Tesla verfasste zu der Zeit auch verschiedene Veröffentlichungen zu dem Thema Kriegstechnik. So im August 1917 in der Zeitschrift Electrical Experimenter einen Artikel, der sich mit phantastischen Waffensystemen gegen U-Boote beschäftigte. Er wollte dazu „konzentrierte Strahlen hoher Frequenz“ aussenden.
Um 1930 konnte Tesla infolge der Wirtschaftskrise kaum noch für seinen Lebensunterhalt aufkommen und lebte auf Kredit in New Yorker Hotels. 1933 erklärte sich die Westinghouse Company bereit, ihm einen monatlichen Betrag für seine beratenden Tätigkeiten zu zahlen. Am 1. Januar 1934 zog er in das Hotel New Yorker. Am Ende seines Erfinderlebens zog er sich mehr und mehr zurück und beschäftigte sich unter anderem um 1935 mit „Strahlenkanonen“.[50]
Tesla wurde am Morgen des 8. Januar 1943 im Alter von 86 Jahren im Hotel New Yorker vom Personal tot aufgefunden; der Arzt trug als Todeszeitpunkt den 7. Januar 1943 im Totenschein ein.[51] Seine Unterlagen und sein Eigentum im Hotelzimmer wurden von US-Beamten des Office of Alien Property Custodian beschlagnahmt, obwohl Tesla US-amerikanischer Staatsbürger war – man fürchtete, dass seine Unterlagen ins Ausland gebracht werden könnten. Die Urne mit seiner Asche befindet sich seit 1952 im Nikola-Tesla-Museum in Belgrad.[52]
Für seine Leistungen wurde Tesla 1916 die AIEE Edison Medal verliehen. In den 1930er-Jahren wurde er mit insgesamt zwölf Ehrendoktorwürden bedacht,[53] unter anderem von der Universität Prag, im Jahr 1937 von der Technischen Hochschule Graz[54] und den Universitäten in Brünn, Bukarest und Paris. Allein in den USA konnte er in etwa 50 Berufsjahren 112 Patente anmelden.
Tesla war 1,88 m groß und wog von 1888 bis etwa 1926 ohne größere Abweichungen 64 kg. Sein Aussehen wurde vom Zeitungsredakteur Arthur Brisbane (ein in den USA bekannter Chefredakteur des 20. Jahrhunderts) als „fast der größte, fast der dünnste und mit Sicherheit der ernsteste Mann, der regelmäßig zu Delmonico geht“ beschrieben.[55][56] Tesla wurde eine elegante, stilvolle Körperhaltung und große Sorgfältigkeit bei Frisur, Körperhygiene und Kleidung nachgesagt: Ein Erscheinungsbild, das er auch der Geschäftsbeziehungen wegen pflegte.[56] Außerdem wurden Tesla helle Augen, „sehr große Hände“ und „bemerkenswert große“ Daumen zugeschrieben.[55]
Tesla war eine polyglotte Person; er beherrschte acht Sprachen fließend: Serbo-Kroatisch, Tschechisch, Englisch, Französisch, Deutsch, Ungarisch, Italienisch und Latein.[56] Außerdem las Tesla viele Bücher, von denen er manche auswendig lernte; er soll ein fotografisches Gedächtnis besessen haben.[57]
Tesla erzählte in seiner Autobiografie My Inventions: The Autobiography of Nikola Tesla, dass er in jungen Jahren oft krank war. So litt er unter blendenden Lichtblitzen, die vor seinen Augen auftraten, oft begleitet von Ideen, Einfallsreichtum bzw. einer Lösung für ein bestimmtes Problem, auf das er gestoßen war.[57] Wenn er nur den Namen eines Gegenstandes hörte, konnte er ihn sich in realistischen Details vorstellen.[57] Tesla visualisierte eine Erfindung in seinem Kopf mit äußerster Präzision, einschließlich aller Dimensionen, bevor er zur Bauphase überging, eine Technik, die manchmal als visuelles Denken bezeichnet wird. Normalerweise zeichnete er aus dem Gedächtnis. Seit seiner Kindheit hatte Tesla häufige Rückblenden zu Ereignissen, die zuvor in seinem Leben passiert waren.[57]
Tesla heiratete nie und erklärte, dass seine Keuschheit für seine wissenschaftlichen Fähigkeiten sehr hilfreich sei.[57] So sagte er als junger Erwachsener, dass er das Gefühl habe, er könne für eine Frau niemals würdig genug sein und dass er Frauen in jeder Hinsicht für überlegen hält. Seine Meinung zu Frauen begann in späteren Jahren zu schwanken, als er zunehmend den Eindruck gewann, dass Frauen versuchten, Männer zu übertreffen und dominant zu sein. Diese „neue Frau“ stieß bei Tesla laut Eigenaussage auf Ablehnung, da er empfand, dass Frauen ihre Weiblichkeit verlieren, wenn sie versuchen, bestimmend zu sein. In einem Interview vom 10. August 1924 erklärte er: „Die Tendenz der Frauen, den Mann beiseite zu schieben und den alten Geist der Zusammenarbeit mit ihm in allen Angelegenheiten des Lebens zu verdrängen, ist für mich sehr enttäuschend.“[58] Obwohl er in späteren Jahren einem Reporter erzählte, dass er manchmal das Gefühl hatte, durch Nichtheirat ein zu großes Opfer für seine Arbeit gebracht zu haben, entschied sich Tesla, sich nicht um Beziehungspflege zu kümmern und stattdessen die Bedürfnisbefriedigung gänzlich in der Arbeit zu suchen.[59][60][61][62]
Dennoch sprachen viele Menschen sehr positiv und bewundernd über Tesla, wenn dieser sich mit anderen Menschen umgab. Robert Underwood Johnson (US-amerikanischer Autor und Diplomat) beschrieb ihn als „besonders anmutig, aufrichtig, bescheiden, kultiviert, großzügig gegenüber Anderen und Kraftvoll“.[59] Seine Sekretärin, Dorothy Skerrit, schrieb: „Sein freundliches Lächeln und sein edler Charakter unterstrichen immer die Gentleman-Eigenschaften, die in seiner Seele so tief verwurzelt waren.“[59][60][61][62] Teslas Freund Julian Hawthorne (US-amerikanischer Journalist und Autor) schrieb, dass „man selten einen Wissenschaftler oder Ingenieur [wie Tesla] traf, der auch Dichter, Philosoph, Sprachwissenschaftler und zugleich Kenner von guter Musik, Essen und Trinken und ein Liebhaber war.“[57]
Tesla war ein Freund unter anderem von Francis Marion Crawford, Robert Underwood Johnson,[63] Stanford White[64] und Kenneth M. Swezey.[65][66] Im mittleren Alter wurde Tesla zudem ein guter Bekannter von Mark Twain. Die beiden verbrachten viel Zeit miteinander, darunter in Teslas Labor.[63] Twain beschrieb insbesondere die Erfindung des Induktionsmotors von Tesla als „das wertvollste Patent seit dem Telefon“.[67] Auf einer Party der Schauspielerin Sarah Bernhardt im Jahr 1896 traf Tesla den indischen Hindu-Mönch Vivekananda und sie sprachen darüber, wie die Energieideen von Tesla mit der vedantischen Kosmologie übereinzustimmen schienen.[68] In den späten 1920er Jahren freundete sich Tesla mit George Sylvester Viereck (einem Dichter, Schriftsteller und späteren Nazi-Propagandisten) an. Tesla nahm gelegentlich an Dinnerpartys teil, die von Viereck und seiner Frau abgehalten wurden.[69][70]
Tesla brachte manchmal offen seine Abneigung gegenüber übergewichtigen Menschen zum Ausdruck; beispielsweise als er eine Sekretärin wegen ihres Gewichts entließ.[57] Mehrmals wies Tesla zudem eine Angestellte an, nach Hause zu gehen und ihr Kleid zu wechseln.[57]

Als Thomas Edison 1931 starb, war es Tesla, der die einzige negative Meinung zu Edison in der New York Times abgab:„Er [Edison] hatte kein Hobby, kümmerte sich um keinerlei Belustigung und lebte unter völliger Missachtung der elementarsten Hygieneregeln … Seine Methode war extrem ineffizient … es sei denn, der blinde Zufall griff ein. … ich war zunächst fast ein trauriger Zeuge seiner Taten, da ich wusste, dass nur ein wenig Theorie und Berechnung ihm 90 Prozent der Arbeit erspart hätten. Aber er hatte eine wahre Verachtung für das Lernen aus Büchern und für mathematische Kenntnisse und vertraute ganz auf seinen erfinderischen Instinkt und den praktischen amerikanischen Ansatz“.[71]Tesla behauptete, niemals länger als zwei Stunden pro Nacht zu schlafen.[56] Er gab jedoch zu, von Zeit zu Zeit „zu dösen“, „um seine Batterien wieder aufzuladen“.[59] Kenneth Swezey, ein Journalist, mit dem Tesla befreundet war, bestätigte, dass Tesla selten schlief. So erinnerte sich Swezey an einen Morgen, als Tesla ihn um 3 Uhr morgens anrief, um mit ihm über eine Theorie zu diskutieren: „…und als er das Gefühl hatte, zur Lösung gekommen zu sein, beendete er plötzlich das Telefonat.“[59]
Während seines zweiten Studienjahres in Graz entwickelte Tesla eine Leidenschaft für Billard, Schach und Kartenspiele und verbrachte manchmal mehr als 48 Stunden am Stück an einem Spieltisch.[56] Einmal soll Tesla in seinem Labor 84 Stunden ohne Pause gearbeitet haben.[56]
Tesla wurde in seinen späteren Jahren Vegetarier und lebte ausschließlich von Milch, Brot, Honig und Gemüsesäften.[72]
Während seiner New Yorker Jahre arbeitete Tesla jeden Tag von 9 Uhr bis mindestens 18 Uhr, um dann um 20:10 Uhr im Delmonicos Restaurant und später im New Yorker Hotel des Waldorf-Astoria zu sein. Dort hatte er eine Stammbedienung und Tesla legte Wert darauf, dass das Essen punktgenau angerichtet war. Tesla nahm anschließend seine Arbeit wieder auf, an der er oft bis 3 Uhr saß.[56] Jede Nacht soll Tesla zudem intensivst seine Zehen geknetet haben, da dies seine Gehirnzellen stimulieren würde.[59]
Tesla wurde als orthodoxer Christ erzogen; betrachtete sich aber später nicht als „Gläubigen im orthodoxen Sinne“. Er glaubte nicht an das Leben nach dem Tod.[73]
In einem Interview mit Arthur Brisbane (in den USA bekannter Chefredakteur des 20. Jahrhunderts) sagte Tesla, er glaube nicht an Telepathie. Im selben Interview sagte Tesla, dass er glaube, dass alle grundlegenden Gesetze auf ein Einziges reduziert werden könnten.[55]
Tesla widersprach der Theorie, dass Atome aus kleineren subatomaren Teilchen bestehen; er meinte, dass es kein Elektron gibt, das eine elektrische Ladung erzeugt. Er glaubte, dass Elektronen, wenn sie überhaupt existierten, ein vierter Materiezustand oder „Subatome“ wären; diese nur in einem experimentellen Vakuum existieren könnten und diese nichts mit Elektrizität zu tun hatten.[56][74] Tesla glaubte, Atome seien unveränderlich. Er glaubte an ein Konzept aus dem 19. Jahrhundert; an einen allgegenwärtigen Äther, der elektrische Energie übertrug.[75]
Tesla war im Allgemeinen gegen Theorien über die Umwandlung von Materie in Energie.[56] Er kritisierte auch Einsteins Relativitätstheorie und sagte:
„Ich bin der Meinung, dass der Raum nicht gekrümmt werden kann, aus dem einfachen Grund, dass er keine Eigenschaften haben kann. Man könnte genauso gut sagen, dass Gott Eigenschaften hat… Hat er nicht, sondern nur Attribute und diese sind von uns selbst gemacht. Von Eigenschaften können wir nur sprechen, wenn es um Materie geht, die den Raum ausfüllt. Zu sagen, dass sich der Raum in Gegenwart großer Körper krümmt, ist gleichbedeutend mit der Aussage, dass etwas auf nichts einwirken kann.“

Tesla behauptete, sein eigenes physikalisches Prinzip in Bezug auf Materie und Energie entwickelt zu haben, an dem er 1892 zu arbeiten begann,[56] und 1937, im Alter von 81 Jahren, behauptete er in einem Brief, eine „dynamische Gravitationstheorie“ abgeschlossen zu haben, „die den müßigen Spekulationen und falschen Vorstellungen, wie dem gekrümmten Raum, ein Ende setzen würde“. Er erklärte, dass die Theorie „in allen Details ausgearbeitet“ wurde und hoffte, sie bald der Welt geben zu können.[77] Eine weitere Erläuterung seiner Theorie wurde in seinen Schriften nie gefunden.[57]Tesla drückte die Überzeugung aus, dass menschliches „Mitleid“ dazu führte, die „natürlichen rücksichtslosen Funktionen der Natur“ zu stören. Obwohl seine Argumentation nicht von einem Konzept einer dominierenden Herrenrasse oder der inhärenten Überlegenheit einer Person gegenüber einer anderen abhing, setzte er sich für Eugenik und Zucht ein. In einem Interview von 1937 erklärte er[78]:
„Die einzige Methode, die mit unseren Vorstellungen von Zivilisation und Rasse vereinbar ist, besteht darin, die Zucht von Unfähigen durch Sterilisation und die bewusste Führung des Paarungsinstinkts zu verhindern […] Die Meinung der Eugenisten ist, dass wir die Ehe schwieriger machen müssen. Sicherlich sollte niemand gestattet werden, Nachkommen zu zeugen, der kein wünschenswerter Elternteil wäre.“
Im Jahr 1926 kommentierte Tesla indirekt die soziale Ungleichheit der Geschlechter und den Kampf der Frauen für die Gleichstellung der Geschlechter, als er prognostizierte, dass Frauen das dominierende Geschlecht werden würden.[79] In seiner Autobiografie schrieb Tesla, dass der Völkerbund kein Mittel gegen die (damals) vorherrschenden Probleme gewesen war.[80]
Tesla wird von Biographen, neben seiner Bedeutung als Erfinder, Physiker und Ingenieur, trotz seiner Befürwortung der Eugenik (welche zu seiner Lebenszeit eine populäre Ansicht war) als – in philosophischer Hinsicht – Humanist angesehen.[81][82][83]
Unternehmen und Produkte
 Technische Einrichtungen
Sonstiges
Anfang November 1915 berichteten Zeitungen, darunter die New York Times,[87] über die bevorstehende Verleihung des Nobelpreises für Physik zu gleichen Teilen an Nikola Tesla und Thomas Edison; faktisch nominiert wurde allerdings nur Edison.[88] Tatsächlich wurde der Physik-Nobelpreis 1915 an William Henry und William Lawrence Bragg verliehen. Tesla selbst wurde dann 1937 für den Nobelpreis nominiert.[89]
Nikola Teslas Schaffen fand Erwähnung in diversen Filmen, Fernsehserien, Büchern und Computerspielen, darunter
Dieser Artikel ist als Audiodatei verfügbar:
Mehr Informationen zur gesprochenen Wikipedia

Jean-Paul Charles Aymard Sartre [ʒɑ̃ˈpɔl saʀtʀ̩] (* 21. Juni 1905 in Paris; † 15. April 1980 ebenda) war ein französischer Romancier, Dramatiker, Philosoph, Religionskritiker und Publizist. Er gilt als Vordenker und Hauptvertreter des Existentialismus und als Paradefigur der französischen Intellektuellen des 20. Jahrhunderts. Er war der langjährige Partner von Simone de Beauvoir.
Sartre wurde in Paris als Sohn des Marineoffiziers Jean-Baptiste Sartre (1874–1906) geboren. Der Vater starb schon 15 Monate nach der Geburt seines Sohnes Jean-Paul an Gelbfieber. Seine junge Mutter Anne-Marie (1882–1969) zog daraufhin zurück zu ihren Eltern. Dort wuchs Sartre unter dem Einfluss seines Großvaters Charles Schweitzer (1844–1935) auf, eines Onkels von Albert Schweitzer und Gymnasiallehrers (agrégé) für das Fach Deutsch. Von ihm und wechselnden Privatlehrern wurde er zu Hause unterrichtet. Er begann sehr früh zu lesen (auch auf Deutsch), erlitt jedoch schon als Junge eine Linsentrübung im rechten Auge, das nach und nach erblindete und nach außen wanderte, so dass er mit der Zeit immer stärker schielte. Bis zum Alter von zehn Jahren hatte er kaum Kontakte außerhalb seiner Familie, in der er einziges Kind war und blieb. Hiernach kam er auf das prestigeträchtige Gymnasium Lycée Henri IV. Mit fast sechzig beschrieb er diese Kindheit in Les mots (Die Wörter).
1917 heiratete seine Mutter wieder und zog mit ihm zu ihrem neuen Mann, einem Freund des Verstorbenen, nach La Rochelle – zwei Veränderungen, die der Zwölfjährige nur schwer verkraftete. Hinzu kam, dass sein Großvater empört mit ihm brach, als er erfuhr, dass der Junge Geld aus der Haushaltskasse genommen hatte, um sich mit Süßigkeiten bei seinen neuen Klassenkameraden einzuschmeicheln.
1920 wurde Sartre nach Paris zurückgeschickt und besuchte – nunmehr als Internatsschüler – wieder das Henri IV. Hier freundete er sich mit einem Klassenkameraden an, dem späteren Schriftsteller-Kollegen Paul Nizan, der ihn in die zeitgenössische Literatur einführte. 1922 legte er das Baccalauréat ab und beschloss, zusammen mit Nizan, ein Studium an der École Normale Supérieure (ENS) anzustreben, der Elitehochschule für die Lehramtsfächer. Beide wechselten deshalb auf das Lycée Louis-le-Grand, dessen Vorbereitungsklassen (classes préparatoires) für die ENS angeblich besser waren als die des Henri IV.
1923 konnte Sartre eine Novelle und einige Romankapitel in kleinen Zeitschriften unterbringen, zugleich begann er, sich für Philosophie zu interessieren. 1924 belegte er den sechsten Rang in der Aufnahmeprüfung (concours) für die ENS. Sein Wohnheimzimmer dort teilte er mit dem ebenfalls aufgenommenen Nizan.
Die vier Jahre auf der ENS waren eine glückliche Zeit für Sartre: Er las viel und arbeitete regelmäßig jeden Tag von 9 bis 13 und von 15 bis 19 Uhr, was er sein ganzes Leben lang beibehielt. Er absolvierte Kurse und Prüfungen in Psychologie, Moralphilosophie, Soziologie, Logik, Metaphysik und Latein, interessierte sich für die neue Kunstform Film und für den aus Amerika importierten Jazz. Auch nahm er Boxunterricht, denn „le petit homme“ (das Männlein), wie er von seinen Freunden genannt wurde, maß nur 1,53 m.[1]
Bei sonntäglichen Besuchen seiner Eltern, die inzwischen nach Paris gezogen waren, führte er hitzige Debatten mit seinem Stiefvater Joseph Mancy, der ihn als „communiste patenté“ (Kommunist mit Brief und Siegel) apostrophierte. Zwar war Sartre, anders als sein Freund Nizan, kein Mitglied der Kommunistischen Partei Frankreichs, doch war er Sympathisant und verweigerte zum Beispiel zusammen mit Nizan die für Studenten der ENS quasi obligatorische Ausbildung zum Reserveoffizier.
Auch ein erster Versuch mit der Liebe fällt in diese Zeit: Sartre hatte auf einer Beerdigung eine entfernte junge Verwandte aus Toulouse kennengelernt, die ihn aber bei ihren seltenen kurzen Treffen ziemlich frustrierte, ähnlich wie später sein Alter Ego Roquentin im Roman La Nausée (Der Ekel) von seiner Freundin Anny frustriert wird.
Philosophisch begann Sartre, der sich in der Familie seines Großvaters und dann seines Stiefvaters immer überzählig (de trop) gefühlt hatte, eine „Theorie der Kontingenz“ zu entwickeln, gemäß der das menschliche Leben ein Zufallsprodukt sei und nicht unbedingt einen von höheren Mächten verbürgten Sinn habe.
1928 erhielt er bei der Rekrutierungsprüfung (agrégation) für das Amt des Gymnasiallehrers nur den für eine Einstellung nicht ausreichenden 50. Platz, angeblich weil er versucht hatte, originelle Ideen zu äußern.
Nachdem Nizan geheiratet hatte, meinte auch Sartre, dies tun zu müssen und ließ seine Eltern um die Hand einer jungen Frau anhalten, die er kennengelernt hatte; er wurde jedoch abgewiesen. Wenig später, bei der Vorbereitung für den zweiten Anlauf auf „l'agreg“, begegnete er seiner künftigen Weggefährtin Simone de Beauvoir. Beide wurden angenommen, Sartre diesmal auf Platz 1, Beauvoir auf Platz 2.
Während Beauvoir als erst 21-jährige Gymnasiallehrerin nach Marseille geschickt wurde, trat Sartre seinen Militärdienst bei den Meteorologen in Tours an. Sein Ausbilder war der ein Jahr ältere ENS-Kamerad Raymond Aron, der spätere bedeutende Soziologe und Philosoph. Da der Dienst ihn wenig beanspruchte, schrieb Sartre viel: Gedichte, den Anfang eines Romans, Entwürfe zu Theaterstücken.
Zum Beginn des Schuljahres 1931, mit 26, wurde er vom Unterrichtsministerium als Gymnasiallehrer für Philosophie nach Le Havre geschickt.
Beauvoir und er trafen sich aber weiterhin regelmäßig in Paris, ihrem dauerhaften Lebensmittelpunkt. Bei seinen Schülern war Sartre bald als interessanter Lehrer beliebt, aber bei den Kollegen als arrogant verschrien. Er begann an einem Factum sur la contingence (Streitschrift über den Zufall) zu arbeiten, einer polemisch-satirischen Schrift gegen die seines Erachtens allzu optimistische und positive Schulphilosophie, die er gemäß Lehrplan verabreichen musste. 1932 reiste er mit Beauvoir in die Bretagne, nach Spanien und das damalige Spanisch-Marokko, was er vom kleinen Erbe der Großmutter Louise Schweitzer geb. Guillemin (1850–1932) bezahlte.
Zum anschließenden Schuljahrswechsel wurde Beauvoir nach Rouen versetzt, nur 90 Kilometer von Le Havre entfernt, so dass man sich nun bequemer treffen konnte. Gemeinsam interessierten sie sich für Sigmund Freud und dessen Psychoanalyse. Sartre entdeckte die Phänomenologie Edmund Husserls, aber auch die Romane Hemingways. 1933 unternahm man wieder gemeinsame Reisen, diesmal nach London und Italien.
Im Herbst 1933 ging Sartre für ein Jahr als Stipendiat an das Institut français in Berlin. Hier las er Husserl und Heidegger, Faulkner und Kafka und begann aus dem factum einen Roman zu entwickeln, das spätere La Nausée (Der Ekel). Die Politik interessierte ihn nur am Rande, die gerade erfolgte Machtübernahme Hitlers hielt er, wie viele linke Intellektuelle, für einen vorübergehenden Spuk. Nach Ablauf des Stipendiums reiste er mit Beauvoir durch Deutschland, Österreich und die 1918 neugeschaffene Tschechoslowakei.
Ab Herbst 1934 unterrichtete er wieder in Le Havre, wo er sich einsam und deplatziert fühlte und schließlich depressiv wurde. Denn auch die allgemeine Stimmung war schlecht in der Hafenstadt, die besonders stark unter der Weltwirtschaftskrise litt, die mit drei, vier Jahren Verspätung nun auch Frankreich getroffen hatte. Sartres Depression verstärkte sich durch Wahn- und Panikphasen, weil er sich 1935, nachdem er eine Doktorarbeit über die Vorstellungskraft zu schreiben begonnen hatte, von einem befreundeten Arzt die Droge Meskalin hatte spritzen lassen. Eine dieser Drogenpsychosen brachte ihn sogar für zwei Wochen in eine Psychiatrische Klinik. Er nahm am 14. Juli 1935 mit Beauvoir an der antifaschistischen Großkundgebung in Paris teil, mit der die französischen Linksparteien und Gewerkschaften gemeinsam auf den wachsenden Druck der faschistischen Kräfte auch in Frankreich reagierten.
1936 beendete Sartre den Roman, an dem er seit Berlin gearbeitet hatte. Er war sehr enttäuscht, als der Gallimard-Verlag das Manuskript ablehnte. Trotzdem schrieb er nun weiter erzählende Texte. In seinen eigenen Augen war er offenbar zum belletristischen Autor geworden, und er wurde von Beauvoir bestärkt, die inzwischen ebenfalls an einem Roman schrieb.
Im Mai und Juni 1936 gingen sie beide zwar aus Prinzip nicht zur Wahl, waren aber begeistert, als die linke „Volksfront“ die Wahlen gewann. Der Ausbruch des Spanischen Bürgerkriegs im Juli bewegte auch Sartre zutiefst. Den Gedanken, sich als Freiwilliger den antifaschistischen Internationalen Brigaden anzuschließen, verwarf er aber, zumal er gerade nach Laon versetzt und dabei zum Lehrer für Vorbereitungsklassen zur ENS befördert worden war. Nach einer Italienreise mit Beauvoir verarbeitete er das Thema Spanischer Bürgerkrieg in der Erzählung Le Mur (Die Mauer), die von André Gide für die Nouvelle Revue Française angenommen wurde und Aufmerksamkeit erregte, als sie im Juli 1937 dort erschien. In Le Mur gestaltet Sartre eine Grenzsituation menschlichen Daseins, in Form einer radikalen Analyse der Todesangst dreier im Spanischen Bürgerkrieg von der Falange zum Tode Verurteilter. Ein Thema, das er später mehrfach neu aufgriff. Ebenfalls 1937 wurde schließlich auch sein Roman angenommen, wobei der Verleger Gallimard vorschlug, den Text zu kürzen und den ursprünglich geplanten Titel Melancholia in La Nausée (eigentlich: die Übelkeit, der Brechreiz) abzuändern.
Zum Schuljahr 1937/38 wurde Sartre in den Pariser Vorort Neuilly versetzt, auch Beauvoir bekam eine Stelle in Paris. Sie wohnten nun in zwei durch ein Stockwerk getrennten Zimmern eines kleinen Hotels (Hotel Mistral) im IV. Arrondissement. Ans Heiraten dachten sie nicht: Sartre legte keinen Wert auf eine bürgerliche Existenz und Beauvoir verstand sich vor allem als Schriftstellerin und dazu gehörte, dass sie weder Ehefrau noch Mutter sein wollte.
Im April 1938 kam mit Erfolg La Nausée heraus: ein Roman, dessen Ich-Erzähler Roquentin ähnliche Sinn- und Selbstfindungsprobleme hat, wie sie auch Sartre in den Jahren von Le Havre hatte, und der so wie dieser die Krise schließlich nicht durch Selbstmord löst, sondern mit dem Entschluss Romancier zu werden. Auch ein Sammelband mit Erzählungen aus den letzten drei Jahren, den Sartre 1939 unter dem Titel Le Mur herausgab, fand erfreuliche Beachtung. Zugleich beauftragte ihn Gide, eine Artikelserie über moderne Autoren für die Nouvelle Revue Française zu schreiben: Sartre hatte seinen Durchbruch geschafft. Er machte sich nun an ein größeres Romanprojekt und begann dessen ersten Band L'Âge de raison (Die Zeit der Reife).
Waren er und Beauvoir bisher fast hochmütig „freischwebende Intellektuelle“ gewesen (eine Wortschöpfung des Soziologen Karl Mannheim), so begannen sie nun, angesichts des zunehmenden Expansionsdrangs Hitlers, sich politisch zu engagieren. Als Frankreich am 3. September 1939 Deutschland den Krieg erklärte, wurde Sartre eingezogen. „La drôle de guerre“, den Krieg, der zunächst keiner war, verbrachte er im Elsass, wo er an seinem Roman und einem Tagebuch schrieb und sich Notizen für eine philosophische Abhandlung machte. Im April 1940 konnte er auf einem Urlaub in Paris den „prix du roman populiste“ entgegennehmen. Während nach dem 10. Mai 1940 der deutsche Angriff Frankreich ins Chaos stürzte, schrieb Sartre fieberhaft an den letzten Seiten von L'Âge de raison. Ende Juni, kurz vor dem Waffenstillstand, geriet er mit seiner Einheit in Gefangenschaft. Hierbei nahm ihm ein deutscher Offizier das fertige Manuskript ab, verwahrte es aber und ließ es ihm später wieder zukommen.
Sartre verbrachte im Stalag XII D in Trier fast glückliche Monate. Er schloss Freundschaften, zum Beispiel mit dem Jesuitenpater Paul Feller (1913–1979), und verfasste ein versteckt politisches Stück, Bariona ou le Fils du tonnerre (B. oder der Sohn des Donners), das er mit Kameraden zu Weihnachten aufführte.[2] Anders als die anderen Gefangenen, die nach und nach als Zwangsarbeiter auf deutsche Fabriken und Bauernhöfe verteilt wurden, kam Sartre mit Hilfe eines Gefälligkeitsattests (Teilerblindung des rechten Auges) im März 1941 frei. Beauvoir, die sich mit den neuen Verhältnissen in Frankreich offenbar arrangiert hatte, war frappiert von der „Starrheit seines Moralismus“, den er aus dem Lager mitbrachte.
Beide aktivierten nun alte Bekanntschaften und gründeten die Widerstandsgruppe Socialisme et liberté (Sozialismus und Freiheit), die sich mehr gegen das Vichy-Regime richtete als gegen die deutschen Besatzer, die man zu dieser Zeit in Frankreich kaum wahrnahm. Sartres Versuche, Kontakte zu kommunistischen Bekannten zu knüpfen und mit ihnen zusammenzuarbeiten, schlugen fehl. Die Kommunisten, die nach dem Verbot von 1939 schon eine Widerstandsorganisation im Untergrund aufgebaut hatten und die nach dem deutschen Überfall auf die Sowjetunion am 22. Juni 1941 mit Attentaten auf deutsche Soldaten begannen, hielten ihn für einen anarcho-linken kleinbürgerlichen Intellektuellen, der für direkte Aktionen ähnlich unbrauchbar war wie die Figur Hugo im Stück Les mains sales (Die schmutzigen Hände). Sie misstrauten ihm auch wegen seiner ungewöhnlich raschen Entlassung aus der Kriegsgefangenschaft und verbreiteten das Gerücht, er sei Agent der deutschen Gestapo.
Im Sommer machte Sartre mit Beauvoir per Fahrrad eine ebenso anstrengende wie abenteuerliche Reise ins unbesetzte Südfrankreich, um Kontakte zu politisch linken Autoren zu suchen, die sich dorthin zurückgezogen hatten. Die Aktion blieb aber erfolglos. Immerhin entwickelte er auf dieser Fahrt die Konzeption für sein Stück Les Mouches (Die Fliegen), in dem ein ihm selbst ähnelnder Oreste den dem Staatschef Pétain ähnelnden Tyrannen Égisthe erschlägt, jedoch vom Volk, das er befreien will, abgelehnt wird und enttäuscht über dessen politische Unreife das Land verlässt. Mit dem Reueritual des Volkes von Argos spielt Sartre auf die Vorwürfe Pétains an, die Franzosen hätten ihre Niederlage selbst verschuldet durch den „Geist der Genusssucht“ („esprit de jouissance“), den sie sich zur Zeit der Volksfront angewöhnt hätten.
Ähnlich frustriert wie sein Oreste, löste Sartre 1942 seine Widerstandsgruppe auf und beschränkte sich auf das Schreiben. Er stellte Les mouches fertig und arbeitete an seinem philosophischen Hauptwerk, L'Être et le néant (Das Sein und das Nichts). Im Oktober wurde er an eines der besten Pariser Gymnasien versetzt, das Lycée Condorcet, wo er einen Posten erhielt, der durch die Zwangspensionierung eines Kollegen jüdischer Herkunft freigeworden war. 1942 beendete er L'Être et le néant und begann den zweiten Band seiner Romantrilogie, Le Sursis (Der Aufschub).
Ab 1942/43 wurde Sartre im nun langsam erstarkenden Widerstand erneut aktiv und trat dem Comité national des écrivains (Nationalkomitee der Schriftsteller) bei.
Im Frühjahr 1943 erschienen trotz Papierknappheit L'Être et le néant und Les mouches. Letzteres wurde am 3. Juni sogar uraufgeführt – mit Plazet der deutschen Zensur, aber nur mäßigem Erfolg. Später im Jahr verfasste Sartre sein erstes Film-Drehbuch Les jeux sont faits (Das Spiel ist aus) sowie in wenigen Tagen das Theaterstück Huis clos (Geschlossene Gesellschaft), ein Drama um einen Mann und zwei Frauen, die sich mit allen Tricks des Psychoterrors gegenseitig das Leben zur Hölle machen, wo sie der Fiktion nach schon sind. Als Huis clos am 27. Mai 1944 einen Skandalerfolg auslöste, bestätigte es Sartre als eine zentrale Figur im intellektuellen Paris der Zeit. Tatsächlich kannte er inzwischen alle Leute, die dort von Belang waren oder es werden sollten, wie Jean Cocteau, Michel Leiris, Albert Camus, Raymond Queneau, Georges Bataille, Boris Vian, Jean Genet, Armand Salacrou und Jacques Lacan.
Nach der alliierten Landung in der Normandie am 6. Juni 1944 zogen er und Beauvoir es vor, Paris zu verlassen. Sie kehrten erst nach dem Beginn des Abzugs der deutschen Truppen (25. August) in die Stadt zurück.
Da Sartre inzwischen gut von seiner Autorentätigkeit leben konnte, ließ er sich vom Schuldienst beurlauben und quittierte diesen schließlich ganz. Als Anfang 1945 sein Stiefvater starb, zog er zu seiner Mutter. Vorübergehende Heiratspläne mit einer Französin, die er im Winter 1944/45 während eines USA-Aufenthaltes kennengelernt hatte, realisierte er nicht.
In den Nachkriegsjahren war Sartre der tonangebende französische Intellektuelle: Sein L'Être et le néant (Das Sein und das Nichts) und der Essay L’existentialisme est un humanisme (Der Existentialismus ist ein Humanismus) von 1946 galten als Hauptwerke der neuen, hauptsächlich von ihm geschaffenen Philosophie des Existenzialismus, dessen Kernaussage ist, dass der Mensch durch den Zufall seiner Geburt in die Existenz „geworfen“ ist und aktiv selbst versuchen muss, dem Leben einen Sinn zu geben.
Seine Romane ließen sich gut verkaufen: L’Âge de raison und Le Sursis erschienen 1946 gemeinsam unter dem Titel Les chemins de la liberté (Die Wege der Freiheit), 1949 kam auch der dritte Teil der Trilogie hinzu mit dem Titel La Mort dans l’âme (Den Tod in der Seele).
Seine Stücke wurden auf allen französischen und vielen europäischen Bühnen gespielt:
Auch als Publizist war Sartre sehr aktiv. Die von ihm gegründete und herausgegebene Zeitschrift Les Temps Modernes (Moderne Zeiten) wurde ein Forum für viele Autoren von Rang.
Entsprechend wurde sein Leben immer bewegter. Er gab Interviews und ging – oft zusammen mit Beauvoir – auf Vortragsreisen im In- und Ausland.
Auch politisch blieb er engagiert: So war er 1948 Mitbegründer des Comité français d’échanges avec l’Allemagne nouvelle und einer kurzlebigen neuen Partei, die einen „dritten Weg“ zwischen Sozialisten und Kommunisten beschreiten sollte. Allerdings schlug er sich 1952, anlässlich der Verhaftung von Jacques Duclos, dem damaligen Fraktionsvorsitzenden der KPF, auf die Seite der Kommunisten, was den Bruch mit etlichen gemäßigt linken Intellektuellen nach sich zog, zum Beispiel mit Camus, dem er Verrat an den Zielen der Linken vorwarf. 1956 kehrte Sartre wiederum den Kommunisten den Rücken, weil er die sowjetische Intervention in Ungarn missbilligte. In seinem 1960 erschienenen Werk Critique de la raison dialectique (Kritik der dialektischen Vernunft) versuchte er, die marxistische Dialektik mit dem Existenzialismus und dessen Betonung des freien Willens zu verbinden.
In den 50er und 60er Jahren war er einerseits ein Kritiker am Stalinismus, verzichtete jedoch nach seinen Reisen in die Sowjetunion auf weitere Kritik. In den Mai-Unruhen 1968 schlug er sich auf die Seite linker Studenten, war dann von 1970 bis 1973 Weggefährte der französischen Maoisten. Bis zuletzt setzte er sich für die Entrechteten dieser Welt ein, wie 1979 mit Raymond Aron für die Kampagne „Ein Schiff für Vietnam“.
Er schrieb auch in diesen Jahren noch viel, zum Beispiel literaturkritische Artikel (gesammelt gedruckt in den Bänden Situations, 1947–65) und literaturtheoretische Essays – insbesondere den politisches Engagement vom Autor fordernden Qu’est-ce que la littérature (Was ist Literatur), 1947 –, aber auch Autorenmonografien über Baudelaire, 1947, Jean Genet, 1952, Mallarmé, 1953, und Gustave Flaubert, 1971–72; hinzu kamen einige Dramen, darunter 1951 Le Diable et le bon Dieu (Der Teufel und der liebe Gott) oder 1959 Les séquestrés d’Altona (Die Eingeschlossenen)[3] sowie 1963 das autobiografische Werk Les Mots (Die Wörter), das seine Kindheit reflektiert.
In der Öffentlichkeit wurde Sartre seit 1949 immer mehr als „maître à penser“ (Vordenker) und Intellektueller wahrgenommen, der seine Stimme zu den großen und auch manchen kleineren Problemen der Nation erhob und der gegen Menschenrechtsverletzungen in den französischen Kolonialkriegen (Algerienkrieg, Indochinakrieg) und später auch in Vietnam (Vietnamkrieg) oder im kommunistischen Ostblock protestierte. Dies verschaffte ihm allerdings nicht nur Bewunderung, sondern auch den Hass vieler rechtsgerichteter Franzosen.[4] 1960 wurde er in die American Academy of Arts and Sciences gewählt.
Am 22. Oktober 1964 wurde Sartre der Nobelpreis für Literatur zuerkannt.[5] Bereits im September war durchgesickert, dass er Favorit für den Preis sei, und Sartre hatte bereits einen Brief an die Schwedische Akademie gesandt, in dem er schrieb, er wolle nicht auf der Liste der möglichen Preisträger stehen und werde den Preis ablehnen, wenn er ihn erhalten sollte. Die Akademie bestätigte den Eingang eines Briefs des Schriftstellers, dessen Inhalt sie als „vertraulich“ bezeichnete, verlieh ihm den Nobelpreis aber dennoch, obwohl die Schwedische Akademie erhebliche Bedenken gegenüber der literarischen Qualität seines Werkes hegte[6]. Daraufhin begründete Sartre seine Ablehnung des Preises in einer Erklärung, die er seinem schwedischen Verleger zukommen ließ und die in der schwedischen Presse veröffentlicht wurde. Eine deutsche Übersetzung erschien am 30. Oktober 1964 in der Wochenzeitung Die Zeit. Sartre nannte zwei Kategorien von Gründen, „persönliche“ und „sachliche“. Zu den persönlichen Gründen zählte er seine Überzeugung, dass ein Autor seine Stellungnahmen nur mit seinen eigenen Mitteln vertreten solle, nämlich dem geschriebenen Wort. „Der Schriftsteller sollte sich also weigern, sich in eine Institution verwandeln zu lassen, selbst wenn es – wie hier – unter den ehrenvollsten Bedingungen geschieht.“ Als wichtigsten sachlichen Grund bezeichnete er seinen Kampf für die friedliche Koexistenz zwischen „dem, was man den Ostblock nennt“, und dem bürgerlichen Westen. Dieser Kampf solle zwischen Menschen und Kulturen stattfinden, ohne die Intervention von Institutionen. Er lehne daher alle öffentlichen Ehrungen durch Institutionen ab, was auch etwa für den Leninpreis gelte.[7] Da eine Ablehnung des Preises in den Statuten aber nicht vorgesehen ist, gilt er als Preisträger.
Beinahe elf Jahre später, im September 1975, berichteten verschiedene Presseagenturen, dass eine briefliche Anfrage bei der Nobelstiftung eingegangen sei, ob Sartre das Preisgeld doch noch erhalten könne. Stig Ramel, der Präsident der Stiftung, bestätigte den Eingang eines solchen Briefes „aus dem Kreis um Sartre“. Er wisse nicht, ob der Brief von Sartre autorisiert sei, könne aber sagen, dass eine Auszahlung nicht möglich sei, weil das Geld den Regeln entsprechend ein Jahr nach der Preisverleihung in den Stiftungsfonds zurückgeflossen sei. Sartre dementierte heftig, einen solchen Wunsch geäußert zu haben, unter anderem in einem Artikel in Le Monde. Er schrieb, seine Haltung zu dem Preis habe sich nicht verändert und es wäre daher absurd, wenn er das Geld beanspruchte. Axel Madsen vermutet „ein paar junge maoistische Freunde von Sartre“ als Absender des Briefs.[8] In seinen Memoiren aus dem Jahre 2000 griff der schwedische Schriftsteller Lars Gyllensten, langjähriges Mitglied der Schwedischen Akademie, das Thema noch einmal auf: „Sartre oder ein ihm Nahestehender“ habe sich im September 1975 über einen Mittelsmann erkundigt, ob das Preisgeld noch zu haben sei.[9] 
Sein Verhältnis zu Beauvoir (nach wie vor per „Sie“) bestand weiter, hatte sich allmählich aber gelockert. Ab 1973 war er praktisch blind und nicht mehr in der Lage zu schreiben. Trotzdem versuchte er weiter präsent zu sein, unter anderem mit Interviews und gelegentlichen öffentlichen Auftritten. 1974 zum Beispiel erregte sein Besuch bei dem seines Erachtens politischen Häftling und RAF-Mitglied Andreas Baader in der JVA Stuttgart öffentliche Aufmerksamkeit.[10] 1977 unterschrieb er wie etwa sechzig andere Intellektuelle auch einen Appell zur Entkriminalisierung der Pädophilie, der in den Zeitungen Libération und Le Monde erschien. Initiator des Appells war der pädophile Schriftsteller Gabriel Matzneff.[11] 1979 nahm er noch an einer Pressekonferenz zugunsten der „Boatpeople“ genannten vietnamesischen Flüchtlinge teil. Im April 1980 veröffentlichte die Zeitung Nouvel Observateur einen Teil der Gespräche, die er mit Benny Lévy geführt hatte. Dieser Dialog, der in Deutschland 1993 unter dem Titel Brüderlichkeit und Gewalt erschien,[12] überraschte die Öffentlichkeit und irritierte auch Beauvoir. Sartre diskutierte mit seinem Gesprächspartner neue Positionen, insbesondere im Hinblick auf die zwischenmenschlichen Beziehungen und die Frage nach den „sozialen Bedingungen gelingender Selbstverhältnisse“.[13] Man sah in diesen Gesprächen eine Annäherung Sartres an Lévys Philosophie und dessen jüdischen Glauben.
Jean-Paul Sartre starb im Alter von 74 Jahren am 15. April 1980 in Paris. Er blieb bis zuletzt eine bekannte Persönlichkeit des öffentlichen Lebens: Sein Tod wurde weltweit wahrgenommen und bei seiner Beerdigung in Paris folgten 50.000 Menschen dem Sarg.
In dem philosophischen Essay Die Transzendenz des Ego (La transcendance de l’ego. Esquisse d'une description phénoménologique), welcher 1936/37 erschien, zeigt sich Sartre als selbständiger Philosoph. Hier bearbeitete Sartre Themen, die ihn von nun an wiederkehrend beschäftigten: Wie ist das Ego beschaffen? Das ICH und das Psychische als Objekte. Das transzendentale Bewusstsein. „Es genügt, daß das ICH zur gleichen Zeit wie die Welt ist und daß die, rein logische, Subjekt-Objekt-Dualität endgültig aus den philosophischen Überlegungen verschwindet. Die Welt hat das ICH nicht geschaffen und das ICH hat die Welt nicht geschaffen, es sind zwei Objekte für das absolute unpersönliche Bewusstsein, durch das sie sich verbunden finden.“[14]
1938 ist Literatur für Sartre Welterschließung und Philosophie Problemlösung. Die Literatur soll, durch raffiniertes Zusammenfügen von Wörtern, die Welt enthüllen. Die Philosophie hingegen arbeitet mit zum Teil vieldeutigen Begriffen. Die Erhöhung der Literatur zum Instrument der Erkenntnis ist selbst schon Teil seines philosophischen Programms. Prosaische Sprache ist dabei auf zweierlei Weise nützlich: Sie ist in der Literatur ein Instrument der Welterschließung, und sie soll möglichst eindeutige Bedeutungen haben. Sartre hat sich früh intensiv mit dem Unterschied zwischen Prosa und Poesie beschäftigt.
Für Sartre ist die Sprache das bevorzugte Instrument und Ausdrucksmittel, die Welt kennenzulernen und darzustellen, eine Auffassung, die in dem Roman Der Ekel (la nausée) implizit zum Ausdruck kommt.
In dem Roman Der Ekel postuliert Sartre einen Gegensatz zwischen Existenz und Sein. Bei Erik Michael Vogt heißt es dazu: „Hinter der Existenz ist das Sein, so wie hinter der Hand Roquentins die Hand Rollebons (…) ist. Die Existenz wird zum Ort einer vorgängigen Inskription durch das Sein.“[15] Die Wahrheit der Existenz kann nur durch die Sprache vermittelt werden und ist insofern niemals rein und im Gegensatz zum Sein niemals ein Zustand ohne Sprache, ohne Zeichen.
Die Philosophie von Die Transzendenz des Ego und Der Ekel mündet in die von Das Sein und das Nichts (L'être et le néant, 1943) – Sartres philosophischem Hauptwerk vor seiner Hinwendung zum Marxismus. In diesem geht es um das Sein und um die zwei Seinsregionen An-sich und Für-sich (Bewusstsein).
Bewusstsein und die Dinge der Welt (An-sich) können niemals dieselbe Identität haben. Ein Bewusstsein ist immer Bewusstsein-von-etwas. Das Sein (des An-sich) bietet sich der Enthüllung nur in der Erscheinung an. „Tatsächlich ist das Sein sich selbst opak, eben weil es von sich selbst erfüllt ist. Das drücken wir besser aus, wenn wir sagen, das Sein ist das, was es ist.“ (Das Sein und das Nichts, S. 42)[16] Das Bewusstsein ist unabhängig vom An-sich, es ist seine eigene Seinsregion. „Das Sein des Bewusstseins bleibt daher kontingent, insofern dieses Sein an sich ist, um sich in Für-sich zu nichten, das heißt, es gehört nicht dem Bewusstsein zu, es sich zu geben oder es von anderen zu empfangen.“ (Das Sein und das Nichts, S. 176)[17] Daraus ergibt sich, dass das Bewusstsein und die Reflexion nicht eins sind. „Die Reflexion ist das Für-sich, das von sich selbst Bewusstsein hat.“ (Das Sein und das Nichts, S. 289)[18] Da sich das Für-sich (das Bewusstsein) verzeitlicht, ist die Reflexion mit dem Für-sich quasi unmittelbar gegeben. Sartre trennt Bewusstsein und Reflexion jedoch strikt voneinander.[19]
Auch das Ego gehört zur Seinsregion des An-sich. „Sobald das Bewusstsein auftaucht, macht es sich durch die reine nichtende Bewegung der Reflexion zu einem personalen: denn was einem Sein die personale Existenz verleiht, ist nicht der Besitz eines Egos – das nur das Zeichen der Persönlichkeit ist –, sondern das Faktum, für sich als Anwesenheit bei sich zu existieren.“[20] Das Bewusstsein setzt sich als Bewusstsein (von) sich. Das Für-sich kann nicht der Grund seines eigenen Seins sein und wird stets durch ein An-sich bedingt. Sartre nennt das Für-sich den „Riss im Sein“, das Nichts, das durch das Sein geseint wird. Das Für-sich hat zu Objekten aller Art, ob materiell, biologisch, psychisch, eine setzende, gesetzte Distanz. Für-sich und An-sich sind zwei verschiedene Seinsregionen desselben Seins. Das Für-sich, das Bewusstsein, ist nicht, es existiert, denn es hält immer Distanz zum Sein, auch zu sich (zu seinem eigenen Sein), es leiht sich fortwährend sein Sein vom Sein, ohne sich vom Sein einfangen zu lassen und dabei zu erstarren; es ist das geseinte Nichts im Sein.
Deutlich wird hierbei, warum Sartres Philosophie dieser Jahre häufig als idealistische (französische) Bewusstseinsphilosophie bezeichnet wird.
Die Implikationen seiner Philosophie dieser Jahre, insbesondere des Ekels und seines ersten philosophischen Hauptwerks Das Sein und das Nichts, lassen sich folgendermaßen zusammenfassen:
Sartre ist Antinaturalist. Er glaubt, im Gegensatz zu vielen seiner Zeitgenossen, an das Böse.[21] Es gibt für ihn eine unüberbrückbare Spannung zwischen Mensch und Welt. „Der Geist der Ernsthaftigkeit“ will sich der Erfahrung der Kontingenz, des Ekels nicht stellen. „Der Bürger“ – nicht als soziologische Kategorie – hält die bestehende Ordnung und Einrichtung der Welt für notwendig und sinnvoll („Geist der Ernsthaftigkeit“) und für rechtmäßig. „Der Bürger“ ist, er ruht im Sein, er ist mit dem Sein verbunden. „Der Bürger“ ist hier als eine ontologische Kategorie zu verstehen. Für Sartre hingegen ist nichts notwendig, es gebe keine natürlichen Beziehungen untereinander und zur Welt. Man könne der Kontingenz nicht entkommen, diese sei absolut, das Leben habe keinen Sinn. Es gebe keine Heilung, die den unüberwindbaren Bruch zwischen dem Bewusstsein und den Dingen der Welt tilgen könne, keine metaphysische Rettung. Auch in die Vergangenheit legt Sartre keinen Sinn, den sie nicht habe. Die Vergangenheit sei nicht gemeinschaftsstiftend. Des Bürgers Autorität beruhe auf dessen und seiner Vorfahren angeblich sinnstiftender und rechtfertigender Vergangenheit. Der Bürger glaube, diese Vergangenheit habe nur auf ihn gewartet, er sei das Ziel vergangener Zeiten gewesen, und das gelte für den einzelnen Bürger wie für das Bürgertum als Ganzes.
Es gibt verschiedene Textstellen in Der Ekel wie in Die Kindheit eines Chefs (L'enfance d'un chef), die diesen typisch bürgerlichen rechtfertigenden Zugriff auf die Vergangenheit beschreiben.
In Der Ekel erlebt der Ich-Erzähler auch die beunruhigenden Seiten an dieser Haltung: „All jene, die zwischen 1875 und 1910 zur Elite von Bouville gehörten, waren da, Männer und Frauen, sorgfältig gemalt von Renaudas und Bordurin.Die Männer haben Sainte-Cécile-de-la-Mer erbaut. Sie haben […] gegründet […] Sie haben den berüchtigten Dockerstreik von 1898 niedergeschlagen und haben 1914 ihre Söhne dem Vaterland geopfert. […] vom Ehrenplatz aus ließ der Kaufmann P[…] einen klaren Blick auf mich fallen. […] Da begriff ich alles, was uns trennte: was ich über ihn denken mochte, berührte ihn nicht; das war nichts weiter als Psychologie, wie sie in Romanen betrieben wird. Aber sein Urteil durchbohrte mich wie ein Schwert.“[22]
Der Bürger könne sich die Welt nicht anders vorstellen als sie sei. Folgerichtig skizziert Sartre ein paar Jahre später den Kollaborateur als einen Menschen, der sich den vollendeten Tatsachen füge. Dabei nehme der Kollaborateur allerdings die Perspektive der Zukunft ein, aus der jede Vergangenheit und jedes Elend zu einem guten Ende führen würde und darin aufgehoben wäre.
Äußerliche Zwänge gesellschaftlicher, natürlicher oder göttlicher Art leugnet Sartre. Dies sind Zufälligkeiten. Es sind jedoch nur die Grenzen der Situation des Menschen, nicht die Grenzen seiner Freiheit. Der Mensch hat die Kontingenz, diese Grenzen zu übernehmen, zu integrieren und damit die Möglichkeit sie zu überschreiten. Freiheit ist somit die winzige Bewegung über das Gegebene hinaus. Der Mensch trägt insofern Verantwortung, als er derjenige ist, der das Gegebene auf sich nimmt und gleichzeitig mit diesem Aufsichnehmen das Gegebene in seiner Freiheit negieren kann.
In einem Turm gefangen kann der Mensch nicht ohne Weiteres flüchten, aber er kann planen zu flüchten, er kann sich mit der Möglichkeit einer Flucht beschäftigen. Der Mensch kann sich jederzeit über die Situation hinaus entwerfen, selbst wenn er dabei scheitert. Das Scheitern ist nicht der Gegensatz zur Freiheit, sondern eine menschliche Möglichkeit, die sich aus seiner Freiheit ergibt. Die Dinge leisten uns keinen Widerstand. Durch unsere Entwürfe können die Dinge zu einem Widerstand werden. Sartres Beispiel: Der Felsen zum Gipfel kann mir nur Widerstand leisten, wenn ich mir vorgenommen habe, den Gipfel zu erklimmen.
Der Mensch ist in Situation frei, nicht im luftleeren Raum: „So ahnen wir langsam das Paradox der Freiheit: es gibt Freiheit nur in Situation, und es gibt Situation nur durch die Freiheit. Die menschliche-Realität begegnet überall Widerständen und Hindernissen, die sie nicht geschaffen hat; aber diese Widerstände und Hindernisse haben Sinn nur in der freien Wahl und durch die freie Wahl, die die menschliche-Realität ist.“[23]
Am bündigsten formuliert er seine These mit dem Satz „Die Existenz geht dem Wesen voraus“ („L'existence précède l’essence“) – einzig sein nacktes Dasein ist dem Menschen vorgegeben; was ihn am Ende ausmacht, muss er erfinden. Dass die Existenz dem Wesen, der Essenz, vorangehe, ist allerdings eine Formulierung, die der Vorsicht bedarf. Die Formulierung stammt aus einem am 29. Oktober 1945 gehaltenen Vortrag,[24] dessen Druckfassung in der Folgezeit misslicherweise einer der bekanntesten Texte Sartres werden sollte. Die Darstellung seiner philosophischen Thesen in diesem Vortrag stellt – auch nach seiner eigenen Anschauung – eine sehr geglättete und nicht sehr gelungene dar. Laut Annie Cohen-Solal „galt“ die Druckfassung des Vortrags „lange Zeit zum größten Entsetzen Sartres als ‚existentialistische Bibel‘, als ein vorzeitiges ‚kleines rotes Buch‘ und vereinfachende Kurzfassung von L'être et le néant“.[25] In diesem Vortrag steht diese Formulierung auch dafür, „dass der Mensch erst existiert, auf sich trifft, in die Welt eintritt, und sich erst dann definiert.“ Eine im Vergleich zu Das Sein und das Nichts doch recht mechanistische Beschreibung, denn dass die Existenz dem Wesen vorangehe, soll ja keine zeitliche Reihenfolge beschreiben, sondern eine ontologische. In der menschlichen-Realität sind Existenz und Wesen natürlich nicht zu trennen. Sartre würde schon zugestehen, dass mit der Existenz des Menschen quasi zeitgleich das Wesen des Menschen hervortrete.
„Was sagt er eigentlich selbst zum Humanismus, zu dieser Philosophie des Menschen, die […] angeprangert wurde und dessen letztes Bollwerk zu sein man ihm vorgeworfen hat? Er ist der Autor eines Buches, Der Ekel, das […] lange vor dem berühmten und vielleicht auch ärgerlichen Vortrag Der Existentialismus ist ein Humanismus das urkomische, aber auch unerbittliche Porträt dessen zeichnet, was er selbst unter Humanismus versteht.[…]Ist der Existentialismus ein Humanismus? Aber nein, ganz sicher nicht. Vielmehr die erste Manifestation des zeitgenössischen Antihumanismus. Dieser frühe Existentialismus macht in einer bis dahin unbekannten Weise und radikaler als alle Repräsentanten des ‚Denkens von ‘68‘ zusammen allen Formen des Humanismus den Prozeß.“[26]
Allerdings postuliert Sartre hier eine Art kategorischen Imperativ, wenn er behauptet, dass die Forderung nach Freiheit durch den einzelnen jene für alle automatisch nach sich ziehe, was Klaus Hartmann eine „ethische Idee ab extra“ genannt hat, die auf der Grundlage von Das Sein und das Nichts nicht zu begründen und wohl der historischen Konjunktur geschuldet ist.
Zu den Texten des antihumanistischen Sartre, des anarchischen libertären Sartre, im Gegensatz zum Sartre der Kritik der dialektischen Vernunft, gehören die Überlegungen zur Judenfrage. Allein das Datum der Veröffentlichung eines Textes zum Antisemitismus zeugt von intellektueller Größe. Ungekürzt erschien der Text erstmals 1946, zu einem Zeitpunkt, als auch in Frankreich die meisten von den Verstrickungen und der Beteiligung am Holocaust nichts wissen wollten. Ein Auszug aus dem letzten Absatz des Textes: „Wir alle sind mit dem Juden solidarisch, weil der Antisemitismus geradewegs zum Nationalsozialismus führt. Und wenn wir die Person des Juden nicht achten, wer soll uns dann achten? […] sagen wir, dass Antisemitismus kein jüdisches Problem ist: es ist unser Problem. […] Kein Franzose wird frei sein, solange die Juden nicht im Besitz ihrer vollen Rechte sind. Kein Franzose wird in Sicherheit sein, solange noch ein Jude in Frankreich und in der ganzen Welt um sein Leben fürchten muss.“[27]
Die Abstraktheit seiner eigenen These von der absoluten Freiheit des Individuums angesichts der historischen Wirklichkeit (Krieg, Holocaust) erfuhr Sartre jedoch am eigenen Leibe, als er einberufen wurde. Auf Grund dieser Erfahrung, die ihm nicht freiwillig widerfuhr, modifizierte er allmählich seine Philosophie. Werke wie Le diable et le bon dieu, 1951, und Critique de la raison dialectique, 1960, bekräftigten nun, dass das Wesen des Menschen, die Realität seines Daseins und Tuns, nachhaltig gesellschaftlich geprägt seien. In seinem Leben wie in seinem Werk wendete sich Sartre zunehmend dem Marxismus zu.
„Das Leben“ habe ihn „‚die Macht der Dinge‘ gelehrt“, so Sartre in einem Interview 1969, und: „Ich bin davon überzeugt, dass der Mensch immer etwas aus dem machen kann, was man aus ihm macht. Heute würde ich den Begriff Freiheit folgendermaßen definieren: Freiheit ist jene kleine Bewegung, die aus einem völlig gesellschaftlich bedingten Wesen einen Menschen macht, der nicht in allem das darstellt, was von seinem Bedingtsein herrührt.“[28] Ganz neu waren diese Einsichten allerdings nicht, denn schon in Das Sein und das Nichts sprach er beispielsweise von dem „Widrigkeitskoeffizient der Dinge“. Nun sind die „Dinge“ insbesondere gesellschaftliche Bedingungen. Auch zur Zeit von Das Sein und das Nichts hat Sartre nicht gemeint, einzelne Entscheidungen könnten unabhängig von realen Begebenheiten getroffen werden. Es ging und geht ihm auch später um den Entwurf – nicht im Sinne einer einzelnen Entscheidung, sondern um den Entwurf, der als Ganzes die Grundlage des individuellen Handelns ist und damit gleichzeitig die Überschreitung der gegenwärtigen vorgefundenen Situation auf eine Zukunft hin; den Entwurf, der das Sein des einzelnen Menschen ausmacht.
In diesem Sinne hat Sartre auch zu der Zeit, als er den Existentialismus in den Marxismus integriert, nicht von der Grundthese, die Existenz komme vor dem Wesen (esse), abgelassen. Das soll heißen: Das Wesen des Menschen als Gattung ist nicht definierbar, nicht unabänderlich geschaffen, nicht vom Himmel gefallen, vielmehr schafft der einzelne Mensch fortwährend sein eigenes Wesen. „Die Existenz kommt vor dem Wesen“ ist eine negative Bestimmung der Gattung Mensch.
Sartre nimmt an der Heidegger-Rezeption in Frankreich verspätet teil. Sartre greift Heideggers Idee auf, dass die Grundverfassung des Menschen, begrifflich gefasst als Dasein, das „In-der-Welt-sein“ ist: Das Sein des Daseins wird vom jeweiligen Dasein selbst entschieden. Heidegger sah in Sartres Subjektphilosophie jedoch einen Rückfall in den neuzeitlichen Subjektivismus. Für Sartre konstituiert sich das Bewusstsein und damit das Subjekt als Für-sich gegenüber dem von ihm wahrgenommenen etwas, dem An-sich. Das Bewusstsein konstituiert sich also, indem es sich negativ – als das, was es nicht ist – bestimmt. Diese Negation ist das Nichts als dem Subjekt Zugehöriges. Während außerdem Sartre die absolute Freiheit des Menschen betont, versucht Heidegger hingegen das Dasein durch Existenzialien zu erfassen. Auf Sartres Ausspruch in seinem Vortrag Der Existenzialismus ist ein Humanismus „wir befinden uns ja auf einer Ebene, wo es nichts gibt außer den Menschen“ reagiert Heidegger in seinem Brief an Jean Beaufret, dem Brief Über den Humanismus, mit „précisément nous sommes sur un plan où il y a principalement l’Être.“[29] In diesem Brief macht Heidegger deutlich, dass seine und Sartres Denkweisen unvereinbar nebeneinander stünden.
Allgemein betrachtet man den Einfluss Heideggers auf Sartre, obwohl man bei den Wegbereitern der Französischen Philosophie der Generation Sartres gern von den drei Hs spricht (Hegel, Husserl, Heidegger), inzwischen als nicht übermäßig groß. Sartres Lesart von Heidegger ist, wie so oft bei ihm, selektiv und darauf bedacht, Anregungen für das eigene philosophische Projekt zu bekommen. „Dasein“ wird auf Grund der Übersetzung Henry Corbins zu „réalité-humaine“ und verliert bei Sartre schnell die Heideggerschen Konnotationen. (Folgerichtig wird in der Neuübersetzung von Das Sein und das Nichts von Hans Schöneberg und Traugott König der Ausdruck „réalité-humaine“ meist nicht mit „Dasein“ rückübersetzt.) Sartre, der durchaus die Problematik der eigenwilligen Übersetzungen Corbins ins Französische sah, eignete sich die Begriffe Heideggers beziehungsweise deren Übersetzungen in einer Weise an, wie diese seiner Philosophie am besten nutzten. Dies galt nicht allein für die Philosophie Heideggers. „Wenn er (Sartre) Autoren zitiert oder paraphrasiert, so tut er das meist aus dem Gedächtnis und […] in interpretierender Weise. Dieser in Frankreich verbreitete unakademische Umgang mit evozierten Texten, der auch Missverständnisse, Irrtümer und falsche Erinnerungen nicht ausschließt, erwies sich bei Sartre – doch nicht nur bei ihm – als außerordentlich produktiv. Nie geht es ihm um den pedantischen Nachweis einer theoretischen Vorläuferschaft oder einer theoretischen Aporie. Vielmehr benutzt er erinnerte Formulierungen anderer als Formulierungshilfen zur Präzisierung seines eigenen Denkens.“[30] So Traugott König in seinem Anhang Zur Neuübersetzung in Das Sein und das Nichts.
Erheblich größer als der Einfluss Heideggers dürfte der von Hegel, Nietzsche und Husserl auf Sartre sein, vermutlich auch der von Bergson und seinem literarischen Vorbild Gide. Neben Heidegger bezieht sich Sartre in Das Sein und das Nichts, und sei es in Abgrenzung, häufig auf Hegel, Bergson und Husserl. Das Werk Husserls ist wohl dasjenige, das Sartre am nachhaltigsten beeinflusst. Dessen Werk lernt Sartre durch die 1930 veröffentlichte Studie von Levinas kennen, La théorie de l'intuition dans la phénoménologie de Husserl, die er 1933 liest, bevor er Husserl 1933–1934 in Berlin im Original studiert. Levinas selbst scheint aus der Ferne auch nicht ganz ohne Wirkung auf Sartres philosophisches Werk zu sein. Auf alle Fälle wendet sich Sartre zeitlebens, bei allen sonstigen Wendungen und positionellen und politischen Veränderungen, einmal damit in Berührung gekommen niemals mehr von der Phänomenologie ab.
In den 1950er-Jahren überprüft Sartre den Standpunkt seines Existentialismus und widmet sich zunehmend den Bereichen Gesellschaft und Geschichte. Der Artikel Questions de la méthode (Fragen der Methode),[31] hervorgegangen aus der Gelegenheitsarbeit Marksizm i Egzystencjalizm für die polnische Zeitschrift Twórczosc ist als das erste größere Dokument dessen zu betrachten, dass Sartre „den Marxismus als die unüberschreitbare Philosophie unserer Zeit“ ansah.[32]
Sartre diagnostiziert in diesem Artikel einen Stillstand des Marxismus.[33] Der Existentialismus hat als Ideologie seine Daseinsberechtigung, denn er ist „die einzige konkrete Annäherung an die Realität“.[34] Zwar „zielen Existentialismus und Marxismus auf ein und dasselbe Objekt; der Marxismus jedoch hat den Menschen in der Idee aufgehen lassen, der Existentialismus hingegen sucht ihn überall, wo er geht und steht, bei seiner Arbeit, zu Hause und auf der Straße.“[35] Sartre wehrt sich gegen eine Idee des Menschen, das Wesen des Menschen ist nicht festgeschrieben. Trotz seiner Hinwendung zum Historischen Materialismus lässt Sartre keineswegs seine Standpunkte widerspruchslos fallen. In der „Schlussfolgerung“[36] heißt es: Die „‚Ideologie‘ der Existenz […] ist in der Tat der Ansicht, dass sich die menschliche Realität in dem Maße, in dem sie sich macht, dem direkten Wissen entzieht.“[37]
Nachdem die Methoden benannt sind, Marxismus und Existentialismus zusammenzuführen, findet das eigentliche Projekt in der Kritik der dialektischen Vernunft statt. Man beachte dabei, dass im Original der Critique de la raison dialectique eben jene Questions de la méthode vorangestellt sind. Die kritische Bestimmung der dialektischen Vernunft stellt dabei die Korrektur des erstarrten Marxismus dar. Der Marxismus macht aus der Dialektik der menschlichen Behandlung der Natur im ersten Schritt eine alles umfassende Naturdialektik, welcher die Menschen vice versa im zweiten Schritt vollkommen unterworfen sind. Aber die menschliche Geschichte ist keine „Totalität“, sondern eine nicht endende „Totalisierung“. Die menschliche Geschichte ist immer unabgeschlossen.
Jedoch kann man für Sartre die Geschichte als Einheit bestimmen, sonst bliebe sie unverständlich. Sartre fasst die Geschichte in der Totalisierung als Bewegung wie als Einheit auf. Aber wie können wir diese Geschichte verstehen? „Wenn die dialektische Vernunft existiert, so kann sie vom ontologischen Standpunkt aus nur die ablaufende Totalisierung sein […]; und vom erkenntnistheoretischen Standpunkt aus kann sie nur die Durchlässigkeit dieser Totalisierung zu einer Erkenntnis sein, deren Verfahrensweisen prinzipiell totalisierende sind. […] So ist also die Dialektik totalisierende Aktivität.“[38] Da die menschliche Praxis, sei es die eines Individuums oder einer Gruppe, intentional ist, ist sie verstehbar.[39] Sartre nennt als weitergehende „Erklärung alle verzeitlichenden und dialektischen Evidenzen, insofern sie alle praktischen Realitäten totalisieren können müssen, und (beschränkt) den Begriff Verstehen auf das totalisierende Begreifen jeder Praxis, insofern sie durch ihren oder ihre Urheber intentional hervorgebracht wird.“[39]
Der Sartre der Kritik der dialektischen Vernunft räumt im Gegensatz zu früher der Gruppe enorme Bedeutung zu. Die Gruppe ist in besonderen geschichtlichen Momenten weder eine zufällige Ansammlung, eine Serie, noch eine Gruppenbildung ohnmächtiger Individuen, sondern eine Gemeinschaft, die sich auf ein Ziel hin totalisiert. Die Individuen finden in einer Gemeinschaft, in der sie mit anderen die Unfreiheit teilen, ihre Freiheit wieder. Doch solche Gemeinschaften sind nicht von langer Dauer. Sie müssen sich institutionalisieren. Hierbei spielt der freiwillige Eid eine Rolle. Der Eid kann den Widerspruch einer Gruppe zwar nicht auflösen, aber er soll der Auflösung der Gruppe vorbeugen.[40] Im Folgenden (eigentlich bis zum Ende des Buches) verherrlicht Sartre die Gewalt als wichtiges gruppenbildendes Moment.[41] Er zeigt sich hier leider als einer der vielen Denker des 20. Jahrhunderts, die die Gewalt als quasi unvermeidbar oder gar als Motor der Geschichte betrachten. Dieser sogenannte „spätere“ Sartre wird aus genau diesem Grund heute von vielen abgelehnt oder zumindest sehr kritisch betrachtet.
In den im französischen Original der Kritik der dialektischen Vernunft als erstes Werk aufgenommenen Fragen der Methode heißt es am Ende der Schlussfolgerung zu der „Notwendigkeit“ der „existentiellen Untersuchungen“ auf Grund der Doktrin der Marxisten: „Solange die Doktrin sich ihrer Anämie nicht bewusst wird, solange sie ihr Wissen auf eine dogmatische Metaphysik (Dialektik der Natur) gründen wird, statt es auf das Verstehen des lebendigen Menschen zu stützen, solange sie unter der Bezeichnung Irrationalismus alle Ideologien abtut, die – wie Marx es getan hat – das Sein vom Wissen trennen und im Rahmen der Anthropologie die Erkenntnis des Menschen auf die menschliche Existenz gründen wollen, solange wird der Existentialismus seine Untersuchungen fortführen.“[42]
Ontologischer Ansatz: Der Mensch ist das einzige Seiende, bei dem die Existenz (dass er ist) der Essenz (was er ist) vorausgeht, was jedoch nicht als zeitliche Reihenfolge zu verstehen ist.
Begründung: Sein Wesen bestimmende Grundzüge (was er sein soll, damit er eigentlich Mensch ist) gibt es nicht. Sartre geht davon aus, dass es keinen Gott gibt, der den Menschen Werte auferlegt haben könnte, und keine außerhalb des Menschen liegende verbindliche Ethik.
Die Lage des Menschen ist also durch absolute Freiheit gekennzeichnet: „Ich bin dazu verdammt, frei zu sein“ oder: „Der Mensch ist der Statthalter des Nichts“ (Heidegger). Dieser Grund-Situation hat sich der Mensch zu stellen. Alles andere wäre eine Selbsttäuschung. „Es gibt keine Natur des Menschen, die den Menschen festlegt, sondern der Mensch ist das, wozu er sich macht.“
Daraus folgen einige Feststellungen:
„Der Mensch ist voll und ganz verantwortlich“, zunächst für seine Individualität: Mit seinem Tun „zeichnet er sein Gesicht“. Gleichzeitig aber auch für die ganze Menschheit, denn mit seinen Entscheidungen zeigt er auch, was der Mensch sein kann. Insofern ist er immer auch ein Gesetzgeber.
„Der Mensch ist Angst.“
„Der Mensch ist Verlassenheit.“
„Der Mensch ist Verzweiflung.“
„Es gibt Wirklichkeit nur in der Tat“: Der Mensch entdeckt sich in seinem Entwurf, er überschreitet sich, indem er sich auf etwas entwirft. Die Liebe existiert für Sartre nur als verwirklichte Beziehung, das Genie nur als verwirklichtes Genie.
Historische Situation und menschliche Bedingung: „Die historische Situation ändert sich. Was sich nicht ändert, ist die Notwendigkeit, in der Welt zu sein, darin an der Arbeit, darin inmitten der anderen zu sein und sterblich zu sein.“
Die Bedeutung der anderen: Um irgendeine Wahrheit über mich zu erfahren, muss ich mich im anderen spiegeln können. Der andere ist für die Erkenntnis, die ich von mir selber habe, unentbehrlich. Die Entdeckung meines Innersten enthüllt mir zugleich den anderen als eine mir gegenüberstehende Freiheit. Man wählt im Angesicht der anderen, und man wählt sich im Angesicht der anderen. Sartre zeigt in einer Analyse des Angeblicktwerdens („Der Blick“ in: Das Sein und das Nichts), wie ich dem Urteil des anderen unterworfen bin: Der andere als das konkurrierende Bewusstsein, das mich als An-Sich betrachtet, das mich in einem bestimmten Moment oder in meiner Rolle festlegt.
Die existentialistische Moral: Sartre betont die Ähnlichkeit mit dem Akt künstlerischen Schaffens. Man muss die Moral mit der Gestaltung eines Kunstwerkes vergleichen. Gründe: Ein Künstler lässt sich nicht durch festgelegte Regeln leiten. Er muss auch kein bestimmtes Bild machen. Der Künstler bindet sich in die Gestaltung seines Bildes ein; und das Bild, das zu machen ist, ist genau das Bild, das er gemacht haben wird. Wir befinden uns mit unserer Moral in einer vergleichbaren, nach Kreativität verlangenden Lage. Der Inhalt ist immer konkret und daher unvorhersehbar; er ist immer erfunden. Was allein zählt, ist, zu wissen, ob die Erfindung, die getätigt wird, im Namen der Freiheit getätigt wird.
Kann ich ein moralisches Urteil über andere fällen? Wenn der Mensch einmal erkannt hat, dass er in Verlassenheit Werte setzt – dann kann er nur eines noch wollen, nämlich die Freiheit als Grundlage aller Werte. So kann ich im Namen der menschlichen Befindlichkeit als Freiheit Urteile fällen über diejenigen, die danach trachten, die Autonomie ihres Daseins und ihre totale Freiheit zu verbergen.
Die Transzendenz ist ein konstitutives Merkmal des Menschen, aber nicht in dem Sinne, dass ein Bezug zu Gott hergestellt wird, was ja als Konsequenz des fehlenden Gottesbeweises nicht mehr möglich ist. Vielmehr ist Transzendenz bei Sartre das Überschreiten der Ichheit.
Der Existentialismus ist ein Humanismus, „weil wir (die Existentialisten) den Menschen daran erinnern, dass es keinen anderen Gesetzgeber als ihn selbst gibt und dass er in der Verlassenheit über sich selbst entscheidet; und weil wir zeigen, dass der Mensch sich menschlich verwirklicht nicht durch Rückwendung auf sich selbst, sondern durch die ständige Suche eines Ziels außerhalb seiner – wie diese Befreiung oder jene konkrete Leistung.“[43]
Sartres Platz in der Literaturgeschichte wird heute vor allem von seinem ersten, viele autobiografische Elemente enthaltenden Roman La nausée (1938) und den Erzählungen des Sammelbandes Le Mur (1939) gesichert. Indem er sich am amerikanischen Montageroman (unter anderem Manhattan Transfer von John Dos Passos) orientierte, leitete er neben Albert Camus, André Malraux, Antoine de Saint-Exupéry und Blaise Cendrars in der französischen Sprache eine stark vom amerikanischen Realismus geprägte Phase ein. Als weniger bedeutend wird inzwischen sein Theater eingeschätzt, das größtenteils politisch motiviert war. Sein persönlichstes Werk, die philosophische Autobiografie Die Wörter (Les mots, 1964), rekonstruiert Sartres Kindheitsgeschichte. Es galt vielen zeitgenössischen Literaturkritikern als ausschlaggebend für die Zuerkennung des Literatur-Nobelpreises.
Neben Sartres philosophischem und literarischem Schaffen steht mit seinen zahlreichen Schriften über Schriftsteller, Dichter, Maler und Bildhauer[44] ein dritter Komplex, der oft nur am Rande erwähnt wird: In diesen Studien, wie zum Beispiel über Charles Baudelaire, Jean Genet, Stéphane Mallarmé, Alexander Calder, Rene Leibowitz, André Masson, Alberto Giacometti, und besonders über Gustave Flaubert (siehe Der Idiot der Familie), aber auch über Jacopo Tintoretto[45] hat er das Verhältnis dieser Künstler zu ihrem Werk untersucht. Das Schaffen dieser Künstler demonstriert nach Sartre, dass die Freiheit eine unbedingte Voraussetzung der Kunst sei.[46] Trotz ihrer politischen Differenzen stimmt er in dieser Hinsicht mit der Auffassung Albert Camus' überein.[47]
Sartre hat den dritten Komplex seines Werkes mit der Verbindung von Kunst und Philosophie konzipiert: "Ich hatte die Idee, die Literatur und die Philosophie zu einer Technik für eine konkrete Aussage zu verbinden – die Philosophie liefert die Methode und die Disziplin, die Literatur gibt das Wort. Mir ging es darum, die seltsamen und konkreten Beziehungen der Menschen zu den Dingen entwirren und später die der Menschen zu sich selbst."[48] Die seltsamen und konkreten Beziehungen hat Sartre mit seinen Künstlerstudien eingehend untersucht und dabei eine Methode entwickelt, wie man Erkenntnisse über die Frage, wie ein Individuum sich zum Künstler macht, gewinnen kann.
Die herkömmliche Trennung von Philosophie und Literatur im Werk Sartres ist zu relativieren, da er selber die hier vorgestellte besondere Bedeutung seiner Studien über Schriftsteller und Künstler unterstreicht: „… alles was ich geschrieben habe ist gleichermaßen literarisch und philosophisch, in den Roman wie in der Kritik. Ja, es gab zwei Werke mit reiner Philosophie: Das Sein und das Nichts und Die Kritik der dialektischen Vernunft,  das ist aber ein bisschen außerhalb von dem, was ich gerne mache. Jean Genet. Komödiant und Märtyrer und Der Idiot der Familie scheinen mir das zu sein, wonach ich gesucht habe: es geht um das Ereignis, das literarisch beschrieben wird und das zur gleichen Zeit auch einen philosophischen Sinn gibt.“[49]
Sartres Studien über die Künstler erlauben die Rekonstruktion einer Ästhetik, mit der er die Entwürfe der Künstler aufgrund der Analyse ihrer Werke untersucht hat, wobei er weniger die Biographie der Künstler als die Deutung und die Wirkung ihrer Werke im Blick hat. Seine Studien enthalten auch immer ein Nachdenken Sartres über die Wirkung seiner eigenen Werke, so wie Les mots (1964) – Die Wörter abgesehen von ihrer Bedeutung als Autobiographie seiner Jugend mit den beiden Kapiteln Schreiben und Lesen die Voraussetzungen für den Beruf des Schriftstellers, reflektieren.
Ursprünglich wollte Sartre eine Ästhetik verfassen, die er nie geschrieben hat. Er hätte gerne versucht "zu beschreiben, sowohl was ein Maler ist und was ein Gemälde ist, um einen Teil eines Ensembles herauszuarbeiten, das die Ästhetik werden sollte."[50]
Sartre erläuterte in einem Interview mit Michel Sicard die Beziehung zwischen seiner Auffassung von der Freiheit des Menschen und seiner Absicht, eine Ästhetik zu formulieren: "Der Maler oder Schriftsteller als ganz und gar dem Werke zugrunde Liegender beginnt als ursprüngliche Intention seiner Freiheit zu existieren: auf dieser Ebene hätte ich in meiner Ästhetik gezeigt, wie die menschliche Freiheit die einzige Möglichkeit zu malen oder zu schreiben ist."[51] Die Verbindung zwischen Freiheit und Kunst ist für Sartre eine conditio sine qua non des künstlerischen Schaffens: "Wenn man mit seiner Freiheit malt oder schreibt, gibt es im Kunstwerk etwas Besonderes und Eigenes: das Kunstwerk ist niemals eine Kopie der Natur (oder des Naturgegenstandes), sondern eine Produktion außerhalb ihrer. Diese spezifisch menschliche Weise -menschlich weil frei – wäre zu untersuchen gewesen."[52] Diese Untersuchung hat Sartre zwar nicht in seiner angekündigten Ästhetik unternommen, aber alle ihre Bestandteile liegen in seinen zahlreichen Interviews zur bildenden Kunst und in seinen Porträtstudien vor.[53]
Je mehr das Kunstwerk den Betrachter anspricht, ihn herausfordert, es zu überschreiten, etwas für sich und andere daraus zu machen, je größer ist der ästhetische Wert es Kunstwerks. Der Künstler ist als Urheber nur noch indirekt an diesem Prozess beteiligt.[54] Dieses Ergebnis seiner Schriften zur bildenden Kunst korrespondiert mit seiner Rezeptionsästhetik, mit der er die Zusammenarbeit von Autor und Leser postuliert, die für das Entstehen eines geistigen Werkes notwendig sei.[55]
Prudhomme (1901) |
Mommsen (1902) |
Bjørnson (1903) |
F. Mistral/Echegaray (1904) |
Sienkiewicz (1905) |
Carducci (1906) |
Kipling (1907) |
Eucken (1908) |
Lagerlöf (1909) |
Heyse (1910) |
Maeterlinck (1911) |
Hauptmann (1912) |
Tagore (1913) |
nicht verliehen (1914) |
Rolland (1915) |
Heidenstam (1916) |
Gjellerup/Pontoppidan (1917) |
nicht verliehen (1918) |
Spitteler (1919) |
Hamsun (1920) |
France (1921) |
Benavente (1922) |
Yeats (1923) |
Reymont (1924) |
Shaw (1925) |
Deledda (1926) |
Bergson (1927) |
Undset (1928) |
Mann (1929) |
Lewis (1930) |
Karlfeldt (1931) |
Galsworthy (1932) |
Bunin (1933) |
Pirandello (1934) |
nicht verliehen (1935) |
O’Neill (1936) |
Martin du Gard (1937) |
Buck (1938) |
Sillanpää (1939) |
nicht verliehen (1940–1943) |
Jensen (1944) |
G. Mistral (1945) |
Hesse (1946) |
Gide (1947) |
Eliot (1948) |
Faulkner (1949) |
Russell (1950) |
Lagerkvist (1951) |
Mauriac (1952) |
Churchill (1953) |
Hemingway (1954) |
Laxness (1955) |
Jiménez (1956) |
Camus (1957) |
Pasternak (1958) |
Quasimodo (1959) |
Perse (1960) |
Andrić (1961) |
Steinbeck (1962) |
Seferis (1963) |
Sartre (1964) |
Scholochow (1965) |
Agnon/Sachs (1966) |
Asturias (1967) |
Kawabata (1968) |
Beckett (1969) |
Solschenizyn (1970) |
Neruda (1971) |
Böll (1972) |
White (1973) |
Johnson/Martinson (1974) |
Montale (1975) |
Bellow (1976) |
Aleixandre (1977) |
Singer (1978) |
Elytis (1979) |
Miłosz (1980) |
Canetti (1981) |
García Márquez (1982) |
Golding (1983) |
Seifert (1984) |
Simon (1985) |
Soyinka (1986) |
Brodsky (1987) |
Mahfuz (1988) |
Cela (1989) |
Paz (1990) |
Gordimer (1991) |
Walcott (1992) |
Morrison (1993) |
Ōe (1994) |
Heaney (1995) |
Szymborska (1996) |
Fo (1997) |
Saramago (1998) |
Grass (1999) |
Gao (2000) |
Naipaul (2001) |
Kertész (2002) |
Coetzee (2003) |
Jelinek (2004) |
Pinter (2005) |
Pamuk (2006) |
Lessing (2007) |
Le Clézio (2008) |
Müller (2009) |
Vargas Llosa (2010) |
Tranströmer (2011) |
Mo (2012) |
Munro (2013) |
Modiano (2014) |
Alexijewitsch (2015) |
Dylan (2016) |
Ishiguro (2017) |
Tokarczuk (2018) |
Handke (2019) |
Glück (2020) |
Gurnah (2021) |
Ernaux (2022)

Charles André Joseph Marie de Gaulle ([ʃaʁl də ɡol] ; * 22. November 1890 in Lille, Département Nord; † 9. November 1970 in Colombey-les-Deux-Églises, Département Haute-Marne) war ein französischer General und Staatsmann. Im Zweiten Weltkrieg führte er den Widerstand des Freien Frankreichs gegen die deutsche Besatzung an. Danach war er von 1944 bis 1946 Präsident der Provisorischen Regierung. Im Zuge des Algerienkriegs wurde er 1958 mit der Bildung einer Regierung als Ministerpräsident beauftragt und setzte eine Verfassungsreform durch, mit der die Fünfte Republik begründet wurde, deren Präsident er von Januar 1959 bis April 1969 war. Die auf ihn zurückgehende politische Ideologie des Gaullismus beeinflusst die französische Politik bis heute.
De Gaulle wuchs in einer katholisch-konservativ geprägten und gleichzeitig sozial fortschrittlichen Intellektuellenfamilie in Lille auf: Sein Großvater war Historiker, seine Großmutter Schriftstellerin. Sein Vater, Henri Charles Alexandre de Gaulle (1848–1932),[1] der an verschiedenen katholischen Privatschulen lehrte, bevor er seine eigene gründete, ließ ihn die Werke von Barrès, Bergson, Péguy und Maurras entdecken. Schließlich hatte er auch eine Vorliebe für den nordfranzösischen Dichter Albert Samain.[2] Väterlicherseits hatte de Gaulle Vorfahren, die zum alten Landadel der Normandie und Burgunds gehörten. Seine Mutter, Jeanne Caroline Marie Maillot (1860–1940), stammte aus einer Familie reicher Unternehmer aus Lille mit französischen, irischen (MacCartan), schottischen (Fleming) und deutschen (Kolb)[3] Vorfahren.[4]
Während der Dreyfus-Affäre distanzierte sich die Familie von reaktionär-nationalistischen Kreisen und unterstützte den aus antisemitischen Gründen verurteilten Alfred Dreyfus. 1908 trat de Gaulle in die Militärschule Saint-Cyr ein, die er 1912 mit Diplom und Beförderung zum Sous-lieutenant (dt.: Leutnant) verließ. Dort lernte er auch Deutsch.[5][6] Anschließend wurde er in die französische Armee übernommen. Er wurde dem 33e régiment d’infanterie (dt.: 33. Infanterieregiment) in Arras zugeteilt, dessen Kommandeur seit 1910 Colonel (dt.: Oberst) Philippe Pétain war.
Zu Beginn des Ersten Weltkriegs stieg er vom Lieutenant zum Capitaine auf. Bereits im ersten Gefecht bei Dinant erlitt de Gaulle am 15. August 1914 eine Verwundung. Er kehrte dann als Chef der 7. Kompanie zum 33e régiment d’infanterie an die Champagne-Front zurück. Am 10. März 1915 wurde er erneut im Gefecht verwundet. Er war entschlossen, weiterzukämpfen, und widersetzte sich seinen Vorgesetzten, indem er auf die feindlichen Gräben feuern ließ. Wegen dieses Akts des Ungehorsams enthob man ihn für acht Tage seiner Funktionen. Dennoch hatte sich de Gaulle als fähiger Offizier hervorgetan und der Kommandant des 33e régiment d’infanterie bot ihm an, sein Adjutant zu werden.
Am 2. März 1916 wurde sein Regiment in der Schlacht um Verdun bei der Verteidigung des Dorfes Douaumont in der Flanke des Forts von Douaumont von den Deutschen attackiert. De Gaulles Kompanie war schließlich fast vollständig vernichtet, die Überlebenden in einer Ruine eingeschlossen. Laut offiziellem Bericht versuchte de Gaulle daraufhin einen Ausbruch, wurde durch einen Bajonettstich schwer verwundet und ohne Bewusstsein aufgefunden. Nach anderer Darstellung mehrerer Beteiligter ergab sich de Gaulle einer deutschen Einheit, ohne einen Ausbruchsversuch unternommen zu haben.
In deutscher Gefangenschaft erholte er sich von seiner Verwundung. Während der Internierung in Deutschland – zunächst in Osnabrück und Neisse[7][8] – brachte man ihn nach zwei erfolglosen Fluchtversuchen von der Festung Rosenberg in Kronach in ein speziell für aufsässige Offiziere vorgesehenes Lager in der Festung Ingolstadt. In der Gefangenschaft lernte er Michail Tuchatschewski kennen. Er versuchte auch von dort zu fliehen. Einmal kam er bis in die Nähe von Ulm, ehe man ihn erneut fasste. 1918 kam de Gaulle schließlich auf die Wülzburg bei Weißenburg in Bayern. Ein „jämmerliches Exil“ („lamentable exile“), mit diesem Ausdruck beschrieb er seiner Mutter sein Schicksal eines Gefangenen.
Um die Langeweile zu ertragen, organisierte de Gaulle für seine Mitgefangenen umfangreiche Exposés über den Stand des laufenden Krieges. De Gaulles fünf Fluchtversuche scheiterten nicht zuletzt an seiner Körpergröße von 1,95 m, mit der er schnell auffiel. Darüber hinaus unterstützte er mehrere teilweise erfolgreiche Fluchtversuche anderer inhaftierter Kameraden. Nach dem Waffenstillstand im November 1918 wurde er von der Wülzburg entlassen. Von den zweieinhalb Jahren der Gefangenschaft behielt er eine bittere Erinnerung und schätzte sich selbst als „Heimkehrer“ und Soldat ein, der seinem Land nichts genützt hatte.
Während des Polnisch-Sowjetischen Krieges 1919/1920 meldete sich de Gaulle freiwillig für den Dienst in der französischen Militärmission in Polen und fungierte ab dem 17. April 1919 als Infanterieausbilder der neugeschaffenen polnischen Armee. Er wollte durch den Einsatz an diesem entlegenen Kriegsschauplatz seiner militärischen Karriere einen Schub geben, da er sich infolge der Kriegsgefangenschaft während des Ersten Weltkrieges kaum hatte Verdienste erwerben können.[9] Da ihm in Frankreich lediglich ein untergeordneter Posten als Referent beim Premierminister angeboten wurde, bei dem er Soldaten und Offiziere für Auszeichnungen vorschlagen sollte, verlängerte de Gaulle seinen Dienst in Polen und nahm im Mai 1920 an dem Angriff der polnischen Armee auf Kiew teil (polnisch-sowjetischer Krieg).[10] Er wurde zum Stabschef General Henri Albert Niessels in Warschau befördert und erhielt die höchste polnische Militärauszeichnung Virtuti Militari. Einige Historiker nahmen fälschlich an, dass die Erfahrungen in Polen de Gaulles Ansichten in Bezug auf den Einsatz von Panzern und Flugzeugen und den Verzicht auf die traditionelle Kriegsführung mittels Schützengräben beeinflussten. Sein Biograph Eric Roussel (* 1951) weist demgegenüber darauf hin, dass das Konzept, Panzer für schnelle Vorstöße unabhängig von der Infanterie zu verwenden, erst 1927 durch den französischen General Aimé Doumenc entwickelt wurde.[9]
Nach seiner Rückkehr aus Polen heiratete de Gaulle im April 1921 Yvonne Vendroux und nahm einen Posten als Lehrer an der renommierten Militärschule Saint-Cyr in Paris an, der Kaderschmiede der französischen Armee. De Gaulle war damit materiell gut abgesichert, geriet aber bald in Konflikt mit seinen Vorgesetzten aufgrund seines selbstbewussten Verhaltens und unkonventioneller Ansichten, die er in seinem Unterricht vertrat. Infolgedessen wurde er nicht befördert und wechselte 1925 in den persönlichen Stab des Marschalls Philippe Pétain. Gegenüber einem Freund soll er geäußert haben, dass er die Militärschule St.-Cyr nicht wieder betreten würde, außer als Direktor.[11]
De Gaulles wichtigste Aufgabe bestand fortan darin, zwei Bücher vorzubereiten, die unter dem Namen des berühmten Marschalls erscheinen sollten, jedoch kam es mit Pétain zu Auseinandersetzungen über den Inhalt der Bücher und zu einer deutlichen Abkühlung in dem zuvor freundschaftlichen Verhältnis.[12] Dennoch förderte Pétain de Gaulles Karriere: Im September 1927 übernahm de Gaulle als Bataillonschef ein aktives Kommando bei den französischen Besatzungstruppen in Trier. Ebenfalls setzte Pétain durch, dass de Gaulle im April 1927 eine Reihe von Vorträgen an der Militärschule St.-Cyr halten durfte, gegen den Willen des Schulleiters, General Pierre Héring. 1932 veröffentlicht de Gaulle den Inhalt dieser Vorträge in seinem Buch Le fil de l'épée. Darin vertrat er die Ansicht, die französische Armee müsse das Amt eines Oberkommandierenden schaffen, der im Fall eines Krieges in alleiniger Verantwortung und mittels diktatorischer Vollmachten das Schicksal des Landes bestimmen solle. Diese Auffassung konnte sich wegen der Rivalität der Generäle im Generalstab und der traditionellen Feindschaft zwischen den Waffengattungen der französischen Streitkräfte nicht durchsetzen.[13]
Von 1929 bis 1931 übernahm de Gaulle ein Kommando im französischen Mandatsgebiet Libanon.[14] Dieser Posten, weit entfernt vom Hauptquartier in Paris, diente kaum seiner Karriere und widersprach zudem seinen persönlichen Ansichten, wonach die Kolonialarmeen bei der Verteidigung Frankreichs nur eine untergeordnete Rolle spielten. Wegen des Zerwürfnisses mit Pétain wurde ihm jedoch kein besseres Kommando angeboten.[15] Von 1932 bis 1937 bekleidete de Gaulle eine untergeordnete Rolle im Nationalen Verteidigungsrat (Conseil supérieur de la défense nationale), dessen Aufgabe unter der Leitung von Marschall Pétain darin bestand, die französischen Streitkräfte auf einen möglichen Krieg vorzubereiten und über Kriegsstrategien, Bewaffnung und Aufstellung zu entscheiden. De Gaulles Rolle beschränkte sich darauf, Denkschriften für die Sitzungen des Verteidigungsrates vorzubereiten. Da er für eine offensive Kriegführung eintrat, die den Ansichten der meisten Generäle entgegenlief, blieben seine Entwürfe weitgehend unbeachtet.[16]
Im Jahr 1934 veröffentlichte de Gaulle sein bis dahin bedeutendstes Buch, eine Sammlung von Aufsätzen unter dem Titel Vers l’Armée de Métier („In Richtung auf eine Berufsarmee“), und forderte darin eine Reorganisation der französischen Armee, die von einer schlecht ausgebildeten Freiwilligenarmee in eine Berufsarmee umgewandelt werden sollte. Allein diese sei in der Lage, im Falle eines Krieges das Land ausreichend zu schützen und moderne Waffen wie Flugzeuge und Panzer wirkungsvoll einzusetzen. Diese Schrift forderte auch zum ersten Mal die Schaffung von Panzerverbänden, die in der Lage wären, mit schnellen, motorisierten Verbänden ins Territorium des Feindes einzudringen, statt hinter der Maginot-Linie defensiv auf dessen Angriff zu warten. Nur so könne Frankreich seine momentane qualitative Überlegenheit nutzen und seine quantitative Unterlegenheit gegenüber Deutschland kompensieren.[17] Diese Forderungen verband de Gaulle erneut mit der Idee, im Falle eines Krieges sämtliche Streitkräfte dem Kommando eines einzelnen Oberbefehlshabers zu unterstellen. Für diesen Posten sah er einen Mann vor, der „stark genug sei, seine Rolle auszufüllen, geschickt darin, die Zustimmung der Menschen zu gewinnen, groß genug für eine große Aufgabe“ – eine Art Diktator, der die Macht im Land übernehmen würde. Nach Ansicht des Historikers Eric Roussel bedeutete diese extreme Forderung einen schweren Fehler, denn dadurch wurde es schwierig, für die als dringlich empfundenen Militärreformen eine Mehrheit im Parlament zu gewinnen: Der sozialistische Ministerpräsident, Léon Blum, etwa befürchtete 1936, durch die Bildung einer Berufsarmee werde die Basis für einen künftigen Staatsstreich geschaffen. Da de Gaulle kaum Unterstützung von Seiten des Generalstabs erwarten konnte, erschien sein Projekt nicht durchführbar.[18]
Unterdessen nahmen ausländische Militärs, insbesondere Heinz Guderian im deutschen Generalstab, de Gaulles Ideen interessiert zur Kenntnis und sahen sich in ihren eigenen Bestrebungen bestärkt, eine moderne Panzerwaffe zu schaffen; de Gaulles Gegner im französischen Generalstab dagegen, besonders die Generäle Maxime Weygand, Maurice Gamelin und Louis Maurin, lehnten den Plan entschieden ab, woraufhin auch Marschall Pétain im März 1935 verlauten ließ, dass er die Reformpläne seines ehemaligen Schützlings nicht unterstützen würde. De Gaulle entfaltete daraufhin in den folgenden Jahren eine politische Kampagne in der Presse und im Parlament, die ihm den Spitznamen Colonel Motors einbrachte, und gewann in allen politischen Lagern Befürworter, sodass am 15. März 1935 zumindest Teile der Reform im französischen Abgeordnetenhaus beschlossen und sechs motorisierte Verbände aufgestellt wurden, deren Angehörige Berufssoldaten sein sollten. Am 25. Dezember 1936 übertrug man de Gaulle das Kommando über einen dieser neuen Panzerverbände, das 507. Panzerregiment in Metz.[19] Im Generalstab wurde die Reform jedoch verwässert, und es wurde bestimmt, dass diese Verbände ausschließlich der Defensive dienen und gemeinsam mit den (langsamen) Infanterieverbänden operieren sollten. Viele Militärhistoriker sehen darin eine wichtige Ursache für die Niederlage der französischen Armee im Mai 1940 gegenüber den schnellen deutschen Panzerarmeen. Obwohl de Gaulle mit seinem Reformkonzept letztlich scheiterte, hatte die politische Kampagne doch den Effekt, ihn bekannt zu machen; sie öffnete ihm den Weg in die Politik und damit auch in seine künftige Rolle eines Führers des französischen Widerstands im Zweiten Weltkrieg (siehe Forces françaises libres, Résistance).
Als der Zweite Weltkrieg ausbrach, war de Gaulle Colonel. Bei der Verteidigung gegen die deutsche Offensive erhielt er am 14. Mai 1940 das Kommando über die neue 4e division cuirassée (4. Panzerdivision). Am 17. Mai führte er mit 200 Panzern ohne Luftunterstützung einen Gegenangriff auf Montcornet nordöstlich von Laon. Er griff von der Aisne her nach Norden an und überrollte deutsche Fahrzeugkolonnen. Erst am Ortsrand von Montcornet gelang es Panzerabwehrkanonen und 8,8-cm-Geschützen, sie zu stoppen. Die Division de Gaulles musste sich nach Luftangriffen, einem Gegenangriff der deutschen 10. Panzer-Division und eigenen schweren Verlusten zurückziehen. Zwei Tage später kam sie nochmals bei Crécy-sur-Serre zum Einsatz. Dort wurde das Gefecht vor allem durch den Einsatz der Luftwaffe entschieden. De Gaulle warf man später vor, keine Luftunterstützung angefordert zu haben. Am 28. Mai hatte er mehr Erfolg, als seine Panzerdivision die Wehrmacht bei Caumont zum Rückzug zwang. Er war in der Phase der deutschen Invasion in Frankreich der einzige französische befehlshabende Offizier, dem es gelang, die Deutschen zu einem Rückzug zu zwingen. Am 1. Juni hatte er den temporären Dienstgrad eines Général de brigade (Brigadegeneral).
Am 6. Juni ernannte Ministerpräsident Paul Reynaud ihn zum Unterstaatssekretär für nationale Verteidigung und zum Verantwortlichen für die Koordination mit Großbritannien. Als Kabinettsmitglied lehnte er den Waffenstillstand von Compiègne (1940) ab und reiste am 15. Juni nach Großbritannien. Dort vereinbarte er mit Winston Churchill am 16. Juni eine Fortsetzung der britisch-französischen Kooperation gegen Deutschland. Als er am Abend nach Bordeaux zurückkehrte, dem provisorischen Sitz der französischen Regierung, schickte sich Marschall Pétain an, legal die Macht zu übernehmen. De Gaulle missbilligte die Politik Pétains, der den kapitulationsähnlichen Waffenstillstand mit dem Deutschen Reich zu unterzeichnen bereit war, und lehnte Pétains Tun als illegitim ab. Mit 100.000 Goldfranken aus einem Geheimfonds Paul Reynauds flog er am Morgen des 17. Juni 1940 von Bordeaux zurück nach London.[20]
Während Pétain ankündigte, mit Deutschland einen Waffenstillstand zu vereinbaren, erlaubte der britische Premierminister Churchill de Gaulle, über den Hörfunk der BBC zum französischen Volk zu sprechen.[21] De Gaulle rief darin französische Offiziere und Soldaten, Ingenieure und Facharbeiter der Waffenindustrie im Vereinigten Königreich auf, ihm zu folgen, und beschwor, dass die Niederlage nicht endgültig sei („Was auch immer geschehen mag, die Flamme des französischen Widerstandes darf nicht erlöschen und wird auch nicht erlöschen“).[22] Er betonte die Bedeutung der Unterstützung durch Großbritannien und der Vereinigten Staaten. In Frankreich konnte man den Appell zuerst am 18. Juni 1940 um 19 Uhr hören. Er wurde zudem in den Zeitungen des noch unbesetzten südlichen Landesteils abgedruckt und in den folgenden Tagen von der BBC wiederholt ausgestrahlt. Der Appell gilt als de Gaulles größte Rede;[21] Régis Debray schreibt, auch wenn de Gaulles Appell „das Gesicht der Welt nicht verändert habe, so habe dank ihm immerhin Frankreich das seine gewahrt.“[20]
Das britische Kabinett hatte im Vorfeld der Rede dem französischen Innenminister, Georges Mandel, vorgeschlagen, nach England zu kommen und einen Appell an die Franzosen zu richten. Im Gegensatz zu Blum, seinem Ministerpräsidenten, hatte Mandel zuvor in mahnenden Reden von der Bedrohung durch das Deutsche Reich staatsmännische Weitsicht bewiesen, doch hatte er es abgelehnt, Frankreich zu verlassen, um sich nicht dem Vorwurf der Fahnenflucht auszusetzen (ebenso wie Blum war er Jude), und stattdessen empfohlen, die führende Aufgabe in London de Gaulle zu übertragen.
Am 25. Juni 1940 gründete de Gaulle in London das Komitee Freies Frankreich (France libre) und wurde Chef der „Freien Französischen Streitkräfte“ (Forces françaises libres, FFL) und des „Nationalen Verteidigungskomitees“. Daraufhin wurde de Gaulle vom Kriegsrat der Vichy-Regierung im August 1940 wegen Hochverrats in Abwesenheit zum Tode verurteilt.
Die meisten Staaten erkannten das Vichy-Regime Marschall Pétains als die legitime Regierung Frankreichs an. Churchill bemühte sich zwar anfangs diplomatisch um das Vichy-Regime, unterstützte dann aber de Gaulle und ließ die in Nordafrika in Mers-el-Kébir unter dem Kommando von Pétains Marineminister, Admiral François Darlan, vor Anker liegende französische Kriegsflotte am 3. Juli 1940 in der Operation Catapult zerstören.
Im Juni 1940 rief er die französischen Kolonien auf, ihn zu unterstützen. Französisch-Äquatorialafrika und Kamerun schlossen sich im August 1940 unter Félix Éboué dem France Libre an.[23] Ab 1942 unterstellten sich auch Diego Suarez auf Madagaskar und Dakar in Französisch-Westafrika dem Freien Frankreich, das vom Comité National Français regiert wurde. De Gaulle sorgte insbesondere dafür, dass Frankreich im Lager der Alliierten durch die Freien Französischen Streitkräfte (FFL), die an verschiedenen Fronten den Kampf fortsetzten, stets präsent war. Unter anderem förderte er mittels André Dewavrin (Colonel Passy), Pierre Brossolette und besonders Jean Moulin die Résistance. Mit der Transformation zur France combattante (Kämpfendes Frankreich) strich er die politische Einheit des France libre mit der Résistance intérieure heraus.
Der Libanon (damals Großlibanon) wurde im September 1941 als eines der ersten französischen Protektorate durch alliierte Truppenverbände der Kontrolle des Vichy-Regimes entzogen. Bei der anschließenden Machtübernahme durch das „Freie Frankreich“ kamen de Gaulle seine Kontakte aus seiner Dienstzeit in Beirut 1929–1931 zugute. General Fuad Schihab, der spätere Staatspräsident, bildete einen Freiwilligenverband von 20.000 Mann, der zu Beginn der Kampagne des Freien Frankreichs einen erheblichen Teil des Truppenkontingents bildete.[14]
De Gaulle konnte Churchill zur Unterzeichnung des Accord de Chequers (7. August 1940) bewegen, demzufolge Großbritannien die Integrität aller französischen Besitzungen und die „integrale Restauration und Unabhängigkeit und die Größe Frankreichs“ erhalten sollte. Außerdem erbot sich die Kriegsregierung Churchill, die Ausgaben des Freien Frankreichs zu finanzieren; de Gaulle bestand aber darauf, dass die Summen rückzahlbare Vorschüsse und keine Spenden seien, die später einen Schatten auf ihn und die Unabhängigkeit seiner Organisation geworfen hätten. Die Vorschüsse wurden noch vor Ende des Krieges zurückgezahlt. 
Trotz der Verträge zwischen Churchill und de Gaulle waren die Beziehungen der beiden Männer angespannt. Mit Blick auf die Nachkriegsordnung bezeichnete Churchill de Gaulle in Telegrammen als „größten einzelnen Feind für den Frieden in Europa“ und „schlimmsten Feind Frankreichs“.[24] Churchill kritisierte, dass de Gaulle „sich als Retter Frankreichs aufspielen will, ohne einen einzigen Soldaten zur Operation beizusteuern“ und dass sein Verhalten und seine Persönlichkeit das größte Hindernis für gute Beziehungen zwischen Frankreich und den Angloamerikanern seien.[24] Über die Invasion in der Normandie informierte Churchill de Gaulle erst fünf Tage vor der Landung.[24]
Auch die Beziehungen de Gaulles zum US-amerikanischen Präsidenten, Franklin D. Roosevelt, waren belastet – Roosevelt misstraute de Gaulle. De Gaulle seinerseits beklagte amerikanische Arroganz und sagte: „Ich bin zu arm, um mich zu beugen.“ Roosevelt unterstellte de Gaulle diktatorische Absichten.[25] Roosevelts langjähriger Intimus, Admiral William Daniel Leahy, war vom 8. Januar 1941 bis zum 1. Mai 1942 US-Botschafter in Vichy-Frankreich.
Trotz de Gaulles Ausschlusses von der anglo-amerikanischen Landung in Nordafrika (Operation Torch) und vor allem trotz Roosevelts Unterstützung für Admiral François Darlan und General Henri Giraud, die nach der Landung in Nordafrika das Vichy-Regime mit US-amerikanischer Duldung in Algier fortzusetzen suchten, gelang es de Gaulle im Mai 1943, in Algier Fuß zu fassen. Er schuf von dort das Französische Komitee für die nationale Befreiung (CFLN), um die politischen Strömungen im befreiten Frankreich zu einen, und stand alsbald an dessen Spitze. Das CFLN bezeichnete sich vom Juni 1944 an als provisorische Regierung der Französischen Republik (Gouvernement provisoire de la République Française, GPRF) und zog am 25. August 1944 in das befreite Paris ein, wo tags darauf auf der Avenue des Champs-Élysées ein von de Gaulle angeführter öffentlicher Triumphzug durchgeführt wurde.[26]
Danach gelang es de Gaulle, eine alliierte Militärregierung für die besetzten Gebiete in Frankreich zu verhindern und schnell den Forces françaises libres die Regierungsgewalt zu übertragen. In weiten Teilen der Bevölkerung wurde er als Befreier gefeiert, obwohl er bei der Landung in der Normandie und dem folgenden Vormarsch der Alliierten keine militärische Rolle gespielt hatte.
Als de Gaulle sich nach dem Einzug in Paris nicht zuerst bei den Forces françaises de l’intérieur (FFI) für ihre Unterstützung bedankte, sondern bei der Gendarmerie, die erst tags zuvor die Seiten gewechselt und sich von Deutschland losgesagt hatte, verstörte er damit viele Widerständler. Doch wollte er mit dieser Geste jedwede Auseinandersetzung unter den bewaffneten Franzosen vermeiden, die den Alliierten einen Anlass für die Errichtung eines Besatzungsregimes hätte liefern können. Zugleich betonte er mit seiner Rückkehr in das Kriegsministerium die Kontinuität der Dritten Französischen Republik und die Illegitimität der Vichy-Regierung. So erklärte de Gaulle, als der Vorsitzende des Conseil National de la Résistance, Georges Bidault, ihn nach seinem Einzug in Paris aufforderte, die Republik auszurufen:
„Die Republik hat nie aufgehört zu bestehen. Das freie Frankreich, das kämpfende Frankreich und das Französische Komitee der nationalen Befreiung haben sie nacheinander verkörpert. Vichy war immer und bleibt null und nichtig. Ich bin der Präsident der Regierung der Republik. Warum sollte ich sie ausrufen?“[27]
Das Vichy-Regime floh, als die Besatzungstruppen der Wehrmacht sich infolge der Operation Dragoon zurückziehen mussten, nach Sigmaringen. Gleichzeitig setzte de Gaulle kompromisslos die Autorität der provisorischen Regierung gegenüber den Organisationen der Résistance durch; er löste deren Einheiten am 28. August 1944 auf und erklärte ihren Kommandanten, sie hätten nun ins zivile Leben zurückzukehren.
De Gaulle wollte die Säuberung des Staates von Kollaborateuren nicht den Siegermächten überlassen, sondern betrachtete dies als originäre Aufgabe der Franzosen selbst. Am 4. April 1944 nahm das CFLN zwei kommunistische Kommissare auf. Am 27. November 1944 amnestierte de Gaulle den bei Kriegsbeginn in die Sowjetunion desertierten Generalsekretär der KPF, Maurice Thorez. Im Februar 1945 erreichte er auf der Konferenz von Jalta die Anerkennung Frankreichs durch die drei großen Alliierten als eine der zukünftigen Besatzungsmächte Deutschlands. Anfang Dezember 1944 unterzeichnete de Gaulle einen auf 20 Jahre abgeschlossenen Hilfs- und Freundschaftsvertrag mit der Sowjetunion.[28] Im Januar 1945 kam es zwischen de Gaulle und den USA zu Unstimmigkeiten bezüglich der Verteidigung Straßburgs während des Unternehmens Nordwind.
De Gaulle stellte seine Vision von der politischen Organisation eines demokratischen französischen Staates am 16. Juni 1946 in Bayeux vor.[29] Die projektierten Reformen beinhalteten insbesondere eine Modernisierung des staatlichen Sozialsicherungssystems und das Frauenwahlrecht.
Bereits am 16. Mai 1945 hatte de Gaulle die Aufnahme Frankreichs als ständiges Mitglied im Weltsicherheitsrat der Vereinten Nationen (UNO) durchgesetzt. Nach dem Krieg wurde er am 13. November 1945 zum Präsidenten der provisorischen französischen Regierung ernannt. Bereits am 20. Januar 1946 trat er jedoch nach Differenzen mit den seit den Wahlen im Oktober im Parlament (der Verfassunggebenden Versammlung) dominierenden Sozialdemokraten und Kommunisten zurück, weil er deren Verfassungsentwurf ablehnte, der die Macht beim Ein-Kammer-Parlament konzentriert hätte. Nachdem der erste Verfassungsentwurf im Referendum im Mai 1946 abgelehnt worden war, warb de Gaulle in seiner Rede von Bayeux am 16. Juni 1946 für seinen eigenen Entwurf einer politischen Ordnung mit einer stärkeren Stellung des Präsidenten. Dieser konnte sich aber auch in der zweiten Verfassunggebenden Versammlung nicht durchsetzen, die die Verfassung der Vierten Republik beschloss, die zwar eine zweite Parlamentskammer, aber ebenfalls ein rein parlamentarisches Regierungssystem vorsah.
Im Jahr 1947 gründete de Gaulle das Rassemblement du peuple français (RPF), eine politische Bewegung, die ihm helfen sollte, seine Vorstellungen von einer neuen Verfassung durchzusetzen. Im selben Jahr hielt er zwei als bedeutend geltende Reden: am 7. April 1947 in Straßburg[30] und am 27. Juli 1947 in Rennes.[31] Das RPF hatte bei den Kommunalwahlen im Oktober 1947 einen ersten Erfolg und stellte anschließend in mehreren großen Städten den Bürgermeister, de Gaulles Bruder Pierre wurde Vorsitzender des Gemeinderats von Paris. Die Forderung nach einer vorgezogenen Parlamentswahl, bei der sich das RPF gute Chancen ausrechnete, lehnten die Regierungsparteien (Sozialisten, Christdemokraten und Liberale) aber ab. Bei der Parlamentswahl 1952 schnitten die Gaullisten, auch aufgrund des zuvor zugunsten der Koalitionsparteien geänderten Wahlrechts, schwächer ab als erwartet. Nachdem seine Partei auch bei der Kommunalwahl im folgenden Jahr viele Bürgermeisterämter und Gemeinderatssitze wieder verloren hatte, gab de Gaulle am 6. Mai 1953 seine politischen Aktivitäten vorerst auf und zog sich nach Colombey-les-Deux-Églises zurück. Die folgende Phase im politischen Abseits bezeichnete er selbst später als traversée du désert („Gang durch die Wüste“). Er schrieb in dieser Zeit drei Bände seiner Memoiren.
Nach dem Scheitern der Vierten Republik in Französisch-Indochina kam es im Zuge des Algerienkrieges 1958 zu einer konstitutionellen Krise: Da sie den Verbleib Algeriens bei Frankreich bedroht sahen, begannen führende Militärs am 13. Mai den Militärputsch in Algier, in dem bald die Rückkehr de Gaulles an die Macht gefordert wurde. Dessen Umfeld stand im Kontakt zu den Putschisten, und am 19. Mai gab er selbst öffentlich bekannt, für ein politisches Amt zur Verfügung zu stehen.
Nachdem die Putschisten in der Opération Résurrection am 24. Mai die Insel Korsika besetzt hatten und damit das französische Mutterland bedrohten, willigten Präsident René Coty und das Parlament in de Gaulles Bedingungen ein: Am 1. Juni 1958 wurde er Ministerpräsident mit weitreichenden Notstandsbefugnissen für die Dauer von sechs Monaten, unter Suspension des Parlaments und mit dem Recht, den Entwurf zu einer neuen Verfassung auszuarbeiten.[32]
Im September nahm das Volk in einem Referendum die neue Verfassung mit dem von de Gaulle favorisierten Präsidialsystem mit 83 % an – die Fünfte Republik entstand. Alle Kolonien (Algerien wurde nicht als Kolonie, sondern als Bestandteil der Republik betrachtet) konnten wählen, ob sie an der Abstimmung teilnehmen oder ihre sofortige Unabhängigkeit wählen wollten – unter Fortfall aller weiteren französischen Unterstützung. Mit Ausnahme Guineas nahmen alle Kolonien an dem Referendum teil. Im November gewann de Gaulle die Parlamentswahlen und erhielt eine komfortable Mehrheit. Am 21. Dezember wurde er in indirekter Wahl mit 78 % der Stimmen zum Präsidenten der Französischen Republik gewählt.
De Gaulle übernahm das Amt des Staatspräsidenten am 8. Januar 1959. Er ergriff einschneidende Maßnahmen, um das Land zu revitalisieren, besonders die Einführung des neuen Franc, der 100 alten Francs entsprach. Er lehnte die Dominanz der USA und der Sowjetunion auf internationaler Ebene ab und behauptete mit dem Aufbau der Atomstreitmacht (erster Kernwaffentest am 13. Februar 1960) Frankreich als unabhängige Großmacht, die mit einer eigenen Nuklearschlagkraft ausgestattet wurde, die letztlich die Großbritanniens, der anderen westeuropäischen Atommacht, noch übertraf.[33]
Jedoch ging es de Gaulle nicht nur um Politik, sondern auch um das nationale Bewusstsein am Ende einer Zeit der Krisen (Weltkrieg, Kolonialkriege etc.). Um die Franzosen zu begeistern, auch den Unpolitischen die nationale Größe vorzuführen und die Identifikation mit den nationalen Herausforderungen und Zielen zu stärken, ließ er z. B. den Spitzensport reorganisieren, setzte mit dem berühmten Bergsteiger Maurice Herzog ein nationales Symbol für erfolgreiche sportliche Leistung als Sportminister ein, zentralisierte die Talentauswahl und Spitzensportförderung, ließ Spitzensportler wie Staatsamateure finanzieren und sorgte für die Übereinstimmung von gesellschaftlichem Anspruch und Spitzensportorganisation.[34]
Als Gründungsmitglied der Europäischen Wirtschaftsgemeinschaft (EWG) legte de Gaulle zweimal – am 14. Januar 1963 und am 19. Dezember 1967 – sein Veto gegen den Beitritt Großbritanniens ein.[35]
Im April 1962 ersetzte de Gaulle den Premierminister, Michel Debré, durch Georges Pompidou. Im September 1962 schlug de Gaulle vor, die Verfassung dahingehend zu ändern, den Präsidenten der Republik per Direktwahl durch die Bevölkerung zu bestimmen. Die Reform der Verfassung trat gegen den Widerstand des Parlaments in Kraft. Im Oktober stellte das Parlament einen Misstrauensantrag gegen die Regierung Pompidou, aber de Gaulle lehnte den vom Premierminister selbst angebotenen Rücktritt ab und entschied, das Parlament aufzulösen. Aus der Neuwahl im November 1962 ging die gaullistische Parlamentsmehrheit gestärkt hervor.[36] Die direkten Präsidentschaftswahlen fanden am 5. und 19. Dezember 1965 statt; in der Stichwahl de Gaulles gegen François Mitterrand erhielt Ersterer 55,2 % der abgegebenen Stimmen. Seine Gegner warfen ihm seinen Nationalismus und die abgeschwächte Wirtschaftskonjunktur in Frankreich vor.
De Gaulle hatte sich zunächst für die Einheit des Mutterlandes und seiner Überseegebiete ausgesprochen; so sah die maßgeblich von ihm geprägte Verfassung der Fünften Republik eine Unabhängigkeit der Kolonialgebiete nicht vor.[37] Unter dem Eindruck des Algerienkriegs ermöglichte jedoch im September 1959 eine Verfassungsänderung den Kolonien Unabhängigkeit unter fortbestehendem französischen Einfluss im Rahmen der Communauté française.[37] Am 18. März 1962 schloss de Gaulle die Verträge von Évian, die auch Algerien ein Recht auf eine Volksabstimmung über seine Unabhängigkeit zusicherten. Diese fand am 8. April 1962 statt. Die Politik der „nationalen Unabhängigkeit“ („l’indépendance nationale“) und der Lösung von „amerikanischer Bevormundung“ wurden von da an verstärkt.
Auf internationalem Parkett stärkte de Gaulle die Unabhängigkeit Frankreichs weiterhin: Er trat 1960 nachdrücklich für ein „Europa der Vaterländer“ (siehe auch Intergouvernementalismus, Souveränismus) unter der Führung Frankreichs ein, für das er neben den EWG-Staaten auch Länder wie Polen, die Tschechoslowakei, Ungarn, Rumänien, Bulgarien und Griechenland gewinnen wollte. Dafür nahm er 1962 den Rücktritt von Premierminister Michel Debré in Kauf.
In seiner Deutschlandpolitik setzte de Gaulle 1945 die Ruhrfrage, die 1948/1949 zur Einrichtung des Ruhrstatuts führte, auf die Tagesordnung der internationalen Politik. Nachdem seine Regierung zunächst das Ziel verfolgt hatte, das Saarland sowie das Rheinland und Westfalen einschließlich des Ruhrgebiets von Deutschland zu lösen, nahm er zusammen mit den anderen Westalliierten großen Einfluss auf die Bildung einer in den Westen zu integrierenden Bundesrepublik Deutschland. Am 9. September 1962 hielt er in Ludwigsburg auf Deutsch eine vielbeachtete Rede an die deutsche Jugend. Sie gilt als ein Meilenstein in den deutsch-französischen Beziehungen und als ein entscheidender Schritt auf dem Weg zum deutsch-französischen Freundschaftsvertrag (Élysée-Vertrag, Januar 1963).[38][39]
De Gaulle verurteilte die Militärhilfe der USA an Südvietnam und forderte die USA im Interesse eines dauerhaften Friedens zum Abzug ihrer Truppen auf. Er verurteilte 1967 den israelischen Schlag gegen die ägyptische Blockade der Meerenge von Tiran (Sechstagekrieg) und die dauerhafte Besetzung des Gazastreifens und des Westjordanlands durch Israel. Unter de Gaulle näherte sich Frankreich, einst engster Verbündeter Israels, der arabischen Welt, insbesondere Ägypten, aber auch Syrien und dem Libanon an, verhängte ein Waffenembargo gegen Israel, ließ die bereits bezahlten Mirage-Kampfflugzeuge nicht an Israel ausliefern und überließ es von da an den Amerikanern, Israel zu bewaffnen. Zur Haltung de Gaulles trugen auch die zunehmenden israelischen Operationen im bis dahin prowestlichen Libanon bei. De Gaulle hatte 1929–1931 (s. o.) im damals als Völkerbundsmandat französisch verwalteten Libanon gelebt und fühlte sich zahlreichen Persönlichkeiten der frankophonen Oberschicht des Landes verbunden, die ihn auch zum Teil im Kampf des Freien Frankreichs im Zweiten Weltkrieg unterstützt hatten.[14] Bis zur Präsidentschaft von Jacques Chirac (1995–2007) war die israelkritische, proarabische Orientierung französischer Außenpolitik eine gaullistische Konstante.
1958 lehnte de Gaulle die Unterstellung der französischen Mittelmeerflotte unter das NATO-Kommando ab. 1964 beendete er das amerikanische Projekt einer multilateralen Atomstreitmacht (MLF). Zwei Jahre später forderte de Gaulle Strukturänderungen der NATO und drohte mit dem Austritt Frankreichs. Nach Auslaufen eines Ultimatums, in dem er den Abzug von NATO-Truppen aus Frankreich bzw. ihre Unterstellung unter französisches Kommando gefordert hatte, zog sich Frankreich 1966 aus der integrierten militärischen Kommandostruktur der NATO zurück, blieb aber NATO-Mitglied. Gleichzeitig wurde das europäische NATO-Hauptquartier SHAPE von Rocquencourt nach Mons in Belgien verlegt.
Am 14. Dezember 1965 erklärte de Gaulle: „Selbstverständlich kann man auf den Stuhl wie ein Zicklein springen und rufen: ‚Europa, Europa, Europa!‘ Aber das führt zu gar nichts und bedeutet gar nichts.“ Dennoch war es Europa, das den Rahmen seiner Ambitionen festlegte, ein Europa, das selbst vom „Atlantik bis zum Ural“ reichte und den Eisernen Vorhang überwand. Tatsächlich war die Hauptstütze der französischen Außenpolitik die Annäherung an den anderen Schwerpunkt des Kontinents, die Bundesrepublik Deutschland, während man den „Angelsachsen“ den Rücken kehrte. Sein vertrauensvolles Verhältnis zum Bundeskanzler, Konrad Adenauer, und seine strategische Ausrichtung verhinderten eine Wiederholung der Politik Georges Clemenceaus, die das schwierige Verhältnis Frankreichs zu Deutschland nach dem Ersten Weltkrieg zusätzlich vergiftet hatte. Gemeinsam betrieben de Gaulle und Adenauer die deutsch-französische Aussöhnung, die mit einem deutsch-französischen Jugendwerk und zahlreichen Begegnungen gefördert wurde. Die Annäherung gipfelte im Élysée-Vertrag vom 22. Januar 1963.
Den Beitritt des Vereinigten Königreichs zur Europäischen Wirtschaftsgemeinschaft versuchte de Gaulle systematisch zu verhindern.[40] Neben der Befürchtung, Großbritanniens „special relationship“ zu den USA könne es zu einem „trojanischen Pferd“ der USA in Europa machen, sollen auch der mögliche Verlust der französischen Hegemonie in der europäischen Gemeinschaft und die Ablösung des Französischen als Arbeitssprache der europäischen Institutionen eine Rolle gespielt haben.[40] Noch beim Begräbnis Adenauers musste de Gaulle vom deutschen Bundespräsidenten zum Händedruck mit dem amerikanischen Präsidenten bewegt werden, nachdem beide sich zuvor demonstrativ aus dem Weg gegangen waren.[41]
De Gaulle war antikommunistisch eingestellt. Allerdings ging er seit seiner Rückkehr an die Macht im Jahr 1958 davon aus, dass keine Bedrohung durch eine russische Invasion bestehe. Er propagierte die Normalisierung der Beziehungen zu den von ihm als „vergänglich“ empfundenen östlichen Regimen. Die Anerkennung der Volksrepublik China (27. Januar 1964) ging in diese Richtung, wie auch seine Reise in die Sowjetunion im Juni 1966.
De Gaulle schuf mit der Communauté française ein Gegenstück zum britischen Commonwealth of Nations, wobei die Communauté Française die Außen-, Verteidigungs- und Währungspolitik bestimmte. Alle ehemaligen Kolonien führten Referenden durch, in denen die Gründung der Communité bestätigt wurde. Lediglich in Guinea entschied die Bevölkerungsmehrheit anders. Mitglieder wurden Dahomey, die Elfenbeinküste, Gabun, Kongo, Madagaskar, Mauretanien, Niger, Obervolta, der Tschad, Senegal, Mali, Togo und Kamerun. Dabei spielte auch die Communauté Financière d’Afrique des CFA-Franc eine Rolle, bei der die französische Zentralbank die Parität des CFA zum französischen Franc jahrzehntelang stabil hielt. Durch Kooperationsabkommen sicherte sich de Gaulle starke französische Einflussmöglichkeiten. Ein Teil der Communauté Française schloss sich zur Westafrikanischen Zollunion (UDAO) zusammen, die 1966 zur Zoll- und Wirtschaftsunion UDEAO ausgebaut wurde. Weitere Einflussmöglichkeiten schuf de Gaulle seinem Land auch mit der Gründung der staatlichen Vorläufergesellschaft von Elf Aquitaine, ERAP, die unter dem Einfluss ihres langjährigen Chefs, des ehemaligen französischen Verteidigungsministers und Gründers des Auslandsgeheimdiensts DGSS, Pierre Guillaumat, dem französischen Nachrichtendienst eine Tarnung sowie finanzielle Ressourcen für Frankreichs Aktivitäten in Afrika bot.
Hauptsächlich in der Außenpolitik kam das gaullistische Denken vom Wesen der Nation zum Ausdruck: „eine gewisse Idee Frankreichs“. De Gaulle schöpfte seine Stärke aus dem Wissen über die Geschichte Frankreichs. Nach ihm war das Gewicht dieser Geschichte derart, dass sie Frankreich eine besondere Position im Konzert der Nationen schenkte. Für ihn und zahlreiche Franzosen waren England und die USA nur Sprösslinge Frankreichs. Gleichfalls bewertete er die Institution der Vereinten Nationen als lächerlich und nannte sie „das Ding“ („le machin“), was ihn jedoch nicht daran hinderte, den ständigen Sitz Frankreichs im Weltsicherheitsrat einzunehmen und für Frankreichs politische Zwecke zu nutzen.
Im Jahr 1962 kommt es zu einem Attentat auf den Staatspräsidenten. Jean Bastien-Thiry, ein von de Gaulle persönlich zum Oberstleutnant beförderter Soldat der französischen Armee, war mit der französischen Algerienpolitik nicht länger einverstanden und beschloss daher, mit Unterstützung der Organisation de l’armée secrète (OAS) den Staatspräsidenten zu entführen oder, falls sich eine Entführung als unmöglich erwies, ihn zu töten. Der Angriff fand am 22. August 1962 auf einer Straßenkreuzung in Clamart bei Paris statt.[42] Der Anschlag scheiterte, da die elf Attentäter das für den Beginn der Aktion verabredete Zeichen in der Dunkelheit nicht sahen und das Feuer auf das Fahrzeug des Präsidenten zu spät eröffneten. Der Citroën DS wurde von mehreren Kugeln getroffen, doch das Präsidentenpaar wurde um wenige Zentimeter verfehlt. „Dies hätte ein schönes, sauberes Ende gemacht“, kommentierte de Gaulle später, als er sich das Einschussloch im Wagen ansah.
Die OAS setzte ihre Aktivitäten nach dem gescheiterten Attentat fort. Bis heute ist de Gaulles Algerienpolitik teilweise umstritten. Bastien-Thiry wurde gefasst, zum Tode verurteilt und am 11. März 1963 hingerichtet. Seine Komplizen kamen mit zum Teil geringeren Strafen davon. De Gaulle hatte eine Begnadigung Bastien-Thirys abgelehnt.
Das Attentat von Petit-Clamart diente Frederick Forsyth als Vorlage für seinen 1971 erschienenen Roman Der Schakal, der zwei Jahre später verfilmt wurde.
Bereits etwa ein Jahr vor dem Zwischenfall in Clamart, am 8. September 1961, war mit dem Attentat von Pont-sur-Seine ein Mordanschlag auf de Gaulle gescheitert. Die Attentäter hatten sich ebenfalls der OAS zugehörig erklärt.
Überzeugt von der strategischen Bedeutung der Atomwaffe, engagierte de Gaulle das Land unter Protest der Opposition für die kostspielige Entwicklung der Force de frappe, von Spöttern, die sie für ein zu klein geratenes „Bömbchen“ („bombinette“) hielten, mitunter als „farce de frappe“ bezeichnet. De Gaulle antwortete ihnen: „In zehn Jahren werden wir etwas haben, womit wir 80 Millionen Russen töten können. Ich glaube nicht, dass man ein Volk angreift, welches die Fähigkeit hat, 80 Millionen Russen zu töten, selbst wenn man 800 Millionen Franzosen töten könnte, vorausgesetzt, es gäbe 800 Millionen Franzosen.“ 1960/61 veranlasste er in der algerischen Wüste vier oberirdische Kernwaffentests; Tausende trugen auf Dauer Gesundheitsschäden davon.[43] Von 1966 bis zum Ende seiner Amtszeit im Jahr 1969 veranlasste er auf Atollen im Pazifik zehn weitere Atomtests, acht davon auf dem Mururoa- und drei auf dem Fangataufa-Atoll.
Für französische Unterstützung in der Berlin-Krise und der Kubakrise versprach der US-amerikanische Präsident, John F. Kennedy, Frankreich Hilfe in der Nuklearfrage, doch löste er sein Versprechen bis zu seiner Ermordung nicht ein. Die Nuklearfrage belastete die franko-amerikanischen Beziehungen während der ganzen 1960er Jahre. Erst mit Richard Nixon gab es ab 1969 erstmals einen amerikanischen Präsidenten, der frankreichfreundlich eingestellt war. Mit ihm teilte de Gaulle seine Geringschätzung für Ideologien, multilaterale Verträge und Institutionen. Nixon umschiffte zunächst den in der Nuklearfrage verpflichtenden Weg über die amerikanische Legislative, bevor er der nuklearen Zusammenarbeit mit Frankreich offiziell den Weg öffnete. Jedoch war das Gros der Arbeit französischerseits bereits geleistet: Am 24. August 1968 war es Frankreich ohne US-Hilfe gelungen, eine Wasserstoffbombe zur Detonation zu bringen (Operation Canopus).
Die Briten, deren Nuklearstreitmacht eng mit der der Amerikaner verknüpft war, fassten es als Ohrfeige auf, dass de Gaulle Frankreich zur dritten Atommacht des Westens machte. Deren Atomstreitkräfte verfügten über landgestützte Mittelstreckenraketen auf dem Plateau d’Albion (mittlerweile geschlossen), seegestützte Mittelstreckenraketen auf U-Booten und Atombomben, die von Flugzeugen abgeworfen werden konnten. Nicht zuletzt um auch auf diesem Gebiet von den beiden anderen Atommächten unabhängig zu sein, forcierte de Gaulle den Bau französischer Kampf- und Zivilflugzeuge (Dassault Mirage III bzw. Sud Aviation Caravelle) und unterzeichnete mit der Bundesrepublik Deutschland einen Vertrag zur Entwicklung des Airbus A300. Auch die europäische Trägerraketentechnik, deren ziviler Zweig die European Launcher Development Organisation (Europa-Rakete) war, wurde von de Gaulle in diesem Zusammenhang vorangetrieben.
Während François Mitterrand sich gegen das Atomprogramm sperrte, übertrug de Gaulle die Aufsicht des Projekts seinem Bruder, Jacques Mitterrand. Mitterrand vollzog in seiner Politik später einen Richtungswechsel: Während seiner Amtszeit als Präsident (ab 1981) führte er die Neutronenbombe ein.
Auf Anregung des französischen Ökonomen Jacques Rueff (1896–1978) war die Währungspolitik unter de Gaulle stark auf Gold ausgerichtet, um Frankreich von anderen nationalen Währungen unabhängig zu machen.[44] Im Februar 1965 kündigte er an, Währungsreserven in US-Dollar im Rahmen des Bretton-Woods-Systems in Gold umzutauschen. Bis zum Sommer 1966 erhöhte Frankreich so den Goldanteil seiner Reserven auf 86 Prozent.[45] Im Unterschied zu anderen Ländern, die im gleichen Zeitraum Dollar in Gold tauschten, darunter auch die Bundesrepublik Deutschland, beließ Frankreich das Gold nicht in den Tresoren der Federal Reserve, sondern bestand darauf, die Goldbarren nach Frankreich zu verschiffen, damit sie nicht „dem Zugriff einer fremden Macht preisgegeben“ seien.[45] Sein Ziel, zum Goldstandard zurückzukehren, erreichte de Gaulle indes nicht.[46]
1967 hielt sich de Gaulle in Kanada auf, um an der 100-Jahr-Feier des Landes und der Weltausstellung Expo 67 teilzunehmen. Dabei provozierte er einen diplomatischen Skandal, als er zum Abschluss einer emotionsgeladenen Rede vor 100.000 Menschen in Montréal, der größten Stadt der überwiegend französischsprachigen Provinz Québec, ausrief: „Vive le Québec libre!“ („Es lebe das freie Québec!“). Bei den Zuschauern auf den Straßen der Stadt lösten seine Worte großen Beifall aus.[47] „Ich werde euch ein Geheimnis verraten, das ihr niemandem weitererzählen sollt“, hatte er zuvor vor dem Bürgern Montréals formuliert, „unterwegs hierher habe ich eine Atmosphäre erlebt, die mich an die Befreiung [Frankreichs am Ende des Zweiten Weltkriegs] erinnerte.“ Lester Pearson, Kanadas Premierminister, nannte de Gaulles Äußerungen „inakzeptabel“; de Gaulle antwortete ihm, dass das Wort „inakzeptabel“ selbst inakzeptabel sei, sagte den vorgesehenen Besuch in der kanadischen Hauptstadt, Ottawa, ab und flog nach Frankreich zurück. De Gaulle erklärte, mit seiner Rede den Frankokanadiern zu helfen, „sich selbst zu befreien“, da „nach einem Jahrhundert der Unterdrückung, das für sie nach der englischen Eroberung folgte, ihnen nunmehr auch das zweite Jahrhundert […] in ihrem eigenen Land weder Freiheit noch Gleichheit noch Brüderlichkeit brachte“.[48] Die New York Times bewertete dies als „groben Akt gaullistischer Einmischung in die inneren Angelegenheiten Kanadas“ und als „Eskalation des Streites, der während des Besuchs General de Gaulles in Kanada begann“; laut einer Umfrage des französischen Magazins L’Express verurteilten 56 Prozent der befragten Pariser das Auftreten ihres Präsidenten.[48]
Die Unruhen im Mai 1968 in Frankreich waren eine weitere Herausforderung für de Gaulle. Am 24. Mai, zwei Wochen nach Beginn der Unruhen, nahm er erstmals in Rundfunk und Fernsehen Stellung zu den Forderungen der Demonstranten und versprach vage, ein Referendum über Reformen auf den Weg zu bringen.[49] Unterdessen forderten die Demonstranten de Gaulle zum Rücktritt auf.[49] Am 29. Mai reiste de Gaulle heimlich ins deutsche Baden-Baden; der Zweck dieser Reise bleibt unklar.[49] Ein als mögliche Erklärung oft genanntes Treffen mit General Jacques Massu hält der Historiker Norbert Frei für unwahrscheinlich, sondern geht davon aus, dass „die Staatskrise in diesem Moment in eine Nervenkrise übergegangen war“ und de Gaulle Abstand brauchte.[49]
Nach seiner Rückkehr nach Colombey-les-Deux-Églises kündigte de Gaulle am 30. Mai 1968 in einer Rundfunkrede Neuwahlen an: „Als Inhaber der Legitimität, die mir die Nation und die Republik verliehen haben, habe ich in den zurückliegenden 24 Stunden alle Eventualitäten erwogen, die es mir ermöglichen würden, diese Legitimität zu erhalten. Ich habe meine Entschlüsse gefasst. Unter den gegenwärtigen Umständen werde ich mich nicht zurückziehen. Ich werde nicht den Premierminister wechseln, der die Anerkennung von uns allen verdient. Ich löse heute die Nationalversammlung auf und beauftrage die Präfekten [...], die Subversion zu jeder Zeit und an jedem Ort zu verhindern. Was die Legislativwahlen angeht, so werden sie in den von der Verfassung vorgesehenen Fristen stattfinden, zumindest bis zu dem Zeitpunkt, an dem man hört, dass das ganze französische Volk mundtot gemacht wurde und man es davon abhält, seinem Willen Ausdruck zu verleihen, es davon abhält zu leben, durch dieselben Maßnahmen, durch die man versucht, die Studenten vom Studieren abzuhalten, die Lehrer vom Lehren, die Arbeiter vom Arbeiten. Die [von den Demonstranten ergriffenen] Mittel sind Einschüchterung, Vergiftung und Tyrannei, seit langem ausgeübt von organisierten Gruppen und einer Partei, die ein durch und durch totalitäres Projekt ist.“ (Hiermit meinte er die Kommunistische Partei Frankreichs.)
In dieser Rede erkannten seine Anhänger den de Gaulle der großen Tage wieder. Am 30. Mai 1968 hielten sie eine Demonstration ab, an der nach Angaben der Organisatoren eine Million, nach Angaben der Polizei 300.000 Menschen teilnahmen. Kurz darauf, im Juni 1968, erwiesen sich die Parlamentswahlen als großer Erfolg für die Gaullisten, die 358 von 487 Sitzen in der Nationalversammlung erhielten. Am 13. Juli 1968 wurde Georges Pompidou im Amt des Premierministers von Maurice Couve de Murville abgelöst.
Im Februar 1969 kündigte de Gaulle an, noch im Frühjahr desselben Jahres ein Referendum über die Reform der Regionalverwaltung und des Senats abzuhalten.[50] Wie 1962 sollte eine Verfassungsänderung ohne Beteiligung des Parlaments durchgesetzt werden.[50] Im April kündigte de Gaulle an, dass er im Falle einer Ablehnung zurücktreten werde.[51] Valéry Giscard d’Estaing mit seiner Partei, den Républicains indépendants, schloss sich mit den Sozialisten zusammen, die de Gaulles Vorhaben ablehnten.[51] Obwohl das eigentliche Ziel einer Regionalreform beim Stimmvolk des traditionell zentralistisch regierten französischen Staates populär war,[52] wurde das Referendum mit 52,46 % der Stimmen abgelehnt, und de Gaulle löste sein Versprechen ein und gab am 28. April 1969 kurz nach Mitternacht seinen Rücktritt vom Amt des Präsidenten der Republik bekannt.
Als Interimspräsident bis zur Neuwahl im Juni 1969 fungierte ordnungsgemäß der Präsident des Senats, Alain Poher. Am 20. Juni 1969 trat der Gaullist Georges Pompidou, der am 15. Juni die Stichwahl für das Präsidentenamt gegen den sozialen Christdemokraten Alain Poher gewonnen hatte, die Nachfolge Charles de Gaulles an.
Nach seinem Rücktritt hielt sich de Gaulle einen Monat in Irland auf, von wo aus er per Brief wählte, und zog sich dann nach Colombey-les-Deux-Églises zurück, wo er an seinem (unvollendeten) Buch Mémoires d’espoir arbeitete. Nach einer Reise nach Spanien im Juni 1970 starb Charles de Gaulle am 9. November 1970 in Colombey-les-Deux-Églises an der Ruptur eines Aortenaneurysmas.[53]
Sein Testament hatte er zur Zeit des Begräbnisses von General Jean de Lattre de Tassigny (1952) abgefasst. Dieser war nach seinem Tod vom offiziellen Frankreich in einer Art und Weise vereinnahmt worden, die de Gaulle verabscheute. Deshalb regelte de Gaulle die Modalitäten seines eigenen Begräbnisses in allen Einzelheiten und bestimmte:
Am 12. November 1970 wurde de Gaulle in Colombey an der Seite seiner Tochter, Anne, beigesetzt. Vom Familienanwesen La Boisserie in die Kirche des Ortes wurde der Sarg auf einem Panzerwagen des Typs Panhard EBR überführt. Der Zeremonie wohnten etwa 350 Compagnons de la Libération bei.[54]
Ebenfalls am 12. November 1970 fand zu Ehren des Verstorbenen in der Kathedrale Notre-Dame de Paris ein großes Requiem für ausländische Staatschefs statt. Anwesend waren US-Präsident Richard Nixon, der sowjetische Präsident Nikolai Podgorny, der britische Premierminister Edward Heath, Josip Broz Tito (Jugoslawien), Indira Gandhi (Indien), Fidel Castro (Kuba), Olof Palme (Schweden), Kaiser Haile Selassie (Äthiopien), Schah Mohammad Reza Pahlavi (Iran), König Bhumibol (Thailand), Königin Juliana (Niederlande), König Baudouin (Belgien), der britische Thronfolger, Prinz Charles, der Fürst von Monaco und der Großherzog von Luxemburg. Neben dem deutschen Bundespräsidenten, Gustav Heinemann, nahmen auch die früheren Bundeskanzler, Ludwig Erhard und Kurt Georg Kiesinger, teil.[55][56]
Zahlreiche öffentliche Straßen und Gebäude in Frankreich sind nach Charles de Gaulle benannt. Im Besonderen die Place Charles-de-Gaulle (Place de l'Étoile) in Paris und der Flughafen Paris-Roissy – Charles de Gaulle. Sein Name wurde auch dem gegenwärtig letzten französischen Flugzeugträger, der Charles de Gaulle, gegeben. Sein Wohnhaus in Colombey, die Boisserie, ist heute ein Museum, ebenso sein Geburtshaus in Lille.[57]
Churchill beschrieb de Gaulle als eine „Figur von echter Größe“.[58] In seinem Nachruf in der Wochenzeitung Die Zeit schrieb Theo Sommer, de Gaulle sei ein Mann des 17. oder 18. Jahrhunderts gewesen, der die Zukunft verfehlte, weil er „Vergangenheit restaurieren“ wollte.[59] Innenpolitisch sei er den Problemen des Landes allein mit „altfränkischer Mythologie“ nicht beigekommen, seine Außenpolitik habe sich als eine unstete Folge leerer Gesten entpuppt, sein exzentrischer Auftritt in Québec (Kanada) könne nur belächelt werden.[59] „Alles in allem hat Charles de Gaulle nicht viel Bleibendes bewirkt. Sein Anspruch war größer als seine Kraft, und es lag etwas Manisches in der Art, wie er diesen Anspruch verfocht. (…) Daß er voll verfehlter Ideen war, ist offenkundig. Niemand jedoch bestreitet, daß auch seine Fehler Format besaßen.“[59] Der linke Revolutionstheoretiker und Philosoph Régis Debray bezeichnete de Gaulle als „super-scharfsichtig“, da viele seiner Vorhersagen (vom Fall des Kommunismus bis zur Wiedervereinigung Deutschlands) sich nach seinem Tod bewahrheiteten.[60]
Der deutsche Historiker Ernst Weisenfeld sah eine große Wertbeständigkeit sämtlicher wichtiger Entscheidungen de Gaulles in seinen zehn Regierungsjahren. Von der Direktwahl des Staatspräsidenten über die Atomwaffen, den Austritt aus der NATO bis hin zur unabhängigen Außenpolitik seien die großen Entscheidungen de Gaulles auch nach seinem Abtritt von der politischen Bühne Bestandteil des Programms aller großen Parteien Frankreichs geworden.[61] Der Historiker Wilfried Loth unterstreicht die historischen Verdienste de Gaulles, die jedoch von seiner Selbststilisierung als „Retter der Nation“ und der Kritik daran überschattet worden seien: „Er hat dem vielfältigen Widerstand gegen die Integration Frankreichs in Hitlers Europa einen Kristallisationspunkt geboten und damit an führender Stelle dazu beigetragen, dass sich mit der Befreiung Frankreichs ein neuer demokratischer Konsens bilden konnte. Er hat die Handlungsfähigkeit des politischen Systems gestärkt und die Modernisierung der Wirtschaft entschieden vorangetrieben. […] Schließlich hat er wichtige Impulse zur Entwicklung eines unabhängigen Europas und zur Überwindung kommunistischer Parteiherrschaft in seinem östlichen Teil gegeben“.[62]
Nach Umfragen betrachten 70 Prozent der französischen Bevölkerung de Gaulle als die wichtigste Gestalt der gesamten französischen Geschichte. Als bleibende Leistungen de Gaulles werden vor allem der entschlossene Widerstand gegen das nationalsozialistische Deutschland und die Verfassung der Fünften Republik genannt.[63]
Charles de Gaulle hatte drei Brüder und eine Schwester:
Charles de Gaulle heiratete am 7. April 1921 Yvonne Vendroux (* 22. Mai 1900 in Calais; † 8. November 1979 in Paris). Der Ehe entstammen drei Kinder:
Der Front-National-Politiker Charles de Gaulle (* 25. September 1948 in Dijon) ist sein Enkel.
Als Präsident Frankreichs war Charles de Gaulle von Amts wegen auch Kofürst von Andorra.
Charles de Gaulle |
Georges Pompidou |
Valéry Giscard d’Estaing |
François Mitterrand |
Jacques Chirac |
Nicolas Sarkozy |
François Hollande |
Emmanuel Macron
Liste aller Präsidenten
Charles de Gaulle (Nationale Verteidigung) und Pierre Guillaumat (Armeen) |
Pierre Guillaumat |
Pierre Messmer |
Michel Debré |
Robert Galley |
Jacques Soufflet |
Yvon Bourges |
Joël Le Theule |
Robert Galley |
Charles Hernu |
Paul Quilès |
André Giraud |
Jean-Pierre Chevènement |
Pierre Joxe |
François Léotard |
Charles Millon |
Alain Richard |
Michèle Alliot-Marie |
Hervé Morin |
Alain Juppé |
Gérard Longuet |
Jean-Yves Le Drian |
Sylvie Goulard |
Florence Parly

Süleyman I., im Deutschen auch Suleiman (سليمان / Süleymān, genannt „der Prächtige“ und in der späteren osmanischen Geschichtsschreibung قانونی / Ḳānūnī /‚der Gesetzgeber‘;[1] geboren 6. November 1494, 27. April 1495 oder Mai 1496 in Trabzon; gestorben 7. September 1566 vor Szigetvár) regierte von 1520 bis 1566 als der zehnte Sultan des Osmanischen Reiches. Er gilt als einer der bedeutendsten Herrscher der Osmanen. Während seiner mehr als vierzigjährigen Herrschaft erreichten die geographische Ausdehnung und die Macht des Reiches ihren Höhepunkt.
Süleyman wurde als Sohn Selims I. und seiner Frau Hafsa Sultan in Trabzon geboren. Als Geburtsjahr werden in den Quellen die Jahre 1494 und 1495 genannt, als Geburtstag sowohl der 27. April als auch der 6. November. Bereits im Jahr 1509 wurde er zum Statthalter von Kaffa ernannt, vier Jahre später (1513) zu dem von Manisa (Magnesia).
Nach dem Tod seines Vaters am 21. September 1520 erbte Süleyman dessen Sultanswürde. Er war auf das Ableben seines Vaters sicher nicht vorbereitet, aber die Voraussetzungen für den Machtwechsel waren nicht schlecht: Seine drei Brüder Murad, Mahmud und Abdullah waren 1514 von ihrem Vater Selim getötet worden, womit Süleyman als einziger Erbe übrig blieb.[3] Allerdings ist auch behauptet worden, dass die Prinzen erst unter Süleymans Regierung getötet wurden.
Süleyman erlernte den Beruf des Goldschmieds. Nach der Tradition des Hauses Osman musste jeder Herrscher ein Handwerk erlernen. Er beherrschte mehrere Sprachen: zwei Turksprachen (Osmanisch und Tschagatai-Türkisch), Arabisch und Persisch.[4]
Zu Süleymans engsten Vertrauten gehörte insbesondere Ibrahim Pascha, ein sowohl in den bildenden Künsten als auch in der Diplomatie versierter, polyglotter Epirote (Grieche). Sein Lieblings-Hofdichter war Hayâlî (1500?–1557). Bereits im Jahr 1520 wurde Roxelane (Hürrem) Süleymans vierte Konkubine; später geriet er unter deren Einfluss.
Süleymans historischer Ruhm gründet sich vor allem darauf, dass er das Osmanische Reich nicht zuletzt durch Vergrößerung des Staatsgebiets auf den Höhepunkt seiner Macht geführt und es zu einem bedeutenden Akteur der europäischen wie nahöstlichen Politik gemacht hat. Während seiner Herrschaft führte er 13 große Feldzüge (zehn auf europäischem Gebiet, drei im vorderen Asien), hinzu kamen mehrere Seekriege im Mittelmeer.
Seinen ersten Feldzug unternahm er ein Jahr nach der Thronbesteigung gegen Ungarn. Als Vorwand diente ihm dazu die ungarische Verweigerung des bei einem Herrschaftswechsel sonst üblichen Tributs. Im Verlauf dieses Feldzugs eroberte er einige Städte und Festungen im Süden Ungarns, darunter Schabatz, Semlin und Belgrad.
1522 griff er die Insel Rhodos an, die nach sechsmonatiger Belagerung am 25. Dezember 1522 kapitulierte und in das Reich eingegliedert wurde. Die verteidigenden Ritter des Johanniterordens erhielten freien Abzug und siedelten sich 1530 auf Malta an (wo sie 1565 nochmals von Süleyman belagert wurden, diesmal allerdings erfolglos). Hierauf zog er im April 1526 mit 100.000 Mann und 300 Kanonen (siehe Topçu) erneut gegen Ungarn. Am 29. August errang er den Sieg in der Schlacht bei Mohács, worauf am 10. September Pest und Buda (Ofen) dem Sieger die Tore öffneten. Ungarn wurde zwischen dem Osmanischen und, zu einem kleineren Teil, dem Habsburgerreich aufgeteilt, was in der Folge zur Entwicklung der österreichisch-ungarischen Monarchie führte.
Nach Unterdrückung eines Aufstandes in Kleinasien unternahm er zugunsten von Johann Zápolya, des Bans von Siebenbürgen, den eine Partei zum König gewählt hatte, 1529 einen 3. Feldzug nach Ungarn, nahm am 8. September Ofen ein und drang am 27. September mit 120.000 Mann bis Wien vor. Diese Erste Wiener Türkenbelagerung gab er aber nach einem Verlust von 40.000 Mann am 14. Oktober auf. Nun wandte sich Süleyman gegen Persien. Im Osmanisch-Safawidischen Krieg von 1532 bis 1555 sandte er im Herbst 1533 ein Heer unter Großwesir Ibrahim nach Asien, wo die Festungen Erciş, Ahlat und Van fielen und er am 13. Juli 1534 die persische Hauptstadt Täbris einnahm. Auch Bagdad wurde noch am 4. Dezember desselben Jahres besetzt und von dort das eroberte Land organisiert.
Währenddessen hatte Süleymans Flotte unter Khair ad-Din Barbarossa den Spaniern 1533 Koroni genommen und 1534 Tunis unterworfen, welches aber 1535 durch Karls V. Tunisfeldzug (Dritter und Vierter Italienischer Krieg (1535–1544)) wieder verloren ging. Mit dem Sieg in der Seeschlacht von Preveza gegen die Heilige Liga im Jahr 1538 sicherte sich die osmanische Flotte die Dominanz im Mittelmeer bis 1571. 1541 unterwarf Süleyman mehr als die Hälfte Ungarns, und Zápolyas Sohn musste sich mit Siebenbürgen begnügen.
Im Jahre 1547 wurde ein fünfjähriger Waffenstillstand mit den Habsburgern bzw. dem Heiligen Römischen Reich geschlossen, nach welchem Süleyman ein jährlicher Tribut von 50.000 Dukaten gezahlt wurde. Hierauf unternahm er einen zweijährigen Krieg gegen Persien und erneuerte 1551 den Krieg in Ungarn, wo erst 1562 ein Friedensabkommen zustande kam.
Schon über 70 Jahre alt, brach Süleyman 1566 zu einem abermaligen Heereszug gegen Ungarn auf, starb aber während der Belagerung von Szigetvár am 5. September 1566.[5]  Auf den Thron folgte ihm sein Sohn Selim II.
Süleyman I. veranlasste folgende Tagebücher über seine Feldzüge:[6]
„Übersicht der Stationen des siegreichen Heeres […] Sr. Majestät Sulaiman – Gott der Erhabene stärke seine Helfer! - […] auf welchem es einmal nach dem anderen nach Ofen ging, um den verfluchten Ferdinand abzuhalten, sodann die Festung Wien belagerte, mit dem deutschen Kaiser und König in offenen Kampf zu treten anfing, einige Festungen der niedrigen Ungläubigen wegnahm und in den meisten Gegenden und Länderstrichen plünderte und raubte.“[7]
Bereits unmittelbar nach Antritt seiner Herrschaft gab Süleyman die durch seinen Vater eingezogenen Güter zurück und startete eine Kampagne zur Bestrafung und Disziplinierung der Staatsdiener. Außerdem betätigte er sich in erheblichem Maße als Gesetzgeber, was ihm auch seinen zweiten Beinamen Ḳānūnī eintrug. Nachdem die grundlegenden Fragen des Straf- wie auch des Verfassungsrechts bereits im Gesetzbuch Mehmeds II. geregelt worden waren, befasst sich der Süleymans Namen tragende Codex vor allem mit Finanz-, Steuer- und Bodenrecht, wobei erstmals auch bestehendes Gewohnheitsrecht kodifiziert wurde. Auch sollten durch ergänzende Bestimmungen Lücken in den Bestimmungen der Scharia ausgefüllt werden. Hierzu stützte er sich auf die Hilfe des Kazasker Mehmed Ebussuud Efendi, den er 1545 zum höchsten islamischen Gelehrten des Reiches, den sog. Scheichülislam erhob, sowie auf seinen Kanzler Celâlzâde Mustafa Çelebi, unter dessen Leitung eine leistungsfähige osmanische Verwaltung entstand.
In den letzten Lebensjahren Süleymans gab es erste Krisensymptome. Die ständigen Feldzüge brachten der Staatskasse ab 1541 nicht mehr genügend Erträge, so dass die Erhebung von Sondersteuern (immer öfter in Geld) zur Regel wurde. Auch kaufte der Staat die Lebensmittel für die Truppen deutlich unter Marktpreis ein und bewirkte so eine Lebensmittelverknappung. Zudem erhoben die Lehensinhaber, die Staatsdomänen, Großwürdenträger und frommen Stiftungen immer höhere Abgaben, zunehmend in Geld, so dass viele Bauern ihren Hof oder ihre Pacht verließen und sich zu Banden zusammenschlossen. Das Heer der Unzufriedenen wurde durch einkommenslose Koran-Studenten vergrößert. So war es möglich, dass 1555 ein falscher Prinz Mustafa mit 40.000 Aufständischen durch Rumelien ziehen konnte.
Einen Namen machte sich Süleyman I. ferner als Bauherr. Insbesondere ließ er von 1549 bis 1557 die nach ihm benannte Süleymaniye-Moschee errichten, eine der kunsthistorisch bedeutendsten Moscheen Konstantinopels. Auch in Rhodos ließ er gleich nach der Eroberung der Stadt die nach ihm benannte Süleyman-Pascha-Moschee errichten (1523). Des Weiteren entstanden in seiner Herrschaftsperiode u. a. die Prinzenmoschee (1548), die Mihrimah-Moschee (1566) und die Rüstem-Pascha-Moschee (1561). Verantwortlich zeichnete jeweils Süleymans Hofarchitekt Mimar Sinan. Daneben setzte der Sultan ein großangelegtes Kanalbauprojekt ins Werk, das die Wasserversorgung der Hauptstadt gewährleisten sollte.
Süleyman I. starb 1566 nach 46-jähriger Regierungszeit, der längsten in der osmanischen Geschichte. Mit ihm ging die Blütezeit der osmanischen Herrschaft zu Ende. Er gilt als der bedeutendste Sultan der Osmanen; in der osmanischen Überlieferung einerseits als Feldherr und Krieger, andererseits aber auch als weiser Gesetzgeber und Staatsmann. In Konstantinopel ließ er zahlreiche prächtige Bauwerke errichten. Außerdem verfasste Süleyman unter dem Pseudonym Muhibbi („geliebter Freund“) selbst Gedichte auf Persisch und in osmanischem Türkisch.
Heutzutage bezeichnet die Geschichtsschreibung über das Osmanische Reich ihn mit der Ordinalzahl I. Insbesondere in der europäischen Literatur findet man aber auch einen Sohn Bayezids I. mit dieser Nummer, der von den europäischen Vasallen des Reichs in der Zeit des Osmanischen Interregnums als Sultan anerkannt wurde.
 Im United States Capitol im Repräsentantenhaus der Vereinigten Staaten wurde Süleyman I. unter 23 Personen als einer der größten Gesetzgeber aller Zeiten mit einem Relief geehrt.
Des Weiteren gibt es verschiedene Statuen und Skulpturen in Ungarn zum Zeichen der türkisch-ungarischen Freundschaft. Dazu gehören unter anderem eine Statue des Süleyman I. im Mohácsi Történelmi Emlékpark (Mohács), eine Statue von Miklós Zrinyi und Süleyman I. im Park der türkisch-ungarischen Freundschaft (Szigetvár) und ein symbolisches Grab und eine Statue mit der Signatur Süleymans, ebenfalls im Park der türkisch-ungarischen Freundschaft.
In der Türkei führte die Verfilmung des Lebens Süleymans unter dem Titel Muhteşem Yüzyıl („Das prächtige Jahrhundert“) zu Demonstrationen und kontroversen Diskussionen. Der türkische Kulturminister Ertuğrul Günay verteidigte die Verfilmung gegen Kritik aus religiösen und konservativen Kreisen.[8]

Für den Begriff freier Wille oder Willensfreiheit gibt es keine allgemein anerkannte Definition. Umgangssprachlich versteht man etwas anderes unter dem freien Willen als im juristischen oder psychologischen Sprachgebrauch. In der Philosophie wird der Begriff nicht einheitlich definiert.
In einem fachübergreifenden Sinne gehört zur Willensfreiheit die subjektiv empfundene menschliche Fähigkeit, bei verschiedenen Wahlmöglichkeiten eine bewusste Entscheidung zu treffen.
Die Begriffe von der „Subjekt“-Stellung des Menschen und von dessen „Autonomie“ oder auch dessen „Moralität“ beruhen auf der Annahme von Entscheidungsfreiheit. Auch die politischen Ideen der „Freiheit“ und der „Demokratie“ setzen diese Annahme voraus.
Doch bereits im griechischen Altertum, aber besonders seit Beginn der Aufklärung, sah sich die Vorstellung eines freien Willens zahlreichen Anzweiflungen ausgesetzt (siehe auch: Geschichte des Freien Willens).
Nach dem Konzept der bedingten Willensfreiheit ist ein Wille frei, wenn eine Person ihren Willen nach ihren persönlichen Motiven und Neigungen bildet und dann das tun kann, was sie will (Handlungsfreiheit). Welcher der konkurrierenden Wünsche eines Menschen sich als Wille herausbildet, hängt nach dieser Vorstellung von seiner Persönlichkeit und von Umwelteinflüssen ab.
Aufgrund der Komplexität der Umstände, die zur Willensbildung führen, sind die Ursachen einer Entscheidung nur teilweise einsehbar. Dennoch wird hier von Freiheit gesprochen, weil die getroffene Wahl den Neigungen und Motiven der Person entspricht und somit deren eigenen Willen darstellt und nicht einen aufgezwungenen.
Es bestehen allerdings Zweifel, ob der Ausdruck Freiheit hier angebracht sei, da die kausalen Ursachen einer Entscheidung für den Entscheider selbst nur zu einem Teil erkennbar seien. Schopenhauers Ausspruch, der Mensch könne tun, was er will, aber er könne nicht wollen, was er will, fasst diese Auffassung pointiert zusammen.
Als eine besondere Form der ‚bedingten‘ Willensfreiheit kann die ‚asymptotische‘ (angenäherte) Willensfreiheit angesehen werden: Da es einerseits einen absolut von allen Zwängen freien Willen nicht geben kann und andererseits doch das bewusste menschliche Denken bzw. das Wissen des Einzelnen, (d. h. die menschliche Persönlichkeit insgesamt) die jeweiligen individuellen Handlungen und Entscheidungen wesentlich beeinflussen, schlägt Philip Clayton das Konzept der asymptotischen Willensfreiheit vor. Diese ist nicht einfach gegeben, sondern sie entwickelt und vervollkommnet sich in dem Maße, wie sich das jeweilige Individuum menschlich vervollkommnet, ohne die absolute Willensfreiheit je zu erreichen.[1][2]
Nach dem Konzept der unbedingten Willensfreiheit besteht keine Beschränkung der Freiheit. Gedacht werden könnte eine solche unbedingte Freiheit jedoch nur dann, wenn ein Wille durch nichts bedingt wäre. Das Problem bei dieser Freiheit ist, dass der Wille, wenn er durch nichts bedingt ist, als zufällig und unmotiviert gelten muss. Es unterläge dann also dem reinen Zufall, was sich zum Willen herausbildet. Er stünde nicht mehr im Einklang mit der Natur und den Neigungen der handelnden Person, wäre von ihr losgelöst und ihr auch nicht mehr zurechenbar.
Dem Konzept des Determinismus liegt die Annahme zugrunde, dass alle Ereignisse, die geschehen, sowohl kausale Folge vorangegangener Ereignisse seien als auch von diesen eindeutig bestimmt würden.
Die Position, dass der Determinismus mit dem freien Willen verträglich sei, bezeichnet man als Kompatibilismus. Kompatibilisten wie Thomas Hobbes definieren Willensfreiheit so, dass eine Person dann frei handelt, wenn sie eine Handlung will und auch anders handeln könnte, wenn sie anders handeln wollte. Innerhalb kompatibilistischer Positionen gibt es unterschiedliche Auffassungen darüber, ob Determinismus und Willensfreiheit lediglich miteinander verträglich seien oder ob der Determinismus sogar eine Voraussetzung für Willensfreiheit darstelle. Erstere bezeichnet man als weichen, letztere als harten Kompatibilismus.
Die Auffassung, dass es keine Rolle spielt, ob die Entscheidungen deterministisch bedingt sind, bezeichnet man als weichen Kompatibilismus. Nach dieser Auffassung ist der Wille frei, da die handelnde Person die determinierenden Faktoren nicht vollständig kenne. Für Vertreter dieser Position bedeutet die Freiheit des Willens letztlich also, nach Gründen zu handeln, die dem Handelnden nicht vollständig bewusst sind. Die erlebte Freiheit bei der Entscheidung ist also nur eine scheinbare Freiheit.
Die Auffassung, dass Willensfreiheit nur dann möglich ist, wenn eine Entscheidung durch in der Vergangenheit liegende Ereignisse bedingt ist, bezeichnet man als harten Kompatibilismus. Frei sei ein Wille demnach dann, wenn er durch Gründe motiviert ist, die im Einklang mit den Werten und Überzeugungen der wollenden und handelnden Person stehen. Die erlebte Freiheit bei der Entscheidung sei die tatsächliche Freiheit. Ein unfreier Wille wäre nach diesem Verständnis ein handlungsleitender Wille, der als auferlegt erlebt würde, etwa aufgrund von Zwangsgedanken oder Beeinflussung durch andere Personen.
Moderne Vertreter des Kompatibilismus sind u. a. Harry Frankfurt, Daniel C. Dennett, Michael Pauen und Peter Bieri.
Einige Philosophen sehen die Konzepte von Willensfreiheit und Determinismus als unvereinbar an. Wenn der Wille wie alles andere in der Welt bedingt sei, so könne er und alle von ihm ausgehenden Entscheidungen und Handlungen nicht frei sein. Diese philosophische Auffassung bezeichnet man als Inkompatibilismus.
Inkompatibilisten gehen davon aus, dass eine Person genau dann einen freien Willen besitze, wenn sie der einzige verursachende Grund (Erstauslöser) für die Handlung sei und sie in einer Entscheidungssituation verschiedene Entscheidungen treffen könne. Es gibt dann neben der Verursachung durch Ereignisse (Ereigniskausalität) oder durch Zufall noch eine dritte, die sog. Akteurskausalität (Substanzkausalität, Agenskausalität). Handlungen werden dann auf Wünsche und Überzeugungen des Handelnden zurückgeführt, die er unterschiedlich gewichtet und die damit die Gründe für seine Handlungen liefern.[3] Diese Definition entspricht der unbedingten Willensfreiheit. Determinismus wird von Inkompatibilisten nicht zwangsläufig als unzutreffend abgelehnt, doch wenn er zuträfe, wäre jede Wahl, die wir treffen, bereits durch frühere Ereignisse bedingt.
Inkompatibilisten lassen sich also in zwei entgegengesetzte Positionen einteilen:
Als Indeterminismus bezeichnet man die dem Determinismus gegensätzliche Auffassung, dass es (zumindest einige) Ereignisse gibt, die nicht durch vorangegangene Ereignisse bedingt sind.
Libertarianer sind der Meinung, dass undeterminierte Handlungen nicht rein zufällig sind, sondern einem substantiellen Willen entspringen, dessen Entscheidungen undeterminiert sind. Dieser Ansatz wird weithin als nicht zufriedenstellend angesehen, da er das Problem nur einen Schritt weiter zurück verlagert (zu dem substantiellen Willen) und nicht erklären kann, was dieser substantielle Wille ist und welchen Gesetzen er im Unterschied zu herkömmlichen Konzepten des Geistes unterworfen ist.
Schon vor dem neueren probabilistischen Verständnis der Lebensvorgänge sah bereits Arthur Schopenhauer ein Argument gegen die Willensfreiheit darin, dass sie eine Verletzung des Kausalitätsprinzips bedeutet, einer Grundfeste des menschlichen Denkens. Der freie Wille sei eine Illusion, in Wahrheit sei der Wille durch chaotische (also äußerst komplexe) Einflüsse außerhalb und innerhalb des Subjekts gesteuert.
In einer 2009 durchgeführten Erhebung wurde der Verbreitungsgrad der dargelegten Positionen unter Philosophen ermittelt. In der Erhebung wurden Mitglieder von insgesamt 99 philosophischen Fakultäten befragt, die vom Philosophical Gourmet Report als hochrangig eingestuft wurden, davon 90 in englischsprachigen Ländern. Von den 931 teilnehmenden Philosophen wurden zur Frage "Freier Wille" die Wahlmöglichkeiten (Optionen) "Ich akzeptiere" oder "Ich neige zu" wie folgt auf vier vorgegebene Kategorien verteilt: "Kompatibilismus" 59,1 %, "Libertarismus" 13,7 %, "Kein freier Wille" 12,2 %, "Anderes" 14,9 %.[4]
Im Verlauf der Geschichte der Naturwissenschaften wurden zahlreiche Versuche unternommen, die vorherrschenden Auffassungen von Willensfreiheit anhand empirisch-naturwissenschaftlicher Modelle und Befunde zu untermauern oder zu hinterfragen. Je nach der zu Grunde gelegten Auffassung von Willensfreiheit können aus den Beiträgen der empirischen Wissenschaften dabei unterschiedliche Schlüsse gezogen werden (siehe dazu Kompatibilismus und Inkompatibilismus).
Das Weltbild der klassischen Mechanik sieht die Welt als deterministisch an. Es enthält die Auffassung, dass bei genügend genauer Information die Zukunft beliebig genau vorhergesagt werden kann. Dagegen ist es in der Quantenmechanik nicht mehr möglich, den Ablauf eines Vorgangs hinsichtlich aller messbarer Größen vorherzusagen, selbst wenn alle prinzipiell zugänglichen Informationen über seinen Anfangszustand bekannt sind (siehe das Gedankenmodell Schrödingers Katze). Nach gängiger (aber nicht unumstrittener) Interpretation ist damit das Naturgeschehen nicht vollständig determiniert, sondern unterliegt in einem fundamentalen Sinne partiell dem Zufall. Der Mathematiker John H. Conway und der Physiker Simon Kochen haben versucht, über ihr Free Will Theorem einen Zusammenhang zwischen menschlicher Entscheidungsfreiheit und quantenmechanischer Unschärfe (Indeterminiertheit) herzustellen.[5]
Durch den Einsatz moderner bildgebender Methoden, vor allem PET und fMRT, ist es möglich geworden, neuronale Vorgänge grob zu beobachten, die dem Prozess der Entscheidungsbildung zugeordnet werden können. Dabei deuten die bisherigen Ergebnisse darauf hin, dass manche Entscheidungen im Gehirn bereits getroffen werden, bevor die Person sich ihrer bewusst wird. Allerdings handelt es sich bei den Entscheidungen in diesen Experimenten bis dato um „folgenlose“ Spontanentscheidungen, z. B. welche Hand zum Greifen benutzt wird. Kritiker wenden deshalb ein, dass eine empfundene Freiheit der Entscheidung insofern doch real sein könnte, als die empfindende Person die Ausführung der Handlung steuern und überwachen könnte und in diesem Prozess die Möglichkeit hätte, die Aktion noch zu unterbrechen oder zu modifizieren. Dies wurde in dem Vorschlag ausgedrückt, die bewusst empfindende Person hätte möglicherweise eine Art „Vetorecht“. Spätere Experimente deuteten jedoch darauf hin, dass auch Veto-Entscheidungen unbewusst getroffen werden und erst nachträglich als freie Entscheidungen empfunden werden.[6]
Nach dem gegenwärtigen Erklärungsmodell der Hirnforschung über die Steuerung der Willkürmotorik haben viele, vor allem grundlegende, Antriebe für das Verhalten des Menschen einen subkortikalen Ursprung – sie entstehen im limbischen Bewertungs- und Gedächtnissystem. Dieses aktiviert die Basalganglien und das Kleinhirn, die wiederum die kortikalen Prozesse in Gang setzen. Dann erst setzt die Empfindung ein, etwas zu wollen. Damit stimmt überein, dass bei Willkürhandlungen zuerst in den Basalganglien und im Kleinhirn neuronale Aktivität auftritt und erst danach in der Großhirnrinde.
Der Psychologe Daniel Wegner führte eine Reihe von Experimenten durch, in denen Menschen eine Illusion der Kontrolle erleben und das Gefühl haben, dass ihr Wille Ereignisse prägt, die tatsächlich von jemand anderem bestimmt werden.[7] Er argumentierte, dass die Leichtigkeit, mit der diese Illusion erzeugt werden könne, zeige, dass das alltägliche Gefühl des freien Willens eine Illusion sei[8][9] und dass in Wirklichkeit sowohl Verhalten als auch der Wille das Produkt anderer, unbewusster mentaler Prozesse seien.[10] Wegner definierte den freien Willen als eine Funktion der Priorität (der Gedanke muss vor der Handlung kommen), der Konsistenz (der Gedanke muss mit der Handlung übereinstimmen) und der Exklusivität (der Gedanke kann nicht mit anderen Ursachen einhergehen).[11] Wegner bestritt jedoch nicht, dass bewusstes Denken Handeln hervorrufen kann, sondern er betonte, dass jeder Zusammenhang zwischen bewusstem Denken und Handeln durch wissenschaftliche Untersuchungen und nicht durch unzuverlässige Selbstbeobachtung und Gefühle bestimmt werden sollte.
Seit den 1950er Jahren wird in der Psychologie nicht nur die Frage nach Willensfreiheit selbst, sondern auch die Funktion des Glaubens an eine solche empirisch untersucht.[12] In diesen Studien wird der Zusammenhang zwischen dem Glauben an Willensfreiheit und anderen Einstellungen und Verhaltensweisen untersucht. So wurden beispielsweise Zusammenhänge zwischen der Überzeugung, dass Menschen einen freien Willen haben können, und verschiedenen kognitiven, sozialpsychologischen und motivationalen Konstrukten berichtet.[13] Diese Herangehensweise, die häufig dem Ansatz der experimentellen Philosophie folgt, hat den Vorteil, dass die metaphysische Frage nach der Existenz eines freien Willens nicht geklärt sein muss, um klare Aussagen über die Funktion des Glaubens an einen freien Willen treffen zu können.[12] Einige der Befunde zum Glauben an einen freien Willen konnten in nachfolgenden Studien nicht repliziert werden, weshalb dieses Forschungsthema im Zusammenhang mit der Replikationskrise diskutiert wird.[14]
Ein viel diskutiertes Experiment (Libet-Experiment) auf diesem Gebiet wurde 1979 von Benjamin Libet durchgeführt. Die Probanden wurden gebeten, in einem beliebigen Moment ihren Finger zu heben, während sie eine Art Uhrzeiger verfolgten. Gleichzeitig wurde eine bestimmte, mit der Fingerbewegung zeitlich gekoppelte, Gehirnaktivität aufgezeichnet. Nach Libets Deutung zeigte das Experiment, dass die Gehirnaktivität, die dazu führte, dass eine Person ihren Finger bewegte, etwa 550 ms vor dem Moment einsetzte, in dem diese Person der Auffassung war, sich bewusst dafür zu entscheiden. Diese vorausgehende und unbewusst bleibende Gehirnaktivität wurde schon 1964 von William Grey Walter und 1965 von Hans Helmut Kornhuber und Lüder Deecke beschrieben,[15][16] und das messbare Korrelat wird unter anderem als Bereitschaftspotential oder auch Vorbereitungspotential[17] bezeichnet. Libet selbst schlussfolgerte daraus, dass die Annahme, der Mensch verfüge über keinen freien Willen, falsch sein müsse: Innerhalb des nachgewiesenen Zeitfensters zwischen Bereitschaftspotential und bewusst empfundener Handlungsentscheidung sei ein „Veto“ möglich. In einer Studie von 2016 wurde Libets Veto mittels eines Brain-Computer-Interface genauer untersucht. Hier zeigte sich, dass beabsichtigte, willkürliche motorische Handlungen bis etwa 200 ms vor der eigentlichen Durchführung unterbunden werden können, und selbst noch nach dem Einsetzen von Muskelaktivität verändert oder abgebrochen werden können.[18]
Die experimentellen Forschungsergebnisse aus der Psychologie von Daniel Wegner stimmten mit den physiologischen Erkenntnissen von Benjamin Libet überein.[11] Wegner und Wheatley konnten im richtungsweisenden "I-Spy"-Experiment zeigen, dass Handlungen immer dann als durch eigene Gedanken verursacht erlebt werden, wenn der Gedanke unmittelbar vor der Handlung erlebt wird, dieser konsistent mit der Handlung ist und es keine andere plausible Ursache für die Handlung gibt.[11]
Ein Nachfolgeexperiment von Haggard und Eimer aus dem Jahr 1999 erweiterte den ursprünglichen Ansatz, indem die Probanden hier nicht nur entscheiden konnten, wann sie ihre Hand bewegten, sondern zusätzlich auch, welche Hand. Damit begegneten die Forscher einem häufig vorgebrachten Einwand gegen das Libet-Experiment, wonach die Probanden keine wirkliche Entscheidung im Sinne einer Wahl unter verschiedenen Optionen treffen konnten und die Resultate deshalb nicht für die menschliche Praxis typisch seien. Die Ergebnisse von Haggard und Eimer bestätigten Libets Daten, wonach das Bereitschaftspotential der bewusst empfundenen Entscheidung vorausgeht.[19]
Bezüglich der von Libet vorgeschlagenen Möglichkeit eines "Vetos" innerhalb eines bestimmten Zeitfensters (s. o.) deuten Experimente von 2009 zur Bewusstheit willentlicher Entscheidungen von Kühn und Brass darauf hin, dass auch Veto-Entscheidungen unbewusst getroffen werden und erst nachträglich als freie Entscheidungen empfunden werden.[6]
In der Nachfolge der Libet-Experimente führte eine Gruppe um Alvaro Pascual-Leone 1992 ein Experiment durch, bei dem die Probanden gebeten wurden, zufällig die rechte oder die linke Hand zu bewegen. Er fand heraus, dass durch die Stimulation der verschiedenen Hirnhälften mittels magnetischer Felder die Wahl der Person stark beeinflusst werden konnte. Normalerweise wählen Rechtshänder die rechte Hand in ca. 60 % aller Fälle. Wurde jedoch die rechte Hirnhälfte stimuliert, wurde die linke Hand in 80 % aller Fälle ausgewählt. (Die rechte Hemisphäre des Hirns ist im Wesentlichen für die linke Körperhälfte zuständig und umgekehrt). Trotz dieses nachweislichen Einflusses von außen berichteten die Probanden weiterhin, dass sie der Überzeugung waren, die Wahl frei getroffen zu haben.[20]
2013 wurde von einer Forschergruppe um John-Dylan Haynes am Berlin Center for Advanced Neuroimaging (BCAN) nachgewiesen, dass nicht nur Entscheidungen für eine Handbewegung, sondern auch Entscheidungen bei der Auswahl einer abstrakten Denkaufgabe (Rechenaufgabe) spezifische, zeitlich vorausgehende Gehirnaktivität aufweisen. Statistische Analysen abgebildeter Gehirnaktivitäten zeigten, dass mit überzufälliger Häufigkeit bestimmte Aktivitätsmuster ca. vier Sekunden vor dem Moment auftraten, in dem die Versuchspersonen selbst sich über ihre Entscheidung bewusst sein konnten. Die Eigenschaften der Aktivitätsmuster waren jeweils typisch für die Art der nachfolgenden Entscheidung.[21] Nach Erscheinen der Studie wies Haynes im Deutschlandfunk darauf hin, dass die Ergebnisse zeigten, "wie stark unsere Entscheidungen von unbewussten Hintergrundprozessen beeinflusst werden. Das ist das eigentlich Interessante, dass wir das Gefühl haben, ich entscheide mich jetzt, aber dass irgendetwas im Gehirn schon unbewusst passiert ist, davor."
Der genaue Zusammenhang zwischen den unbewussten Hirnprozessen und der Sekunden später bewusst getroffenen Entscheidung sei jedoch noch unklar. Was jetzt gebraucht werde, seien „20 Jahre Forschung zum Thema Gehirnmechanismen des freien Willens“.[22]
Neueste (Stand 2015) experimentelle Forschungsergebnisse u. a. von Haynes weisen aber darauf hin, dass solche Gehirnaktivitäten – nachdem sie unwillkürlich gestartet wurden – willentlich gestoppt werden können: "Die Probanden sind den frühen Hirnwellen nicht unkontrollierbar unterworfen. Sie waren dazu in der Lage, aktiv in den Ablauf der Entscheidung einzugreifen und eine Bewegung abzubrechen", so Haynes. "Dies bedeutet, dass die Freiheit menschlicher Willensentscheidungen wesentlich weniger eingeschränkt ist, als bisher gedacht. Dennoch gibt es einen Punkt im zeitlichen Ablauf von Entscheidungsprozessen, ab dem eine Umkehr nicht mehr möglich ist, den ‚point of no return’."[18][23]
Eine spezielle Erscheinungsform des freien Willens ist die Selbstkontrolle einer handelnden Person. Sie hat große Bedeutung in allen sozialen Bereichen. Erforscht wird sie in der empirischen Grundlagenforschung der Psychologie.
So konnten zum Beispiel mehrere – in neuerer Zeit auch sehr umfangreiche – Erhebungen zeigen, dass das Ausmaß der Selbstkontrolle während der Kindheit einen starken Einfluss hat auf spätere Erfolge im Leben, so in den Bereichen Gesundheit, materieller Wohlstand und Zufriedenheit, und zwar unabhängig von Intelligenz und sozialem Status. Gleichzeitig führte ein höheres Ausmaß an Selbstkontrolle während der Kindheit im späteren Leben zu geringeren sozialen Kosten durch medizinische Behandlung, Sozialleistungen und Strafverfolgung.[24][25][26]
Das Empfinden eines freien Willens wird in all diesen Studien als selbstverständlich vorausgesetzt, und es wird für die Untersuchungsergebnisse als unerheblich angesehen, ob die erfassten Personen Ansichten darüber haben, ob die Freiheit ihres Willens real oder vorgestellt sei.
Nach VDI-Richtlinie VDI/VDE: 2653[27]  ist „Ein technischer Agent () eine abgrenzbare (Hardware- oder/und Software-) Einheit mit definierten Zielen. Ein technischer Agent ist bestrebt, diese Ziele durch selbständiges Verhalten zu erreichen und interagiert dabei mit seiner Umgebung und anderen Agenten.“ Hardware-Agenten (z. B. Roboterfußball, autonomes Fahren, Militärroboter) und Software-Agenten können also eigenständige Entscheidungen treffen und eigenständig aktiv werden (handeln), unbeeinflusst von menschlichen Eingriffen. Sie können situationsangemessen agieren, um vorgegebene Aufgaben (allein oder im Team) zu erledigen.
Man unterscheidet (bei Software-Agenten) unterschiedliche Agententypen.
Bei autonomen Systemen unterscheidet man zudem zwischen reaktiven Aktionen (Reaktion auf die Umgebung) und proaktiven Aktionen (selbständiges, zielorientiertes Verhalten mit Eigeninitiative).[28]
Als Kriterium für eine freie Entscheidung gelten in der Philosophie
Ersetzt man die Begriffe Person durch Agent bzw. Akteur, Wunsch durch desire und Wille durch intention, commitment oder Ziel, werden diese Anforderungen offensichtlich auch von bestimmten Agenten erfüllt, obwohl hier die (übergeordneten) Ziele vorgegeben sind. Fraglich ist aber, ob diese Wünsche und Motive bei Personen nicht ebenfalls genetisch (oder ontogenetisch) vorgegeben und somit ebenfalls nur bedingt frei sind (Maslowsche Bedürfnishierarchie).[33] Beckermann hält es nicht für besonders sinnvoll zu sagen, die Natur manipuliere uns dadurch oder mache uns dadurch unfrei, dass sie uns diese Wünsche mit auf den Weg gibt. Unsere Freiheit beruht vielmehr darauf, dass sich in uns Menschen im Laufe der Zeit die Fähigkeit entwickelt hat, uns unserer Wünsche bewusst zu werden und über sie nachzudenken.[3] Daraus resultieren persönliche Entscheidungen. Ein Wille wird dann als Anstreben von selbst festgelegten Zielen und deren Umsetzung in die Tat gesehen.
Daraus leitet sich die Frage ab, ob die notwendigen Bedingungen für Willens-, Entscheidungs- und Handlungsfreiheit auch dann erfüllt sind, wenn autonome Agenten durch streng determiniertes, algorithmusbasiertes Planen und Schlussfolgern[29] in einer modellierten Welt geeignete Aktionen finden, um ein Ziel auszuwählen und ihm näherzukommen[30] und zwar auch ohne begleitendes Bewusstsein? Oder ist die Fähigkeit zur (Selbst-)Lokalisierung bereits eine Vorstufe von Selbstbewusstsein? Kann man zudem Ergebnisse aus maschinellem Planen und Schlussfolgern mit dem Ergebnis menschlicher Überlegungen[34] gleichsetzen? Und setzt Wollen bewusste Wünsche und Vorstellungen voraus und bedarf es überhaupt eines Wollens und eines Bewusstseins im menschlichen Sinne, wenn zwischen Alternativen entschieden wird zur Erreichung von Zielen?[35]
Bei autonomen Systemen (insbesondere beim autonomen Fahren) stellt sich zudem die Frage, wodurch das Verhalten beeinflusst (begründet) wird und wie vertrauenswürdig es ist[36] sowie die Frage nach der Verantwortung und Schuldfähigkeit technischer Systeme.[37]
Im biologischen Sinne wird der Wille eines Menschen auch durch Erbanlagen und Umwelteinflüsse bestimmt. Eine kontrovers geführte Debatte der Biologie ist die Frage, ob das Verhalten des Menschen eher aufgrund seiner Evolutionsgeschichte (Phylogenese) oder eher aufgrund seiner persönlichen (ontogenetischen) Prägung bestimmt ist. Also: Wie festlegend sind Humangenetik und biologische Grundlagen für die Freiheit menschlichen Verhaltens und Denkens im Gegensatz zur Prägung durch Kultur und Umgebung? Genetische Studien haben viele spezifische genetische Faktoren identifiziert, die die Persönlichkeit und damit die Freiheit eines Individuums beeinflussen. Beispiele dafür sind das Down-Syndrom bis hin zu eher subtilen Effekten wie der statistischen Disposition für Schizophrenie. In letzteren und vielen anderen Fällen handelt es sich um ein Wechselspiel zwischen Disposition und Umwelt, das die individuelle Freiheit des Denkens und Handelns beschränkt.
In der von Richard M. Ryan und Edward L. Deci seit 2000 vertretenen Selbstbestimmungstheorie (SDT)[38][39] ist der Begriff Autonomie von zentraler Bedeutung. Er wurde hier definiert als ein Gefühl völliger Freiwilligkeit (urge to be causal agent of one's own life), also als subjektiv wahrgenommener eigener freier Wille. Dabei gehört Autonomie aus Sicht dieser Theorie zusammen mit Kompetenz und sozialer Eingebundenheit zu den drei universalen psychologischen Grundbedürfnissen, die für die Qualität von Verhalten sowie damit verbundenem Wohlbefinden von Bedeutung seien. Diese Grundbedürfnisse hätten sich im Laufe der Evolutionsgeschichte der Menschheit als diejenigen Mechanismen herausgebildet, mit denen der Einzelne sich am besten an die Anforderungen seines sozialen und physikalischen Umfeldes anpassen könne. Das Bedürfnis nach Autonomie beschreibe dabei eine tief im Organismus verwurzelte Tendenz zur Selbstregulation der eigenen Handlungen und Kohärenz seiner Verhaltensziele.
Zur Beschreibung von Verhalten dient in der Selbstbestimmungstheorie ein Motivationsbegriff, der als Kenngrößen nicht nur die Motivationsstärke, sondern daneben auch den, ebenfalls als Kontinuum verstandenen, Autonomiegrad besitzt. Dieser erstreckt sich von fremd reguliertem Verhalten, beispielsweise durch äußere Belohnungen oder Zwang, über nur eingeschränkt internalisierte Regulierung (Vermeidung von Schuldgefühlen oder Angst), bis hin zu autonomer Motivation, bei der das Verhalten vollständig in das Selbstgefühl integriert ist. Im Vergleich zu einem fremd regulierten Verhalten gleicher Motivationsstärke sei autonom reguliertes Verhalten durch größere Effizienz, insbesondere in Bezug auf Problemlösungsverhalten und Durchhaltevermögen, sowie durch größeres Wohlbefinden gekennzeichnet.
Vertreter der extremen Linken ziehen die Diskussion um den freien Willen dazu heran, der gängigen Psychologie eine politische Entmündigung des Bürgers zu unterstellen. Von der Psychoanalyse Freuds bis zur Verhaltenspsychologie Skinners sei die Psychologie reaktionär, weil sie auf das Unterbewusste abhebe, politische Faktoren ignoriere und dem Individuum den freien Willen abspreche, also auch die Fähigkeit, bewusst politisch zu handeln:
Das Wort Freiheit findet in theologischen Diskussionen nicht immer genau dieselbe Verwendung wie in philosophischen, sondern schließt auch bestimmte Aspekte ein, die von einem religiös begründeten Verständnis abhängen. Ein allgemein verbreiteter Konsens bezüglich der Details des Freiheitsbegriffs besteht ebenso wenig wie in der Philosophie.
Von Kritikern bestimmter religionsphilosophischer und theologischer Freiheitsinterpretationen wird häufig ein Problem für die Annahme angeführt, dass die menschliche Freiheit vor dem Hintergrund göttlicher Allwissenheit (Omniszienz) widerspruchsfrei bestehen könne: Wenn Gott allwissend ist, wie kann dann der Mensch frei in seinen Entscheidungen sein? Denn wenn Gott alle Fakten kennt, weiß er auch, welche Entscheidung ein Mensch zu einem bestimmten Zeitpunkt treffen wird. Es bestünden also aufgrund des göttlichen Vorwissens keine alternativen Handlungsmöglichkeiten. Diese zeichnen jedoch gerade eine in Freiheit gefällte Entscheidung aus. Noch verschärft wird dieses Dilemma dann, wenn man – wie in vielen Religionen der Fall – Gott als einem übermächtigen Wesen über das bloß beobachtende Vorherwissen hinaus auch eine die Geschicke der Welt oder das Schicksal des individuellen Menschen lenkende oder fügende Funktion zuweist (Vorsehungsglaube) oder sogar annimmt, die sittlich-religiöse Letztbestimmung beziehungsweise das Seelenheil eines Menschen werde durch göttlichen Ratschluss im Voraus unabwendbar festgelegt (Prädestinationsglaube).
Auch die Gegenthese, göttliche Allwissenheit und menschliche Entscheidungsfreiheit würden einander keineswegs widersprechen, z. B. wenn Gott (als der Raumzeit entzogener Beobachter) nur die Entscheidung vorhersieht, nicht aber beeinflusst,[41] wurde und wird in Theologie und Philosophie vertreten.[42][43] Verschiedene spätscholastische Positionen hierzu werden unter dem Schlagwort Voluntarismus zusammengefasst.
Diesen Ansichten gegenüber steht das theologische Konzept eines Wirklichkeitsganzen, welches sich sowohl aus der natürlichen (Diesseits) als auch aus der transzendenten Wirklichkeit (Jenseits) zusammensetze. Beide Wirklichkeiten existierten auch im Menschen, weshalb die Prozesse der Handlungsfindung sowohl biologisch als auch göttlich motiviert seien. Es brächten also jeweils zwei Grundmotivationen Optionen für einen möglichen Wahlentscheid hervor, womit der Mensch zumindest zwischen diesen beiden Grundmotivationen bei Handlungen frei entscheiden könne.
Eine klassische Behandlung des Problems findet sich bereits in der Theodizee von Gottfried Wilhelm Leibniz, der die Frage behandelt, inwiefern Gott für das malum morale, also für die Entscheidungen der Menschen für das Böse, verantwortlich gemacht werden kann.
Im Christentum hat die Frage nach der Willensfreiheit im engeren Sinne eine wichtige Stellung, weil damit das Problem angesprochen wird, inwiefern der Mensch aus eigener Kraft vor Gott gerecht werden und sich dem Heil zuwenden kann. Im christlichen Kontext behandelt die Frage nach der Willensfreiheit demnach das Verhältnis von Freiheit und Gnade Gottes.
Die Bibel enthält sowohl Verse, welche die Freiheit des Menschen, selbst zu entscheiden, unterstreichen, als auch solche, die diese Freiheit einschränken oder aufheben. Besonders zu erwähnen sind hierzu Paulus’ Ausführungen zur souveränen Bestimmung des Menschen zu Heil oder Unheil durch Gott (Röm 9,20–23 EU). Augustinus vertrat unter anderem im Streit mit dem Pelagianismus die Position, dass es keine absolute Willensfreiheit gebe. Diese Fähigkeit habe der Mensch durch den Sündenfall verloren. In De Civitate Dei (De Civ. XII, 6–9) und in De libero arbitrio (De lib. arb. I, 12. III, 3) argumentiert Augustinus jedoch dafür, dass der Mensch durch die Gnade Gottes sich entscheiden kann, weil sein Wissen unvollkommen ist. Willensentscheidungen sind nicht kausal verursacht. Dies gilt, obwohl Gott allwissend ist und aufgrund dessen die menschlichen Entscheidungen vorhersehen kann.[44]  „Der Wille, der jedwede Handlung auslöst, wird einzig und allein durch eine Vorstellung gewonnen. Was der Mensch für sich wählt, was er von sich weist, liegt in seiner Macht. Es muss zugegeben werden, dass der Geist sowohl von höheren als auch von niederen Vorstellungen berührt wird, und das vernünftige Wesen aus beiden die Auswahl trifft, die es will, und dass sich aus dem Verdienst dieser Wahl sowohl Elend als auch Glückseligkeit ergeben.“ (lib.arb, III, 74)
Martin Luther betonte in seiner Schrift De servo arbitrio die Unfreiheit des menschlichen Willens hinsichtlich des Heils und auch grundsätzlich die Unmöglichkeit eines freien Willens.[45] Diese Position führte in der Zeit der Reformation zum öffentlichen Bruch zwischen Martin Luther und Erasmus von Rotterdam. Johannes Calvin ging weiter als Luther und vertrat die Lehre einer doppelten Prädestination, gegen die sich später u. a. der reformierte Theologe Jacobus Arminius wandte. Anders die radikal-reformatorischen Unitarier, die sich in dem 1605 erstmals erschienenen Rakauer Katechismus für den freien Willen und gegen die Erbsünde aussprachen.[46] Auch im 1864 von József Ferencz für die ungarischen und siebenbürgischen Unitarier verfassten Katechismus wird der Freie Wille betont.[47]
Innerhalb des breiten Spektrums christlicher Kirchen neigen Theologen mancher Konfessionen heute stärker dazu, den freien Willen zu betonen als andere. So heben römisch-katholische Theologen den freien Willen des Menschen hervor: Es liege an jedem Einzelnen, die göttliche Liebe als Motivation bei Handlungen zu bevorzugen bzw. die Gnadengaben Gottes anzunehmen und er könne sich auch in Freiheit dazu entscheiden, sie abzulehnen (dies betont etwa Karl Rahner). Auch die meisten Freikirchen, die nicht aus dem Pietismus entstanden sind, sehen einen freien Willen des Menschen als gegeben an. Lutherische und calvinistische Kirchen stehen dem tendenziell entgegen.
Die meisten Kirchen erkennen die Einschränkung des freien Willens etwa durch psychische Zwänge an. Die katholische Kirche geht davon aus, dass im Falle einer Besessenheit durch Dämonen bzw. Geister der freie Wille des Besessenen ebenfalls eingeschränkt oder aufgehoben ist.
Im Islam sind Prädestinationslehren weit verbreitet, doch haben Qadariten und Muʿtaziliten die Willensfreiheit des Menschen gelehrt. Auch im Hinduismus gehen einige Strömungen von Prädestination aus, andere betonen die Freiheit des Menschen. Der Buddhismus verneint die absolute Willensfreiheit,[48] während die Idee der Willensfreiheit im Judentum ein zentrales Dogma darstellt (siehe Dtn 11,26 EU).
Die verfassungsrechtliche Leitidee der Menschenwürde (Art. 1 Abs. 1 Grundgesetz, auch Art. 1 der Grundrechtecharta der Europäischen Union) beruht nach Ansicht des Bundesverfassungsgerichts auf der Entscheidungsfreiheit: „Dem Schutz der Menschenwürde liegt die Vorstellung vom Menschen als einem geistig–sittlichen Wesen zugrunde, das darauf angelegt ist, sich in Freiheit selbst zu bestimmen und zu entfalten.“ Aus diesem Ansatz leitet das Bundesverfassungsgericht dann auch den Verfassungsrang des, jedenfalls für das deutsche Strafrecht maßgeblichen, Schuldprinzips ab.[49]
Auch der deutsche (Bundes-)Gesetzgeber setzt die Fähigkeit der freien Entscheidung des erwachsenen Menschen voraus:
So bestimmt § 104 Nr. 2 BGB die Geschäftsunfähigkeit als einen „die freie Willensbildung ausschließenden Zustand“ und setzt damit die Willensfreiheit des Individuums als eine nur im Ausnahmefall wegfallende Grundeigenschaft voraus.[50] Ohne diese Prämisse wäre vor allem das Prinzip der Privatautonomie, auf dem das deutsche Privatrecht wesentlich beruht, erheblich in Zweifel gezogen.[51]
Ebenso geht das Strafgesetzbuch von der Voraussetzung der freien Entscheidung aus: Nur „wer bei Begehung der Tat wegen einer krankhaften seelischen Störung, wegen einer tiefgreifenden Bewusstseinsstörung oder wegen Schwachsinns oder einer schweren anderen seelischen Abartigkeit unfähig ist, das Unrecht der Tat einzusehen oder nach dieser Einsicht zu handeln“, handelt gemäß § 20 StGB „ohne Schuld“.
Aus einem Beschluss des Bayerischen Obersten Landesgerichts:[52] „Die Bestellung eines Betreuers von Amts wegen, also ohne Antrag des Volljährigen und, wie hier, gegen seinen Willen, setzt aber voraus, dass der Betreute aufgrund einer psychischen Erkrankung seinen Willen nicht frei bestimmen kann. Dies sagt das Gesetz zwar nicht ausdrücklich, ergibt sich aber aus einer verfassungskonformen Auslegung des Gesetzes. Denn der Staat hat wegen entsprechender Verfassungsbestimmungen nicht das Recht, seine erwachsenen und zur freien Willensbestimmung fähigen Bürger zu bessern oder zu hindern, sich selbst zu schädigen“.[53] Siehe auch die Neufassung von § 1896 Abs. 1a BGB (seit 1. Juli 2005). Im Grundsatz muss jede Entscheidung des Betreuers im Sinn des freien Willens des Betreuten getroffen werden. Das gebietet das in Art. 2 Abs. 1 GG verankerte Grundrecht auf Selbstbestimmung.
Harte Deterministen verwerfen das Konzept der moralischen Verantwortlichkeit. Wie kann man jemanden moralisch verantwortlich machen, wenn er in jeder Situation immer nur eine Möglichkeit zu handeln hat? Dass die Entscheidungen nicht unter Einschränkung der Handlungsfreiheit entstehen, ändere nichts an der Tatsache, dass der Determinismus den Handelnden von moralischer Verantwortlichkeit entbinde. Die Gegenposition besagt, dass trotz des bestehenden Determinismus ein Individuum die moralische Verantwortung für seine Handlungen tragen müsse und insofern ggf. gesellschaftliche und juristische Konsequenzen gerechtfertigt seien.
Kompatibilisten argumentieren dagegen, dass der Determinismus gerade eine Vorbedingung für moralische Verantwortlichkeit sei. Man könne niemanden für etwas verantwortlich machen, es sei denn, seine Handlungen wurden durch seinen Charakter, seine Motive und Werte bestimmt.
Libertarianer halten an der Idee des freien Willens und somit auch an moralischer Verantwortlichkeit fest.
Befürworter moralischer Verantwortlichkeit unterstellen Entscheidungsfreiheit oder sind der Meinung, unsere Gesellschaftsordnung würde auseinanderbrechen, wenn sich niemand mehr für seine Taten moralisch verantwortlich fühlte.
Weiterhin wird argumentiert, dass der juristische Grundsatz „Keine Strafe ohne Schuld“ nicht mehr anwendbar wäre, wenn man Entscheidungsfreiheit und somit auch persönliche Schuld verwerfen würde.
Es gibt jedoch auch Meinungen,[54] die einem Menschen auch im Determinismus moralische Verantwortung zuschreiben. Im Wesentlichen geht es dabei um die Frage der Urheberschaft. Wer behauptet, nicht er sei der Urheber seiner Handlung, sondern seine Neuronen und die in seinem Körper ablaufenden physischen Prozesse hätten die Tat herbeigeführt, verkennt, dass Neuronen und physische Prozesse ein Teil von ihm sind, und begibt sich damit in einen Widerspruch. Der Begriff der Verantwortung als individuelle Zuschreibbarkeit des Verhaltens verliert also in einer deterministisch bestimmten Welt keineswegs seinen Sinn. Der Determinismus liefert demzufolge auch keine Begründung dafür, dass unser Rechtssystem geändert werden müsste und einige Deterministen argumentieren seit geraumer Zeit für ein deterministisches Strafrecht.[55]
Wolfgang Prinz ist der Ansicht, dass es im Bereich des sozialen Miteinanders sowie in Moral und Recht nicht von Bedeutung sei, ob die Menschen faktisch einen freien Willen besitzen. Vielmehr sei es von Belang, dass die Menschen über eine Freiheitsintuition verfügen, die in ihrer Wahrnehmung ebenso real sei wie die tatsächliche Existenz des freien Willens. Diese Freiheitsintuition führe dazu, dass Menschen bereit sind, für ihre Handlungen Verantwortung zu übernehmen und anderen Menschen für deren Handlungen Verantwortung zuzuschreiben.[56]
„Die Daumenschraube eines jeden finden: Dies ist die Kunst, den Willen Anderer in Bewegung zu setzen. Es gehört mehr Geschick als Festigkeit dazu. Man muss wissen, wo einem Jeden beizukommen sei. Es gibt keinen Willen, der nicht einen eigentümlichen Hang hätte, welcher, nach der Mannigfaltigkeit des Geschmacks, verschieden ist. Alle sind Götzendiener, Einige der Ehre, Andere des Interesses, die meisten des Vergnügens. Der Kunstgriff besteht darin, dass man diesen Götzen eines Jeden kenne, um mittels desselben ihn zu bestimmen. Weiß man, welches für jeden der wirksame Anstoß sei, so ist es, als hätte man den Schlüssel zu seinem Willen. Man muß nun auf die allererste Springfeder oder das primum mobile in ihm zurückgehen, welches aber nicht etwa das Höchste seiner Natur, sondern meistens das Niedrigste ist: denn es gibt mehr schlecht- als wohlgeordnete Gemüter in dieser Welt. Jetzt muss man zuvörderst sein Gemüt bearbeiten, denn ihm durch ein Wort den Anstoß geben, endlich mit seiner Lieblingsneigung den Hauptangriff machen; so wird unfehlbar sein freier Wille schachmatt.“
„Eben so muß der entschlossenste Fatalist, der es ist, so lange er sich der bloßen Speculation ergiebt, dennoch, so bald es ihm um Weisheit und Pflicht zu thun ist, jederzeit so handeln, als ob er frei wäre, und diese Idee bringt auch wirklich die damit einstimmige That hervor und kann sie auch allein hervorbringen. Es ist schwer, den Menschen ganz abzulegen“
„Das Verlangen nach ‚Freiheit des Willens‘, in jenem metaphysischen Superlativ-Verstande, wie er leider noch immer in den Köpfen der Halb-Unterrichteten herrscht, das Verlangen, die ganze und letzte Verantwortlichkeit für seine Handlungen selbst zu tragen und Gott, Welt, Vorfahren, Zufall, Gesellschaft davon zu entlasten, ist nämlich nichts Geringeres, als eben jene causa sui zu sein und, mit einer mehr als Münchhausen’schen Verwegenheit, sich selbst aus dem Sumpf des Nichts an den Haaren in’s Dasein zu ziehn.“
„Ich lache eures freien Willens und auch eures unfreien: Wahn ist mir das, was ihr Willen heißt, es giebt keinen Willen.“
„Ich weiß ehrlich nicht, was die Leute meinen, wenn sie von der Freiheit des menschlichen Willens sprechen. Ich habe zum Beispiel das Gefühl, dass ich irgend etwas will; aber was das mit Freiheit zu tun hat, kann ich überhaupt nicht verstehen. Ich spüre, dass ich meine Pfeife anzünden will und tue das auch; aber wie kann ich das mit der Idee der Freiheit verbinden? Was liegt hinter dem Willensakt, dass ich meine Pfeife anzünden will? Ein anderer Willensakt? Schopenhauer hat einmal gesagt: ‚Der Mensch kann tun was er will; er kann aber nicht wollen was er will.‘“
„Nehmen wir an, Sie hätten einen unbedingt freien Willen. Es wäre ein Wille, der von nichts abhinge: ein vollständig losgelöster, von allen ursächlichen Zusammenhängen freier Wille. Ein solcher Wille wäre ein aberwitziger, abstruser Wille. Seine Losgelöstheit nämlich würde bedeuten, dass er unabhängig wäre von Ihrem Körper, Ihrem Charakter, Ihren Gedanken und Empfindungen, Ihren Phantasien und Erinnerungen. Es wäre, mit anderen Worten, ein Wille ohne Zusammenhang mit all dem, was Sie zu einer bestimmten Person macht. In einem substantiellen Sinn des Wortes wäre er deshalb gar nicht Ihr Wille.“

Der Buddhismus ist eine der großen Weltreligionen. Im Gegensatz zu anderen großen Religionen ist der Buddhismus keine theistische Religion, hat also als sein Zentrum nicht die Verehrung eines allmächtigen Gottes. Vielmehr gründen sich die meisten buddhistischen Lehren auf umfangreiche philosophisch-logische Überlegungen[1] in Verbindung mit Leitlinien der Lebensführung, wie es auch im chinesischen Daoismus und Konfuzianismus der Fall ist. Zudem ist die Praxis der Meditation und daraus herrührendes Erfahrungswissen ein wichtiges Element im Buddhismus.
Wie andere Religionen umfasst auch der Buddhismus ein weites Spektrum an Erscheinungsformen, die sowohl philosophische Lehre beinhalten als auch Klosterwesen, kirchen- oder vereinsartige Religionsgemeinschaften und einfache Volksfrömmigkeit. Sie werden im Fall des Buddhismus aber durch keine zentrale Autorität oder Lehrinstanz, die Dogmen verkündet, zusammengehalten.
Gemeinsam ist allen Buddhisten, dass sie sich auf die Lehren des Siddhartha Gautama berufen, der in Nordindien lebte, nach den heute in der Forschung vorherrschenden Datierungsansätzen im 6. und möglicherweise noch im frühen 5. Jahrhundert v. Chr. Er wird als der „historische Buddha“ bezeichnet, um ihn von den mythischen Buddha-Gestalten zu unterscheiden, die nicht historisch bezeugt sind. „Buddha“ bedeutet wörtlich „der Erwachte“ und ist ein Ehrentitel, der sich auf ein Erlebnis bezieht, das als Bodhi („Erwachen“) bezeichnet wird. Gemeint ist damit nach der buddhistischen Lehre eine fundamentale und befreiende Einsicht in die Grundtatsachen allen Lebens, aus der sich die Überwindung des leidhaften Daseins ergibt. Diese Erkenntnis nach dem Vorbild des historischen Buddha durch Befolgung seiner Lehren zu erlangen, ist das Ziel der buddhistischen Praxis – wobei von den beiden Extremen der selbstzerstörerischen Askese und des ungezügelten Hedonismus, aber auch generell von Radikalismus abgeraten wird, vielmehr soll ein Mittlerer Weg eingeschlagen werden.[2] In diesem Zusammenhang stellen die Aussagen des Religionsgründers Buddha in der Überlieferung die zentrale Autorität dar, und es gibt einen historisch gewachsenen Kanon an Texten, mit dem im Rahmen von Buddhistischen Konzilien die Grundlinien der Religion bestimmt worden sind. Gleichwohl handelt es sich nicht um Dogmen im Sinne einer Offenbarungsreligion, deren Autorität sich auf den Glauben an eine göttlich inspirierte heilige Schrift stützt. Dementsprechend wird der Buddha im Buddhismus verehrt, aber nicht in einem engeren Sinne angebetet.
Der Buddhismus hat weltweit je nach Quelle und Zählweise zwischen 230 und 500 Millionen[3][4] Anhänger – und ist damit die viertgrößte Religion der Erde (nach Christentum, Islam und Hinduismus). Der Buddhismus stammt aus Indien und ist heute am meisten in Süd-, Südost- und Ostasien verbreitet. Etwa die Hälfte aller Buddhisten lebt in China.[5] Er hat seit dem 19. Jahrhundert aber auch begonnen, in der westlichen Welt Fuß zu fassen.
Der Buddhismus entstand auf dem indischen Subkontinent durch Siddhartha Gautama. Der Überlieferung zufolge erlangte er im Alter von 35 Jahren durch das Erlebnis des „Erwachens“ eine innere Transformation. Zunächst habe er es nicht für möglich gehalten, über seine Einsichten überhaupt zu sprechen, habe sich aber dann dazu bewegen lassen, sie in eine ausformulierte Lehre zu kleiden, um sie nach Möglichkeit weiterzugeben.[6] Er gewann bald Schüler und gründete die buddhistische Gemeinde. Bis zu seinem Tod im Alter von etwa 80 Jahren wanderte er schließlich lehrend durch Nordindien.
Von der nordindischen Heimat Siddhartha Gautamas verbreitete sich der Buddhismus zunächst auf dem indischen Subkontinent, auf Sri Lanka und in Zentralasien[7]. Insgesamt sechs buddhistische Konzile trugen zur „Kanonisierung“ der Lehren und, gemeinsam mit der weiteren Verbreitung in Ost- und Südostasien, zur Entwicklung verschiedener Traditionen bei. Der nördliche Buddhismus (Mahayana) erreichte über die Seidenstraße Zentral- und Ostasien, wo sich weitere Traditionen, wie etwa Chan (China), Zen (Japan) und Amitabha-Buddhismus (Ostasien), entwickelten. In die Himalaya-Region gelangte der Buddhismus auch direkt aus Nordindien; dort entstand der Vajrayana (Tibet, Bhutan, Nepal, Mongolei u. a.).[8] Aspekte des Buddhismus drangen auch in andere religiöse Traditionen ein oder gaben Impulse zu deren Institutionalisierung (vgl. Bön und Shintō bzw. Shinbutsu-Shūgō). Von Südindien und Sri Lanka gelangte der südliche Buddhismus (Theravada) in die Länder Südostasiens, wo er den Mahayana verdrängte. Der Buddhismus trat in vielfältiger Weise mit den Religionen und Philosophien der Länder, in denen er Verbreitung fand, in Wechselwirkung. Dabei wurde er auch mit religiösen und philosophischen Traditionen kombiniert, deren Lehren sich von denen des ursprünglichen Buddhismus stark unterscheiden.[9]
Die Grundlagen der buddhistischen Praxis und Theorie sind vom Buddha in Form der Vier Edlen Wahrheiten[11][12][13] formuliert worden: Die Erste Edle Wahrheit lautet, dass das Leben in der Regel vom Leiden (dukkha) an Geburt, Alter, Krankheit und Tod geprägt ist, sowie von subtileren Formen des Leidens, die vom Menschen oft nicht als solches erkannt werden, wie etwa das Hängen an einem Glück, das jedoch vergänglich ist (in diesem Zusammenhang wird darauf hingewiesen, dass das Wort „dukkha“ sich auch auf Bedeutungen wie „Unbefriedigtsein, Frustration“ erstreckt). Die Zweite Edle Wahrheit lautet, dass dieses Leid in Abhängigkeit von Ursachen entsteht, nämlich im Wesentlichen durch die Drei Geistesgifte, die in deutscher Übersetzung meist als „Gier“, „Hass“ und „Unwissenheit / Verblendung“ bezeichnet werden. Die Dritte Edle Wahrheit besagt, dass das Leiden, da durch Ursachen bedingt, zukünftig aufgehoben werden kann, wenn nur diese Ursachen aufgelöst werden können, und dass dann vollständige Freiheit von Leiden erlangt werden kann (also auch Freiheit von Geburt und Tod). Die Vierte Edle Wahrheit besagt, dass es Mittel zu dieser Auflösung der Leidensursachen gibt, und damit zur Entstehung von wirklichem Glück: Dies ist die Praxis der Übungen des Edlen Achtfachen Pfades.[14] Sie bestehen in: rechter Erkenntnis, rechter Absicht, rechter Rede, rechtem Handeln, rechtem Lebenserwerb, rechter Übung, rechter Achtsamkeit und rechter Meditation, wobei mit recht die Übereinstimmung der Praxis mit den Vier Edlen Wahrheiten, also der Leidvermeidung gemeint ist[15].
Nach der buddhistischen Lehre sind alle unerleuchteten Wesen einem endlosen leidvollen Kreislauf (Samsara[16]) von Geburt und Wiedergeburt unterworfen,[17] Ziel der buddhistischen Praxis ist, aus diesem Kreislauf des ansonsten immerwährenden Leidenszustandes herauszutreten. Dieses Ziel soll durch die Vermeidung von Leid, also ethisches Verhalten, die Kultivierung der Tugenden (Fünf Silas), die Praxis der „Versenkung“ (Samadhi, vgl. Meditation) und die Entwicklung von Mitgefühl (hier klar unterschieden von Mitleid) für alle Wesen und allumfassender Weisheit (Prajna) als Ergebnisse der Praxis des Edlen Achtfachen Pfades erreicht werden. Auf diesem Weg werden Leid und Unvollkommenheit überwunden und durch Erleuchtung (Erwachen) der Zustand des Nirwana[18] realisiert. Nirwana ist nicht einfach ein Zustand, in dem kein Leid empfunden wird, sondern eine umfassende Transformation des Geistes, in dem auch alle Veranlagungen, Leiden je hervorzubringen, verschwunden sind.[19] Es ist ein transzendenter Zustand, der nicht sprachlich oder vom Alltagsverstand erfasst werden kann, aber im Prinzip von jedem fühlenden Wesen verwirklicht werden könnte.
Indem jemand Zuflucht zum Buddha (dem Zustand), zum Dharma[20] (Lehre und Weg zu diesem Zustand) und zur Sangha[21] (der Gemeinschaft der Praktizierenden) nimmt, bezeugt er seinen Willen zur Anerkennung und Praxis der Vier Edlen Wahrheiten und seine Zugehörigkeit zur Gemeinschaft der Praktizierenden des Dharma. Die Sangha selbst unterteilt sich in die Praktizierenden der Laien-Gemeinschaft und die ordinierten der Mönchs- bzw. Nonnenorden.[22]
Die Lebensdaten Siddhartha Gautamas gelten traditionell als Ausgangspunkt für die Chronologie der südasiatischen Geschichte, sie sind jedoch umstritten. Die herkömmliche Datierung (563–483 v. Chr.) wird heute kaum noch vertreten. Die neuere Forschung geht davon aus, dass Siddhartha nicht 563 v. Chr. geboren wurde, sondern mehrere Jahrzehnte, vielleicht ein Jahrhundert später. Die gegenwärtig vorherrschenden Ansätze für die Datierung des Todes schwanken zwischen ca. 420 und ca. 368 v. Chr.[23][24][25]
Nach der Überlieferung wurde Siddhartha in Lumbini im nordindischen Fürstentum Kapilavastu, heute ein Teil Nepals, als Sohn des Herrscherhauses von Shakya geboren. Daher trägt er den Beinamen Shakyamuni, „Weiser aus dem Hause Shakya“.[26]
Im Alter von 29 Jahren wurde ihm bewusst, dass Reichtum und Luxus nicht die Grundlage für Glück sind. Er erkannte, dass Leid wie Altern, Krankheit, Tod und Schmerz untrennbar mit dem Leben verbunden ist, und brach auf, um verschiedene Religionslehren und Philosophien zu erkunden, um die wahre Natur menschlichen Glücks zu finden. Sechs Jahre der Askese, des Studiums und danach der Meditation führten ihn schließlich auf den Weg der Mitte. Unter einer Pappelfeige in Bodhgaya im heutigen Nordindien hatte er das Erlebnis des Erwachens (Bodhi). Wenig später hielt er in Isipatana, dem heutigen Sarnath, seine erste Lehrrede und setzte damit das „Rad der Lehre“ (Dharmachakra) in Bewegung.
Danach verbrachte er als ein Buddha den Rest seines Lebens mit der Unterweisung und Weitergabe der Lehre, des Dharma, an die von ihm begründete Gemeinschaft. Diese Vierfache Gemeinschaft bestand aus den Mönchen (Bhikkhu) und Nonnen (Bhikkhuni) des buddhistischen Mönchtums sowie aus männlichen Laien (Upāsaka) und weiblichen Laien (Upasika). Mit seinem (angeblichen) Todesjahr im Alter von 80 Jahren beginnt die buddhistische Zeitrechnung.
Drei Monate nach dem Tod des Buddha traten seine Schüler in Rajagarha zum ersten Konzil (sangiti[27]) zusammen, um den Dhamma (die Lehre) und den Vinaya (die Mönchsregeln) zu besprechen und gemäß den Unterweisungen des Buddha festzuhalten. Die weitere Überlieferung erfolgte mündlich. Etwa 100 Jahre später fand in Vesali das zweite Konzil statt. Diskutiert wurden nun vor allem die Regeln der Mönchsgemeinschaft, da es bis dahin bereits zur Bildung verschiedener Gruppierungen mit unterschiedlichen Auslegungen der ursprünglichen Regeln gekommen war.
Während des zweiten Konzils und den folgenden Zusammenkünften kam es zur Bildung von bis zu 18 verschiedenen Schulen (Nikaya-Schulen), die sich auf unterschiedliche Weise auf die ursprünglichen Lehren des Buddha beriefen. Daneben entstand auch die Mahasanghika, die für Anpassungen der Regeln an die veränderten Umstände eintrat und als früher Vorläufer des Mahayana betrachtet werden kann. Die ersten beiden Konzile sind von allen buddhistischen Schulen anerkannt.[27] Die anderen Konzilien werden nur von einem Teil der Schulen akzeptiert.[27] Die Historizität der Konzile stuft der Sinologe Helwig Schmidt-Glintzer allerdings als unwahrscheinlich ein.[27]
Im 3. Jahrhundert v. Chr. trat in Pataliputra (heute Patna), unter der Schirmherrschaft des Königs Ashoka und dem Vorsitz des Mönchs Moggaliputta Tissa, das 3. Konzil zusammen. Ziel der Versammlung war es, sich wieder auf eine einheitliche buddhistische Lehre zu einigen. Insbesondere Häretiker sollten aus der Gemeinschaft ausgeschlossen und falsche Lehren widerlegt werden. Im Verlauf des Konzils wurde zu diesem Zweck das Buch Kathavatthu verfasst, das die philosophischen und scholastischen Abhandlungen zusammenfasste. Dieser Text wurde zum Kernstück des Abhidhammapitaka, einer philosophischen Textsammlung. Zusammen mit dem Suttapitaka, den niedergeschriebenen Lehrreden des Buddha, und dem Vinayapitaka, der Sammlung der Ordensregeln, bildet es das in Pali verfasste Tipitaka (Sanskrit: Tripitaka, deutsch: „Dreikorb“, auch Pali-Kanon), die älteste große Zusammenfassung buddhistischen Schriftgutes.
Nur diese Schriften wurden vom Konzil als authentische Grundlagen der buddhistischen Lehre anerkannt, was die Spaltung der Mönchsgemeinschaft besiegelte. Während der Theravada, die Lehre der Älteren, sich auf die unveränderte Übernahme der ursprünglichen Lehren und Regeln einigte, legte die Mahasanghika keinen festgelegten Kanon von Schriften fest und nahm auch Schriften auf, deren Herkunft vom Buddha nicht eindeutig nachgewiesen werden konnte.
In den folgenden Jahrhunderten verbreitete sich die Lehre in Süd- und Ostasien. Während der Regierungszeit des Königs Ashoka (3. Jahrhundert v. Chr.) verbreitete sich der Buddhismus über ganz Indien und weit darüber hinaus. Auch Teile von Afghanistan gehörten zu seinem Reich. Im Grenzgebiet zu Pakistan entstand dort, beeinflusst von griechischen Bildhauern, die mit Alexander dem Großen ins Land gekommen waren, in Gandhara die graeco-buddhistische Kultur, eine Mischung von indischen und hellenistischen Einflüssen. In deren Tradition entstanden unter anderem die Buddha-Statuen von Bamiyan.
Ashoka schickte Gesandte in viele Reiche jener Zeit. So verbreitete sich die Lehre allmählich über die Grenzen jener Region, in welcher der Buddha gelebt und gelehrt hatte, hinaus. Im Westen reisten Ashokas Gesandte bis in den Nahen Osten, Ägypten, zu den griechischen Inseln und nach Makedonien. Über Sri Lanka gelangte die Buddha-Lehre in den folgenden Jahrhunderten zum malayischen Archipel (Indonesien, Borobudur) und nach Südostasien, also Kambodscha (Funan, Angkor), Thailand, Myanmar (Pegu) und Laos. Im Norden und Nordosten wurde der Buddhismus im Hochland des Himalaya (Tibet) sowie in China, Korea und in Japan bekannt.
Während der Buddhismus so weitere Verbreitung fand, verschwand er aus den meisten Gegenden Indiens ab dem 12. Jahrhundert. Die Gründe[28] werden zum einen in der gegenseitigen Durchdringung von Buddhismus und Hinduismus gesehen, zum anderen in der moslemischen Invasion Indiens, in deren Verlauf viele Mönche getötet und Klöster zerstört wurden. Auch die heute noch bekannten letzten Hochburgen des Buddhismus auf dem indischen Subkontinent (Sindh, Bengalen) gehörten zu den islamisierten Gebieten. Auf dem malayischen Archipel (Malaysia, Indonesien) sind heute (mit Ausnahme Balis) nur noch Ruinen zu sehen, die zeigen, dass hier einstmals buddhistische Kulturen geblüht hatten.[29]
Eine vielfältige Weiterentwicklung der Lehre war durch die Worte des Buddha vorbestimmt: Als Lehre, die ausdrücklich in Zweifel gezogen werden darf, hat der Buddhismus sich teilweise mit anderen Religionen vermischt, die auch Vorstellungen von Gottheiten kennen oder die die Gebote der Enthaltsamkeit weniger streng oder gar nicht handhabten.
Der Theravada („die Lehre der Ältesten“) hält sich an die Lehre des Buddha, wie sie auf dem Konzil von Patna festgelegt wurde. Er ist vor allem in den Ländern Süd- und Südostasiens (Sri Lanka, Myanmar, Thailand, Laos und Kambodscha) weit verbreitet. Der Mahayana („das große Fahrzeug“) durchmischte sich mehr mit den ursprünglichen Religionen und Philosophien der Kulturen, in denen der Buddhismus einzog. So kamen z. B. in China Elemente des Daoismus hinzu, wodurch schließlich die Ausprägung des Chan-Buddhismus und später in Japan Zen entstand.
Insbesondere der Kolonialismus des 19. Jahrhunderts hat in vielen Ländern Asiens zu einer Renaissance des Buddhismus geführt. Die Schaffung einer internationalen buddhistischen Flagge 1885 ist dafür ein symbolischer Ausdruck. Besonders den Initiativen von Thailand und Sri Lanka ist die 1950 erfolgte Gründung der World Fellowship of Buddhists (WFB) zu verdanken.
Heute leben weltweit näherungsweise 450 Millionen Buddhisten. Diese Zahl ist jedoch nicht verbindlich, da es starke Schwankungen zwischen einzelnen Statistiken gibt. Die Länder mit der stärksten Verbreitung des Buddhismus sind China, Bhutan, Japan, Kambodscha, Laos, Mongolei, Myanmar, Sri Lanka, Südkorea, Taiwan, Thailand und Vietnam.
In Indien beträgt der Anteil an der Bevölkerung heute weniger als ein Prozent. Neuerdings erwacht jedoch wieder ein intellektuelles Interesse an der buddhistischen Lehre in der gebildeten Schicht. Auch unter den Dalit („Unberührbaren“) gibt es, initiiert durch Bhimrao Ramji Ambedkar, den „Vater der indischen Verfassung“, seit 1956 eine Bewegung, die in der Konversion zum Buddhismus einen Weg sieht, der Unterdrückung durch das Kastensystem zu entkommen.
Seit dem 19. und insbesondere seit dem 20. Jahrhundert wächst auch in den industrialisierten Staaten Europas, den USA und Australien die Tendenz, sich dem Buddhismus als Weltreligion zuzuwenden. Im Unterschied zu den asiatischen Ländern gibt es im Westen die Situation, dass die zahlreichen und oft sehr unterschiedlichen Ausprägungen der verschiedenen Lehrrichtungen nebeneinander in Erscheinung treten.
Organisationen wie die 1975 gegründete EBU (Europäische Buddhistische Union) haben sich zum Ziel gesetzt, diese Gruppen miteinander zu vernetzen und sie in einen Diskurs mit einzubeziehen, der einen längerfristigen Prozess zur Inkulturation und somit Herausbildung eines europäischen Buddhismus begünstigen soll. Ein weiteres Ziel ist die Integration in die europäische Gesellschaft, damit die buddhistischen Vereinigungen ihr spirituelles, humanitäres, kulturelles und soziales Engagement ohne Hindernisse ausüben können.
In vielen Ländern Europas wurde der Buddhismus gegen Ende des 20. Jahrhunderts öffentlich und staatlich als Religion anerkannt. In Europa erhielt der Buddhismus zuerst in Österreich die volle staatliche Anerkennung (1983).[30] In Deutschland und der Schweiz ist der Buddhismus staatlich nicht als Religion anerkannt.
Siehe auch:
In seiner ursprünglichen Form, die aus der vorliegenden ältesten Überlieferung nur eingeschränkt rekonstruierbar ist, und durch seine vielfältige Fortentwicklung ähnelt der Buddhismus teils einer in der Praxis angewandten Denktradition oder Philosophie.
Der Buddha selbst sah sich weder als Gott noch als Überbringer der Lehre eines Gottes. Er stellte klar, dass er die Lehre, Dhamma (Pali) bzw. Dharma (Sanskrit), nicht aufgrund göttlicher Offenbarung erhalten, sondern vielmehr durch eigene meditative Schau (Kontemplation) ein Verständnis der Natur des eigenen Geistes und der Natur aller Dinge gewonnen habe. Diese Erkenntnis sei jedem zugänglich, der seiner Lehre und Methodik folge. Dabei sei die von ihm aufgezeigte Lehre nicht dogmatisch zu befolgen. Im Gegenteil warnte er vor blinder Autoritätsgläubigkeit und hob die Selbstverantwortung des Menschen hervor. Er verwies auch auf die Vergeblichkeit von Bemühungen, die Welt mit Hilfe von Begriffen und Sprache zu erfassen, und mahnte gegenüber dem geschriebenen Wort oder feststehenden Lehren eine Skepsis an, die in anderen Religionen in dieser Radikalität kaum anzutreffen ist.
Von den monotheistischen Religionen (Judentum, Christentum, Islam) unterscheidet der Buddhismus sich grundlegend. So kennt die buddhistische Lehre weder einen allmächtigen Gott noch eine ewige Seele.[31] Das, und auch die Nichtbeachtung des Kastensystems, unterscheidet ihn auch von Hinduismus und Brahmanismus, mit denen er andererseits die Karma-Lehre teilt. In deren Umfeld entstanden, wird er mitunter als eine Reformbewegung zu den vedischen Glaubenssystemen Indiens betrachtet.[32] Mit dieser antiritualistischen und antitheistischen Haltung ist die ursprüngliche Lehre des Siddhartha Gautama sehr wahrscheinlich die älteste hermeneutische Religion der Welt.[33][34]
Dharma (Sanskrit) bzw. Dhamma (Pali) bezeichnet im Buddhismus im Wesentlichen zweierlei:
Kern der Lehre des Buddha sind die von ihm benannten Vier Edlen Wahrheiten, aus der vierten der Wahrheiten folgt als Weg aus dem Leiden der Achtfache Pfad.
Im Zentrum der „Vier edlen Wahrheiten“ steht das Leiden (dukkha), seine Ursachen und der Weg, es zum Verlöschen zu bringen. Der Achtfache Pfad ist dreigeteilt, die Hauptgruppen sind: die Einsicht in die Lehre, ihre ethischen Grundlagen und die Schwerpunkte des geistigen Trainings (Meditation/Achtsamkeit).
Die „bedingte Entstehung“, auch „Entstehen in Abhängigkeit“ bzw. „Konditionalnexus“[35] (Pali: Paticcasamuppada, Sanskrit: Pratityasamutpada), ist eines der zentralen Konzepte des Buddhismus. Es beschreibt in einer Kette von 12 miteinander verwobenen Elementen die Seinsweise aller Phänomene in ihrer dynamischen Entwicklung und gegenseitigen Bedingtheit. Die Essenz dieser Lehre kann zusammengefasst werden in dem Satz: „Dieses ist, weil jenes ist“.
Kamma (Pali) bzw. Karma (Sanskrit) bedeutet „Tat, Wirken“ und bezeichnet das sinnliche Begehren und das Anhaften an die Erscheinungen der Welt (Gier, Hass, Ich-Sucht), die Taten, die dadurch entstehen, und die Wirkungen von Handlungen und Gedanken in moralischer Hinsicht, insbesondere die Rückwirkungen auf den Akteur selbst. Es entspricht in etwa dem Prinzip von Ursache und Wirkung. Karma bezieht sich auf alles Tun und Handeln sowie alle Ebenen des Denkens und Fühlens. All das erzeugt entweder gutes oder schlechtes Karma oder kann karmisch gesehen neutral sein.
Gutes wie schlechtes Karma erzeugt die Folge der Wiedergeburten, das Samsara. Höchstes Ziel des Buddhismus ist es, diesem Kreislauf zu entkommen, indem kein Karma mehr erzeugt wird – Handlungen hinterlassen dann keine Spuren mehr in der Welt. Im Buddhismus wird dies als Eingang ins Nirwana bezeichnet.
Da dieses Ziel in der Geschichte des Buddhismus oft als unerreichbar in einem Leben galt, ging es, besonders bei den Laien, mehr um das Anhäufen guten Karmas als um das Erreichen des Nirwana in diesem Leben. Gekoppelt daran ist der Glaube, dass das erworbene Verdienst (durch gute Taten, zeitweiligen Beitritt in den Sangha, Spenden an Mönche, Kopieren von Sutras und vieles mehr) auch rituell an andere weitergegeben werden könne, selbst an Verstorbene oder ganze Nationen.
Der den wichtigen indischen Religionen gemeinsame Begriff Samsara, „beständiges Wandern“, bezeichnet den fortlaufenden Kreislauf des Lebens aus Tod und Geburt, Werden und Vergehen. Das Ziel der buddhistischen Praxis ist, diesen Kreislauf zu verlassen. Samsara umfasst alle Ebenen der Existenz, sowohl jene, die wir als Menschen kennen, wie auch alle anderen, von den Höllenwesen (Niraya Wesen) bis zu den Göttern (Devas). Alle Wesen sind im Kreislauf des Lebens gefangen, daran gebunden durch Karma: ihre Taten, Gedanken und Emotionen, durch Wünsche und Begierden. Erst das Erkennen und Überwinden dieser karmischen Kräfte ermöglicht ein Verlassen des Kreislaufs. Im Mahayana entstand darüber hinaus die Theorie der Identität von Samsara und Nirwana (in westlich-philosophischen Begriffen also Immanenz statt Transzendenz).
Die Astika-Schulen der indischen Philosophie lehrten das „Selbst“ (p. attā, skt. ātman), vergleichbar mit dem Begriff einer persönlichen Seele. Der Buddha verneinte die Existenz von ātta als persönliche und beständige Einheit. Im Gegensatz dazu sprach er von dem „Nicht-Selbst“ (p. anattā, skt. anātman). Die Vorstellung von einem beständigen Selbst ist Teil der Täuschung über die Beschaffenheit der Welt. Gemäß der Lehre des Buddhas besteht die Persönlichkeit mit all ihren Erfahrungen und Wahrnehmungen in der Welt aus den Fünf Gruppen, (p. khandhā, skt. skandhas): Körper, Empfindungen, Wahrnehmungen, Geistesregungen und Bewusstsein. Das Selbst ist aus buddhistischer Sicht keine konstante Einheit, sondern ein von beständigem Werden, Wandeln und Vergehen gekennzeichneter Vorgang.
Vor diesem Hintergrund hat das zur Zeit des Buddha bereits existierende Konzept der Wiedergeburt, punabbhava, (p.; puna ‚wieder‘, bhava ‚werden‘)[36] im Buddhismus eine Neudeutung erfahren, denn die traditionelle vedische Reinkarnationslehre basierte auf der Vorstellung einer Seelenwanderung. Wiedergeburt bedeutet im Buddhismus aber nicht individuelle Fortdauer eines dauerhaften Wesenskernes, auch nicht Weiterwandern eines Bewusstseins nach dem Tode. Vielmehr sind es unpersönliche karmische Impulse, die von einer Existenz ausstrahlend eine spätere Existenzform mitprägen.
Bodhi ist der Vorgang des „Erwachens“, oft ungenau mit dem unbuddhistischen Begriff „Erleuchtung“ wiedergegeben. Voraussetzungen sind das vollständige Begreifen der „Vier edlen Wahrheiten“, die Überwindung aller an das Dasein bindenden Bedürfnisse und Täuschungen und somit das Vergehen aller karmischen Kräfte. Durch Bodhi wird der Kreislauf des Lebens und des Leidens (Samsara) verlassen und Nirwana erlangt.
Die buddhistische Tradition kennt drei Arten von Bodhi:
Nirwana (Sanskrit) bzw. Nibbana (Pali) bezeichnet die höchste Verwirklichungsstufe des Bewusstseins, in der jede Ich-Anhaftung und alle Vorstellungen/Konzepte erloschen sind. Nirwana kann mit Worten nicht beschrieben, es kann nur erlebt und erfahren werden als Folge intensiver meditativer Übung und anhaltender Achtsamkeitspraxis. Es ist weder ein Ort – also nicht vergleichbar mit Paradies-Vorstellungen anderer Religionen – noch eine Art Himmel und auch keine Seligkeit in einem Jenseits. Nirwana ist auch kein nihilistisches Konzept, kein „Nichts“, wie westliche Interpreten in den Anfängen der Buddhismusrezeption glaubten, sondern beschreibt die vom Bewusstsein erfahrbare Dimension des Letztendlichen. Der Buddha selbst lebte und unterrichtete noch 45 Jahre, nachdem er Nirwana erreicht hatte. Das endgültige Aufgehen oder „Verlöschen“ im Nirwana nach dem Tod wird als Parinirvana bezeichnet.
Weder das rein intellektuelle Erfassen der Buddha-Lehre noch das Befolgen ihrer ethischen Richtlinien allein reicht für eine erfolgreiche Praxis aus. Im Zentrum des Buddha-Dharma stehen daher Meditation und Achtsamkeitspraxis. Von der Atembeobachtung über die Liebende-Güte-Meditation (metta), Mantra-Rezitationen, Gehmeditation, Visualisierungen bis hin zu thematisch ausgerichteten Kontemplationen haben die regionalen buddhistischen Schulen eine Vielzahl von Meditationsformen entwickelt. Ziele der Meditation sind vor allem die Sammlung und Beruhigung des Geistes (samatha), das Trainieren klar-bewusster Wahrnehmung, des „tiefen Sehens“ (vipassana), das Kultivieren von Mitgefühl mit allen Wesen, die Schulung der Achtsamkeit sowie die schrittweise Auflösung der leidvollen Ich-Verhaftung.
Achtsamkeit (auch Bewusstheit, Vergegenwärtigung) ist die Übung, ganz im Hier und Jetzt zu verweilen, alles Gegenwärtige klarbewusst und nicht wertend wahrzunehmen. Diese Hinwendung zum momentanen Augenblick erfordert volle Wachheit, ganze Präsenz und eine nicht nachlassende Aufmerksamkeit für alle im Moment auftauchenden körperlichen und geistigen Phänomene.
Es gibt drei Hauptrichtungen des Buddhismus: Hinayana („Kleines Fahrzeug“), aus dessen Tradition heute nur noch die Form des Theravada („Lehre der Älteren“) existiert, Mahayana („Großes Fahrzeug“) und Vajrayana (im Westen meist als Tibetischer Buddhismus bekannt oder irreführenderweise als „Lamaismus“ bezeichnet). In allen drei Fahrzeugen sind die monastischen Orden Hauptträger der Lehre und für deren Weitergabe an die folgenden Generationen verantwortlich. Üblicherweise gilt auch der Vajrayana als Teil des großen Fahrzeugs. Der Begriff Hinayana wurde und wird von den Anhängern der ihm zugehörigen Schulen abgelehnt, da er dem Mahayana entstammt.
Theravada bedeutet wörtlich „Lehre der Ordens-Älteren“ und geht auf diejenigen Mönche zurück, welche die Lehrreden noch direkt vom Buddha gehört haben, z. B. Ananda, Kassapa, Upali. Der Theravada-Buddhismus ist die einzige noch bestehende Schule der verschiedenen Richtungen des Hinayana. Seine Tradition bezieht sich in ihrer Praxis und Lehre ausschließlich auf die ältesten erhaltenen Schriften der buddhistischen Überlieferung, die im Tipitaka (Pali) (auch Tripitaka (Sanskrit) oder Pali-Kanon), zusammengefasst sind. Dieser „Dreikorb“ (Pitaka: Korb) besteht aus folgenden Teilen:
Die Betonung liegt im Theravada auf dem Befreiungsweg des einzelnen aus eigener Kraft nach dem Arhat-Ideal und der Aufrechterhaltung und Förderung des Sangha. Theravada ist vor allem in den Ländern Süd- und Südostasiens (Sri Lanka, Myanmar, Thailand, Laos und Kambodscha) verbreitet.
Der Hinayana-Buddhismus (Sanskrit, n., हीनयान, hīnayāna, „kleines Fahrzeug“) bezeichnet einen der beiden großen Hauptströme des Buddhismus. Hinayana ist älter als die andere Hauptrichtung, der Mahayana. Im Hinayana strebt ein Mensch nach dem Erwachen, um selbst nicht mehr leiden zu müssen. Hinayana bezieht sich also nur auf eine Person, die danach strebt, vollkommen zu sein. In diesem Aspekt unterscheidet er sich vom Mahayana, in dem versucht wird, auch andere Lebewesen zum Erwachen zu führen.
Der Mahayana-Buddhismus („großes Fahrzeug“) geht im Kern auf die Mahasanghika („große Gemeinde“) zurück, eine Tradition, die sich in der Folge des zweiten buddhistischen Konzils (etwa 100 Jahre nach dem Tod des Buddha) entwickelt hatte. Der Mahayana verwendet neben dem Tripitaka auch eine Reihe ursprünglich in Sanskrit abgefasster Schriften („Sutras“), die zusammen den Sanskrit-Kanon bilden. Zu den bedeutendsten Texten gehören das Diamant-Sutra, das Herz-Sutra, das Lotos-Sutra und die Sutras vom reinen Land.[37] Ein Teil dieser Schriften ist heute nur noch in chinesischen oder tibetischen Übersetzungen erhalten.
Im Unterschied zur Theravada-Tradition, in der das Erreichen von Bodhi durch eigenes Bemühen im Vordergrund steht, nimmt im Mahayana das Bodhisattva-Ideal eine zentrale Rolle ein. Bodhisattvas sind Wesen, die als Menschen bereits Bodhi erfuhren, jedoch auf das Eingehen in das Parinirvana verzichteten, um stattdessen allen anderen Menschen, letztlich allen Wesen, zu helfen, ebenfalls dieses Ziel zu erreichen.
Bedeutende Schulen des Mahayana sind beispielsweise die des Zen-Buddhismus, des Nichiren-Buddhismus und des Amitabha-Buddhismus.
Vajrayana („Diamantfahrzeug“) ist eigentlich ein Teil des Mahayana. Im Westen ist er meist fälschlicherweise nur als Tibetischer Buddhismus oder als Lamaismus bekannt, tatsächlich ist er jedoch eine Sammelbezeichnung für verschiedene Schulen, die außer in Tibet auch in Japan, China und der Mongolei (geschichtlich auch in Indien und Südostasien) verbreitet sind.
Er beruht auf den philosophischen Grundlagen des Mahayana, ergänzt diese aber um tantrische Techniken, die den Pfad zum Erwachen deutlich beschleunigen sollen. Zu diesen Techniken gehören neben der Meditation unter anderem Visualisierung (geistige Projektion), das Rezitieren von Mantras und weitere tantrische Übungen, zu denen Rituale, Einweihungen und Guruyoga (Einswerden mit dem Geist des Lehrers) gehören.
Diese Seite des Mahayana legt besonderen Wert auf geheime Rituale, Schriften und Praktiken, welche die Praktizierenden nur schrittweise erlernen. Daher wird Vajrayana innerhalb des Mahayana auch „esoterische Lehre“ genannt, in Abgrenzung von „exoterischen Lehren“, also öffentlich zugänglichen Praktiken wie dem Nenbutsu des Amitabha-Buddhismus.
Der tibetische Buddhismus legt besonderen Wert auf direkte Übertragung von Unterweisungen von Lehrer zu Schüler. Eine wichtige Autorität des tibetischen Buddhismus ist der Dalai Lama.
Die vier Hauptschulen des Tibetischen Buddhismus sind:
Der Tibetische Buddhismus ist heute in Tibet, Bhutan, Nepal, Indien (Ladakh, Sikkim), der Mongolei und Teilen Russlands (Burjatien, Kalmückien, Tuwa, Republik Altai) verbreitet.
Etwa im 9. Jahrhundert verbreitete sich der Vajrayana auch in China. Als eigene Schule hielt er sich nicht, hatte aber Einfluss auf andere Lehrtraditionen dort. Erst in der Qing-Zeit wurde der Vajrayana der Mandschu unter Förderung der tibetischen Richtungen wieder eine staatliche Religion.
Er wurde noch im gleichen Jahrhundert seiner Einführung in China nach Japan übertragen. Dort wird Vajrayana in der Shingon-Schule gelehrt. Mikkyō (jap. Übersetzung von Mizong) hatte aber Einfluss auf Tendai und alle späteren Hauptrichtungen des japanischen Buddhismus.
Buddhistische Zeremonien, Feste und Feiertage werden auf unterschiedliche Art und Weise zelebriert. Einige werden in Form einer Puja gefeiert, was im Christentum etwa einer Andacht – ergänzt durch eine Verdienstübertragung – entsprechen würde. Andere Feste sind um zentrale Straßenprozessionen herum organisiert. Diese können dann auch Volksfest-Charakter mit allen dazugehörigen Elementen wie Verkaufsständen und Feuerwerk annehmen. In Japan zum Beispiel werden sie dann Matsuris genannt. Die Termine für die Feste richteten sich ursprünglich hauptsächlich nach dem Lunisolarkalender. Heute sind dagegen einige auf ein festes Datum im Sonnenkalender festgelegt.

Das Wort Mann bezeichnet einen männlichen erwachsenen Menschen und bezieht sich auf das biologische Geschlecht, im modernen Sprachgebrauch auch auf die Geschlechtsidentität. Männliche Kinder und Jugendliche werden als Jungen bezeichnet (auch Knaben, Buben). Die höfliche Anrede für einen Mann lautet im Deutschen Herr, gefolgt vom Familiennamen des Angesprochenen; entsprechend wird in einigen Sportarten zwischen „Damen- und Herrensport“ unterschieden.
Die Entwicklung des biologischen Geschlechts ist genetisch bedingt durch ein Chromosomenpaar XY, wobei vor allem durch das Y-Chromosom sowie das männliche Sexualhormon Testosteron die Entwicklung männlicher primärer und sekundärer Geschlechtsmerkmale gesteuert wird. Männer produzieren Spermien, mit denen Eizellen befruchtet werden können. Sie sind im Gegensatz zu Frauen mit typischer genetischer Entwicklung in keiner Phase ihres Lebens in der Lage, schwanger zu werden. Zudem gibt es transgender Männer, deren Geschlechtsidentität von dem ihnen nach der Geburt zugewiesenen Geschlecht abweicht, sowie intergeschlechtliche Personen mit Geschlechtsmerkmalen, die nicht mit der geschlechtsordnenden Unterscheidung in „Mann“ oder „Frau“ übereinstimmen.
Aufgrund der grundlegenden Einflüsse der Geschlechtlichkeit auf die menschliche Gesellschaft ist die Bezeichnung Mann mit vielen weiteren, miteinander verknüpften und teilweise sehr gefühlsbetonten Bedeutungen beladen (siehe auch Männlichkeit). Als Symbol für den Mann, das männliche Geschlecht und die Männlichkeit wird allgemein das Marssymbol ♂ verwendet (ein Schild mit Speer).
Das gemeingerm. Wort mhd., ahd. man geht auf idg. manu- oder monu- „Mensch, Mann“ zurück. Welche Vorstellung dieser Benennung des Menschen zugrunde liegt, ist nicht sicher zu klären.[1]
Aus molekularbiologischer Sicht unterscheidet sich der Mann von der Frau durch das Chromosomenpaar XY (siehe jedoch auch XX-Mann) in den Geschlechtschromosomen (statt XX bei der Frau, siehe aber auch XY-Frau). Dieser Unterschied führt zu einem Geschlechtsdimorphismus. Durch das Zusammentreffen eines X-Chromosoms von mütterlicher Seite (Eizelle) und eines Y-Chromosoms von väterlicher Seite (Spermium) in der Zygote entsteht dieser in Bezug auf die Ausbildung der Geschlechtsorgane schon während der Embryonalentwicklung. Männer besitzen in ihrem Chromosomensatz also in der Regel je ein X- und das geschlechtsbestimmende Y-Chromosom. Letzteres enthält eine Region, die man Sex determining region of Y (SRY) nennt und die beim Mann für die embryonale Produktion des Hoden-determinierenden Faktors (TDF für englisch: Testis-determining factor), eines Proteins, verantwortlich ist. Wird TDF gebildet, kommt es zur Ausbildung männlicher Merkmale, fehlt es dagegen, bilden sich weibliche Merkmale.
Männer unterscheiden sich körperlich von Frauen durch die unterschiedlichen primären und sekundären Geschlechtsmerkmale. Der Bau der primären Geschlechtsmerkmale ist hauptsächlich durch die Funktion bei der Fortpflanzung bedingt. Dabei handelt es sich vor allem um
Zu fehlenden Geschlechtsmerkmalen und Mehrgeschlechtlichkeit siehe Intersexualität und Transgender.
Weibliche und männliche Becken unterscheiden sich. Das Hüftbeinloch hat bei Frauen eine ovale Form, und die Beckenschaufeln sind breiter. Das männliche Becken dagegen ist eher hoch, schmal und eng. Das wichtigste Merkmal zur Unterscheidung ist der Winkel der Schambeinfuge. Er ist beim weiblichen Becken größer als 90° und beim männlichen kleiner als 90°.
Ebenso unterscheiden sich die Schädelformen. Nur Männerschädel weisen über den Augen eine deutlich ausgeprägte Wulst auf.
Die Fingerproportionen von Männern und Frauen sind unterschiedlich. Bei den meisten Männern ist der Ringfinger länger als ihr Zeigefinger. Bei Frauen ist hingegen der Zeigefinger länger als der Ringfinger oder zumindest gleich lang. Eine gängige Hypothese sieht die Ursache für das unterschiedliche Fingerlängenverhältnis im Testosteronspiegel im Mutterleib.[2]
Das Erscheinungsbild der männlichen Haut (fettiger und großporiger als weibliche Haut) wird vorwiegend durch hormonell bedingte Hautunterschiede beeinflusst, z. B. durch die Hautdicke und die erhöhte Talgdrüsensekretion.
Die dickere männliche Haut hat ein höheres Wasserbindungsvermögen, was die Haut gespannter und fester aussehen lässt. Die erhöhte Talgproduktion ist verantwortlich für eine ausreichende Menge an Feuchtigkeit in der Haut und für die Zusammensetzung des sogenannten Hydrolipidfilms. Dieser Film regelt den Wassergehalt der tiefer liegenden Schichten, hemmt die Austrocknung und gibt der Haut ein glattes, geschmeidiges Aussehen.
Zudem hat Männerhaut eine geringere Neigung zur Faltenbildung. Falten zeigen sich beim Mann meist später als bei Frauen und auch nicht als kleine Knitterfältchen, sondern mehr als tiefe („markante“) Falten.
Infolge der erhöhten Talgproduktion kann es vor allem bei jungen Männern öfter zu Mitessern und Akne kommen. Diese entstehen, wenn sich der Talg zusammen mit abgestorbenen Hautzellen an den Poren festsetzt. Eine Reizung der Mitesser führt zur Ansiedlung von Keimen und entzündlichen Pickeln.
Die Fachrichtung der Medizin, die sich mit den Erkrankungen des Mannes befasst, heißt Andrologie. Allgemein können Männer von den gleichen Krankheiten befallen werden wie Frauen, es existieren jedoch auch einige geschlechtsspezifische Erkrankungen; für einige weitere Erkrankungen zeigen Frauen und Männer deutlich unterschiedliche Krankheitsverteilungen (etwa für die Rot-Grün-Sehschwäche und andere X-chromosomale Erbkrankheiten).
Im Deutschen wird als Junge (in Österreich, Bayern und der Deutschschweiz: Bub; veraltet auch: Knabe) übergreifend – wenn auch nicht sehr strikt darauf beschränkt – eine männliche Person vor dem Erreichen der Volljährigkeit bezeichnet. In den meisten Gesellschaften wird das Überschreiten der Grenze vom Jungen zum Mann (häufig gleichgesetzt mit der Geschlechtsreife) als wichtiger Schritt im Leben eines männlichen Individuums angesehen und oft mit Initiationsriten unterschiedlichster Form zelebriert (als Beispiele sind hier etwa Beschneidungen oder Subinzision bei verschiedenen indigenen Völkern, die Firmung respektive Konfirmation in den christlichen Religionen, die Bar Mizwa im Judentum, die Jugendweihe bei den Freidenkern und in der DDR, aber auch einfach das Feiern des achtzehnten Geburtstages zu nennen), die jedoch in der „modernen“ westlichen Gesellschaft eher symbolischen Wert haben und in ihrer Funktion durch die Volljährigkeit ersetzt wurden. Mit Erreichen dieser Grenze werden den jungen Männern, ebenso wie den jungen Frauen, neue Rechte wie etwa die Heirats- und volle Geschäftsfähigkeit und Pflichten wie z. B. die Verantwortung für elterliches Haus und Hof verliehen.
Die Soziologie dokumentiert diverse Unterschiede (Geschlechterrollen) im Verhalten von Männern und Frauen in der menschlichen Gesellschaft sowie signifikante Unterschiede in den Rollenverteilungen (Männlichkeit, Weiblichkeit).
Die Bandbreite verschiedener Verhaltensweisen von Männern und Frauen ist sehr groß und oftmals nicht eindeutig oder nur unscharf zu belegen. Männern wird als Beispiel eine größere Durchsetzungskraft und höheres Wettbewerbs-, aber auch Aggressionspotential nachgesagt, was auf Testosteron zurückgeführt wird. Bei Frauen hingegen seien häufig die sozialen Fähigkeiten stärker ausgeprägt. In den westlichen Kulturen hatte der soziale Status des Mannes für Frauen höhere Bedeutung als umgekehrt, was jedoch in der Gegenwart nicht mehr allgemein gültig ist. Der Verhaltensforscher Karl Grammer formuliert: „Attraktivität wird für Status verkauft“.[3]
Von der Annahme ausgehend, dass Natur nicht unabhängig von Kultur gedacht werden kann und dass Biologisches daher nur im Diskurs, also als Sozial-Kulturelles zu haben ist, zog die Philosophin Judith Butler in einflussreichen Beiträgen radikal in Zweifel, dass das (ausschließlich zweigeteilte) soziale Geschlecht (von Mann oder Frau) eine Widerspiegelung oder ursächliche Folge einer körperlich-organischen Ausstattung sei.[4]
Das Wort Mann findet sich in Wanders Deutschem Sprichwörter-Lexikon immerhin in knapp zweitausend Sprichwörtern (Band 3; Seiten 362–446), sieht man einmal davon ab, dass es weitere Sprichwörter mit Männchen und zusammengesetzten Wörtern (Männerwitz, Mannesbürde usw.) gibt. Gegenüber den Wörtern Bube und Mädchen ist diese Anzahl enorm höher; Frau gibt es knapp tausend Mal in Sprichwörtern bei Karl Friedrich Wilhelm Wander, Weib allerdings auch in 1434 Sprichwörtern (abgesehen von vielen weiteren mit Weib zusammengesetzten Wörtern). Gemäß dem Deutschen Wörterbuch der Gebrüder Grimm lautet der Plural im Mittelhochdeutschen noch auf "die Mann", nicht "die Männer". Es ist aber demnach nicht dieser Plural, der sich in auch aktuelleren Wendungen wie "Drei Mann in einem Boot", "Alle Mann von Bord" oder auch "Alle Mann an Deck" wiederfindet, sondern ein Singular, dazu bestimmt, den Einzelnen innerhalb der Gesamtzahl hervorzuheben.
Das Symbol für einen Mann ist ♂, das Marssymbol. Männliche Säuglinge werden seit dem ersten Drittel des 20. Jahrhunderts in westlichen Kulturen oft mit der Farbe Blau in Verbindung gebracht, im Gegensatz zu Rosa für weibliche Säuglinge.

Der Sozialismus (von lateinisch socialis ‚kameradschaftlich‘) ist eine der im 19. Jahrhundert entstandenen drei großen politischen Ideologien neben dem Liberalismus und Konservatismus. Es gibt keine eindeutige Definition des Begriffs. Er umfasst eine breite Palette von politischen Ausrichtungen. Diese reichen über sich als revolutionär verstehende (Kampf-)Bewegungen und Parteien, die den Kapitalismus schnell und gewaltsam überwinden wollen, bis zu reformatorischen Linien, die Parlamentarismus und Demokratie akzeptieren (demokratischer Sozialismus). Demzufolge wird auch grob zwischen den Ausrichtungen von Kommunismus, Sozialdemokratie oder Anarchismus differenziert. Sozialisten betonen im Allgemeinen die Grundwerte Gleichheit, Gerechtigkeit, Solidarität und je nach Strömung auch Freiheit.[1][2][3][4][5] Sie heben oft die enge Wechselbeziehung zwischen praktischen sozialen Bewegungen und theoretischer Gesellschaftskritik hervor, wobei sie das Ziel verfolgen, mit Blick auf eine sozial gerechte Wirtschafts- und Sozialordnung beide zu versöhnen.
Historisch bestehen und bestanden in vielen Staaten Systeme, die – teils als Eigenbezeichnung – mit Realsozialismus, aber auch als Staatssozialismus bezeichnet werden und sich grundsätzlich als autoritäre oder als totalitäre Systeme einordnen lassen; zu nennen sind u. a. die Sowjetunion, Volksrepublik China, Nordkorea, die DDR oder Kuba. Daneben existierten und existieren auch weitere sich als sozialistisch bezeichnende oder so bezeichnete Staaten, die sich allerdings teilweise erheblich von den realsozialistischen Staaten unterscheiden (siehe Liste sozialistischer Staaten).
Als socialistae (lateinisch) oder socialisti (italienisch) wurden im 18. Jahrhundert von römisch-katholischen Theologen, die der Aufklärung kritisch bis ablehnend gegenüberstanden, polemisch die Vertreter des modernen Naturrechts in der Art von Hugo Grotius und Samuel von Pufendorf benannt.[6] 1762 verfasste Jean-Jacques Rousseau sein Werk Du contrat social, in dem der Staat auf dem Kontrakt (Vertrag) freier Individuen beruht. Seit 1793 wird in Deutschland für Anhänger des Pufendorfschen Solidaritätsprinzips der rechtsphilosophische Terminus „Sozialisten“ verwendet.[7]
Erstmals findet sich das Wort Sozialismus 1803 in der italienischen Form socialismo. Giacomo Giuliani verwendet diesen Begriff in seiner Kritik an Rousseau positiv auf die Gesellschaftsordnung, allerdings indem er es für den göttlichen Willen hielt, dass die Gesellschaft durch Hierarchien zwischen den Menschen gekennzeichnet sei. Eine solche religiöse Umdeutung wurde jedoch stark kritisiert, weil der Begriff mit dem Liberalismus der Aufklärung in ursächlichem Zusammenhang gesehen wurde.[8]
Die ersten Nachweise der Verwendung des Worts socialist im Englischen fand man im Jahre 1824, das eigentliche französische socialisme erstmals 1832,[9] geprägt von Joncières, weiter verbreitet von Leroux und Reybaud.[10]
Eine Übertragung des ursprünglichen Begriffsadjektivs sozial in die heutige, deutsche Gesellschaftssprache ist in der Nähe von gemeinsam, gerecht oder etwa gesellschaftlich zumutbar, der Gemeinschaft zuträglich zu suchen.
Das Adjektiv sozialistisch dagegen wurde von Anfang an politisch verstanden. Es ist gesellschaftlich gesehen eine Weiterentwicklung der sozialen Gedanken der Aufklärung insofern, als diese Gleichheit nicht nur dem Recht, sondern auch dem Besitz zugestanden werden soll.
Was unter Sozialismus zu verstehen sei, ist seit langem umstritten. Schon in den 1920er Jahren sammelte der Soziologe Werner Sombart 260 Definitionen von Sozialismus.[11]
Eine allgemein anerkannte, wissenschaftlich gültige Definition existiert nicht. Vielmehr zeichnet sich der Wortgebrauch durch eine große Bedeutungsfülle und begriffliche Unschärfe aus und unterliegt einem ständigen Bedeutungswandel. Deswegen werden dem Begriff zur näheren Präzisierung häufig Adjektive (proletarisch, wissenschaftlich, demokratisch, christlich, genossenschaftlich, konservativ, utopisch) vorangesetzt. Weitere Beispiele für solche Spezifizierungen sind etwa Agrarsozialismus, Staatssozialismus oder Reformsozialismus.[12]
Einen kleinsten gemeinsamen Nenner des Begriffs können folgende Definitionen geben:
„Sozialismus bezieht sich auf ein weites Spektrum ökonomischer Theorien sozialer Organisation, welche sich kollektiven Besitz und politische Administration zum Ziel der Schaffung einer egalitären Gesellschaft zum Ziel gesetzt haben.“[13]
„Sozialismus bezeichnet Ideologien, welche die Überwindung des Kapitalismus und die Befreiung der Arbeiterklasse aus Armut und Unterdrückung (soziale Frage) zugunsten einer an Gleichheit, Solidarität und Emanzipation orientierten Gesellschaftsordnung propagieren.“[14]
„Er definiert die als Gegenmodell zum Kapitalismus entwickelte politische Lehre, die bestehende gesellschaftliche Verhältnisse mit dem Ziel sozialer Gleichheit und Gerechtigkeit verändern will, und eine nach diesen Prinzipien organisierte Gesellschaftsordnung sowie eine politische Bewegung, die diese Gesellschaftsordnung anstrebt.“[15]
Die Bedeutungsvielfalt wird zusätzlich dadurch gesteigert, da der Begriff Sozialismus sowohl Methoden und Zielvorstellungen, gesellschaftlich-politische Bewegungen als auch historisch-gesellschaftliche Phasen und existierende Gesellschaftssysteme bezeichnen kann:
Nach dem Politikwissenschaftler Günter Rieger lassen sich sozialistische Ideologien zum einen nach ihrer Haltung zum Staat unterscheiden (Staatssozialismus versus Anarchismus), zum anderen nach dem Weg, auf dem die angestrebte Umgestaltung der Gesellschaft erreicht werden soll (Revolution versus Reform), sowie drittens danach, welcher Stellenwert unterschiedlichen sozialen und ökonomischen Interessen der Beteiligten eingeräumt wird (Klassenantagonismus versus Pluralismus).[17]
Eine explizit sozialistische Bewegung entwickelte sich erst infolge von Aufklärung und industrieller Revolution zwischen Ende des 18. Jahrhunderts und Mitte des 19. Jahrhunderts. Sie war eng verwoben mit der Entstehung der Arbeiterbewegung. Wie bei allen -ismen trat der Sozialismus historisch in vielfältigen Formen auf: von den genossenschaftlichen Ideen der Frühsozialisten über die parteipolitische Organisation in sozialdemokratischen, sozialistischen und danach kommunistischen Parteien, die im Verlauf des 20. Jahrhunderts oft unterschiedliche Ausprägungen annahmen.
Frühsozialisten wie François Noël Babeuf, Claude-Henri Comte de Saint-Simon, Louis-Auguste Blanqui, Charles Fourier, Pierre-Joseph Proudhon, William Godwin, Robert Owen oder Moses Hess legten politische Konzepte von quasi-absolutistischen Diktaturen bis hin zu einem anarchistischen Föderalismus vor. Einig waren sie sich einerseits in einer abwehrenden Reaktion gegen Effekte des Frühkapitalismus wie in der Hoffnung auf eine Gesellschaft, die mittelalterliche Standesunterschiede ebenso überwinden würde wie neuere Klassengegensätze. Oftmals argumentierten sie sehr moralisch. Eine sozialwissenschaftlich inspirierte Analyse, wie sie von Karl Marx geleistet wurde, gab es noch nicht.
Sozialstrukturell gesehen wurde der Frühsozialismus nicht von der Arbeiterklasse getragen, sondern von Handwerkern und Kleinbürgertum. Diese begannen bereits die Verwerfungen der industriellen Revolution zu spüren, ohne dass es schon zur Bildung eines Industrieproletariats gekommen wäre.
Einige wie Robert Owen versuchten den Aufbau abgeschlossener sozialistischer Gemeinschaften in einer so empfundenen feindlichen Umwelt. Die meisten Sozialisten zielten auf eine grundlegende Veränderung der gesamten Gesellschaft.
Sozialistisch inspirierte Aktivisten beteiligten sich an der französischen Revolution von 1789 bis 1799 und an den im Wesentlichen als bürgerlich geltenden europäischen Revolutionen bis 1848/1849 (siehe Julirevolution 1830, Februarrevolution 1848 und Märzrevolution 1848/1849); einen letzten Höhepunkt im 19. Jahrhundert hatten diese frühsozialistischen Bewegungen in der Pariser Kommune von 1871, die als erste proletarische Revolution gilt und die schon nach kurzer Zeit blutig niedergeschlagen wurde.
Durch die historische Entwicklung bedingt wurden die Diskussionslinien danach klarer: Die vielfältigen Ansätze des Frühsozialismus spalteten sich in drei Hauptlinien, den Anarchismus und die vom Marxismus inspirierten kommunistischen und sozialdemokratischen Bewegungen. Vereinzelt, wie im 20. Jahrhundert bei den russischen Revolutionen von 1905 und der Februarrevolution 1917 (bei der Oktoberrevolution 1917 nur noch sehr bedingt), der Münchner Räterepublik 1919 oder dem Spanischen Bürgerkrieg 1936 bis 1939 kam es zur Zusammenarbeit der drei Gruppen. Diese war jedoch jeweils nur kurzfristig, meist von heftigen internen Auseinandersetzungen geprägt und endete im Sieg einer Gruppe oder der Niederlage aller.
Auch die Anarchisten verstanden sich in sozialistischer Tradition:
„Was im Juni 1848 unterlag, war nicht der Sozialismus im Allgemeinen, nur der Staatssozialismus, der autoritäre und reglementmäßige Sozialismus, der geglaubt und gehofft hatte, dass der Staat den Bedürfnissen und legitimen Wünschen der Arbeiterklasse volle Befriedigung gewähren werde und mit seiner Allmacht eine neue soziale Ordnung einführen wolle und könne.“[18]
Die Theorie des Anarchismus lehnt daher staatliche Strukturen als Herrschaftsinstrument ab. Der Anarchismus baut auf die freiwillige Verbindung der Individuen in Kollektiven, Räten und Kommunen, um dieselben Ziele zu erreichen. Der Anarchismus strebt eine Synthese zwischen individueller Freiheit und kollektiver Verantwortung an und unterscheidet sich von den autoritären Strömungen. Statt des Staates wird beispielsweise von Bakunin vorgeschlagen:
„Die Gesellschaft so zu organisieren, dass jedes auf die Welt kommende männliche oder weibliche Wesen ungefähr gleiche Mittel zur Entwicklung seiner Fähigkeiten und ihrer Nutzbarmachung durch die Arbeit vorfindet…“[19]
Die Bewegung des Religiösen Sozialismus entstand mit der erstarkenden Arbeiterbewegung in Mitteleuropa seit dem 19. Jahrhundert vor allem unter sozial engagierten Christen, zum Teil auch Juden.
Dass der Sozialismus, der den demokratischen Radikalismus der deutschen Handwerker, Arbeiter und Intellektuellen ablöste, sich als religiöser Sozialismus konstituierte, ist entscheidend auf den Schneidergesellen Wilhelm Weitling, das Haupt der Bewegung zu Beginn der 1840er Jahre, zurückzuführen. Seine sozialistische, am Ideal der Gütergemeinschaft orientierte Gesellschaftsutopie begründete Weitling in der Schrift Die Menschheit wie sie ist und sein sollte 1839/40, aber auch noch in seinem Evangelium eines armen Sünders 1843 überwiegend christlich-religiös.[20][21]
Besonders seit der Erfahrung des Ersten Weltkriegs gewann unter Juden die Überzeugung an Boden, dass dauerhafter Frieden entsprechend der Tora und dem Evangelium nur verwirklicht werden könne, wenn der auf Egoismus, Konkurrenz und Ausbeutung gegründete Kapitalismus überwunden werde.
Hermann Samuel Reimarus, Karl Kautsky, R. Eisler, Samuel George Frederick Brandon, und andere beriefen sich in ihrem „sozialen und politischen Kampf gegen bestehende Ordnungen“ auf Person und Handeln Jesu, und betonten seine Nähe zur Bewegung der Zeloten.[22]
Andere wie z. B. der Theologe Hans Küng, halten eine Inanspruchnahme Jesu für sozialrevolutionäre Bestrebungen für konstruiert.[23]
Laut Friedrich Engels bedeutete Sozialismus noch 1847 eine Bourgeoisbewegung, Kommunismus indes eine Arbeiterbewegung (Cabet, Weitling), weswegen Karl Marx und Engels damals noch der Bezeichnung „Kommunisten“ den Vorzug gaben. Erst 1887 bekannten sich sogar die englischen Gewerkschaften zum Sozialismus.[24]
Der Marxismus hatte lange Zeit die Deutungshoheit in der sozialistischen Bewegung. Nach dem Verfall der ersten Internationale 1876 bis über den größten Teil des gesamten 20. Jahrhunderts hinweg wurden Diskussionen innerhalb des und über den Sozialismus überwiegend mit den von Marx und Engels geprägten Begriffen geführt.
Marx und Engels betrachteten den Frühsozialismus als Utopischen Sozialismus und stellten ihm den wissenschaftlichen Sozialismus gegenüber. Nach der Theorie von Marx und Engels stehen sich in der Epoche des Kapitalismus die Kapitalistenklasse (Privateigentümer auf Produktionsmittel) und die Arbeiterklasse (Proletariat) als Gegenspieler gegenüber. Die Arbeiter seien gezwungen ihre Arbeitskraft an die Kapitalisten zu verkaufen. Der jeweilige Kapitalist stelle die Arbeiter als Lohnabhängige ein und profitiere von deren Arbeit, weil er den Arbeitern immer nur einen Teil des durch ihre Arbeit erwirtschafteten Geldes auszahle, den Rest behalte er für sich. Demnach entstehe Ausbeutung. Die verschiedenen Interessen der beiden Klassen würden sich in einem stetigen Widerstreit befinden, also in einem Klassenkampf. Die Zuspitzung dieses Widerstreits würde es nach Marx und Engels erforderlich machen, dass die organisierte Arbeiterklasse die Macht erobern müsse, um sich selbst zu befreien.[25] Nach Marx ist die Diktatur des Proletariats mit ihrer Aufgabe die Aufhebung des Privateigentums an Produktionsmitteln die Voraussetzung der klassenlosen Gesellschaft (Kommunismus). Nach Friedrich Engels wird diese Diktatur eine demokratische Herrschaft der Mehrheit über die Reste der Ausbeuterklasse sein. Marx und er forderten Verstaatlichungen aller Produktionsmittel, zum Beispiel im Manifest der Kommunistischen Partei:
„Das Proletariat wird seine politische Herrschaft dazu benutzen, der Bourgeoisie nach und nach alles Kapital zu entreißen, alle Produktionsinstrumente in den Händen des Staats, d. h. des als herrschende Klasse organisierten Proletariats, zu zentralisieren und die Masse der Produktionskräfte möglichst rasch zu vermehren.“[26]
 Wie die Gesellschaftsform nach der Entwicklung vom Sozialismus zum Kommunismus, also der klassenlosen Gesellschaft, genauer aussehen werde, wurde von Marx und Engels bewusst nicht genauer ausgemalt und werde sich der Theorie folgend anhand konkreter gesellschaftlicher Entwicklungen und Widersprüche zeigen.
Zwei bekannte Zitate, die sich um die Entwicklung zur höheren Phase der kommunistischen Gesellschaft drehen:
„In einer höheren Phase der kommunistischen Gesellschaft, nachdem die knechtende Unterordnung der Individuen unter die Teilung der Arbeit, damit auch der Gegensatz geistiger und körperlicher Arbeit verschwunden ist; nachdem die Arbeit nicht nur Mittel zum Leben, sondern selbst das erste Lebensbedürfnis geworden; nachdem mit der allseitigen Entwicklung der Individuen auch ihre Produktivkräfte gewachsen und alle Springquellen des genossenschaftlichen Reichtums voller fließen – erst dann kann der enge bürgerliche Rechtshorizont ganz überschritten werden und die Gesellschaft auf ihre Fahne schreiben: Jeder nach seinen Fähigkeiten, jedem nach seinen Bedürfnissen!“[27]
„Sobald es keine Gesellschaftsklasse mehr in der Unterdrückung zu halten gibt, sobald mit der Klassenherrschaft und dem in der bisherigen Anarchie der Produktion begründeten Kampf ums Einzeldasein auch die daraus entspringenden Kollisionen und Exzesse beseitigt sind, gibt es nichts mehr zu reprimieren, das eine besondre Repressionsgewalt, einen Staat, nötig machte. Der erste Akt, worin der Staat wirklich als Repräsentant der ganzen Gesellschaft auftritt – die Besitzergreifung der Produktionsmittel im Namen der Gesellschaft, ist zugleich sein letzter selbständiger Akt als Staat. Das Eingreifen einer Staatsgewalt in gesellschaftliche Verhältnisse wird auf einem Gebiete nach dem andern überflüssig und schläft dann von selbst ein. An die Stelle der Regierung über Personen tritt die Verwaltung von Sachen und die Leitung von Produktionsprozessen. Der Staat wird nicht ‚abgeschafft‘, er stirbt ab.“[28]
Die Phase der Diktatur wurde von Wladimir Iljitsch Lenin als eigenständige Gesellschaftsformation verstanden, die er als Sozialismus bezeichnete. In ihr würden die Proletarier die Produktionsverhältnisse durch Vergesellschaftung der Produktionsmittel so verändern, dass schließlich die Klassengegensätze selbst aufgehoben würden. Der Staat, von Marx als Instrument der Unterdrückung einer Klasse durch die andere gedacht, werde somit überflüssig und sterbe ab, woraus die letzte Gesellschaftsformation der Menschheitsgeschichte möglich werde, der Kommunismus.[29]
Im sogenannten Revisionismusstreit innerhalb der deutschen Sozialdemokratie grenzten sich Marxisten, die auf eine Revolution setzten, von solchen ab, die den Sozialismus auf dem Wege von Reformen herbeiführen wollten. Rosa Luxemburg betonte hierbei die Unumgänglichkeit der Revolution, indem sie zum Beispiel schrieb:
„Für die Sozialdemokratie besteht zwischen der Sozialreform und der sozialen Revolution ein unzertrennlicher Zusammenhang, indem ihr der Kampf um die Sozialreform das Mittel, die soziale Umwälzung aber der Zweck ist.“[30]
Ihr parteiinterner Gegner Eduard Bernstein vertrat die Ansicht, die Sozialdemokratie könne die angestrebte grundlegende Erneuerung der Gesellschaft durch einen beständigen Reformprozess erreichen. Er stellte die Notwendigkeit der proletarischen Revolution in Frage und propagierte die Teilhabe am politischen System des Kaiserreiches. In der Weimarer Republik und den ersten zwanzig Jahren der Bundesrepublik wurde diese Differenzierung durchgehalten.
Als real existierenden Sozialismus bezeichneten sich jene Staaten, die seit 1917 von einer Kommunistischen Partei, in der Regel in einem Ein-Parteien-System, regiert wurden: besonders die Sowjetunion mit der KPdSU und die ab 1945 an ihrem System ausgerichteten Staaten des europäischen „Ostblocks“, darunter: Polen, ČSSR, Ungarn, Bulgarien, Rumänien, Deutsche Demokratische Republik sowie die Mongolische Volksrepublik. Weiterhin bestehen bis heute einige weitere sehr unterschiedliche, sich teilweise widersprechende von manchen als realsozialistisch bezeichnete Systeme wie die Volksrepublik China (seit 1949), im nach dem Vietnamkrieg vereinigten Vietnam (spätestens seit 1975), Laos (seit 1975), Kuba (seit 1959) oder Nordkorea (seit 1948).
Mit der Oktoberrevolution 1917 in Russland sollten die Ideen des Sozialismus erstmals in einem großen Flächenstaat in die Praxis umgesetzt werden. Der Begriff des Realsozialismus sollte erklären, warum viele Vorhersagen der marxschen Theorie wie die Weltrevolution und die rasche Entwicklung größeren Wohlstands in den sozialistischen Staaten nicht eintraten und diese Staaten sich dennoch weiter zum Kommunismus entwickelten, allerdings mit Problemen der Realpolitik zu kämpfen hatten.
Stalin vertrat nach Lenins Tod die Theorie vom möglichen „Sozialismus in einem Land“, der sich unabhängig von der Weltrevolution etablieren und halten könne. Trotzki stellte dagegen seine Theorie der permanenten Revolution auf, um bürokratische Erstarrung einer Sozialrevolution durch erneute innenpolitische Umwälzungen und Revolutionierung weiterer Länder zu verhindern. Nachdem sich Stalin gegen Trotzki durchgesetzt hatte, gab die von ihm beherrschte KP die ursprünglichen Ziele auch der Bolschewiki auf, die eine Demokratisierung nach erfolgreichem Aufbau sozialistischer Produktionsverhältnisse in Aussicht gestellt hatten. Stalins rigorose Zwangsmaßnahmen zur forcierten Industrialisierung, Kollektivierung der Landwirtschaft, ethnischen Homogenisierung und Ausschaltung jeder möglichen Opposition – zusammengefasst als Stalinismus – aber auch die ähnliche Politik seiner Nachfolger und die ständigen schweren Verstöße gegen die Menschenrechte in realsozialistischen Staaten haben diese Systeme weltweit diskreditiert. Die faktisch nationale, diktatorisch-technokratische Machtpolitik und das imperialistische Hegemoniestreben solcher Staaten gefährdete aus Sicht vieler Kritiker alle weiteren Anläufe zu einem von der Sowjetunion oder China unabhängigen Sozialismus.
Realsozialismus wird dabei entweder als logische Konsequenz des marxschen Sozialismusmodells oder als dessen Verkehrung ins Gegenteil kritisiert, sodass viele Kritiker diesen Staaten das Recht absprachen, sich sozialistisch zu nennen.
Seit der Wende und friedlichen Revolution von 1989 gilt der Realsozialismus trotz einiger noch bestehender Systeme dieser Art als historisch gescheitert. Als Hauptursachen für das Scheitern des Realsozialismus sehen viele folgende Entwicklungen:
In der europäischen Sozialdemokratie setzte sich seit etwa 1900 der Reformismus durch, der den Sozialismus nicht durch eine soziale Revolution, sondern durch demokratische Reformen erreichen zu können glaubt. Damit wurden sozialdemokratische Gründungsprogramme, die Sozialismus gemäß der marxschen Theorie vom Klassenkampf als Ergebnis krisenhafter Zuspitzungen der sozialen Gegensätze und revolutionärer Umgestaltungen erwarteten, zuerst in der praktischen Alltagspolitik und dann theoretisch aufgegeben.
In Deutschland begann die Auseinandersetzung um einen revolutionären oder reformistischen Weg zum Sozialismus mit Veröffentlichungen Eduard Bernsteins, die 1896 die Revisionismusdebatte auslösten. Zwar fand Bernsteins Position in der SPD zunächst keine Mehrheit, doch setzte sie sich nach dem Tod des Parteivorsitzenden August Bebel 1913 unter seinem Nachfolger Friedrich Ebert mehr und mehr durch. Hieraus und aus der Zustimmung der SPD-Reichstagsfraktion zu den Kriegsanleihen zur Finanzierung des Ersten Weltkriegs 1914, an dem die Sozialistische Internationale zerbrach, wurden ideologische Auseinandersetzungen innerhalb der Sozialdemokratie manifest, die schließlich zur Spaltung der SPD in USPD und MSPD führte. Sie verschärften sich seit der Oktoberrevolution in Russland 1917. Es kam zu einer Spaltung zwischen Sozialisten und Kommunisten, die eigene kommunistische Parteien gründeten. Der Bruch zwischen beiden Lagern zeigte sich besonders am Verhältnis zum sogenannten Realsozialismus sowjetischer Prägung. Die Anfang 1919 gegründete Kommunistische Partei Deutschlands (KPD) beanspruchte als Nachfolgerin des Spartakusbundes, mit dem proletarischen Internationalismus die besten sozialdemokratischen Traditionen zu bewahren. Mit der Ermordung der Spartakusführer und KPD-Gründer Rosa Luxemburg und Karl Liebknecht wurde die Spaltung der deutschen Arbeiterbewegung in die reformorientierte SPD und die marxistisch-revolutionäre KPD unumkehrbar, während die USPD bis 1922 zwischen diesen beiden Polen zerrieben wurde und danach keine bedeutende Rolle in der Weimarer Republik mehr spielte.
In Russland spaltete sich die Sozialdemokratie schon 1903 in die reformorientierten Menschewiki (= Minderheitler) und die marxistisch-revolutionären Bolschewiki (= Mehrheitler), deren Gegensatz nach vorübergehender neuer Zusammenarbeit 1912 endgültig wurde. Den Menschewiki gelang unter Kerenski mit der Februarrevolution 1917 der Sturz des Zaren und die Regierungsbildung, doch setzten sie den Krieg gegen Deutschland für Gebietsgewinne fort. Die theoretische, nach seiner Rückkehr aus dem Exil 1917 auch die praktische Führung der Bolschewiki übernahm Lenin. Durch das Angebot eines Sofortfriedens gewann er eine Mehrheit im Rätekongress, die er für eine erneute Revolution – diesmal gegen das russische Parlament in Petersburg – nutzte. Nach dem fünfjährigen Russischen Bürgerkrieg gegen verschiedene zarentreue „Weiße Truppen“ (vgl. Weiße Armee) gründeten die Bolschewiki die UdSSR mit der seit 1952 KPdSU genannten alleinherrschenden Staatspartei. Damit verlor die unterlegene russische Sozialdemokratie fast bis zum Ende der Sowjetunion 1990 jede machtpolitische Bedeutung.
Die innersozialistischen Gegensätze in der „Systemfrage“, die in Deutschland zugunsten der Reformisten, in Russland zugunsten der Leninisten ausgegangen waren, vertieften nach dem Rechtsruck der Weimarer Republik ab 1923 die Spaltung zwischen Sozialdemokraten und Kommunisten und schwächten so die Zukunftsperspektiven des Sozialismus weltweit. Obwohl die SPD bis zu ihrem Heidelberger Programm von 1925 am Ziel einer Ablösung der kapitalistischen durch eine sozialistische Wirtschaftsordnung festhielt, ging sie im politischen Alltag den Weg einer Reformpartei, die ihre Ziele parlamentarisch durch Kompromisse und Koalitionen – auch mit gegnerischen Kräften der Gesellschaft – allmählich durchzusetzen suchte. Obwohl sie eine der größten demokratischen Parteien in der ersten deutschen Republik blieb und die meisten Regierungen mittrug, geriet sie bald in die politische Defensive gegenüber deutschnationalen und rechtsradikalen Parteien, bis sie 1933 kurz nach der KPD mit allen übrigen Parteien außer der NSDAP vom neuen Regime des Nationalsozialismus verboten, ihre Führungskräfte verfolgt und ihre Strukturen zerschlagen wurden.
Nach dem Ende der NS-Diktatur konnte die SPD sich regenerieren und griff nun auf sozialistische Ziele zurück, die das Wiedererstarken des Faschismus durch energische Eingriffe in den Monopolkapitalismus verhindern sollten. Doch erst nach ihrer Wende zur Marktwirtschaft im Godesberger Programm 1959 wandelte sie sich zur Volkspartei. Dabei definierte sie „Sozialismus“ nun in ausdrücklicher Abgrenzung vom Sowjetkommunismus als „Demokratischen Sozialismus“, um damit ihre Anerkennung des pluralistischen Systems der westlichen Demokratien zu zeigen. So befreite die SPD sich allmählich aus ihrer Oppositionsrolle und stellte mit Willy Brandt 1969 erstmals den Bundeskanzler der Bundesrepublik Deutschland. Dessen Regierungserklärung versprach „mehr Demokratie“, jedoch keinen Sozialismus im Sinne der alten SPD-Programme mehr.
In der Sowjetischen Besatzungszone war es unter sowjetischem Einfluss zur Zwangsvereinigung der SPD mit der dominierenden KPD zur SED gekommen, die in der DDR von 1949 bis zu deren Niedergang 1989/1990 an der Macht blieb und sich an der KPdSU und dem politischen System der UdSSR ausrichtete. Dort wurde der Sozialismus weiterhin als Gegensatz zum westlichen Kapitalismus und Vorstufe zum Kommunismus aufgefasst.
Seit dem Scheitern des Realsozialismus leiteten sozialdemokratische Regierungen in Europa eine zunehmende Öffnung zur „Neuen Mitte“ ein. In der SPD begann dieser Prozess etwa 1999 mit dem „Schröder-Blair-Papier“, einer gemeinsamen Erklärung von SPD-Kanzler Gerhard Schröder und dem damaligen britischen Premier Tony Blair von der Labour Party, und führte über die Hartz-IV-Gesetze 2002 bis zur Debatte über die Streichung des demokratischen Sozialismus aus dem Parteiprogramm.
Globalisierungskritiker wie Attac und ehemalige SPD-Linke wie Oskar Lafontaine sehen darin eine Abkehr von sozialdemokratischen Grundwerten und eine Wende zum Neoliberalismus, der für sie eine besonders aggressive Steigerung des internationalen Kapitalismus ist.
Die SPD sieht sich jedoch nach wie vor als sozialistische Partei, ist Mitglied der Sozialistischen Internationale und bekennt sich in ihrem Hamburger Parteiprogramm (2007) ausdrücklich in der Tradition der „marxistischen Gesellschaftsanalyse“ zum Demokratischen Sozialismus.
Schon der Philosoph Johann Gottlieb Fichte rückte in seinen späteren Schriften vom liberalen Staatsmodell ab und ersetzte es durch ein sozialistisches, welches er im Zuge der antinapoleonischen Freiheitskriege mit nationalistischen Gedanken auflud. Er propagierte nun einen nationalen Sozialismus, der eine Mitte zwischen reinem Nachtwächterstaat und reinem Wohlfahrtsstaat bilden sollte. Sein nationaler Sozialismus orientierte sich dabei an einer vorkapitalistischen Wirtschaftsform. Die Wirtschaft sollte eine ständisch organisierte staatliche Planwirtschaft sein.[33]
Das Verhältnis von Sozialismus und Nationalsozialismus ist unter Wissenschaftlern umstritten, was vor allem an den unterschiedlichen Verwendungen des Sozialismusbegriffs liegt. So wird die starke antiliberale Tendenz des Nationalsozialismus mitunter als „sozialistisch“ bezeichnet. Ein wesentlicher Teil der Propaganda des Nationalsozialismus waren wirtschafts- und sozialpolitische Versprechungen. Der Nationalsozialismus gab vor, im Kontrast zu den unerfüllt gebliebenen Versprechungen des Sozialismus und angesichts des Elends der Weltwirtschaftskrise ein „Sozialismus der Tat“ zu sein.[34] Dabei grenzte er sich scharf vom Marxismus ab, dessen Anhänger in der Zeit des Nationalsozialismus verfolgt und ermordet wurden.
Der Rechtswissenschaftler Johann Braun schreibt:
„Eine sozialistische Utopie liegt auch dem Nationalsozialismus zugrunde. Zwar zielt dieser nicht auf einen Sozialismus für alle ab, also nicht auf einen internationalen, sondern auf einen nationalen Sozialismus; aber die Logik des utopischen Rechtsdenkens herrscht auch hier.“[35]
Der SPD-Politiker Rudolf Breitscheid meinte auf dem Leipziger Parteitag 1931, dass „selbst der Nationalsozialismus gezwungen sei, sich ein sozialistisches Aushängeschild zu geben“. Dies zeige, „dass zuletzt doch der Gedanke des Sozialismus marschiere.“
Die sozialistischen Gruppierungen in der NSDAP wie etwa der sozialrevolutionäre Flügel um Otto Strasser verließen vor der Machtergreifung die Partei. Die Otto-Strasser-Gruppe schrieb 1930 unter dem Titel „Die Sozialisten verlassen die NSDAP“:
„Für uns bedeutet Sozialismus Bedarfswirtschaft der Nation unter Anteilnahme der Gesamtheit der Schaffenden an Besitz, Leitung und Gewinn der ganzen Wirtschaft dieser Nation, d. h. also unter Brechung des Besitzmonopols des heutigen kapitalistischen Systems und vor allem unter Brechung des Leitungsmonopols, das heute an den Besitztitel gebunden ist.“[36]
Für andere bezog der Nationalsozialismus einen wesentlichen Teil seiner ideologischen Wirkung aus der Zusammenführung von Nationalismus und Sozialismus.[37] Gemäß Götz Aly ist der Sozialismus im Begriff Nationalsozialismus nicht nur als Propagandaformel zu betrachten, vielmehr gehöre der Nationalsozialismus in die große egalitäre Grundtendenz des 20. Jahrhunderts.[38]
Laut Joachim Fest ist „die Diskussion über den politischen Standort des Nationalsozialismus nie gründlich geführt worden“. Stattdessen habe man „zahlreiche Versuche unternommen, jede Verwandtschaft von Hitlerbewegung und Sozialismus zu bestreiten“. Zwar habe Hitler keine Produktionsmittel verstaatlicht, aber „nicht anders als die Sozialisten aller Schattierungen die soziale Gleichschaltung vorangetrieben“.[39]
Der Historiker Henry A. Turner dagegen glaubt nicht, dass Hitler je Sozialist war. Er habe sich stets zum Privateigentum und zum liberalen Konkurrenzprinzip bekannt, aber nicht aus einem echten Liberalismus heraus, sondern auf Grund seiner sozialdarwinistischen Grundannahmen. Im Sinne eines Primats der Politik habe er postuliert, die Wirtschaft müsse stets unter der vollständigen Kontrolle der Politik stehen. Eine konsistente ökonomische Theorie habe der Nationalsozialismus nie entwickelt.[40] Der Sozialhistoriker Hans-Ulrich Wehler urteilt, dass der Sozialismus im Nationalsozialismus „allenfalls in verballhornter Form“ fortlebte, nämlich in der Ideologie der Volksgemeinschaft.[41]
Aus der Außerparlamentarischen Opposition der 1960er Jahre gingen seit 1970 zum einen eine Reihe von K-Gruppen, zum anderen „undogmatische“ und „antiautoritäre“ Gruppen hervor, die als „Neue Linke“ zusammengefasst werden. Unter ihnen war das 1969 gegründete Sozialistische Büro in Offenbach eine der einflussreichsten Organisationen. Studentenführer wie Rudi Dutschke vertraten einen demokratischen Sozialismus, den sie sowohl gegen die Sozialdemokratie als auch gegen den Realsozialismus abgrenzten. Sie blieben meist außerhalb von Parteien in verschiedenen Neuen sozialen Bewegungen engagiert und hatten kaum Rückhalt in der Arbeiterschaft und bei Gewerkschaften, gewannen aber mit Gründung und Aufstieg der neuen Partei Die Grünen parlamentarischen Einfluss. Kulturell erreichte die Deutsche Studentenbewegung der 1960er Jahre eine Liberalisierung der Gesellschaft und differenziertere Haltung zum Ideal des Sozialismus als im Kalten Krieg, wo dieser Begriff fast nur mit diktatorischen Zuständen östlicher Systeme identifiziert wurde.
Demokratischer Sozialismus, zwischen 1928 und 1934 aus kommunistischer Sicht im Zusammenhang mit der SPD noch als Sozialfaschismus verschrien, wurde auch in der DDR von der kommunistischen SED meist als ein Synonym für Sozialdemokratie definiert und als „Sozialdemokratismus“[42] ideologisch abgewertet. Nach der Wende in der DDR erklärte die gestürzte SED diesen Begriff aber zu ihrer Leitidee, indem sie sich 1990 zur Partei des Demokratischen Sozialismus (PDS) umbenannte und sich programmatisch wandelte. 2005 benannte sich die PDS in Die Linkspartei um und vereinte sich am 16. Juni 2007 mit der westdeutschen WASG zur neugebildeten Partei Die Linke.
In anderen Staaten Westeuropas hatten kommunistische Parteien schon seit den 1960er Jahren einen antisowjetkommunistischen Kurs zum Eurokommunismus eingeschlagen: etwa die Kommunistische Partei Italiens, die sich 1990 umbenannte in „Demokratische Partei der Linken“ (italienisch Partito Democratico della Sinistra – PDS) oder die Kommunistische Partei Frankreichs (KPF, französisch PCF). Diese ehemals kommunistischen Parteien setzen zum einen auf einen Ausbau des Sozialstaats und eine Zähmung des Kapitalismus durch gesetzliche Eingriffe, zum anderen wollen sie den Parlamentarismus stärker mit Plebisziten und direkter Demokratie ergänzen.
Im Vorfeld der Wahlen zum russischen Staatspräsidenten hat auch der letzte Präsident der früheren UdSSR, Michail Gorbatschow, im Oktober 2007 eine sozialdemokratische Bewegung gegründet, um Tendenzen zu einer neuen Diktatur, Abbau von sozialen Rechten und Massenverarmung in Russland zu begegnen.[43]
Eine wissenschaftliche Debatte über Sozialismus als alternativen Gesellschaftsentwurf, wie es sie während der deutschen Studentenbewegung der 1960er Jahre an den Universitäten gab, findet heute kaum mehr statt. Nur einzelne Sozialwissenschaftler wie Wolfgang Fritz Haug fordern angesichts eines Turbokapitalismus heutzutage und der damit verbundenen Lebensweisen, aus den historischen Erfahrungen zu lernen und das sozialistische Projekt zu aktualisieren. Eine kritische Bestandsaufnahme unternimmt unter anderem die Zeitschrift Das Argument und die dort ebenfalls angesiedelte Edition des Historisch-kritischen Wörterbuchs des Marxismus (HKWM). Auch im Umfeld der zur Partei Die Linke gehörenden Rosa-Luxemburg-Stiftung wird eine zukünftige alternative Lebensweise mit Sozialismus diskutiert.
Der Sozialphilosoph Axel Honneth hat mit seiner Schrift Die Idee des Sozialismus eine Kritik der ursprünglichen Idee des in der Industriellen Revolution wurzelnden Sozialismus vorgelegt und als dessen Kerngedanken die „soziale Freiheit“ neu definiert. Sozialismus bedeute heute experimentelle politische Ankersetzung auf dem Weg zu einer solidarischen Gesellschaft, die nicht nur auf der wirtschaftlichen, sondern auch in der politischen Ebene und in den persönlichen Beziehungen (insbesondere zwischen den Geschlechtern) anzustreben sei.[44]
Ebenfalls eine Neuinterpretation stellt der politische Soziologe Heinz Dieterich mit seinem Konzept vom Sozialismus des 21. Jahrhunderts dar, in dem er versucht, marxistische Werttheorie mit basisdemokratischen Elementen zu verknüpfen, der dann eine nicht-marktwirtschaftliche, demokratisch von den unmittelbar Wertschaffenden bestimmende Äquivalenzökonomie zu Grunde liegt. Versuche, diese neue Theorie in die Praxis umzusetzen, finden sich derzeit in Venezuela (Bolivarismus) und Bolivien. Die Theorie eines Demokratischen Konföderalismus wird gegenwärtig in verschiedenen kurdischen Organisationen und Lokalverwaltungen sozialistischer Prägung zu realisieren versucht (Rojava, YPG).
Wolfram Elsner, Professor für Volkswirtschaftslehre an der Universität Bremen, sieht im Sozialismus chinesischer Prägung „gegenüber dem alten, eurozentrierten Sozialismusentwurf“, aber auch gegenüber dem „neoliberalen Finanzmarktkapitalismus“ ein „effektiveres Modell“. In seinem 2020 erschienenen Buch Das chinesische Jahrhundert schreibt er: „China ist heute fähig, die jahrzehntelange Diskreditierung und Tabuisierung jeder Idee von realem Sozialismus wieder aufzubrechen, vor allem weil es zeigt, dass Sozialismus im 21. Jahrhundert kein statisches, bürokratisches Armutssystem mehr ist, sondern diesbezüglich den real existierenden Kapitalismus sogar überflügeln und die menschlichen Perspektiven erweitern kann.“[45]
Der Sozialismus war stets Kritik seitens seiner ideologischen Gegner ausgesetzt. Andererseits gab es aus den zahlreichen einzelnen sozialistischen Strömungen Kritik an Nebenströmungen und den bestehenden sozialistischen Verhältnissen.
Fjodor Michailowitsch Dostojewski, der in seiner Jugend selber Sozialist gewesen war, verurteilte später die Idee des Sozialismus. Einerseits machte er die Bedeutung der Kunst als Anästhetikum geltend und erkannte in provokanten Formeln wie „ein Paar Stiefel sei wichtiger als Shakespeare und eine Eierverkauferin nötiger als Puschkin“ des Nihilisten Dmitri Iwanowitsch Pissarew den Versuch, die Kunst durch das allgemeine Glück überflüssig werden zu lassen. Die persönliche Freiheit schätzte er besonders hoch, weshalb er seit Verbrechen und Strafe (1866) gegen die aufkommende Milieutheorie der gerade entstehenden Soziologie polemisierte. Gleichzeitig erkannte er im Atheismus der russischen Frühsozialisten den Kern ihrer Vorstellung von Perfektibilität und in der daran anschließenden Forderung nach einer Revolution mit der Errichtung einer utopischen Ordnung das Ende der Freiheit, wofür er in seinem Roman Die Dämonen (1873) das Bild vom Kristallpalast aus Nikolai Gawrilowitsch Tschernyschewskis Roman Was tun? aufgriff und nicht als Ausdruck menschlicher Schöpferkraft und Selbstbefreiung durch Technik wertete, sondern als Fortschrittsglauben, Materialismus, Ausdruck von Sterilität und Durchrationalisierung der Massen, womit er seinen Einspruch für die Fehlbarkeit des Menschens geltend machte. Gemeinsam mit dem Konservativen Konstantin Petrowitsch Pobedonoszew entwickelte er in der Zeitschrift Der Staatsbürger eine antiliberale wie antisozialistische Vorstellung von der russischen Orthodoxie und dem Zarenreich als Träger heilsgeschichtlicher Sendung. Die Begegnung mit Wladimir Sergejewitsch Solowjow führte ihn zu einer ethischen Kritik am Sozialismus, wonach er seine Ablehnung zwar beibehielt, jedoch andere Akzente setze.[46]
Stark beeinflusst durch Dostojewskis Werke war der deutsche Philosoph Friedrich Nietzsche. Er wies darauf hin, dass der Sozialismus der jüngere Bruder des fast abgelebten Despotismus sei, den er beerben wolle. Er brauche eine Fülle an Staatsgewalt und strebe die Vernichtung des Individuums an. Der cäsarische Gewaltstaat, den die Sozialisten seiner Meinung nach anstrebten, brauche die Niederwerfung aller Bürger und könne sich nur durch äußersten Terrorismus Hoffnung auf Existenz machen. Er bereite sich im Stillen auf eine Schreckensherrschaft vor und verwende missbräuchlich den Begriff der Gerechtigkeit. Der Sozialismus lehre die Gefahr der Anhäufung von Staatsgewalt und werde den Ruf nach so wenig Staat wie möglich provozieren.[47]
Seit dem Beginn der Auseinandersetzung in Frankreich zwischen der politischen Ökonomie und dem Sozialismus wurde den sozialistischen Kritikern der Marktwirtschaft vorgeworfen, dass sie über keine praxistauglichen Alternativen verfügten, bzw. dass verschiedene bereits gemachte Experimente schmählich gescheitert seien. Unter den neueren Ökonomen warf Eugen von Böhm-Bawerk, ein Vertreter der Österreichischen Schule, in Kapital und Kapitalzins (1884–1902) dem Marxismus gegenüber erstmals das Problem der Wirtschaftsrechnung im Sozialismus auf, ein Argument, das von Ludwig von Mises in der Folge ausgebaut wurde. Der Sozialismus negiere den gesamten Marktprozess und damit würden Marktpreise als Signale für Knappheit fehlen. Dadurch gebe keinerlei Möglichkeit, Investitionsalternativen rational zu bewerten, wie Mises aus seiner Handlungstheorie deduktiv herleitete. Allerdings komme es in einer gemischten Wirtschaftsform mit Privateigentum an Produktionsmitteln und staatlicher Interventionen letztlich zum gleichen Problem, nur moderater, da in dem Ausmaß, wie der Staat in den Markt eingreife, auch hier die Bildung von sinnvollen Preisen durchkreuzt und damit die Richtung der Produktion verändert würden. Der Regierung bleibe nur, entweder zu einem freien Markt zurückzukehren oder aber zu versuchen, durch weitere Interventionen, die ihrerseits wieder die wettbewerbliche Struktur der Marktpreise stören würden, die Schieflage zu korrigieren. Die Wirtschaft jedes interventionistischen Staates sei daher unvermeidlich instabil.[48]
Milton Friedman betont, sozialistisch gesteuerte Volkswirtschaften würden generell qualitativ schlechtere Produkte zu höheren Preisen produzieren.[49]
Nach Ansicht Friedrich August von Hayeks kollidiert die Vergesellschaftung der Produktionsmittel zwangsläufig mit den Individualrechten und der Rechtsstaatlichkeit. Die Wahrung der Rechtsstaatlichkeit würde eine Selbstbeschränkung der Planungsbehörden erfordern, zu der diese nicht in der Lage seien, da sie sonst ihren Aufgaben nicht nachkommen könnten.[50]
Laut dem Ökonomen Jürgen Pätzold verlange zentrale Planung „in gesellschaftspolitischer Hinsicht den Kollektivismus und in staatspolitischer Hinsicht den Totalitarismus des Einparteiensystems“. Demgegenüber setze eine funktionierende Marktwirtschaft voraus, dass sie in ein System politischer und ökonomischer Freiheiten eingebettet sei. Ein solches System der Freiheiten sei mit einer Zentralverwaltungswirtschaft unvereinbar, da darin die Handlungs- und Bewegungsfreiheit der Individuen einen latenten Störfaktor bilde, der versuche, den Staat zurückzudrängen.[51]
Während Trotzki die Sowjetunion noch als einen – zwar „bürokratisch degenerierten“ – Arbeiterstaat ansah, verbreitete Tony Cliff und die von seinen Ideen beeinflusste International Socialist Tendency die Version eines staatskapitalistischen Systems mit allen Merkmalen kapitalistischer Klassenherrschaft.
Die Budapester Schule um Ágnes Heller und Ferenc Fehér analysierte mit marxistischem Instrumentarium die Sowjetgesellschaften als totalitäre Systeme mit einer „Diktatur über die Bedürfnisse“.
Eine marxistisch fundierte Analyse und Kritik des „real existierenden Sozialismus“ als einer „nichtkapitalistischen“ Klassengesellschaft unter der Diktatur von Partei und Bürokratie legte  Rudolf Bahro 1977 mit seiner bekannten Publikation Die Alternative vor.
2018 versuchte der Sozialwissenschaftler Ulrich Knappe 2018, das ökonomische Wesen des vergangenen „paradoxen“, gemeint war: real existierenden Sozialismus mit Hilfe der Marxschen Gesellschaftsanalyse am Beispiel von Russland (Sowjetunion) und China zu entziffern.[52]
Der poststrukturalistische Soziologe und Philosoph Jean Baudrillard kritisiert in Die göttliche Linke – Chronik der Jahre 1977–1984 mit Blick auf die französischen Verhältnisse die aus seiner Sicht nicht mehr zeitgemäßen Ziele des Sozialismus. Während der Sozialismus noch immer von einer transparenten und kohärenten Gesellschaft träume, hätten die Menschen ein solches Bedürfnis nach Anschluss, Kontakt und Kommunikation kaum noch. Nach dem Philosophen Wolfgang Welsch könne ein Baudrillard diese Sozialismus-Kritik schwerlich äußern. Baudrillards Kritik sei dabei bloß narzisstisch und ein Vehikel, um seine eigene antiquierte Diagnose als aktuell erscheinen zu lassen.[53]
Kritik
Wissenschaftliche Zeitschriften

Das Commonwealth of Nations oder kurz das Commonwealth (bis 1947 British Commonwealth of Nations) ist eine lose Verbindung souveräner Staaten, die in erster Linie vom Vereinigten Königreich Großbritannien und Nordirland und dessen ehemaligen Kolonien gebildet wird. Die Gründung geht auf das Jahr 1931 zurück. Sportliches Großereignis des Commonwealth sind die alle vier Jahre stattfindenden Commonwealth Games.
Das Commonwealth of Nations ist eine Vereinigung unabhängiger Staaten, die heute als Nachfolger des British Empire gesehen werden kann. Die Institutionalisierung des British Commonwealth of Nations war Anfang des 20. Jahrhunderts eine Reaktion des Vereinigten Königreiches auf die Autonomiebestrebungen seiner Dominions (Kanada, Südafrika, Australien und Neuseeland) und sollte diese dadurch an das Empire binden.
Im Balfour-Bericht vom 18. November 1926 wurde festgelegt, dass die Dominions autonome Gemeinschaften innerhalb des British Empires sind. Alle haben die gleichen Rechte, sind in keiner Weise anderen untergeordnet, aber als Mitglieder des Commonwealth verbunden durch die Treue zur Krone („autonomous Communities within the British Empire, equal in status, in no way subordinate one to another in any aspect of their domestic or external affairs, though united by a common allegiance to the Crown, and freely associated as members of the British Commonwealth“). Nochmals niedergeschrieben wurde der Status der Mitgliedstaaten am 11. Dezember 1931 im Statut von Westminster. Im Commonwealth gab es keine festgesetzten Statuten und keine Verfassung. Rein konstitutionell gesehen bestand die einzige Verbindung zwischen dem Vereinigten Königreich und den Dominions in der Treue zur Krone.
Mit den Beitritten von Indien (1947), Ceylon (heute Sri Lanka) (1948) und Pakistan (1949), die vor ihrer Unabhängigkeit zu Britisch-Indien gehörten, entstand das moderne Commonwealth (New Commonwealth). Diese Veränderungen wurden in der Erklärung von London am 28. April 1949 festgehalten. 1952 wurden die bisherigen Dominions umbenannt in Commonwealth Realms. 1957 trat mit der ehemaligen britischen Kolonie Goldküste/Ghana erstmals ein zentralafrikanisches Land dem Commonwealth bei.
Das Commonwealth wurde schließlich zu einem „Auffangbecken“ für die ehemaligen britischen Kolonien, wobei es seit der Ausrufung der Republik in Indien 1950 nicht mehr zwingend ist, dass ein Mitgliedsstaat den britischen König oder die britische Königin auch als sein eigenes Staatsoberhaupt anerkennt. Innerhalb von wenigen Jahren verdoppelte sich die Anzahl der Mitglieder. Bestand die Organisation 1955 noch aus acht Mitgliedern, so waren es 1964 bereits 20. Infolge dieser Erweiterung wurde 1965 das Commonwealth Secretariat gegründet. Aus dem Commonwealth of Nations wurde im Zuge dieser Entwicklung die multiethnische und multikulturelle Organisation, die sie heute darstellt. Seit den Beitritten Mosambiks (1995), Ruandas (2009) sowie Gabuns und Togos (beide 2022) sind auch Länder vertreten, die nie zum Britischen Reich gehörten, sondern portugiesische, belgische, französische oder deutsche Kolonie bzw. als Mandatsgebiet des Völkerbundes und später UN-Treuhandgebiet unter belgischer oder französischer Verwaltung waren. Die Motivation von Staaten, die nie britische Kolonie waren, dem Commonwealth beizutreten, liegt vor allem darin, damit Anschluss an eine größere anglophone Gemeinschaft zu gewinnen und sich aus der Abhängigkeit von der früheren Kolonialmacht zu lösen.[1][2]
Bis 1962 galten Commonwealth-Bürger generell als British Subject und waren somit auch zur Einwanderung nach Großbritannien berechtigt. Dieses Recht endete mit dem Commonwealth Immigrations Act im Jahr 1962.
Das Commonwealth of Nations umfasst 56 Mitgliedstaaten, von denen 15 (die sogenannten Commonwealth Realms) in Personalunion verbunden sind (Stand: November 2021). Formal sind die Kronen der 15 Commonwealth Realms getrennt, dennoch ist die britische Monarchie die prominenteste. Um die eigene Souveränität zu betonen, wird aber seit den 1970er Jahren z. B. in Kanada, Australien und Neuseeland mit Bezug auf das eigene Staatsoberhaupt nicht mehr vom britischen König, sondern offiziell von dem King of Canada, King of Australia, King of New Zealand gesprochen.
Heute leben 29,4 Prozent der Weltbevölkerung (rund zwei Milliarden Menschen) in Mitgliedstaaten des Commonwealth: Indien ist dabei mit Abstand das bevölkerungsreichste Mitglied mit über 1,3 Milliarden Menschen. Zu den bevölkerungsreichsten Mitgliedern des Commonwealth gehören unter anderem noch Pakistan mit etwa 170 Millionen Einwohnern, Bangladesch mit 231 Millionen und Nigeria mit über 210 Millionen. Aber auch Staaten wie die Inselkette Tuvalu, auf der nur etwa 11.700 Menschen leben, gehören dem Bund an.
Das kumulative Bruttoinlandsprodukt (BIP) der 56 Mitgliedstaaten lag 2022 bei 13 Billionen US-Dollar und war damit mehr als doppelt so hoch wie das BIP Japans (ca. 5 Bio. US-Dollar), aber deutlich kleiner als das BIP der Vereinigten Staaten (23 Bio. US-Dollar). Größte Volkswirtschaft war Indien (3,5 Bio. US-Dollar), gefolgt vom Vereinigten Königreich (3,4 Bio. US-Dollar), Kanada (2,2 Bio. US-Dollar) und Australien (1,7 Bio. US-Dollar). Das Vereinigte Königreich wickelte 2020 etwa 8,7 % seines Außenhandels mit den Commonwealth-Staaten ab, was anteilig etwa dem Handel mit Deutschland entsprach.[2]
Wenn ein Commonwealth-Mitgliedstaat beschließt, eine Republik zu werden, tritt er damit formell aus dem Bund aus. Anschließend stellt dieser Staat einen Antrag auf Wiederaufnahme, der automatisch gewährt wird. Die Republik Irland verzichtete nach ihrem Austritt aus dem Commonwealth – welcher am 18. April 1949 im Ireland Act 1949 akzeptiert wurde – darauf, sich um eine Wiederaufnahme zu bewerben.
Das Commonwealth Office in London ist die Zentrale dieser Staatenverbindung sui generis. Ähnlich wie bei der UNO in New York entsendet jeder Mitgliedstaat einen Vertreter dorthin, sodass ein ständiger Informationsaustausch stattfinden kann. Zusätzlich treffen sich die Staats- und Regierungschefs der Commonwealth-Länder alle zwei Jahre zu einem einwöchigen Gipfeltreffen. Hierbei werden wichtige politische und wirtschaftliche Fragen sowie die Weltlage diskutiert. Auch Sanktionen gegen einzelne Staaten, wie beispielsweise 2001 gegen Simbabwe[3], werden hier beschlossen. Am 22. November 2007 hat ein Komitee der Außenminister beschlossen, Pakistan so lange von den Sitzungen auszuschließen, bis die Demokratie wiederhergestellt sei und das Gesetz in dem Land wieder gelte.[4] Am 1. September 2009 gab der Generalsekretär Kamalesh Sharma den Ausschluss Fidschis bekannt, nachdem dessen Regierung zuvor die Rückkehr zur Demokratie nach dem Putsch von 2006 verweigert hatte.[5] Die Fidschi-Inseln waren bereits im Juni 2000 aus ähnlichen Gründen von der Versammlung suspendiert worden.[6]
Der CHOGM-Gipfel (englisch: Commonwealth Heads of Government Meeting) findet traditionell jedes zweite Jahr in einem anderen Mitgliedstaat statt. Jener wird traditionell durch den britischen Monarchen (bis 2022 Elisabeth II.), als Oberhaupt des Commonwealth oder wie zuletzt in Ruanda durch dessen Erben und damaligen Vertreter Prinz Charles eröffnet. Die Rolle des Monarchen ist jedoch eine rein symbolische, weshalb die tagespolitische Arbeit in der Führung des Commonwealth von einem Generalsekretär wahrgenommen wird, der von den Regierungschefs der Mitgliedstaaten gewählt wird. Zurzeit ist dies Patricia Scotland. Daneben gibt es einen amtierenden Vorsitzenden der Staatengemeinschaft. Als solcher fungiert der Regierungschef des Landes, in dem das Gipfeltreffen stattfindet; seine Amtszeit läuft bis zum nächsten Gipfel. Seit 2022 hat diese Position Paul Kagame, der Präsident Ruandas, inne.
Bei dem Gipfeltreffen kommen wichtige Staats- und Regierungschefs oder deren Vertreter zusammen und besprechen dort gemeinsamen relevante Themen. Bis 1972 hieß die Zusammenkunft Commonwealth Prime Ministers' Conference, bei der jedoch nur die wichtigsten Premierminister aus den Dominien zusammenkamen.
Königin Elizabeth II. und Prime Minister aus dem Commonwealth auf der Konferenz 1960, Windsor Castle
Allererste Konferenz in London 1944 mit Winston Churchill als Gastgeber
CHOGM 2009 in Port of Spain, Trinidad and Tobago
Traditionelles Gruppenfoto in Perth 2011
Elisabeth II. eröffnete das CHOGM-Treffen 2011 in Perth, Australien
Grenadier Guards mit den Flaggen der Commonwealth-Länder in London, 2018
Camilla beim CHOGM 2018 Big Lunch
König Georg V.1931–1936
König Eduard VIII.1936
König Georg VI.1936–1952
Königin Elisabeth II.1952–2022
König Charles III.seit 2022
Ist eine Organisation, ähnlich dem Volksbund Deutscher Kriegsgräberfürsorge, der sich um die Errichtung, Bebauung und Betreuung von Soldatenfriedhöfen kümmert. Mitglieder dieser Organisation sind Australien, Kanada, Indien, Neuseeland, Südafrika und das Vereinigte Königreich. Präsident ist Edward, 2. Duke of Kent, und als Cousin von Elisabeth II. Mitglied der königlichen Familie.
Die CSPOC (eng: Conference of Speakers and Presiding Officers of the Commonwealth) ist ein Treffen der Speaker einiger Commonwealth-Länder. Der Begriff Speaker fand in England 1377 als erstes Anwendung und wurde dann übers ganze Commonwealth verbreitet. Die Konferenz findet jeweils in einem anderen Land statt.[7]
Dabei handelt es sich um die weltweit drittgrößte internationale Sportveranstaltung mit insgesamt 72 teilnehmenden Nationen und Territorien (mit den britischen Überseegebieten). Begonnen werden diese alle viere Jahre stattfindenden Spiele mit der Queens Baton Relay, einem Staffellauf des Baton mit einer Nachricht der Queen durchs gesamte Commonwealth, ähnlich dem des olympischen Feuers. Traditionell vom Oberhaupt oder einem ihrer Vertreter aus der königlichen Familie, werden die Spiele dann eröffnet und am Ende geschlossen. Zurzeit nehmen bei den Spielen mehr als 6000 Athleten in über 20 verschiedenen Sportarten teil.
Der Begriff Hochkommissar (eng: High Commissioner) bezeichnet den höchsten diplomatischen Vertreter eines Commonwealth-Landes in einem anderen Land des Commonwealth. Der Commonwealth of Nations umfasst derzeit 56 Mitgliedsstaaten, von denen 15 den britischen Monarchen als ihr Staatsoberhaupt anerkennen. Aus diesem Grund werden die diplomatischen Beziehungen zwischen diesen 56 unabhängigen Ländern nicht auf dem Niveau von Botschaftern, sondern von Hochkommissaren geführt.
Sowohl in den britischen Hochkommissariaten als auch in den Botschaften wurde jährlich der Geburtstag von Königin Elizabeth II. gefeiert. Dazu werden traditionell wichtige Politiker des jeweiligen Landes oder wichtige Vertreter aus Kunst, Kultur, Sport sowie anderen Alltagsbereichen eingeladen.
56 Staaten sind gegenwärtig Mitglied[8] im Commonwealth of Nations (die Jahreszahlen nennen das Beitrittsjahr). Unter ihnen wird unterschieden zwischen Commonwealth Realms, die den britischen Monarchen als Staatsoberhaupt haben, und sonstigen Mitgliedern:

Rassismus oder Rassenideologie ist eine Weltanschauung, nach der Menschen aufgrund äußerlicher Merkmale oder negativer Fremdzuschreibungen, die übertrieben, naturalisiert oder stereotypisiert werden, als „Rasse“, „Volk“ oder „Ethnie“ kategorisiert und ausgegrenzt werden. Bis ins 20. Jahrhundert wurden dazu vor allem aufgrund biologischer Merkmale (Hautfarbe, Formen von Gesicht und Körper usw.) angebliche „Menschenrassen“ in heute obsoleten Rassentheorien konstruiert und damit Sklaverei, Assimilationspolitik, Ethno- oder Genozid gerechtfertigt.
Rassisten und Rassenideologen betrachten Menschen, die ihren eigenen Merkmalen möglichst ähnlich sind, meist als höherwertig, während alle anderen (oftmals abgestuft) als geringerwertig betrachtet werden (Chauvinismus). Dieser hierarchischen Herabsetzung geht eine oft penible Zuordnung von Menschen zu Gruppen voraus (Diskriminierung), wobei Misch- und Mehrfachidentitäten sowie Gruppenübertritte als schwerwiegende Problemfälle begriffen werden. Oft möchten Rassenideologen einen normalen Verkehr der Gruppen untereinander erschweren (Segregation) und dabei insbesondere die Vermischung durch familiäre Verbindungen und Zeugung von Nachkommenschaft verhindern.
Seit der Ächtung von Rassismus durch die UN nach dem Zweiten Weltkrieg unternimmt die sog. Neue Rechte (auf dem Wege der strategischen Erringung kultureller Hegemonie) die vorsichtige Wiederbelebung diskriminierender Grundkonzepte in Gestalt der Idee eines Kulturrassismus (siehe Rassismus ohne Rassen und Ethnopluralismus).[1] Auf diese Weise können seitens des politischen Rechtsextremismus beispielsweise eine rigide Grenzpolitik gerechtfertigt oder Flüchtlinge diskriminiert werden. Rassismus, bzw. durch Rassismus begründbare Hierarchien und Vorrechte sollen den Zugang zu Ressourcen (z. B. Gelder, Rohstoffe, Boden), Orten (z. B. Nationalstaaten, bestimmte Stadtviertel) und Positionen (z. B. politische Posten oder auch Positionen in Betrieben) erschweren und dienen zur Legitimation von Machtausübung, Verletzung der allgemeinen Menschenrechte, Gewalt oder Diskriminierung.
Der Begriff Rassismus entstand zu Beginn des 20. Jahrhunderts in der kritischen Auseinandersetzung mit auf Rassentheorien basierenden politischen Konzepten. In anthropologischen Theorien über den Zusammenhang von Kultur und rassischer Beschaffenheit wurde der Begriff der Rasse mit dem ethnologisch-soziologischen Begriff „Volk“ vermengt, z. B. von der völkischen Bewegung. Rassismus zielt dabei nicht auf subjektiv wahrgenommene Eigenschaften einer Gruppe, sondern stellt deren Gleichrangigkeit und im Extremfall deren Existenzberechtigung in Frage. Rassistische Diskriminierung versucht typischerweise, auf projizierte genetische und davon abgeleitete persönliche Unterschiede zu verweisen.
Unabhängig von seiner Herkunft oder Nationalität kann jeder Mensch von Rassismus betroffen sein. Das Internationale Übereinkommen zur Beseitigung jeder Form von Rassendiskriminierung unterscheidet nicht zwischen rassistischer und ethnischer Diskriminierung.[2] Ein erweiterter Rassismusbegriff kann auch eine Vielzahl anderer Kategorien einbeziehen. Menschen mit rassistischen Vorurteilen diskriminieren andere aufgrund solcher Zugehörigkeit; institutioneller Rassismus verweigert bestimmten Gruppen Vorteile und Leistungen oder privilegiert andere. Rassistische Theorien und Argumentationsmuster dienen der Rechtfertigung von Herrschaftsverhältnissen und der Mobilisierung von Menschen für politische Ziele.[3] Die Folgen von Rassismus reichen von Vorurteilen und Diskriminierung über Rassentrennung, Sklaverei und Pogrome bis zu sogenannten „ethnischen Säuberungen“ und Völkermord.
Biologisch lässt sich eine Unterteilung der rezenten Art Homo sapiens in „Rassen“ beziehungsweise Unterarten nicht rechtfertigen. Zur Untersuchung bestimmter geographisch voneinander abweichender Merkmale des Menschen werden in der Humanbiologie stattdessen einzelne Populationen abgegrenzt, die nur auf das untersuchte Merkmal bezogen sind oder im Vorfeld willkürlich vorgenommen werden. Auch wenn daraus Erkenntnisse über die Abstammungsgeschichte des Menschen gewonnen werden und der Laie scheinbare Ähnlichkeiten zu Rassekonzepten zu erkennen glaubt, sind sie weder für taxonomische Zwecke geeignet noch belegen sie die biosystematische Unterteilung des Menschen in Untergruppen.
Der Begriff des Rassismus überlappt mit dem der Fremdenfeindlichkeit und lässt sich oft nur ungenau von diesem unterscheiden. Teile der Sozialwissenschaft unterscheiden zwischen Fremdenfeindlichkeit und Rassismus.
Rassismus, im strengen Sinne des Wortes, erklärt soziale Phänomene anhand pseudowissenschaftlicher Analogieschlüsse aus der Biologie. Als Reaktion auf die egalitären Universalitätsansprüche der Aufklärung versucht er eine scheinbar unantastbare Rechtfertigung sozialer Ungleichheit durch den Bezug auf naturwissenschaftliche Erkenntnisse. Kultur, sozialer Status, Begabung und Charakter, Verhalten etc. gelten als durch die erbbiologische Ausstattung determiniert. Eine vermeintlich natur- oder gottgegebene, hierarchisch-autoritäre Herrschaftsordnung und die daraus gefolgerten Handlungszwänge dienen der Rechtfertigung von Diskriminierung, Ausgrenzung, Unterdrückung, Verfolgung oder Vernichtung von Individuen und Gruppen – sowohl auf individueller als auch auf institutioneller Ebene. Unterschiede in Hautfarbe, Sprache, Religion und Kultur stabilisieren die Abgrenzung zwischen den verschiedenen Gruppen und sollen die Vorrangstellung des Eigenen vor dem Fremden sichern. Der zivilisatorische Fortschritt der Moderne wird als dekadente, einer natürlichen Ungleichheit der Menschen widersprechende Verfallsgeschichte interpretiert.[4]
Der Historiker Imanuel Geiss sieht in den historischen Grundlagen des indischen Kastenwesens die „älteste Form quasi-rassistischer Strukturen“.[5] Laut Geiss nahmen sie ihren Anfang spätestens mit der Eroberung Nordindiens durch die Arier gegen 1500 v. Chr.; „Hellhäutige Eroberer pressten unterworfene Dunkelhäutige als ‚Sklaven‘ in die Apartheid einer Rassen-Kasten-Gesellschaft, die sich auf Dauer in der ursprünglichen Form nicht halten ließ, aber zur extremen Fragmentierung und Abschottung der Kasten als unübersteigbare Lebens-, Berufs-, Wohn-, Essens- und Ehegemeinschaften führte“ (ebenda).[5] Im antiken Griechenland wurden die Barbaren zwar nicht als „rassisch minderwertig“, sondern „nur“ als kulturell, bzw. zivilisatorisch Zurückgebliebene betrachtet,[6] aber auch hier sprechen einige Historiker von prototypischem oder auch „Proto-Rassismus“.
Der „moderne“ Rassismus entstand im 14. und 15. Jahrhundert und wurde ursprünglich eher religiös begründet (Fredrickson, S. 14).[7] Ab 1492, nach der Reconquista, der Rückeroberung Andalusiens durch die Spanier, wurden Juden und Muslime als „fremde Eindringlinge“ oder schlicht als „marranos“ (Schweine) verfolgt und aus Spanien vertrieben. Zwar existierte die formale Möglichkeit der (mehr oder weniger freiwilligen) Taufe, um Vertreibung oder Tod zu entrinnen, jedoch wurde angenommen bzw. unterstellt, dass die Conversos (konvertierte Juden) oder Moriscos (konvertierte Mauren) weiterhin heimlich ihren Glauben ausübten,[8] wodurch den Konvertiten faktisch die Möglichkeit genommen wurde, vollwertige Mitglieder der Gesellschaft zu werden. Das „Jüdische“ oder das „Islamische“, aber auch das „Christliche“, wurde zum inneren Wesen, zur „Essenz“ des Menschen erklärt und die Religionszugehörigkeit so zur unüberwindlichen Schranke. Die Vorstellung, die Taufe oder Konversion reiche nicht, um den Makel zu tilgen, essentialisiert oder naturalisiert die Religion und gilt vielen Historikern daher als Geburt des modernen Rassismus. Die Vorstellung, ein Jude oder Moslem behielte auch dann sein jüdisches oder muslimisches „Wesen“, wenn er seine Religion geändert hat – es liege ihm gewissermaßen im Blute –, ist im Kern rassistisch. „Die alte europäische Überzeugung, dass Kinder dasselbe ‚Blut‘ haben wie ihre Eltern, war eher eine Metapher und ein Mythos als ein empirischer wissenschaftlicher Befund, aber sie sanktionierte eine Art genealogischen Determinismus, der in Rassismus umschlägt, wenn er auf ganze ethnische Gruppen angewandt wird“ (Fredrickson, S. 15).[7] Die Estatutos de limpieza de sangre („Statuten von der Reinheit des Blutes“), erstmals niedergelegt 1449 für den Rat der Stadt Toledo, werden in historischen Überblicksdarstellungen als Vorläufer der Nürnberger Rassegesetze wahrgenommen. Der Historiker Max Sebastián Hering Torres weist in seiner Darstellung der entsprechenden Verordnungen darauf hin, dass der rassistische Ansatz der Limpieza außer Frage steht und Elemente dieses Konzepts an Passagen der Nürnberger Rassegesetze erinnern, dass aber das dort formulierte Blutreinheitskonzept den Nationalsozialisten unbekannt war. Die eigene Qualität der NS-Rassengesetze lasse eine lineare Interpretation vom vormodernen Antijudaismus bis hin zum NS-Antisemitismus und Holocaust von Millionen Juden in Konzentrationslagern nicht zu.[9][10][11] Die rassistische Doktrin von der „Reinheit des Blutes“ stigmatisierte eine ganze ethnische Gruppe aufgrund von Kriterien, an denen die Betroffenen weder durch Bekehrung noch durch Assimilation etwas ändern konnten.[12]
Aus der christlichen Glaubensgemeinschaft, der eigentlich jeder angehört, der durch die Taufe zu einem Teil der Gemeinschaft geworden ist, war eine Abstammungsgemeinschaft, ein Rassenäquivalent, geworden – ein Vorgang, in dem sich fast 500 Jahre vor dem Nationalsozialismus das rassistische Ideologem vom „Volkskörper“ mit den damit einhergehenden Vorstellungen, beispielsweise von der „Unreinheit des jüdischen Blutes“, ankündigt.
Dieser mittelalterliche Rassismus blieb jedoch zunächst eingebunden in den Zusammenhang mythischer und religiöser Vorstellungen, es fehlte der Bezug auf eine naturwissenschaftlich begründete Biologie. Erst als religiöse Gewissheiten in Frage gestellt und die Trennung zwischen Körper und Seele zugunsten eines materialistisch-naturwissenschaftlichen Weltbildes aufgehoben wurden, waren die geistesgeschichtlichen Voraussetzungen für einen Rassismus neuzeitlicher Prägung gegeben.[13] „Der Rassismus konnte sich in dem Maße zu einer komplexen Bewusstseinsform entwickeln, wie sich rassistische Bewusstseinselemente aus den theologischen Bindungen des Mittelalters „emanzipieren“ konnten.“[14] Pseudowissenschaftliche Rassentheorien sind gewissermaßen ein „Abfallprodukt der Aufklärung“,[15] deren scheinbar naturwissenschaftliche Argumentation auch und gerade von großen Aufklärern rezipiert wurde. „Mit ihrem leidenschaftlichen, manchmal an Fanatismus grenzenden Bestreben, die Welt ‚logisch‘ zu ordnen, mit ihrer Manie, alles zu klassifizieren, haben die Philosophen und Gelehrten der Aufklärung dazu beigetragen, jahrhundertealten rassistischen Vorstellungen eine ideologische Kohärenz zu geben, die sie für jeden anziehend machte, der zu abstraktem Denken neigte.“[16]
So schrieb Voltaire 1755: „Die Rasse der Neger ist eine von der unsrigen völlig verschiedene Menschenart, wie die der Spaniels sich von der der Windhunde unterscheidet […] Man kann sagen, dass ihre Intelligenz nicht einfach anders geartet ist als die unsrige, sie ist ihr weit unterlegen.“[17][18] Ursprünglich metaphysisch und religiös begründet, erhielt der Rassismus durch die Aufklärung ein weiteres, ein säkulares Fundament.
Teilte 1666 der Leydener Professor Georgius Hornius die Menschheit in Japhetiten (Weiße), Semiten (Gelbe) und Hamiten (Schwarze), weil er gemäß der biblischen Überlieferung glaubte, die gesamte Menschheit stamme von den drei Söhnen Noachs, Japhet, Sem und Ham ab, so stellte keine 20 Jahre später, 1684, der französische Gelehrte François Bernier eine Rassensystematik vor, in der er die Menschen anhand äußerer Merkmale wie Hautfarbe, Statur und Gesichtsform in vier bis fünf ungleich entwickelte Rassen kategorisierte. Lastete auf den Schwarzen zuvor der Fluch des Ham[19] und auf den Juden die kollektive „Schuld des Gottesmordes“, so wurden nun »wissenschaftliche« Gründe angeführt, die deren »rassische« Andersartigkeit oder Minderwertigkeit »beweisen« sollten.
Naturforscher wie Carl von Linné, Georges-Louis Leclerc de Buffon, Johann Friedrich Blumenbach, Immanuel Kant und viele andere katalogisierten und klassifizierten Tier- und Pflanzenreich, aber auch die damals bekannte Menschheit und schufen so die Grundlagen der „Naturgeschichte des Menschen“, der Anthropologie. Doch war deren Arbeit von Anfang an durch überlieferte Mythen und Vorurteile belastet. Besonders die von der mittelalterlichen Theologie überlieferte und in die säkulare neuzeitliche Wissenschaft übernommene Scala Naturae, die »Stufenleiter der Wesen«, spielte dabei eine gewichtige Rolle. Diese Vorstellung ordnete allem Leben einen festen Platz in einer Hierarchie »niederer« und »höherer« Wesen zu. Sie trug einerseits zur Bildung von Theorien über Evolution und Höherentwicklung bei, führte jedoch andererseits, übertragen auf den Menschen, zur Unterscheidung älterer und jüngerer »Rassenschichten«, die mit »primitiv« und »fortschrittlich« gleichgesetzt wurden.[20] So wurde die Gattung Homo 1758 von Carl von Linné in der 10. Auflage von Systema Naturae eingeführt.[21] Schon zuvor hatte er vier räumlich getrennt lebende Varianten des anatomisch modernen Menschen anhand ihrer Hautfarbe unterschieden, nun aber erweiterte er die Charakterisierung dieser vier geografischen Varietäten des Menschen um die Merkmale Temperament und Körperhaltung: Die Europäer unterschieden sich ihm zufolge von den anderen menschlichen Varietäten durch die Merkmale weiß, sanguinisch, muskulös („albus, sanguineus, torosus“), die Amerikaner durch die Merkmale rot, cholerisch, aufrecht („rufus, cholericus, rectus“), die Asiaten durch die Merkmale gelb, melancholisch, steif („luridus, melancholicus, rigidus“) und die Afrikaner durch die Merkmale schwarz, phlegmatisch, schlaff („niger, phlegmaticus, laxus“). „Hätten sich die Anthropologen darauf beschränkt, die Menschengruppen nach ihren physischen Merkmalen zu gliedern und daraus keine weiteren Schlüsse zu ziehen, wäre ihre Arbeit so harmlos wie die des Botanikers oder Zoologen und lediglich deren Fortsetzung gewesen. Doch stellte sich schon gleich zu Beginn heraus, daß diejenigen, die die Klassifikationen vornahmen, sich das Recht anmaßten, über die Eigenschaften der Menschengruppen, die sie definierten, zu Gericht zu sitzen: indem sie von den physischen Merkmalen Extrapolationen auf geistige oder moralische vornahmen, stellten sie Hierarchien von Rassen auf.“[22] „Was immer Linné, Blumenbach und andere Ethnologen des 18. Jahrhunderts beabsichtigt hatten – sie waren jedenfalls die Wegbereiter für einen säkularen beziehungsweise „wissenschaftlichen“ Rassismus“ (Fredrickson, S. 59).[7]
Durch die Wertung phänotypischer Merkmale anhand ästhetischer Kriterien sowie ihrer Verknüpfung mit geistigen, charakterlichen oder kulturellen Fähigkeiten bereiteten die im 18. Jahrhundert ausgearbeiteten Rassentypologien den Boden für den voll entfalteten biologischen Rassismus des 19. und 20. Jahrhunderts (vgl. Fredrickson, S. 61–63).[7] Joseph Arthur Comte de Gobineau, den Poliakov als den „großen Herold biologisch gefärbten Rassismus“ bezeichnet, gilt mit seinem vierbändigen, zunächst in Frankreich erschienenen Versuch über die Ungleichheit der Menschenrassen (1853–1856) als Erfinder der arischen Herrenrasse und Begründer der modernen Rassenlehre bzw. als theoretischer Vordenker des modernen Rassismus.[23] Den Niedergang seines Standes erklärte der französische Adlige als Folge der rassischen Degeneration. Zudem prophezeite er, dass die Vermischung des Blutes unterschiedlicher Rassen unweigerlich zum Aussterben der Menschheit führe.[24]
Im 20. Jahrhundert haben sich in vielen Ländern ausgeprägte Formen des Rassismus herausgebildet, die zum Teil zu offiziellen Ideologien der jeweiligen Staaten wurden – Beispiele sind:
Seit der UNESCO-Deklaration gegen den „Rasse“-Begriff[25] auf der UNESCO-Konferenz Gegen Rassismus, Gewalt und Diskriminierung im Jahre 1995 im österreichischen Stadtschlaining wird nicht nur jede biologische, sondern auch jede soziologische Ableitung rasseähnlicher Kategorien geächtet. Diese Ächtung wird wie folgt begründet:[26]
Der bedeutende italienische Populationsgenetiker Cavalli-Sforza, Professor an der Stanford University in Kalifornien, kommt in seinem monumentalen Werk „The History and Geography of Human Genes“ zum Ergebnis, dass es keine wissenschaftliche Basis für die Unterscheidung von Menschenrassen gibt. Die Einteilung der Menschheit in taxonomische Untergruppen sei im Kern willkürlich und nicht mittels statistischer Methoden reproduzierbar. Die geringen genetischen Unterschiede, die zwischen bestimmten Populationen überhaupt nachweisbar seien, sind aufgrund des geringen evolutionären Alters der modernen Menschheit sehr gering und zudem vermutlich durch Wanderungen und anschließende Vermischung bis fast zur Unkenntlichkeit verwischt. Die optisch auffälligen Unterschiede, etwa der Hautfarbe, korrelieren zudem überhaupt nicht mit diesen genetisch definierten Populations-Clustern. Keine Population besitzt eigene Gene, und selbst eigene Allele sind bedeutungslos, wesentliche Unterschiede bestehen nur in deren Frequenz. Je nach gewähltem genetischen Marker sind die genetischen Cluster zudem verschieden umgrenzt und nicht stabil.[27]
Der 21. März ist der Internationale Tag gegen Rassismus. Im Jahr 2018 stand dort die Förderung von Toleranz, Inklusion und Respekt für Diversität im Vordergrund.[28] UN-Sonderberichterstatter für Rassismus und Fremdenfeindlichkeit ist E. Tendayi Achiume.
Rassismus als soziales und psychisches Phänomen existiert unabhängig von Rassentheorien,[29] als rassistisch zu beschreibende Gruppenkonflikte lassen sich bis in die frühe Menschheitsgeschichte nachweisen.[5] Rassismus als systematisches Lehrgebäude dagegen entwickelte sich seit dem ausgehenden 18. Jh. in Europa und der angelsächsischen Welt.[4]
Der Begriff „Rassismus“ tauchte erst auf, als am Rassenbegriff oder zumindest an einigen seiner Verwendungen Zweifel aufkamen. Er entstand im frühen 20. Jahrhundert in der Auseinandersetzung mit völkischen Theorien. In der Endung ‚-ismus‘ sollte sich die Auffassung von Historikern und anderen Autoren niederschlagen, „dass es sich dabei um fragwürdige Ansichten und Überzeugungen handele, nicht um unbestreitbare Naturtatsachen“ (Fredrickson, S. 159).[7] Die Rassisten selbst hingegen verstanden sich positiv als Vertreter einer „Rassenkunde“ oder „Rassenlehre“ und lehnten infolgedessen «Rassismus» zur Umschreibung ihrer Ansichten ab (Geiss, S. 17 und 341).[5] Meyers Lexikon definierte 1942 Rassismus folgendermaßen:
„Rassismus, urspr. Schlagwort des demokr.-jüd. Weltkampfes gegen die völkischen Erneuerungsbewegungen und deren Ideen u. Maßnahmen, ihre Völker durch Rassenpflege zu sichern und das rassisch wie völkisch und politisch-wirtschaftlich zerstörende Judentum sowie anderweitiges Eindringen fremden Blutes abzuwehren und auszuschlagen, als inhuman und ihre Träger als ‚Rassisten‘ zu verleumden.“[30]
Pionierarbeit in vielerlei Hinsicht leistete Théophile Simar. Sein 1922 erschienenes Werk Étude critique sur la formation de la doctrine des races au XVIIIe siècle et son expansion au XIXe siècle gilt als das erste, in dem die Begriffe „Rassismus“ und „rassistisch“ Anwendung fanden. Darin setzte er sich äußerst kritisch mit der These der germanischen bzw. teutonischen Überlegenheit über die anderen europäischen – besonders die romanischen – Völker auseinander und kam dabei zu dem Schluss, dass derartige Konzepte wissenschaftlich nicht stichhaltig seien und ausschließlich politischen Zwecken dienen (Fredrickson, S. 161–162).[7]
Im Jahre 1935 kritisierten Julian Huxley und Alfred C. Haddon in ihrem Buch We Europeans: A survey of Racial problems, dass es für die Idee verschiedener, voneinander abgegrenzter Menschenrassen keinerlei wissenschaftliche Beweise gebe. Klassifikationen anhand genetischer oder somatischer Merkmale und darauf basierende Bewertungen sowie jede Form von „Rassenbiologie“ lehnten sie als pseudowissenschaftlich ab. Sie forderten daher, das Wort Rasse aus dem wissenschaftlichen Vokabular zu streichen und durch die Bezeichnung „ethnische Gruppe“ zu ersetzen. Die Rassentheorien der Nazis bezeichneten sie als „Glaubensbekenntnis eines leidenschaftlichen Rassismus“. „Der Rassismus ist ein Mythos und ein gefährlicher dazu. Er ist ein Deckmantel für selbstsüchtige ökonomische Ziele, die in ihrer unverhüllten Nacktheit hässlich genug aussehen würden.“ Die biologische Anordnung der europäischen Menschentypen sei ein subjektiver Vorgang und der Mythos des Rassismus ein Versuch, den Nationalismus zu rechtfertigen.[31]
Jacques Barzun klassifizierte in seinem richtungsweisenden Werk Race: a Study in Superstition von 1937 den „Rassengedanken“ (racialism)[32] als modernen Aberglauben und eine Form irregeleiteten Denkens.[33] Rasse, so erklärte er, „war in Deutschland ein Mittel, um dem deutschen Volk nach der nationalen Erniedrigung von Versailles und danach ein Gefühl der Selbstachtung zurückzugeben.“ Er beschreibt ferner, wie auch schon früher und an anderen Orten Rassismus dazu benutzt wurde, um dem «Nationalen» Aufschwung zu verleihen (vgl. Fredrickson, S. 167).[7] Bereits im ersten Kapitel wies er darauf hin, dass nicht nur die deutsche Einstellung gegenüber den Juden rassistisch sei, sondern ebenso die Annahme der «weißen Überlegenheit gegenüber den Schwarzen», die Furcht vor der asiatischen „Gelben Gefahr“ oder die Überzeugung, Amerika müsse die angelsächsische Rasse davor beschützen, durch südeuropäisches, jüdisches oder das „Blut der Neger“ verunreinigt zu werden. Seine umfassende Analyse der rassistischen Ideenwelt seiner Zeit beinhaltete u. a.:
Größeren Bekanntheitsgrad erlangte der Begriff „Rassismus“ erst durch den Sexualwissenschaftler Magnus Hirschfeld, dessen zwischen 1933 und 1934 verfasste Analyse und Widerlegung der nationalsozialistischen Rassendoktrin posthum, in englischer Übersetzung, unter dem Titel Racism veröffentlicht wurde. In dem 1938 erschienenen Werk erklärte Hirschfeld den Aufstieg des deutschen Antisemitismus als Folge der Probleme, die aus der Niederlage im Ersten Weltkrieg erwuchsen. Rassismus diene als Sicherheitsventil gegen ein Katastrophengefühl und scheine für die Wiederherstellung der Selbstachtung zu sorgen, zumal er sich gegen einen leicht erreichbaren und wenig gefährlichen Feind im eigenen Land richte und nicht gegen einen achtenswerten Feind jenseits der nationalen Grenzen.[34] Dem Konzept der „Rasse“ konnte auch er nichts abgewinnen, was von wissenschaftlichem Wert wäre; stattdessen empfahl er die Streichung des Ausdrucks, „soweit damit Unterteilungen der menschlichen Spezies gemeint sind“.[35] Doch bot auch Hirschfeld keine formale Definition des «Rassismus» und machte auch nicht deutlich, worin seiner Ansicht nach der Unterschied zum Begriff der «Xenophobie» besteht, den er ebenfalls verwandte.
Die erste Rassismus-Definition stammt von der Amerikanerin Ruth Benedict. In ihrem 1940 erschienenen Buch Race – Science and Politics bezeichnet sie Rassismus als „das Dogma, dass eine ethnische Gruppe von Natur aus zu erblicher Minderwertigkeit und eine andere Gruppe zu erblicher Höherwertigkeit bestimmt ist. Das Dogma, dass die Hoffnung der Kulturwelt davon abhängt, manche Rassen zu vernichten und andere rein zu erhalten. Das Dogma, dass eine Rasse in der gesamten Menschheitsgeschichte Träger des Fortschritts war und als einzige auch künftig Fortschritt gewährleisten kann“.
Bereits diese frühe Definition verwendet „Rasse“ und „ethnische Gruppe“ synonym, der Terminus „Rasse“ wird dabei als soziologische Kategorie aufgefasst und kommt ohne biologischen Bezug aus. Benedict unterschied zunächst scharf zwischen religiösen und rassischen Differenzkonzepten und versuchte so, den Rassismusbegriff auf den biologischen Rassismus einzugrenzen. Im weiteren Verlauf ihrer Studien gab sie diese Trennung jedoch auf und leitete eine «funktionale Äquivalenz» zwischen religiösem Fanatismus und solchen Abneigungen her, die mit Merkmalen der physischen Erscheinung oder der Abstammung gerechtfertigt werden. Beide führen, so Benedict, zu Formen der Verfolgung, für die lediglich unterschiedliche Rechtfertigungen formuliert werden, die sich aber in ihrem Wesen nicht unterscheiden. „In den Augen der Geschichte jedenfalls bleibt der Rassismus lediglich ein anderes Beispiel für die Verfolgung von Minderheiten zum Vorteil derer, die an der Macht sind“ (Fredrickson, S. 168).[7] Populär wurde Benedicts Definition durch Martin Luther King, der sie mehr als 25 Jahre später in seinem Buch Where do we go from here: Chaos or Community? verwandte.
In den 1950er Jahren erschien eine wissenschaftlichen Schriftenreihe der UNESCO zur Rassentheorie. Daran war u. a. der französische Ethnologe Claude Lévi-Strauss beteiligt. Zu seinem Beitrag, der 1952 in französischer Sprache und erst 1972 unter dem Titel Rasse und Geschichte in deutscher Sprache im Suhrkamp-Verlag erschien, schrieb er: „Es mag überraschen, wenn in einer Schriftenreihe, die sich den Kampf gegen den Rassismus zum Ziel gesetzt hat, vom Beitrag der Menschenrassen zur Weltzivilisation gesprochen wird.“[36]
1965 definierte die UNO im Internationalen Übereinkommen zur Beseitigung jeder Form von Rassendiskriminierung den Begriff der „Rassendiskriminierung“ als „jede auf der Rasse, der Hautfarbe, der Abstammung, dem nationalen Ursprung oder dem Volkstum beruhende Unterscheidung, Ausschließung, Beschränkung oder Bevorzugung, die zum Ziel oder zur Folge hat, dass dadurch ein gleichberechtigtes Anerkennen, Genießen oder Ausüben von Menschenrechten und Grundfreiheiten im politischen, wirtschaftlichen, sozialen, kulturellen oder jedem sonstigen Bereich des öffentlichen Lebens vereitelt oder beeinträchtigt wird.“
Die Europäische Kommission gegen Rassismus und Intoleranz definiert Rassismus als „die Überzeugung, dass ein Beweggrund wie Rasse, Hautfarbe, Sprache, Religion, Staatsangehörigkeit oder nationale oder ethnische Herkunft die Missachtung einer Person oder Personengruppe oder das Gefühl der Überlegenheit gegenüber einer Person oder Personengruppe rechtfertigt“.[37]
In der Wissenschaft existieren heute verschiedene Definitionen des Begriffs Rassismus. Tragweite, Gültigkeit und Erklärungsmacht der jeweiligen Definitionen variieren je nach Deutungsebene und Schwerpunkt. Der Begriff ist stark ideologisiert, so dass die Akzeptanz oder Ablehnung verschiedener Definitionen auch von politischen oder ethischen Präferenzen abhängen kann. Die jeweils extremsten Deutungen weiten den Begriff entweder sehr aus, bis hin zum sogenannten „Speziesismus“, oder schränken ihn stark ein, so dass er lediglich den „klassischen“, also auf Rassentheorien basierenden Rassismus umfasst.[38] Definitionsgegenstände können historische Tatbestände sein, praktische Strukturen und Prozesse, aber auch Theorien, Ideologien, Denkmethoden und abstrakte Konzepte oder der «Rassismus an sich».
Der marxistische Rassismusforscher Étienne Balibar stellte fest, „dass es nicht «einen» invarianten Rassismus, sondern «mehrere» Rassismen gibt, die ein ganzes situationsabhängiges Spektrum bilden […] Eine bestimmte rassistische Konfiguration hat keine festen Grenzen, sie ist ein Moment einer Entwicklung, dass je nach seinen eigenen latenten Möglichkeiten, aber auch nach den historischen Umständen und den Kräfteverhältnissen in den Gesellschaftsformationen einen anderen Platz im Spektrum möglicher Rassismen einnehmen kann.“[39]
Der Historiker Patrick Girard sah bereits 1976 die Notwendigkeit eines differenzierteren Rassismusbegriffes: „Zum Beispiel waren offensichtlich Juden, Indianer und Schwarze alle Opfer verschiedener Spielarten des Rassismus. Sie waren das aber auf Grund ganz unterschiedlicher Voraussetzungen in ganz verschiedenen Epochen und aus ganz verschiedenen Gründen. Daher ist es vorzuziehen, von «Rassismen» und nicht von «Rassismus» zu sprechen, wobei der Antisemitismus, wie wir sehen werden, eine Sonderstellung einnimmt“.[40]
Auch Soziologen wie Stuart Hall unterscheiden aus praktischen und analytischen Erwägungen heraus zwischen dem «allgemeinen Rassismus» und seinen verschiedenen Ausformungen, den Rassismen:
„Es gibt keinen Rassismus als allgemeines Merkmal menschlicher Gesellschaften, nur historisch-spezifische Rassismen.“
„Empirisch hat es viele Rassismen gegeben, wobei jeder historisch spezifisch und in unterschiedlicher Weise mit den Gesellschaften verknüpft war, in denen er aufgetreten ist.“
„Ich habe bislang über den allgemeinen Begriff des Rassismus gesprochen, über Rassismus im allgemeinen. Aber wo immer wir Rassismus vorfinden, entdecken wir, daß er historisch spezifisch ist, je nach der bestimmten Epoche, nach der bestimmten Kultur, nach der bestimmten Gesellschaftsform, in der er vorkommt. Diese jeweiligen spezifischen Unterschiede muß man analysieren. Wenn wir über konkrete gesellschaftliche Realität sprechen, sollten wir also nicht von Rassismus, sondern von Rassismen sprechen.“
In gleicher Weise argumentiert der Historiker George M. Fredrickson:
„Diese Kontinuitäten [strukturelle Ähnlichkeiten von biologisch begründetem und «neuem kulturellem Rassismus»] weisen meiner Ansicht nach darauf hin, dass es eine allgemeine Geschichte des Rassismus und eine Geschichte partikulärer Rassismen gibt; doch um die verschiedenen Formen und Funktionen des allgemeinen Phänomens zu verstehen, mit denen wir uns befassen, ist es notwendig, den jeweils spezifischen Kontext zu kennen.“[43]
Die Soziologen Loïc Wacquant und Albert Memmi empfehlen, „ein für alle mal auf die allzu dehnbare Reizvokabel Rassismus zu verzichten oder sie allenfalls zur Beschreibung empirisch analysierbarer Doktrinen und Überzeugungen von Rassen zu verwenden;“[44] bzw. den Terminus «Rassismus», wenn überhaupt, dann ausschließlich zur Bezeichnung des Rassismus im biologischen Wortsinne zu gebrauchen (Memmi, S. 121).[45]
Memmi fasst den «Rassismus im weiteren Sinne» als einen «allgemeinen Mechanismus» auf, der jedoch in verschiedenen Spielarten auftritt, von denen der «Rassismus im engeren Sinne» nur eine ist. Weil ein Rassismus sich ohne ein Verständnis des anderen nur unzureichend begreifen lasse und der «Rassismus im weiteren Sinne» wesentlich stärker verbreitet sei, schien es ihm sinnvoll, „den biologischen Rassismus, historisch eine relativ junge Erscheinung, einer allgemeineren und viel älteren Verhaltensweise unterzuordnen“ (Memmi, S. 97).[45] „Tatsächlich stützt sich die rassistische Anklage bald auf einen biologischen und bald auf einen kulturellen Unterschied. Einmal geht sie von der Biologie, dann wieder von der Kultur aus, um daran anschließend allgemeine Rückschlüsse auf die Gesamtheit der Persönlichkeit, des Lebens und der Gruppe des Beschuldigten zu ziehen. Manchmal ist das biologische Merkmal nur undeutlich ausgeprägt, oder es fehlt ganz. Kurz, wir stehen einem Mechanismus gegenüber, der unendlich mannigfaltiger, komplexer und unglücklicherweise auch stärker verbreitet ist, als der Begriff Rassismus im engen Wortsinne vermuten ließe. Es ist zu überlegen, ob man ihn nicht besser durch ein anderes Wort oder eine andere Wendung ersetzt, die sowohl die Vielfalt als auch die Verwandtschaft der einzelnen Formen des Rassismus zum Ausdruck bringt“ (Memmi, S. 165–166).[45] „Der Begriff Rassismus passt genau für die biologische Bedeutung“ und solle daher künftig ausschließlich für den Rassismus im biologischen Sinne gebraucht werden. Zur Bezeichnung der allgemeinen Erscheinung schlug Memmi ursprünglich Ethnophobie vor, entschied sich jedoch 1982 für den Begriff Heterophobie, denn „damit ließen sich jene phobischen und aggressiven Konstellationen begrifflich fassen, die gegen andere gerichtet sind und mit unterschiedlichen – psychologischen, kulturellen, sozialen oder metaphysischen – Argumenten gerechtfertigt werden, und von denen der Rassismus im engeren Sinne lediglich eine Variante wäre“ (Memmi, S. 121–122).[45]
„Mit «Rassismus» soll ausschließlich die Ablehnung des anderen unter Berufung auf rein biologische Unterschiede, mit «Heterophobie» soll die Ablehnung des anderen unter Berufung auf Unterschiede jedweder Art gemeint sein. Damit wird der Rassismus zu einem Sonderfall der Heterophobie“ (Memmi, Seite 124).[45] Mit dem Begriff «Heterophobie» ließen sich nach Ansicht Memmis auch weitere terminologische Probleme lösen, weil er einerseits alle Spielarten einer „aggressiven Ablehnung des anderen“ erfasse und sich umgekehrt auch leicht in seine verschiedenen Formen ummünzen lasse. „Statt von Antisemitismus[46] zu sprechen, einem offensichtlich ungenauen Terminus,[47] könnte man den Begriff «Judenphobie» gebrauchen, der eindeutig die Angst vor dem Jüdischen und dessen Ablehnung bezeichnet; dasselbe gilt für die Begriffe ‚Negrophobie‘, ‚Arabophobie‘ usw.“ (Memmi, S. 123).[45]
Die in der Rassismusforschung aktuell akzeptierte Definition stammt von dem französischen Soziologen Albert Memmi:
„Der Rassismus ist die verallgemeinerte und verabsolutierte Wertung tatsächlicher oder fiktiver Unterschiede zum Nutzen des Anklägers und zum Schaden seines Opfers, mit der seine Privilegien oder seine Aggressionen gerechtfertigt werden sollen“
Diese Definition ist nicht auf rassenbiologisch begründete Rassismen beschränkt, so stützt sich die „rassistische Anklage bald auf einen biologischen und bald auf einen kulturellen Unterschied. Einmal geht sie von der Biologie, dann wieder von der Kultur aus, um daran anschließend allgemeine Rückschlüsse auf die Gesamtheit der Persönlichkeit, des Lebens und der Gruppe des Beschuldigten zu ziehen.“ (Memmi, S. 165 f.).[45][51]
Sie enthält drei Elemente, die Memmi für wesentlich erachtet und denen auch in der aktuellen Rassismusforschung zentrale Bedeutung zukommt.[52] Memmi betont, dass keines dieser Elemente für sich allein schon den Rassismus ausmache, dieser entstehe erst durch die Verknüpfung (Memmi, S. 44).
Die Grundlage des Rassismus besteht in der nachdrücklichen (Über-)Betonung oder Konstruktion tatsächlicher oder fiktiver Unterschiede zwischen Rassist und Opfer. „Der Unterschied ist der Angelpunkt rassistischer Denk und Handlungsweise“ (Memmi, S. 48).[45] Memmi weist ausdrücklich darauf hin, dass es sich dabei um einen «allgemeinen Mechanismus» handelt, er „[Der Rassismus] beschränkt sich weder auf die Biologie noch auf die Ökonomie, die Psychologie oder die Metaphysik; er ist eine vielseitig verwendbare Beschuldigung, die von allem Gebrauch macht, was sich anbietet, selbst von dem, was gar nicht greifbar ist, weil sie es je nach Bedarf erfindet“ (Memmi, S. 83).[45] „Die Rassisten verabscheuen die Araber jetzt nicht mehr wegen ihrer sonnenverbrannten Haut oder ihrer levantinischen Gesichtszüge, sondern weil sie – «machen wir uns doch nichts vor» – einer lächerlichen Religion anhängen, ihre Frauen schlecht behandeln, grausam oder einfach rückständig sind“ (Memmi, S. 101).[45] Die Benutzung des Unterschiedes sei zwar für die rassistische Argumentation unentbehrlich, „aber es ist nicht der Unterschied, der stets den Rassismus nach sich zieht, es ist vielmehr der Rassismus, der sich den Unterschied zunutze macht“. Dabei spiele es keine Rolle, ob der Unterschied real sei oder reine Fiktion, für sich allein wichtig oder unbedeutend. „Wenn es keinen Unterschied gibt, dann wird er vom Rassisten erfunden; gibt es ihn hingegen, dann wird er von ihm zu seinem Vorteil interpretiert“ (Memmi, S. 167).[45]
Das bloße Aufzeigen einer Verschiedenheit zwischen zwei Individuen oder Gruppen stellt, so Memmi, für sich allein genommen noch keinen Rassismus dar. „Der Rassismus liegt nicht in der Feststellung eines Unterschieds, sondern in dessen Verwendung gegen einen anderen“ (Memmi, S. 214).[45] „Der Rassismus ist die Wertung […]“, er beginnt dort, wo der Unterschied eine Interpretation[53] erfährt und ihm eine (zusätzliche) Bedeutung beigemessen wird, in der Art, dass sie (ab)wertend wirkt und Nachteile für den Bewerteten nach sich zieht.[54] „Erst im Kontext des Rassismus nimmt diese Betonung des Unterschieds eine besondere Bedeutung an […]“ (Memmi, S. 166).[45] Die Hervorhebung von tatsächlichen oder eingebildeten Unterschieden ist für Memmi lediglich ein „bequemes Werkzeug für etwas ganz anderes, nämlich die Infragestellung des Opfers“, woraus sich als Konsequenz ergibt, dass die Merkmale des anderen stets negative sind, sie bezeichnen etwas Schlechtes, während die Merkmale des Rassisten gut sind. „Der Rassist ist liebenswert, weil sein Opfer verabscheuungswürdig ist. Die Welt des Rassisten ist die des Guten, die Welt seines Opfers die des Bösen“ (Memmi, S. 98–99).[45]
Verallgemeinerung wird von Memmi in zweifacher Hinsicht aufgefasst. Sie drückt sich zum einen als „Entindividualisierung“ oder „Entpersönlichung“, die gleichsam mit einer „Entmenschlichung“ einhergeht, zum anderen als „Verabsolutierung“ oder „Verewiglichung“ aus; er spricht in diesem Sinne von einer „doppelten Verallgemeinerung“. „Die Beschuldigung richtet sich fast immer zumindest implizit gegen fast alle Mitglieder der Gruppe, so daß jedes andere Mitglied derselben Beschuldigung ausgesetzt ist, und sie ist zeitlich unbegrenzt, so daß kein denkbares Ereignis in der Zukunft dem Prozeß jemals ein Ende machen kann“ (Memmi, S. 114).[45] Das Individuum wird nicht mehr für sich betrachtet, sondern als Mitglied einer Gruppe, deren Eigenschaften es zwangsläufig, a priori besitzt, es wird entindividualisiert. „Zugleich verdient die gesamte Fremdgruppe, der das Stigma des Schädlichen und Aggressiven anhaftet, daß man sie angreift; umgekehrt verdient jeder Angehörige der Fremdgruppe a priori die Sanktion […]“ (Memmi, S. 116).[45] Mit dem Verlust der Individualität geht der Verlust der persönlichen und menschlichen Rechte und Würde einher. Der Mensch wird nicht in differenzierender Weise beschrieben; „er hat nur das Recht darauf, in einem anonymen Kollektiv zu ertrinken“ (vgl. Memmi, S. 183–186).[45] Jeder wirkliche oder erfundene Mangel des Einzelnen wird auf die ganze pseudoverwandtschaftliche Gruppe ausgedehnt, und gleichzeitig wird der Einzelne aufgrund eines kollektiven Makels verurteilt. „Individuelles und kollektives Merkmal stehen in einer Art dialektischem Verhältnis zueinander“ (vgl. Memmi, S. 170 f.).[45]
Die andere Form der Verallgemeinerung ist die zeitliche Unbegrenztheit der Beschuldigungen. „Der Rassist möchte in dem Stempel, den er dem Gesicht seines Opfers aufdrückt, dessen endgültige Züge sehen. Nicht nur, daß das Opfer einer Gruppe angehört, deren Mitglieder alle diese Makel tragen, sie tun es außerdem für immer. Damit hat alles seine Ordnung für die Ewigkeit. Ein für allemal sind die Bösen böse und die Guten gut […]“ (Memmi, S. 117 f.).[45]
Für Memmi dient Rassismus primär der Herrschaftssicherung, Sinn und Zweck des Rassismus liegt in der Vorherrschaft (Memmi, S. 60).[45] Sekundär kompensiert er psychische Defizite, „man festigt die eigene Position gegen den Anderen. Psychoanalytisch gesprochen ermöglicht der Rassismus eine individuelle und kollektive Stärkung des Ichs“ (Memmi, S. 160).[45] „Um groß zu sein, genügt es dem Rassisten, auf die Schultern eines anderen zu steigen“ (Memmi, S. 202).[45]
Während bei Memmi die Wertung ein zentrales Element darstellt, verzichtet George M. Fredrickson vollständig auf dieses Kriterium, wodurch seine Definition auch bestimmte ethnozentrische, vor allem aber ethnopluralistische Konzepte einschließt (vgl. Fredrickson, S. 18 f.).[7] Fredricksons Theorie oder Konzeption des Rassismus aus dem Jahr 2002 basiert lediglich auf zwei Komponenten: „Differenz“ und „Macht“.
„Rassismus entspringt einer Denkweise, wodurch «sie» sich von «uns» dauerhaft unterscheiden, ohne dass es die Möglichkeit gäbe, die Unterschiede zu überbrücken. Dieses Gefühl der Differenz liefert ein Motiv beziehungsweise eine Rechtfertigung dafür, dass «wir» unseren Machtvorteil einsetzen, um den ethnorassisch Anderen auf eine Weise zu behandeln, die wir als grausam oder ungerecht ansehen würden, wenn Mitglieder unserer eigenen Gruppe davon betroffen wären.“
„Wollten wir eine knappe Formulierung wagen, so könnten wir sagen, dass Rassismus vorliegt, wenn eine ethnische Gruppe oder ein historisches Kollektiv auf der Grundlage von Differenzen, die sie für erblich und unveränderlich hält, eine andere Gruppe beherrscht, ausschließt oder zu eliminieren versucht.“
Nicht die „Differenz“, sondern bereits das „Gefühl der Differenz“ dient – nach Fredrickson – Rassisten als Motiv zur Machtausübung bzw. als Rechtfertigung, um „ethnorassisch Andere“ grausam oder ungerecht zu behandeln. Zur Konstruktion von „wir“ und „sie“ bedarf es keines realen Unterschiedes, es reicht bereits ein «gefühlter Unterschied». Weder konkretisiert er die Art der Machtausübung, diese kann von „einer inoffiziellen, aber durchgängig praktizierten sozialen Diskriminierung bis zum Völkermord“ reichen (Fredrickson, S. 16 f.),[7] noch legt er fest, ob die Differenz biologischer, kultureller, religiöser oder sonstiger Natur ist. „Gewöhnlich greift die Wahrnehmung des Anderen als ‚Rasse‘ jedoch Differenzen auf, die in irgend einem Sinne „ethnisch“ sind. Nach der Definition des Politikwissenschaftlers Donald L. Horowitz gründet Ethnizität „auf einem Mythos gemeinsamer Abstammung, die zumeist mit vermeintlich angeborenen Merkmalen einhergeht. Eine gewisse Vorstellung von Merkmalszuschreibung und einer daraus resultierenden Affinität sind vom Konzept der Ethnizität untrennbar.“ Die Kennzeichen und Identifizierungsmerkmale, an die man dabei gewöhnlich denkt, sind Sprache, Religion, Bräuche sowie (angeborene oder erworbene) physische Eigenschaften. Eines oder mehrere davon (manchmal alle), können als Quellen ethnischer Verschiedenheit dienen; jedes von ihnen kann Verachtung, Diskriminierung oder Gewalt seitens der anderen Gruppe hervorrufen, die das Merkmal oder die Merkmale, die zum Kriterium des ethnisch Anderen geworden sind nicht teilt. Man kann, wie ich es in einem früheren Essay einmal getan habe, das Wesen des Rassismus als hierarchisch geordnete Ethnizität beschreiben; mit anderen Worten, Differenz wird unter Einsatz von Macht zu etwas, das Haß erregt und Nachteile mit sich bringt“ (Fredrickson, S. 142).[7]
Während Memmi den Fokus auf die Hierarchisierung, also die Wertung, der Differenzen legt, betont Fredrickson besonders deren Verabsolutierung; die «Differenz», die „ethnorassische“ Andersartigkeit muss dauerhaft sein und ohne die Möglichkeit, die Unterschiede zu überbrücken. Die Gruppenkonstruktion wird dadurch biologisiert oder auch essentialisiert, dass die ethnischen, kulturellen oder sonstigen Differenzen zu unüberbrückbaren, quasibiologischen Unterschieden erklärt werden; die Gruppenkonstruktion wird zum Rassenäquivalent. „Zwar mögen Shoah und Entkolonialisierung auf Dauer Regimes in Mißkredit gebracht haben, die ich als ‚offen rassistisch‘ bezeichnet habe; doch sollte diese gute Nachricht nicht zu der Überzeugung aufgebauscht werden, der Rassismus als solcher sei tot oder liege im Sterben […] Was als «neuer Rassismus» in den USA, Großbritannien und Frankreich bezeichnet wurde, ist eine Denkweise, die kulturelle Differenzen anstelle von genetischer Ausstattung verdinglicht und zu Wesensunterschieden erstarren lässt, die also mit anderen Worten Kultur zum funktionalen Äquivalent von Rasse macht“ (Fredrickson, S. 144).[7] „Von der Existenz einer rassistischen Einstellung kann man sprechen, wenn Differenzen, die sonst als ethnokulturelle betrachtet werden, für angeboren, unauslöschlich und unveränderbar erklärt werden“ (Fredrickson, S. 13).[7]
Rassismus, so Fredrickson, „leugnet die Möglichkeit, dass die Rassisten und ihre Opfer in derselben Gesellschaft zusammenleben können, es sei denn auf der Grundlage von Herrschaft und Unterordnung“. In Anlehnung an Pierre-André Taguieff spricht er von Rassismen der Inklusion und solchen der Exklusion.[55] „Ebenfalls gilt als ausgeschlossen, dass die ethnorassische Differenz aufgehoben werden kann, wenn Menschen ihre Identität ändern“ (Fredrickson, S. 17).[7] Dauerhaftigkeit und Unüberbrückbarkeit der Differenz sind für Fredrickson das entscheidende Merkmal, um Rassismen von anderen Formen der Intoleranz und Diskriminierung abzugrenzen. „Es könnte sinnvoll sein, einen anderen Begriff, etwa ‚Kulturalismus‘, zu verwenden, um die Unfähigkeit oder die mangelnde Bereitschaft zur Duldung kultureller Differenzen zu beschreiben; doch wenn eine echte Assimilation angeboten wird, würde ich auf die Verwendung des Rassismusbegriffs verzichten“ (Fredrickson, S. 14–15).[7]
Jedoch gelte es zwischen verschiedenen Konzeptionen von Kultur zu unterscheiden. „Geht man davon aus, dass Kultur historisch konstruiert ist und etwas Fließendes, zeitlich und räumlich Variables darstellt, das sich an äußere Umstände anpassen kann, dann ist der Begriff Kultur dem der Rasse diametral entgegengesetzt. Aber Kultur kann in einem solchen Maße verdinglicht und essentialisiert werden, dass sie zum funktionalen Äquivalent des Rassenbegriffs wird“ (Fredrickson, S. 15).[7] „Ein deterministischer kultureller Partikularismus kann das gleiche bewirken wie ein biologisch begründeter Rassismus […]“ (Fredrickson, S. 16)[7] Die Grenzlinie zwischen „Kulturalismus“ und Rassismus ist, nach Fredrickson, rasch überschritten, „Kultur und sogar Religion können so sehr zu Wesensmerkmalen erstarren, dass sie als funktionales Äquivalent für biologischen Rassismus dienen können. Das gilt seit einiger Zeit in gewissem Umfang für die Wahrnehmung der Schwarzen in den USA und Großbritannien sowie für die der Muslime in einigen vorwiegend christlichen Nationen“ (Fredrickson, S. 148).[7]
Für Christoph Butterwegge ist Rassismus ein „Denken, das nach körperlichen bzw. nach kulturellen Merkmalen gebildeten Großgruppen unterschiedliche Fähigkeiten, Fertigkeiten, und/oder Charaktereigenschaften zuschreibt, wodurch selbst dann, wenn keine gesellschaftliche Rangordnung (Hierarchie) zwischen ihnen entsteht, die Ungleichverteilung sozialer Ressourcen und politischer Rechte erklärt, also die Existenz von Privilegien bzw. der Anspruch darauf legitimiert, die Gültigkeit universeller Menschenrechte hingegen negiert wird.“[56]
Nach Manfred Kappeler benachteiligt Rassismus größere Gruppen von Menschen aufgrund ihrer biologisch oder kulturell begründeten Fremdheit und bestreitet ihren Anspruch auf Menschen- bzw. Bürgerrechte sowie Menschenwürde. Sein „zutiefst inhumaner Kern“ bestehe darin, dass er Menschen nicht als Persönlichkeiten mit eigenen Anlagen und Begabungen, sondern nur als Mitglieder ihrer »Rasse« oder ihres «Kulturkreises» ansehe und ihnen damit jede individuelle, über vermeintliche Kollektiveigenschaften hinausgehende Entwicklungsmöglichkeit abspreche.[57]
Für Philomena Essed ist Rassismus „eine Ideologie, eine Struktur und ein Prozeß, mittels derer bestimmte Gruppierungen auf der Grundlage tatsächlicher oder zugeschriebener biologischer oder kultureller Eigenschaften als wesensmäßig andersgeartete und minderwertige «Rassen» oder ethnische Gruppen angesehen werden. In der Folge dienen diese Unterschiede als Erklärung dafür, daß Mitglieder dieser Gruppierungen vom Zugang zu materiellen und nicht-materiellen Ressourcen ausgeschlossen werden. Rassismus schließt immer den Gruppenkonflikt hinsichtlich kultureller und materieller Ressourcen ein.“ „[…] Rassismus ist ein strukturelles Phänomen. Das bedeutet, daß ethnisch spezifizierte Ungleichheit in ökonomischen und politischen Institutionen, im Bereich von Bildung und Erziehung und in den Medien wurzelt und durch diese Strukturen reproduziert wird.“[58]
Damit erweitert sie den Begriff «Rassismus» dahingehend, dass sie damit nicht nur eine Ideologie oder konkrete historische Erscheinungsformen verbindet, sondern auch reale Strukturen und Prozesse, wodurch ihre Definition auch Phänomene, beispielsweise Alltagsrassismus oder institutionellen Rassismus, enthält.
Der Soziologe und Rassismusforscher Robert Miles, der mit seinem 1989 im Original und 1991 in deutscher Übersetzung erschienenen Buch über den Rassismus einen wesentlichen Beitrag zur Begriffsgeschichte leistete, versteht unter Rassismus einen „Prozess der Konstruktion von Bedeutungen“,[59] dessen Funktionsweise darin bestehe, „dass bestimmten phänotypischen und/oder genetischen Eigenschaften von Menschen Bedeutungen dergestalt zugeschrieben werden, dass daraus ein System von Kategorisierungen entsteht, wobei den unter die Kategorien subsumierten Menschen zusätzliche (negativ bewertete) Eigenschaften zugeordnet werden“.[60] Diese Definition betont den ideologischen Aspekt des Rassismus. Gleichzeitig verknüpft sie ihn mit dem „Prozess der Rassenkonstruktion“ und beschränkt ihn so auf seine klassische Variante.
Mark Terkessidis hat 1998 die Verengung der Rassismusdiskussion auf Vorurteile und Ideologie kritisiert.[61] In Anlehnung an Immanuel Wallerstein versteht er Rassismus als eine Trennung zwischen „Uns“ und „Ihnen“, die in der Moderne durch Ausschluss durch Einbeziehung konstituiert wurde. Durch die Sklaverei, die Kolonisierung und später durch die Arbeitsmigration wurden jeweils Gruppen von Menschen in ein System einbezogen und durch spezifische Ausgrenzungspraxen ausgeschlossen. Das „rassistische Wissen“ entstand, um die Praxis der Diskriminierung und die so entstandenen Trennungen zu legitimieren und zu erklären.
Terkessidis definiert Rassismus in drei Punkten: 1. Ausgrenzungspraxis (in Anlehnung an Robert Miles verstanden als Benachteiligung bei Verteilung gesellschaftlicher Ressourcen, Dienstleistungen und Positionen); 2. Rassifizierung (Festlegung einer Gruppe als natürliche Gruppe und gleichzeitig Festlegung der „Natur“ dieser Gruppe) und 3. „Differenzierende Macht“ (eine Form von Gewaltverhältnis, etwa die Macht, bestimmte Personen zu beherrschen, sie Sondergesetzgebungen zu unterstellen oder abzuschieben etc.). Nur wenn diese Elemente zusammenkommen, könne sinnvoll von Rassismus gesprochen werden.
Terkessidis weist auch darauf hin, dass der Aspekt der „Abwertung“ nicht immer vorhanden sein muss, sondern die Trennung zwischen „Uns“ und „Ihnen“ entlang letztlich beliebiger Eigenschaften selbst schon rassistischen Charakter haben kann. Insofern begreift er Rassismus als einen Apparat, in dem sich diskriminatorische Praxis und Wissensbestände ständig stützen. 
Fredrickson bemerkt, dass der Begriff „Rassismus“ häufig unpräzise und unreflektiert verwendet würde, „um die feindseligen oder negativen Gefühle eines ‚Volkes‘ oder einer ethnischen Gruppe gegenüber einer anderen und die aus dieser Einstellung resultierenden Handlungsweisen zu beschreiben“ (Fredrickson, S. 9).[7] Auf einem Workshop „Neue Begriffe für die Einwanderungsgesellschaft“ einigten sich 2013 die Teilnehmer der Gruppe „Rassismus“ darauf, dass Fälle von Rassismus dann vorlägen, wenn Menschen aufgrund von Zuschreibungen diskriminiert oder verfolgt würden. Rassistisches Denken gehe von der unveränderlichen Zugehörigkeit des Menschen zu einer Gruppe aus, die als der „eigenen“ Gruppe des Zuschreibenden unterlegen bewertet werde.[62]
Kurt Horstmann schlug vor, nicht jegliche Diskriminierung irgendwelcher Gruppen als Rassismus zu bezeichnen, und hält es für angebracht, etwa in der Flüchtlingsforschung auf den Ausdruck „Rassismus“ zu verzichten und stattdessen auf die Begriffe „Fremdenfeindlichkeit“, „Xenophobie“, „Ausländerfeindlichkeit“ und dergleichen auszuweichen.[63]
Canan Topçu, die mit ihren Eltern als achtjähriges Kind nach Deutschland gekommen war und dort aufgewachsen ist, bezeichnet die hier geführte Rassismusdebatte als „Desintegrationsdebatte“ und kritisiert insbesondere den Essayband Eure Heimat ist unser Albtraum: „Richtung und Tonalität der Rassismuskritik wird bestimmt von einer jungen akademisch gebildeten Generation, die einerseits darauf pocht, nicht auf Herkunft reduziert, sondern als ‚von hier‘ wahrgenommen zu werden, andererseits aber selbst Identitätspolitik betreibt – nicht nur durch die Selbstbeschreibung als People of Color, sondern auch im Zelebrieren von Elementen aus der Herkunftskultur. Politisch problematisch ist die moralische Überlegenheit, die aus der Betroffenheit abgeleitet wird, ohne selbst auf Ressentiments zu verzichten oder Ausgrenzung zu betreiben.“[64]
Die Frage, ob es im alten Griechenland und im alten Rom Rassismus gegeben habe, wird unterschiedlich beantwortet. Sie ist im Zusammenhang damit zu sehen, wie die antiken Griechen seit Homer und Herodot den Begriff „Barbaren“ verwendeten: Dieser bezog sich offenbar nur auf die Sprache.
David Theo Goldberg, der das „Konzept der Ausschließung“ als zentral für die Untersuchung und Unterscheidung rassistischer Diskriminierungen betrachtet,[65] verneint Rassismus, weil die Griechen die „Barbaren“ gerade nicht kategorisch verabscheuten (siehe Homer, Herodot, Aischylos, Xenophon und andere).
Auch Yves Albert Dauge bestreitet, dass es in der römischen Welt Rassismus gegeben habe.[66] Obschon in der Antike Überlegenheitsgefühle eines Stammes oder Volkes über andere Gruppen und ethnische, religiöse oder kulturelle Stereotype verbreitet waren, existiert für die Begriffe „Rasse“ oder „Rassismus“ kein exaktes Äquivalent in der griechischen oder lateinischen Sprache. Aus dem gleichen Grunde sieht auch Christopher Tuplin keine Veranlassung, von Rassismus in der griechischen Welt zu sprechen; die Diskussion des Rassismus müsse seiner Meinung nach eine Definition von Rasse einschließen.[67]
Autoren wie Christian Delacampagne oder Benjamin Isaac sind anderer Auffassung und betonen, dass einerseits dem Rassenbegriff analoge ideologische Konstruktionen existiert hätten und andererseits Rassismus ohnehin im Kern kulturell argumentiere.[68][69] Beide verweisen ausführlich auf Aristoteles’ Konstruktion des Barbaren und eine mit ihr betriebene Legitimation der Sklaverei. Barbaren sei ein minderes Menschsein zugeschrieben worden, weil sie nur bedingt über Vernunft verfügten.[70]
Benjamin Isaac benutzt für die Antike, neben „frühem Rassismus“ oder „antikem Rassismus“, hauptsächlich den Begriff „Proto-Rassismus“, der in den 1970er Jahren vom Ägyptologen Jean Yoyotte geprägt wurde.[71] Er will damit zweierlei zum Ausdruck bringen: Zwar habe es in der Antike eine Art von Rassismus gegeben, aber dieser habe sich vom klassischen Rassismus unterschieden, wie er sich im 18. und 19. Jahrhundert entwickelt hat. Doch ist der antike Rassismus insofern Proto-Rassismus, also Vorläufer des Rassismus, als er – nach Isaac – späteres rassistisches Denken beeinflusst hat. Für Isaac zeichnet sich Rassismus dadurch aus, dass hierbei Individuen oder ganze Gruppen von Menschen mit unveränderlichen körperlichen oder geistigen Eigenschaften in Verbindung gebracht werden. Einige Stereotype seien bereits in der Antike zur Legitimierung imperialistischer Aggressionen gegenüber „minderwertigen“ Völkern benutzt worden.
Die griechisch-römische Antike kenne zwar keine Theorie eines biologischen Determinismus, dennoch finde sich schon früh, spätestens ab dem 5. Jahrhundert v. Chr., die Vorstellung, dass Menschen je nach ihrer geografischen Herkunft entsprechende Eigenschaften besitzen.[72] Nach dieser Theorie seien die Menschen im heißen Süden intelligenter, wenn auch ängstlicher und zaghafter als die Menschen im kalten Norden, die auf Grund der unwirtlichen Landschaft erfinderisch, impulsiv, wenn auch leichtsinnig seien.[73] Athen und später dann Rom hätten sich als ideale Mitte zwischen Extremen gesehen, wobei das angenehme Klima Griechenlands und Italiens als Argument gedient habe. Proto-Rassismus gibt es nach Isaac zum einen also in diesen anthropogeografischen Vorstellungen – zum anderen hat vor allem Aristoteles (und nach ihm andere) die Ansicht vertreten, dass gewisse Menschen zum Sklavendasein geboren wurden. Es gibt gemäß dieser Ansicht Menschen höherer Ordnung und solche einer niedrigeren Ordnung. Auch diese Unterscheidung zeugt, nach Isaac, von Proto-Rassismus: The question to be considered is what are the explanations given in ancient literature for the presumed superiority or inferiority of specific groups. If these consist of theories regarding heredity or unalterable exterior influences, it is possible to speak of proto-racism.
Ansätze zu einem (Proto-)Rassismus zeigte sich nach Isaac in der Antike als sogenannte „Klimatheorie“. Sie spiegelt sich erstmals in der hippokratischen Schrift Über die Umwelt (lateinisch De aeribus aquis locis). Im Hinblick auf das mythische Volk der „Makrokephalen“, welches der Verfasser von De aeribus beschrieb, wird dies mit der Vorstellung der Vererbbarkeit der entsprechenden Merkmale vermengt. Die Ausführung der Theorie bleibt jedoch uneindeutig – sicher nicht zuletzt wegen des beschränkten Wissens damaliger Zeit hinsichtlich der Erbbiologie. Der Klimatheorie ist in De aeribus immer die Theorie der Inferiorität von Fremdvölkern aufgrund ihrer politischen Verfassung (Despotie) beigeordnet. Ob nun die Politik und Ordnung (Nomos) oder die Natur des Menschen (Physis) ausschlaggebend für das Bild des Fremden sein sollte, ist nicht genau zu beantworten.
Vincent Rosivach schrieb,[74] dass das (meist) rote und blonde Haar der Thraker und anderer Völker im Norden Griechenlands oft als Kennzeichen der minderwertigen Menschen galt. Thraker bildeten eine ethnisch geschlossene Gruppe von Sklaven im Athen archaischer Zeit. Sie sind unter Solon angekauft worden. Menschen mit diesem Genotyp traten in Athen fast ausschließlich als Sklaven auf. Entsprechende Assoziationen seitens der griechischen Bevölkerung waren die Folge. In Komödien wurden die Charaktere von Sklaven ausschließlich mit rotem Haar dargestellt. „Rot-“ bzw. „Blondschopf“ waren typische Sklavennamen.
Gegen die Annahme der Existenz eines Hautfarbenrassismus in der Antike wendete sich seit den 1980er Jahren Frank M. Snowden, Jr.
Platon setzt in seiner Politeia die drei Seelenteile in Beziehung zu den einzelnen Fremdvölkern zugewiesenen Charaktereigenschaften; ihm gelten Thraker und Skythen als kriegerisch, Phönizier und Ägypter als erwerbsstrebig.[75] Sein Schüler Aristoteles nennt die gleichen Beispiele kriegerischer Völker.[76] Thraker und Skythen, die beiden Fremdvölker im Norden, werden also von beiden als kriegerisch benannt; als zum Herrschen bzw. zur besten Herrschaft geeignet nennen beide ausschließlich das eigene Volk.
Eine einfachere Differenzierung als Platon nimmt Aristoteles vor, wenn er ein Europa-Asien-Gefälle unter den nichtgriechischen Völkern behauptet, die kleinasiatischen seien „sklavischer“.[77] Nach Aristoteles seien diejenigen, die von Natur aus sklavisch seien, nicht eindeutig von der Natur durch körperliche Erscheinung und charakteristische Merkmale gekennzeichnet.[78] Die servile Eigenart wird den Barbaren insbesondere deswegen von Aristoteles zugesprochen, da es ihnen an den politischen Strukturen mangele, die eine Gemeinschaft der Freien und Gleichen ermöglichen.[79]
In Asien gibt es ebenfalls weit zurückreichende Formen Diskriminierung, die klassenbezogene und kulturbezogene Grundlagen hatten und auch ohne Rassenbegriff funktionierten. Die Chinesische Kultur entwickelte schon Jahrhunderte vor den Griechen kulturalistische Vorstellungen von Barbaren. Nachdem sie ursprünglich davon ausgingen, dass diese durch den Kontakt mit der chinesischen Kultur zivilisiert werden könnten, wurden sie schließlich mit Tieren verglichen, die kulturell grundsätzlich defizitär seien. Frank Dikötter hat darauf hingewiesen, dass es im Kaiserreich China eine lang währende eigene rassistische Tradition gab, ehe man dort mit dem europäischen Rassengedanken in Kontakt kam.
Das gilt auch für Indien, wo Kastenschema und Unberührbarkeit mit Hilfe von organischen Metaphern (Purusha) und Vermischungsverboten legitimiert wurden. Diese Biologisierung sozialer Unterschiede war durchaus nicht einzigartig. Sie wurde im Zuge des europäischen Imperialismus und mit Hilfe des auf sie gestützten arischen Mythos einer völkischen Interpretation unterzogen, die behauptete, das Kastenschema wäre das Produkt hellhäutiger arischer Einwanderer, die die dunkelhäutige Urbevölkerung unterworfen hätten. Gail Omvedt schreibt dazu: „Punjabi Brahmans and Punjabi Untouchables were ethnically the same, and Tamil Brahmans and Tamil Untouchables were not racially different.“ (etwa: „Die Brahmanen des Pundschab und die Unberührbaren des Pundschab waren ethnisch identisch, und die tamilischen Brahmanen unterschieden sich in der Rasse nicht von den tamilischen Unberührbaren.“)
Sozial begründete Kastendifferenzen gab es auch in Japan. Die rassistische Diskriminierung der Buraku, einer mit niederen und als unrein geltenden Tätigkeiten beschäftigten Kaste, reicht bis ins 14. Jahrhundert zurück. Neben diesem nach innen gerichteten Rassismus gab es auch die nach außen gerichtete rassistische Diskriminierung der Ainu. Sowohl auf die Buraku als auch auf die Ainu wurde später der von den Europäern entlehnte Rassenbegriff angewandt und so, wie Richard Siddle, Michael Weiner und andere gezeigt haben, deren auf Kastendenken und Kulturchauvinismus gestützte Diskriminierung übernommen.
Der Proto-Rassismus des europäischen Mittelalters ist widersprüchlich. Einmal ist es die Zeit eines umkämpften Bildes vom Afrikaner, zu dem Peter Martin Material zusammengetragen hat, das auch Wolfram von Eschenbachs schöne, schwarze Königin Belakane und den schwarzen, moslemischen Teufeln des Rolandsliedes zeigt. Später treten mit den judenfeindlichen Pogromen während des ersten Kreuzzuges und der großen Pest Ideologien und Praktiken der Ausgrenzung und Vernichtung zutage, die für Léon Poliakov und andere zur Geschichte des Antisemitismus und Rassismus gehören. Entgegenhalten lässt sich dem allerdings, dass die Ablehnung der Juden (siehe Antijudaismus) und Muslime sich vornehmlich religiös artikulierte.
Eine ausgeprägte Manifestation des Hautfarbenrassismus (Abwertung anderer wegen ihrer Hautfarbe) findet sich in der Zeit des Mittelalters in der arabischen Welt.[80] Als Erklärung musste die Sonneneinstrahlung herhalten, die Kinder würden dadurch im Mutterleib zu lange gekocht, wie ein anonymer Autor im Irak im 10. Jahrhundert schrieb: „so dass das Kind zwischen Schwarz und dunkel gerät, zwischen übelriechend und stinkend, kraushaarig, mit unebenmäßigen Gliedern, mangelhaftem Verstand und verkommenen Leidenschaften, wie etwa die Zanj (dh Ostafrikaner), die Äthiopier und andere Schwarze, die ihnen ähneln“.[81] Ebenfalls im 10. Jahrhundert bemerkt der 946 in Jerusalem geborene Geograph Al-Maqdisi: „Es gibt bei ihnen keine Ehen: das Kind kennt seinen Vater nicht; und sie essen Menschen… Was die Zanj angeht, so sind es Menschen von schwarzer Farbe, flachen Nasen… und geringem Verstand oder Intelligenz“.[82]
Auch die berühmtesten Gelehrten der arabischen und persischen Welt vertraten die Auffassung von der Minderwertigkeit der Schwarzen, so beispielsweise der persische Arzt Ibn Sina (Avicenna), der jüdisch-andalusische Philosoph Mosche ben Maimon (Maimonides), Sa’id al-Andalusi aus Toledo, welcher die angenommene Minderwertigkeit der Schwarzen auf zu starke Sonneneinwirkung zurückführte,[83] und Ibn Khaldun, welcher die angenommene Minderwertigkeit der Schwarzafrikaner zur Legitimation des ausgeprägten Sklavenhandels und Sklavenraubs, den die Araber in Subsahara-Afrika unternahmen, heranzog: „Daher sind in der Regel die schwarzen Völker der Sklaverei unterwürfig, denn (sie) haben wenig Menschliches an sich und haben Eigenschaften, die ganz ähnlich denen von stummen Tieren sind, wie wir festgestellt haben“[84] „sie haben die Angewohnheiten von Tieren, nicht von Menschen und essen einander auf“.[85] Die Konsequenzen waren beispielsweise, dass weiße Sklavinnen teurer waren als schwarze sowie dass die Aufstiegsmöglichkeiten für schwarze Sklaven geringer waren als für weiße,[86] obgleich aus Sicht des islamischen Rechts die Grundlage für die Versklavung in beiden Fällen gleich war (nämlich der Unglaube).

Das Jahr 1492 gilt mit der europäischen Entdeckung Amerikas und dem Alhambra-Edikt als Symbol für eine Vermengung und Überlagerung unterschiedlicher praktischer und ideologischer Formen von Diskriminierung.
Norman Roth und andere haben gezeigt, wie der Antisemitismus durch die Idee von der „Reinheit des Blutes“ (spanisch limpieza de sangre) in der Politik gegenüber den Juden seine moderne Form anzunehmen begann. Mit der Frage nach der Blutsreinheit und der Herkunft wurde bis zu einem Sechzehntelanteil – also über vier Generationen – angeblich jüdischen Blutes geforscht. Es galt sogar als gefährlich, christliche Kinder von Ammen aus konvertierten Familien stillen zu lassen, weil sich deren Milch angeblich schädlich auswirken könne.
Erste Begegnungen der Seefahrer aus Spanien 1492 mit dem indigenen Volk der Arawak verliefen friedlich. In seinem Logbuch betrachtete sie Christoph Kolumbus aber bereits zu diesem Zeitpunkt als zukünftige Untertanen oder gar als Sklaven.[87] Die Eroberung Amerikas hatte mit der massenweisen Versklavung und dem Genozid[88] an den Indianern, der nach Jared Diamond jedoch vor allem durch eingeschleuste Seuchen erfolgte,[89] und dem anschließenden „Ersatz“ durch Verschleppung afrikanischer Sklaven gleich zwei rassistische Dimensionen. In der Auseinandersetzung zwischen Bartolomé de Las Casas und Juan Ginés de Sepúlveda über die Frage, ob die indigene Bevölkerung des späteren Amerika Menschen seien und wie sie behandelt werden müssten, wurde auf den von Aristoteles geprägten Begriff des Barbaren zurückgegriffen. Andererseits begann sich aufgrund der Herausbildung einer vielfältig gemischten Gesellschaft ein an Hautfarben orientiertes Kastensystem zu entwickeln, das zahlreiche Schattierungen kannte. Imanuel Geiss hat eine der gängigen Unterteilungen dokumentiert:
„Aus Spanier und Indianerin entsteht Mestize. Aus Spanier und Mestizin entsteht Kastize. Aus Kastize und Spanierin entsteht Spanier. Aus Spanier und Negerin entsteht Mulatte. Aus Spanier und Mulattin entsteht Morisco. Aus Spanier und Morisca entsteht Albino. Aus Spanier und Albina entsteht Torna Atras. Aus Indianer und Negerin entsteht Lobo. Aus Indianer und Mestizin entsteht Coyote. Aus Lobo und Indianerin entsteht Chino. Aus Chino und Negerin entsteht Cambuxo. Aus Cambuxo und Indianerin entsteht Tente en el aire. Aus Tente en el aire und Mulattin entsteht Albarasado. Aus Albarasado und Indianerin entsteht Varsino. Aus Varsino und Cambuxa entsteht Campamulatte.“
Mestize, Gemälde 1780, Colección de Malu y Alejandra Escandón, Ciudad de México
„Lobo“, Gemälde ca. 1780,
Mulatte, Gemälde ca. 1780, Collección de Malu y Alejandra Escandon, Ciudad de México
Colin Tatz, Direktor des Centre for Comparative Genocide Studies in Sydney, erläutert in diesem Zusammenhang, dass dieser sogenannte Rassismus ohne Rassen kein neues, sondern ein altes Konzept ist. Den europäischen Kolonialherren in Amerika stand der Rassenbegriff noch nicht zur Verfügung. Sie bedienten sich zur Legitimation ihres Vorgehens der überkommenen kulturalistischen Vorstellung von Barbaren als minderwertigen Menschen.
Im Zuge der Besiedelung Amerikas kamen weitere rassistische Aspekte zum Ausdruck: als Eroberung mit ausgrenzenden Folgen für die Indianer, als transatlantische Sklaverei und als Machtkampf um die Teilhabe an einer postulierten weißen Vorherrschaft.
Die europäische Kolonisierung Amerikas ab dem 16. Jahrhundert ging mit Massenversklavungen und dem atlantischen Sklavenhandel einher, durch die Afrikaner in alle Teile Amerikas verschleppt und als billige Arbeitskräfte eingesetzt wurden: in britischen, niederländischen, französischen und spanischen Kolonien (später USA, Brasilien und die europäischen Kolonien in der Karibik).[90]
Sklaverei existierte auch bei den Indianern Nordamerikas, jedoch nicht in allgemeiner Verbreitung. Zunächst nutzten sie wie die Europäer zur Legitimation ihres Vorgehens überkommene Vorstellungen über die in Kriegen Unterlegenen, und die Gouverneure der Kolonien versuchten eine Aversion zwischen Indianern und Schwarzen zu schüren, um Kooperation oder Kollusion zu verhindern. Während z. B. die Seminolen entflohenen afroamerikanischen Sklaven Zuflucht gewährten (Schwarze Seminolen), führten etwa die Cherokee nach ihrer versuchten Anpassung an die Gesellschaft der europäischen Einwanderer (siehe Fünf Zivilisierte Stämme) ebenfalls die Sklaverei ein und betrieben sie in ähnlicher Härte wie die europäischen bzw. US-amerikanischen Sklavenbesitzer.[91][92][93]
Die transatlantische Sklaverei war ein System, das neben seinem ökonomischen Kalkül den „sozialen Tod“ der Sklaven bezweckte. Laut Orlando Pattersons Analyse liegt der Kern rassistischer Diskriminierung in der Zerstörung der sozialen und kulturellen Identität derer, die ihr unterworfen sind bzw. werden. Schätzungen über die Anzahl der Betroffenen schwanken zwischen 11 Millionen und 15 Millionen. Die wichtigsten europäisch geprägten Betreiber dieser Politik waren im 18. Jahrhundert (laut Zahlen, die Albert Wirz wiedergab): „1. England mit einem Anteil von 41,3 %, 2. Portugal (29,3 %), 3. Frankreich (19,2 %), 4. Holland (5,7 %), 5. Brit. Nordamerika/USA (3,2 %), 6. Dänemark (1,2 %), 7. Schweden und Brandenburg (0,1 %).“
Ab dem 17. Jahrhundert entwickelte sich der Besitz von Sklaven neben dem Landbesitz zu einem zentralen Statusmerkmal.[94] Die Sklavenfrage entzweite in den USA zunehmend die Süd- von den Nordstaaten. In den Nordstaaten setzte die Industrialisierung ein und die Anzahl der Sklaven nahm langsam ab,[95] während die Besitzer der riesigen Reis- und Baumwollplantagen in den Südstaaten weiterhin Sklaverei in wachsendem Ausmaß betrieben.
In der viel beachteten Präambel zur Unabhängigkeitserklärung hatte Thomas Jefferson das Leben, die Freiheit und das Streben nach Glück zum unveräußerlichen Menschenrecht erklärt. Die Sklaverei geriet (obwohl sie dort nicht direkt angesprochen wurde) unter Rechtfertigungsdruck.[96]
Anfangs wurde die Sklaverei überwiegend mit religiösen und philosophischen Erwägungen verteidigt; später verwendeten Befürworter überwiegend „wissenschaftliche“ Rechtfertigungen. Zum Beispiel wurden unterstellte biologische Unterschiede wie etwa eine andere Blutfarbe oder die angeblich kleineren Gehirne von Schwarzen als Beweis(e) für die Unterlegenheit der schwarzen „Rasse“ gewertet. Auch statistische und psychologische Argumente wurden verwendet, wie z. B. die Behauptung, dass Geisteskrankheiten unter Sklaven viel seltener sind als unter freien Schwarzen. „Drapetomanie“ (der Wunsch wegzulaufen) wurde als eine psychiatrische Diagnose erfunden.[97] Solche Rassismen (wissenschaftlicher Rassismus), die angebliche Erkenntnisse aus den Natur- und Sozialwissenschaften heranziehen, um rassistische Praktiken zu begründen und zu rechtfertigen, nahmen nach der Abschaffung der Sklaverei noch deutlich zu.[98]
Der Rassismus entwickelte sich unterschiedlich, die Bewegung zur Abschaffung der Sklaverei (siehe Abolitionismus) hatte in den Nordstaaten stärkeren Zulauf als in den Südstaaten. Auch nach der formalen Abschaffung der Sklaverei unter Abraham Lincoln existierten jedoch noch weiterhin Probleme des Rassismus, und noch bis ins 20. Jahrhundert wurde von einigen Historikern die These vertreten, dass die Sklaverei für Schwarze zu ihrer Zivilisierung nötig sei.
Im 17. Jahrhundert war der Rassismus unter den weißen Bediensteten, die ähnliche Arbeiten verrichteten, in den Kolonien noch kaum ausgebreitet. Der Historiker Kenneth M. Stampp, Verfasser mehrerer Standardwerke zur Sklavereigeschichte, beurteilte die schwarzen und weißen Arbeiter allgemein als „bemerkenswert uninteressiert an den sichtbaren Unterschieden“.[99] Dies sorgte für Unbehagen bei den Besitzern, und es wurde als Gegenmaßnahme z. B. in Virginia 1691 ein Gesetz zum Verbot von Ehen zwischen Weißen und Schwarzen oder Indianern erlassen.[99] Teils halfen die weißen Arbeiter auch den schwarzen Sklaven bei Widerstandsaktionen. Ab dem 18. Jahrhundert nahmen mit dem Anwachsen der rasseneingeteilten Sklaverei und dem Einsetzen der weißen Arbeiter als deren bezahlte Aufseher der Rassismus zu und die Rebellionen von weißen Bediensteten ab.[100]
Das System der White Supremacy nahm in Amerika unterschiedliche Formen an, die jeweils Weißsein als zentrale Norm der Teilhabe an politischen Rechten und sozialen Entfaltungsmöglichkeiten setzten.[101] In Brasilien schlug sie sich unter anderem in der Politik des branqueamento nieder, mit der die „weißen“ Brasilianer die „brasilianische Rasse“ verbessern und durch Zumischung von mit Hilfe von europäischen Einwanderern importierten „weißen Blutes“ das „schwarze Element“ in der brasilianischen Bevölkerung bis zum Jahre 2012 zum Verschwinden bringen wollten. Brasilien gilt auch als extremes Beispiel für die „soziale Konstruktion“ von Rasse, wo eine direkte Zuweisung von Hautfarbe und sozialem Erfolg (bis heute) der Fall ist und sich bei einer Person der soziale Aufstieg auch in der Einordnung in eine „weißere“ Farbklasse widerspiegelt.[102]
In den USA kam die White Supremacy nicht nur in der Politik der Rassentrennung zum Ausdruck, sondern äußerte sich auch als Verdacht ungenügender „Weißheit“ gegenüber verschiedenen europäischen Einwanderergruppen. Karen Brodkin hat für die Juden und Noel Ignatiev für die Iren beschrieben, wie diese in langwierigen und schmerzhaften Prozessen „weiß werden“ beziehungsweise Anteil an der lokalen Führungsschicht erlangen konnten. Die irischstämmigen Amerikaner hätten ihre „Weiße“ in einem rassistischen Qualifikationsprozess, das heißt durch teilweise gewalttätige wie gehässige Absetzbewegungen von anderen Minderheiten, überhaupt erst errungen.
Umgekehrt stellte der Anthropologe John Ogbu die umstrittene These vom „acting white“ (weiß agieren oder auch schauspielern) auf, nach der die schwarze Minderheit (ehemaliger Sklaven) in den USA einen als kastenartig beschriebenen internen Zusammenhalt aufweise und dadurch Schwarzen selber den Aufstieg verwehre.
Vertreter der Black Supremacy (englisch für „schwarze Vorherrschaft“, „Überlegenheit der Schwarzen“), Ideologien, die insbesondere in den 1920er Jahren und 1960er Jahren eine Überlegenheit schwarzer Menschen gegenüber nicht-schwarzen Menschen propagierten, sehen in ihr eine Antwort auf und einen Gegenbegriff zu White Supremacy.[103]
Im Zeitalter des Imperialismus ließ Leopold von Belgien eine Schreckensherrschaft (Kongogräuel) im Kongo errichten. In Australien führte der Rassismus der Arbeiterbewegung zur exklusiven „weißen“ Staatsgründung unter dem Motto „White Australia“. In Ostasien fiel das europäische Vorbild auf fruchtbaren Boden und ließ Japan sich als Hoffnung der nichtweißen Rassen präsentieren. in den USA wurde die Ideologie des manifest destiny auf imperiale Politik übertragen und als Zivilisationsmission ausgegeben.
Mit dem stärkeren Nachwandern von britischen und europäischen Frauen und nach dem Sepoyaufstand wurden die Angloinder von Briten wie Indern stärker separiert und gemieden und spielen bis in die Gegenwart als Anglo-Indian eine besondere Rolle.
Die Modernisierung der Meiji-Zeit führte in Japan auch zur Entwicklung imperialistischer Ambitionen, die unter anderem im Ersten Japanisch-Chinesischen Krieg und im Russisch-Japanischen Krieg umgesetzt wurden. Unter der Parole „Asien den Asiaten!“ bediente man sich dabei einerseits einer ideologischen Umkehrung des europäisch-amerikanischen Stereotyps von der „Gelben Gefahr“ und warnte die asiatische Staatengemeinschaft vor der „weißen Gefahr“. Andererseits wurde die eigene aggressive und expansionistische Kolonialpolitik mit rassistischem Paternalismus legitimiert. Danach sollte sich die asiatische Bevölkerung aus den „fünf Rassen“ der Japaner, Chinesen, Koreaner, Mandschu und Mongolen zusammensetzen, von denen die japanische „Yamato-Rasse“ am weitesten entwickelt und am fortschrittlichsten und deswegen berufen wäre, die anderen zu erleuchten, kulturell und moralisch zu vervollkommnen und vor allem zu führen. Bis heute werden – so Jared Diamond – in Japan Untersuchungen, nach denen mit gewisser Wahrscheinlichkeit die Japaner selber hauptsächlich von koreanischen Einwanderern abstammen, nicht ohne Widerstände zur Kenntnis genommen.
Als der von Japan bei den Friedensverhandlungen von Versailles eingebrachte Vorschlag einer Erklärung zur Gleichberechtigung der Rassen trotz mehrheitlicher Zustimmung zurückgewiesen wurde, verstärkte dieses seine imperialistischen Anstrengungen im pazifischen Raum.[104] Die sich zuspitzenden Widersprüche zwischen den japanischen und den Ambitionen Englands und der USA führten schließlich zu der als „Rassenkrieg“ geführten militärischen Auseinandersetzung, die John Dower, Gerald Horne und andere beschrieben haben.
Historisch gesehen gab es in Japan stets eine Diskriminierung der Buraku. Noch heute werden viele Menschen der Minderheit der Buraku in Japan diskriminiert. Obwohl sie sich weder in Religion, Sitten noch im Aussehen merklich von anderen Japanern unterscheiden, galten sie als eigene Rasse. Sie wurden teilweise sogar als Hinin (非人, „Nicht-Menschen“) bezeichnet. Sie mussten in bestimmten Ortschaften leben, ihre Kinder durften keine normalen Schulen besuchen und sie durften nur als unrein betrachtete Berufe wie den des Totengräbers ausüben. Im Jahre 1871 wurden die Buraku den anderen Japanern rechtlich gleichgestellt.
Noch heute haben die Buraku mit Diskriminierung zu kämpfen. Da auch der Familienname Auskunft über die Herkunft geben kann, ist es den Nachfahren der Burakumin seit einigen Jahren erlaubt, ihren Namen zu ändern.
Von 1915 bis 1917 wurden im heutigen Ostanatolien ansässige Armenier im Osmanischen Reich Opfer eines Genozids.
Ansätze rassistischer Theoriebildung gab es in Deutschland bereits in der ersten Hälfte des 19. Jahrhunderts, die unter anderem von Ernst Moritz Arndt und Friedrich Ludwig Jahn[105] rezipiert wurden.
Ab 1884 beteiligte sich Deutschland mit dem Erwerb der deutschen Kolonien und Schutzgebiete am Imperialismus und Kolonialismus. Auch in Deutschland berief man sich auf die angebliche Überlegenheit der Nordeuropäer.
Das wirtschaftlich und militärisch erstarkte Deutschland widmete sich zunehmend der Weltpolitik. Unter dem Einfluss der um die Jahrhundertwende aufkommenden Alldeutschen Bewegung und Völkischen Bewegung erstarkte der Antisemitismus und Antislawismus. Die Idee vom Lebensraum im Osten zulasten „minderwertiger“ Völker wurde geboren.
Seit den Teilungen Polens lebten im deutschen Kaiserreich auch zahlreiche Polen. Ab 1880 betrieb das Deutsche Reich im geteilten Polen eine verschärfte Germanisierungspolitik, durch die Schaffung der „Preußischen Ansiedlungskommission“ sollten laut Bismarck deutsche Neuansiedler einen „lebendigen Wall gegen die slawische Flut“ bilden.[106] Im Laufe der Industrialisierung setzten die ostelbischen Großgrundbesitzer viele polnische Arbeiter ein, die als minderwertige Slawen angesehen und diskriminiert wurden.[107] Die im Bergbau tätigen Ruhrpolen galten als „Lohndrücker und Einschlepper von Krankheiten“ und unterlagen kommunalen Polenüberwachungsstellen.[108]
1899 wurde die „Reichszentrale zur Bekämpfung des Zigeunerunwesens“, kurz „Zigeunerzentrale“, in München zur polizeilichen Erfassung von Roma, Sinti und Anderen, die als „Zigeuner“ oder „Jenische“ als „nach Zigeunerart umherziehende Personen“ bezeichnet wurden, gegründet. Sie wurden systematisch diskriminiert, durch Sondergesetze kriminalisiert und unabhängig von Straftaten sukzessive erkennungsdienstlich erfasst.
1900 kam es in China zum Boxeraufstand gegen die Kolonialmächte. Diese schlugen den Aufstand unter deutscher Beteiligung in einer brutalen Art der Kriegsführung auch gegen die Zivilbevölkerung nieder. Die „Strafexpeditionen“, welche das deutsche Expeditionskorps ab September 1900 durchführte, waren in besonderer Weise von einem rassistischen Rachegedanken geleitet.
Kaiser Wilhelm II. hatte den deutschen Soldaten auf den Weg gegeben, sie mögen den Namen Deutschland in China in einer solchen Weise bekannt werden lassen, „dass niemals wieder ein Chinese es wagt, etwa einen Deutschen auch nur scheel anzusehen“. Diese sogenannte „Hunnenrede“ wird rückblickend als brutaler Ausdruck „eines sozialdarwinistisch aufgeladenen Gesinnungsmilitarismus“ rezipiert. Der Sinologe Klaus Mühlhahn entdeckte in Wilhelms Rede zahlreiche religiöse Ausdrücke, die ihn veranlassten, den Boxerkrieg vor allem als einen Glaubenskrieg zu deuten.[109]
Der Aufstand der Herero und Nama in Deutsch-Südwestafrika führte 1904 zum Völkermord an den Herero und Nama.[110] Im Konzentrationslager Shark Island (Haifischinsel) wurden vereinzelt medizinische Menschenversuche an Häftlingen durchgeführt. Leichenpräparate von Gefangenen wurden auch zur Rassenforschung nach Deutschland gesandt.[111][112] Die deutsche Literatur der Zeit schwelgte in rassistischen Phantasien und forderte kurzen Prozess mit der „schwarzen Masse“.[113]
Ab 1905 erfolgte in den Kolonien ein Verbot der „standesamtlichen Eheschließung zwischen Weißen und Eingeborenen“ und außereheliche Sexualbeziehungen wurden von der Gesellschaft geächtet, um die „Verkafferung“ zu unterbinden. 1912 kam es zur Mischehendebatte im deutschen Reichstag.[114] Die Verbote bestanden bis zum Verlust der Kolonien im Ersten Weltkrieg weiter.
Während des Ersten Weltkriegs kämpften hunderttausende Afrikaner, Inder und Angehörige anderer Nationen im Dienste ihrer Kolonialmächte England (z. B. Gurkha) und Frankreich (z. B. Tirailleurs sénégalais) auf dem westeuropäischen Kriegsschauplatz. In der deutschen Presse wurden in den Kriegsjahren von 1914–1918 diese afrikanischen und asiatischen Soldaten als besonders bestialische und lüsterne Kämpfer dargestellt.[115]
Wegen des wachsenden Antisemitismus im Offizierskorps verbunden mit dem Vorwurf des Drückebergertums an die Juden wurde 1916 die Judenzählung im Deutschen Heer angeordnet.
In der Weimarer Republik – wie auch in Österreich – wurden die Juden im Rahmen der Dolchstoßlegende als hinterhältige Kriegsgewinnler dargestellt und es wurden jüdische Kriegsgräber geschändet. Als Gegenreaktion wurde der Reichsbund jüdischer Frontsoldaten gegründet. Rechtsradikale und völkische Gruppen riefen offen zum Mord an exponierten jüdischen Politikern wie z. B. dem Außenminister Walter Rathenau auf und es kam zu zahlreichen Gewalttaten.
1920 verkündete die Nationalsozialistische Deutsche Arbeiterpartei ihr 25-Punkte-Programm, das in den Punkten 4 bis 8 antisemitisch geprägt war. Die Deutsche Burschenschaft als Dachverband der deutschen und österreichischen Burschenschaften beschloss in Eisenach, den Rassestandpunkt einzuführen, so dass nur noch deutsche Studenten arischer Abstammung aufgenommen werden sollten.[116]
Von 1923 bis 1945 gab Julius Streicher die antisemitische Wochenzeitung Der Stürmer heraus. Ziel und Inhalt war die Diffamierung der Juden in Hetzartikeln.[117]
Die Agitation gegen die Besetzung des Rheinlandes war nicht nur in den Kampfblättern der extrem rechten Parteien bzw. politischen Gruppierungen von „rassistischer Begleitmusik“ durchzogen. Anlass boten hier besonders die teilweise aus Afrika stammenden französischen Besatzungstruppen. Die in dieser Zeitspanne geborenen Kinder einiger schwarzer Soldaten und deutscher Frauen wurden als Schwarze Schmach und zum Teil als „Gefahr für die deutsche Rassenreinheit“ instrumentalisiert. Die betroffenen Kinder wurden als sogenannte „Rheinlandbastarde“ später von den NS-Behörden erfasst und illegal zwangssterilisiert.[118]
Die neuen Musikrichtungen wie Swing und Jazz wurden von vielen Menschen speziell aus der völkischen Bewegung als undeutsch und „Negermusik“ angesehen und es kam zu häufigen Störungen von Musikveranstaltungen wie der Oper Jonny spielt auf. 1930 veröffentlichte der thüringische Volksbildungs- und Innenminister, der Nationalsozialist Wilhelm Frick, einen Erlass wider die Negerkultur für deutsches Volkstum.[119]
Rassismus war ein Teil der Ideologie des Nationalsozialismus. Nach der sogenannten „Rassenkunde“ postulierte die NS-Forschung die Existenz von Menschenrassen, die sie jeweils als unterschiedlich wertvoll ansahen und nach diesen Ansichten in eine Hierarchie einordneten. Sie teilten die gesamte Menschheit in drei Gruppen ein:
Juden wurden dabei der semitischen Rasse zugerechnet. Die Umsetzung der Theorie fand sich in den entsprechenden Nürnberger Rassengesetzen von 1835. Sie umfassten
Als „hochwertig“ eingestufte Menschen konnten nur aus der ersten Gruppe der kulturstiftenden Rassen stammen. Sexueller Kontakt zwischen Menschen, die einer „hochwertigen Rasse“ und denen, die einer „minderwertigen Rasse“ zugeordnet wurden, wurde als „Rassenschande“ bezeichnet. Die Nationalsozialisten unterstellten bestimmten von ihnen definierten Gruppen, die wie Juden oder die von ihnen als „Zigeuner“ Bezeichneten der Gruppe 3 zugerechnet wurden, dass sie „die Herrenrasse zersetzen“ wollten. Daher müssten sie zum Schutz der sogenannten „Volksgemeinschaft“ vernichtet werden. Auch die Slawen galten als minderwertige Menschen (im Sprachgebrauch der Nationalsozialisten „Untermenschen“) und wurden zur Gruppe 3 gerechnet.
Die theoretischen, pseudo-wissenschaftlichen und pseudo-juristischen Grundlagen lieferten neben Adolf Hitler selbst (Mein Kampf[122]) primär die NS-Ideologen Alfred Rosenberg und Hans F. K. Günther, der Justizminister Otto Georg Thierack, der Präsident am Volksgerichtshof und Richter Roland Freisler und einige weitere, in zahlreichen Publikationen. Allerdings ist dabei zu bemerken, dass ihre Gedanken wohl zumeist auf älteren rassistischen Theorien aufbauten und der Rassismus bis 1933 in ganz Europa relativ stark verbreitet war. Neu war am NS-Rassismus, dass die Wissenschaftsfreiheit unter politischen Vorbehalt gestellt wurde.[123] Unter den zahlreichen Rassetheoretikern des 19. und frühen 20. Jahrhunderts hatten der Franzose Arthur de Gobineau (1816–1882) mit dem Versuch über die Ungleichheit der Menschenrassen und der britisch-deutsche Schriftsteller Houston Stewart Chamberlain (1855–1927) mit den Grundlagen des neunzehnten Jahrhunderts den stärksten Einfluss auf die nationalsozialistische Rassenideologie. Zu den Bewunderern Chamberlains gehörten Kaiser Wilhelm II., Rosenberg und Hitler, der Chamberlain 1923 in Bayreuth traf.[124]
Die Opfer des NS-Rassismus wurden in der Zeit des Nationalsozialismus verfolgt, zwangssterilisiert, deportiert und ermordet. Die gesamte Gesundheitsvorsorge, Sozialpolitik sowie die Bevölkerungspolitik wurden unter „rassischen“ Gesichtspunkten gleichgeschaltet, die auch die Zulässigkeit von Eheschließungen bestimmten. Zu diesem Programm gehörten auch Ahnenpässe. Der aufgrund dieser Ahnenpässe zu führende Ariernachweis bzw. der „Große Ariernachweis“ war Bedingung für eine Karriere bei der SS. NS-Stellen verwendeten Eintragungen zu Geburten in alten Kirchenbüchern (mit ihnen ließen sich Stammbäume verifizieren); die Pfarrämter von Kirchengemeinden lieferten ihnen diese Informationen.
Eine direkte Folge des NS-Rassismus war der nationalsozialistische Völkermord an 5,6 bis 6,3 Millionen europäischen Juden während des Zweiten Weltkriegs, rund zwei Drittel aller damals lebenden europäischen Juden.[125]
1946 wurde die „Zigeunerpolizei“ bzw. „Landfahrerstelle“ in München nach dem Vorbild der Reichszentrale zur Bekämpfung des Zigeunerunwesens eingerichtet und im bayerischen Landeskriminalamt angesiedelt. Diese Behörde wurde 1970 wegen Grundgesetzwidrigkeit aufgelöst.
Der Bundesgerichtshof lehnte es 1956 ab, einem „Zigeunermischling“ Entschädigung für seine Zwangsumsiedlung im Jahre 1940 zu zahlen. Die von den Nationalsozialisten betriebene Ausgrenzungs- und Umsiedlungspolitik der „Zigeuner“ sei nicht „rassisch“ motiviert gewesen, sondern eine damals „übliche polizeiliche Präventivmaßnahme“ zur „Bekämpfung der Zigeunerplage“. 2015 distanzierten sich Richter des BGH von der Urteilspraxis ihrer Vorgänger, von denen viele bereits vor 1945 als Richter aktiv gewesen waren.[126]
1950 wurde die Konvention zum Schutze der Menschenrechte und Grundfreiheiten (Europäische Menschenrechtskonvention) des Europarates beschlossen. Die Vertragsstaaten vereinbarten ein Diskriminierungsverbot nach Rasse, Hautfarbe, Sprache und Religion (Artikel 14[127] und 12. Protokoll).
Die Bundesrepublik trat dem Internationalen Übereinkommen zur Beseitigung jeder Form von Rassendiskriminierung (ICERD) einem Menschenrechtsabkommen der Vereinten Nationen bei, das 1969 in Kraft trat. Es richtet sich gegen jede rassistische Diskriminierung aufgrund von Rasse, Hautfarbe, Abstammung, nationaler und ethnischer Herkunft.
In den 1990er Jahren kam es in der Bundesrepublik Deutschland, vermehrt in den Neuen Bundesländern, zu rassistisch motivierten Pogromen und Anschlägen. Die aufsehenerregendsten waren der Mordanschlag von Mölln, der Mordanschlag von Solingen, die Ausschreitungen in Rostock-Lichtenhagen, die Ausschreitungen von Hoyerswerda, die Hetzjagd in Guben, der Mordanschlag auf den Angolaner Amadeu Antonio Kiowa und die Magdeburger Himmelfahrtskrawalle. Viele dieser Ausschreitungen und Morde wurden von Jugendlichen oder jungen Erwachsenen verübt, die der sogenannten Neonaziszene zuzurechnen sind. Auch Sachbeschädigungen, die sich zum Beispiel gegen jüdische Friedhöfe richten oder als rassistische Graffiti sichtbar werden, waren keine Ausnahme.[128] Der sog. „Asylkompromiss“ von 1992 schränkte das Grundrecht auf Asyl ein.
Vorfälle mit rassistischem Hintergrund waren zuvor in West-Deutschland nur vereinzelt öffentlich wahrgenommen worden, wie zum Beispiel die 1981 erfolgte Selbsttötung des elfjährigen Tadesse Söhl, über dessen Beweggründe es erst infolge literarischer und filmischer Verarbeitung in den 1990er Jahren zur öffentlichen Diskussion kam.
Laut einem Bericht der Bundeszentrale für Politische Bildung über rassistische Vorurteile, geschrieben von Werner Bergmann, gab es von 1990 bis 2003 mehr als 100 Todesopfer rechtsextremer Gewalt in der Bundesrepublik Deutschland. Im Bericht wird erwähnt, dass in der Vergangenheit der Europarat und die Vereinten Nationen mehrmals Kritik am Vorgehen der deutschen Polizei gegenüber Ausländern geübt hätten. Einem Bericht der Europäischen Kommission gegen Rassismus und Intoleranz (ECRI) von 2003 zufolge sind „Schwarze“ als eine „äußerlich erkennbare Minderheit“ in Deutschland besonders von Rassismus betroffen.[129][130] Das Bundesamt für Verfassungsschutz zählt in seinem Bericht über das Jahr 2005 insgesamt 355 Straftaten mit fremdenfeindlichen und 49 Straftaten mit antisemitischen Motiven auf.[131]
In den Jahren 2000 bis 2006 wurden vermutlich durch den rechtsterroristischen Nationalsozialistischen Untergrund (NSU) zahlreiche völkisch-rassistisch motivierte Morde und Bombenanschläge begangen. Zwölf parlamentarische Untersuchungsausschüsse und ein Gerichtsprozess haben seitdem versucht, die einhergehenden zahlreichen Ermittlungsfehler zu klären.
Anhand der von Thilo Sarrazin mit abwertenden Aussagen zu Türken und Arabern in den Jahren 2009 (Interview in Lettre International) und 2010 (Deutschland schafft sich ab) ausgelösten und teilweise rassistisch geführten Migrationsdebatte zeigten die ICERD-Rüge der Vereinten Nationen und der 5.ECRI-Prüfbericht des Europarates den mangelhaften Schutz vor Diskriminierung und Hassreden in Deutschland auf. Die Bundesregierung versprach eine Untersuchung.[132][133]
Die Leiterin des Netzwerks „Schule ohne Rassismus – Schule mit Courage“ Sanem Kleff äußerte Mitte März 2018 in einem Interview, dass zwar entsprechende Zahlen im Zusammenhang mit der Ausübung körperlicher Gewalt zurückgingen, verbale Angriffe an deutschen Schulen jedoch massiv zunähmen.[134]
Der Sozialwissenschaftler Johannes Zuber kam in seiner 2015 veröffentlichten Studie zu dem Schluss, dass der gegenwärtige Rassismus in Deutschland kein Randphänomen darstelle, wie dies Politik und gesellschaftliche Eliten überwiegend behaupteten, sondern wieder ein Bestandteil des Lebensalltags in der deutschen Gesellschaft sei. Die biologistisch-rassistische Ideologie bleibe der theoretische Mittelpunkt abwertender, ausgrenzender sowie diskriminierender Praktiken und Verhaltensweisen. Erschreckend dabei scheinen aus heutiger Perspektive die tiefen Wurzeln, die biologistisch-rassistische sowie partiell nationalsozialistische und eugenische Theoreme in der deutschen Gesellschaft aufweisen.[135]
Zwischen 1848 und 1868 wurden im Kaiserreich Österreich viele vorher geltende diskriminierende Regelungen gegenüber Juden in Österreich aufgehoben. Die im Reichsteil Cisleithanien agierende Christlichsoziale Partei (CS) und ihre Vorläufer waren offen antisemitisch und machten die Juden, die sie als Vertreter des Finanzkapitals erachtete, für die wirtschaftliche Misere in Österreich nach dem Börsenkrach 1873 verantwortlich. Die wichtigste Figur der Partei war Karl Lueger. Bis in die 1890er-Jahre trugen die Wahlvereinigungen um Lueger noch Bezeichnungen wie „Antisemiten und Christlichsoziale“ oder nur „Antisemiten“.[136] Lueger war von 1897 bis zu seinen 1910 Wiener Bürgermeister. Zu Beginn des 20. Jahrhunderts machten Juden immerhin rund 4,5 Prozent der Gesamtbevölkerung von Österreich-Ungarn aus.
Die Historikerin Brigitte Hamann urteilte über Luegers Antisemitismus:
„Politisch ist es bedeutungslos, ob und wie viele jüdische Freunde Lueger privat gehabt haben mag. Von Bedeutung allein ist die Wirkung seiner aufhetzenden Reden – und diese war verheerend. […] Auch wenn kein Jude ermordet wurde, verrohten die Menschen, die von ihrem verehrten Idol in alten Vorurteilen bestätigt wurden.“[137]

Die CS trat auch in der Ersten Republik gegen die „Vorherrschaft des Judentums“ auf und war offen antisemitisch.[138] Die Haltung der austrofaschistischen Regierung von 1933/34 bis 1938 zur jüdischen Gemeinde in Österreich war dagegen zwiespältig. Einerseits wurden keine antijüdischen Gesetze erlassen, und jüdische Bürger konnten problemlos der Vaterländischen Front beitreten und sich in ihr betätigen. Andererseits unternahm das Regime keinerlei ernsthafte Anstrengungen, um die Juden in Österreich vor Übergriffen der Bevölkerung zu schützen.[139]Unmittelbar nach dem Anschluss Österreichs an den NS-Staat kam es in den Wochen nach dem 12. März 1938 zu pogromartigen Ausschreitungen gegen Juden und deren Eigentum. Mit Unterstützung der NSBO und nationalsozialistischer Mittelstandsorganisationen setzte ein regelrechter Arisierungswettlauf ein. Tausende von österreichischen Nationalsozialisten und deren Mitläufer nisteten sich im rechtsfreien Raum als kommissarische Verwalter in jüdischen Geschäften und Betrieben ein und konfiszierten gegen unleserliche Quittungen eigenmächtig Vermögen jüdischer Bürger.[140]
Rassismus in Südkorea wurde, insbesondere in den südkoreanischen Medien, als ein weit verbreitetes gesellschaftliches Problem anerkannt.[141] Zeitungen haben häufig über die Diskriminierung von Einwanderern berichtet und sie kritisiert, z. B. in Form von Unterschreitung des Mindestlohns, Einbehaltung von Löhnen, unsicheren Arbeitsbedingungen, körperlichem Missbrauch oder allgemeiner Verunglimpfung.[141]
In den deutschsprachigen Ländern wird bisweilen angenommen, dass Rassismus zumeist in Form von Fremdenfeindlichkeit bzw. Xenophobie (von griechisch ξενοφοβία „Furcht vor dem Fremden“, von ξένος xénos „fremd“, „Fremder“ und φοβία phobía „Furcht“) auftritt. Allerdings sind Rassismus und Xenophobie nicht einfach gleichzusetzen. Der Sozialwissenschaftler Dieter Staas weist darauf hin, dass Fremdenfeindlichkeit rassistisch motiviert sein kann, es aber nicht muss: Wenn zwei soziale Gruppen miteinander um Ressourcen konkurrieren oder miteinander schlechte Erfahrungen gemacht haben, stehen sie sich oft feindlich gegenüber, ohne den anderen rassistisch abzuwerten. Eine klare Trennung der Begriffe sei aber nur analytisch möglich, in der Realität enthalte Fremdenfeindlichkeit häufig rassistische Elemente.[142] Der Historiker Georg Kreis sieht ebenfalls keine scharfen Grenzen zwischen Rassismus und Fremdenfeindlichkeit: Aus der Opfersicht sei es wenig bedeutsam, welcher analytischen Kategorie eine Tat zugeschrieben werden. Beide Diskriminierungsformen gingen ineinander über.[143]
Rassismus wird oft nicht als solcher, sondern als Fremdenfeindlichkeit wahrgenommen. Diese Annahme wird unterstützt durch Untersuchungen in der Schweiz, wo aufgrund einer Studie der Eidgenössischen Kommission gegen Rassismus anzunehmen ist, dass Rassismus im engeren Sinne in der Schweiz sehr viel weiter verbreitet ist als ursprünglich angenommen.[144] So sind Schwarze trotz Assimilierung, Integration und Einbürgerung auch nach Jahrzehnten gesellschaftlich marginalisiert und werden, teilweise sogar unter eindeutiger Nennung der Hautfarbe als abwertender Faktor, bei Bewerbungen zurückgewiesen. Auch in Deutschland gilt Rassismus auf dem Arbeitsmarkt, in Berufsschulen, in Behörden, auf dem Wohnungsmarkt oder im öffentlichen Raum als weit verbreitetes Phänomen, das eine gesellschaftliche Teilhabe der Betroffenen deutlich erschwert.[145]
Laut der österreichischen Kulturanthropologin Christa Markom wird der Begriff Xenophobie in der sozialwissenschaftlichen Forschung abgelehnt, da er mit dem Wortbestandteil -phobie Rassismus verharmlose oder legitimiere, ganz als ob Rassisten nur von Furcht geleitet und somit nicht Herr ihrer Handlungen wären.[146]
In der Rassismusforschung wird vermehrt darauf hingewiesen, dass Rassismus kein individuelles Problem ist, sondern dass rassistisches Wissen von gesellschaftlichen Diskursen bestimmt werde. Nach Arndt ist Rassismus „an gesellschaftliche Gegebenheiten geknüpft, die sehr widerstandsfähig und resistent, vielleicht sogar irreparabel sind.“ Das bedeutet, dass Rassismus „(k)ein individuelles Problem“ ist und deshalb „auch nicht individuell bewältigbar“ ist. Dazu gehöre es auch, „sich bewusst zu machen, dass durch die Omnipräsenz des Rassismus in Vergangenheit und Gegenwart sozialpolitische Identitäten gewachsen sind – dass das Herzstück des Rassismus die Konstruktion und Hierarchisierung von Schwarzen und Weißen ist.“ Arndt beschreibt die gesellschaftlichen Aspekte dieser Konstruktionen: „In der vom Rassismus geprägten Sozialisation wurden diese Konstrukte vermittelt und globalen Macht- und Herrschaftsverhältnissen zugrunde gelegt. Eine Realität soziopolitischer Identitäten wurde geschaffen. Wir werden nicht als Schwarze oder Weiße geboren, sondern zu diesen gemacht. Dies macht es erforderlich, Schwarze und Weiße Erfahrungen und Perspektiven wahrzunehmen und zu repräsentieren. Wo dies ignoriert wird, kann Rassismus nicht überwunden werden.“[147]
Seit den 1990er Jahren findet auch ein Perspektivwechsel in der Wissenschaft statt. So sind – wie in der Kritischen Weißseinsforschung – nicht vorrangig die Objekte des Rassismus der Gegenstand der Forschung, sondern die Strukturen, die Rassismus ermöglichen.[148]
Die Rassismusforscher Aurelien Mondon und Aaron Winter sehen 2020 ein Wiederkehren rassistischer Erscheinungen in der westlichen Welt bis in den Mainstream hinein. Verantwortlich gemacht würden durch (links-)liberale Medien, Politiker und Akademiker jedoch bloß die Wähler rechter Parteien, die oftmals aus der Arbeiterklasse kommen und selbst marginalisiert sind. Dabei werde überdeckt, wie in der derzeit kapitalistisch-neoliberalen Version des vorherrschenden Liberalismus selbst struktureller Rassismus existiere und sie gleichzeitig ihr Versprechen von sozialer Gerechtigkeit nicht eingelöst habe. Medial sei über die extreme Rechte oft als „Stimme des Volkes“ berichtet worden, die antagonistisch der derzeit vermeintlich perfekten, toleranten und liberalen Gesellschaft gegenüberstehe. Tatsächlich sei die extreme Rechte aber bloß eine Fortsetzung und Steigerung des kapitalistisch-neoliberalen Systems. Echte Alternativen zum derzeit existierenden System – die etwa durch Bernie Sanders, Jeremy Corbyn und Jean-Luc Mélenchon aufgezeigt würden – seien vom liberalen Mainstream nicht als valide Alternative dargeboten und sogar stärker bekämpft worden als die extreme Rechte selbst.[149]
Der Sonderberater der Vereinten Nation für die Verhinderung von Genoziden teilte Anfang 2013 mit, dass weltweit die Gefahr von religiös und ethnisch motivierter Gewalt möglicherweise höher sei als jemals zuvor, und nannte Spannungen in der Demokratischen Republik Kongo, dem Irak, Kirgisistan, Mali, Myanmar, Pakistan, Sudan und in Syrien als Beispiele.[150]
Über die Ursachen rassistischen Denkens  gibt es schon immer verschiedene Vorstellungen. Nach rationalistisch orientierten Theorien bildete sich der klassische Rassismus im 18. Jahrhundert heraus. Führende Theoretiker der westlichen Welt (wie Immanuel Kant und Georg Wilhelm Friedrich Hegel) versuchten damals, die rassischen Unterschiede wissenschaftlich zu erklären. Sie nahmen an, dass die menschlichen Rassen nicht nur biologische (vorwiegend körperliche) Unterschiede aufweisen, sondern auch feststehende und unveränderbare Merkmale hinsichtlich ihrer Mentalität und ihres Charakters. Später schien die moderne Biologie und Genetik im Gefolge von Charles Darwin dazu Anhaltspunkte zu liefern. Andere Vertreter der Aufklärung, wie Johann Gottfried Herder, distanzierten sich dagegen klar von der Einteilung der Menschen in Rassen.[151] Herder schrieb:
„Ich sehe keine Ursache dieser Benennung. Rasse leitet auf eine Verschiedenheit der Abstammung, die hier entweder gar nicht stattfindet, oder in jedem dieser Weltstriche unter jeder dieser Farben die verschiedensten Rassen begreift. […] Kurz, weder vier oder fünf Rassen, noch ausschließende Varietäten gibt es auf der Erde.“[152]
Psychologisch orientierte Theorien sehen die Ursachen rassistischen Denkens vor allem in psychisch begründeten Abgrenzungstendenzen zwischen der eigenen Gruppe und Fremdgruppen, die der Stärkung des Identitäts- und Selbstwertgefühls dienen und meist mit stereotypen Vorurteilen und Klischees gegenüber den „Anderen“ und „Fremden“ einhergehen.
Dabei kommt der Projektion eigener psychischer Komponenten auf die fremde Gruppe als Mittel zur Bewältigung eigener innerer Konflikte besondere Bedeutung zu (siehe Abwehrmechanismus). So sieht die Psychoanalytikerin Julia Kristeva die Abwehr des Fremden als Abwehr projizierter unbewusster, angstauslösender Aspekte des Eigenen, bei der all jene Komponenten des Fremden Angst auslösen, die nicht in den eigenen „symbolischen Haushalt“ zu integrieren seien.
„Der Fremde, Figur des Hasses und des anderen, ist weder das romantische Opfer unserer heimischen Bequemlichkeit noch der Eindringling, der für alle Übel des Gemeinwesens die Verantwortung trägt. […] Auf befremdliche Weise ist der Fremde in uns selbst.“[153]
Sie befürwortet das Eingeständnis und das Akzeptieren der Nichtintegrierbarkeit des Fremden und spricht sich für ein Auskommen mit ihm jenseits traditioneller Strategien wie Nivellierung, Ausgrenzung, Auslöschung, Überhöhung oder Erniedrigung aus.[154]
Eher gruppenpsychologisch orientierte Ansätze wie die Theorie der Sozialen Identität nach Henri Tajfel verweisen auf die Relevanz der Zugehörigkeit zu bestimmten sozialen Gruppen für das Selbstbild eines Individuums. Nach ihm konstituiere sich eine Gruppe in Abgrenzung zu anderen Gruppen, wobei bestimmte Unterscheidungsmerkmale stereotypisierend und zum Teil abwertend hervorgehoben würden.
Soziologisch orientierten Theorien (siehe unter Begriffliche Dimensionen) gilt Rassismus als Ideologie, die der Aufwertung der eigenen Gruppe und der Stabilisierung des eigenen Selbstgefühls dient und in diesem Sinn eine Abwertung und Ausgrenzung anderer Menschen vornimmt.
Der Rassismus ist von Formen kultureller oder religiöser Intoleranz abzugrenzen, die auf der Basis der gleichen psychischen Mechanismen ebenfalls zu Ablehnung und Unterdrückung anderer Menschengruppen führen. Anders als beim Rassismus wird die Differenz zur eigenen Gruppe in diesen Fällen aber nicht als erblich und unveränderbar angesehen. Durch die religiöse Konversion oder die Annahme einer anderen kulturellen Identität ist eine Integration unterschiedlicher Bevölkerungsgruppen grundsätzlich möglich.
Peter Schmitt-Egner kritisiert sowohl sozialpsychologische als auch ökonomisch-funktionalistische Erklärungen des Rassismus. Ausgehend von Karl Marx’ Werttheorie beabsichtigt Schmitt-Egner stattdessen, „den Rassismus als gesellschaftlich notwendigen Schein der bürgerlichen Gesellschaft nachzuweisen, d. h. zu entwickeln, wie sich in den Widersprüchen der Ökonomieform die objektive Möglichkeit des Rassismus verbirgt.“[155]
Auf internationaler Ebene arbeiten mehrere Organisationen an der Prävention und Bekämpfung von Rassismus. Auf Ebene des Europarates führt ECRI und auf der Ebene der UNO führt CERD ein regelmäßiges Monitoring der Mitgliedsstaaten in Hinblick auf Rassismus durch. Beide Kommissionen richten in ihren Länder-Monitoringberichten Empfehlungen an die Behörden der Mitgliedsstaaten zur Vorbeugung und Bekämpfung von Rassismus.[156]
Auf dieser Basis haben die 47 Mitgliedsstaaten des Europarats in den Strafgesetzbüchern Regeln zur Bestrafung von sogenannter Hasskriminalität erlassen. Zur Hasskriminalität gehören alle rassistisch motivierten Straftaten, z. B. Völkermord und andere rassistisch motivierte Straftaten gegen die Menschlichkeit und Kriegsverbrechen, rassistisch motivierter Mord, Körperverletzung und Brandstiftung, Volksverhetzung, rassistisch motivierte Beleidigungen, Verleumdungen und Bedrohungen und die Leugnung von Völkermord. Die OSZE sammelt Statistiken zu rassistisch motivierten Straftaten in ihren Mitgliedsstaaten.[157]
Fast alle Mitgliedsstaaten des Europarats haben auch ein Anti-Diskriminierungsgesetz erlassen, das u. a. rassistische Diskriminierungen verbietet. Die 28 EU-Staaten haben sich zum Erlass solcher Gesetze zudem in den EU-Gleichheitsrichtlinien verpflichtet. Deutschland hat das sogenannte Allgemeine Gleichbehandlungsgesetz erlassen.
Fast alle Mitgliedsstaaten des Europarats haben auch eine oder mehrere nationale Gleichstellungsbehörden oder Antidiskriminierungsstelle eingerichtet, deren Aufgabe es ist, rassistischer Diskriminierung auf nationaler, regionaler und lokaler Ebene vorzubeugen und zur Bekämpfung von Rassismus beizutragen. In Deutschland ist das auf Bundesebene die Antidiskriminierungsstelle des Bundes.
ECRI hilft mit ihren 16 Allgemeinen Politikempfehlungen[158] und CERD mit ihren 35 General Recommendations[159] den Mitgliedsstaaten und den Gleichheitsbehörden mit konkreten Empfehlungen bei ihrer Arbeit.
Die von der Stiftung für die Internationalen Wochen gegen Rassismus[160] organisierten „Internationalen Wochen gegen Rassismus“ fanden 2018 vom 12. bis 25. März statt (s. a. Stiftung gegen Rassismus und Antisemitismus).[161]
Der 21. März wurde 1966 von den Vereinten Nationen zum Internationalen Tag gegen Rassismus erklärt. Anlass war das Massaker 1960 in Sharpeville, Südafrika, mit 69 Toten. Sechs Jahre danach fasste die Generalversammlung der Vereinten Nationen die Resolution 2142 (XXI), die zur „Elimination of all forms of racial discrimination“ auffordert.
Insbesondere ausgelöst durch die Black-Lives-Matter-Proteste im Jahr 2020 werden in den Vereinigten Staaten vermehrt sogenannte Anti-Rassismus-Trainings nachgefragt, zum Beispiel von Unternehmen, die damit ihre Mitarbeiter sensibilisieren wollen. Darin werden die Teilnehmer über systemischen Rassismus und implizite Vorurteile aufgeklärt. Solche Anti-Rassismus-Trainings beinhalten meist mehrere Sitzungen, die innerhalb von mehreren Tagen, Wochen oder Monaten absolviert werden.[162] Von zentraler Bedeutung bei solchen Trainings ist es auch, die (Weißen) Teilnehmer über ihre Privilegien aufzuklären.[162] Allerdings ist das bloße Informieren über die Existenz der Privilegien weißer Menschen allein nicht ausreichend: Eine empirische Untersuchung der Psychologin Erin Cooley ergab, dass das Lesen eines Textes über weiße Privilegien bei den Testpersonen nicht automatisch die Empathie für benachteiligte Schwarze erhöhe, sondern dazu führen könne, dass sich stattdessen die Empathie für ökonomisch benachteiligte Weiße reduziere.[163][164]

Türkische Sprache, kurz Türkisch, auch als Türkeitürkisch oder Osmanisch-Türkisch bezeichnet,[1] ist die Bezeichnung einer agglutinierenden Sprache, die zum oghusischen Zweig der Turksprachen gehört und die als meistgesprochene Turksprache gilt. Türkisch ist die Amtssprache der Türkei und neben dem Griechischen auch auf Zypern (faktisch ausschließlich in der international nicht anerkannten Türkischen Republik Nordzypern). Außerdem wird Türkisch als lokale Amtssprache in Nordmazedonien, Rumänien und im Kosovo verwendet. Eigenbezeichnungen sind Türk dili, Türkçe [tyɾkt͡ʃɛ]  und Türkiye Türkçesi.
Die türkische Sprache selbst weist eine Reihe von Dialekten auf, von denen der Istanbuler Dialekt von besonderer Bedeutung ist. Seine Phonetik ist die Basis der heutigen türkischen Hochsprache.[2] Bei der Einführung des lateinischen Alphabets für die türkische Sprache im Jahr 1928 wurde nicht auf die historische Orthographie des Osmanisch-Türkischen zurückgegriffen, sondern die Aussprache von Istanbul als Grundlage der Verschriftung herangezogen.[3] Die Dialekte innerhalb der Türkei werden in Gruppen der Schwarzmeerregion (Karadeniz Şivesi), Ostanatolien (Doğu Anadolu Şivesi), Südostanatolien (Güneydoğu Anadolu Şivesi), Zentralanatolien (İç Anadolu Şivesi), Ägäis (Ege Şivesi) und Mittelmeerregion (Akdeniz Şivesi) eingeteilt.
Die Alternativbenennung „Türkeitürkisch“ umfasst aber nicht nur die Türkei, sondern auch alle Gebiete des ehemaligen Osmanischen Reichs. Das bedeutet, dass auch die Balkan- oder Zyperntürken ein „Türkeitürkisch“ sprechen.[4]
Das heutige Türkisch ist die Muttersprache von etwa 80 Prozent der Menschen in der Türkei (das waren Ende 2015 gut 63 Millionen Menschen) und war dies nach Schätzungen im Jahr 1979 auch für 37.000 Menschen in Usbekistan, Kasachstan, Kirgisistan, Tadschikistan und Aserbaidschan. Türkisch war 2011 zudem die Muttersprache für 606.000 Menschen in Bulgarien,[5] für ungefähr 290.000 Menschen in der Türkischen Republik Nordzypern[6] und 1976 die von 128.380 Menschen in Griechenland.
63.600 Sprecher lebten 1984 in Belgien, etwa 70.000 in Österreich (Ethnologue 2009) und mehr als 1,5 Millionen in Deutschland.[7] Laut Mikrozensus 2021 ist Türkisch bei Haushalten in Deutschland die am häufigste neben Deutsch genutzte Sprache.[8] 1982 sprachen in Rumänien noch 14.000 Menschen die türkische Sprache, und auf dem Gebiet des früheren Jugoslawien, insbesondere im Kosovo, wo es auch Amtssprache ist, und in Nordmazedonien, sprechen 250.000 Personen türkisch.
1990 war Türkisch im Irak noch für rund 3.000 und im Iran für 2.500 Menschen die Muttersprache. In den USA lebten 1970 24.123 Sprecher des Türkischen, und für Kanada wurden 1974 8.863 türkische Muttersprachler angegeben. In Frankreich gaben 1984 rund 135.000 und in den Niederlanden knapp 150.000 Menschen Türkisch als Muttersprache an. 1988 wurden in Schweden rund 5.000 Türkischsprachige registriert.
2009 sprachen etwa 85 Millionen Menschen Türkeitürkisch, darunter 65 Millionen als Muttersprache und 20 Millionen als Zweitsprache.
Als engste Verwandte des Türkeitürkischen gilt heute das Aserbaidschanische. Vielfach wird die Sprache der südosteuropäischen Gagausen (Republik Moldau und Balkan) als ein Dialekt des Türkeitürkischen angesehen, was allerdings umstritten ist. So führen die Turkologen Westeuropas das Gagausische als eigene Sprache und jene in den Turkstaaten auf Grund des geringen Abstandes als einen Dialekt des Türkeitürkischen an. Gegenseitige mündliche wie schriftliche Kommunikation zwischen Sprechern des Türkischen, Aserbaidschanischen und Gagausischen ist ohne größere Schwierigkeiten möglich. Das Sprachverhältnis ist etwa vergleichbar mit der Verwandtschaft zwischen Dänisch und Norwegisch.
Einen etwas größeren Sprachabstand gegenüber dem Türkischen hat das Turkmenische, daher ist eine Unterhaltung beispielsweise zwischen Türkisch- und Turkmenischsprechern in Wort oder Schrift deutlich mühsamer. Das Verhältnis entspricht ungefähr dem Sprachabstand zwischen dem Schwedischen und dem Dänischen. Die sprachlichen Unterschiede rühren hauptsächlich daher, dass das Turkmenische, das bis heute dialektal stark zersplittert ist, unter dem Einfluss verschiedener Sprachen wie des Persischen und des Russischen, nicht zuletzt auch unter dem Einfluss nicht-oghusischer, zentralasiatischer Turksprachen, wie des Tschagataischen stand.
Aufgrund dieser unterschiedlichen Sprachabstände fasst man innerhalb der oghusischen Sprachen Türkisch, Gagausisch und Aserbaidschanisch als westoghusische Sprachen zusammen, während das Turkmenische einem ostoghusischen Zweig zugerechnet wird. Zwischen diesen Gruppen stehen einige nicht verschriftete bzw. standardisierte Dialekte im Iran, die zum Teil als Südoghusisch zusammengefasst werden, zum Teil unter dem Begriff Chorasan-Türkisch und Usbek-Oghusisch[9].
Prototürkisch ist eine hypothetisch erschlossene Rekonstruktion, aus der sämtliche gegenwärtigen und erloschenen Turksprachen sprachwissenschaftlich hergeleitet werden. Es handelt sich dabei nicht um eine Sprache, die zu irgendeinem Zeitpunkt an irgendeinem Ort tatsächlich gesprochen wurde, sondern um ein theoretisches Modell zur Erklärung von Gemeinsamkeiten und Unterschieden zwischen bekanntermaßen verwandten Sprachen. Gleichwohl besteht ein zeitlicher Rahmen von 4500/4000 v. Chr., dem Beginn des Neolithikums, und ca. 500 n. Chr., als mit dem Alttürkischen die erste bekannte Turksprache fassbar wird. Grundlage dieser Rekonstruktion ist vor allem das Alttürkische als der früheste bekannte und erforschte Vertreter einer Gruppe von Turksprachen, die als Gemeintürkisch (Common Turkic) zusammengefasst sind und das Tschuwaschische, das als einzige gesprochene Turksprache dieser Gruppe nicht angehört und dessen Vorläufer nur spärlich überliefert sind.[10][11]
Alttürkisch ist die früheste schriftlich überlieferte Turksprache. Obwohl in mehrere dialektische Formen aufgespalten, bildet diese Sprache eine Koiné, die in Zentralasien von der Chinesischen Mauer bis nach Transoxanien ohne erkennbare ethnische Unterschiede verwendet wurde. Diese Sprache ist in drei Textkorpora erhalten: die Runeninschriften vom Orchon und oberen Jenissei, eine Fülle von Schriften in verschiedenen Alphabeten meist religiösen, zum Großteil buddhistischen Inhalts, die dem uigurischen Reich und seinen ostturkestanischen Nachfolgestaaten zugeordnet wird und Schriften in arabischer Schrift aus dem Karachanidenreich, womit der Beginn der Islamisierung mit dem Eindringen von Wörtern persischer und arabischer Herkunft greifbar wird.[12][13]
Zum letzten Textkorpus gehört Mahmud al-Kāschgharīs Lexikon „Sammlung der Dialekte der Türken“ (dīwān lughāt at-turk) aus dem 11. Jahrhundert. Dieses monumentale türkisch-arabische Wörterbuch enthält neben der Übertragung von Wortmaterial eine Fülle von historischen, geographischen und folkloristischen Einzelheiten.
Die Formen des Alttürkischen repräsentieren eine Sprache, in der die Merkmale der späteren Einzelsprachen noch vereinigt sind.[14][15] Namentlich die Runeninschriften repräsentieren einige Merkmale, die für die Oghusischen Sprachen, wozu das heutige Türkische gehört, typisch sind. Alttürkisch mit seinen zuletzt fast ausschließlich buddhistischen Inhalten entwickelte sich aber getrennt davon weiter und erlosch mit dem Vordringen des Islam im 17. Jahrhundert. Seinen Platz nimmt heute das Uigurische ein.
Ab dem 11. Jahrhundert bilden sich im westlichen Steppenraum, in (West-)Turkestan, Osteuropa und Vorderasien, nunmehr als Einzelsprachen, die mitteltürkischen Sprachen aus. In diese Zeit fällt die Entstehung des türkischen Volkstums auf dem Gebiet der heutigen Türkei. Die Eroberungen der Seldschuken in dieser Zeit führten dazu, dass Angehörige türkischer Völker, zumeist Oghusen, ihre Heimat in Zentralasien nördlich des Syr-Darya in der Kasachensteppe verließen, sich allmählich islamisierten und den Seldschuken in den Iran und das westliche Vorderasien (Syrien, Irak) folgten. Im Gefolge der Schlacht von Mantzikert im Jahre 1071 wanderte ein Teil dieser Stämme nach Anatolien ein. In der Folgezeit wurde der Zustrom von Einwanderern nach Anatolien durch Flüchtlinge vor den Eroberungen Dschingis Khans und seiner Nachfolger noch verstärkt.
Als mitteltürkische Sprachen gelten das Choresm-Türkische, das Tschagataische, die Formen des Kiptschakischen und die Sprache des Codex Cumanicus. Teilweise, wegen des islamischen Bezugs, wird auch das Karachanidische nicht dem Alt-, sondern dem Mitteltürkischen zugeordnet. Von der Sprachstufe her wird auch das altanatolische Türkisch dazu gerechnet, teils aber wegen der kontinuierlichen Weiterentwicklung als Altosmanisch der türkischen Sprache als Entwicklungsstadium und damit dem Neutürkischen zugerechnet.[16][17] Sie steht damit an der Grenze zwischen den frühen, nations- und ethnieübergreifenden früheren Turksprachen und den modernen Nationalsprachen.
Die Benennung des altanatolischen Türkischen bzw. des Altosmanischen ist strittig, weil diese Sprache weder auf das Osmanische Reich, noch auf Anatolien beschränkt ist. Es ist sowohl das Türkisch der Rum-Seldschuken und der vorosmanischen anatolischen Fürstentümer als auch die Sprache des Osmanischen Reichs bis ins 16. Jahrhundert.
Die frühesten Werke in türkischer Sprache in Anatolien stammen vom 13. Jahrhundert und gehen damit der Gründung der osmanischen Dynastie kurz voraus. Die ersten Zeugnisse türkischer Sprache sind einige Verse von Sultan Veled, dem Sohn des Mystikers Dschalal ad-Din ar-Rumi. Die Seldschuken-Dynastie, die Anatolien ab dem Ende des 11. Jahrhunderts beherrschte, verwendete die türkische Sprache weder in der Verwaltung ihres Reiches, noch förderte sie ihre Verwendung in der Literatur. Erst nachdem die Macht der Seldschuken-Herrscher in Anatolien im 13. Jahrhundert durch eine Vielzahl türkischer Fürstentümer (Beyliks) ersetzt worden war, begann Türkisch als Verwaltungssprache in Anatolien verwendet zu werden, weil diese Herrscher des Arabischen nur unzureichend mächtig waren. Gleichzeitig kam es zu einem Aufschwung von Literatur in türkischer Sprache (siehe z. B. Yunus Emre).[18] Das Persische, bislang die vorherrschende Sprache der Literatur und Poesie im seldschukischen Anatolien, verlor mit dem Hof der seldschukischen Herrscher seinen wichtigsten Mäzen.
Die türkischen Dialekte in Anatolien und den angrenzenden Gebieten in Nordsyrien, Nordmesopotamien und Nordwestiran, später auch auf dem Balkan und in Transkaukasien, bildeten ab dem 12. Jahrhundert ein Dialektkontinuum eng miteinander verwandter und ineinander übergehender Dialekte, das bis zum 16. Jahrhundert großenteils unter die Herrschaft des Osmanischen Reichs geriet. In den Gebieten der heutigen Republik Aserbaidschan und der gleichnamigen iranischen Region, in denen die osmanische Herrschaft nur vorübergehend war und in denen sich die Bevölkerung durch die schiitische Konfession von den sunnitischen Osmanen abhob, entwickelte sich in der Folgezeit die aserbaidschanische Sprache. Diese blieb gleichwohl bis in die 1930er Jahre dem anatolischen und osmanischen Türkisch eng verbunden. Während sich in den dann durchgeführten jeweiligen Sprachreformen das Türkische an der Sprache von Istanbul orientierte, wurde für das (Nord-)Aserbaidschanische die Sprache von Baku maßgeblich. Türkisch wurde im neuen türkischen Alphabet geschrieben, während das Aserbaidschanische ein davon abweichendes Lateinalphabet und während der Zugehörigkeit zur Sowjetunion auch ein kyrillisches Alphabet verwendete. Manche Autoren sehen gleichwohl Türkisch und Aserbaidschanisch als zwei Formen derselben Sprache an.[19]
Die türkische Sprache in ihrer „Standardform“ als Amtssprache des Osmanischen Reichs und als Sprache der osmanischen Literatur nahm ab dem Ende des 15. Jahrhunderts arabische und persische Elemente auf.[20] Diese Entwicklung lag an der damaligen Dominanz der arabischen und persischen Sprachen in der islamischen Hochkultur, die die osmanische Elite zum Nachahmen und Weiterentwickeln dieser Sprachen motivierte.[18]
Im Jahre 1928 erfassten die laizistischen und kemalistischen Reformen nach der Ausrufung der Republik Türkei im Jahr 1923 auch die Sprache. Die Einführung der lateinischen Schrift für die türkische Sprache in Staaten der Sowjetunion erleichterte die Verwendung des lateinischen Alphabets auch in der Türkei. Die Kontakte zu anderen Turkvölkern jenseits der Grenze sollten gewahrt werden, im Übrigen vergrößerte dieser Reformschritt die kulturelle Distanz zur osmanischen und mehr noch zur islamischen Vergangenheit der Türken. Die Säkularisierung der modernen Türkei setzte sich damit fort. Erwägungen, die arabische Schrift des Türkischen zu reformieren oder sogar durch die lateinische Schrift zu ersetzen, waren in der Türkei allerdings nicht neu. In der Tanzimat-Ära hatte bereits der Bildungsminister Münif Pascha mit diesem Gedanken gespielt. Münif Pascha sah die arabische Schrift als Ursache für den verbreiteten Analphabetismus in der damaligen Türkei.
Nach der Gründung der Türkischen Republik 1923 begann man in den 1930er Jahren, fremde Lehnwörter durch teils bereits vorhandene, teils neugebildete türkische Wörter zu ersetzen. Diese Ersetzungen des hergebrachten Wortschatzes sind bis heute nicht vollständig durchgeführt, so dass sich immer noch viele Wörter arabischen und persischen Ursprungs finden. In vielen türkischen Dialekten sind Gräzismen vertreten, die in der bäuerlichen Terminologie oder in der Seefahrt-, Fischfang-, Weinbau-, Bienenzuchtterminologie vorkommen. Armenismen kommen in türkischen Dialekten seltener vor.[21] Seit dem 19. Jahrhundert kam vor allem Vokabular französischer, im 20. Jahrhundert auch englischer Herkunft hinzu.
Die Türk Dil Kurumu, die Gesellschaft der türkischen Sprache, ist eine staatliche Einrichtung, die 1932 zur Returkisierung bzw. Modernisierung der türkischen Sprache gegründet wurde. Oberstes Ziel dieser Gesellschaft war anfangs, zahlreiche arabische und persische Wörter durch traditionelle türkische Entsprechungen und, wenn es diese nicht gab, ohne Rücksicht auf die anderen Turksprachen durch eigens geschaffene „neu-türkische“ Wörter zu ersetzen. Die 1951 entstaatlichte und mit der Verfassung von 1982 der Atatürk Kültür, Dil ve Tarih Yüksek Kurumu unterstellte und wieder unter staatlichen Einfluss gebrachte Türk Dil Kurumu betreibt heute keine Sprachreformpolitik mehr.[22]
Das Phonem ​/⁠ɣ⁠/​ (normalerweise yumuşak g genannt („weiches g“)), ğ erscheint niemals am Wortanfang, sondern folgt stets einem Vokal. Am Wortende oder vor Konsonanten wird es nicht selbst realisiert, sondern zeigt sich in Form einer langen Aussprache des vorhergehenden Vokals.
In Wörtern türkischen Ursprungs stellen die Laute ​/⁠c⁠/​, ​/⁠ɟ⁠/​ und ​/⁠l⁠/​ Allophone von ​/⁠k⁠/​, ​/⁠g⁠/​ und ​/⁠ɫ⁠/​ dar; die ersteren erscheinen vor Vorderzungenvokalen, die letzteren vor Hinterzungenvokalen. Die Verteilung dieser Phoneme ist jedoch in Wörtern und Eigennamen fremdsprachlichen Ursprungs oft unvorhersehbar. In solchen Wörtern erscheinen ​/⁠c⁠/​, ​/⁠ɟ⁠/​ und ​/⁠l⁠/​ oft vor Hinterzungenvokalen.[23]
Das Türkische ist (ähnlich wie das Deutsche) auslautverhärtend, d. h. stimmhafte Konsonanten werden am Ende eines Wortes stimmlos, so lautet der eigentliche Stamm von kebap kebab-. Die Auslautverhärtung im Türkischen betrifft nur Verschlusslaute und Affrikaten, nicht die Frikative.
Die Vokale der türkischen Sprache sind, in ihrer alphabetischen Reihenfolge, a, e, ı, i, o, ö, u und ü. Der Buchstabe e wird sowohl regelmäßig für den ungerundeten fast offenen Vorderzungenvokal, als auch seltener für den ungerundeten halbgeschlossenen Vorderzungenvokal verwendet.[24] Das <ı> ohne Punkt ist der ungerundete geschlossene Hinterzungenvokal ​[⁠ɯ⁠]​. Im Türkischen gibt es keine Diphthonge; wenn zwei Vokale aufeinandertreffen, was selten und nur in Lehnwörtern geschieht, wird jeder Vokal einzeln ausgesprochen. Jedoch kann eine Art Diphthong auftreten, wenn das yumuşak g zwischen zwei Vokalen steht. So kann das Wort soğuk („kalt“) von manchen Sprechern [soʊk] ausgesprochen werden.
Die älteste türkische Schrift ist die türkische Runenschrift mit 38 Zeichen.
Ab dem 10. Jahrhundert galten die Oghusen als islamisiert, und sie übernahmen die arabische Schrift, die um vier von Persern hinzugefügte Konsonanten ergänzt war. Das osmanisch-türkische Alphabet beinhaltete darüber hinaus noch einen von den Türken selbst hinzugefügten Konsonanten, den kāf-i nūnī oder sağır kef (ñ / ﯓ).
Anfang 1926 nahm Mustafa Kemal Atatürk im aserbaidschanischen Baku an einem Kongress der Turkologen teil, bei dem unter anderem die Schaffung einer Lateinschrift für die Turkvölker gefordert wurde. Aserbaidschan hatte schon seit 1922 eine lateinisch-basierte Schrift: das einheitliche türkische Alphabet.
Seit 1928 wird das Türkeitürkische durch eine von Kemal Atatürk mitentwickelte Variante der lateinischen Schrift wiedergegeben. Atatürk nannte dieses neue Schriftsystem Neues türkisches Alphabet. Grundlage für die Neuschreibung der Wörter wie für die allgemeine Sprachreform war der Istanbuler Dialekt. Bei den Schreib- und Ausspracheregeln existieren deshalb keine Ausnahmen.
Das heutige Alphabet des Türkeitürkischen umfasst 29 Buchstaben, wobei jedem Buchstaben ein Laut zugeordnet ist:
a b c ç d e f g ğ h ı i j k l m n o ö p r s ş t u ü v y z
Die in der lateinischen Schrift vorkommenden Buchstaben q, w und x sowie die in der deutschen Schrift verwendeten Buchstaben ä und ß kommen nicht vor. j erscheint nur in einigen Fremdwörtern wie jakuzi „Whirlpool“. Im Türkischen wird der Buchstabe i (i mit Punkt) auch als Großbuchstabe mit einem Punkt geschrieben (İ), während der Großbuchstabe I dem Kleinbuchstaben ı (ı ohne Punkt) entspricht.
Sofern nicht ohnehin ein Buchstabieralphabet benutzt wird, ist die Benennung der Buchstaben regelmäßig: Vokale werden als solche ausgesprochen, Konsonanten mit einem e ergänzt. Die Stadt İzmir würde also i ze me i re buchstabiert.
Die übrigen Laute werden wie im Deutschen ausgesprochen.
Bei der Bildung von Begriffen aus Wörtern, die auf Konsonanten enden, und bei Suffixen muss stets die Stimmhaftigkeit des Endkonsonanten beachtet werden. Es wird unterschieden zwischen stimmhaften und stimmlosen Konsonanten. Die stimmlosen Konsonanten ç, f, h, k, p, s, ş und t lassen sich mit den Merksätzen Çift Haseki Paşa („doppelter Haseki-Pascha“) oder Fe Paşa çok hasta („Fe Pascha ist sehr krank“) einprägen. Wird nun ein Suffix an einen stimmlosen Konsonanten angehängt, so muss ein stimmhafter Konsonant im Suffixanlaut angepasst werden.
Umgekehrt verwandeln sich die stimmlosen Konsonanten p, t, k und ç im Auslaut eines Wortes oft in ihr stimmhaftes Pendant (b, d, g bzw. ğ und c), wenn vokalisch anlautende Suffixe angeschlossen werden. Davon sind besonders mehrsilbige Substantive betroffen und von diesen wiederum sehr viele, die auf -k enden. Viele dieser im Türkischen auf -p, -t oder -ç auslautenden Wörter sind Fremdwörter, die in der Ursprungssprache (zumeist Arabisch) auf -b, -d oder -c (bzw. ǧ) auslauteten und bei vokalischem Suffixanlaut ihre ursprüngliche Form wieder annehmen. Dies ist aber eher eine orthographische Regel, die in der gesprochenen Sprache nicht immer befolgt wird. Auch auslautendes, ursprüngliches -g kann davon betroffen sein: renk (bedeutet „Farbe, Farbton“), aber menekşerengi („veilchenfarbig, violett“).
Vor allem bei der Bildung von Lokativ- und Possessivverbindungen sowie Deklinationen spielt dies eine Rolle.
In einigen Wörtern wird der Zirkumflex (ˆ, türkisch uzatma işareti) verwendet. Dieses Zeichen gibt die Länge des Vokals an und dient oft zur Unterscheidung zweier ansonsten gleich geschriebener Wörter (beispielsweise adet ‚Anzahl‘ gegenüber âdet ‚Gewohnheit‘), ist jedoch heute in den meisten Fällen außer Gebrauch geraten. Der Zirkumflex kann auch die Palatalisierung eines Konsonanten anzeigen (beispielsweise kâğıt ‚Papier‘) und findet sich dann häufiger.
Vorgeschichte: Osmanische Sprache#Vokalharmonie
Eine Besonderheit der türkischen Sprache ist die Lautharmonie (Palatalharmonie), welche zwischen hellen (vorderen) und dunklen (hinteren) Vokalen und Konsonanten unterscheidet. Das Gesetz der Lautharmonie zieht sich durch die gesamte türkische Formenlehre. Im modernen türkischen Lateinalphabet werden die vorderen und hinteren Varianten der Buchstaben g, k und l graphisch nicht unterschieden, man spricht daher auch von Vokalharmonie. Ein rein türkisches Wort (bis auf wenige Ausnahmen) enthält nur Vokale aus der Reihe der hellen (e, i, ö, ü) oder der dunklen (a, ı, o, u) Vokale. Wird ein Suffix an ein Wort angehängt, muss der Vokal des Suffixes dem sich anpassen, bei Fremdwörtern und den sehr seltenen türkischen Wörtern, die nicht der Vokalharmonie unterliegen, ist der Vokal der letzten Silbe des Grundwortes in der Regel maßgeblich. Bei Fremdwörtern, die auf k, g und l enden, entscheidet hingegen die Färbung dieses Konsonanten, wobei l meist als heller Laut gilt. So wird von gol (von englisch: goal, geschossenes Fußball- oder anderes Tor) golcü (Torschütze) abgeleitet und der Plural lautet goller.
Je nachdem, ob zur Palatal- auch die Labialharmonie hinzutritt, die auch zwischen runden und nicht runden Vokalen unterscheidet, spricht man von kleiner und großer Vokalharmonie.
Sie unterscheidet nur zwischen hellen und dunklen Vokalen. Die Vokale der Suffixe nach der kleinen Vokalharmonie sind zweifach und lauten e/a. Sie sind also nicht-hoch.
Die kleine Vokalharmonie tritt u. a. bei den Pluralsuffixen und einigen Kasussuffixen auf.
Bei der großen Vokalharmonie haben die Suffixe vier (statt zwei) mögliche Formen. Sie werden bei den Suffixen verwendet, die mit einem der hohen Vokale i/ı/ü/u vokalisiert werden. Der Vokal richtet sich jeweils nach dem (letzten) Vokal im Grundwort. Es gilt folgendes Schema:
Die große Vokalharmonie erfolgt u. a. bei allen Personal- und Possessivsuffixen, beim Fragesuffix mi und bei den Kasussuffixen des Genitivs und des Akkusativs.
Als weiteres Beispiel für die große Vokalharmonie dient die Endung -li/-lı/-lu/-lü; („aus … stammend“): Berlinli (der Berliner/die Berlinerin), aber: Ankaralı, Bonnlu, Kölnlü.
Es kommt vor, dass infolge der Vokalharmonie mehrere Endungen mit dem gleichen Vokal aufeinander folgen (zum Beispiel huzursuzsunuz: ihr seid unruhig, üzgünsünüz: „ihr seid traurig, es tut euch leid“).
Vorgeschichte: Osmanische Sprache#Konsonantenharmonie
Die stimmlosen Verschlusslaute t und k werden in ihre stimmhaften Entsprechungen umgewandelt, wenn ihnen ein Vokal folgt. Aus t wird d, aus k wird ğ. Beispiele: gitmek (gehen) ⇒ gider – er geht; büyük – groß, büyüğüm – ich bin groß (s. Stimmhafte und stimmlose Konsonanten).
Die türkischen Sprachen sind agglutinierend und unterscheiden sich somit wesentlich von den indogermanischen Sprachen. Agglutination bedeutet, dass grammatische Formen durch eine (eindeutige) Endung angezeigt werden. Dabei können mehrere Endungen aufeinander folgen, wobei die Reihenfolge festgelegt ist.
Beispiel:
Uçurtmayı vurmasınlar. – „Sie sollen den Drachen nicht runterschießen.“ (Filmtitel[26])
Man könnte den Satz wie folgt zerlegen:  Uçurtma-yı vur-ma-sın-lar. – [Ein/Den] Drachen-den runterschießen-nicht-sollen-sie.Die Endung -yı zeigt den bestimmten Akkusativ an; -ma steht für die Verneinung; -sın steht für den Imperativ, -lar für die 3. Person Mehrzahl.
In der türkischen Sprache gibt es keinen Artikel. Zur Kennzeichnung einer einzelnen, individuellen, aber nicht weiter bestimmten Sache (das sind Fälle, in denen im Deutschen der unbestimmte Artikel verwendet wird) kann im Türkischen das Zahlwort bir (eins) stehen. Unbestimmtheit kann auch durch andere grammatische Mittel ausgedrückt werden, dabei bleibt aber offen, ob es sich um einen oder mehrere Gegenstände handelt.[27]
Weiterhin kennt das Türkische kein grammatisches Geschlecht,[28] so kann kardeş sowohl Bruder als auch Schwester bedeuten, sofern es nicht durch erkek bzw. kız näher definiert wird oder sich aus dem Zusammenhang ergibt (Ayşe kardeşim – meine Schwester Ayşe; erkek kardeşim hasta – mein Bruder ist krank). Das Pronomen o steht für er, sie oder es.
Türkisch weist die Satzstellung Subjekt – Objekt – Verb auf, ist also eine SOV-Sprache. Eine weitere Besonderheit für Sprecher der meisten europäischen Sprachen ist, dass es keine Präpositionen gibt, sondern ausschließlich Postpositionen verwendet werden. Beispiele: Fatma için – für Fatma; gül gibi – wie (eine) Rose.
Vorgeschichte: Osmanische Sprache#Fälle
Im Türkischen zählt man gemeinhin sechs Fälle: Nominativ, Genitiv, Dativ, Akkusativ (bestimmt: eigene Endung; unbestimmt: formengleich mit dem Nominativ), Lokativ und Ablativ. Die entsprechenden Endungen sind
Bei Eigennamen wird vor dem Suffix (oder ggf. vor den jeweiligen Bindekonsonanten -y-, -n- oder -s-) ein Apostroph geschrieben, Bsp.: Ali'nin annesi – Alis Mutter; Deniz'e – [dem/der] Deniz (aber denize – ans Meer), Ankara'ya – nach Ankara; Barış'ı – [den] Barış (aber barışı – den Frieden); Eskişehir'de – in Eskişehir (aber şehirde – in der Stadt); Kıbrıs'tan – aus/von Zypern; auch bei aus mehreren Wörtern bestehenden Eigennnamen: Lozan Antlaşması'nda – im Vertrag von Lausanne. Dieser Apostroph wird nicht gesprochen, im Gegensatz zu dem Apostroph, der vor allem in älteren Texten zwischen Konsonant und Vokal für ein ursprüngliches arabisches Hamza oder ʿAin geschrieben wird, das im Anlaut und Inlaut (im Auslaut stumm) als Stimmansatz gesprochen wird, Bsp.: men bzw. meni – das Verbot, Akkusativ: men'i.
Zur Kennzeichnung eines Plurals wird das Suffix -lar, -ler verwendet. Es tritt vor allen anderen Suffixen, also auch vor den Kasussuffixen, unmittelbar an den Wortstamm. Bsp.: hoca – der Hodscha, hocalar – die Hodschas; göl – der See, göller – die Seen; göllerde – in den Seen (Lokativ Plural).
Gelegentlich sind im Türkischen noch Reste von Kasusbildungen vorhanden, deren Suffixe aber nicht mehr produktiv sind und die nur mehr in festen Redewendungen und bei bestimmten Vokabeln vorkommen. Hierzu zählt ein archaischer Instrumentalis mit der Endung -in / -ın; diese Form ist in wenigen Worten heute noch anzutreffen, z. B. yazın (im Sommer, sommers), kışın (im Winter, winters), gelmeksizin (ohne zu kommen). In früherer Zeit war dieser Kasus noch weiter verbreitet.
Weiter werden gelegentlich auch Ableitungen, die anderwärts der Wortbildung zugerechnet werden, als eigene Kasusbildungen geführt, wie die auf -ce/-ca als Äquativ oder Relativ oder auf -siz/-sız/-suz/-süz als Abessiv.[29][30] Zusammenziehungen mit der Postposition ile (mit), die unter Wegfall des anlautenden i- in der Form -le/-la enklitisch an das Bezugswort treten, werden von Korkut Buğday als Instrumentalis bezeichnet,[31] anderwärts aber zutreffender als Komitativ.[29] Die angeführten Beispiele: halk ile ⇒ halkla (mit dem Volk), eşim ile ⇒ eşimle (mit meinem Partner); nach Vokal verwandelt sich das -i- zum -y-, ümidi ile ⇒ ümidiyle (mit der Hoffnung), araba ile ⇒ arabayla (mit dem Wagen) sind oft keine Antworten auf die Frage Womit?, sondern auf die Frage Mit wem/was?.
Im Türkischen werden alle Kategorien der Konjugation, nämlich Genus verbi, Tempus und Person, jeweils durch verschiedene aneinanderzureihende Suffixe ausgedrückt. Lediglich Person und Numerus werden durch dieselbe Klasse von Suffixen ausgedrückt, es gibt also nur ein Suffix für die 1. Person Plural und nicht ein Suffix für die 1. Person generell und eines zusätzlich für den Plural. Eine Ausnahme von dieser Regel betrifft die 3. Person. Hier ist das Personalsuffix (in der Mehrzahl der Tempora wird die 3. Person allerdings durch die Grundform, also ohne Suffix, ausgedrückt) in Singular und Plural gleich und der Plural wird durch das Pluralsuffix -ler/-lar wiedergegeben. Der Plural der 3. Person wird beim Prädikat aber nur dann zum Ausdruck gebracht, wenn der Satz kein ausdrückliches pluralisches Subjekt enthält, das Subjekt also im Prädikat inhärent enthalten ist.
Beispiele für die 3. Person:
im Singular: Ahmet geliyor – Ahmet kommt. Mit inhärentem Subjekt: Geliyor – Er kommt. Mit Verwendung des Personalpronomens: O geliyor – Er kommt.
im Plural: Öğrenciler geliyor – Die Schüler kommen. Mit inhärentem Subjekt: Geliyorlar – Sie kommen. Mit Verwendung des Personalpronomens: Onlar geliyor – Sie kommen.
An Genera Verbi gibt es im Türkischen weit mehr als im Deutschen. Neben dem Passiv existieren ein Kausativ, ein Reflexiv und ein Reziprok. Die Suffixe können auch kombiniert werden.
Beispiele:
für Kausativ: doğmak – zur Welt kommen, doğ-ur-mak – gebären, doğur-t-mak – entbinden, doğurt-tur-mak – entbinden lassen
für Reflexiv: sevmek – lieben, sev-in-mek – sich freuen, (Kombination mit Kausativ:) sevin-dir-mek – erfreuen, (mit Passiv:) sevindir-il-mek – erfreut werden
für Reziprok: öpmek – küssen, öp-üş-mek – sich küssen
Diesen Genera verbi stehen die Verneinungs- und die Unmöglichkeitformen nahe: Grundform: gelmek – kommen, verneinte Form: gel-me-mek – nicht kommen, Unmöglichkeitform: gel-eme-mek – nicht kommen können.
Im Türkischen gibt es kein etwa dem Deutschen oder Lateinischen vergleichbares, auf die Einteilung in Präsens, Perfekt und Futur gestütztes Tempussystem. Auch ein Modus als eigenständige Kategorie ist nicht vorhanden. Dafür haben aber die türkischen Tempora auch modale oder aspektbezogene Bedeutungen, einige haben sogar hauptsächlich modale Bedeutung. An einfachen Tempora, die eine zeitbezogene Bedeutung haben, sind zu nennen: (bestimmtes) Präsens, Aorist, (bestimmtes) Präteritum, Perfekt, auch unbestimmtes Präteritum genannt, und Futur. An weiteren „Zeitformen“ existieren der selten gewordene Optativ, der Nezessitativ (Notwendigkeitsform) sowie der Konditionalis. Im Gegensatz zu dem Konditional der indoeuropäischen Sprachen bezeichnet der Konditionalis im Türkischen nicht die (im Hauptsatz stehende) bedingte Handlung, sondern die (im Konditionalsatz stehende) bedingende Handlung.
Mit Ausnahme der Konditionalsätze werden Nebensätze fast nur durch Verbalnomina (Partizipien und Infinitive) sowie sogenannte Konverben ausgedrückt. Diese können auch ein eigenes Subjekt haben.
Die Konjugation türkischer Verben erfolgt nach sehr festen Gesetzmäßigkeiten. In der nachfolgenden Tabelle sind am Beispiel von gelmek (kommen) die einfachen Tempora dargestellt.
*) zu -yor- siehe die Anmerkung am Ende der folgenden Tabelle
Aus diesen Grundformen lassen sich durch beliebig viele Kombinationen mit Hilfsverben eine fast unbegrenzte Zahl weiterer Zeiten bilden, deren nuancenhaften Unterschiede im Deutschen oft kaum wiederzugeben sind. Das gebräuchlichste Hilfsverbum ist die Kopula sein, die in eigenen Formen nur im Präteritum (idi), im Perfekt (imiş) und im Konditionalis (ise) (sowie als Konverb iken) vorkommt. Die Formen des Hilfsverbums werden der modifizierten Form nachgestellt, übernehmen von dieser die Personalsuffixe und verschmelzen mit dieser oft zu einem Wort und gleichen sich dann wie Suffixe entsprechend der Vokalharmonie an: gelmiş idi > gelmişti. Für das Wort bulmak – finden lauten diese Formen bulmuş idi > bulmuştu. Die Zeitformen der Kopula haben z. T. eine von den Zeitformen der Vollverben abweichende Bedeutung und allein stehend eine besondere Art der Verneinung. Eine Auswahl der zusammengesetzten Zeiten, die auch im Deutschen eine Entsprechung haben, ist im Folgenden aufgeführt.
-yor- unterliegt weder im osmanischen noch im modernen Türkisch der Vokalharmonie.[32] Es ist der Rest eines ursprünglich selbständigen Wortes.[33]

Neben diesen Formen existieren weitere Zeiten, etwa Umschreibungen mit dem Infinitiv, auf die hier aber nicht näher eingegangen werden kann. Es bestehen auch weitere Kombinationsmöglichkeiten mit Hilfsverben.
Zum Beispiel lassen sich Präsens und Präteritum des Hilfsverbums (idi) zur „-iyordu“-Vergangenheit, einem Imperfekt, kombinieren, das eine dauernde oder dauernd dargestellte Handlung in der Vergangenheit wiedergibt: geliyordu (er kam, entspricht etwa: he was coming im Englischen) oder der Aorist mit imiş: gelirmiş (er soll kommen als Ausdruck einer auf Mitteilungen anderer beruhender Erwartung). İmiş ist zwar formal ein Perfekt, hat aber keine Zeitbedeutung, sondern vermittelt lediglich den Aspekt des Hörensagens.[34]
Die Verneinung wird in der Regel mit dem verneinten Verbalstamm (s. o.) gebildet. Eine Ausnahme bildet der Aorist, der ein eigenes Suffix für die verneinte Form besitzt. Z. B. lautet im Präteritum die verneinte Form von geldi – er ist gekommen gelmedi – er ist nicht gekommen. Im Aorist ist dies anders, die Verneinung von gelir – er kommt (schon) lautet gelmez – er kommt (definitiv) nicht. Weitere Ausnahmen sind die Kopula, die mit değil verneint wird, und das wichtige, zwischen Nomen und Verbum schwankende Wort var -vorhanden (sein), das zum Ausdruck für Besitz und Eigentum verwendet wird und mit yok ein eigenes Wort für die Verneinung besitzt.
Es gibt sowohl den vollen als auch den verkürzten Infinitiv. Der volle Infinitiv endet je nach Vokalharmonie auf -mek oder -mak. Der verkürzte Infinitiv endet auf -me oder -ma (gelme – das Kommen; gitme – das Gehen; yumurtlama – das Eierlegen; eskiden kalma – seit Alters her geblieben; doğma büyüme – geboren (und) aufgewachsen; dondurma – Speiseeis (wörtl.: eingefroren); dolma – gefüllt(-e Weinblätter/Paprikaschoten etc.)).[35]
Im Türkischen werden die Personalsuffixe direkt an die Zeitformen angehängt. Die endungslosen Zeitformen von Aorist und Perfekt können auch adjektivisch als Partizipien verwendet werden:
gelmiş-im (mit Personalsuffix): ich bin gekommen (Perfekt), dagegen: gelmiş bir tren (als adjektivisches Partizip): ein (an)gekommener Zug
Die Personalsuffixe können auch unmittelbar an ein Nomen, gleich ob Substantiv oder Adjektiv, treten. In diesem Fall nehmen sie die Bedeutung einer Kopula an. Bei der Wahl der Suffixe ist auf die große Vokalharmonie zu achten.
In der dritten Person wird das Pluralsuffix weggelassen, wenn es zum Verständnis nicht erforderlich ist, weil etwa das Subjekt schon ein Plural ist: Evler büyük. (Die Häuser sind groß.)
Die Zugehörigkeitsverhältnisse (Possessivverbindungen) werden im Türkischen so gebildet, dass direkt am betreffenden Nomen die Possessivendung angehängt wird. Dabei wird die große Vokalharmonie berücksichtigt. Ist der letzte Buchstabe des Wortes ein Konsonant, wird zudem auf dessen Stimmhaftigkeit geachtet.
Eine wichtige Rolle spielen die Possessivsuffixe bei der Bildung von Genitivkonstruktionen. Der Genitiv drückt aus, dass eine andere Sache oder Person der oder zu der Person oder Sache gehört, die im Genitiv steht. Dieses in Genitiv stehende Substantiv oder Pronomen wird vorangestellt, und das Wort, das die zugehörige Person oder Sache bezeichnet, nimmt das passende Possessivsuffix an.
Beispiele mit Hervorhebung des Genitiv- und des Possessivsuffixes:
Fällt das Genitivsuffix weg, verliert das betreffende Substantiv seine Individualität und wird zum Typ. Das Possessivsuffix der 3. Person (-(s)i/ı/ü/u) verknüpft dann zwei Substantive zu einem neuen Begriff.
Die Reihenfolge, in der die verwendeten Suffixe an Substantive angehängt werden, ist strikt definiert. Als erstes wird das Pluralsuffix angehängt, dann folgen der Reihe nach das Possessivsuffix, das Kasussuffix und zum Schluss ein Personalsuffix:
Das türkische Präsens hat dieselbe Bedeutung wie das deutsche Präsens. Das Besondere daran ist aber, dass es bei jedem Verb exakt gleich konjugiert wird und keine Ausnahmen kennt.
Bildungsregel: Verbstamm (+Bindevokal entsprechend der großen Vokalharmonie) + -yor + Personalsuffix -um/-sun/-/-uz/-sunuz/-lar
Beispiel gülmek (lachen):
Beispiel uyumak (schlafen):
Beispiel aramak (suchen):
Die „di-Vergangenheit“ hat die gleiche Funktion wie das Perfekt oder das Imperfekt im Deutschen und wird gebraucht bei abgeschlossenen Handlungen.
Bildungsregel: Verbstamm + -di/-dı/-dü/-du + Personalsuffix -m/-n/-/-k/-niz/-ler.
Beispiel gitmek (gehen)
Im Türkischen werden Satzanfänge, Eigennamen, Titel, Beinamen und Anreden groß geschrieben. Sprach-, Religions-, Volks-, Stammes- und Clanzugehörigkeit erscheinen ebenfalls mit initialen Großbuchstaben. Zusätzlich gibt es noch zahlreiche Sonderregeln.[36]
Der Stamm von ursprünglichen türkischen Wörtern war in der Literatursprache des Osmanischen Reichs zu Gunsten von Lehnwörtern aus dem Persischen (Kunst, Kultur und Lebensart) und dem Arabischen (Religion) möglichst klein gehalten worden und galt als bäuerlich. Seit der zweiten Hälfte des 19. Jahrhunderts begann sich dies zunehmend zu ändern und gipfelte in der kemalistischen Sprachreform der 1930er Jahre. Nicht alle diese Lehnwörter konnten im modernen Türkisch durch alte türkische Wörter oder durch türkische Neuschöpfungen ersetzt werden. Der Umfang des Gebrauchs und der Verständlichkeit dieser Lehnwörter ist aber vom Kreis der Verwender und vom Publikum abhängig.[37]
Die neueste Ausgabe des Büyük Türkçe Sözlük („Großes Türkisches Wörterbuch“), des offiziellen Wörterbuches der türkischen Sprache, veröffentlicht durch das Institut für die türkische Sprache Türk Dil Kurumu, beinhaltet 616.767 Wörter, Ausdrücke, Begriffe und Nomen.[38]
Obwohl bei der kemalistischen Sprachreform viele arabische und persische Wörter durch türkische ersetzt worden sind, liefert die arabische Sprache neben dem Französischen besonders viele Lehnwörter. Viele der Lehn- und Fremdwörter arabischen Ursprungs sind über das Persische entlehnt.
Die folgenden statistischen Angaben nach einem türkischen Wörterbuch von 2005 erfassen alle Wörter der Schriftsprache.
Insgesamt 14,18 % (14.816 von 104.481) der Wörter im Türkischen sind Lehnwörter. Lehnwörter stammen aus folgenden Sprachen (Rangfolge nach der Anzahl der Wörter):[39]
Im Jahre 1973 untersuchte die Wissenschaftlerin Kâmile İmer anhand fünf türkischer Tageszeitungen (Ulus, Akşam, Cumhuriyet, Milliyet und Hürriyet) den Wortgebrauch in der Presse, wo der sich stark verändernde Sprachanteil von Lehnwörtern ersichtlich wird:[41]
Einige Beispiele für Lehnwörter aus anderen Sprachen:
Anzahl der türkischen Wörter in anderen Sprachen:[42][43]
Beispielwörter mit türkischer Herkunft:

Genji Monogatari (japanisch 源氏物語, „Die Geschichte vom Prinzen Genji“) ist der erste psychologische Roman der japanischen Literaturgeschichte und wird der Hofdame Murasaki Shikibu (ca. 978–1014) zugeschrieben. Gelegentlich wird er als der erste Roman überhaupt angesehen, was jedoch umstritten ist. Die Geschichte vom Prinzen Genji hat einen festen Stellenwert in der japanischen Kultur und gilt als ein Werk von herausragendem Rang.
Protagonisten sind Genji, spätgeborener Sohn eines alternden Tennō, den sein Vater zwar bevorzugt, aber nicht über seinen gesetzlichen Erben stellen kann, und dessen Konkubine Murasaki. Genji wird traditionsgemäß in die Familie der Minamoto (alias Genji) ausgegliedert, muss nicht arbeiten und verbringt seine Zeit mit den schönen Künsten wie Malen, Dichtung und Kalligrafie, und mit militärischen Sportarten. Sehr früh entwickelt sich auch sein Interesse für das andere Geschlecht, und er kann dank seiner gehobenen Stellung seine Gelüste befriedigen. Das Ergebnis sind viele ganz unterschiedliche Affären mit Frauen. So trifft er zum Beispiel auf ein Mädchen, Murasaki, das ihn fasziniert, da sie die Nichte einer von ihm früher verehrten Hofdame und dieser ähnlich ist.
Nach der Abdankung des alten Tennō gibt es Auseinandersetzungen mit dem neuen Kaiser und vor allem dessen Mutter, die früher zugunsten von Genjis Mutter vernachlässigt worden war. Genji geht freiwillig in die Verbannung, kann aber später an den Hof zurückkehren. Auch fernab des Hofes hat er eine Beziehung und zeugt sein erstes Kind, kann jedoch seine Geliebte nicht mit zurück an den Hof nehmen.
Zurückgekehrt in die Hauptstadt und in seine vorherige gehobene Position, setzt er seine Abenteuer mit Frauen fort. Er nimmt Murasaki zu sich und erzieht sie wie sein eigenes Kind, kann aber auch bei ihr nicht der Versuchung widerstehen, sie zu seiner Geliebten zu machen. Er schafft es zeitlebens nicht, einer Dame treu zu bleiben, und beherbergt auch mehrere Damen gleichzeitig in seinem Haus, die oft wirtschaftlich von ihm abhängig sind.
Das Kapitel Otome (Kapitel 21) enthält eine Beschreibung, wie am Hof zur Unterhaltung musiziert wurde: Bei einer Zusammenkunft der Adligen trägt ein Freund Genjis, der Naidaijin (Lordsiegelbewahrer, Minister), ein improvisiertes Gedicht vor, in dem es heißt, er, der Naidaijin, habe die wagon (Wölbbrettzither) sorgfältig auf die richtigen Töne gestimmt und mit ihr seine Verse begleitet. Er habe so feinsinnig in bedeutungslosen Silben gesungen, dass er alle anwesenden Damen beeindruckte.[1]
Nach Murasakis Tod scheint Genji seinen Lebenswillen zu verlieren. So dreht sich das Kapitel Maboroshi (Kapitel 41) um seine Gedanken über die Vergänglichkeit. Wie und wann er stirbt, wird in der Geschichte jedoch nicht erläutert; das nächste Kapitel Kumogakure ist ohne Inhalt und wahrscheinlich absichtlich von der Autorin so verfasst worden.
Protagonisten des letzten Viertels des Buchs, der sogenannten „Uji-Kapitel“, die nach Genjis Tod spielen, sind seine Söhne Niou und Kaoru, von denen nur einer sein leibliches Kind ist. Ihre Geschichte endet jedoch sehr abrupt, ohne Abschluss.
Seit Jahrhunderten sind sich die Gelehrten uneins darüber, ob wirklich alle 54 Kapitel des Genji Monogatari von der gleichen Autorin stammen. Manche glauben, dass die Kapitel ab 33 von Murasakis Tochter geschrieben wurden, andere vermuten einen Autorenwechsel nach dem Tod Genjis, also ab Kapitel 42. Ebenfalls unklar ist, ob die heute erhaltene Fassung vollständig ist, noch weitere Kapitel existierten oder die Autorin nie ein wirkliches Ende der Geschichte plante. Der einzige handfeste Anhaltspunkt ist ein genau datierbarer Tagebucheintrag in dem so genannten Sarashina Nikki, in dem die Autorin ihrer Freude darüber Ausdruck gibt, eine vollständige Kopie des Genji Monogatari erhalten zu haben.
Auffällig ist, dass die weibliche Hauptfigur des Buches genauso heißt wie die Autorin. Jedoch hat hier nicht die Autorin die Protagonistin nach sich benannt, sondern es ist umgekehrt: Der wahre Name der Verfasserin ist unbekannt; man weiß nur, dass sie Hofdame der Kaiserin war. Deshalb wurde sie von der Nachwelt Murasaki getauft.
Obwohl sich das Klassischjapanische des Genji Monogatari weit weniger vom heutigen Japanisch unterscheidet als das Mittelhochdeutsch vom heutigen Deutsch, ist das Buch für einen heutigen Japaner nahezu unlesbar. Dies liegt neben der komplexen, von Höflichkeitsformen durchdrungenen Grammatik des alten Japanisch auch daran, dass sehr viele Dinge nur angedeutet werden, einschließlich der Personennamen. Tatsächlich ist fast keine der Personen im Buch benannt, da dies als unhöflich galt. Stattdessen werden die Personen durch ihren Rang (bei Männern), Verwandtschaftsbeziehungen oder Kleidung (bei Frauen) oder durch vorherige Äußerungen in der Konversation identifiziert, wodurch es sehr schwer wird, den Überblick zu behalten. Eine weitere Komplikation ist die in der Heian-Zeit übliche idiomatische Verwendung von bekannten Gedichten oder Variationen davon in der Konversation, die oft nur in Bruchstücken wiedergegeben sind. Wer die zitierten alten Gedichte (meist in der Tanka-Form) nicht kennt, kann somit oftmals nicht verstehen, was ein Sprecher aussagen will.
Für diese Probleme gibt es zwei verbreitete Lösungen: Es werden einerseits Originaltexte mit ausführlichen Anmerkungen veröffentlicht, andererseits gibt es auch modernisierte Fassungen, in denen unter anderem die Personen mit Namen versehen werden. Beispielsweise ist Genjis erste Ehefrau per Konvention als Aoi bekannt, nach dem Titel des Kapitels, in dem sie stirbt.
Die frühesten noch erhaltenen Versionen sind Papierrollen aus dem 12. Jahrhundert. Der größte Teil wird heute im Tokugawa-Kunstmuseum in Nagoya aufbewahrt, ein kleinerer Teil im Gotō-Kunstmuseum in Tokio. Da die Rollen äußerst empfindlich sind, liegen sie versiegelt und sind der Öffentlichkeit nicht zugänglich. Die Rollen sind Nationalschätze Japans.
Der Roman diente als Vorlage für zahlreiche japanische Illustrationen des 17. Jahrhunderts, die lesende und schreibende Frauen darstellen.[2]
1951 wurde die Geschichte von Genji von Kozaburo Yoshimura verfilmt, mit Kazuo Hasegawa und Michiyo Kogure in den Hauptrollen. 1966 drehte Kon Ichikawa eine weitere moderne Verfilmung.
In der Belletristik ist die Geschichte von Liza Dalby als „Pflaumenblüten im Schnee“ bearbeitet.
Es gibt auch eine Manga-Adaption des Stoffs mit dem Titel Asakiyumemishi, von Yamato Waki aus dem Jahre 1980. Diese erschien 1992 als Genji Monogatari in Deutschland, wurde aber nach drei Bänden eingestellt.
Eine indirekte Adaption ist das Videospiel Genji, das 2005 für die Playstation 2 erschienen ist.
Im Jahr 2009 wurde eine 11-teilige Anime-Serie mit dem Namen Genji Monogatari Sennenki: Genji (源氏物語千年紀 Genji) veröffentlicht.
Seit 2011 erscheint der Manga Minamoto-kun Monogatari von Minori Inaba, der das Genji Monogatari als Vorlage hat.

Der Merkur ist mit einem Durchmesser von knapp 4880 Kilometern der kleinste, mit einer durchschnittlichen Sonnenentfernung von etwa 58 Millionen Kilometern der sonnennächste und somit auch schnellste Planet im Sonnensystem. Er hat mit einer maximalen Tagestemperatur von rund +430 °C und einer Nachttemperatur bis −170 °C die größten Oberflächen-Temperaturschwankungen aller Planeten.
Aufgrund seiner Größe und seiner chemischen Zusammensetzung zählt er zu den erdähnlichen Planeten.
Wegen seiner Sonnennähe ist er von der Erde aus schwer zu beobachten, da er nur einen maximalen Winkelabstand von etwa 28° von der Sonne erreicht. Freiäugig ist er nur maximal eine Stunde lang entweder am Abend- oder am Morgenhimmel zu sehen, teleskopisch hingegen auch tagsüber. Details auf seiner Oberfläche sind ab einer Fernrohröffnung von etwa 20 cm zu erkennen.
In 46 % der Zeit ist Merkur der am nächsten bei der Erde befindliche Planet.
Benannt ist der Merkur nach dem Götterboten Mercurius, dem römischen Gott der Händler und Diebe. Sein astronomisches Symbol ist ☿.
Als sonnennächster Planet hat Merkur auf einer Umlaufbahn mit der großen Halbachse von 0,387 AE (57,9 Mio. km) – bei einer mittleren Entfernung zum Sonnenzentrum von 0,403 AE (60,4 Mio. km) – mit knapp 88 Tagen auch die kürzeste Umlaufzeit. Mit einer numerischen Exzentrizität von 0,2056 ist die Umlaufbahn des Merkur stärker elliptisch als die aller anderen großen Planeten des Sonnensystems. So liegt sein sonnennächster Punkt, das Perihel, bei 0,307 AE (46,0 Mio. km) und sein sonnenfernster Punkt, das Aphel, bei 0,467 AE (69,8 Mio. km). Ebenso ist die Neigung seiner Bahnebene gegen die Erdbahnebene mit 7° größer als die aller anderen Planeten. Eine dermaßen hohe Exzentrizität und Bahnneigung sind ansonsten eher typisch für Zwergplaneten wie Pluto und Eris.
Für eine komplette Periheldrehung von 360° benötigt der Merkur rund 225.000 Jahre bzw. rund 930.000 Umläufe und erfährt so je Umlauf ein um rund 1,4″ gedrehtes Perihel.
Bereits die newtonsche Mechanik sagt voraus, dass der gravitative Einfluss der anderen Planeten das Zweikörpersystem Sonne-Merkur stört. Durch diese Störung führt die große Bahnachse der Merkurbahn eine langsame rechtläufige Drehung in der Bahnebene aus. Der Merkur durchläuft also streng genommen keine Ellipsen-, sondern eine Rosettenbahn. In der zweiten Hälfte des 19. Jahrhunderts waren die Astronomen in der Lage, diese Veränderungen, insbesondere die Lage des Merkurperihels, mit großer Genauigkeit zu messen. Urbain Le Verrier, der damalige Direktor des Pariser Observatoriums, bemerkte, dass die Präzession (Drehung) des Perihels für Merkur 5,74″ (Bogensekunden) pro Jahr beträgt. Dieser Wert konnte allerdings nicht völlig mit der klassischen Mechanik von Isaac Newton erklärt werden. Laut der newtonschen Himmelsmechanik dürfte er nur 5,32″ betragen, der gemessene Wert ist also um 0,43″ pro Jahr zu groß, der Fehler beträgt also 0,1″ (bzw. 29 km) pro Umlauf. Darum vermutete man neben einer verursachenden Abplattung der Sonne noch einen Asteroidengürtel zwischen dem Merkur und der Sonne oder einen weiteren Planeten, der für diese Störungen verantwortlich sein sollte.
Die Existenz dieses weiteren Planeten galt als so wahrscheinlich, dass mit Vulkan bereits ein Name festgelegt wurde. Dennoch konnte trotz intensiver Suche kein entsprechendes Objekt innerhalb der Merkurbahn gefunden werden. Dies wurde zunächst auf die große Nähe zur Sonne zurückgeführt, die eine visuelle Entdeckung des Planeten erschwerte, da die Sonne ihn überstrahlte.
Die Suche nach Vulkan erübrigte sich erst dann vollständig, als die Allgemeine Relativitätstheorie die systematische Abweichung zwischen der berechneten und der beobachteten Bahn nicht mit einem zusätzlichen Massenkörper erklärte, sondern mit relativistischer Verzerrung der Raumzeit in Sonnennähe. Der anhand der ART berechnete Überschuss von 43,03″ (Unsicherheit: 0,03″) je Jahrhundert stimmt gut mit der beobachteten Differenz von 42,96″ (Unsicherheit: 0,94″) überein.[3] 
Konstantin Batygin und Gregory Laughlin von der University of California, Santa Cruz sowie davon unabhängig Jacques Laskar vom Pariser Observatorium haben durch Computersimulationen festgestellt, dass das innere Sonnensystem auf lange Sicht nicht stabil ist. In ferner Zukunft – in einer Milliarde Jahren oder mehr – könnte Jupiters Anziehungskraft Merkur aus seiner jetzigen Umlaufbahn herausreißen, indem ihr Einfluss nach und nach Merkurs große Bahnexzentrizität weiter vergrößert, bis der Planet in seinem sonnenfernsten Punkt die Umlaufbahn der Venus kreuzt.[4]
Daraufhin könnte es vier Szenarien geben: Merkur stürzt in die Sonne; er wird aus dem Sonnensystem geschleudert; er kollidiert mit der Venus oder mit der Erde. Die Wahrscheinlichkeit, dass eine dieser Möglichkeiten eintrifft, bevor sich die Sonne zu einem Roten Riesen aufblähen wird, liegt jedoch nur bei rund 1 %.[5]
Die Achse von Merkurs rechtläufiger Rotation steht fast senkrecht auf seiner Bahnebene. Deswegen gibt es auf dem Merkur keine Jahreszeiten mit unterschiedlicher Tageslänge. Allerdings variiert die Sonneneinstrahlung aufgrund der Exzentrizität der Bahn beträchtlich: Im Perihel trifft etwa 2,3-mal so viel Energie von der Sonne auf die Merkuroberfläche wie im Aphel. Dieser Effekt, der beispielsweise auf der Erde wegen der geringen Exzentrizität der Bahn klein ist (7 %), führt zu Jahreszeiten auf dem Merkur.[6]
Radarbeobachtungen zeigten 1965,[7] dass der Planet nicht, wie ursprünglich von Giovanni Schiaparelli 1889 angenommen,[8] eine einfache gebundene Rotation besitzt, das heißt, der Sonne immer dieselbe Seite zuwendet (so, wie der Erdmond der Erde immer dieselbe Seite zeigt). Vielmehr besitzt er als Besonderheit eine gebrochen gebundene Rotation und dreht sich während zweier Umläufe exakt dreimal um seine Achse. Seine siderische Rotationsperiode beträgt zwar 58,646 Tage, aber aufgrund der 2:3-Kopplung an die schnelle Umlaufbewegung mit demselben Drehsinn entspricht der Merkurtag – der zeitliche Abstand zwischen zwei Sonnenaufgängen an einem beliebigen Punkt – auf dem Planeten mit 175,938 Tagen auch genau dem Zeitraum von zwei Sonnenumläufen. Nach einem weiteren Umlauf geht die Sonne dementsprechend am Antipodenort auf. Durchläuft der Merkur den sonnennächsten Punkt seiner ziemlich stark exzentrischen Bahn, das Perihel, steht das Zentralgestirn zum Beispiel immer abwechselnd über dem Calorisbecken am 180. Längengrad oder über dessen chaotischem Antipodengebiet am Nullmeridian im Zenit. Während des Merkurs höchsten Bahngeschwindigkeiten im Perihelbereich ist die Winkelgeschwindigkeit seiner Bahnbewegung größer als die seiner Rotation, sodass die Sonne am Merkurhimmel eine rückläufige Schleifenbewegung vollführt.
Zur Erklärung der Kopplung von Rotation und Umlauf wird unter Caloris Planitia (der „heißen“ Tiefebene) eine Massekonzentration ähnlich den sogenannten Mascons der großen, annähernd kreisförmigen Maria des Erdmondes, angenommen, an der die Gezeitenkräfte der Sonne die vermutlich einst schnellere Eigendrehung des Merkurs zu dieser ungewöhnlichen Resonanz heruntergebremst haben.
Der Merkur hat keinen Mond. Die Existenz eines solchen wurde auch niemals ernsthaft in Erwägung gezogen. Es besteht jedoch seit Mitte der 1960er Jahre von verschiedenen Wissenschaftlern die Hypothese, dass der Merkur selbst einmal ein Mond der Venus war. Anlass zu der Annahme gaben anfangs nur einige Besonderheiten seiner Umlaufbahn. Später kamen seine spezielle Rotation sowie die zum Erdmond analoge Oberflächengestalt von zwei auffallend unterschiedlichen Hemisphären hinzu. Mit dieser Annahme lässt sich auch erklären, warum die beiden Planeten als einzige im Sonnensystem mondlos sind.[9][10]
Am 27. März 1974 glaubte man, einen Mond um den Merkur entdeckt zu haben. Zwei Tage, bevor Mariner 10 den Merkur passierte, fing die Sonde an, starke UV-Emissionen zu messen, die kurz darauf aber wieder verschwanden. Drei Tage später tauchten die Emissionen wieder auf, schienen sich aber vom Merkur fortzubewegen. Einige Astronomen vermuteten einen neu entdeckten Stern, andere wiederum einen Mond. Die Geschwindigkeit des Objekts wurde mit 4 km/s berechnet, was etwa dem erwarteten Wert eines Merkurmondes entsprach. Einige Zeit später konnte das Objekt schließlich als Stern 31 Crateris identifiziert werden.[11]
Merkur gleicht äußerlich dem planetologisch-geologisch inaktiven Erdmond, doch das Innere entspricht anscheinend viel mehr dem der geologisch sehr dynamischen Erde.
Der Merkur hat keine Atmosphäre im herkömmlichen Sinn, denn sie ist dünner als ein labortechnisch erreichbares Vakuum, ähnlich wie die Atmosphäre des Mondes. Die „atmosphärischen“ Bestandteile Wasserstoff H2 (22 %) und Helium (6 %) stammen sehr wahrscheinlich aus dem Sonnenwind, wohingegen Sauerstoff O2 (42 %), Natrium (29 %) und Kalium (0,5 %) vermutlich aus dem Material der Oberfläche freigesetzt wurden (die Prozentangaben sind ungenaue Schätzungen für die Volumenanteile der Gase). Der Druck der Gashülle beträgt nur etwa 10−15 Bar am Boden von Merkur und die Gesamtmasse der Merkuratmosphäre damit nur etwa 1000 Kilogramm.[1]
Aufgrund der hohen Temperaturen und der geringen Anziehungskraft kann der Merkur die Gasmoleküle nicht lange halten, sie entweichen durch Photoevaporation stets schnell ins All. Bezogen auf die Erde wird jener Bereich, für den dies zutrifft, Exosphäre genannt; es ist die Austauschzone zum interplanetaren Raum. Eine ursprüngliche Atmosphäre als Entgasungsprodukt des Planeteninnern ist dem Merkur längst verloren gegangen; es gibt auch keine Spuren einer früheren Erosion durch Wind und Wasser. Allerdings enthält die Exosphäre geringe Anteile von Wasserdampf, wie Messungen der Merkur-Sonde Messenger zwischen 2011 und 2015 ergaben. Er könnte entweder aus den Schweifen vorbeiziehender Kometen oder aus den Wassereisvorkommen auf den Böden von Kratern in den Polarregionen des Planeten stammen.[12] Das Fehlen einer richtigen Gashülle, welche für einen gewissen Ausgleich der Oberflächentemperaturen sorgen würde, bedingt in dieser Sonnennähe extreme Temperaturschwankungen zwischen der Tag- und der Nachtseite. Gegenüber den Nachttemperaturen, die bis auf −173 °C sinken, wird die während des geringsten Sonnenabstands beschienene Planetenseite bis auf +427 °C aufgeheizt. Während des größten Sonnenabstands beträgt die höchste Bodentemperatur bei der großen Bahnexzentrizität des Merkur noch rund +250 °C.
Wegen der schwierigen Erreichbarkeit auf der sonnennahen Umlaufbahn und der damit verbundenen Gefahr durch den intensiveren Sonnenwind haben bislang erst zwei Raumsonden, Mariner 10 und Messenger, den Planeten besucht und eingehender studiert. Bei drei Vorbeiflügen in den 1970er Jahren konnte Mariner 10 lediglich etwa 45 % seiner Oberfläche kartieren. Die Merkursonde Messenger hatte gleich bei ihrem ersten Vorbeiflug im Januar 2008 auch einige von Mariner 10 nicht erfasste Gebiete fotografiert und konnte die Abdeckung auf etwa 66 % erhöhen.[13] Mit ihrem zweiten Swing-by im Oktober 2008 stieg die Abdeckung auf rund 95 %.[14]
Die mondähnliche, von Kratern durchsetzte Oberfläche aus rauem, porösem, dunklem Gestein reflektiert das Sonnenlicht nur schwach. Die mittlere sphärische Albedo beträgt 0,06, das heißt, die Oberfläche streut im Durchschnitt 6 % des von der Sonne praktisch parallel eintreffenden Lichtes zurück. Damit ist der Merkur im Mittel noch etwas dunkler als der Mond (0,07).
Anhand der zerstörerischen Beeinträchtigung der Oberflächenstrukturen untereinander ist, wie auch bei Mond und Mars, eine Rekonstruktion der zeitlichen Reihenfolge der prägenden Ereignisse möglich. Es gibt in den abgelichteten Gebieten des Planeten keine Anzeichen von Plattentektonik; Messenger hat aber zahlreiche Hinweise auf vulkanische Eruptionen gefunden.
Die Oberfläche des Merkur ist mit Kratern übersät. Die Verteilung der Einschlagstrukturen ist gleichmäßiger als auf dem Mond und dem Mars; demnach ist das Alter seiner Oberfläche gleichmäßig sehr hoch.[14] Mit ein Grund für die hohe Kraterdichte ist die äußerst dünne Atmosphäre, die ein ungehindertes Eindringen von Kleinkörpern gestattet. Die große Anzahl der Krater je Fläche – ein Maß für das Alter der Kruste – spricht für eine sehr alte, das heißt, seit der Bildung und Verfestigung des Merkurs von vor etwa 4,5 bis vor ungefähr 4 Milliarden Jahren sonst wenig veränderte Oberfläche.
Wie auch beim Mond zeigen die Krater des Merkurs ein weiteres Merkmal, das für eine durch Impakt entstandene Struktur als typisch gilt: Das hinausgeschleuderte und zurückgefallene Material, das sich um den Krater herum anhäuft; manchmal in Form von radialen Strahlen, wie man sie auch als Strahlensysteme auf dem Mond kennt. Sowohl diese speichenartigen Strahlen als auch die Zentralkrater, von denen sie jeweils ausgehen, sind aufgrund des relativ geringen Alters heller als die Umgebung. Die ersten Beobachtungen der Strahlen des Merkurs machte man mit den Radioteleskopen Arecibo und Goldstone und mithilfe des Very Large Array (VLA) des nationalen Radioobservatoriums der Vereinigten Staaten (siehe auch Astrogeologie).
Der erste Krater, der durch die Raumsonde Mariner 10 während ihrer ersten Annäherung erkannt wurde, war der 40 km breite, aber sehr helle Strahlenkrater Kuiper (siehe Bild rechts). Der Krater wurde nach dem niederländisch-US-amerikanischen Mond- und Planetenforscher Gerard Kuiper benannt, der dem Mariner-10-Team angehörte und noch vor der Ankunft der Sonde verstarb.
Nördlich des Äquators liegt Caloris Planitia, ein riesiges, kreisförmiges, aber ziemlich flaches Becken. Mit einem Durchmesser von etwa 1550 km ist es das größte bekannte Gebilde auf dem Merkur. Es wurde vermutlich vor etwa 3,8 Milliarden Jahren von einem über 100 km großen Einschlagkörper erzeugt. Der Impakt war so heftig, dass durch die seismischen Schwingungen um den Ort des Einschlags mehrere konzentrische Ringwälle aufgeworfen wurden und aus dem Innern des Planeten Lava austrat. Die von Messenger neu entdeckten vulkanischen Strukturen finden sich insbesondere im Umfeld und auch im Inneren des Beckens.[13] Das Beckeninnere ist von dem Magma aus der Tiefe anscheinend aufgefüllt worden, ähnlich wie die Marebecken des Mondes. Den Boden des Beckens prägen viele konzentrische Furchen und Grate, die an eine Zielscheibe erinnern und ihm Ähnlichkeit mit dem annähernd vergleichbar großen Multiringsystem auf dem Mond geben, in dessen Beckenzentrum das Mare Orientale liegt. Das ziemlich flache Caloris-Becken wird von den Caloris Montes begrenzt, einem unregelmäßigen Kettengebirge, dessen Gipfelhöhen lediglich etwa 1 km erreichen.
Auch andere flache Tiefebenen ähneln den Maria des Mondes. Mare (Mehrzahl: Maria, deutsch ‚Meere‘) ist in der Selenologie – der „Geologie“ des Erdtrabanten – der lateinische Gattungsname für die glatten und dunklen Basaltflächen, die zahlreiche Krater und Becken des Mondes infolge von aus Bodenspalten emporgestiegener und erstarrter Lava ausfüllen. Die glatten Ebenen des Merkurs sind aber nicht dunkel wie die „Mondmeere“. Insgesamt sind sie anscheinend auch kleiner und weniger zahlreich. Sie liegen alle auf der Nordhalbkugel im Umkreis des Caloris-Beckens. Ihre Gattungsbezeichnung ist Planitia, lateinisch für Tiefebene.
Dass sich die mareähnlichen Ebenen auf dem Merkur nicht wie die Maria des Mondes mit einer dunkleren Farbe von der Umgebung abheben, wird mit einem geringeren Gehalt an Eisen und Titan erklärt. Damit ergibt sich jedoch ein gewisser Widerspruch zu der hohen mittleren Dichte des Planeten, die für einen verhältnismäßig sehr großen Metallkern spricht, der vor allem aus Eisen besteht. Dunkle Böden wurden durch Messenger im Caloris-Becken nur als Füllung kleinerer Krater gefunden, und obwohl für deren Material ein vulkanischer Ursprung vermutet wird, zeigen die Messdaten, anders als bei solchem Gestein zu erwarten ist, ebenfalls nur einen sehr geringen Anteil an Eisen. Das Metall ist in Merkurs Oberfläche zu höchstens 6 Prozent enthalten.[16]
Zwei Formationen findet man ausschließlich auf der Merkuroberfläche:
Der in der Planetengeologie profilierte amerikanische Geologe Robert G. Strom hat den Umfang der Schrumpfung der Merkuroberfläche auf etwa 100.000 km² abgeschätzt. Das entspricht einer Verringerung des Planetenradius um bis zu etwa 2 km. Neuere Schätzungen, die wesentlich auf den Messungen der Raumsonde Messenger beruhen, kommen auf einen deutlich höheren Wert von etwa 7 km Kontraktion.[17]
Als Ursache der Kontraktion wird die Abkühlung des Planeten im Anschluss an eine heiße Phase seiner Entstehung gesehen, in der er ähnlich wie die Erde und der Mond von vielen großen Asteroideneinschlägen bis zur Glutflüssigkeit aufgeheizt worden sein soll. Dieser Abschnitt der Entwicklung nahm demnach erst vor etwa 3,8 Milliarden Jahren mit dem „Letzten Schweren Bombardement“ seinen Ausklang, während dessen Nachlassens die Kruste langsam auskühlen und erstarren konnte. Einige der gelappten Böschungen wurden offenbar durch die ausklingende Bombardierung wieder teilweise zerstört. Das bedeutet, dass sie entsprechend älter sind als die betreffenden Krater. Der Zeitpunkt der Merkurschrumpfung wird anhand des Grades der Weltraum-Erosion – durch viele kleinere, nachfolgende Einschläge – vor ungefähr 4 Milliarden Jahren angenommen, also während der Entstehung der mareähnlichen Ebenen.
Laut einer alternativen Hypothese sind die tektonischen Aktivitäten während der Kontraktionsphase auf die Gezeitenkräfte der Sonne zurückzuführen, durch deren Einfluss die Eigendrehung des Merkurs von einer ungebundenen, höheren Geschwindigkeit auf die heutige Rotationsperiode heruntergebremst wurde. Dafür spricht, dass sich diese Strukturen wie auch eine ganze Reihe von Rinnen und Bergrücken mehr in meridionale als in Ost-West-Richtung erstrecken.
Nach der Kontraktion und der dementsprechenden Verfestigung des Planeten entstanden kleine Risse auf der Oberfläche, die sich mit anderen Strukturen, wie Kratern und den flachen Tiefebenen überlagerten – ein klares Indiz dafür, dass die Risse im Vergleich zu den anderen Strukturen jüngeren Ursprungs sind. Die Zeit des Vulkanismus auf dem Merkur endete, als die Kompression der Hülle sich einstellte, sodass dadurch die Ausgänge der Lava an der Oberfläche verschlossen wurden. Vermutlich passierte das während einer Periode, die man zwischen die ersten 700 bis 800 Millionen Jahre der Geschichte des Merkurs einordnet. Seither gab es nur noch vereinzelte Einschläge von Kometen und Asteroiden.
Eine weitere Besonderheit gegenüber dem Relief des Mondes sind auf dem Merkur die sogenannten Zwischenkraterebenen. Im Unterschied zu der auch mit größeren Kratern gesättigten Mondoberfläche kommen auf dem Merkur zwischen den großen Kratern relativ glatte Ebenen mit Hochlandcharakter vor, die nur von verhältnismäßig wenigen Kratern mit Durchmessern von unter 20 km geprägt sind. Dieser Geländetyp ist auf dem Merkur am häufigsten verbreitet. Manche Forscher sehen darin die ursprüngliche, verhältnismäßig unveränderte Merkuroberfläche. Andere glauben, dass ein sehr früher und großräumiger Vulkanismus die Regionen einst geglättet hat. Es gibt Anzeichen dafür, dass sich in diesen Ebenen die Reste größerer und auch vieler doppelter Ringwälle gleich solchen des Mondes noch schwach abzeichnen.
Für die Polregionen des Merkurs lassen die Ergebnisse von Radaruntersuchungen die Möglichkeit zu, dass dort kleine Mengen von Wassereis existieren könnten. Da des Merkurs Rotationsachse mit 0,01° praktisch senkrecht auf der Bahnebene steht, liegt das Innere einiger polnaher Krater stets im Schatten. In diesen Gebieten ewiger Nacht sind dauerhafte Temperaturen von −160 °C möglich. Solche Bedingungen können Eis konservieren, das z. B. durch eingeschlagene Kometen eingebracht wurde. Die hohen Radar-Reflexionen können jedoch auch durch Metallsulfide oder durch die in der Atmosphäre nachgewiesenen Alkalimetalle oder andere Materialien verursacht werden.
Im November 2012 veröffentlichte Messungen der Raumsonde Messenger weisen auf Wassereis im Inneren von Kratern am Merkurnordpol hin, die ständig im Schatten liegen.[18] Außerdem wurden Spuren von organischen Molekülen (einfache Kohlenstoff- und Stickstoffverbindungen) gefunden. Da diese Moleküle als Grundvoraussetzungen für die Entstehung von Leben gelten, rief diese Entdeckung einiges Erstaunen hervor, da dies auf dem atmosphärelosen und durch die Sonne intensiv aufgeheizten Planeten nicht für möglich gehalten worden war. Es wird vermutet, dass diese Spuren an Wasser und organischer Materie durch Kometen, die auf dem Merkur eingeschlagen sind, eingebracht wurden.[19] 
Die Radiowellen, die vom Goldstone-Radioteleskop des NASA Deep Space Network ausgesandt wurden, hatten eine Leistung von 450 Kilowatt bei 8,51 Gigahertz; die vom VLA mit 26 Antennen empfangenen Radiowellen ließen helle Punkte auf dem Radarschirm erscheinen, Punkte, die auf depolarisierte Reflexionen von Wellen vom Nordpol des Merkurs schließen lassen.
Die Studien, die mit dem Radioteleskop von Arecibo gemacht wurden, das Wellen im S-Band (2,4 GHz) mit einer Leistung von 420 kW ausstrahlte, gestatteten es, eine Karte von der Oberfläche des Merkurs anzufertigen, die eine Auflösung von 15 km hat. Bei diesen Studien konnte nicht nur die Existenz der bereits gefundenen Zonen hoher Reflexion und Depolarisation nachgewiesen werden, sondern insgesamt 20 Zonen an beiden Polen. Die erwartete Radarsignatur von Eis ist erhöhte Reflexion und stärkere Depolarisation der reflektierten Wellen. Silikatgestein, das den größten Anteil der Oberfläche ausmacht, zeigt dieses nicht.
Andere Untersuchungsmethoden der zur Erde zurückgeworfenen Strahlen legen nahe, dass die Form dieser Zonen kreisförmig sind, und dass es sich deshalb um Krater handeln könnte.
Am Südpol des Merkurs scheinen sich Zonen hoher Reflexion mit dem Chao Meng-Fu Krater und kleinen Gebieten zu decken, in denen ebenfalls bereits Krater identifiziert wurden.
Am Nordpol gestaltet sich die Situation etwas schwieriger, weil sich die Radarbilder mit denen von Mariner 10 offenbar nicht decken lassen. Es liegt deshalb nahe, dass es Zonen hoher Reflexion geben kann, die sich nicht mit der Existenz von Kratern erklären lassen.
Die Reflexionen der Radarwellen, die das Eis auf der Oberfläche des Merkurs erzeugt, sind geringer als die Reflexionen, die sich mit reinem Eis erzeugen ließen; eventuell liegt es am Vorhandensein von Staub, der die Oberfläche des Kraters teilweise überdeckt.
In der planetaren Nomenklatur der Internationalen Astronomischen Union (IAU) sind für die Bezeichnung von Oberflächenstrukturen auf dem Merkur folgende Konventionen festgelegt:[20]
Ferner wurde die einzige Hochebene (Catuilla Planum) nach dem quechuanischen Wort für (den Planet) Merkur benannt.[21] Für die 32 benannten Albedomerkmale – Gebiete mit besonderem Rückstrahlvermögen – wurde ein Großteil der Namen aus der Merkurkartierung von Eugène Michel Antoniadi übernommen.[22]
Der Merkur ist ein Gesteinsplanet wie die Venus, die Erde und der Mars und ist von allen der kleinste Planet im Sonnensystem. Sein Durchmesser beträgt mit 4878 km nur knapp 40 Prozent des Erddurchmessers. Er ist damit sogar kleiner als der Jupitermond Ganymed und der Saturnmond Titan, dafür aber jeweils mehr als doppelt so massereich wie diese sehr eisreichen Trabanten.
Das Diagramm zeigt, wie stark die mittlere Dichte der erdähnlichen Planeten einschließlich des Erdmondes bei ähnlicher chemischer Zusammensetzung mit dem Durchmesser im Allgemeinen ansteigt. Der Merkur allerdings hat mit 5,427 g/cm³ fast die Dichte der weit größeren Erde und liegt damit für seine Größe weit über dem Durchmesser-Dichte-Verhältnis der anderen. Das zeigt, dass er eine „schwerere“ chemische Zusammensetzung haben muss: Sein sehr großer Eisen-Nickel-Kern soll zu 65 Prozent aus Eisen bestehen, etwa 70 Prozent der Masse des Planeten ausmachen und einen Durchmesser von etwa 3600 km haben. Jüngere Forschungsergebnisse zeigen sogar einen Kerndurchmesser von 4100 km,[23][24] rund 84 Prozent des Planetendurchmessers, womit der Kern größer als der Erdmond wäre. Auf den wohl nur 600 km dünnen Mantel aus Silikaten entfallen rund 30 Prozent der Masse, bei der Erde sind es 62 Prozent. Die Kruste ist mit einigen 10 km relativ dick und besteht überwiegend aus Feldspat und Mineralien der Pyroxengruppe, ist also dem irdischen Basalt sehr ähnlich. Die dennoch etwas höhere Gesamtdichte der Erde resultiert aus der kompressiveren Wirkung ihrer starken Gravitation.[25]
Des Merkurs relativer Gehalt an Eisen ist größer als der jedes anderen großen Objektes im Sonnensystem. Als Erklärung werden verschiedene Hypothesen ins Feld geführt, die alle von einem ehemals ausgeglicheneren Schalenaufbau und einem entsprechend dickeren, metallarmen Mantel ausgehen:
So geht eine Theorie davon aus, dass der Merkur ursprünglich ein Metall-Silikat-Verhältnis ähnlich dem der Chondrite, der meistverbreiteten Klasse von Meteoriten im Sonnensystem, aufwies. Seine Ausgangsmasse müsste demnach etwa das 2,25-fache seiner heutigen Masse gewesen sein. In der Frühzeit des Sonnensystems, vor etwa 4,5 Milliarden Jahren, wurde der Merkur jedoch – so wird gemutmaßt – von einem sehr großen Asteroiden mit etwa einem Sechstel dieser Masse getroffen. Ein Aufschlag dieser Größenordnung hätte einen Großteil der Planetenkruste und des Mantels weggerissen und lediglich den metallreichen Kern übrig gelassen. Eine ähnliche Erklärung wurde zur Entstehung des Erdmondes im Rahmen der Kollisionstheorie vorgeschlagen. Beim Merkur blieb jedoch unklar, weshalb nur ein so geringer Teil des zersprengten Materials auf den Planeten zurückfiel. Nach Computersimulationen von 2006 wird das mit der Wirkung des Sonnenwindes erklärt, durch den sehr viele Teilchen verweht wurden. Von diesen Partikeln und Meteoriten, die nicht in die Sonne fielen, sind demnach die meisten in den interstellaren Raum entwichen und 1 bis 2 Prozent auf die Venus sowie etwa 0,02 Prozent auf die Erde gelangt.
Eine andere Theorie schlägt vor, dass der Merkur sehr früh in der Entwicklung des Sonnensystems entstanden sei, noch bevor sich die Energieabstrahlung der jungen Sonne stabilisiert hat. Auch diese Theorie geht von einer etwa doppelt so großen Ursprungsmasse des innersten Planeten aus. Als der Protostern sich zusammenzuziehen begann, könnten auf dem Merkur Temperaturen zwischen 2500 und 3500 K (Kelvin), möglicherweise sogar bis zu 10.000 K geherrscht haben. Ein Teil seiner Materie wäre bei diesen Temperaturen verdampft und hätte eine Atmosphäre gebildet, die im Laufe der Zeit vom Sonnenwind fortgerissen worden sei.
Eine dritte Theorie argumentiert ähnlich und geht von einer langanhaltenden Erosion der äußeren Schichten des Planeten durch den Sonnenwind aus.
Nach einer vierten Theorie wurde der Merkur kurz nach seiner Bildung von einem oder mehreren Protoplaneten gestreift, die doppelt bis viermal so schwer waren wie er – wobei er große Teile seines Gesteinsmantels verlor.[26]
Trotz seiner langsamen Rotation besitzt der Merkur eine Magnetosphäre, deren Volumen etwa 5 Prozent der Magnetosphäre der Erde beträgt. Es hat mit einer mittleren Feldintensität von 450 Nanotesla an der Oberfläche des Planeten ungefähr 1 Prozent der Stärke des Erdmagnetfeldes. Die Neigung des Dipolfeldes gegen die Rotationsachse beträgt rund 7°. Die Ausrichtung der Magnetpole entspricht der Situation der Erde, das heißt, dass beispielsweise der magnetische Nordpol des Merkurs im Umkreis seiner südlichen Rotationsachse liegt. Die Grenze der Magnetosphäre befindet sich in Richtung der Sonne lediglich in einer Höhe von etwa 1000 Kilometern, wodurch energiereiche Teilchen des Sonnenwinds ungehindert die Oberfläche erreichen können. Es gibt keine Strahlungsgürtel.[27] Insgesamt ist Merkurs Magnetfeld asymmetrisch. Es ist auf der Nordhalbkugel stärker als auf der Südhalbkugel, sodass der magnetische Äquator gegenüber dem geografischen Äquator rund 500 Kilometer nördlich liegt. Dadurch ist die Südhalbkugel für den Sonnenwind leichter erreichbar.[28]
Möglicherweise wird Merkurs Dipolfeld ganz ähnlich dem der Erde durch den Dynamo-Effekt zirkulierender Schmelzen im Metallkern erzeugt; dann müsste seine Feldstärke aber 30-mal stärker sein, als von Mariner 10 gemessen. Einer Modellrechnung zufolge (Ulrich Christensen 2007 im Max-Planck-Institut für Sonnensystemforschung Katlenburg-Lindau)[29] werden große Teile eines im Inneren entstehenden, fluktuierenden Feldes durch elektrisch leitende und stabile Schichtungen des äußeren, flüssigen Kerns stark gedämpft, sodass an der Oberfläche nur ein relativ schwaches Feld übrig bleibt.
Eigentlich sollte der Merkur aufgrund seiner geringen Größe – ebenso wie der wesentlich größere und bereits erstarrte Mars – seit seiner Entstehung schon längst zu stark abgekühlt sein, um in seinem Kern Eisen oder ein Eisen-Nickel-Gemisch noch flüssig halten zu können. Aus diesem Grund wurde eine Hypothese aufgestellt, welche die Existenz des Magnetfeldes als Überbleibsel eines früheren, mittlerweile aber erloschenen Dynamo-Effektes erklärt; es wäre dann das Ergebnis erstarrter Ferromagnetite. Es ist aber möglich, dass sich zum Beispiel durch Mischungen mit Schwefel eine eutektische Legierung mit niedrigerem Schmelzpunkt bilden konnte. Durch ein spezielles Auswertungsverfahren konnte bis 2007 ein Team amerikanischer und russischer Planetenforscher um Jean-Luc Margot von der Cornell-Universität anhand von Radarwellen die Rotation des Merkurs von der Erde aus genauer untersuchen und ausgeprägte Schwankungen feststellen, die mit einer Größe von 0,03 Prozent deutlich für ein teilweise aufgeschmolzenes Inneres sprechen.[30]
Nach der herkömmlichen Theorie zur Entstehung des Planetensystems der Sonne ist der Merkur wie alle Planeten aus einer allmählichen Zusammenballung von Planetesimalen hervorgegangen, die sich zu immer größeren Körpern vereinten. In der letzten Phase der Akkretion schluckten die größeren Körper die kleineren und in dem Bereich des heutigen Merkurorbits bildete sich binnen etwa 10 Millionen Jahren der sonnennächste Planet.
Mit der Aufheizung des Protoplaneten, also des „Rohplaneten“, durch den Zerfall der radioaktiven Elemente und durch die Energie vieler großer und andauernder Einschläge während des Aufsammelns der kleineren Brocken begann das, was man mangels eines merkurspezifischen Begriffes als geologische Entwicklung bezeichnen kann. Der bis zur Glut erhitzte Körper differenzierte sich durch seine innere Gravitation chemisch in Kern, Mantel und Kruste. Mit dem Ausklingen des Dauerbombardements konnte der entstandene Planet beginnen, sich abzukühlen, und es bildete sich aus der äußeren Schicht eine feste Gesteinskruste.
In der folgenden Etappe sind anscheinend alle Krater und andere Spuren der ausklingenden Akkretion überdeckt worden. Die Ursache könnte eine Periode von frühem Vulkanismus gewesen sein. Dieser Zeit wird die Entstehung der Zwischenkraterebenen zugeordnet sowie die Bildung der gelappten Böschungen durch ein Schrumpfen des Merkurs zugeschrieben.
Das Ende des Schweren Bombardements schlug sich in der Entstehung des Caloris-Beckens und den damit verbundenen Landschaftsformen im Relief als Beginn der dritten Epoche eindrucksvoll nieder.
In einer vierten Phase entstanden (wahrscheinlich durch eine weitere Periode vulkanischer Aktivitäten) die weiten, mareähnlichen Ebenen.
Die fünfte und seit etwa 3 Milliarden Jahren noch immer andauernde Phase der Oberflächengestaltung zeichnet sich lediglich durch eine Zunahme der Einschlagkrater aus. Dieser Zeit werden die Zentralkrater der Strahlensysteme zugeordnet, deren auffällige Helligkeit als ein Zeichen der Frische angesehen werden.
Die Abfolge der Ereignisse hat im Allgemeinen eine überraschend große Ähnlichkeit mit der Geschichte der Oberfläche des Mondes; in Anbetracht der ungleichen Größe, der sehr verschiedenen Orte im Sonnensystem und den damit verbundenen unterschiedlichen Bedingungen war das nicht zu erwarten.
Der Merkur ist mindestens seit der Zeit der Sumerer (3. Jahrtausend v. Chr.) bekannt. Die Griechen der Antike gaben ihm zwei Namen, Apollo, wenn er am Morgenhimmel die Sonne ankündigte, und Hermes, wenn er am Abendhimmel der Sonne hinterherjagte.[31]
Die griechischen Astronomen wussten allerdings, dass es sich um denselben Himmelskörper handelte. Nach nicht eindeutigen Quellen hat Herakleides Pontikos möglicherweise sogar schon geglaubt, dass der Merkur und auch die Venus um die Sonne kreisen und nicht um die Erde. 1543 veröffentlichte Nikolaus Kopernikus sein Werk De revolutionibus orbium coelestium (lat.: Über die Umschwünge der himmlischen Kreise), indem er die Planeten ihrer Geschwindigkeit nach in kreisförmigen Bahnen um die Sonne anordnete, womit der Merkur der Sonne am nächsten war.
Die Römer benannten den Planeten wegen seiner schnellen Bewegung am Himmel nach dem geflügelten Götterboten Mercurius.
Die Umlaufbahn des Merkurs bereitete den Astronomen lange Zeit Probleme. Kopernikus etwa schrieb dazu in De revolutionibus: „Der Planet hat uns mit vielen Rätseln und großer Mühsal gequält als wir seine Wanderungen erkundeten“. 1629 gelang es Johannes Kepler mithilfe von Beobachtungsdaten seines Vorgängers Tycho Brahe erstmals einen sogenannten Merkurtransit für den 7. November 1631 (auf etwa einen halben Tag genau) vorherzusagen. Als Pierre Gassendi diesen Durchgang vor der Sonne beobachten konnte, stellte er feste, dass der Merkur nicht wie von Ptolemäus im 2. Jahrhundert geschätzt ein Fünfzehntel des Sonnendurchmessers maß, sondern um ein Vielfaches kleiner war.[31]
Nach der Erfindung des Fernrohrs entdeckte Giovanni Battista Zupi im Jahre 1639, dass der Merkur Phasen zeigt wie der Mond, und bewies damit seinen Umlauf um die Sonne. Als Sir Isaac Newton 1687 die Principia Mathematica veröffentlichte und damit die Gravitation beschrieb, konnten die Planetenbahnen nun exakt berechnet werden. Der Merkur jedoch wich immer von diesen Berechnungen ab, was Urbain Le Verrier (der Entdecker des Planeten Neptun) 1859 dazu veranlasste, einen weiteren noch schnelleren sonnennäheren Planeten zu postulieren: Vulcanus. Erst Albert Einsteins Relativitätstheorie konnte diese Abweichungen in Merkurs Umlaufbahn richtig erklären.[32]
Die ersten, nur sehr vagen Merkurkarten wurden von Johann Hieronymus Schroeter skizziert. Die ersten detaillierteren Karten wurden im späten 19. Jahrhundert, etwa 1881 von Giovanni Schiaparelli und danach von Percival Lowell angefertigt. Lowell meinte, ähnlich wie Schiaparelli bei seinen Marsbeobachtungen auf dem Merkur Kanäle erkennen zu können. Besser, wenn auch immer noch sehr ungenau, war die Merkurkarte von Eugène Michel Antoniadi aus dem Jahr 1934. Antoniadi ging dabei von der geläufigen, aber irrigen Annahme aus, dass der Merkur eine gebundene Rotation von 1:1 um die Sonne aufweist. Für seine Nomenklatur der Albedomerkmale bezog er sich auf die Hermes-Mythologie. Audouin Dollfus übernahm sie großteils für seine genauere Karte von 1972. Die Internationale Astronomische Union (IAU) billigte diese Nomenklatur für heutige Merkurkarten auf der Grundlage der Naherkundung. Für die topografischen Strukturen wurde ein anderes Schema gewählt. So bekamen die den Maria des Mondes ähnlichen Tiefebenen den Namen des Gottes Merkur in verschiedenen Sprachen.
Im Koordinatensystem des Merkurs werden die Längengrade von Ost nach West zwischen 0 und 360° gemessen. Der Nullmeridian wird durch den Punkt definiert, der am ersten Merkurperihel nach dem 1. Januar 1950 die Sonne im Zenit hatte. Die Breitengrade zwischen 0° und 90° werden nach Norden positiv und nach Süden negativ gezählt.
Gesteinsbrocken des Merkurs, die durch den Einschlag größerer Asteroiden ins All geschleudert wurden, können als Meteoriten im Laufe der Zeit auch die Erde erreichen. Als mögliche Merkurmeteoriten werden der Enstatit-Chondrit Abee und der Achondrit NWA 7325 diskutiert.
Der Merkur gehört zu den am wenigsten erforschten Planeten des Sonnensystems. Dies liegt vor allem an den für Raumsonden sehr unwirtlichen Bedingungen in der Nähe der Sonne, wie der hohen Temperatur und intensiven Strahlung, sowie an zahlreichen technischen Schwierigkeiten, die bei einem Flug zum Merkur in Kauf genommen werden müssen. Selbst von einem Erdorbit aus sind die Beobachtungsbedingungen zu ungünstig, um den Planeten mit Teleskopen zu beobachten. Der Spiegel des Hubble-Weltraumteleskops nähme durch die Strahlung der Sonne großen Schaden, wenn er auf einen dermaßen sonnennahen Bereich ausgerichtet würde.
Der mittlere Sonnenabstand des Merkurs beträgt ein Drittel desjenigen der Erde, sodass eine Raumsonde über 91 Millionen Kilometer in den Gravitationspotentialtopf der Sonne fliegen muss, um den Planeten zu erreichen. Von einem stationären Startpunkt bräuchte die Raumsonde keine Energie, um in Richtung Sonne zu fallen. Da der Start aber von der Erde erfolgt, die sich mit einer Orbitalgeschwindigkeit von 30 km/s um die Sonne bewegt, verhindert der hohe Bahndrehimpuls der Sonde eine Bewegung Richtung Sonne. Daher muss die Raumsonde eine beträchtliche Geschwindigkeitsänderung aufbringen, um in eine Hohmannbahn einzutreten, die in die Nähe des Merkurs führt.
Zusätzlich führt die Abnahme der potenziellen Energie der Raumsonde bei einem Flug in den Gravitationspotentialtopf der Sonne zur Erhöhung ihrer kinetischen Energie, also zu einer Erhöhung ihrer Fluggeschwindigkeit. Wenn man dies nicht korrigiert, ist die Sonde beim Erreichen des Merkurs bereits so schnell, dass ein sicherer Eintritt in den Merkurorbit oder gar eine Landung erheblich erschwert werden. Für einen Vorbeiflug ist die hohe Fluggeschwindigkeit allerdings von geringerer Bedeutung. Ein weiteres Hindernis ist das Fehlen einer Atmosphäre; dies macht es unmöglich, treibstoffsparende Aerobraking-Manöver zum Erreichen des gewünschten Orbits um den Planeten einzusetzen. Stattdessen muss der gesamte Bremsimpuls für einen Eintritt in den Merkurorbit mittels der bordeigenen Triebwerke durch eine Extramenge an mitgeführtem Treibstoff aufgebracht werden.
Diese Einschränkungen sind mit ein Grund dafür, dass der Merkur vor Messenger nur mit der einen Raumsonde Mariner 10 erforscht wurde. Eine dritte Merkursonde BepiColombo wurde am 20. Oktober 2018 gestartet.
Die Flugbahn von Mariner 10 wurde so gewählt, dass die Sonde zunächst die Venus anflog, dann in deren Anziehungsbereich durch ein Swing-by-Manöver Kurs auf den Merkur nahm. So gelangte sie auf eine merkurnahe Umlaufbahn um die Sonne, die mit einer Trägerrakete vom Typ Atlas-Centaur nur auf diese Weise erreicht werden konnte; ohne den Swing-by an der Venus hätte Mariner 10 eine deutlich größere und teurere Titan IIIC benötigt. Der schon lange an der Erforschung des innersten Planeten interessierte Mathematiker Giuseppe Colombo hatte diese Flugbahn entworfen, auf welcher der Merkur gleich mehrmals passiert werden konnte, und zwar immer in der Nähe seines sonnenfernsten Bahnpunktes – bei dem die Beeinträchtigung durch den Sonnenwind am geringsten ist – und am zugleich sonnennächsten Bahnpunkt von Mariner 10. Die anfänglich dabei nicht vorhergesehene Folge dieser himmelsmechanischen Drei-Körper-Wechselwirkung war, dass die Umlaufperiode von Mariner 10 genau zweimal so lang geriet wie die vom Merkur. Bei dieser Bahneigenschaft bekam die Raumsonde während jeder Begegnung ein und dieselbe Hemisphäre unter den gleichen Beleuchtungsverhältnissen vor die Kamera und erbrachte so den eindringlichen Beweis für die genaue 2:3-Kopplung von Merkurs Rotation an seine Umlaufbewegung, die nach den ersten, ungefähren Radarmessungen Colombo selbst schon vermutet hatte. Durch dieses seltsame Zusammentreffen konnten trotz der wiederholten Vorbeiflüge nur 45 Prozent der Merkuroberfläche kartiert werden.
Mariner 10 flog im betriebstüchtigen Zustand von 1974 bis 1975 dreimal am Merkur vorbei: Am 29. März 1974 in einer Entfernung von 705 km, am 21. September in rund 50.000 km und am 16. März 1975 in einer Entfernung von 327 km. Zusätzlich zu den herkömmlichen Aufnahmen wurde der Planet im infraroten sowie im UV-Licht untersucht, und über seiner den störenden Sonnenwind abschirmenden Nachtseite liefen während des ersten und dritten Vorbeifluges Messungen des durch die Sonde entdeckten Magnetfeldes und geladener Partikel.
Eine weitere Raumsonde der NASA, Messenger, startete am 3. August 2004 und schwenkte im März 2011 als erste Raumsonde in einen Merkurorbit ein, um den Planeten mit ihren zahlreichen Instrumenten eingehend zu studieren und erstmals vollständig zu kartografieren.[33] Die Raumsonde widmete sich dabei der Untersuchung der geologischen und tektonischen Geschichte Merkurs sowie seiner Zusammensetzung. Weiterhin suchte die Sonde nach dem Ursprung des Magnetfeldes, bestimmte die Größe und den Zustand des Planetenkerns, untersuchte die Polarkappen des Planeten und erforschte die Exosphäre sowie die Magnetosphäre. Um sein Ziel zu erreichen, flog Messenger eine sehr komplexe Route, die ihn in mehreren Fly-by-Manövern erst zurück zur Erde, dann zweimal an der Venus sowie dreimal am Merkur vorbeiführte. Der erste Vorbeiflug am Merkur fand am 14. Januar 2008 um 20:04 Uhr MEZ statt und der zweite am 6. Oktober 2008. Dabei wurden bereits Untersuchungen der Oberfläche durchgeführt und Fotos von bisher unbekannten Gebieten aufgenommen. Der dritte Vorbeiflug, durch den die Geschwindigkeit der Sonde verringert wurde, erfolgte am 30. September 2009. Da die Sonde kurz vor der Passage unerwartet in den abgesicherten Modus umschaltete, konnten für geraume Zeit keine Beobachtungsdaten gesammelt und übertragen werden.[34] Die gesamte Reise nahm etwa 6,5 Jahre in Anspruch. Die darauf folgende Mission im Merkurorbit ist in Jahresabschnitte geteilt, welche jeweils am 18. März beginnen. Vom 18. März 2011 bis 18. März 2012 wurden während der sogenannten primären Mission die wichtigsten Forschungen vorgenommen; anschließend begann die erste erweiterte Mission, welche bis zum 18. März 2013 lief. Danach wurde die Mission noch einmal bis März 2015 verlängert. Gegen Ende der Mission wurde die Sonde in Umlaufbahnen um den Planeten gebracht, deren niedrigster Punkt nur 5,3 km über der Oberfläche lag. Der verbleibende Treibstoff für die Triebwerke der Sonde wurde genutzt, um dem bremsenden Effekt der schwachen, aber doch vorhandenen Atmosphäre entgegenzuwirken. Die letzte dieser Kurskorrekturen erfolgte am 25. März 2015. Am 30. April 2015 stürzte die Sonde dann auf die erdabgewandte Seite des Merkurs.[35]
Die europäische Raumfahrtorganisation ESA und die japanische Raumfahrtbehörde JAXA erforschen den sonnennächsten Planeten mit der kombinierten Merkursonde BepiColombo. Das gemeinsame Unternehmen ist nach dem Spitznamen des 1984 verstorbenen Giuseppe Colombo benannt und besteht aus zwei am Ziel getrennt eingesetzten Orbitern: einem Fernerkundungsorbiter für eine 400 km × 1500 km messende polare Umlaufbahn und einem Magnetosphärenorbiter für einen polaren Merkurumlauf von 400 km × 12.000 km. Die Komponenten werden sich jeweils der Untersuchung des Magnetfeldes sowie der geologischen Zusammensetzung in Hinsicht der Geschichte des Merkurs widmen. Die Sonde startete am 20. Oktober 2018, ihre Reise zum Merkur wird mit Ionentriebwerken und Vorbeiflügen an den inneren Planeten unterstützt und soll 2025 in eine Umlaufbahn eintreten. Am Ziel wird die Sonde Temperaturen von bis zu 250 °C ausgesetzt sein und soll mindestens ein Jahr lang (d. h. über vier Merkurjahre) Daten liefern.
Der Merkur kann sich als innerster Planet des Sonnensystems nur bis zu einem Winkel von maximal 28 Grad (größte Elongation) von der Sonne entfernen und ist daher schwierig zu beobachten. Dem in Frauenburg tätigen Nikolaus Kopernikus war es beispielsweise nie gelungen, den Merkur zu beobachten.[36] Der Merkur kann in der Abend- oder Morgendämmerung als orangefarbener Lichtpunkt mit einer scheinbaren Helligkeit von etwa 1 mag bis maximal −1,9 mag in der Nähe des Horizonts mit bloßem Auge wahrgenommen werden. Bei Tagbeobachtungen ist er – je nach Sichtverhältnissen – ab einer Fernrohröffnung von etwa 10 bis 20 cm gut zu erkennen.
Durch die Horizontnähe wird seine Beobachtung mit Teleskopen sehr erschwert, da sein Licht eine größere Strecke durch die Erdatmosphäre zurücklegen muss und durch Turbulenzen, Lichtbrechung und Absorption gestört wird. Der Planet erscheint meist als verwaschenes, halbmondförmiges Scheibchen im Teleskop. Auch mit leistungsfähigen Teleskopen sind kaum markante Merkmale auf seiner Oberfläche auszumachen.
Da die Merkurbahn stark elliptisch ist, schwanken die Werte seiner größten Elongation zwischen den einzelnen Umläufen von 18 bis 28 Grad.
Bei der Beobachtung des Merkurs sind – bei gleicher geographischer nördlicher oder südlicher Breite – die Beobachter der Nordhalbkugel im Nachteil, denn die Merkur-Elongationen mit den größten Werten finden zu Zeiten statt, bei denen für einen Beobachter auf der Nordhalbkugel die Ekliptik flach über dem Horizont verläuft und der Merkur in der hellen Dämmerung auf- oder untergeht. In den Breiten Mitteleuropas ist er dann mit bloßem Auge nicht zu sehen. Die beste Sichtbarkeit verspricht eine maximale westliche Elongation (Morgensichtbarkeit) im Herbst, sowie eine maximale östliche Elongation (Abendsichtbarkeit) im Frühling.
In großer Höhe über dem Horizont kann der Merkur mit bloßem Auge nur während einer totalen Sonnenfinsternis gesehen werden.
Wegen der großen Bahnneigung zieht der Planet nur alle paar Jahre vor der Sonnenscheibe vorbei (siehe nächster Abschnitt).
Hingegen kann er gerade deshalb manchmal doppelsichtig werden, indem er mit freiem Auge sowohl in der hellen Morgen- wie in der hellen Abenddämmerung beobachtbar sein kann. Dies ist in den Tagen um die Untere Konjunktion möglich, wenn er nicht knapp an der Sonne vorbeizieht, sondern bis zu 8° nördlich von ihr.
Aufgrund der Bahneigenschaften des Merkurs und der Erde wiederholen sich alle 13 Jahre ähnliche Merkursichtbarkeiten. In diesem Zeitraum finden im Allgemeinen auch zwei sogenannte Transits oder Durchgänge statt, bei denen der Merkur von der Erde aus gesehen direkt vor der Sonnenscheibe als schwarzes Scheibchen zu sehen ist. Ein solcher Transit des Merkurs ist sichtbar, wenn er bei der unteren Konjunktion – während er die Erde beim Umlauf um die Sonne auf seiner Innenbahn überholt – in der Nähe eines seiner beiden Bahnknoten steht, also die Erdbahnebene kreuzt. Ein solches Ereignis ist aufgrund der entsprechenden Geometrie nur zwischen dem 6. und dem 11. Mai oder zwischen dem 6. und dem 15. November möglich, da die beiden Bahnknoten am 9. Mai oder am 11. November von der Erde aus gesehen vor der Sonne stehen. Der letzte Merkurdurchgang fand am 11. November 2019 statt, der nächste folgt am 13. November 2032.
In der folgenden Tabelle sind die speziellen Konstellationen des Merkurs für das Jahr 2021 angegeben. Östliche Elongation bietet Abendsichtbarkeit, westliche Elongation Morgensichtbarkeit:
In der altägyptischen Mythologie und Astronomie galt der Merkur hauptsächlich als Stern des Seth. Sein Name Sebeg (auch Sebgu) stand für eine weitere Erscheinungsform der altägyptischen Götter Seth und Thot. Im antiken Griechenland bezog man den Planeten auf den Gott und Götterboten Hermes, assoziierte ihn aber auch mit den Titanen[37] Metis und Koios. Der zumeist nur in der Dämmerung und dann auch nur schwer zu entdeckende, besonders rastlose Planet wurde auch als Symbol für Hermes als Schutzpatron der Händler, Wegelagerer und Diebe gesehen.
Bei den Römern entsprach Hermes spätestens in der nachantiken Zeit dem Mercurius, abgeleitet von mercari (lat. für Handel treiben). Der von ihnen nach dem Merkur benannte Wochentag dies Mercurii ist im Deutschen der Mittwoch. In der Zuordnung der Wochentage besteht die namentliche Verbindung des Merkurs mit dem Mittwoch noch im Französischen (mercredi), im Italienischen (mercoledì), im Spanischen (miércoles), im Rumänischen (miercuri) und im Albanischen (e mërkurë). Den Germanen wird als Entsprechung des Gestirns der Gott Odin bzw. Wotan zugeschrieben, dem ebenso der Mittwoch (im Englischen wednesday, im Niederländischen woensdag) zugeordnet wurde.
Im Altertum und in der Welt der mittelalterlichen Alchemisten hat man dem eiligen Wandelstern als Planetenmetall das bewegliche Quecksilber zugeordnet. In vielen Sprachen basiert der Name des Metalls heute noch auf diesem Wortstamm (englisch mercury, französisch mercure).
In der Musik hat Gustav Holst dem Merkur in seiner Orchestersuite The Planets (Die Planeten, 1914–1916) den dritten Satz gewidmet: Mercury, the Winged Messenger (Merkur, der geflügelte Bote).[38]
In der Unterhaltungsliteratur schrieb Isaac Asimov im Jahr 1956 für seine Lucky-Starr-Reihe den Science-Fiction-Roman Lucky Starr and the Big Sun of Mercury. Darin startet auf dem Planeten der lebensfeindlichen Temperaturextreme ein Projekt neuer Energiegewinnungs- und -transportmethoden für den wachsenden Energiebedarf der Erde, das jedoch von Sabotage betroffen ist. Die deutsche Ausgabe erschien erstmals 1974 unter dem Titel Im Licht der Merkur-Sonne.[39]
In dem Film Sunshine, von Regisseur Danny Boyle im Jahr 2007 in die Kinos gebracht, dient eine Umlaufbahn um den Merkur als Zwischenstation für ein Raumschiff, dessen Fracht die Sonne vor dem Erlöschen bewahren soll.
Der im Jahr 2012 erschienene Roman 2312 von Kim Stanley Robinson handelt in eben jenem Jahr 2312, unter anderem in Merkurs Hauptstadt Terminator, die sich ständig auf Schienen entlang des Äquators bewegt und plötzlich mit gezielten Meteoroiden angegriffen wird.[40]
Medien

Die Art, auch Spezies oder Species genannt,[1] ist in der Biologie (einschließlich Virologie und Palichnologie) die Grundeinheit der Systematik. Jede biologische Art ist ein Resultat der Artbildung. Bislang gelang keine allgemeine Definition der Art, welche die theoretischen und praktischen Anforderungen aller biologischen Teildisziplinen gleichermaßen erfüllt. Vielmehr existieren in der Biologie verschiedene Artkonzepte, die zu unterschiedlichen Klassifikationen führen. Historisch wie auch aktuell spielen zwei Ansätze von Artkonzepten eine wichtige Rolle:
Mit dem Aufkommen der Kladistik ist seit den 1950er Jahren der auf dem biologischen Artbegriff beruhende, chronologisch definierte phylogenetische Artbegriff hinzugekommen, nach dem eine Art mit der Artspaltung, also der Bildung zweier Arten aus einer Ursprungsart, beginnt und mit ihrer erneuten Artspaltung oder aber ihrem Aussterben endet.
Das Problem der Artdefinition besteht aus zwei Teilproblemen:
Die Hauptunterschiede der verschiedenen Artkonzepte liegen dabei auf der Ebene der Rangbildung. Eine Gruppe von Lebewesen unabhängig von ihrem Rang bezeichnen Taxonomen als Taxon (in der Botanik auch Sippe).
In biologischen Fachtexten wird bei Bezug auf eine unbestimmte Art oft abgekürzt „spec.“ oder „sp.“ (von lateinisch species), bei mehreren Arten auch „spp.“ (species pluralis).
Eine Art als Taxon ist eine gemäß den Regeln der Taxonomie und der biologischen Nomenklatur formal beschriebene und benannte Form von Lebewesen. Eine taxonomische Art stellt eine wissenschaftliche Hypothese dar und kann unabhängig von einem Artkonzept sein, sofern man zumindest akzeptiert, dass Arten reale und individuelle Erscheinungen der Natur sind.[3] Die Art ist eine Rangstufe der klassischen, auf Carl von Linné zurückgehenden Taxonomie. Einige rein merkmalsbezogen arbeitende Systematiker sind der Ansicht, Arten wären mehr oder weniger willkürlich zusammengestellte, künstliche Gruppen, nur die Individuen seien letztlich real: Manche gehen dabei so weit, dass der Artbegriff wie alle anderen Rangstufen ihrer Ansicht nach besser abgeschafft werden sollten und durch neue Konzepte wie die Least-inclusive taxonomic unit ersetzt werden sollten.[4] Die meisten Biologen sind aber der Ansicht, dass Arten natürliche Einheiten mit realer Existenz darstellen; es gäbe dann Artkriterien, an denen sich reale Arten identifizieren ließen. Dieser Vorstellung liegt letztlich eine Unterscheidung zwischen durch Genfluss oder horizontalen Gentransfer geprägten Einheiten unterhalb des Artniveaus und den Arten, bei denen dies nicht zutrifft (engl. lineages), zu Grunde. Für viele Biologen, darunter Anhänger eines phylogenetischen Artkonzepts (vgl. unten), sind sie sogar die einzigen in diesem Sinne natürlichen taxonomischen Einheiten.[5][6]
Der wissenschaftliche Name einer Art (oft lateinischen oder griechischen Ursprungs) setzt sich nach der von Carl von Linné 1753 eingeführten binären Nomenklatur aus zwei Teilen zusammen, die beide kursiv geschrieben werden (diese Nomenklatur wurde 2021 auch in der Virologie für neue Virusarten eingeführt, die bestehenden anderweitigen Artnamen werden nach und nach umbenannt). Der erste Teil dieses Namens ist der groß geschriebene Gattungsname. Der zweite Teil wird immer klein geschrieben und in der Botanik sowie bei Prokaryoten als Epitheton („specific epithet“[7][8]) bezeichnet, in der Zoologie als Artname oder Artzusatz („specific name“[9]). Um Verwechslungen zwischen dem Artzusatz und dem gesamten Artnamen, also dem Binomen aus Gattungsname und Artzusatz, zu vermeiden, werden in der Zoologie entweder die eindeutigen englischen Begriffe verwendet oder hinzugefügt oder gelegentlich und informell auch Begriffe wie „epithetum specificum“ oder „epitheton specificum“ verwendet.[10]
Beispiele
Sowohl in der Botanik (Code Article 46) als auch in der Zoologie (Code Article 51) wird empfohlen, dem wissenschaftlichen Artnamen die Namen der Autoren beizufügen, welche die Art beschrieben haben, zumindest, wenn es um taxonomische oder nomenklatorische Fragen geht. Dies ist zum Beispiel wichtig, um Homonyme zu erkennen, das sind Fälle, in denen zwei Autoren versehentlich zwei verschiedene Arten mit demselben Namen benannt haben. Im Geltungsbereich des Internationalen Codes der Nomenklatur für Algen, Pilze und Pflanzen wird es empfohlen, die Autorennamen abzukürzen, wobei in der Regel das Namensverzeichnis von Brummit und Powell als Grundlage dient (vergleiche Artikel Autorenkürzel der Botaniker und Mykologen), „L.“ steht beispielsweise für Linné.
Nach den Internationalen Regeln für die Zoologische Nomenklatur sollen zumindest einmal in jedem wissenschaftlichen Text dem Artnamen die Autor(en) und das Jahr der Publikation hinzugefügt werden (Code Recommendation 51a). Wenn im entsprechenden Fachgebiet zwei Autoren mit demselben Nachnamen tätig waren, soll der abgekürzte Vorname hinzugefügt werden, um Eindeutigkeit herzustellen. Wenn die Art heute in eine andere Gattung gestellt wird als in die, in der sie ursprünglich beschrieben wurde, müssen Autor(en) und Jahr in Klammern gesetzt werden (Code Article 51.3). Zwischen Autor und Jahr wird in der Regel ein Komma gesetzt.
Die Philosophen der Antike kannten noch keine systematischen Konzepte und somit keinen Artbegriff im heutigen Sinne. Von Aristoteles sind als erstem Philosophen Schriften bekannt, in denen zwei getrennte – allgemein philosophisch zu verstehende – Begriffe είδος (eidos, ins Deutsche mit „Art“ übersetzt) und γένος (genos, deutsch „Gattung“) voneinander abgrenzt werden. In seinen Kategorien charakterisiert er anhand eines Beispiels aus der Welt der Lebewesen diese als zweite Wesenheiten (δεύτεραι ουσίαι), die in dem Einzelnen vorhanden sind. So ist ein einzelner Mensch in der Art Mensch vorhanden und ein einzelnes Pferd in der Art Pferd, beide gehören jedoch zur Gattung des Lebenden (ζῷον zoon).[11]
In seiner Historia animalium (Περί τα ζώα ιστοριών) wendet Aristoteles die Begriffe είδος und γένος auch auf das Tierreich an, ohne dabei jedoch eine taxonomische Ordnung aufzustellen. Vielmehr spricht er von der Überlappung von Eigenschaften der Tierarten (ἐπάλλαξις epállaxis) und der Notwendigkeit, eine einzelne Art anhand mehrerer nebengeordneter Merkmale zu definieren. Dennoch beschäftigt er sich bei der Beschreibung der Arten mit einzelnen charakteristischen Merkmalen.[12] Der Begriff είδος wird auch nicht im Sinne eines heutigen Artbegriffes konsequent als unterste Kategorie zwischen dem einzelnen Lebewesen und γένος verwendet, vielmehr kann die Bedeutung meist am besten mit „Form“, „Gestalt“ oder „Wesen“ wiedergegeben werden, während Tierarten in der Regel mit γένος bezeichnet werden.[13][14]
Laut biblischer Schöpfungs­geschichte im 1. Buch Mose schuf Gott zwischen dem 3. und 6. Schöpfungstag die Pflanzen und Tiere, „ein jegliches (jedes) nach seiner Art“ (zehnmal Zitat „nach seiner Art“, Genesis 1,11–27 LUT, zu verstehen als „Wesensart“, hebräisch min מין bzw. למינה, Genesis 1,11–27 OT). In der Septuaginta wird מין mit γένος (κατὰ γένος „nach/gemäß der Art“, Genesis 1,11–27 LXX) übersetzt, in der Vulgata dagegen uneinheitlich, manchmal mit genus und manchmal mit species, wobei auch die Präpositionen wechseln (secundum speciem suam, secundum species suas, in species suas, juxta genus suum, secundum genus suum, in genere suo, Genesis 1,11–27 VUL). Es wird hier auch eine Aussage zur Fortpflanzung der Pflanzen und Tiere „nach ihrer Art“ getroffen, indem Gott in Genesis 1,11 LUT spricht: „Es lasse die Erde aufgehen Gras und Kraut, das Samen bringe, und fruchtbare Bäume auf Erden, die ein jeder nach seiner Art Früchte tragen, in denen ihr Same ist“, sowie in Genesis 1,22 LUT zu den Tieren des Wassers und der Luft: „Seid fruchtbar und mehret euch.“
Diese biblischen Aussagen wie auch Aristoteles waren bis in die Neuzeit prägend für die Vorstellungen der Gelehrten des Abendlandes. Pierre Duhem führte 1916 für die philosophische Auffassung vom Wesen oder der „Essenz“ eines Individuums den Begriff des Essentialismus ein.[15]
Nach Auffassung von Ernst Mayr stimmte die auf dem „Schöpfungsglauben beruhende Interpretation des Artbegriffes der christlichen Fundamentalisten“ recht gut mit der letztendlich auf Platon zurückgehenden Vorstellung einer „unveränderlichen Essenz“ (είδος als Wesen) überein und bildeten die Grundlage für einen „essentialistischen Artbegriff“, wie er vom Mittelalter bis ins 19. Jahrhundert hinein dominierte. Hiernach gehörten alle Objekte, welche dieselbe Essenz gemeinsam haben, derselben Art an.[16] Laut Mayr war „[d]er Essentialismus mit seiner Betonung von Diskontinuität, Konstanz und typischen Werten (Typologie)“ der Hintergrund für typologische Artkonzepte, nach denen ein Individuum auf Grund seiner – in der Regel morphologischen – Merkmale (Typus) immer eindeutig einer bestimmten Art angehört und der Hintergrund dafür, dass Darwins „These von der Evolution durch natürliche Selektion daher als unannehmbar befunden“ wurde.[17]
Erkennbar ist dies auch bei John Ray, der 1686 in seiner Historia plantarum generalis die Arten der Pflanzen als Fortpflanzungsgemeinschaften mit beständigen Artkennzeichen definiert, nachdem er „lange Zeit“ nach Anzeichen für ihre Unterscheidung geforscht habe: „Uns erschien aber keines [kein Anzeichen] zuverlässiger als die gesonderte Fortpflanzung aus dem Samen. Welche Unterschiede auch immer also im Individuum oder der Pflanzenart aus dem Samen derselben hervorgehen, sie sind zufällig und nicht für die Art kennzeichnend. […] Denn die sich nach ihrer Art unterscheiden, bewahren ihre Art beständig, und keine entspringt dem Samen der anderen oder umgekehrt.“[18]
Carl von Linné stellte mit Species Plantarum (1753) und Systema Naturae (1758) als erster ein enkaptisches, auf hierarchisch aufbauenden Kategorien (Klasse, Ordnung, Gattung, Art und Varietät, jedoch noch nicht Familie) beruhendes System der Natur auf, wobei er für die Art die binäre Nomenklatur aus Gattungsnamen und Artepitheton einführte. Hierarchisch bedeutet dabei, dass die Einheiten auf unterschiedlichen Ebenen zu Gruppen zusammengefasst werden, wobei die in der Hierarchie höherstehenden Gruppen durch allgemeine, die tieferstehenden Gruppen durch immer speziellere Merkmale zusammengefasst werden (ein bestimmtes Individuum gehört also seiner Merkmalskombination gemäß in eine Art, eine Gattung, eine Familie usw.). Enkaptisch bedeutet, dass die in der Hierarchie tieferstehenden Gruppen in jeweils genau eine Gruppe der höheren Hierarchiestufe eingeschachtelt werden, also zum Beispiel jede Art in eine und genau eine, Gattung. In seiner Philosophia botanica formuliert er: „Es gibt so viele Arten, wie viele verschiedene Formen das unendliche Seiende am Anfang schuf; diese Formen, nach den hineingegebenen Gesetzen der Fortpflanzung, brachten viele [weitere Formen] hervor, doch immer ähnliche.“[19] Darüber hinaus bezeichnet er die Art und die Gattung als Werk der Natur, die Varietät als Werk des Menschen, Ordnung und Klasse dagegen als vom Menschen geschaffene Einheit. „Die Arten sind unveränderlich, denn ihre Fortpflanzung ist wahres Fortdauern.“[20]
Während Georges-Louis Leclerc de Buffon 1749 noch verneint, dass es in der Natur irgendwelche Kategorien gäbe,[21] revidiert er später diese Sicht für die Art und formuliert einen typologischen Artbegriff mit einer Konstanz der Arten: „Der Abdruck jeder Art ist ein Typ, dessen wesentliche Merkmale in unveränderlichen und beständigen Wesenszügen eingeprägt sind, doch alle Nebenmerkmale variieren: Kein Individuum gleicht vollkommen dem anderen.“[22]
Jean-Baptiste de Lamarck, der bereits von einer Transformation der Arten ausgeht, betrachtet dagegen die Art und alle anderen Kategorien als künstlich. 1809 äußert er sich in seiner Philosophie zoologique: „Die Natur hat nicht wirklich Klassen, Ordnungen, Familien, Gattungen, beständige Arten herausgebildet, sondern allein Individuen.“[23] Dies hindert ihn jedoch nicht daran, auf dem Gebiet der Taxonomie sehr produktiv zu sein, deren Kategorien er praktisch zu nutzen weiß.[24]
Charles Darwin, der von der Art sogar im Titel seines Grundlagenwerkes On the Origin of Species (Über die Entstehung der Arten) von 1859 spricht, scheut sich vor einer Formulierung eines Artbegriffs.[25] Laut Ernst Mayr kann man aus seinen Notizbüchern aus den 1830er Jahren schließen, dass er damals die Vorstellung von einer Art als Fortpflanzungsgemeinschaft hatte.[26] In seiner Entstehung der Arten bezeichnet er jedoch die Begriffe der Art und der Varietät unmissverständlich als künstlich: „Aus diesen Bemerkungen geht hervor, dass ich den Kunstausdruck „Species“ als einen arbiträren und der Bequemlichkeit halber auf eine Reihe von einander sehr ähnlichen Individuen angewendeten betrachte und dass er von dem Kunstausdrucke „Varietät“, welcher auf minder abweichende und noch mehr schwankende Formen Anwendung findet, nicht wesentlich verschieden ist. Ebenso wird der Ausdruck „Varietät“ im Vergleich zu bloßen individuellen Verschiedenheiten nur arbiträr und der Bequemlichkeit wegen benutzt.“[27]
Ähnlich äußert sich auch Alfred Russel Wallace 1856 in seiner Grundlagenarbeit über die Ritterfalter (Papilionidae) im Malaiischen Archipel, in der er verschiedene Verläufe der Evolution durch natürliche Zuchtwahl erklärt. Er bezeichnet Arten als „lediglich stark gekennzeichnete Rassen oder Lokalformen“ und geht dabei auch darauf ein, dass Individuen unterschiedlicher Arten generell als unfähig angesehen werden, fruchtbare gemeinsame Nachkommen zu zeugen, doch sei es nicht einmal in einem von tausend Fällen möglich, das Vorliegen einer Vermischung zu überprüfen.[28]
Seit Darwin ist die Ebene der Art gegenüber unterscheidbaren untergeordneten (Lokalpopulationen) oder übergeordneten (Artengruppen bzw. höheren Taxa) nicht mehr besonders ausgezeichnet. Innerhalb der Taxonomie unterlag die Artabgrenzung Moden und persönlichen Vorlieben, es gibt Taxonomen, die möglichst jede unterscheidbare Form in den Artrang erheben wollen („splitter“), und andere, die weitgefasste Arten mit zahlreichen Lokalrassen und -populationen bevorzugen („lumper“).
Ende des 19. Jahrhunderts wurden biologische Artkonzepte einer Fortpflanzungsgemeinschaft diskutiert.[29] Erwin Stresemann äußert in diesem Sinne bereits 1919 in einem Artikel über die europäischen Baumläufer klare Vorstellungen über Artbildung und genetische Isolation: „Es will nur die Tatsache im Namen zum Ausdruck bringen, dass sich die [im Laufe der geographischen Separation] zum Rang von Spezies erhobenen Formen physiologisch so weit voneinander entfernt haben, dass sie, wie die Natur beweist, wieder zusammenkommen können, ohne eine Vermischung einzugehen.“[30]
Beherrschend im wissenschaftlichen Diskurs wurden die biologischen Artkonzepte der Fortpflanzungsgemeinschaft mit Theodosius Dobzhansky und Ernst Mayr seit der 2. Hälfte des 20. Jahrhunderts. Dobzhansky verknüpft den Artbegriff – ähnlich wie Stresemann – mit der Artbildung und definiert 1939 Arten als das „Stadium des Evolutionsvorgangs […], in dem Formengruppen, die sich bisher untereinander fortpflanzen oder jedenfalls dazu fähig waren, in zwei oder mehr gesonderte Gruppen aufgeteilt werden, die sich aus physiologischen Ursachen nicht untereinander fortpflanzen können“,[31] während Mayr 1942 formuliert: „Arten sind Gruppen von natürlichen Populationen, die sich tatsächlich oder potentiell untereinander vermehren und fortpflanzungsmäßig von anderen derartigen Gruppen getrennt sind.“[32][33] In einem erweiterten biologischen Artbegriff bezieht Mayr 2002 die ökologische Nische mit in die Begriffsdefinition ein: „Eine Art ist eine Fortpflanzungsgemeinschaft von (fortpflanzungsmäßig von anderen isolierten) Populationen, die eine spezifische Nische in der Natur einnimmt.“[34] Mayr stellt die Bedeutung der Art in der Biologie als natürliche „Einheit der Evolution, der Systematik, der Ökologie und der Ethologie“ heraus und hebt sie hierin von allen anderen systematischen Kategorien ab.[35]
Aus praktischen Erwägungen überdauern bis heute auch typologische Artkonzepte. Nach wie vor benennt die als Autorität bezeichnete Person, welche die Artbeschreibung einer neuen Art (species nova) als erste veröffentlicht, diese anhand der arttypischen Merkmale des Typusexemplars mit einem selbst gewählten Artnamen aus dem Gattungsnamen und dem Artepitheton.[36]
Demgegenüber hebt der britische Paläoanthropologe Chris Stringer hervor: Alle Art-Konzepte sind „von Menschen erdachte Annäherungen an die Realität der Natur.“[37]
In der Debatte um Essentialismus in der Geschichte der Biologie hebt Mary Winsor hervor, dass etwa die Verwendung von Typusarten als Prototypen für höhere Kategorien unvereinbar mit dem Essentialismus sei,[38] und John S. Wilkins betont, dass die – von Winsor als „Methode der Exemplare“ bezeichnete – Typologie der Biologen und der Essentialismus keineswegs zwangsläufig verknüpft sind.[39] Während Essenzen definierbar und allen Angehörigen einer Art eigen seien, ließen sich Typen instantiieren und seien variabel.[40]
Laut Ernst Mayr beginnt die Geschichte des Artbegriffs in der Biologie mit Carl von Linné.[41] Er hebt in seinen Arbeiten zur Wissenschaftsgeschichte hervor, dass der Essentialismus das abendländische Denken in großem Ausmaß beherrscht habe. Er setzt typologische mit essentialistischen Artbegriffen gleich, die mit Darwin's These von der Evolution durch natürliche Selektion „absolut unvereinbar“ sind.[17]
„Darwin, einer der ersten Denker, der den Essentialismus (wenigstens zum Teil) ablehnte, wurde von den zeitgenössischen Philosophen (die alle Essentialisten waren) überhaupt nicht verstanden, und seine These von der Evolution durch natürliche Selektion daher als unannehmbar befunden. In essentialistischer Sicht ist eine echte Veränderung nur durch saltationistische, sprunghafte Entstehung neuer Wesenheiten möglich. Da die Evolution, wie Darwin sie erklärt, zwangsläufig allmählich, in fast unmerklichen Schritten erfolgt, ist sie mit dem Essentialismus absolut unvereinbar.“[42]
Typologisch definierte Arten sind Gruppen von Organismen, die in der Regel nach morphologischen Merkmalen (morphologisches Artkonzept) unterschieden werden. Es können aber auch andere Merkmale wie zum Beispiel Verhaltensweisen in analoger Weise verwendet werden.[43] Eine nach morphologischen Kriterien definierte Art wird Morphospezies genannt.
Beispiele:
In der Paläontologie kann in der Regel nur das morphologische Artkonzept angewandt werden. Da die Anzahl der Funde oft begrenzt ist, ist die Artabgrenzung in der Paläontologie besonders subjektiv. Beispiel: Die Funde von Fossilien zweier Individuen in der gleichen Fundschicht, also praktisch gleichzeitig lebend, unterscheiden sich stark voneinander:
Diese Probleme werden mit zunehmender Zahl der Funde und damit Kenntnis der tatsächlichen Variationsbreite geringer, lassen sich aber nicht vollständig beseitigen.
Das morphologische Artkonzept findet häufig Verwendung in der Ökologie, Botanik und Zoologie. In anderen Bereichen, wie etwa in der Mikrobiologie oder in Teilbereichen der Zoologie, wie bei den Nematoden, versagen rein morphologische Arteinteilungsversuche weitgehend.
Bakterien zeigen nur wenige morphologische Unterscheidungsmerkmale und weisen praktisch keine Rekombinationsschranken auf. In Ermangelung eindeutiger Abgrenzungsdefinitionen erstellte das International Committee on Systematics of Prokaryotes (ICSP) 2001 das weitest verbreitete Artenkonzept für Bakterien („Phylo-phenetic species concept“): „A monophyletic and genomically coherent cluster of individual organisms that show a high degree of overall similarity in many independent characteristics, and is diagnosable by a discriminative phenotypic property.“ (Ein monophyletisch und genomisch kohärentes Cluster einzelner Organismen, die in vielen unabhängigen Merkmalen einen hohen Grad an Gesamtähnlichkeit aufweisen und durch eine diskriminative phänotypische Eigenschaft diagnostizierbar sind.)[44]
In der Praxis wird überwiegend der Stoffwechsel als Unterscheidungskriterium von Stämmen herangezogen. Weil ein allgemein akzeptiertes Artkriterium fehlt, stellen Bakterienstämme so die derzeit tatsächlich verwendete Basis zur Unterscheidung dar. Anhand biochemischer Merkmale wie etwa der Substanz der Zellwand unterscheidet man die höheren Bakterientaxa. Man testet an bakteriellen Reinkulturen zu ihrer „Artbestimmung“ deren Fähigkeit zu bestimmten biochemischen Leistungen, etwa der Fähigkeit zum Abbau bestimmter „Substrate“, z. B. seltener Zuckerarten. Diese Fähigkeit ist leicht erkennbar, wenn das Umsetzungsprodukt einen im Kulturmedium zugesetzten Farbindikator umfärben kann. Durch Verimpfung einer Bakterienreinkultur in eine Reihe von Kulturgläsern mit Nährlösungen, die jeweils nur ein bestimmtes Substrat enthalten („Selektivmedien“), bekommt man eine sog. „Bunte Reihe“, aus deren Farbumschlägen nach einer Tabelle die Bakterienart bestimmt werden kann. Dazu wurden halbautomatische Geräte („Mikroplatten-Reader“) entwickelt.
Seit entsprechende Techniken zur Verfügung stehen (PCR), werden Bakterienstämme auch anhand der DNA-Sequenzen identifiziert oder unterschieden. Ein weithin akzeptiertes Maß ist, dass Stämme, die weniger als 70 % ihres Genoms gemeinsam haben, als getrennte Arten aufzufassen sind.[45] Ein weiteres Maß beruht auf der Ähnlichkeit der 16S-rRNA-Gene. Nach DNA-Analysen waren dabei zum Beispiel weniger als 1 % der in natürlichen Medien gefundenen Stämme auf den konventionellen Nährmedien vermehrbar. Auf diese Weise sollen in einem ml Boden bis zu 100.000 verschiedene Bakteriengenome festgestellt worden sein, die als verschiedene Arten interpretiert wurden. Dies ist nicht zu verwechseln mit der Gesamtkeimzahl, die in der gleichen Größenordnung liegt, aber dabei nur „wenige“ Arten umfasst, die sich bei einer bestimmten Kulturmethode durch die Bildung von Kolonien zeigen.
Viele Unterscheidungskriterien sind rein pragmatisch. Auf welcher Ebene der Unterscheidung man hier Stämme als Arten oder gar Gattungen auffasst, ist eine Sache der Konvention. Die physiologische oder genetische Artabgrenzung bei Bakterien entspricht methodisch dem typologischen Artkonzept. Ernst Mayr, leidenschaftlicher Anhänger des biologischen Artkonzepts, meint daher: „Bakterien haben keine Arten“.
Daniel Dykhuizen macht darauf aufmerksam, dass – entgegen mancher Anschauung – Transformationen, Transduktionen und Konjugationen (als Wege des DNA-Tauschs zwischen Stämmen) nicht wahllos, sondern zwischen bestimmten Formen bevorzugt, zwischen anderen quasi nie ablaufen. Demnach wäre es prinzipiell möglich, ein Artkonzept für Bakterien entsprechend dem biologischen Artkonzept bei den Eukaryonten zu entwickeln.[46] Frederick M. Cohan versucht dagegen auf Basis von Ökotypen, ein Artkonzept zu entwickeln.[47]
Gegen Ende des 19., Anfang des 20. Jahrhunderts begann sich in der Biologie allmählich das Populationsdenken durchzusetzen, was Konsequenzen für den Artbegriff mit sich brachte. Weil typologische Klassifizierungsschemata die realen Verhältnisse in der Natur nicht oder nur unzureichend abzubilden vermochten, musste die biologische Systematik einen neuen Artbegriff entwickeln, der nicht auf abstrakter Unterschiedlichkeit oder subjektiver Einschätzung einzelner Wissenschaftler basiert, sondern auf objektiv feststellbaren Kriterien. Diese Definition wird als biologische Artdefinition bezeichnet, „Sie heißt „biologisch“ nicht deshalb, weil sie mit biologischen Taxa zu tun hat, sondern weil ihre Definition eine biologische ist. Sie verwendet Kriterien, die, was die unbelebte Welt betrifft, bedeutungslos sind.“[48] Eine biologisch definierte Art wird als Biospezies bezeichnet.
Der neue Begriff stützte sich auf zwei Beobachtungen: Zum einen setzen sich Arten aus Populationen zusammen und zum anderen existieren zwischen Populationen unterschiedlicher Arten biologische Fortpflanzungsbarrieren. „Die [biologische] Art besitzt zwei Eigenschaften, durch die sie sich grundlegend von allen anderen taxonomischen Kategorien, etwa dem Genus, unterscheidet. Erstens einmal erlaubt sie eine nichtwillkürliche Definition – man könnte sogar so weit gehen, sie als „selbstoperational“ zu bezeichnen –, indem sie das Kriterium der Fortpflanzungsisolation gegenüber anderen Populationen hervorhebt. Zweitens ist die Art nicht wie alle anderen Kategorien auf der Basis von ihr innewohnenden Eigenschaften, nicht aufgrund des Besitzes bestimmter sichtbarer Attribute definiert, sondern durch ihre Relation zu anderen Arten.“[49] Das hat – zumindest nach der Mehrzahl der Interpretationen – zur Folge, dass Arten nicht Klassen sind, sondern Individuen.[50]
Das Kriterium der Fortpflanzungsfähigkeit bildet den Kern des biologischen Artbegriffs oder der Biospezies. Eine Biospezies ist eine Gruppe sich tatsächlich oder potentiell miteinander fortpflanzender Individuen, die voll fertile Nachkommen hervorbringen:
Dabei sollen die Isolationsmechanismen zwischen den einzelnen Arten biologischer Natur sein, also nicht auf äußeren Gegebenheiten, räumlicher oder zeitlicher Trennung basieren, sondern Eigenschaften der Lebewesen selbst sein:
Die Kohäsion der Biospezies, ihr genetischer Zusammenhalt, wird durch physiologische, ethologische, morphologische und genetische Eigenschaften gewährleistet, die gegenüber artfremden Individuen isolierend wirken. Da die Isolationsmechanismen verhindern, dass nennenswerte zwischenartliche Bastardisierung stattfindet, bilden die Angehörigen einer Art eine Fortpflanzungsgemeinschaft; zwischen ihnen besteht Genfluss, sie teilen sich einen Genpool und bilden so eine Einheit, in der evolutionärer Wandel stattfindet.
Beispiele:
Das biologische Artkonzept findet häufig Verwendung in der Ökologie, Botanik und Zoologie, besonders in der Evolutionsbiologie. In gewisser Weise bildet es das Standardmodell, aus dem die anderen modernen Artkonzepte abgeleitet sind oder gegen welches sie sich in erster Linie abgrenzen. Die notwendigen Charaktere (Fehlen natürlicher Hybriden bzw. gemeinsamer Genpool) sind bisweilen umständlich zu überprüfen, in bestimmten Bereichen, wie etwa in der Paläontologie, versagen biologische bzw. populationsgenetische Artabgrenzungen weitgehend.
Nach diesem Konzept wird eine Art als (monophyletische) Abstammungsgemeinschaft aus einer bis vielen Populationen definiert.
Eine Art beginnt nach einer Artspaltung (siehe Artbildung, Kladogenese) und endet
Phylogenetische Anagenese ist die Veränderung einer Art im Zeitraum zwischen zwei Artspaltungen, also während ihrer Existenz. Solange keine Aufspaltung erfolgt, gehören alle Individuen zur selben Art, auch wenn sie unter Umständen morphologisch unterscheidbar sind.
Das phylogenetische Artkonzept beruht auf der phylogenetischen Systematik oder „Kladistik“ und besitzt nur im Zusammenhang mit dieser Sinn. Im Rahmen des Konzepts sind Arten objektive, tatsächlich existierende biologische Einheiten. Alle höheren Einheiten der Systematik werden nach dem System „Kladen“ genannt und sind (als monophyletische Organismengemeinschaften) von Arten prinzipiell verschieden. Durch die gabelteilige (dichotome) Aufspaltung besitzen alle hierarchischen Einheiten oberhalb der Art (Gattung, Familie etc.) keine Bedeutung, sondern sind nur konventionelle Hilfsmittel, um Abstammungsgemeinschaften eines bestimmten Niveaus zu bezeichnen. Der wesentliche Unterschied liegt weniger in der Betrachtung der Art als in derjenigen dieser höheren Einheiten. Nach dem phylogenetischen Artkonzept können sich Kladen überlappen, wenn sie hybridogenen Ursprungs sind.
Ein weiterer Versuch, Arten in der Zeit klar abzugrenzen, ist das chronologische Artkonzept (Chronospezies). Auch hier wird die Art zunächst anhand eines anderen Artkonzepts definiert (meist das morphologische Artkonzept). Dann werden nach den Kriterien dieses Konzepts auch die Artgrenzen zwischen in einer Region aufeinanderfolgenden Populationen definiert. Dieses Konzept findet vorwiegend in der Paläontologie Anwendung und ist daher in der Regel eine Erweiterung des morphologischen Artkonzeptes um den Faktor Zeit:
Dieses Konzept ist dann gut anwendbar, wenn praktisch lückenlose Fundfolgen vorliegen.
In der Paläontologie, speziell in der Paläoanthropologie erweist sich die Zuordnung zu Arten und sogar die Zuordnung zu Gattungen allein anhand fossiler Knochen als schwierig. Anstelle einer kontravalenten Zuordnung wird daher von John Francis Thackeray eine wahrscheinlichkeitstheoretische Zuordnung vorgeschlagen. Anstelle der Frage, ob ein Fossil zur Spezies A und ein anderes zur Spezies B gehört, wird die Wahrscheinlichkeit, dass beide zur selben Spezies gehören, errechnet. Dazu wird eine möglichst große Reihe von Paaren unterschiedlicher morphometrischer Messpunkte von je zwei Individuen verglichen, bei denen die Artzugehörigkeit unsicher ist. Die Messwertpaare weichen stets voneinander ab. Sie streuen in Form einer Gaußschen Normalverteilung. Innerhalb dieser Verteilung wird definiert, in welchem Intervall um den Mittelwert (z. B. 2 Sigma) beide Individuen als derselben Art zugehörig betrachtet werden. Liegen die Messpunkte außerhalb des vorgegebenen Intervalls, werden die beiden Individuen als zwei verschiedene Arten betrachtet.[52]
Anfang des 21. Jahrhunderts waren zwischen 1,5 und 1,75 Millionen Arten beschrieben, davon rund 500.000 Pflanzen.[53] Es ist jedoch davon auszugehen, dass es sich bei diesen nur um einen Bruchteil aller existierenden Arten handelt. Schätzungen gehen davon aus, dass die Gesamtzahl aller Arten der Erde deutlich höher ist. Die weitestgehenden Annahmen reichten dabei Ende der 1990er-Jahre bis zu 117,7 Millionen Arten; am häufigsten jedoch wurden Schätzungen zwischen 13 und 20 Millionen Arten angeführt.[54][55] Eine 2011 veröffentlichte Studie schätzte die Artenzahl auf 8,7 ± 1,3 Millionen, davon 2,2 ± 0,18 Millionen Meeresbewohner; diese Schätzung berücksichtigte allerdings nur Arten mit Zellkern (Eukaryoten), also nicht die Prokaryoten und auch nicht Viren, Viroide und Prionen.[56]
Jay Lennon und Kenneth Locey von der Indiana University schätzten auf Basis der Ergebnisse von 3 Großprojekten, die Mikroben in Medizin, Meer und Boden behandeln, die Artenanzahl auf der Erde im März 2016 auf 1 Billion (1012). Insbesondere die kleinen Lebensformen der Bakterien, Archaeen und Pilze wurden bisher stark unterschätzt. Moderne Genom-Sequenzierung macht genaue Analysen möglich.[57][58]
Über die Gesamtzahl aller Tier- und Pflanzenarten, die seit Beginn des Phanerozoikums vor 542 Mio. Jahren entstanden, liegen nur Schätzungen vor. Wissenschaftler gehen von etwa einer Milliarde Arten aus, manche rechnen sogar mit 1,6 Milliarden Arten. Weit unter einem Prozent dieser Artenvielfalt ist fossil erhalten geblieben, da die Bedingungen für eine Fossilwerdung generell ungünstig sind. Zudem zerstörten Erosion und Plattentektonik im Laufe der Jahrmillionen viele Fossilien. Forscher haben bis 1993 rund 130.000 fossile Arten wissenschaftlich beschrieben.[59]
Es kann gezeigt werden, dass bei Verwendung des phylogenetischen Artkonzepts mehr Arten unterschieden werden als beim biologischen Artkonzept. Die Vermehrung der Artenzahl, z. B. innerhalb der Primaten, die ausschließlich auf das verwendete Artkonzept zurückgehen, ist als „taxonomische Inflation“ bezeichnet worden.[60] Dies hat Folgen für angewandte Bereiche, wenn diese auf einem Vergleich von Artenlisten beruhen. Es ergeben sich unterschiedliche Verhältnisse beim Vergleich der Artenzahlen zwischen verschiedenen taxonomischen Gruppen, geographischen Gebieten, beim Anteil der endemischen Arten und bei der Definition der Schutzwürdigkeit von Populationen bzw. Gebieten im Naturschutz.
Für detaillierte und aktuelle Diskussionen spezieller Themen:

Die Muskulatur ist ein Organsystem in Gewebetieren und bezeichnet eine Gesamtheit von Muskeln. Der Begriff bezieht sich z. B. bei den Bezeichnungen Bauchmuskulatur oder Rückenmuskulatur auf die Muskelgruppen des jeweiligen Körperabschnitts und ihre Wechselwirkung.
Ein Muskel (lateinisch musculus ‚Mäuschen‘, mittelhochdeutsch auch mūs[1] – ein angespannter Muskel sieht unter der Haut wie eine Maus aus) ist ein kontraktiles Organ, welches durch die Abfolge von Kontraktion und Relaxation innere und äußere Strukturen des Organismus bewegen kann. Diese Bewegung ist sowohl die Grundlage der aktiven Fortbewegung des Individuums und der Gestaltveränderung des Körpers als auch vieler innerer Körperfunktionen.
Die grundlegende Einteilung der Muskulatur bei Säugetieren einschließlich des Menschen erfolgt über den histologischen Aufbau und den Mechanismus der Kontraktion. Demnach unterscheidet man glatte Muskulatur und quergestreifte Muskulatur. Letztere lässt sich weiter in die Herzmuskulatur und die Skelettmuskulatur unterteilen. Weitere Unterscheidungsmöglichkeiten ergeben sich durch die Form, die Fasertypen und funktionelle Aspekte (siehe unten). Das einem Muskel zugrundeliegende Gewebe ist das Muskelgewebe, welches aus charakteristischen Muskelzellen besteht. Beim Skelettmuskel werden die Muskelzellen als Muskelfasern bezeichnet.
Die Bezeichnung der zytologischen Strukturen der Muskelzellen unterliegt einer für die Muskulatur spezifischen Nomenklatur und unterscheidet sich deshalb teilweise von der anderer Zellen:
Die gestreifte Muskulatur stammt von den Myotomen der Somiten der Leibeswand ab, die glatte aus dem Mesoderm der Splanchnopleura, sodass diese auch als Eingeweidemuskulatur bezeichnet wird. Im Bereich des Kopfdarms wird die viszerale Muskulatur von den Hirnnerven innerviert und ist quergestreift, während die restliche Eingeweidemuskulatur aus glatten Muskelfasern besteht.
Ein Muskel lässt sich auf verschiedene Weise einordnen, wobei diese Einteilung nicht direkt und eindeutig ist. Oft überschneiden sich die Eigenschaften. Je nach Blickwinkel werden sie durch ihre Zellstruktur, Form oder Funktion unterschieden. Weiterhin lassen sich Typen von Muskelfasern unterscheiden, die in einem Muskel vermischt vorkommen.
Anatomisch
Unterteilt wird auch in:
Zytologisch (s. o.) und Funktional (s. u.)
Extrafusale Fasern (auch twitch fibers = ‚Zuckungsfasern‘) (Arbeitsmuskulatur)
Intrafusale Fasern (Muskelspindeln) dienen als Dehnungsrezeptoren und zur Einstellung der Empfindlichkeit der Muskelspindeln.
Das Verhältnis der Zusammensetzung eines Skelettmuskels aus verschiedenen Muskelfasertypen ist weitestgehend genetisch bestimmt und ist durch ein gezieltes Ausdauer- beziehungsweise Krafttraining begrenzt veränderbar. Dieses verändert nicht das Verhältnis zwischen Typ-I- und Typ-II-Fasern, aber wohl das zwischen Typ-II-A und Typ-II-X. Aus vielen II-X-Fasern werden II-A-Fasern gebildet (z. B. im Musculus trapezius bei Krafttraining Gehalt an II-A von 27 % auf bis zu 44 % aller Fasern). Die Verteilung der verschiedenen Muskelfasern in einem Muskel ist nicht homogen, sondern unterschiedlich an Ursprung, Ansatz bzw. im Inneren und an der Oberfläche des Muskels.
Die Kontraktion ist ein mechanischer Vorgang, der durch einen Nervenimpuls ausgelöst wird. Dabei schieben sich Eiweißmoleküle (Aktin und Myosin) ineinander. Dieses wird durch schnell aufeinanderfolgende Konformationsänderungen der chemischen Struktur möglich, wodurch die Fortsätze der Myosinfilamente – vergleichbar mit schnellen Ruderbewegungen – die Myosinfilamente in die Aktinfilamente hineinziehen. Hört der Nerv auf, den Muskel mit Impulsen zu versorgen, erschlafft der Muskel, man spricht dann von Muskelrelaxation.
Je nach Kraft- (Spannungs-) bzw. Längenänderung des Muskels lassen sich mehrere Arten der Kontraktion unterscheiden:
Aus diesen elementaren Arten der Kontraktion lassen sich komplexere Kontraktionsformen zusammensetzen. Sie werden im alltäglichen Leben am häufigsten benutzt. Das sind z. B.
Hinsichtlich der resultierenden Längenänderung des Muskels und der Geschwindigkeit, mit der diese erfolgt, lassen sich Kontraktionen z. B. folgendermaßen charakterisieren:
Jeder Muskel ist von einer elastischen Hülle aus Bindegewebe (Faszie) ummantelt, die mehrere Fleischfasern (auch Sekundärbündel) umschließt, welche wiederum mit Bindegewebe (Perimysium externum und Epimysium) umschlossen und zusammengehalten werden, das von Nerven und Blutgefäßen durchsetzt ist und sich an der Faszie befestigt. Jede Fleischfaser unterteilt sich in mehrere Faserbündel (auch Primärbündel), die zueinander verschiebbar gelagert sind, damit der Muskel biegsam und anschmiegend ist. Diese Faserbündel sind eine Vereinigung von bis zu zwölf Muskelfasern, die durch feines Bindegewebe mit Kapillargefäßen vereint sind.
Aktiv wird der Muskel, indem er sich anspannt (Kontraktion), anschließend wieder entspannt, eine Bewegung und eine Kraft ausübt. Eine Muskelkontraktion wird von elektrischen Impulsen (Aktionspotentialen) ausgelöst, die vom Gehirn oder Rückenmark ausgesandt und über die Nerven weitergeleitet worden sind.
Bei der Muskelfaser handelt es sich um ein Syncytium, das heißt um eine Zelle, die aus mehreren determinierten Vorläuferzellen (Myoblasten) entsteht und daher mehrere Kerne enthält. Die Muskelfaser kann eine beachtliche Länge von mehr als 30 cm und ungefähr 0,1 Millimeter Dicke erreichen. Sie ist teilungsunfähig, was der Grund ist, warum bei einem Verlust der Faser kein Ersatz nachwachsen kann und bei Muskelzuwachs sich lediglich die Faser verdickt. Das heißt, dass von Geburt an die Obergrenze der Muskelfasern festgelegt ist. Neben den üblichen Bestandteilen einer tierischen Zelle machen hauptsächlich Myofibrillen, das sind feinste Fäserchen, zu etwa 80 Prozent die Fasermasse aus. Die Membranhülle von Muskelfasern nennt man Sarkolemma.
Im Hinblick auf ihre Zusammenarbeit werden Muskeln in gegenspielende und zusammenwirkende unterteilt. Agonisten (Spieler) und Antagonisten (Gegenspieler) haben zueinander eine entgegengesetzte Wirkung. Synergisten dagegen haben eine gleiche oder ähnliche Wirkung und arbeiten deshalb bei vielen Bewegungsabläufen zusammen.
Beispiel: äußere und innere Muskeln des Oberschenkels, mit welchen man die Beine spreizen und zusammenführen kann.
Jeder gesunde Mensch besitzt 656 Muskeln, wobei diese beim Mann etwa 40 %, bei der Frau etwa 32 % der Gesamtkörpermasse ausmachen, die Muskulosität hängt insgesamt aber von der Lebensweise ab.
Der flächenmäßig größte Muskel des Menschen ist der Große Rückenmuskel (Musculus latissimus dorsi), der dem Volumen nach größte Muskel ist der Musculus gluteus maximus (größter Gesäßmuskel), der stärkste der Kaumuskel (Musculus masseter), der längste der Schneidermuskel (Musculus sartorius), die aktivsten die Augenmuskeln und der kleinste der Steigbügelmuskel (Musculus stapedius). Aufgrund des Umfangs mechanischer Arbeit, die die Muskeln leisten müssen, sind sie neben dem Nervensystem einer der Hauptabnehmer von Körperenergie (siehe Muskelarbeit und Muskelleistung).
Beim Neugeborenen ist die Muskulatur im Rumpf weiter entwickelt als die in den Extremitäten. Der Muskelanteil beträgt etwa 21 Prozent des Körpergewichts. Während des Wachstums nimmt die Muskelmasse beim Mann etwa um das 32,8-Fache zu, die Gesamtkörpermasse jedoch nur etwa um das 19,4-Fache. Bei Männern schließt die Entwicklung der Muskulatur im Zeitraum zwischen dem 23. und dem 27. Lebensjahr ab, bei Frauen zwischen dem 19. und 23. Lebensjahr. Die Muskelmasse beim Mann liegt bei etwa 37–57 %, während sie bei der Frau etwa 27–43 % beträgt.
Im höheren Alter geht die Entwicklung der Muskeln zurück zu einem Zustand ähnlich dem vor der vollständigen Ausbildung. Dies betrifft also vor allem einen Abbau der Muskeln in den Beinen.[3]
Aufgrund seiner mikroskopischen Anatomie kann sich ein Muskel weder vollkommen zusammenziehen (das Sarkomer kann sich nur um ca. 30 % verkürzen), noch unbegrenzt dehnen (das Sarkomer würde ansonsten reißen). Daraus ergeben sich zwei verschiedene Formen physiologischer Insuffizienz eines Muskels:
Bei zweigelenkigen Muskeln ist es möglich, der Muskelinsuffizienz (bezüglich der Muskelwirkung auf ein Gelenk) entgegenzuwirken, indem man den Muskel im anderen Gelenk dehnt (bzw. den Antagonisten verkürzt). So wirkt beispielsweise der Musculus biceps brachii bezüglich seiner Beugekraft im Ellbogengelenk stärker, wenn der Arm retrovertiert ist (also das Ellenbogengelenk hinter dem Körper), da nun der aktiven Insuffizienz des Muskels durch Dehnung im Schultergelenk (der lange Bizepskopf überzieht beide Gelenke) entgegengewirkt wird.
Siehe auch:

Gehörlosigkeit bezeichnet das vollständige oder weitgehende Fehlen der Hörfähigkeit bei Menschen. Laut dem Deutschen Gehörlosenbund sind etwa 0,1 % der Bevölkerung in Industrienationen von Gehörlosigkeit betroffen. 
Der medizinische Ausdruck für Taubheit lautet lateinisch Surditas. Der Ausdruck gehörlos entstand im deutschen Sprachraum nach der Einführung der allgemeinen Schulbildung für taube Kinder im letzten Viertel des 18. Jahrhunderts (Der mittelhochdeutsche Terminus für Taubheit bzw. Gehörlosigkeit war ungehörde[1]). Tritt eine Hörschädigung erst nach dem Alter des natürlichen Spracherwerbs ein, spricht man von „postlingualer“ oder „Spät-Ertaubung“.
Circa 98 % der so genannten nicht hörenden Menschen haben ein Restgehör. Dabei ist der Begriff Gehörlosigkeit synonym zu Begriffen wie hochgradige Schwerhörigkeit, hochgradige Hörschädigung, Resthörigkeit oder Taubheit. Es handelt sich um Einschränkungen der Hörfähigkeit, bei denen akustisch entweder gar nichts oder entsprechende Reize nur noch mit Hörhilfen wie einem Hörgerät oder z. B. einem Cochlea-Implantat wahrgenommen werden können. Ob Gesprochenes mit diesen Hörhilfen verstanden wird, ist individuell verschieden.
Die Bezeichnung taubstumm wird von gehörlosen Personen als diskriminierend empfunden, weil der Wortteil 'stumm' eine negative Konnotation enthält und gegen gehörlose Personen gerne und oft in der Bedeutung von „dumm“ oder „unfähig“ gehandhabt wird.  Gehörlose Menschen erachten dabei Sprechfähigkeit weniger wesentlich als Kommunikationsfähigkeit. Sie können durchaus kommunizieren, sei es in Gebärdensprache, sei es in Lautsprache. Daher wollen nicht hörende Menschen im Deutschen gerne auch lediglich so bezeichnet werden.
Oralismus bezeichnet eine alleine auf Sprache fixierte Kommunikationserziehung von tauben und schwerhörigen Kindern, bei der auf Gebärdensprache weitgehend verzichtet werden soll.
In der International Statistical Classification of Diseases and Related Health Problems (ICD-10) wird Taubheit zusammen mit sonstiger Schwerhörigkeit als Hörverlust in den Abschnitten H90 und H91 kodiert.
Taubheit kann auch anders als durch eine Beeinträchtigung der Hörorgane bedingt sein. So bezeichnet „Zentrale Taubheit“ den Sachverhalt, dass die Hörorgane intakt und funktionsfähig sind, jedoch im Gehirn keine Verarbeitung der Höreindrücke erfolgt. Davon abzugrenzen ist psychogene Taubheit, die im Kapitel F als psychische Störung kodiert wird.
In Bezug auf Taubheit (lateinisch: Surditas) wird nach totaler Taubheit für alle Schallreize oder noch vorhandener Wahrnehmung einzelner Töne unterschieden. Das physikalisch definierte Ausmaß der Taubheit wird in der Regel mit einem audiometrischen Verfahren festgestellt, dessen Ergebnis das Audiogramm ist. Aus diesem lässt sich der Grad der Hörbehinderung feststellen.
Erworbene Taubheit (Innenohrschaden) kann als Folge von z. B. (Meningokokken-)Meningitis, Enzephalitis, Scharlach, Masern, Tuberkulose, Osteomyelitis, Mittelohr-Erkrankungen, Otosklerose, (Baro-)Trauma u. a. (bei absoluter Taubheit stets mit Innenohr- oder Hörnervbeteiligung) auftreten.
Angeborene Taubheit kann entweder vorgeburtlich durch Röteln-Embryopathie, Rh-Inkompatibilität mit Kernikterus, Labyrinthitiskonnatale (Syphilis) oder  Vererbung (meist autosomal-rezessiv) sowie durch Syndrome entstehen.
Bekannte Syndrome sind unter anderem das Usher-Syndrom (Einschränkung des Sichtfelds) oder das Waardenburg-Syndrom (Pigmentanomalien in Haut, Haaren oder in den Augen, beispielsweise verschiedene Irisfarben). Weitere Syndrome sind z. B. das Alport-, Jervell-Lange-Nielsen-, Cockayne- und Pendred-Syndrom.
Eine von Geburt an vorliegende Beeinträchtigung des Hörens wurde häufig erst spät erkannt. Das Alter bei der Erkennung von Taubheit liegt ohne entsprechende Neugeborenenhörscreeningprogramme im statistischen Durchschnitt bei etwas mehr als zwei Jahren. Seit 2009 ist das Neugeborenenhörscreening in Deutschland eine Leistung der gesetzlichen Krankenversicherung. In Österreich und der Schweiz und weiteren Ländern gibt es ähnliche Programme. Bei diesem Verfahren wird das Neugeborene bereits ein oder zwei Tage nach der Geburt (in der Klinik) auf seine Hörfähigkeit getestet, um möglichst frühzeitig eine angeborene Hörstörung zu erkennen.
Die Diagnose erfolgt durch spezielle Hörtests. Bei Neugeborenen und Kleinkindern wurden früher akustische Signalgeber verwendet, die einen reflektorischen Lidschluss auslösen sollten. Bei seinem Ausbleiben wurde eine  Gehörlosigkeit vermutet.[2] Das Verfahren weist jedoch erhebliche Ungenauigkeiten auf, weshalb es zur aussagekräftigen Diagnostik frühkindlicher Hörschädigungen unbrauchbar ist. Standard heute sind bei Neugeborenen und kooperationsunwilligen/-fähigen Patienten OAE-Verfahren und die BERA, darüber hinaus altersabhängig unterschiedliche Hörtestverfahren.
Es gibt eine Reihe von Störungen, von denen die Gehörlosigkeit genau abzugrenzen ist, z. B.
Diese Störungen lassen sich z. B. durch weitere Merkmale (wie etwa Sozialverhalten, Kommunikation, Sprechen oder Nicht-Sprechen) differenzieren.
Die spezifische Sprache der gehörlosen Personen ist traditionell die Gebärdensprache ihrer betreffenden gebärdensprachlichen Umgebung, die sich immer da entwickelt, wo zwei oder mehr gehörlose Menschen sich treffen. Personen, bei denen die Gebärdensprache die Muttersprache ist, denken auch in dieser Sprache.
Gebärdensprachen werden auch von Hörenden benutzt, nicht nur im Umgang mit gehörlosen Personen, sondern auch untereinander, z. B. von Verwandten und Freunden von gehörlosen Personen, Gebärdensprachdolmetschern, Pädagogen oder allgemein an Gebärdensprache interessierten Menschen und untereinander bei den nordamerikanischen Indianern und Warlpiri‐Aborigines in Australien. Zudem sind die Gebärdensprachen aufgrund ihrer Besonderheiten für Linguisten ein hochinteressantes Forschungsgebiet.
Gebärdensprachen sind vollwertige Sprachen, die alle Eigenschaften einer gesprochenen Sprache aufweisen. Sie besitzen eine eigene Grammatik, wobei der Gebärdenraum – der Raum vor dem Körper des Gebärdenden – eine große Rolle spielt. Jede einzelne Gebärde kann phonologisch in Phonemen zerlegt werden, die in den vier Parametern Handkonfiguration, Handorientation, Bewegung und Lokalität zusammengefasst sind. Ferner spielen Körperhaltung, Bewegungsdynamik, Mimik und manchmal ein lautlos mitgesprochenes Wort zusätzliche Rollen.
Die Gebärdensprache ist nicht universal. Sie können einander unverständlich sein.
Es hat sich eine Konvention etabliert, dass eine eigene Gebärdensprache in jedem Land mit eigenem Kürzel kursiert (ASL für Nordamerika, LSF für Frankreich, DGS für Deutschland, ÖGS für Österreich usw.).
Die Entwicklung einer Gebärdensprache erfolgte stets unabhängig von der umgebenden Lautsprache. Aber es gibt auch einen Gebärdenkode der die Gebärdensprache umgebenden Lautsprache, die im deutschsprachigen Raum als Lautsprachbegleitende(s) Gebärden (LBG) geläufig ist, aber anderswo gewöhnlich mit gebärdetem Englisch, Spanisch, Russisch usw. bezeichnet wird. Es gibt ferner lokale Dialekte, zum Beispiel wird die Deutschschweizer Gebärdensprache (DSGS) in fünf verschiedenen Dialekte unterteilt: Zürcher, Berner, Luzerner, Basler und St. Galler-Gebärdendialekt. Die am weitesten verbreitete Gebärdensprache dürfte die American Sign Language (ASL) sein, die nicht nur in Nordamerika, sondern auch in meisten karibischen Inseln, einigen mittelamerikanischen, afrikanischen und asiatischen Nationen verbreitet ist.
Die Gebärdensprache wird in einigen Ländern als Minderheitensprache anerkannt, so in Österreich durch die Bundesverfassung (Art. 8, Abs. 3). Die Gebärdensprache in Uganda ist verfassungsmäßig anerkannt, und die Neuseeländische Gebärdensprache (NZSL) ist eine offizielle Amtssprache Neuseelands. Im Schweizer Kanton Zürich ist die Gebärdensprache im Sinne der Sprachenfreiheit verfassungsmäßig anerkannt. In einigen Ländern werden durch Gesetze oder Regelungen Gebärdensprach-Dolmetschdienste im Umgang mit Behörden oder Gericht angeordnet. Die Gebärdensprache wird zunehmend als Fremdsprache in den Universitäten oder Volkshochschulen gelehrt.
Ein Teil der schwerhörigen und gehörlosen Menschen verstehen die Laut- und damit oft auch die Schriftsprache nicht so gut wie Normalhörende. Texte sollten deshalb barrierefrei sein und eine einfache Sprache verwenden.
In der Deutschschweiz wird gehörlosen Menschen Schweizer Hochdeutsch gelehrt, diese beherrschen in der Folge nur schlecht bis gar kein Schweizerdeutsch, es sei denn, das Schweizerdeutsche wird auf privater Ebene gelehrt. Das führt zur Situation, dass im Verkehr mit hörenden Menschen Hochdeutsch gesprochen wird, das Schweizer oft nur passiv beherrschen. Dadurch wird die Kommunikation zusätzlich erschwert. Die Muttersprache von Schweizer Codas ist in der Folge nicht Schweizerdeutsch, sondern Schweizer Hochdeutsch, neben der Gebärdensprache. Eine ähnliche Situation ist aus Luxemburg und Südtirol bekannt, dort wird gehörlosen Kindern Deutsch als Erstsprache beigebracht, die weiteren landesüblichen Sprachen werden folglich schlecht oder gar nicht beherrscht.
Die frühkindliche Taubheit beeinträchtigt den Spracherwerb, weil rund 90 % tauber Kinder Eltern haben, die hören können und keine Gebärdensprachkenntnisse aufweisen. Ihre Erziehung und schulische Bildung sind zumeist monolingual in der Landessprache und oral ausgerichtet, oft unter Vermeidung bzw. Unterdrückung der Gebärdensprache.
Die besonderen Schulen, die sich der Erziehung tauber Kinder widmen, gewannen damit eine weit über die Bildung hinausgehende Bedeutung als Entstehungshort einer kulturellen Gemeinschaft tauber Menschen.
Bereits im 18. Jahrhundert bildeten sich zwei gegensätzliche Unterrichtsansätze heraus, ob taube Kinder monolingual in der Landessprache oder bilingual mit Zusatz von Gebärdensprache unterrichtet werden sollten: die französische Methode von Abbé de l’Epée und die deutsche von Samuel Heinicke. Um die Wirksamkeit und die Nützlichkeit der beiden Ansätze entbrannte ein Streit, der bis heute andauert. Er ist als der „Methodenstreit“ zwischen der „deutschen“ oder „oralen“ Methode und der „französischen“, gebärdensprachlichen Methode bekannt worden.
Der Streit fand beim Mailänder Kongress von 1880 seinen Höhepunkt. Dort entschieden sich führende Pädagogen in einer Resolution, dass alle tauben Kinder ausschließlich lautsprachlich geschult werden sollen. Fortentwicklungen der Medizin und der Technik suggerierten die jeweils bald bevorstehende Heilbarkeit von Taubheit und wirkten zusätzlich fördernd für die „orale“ Methode. In den 1950er Jahren wurde schließlich die so genannte auditiv-verbale Methode entwickelt, bei der taube Kinder nicht mehr nur artikulieren und Lippenablesen lernen, sondern – sofern Hörreste vorhanden waren – auch das Hören trainieren sollten. Die Auseinandersetzung hat sich an den Sonderschulen jetzt verlagert auf die Polarität zwischen rein lautsprachlich orientiertem Monolingualismus und dem Bilingualismus, der neben dem Gebrauch der Gebärdensprache für die parallele Lehre und Verwendung der Lautsprache plädiert.
Die aktuellen Ansätze zur schulischen Bildung gehörloser Kinder sind mittlerweile sehr differenziert geworden. Im deutschsprachigen Raum war bisher die Beschulung in einer Sonderschule für Gehörlose oder – bei größerem Resthörvermögen – einer Schule für Schwerhörige der Standard. Um das Jahr 2000 herum standen in Deutschland für schätzungsweise 10.000 bis 20.000 taube oder hochgradig schwerhörige Schüler etwa 60 Sonderschulen zur Verfügung. Das Rheinisch-Westfälische Berufskolleg für Hörgeschädigte in Essen ist die größte Sonderschule für Schwerhörige und Gehörlose in Deutschland und führt Bildungsgänge bis zur Fachhochschulreife und zur allgemeinen Hochschulreife. Die Schule wird von ca. 900 Schülerinnen und Schülern aus ganz Deutschland, zum Teil auch aus dem deutschsprachigen Ausland besucht. Die weltweit einzige Volluniversität speziell für Schwerhörige und Gehörlose ist die Gallaudet University in Washington, D.C., die etwa 1700 Studenten hat und seit 1988 auch von Gehörlosen geleitet wird.
Wegen der geringen Klassenfrequenzen lokaler Schulen bestimmten vor allem die schwächeren Kinder das Niveau an den Sonderschulen. Dies führte zunächst zu einer Abwanderung von den Gehörlosenschulen zu den Schwerhörigenschulen. Inzwischen hat sich, ausgehend von den körperbehinderten Kindern, der Gedanke der Integration auch auf das Feld der Hörgeschädigten übertragen, mit der Folge eines Trends zur Abwanderung an die Regelschule.
Begünstigt wird diese Diversifizierung in Deutschland  auch davon, dass letztlich die Eltern bestimmen können, welche Schule ihr Kind besucht, und diese das in ihren Augen gegebene Optimum zu wählen versuchen. Bei den Schulbehörden in Deutschland  wird verschiedentlich auch der Regelschulbesuch mit dem Argument der „Integration“ offensiv gefördert, wobei im Hintergrund jedoch oft die Erwartung der Kostendämmung durch Einsparungen von Sonderschul-Pädagogen und separaten Schulen steht.
Der „integrative“ Schulbesuch an einer Regelschule hat keine einheitliche Fassung, es gibt neben dem sonderpädagogisch völlig unbegleiteten Regelschulbesuch noch den sonderpädagogisch und/oder von einer Gebärdensprachdolmetscherin begleiteten Schulbesuch sowie sehr vereinzelt auch das Konzept der „umgekehrten“ Integration, bei der in eine Sonderschule nicht behinderte Kinder aufgenommen werden.
Je weniger sonderpädagogische oder sprachliche Unterstützung bei einem „integrativen“ Regelschulbesuch erfolgt, umso mehr ist der Erfolg dieses Schulbesuchs von besonders hoch entwickelten Fähigkeiten und Talenten des Kindes abhängig. Unberücksichtigt bleibt bei der Diskussion der integrativen Beschulung in der Regel die „Gefühlslage“ des Kindes, das im Klassenverband der anderen Kinder mehr oder weniger eine Sonderstellung einnimmt, die zusätzlich zum Unterrichtsstoff auch psychisch verarbeitet werden muss.
Da taube Personen durch ihre Kommunikationsbehinderung in der Gesellschaft häufig isoliert sind, werden soziale Kontakte gern innerhalb von Gehörlosenkreisen gepflegt. Die über Jahrhunderte hinweg gepflegte Gemeinschaft mit gleichartig Betroffenen führte zumindest im außerberuflichen, privaten Bereich zur Entwicklung einer eigenen Kultur.
Zur speziellen Kultur der Gehörlosen gehört neben der Gebärdensprache beispielsweise, dass es in sämtlichen größeren Städten einen Verein und einen festen Treffpunkt, oft „Clubheim“ genannt, gibt. Stark entwickelt ist zudem der Gehörlosensport. So werden weltweit die Deaflympics jeweils ein Jahr nach den Olympischen Spielen veranstaltet.
Auch in den „schönen Künsten“ haben sich eigene Strukturen gebildet, so z. B. mit dem Gehörlosentheater, Gebärdensprachchören und den Kulturtagen der Gehörlosen.
Wichtiger Bestandteil der Gehörlosen-Kultur sind auch deren meist hörende Kinder, die der Gemeinschaft oft lebenslang verbunden bleiben und auch ihre eigenen Vereinigungen haben. Sie sind international unter dem Akronym CODA – Children of Deaf Adults – bekannt.
Gehörlose, die in der Gehörlosen- und Gebärdensprachgemeinschaft leben, lehnen medizinische und juristische Definitionen von Gehörlosigkeit ab, nach denen sie unvollständig, reparaturbedürftig und behindert sind. Nach ihrem Selbstverständnis handelt es sich bei der Gehörlosengemeinschaft um eine sprachliche und kulturelle Minderheit.
Als politische, soziale und kulturelle Interessenvertretung der Gehörlosen im deutschsprachigen Raum betrachten sich der Deutsche Gehörlosen-Bund e.V. (DGB), der Österreichische Gehörlosenbund (ÖGLB), der Schweizerische Gehörlosenbund (SGB) und der Weltverband der Gehörlosen (WFD).
Als politische und soziale – jedoch nicht kulturelle – Interessenvertretung im deutschsprachigen Raum für lautsprachlich kommunizierende Hörgeschädigte bzw. Hörbeeinträchtigte betrachten sich der deutsche Förderverein LKHD – Lautsprachlich Kommunizierende Hörgeschädigte Deutschland e. V. (Kurzform Förderverein LKHD oder auch LKHD) und die Schweizer Selbsthilfeorganisation lkh.ch, Lautsprachlich Kommunizierende Hörbeeinträchtigte (lkh.ch, vormals LKH Schweiz).
Zum Verstehen lautsprachlicher Informationen sind gehörlose Personen auf das Lippenlesen und/oder auf technische Hilfsmittel angewiesen. Sowohl visuell von den Lippenstellungen wahrnehmbare Sprechtöne als auch die eventuell mit Hilfsmitteln gehörten Töne sind für sie nur bruchstückhaft wahrnehmbar. Die übermittelte Information muss daher teilweise „erraten“ werden, wobei Hinweise aus dem Kontext der Umgebung und aus vorhergehenden Sätzen herangezogen werden. Bei größerem Umfang oder je nach Komplexität – z. B. in einem Vortrag – ist das sehr anstrengend bis gar unmöglich.
Vielfach wird bei nicht direkt therapierbarer Taubheit als medizinische Maßnahme eine technische Hörhilfe verschrieben bzw. angewendet. Technische Hörhilfen sind das Hörgerät sowie die chirurgisch eingesetzten Cochlea- (CI) und Hirnstamm-Implantate (Auditory Brainstem Implant, ABI). Der Erfolg dieser technischen Hilfsmittel ist individuell sehr unterschiedlich. Bei hochgradiger Schwerhörigkeit oder Taubheit können die derzeit bekannten Hörhilfen nicht den Umfang und die Differenzierung von Tönen und Geräuschen vermitteln, wie sie ein Mensch mit normalem Hörvermögen hat.
Das führt dazu, dass Hörhilfen allein zwar ein Hörerlebnis vermitteln, jedoch meist nicht ausreichen, um damit unmittelbar die Lautsprache zu verstehen. Dazu muss der Hörhilfen-Einsatz in der Regel von einem speziellen Training begleitet werden. Das hörgeschädigte Kind ist daher nicht nur auf technische Hilfsmittel, sondern auch auf eine spezielle Hör- und Sprecherziehung angewiesen, mit der – je nach Begabung und Übung – die Lautsprache erlernt werden kann. Für die eigene Sprech-Schulung ist die auditive Rückkopplung oft nicht genügend nuanciert und die komplexe Kontrolle des Sprechapparates ist schwierig.
Dank besserer Förderungsmöglichkeiten gelingt es immer mehr Gehörlosen, die Lautsprache so weit zu beherrschen, dass ein dauerhafter sozialer Kontakt mit der Mehrheitsgesellschaft entsteht. Dazu haben sich im deutschsprachigen Raum im Kreis dieser sogenannten „Lautsprachlich kommunizierenden Hörgeschädigten“ auch eigene Vereine mit vereinsinternen Aktivitäten gegründet.

Die Tuberkulose (kurz Tb oder Tbc; so benannt von dem Würzburger Kliniker Johann Lukas Schönlein wegen des charakteristischen histopathologischen Bildes, von lateinisch Tuberculosis, von lateinisch tuberculum‚ kleine Geschwulst) ist eine weltweit durch Bakterien verbreitete Infektionskrankheit. Die früher auch als Schwindsucht bezeichnete Erkrankung wird durch verschiedene Mykobakterien verursacht. Beim Menschen ist die Lungentuberkulose die häufigste Form. Bei Immundefekten zeigt sich vermehrt auch ein Befall außerhalb der Lunge.
Die Tuberkulose, an der weltweit etwa 10 Millionen Menschen pro Jahr erkranken, führt die weltweite Statistik der tödlichen Infektionskrankheiten an. Nach dem Global tuberculosis report der Weltgesundheitsorganisation (WHO) starben 2015 etwa 1,4 Millionen Menschen an Tuberkulose. Dazu kamen noch 400.000 Todesfälle von zusätzlich[1] HIV-Infizierten.[2] Tuberkulose wird (zumindest heutzutage in Deutschland) am häufigsten durch Mycobacterium tuberculosis verursacht, seltener – in absteigender Folge – durch Mycobacterium bovis, Mycobacterium africanum oder Mycobacterium microti.
Die Beschreibung des Erregers Mycobacterium tuberculosis durch Robert Koch war 1882 ein Meilenstein der Medizingeschichte.[3] Die Tuberkulose wird deshalb auch Morbus Koch genannt. Die Bezeichnungen Schwindsucht (auch Phthisis oder Phthise) oder umgangssprachlich „die Motten“, Weiße Pest und Weißer Tod[4] sind veraltet, ebenso wie die Termini Lungendärre, Darre und Dörre.[5][6]
Nur etwa 5–10 % der mit Mycobacterium tuberculosis Infizierten erkranken tatsächlich im Laufe ihres Lebens. Betroffen sind besonders Menschen mit geschwächtem Immunsystem oder genetisch bedingter Anfälligkeit. Die Übertragung erfolgt in der Regel durch Tröpfcheninfektion von erkrankten Menschen in der Umgebung. Sind Keime im Auswurf (Sputum) nachweisbar, spricht man von offener Tuberkulose, bei Keimnachweis in anderen äußeren Körpersekreten von potentiell offener Tuberkulose. Durch tiefes Ausatmen[7] oder Husten entsteht ein infektiöses Aerosol, das seine Ansteckungsfähigkeit durch Sedimentation, Durchlüftung und natürliche UV-Lichtquellen verliert. Da Rinder ebenfalls an der Tuberkulose erkranken können, war in Westeuropa früher (nicht-pasteurisierte) Rohmilch eine verbreitete Infektionsquelle und ist es in Teilen der Welt bis heute.[8] Aufgrund der Übertragbarkeit von Tieren auf Menschen zählt die Tuberkulose zu den Zoonosen. Umgekehrt ist die Übertragung von Menschen auf Tiere ein wichtiger Aspekt beim Artenschutz seltener Primaten.
Erst mit dem direkten Nachweis der Erreger oder deren Erbgut ist die Erkrankung labordiagnostisch bestätigt. Indirekte, d. h. immunologische Befunde oder Hauttests tragen nur zur Diagnose bei, da man durch sie nicht zwischen einer Erkrankung und einer stattgehabten Infektion unterscheiden kann. Auch können sie bei einer zusammengebrochenen Immunabwehr falsch-negativ ausfallen.
Zur Behandlung stehen verschiedene Antibiotika zur Verfügung, die speziell gegen Mykobakterien wirksam sind und deshalb auch Antituberkulotika genannt werden. Diese müssen zur Vermeidung von Resistenzentwicklungen und Rückfällen unbedingt in Kombination und nach Vorgabe der WHO über mindestens ein halbes Jahr, also weit über das Bestehen der Beschwerden hinaus, eingenommen werden. Es existiert eine Impfung, die aber wegen unzureichender Wirksamkeit in Deutschland seit 1998 nicht mehr empfohlen und auch nicht mehr verfügbar ist. Eine Primärprophylaxe mit einem antituberkulös wirksamen Medikament wird in Deutschland in erster Linie bei Kindern oder schwerst immunologisch beeinträchtigten Kontaktpersonen empfohlen. Bei Erwachsenen, die über ein intaktes Immunsystem verfügen (und daher als immunkompetent bezeichnet werden), erfolgt hingegen eine Sekundärprophylaxe oder Prävention erst nach Feststellung einer Infektion mittels vorbeugender Gabe von antituberkulös wirksamen Medikamenten unter Beachtung der Resistenzlage. Die Tuberkulose ist in der Europäischen Union und im größten Teil der Welt eine meldepflichtige Krankheit.
Etwa ein Drittel der Weltbevölkerung ist mit Tuberkuloseerregern infiziert. Nur ein geringer Teil der Infektionen führt jedoch zu einer Erkrankung. Nach dem Tuberkulosebericht der WHO (Global tuberculosis report 2016) gab es im Jahr 2015 weltweit 10,4 Millionen Neuinfektionen und 1,8 Millionen Todesfälle. Beide Zahlen sanken seit 1990 stetig.[9]
Die Möglichkeiten zur Behandlung sind oft unzureichend, da sie teure Antibiotika erfordert, lang dauert und angesichts der sozialen Lebensumstände der Betroffenen oft undurchführbar ist. Auch fehlen in betroffenen Regionen oft Laboratorien zur Diagnose und Behandlung. Besonders in Osteuropa ist durch Armut und Mängel im Gesundheitswesen eine besorgniserregende Zunahme der Tuberkulose zu verzeichnen, vor allem auch mit multiresistenten Erregerstämmen. Auch weltweit wird die Krankheit immer häufiger durch solche medikamentenresistenten Tuberkulosestämme verursacht.
Besonders problematisch ist eine Tuberkuloseinfektion bei HIV-Infizierten mit manifestem AIDS. Durch die Immunschwäche erhöht HIV die Wahrscheinlichkeit des Ausbruchs einer Tuberkuloseerkrankung um ein Vielfaches. Tuberkulose ist in Afrika neben AIDS die häufigste Todesursache. Beide Krankheiten treten besonders bei Bewohnern von Metropolenslums in enger Wechselbeziehung zueinander auf. Dabei führt die Immunschwäche durch HIV oft zu negativen Ergebnissen bei Tuberkulose-Routineuntersuchungen, obwohl die Krankheit vorliegt (siehe auch Fehler 1. und 2. Art). Das liegt daran, dass die Hauttests (Tuberkulin-Test, Tine-Test) die immunologische Reaktion auf Erregerbestandteile prüfen, die aber durch AIDS gehemmt ist. Der Verlauf der Tuberkulose ist dann erheblich beschleunigt. In armen Ländern gilt TBC als Zeichen des Ausbruchs von AIDS und führt bei der Mehrheit aller HIV-Erkrankten zum Tod. Die WHO fordert und fördert daher eine weltweite Koordination der Tuberkulose- und AIDS-Forschung.
Überraschend fand eine italienische Studie eine Prävalenz (Krankheitshäufigkeit) latenter Tuberkuloseinfektionen von neun Prozent unter gesunden Angestellten im Gesundheitsbereich und von 18 Prozent unter gut 400 an Psoriasis Erkrankten. Auch 30 Prozent der Kranken mit Lungenentzündung und Lungenkrebs waren latent infiziert.[10]
Dem Robert Koch-Institut (RKI) wurden 2016 in Deutschland 5915 Tuberkulosekranke gemeldet,[11] darunter 233 Kinder unter 15 Jahren (2005: 230). 2016 kamen in Deutschland 7,2 Erkrankungen auf 100.000 Einwohner. Die offizielle Statistik gab für 2015 100 Todesopfer an. Die Daten dürften nicht ganz den realen Zahlen entsprechen, da die Dunkelziffer bei dieser Krankheit wegen ihrer unspezifischen Symptome relativ hoch ist. Nach einer Pathologiestudie aus Deutschland war die Diagnose bei lediglich einem Drittel der postmortal diagnostizierten Tuberkulosen zu Lebenszeiten gestellt worden.
In Deutschland ist die Krankheit besonders in Hamburg, Bremen und Berlin verbreitet. Bei im Lande geborenen Erkrankten überwiegen die älteren Jahrgänge aufgrund Aktivierungs- und Reaktivierungsneigung infolge der abnehmenden Immunabwehr. Unter den Migranten überwiegen die mittleren Jahrgänge, da hier eher frische Infektionen erkrankungsauslösend sind. Die vorläufige Tuberkulosestatistik für 2017 zeigt in Deutschland ein Plateau auf der Höhe von 2016 an, nachdem ein Anstieg der Erkrankungen an Tuberkulose, bedingt durch erhöhte Zuwanderung im Herbst 2015 zu verzeichnen war. In der Schweiz und in Österreich haben die Fallzahlen bis 2017 ebenfalls leicht abgenommen. Ein befürchteter stärkerer Anstieg der Fallzahlen durch die Migrationswelle von 2017 ist bislang demnach ausgeblieben.
In Österreich wurden im Jahr 2015 583 Erkrankungen an Tuberkulose erfasst, die Schweiz verzeichnete im gleichen Jahr 546 Erkrankungen.
In der folgenden Tabelle sind die Neuerkrankungen pro 100.000 Einwohner (Inzidenz) und die Neuerkrankungszahlen pro Jahr in Deutschland (D), der Schweiz (CH) und Österreich (A) aufgeführt.[12][13][14][15][16][17][18][19][20]
 * nur ansteckungsfähige
 ** Vorläufige Zahlen
Der wichtigste Erreger der Tuberkulose, Mycobacterium tuberculosis, ist ein aerobes grampositives Stäbchen-Bakterium, das sich alle 16 bis 20 Stunden teilt. Verglichen mit anderen Bakterien, die Teilungsraten im Bereich von Minuten haben, ist dies extrem langsam. Der mikroskopische Nachweis gelingt durch die typischen Färbeeigenschaften: Das Bakterium behält seine Färbung nach Behandlung mit einer sauren Lösung und wird deshalb als säurefestes Stäbchen bezeichnet. In der gebräuchlichsten Färbung dieser Art, der Ziehl-Neelsen-Färbung, heben sich die rot eingefärbten Keime vor einem blauen Hintergrund ab. Der Nachweis gelingt weiterhin durch Fluoreszenzmikroskopie und durch die Auramin-Rhodamin-Färbung. In der Gram-Färbung stellen sich Mykobakterien kaum dar, der Aufbau des Peptidoglycans ähnelt jedoch stark dem grampositiver Bakterien, so dass M. tuberculosis formal als grampositiv klassifiziert wird. Dies wurde durch Sequenzanalysen der RNA bestätigt.
Zur gleichen Bakteriengruppe gehören weitere Mykobakterien, die ebenfalls zu den Erregern der Tuberkulose gezählt werden: M. bovis, M. africanum und M. microti. Diese Erreger werden in Deutschland nur sporadisch bei tuberkulösen Erkrankungen gefunden. M. kansasii und auch M. avium können in seltenen Fällen, wie eine Reihe weiterer Mykobakterien, tuberkuloseähnliche Krankheitsbilder verursachen. Eine Ansteckungsgefahr geht jedoch von atypischen Mykobakterien (mycobacteria other than tuberculosis, MOTT) in der Regel nicht aus.
M. tuberculosis, M. bovis, M. africanum, M. microti, M. canetti, M. pinnipedi, M. caprae und der Impfstamm Bacillus Calmette-Guérin (BCG) werden zusammengefasst als Mycobacterium tuberculosis complex.
Einatmung infektiöser Tröpfchen (Aerosole) stellt den häufigsten und somit wichtigsten Übertragungsweg dar: Für eine Infektion genügt hierbei die Inhalation einiger Mikrotröpfchen (2–5 µm Durchmesser), die jeweils 1–3 Erreger enthalten.[21][22] Weitaus seltener ist die Übertragung über den Blutweg, durch Organtransplantationen oder über andere Körpersekrete. Prinzipiell ist jeder der folgenden Übertragungswege möglich und in der Literatur als gesichert beschrieben:
Wiederholt nachgewiesen wurde der Befall von Kakerlaken und deren Kot mit Mycobacterium tuberculosis. Dieser Umstand wird regelmäßig von Schädlingsbekämpfern angeführt. Eine Sichtung der Literatur brachte keinen einzigen gesicherten Übertragungsfall.
Ausscheidungstuberkulose ist ein nicht mehr gebräuchlicher medizinischer Begriff für jene Formen der Tuberkulose, bei denen die Krankheitserreger über die Ausscheidungsorgane im Körper verbreitet werden und zu einem sekundären Befall anderer Organe führen.
Die aerogenen Infektionen gehen meist von Erwachsenen aus, da bei Kindern selbst bei offener Lungentuberkulose die ausgeschiedene Bakterienmenge zu gering ist (paucibacilläre = erregerarme Tuberkulose). Organbeteiligungen außerhalb der Lungen stellen mit Ausnahme der hochkontagiösen Kehlkopftuberkulose (Larynxtuberkulose) nur dann ein Infektionsrisiko dar, wenn die Infektionsherde durch natürliche Wege (Magen-Darm-Trakt) oder Fistelbildung mit dem Körperäußeren verbunden sind[15] oder es bei diagnostischen Punktionen/Eingriffen zu Nadelstichverletzungen oder Kontakt zu Wunden kommt. Eine historisch bedeutsame, heute fast vergessene Sonderform stellten die Leichentuberkel (engl. prosectors wart) dar, wobei sich Anatomen, Pathologen, Schlachter etc. über Handwunden erneut infizieren. Auch eine Infektion durch infizierte Milch ist möglich. Solche Infektionen sind in Industrieländern, wo die Rinderbestände weitgehend tuberkulosefrei sind und die Milch pasteurisiert wird, jedoch inzwischen sehr selten geworden. Neugeborene von Müttern mit Lungentuberkulose stecken sich nur selten über die Blutbahn an. Hat die Tuberkulose-Infektion allerdings den Mutterkuchen (Plazenta) ergriffen, kann sich das Kind durch das Verschlucken bakterienhaltigen Fruchtwassers infizieren. Wenn bei der Mutter die ableitenden Harnwege und damit verbunden die Geschlechtsorgane befallen sind, kann sich das Neugeborene bei der Geburt anstecken.[23]
Nach der Infektion werden die Erreger in den meisten Fällen schon in den Atemwegen abgewehrt. Von allen Infizierten erkrankt nur etwa ein Zehntel tatsächlich an Tuberkulose. Ob ein Organismus sich ausreichend gegen die Mykobakterien wehren kann, hängt von vielen Faktoren ab. Wichtig sind der Ernährungszustand, eine genetische Disposition (es gibt etwa 20 bekannte Genpolymorphismen, die das Erkrankungsrisiko bis auf den Faktor 5 steigern) sowie eine medikamentöse, infektbedingte oder toxische Unterdrückung der Immunabwehr, aber auch die Menge der aufgenommenen Bakterien und die Häufigkeit des Kontaktes. Auch die Größe und Durchlüftung eines Raumes und das Fehlen von UV-Lichtquellen können als Faktoren eine Rolle spielen.
Ein erstes Abwehrbollwerk stellen spezialisierte Fresszellen (Alveolarmakrophagen) in den Lungenbläschen dar. Diese können die Erreger zwar in ihr Zellinneres aufnehmen (phagozytieren), dann aber nicht abtöten. Auch weitere herbeigerufene Fresszellen sind dazu nicht in der Lage. Der Vorgang der Phagozytose wird durch verschiedene Stoffe an der Oberfläche von Krankheitserregern aktiviert. Das können zum einen Bestandteile der Zellwand, aber auch Moleküle des Wirts sein, die sich an die Zellwand des Eindringlings gebunden haben. Mykobakterien verhindern, dass die Zellbestandteile der Fresszellen, in denen sie sich befinden, die so genannten Phagosomen, weiter reifen. Dies sichert das Überleben von Mycobacterium tuberculosis. Das Immunsystem bildet deshalb um den anfänglichen Infektionsherd einen Wall aus mehreren Ringen verschiedener Abwehrzellen. Dieser Abwehrwall aus Fresszellen (Makrophagen), so genannten Epitheloidzellen, Langhans-Riesenzellen und Lymphozyten formiert sich um einen zentralen Entzündungsherd mit Gewebsuntergang (Nekrose). Diese besondere Form der Nekrose, die auch pathognomonisch für die TBC ist, wird Verkäsende Nekrose genannt. Das gesamte Gebilde wird als tuberkulöses Granulom (oder Tuberkulom[24]) bezeichnet. Es isoliert Mycobacterium tuberculosis am Ort des Eindringens und verhindert eine Weiterverbreitung. Dazu ist ein funktionierendes Zusammenspiel der verschiedenen Abwehrzellen erforderlich, die sich gegenseitig über verschiedene Botenstoffe (Zytokine) herbeirufen und aktivieren. Insbesondere die Ausschüttung von Tumornekrosefaktor sorgt für nitrosativen Stress im Phagosom, der zusammen mit der Einkapselung das Bakterium zwingt, in einen Ruhezustand überzugehen.[25]
Die Mykobakterien wiederum reagieren auf die Abkapselung mit einer Veränderung ihres Aktivitätszustandes. Seit der Entschlüsselung des Genoms der wichtigsten Mykobakterienstämme 1998[26] sind verschiedene Mechanismen hierzu entdeckt worden. Sie sind dabei in der Lage, ihren Stoffwechsel im Granulom vorübergehend einzustellen oder so umzustellen, dass sie die hier vorkommenden Fette verstoffwechseln und dadurch besonders wenig Sauerstoff benötigen. Sie befinden sich nun im Stadium der Dormanz, d. h., sie teilen sich noch seltener. Aus dieser schlummernden Primärinfektion kann durch erneuten Übergang in einen aktiven Zustand eine (postprimäre) aktive Tuberkulose entstehen. Da eine solche aber auch nachgewiesenermaßen durch eine (exogene) Reinfektion (exogene Neuherdsetzung[27]) entstehen kann, muss man davon ausgehen, dass eine vorangegangene Infektion keinen ausreichenden Schutz vor dem Ausbruch der Erkrankung bei erneutem Kontakt darstellt. Dies macht deutlich, warum es so schwierig ist, einen wirksamen Impfstoff gegen Tuberkulose zu entwickeln.[28]
Grundsätzlich wird der Erkrankungsverlauf bei der Tuberkulose in verschiedene Stadien eingeteilt. Krankheitszeichen, die sich direkt nach der Infektion manifestieren, werden als Primärtuberkulose bezeichnet. Da die Bakterien aber auch bei intakter Immunabwehr ohne Krankheitszeichen oder nach durchgemachter Primärtuberkulose lebenslang im Körper schlummern und jederzeit wieder reaktiviert werden können, spricht man bei einer nicht zur Erkrankung führenden Erstinfektion von einer latenten Tuberkuloseinfektion (LTBI) bzw. nach einer Ersterkrankung von einer postprimären Tuberkulose oder auch Sekundärtuberkulose. Da sich die Infektion zwar zumeist an der Lunge, aber eben prinzipiell auch an jedem anderen Organ abspielen kann, wird außerdem die Lungentuberkulose von der Organtuberkulose unterschieden.
Nach der Ansteckung über infizierte Tröpfchen bilden sich als Reaktion auf die Bakterien in den folgenden drei bis sechs Wochen in der Lunge der betroffenen Person kleine Entzündungen mit Beteiligung des zugehörigen Lymphknotens (Primärkomplex). Die Entzündungsherde (Frühinfiltrate) werden von Blutabwehrzellen eingeschlossen. Es bilden sich kleine Knötchen (Tuberkel). So abgekapselt verursachen die Tuberkuloseherde keine Beschwerden und haben in der Regel auch keinen Anschluss an die Atemwege (das Bronchialsystem). Man spricht bei dieser Primärtuberkulose (früher auch Primärherdphthise oder Erstherdtuberkulose[29]) von einer geschlossenen Tuberkulose, die definitionsgemäß nicht ansteckend ist, da keine Krankheitserreger ausgeschieden werden. Die Mykobakterien können aber jahrelang im Körper überleben.
Ist das infizierte Individuum nicht in der Lage, die Erreger auf diese Weise abzukapseln, kann aber auch eine aktive Infektion mit meist uncharakteristischen Symptomen (B-Symptomatik) auftreten, weil sich die Erreger immer weiter ausbreiten. Dazu können Müdigkeit und Schwäche, Appetitlosigkeit und Gewichtsabnahme, geschwollene Lymphknoten, leichtes Fieber, besonders in den Nachmittagsstunden, Nachtschweiß und ständiges Hüsteln ohne viel Auswurf gehören. Heiserkeit kann ein Hinweis auf eine Kehlkopfbeteiligung mit erhöhter Ansteckungsgefährdung sein. Bei kräftigen Erkrankten können diese Symptome trotz Ansteckungsgefahr schwach ausgeprägt sein und mitunter fehlen. Durch Bildung von Exsudat im Rahmen einer Infektion der Pleura kann es bei der Tuberkulose auch zur Entstehung eines Pleuraergusses kommen (Der pH-Wert der Pleuraflüssigkeit zeigt dann eine Azidose).[30] Schwere Verläufe mit blutigem Auswurf (Hämoptoe), starker Blutarmut und Untergewicht sind auch in Mitteleuropa nicht rar. Die Zahl der Todesfälle an Tuberkulose ist in den letzten Jahrzehnten weit weniger abgeflacht als die Zahl der gesamten Erkrankungsfälle. Seit den 1980er Jahren zeigte sich vor allem ein Rückgang der leichteren geschlossenen Formen.[31]
Kommt es bei geschwächten Personen zu einer Aussaat der Mykobakterien über die Blutbahn mit Beteiligung beider Lungenhälften und vieler Organe gleichzeitig, spricht man von einer Miliartuberkulose. Sie ist nach der im Röntgenbild sich darstellenden miliaren (hirsekornartigen, von lateinisch milium „Hirse, Hirsekorn“) Aussaat[32] (Metastasierung) benannt und stellt sich als schweres Krankheitsbild mit erheblicher Beeinträchtigung des Allgemeinzustandes, Fieber, Appetitlosigkeit, Gewichtsverlust, Husten und Luftnot dar. Auch eine Hirnhautentzündung (tuberkulöse Meningitis) kann auf diesem Weg entstehen. Diese zeigt sich zunächst in uncharakteristischen Symptomen wie Irritabilität und Wesensveränderung. Später kann es zu meningitischen Zeichen mit Kopfschmerzen, Nackensteifheit, Halluzinationen, Bewusstseinsstörungen, Krampfanfällen sowie Fieber, also einer schweren Beeinträchtigung des Allgemeinzustandes kommen. Unbehandelt führt sie zu Koma und Tod. Es können, wie Cohnheim schon 1866 festgestellt hat, als Symptom der akuten Miliartuberkulose zudem Tuberkel der Aderhaut (Chorioidaltuberkel) auftreten.[33][34] Bei extremer Abwehrschwäche kann es zu einer fulminanten Sepsis mit in der Regel tödlichem Ausgang kommen, die vielfach als Landouzy-Sepsis bezeichnet wird.
Bei mindestens zehn Prozent der Menschen, die sich mit Tuberkulose angesteckt haben, bricht die Krankheit zu einem späteren Zeitpunkt als sekundäre Tuberkulose aus.[31] Die Patienten klagen dann oft über verschiedene Symptome: über Wochen anhaltender Husten mit Abhusten von gelblich-grünem Schleim, Abgeschlagenheit, Müdigkeit, subfebrile Temperaturen zum Abend hin und Nachtschweiß. Beim Husten können Schmerzen in der Brust auftreten und es kann zu Atemnot kommen. Blutiger Auswurf kann Ausdruck einer Arrosion der Bronchien oder der Luftröhrenschleimhaut sein (Bronchialtuberkulose, Trachealschleimhauttuberkulose[35]), oft liegt dann bereits eine offene ansteckungsfähige Erkrankung vor. Blutiger Auswurf sollte daher umgehend ärztlich abgeklärt werden.
Die Tuberkulose-Bakterien vermehren sich in der Lunge und zerstören das Gewebe. Das zerstörte Gewebe bekommt bei Arrosion kleinerer oder mittlerer Äste des Bronchialbaumes Anschluss an die Atemwege und wird dann ausgehustet. Der Auswurf enthält jetzt Bakterien – der Patient leidet an offener Tuberkulose. Im fortgeschrittenen Stadium können durch Aussaat der Bakterien über die Blutbahn (hämatogene Streuung) weitere Organe befallen werden. Dann treten beispielsweise schmerzhafte Schwellungen an Knie- und anderen Gelenken (Poncet-Krankheit oder Morbus Poncet, benannt nach dem Chirurgen Antonin Poncet, der das tuberkulöse Rheumatoid[36] bzw. den Rheumatismus tuberculosus 1897[37] beschrieben hatte[38]) oder der Wirbelsäule auf (Gelenktuberkulose, Knochentuberkulose[39]). Eine Sonderform der Tuberkulose ist die in Mitteleuropa früher unter anderem als durch Trinken von roher Milch und der damit verbundenen Infektion mit Mycobacterium bovis[40] angesehene, inzwischen sehr selten gewordene Hauttuberkulose (Lupus vulgaris). Nicht abheilende kleine Wunden, Risse, warzenartige Eiterherde und umschriebene Geschwüre sind u. a. typische Symptome der Hauttuberkulose.
Neben der Beteiligung der Lunge, die mit etwa 80 % das mit Abstand am häufigsten betroffene Organ ist, kann sich die Tuberkulose auch in zahlreichen anderen Organen manifestieren. Diese Organtuberkulose (früher auch Organphthise genannt) kann entweder durch primäre Infektion an anderen Eintrittspforten als den Atemwegen oder aber durch Streuung über die Blutbahn im Rahmen der Primärtuberkulose der Lungen entstehen. Hiervon sind wiederum die Lymphknoten am häufigsten betroffen (Lymphknotentuberkulose).
Zur Diagnosestellung tragen die Erhebung der Infektionsanamnese, eine Tuberkulin-Hauttestung, ein Interferon-γ-Bluttest, Gewebsuntersuchungen, eine bildgebende Diagnostik und wenn irgend möglich der kulturelle Erregernachweis bei. Die Diagnose gilt nur dann als gesichert, wenn die Falldefinitionen, in Deutschland die des Robert Koch-Institutes, erfüllt sind, zum Beispiel wenn neben dem klinischen Bild ein kultureller Erregernachweis von Mycobacterium tuberculosis vorliegt.[51] Diese Methoden können bei speziellen Fragestellungen durch moderne molekularbiologische oder immunologische Testverfahren ergänzt werden. Eine sichere Diagnose wird jedoch durch die höchst unterschiedliche Präsentation erschwert.[52]
Beim Tuberkulin-Hauttest (auch Mendel-Mantoux-Test genannt) wird eine definierte Menge gereinigter und filtrierter Antigene aus Mykobakterien (Tuberkulin) in die Oberhaut (Epidermis) gespritzt. Ebenfalls gebräuchliche Stempeltests sind sehr unzuverlässig und daher nicht empfehlenswert. Hat das Immunsystem des getesteten Menschen schon einmal Kontakt mit Mykobakterien gehabt, tritt an der entsprechenden Stelle innerhalb von drei Tagen eine Abwehrreaktion mit Einwanderung von Abwehrzellen in die Haut ein, die zu einer Verdickung führt. Es handelt sich hier um eine Typ IV Reaktion (nach COOMBS). Bereits sechs Wochen nach einer Infektion mit TBC wird der Test positiv.
Eine tastbare Verhärtung an der Teststelle bezeichnet man als positive Reaktion. Dies kann bedeuten, dass eine Tuberkulose-Infektion stattgefunden hat. Über eine Erkrankung sagt der Test allerdings nichts aus. Auch nach einer Tuberkulose-Schutzimpfung ist eine positive Testreaktion möglich. Bleibt die Haut an der Teststelle unverändert oder zeigt sich nur eine Rötung, wird dies als negativ bewertet. Eine Tuberkulose-Infektion ist dann mit hoher Wahrscheinlichkeit ausgeschlossen.
Der Tuberkulin-Test ist ungefährlich und gut verträglich. Er kann auch bei Schwangeren, stillenden Müttern oder Kleinkindern ohne Bedenken durchgeführt werden.
Tuberkulin-Tests sind nur eingeschränkt verlässlich. Einerseits können sie gerade bei schweren Verläufen, wie einer Miliartuberkulose, negativ bleiben. Andererseits führen eine frühere Impfung oder ein Kontakt zu atypischen Mykobakterien (mycobacteria other than tuberculosis, MOTT) zu einer falsch positiven Reaktion.
Besteht aufgrund von Symptomen und Vorgeschichte der Verdacht auf eine Tuberkulose, so sind auch bei negativem Tuberkulin-Test die Röntgenuntersuchung oder bei besonderen Fragestellungen die CT der Lunge gut brauchbare bildgebende Untersuchungsverfahren. Sie lassen oft das charakteristische, mottenfraßartige Bild des Lungenbefalls der Tuberkulose erkennen, welches der Erkrankung auch den Beinamen die Motten eingebracht hat. Auch bei geschlossener Tuberkulose zeigen diese Untersuchungen einen Befund. Nachteilig ist aber, dass auf einem Röntgenbild oft nicht ausreichend sicher zwischen einer Tuberkulose und anderen Lungenerkrankungen differenziert werden kann.
Bei Kindern unter 15 Jahren und bei Schwangeren sollte bei vermuteter Tuberkulose statt einer Röntgenuntersuchung ein immunologisches Testverfahren wie γ-Interferon-Test oder ein Tuberkulin-Hauttest bevorzugt werden.[53]
Die Diagnose der Tuberkulose ist gesichert, wenn ein kultureller Nachweis der Erreger vorliegt. Dies gelingt aber ohne weiteres nur aus dem Auswurf (Sputum) bei offener Tuberkulose, wenn also die tuberkulösen Gewebeveränderungen Anschluss an das Bronchialsystem, die ableitenden Harnwege oder den Darm haben und ausgeschieden werden können. Andernfalls kann versucht werden, Material durch Punktionen mit Nadeln oder direkt durch Entnahme von Gewebe zu gewinnen. Der Vorteil des kulturellen Nachweises liegt in der Möglichkeit, eine Resistenztestung durchführen zu können und sollte daher immer angestrebt werden, da die Behandlung dann gezielt erfolgen kann.
Da Kinder erstens kaum Auswurf haben und zweitens dabei nur wenige Bakterien hochhusten, ist eine herkömmliche Sputumuntersuchung bei ihnen kaum zuverlässig. Im Kindesalter wird deshalb der Magennüchternsaft untersucht, denn hier sammelt sich das gesamte Sekret, das die Kinder während der Nacht nach oben gehustet und anschließend verschluckt haben. Die Mykobakterien wiederum sind säurefest und im Magensaft überlebensfähig. Südafrikanische Forscher konnten nachweisen, dass der Erregernachweis auch bei Säuglingen und Kindern aus dem Sputum möglich ist. Wenn man sie zuvor mit einer hochprozentigen Salzlösung inhalieren lässt (induziertes Sputum), gelingt der Nachweis von Mykobakterien aus dem anschließend ausgehusteten Sekret mit gleicher Zuverlässigkeit wie aus dem Magensaft.[54]
Auf herkömmlichen, festen Nährböden muss man des langsamen Keimwachstums wegen auf ein Ergebnis vier bis sechs Wochen warten. In Flüssigkulturen mit modernen Nachweismethoden für ein Mykobakterien-Wachstum gelingt der Nachweis möglicherweise schon nach etwa zwei Wochen.[23] Der früher häufig verwendete Tierversuch, bei dem Meerschweinchen das zu untersuchende Material in die Bauchhöhle gespritzt bekamen, wird nicht mehr eingesetzt. Moderne Nachweismethoden schließen molekulargenetische Methoden wie die Polymerase-Kettenreaktion ein.
Als weitere Diagnosemöglichkeit steht neben dem Tuberkulin-Hauttest seit 2005 ein weiteres immunologisches Testverfahren zur Verfügung, der sogenannte γ-Interferon-Test. Dabei werden Abwehrzellen aus dem Blut der Testperson mit einer Mischung aus Antigenen von Mycobacterium tuberculosis stimuliert. Hatte der Betreffende aufgrund einer Tuberkulose-Infektion schon mit dem Erreger Kontakt, so bilden sie vermehrt den Botenstoff Interferon-γ. Die Konzentration dieses Interferon-γ kann im Zellüberstand bestimmt werden und liegt bei Blutproben infizierter Menschen deutlich über derjenigen in einer mitzuführenden Negativkontrolle. Da die gewählten Antigene nur in Mycobacterium tuberculosis vorkommen sollten, nicht aber in den meisten atypischen Mykobakterien und auch nicht in den Impfstämmen der für die BCG-Impfung verwendeten Mykobakterien, lässt sich in der Theorie bei positivem Tuberkulin-Hauttest durch dieses Verfahren zwischen einer Infektion durch Tuberkulose-Bakterien und durch atypische Mykobakterien unterscheiden. Die Sensitivität dieser Tests wird in verschiedenen Arbeiten mit 82 % bis 100 %, die Spezifität mit 98 % angegeben.[51]
Die Durchführung der Tests ist jedoch in Praxis mit Schwierigkeiten und Unsicherheiten verbunden. Das Zeitfenster für die Inkubation und auch die dabei notwendige Temperaturkonstanz von 37 °C bieten Fehlerquellen, ebenso die notwendige Erfahrung mit der Methode im Labor selbst. Die angegebenen Werte für die Sensitivität und Spezifität werden daher in der Praxis bei weitem nicht erreicht. Bei den beiden eingeführten Testen scheint der ELISPOT (speziell der T-SPOT-Test) insbesondere bei Kindern und bei extrem niedriger Anzahl von Helferzellen im Vorteil zu sein. Aber auch hier bestehen Fehlerquellen in der präanalytischen Phase.
Wie bei allen Untersuchungsmethoden, deren Sensitivität und Spezifität ja nicht 100 % beträgt, ist die Aussagekraft auch abhängig von der Häufigkeit der echten Infektion. Deshalb eignen sich auch diese In-vitro-Tests nicht zum Screening von Bevölkerungs- oder Berufsgruppen mit niedriger Durchseuchung. Bei zweifelhafter Exposition und positivem Ausfall[55] oder gesicherter enger Exposition und negativem Ausfall[56] empfiehlt sich daher eine genaue klinische Überprüfung mit der Methode nach Mendel-Mantoux und gegebenenfalls eine Wiederholung mit dem alternierenden Y-Interferontest. Auch der Zeitpunkt der Infektion ist mit den neueren In-vitro-Testverfahren nicht bestimmbar. Der Test wird häufig nach ausgeheilter Infektion in den folgenden Jahrzehnten wieder negativ. Es gibt Berichte über Störungen durch frühere BCG-Impfungen oder Boostereffekte nach vorausgegangener Testung nach Mendel-Mantoux.
2010 stellte eine britische Forschergruppe einen Test vor, der auf Änderung der Transkriptionssignatur in neutrophilen Granulozyten beruht. Durch diesen Test soll es möglich sein, eine abgelaufene von einer aktiven Infektion zu unterscheiden. Die Marktreife bleibt noch abzuwarten.[57]
Da sich die Erreger nur sehr langsam teilen und außerdem in den tuberkulösen Granulomen lange Zeit ruhen können, ist die Gefahr der Resistenzentwicklung bei Mykobakterien besonders hoch. Bei gesicherter Tuberkulose oder auch nur hochgradigem Tuberkuloseverdacht müssen daher alle Patienten mit einer Kombinationstherapie aus mehreren, speziell gegen Mycobacterium tuberculosis wirksamen Antibiotika (auch Antituberkulotika genannten Medikamenten) behandelt werden. Außerdem muss die Behandlungsdauer, ebenfalls wegen der niedrigen Teilungsgeschwindigkeit, unbedingt ausreichend lang sein, um Rückfälle zu vermeiden.
Gemäß der Leitlinien von 2011 soll die Therapie einer unkomplizierten Tuberkulose aus einer vierfachen Kombination von Isoniazid, Rifampicin, Ethambutol und Pyrazinamid bestehen und zunächst für zwei Monate erfolgen. Anschließend muss die Behandlung für weitere vier Monate mit Isoniazid und Rifampicin fortgesetzt werden. Sie dauert also insgesamt mindestens ein halbes Jahr. Bei Kindern wird anfangs nur eine Dreifachkombination (ohne Ethambutol) eingesetzt. Dies ist in besonders leichten Fällen ausnahmsweise auch bei Erwachsenen möglich. Als Reservemedikament bei Unverträglichkeiten steht noch Streptomycin zur Verfügung. Thiacetazon, eine sechste Substanz, wird aufgrund eines ungünstigen Nebenwirkungsprofils in den Industrienationen nicht angewandt. Es wird für die Behandlung von gleichzeitig an HIV erkrankten Patienten nicht empfohlen. Allerdings ist die Mehrzahl der Tuberkulosekranken in einigen armen Ländern, wo die Substanz wegen des günstigen Preises weiterhin zum Einsatz kommt, gleichzeitig HIV-positiv.[58]
Die häufigste Nebenwirkung von Isoniazid ist eine periphere Polyneuropathie. Des Weiteren kann es wie bei Rifampicin und Pyrazinamid auch zu Leberschäden kommen. Ethambutol kann eine Entzündung des Sehnervs (Nervus opticus) hervorrufen, Streptomycin schädigt Nieren und Innenohr. Diese Organe sollten vor Beginn untersucht und im Verlauf der Therapie überwacht werden.
Da sich die Patienten oft relativ gesund fühlen, nehmen viele die Tabletten von sich aus nach gewisser Zeit nicht mehr regelmäßig ein (man spricht hier von einer niedrigen Compliance). Nachdem die Zulassung mehrerer neuer Tuberkulosemedikamente bevorsteht, wird in Mausstudien bereits erprobt, mit welcher Medikamentenkombination eine Verkürzung der Behandlungszeit möglich wäre. Mit der Kombination TMC207, Pyrazinamid und Rifapentin/Moxifloxazin konnten bei der Maus trotz Verkürzung auf zwei Monate 100 Prozent der Keime getötet werden.[59]
Sollte sich in der mikrobiologischen Bakterienkultur eine Resistenz finden, muss im Sinne einer spezifischen Therapie ein Wechsel auf andere Antibiotika ins Auge gefasst werden, gegen die der konkrete Bakterienstamm tatsächlich empfindlich ist. In Flüssigkulturen (Bactec MGIT) kann ein Wachstum von Mykobakterien bei hoher Ausgangskonzentration und ohne Vorbehandlung bereits nach einer Woche nachgewiesen werden. Ein positives Wachstum im Flüssigkultursystem allein erlaubt noch keine Artdiagnose der Mykobakterien, sie stellt jedoch die Basis für eine genaue Speziesdifferenzierung mittels weiterführender diagnostischer Methoden dar. Die parallele Bebrütung fester Nährböden (Löwenstein-Jensen- und Stonebrink-Medium) dauert meist länger, erlaubt aber eine Beurteilung der Koloniemorphologie. Das Endergebnis der Kultur liegt in der Festkultur nach max. 8–10 Wochen vor. Die konventionelle Resistenzüberprüfung der Standardmedikamente dauert mindestens zehn Tage. Es gibt mittlerweile kommerzielle Schnellteste, die mit molekularbiologischen Methoden Resistenzen früher nachweisen können. Diese neueren Methoden haben sich in der Praxis allerdings noch nicht bewährt.
Bei Vorliegen von Resistenzen gegen die Standardmedikamente soll nach Austestung aller zur Verfügung stehenden Antituberkulotika die Behandlung um mindestens zwei wirksame Substanzen erweitert werden.[60] Angewandt werden Kombinationen verschiedener Wirkstoffe: Die Aminoglykoside Capreomycin und Kanamycin, die Fluorchinolone Ofloxacin/Levofloxacin, Ciprofloxacin und Moxifloxacin, die Thionamide Ethionamid, Prothionamid sowie die bakteriostatisch wirksamen Substanzen 4-Aminosalicylsäure (PAS) und Cycloserin bzw. Terizidon.
Zur Sekundärtherapie bei Resistenzen kommen auch Streptomycin, Amikacin und Protionamid in Betracht.[61]
Das Antibiotikum Linezolid galt einige Zeit als Wunderwaffe gegen multiresistente Tuberkulose und findet noch heute bei besonders schweren Fällen Anwendung. Jedoch entwickelten in einer rezenten Studie von Lee[62] 82 % der Patienten möglicherweise Linezolid-assoziierte Nebenwirkungen. Häufigste unerwünschte Wirkungen sind Myelosuppression mit Anämie und Neutropenie, optische Neuropathie und periphere Neuropathie.[63]
Die Behandlung einer multiresistenten Tuberkulose (englisch multidrug-resistant tuberculosis – MDR-TB) bedeutet die Einnahme mehrerer Medikamente gleichzeitig über einen Zeitraum von mindestens 21 Monaten. In den ersten drei Monaten erhalten die Patienten eine Mischung aus fünf verschiedenen Medikamenten. Grundsätzlich sind die Chancen auf eine erfolgreiche Behandlung einer multiresistenten Tuberkulose geringer als bei der Behandlung einer unkomplizierten Tuberkulose, selbst wenn die Patienten die effizienteste Therapie erhalten.
Die Anwendung von Ofloxazin und Levofloxazin ist durch vergleichsweise hohe Produktpreise in ärmeren Ländern kaum durchführbar. Beide Wirkstoffe stehen unter Patentschutz der Hersteller. Capreomycin wird nur von einem einzigen Hersteller (Eli Lilly) vertrieben, zu einem Preis, der die Verwendung enorm einschränkt.
Mittlerweile werden in die Behandlung der multiresistenten Tuberkulose auch neuere Arzneistoffe einbezogen. So wurden im Mausmodell erfolgreich Kombinationen aus Bedaquilin,[64] PA-824 und Sutezolid sowie Rifapentin eingesetzt.[65]
Bedaquilin (Handelsname Sirturo) wurde zwischenzeitlich von der Europäischen Kommission zugelassen.[66][67] Zwei weitere Arzneistoffe – die neue chemische Entität Delamanid (Handelsname: Deltyba; Hersteller: Otsuka Pharmaceuticals) und die seit langem bekannte Substanz 4-Aminosalicylsäure (Handelsname: Granupas; Hersteller: Lucane) – wurden im Mai 2014 europaweit zugelassen.[68][69][70][71]
In einer türkischen Studie führte die zusätzliche Anwendung einer pulmonalen Resektion bei 12 von 13 MDR-TB-Patienten zu einer dauerhaften Heilung.[72]
Thioridazin mindert unter anderem die Aktivität der Antibiotikum-Resistenzen in Mycobacterium tuberculosis,[73] weshalb der Einsatz in Kombination mit Antibiotika untersucht wird.[74][75]
Bei zusätzlichen Komplikationen, wie zum Beispiel Verlegung eines Teils der Atemwege durch einen beteiligten Lymphknoten, soll die Behandlung auf insgesamt neun bis zwölf Monate ausgedehnt werden. Eine Miliartuberkulose oder eine tuberkulöse Hirnhautentzündung (Meningitis) machen eine initiale Vierfachtherapie auch im Kindesalter über dann eher drei Monate und eine Ausdehnung der Gesamtbehandlungsdauer auf neun bis zwölf Monate erforderlich. Außerdem sollen die Patienten für mindestens sechs Wochen mit Prednisolon beziehungsweise bei der Meningitis mit Dexamethason in absteigender Dosierung behandelt werden.[76]
Eine besondere therapeutische Herausforderung stellt die Behandlung der Tuberkulose bei gleichzeitig HIV-infizierten Patienten dar. Insbesondere das Standardmedikament Rifampicin darf wegen erheblicher Wechselwirkungen nicht gleichzeitig mit bestimmten Wirkstoffen, die zur Therapie der HIV-Infektion eingesetzt werden, verabreicht werden. Daher muss von entsprechend erfahrenen Fachärzten entweder die HIV-Therapie oder die tuberkulostatische Therapie umgestellt werden.
Aufgrund der Schwierigkeit und Langwierigkeit der Standardtherapie wurde versucht, durch Ergänzung diverser Stoffe die Therapie zu unterstützen. Am aussichtsreichsten zeigten sich hierbei L-Arginin, eine Aminosäure, die die Bildung von reaktiven Stickstoffspezies in Makrophagen unterstützen soll, so durch täglichen Verzehr von 30 Erdnüssen, die etwa 1 g L-Arginin enthalten,[77] und Vitamin D, dessen unterstützende Rolle bei Infektionen allgemein belegt ist. Beide Stoffe sind in Erkrankten zu wenig vorhanden.[78][79][80]
Im Jahr 1903 errichtete Auguste Rollier eine Anstalt zur heliotherapeutischen Behandlung bestimmter Tuberkuloseformen.[81] Eine hochdosierte Vitamin-D-Therapie bei Patienten mit Lungentuberkulose beschleunigte in einer randomisierten Studie der Queen Mary University of London sogar die mikroskopische Sputumkonversion, d. h. die Zeitdauer, während der die Patienten ansteckend sind, verringerte sich signifikant von 36 auf 23 Tage. Dabei wurde die Konzentration verschiedener entzündlicher Zytokine und Chemokine im Blut vermindert und lebensbedrohliche Entzündungserscheinungen gingen schneller zurück als in der Kontrollgruppe.[82][83]
Weiterhin untersuchten Subbian und andere in einer Studie mit Kaninchen den Einfluss eines PDE-4-Hemmers auf die angeborene Immunantwort in der Lunge und den Effekt, den ihre Abschwächung wiederum auf die Expression verschiedener Bakteriengene hatte. Es zeigte sich eine verminderte Expression der INH-Resistenzgene und, damit zusammenhängend, eine bessere Clearance des Gewebes bei Behandlung mit Isoniazid.[84]
Eine andere Möglichkeit besteht in der Abschwächung der Granulombildung in der Lunge mittels Laktoferrin. Hierzu gab es 2010 noch keine klinischen Studien.[85]
Stationär betreute Patienten mit einer ansteckungsfähigen Lungentuberkulose sind für die Dauer der Ansteckungsfähigkeit in einem Einzelzimmer mit eigener Nasszelle zu isolieren. Die Dauer der Isolierung wird mit mindestens 14 Tagen nach Einleitung einer wirksamen Therapie mit nachgewiesener Sputum-Konversion (bei mikroskopisch positiven Personen) und klinischem und radiologischem Ansprechen als ausreichend angesehen. Personen mit extrapulmonaler Tuberkulose gelten in der Regel als nicht ansteckungsfähig und müssen nicht isoliert werden. Kranke sollen Mund-Nasen-Schutz tragen, medizinisches Personal trägt FFP2-Maske. Das Tragen von FFP3-Masken für bestimmte medizinische Untersuchungen kann mangels Evidenz für eine höhere Schutzwirkung im Vergleich zu FFP2-Masken entfallen. Schutzkittel, Schutzhandschuhe und Schutzbrille sind nicht generell nötig, allenfalls bei gesteigerter Gefahr einer Aerosolbildung (z. B. bei Bronchoskopien). Eine Übertragung über Oberflächen stellt keine relevante Infektionsquelle dar. Daher reicht die tägliche Flächendesinfektion im Krankenzimmer der häufig berührten und personennahen Flächen. Gleiches gilt für Funktionsbereiche. Die Einwirkzeit muss nicht abgewartet werden, da die Abtötung der Erreger logarithmisch erfolgt und die Bakterienlast bereits nach Antrocknung des Desinfektionsmittels deutlich reduziert ist, die Flächen somit wieder benutzt werden können. Das bedeutet z. B. in der Röntgenabteilung, dass nach Desinfektion der Kontaktflächen und Antrocknung des Desinfektionsmittels die Räume wieder genutzt werden können. Anders verhält es sich bei sichtbarer Kontamination, z. B. durch Sputum, Sekret oder Blut im Rahmen einer Bronchoskopie. In diesem Fall ist zunächst die Fläche mechanisch zu reinigen, bevor nachfolgend ein tuberkulozides Desinfektionsmittel unter Einhaltung der Einwirkzeit eingesetzt wird. Ein tuberkulozides Desinfektionsmittel ist ebenfalls unter Einhaltung der Einwirkzeit bei der Entlassung der Patienten im Rahmen der Schlussdesinfektion anzuwenden.[86]
Da es derzeit keinen wirksamen Impfschutz gegen Tuberkulose gibt, besteht die wichtigste vorbeugende Maßnahme darin, infizierte Personen möglichst frühzeitig zu entdecken und sowohl rasch als auch effektiv zu behandeln. Wegen der geringen Fallzahl in Deutschland sind hierzu Reihenuntersuchungen weder in Form von Tuberkulintestungen noch von Röntgenuntersuchungen sinnvoll. Die aktive Suche nach infizierten Personen in Form einer Umgebungsuntersuchung von Kontaktpersonen von Patienten mit infektiöser Tuberkulose ist eine unverzichtbare Voraussetzung zur Verringerung der Erkrankungshäufigkeit. Besonders wichtig ist die Untersuchung und Kontrolle von medizinischem Personal nach Tuberkulosekontakt, weil diese Personen im Fall einer Infektion eine große Zahl von Patienten anstecken können.[87] Zur Gruppe der Personen mit erhöhtem Tuberkuloserisiko, bei denen aktiv nach einer Infektion gesucht werden soll, gehören außerdem zum Beispiel Personen aus Ländern mit hoher Tuberkuloserate, Obdachlose, Drogenabhängige, Gefängnisinsassen, aber auch HIV-Positive.[15] Das persönliche Erkrankungsrisiko kann heute unter Einbeziehungen wichtiger Einflussgrößen algorithmisch eingeschätzt werden. Ein ausgefeiltes, auch Medikamentenunverträglichkeiten berücksichtigendes Beispiel dafür ist der Online TST/IGRA Interpreter, der an der kanadischen McGill-Universität von Dick Menzies und seinen Mitarbeitern entwickelt wurde.[88]
Bis 1998 wurde in Deutschland eine aktive Schutzimpfung (Lebendimpfung) mit dem abgeschwächten Mykobakterien-Impfstamm Bacillus Calmette-Guérin (BCG) gegen die Tuberkulose durchgeführt. Aufgrund der nachlassenden Schutzwirkung, lokaler Komplikationen und Nebenwirkungen und der geänderten epidemiologischen Situation wird die Indikation zur BCG-Impfung seit 1998 in Deutschland nur noch selten gestellt. Bei der Einführung des Impfstoffs kam es 1930 in Lübeck zum Lübecker Impfunglück. Dabei wurden 208 Kinder durch eine fehlerhafte Verarbeitung der aus Paris bezogenen BCG-Kultur zu Impfstoff mit virulenten Tuberkulosebakterien infiziert. 77 von ihnen starben. Durch diesen Impfunfall weiß man aus der Beobachtung dieser Kinder heute viel über den Verlauf der Krankheit. Die Einführung der Impfung in Deutschland wurde dadurch aber bis in die Zeit nach dem Zweiten Weltkrieg verzögert. Die BCG-Impfung wird heute von der Ständigen Impfkommission nicht mehr empfohlen, da die eingeschränkte Wirksamkeit die Impfkomplikationen nicht aufwiegen konnte. Außerdem ist bei geimpften Personen der Tuberkulintest auch nach Jahrzehnten noch gelegentlich leicht bis mäßig positiv. Aus diesem Grund wird dieser Test (etwa bei einem Kontakt mit einer Person, die an offener TBC erkrankte) erst bei einer stärkeren Reaktion in Form einer verhärteten Schwellung (Induration) von mehr als 15 mm Querdurchmesser zur Unterarmachse als positiv gewertet. Auch weltweit konnte die BCG-Impfung die Verbreitung der Tuberkulose nicht eindämmen, obwohl sie zu den am weitesten verbreiteten Impfungen gehört. Lediglich die besonders fulminanten und gefürchteten Verläufe im Kindesalter in Form einer tuberkulösen Meningitis oder einer Miliartuberkulose vermag die BCG-Impfung wohl relativ zuverlässig zu verhindern. Derzeit versuchen verschiedene Forscher, die Wirksamkeit des BCG-Impfstammes durch gentechnische Veränderungen zu erhöhen, indem die Impfbakterien zusätzliche Antigene produzieren, durch die das Immunsystem besser auf die echten Mykobakterien reagieren kann.[28]
Der BCG-Impfstoff ist besonders in den Tropen und in den Subtropen von sehr schlechter Wirksamkeit. Tiermodelle wie auch Impfstudien mit Menschen zeigten, dass die schlechte Wirksamkeit von bereits existierenden Immunantworten auf boden- und (trink-)wasserbewohnende, nichtpathogene Mycobacterium-Arten herrührt. Viele der Mykobakterien besitzen kreuz-reaktive Antigene, so dass eine Infektion mit dem einen Mykobakterium einen gewissen Schutz gegen die Infektion mit dem anderen verleiht. Dies hat Folgen für den Impfschutz: Einerseits bestehen Antikörper gegen den BCG-Lebendimpfstoff – der Impfstoff wird vom Körper vernichtet, bevor er selbst das Immunsystem stimulieren kann. Und zweitens besteht durch die im täglichen Leben aufgenommenen Mykobakterien ein immerhin so guter Schutz gegen das Tuberkulose-Bakterium, dass die Impfung keinen nennenswerten zusätzlichen Schutz bringt. Man vermutet, dass durch die bessere Hygiene und Trinkwasseraufbereitung in Industrieländern dieser natürliche Impfschutz fehlt und dadurch die BCG-Impfung bislang effektiv war.[89]
In den 1970er Jahren wurde in Indien eine Tuberkulose-Impfstoff-Studie an 260.000 Menschen durchgeführt. Diese ergab, dass mehr Fälle von Tuberkulose bei den Geimpften als bei den Ungeimpften auftraten.[90]
Eine Phase-I-Studie mit einem neuen Impfstoff VPM1002[91] wurde in den Jahren 2009 und 2010 in Neuss mit 80 Probanden getestet und mit guter Verträglichkeit bewertet. Der Impfstoff VPM1002 wird in einer Phase-III-Studie bis 2020 an 2000 Menschen in Indien getestet.[92]
Der bisher vielversprechende Impfstoff MVA85A zeigte 2013 in der Phase-II-Studie Schwächen auf. Es gibt bei geimpften gesunden und HIV-negativen Säuglingen keine Verbesserung zum bisherigen Impfstoff. Jedoch ist abzuwarten, ob der Impfstoff bei Erwachsenen oder HIV-Positiven (die den Lebendimpfstoff nicht erhalten dürfen) einen Vorteil bringt.[93] Neben diesem sind derzeit 12 weitere Tuberkulose-Impfstoffe in der klinischen Phase, AERAS-402/Crucell Ad35 und GSK M72 werden aktuell an Erwachsenen und Kindern in Südafrika getestet.[94]
Ende 2010 konnte gezeigt werden, dass im Mausmodell intranasale Impfung mit mRNA (Hsp-65) von M. leprae effektiv und sicher vor Infektion mit M. tuberculosis schützt.[95]
Eine weitere Neuentwicklung ist H4:IC31.[96] Dabei handelt es sich um ein rekombinantes Fusionsprotein H4 und das Adjuvans IC31.
H4 besteht aus den Tuberkel-Antigenen Ag85B und TB10.4. Ag85B wird auch α-Antigen[97] genannt und ist eine Mycolyl-Transferase. TB10.4 ist eines von drei sehr ähnlichen Proteinen der ESAT-6-Gruppe von Mycobacterium tuberculosis. Eine Prüfung in Südafrika an BCG-geimpften Jugendlichen verlief vielversprechend.
Da kleine Kinder unter fünf Jahren nach einer Infektion häufiger und schneller erkranken als Erwachsene (20 % der angesteckten Kinder erkranken nach der Literatur bei einer Mindestlatenzzeit von etwa drei Wochen bis zu Jahren oder gar Jahrzehnten), gelten bei ihnen nach Kontakt zu an Tuberkulose Erkrankten besondere Vorsorgemaßnahmen. Auch bei negativer Tuberkulin-Testung sollten sie nach den Richtlinien der Schweizer Lungenliga für zwei Monate prophylaktisch mit Antituberkulotika (z. B. Isoniazid) behandelt werden. Wenn nach diesen zwei Monaten der Tuberkulin-Test immer noch negativ ist, kann die Behandlung beendet werden. Ist der Tuberkulin-Test in der Zwischenzeit aber positiv geworden, muss zum einen eine aktive Tuberkulose durch eine Röntgenuntersuchung der Lungen ausgeschlossen werden. In Europa geschieht dies durch eine Brustaufnahme; in Übersee wie in Australien empfiehlt man dagegen gerade bei Kindern eine Computertomographie der Brustorgane. Die Behandlung mit dem Antituberkulotikum wird dann für weitere Monate als Chemoprävention fortgesetzt. Wenn die Erreger bei der Infektionsquelle bekanntermaßen resistent gegen das Antituberkulotikum sind, muss die Chemoprophylaxe selbstverständlich mit einem anderen Wirkstoff, vorzugsweise Rifampicin erfolgen. Bei Mehrfachresistenzen soll sie sogar mit zwei verschiedenen wirksamen Substanzen durchgeführt werden.[31]
In Deutschland ist „behandlungsbedürftige“ Tuberkulose beim Menschen eine meldepflichtige Krankheit[98] im Sinne des Infektionsschutzgesetzes (IfSG), auch wenn ein bakteriologischer Nachweis nicht vorliegt (§ 6 Absatz 1 Satz 1 Nummer 1a Buchstabe a IfSG). Erkrankung und Tod sind zu melden. Dem Gesundheitsamt ist darüber hinaus zu melden, „wenn Personen, die an einer behandlungsbedürftigen Lungentuberkulose erkrankt sind, eine Behandlung verweigern oder abbrechen“ (§ 6 Absatz 1 Satz 2 IfSG). Außerdem besteht auch für die Leitungen von Gemeinschaftseinrichtungen eine Benachrichtigungspflicht[99] nach § 34 Absatz 6 IfSG.
Beim Tier ist die Erkrankung in Deutschland eine meldepflichtige Tierkrankheit nach § 26 Tiergesundheitsgesetz (TierGesG) in Verbindung mit § 1 und der Anlage der Verordnung über meldepflichtige Tierkrankheiten.[100] Ausgenommen sind davon Mycobacterium-bovis- inklusive deren Subspezies-Infektionen, die sogar anzeigepflichtige Tierseuchen sind nach § 4 TierGesG in Verbindung mit § 1 der Verordnung über anzeigepflichtige Tierseuchen sind.[101][100]
In Österreich ist Tuberkulose beim Menschen eine meldepflichtige Krankheit gemäß § 3 Tuberkulosegesetz[102]. Danach ist jeder Nachweis eines Tuberkuloseerregers, jede aktive oder ansteckende Tuberkuloseerkrankung sowie jeder darauf zurückzuführende Todesfall meldepflichtig. Zudem ist jeder Verdacht auf eine Tuberkuloseerkrankung zu melden, wenn sich die an der Tuberkulose erkrankte Person der diagnostischen Abklärung entzieht. Es besteht nach § 2 Tuberkulosegesetz auch die Pflicht, sich behandeln zu lassen (Behandlungspflicht).
In Österreich ist Tuberkulose der Rinder anzeigepflichtig nach § 16 Tierseuchengesetz.
In der Schweiz ist Tuberkulose beim Menschen ebenfalls eine meldepflichtige Krankheit und zwar nach dem Epidemiengesetz (EpG) in Verbindung mit der Epidemienverordnung und Anhang 1 der Verordnung des EDI über die Meldung von Beobachtungen übertragbarer Krankheiten des Menschen. Meldungpflichtig sind der Beginn einer Behandlung mit drei verschiedenen Antituberkulotika oder der Nachweis von Mykobakterien des Tuberculosis-Komplexes in klinischem Material.
In der Schweiz ist Tuberkulose als auszurottende Tierseuche im Sinne von Artikel 3 Tierseuchenverordnung (TSV) mit umfassenden Pflichten nach den Artikeln 158–165a[103] meldepflichtig.[104]
Tuberkulosen kommen bei nahezu allen Wirbeltieren vor und können neben M. tuberculosis von zahlreichen anderen Mykobakterien des Mycobacterium tuberculosis-Komplexes ausgelöst werden.
M. tuberculosis kann sowohl bei Haustieren als auch bei Wildtieren (wie zum Beispiel Hirschen oder Springböcken[105]) eine Erkrankung hervorrufen. Beschrieben ist die Infektion aufgrund des engeren Kontakts zum Menschen bei vielen domestizierten Arten, z. B. bei Haushunden, Hauskatzen und Papageien, und bei Zootieren wie Elefanten.[106]
Bei den meisten Tierarten sitzt der Primärherd vor allem in der Lunge, die Erkrankung gleicht also der Lungentuberkulose des Menschen. Bei Schweinen sind nahezu ausschließlich die mesenterialen Lymphknoten betroffen. Bei Rindern verläuft die Infektion mit M. tuberculosis zumeist ohne pathologische Prozesse, von Bedeutung ist aber, dass der Erreger mit der Milch ausgeschieden wird, weshalb die Pasteurisierung der Milch eine wesentliche Maßnahme für die Bekämpfung der Tuberkulose des Menschen war. Rohmilch sollte allenfalls aus tuberkulosefreien Rinderbeständen konsumiert werden.
Die Tuberkulose der Rinder ist von den Tiertuberkulosen für den Menschen am bedeutsamsten. Ihr Erreger, Mycobacterium bovis und Mycobacterium caprae, besitzen zwar eine relativ hohe Wirtsspezifität, können aber auch Erkrankungen bei Menschen und anderen Säugetieren (einschließlich vieler Haustiere und zahlreicher Wildtiere) verursachen, so dass es sich um einen Zoonose-Erreger handelt. Die Tuberkulose der Rinder ist eine anzeigepflichtige Tierseuche.[107] Für Infektionen bei anderen Haussäugetieren als Rindern sowie bei Wildsäugetieren besteht Meldepflicht.[108]
Die Geflügeltuberkulose wird durch M. avium verursacht. Sie war eine der häufigsten Erkrankungen bei Haushühnern, ist heute allerdings selten. Prinzipiell sind alle Vogelarten, aber auch der Mensch, Rinder, Schweine, Schafe, Ziegen, Katzen und vor allem das Kaninchen empfänglich. Die Geflügeltuberkulose zählt zu den meldepflichtigen Tierkrankheiten.[109]
Bei Schlangen ist die Tuberkulose selten und verläuft meist chronisch mit Tuberkelbildung in inneren Organen, der Unterhaut oder im Maul. Hauptsächliche Erreger sind M. thamnopheos, M. marinum und M. chelonae. Bei Echsen ist die Erkrankung ebenfalls selten und verläuft als unspezifische Allgemeinerkrankung oder unter Manifestation in der Haut. Haupterreger sind M. ulcerans, M. marinum und M. thamnopheos. M. ulcerans ist beim Menschen Erreger des Buruli-Ulkus.
Die Fischtuberkulose wird durch M. marinum, M. fortuitum und M. chelonae hervorgerufen und befällt sowohl Süß- als auch Salzwasserfische. M. chelonae kann bei Verfütterung infizierter Fische auch bei Schildkröten geschwürige Veränderungen im oberen Verdauungstrakt, Lungenentzündungen und Hauterkrankungen auslösen.
Die Paratuberkulose ist eine durch M. paratuberculosis hervorgerufene Darmerkrankung der Wiederkäuer.
Zwei unterschiedliche, der Tuberkulose sehr ähnliche Krankheitsbilder, die jedoch nicht durch Mykobakterien verursacht werden, bezeichnet man als Pseudotuberkulose.
Die Pseudotuberkulose der Ziegen und Schafe – seltener sind Rinder, Pferde, Schweine und zunehmend auch Kamele betroffen – ist eine Infektionskrankheit, die durch das mit den Mykobakterien verwandte Bakterium Corynebacterium pseudotuberculosis verursacht wird. Beim Menschen kann es nach massivem Kontakt mit Corynebacterium pseudotuberculosis zu Infektionen mit Lymphknotenentzündungen kommen.[110]
Ebenso wird die bei Hasen, Nagetieren und Vögeln, durch Yersinia pseudotuberculosis hervorgerufe Yersiniose als Pseudotuberkulose bezeichnet, bei Nagetieren auch Rodentiose genannt. Yersinia pseudotuberculosis ist für viele Säugetier- und Vogelarten potenziell pathogen, so auch für den Menschen.
Untersuchungen eines etwa 500.000 Jahre alten Fossils des Frühmenschen Homo erectus aus der Türkei zeigten, dass die Tuberkulose wesentlich früher in der Menschheitsgeschichte auftrat als bislang gedacht. Am Schädeldach fanden sich Spuren einer durch Tuberkulose ausgelösten Hirnhautentzündung (Leptomeningitis tuberculosa). Die Forscher mutmaßen, dass dieser aus Afrika stammende Frühmensch dunkelhäutig gewesen sei und daher im Vergleich zu hellhäutigen Menschen deutlich weniger Vitamin D produzieren konnte, was ihn folglich besonders anfällig für diese Erkrankung gemacht haben könnte.[111][112] Diese allerdings nur auf morphologischen Skelettveränderungen beruhende Annahme einer Erkrankung an Tuberkulose sowie der weitere, nunmehr erstmals molekularbiologisch abgesicherte Befund einer 9000 Jahre alten Probe[113] bestätigen die Annahme der modernen Forschung, dass die Tuberkulose nicht in der Jungsteinzeit im Verlaufe der Domestikation des Viehs von diesem auf den Menschen übersprang, sondern sich während eines langen, wesentlich früher beginnenden Zeitraumes parallel mit dem Homo erectus entwickelte. Auch Skelettüberreste von prähistorischen Menschen, die auf ca. 4000 v. Chr. datiert wurden, zeigen Spuren der Krankheit. Auch handelt es sich nicht etwa um geographisch isolierte Befunde, vielmehr wurde Mycobacterium tuberculosis schon 2001 in Wyoming, Nordamerika in 17.000 Jahre alten Funden nachgewiesen.[114]
Tuberkulöse Zerstörung fand sich in Knochen ägyptischer Mumien von 3000 bis 2400 v. Chr. Vergleichbare Befunde aus Altamerika datieren um 2000 v. Chr. Nach den schriftlichen Überlieferungen gibt es Hinweise auf eine Tuberkuloseepidemie in Indien um 1300 v. Chr.
Im 5. Jahrhundert v. Chr. kennzeichnete Hippokrates die Schwindsucht (griechisch φθίσις phthísis „Schwund, Auszehrung“)[115] als eine der Epidemien, die fast immer tödlich war. Von ihm sind eindrucksvolle Krankheitsbeschreibungen überliefert.[116]
Im frühen Mittelalter spielte die Tuberkulose in Europa aufgrund der dünnen Besiedelung eine untergeordnete Rolle. Stärker war sie lediglich in den wenigen Ballungsgebieten vertreten. Dazu gehörte in erster Linie Byzanz. Die Tuberkuloseopfer der Zeit stammen aus allen Klassen bis hin zu Angehörigen des Kaiserhauses. Die medizinische Literatur aus Byzanz beschreibt zu allen Zeiten tuberkulöse Krankheitsbilder. Wesentliche Neuerungen in der Therapie der Tuberkulose wurden dagegen im Mittelalter nicht eingeführt. Bis in die frühe Neuzeit verharrte man auf dem Stand der hippokratischen Schriften und derer Galens. Eine der wenigen Ausnahmen war im 6. Jahrhundert Alexander von Tralleis aus Lydien, der therapeutische Maßnahmen weiterentwickelte oder ausdifferenzierte.
Nach einem vorausgehenden Ausbruch in Italien in der zweiten Hälfte des 15. Jahrhunderts begann im 17. Jahrhundert die größte und längste geschichtliche Tuberkulosewelle. Sie erreichte ihren Höhepunkt im 18. Jahrhundert und hält nach einem temporären Aufflackern der Epidemie kurz nach dem Ersten und dem Zweiten Weltkrieg in letzten Ausläufern bis heute an.
Hinsichtlich der Auffassung der Pathogenese existierten in den europäischen Ländern des 18. Jahrhunderts unterschiedliche Traditionen. Viele Medizinautoren dieser Zeit betrachteten die Tuberkulose bzw. die nicht immer damit synonyme Schwindsucht[117] (von mittelhochdeutsch swinden „schwinden, abnehmen, abmagern usw.“) als die schlimmste unter den damals bekannten Seuchen. Ursächlich sahen sie eine Ungleichverteilung der Körpersäfte, unheilvolle Ausdünstungen des Bodens, die Verstädterung oder den Verfall der Sitten. Hingegen neigte man in Italien traditionell zu einer contagiösen, d. h. übertragbaren Ursache. Konsequenterweise führte die Republik Venedig Mitte des 18. Jahrhunderts die schriftliche Meldepflicht bei Erkrankungen an der Phthise (Schwindsucht bei Lungentuberkulose) ein. Die persönliche Habe der an der Erkrankung verstorbenen Personen wurde zur Minderung der Ansteckungsgefahr verbrannt.
In England und den nordischen Ländern vertrat man dagegen gemeinhin die Ansicht, die Erkrankung beruhe auf einer hereditären (erblichen) Ursache. Eine große Ausnahme stellte in England die Veröffentlichung Benjamin Martens von 1720 dar: A New Theory of Consumptions, in der Marten 162 Jahre vor Robert Koch die Ursache der Erkrankung einer Infektion durch Mikroorganismen zuschrieb. Martens in kleiner Auflage erschienene Veröffentlichung fand jedoch keine weitere Rezeption. In Frankreich, Deutschland und der Schweiz mischten sich Vertreter beider Schulen. Zum Teil vertrat man hier ausdrücklich die hereditäre Theorie, leitete aber gleichzeitig Maßnahmen zur Minderung einer Ansteckungsgefahr ein.
Eine herausragende Rolle unter den Phthisiologen seiner Zeit nahm Johann Jakob Wepfer in Schaffhausen ein, der bei seinem Studienaufenthalt in Italien die Idee von der kontagiösen Ätiologie der Erkrankung übernommen hatte. Er beschrieb so als erster die Entstehung der Lungencavernen (hanc calamitatem) aus Tuberkeln (tubercula). Seine Arbeiten und Untersuchungen zur Epidemiologie der Tuberkulose gingen in vielem qualitativ über die Leistungen der folgenden zwei Jahrhunderte hinaus. Sie wurden erst posthum 1727 durch den Sohn veröffentlicht und blieben außerhalb eines kleinen Expertenzirkels unbekannt.
Zu Beginn des 19. Jahrhunderts behandelte Thomas Beddoes die „Schwindsucht“ bzw. Lungentuberkulose und andere Erkrankungen der Atemwege mit Inhalation bestimmter Gase.[118]
Aufgrund der Vielzahl ihrer Symptome wurde die Krankheit bis ins 19. Jahrhundert nicht von anderen mit ähnlichen Symptomen wie der heute seltenen Skrofulose abgegrenzt. Erst 1819 erklärte René Laënnec die Einheitlichkeit von Tuberkeln mit Miliarknötchen und (tuberkulösen) Kavernen und erkannte, dass die tuberkulöse Materie sich neben der Lunge auch in anderen Organen bilden kann. Erst 1839 wurde von Johann Lukas Schönlein[119] der einheitliche Krankheitsbegriff Tuberkulose geprägt.
Tuberkulose fand im 19. und frühen 20. Jahrhundert allgemeines Interesse als die endemische Krankheit der städtischen Armen. 1815 wurde in England ein Viertel der Todesfälle und 1918 in Frankreich ein Sechstel der Todesfälle durch Tuberkulose verursacht. In der Altersgruppe der 15- bis 40-Jährigen war um 1880 jeder zweite Todesfall in Deutschland auf diese Krankheit zurückzuführen. Auch in ländlichen Gegenden stellte die Tuberkulose die häufigste Todesursache dar. Von den 2188 in der Liechtensteiner Gemeinde Triesen verzeichneten Todesfällen der Jahre 1831 bis 1930 gingen 15 % auf das Konto der Tuberkulose.[120]
Im 19. Jahrhundert entwickelte sich die Luftkur, bei der die Patienten mehrere Stunden täglich an der freien Luft liegen mussten, zur bevorzugten Therapie für Tuberkulose. Diese fand in eigenen Tuberkulose-Sanatorien (Lungenheilstätten) statt; das erste weltweit errichtete Hermann Brehmer 1855 im niederschlesischen Görbersdorf (heute Sokołowsko, Polen).
Nachdem man auch im Norden erkannt hatte, dass die Krankheit ansteckend ist, wurde die Tuberkulose in den 1880er Jahren in Großbritannien meldepflichtig. Es gab damals Kampagnen zum Vermeiden des Ausspuckens auf öffentlichen Plätzen. Die angesteckten Armen wurden angeregt, in Sanatorien zu gehen, die eher Gefängnissen ähnelten. Trotz des behaupteten Nutzens der Frischluft und der Arbeit im Sanatorium verstarben 75 Prozent der Insassen innerhalb von fünf Jahren (1908).
Neben solchen Maßnahmen, die immer noch dem hygienisch-diätetischen Behandlungskonzept anhingen, gab es im 19. Jahrhundert mit zunehmend besseren chirurgischen Möglichkeiten auch sehr unterschiedliche lokale Behandlungskonzepte. Insbesondere die Pneumothorax-Technik bzw. Pneumolyse und die Thorakoplastik[121] fanden in zahlreichen Varianten verbreitete Anwendung. Bei der Pneumothorax-Technik wurde ein betroffener Lungenflügel künstlich zum Kollabieren gebracht, um die Lunge zum Stillstand und zur Ausheilung der Veränderungen zu veranlassen. Der italienische Mediziner Carlo Forlanini gilt (seit 1892) als Erfinder dieser Kollapstherapie der Lungentuberkulose.[122][123] Diese Technik war aber von geringem Nutzen und wurde nach 1946 allmählich eingestellt. Daneben entwickelten sich immer feinere Resektionsverfahren, mit denen betroffene Lungenabschnitte entfernt wurden. Der international renommierte französische Herzchirurg Théodore Tuffier[124] resezierte bei Tuberkulose 1891 als Erster eine rechte Lungenspitze mittels „Apikolyse“ (Ausschälung der Lungenspitze aus ihren Verwachsungen). Mit seiner Schrift Über die chirurgische Behandlung der Lungentuberkulose (1910) fördert er die moderne Lungenchirurgie.[125][126]
Das Bakterium Mycobacterium tuberculosis beschrieb Robert Koch am 24. März 1882. Er erhielt 1905 für die Entdeckung des Erregers den Nobelpreis für Physiologie oder Medizin. Koch glaubte nicht, dass sich die bovine und menschliche Tuberkulose ähnlich waren, was die Erkennung infizierter Milch als Quelle der Erkrankung verzögerte. Später wurde diese Quelle durch Pasteurisierung beseitigt. Koch braute 1890 einen Glycerin-Extrakt der Tuberkelbazillen als Hilfsmittel zur Behandlung der Tuberkulose und nannte ihn Tuberkulin. Es war bei einer zunächst euphorisch begrüßten Anwendung jedoch nicht wirkungsvoll. Die Beobachtung lokaler Hautreaktionen bei der Anwendung von Tuberkulin führte aber später zur Entwicklung eines Testverfahrens zum Nachweis der Ansteckung respektive Erkrankung durch Clemens von Pirquet 1907, Felix Mendel und Charles Mantoux jeweils um 1910.
1883 wies Robert Koch erstmals Tuberkelbazillen im Gewebe von Lupus vulgaris, einer zu Beginn des 19. Jahrhunderts erstmals als eigenständiges Krankheitsbild beschriebenen Hauterkrankung, nach und zeigte somit deren Ursache als Hauttuberkulose.[127] Im Blut von an akuter Miliartuberkulose Verstorbener wies Anton Weichselbaum im Jahr 1884 Tuberkelbazillen nach.[128] Eine von dem Chirurgen Ferdinand Sauerbruch in München, wo dieser an der Universitätsklinik eine Lupusstation eingerichtet hatte (im ersten Viertel des 20. Jahrhunderts) gemäß den Erfahrungen eines Bielefelder Arztes namens Gerson durchgeführte mineralhaltige, aber kochsalzfreie Diät zeigte offenbar (angeblich in 448 von 450 Fällen) Erfolge bei der Behandlung der Hauttuberkulose.[129] Niels Ryberg Finsen begründete 1896 die Lichttherapie der Hauttuberkulose.[130]
Im Jahr 1903 behandelte Oskar Bernhard (1861–1939) die „chirurgische Tuberkulose“ mit Bestrahlung durch Sonnenlicht.[131] Den ersten Erfolg mit Immunisierung gegen Tuberkulose hatten 1906 Albert Calmette und Camille Guérin mit ihrem BCG-Impfstoff. Er wurde erstmals am 18. Juli 1921 in Frankreich am Menschen angewendet. Nationalistische Strömungen, die das Lübecker Impfunglück für ihre Zwecke nutzten, verhinderten in Deutschland den weitverbreiteten Gebrauch bis nach dem Zweiten Weltkrieg.
Dennoch sank die durch Tuberkulose verursachte Sterblichkeitsrate in den hundert Jahren von 1850 bis 1950 in Europa deutlich von 500 auf 50 pro Jahr, bezogen auf 100.000 Einwohner. Verbesserungen im öffentlichen Gesundheitswesen, vor allem die Einrichtung eines dichten Netzes an Tuberkulosefürsorgestellen ab 1905,[132] verringerten die Erkrankungszahl schon vor Einführung von Antibiotika. Dabei wechselte das Konzept mehrfach. Bis 1945 stand die Heilstättenbehandlung von Frühfällen und leicht erkrankten Fällen im Vordergrund. Aus den Erfahrungen der kriegsbeeinflussten Jahre 1917 bis 1919 wurde 1945 das stationäre Behandlungskonzept grundlegend geändert. Primär kamen nur noch schwer erkrankte und ansteckungsverdächtige Patienten in die Heilstätten. Die Heilstätten wurden apparativ aufgerüstet, um unter Einschluss lungenchirurgischer Verfahren eine Maximalversorgung anbieten zu können.
Mit der Entwicklung des Antibiotikums Streptomycin durch Selman Waksman[133] im Jahr 1943 wurde neben der Prävention die aktive Behandlung möglich. Den Erfolg trübten allerdings häufige Resistenzen der Mykobakterien gegen Streptomycin. Die fast gleichzeitige Herstellung der 1945, befürwortet von Jörgen Lehmann, in die Tuberkulosetherapie eingeführten[134] Paraaminosalicylsäure (PAS) fand zunächst kaum Beachtung, obwohl schon die Kombination dieser beiden Substanzen die Bildung resistenter Stämme erschwert. Ab 1952 fand Isoniazid als weiteres Tuberkulosemedikament zunehmende Verwendung. Die Kombinationstherapie zur Vermeidung von Resistenzbildungen wurde von dieser Zeit an Standard der Tuberkulosebehandlung. Der bis heute anhaltende Durchbruch in der antituberkulotischen Behandlung wurde ab den 1960er Jahren durch das Hinzukommen von Ethambutol und zuletzt Rifampicin erzielt.
Bedingt durch die Abschaffung des öffentlichen Gesundheitswesens im New York der 1970er Jahre kam es dort in den 1980er Jahren zu einer Zunahme an Erkrankungen. Die Zahl derer, die ihre Medikamente nicht einnehmen konnten, war hoch. In der Folge erlitten in New York mehr als 20.000 Menschen eine vermeidbare Infektion mit antibiotikaresistenten Erregerstämmen.
Seit dem Auftreten antibiotikaresistenter Stämme (d. h. resistent gegen mindestens Rifampicin und Isoniazid) in den 1980er Jahren geht die Hoffnung zurück, dass man die Krankheit vollständig ausrotten könnte. So gab es um 1955 in Großbritannien 50.000 Tuberkulose-Fälle. Von 1987 bis 2001 stieg die Zahl Tuberkulosekranker in Großbritannien dann wieder von 5500 auf über 7000 bestätigte Fälle an.
Das Wiederaufleben der Tuberkulose veranlasste die WHO 1993 dazu, einen globalen Gesundheitsnotfall auszurufen. 1996 erklärte sie den 24. März zum Welttuberkulosetag.
In Ländern außerhalb Europas und Nordamerika, wie z. B. Bangladesch bleibt Tuberkulose auch im 21. Jahrhundert weiterhin eines der Hauptprobleme für die Gesundheitsversorgung. Eine erneute Zunahme von Fällen wurde mehrfach mit dem Problem des Klimawandels in Verbindung gebracht.[135]
Aufgrund ihrer enormen Bedeutung spiegelt sich die Krankheit vielfach in der Kunst wider. Manche Künstler verarbeiteten die Konfrontation mit dem frühen (eigenen) Tod. Bereits in der darstellenden Kunst der Ägypter findet sich ab dem mittleren Reich die Darstellung des Gibbus, des markanten äußeren Ausdrucks von Pott’s Disease, der Wirbelsäulentuberkulose. Vergleichbare Darstellungen sind auch aus den altamerikanischen Kulturen überliefert.
Am 1. Dezember 2011 wurde im Rohrbacher Schlösschen in Heidelberg auf dem Gelände der Thorax-Klinik das Museum für Tuberkulose eröffnet. Es besteht zu einem großen Teil aus den Exponaten und der Fachliteratur des früheren Tuberkulose-Archivs in Fulda, das der Lungenfacharzt Robert Kropp leitete; es ist seit Anfang 2012 öffentlich zugänglich.[136][137]
Jedes Jahr findet am 24. März der Welttuberkulosetag statt. Er wurde von der Weltgesundheitsorganisation (WHO) ausgerufen und wird von der Europäischen Arzneimittelagentur (EMA) unterstützt. Er soll das öffentliche Bewusstsein wachhalten und darauf hinweisen, dass die Tuberkulose in vielen Ländern der Welt, überwiegend in Entwicklungsländern, immer noch als Epidemie auftritt. Das Datum erinnert an den Tag, an dem Robert Koch 1882 erklärte, dass er den Erreger der Tuberkulose entdeckt habe.[67][138]

Die Säugetiere (Mammalia) sind eine Klasse der Wirbeltiere. Zu ihren kennzeichnenden Merkmalen gehören das Säugen des Nachwuchses mit Milch, die in den Milchdrüsen der Weibchen produziert wird, sowie das Fell aus Haaren, das sie in Kombination mit der gleichwarmen Körpertemperatur relativ unabhängig von der Umgebungstemperatur macht. Bis auf wenige Ausnahmen (Kloakentiere) sind Säugetiere lebendgebärend. Säugetiere sind an Land am artenreichsten verbreitet, doch bevölkern sie auch Luft und Wasser. Das Verhaltensspektrum der Säugetiere ist breit und flexibel, einige Gruppen zeigen komplexe soziale Gefüge. 
Zur Systematik der Säugetiere wurden, im Jahr 2022, insgesamt 6596 rezente Arten gezählt.[1] Fast 200 neu entdeckte Arten wurden nach 2018 erstmals beschrieben, als zwischen 6399 Spezies innerhalb der Säugetiere unterschieden wurde.[2]
Die Säugetiere werden in drei Unterklassen eingeteilt: die eierlegenden Ursäuger (Protheria), die Beutelsäuger (Metatheria) und die Höheren Säugetiere oder Plazentatiere (Eutheria), zu denen auch der Mensch zählt. Diejenige Richtung der speziellen Zoologie, die sich der Erforschung der Säugetiere widmet, wird als Mammalogie bezeichnet.
Säugetiere zählen zu den Landwirbeltieren (Tetrapoda) innerhalb des Taxons der Wirbeltiere (Vertebrata) und teilen somit die Merkmale dieser Gruppen, die hier nicht einzeln wiedergegeben werden.
Ein Fellkleid aus Haaren ist eines der wichtigsten Merkmale der Säugetiere. Auch wenn manche Arten (zum Beispiel die Wale) praktisch haarlos sind, haben sie sich doch aus behaarten Vorfahren entwickelt und zeigen zumindest in ihrer Embryonalentwicklung Haarwuchs. Die meisten Säugetierarten sind zeit ihres Lebens am überwiegenden Teil des Körpers behaart. Haare bestehen hauptsächlich aus dem Protein Keratin. Die Haare der Tiere können mehrere Funktionen haben:
Säugetiere sind in der Regel durch ein heterodontes Gebiss mit vier verschiedenen Zahntypen charakterisiert, die Schneidezähne (Incisivi), Eckzähne (Canini), und zwei Arten von Backenzähnen (Prämolaren und Molaren). Die Zahl der einzelnen Zahntypen wird mit der Zahnformel wiedergegeben. Ein heterodontes Gebiss ist ein wichtiges Unterscheidungsmerkmal von den homodonten (gleichförmigen) Gebissen der Reptilien und vor allem bei der Einordnung von Fossilien von Bedeutung. Bei den meisten Säugetieren gibt es einen einmaligen Zahnwechsel (Diphyodontie). Zunächst werden Milchzähne angelegt (lacteale Dentition), die später durch die „zweiten“ oder bleibenden Zähne (permanente Dentition) ersetzt werden. Lediglich die Molaren werden nicht ersetzt, sondern kommen erst mit den bleibenden Zähnen.
Eine Reihe von Säugetiergruppen besitzt wurzellose Zähne, die zeitlebens weiterwachsen und durch Abrieb abgenutzt werden. Dazu zählen beispielsweise die Nagezähne der Nagetiere oder die Stoßzähne der Elefanten, des Narwals, des Walrosses und anderer Arten.
Ein Exklusivmerkmal der Säugetiere sind die drei Gehörknöchelchen Hammer (Malleus), Amboss (Incus) und Steigbügel (Stapes). Diese befinden sich im Mittelohr; sie nehmen die Schwingungen des Trommelfells auf und leiten sie an das ovale Fenster des Innenohres weiter.
Stammesgeschichtlich können die Gehörknöchelchen von Bestandteilen ursprünglicher Kiemen- bzw. Kieferbögen abgeleitet werden: Der Steigbügel vom Hyomandibulare, welches bei den Fischen Bestandteil des Suspensoriums und bei anderen Landwirbeltieren als Columella ausgebildet ist, Amboss und Hammer vom Quadratum sowie von einem Teil des durch Knochen ersetzten Meckelschen Knorpels, dem Articulare. Das Trommelfell wird von einem fast ringförmigen Knochen, dem Tympanicum, umschlossen.
Bei den anderen Wirbeltieren bilden Quadratum und Articulare das primäre Kiefergelenk, welches bei den Säugetieren während der fetalen Entwicklung durch ein an anderer Stelle entstehendes, sekundäres Kiefergelenk ersetzt wird. Dieses wird von den Deckknochen Dentale und Squamosum gebildet. Der Übergang vom primären zum sekundären Kiefergelenk wurde funktionell möglich, als die Gelenkachsen beider infolge der Größenzunahme des Gehirns bzw. Hirnschädels bei den Cynodontia in eine Linie zusammenfielen.
Im Zuge ihrer Entwicklungsgeschichte haben die Säugetiere nahezu alle Lebensräume besiedelt und sich dabei in eine Vielzahl von Formen aufgeteilt. Eine Reihe von Arten hat sich an eine aquatische (wasserlebende) Lebensweise angepasst; am spezialisiertesten sind die Wale, deren Körperbau Ähnlichkeiten mit den Fischen aufweist. Die Vordergliedmaßen sind zu Flossen (Flipper) umgestaltet, die Hintergliedmaßen sind rückgebildet und der Schwanz ist zu einer Fluke umgebildet. Bei anderen Taxa wie Robben und Seekühen ist die Anpassung an das Wasser weniger weit fortgeschritten. Die Fledertiere sind neben den Vögeln und den ausgestorbenen Flugsauriern die einzigen Wirbeltiere, die zum aktiven Fliegen fähig sind. Sie weisen stark verlängerte Finger auf, die die Flughaut aufspannen. Daneben hat eine Reihe von Säugetiertaxa unabhängig voneinander Gleitmembranen entwickelt, die ihnen einen passiven Gleitflug ermöglichen: dazu zählen die Riesengleiter, die Gleit- und Dornschwanzhörnchen aus der Gruppe der Nagetiere sowie drei Familien gleitender Beuteltiere (die Gleit-, Ring- und Zwerggleitbeutler). Verschiedenste Säugetiere sind an eine unterirdisch-grabende Lebensweise angepasst. Diese haben einen walzenförmigen Körperbau mit kurzen, oft zu Grabwerkzeugen erweiterten Gliedmaßen entwickelt. Zahlreiche Arten führen eine arboreale (baumbewohnende) Lebensweise – diese sind oft durch greiffähige Pfoten mit opponierbarem Daumen und Greifschwanz charakterisiert. Bewohner von Grasländern und anderen offenen Habitaten weisen oft eine Reduktion der Zehenanzahl und die Herausbildung von verhornten Zehen oder Hufen auf, andere haben stark vergrößerte Hinterbeine und eine springende Fortbewegung entwickelt. Viele Arten, vorwiegend kleinere, versteckt lebende, weisen hingegen einen gedrungenen Körperbau mit kurzen Gliedmaßen auf – darunter zahlreiche Nagetiere und Insektenfresser.
Auch bei der Größe gibt es beträchtliche Unterschiede: Als kleinste Säugetiere gelten die Schweinsnasenfledermaus und die Etruskerspitzmaus, die jeweils nur 2 Gramm Körpergewicht erreichen. Der Blauwal hingegen gilt als das größte Tier, das jemals auf der Erde lebte, und erreicht in Ausnahmefällen bis zu 150 Tonnen Gewicht, was das 75-Millionen-fache der kleinsten Säuger darstellt.
Säugetiere sind weltweit verbreitet, sie finden sich auf allen Kontinenten, in allen Ozeanen sowie auf den meisten Inseln. Ursäuger sind auf Australien und Neuguinea beschränkt, Beutelsäuger leben einerseits auf dem australischen Kontinent und Südostasien östlich der Wallace-Linie und andererseits in Nord-, Mittel- und Südamerika. Höhere Säugetiere haben eine weltweite Verbreitung, waren aber bis zur Ankunft des Menschen in Australien nur durch relativ wenige Arten vertreten, namentlich Fledertiere und Echte Mäuse. Auf abgelegenen Inseln gab es bis zur Ankunft des Menschen nur eine eingeschränkte Säugetierfauna; so waren auf vielen Inseln, darunter Neuseeland, Fledertiere die einzigen Säuger.
Säugetiere haben nahezu alle Regionen der Erde besiedelt und kommen in den meisten Lebensräumen vor. Man findet sie in Wüsten und Wäldern, im Hochgebirge und auch in den Polarregionen. Zu den wenigen Regionen, in denen sich (zumindest bis auf zeitweilige Aufenthalte des Menschen) keine Säuger finden, zählt das Innere des antarktischen Kontinents. Mehrere Gruppen von Säugetieren, die Meeressäugetiere, haben sich dem Leben im Meer angepasst; in der Tiefsee finden sich allerdings nur wenige spezialisierte Walarten.
So unterschiedlich die Säugetiere in Bezug auf ihren Körperbau und ihre Lebensräume sind, so unterschiedlich sind auch ihre Lebensweisen. Es finden sich tag-, dämmerungs- und nachtaktive sowie kathemerale (sowohl am Tag als auch in der Nacht aktive) Arten. Auch im Sozialverhalten gibt es beträchtliche Unterschiede: neben strikt einzelgängerischen Arten gibt es andere, die in Gruppen von bis zu Tausenden von Tieren zusammenleben. Manche Arten haben komplexe Verhaltensmuster entwickelt, sie etablieren eine strenge Rangordnung innerhalb der Gruppe und kommunizieren untereinander mittels Lauten, Gesten oder Körperhaltungen. Obwohl es die Ausnahme ist, so gibt es auch Säugetiere, die Gifte zur Verteidigung oder zur Jagd einsetzen (siehe: Giftige Säugetiere).
Einige Säugetiere vermeiden klimatisch extreme Zeiten und den damit verbundenen Nahrungsmangel, indem sie in einen Winterschlaf oder einen Torpor (Starrezustand) verfallen, etwa in kalten oder trockenen Jahreszeiten. Dabei fällt die Körpertemperatur nahezu auf die Umgebungstemperatur ab, Atmung und Herzschlag verlangsamen sich und der Stoffwechsel wird reduziert.
Der Geruchssinn spielt eine bedeutende Rolle in der Lebensweise der Säugetiere, unter anderem bei der Nahrungssuche und bei der Fortpflanzung, wo Pheromone die Paarungsbereitschaft signalisieren. Auch für das Territorialverhalten ist der Geruch bedeutend, etliche Arten markieren ihr Territorium mittels Urin, Kot oder spezieller Drüsensekrete.
Im Allgemeinen ist bei Säugetieren das Gehör gut entwickelt. Eine Sonderform ist die Echoortung, bei der anhand des zurückkehrenden Echos ausgesandter Schallwellen die eigene Position bestimmt oder Beute lokalisiert werden kann. Bei zwei Taxa, den Zahnwalen und den Fledermäusen, ist die Echolokation besonders ausgeprägt, sie findet sich aber auch bei anderen Gruppen.
Auch der Tastsinn dient der Wahrnehmung der Umwelt. Viele Arten haben zu diesem Zweck spezielle Tasthaare (Vibrissae) entwickelt, die außerordentlich empfindlich sind und durch Muskelbewegungen gesteuert werden können. Auch die Haut selbst ist ein Sinnesorgan, bestimmte Körperteile sind besonders reich an Mechanorezeptoren, zum Beispiel die Fingerspitzen der Primaten oder die Nasen- beziehungsweise Rüsselregion vieler Arten. Der bestentwickelte Tastsinn aller Säuger wird im Allgemeinen dem Sternmull zugesprochen. Erwähnt seien in diesem Zusammenhang noch die feinen Elektrorezeptoren im Schnabel der Kloakentiere, die auf die Muskelbewegung der Beutetiere reagieren. Auch in der sozialen Interaktion ist der Tastsinn oft bedeutend, zum Beispiel bei der von vielen Tieren praktizierten gegenseitigen Fellpflege („Grooming“).
Die Bedeutung des Gesichtssinnes ist stark unterschiedlich. Oft spielt er jedoch nur eine untergeordnete Rolle, insbesondere bei unterirdisch lebenden Tieren, deren Augen oft rückgebildet sind. Große Augen und ein relativ gutes Sehvermögen haben dagegen beispielsweise die Katzen und die Primaten. Auch die Position der Augen ist ausschlaggebend: während Räuber meist nach vorne gerichtete Augen haben, die ein räumliches Sehen und somit eine genauere Entfernungsabschätzung ermöglichen, sind die Augen von Beutetieren oft seitlich angebracht, was einem nahezu vollständigen Rundumblick und der frühestmöglichen Erkennung von Gefahren dient.
Eine Gemeinsamkeit aller Säugetiere ist der verglichen mit anderen Tieren gleicher Größe hohe Energie- und demzufolge Nahrungsbedarf, der eine Folge der gleich bleibenden Körpertemperatur ist. Einige Arten verzehren täglich nahezu Nahrung im Ausmaß ihres eigenen Körpergewichtes. Bei der Art der Nahrung gibt es eine gewaltige Bandbreite, es finden sich Pflanzenfresser (Herbivoren), Fleischfresser (Carnivoren) und ausgeprägte Allesfresser (Omnivoren). Die Anzahl und der Bau der Zähne sowie die Ausgestaltung des Verdauungstraktes spiegeln die Ernährungsweise wider. Fleischfresser haben einen kurzen Darm, um die rasch entstehenden Fäulnisgifte ihrer Nahrung zu vermeiden. Pflanzenfresser, deren Nahrung im Allgemeinen schwerer verdaulich ist, haben eine Reihe von Strategien entwickelt, um die Inhaltsstoffe bestmöglich verwerten zu können. Dazu gehören unter anderem ein längerer Darm, ein mehrkammeriger Magen (zum Beispiel bei Wiederkäuern oder Kängurus) oder die Caecotrophie, das nochmalige Verzehren des Kotes bei Nagetieren und Hasen. Rein blätterfressende (folivore) Arten (zum Beispiel Koalas oder Faultiere) nutzen ihre nährstoffarme Nahrung bestmöglich aus, indem sie ausgesprochen lange Ruhephasen einlegen.
Eine Form des Lernverhaltens ist die Prägung, bei Säugetieren ist die olfaktorische Prägung, das heißt die Sensibilisierung für verschiedene Gerüche, häufiger als bei anderen Wirbeltiergruppen. Oft dient die Prägung zur Erkennung von Verwandten, etwa der Mutter oder den Geschwistern. Mit prägungsähnlichen Erfahrungen kann auch die Nahrungspräferenz bestimmt werden. Gelernte Aktionen können auch tradiert, das heißt weitergegeben werden. Voraussetzung dafür ist das Leben in Gruppen mit Sozialstrukturen. Die meisten Säugetiere zeigen in der Jugendphase Spielverhalten, manche sogar bis ins hohe Alter. Häufig kommt es zu Sozialspielen mit Spielpartnern, in denen beispielsweise von fleischfressenden Tieren das Anschleichen an die Beute oder bei Huftieren die Flucht eingeübt wird. Oft erfolgen anschließend Rollenwechsel von Angreifern und Verteidigern. Auch Objektspiele kommen vor, indem Gegenstände berührt oder in Bewegung versetzt werden.
Die meisten Säugetierarten sind entweder polygyn (ein Männchen paart sich mit mehreren Weibchen) oder promiskuitiv (Männchen und Weibchen paaren sich mit mehreren Partnern). Da das Tragen und das Säugen für die Weibchen zeit- und energieintensiv sind, könnten die Männchen mehr Jungtiere zeugen als die Weibchen gebären können. Daraus ergibt sich in vielen Fällen ein polygynes Verhalten, bei dem sich relativ wenige Männchen mit vielen Weibchen fortpflanzen und sich vielen Männchen keine Paarungsmöglichkeit bietet. Eine Folge davon sind oft heftige Rivalenkämpfe zwischen den Männchen um das Paarungsvorrecht und in manchen Fällen eine Wahlmöglichkeit seitens des Weibchens. Daraus resultieren bei vielen Säugetieren komplexe Verhaltensweisen oder anatomische Merkmale in Hinblick auf die Fortpflanzung. Viele Arten sind durch einen Geschlechtsdimorphismus (Männchen sind oft deutlich größer und schwerer als Weibchen) charakterisiert, auch als eine Folge des Selektionsdruckes der Männchen im Hinblick auf eine Verbesserung der Paarungschance.
Schätzungen zufolge leben drei Prozent aller Säugetierarten in monogamen Beziehungen, in welchen sich ein Männchen während der Paarungszeit nur mit einem einzigen Weibchen fortpflanzt. In diesen Fällen beteiligt sich das Männchen meistens zumindest teilweise an der Jungenaufzucht. Manchmal hängt das Paarungsverhalten auch von den Umweltbedingungen ab: bei knappen Ressourcen paart sich das Männchen nur mit einem Weibchen und hilft bei der Aufzucht mit, bei Nahrungsreichtum kann das Weibchen das Jungtier allein großziehen und die Männchen paaren sich mit mehreren Partnerinnen.
Die Polyandrie (ein Weibchen paart sich mit mehreren Männchen) findet sich nur selten im Säugetierreich, zum Beispiel bei manchen Krallenaffen. Bei diesen Tieren kümmert sich hauptsächlich das Männchen um den Nachwuchs.
Erwähnt seien noch manche Arten der Sandgräber, einer in Afrika lebenden Nagetiergruppe, wie der Nackt- oder der Graumull. Diese pflegen eine eusoziale Lebensweise: Ähnlich wie bei manchen Insekten ist in einer Kolonie ein einziges Weibchen, die „Königin“, fruchtbar und paart sich mit mehreren Männchen, während die übrigen Tiere als unfruchtbare Arbeiter die notwendigen Tätigkeiten zur Versorgung der Gruppe verrichten.
Die Gebärweise unterscheidet sich bei den drei Unterklassen der Säugetiere am augenfälligsten.
Merkmal der Ursäuger ist eine gemeinsame Körperöffnung für die Ausscheidungs- und Fortpflanzungsorgane, die Kloake. Der Penis der Männchen ist ausschließlich samenführend und an der Spitze gespalten. Die Ursäuger unterscheiden sich von allen anderen Säugetieren darin, dass sie nicht lebendgebärend sind, sondern Eier legen. Diese sind klein (rund 10 bis 15 Millimeter Durchmesser) und ähneln mit ihrer ledrigen Schale und dem großen Dotter mehr Reptilien- als Vogeleiern. Die ein bis drei Eier werden vom Weibchen rund zehn Tage lang bebrütet. Neugeschlüpfte Ursäuger sind nackt und klein und sind in ihrem embryoartigen Zustand mit neugeborenen Beuteltieren vergleichbar. Ein Beispiel für Ursäuger ist das Schnabeltier (Ornithorhynchus anatinus), das an der Ostküste Australiens beheimatet ist.
Die Beutelsäuger unterscheiden sich im Bau der Fortpflanzungsorgane deutlich von Höheren Säugetieren. Bei ihnen ist der Fortpflanzungstrakt verdoppelt, Weibchen haben zwei Uteri und zwei Vaginae, auch die Männchen besitzen einen gespaltenen oder doppelten Penis mit davorliegendem Scrotum. Die Tragzeit ist kurz (12 bis 43 Tage), Rekordhalter ist die Schmalfußbeutelmaus Sminthopsis macroura mit nur 10,5 bis 11 Tagen. Die meisten Arten entwickeln keine Plazenta, allerdings ist bei manchen Beutelsäugern (zum Beispiel Koalas oder Nasenbeutlern) ein primitiver Mutterkuchen vorhanden. Die Neugeborenen kommen durch einen zwischen den Vaginae liegenden Geburtskanal zur Welt, der bei vielen Arten eigens für die Geburt angelegt wird. Neugeborene Beutelsäuger sind klein und im Vergleich zu den Höheren Säugetieren unterentwickelt. Das Gewicht des Wurfes beträgt stets weniger als 1 % des Gewichts der Mutter, die Babys der Rüsselbeutler wiegen gar nur fünf Milligramm und sind somit die kleinsten neugeborenen Säugetiere überhaupt. Neugeborene Beutelsäuger haben erst rudimentär entwickelte Organe, lediglich die Vordergliedmaßen sind gut entwickelt, da der Nachwuchs aus eigener Kraft zu den Zitzen der Mutter krabbeln muss.
Viele, aber bei weitem nicht alle Beutelsäuger besitzen einen Beutel, in welchem sich die Zitzen befinden. Die Weibchen mancher Arten haben einen permanenten Beutel, bei anderen wird er erst während der Tragzeit ausgebildet, wieder bei anderen hängen die Jungtiere frei an der Zitze der Mutter, lediglich durch ihr Fell oder Hautfalten verborgen. Neugeborene hängen sich mit dem Mund an die Zitze und bleiben die ersten Lebenswochen fix damit verbunden. Die Säugezeit dauert im Vergleich zu den Höheren Säugetieren länger.
Früher wurde die Gebärweise der Beutelsäuger als eine primitive, im Vergleich zu den Höheren Säugetieren unterentwickelte Methode betrachtet. Auch die Verdrängung mancher Beuteltiere durch eingeschleppte Plazentatiere hat zu diesem Vorurteil beigetragen. Abgesehen davon, dass dieses „Fortschrittsvorurteil“ hin zur Entwicklung des Menschen in der modernen Systematik weitgehend abgelöst wurde und etliche Beuteltierarten ihr Verbreitungsgebiet sehr erfolgreich ausgedehnt haben, bietet die Fortpflanzungsmethode der Beutelsäuger auch Vorteile: zum einen ist die für die Mutter anstrengende Tragzeit verkürzt, zum anderen kann weit schneller als bei Plazentatieren erneut ein Jungtier zur Welt gebracht werden, sollte das früher geborene sterben.
Die Höheren Säugetiere oder Plazentatiere umfassen bei weitem die meisten Arten. Beide deutsche Namen für dieses Taxon sind aber etwas unglücklich gewählt: Das Wort „höher“ spiegelt einen Fortschritt wider, der in der modernen Systematik nicht haltbar ist, und auch manche Beutelsäuger haben eine einfache Plazenta.
Schlüsselmerkmal der Höheren Säugetiere ist der Trophoblast (die äußere Zellschicht eines befruchteten Eis). Diese Schicht stellt eine immunologische Barriere dar und ermöglicht ein langes Heranwachsen im Mutterleib. Beutelsäuger haben keinen Trophoblast, die Tragezeit muss beendet sein, bevor die Immunabwehr der Mutter voll wirksam wird. Die Plazenta der Höheren Säugetiere ist durch das Allantochorion (eine Zottenhaut) charakterisiert. Die Zotten (Villi) sorgen für eine effizientere Ernährung des Keimes.
Die Dauer der Schwangerschaft und die Anzahl der Neugeborenen ist auch von der Lebensweise abhängig. Nesthocker (zum Beispiel Raubtiere oder Nagetiere) haben eher eine kurze Tragzeit und eine hohe Wurfgröße, während Nestflüchter (zum Beispiel Paarhufer und Wale) eine lange Tragzeit und eine niedrige Wurfgröße aufweisen. So beträgt die Trächtigkeitsdauer bei manchen Hamsterarten nur 16 Tage, während sie bei Afrikanischen Elefanten bis zu 25 Monate dauern kann.
Das namensgebende Merkmal der Säugetiere ist, dass das Weibchen die neugeborenen Kinder mit Milch ernährt, einer Nährflüssigkeit, die in Milchdrüsen produziert wird. Diese setzen sich aus äußerlich abgrenzbaren Drüsenkomplexen („Mammarkomplex“) zusammen, von denen jeder meist in einer Warze endet, die Zitze, beim Menschen auch Brustwarze, genannt wird. Eine Ausnahme bilden die Ursäuger, wo die Neugeborenen die Milch direkt von den Milchdrüsenfeldern aus dem Fell der Mutter lecken. Die Anzahl der Drüsenkomplexe ist je nach Art unterschiedlich und hängt mit der durchschnittlichen Wurfgröße zusammen, so haben Menschen oder Pferde nur zwei, Große Tenreks hingegen 24 oder bis zu 32. Die Ernährung mit Milch wird als Säugen beziehungsweise beim Menschen als Stillen bezeichnet und solange durchgeführt, bis das Jungtier fähig ist, feste Nahrung zu sich zu nehmen.
Das Säugen hat große Konsequenzen für Jungtiere und Weibchen. Neugeborene erhalten ohne viel Aufwand eine fett- und nährstoffreiche Nahrung, die ein schnelles Wachstum gewährleistet, sind aber im Gegenzug auf die Präsenz der Mutter angewiesen. Ein Ammenverhalten, das heißt, dass Weibchen auch fremde Kinder säugen, ist nur von wenigen Arten (zum Beispiel Löwen) bekannt. Mit dem Säugen gehen in den meisten Fällen auch eine intensive Brutpflege und ein fürsorgliches Verhältnis zu den Jungen einher. Für die Weibchen wiederum bedeutet das Säugen, viel Zeit und Energie investieren zu müssen.
So unterschiedlich die Gestalt und Lebensweise der Säugetiere ist, so unterschiedlich ist auch ihre Lebenserwartung. Generell leben kleinere Arten weniger lang als größere Arten, die Fledertiere bilden jedoch eine Ausnahme von diesem Muster. Während männliche Breitfuß-Beutelmäuse durchweg im Alter von rund elf Monaten sterben, nachdem sie sich das erste Mal fortgepflanzt haben, können größere Säugerarten mehrere Jahrzehnte alt werden. Von den an Land lebenden Arten kommt keine an das Alter des Menschen heran, bei dem durch die Verbesserung der Medizin mittlerweile ein Höchstalter von 122 Jahren (Jeanne Calment) belegt ist. Neben dem Menschen dürften die Elefanten mit bis zu 80 Jahren die Landsäugetiere mit der höchsten Lebenserwartung sein. Allerdings werden manche Walarten deutlich älter, das bisher älteste bekannte Säugetier war ein Grönlandwal mit 211 Jahren.
Anmerkung: Obwohl auch der Mensch zoologisch zu den Säugetieren gehört, wird er selbst im Folgenden nicht behandelt. Stattdessen wird das Verhältnis des Menschen zu den übrigen Säugetieren thematisiert.
Säugetiere haben die menschliche Geschichte entscheidend mitgeprägt. Schon seit jeher haben Menschen ihr Fleisch gegessen und ihr Fell und ihre Knochen verarbeitet. Sie wurden als Reit- und Arbeitstiere eingesetzt; bis heute werden sie als Milchlieferanten, als Wach- und Labortiere verwendet. Umgekehrt haben auch die Menschen prägenden Einfluss auf die meisten Säugetierarten. Manche Gattungen haben im Gefolge des Menschen ihr Verbreitungsgebiet drastisch vergrößert oder sind als Neozoen in fremden Regionen eingebürgert worden. Vielfach jedoch sind durch Bejagung und Zerstörung des Lebensraumes ihre Populationen eingeschränkt und ihr Verbreitungsgebiet drastisch verringert worden. Eine ganze Reihe von Säugern ist schließlich durch direkten oder indirekten menschlichen Einfluss unwiederbringlich von der Erde verschwunden.
Eine Reihe von Säugetierarten wird vom Menschen wegen ihres, meist wirtschaftlichen, Nutzens gehalten. Zu diesem Zweck domestizierte Tiere werden als Nutztiere bezeichnet. Es werden darüber hinaus Wildtiere gejagt oder halbdomestizierte Tiere im Freiland gehalten und später gefangen (Beispiele sind Hutewälder oder die Rinder- und Pferdezucht in Amerika).
Aus vielen der oben genannten Gründe beschränkte sich der Mensch nicht nur auf die Jagd, sondern versuchte auch, gewisse Tierarten in seiner Nähe zu halten und nachzuzüchten. Die Domestizierung von Nutztieren begann zumindest vor rund 10.000 bis 15.000 Jahren, beim Haushund deuten genetische Studien allerdings an, dass dieser Prozess schon vor mehr als 100.000 Jahren begonnen haben könnte. Im achten Jahrtausend v. Chr. dürften bereits Wildziege, Wildschaf und Wildrind, etwas später auch das Wildschwein zu Hausziege, Hausschaf, Hausrind und Hausschwein domestiziert worden sein. Nutztiere dienten zunächst vorwiegend als Nahrungsmittellieferanten, später wurden dann auch Tiere zur Arbeitstätigkeit eingesetzt, so seit rund 3000 v. Chr. das Hauspferd und das Lama. Der Prozess der Domestizierung verlief vielschichtig, genetische Studien deuten an, dass bei vielen Haustieren in unterschiedlichen Regionen dieser Schritt mehrmals unabhängig voneinander vonstattenging. Weitere domestizierte Säugetiere sind Rentier, Dromedar, Hauskatze, Frettchen, Esel, Farbmaus, Farbratte, Goldhamster, Kaninchen und Meerschweinchen.
Als Schädlinge werden Tierarten bezeichnet, die dem Menschen gegenüber Schaden anrichten. Der Begriff ist abhängig von Wertvorstellungen und vor allem der wirtschaftlichen Perspektive und daher kein Begriff der Biologie.
Eine Reihe von Säugetieren gilt als Landwirtschafts- oder Nahrungsmittelschädlinge, das heißt, sie ernähren sich entweder direkt in den zur Nahrungsmittelproduktion genutzten Gebieten oder an Aufbewahrungsorten von den vom Menschen produzierten Nahrungsmitteln. Durch die großflächige Einführung von Agrarflächen kommt es zu einem Überangebot an Nahrung für manche Tierarten, das in deren starker Vermehrung und somit weiterer Schädigung resultiert. Vor allem in Entwicklungsländern lässt sich dieser Trend beobachten. Zu den in Mitteleuropa bekanntesten Nahrungsmittelschädlingen zählen Mäuse, insbesondere die Hausmaus und Ratten wie die Haus- oder Wanderratte, die sich als Kulturfolger dem Menschen angeschlossen haben und eine weltweite Verbreitung erlangt haben. Einige Tiere (darunter Flughunde und zahlreiche Nagetierarten) ernähren sich direkt von den Feldfrüchten, andere sorgen durch ihre unterirdische Lebensweise für Schäden an den Wurzeln. Die Viehwirtschaft sieht in fleischfressenden Tieren, vor allem Raubtieren eine Nahrungskonkurrenz, zumindest zwei Arten, der Falklandfuchs und der Beutelwolf sind durch Bejagung ausgestorben. In analoger Weise sieht die Fischerei Robben und andere fischfressende Säuger als wirtschaftliche Gefahr und verfolgt sie.
Das Ausmaß der tatsächlichen Bedrohung, die als „Schädlinge“ bezeichnete Tiere anrichten, ist ungewiss und dürfte oft übertrieben dargestellt werden. Häufig ist der Mensch die Hauptursache dafür, indem er massiv in den natürlichen Lebensraum der Tiere eingreift. Durch die Umwandlung der Habitate in landwirtschaftlich genutzte Flächen und die Verringerung des Nahrungsangebotes werden viele Arten gezwungen, sich neue Nahrungsquellen zu erschließen. Diese stehen dann in Konkurrenz zu den wirtschaftlichen Interessen und leiten die Verfolgung ein. Trotzdem wird mit exzessiven Bejagungen, Vergiftungen und mit anderen Methoden Jagd auf diese „Schädlinge“ gemacht, was sich oft fatal auf die Population auswirkt.
Menschen sind manchmal auch direkten Bedrohungen durch die Säugetiere ausgesetzt. Im Bewusstsein verankert sind dabei vorwiegend die Fälle der großen menschenfressenden Raubtiere, wobei insbesondere der Tiger einen Ruf als „Menschenfresser“ genießt. Tötungen durch Raubtierbisse beschränken sich jedoch auf wenige Einzelfälle im Jahr. Ungleich gefährlicher sind Säugetiere jedoch als Krankheitsüberträger. So sterben jedes Jahr 40.000 bis 70.000 Menschen an der Tollwut, die meisten davon in unterentwickelten Ländern. Hauptübertragungsursache ist der Biss durch infizierte Tiere wie Hunde, Katzen, Dachse, Waschbären und Fledermäuse. Eine weitere berüchtigte Krankheit ist die Pest, die durch auf Hausratten und anderen Nagetieren parasitierende Flöhe, in seltenen Fällen auch direkt übertragen wird. Pest-Epidemien und -Pandemien kosteten Millionen Menschen das Leben, bei der als Schwarzer Tod bekannten Pandemie Mitte des 14. Jahrhunderts starben schätzungsweise ein Drittel der Menschen in Europa.
Viele Säugetiere spielen in der Kulturgeschichte eine bedeutende Rolle. Auffallend große, starke oder gefährliche Tiere dienen als Wappentiere, als Totem- oder Clansymbole. Als „Heilige Tiere“ gelten manche Arten als Manifestationen von Göttern und genossen besonderen Schutz, so heilige Kühe und Hanuman-Languren in Indien oder Katzen und Schakale im alten Ägypten. Auf der anderen Seite wurden manche Säugetiere als Vertreter dämonischer Mächte gesehen, so Fledermäuse oder Katzen. Stereotype Vorstellungen von Eigenschaften bestimmter Tierarten, wie der sture Esel oder der schlaue Fuchs finden sich in zahllosen Erzählungen und Märchen und prägen zum Teil bis heute den Schimpfwortschatz.
Durch vielfältige Eingriffe in die Natur ist der Mensch für den Populationsrückgang oder das Aussterben vieler Säugetierarten verantwortlich. Inwieweit die Bejagung für das Aussterben zahlreicher Großsäuger am Ende des Pleistozäns (vor 50.000 bis 10.000 Jahren) schuld ist, ist umstritten, dieses Aussterben korreliert zumindest teilweise mit der weltweiten Ausbreitung des Menschen (siehe dazu auch den Punkt unter Entwicklungsgeschichte). Aus Berichten und Darstellungen lässt sich zumindest ein deutlicher Schwund des Verbreitungsgebietes für zahlreiche Spezies seit der Antike ableiten. Auch die heutige Situation ist für viele Säugetierarten besorgniserregend. So kommt eine unter der Federführung der International Union for Conservation of Nature (IUCN) stehende Kommission aus rund 1.700 Wissenschaftlern aus 130 Ländern zu dem Ergebnis, dass heute mindestens 20–25 % – unter Umständen aber bis zu 36 % – aller Land- und Meeressäugetierarten vom Aussterben bedroht sind.[3][4][5] Die IUCN listet 514 Arten, also rund 10 %, als stark bedroht (critically endangered) oder bedroht (endangered), insgesamt sind mindestens 1.141 der derzeit 5.487 rezenten Säugetierarten akut bedroht. Drei Arten, das Przewalski-Pferd, die Säbelantilope und der Schwarzfußiltis, gelten als in freier Wildbahn ausgestorben (extinct in the wild), das heißt, es gibt nur mehr die Bestände in menschlichen Zuchtprogrammen. Die Gründe für die Gefährdung zahlreicher Arten liegen hauptsächlich im zunehmenden Verlust des Lebensraumes durch Umwandlung in landwirtschaftlich genutzte Gebiete und Siedlungen, in der Umweltverschmutzung und in der Bejagung, da man viele Arten als nützlich oder schädlich ansieht. Ein weiterer Faktor ist die Schädigung des natürlichen Gleichgewichts durch die absichtliche oder unbewusste Einschleppung von Neozoen. Die Verfolgung durch verwilderte Hauskatzen und Haushunde sowie die Nahrungskonkurrenz durch Mäuse, Ratten, Hasen und andere stellen insbesondere in Regionen, wo diese Arten natürlicherweise nicht heimisch waren (wie zum Beispiel Australien oder viele Inseln), ein großes Problem dar.
Die oben genannten Gründe haben dazu geführt, dass laut IUCN 73 Säugetierarten in den letzten Jahrhunderten ausgestorben sind, dazu zählen der Schweinsfuß-Nasenbeutler, vier Känguruarten, der Beutelwolf, der Falklandfuchs, drei Gazellenarten, der Blaubock, die Stellersche Seekuh, zwölf Fledertierarten und zahlreiche Nagetiere wie etliche Baumratten und Riesenhutias. Es steht zu erwarten, dass diese Liste in den nächsten Jahren noch länger werden wird.
Die Säugetiere sind wahrscheinlich – entgegen anders lautenden Theorien, die Mitte des 20. Jahrhunderts verbreitet waren – eine monophyletische Gruppe: Sie stammen alle von einem gemeinsamen Vorfahren ab und umfassen auch alle Nachkommen dieses Vorfahren. Die drei Untergruppen, Ursäuger, Beutelsäuger und Höhere Säugetiere, sind ebenfalls jeweils monophyletische Taxa. Die meisten Systematiken fassen die Beutel- und Höheren Säuger zum Taxon Theria zusammen und stellen dieses den Ursäugern gegenüber. Einige Forscher vertreten aber die Ansicht, die Ursäuger hätten sich aus den Beutelsäugern entwickelt.
Ungleich unübersichtlicher wird das Bild, wenn fossile Taxa in den Stammbaum eingebunden werden. Neben den üblichen Meinungsunterschieden der Wissenschaftler kommt hinzu, dass von zahlreichen Gattungen lediglich Zähne und Kieferteile gefunden wurden. Die detaillierte Untersuchung der Zähne ist daher eines der Schlüsselkriterien zur Bestimmung der Evolution der Säugetiere.
Unstrittig ist, dass sich die Säugetiere aus den Synapsiden entwickelt haben, einer Reptiliengruppe, die durch ein einzelnes Schädelfenster charakterisiert war und ihre Blütezeit im Perm-Zeitalter hatte. Innerhalb der Synapsiden entwickelten sich die Therapsiden, die sogenannten „Säugerähnlichen Reptilien“, die bereits einige der Säugermerkmale wie ein differenziertes Gebiss und möglicherweise Körperbehaarung aufwiesen. Eine Gruppe der Therapsiden waren die Cynodontia, die unter anderem durch ein vergrößertes Gehirn und eine spezielle Kieferform gekennzeichnet waren. Die Säugetiere und ihre näheren Verwandten werden im Taxon der Eucynodontia zusammengefasst, deren bekanntester Vertreter Cynognathus war. Als Schwestertaxon der Säuger gelten entweder die Tritheledontidae, eine Gruppe sehr kleiner, fleischfressender Tiere oder die Tritylodontidae, eine Gruppe bis zu 1 Meter langer Pflanzenfresser. Für jede der beiden Gruppen sprechen gewisse anatomische Merkmale, die Mehrheit der Forscher gibt jedoch den Tritheledontidae den Vorzug.
Die Nicht-Säugetiere innerhalb der Therapsiden wurden nach und nach von den Dinosauriern verdrängt, die letzten starben in der Unterkreide aus.
Umstritten ist, welches Tier als das älteste Säugetier zu bewerten ist. Einige Tiere weisen im Bau des Ohres, des Unterkiefers, des Kiefergelenkes und der Zähne einen Übergangsstatus zwischen Reptilien und Säugern auf, manche Forscher bezeichnen sie deshalb als Mammaliaformes, also „Säugerartige“ oder Proto-Mammalia und ordnen sie noch nicht den Säugetieren im eigentlichen Sinn (sensu stricto) zu, andere fassen die Säuger weiter (sensu lato) und rechnen diese bereits dazu.
Die Säugetiere im engeren Sinn (Mammalia sensu stricto), in Abgrenzung zu den Säugetieren im weiteren Sinn beziehungsweise Mammaliaformes (siehe oben), werden definiert als die Gruppe, die den letzten gemeinsamen Vorfahren aller heutigen Säugetiere sowie dessen Nachkommen umfasst. Dieses Taxon ist zumindest seit dem mittleren Jura belegt, die Entwicklungsgeschichte innerhalb dieser Gruppe ist jedoch in einem hohen Ausmaß umstritten.
Generell waren die Säugetiere des Mesozoikums klein, die meisten erreichten nur die Größe von Mäusen oder Ratten. Aus den Zähnen schließt man bei den meisten Arten auf eine aus Insekten und anderen Wirbellosen bestehende Nahrung, aus der Form des Gehirns und der Sinnesorgane auf eine hauptsächlich nachtaktive Lebensweise. Es bleibt die Frage, warum der Großteil der mesozoischen Säuger in Größe, Körperbau und Lebensweise relativ einheitlich blieb, zumal es in einem entwicklungsgeschichtlich sehr kurzen Zeitraum (rund 5 Millionen Jahre) nach dem Beginn des Känozoikums zu einer enormen Radiation bei der Größe und Ernährungsweise kam. Generell wird diese Frage mit der Konkurrenz durch die Dinosaurier beantwortet, die, solange sie existierten, durch den ausgeübten Selektionsdruck größere Säuger verhinderten. Diese Sichtweise wird manchmal in Frage gestellt: Aufgrund des enormen Größenunterschiedes und der unterschiedlichen Lebensweise mit den Dinosauriern, die vermutlich tagaktiv waren, hätte es zumindest eine Reihe mittelgroßer Säuger geben können. Daher wurden verschiedene physiologische Einschränkungen postuliert, zum Beispiel eine mangelnde Fähigkeit zur Kühlung der Körpertemperatur oder die noch nicht völlig ausgereiften Kau- und Verdauungsapparate.
In jüngerer Zeit gab es allerdings einige neue Funde, die auf eine höhere Spezialisierung der mesozoischen Säuger hinweisen. So war Castorocauda zumindest teilweise wasserbewohnend, Volaticotherium war mit Gleitmembranen ausgestattet und Fruitafossor zeigt eine an Ameisenbären erinnernde Anpassung an eine insektenfressende Lebensweise. Repenomamus schließlich, der in der Unterkreide in China lebte, erreichte eine Länge von über 1 Meter und sein Gewicht wird auf 12 bis 14 Kilogramm geschätzt. Er ist der bislang größte aus dem Mesozoikum bekannte Säuger und hat sich auch von kleinen Dinosauriern ernährt.
Die Beutelsäuger waren, abgesehen von vereinzelten Funden in Ostasien, auf Nordamerika beschränkt. Zu den ältesten heute noch bestehenden Gruppen gehören die Beutelratten, deren Vorfahren schon aus dieser Zeit bekannt sind.
Die Höheren Säugetiere spalteten sich in die heute durch molekulargenetische Untersuchungen bestimmten Überordnungen (Nebengelenktiere, Afrotheria, Laurasiatheria, Euarchontoglires) auf, was durch tektonische Verschiebungen, unter anderem das Auseinanderbrechen Gondwanas gefördert wurde. Diese Aufspaltungen werden allerdings hauptsächlich durch molekulargenetische Berechnungen belegt, Fossilienfunde von Höheren Säugetieren aus der Oberkreide sind sehr selten und bislang nur aus Nordamerika und Ostasien belegt. Zu den bekanntesten Gattungen dieser Epoche zählen Asioryctes, die Leptictida, die möglicherweise Vorfahren der Insektenfresser sind, die Zalambdalestidae (mögliche Vorfahren der Nagetiere), die Zhelestidae (mögliche Vorfahren der „Huftiere“) und Cimolestes (eventuell ein Urahn der Raubtiere). Generell ist aber die Zuordnung zu heutigen Taxa umstritten, zweifelsfrei mit heutigen Arten verwandte Säugetiere traten erst im Paläozän auf.
Mit Ausnahme der Multituberculata dürften am Ende der Kreidezeit die meisten der oben beschriebenen Seitenlinien der Säugetiere ausgestorben gewesen sein.
Mit dem Aussterben der Dinosaurier wurden viele ökologische Nischen frei, die von einer Vielzahl neu entstehender Säugetiergruppen besetzt wurden. Im Verlauf des Känozoikums entwickelten sich die Säugetiere zu der dominanten Wirbeltiergruppe auf dem Land. Es bildeten sich die heutigen Ordnungen heraus, wobei die Entwicklungsgeschichte keineswegs geradlinig verlief, sondern durch evolutionäre Sackgassen, Verdrängungsprozesse und wieder gänzlich ausgestorbene Säugetiergruppen geprägt war. Die Entwicklungslinien in manchen Gruppen (zum Beispiel bei Pferden oder Rüsseltieren) sind dabei relativ gut durch Fossilienfunde belegt und erforscht. Eine besondere Rolle nahm Südamerika ein, das während der längsten Zeit des Känozoikums von anderen Kontinenten getrennt war. Durch die Insellage drangen viele Arten in ökologische Nischen vor und es entwickelte sich eine einzigartige Fauna, unter anderem mit Sparassodonta („Beutelhyänen“), einer Gruppe fleischfressender Beuteltiere, mit den Paucituberculata, einer formenreichen Beuteltiergruppe, die heute noch in den Mausopossums weiterlebt und mit den Südamerikanischen Huftieren (Meridiungulata). Nach Entstehen der mittelamerikanischen Landbrücke drangen Säuger aus dem Norden vor und verdrängten die einheimischen Arten größtenteils.
Die meisten Säugetierordnungen sind seit dem Eozän belegt, darunter auch die Vorfahren der wohl spezialisiertesten Gruppen, der Fledertiere und Wale. Im gleichen Zeitabschnitt bildeten sich die ersten riesenhaften Formen wie Uintatherium; diese Entwicklung gipfelte in Paraceratherium (auch unter den Namen Baluchitherium oder Indricotherium bekannt), dem mit 5,5 Metern Schulterhöhe und 10 bis 15 Tonnen Gewicht größten bekannten Landsäugetier.
Ihre größte Artenvielfalt erreichten die Säuger im Miozän; seither verschlechterten sich die Klimabedingungen kontinuierlich, bis hin zu den Eiszeiten des Pleistozän. Die klimatischen Verschiebungen, verbunden mit den Einflüssen des Menschen, sorgen seither für einen Rückgang der Artenvielfalt.
Am Ende des Pleistozäns (vor 50.000 bis 10.000 Jahren) kam es weltweit zu einem Massenaussterben von großen Säugetieren. Mit Ausnahme Afrikas und des südlichen Asiens starben alle Arten mit über 1000 Kilogramm Gewicht und 80 % aller Arten mit 100 bis 1000 Kilogramm Gewicht aus. In Australien fand dieser Prozess vor rund 51.000 bis 38.000 Jahren statt, hier verschwanden unter anderem Diprotodons (nashorngroße Beuteltiere), Beutellöwen (Thylacoleo carnifex), und bis zu 3 Meter hohe Riesenkängurus (Gattung Procoptodon). In Eurasien erstreckte sich dieser Vorgang über einen längeren Zeitraum, von vor 50.000 bis 10.000 Jahre, und erreichte mit dem Ende der letzten Kaltzeit seinen Höhepunkt. Zu den in Europa um 10.000 vor Christus ausgestorbenen Tieren zählen unter anderem das Wollhaarmammut (Mammuthus primigenius), das Wollnashorn (Coelodonta antiquitatis), der Riesenhirsch (Megaloceros giganteus), das Steppenwisent (Bos priscus), der Höhlenlöwe (Panthera spelaea) und der Höhlenbär (Ursus spelaeus). In Amerika lag das Aussterben in einem engen Zeitrahmen (vor rund 11.000 bis 8.000 Jahren), hier verschwanden unter anderem die Mammuts, das Amerikanische Mastodon und andere Rüsseltiere, Säbelzahnkatzen, Riesenfaultiere und Riesengürteltiere (Glyptodontidae).
Inwieweit klimatische Veränderungen oder die Bejagung durch den Menschen (Overkill-Hypothese) die Hauptschuld dafür tragen, ist immer noch umstritten. Für die Bejagung sprechen die Tatsachen, dass der Zeitpunkt des Aussterbens zumindest zum Teil mit der weltweiten Ausbreitung des Menschen übereinstimmt und dass bei keiner der früheren Aussterbephasen eine derartige Einschränkung der Größe beobachtet werden konnte. Auch müssten die klimatischen Vorgänge am Ende der letzten Kaltzeit eher zu einer Erhöhung der Artenanzahl beigetragen haben, wie sie meist in wärmeren Perioden beobachtet werden kann. Vertreter der Bejagungshypothese führen auch einen analogen Vorgang auf Inseln, die erst später besiedelt wurden, an. So sind auf Madagaskar, wo erst seit rund 1500 Jahren Menschen leben, in den darauf folgenden Jahrhunderten unter anderem die dortigen Flusspferde und zahlreiche große Primatenarten verschwunden, darunter die Riesenlemuren Megaladapis. Gegner der Bejagungshypothese behaupten, die primitiven Jagdmethoden der frühen Menschen hätten keinen so großen Einfluss auf die Populationsgröße haben können, und verweisen auf Afrika, wo es schon viel länger Menschen gegeben hat und wo es zu keinem nennenswerten Massenaussterben gekommen ist. Auch seien die klimatischen Veränderungen dermaßen komplex gewesen, dass eine Vielzahl von Faktoren berücksichtigt werden müsste.
In jüngerer Zeit mehren sich die Thesen, dass eine Vermischung beider Faktoren die Schuld am Massenaussterben trägt. So sei für die durch klimatische Veränderungen bereits in Mitleidenschaft gezogenen Populationen die Jagd der ausschlaggebende Punkt für die Ausrottung gewesen. Auch ökologische Faktoren können eine Rolle gespielt haben: So führte die Dezimierung großer Grasfresser zur Ausbreitung von Wäldern, was sich fatal auf die noch vorhandenen Populationen auswirkte. Andere Forscher geben auch den ausgedehnten Brandrodungen eine Teilschuld.
In dieser Diskussion spielt aber nicht nur der rein wissenschaftliche Aspekt eine Rolle, sondern auch die anthropologische Komponente, je nachdem ob man in diesem Massenaussterben das letzte einer langen Reihe von natürlichen Aussterbevorgängen in der Natur sieht oder den ersten von vielen zerstörerischen Eingriffen des Menschen in seine Umwelt.
Zurzeit (2021) stuft die IUCN von 5.940 gelisteten Arten, 85 Arten bereits als ausgestorben (Extinct) ein. 2 Arten gelten als in der Natur ausgestorben (Extinct in the Wild), 225 Arten (Critically Endangered) vom Aussterben bedroht, 542 Arten als stark gefährdet (Endangered) und 538 Arten als gefährdet (Vulnerable), insgesamt 1.307 Arten. 845 Arten können aktuell nicht bewertet werden (data deficient).[8]
Anschließend ein etwas vereinfachtes Kladogramm der Landwirbeltiere, gefolgt von ausführlicheren Darstellungen über eventuelle Unsicherheiten und Streitpunkte.
Die Säugetiere werden in drei Unterklassen mit rund 25 bis 30 Ordnungen unterteilt, die ihrerseits bei den Beutelsäugern und höheren Säugetieren noch einmal zwei beziehungsweise vier übergeordneten Gruppen zugeteilt werden können. Eine detailliertere Systematik mit allen Familien findet sich unter Systematik der Säugetiere.
Einige Bemerkungen zu dieser Systematik:
Der unter Systematik gezeigte Stammbaum stützt sich teilweise auf molekulargenetische Analysen. Da diese bei ausgestorbenen Tiergruppen nicht möglich sind, lassen sie sich nur schwer in die Systematik einordnen. Existierende Systeme, wie das von Malcolm C. McKenna and Susan K. Bell, die sowohl lebende als auch ausgestorbene Säugerordnungen enthalten, widersprechen sich teilweise mit der hier gewählten Systematik. Deshalb werden hier die ausgestorbenen Säugetierordnungen der Beutelsäuger (Metatheria) und der Höheren Säugetiere (Eutheria) extra aufgelistet.
Ausgestorbene Ordnungen der Beutelsäuger:
Ausgestorbene Ordnungen der Höheren Säugetiere:
Ältere Säugetierordnungen, die weder zu Beuteltieren noch zu Höheren Säugern gehören, sind weiter oben bei den Säugetieren im engeren Sinne aufgeführt.

Als Baum (von westgerm. mhd., ahd. boum, Herkunft ungeklärt[1], Teil der Swadesh-Liste; im Behördendeutsch auch Großgrün[2]) wird im allgemeinen Sprachgebrauch eine verholzte Pflanze verstanden, die aus einer Wurzel, einem daraus emporsteigenden, hochgewachsenen Stamm und einer belaubten oder benadelten Krone besteht.
Die Botanik definiert Bäume als ausdauernde und verholzende Samenpflanzen, die eine dominierende Sprossachse aufweisen, die durch sekundäres Dickenwachstum an Umfang zunimmt. Diese Merkmale unterscheiden einen Baum von Sträuchern, Farnen, Palmen und anderen verholzenden Pflanzen. Im Gegensatz zu ihren entwicklungsgeschichtlichen Vorläufern verfügen die meisten Bäume zudem über wesentlich differenziertere Blattorgane, die mehrfach verzweigten Seitentrieben (Lang- und Kurztrieben) entspringen. Stamm, Äste und Zweige verlängern sich jedes Jahr durch Austreiben von End- und Seitenknospen, verholzen dabei und nehmen kontinuierlich an Umfang zu. Im Gegensatz zum Strauch ist es besonderes Merkmal der Bäume, dass die Endknospen über die Seitenknospen dominieren (Apikaldominanz) und sich dadurch ein vorherrschender Haupttrieb herausbildet (Akrotonie).
Die Voraussetzungen für die Entstehung und Verbreitung der Bäume waren:
Die Vorläufer der Bäume kennt man aus dem Karbon. Sie gehörten zu den Schachtelhalmgewächsen, den Bärlappgewächsen und den Farnen. Sie besaßen verholzte Stämme, die auch ein sekundäres Dickenwachstum aufwiesen. Fossile Gattungen sind beispielsweise Lepidodendron und Sigillaria. Die verdichteten Sedimente dieser Wälder bilden die Steinkohle.
Die weitere Evolution der Pflanzen brachte im Perm die Samenpflanzen hervor. Die Nacktsamer breiteten sich als erste Bäume rasch aus, erreichten wohl in der Trias (vor etwa 200 Millionen Jahren) ihre größte Artenvielfalt, bis sie im Paläogen (vor etwa 60 Millionen Jahren) von den Angiospermen in ihrer Bedeutung abgelöst wurden.[3] Von den bekannten 220.000 Blütenpflanzen sind etwa 30.000 Holzarten, so dass etwa jede achte Blütenpflanze ein Baum oder Strauch ist. Die meisten Baumarten zählen zu den Bedecktsamern (Angiospermen). Die Gymnospermen (Nacktsamer) umfassen nur ungefähr 800 Arten, bedecken aber immerhin ein Drittel der Waldfläche der Erde.
Die globale Verteilung der Baumarten wurde vor allem durch die klimatischen Verhältnisse und durch die Kontinentalverschiebung geprägt. Während zum Beispiel die Buchengewächse (Fagaceae) eine typische Familie der Nordhemisphäre sind, ist beispielsweise die Familie Podocarpaceae vorwiegend in der Südhemisphäre verbreitet. Die heutige natürliche Artenverteilung wurde stark von den quartären Eiszeiten beeinflusst. Das gleichzeitige Vordringen der skandinavischen und alpinen Gletschermassen Europas hat zu einer Verdrängung zahlreicher Spezies geführt und die im Vergleich zu Nordamerika auffällige Artenarmut in Zentraleuropa verursacht. So stehen etwa der einzigen in den montanen Regionen Mitteleuropas heimischen Fichtenart, der Gemeinen Fichte (Picea abies), zahlreiche Fichtenarten auf dem nordamerikanischen Kontinent gegenüber.
Baumförmige Lebensformen kommen in verschiedenen Pflanzengruppen vor: „Echte“ Bäume sind die Laubbäume unter den Bedecktsamern und die baumförmigen Nacktsamer, zu denen Nadelholzgewächse wie die Koniferen gehören, aber auch Ginkgo biloba (als einziger noch existierender Vertreter der Ginkgogewächse) sowie zahlreiche Vertreter der fiederblättrigen Nacktsamer (Cycadophytina). Eigentümlichster Baum ist wohl die in Namibia vorkommende Welwitschia mirabilis, deren Stamm im Boden verbleibt. Daneben können auch die Palmen und die Baumfarne eine baumähnliche Form ausbilden. Diese Gruppen besitzen aber kein echtes Holz (sekundäres Xylem) und gelten daher nicht als Bäume. Eine Sonderstellung nimmt der Drachenbaum (Dracaena) ein. Dieser gehört zwar zu den Einkeimblättrigen, hat aber ein atypisches sekundäres Dickenwachstum.
Baumähnliche Formen finden sich hauptsächlich in rund 50 höheren Pflanzenfamilien. Dagegen fehlt die Baumform bei Algen, Moosen, Liliengewächsen, Iridaceae, Hydrocharitaceae, Orchideen, Chenopodiaceae, Primelgewächsen und meist auch bei den Convolvulaceae, Glockenblumengewächsen, Cucurbitaceae, Doldengewächsen, Saxifragaceae, Papaveraceae, Ranunculaceae oder Caryophyllaceae.
Bäume kommen heute innerhalb der Nacktsamer (Gymnospermae) einerseits in Form der Ginkgoopsida mit der Art Ginkgo, andererseits der nadelblättrigen Nacktsamer (Coniferopsida, „Nadelbäume“) vor. Dominiert werden die Arten vor allem von der Ordnung Pinales mit den Familien Pinaceae (Fichten, Kiefern, Tannen, Douglasien, Lärchen, Goldlärche), Cupressaceae (Zypressen, Scheinzypressen, Sumpfzypressen, Lebensbäume, Wacholder, Mammutbäume), Podocarpaceae (Steineiben, Harzeiben), Araucariaceae (Araukarien, Kauri-Bäume), Taxaceae (Eiben) und Cephalotaxaceae (Kopfeiben).
Viele Baumarten kommen aber auch innerhalb der Bedecktsamer (Angiospermen) vor. Die verschiedenen Unterklassen haben hier unterschiedliche Laubbaumtypen hervorgebracht. Zu den bedeutendsten gehören die Buchengewächse (Fagaceae), zu denen neben den Buchen (Fagus spp.) auch die Eichen (Quercus spp.) und die Kastanien (Castanea) gezählt werden. Ebenfalls bedeutend sind die Birkengewächse (Betulaceae) mit den Birken und Erlen sowie die Nussbäume (Juglandaceae), die Ulmen (Ulmaceae) und die Maulbeergewächse (Moraceae). Zu den Rosiden zählen die Linden aus der Familie der Malvengewächse, die Obstgehölze aus der Familie der Rosengewächse (Rosaceae) sowie die Leguminosen (Fabales) mit sehr zahlreichen, vor allem tropischen Arten. Neben der Gattung Dalbergia (Palisanderbäume) gehört auch die Gattung Robinia in diese Gruppe. Wirtschaftlich bedeutsam sind die Zedrachgewächse (Meliaceae) mit den Gattungen Entandrophragma (Mahagonibäume) und Cedrela sowie die Familie der Dipterocarpaceae mit der Gattung Shorea (Meranti, Bangkirai).
Baumartige Lebensformen zeigen eine große Variationsbreite in ihrem Aufbau (Morphologie). Assoziiert wird mit dem Begriff Baum der Aufbau aus Baumkrone, Baumstamm und Baumwurzeln. Bei den baumartigen Farnen und den meisten Palmen finden sich einfache Stämme, die keine Äste ausbilden, sondern schopfartig angeordnete, häufig gefiederte Blätter. Vor allem zeigen sie kein sekundäres Dickenwachstum und sind damit keine echten Bäume.
Bei den echten Bäumen wächst aus dem Spross der Keimpflanze durch Längen- und sekundäres Dickenwachstum der künftige Baumstamm heran: Es bildet sich der Spross an der Spitze durch die sich ständig erneuernde Gipfelknospe aufrecht weiter und wird zum geraden, bis zur höchsten Kronenspitze durchgehenden Baumstamm (Monopodium). In der Spitzenknospe gebildete Wuchsstoffe (Auxine) unterdrücken die Aktivität der Seitenknospen. Bei vielen Baumarten lässt diese Dominanz des Haupttriebs mit dem Alter nach und es bildet sich eine typische, verzweigte Laubbaumkrone.
Bei anderen Gehölzen wie der Buche oder der Hainbuche übernimmt eine subterminale Seitenknospe die Führung (Sympodium). Bei Bäumen entsteht so eine aufrechte „Scheinachse“ (Monochasium). Im späteren Verlauf lässt die Dominanz der führenden Knospe nach und aus weiteren Seitenknospen entwickeln sich stärkere Äste, die schließlich eine Krone bilden. Dies geschieht meist früher als bei Bäumen mit monopodialem Wuchs.
Sträucher hingegen sind durch das völlige Fehlen der apikalen Dominanz gekennzeichnet. Zahlreiche bodenbürtige Seitentriebe bilden hier eine weit verzweigte Wuchsform.
Bei Gehölzen bildet sich an den Wuchsachsen während der Vegetationsperiode je ein Triebabschnitt (Jahrestrieb), dessen Beginn lange an den schmalen ringförmigen Blattnarben der ehemaligen Knospenschuppen erkennbar ist. Ein weiterer Austrieb nach der Vegetationsperiode wird als Johannistrieb (Prolepsis) bezeichnet. Tropische Arten neigen zu mehrfachem Austrieb.
Aus der Zahl der Jahrestriebe und dem Grad der Verzweigung lässt sich das Alter eines Astes ermitteln. Diese Altersbestimmung wird jedoch bei zahlreichen Arten (zum Beispiel Fichte oder Tanne) und regelmäßig bei älteren Bäumen durch die Ausbildung von sogenannten Proventivtrieben erschwert, die aus „schlafenden“ Knospen austreiben. Die regelmäßige Bildung von Proventivtrieben wird als Reiteration (sprich: Re-Iteration) bezeichnet. Diese Wiederholungstriebe dienen der Erneuerung der Krone und verschaffen Bäumen die Möglichkeit, alternde Äste zu ersetzen sowie auf Stress (Schneebruch, Insektenkalamitäten) zu reagieren.
Bäume können ein Alter von mehreren 100 Jahren, an bestimmten Standorten sogar von mehreren 1000 Jahren erreichen. Als ältester Baum der Welt gilt (Stand: 2008) die 9550 Jahre alte Fichte Old Tjikko im Nationalpark Fulufjället im mittelschwedischen Bezirk Dalarna.[4] Unter dieser Fichte wurden drei weitere „Generationen“ (375, 5660 und 9000 Jahre alt) mit identischem Erbmaterial gefunden. Die Zahl der über 8000 Jahre alten Fichten wird auf etwa 20 Stück geschätzt. Damit ist die Fichte rund doppelt so alt wie die nordamerikanischen Kiefern, die mit 4000 bis 5000 Jahren bislang als die ältesten lebenden Bäume galten. Die nachweislich ältesten Bäume Mitteleuropas werden auf etwa 600 bis 700 Jahre datiert.
Wächst der Baum unter im Jahresrhythmus schwankenden klimatischen Bedingungen, wird während der Vegetationsperiode ein Jahresring angelegt. Mit Hilfe dieser Ringe lassen sich das Alter eines Baumes und dessen Wuchsbedingungen in den einzelnen Jahren ablesen. Die Dendrochronologie nutzt dies, um altes Holz zu datieren und das Klima einer Region bis zu mehreren 1000 Jahren zu rekonstruieren.
Seine Entwicklung bringt für den Baum zahlreiche Probleme und Schädigungen mit sich. Hierunter fallen vor allem:
Bei Jungbäumen kommt es insbesondere zu:
Einige wichtige Krankheiten, von denen Bäume befallen werden können, sind Brand, Krebs, Rost, Mehltau, Rotfäule, Weißfäule, Braunfäule und Harzfluss. Zu Missbildungen an Bäumen zählen die Maserkröpfe, die Hexenbesen oder Wetterbüsche sowie die Gallen.


Der Baumstamm, in der Dendrologie Schaft genannt, ist die verholzende Hauptachse (Caulom) der Baumpflanze.
Ein Querschnitt durch einen Baumstamm zeigt verschiedene Zonen. Ganz innen befinden sich das aus Primärgewebe bestehende Mark und das tote Kernholz. Bestimmte Baumarten (z. B. Buche, Esche) bilden fakultativ einen Falschkern aus, der sich in den Eigenschaften vom echten Kernholz unterscheidet. Weiter außen befindet sich das Splintholz, das der Leitung und Speicherung dient und sich bei sogenannten Kernholzbäumen farblich meist deutlich vom Kernholz abhebt. Bei der Eiche, der Eibe und der Robinie ist dies sehr gut sichtbar. Die Fichte hat einen farblosen Kern (Reifholz).
Die äußerste Schicht bildet die Baumrinde. Sie besteht aus der Bastschicht, die in Wasser gelöste Nährstoffe transportiert, und der Borke, die den Stamm vor Umwelteinflüssen (UV-Einstrahlung, Hitze, mechanische und biotische Schäden) schützt.
Zwischen der Bastschicht und dem Holz befindet sich bei Gymnospermen und Dikotyledonen das Kambium. Diese Wachstumsschicht bildet durch sekundäres Dickenwachstum nach innen Holz (Xylem) und nach außen Bast (Phloem). Das Holz zeichnet sich durch die Einlagerung von Lignin in die Zellwand aus. Dadurch werden die Zellen versteift und bilden ein festes Dauergewebe. Das sekundäre Dickenwachstum, die Lignifizierung der hölzernen Zellwand und die Vermehrung durch Samen verschafften den Bäumen in den meisten Biomen der Erde einen Vorteil gegenüber anderen Pflanzen und haben dort zur Entwicklung großflächiger Waldbestände geführt. Ausnahmen bilden die Wüsten, die arktischen Tundren und die zentralkontinentalen Steppen.
Hinsichtlich des inneren Baus des Baumstamms weichen die zu den Einkeimblättrigen gehörenden Palmen von den echten Bäumen erheblich ab. Bei ersteren stehen die Gefäßbündel im Grundgewebe zerstreut, weshalb es keinen Kambium­ring, keinen Holzzylinder und somit kein fortdauerndes sekundäres Dickenwachstum des Stammes gibt. Bei den zu den Dikotyledonen oder Gymnospermen gehörenden Bäumen besitzt der Stamm schon in der frühesten Jugend als dünner Stängel einen unter der Rinde gelegenen Kreis von Leitbündeln, der den Rindenbereich vom innen liegenden Mark scheidet. Dieser Leitbündelring stellt in seiner inneren, dem Mark anliegenden Hälfte das Holz und im äußeren, an die Rinde angrenzenden Teil den Bast dar; zwischen beiden zieht sich der Kambiumring hindurch. Dieser wird aus zarten, saftreichen, sich ständig teilenden Zellen gebildet und vergrößert durch seinen laufenden Zellvermehrungsprozess die beiderseits ihm anliegenden Gewebe. So wird alljährlich an der Außenseite des Holzringes eine neue Zone Holzgewebe angelegt, wodurch die Jahresringe des auf diese Weise erstarkenden Holzkörpers entstehen, die als konzentrische Linien am Stammquerschnitt wahrnehmbar sind. Andererseits erhält aber auch der weiter außen liegende Bast an seiner Innenseite einen jährlichen, wenn auch weit geringeren Zuwachs. Auf diese Weise kommt die dauernde Verdickung des Stammes und aller Äste sowie auch der Wurzeln zustande.
Auch in der Wurzelbildung unterscheiden sich die Bäume untereinander. Neben der genetischen Festlegung steuern die Erfordernisse der Verankerung des Baumes im Boden ebenso wie die Notwendigkeit der Versorgung der Pflanze mit Wasser und Nährstoffen die Intensität und Art des Wurzelwachstums. Man spricht entsprechend der Form des Wurzelstocks von Pfahlwurzel, Flachwurzel oder Herzwurzel. Bei der Pfahlwurzel wächst die Hauptwurzel senkrecht in den Boden hinab, was besonders für die Eiche charakteristisch ist. Flachgründige Böden und hoch anstehendes Grundgestein oder Grundwasser begünstigen z. B. die Bildung von Flachwurzeln. Trockene Böden begünstigen eine Bildung von Pfahlwurzeln. Die überwiegende Masse des Wurzelstocks machen bei den Bäumen nicht die verholzten Wurzelteile, sondern die mit einer Mykorrhiza vergesellschafteten Feinwurzeln aus.[5] Im Boden verbinden sich viele Wurzeln symbiotisch mit Pilzmycelen. Bäume erhalten Mineralien wie Phosphor von den Pilzen, während Pilze von den Bäumen die Kohlenhydratprodukte der Photosynthese gewinnen.[6] Die Pilze können verschiedene Bäume miteinander verbinden, und es bildet sich ein Netzwerk, das Nährstoffe und Signale überträgt.[7][8] Die Gesamtwurzelmasse reicht oft an die Masse der oberirdischen Pflanzenteile heran. Bei einkeimblättrigen baumähnlichen Lebensformen endet der Stamm nahe unter der Bodenfläche und es entwickelt sich ein sprossbürtiges Wurzelsystem (Homorhizie).
An alten Bäumen finden sich meist junge Adventivwurzeln, die alte, ineffektive Wurzeln ersetzen. Bei einigen Baumarten bilden oberflächennahe Wurzeln eine sogenannte Wurzelbrut, eine Form der vegetativen Vermehrung. Wurzelkappungen infolge von Baumaßnahmen können das Absterben von Wurzelteilen bewirken und führen zum Eindringen von holzzerstörenden Pilzen in den Baum. Dies ist die häufigste Ursache von irreparablen Baumschäden im städtischen Bereich.
Bäume tragen Laubblätter oder Nadelblätter, die entweder mehrjährig am Baum verbleiben (immergrüne Arten) oder am Ende einer Vegetationsperiode abgeworfen werden (laubabwerfende Arten). Dazwischen liegen noch die halbimmergrünen Arten, die am Ende einer Vegetationsperiode nur einen Teil ihrer Blätter verlieren, bei Neuaustrieb dann aber die vorjährigen ersetzen. Die Nadelgehölze sind mit Ausnahme der Gattungen Lärchen (Larix) und Goldlärchen (Pseudolarix) immergrüne Arten. In den borealen und hochmontanen Biomen der Nordhalbkugel haben sich die immergrünen Nadelgehölze durchgesetzt, da sie zu Beginn der Vegetationsperiode bei ausreichender Temperatur sofort mit der Assimilation beginnen können, ohne zunächst Assimilationsorgane bilden zu müssen wie die laubabwerfenden Baumarten.
Die Gestalt der Blätter (Laub) ist ein wichtiges Bestimmungsmerkmal. Anordnung, Form, Größe, Farbe, Nervatur und Zähnung sowie haptische Eigenschaften können zur Differenzierung herangezogen werden. Nicht minder brauchbar zur Unterscheidung im winterlichen Zustand sind die (Blatt-)Knospen des Baumes. Eine eindeutige taxonomische Identifizierung der Arten ist allerdings nur anhand der Blüten oder Früchte möglich. Manche Bäume sind mit Dornen ausgestattet. Dies sind entweder kurze Zweige, die mit dorniger Spitze enden (Weißdorne, Wildformen von Obstbäumen) oder es sind stachelartig ausgebildete Nebenblätter wie etwa bei der Gewöhnlichen Robinie.
Ein europäischer Laubbaum trägt durchschnittlich 30.000 Blätter, die zusammen eine enorme Transpirationskapazität haben. An warmen Sommertagen kann der Baum mehrere hundert Liter Wasser verdunsten. Beispiel einer 80-jährigen, alleinstehenden Rotbuche:[9] In diesem Lebensalter ist der Baum 25 Meter hoch, und seine Baumkrone mit einem Durchmesser von 15 Meter bedeckt eine Standfläche von 160 m². In ihren 2700 m³ Rauminhalt finden sich 800.000 Blätter mit einer gesamten Blattoberfläche von 1600 m², deren Zellwände zusammen eine Fläche von 160.000 m² ergibt. Pro Stunde verbraucht diese Buche 2,352 kg Kohlenstoffdioxid, 0,96 kg Wasser und 25.435 Kilojoule Energie (das ist die in Form von Traubenzucker gespeicherte Energie, die eingestrahlte Sonnenenergie ist etwa siebenmal größer); im gleichen Zeitraum stellt sie 1,6 kg Traubenzucker her und deckt mit 1,712 kg Sauerstoff den Verbrauch von zehn Menschen. Die 15 m³ Holz des Baumes wiegen trocken 12.000 kg, allein 6000 kg davon sind Kohlenstoff.
Die Blüten der Bäume aus gemäßigten Breiten sind manchmal verhältnismäßig unscheinbar; bei einigen Taxa sind einzelne Blütenblattkreise reduziert. Einige Baumarten gemäßigter Breiten haben eingeschlechtige Blüten. Dabei sitzen die Blüten beider Geschlechter entweder auf demselben Baum (einhäusig getrenntgeschlechtig, zum Beispiel Eiche, Buche, Hainbuche, Birke, Erle und Nussbaum) oder auf verschiedenen (zweihäusig getrenntgeschlechtig), so dass man männliche und weibliche Bäume zu unterscheiden hat (unter anderem bei Weiden und Pappeln). Andere Bäume wie Obstbäume, Rosskastanie und viele Bäume der wärmeren Klimate haben Zwitterblüten, die sowohl Staub- als auch Fruchtblätter ausbilden.
Die Frucht- und Samenbildung zeigt weniger Eigentümlichkeiten. Bei den meisten Bäumen fällt die Reife in den Sommer oder Herbst desselben Jahres; nur bei den Kiefernarten erlangen die Samen und die sie enthaltenden Zapfen erst im zweiten Herbst nach der Blüte vollständige Ausbildung. Die Früchte sind meistens nussartig mit einem einzigen ausgebildeten Samen, oder sie bestehen aus mehreren einsamigen, nussartigen Teilen, wie bei den Ahornen. Saftige Steinfrüchte, ebenfalls mit einem oder wenigen Samen, finden sich bei den Obstbäumen, Kapseln mit zahlreichen Samen bei den Weiden und Pappeln.
Wie bei allen Pflanzen unterliegen auch bei Bäumen der Stoffwechsel und das Wachstum sowohl endogenen (genetisch festgelegten) als auch äußeren Einflüssen. Zu letzteren zählen vor allem die Standortverhältnisse, das Klima und die Konkurrenz mit anderen Organismen beziehungsweise deren schädigende Wirkung. Während der Vegetationsperiode sorgen die Spitzenmeristeme und das Kambium für stetigen Längen- und Dickenzuwachs. Beginn und Ende der Vegetationsperiode sind je nach Baumart durch die Witterung und die Wasserverfügbarkeit beziehungsweise durch die Tageslänge bestimmt. Das Wachstum wird dabei durch Phytohormone gesteuert und die Akkumulation von Biomasse gezielt optimiert. Bäume sind so in der Lage, sich an ändernde Wuchsbedingungen anzupassen und gerichtete Festigungs-, Leit-, Speicher- oder Assimilationsgewebe anzulegen.
Die Produktion neuen Gewebes mit dem sekundären Dickenwachstum und die Anlage neuer Jahrestriebe bewirkt, dass sich ein Baum ständig von innen nach außen erneuert. Der amerikanische Baumbiologe Alex Shigo hat daraus das Konzept der Kompartimentierung entwickelt, das den Baum als ein Ensemble zusammenwirkender Kompartimente sieht. Auf Verletzungen reagiert der Baum, anders als Tiere und Menschen, durch Abschottungsreaktionen und Aufgabe der eingekapselten Kompartimente (CODIT-Modell). Durch adaptives Wachstum optimiert er zudem seine Gestalt.
Computermodellierungen des Karlsruher Physikers und Biomechanikers Claus Mattheck konnten zeigen, dass Bäume durch adaptives Wachstum eine mechanisch optimale Gestalt anstreben und zum Beispiel Kerbspannungen in Verzweigungen vermeiden, so dass die Gefahr von Brüchen minimiert wird. Diese Erkenntnisse haben zu Optimierungen unter anderem im Maschinenbau geführt.
Der Wassertransport wird in den Nadelgehölzen durch die Tracheiden, in den Laubbäumen durch die effektiveren Gefäße (Poren) bewerkstelligt. Letztere sind bei den Laubbäumen entweder zerstreut (zum Beispiel bei Buche, Ahorn, Pappel) oder ringförmig (zum Beispiel bei Eiche, Ulme, Esche) im Jahresring angeordnet. Beispielsweise kann eine Eichenpore mit 400 µm Durchmesser 160.000-mal mehr Wasser als eine Nadelholztracheide mit 20 µm Durchmesser im gleichen Zeitraum transportieren.
Nach überwiegend vertretener Lehre funktioniert der Wassertransport der Bäume durch Saugspannungen in den Leitgeweben infolge Verdunstung an den Stomata der Blätter (Kohäsionstheorie). Dabei müssen Baumhöhen bis über 100 Meter überwunden werden können, was nach dieser Theorie nur mit enormen Drücken möglich ist. Kritiker dieser Lehre behaupten, dass schon bei wesentlich geringeren Höhen die Saugspannung zum Abriss des Wasserfadens in den Kapillaren führen müsste. Als gesichert gilt allerdings, dass im Frühjahr Zucker in den Speicherzellen mobilisiert werden und durch den aufgebauten osmotischen Druck Wasser aus den Wurzeln nachfließt. Dabei werden im Bodenwasser gelöste Nährsalze (vor allem K, Ca, Mg, Fe) vom Baum aufgenommen. Erst nach Ausdifferenzierung der Blätter werden die in der Krone erzeugten Assimilate über den Bast stammabwärts transportiert und stehen für das Dickenwachstum zur Verfügung. Eine Ausnahme bilden die ringporigen Laubbäume, bei denen die ersten Frühholzporen aus den im Vorjahr gebildeten Reservestoffen gebildet werden.
Die süßen „Baumsäfte“ wurden von Menschen durch Einschneiden der Rinde abgezapft und durch Einkochen zu Sirupen weiterverarbeitet, beispielsweise Ahornsirup oder der Saft der Manna-Esche. Palmzucker oder Palmsirup allerdings ist ein Extrakt aus dem Blütensaft der Nipa- und Zuckerpalme (Unterfamilie Arecoideae), Agavensirup stammt aus dem „Saft“ der zu den Stauden gehörenden Agaven, Birkenzucker wurde ursprünglich in Finnland direkt aus der Birkenrinde gewonnen.
Die Hydrologie beziehungsweise Bodenökologie unterscheidet zwischen dem Niederschlag, welcher im Bereich der Baumkrone auf den Boden trifft (Kronendurchlass) und dem Anteil, welche am Stamm herabfließt (Stammabfluss). Ein Teil des Niederschlags verdunstet direkt vom Baum (Interzeption) und erreicht den Boden nicht. Um die physikalisch grenzwertige Wasserversorgung sehr hoher Bäume von den Wurzeln zur Krone auszugleichen, ist etwa der Küstenmammutbaum in der Lage, zusätzlich Wasser mit den Nadeln aufzunehmen.[10]
Dort wo Bäume ausreichend Licht, Wärme und Wasser vorfinden, bilden sie Wälder. Im Jahr 2000 waren laut FAO 30 Prozent der Festlandmasse der Erde bewaldet. Pro Hektar binden Waldbäume zwischen 60 und 2000 Tonnen organisches Material und sind damit die größten Biomassespeicher der Kontinente. Die Gesamtmenge der 2005 weltweit in den Wäldern akkumulierten Holzmasse betrug 422 Gigatonnen. Da etwa die Hälfte der Holzsubstanz aus Kohlenstoff besteht, sind Wälder nach den Ozeanen die größten Kohlenstoffsenken der Biosphäre und damit für die CO2-Bilanz der Erdatmosphäre bedeutsam.
Die mit der Bestandsbildung von Bäumen einhergehende Konkurrenz um Ressourcen führt zu einer Anpassung des Habitus gegenüber den freistehenden Exemplaren (Solitäre). Natürlicher Astabwurf innerhalb der Schattenkrone sowie Verlagerung der Assimilation in die Lichtkrone sind Optimierungsreaktionen der Bäume, die zu einem hohen, schlanken Wuchs mit kleinen Kronen und oft zu hallenartigen Beständen führen (zum Beispiel Buchen-Altbestände). Eine Ausnahme und Besonderheit bezüglich der Biomasseproduktion stellen die über das Kronendach ragenden Emergenten (Urwaldriesen) vieler Regenwälder dar.
Die heutige Ausbreitung und Artenzusammensetzung der Wälder steht stark unter dem Einfluss der wirtschaftlichen Tätigkeit des Menschen. Der Übergang von der Jäger- und Sammlerkultur zum Ackerbau ging in den dicht besiedelten Regionen mit der Zurückdrängung der Wälder einher. Nützlich waren Bäume den Menschen zunächst vorwiegend als Brennholz (Niederwald­wirtschaft). Im Laufe der Entwicklung wurde die Gewinnung von Nutzholz aus Hochwäldern immer wichtiger. Diese Entwicklung hält an. Laut FAO wurden noch Ende der 1990er-Jahre weltweit 46 Prozent des weltweiten Holzeinschlags (3,2 Milliarden m³) als Brennholz genutzt, in den Tropen waren es sogar 86 Prozent. Die extensive Waldvernichtung in Zentraleuropa während des Mittelalters hat in der Neuzeit zur Einführung des Prinzips der nachhaltigen Waldbewirtschaftung geführt, nach dem nur so viel Holz entnommen werden darf, wie nachwächst.
In den Primärwäldern der feuchten Tropen findet sich die größte Artenvielfalt aller Waldtypen.[11] Wichtige tropische Familien sind die Wolfsmilchgewächse (Euphorbiaceae), Seifenbaumgewächse (Sapindaceae), Bombacaceae, Byttnerioideae (zu den Malvaceae), Mahagonigewächse (Meliaceae), Hülsenfrüchtler (Fabaceae), Caesalpiniaceae, Verbenaceae, Sterculiaceae, Dipterocarpaceae und Sapotaceae.
In der subtropischen Zone findet man Bäume unter den immergrünen Myrtengewächsen (Myrtaceae) und Lorbeergewächsen (Lauraceae) sowie Silberbaumgewächsen (Proteaceae), denen sich in der wärmeren gemäßigten Zone andere immergrüne Bäume anschließen, so die immergrünen Eichen, Granatbäume, Orangen- und Zitronenbäume sowie Ölbäume.
Dagegen sind in der gemäßigten Zone die laubwechselnden Bäume vorherrschend. Hier sind Wälder von Eichen, Buchen und Hainbuchen charakteristisch. Zu den in Mitteleuropa heimischen Laubbäumen zählen die Ahorne, Birken, Buchen, Eichen, Erlen, Eschen, Linden, Mehlbeeren, Pappeln, Ulmen und Weiden. Typische Nadelbäume sind die Fichten, Kiefern, Lärchen, Tannen und Eiben. In Mitteleuropa häufig vorkommende Baumarten, die in diesem Gebiet ursprünglich nicht beheimatet sind, sind die Gewöhnliche Robinie, der Walnussbaum und viele Obstbäume. Eine detaillierte Aufstellung bietet die Liste von Bäumen und Sträuchern in Mitteleuropa.
Und obgleich auch hier bereits Nadelhölzer in zusammenhängenden Waldungen auftreten, werden die Nadelwälder erst in der subarktischen (borealen) Zone vorherrschend, wo die Laubbäume nach und nach verdrängt werden. Artenvielfalt wie auch Wuchshöhe der Bäume nehmen mit zunehmender Annäherung an den Polarkreis ab. Eichen, Linden, Eschen, Ahorne und Buchen finden sich in Schweden nur noch diesseits des 64. Grades nördlicher Breite. Jenseits dieser Breite besteht die Baumvegetation hauptsächlich aus Fichten und Tannen, die in zusammenhängenden Wäldern nordöstlich noch über den 60. Grad hinausreichen, sowie aus Birken, die in zusammenhängenden Beständen sich fast bis zum 71. Grad nördlicher Breite erstrecken, und zum Teil aus Erlen und Weiden.
Auch die Höhe über dem Meeresspiegel hat auf die Ausbreitung und Höhe der Bäume (in Abhängigkeit von der geographischen Breite) einen bedeutenden Einfluss. In den Anden finden sich noch bis in 5000 m Höhe Polylepis-Bäume. Unter 30 Grad nördlicher Breite, wo die Schneegrenze bei 4048–4080 m liegt, kommen auf dem Himalaja, nördlich von Indien, noch in 3766 m Höhe Baumgruppen vor, die aus Eichen und Fichten bestehen. Ebenso sind in Mexiko, unter 25–28 Grad nördlicher Breite, die Gebirge bis 3766 m mit Fichten und bis 2825 m hoch mit mexikanischen Eichen bedeckt. In den Alpen des mittleren Europas endet der Holzwuchs bei einer Höhe von 1570 m, im Riesengebirge bei 1193 m und auf dem Brocken bei 1005 m. Eichen und Tannen stehen auf den Pyrenäen noch bis zu einer Höhe von 1883 m; dagegen wächst die Fichte auf dem Sulitelma in Lappland, bei 68 Grad nördlicher Breite, kaum in einer Höhe von 188 m, die Birke kaum in einer von 376 m.
Insgesamt gibt es auf der Erde etwa 73.200 Baumarten, 19 % dieser Arten kommen in Eurasien vor, 8 % in Nordamerika, 13 % in Afrika, 8 % in Ozeanien und der artenreichste Kontinent mit 49 % aller Arten ist Südamerika. Von den etwa 73.000 Arten sind (Stand Januar 2022) etwa 9200 Arten laut einer Einschätzung von Wissenschaftlern nicht entdeckt und beschrieben.[12] Der weltweite Datensatz der erfassten Baumarten umfasst Stand Januar 2022 insgesamt 64.100 Baumarten.[13]
Die wissenschaftliche Lehre von den Bäumen (Gehölzen) ist die Dendrologie. Anpflanzungen von Bäumen in systematischer oder pflanzengeographischer Anordnung, die Arboreten, dienen ihr zu Beobachtungs- und Versuchszwecken. Gehölze können vegetativ, das heißt durch Pflanzenteile, oder generativ durch Aussaat vermehrt werden. In Baumschulen findet eine gezielte Auslese, Anzucht und Vermehrung von Bäumen und Sträuchern statt. Neben der forstlichen Nutzung finden Bäume reichliche Verwendung im Garten- und Landschaftsbau. Mit der Baumpflege hat sich ein eigener Berufsstand zum Erhalt und zur fachgerechten Behandlung von Bäumen in urbanen Regionen entwickelt.
„Kein anderes Geschöpf ist mit dem Geschick der Menschheit so vielfältig, so eng verknüpft wie der Baum.“
Das schrieb der Historiker Alexander Demandt und hat dem Baum mit Über allen Wipfeln – Der Baum in der Kulturgeschichte ein umfangreiches Werk gewidmet. Für ihn beginnt die Kulturgeschichte mit dem Feuer, das der Blitz in die Bäume schlug, und mit dem Werkzeug, für das Holz zu allen Zeiten unentbehrlich war.
Neben der wichtigen Funktion der Bäume bei der Gestaltung von Kulturlandschaften begleitet vor allem die Holznutzung die Entwicklung der Menschheit. Abgesehen von der vor allem in Entwicklungsländern immer noch weit verbreiteten Brennholznutzung ist Holz ein vielseitiger Bau- und Werkstoff, dessen produzierte Menge die Produktionsmengen von Stahl, Aluminium und Beton weit übersteigt. Damit ist Holz nach wie vor der wichtigste Bau- und Werkstoff weltweit; Bäume sind dementsprechend eine bedeutende Rohstoffquelle.
Neben der Holznutzung werden Bäume auch zur Gewinnung von Blüten, Früchten, Samen oder einzelnen chemischen Bestandteilen (Terpentin, Zucker, Kautschuk, Balsame, Alkaloide und so weiter) genutzt. In der Forstwirtschaft der industrialisierten Länder spielen diese Nutzungen eine untergeordnete Rolle. Lediglich der Obstbau als Teilbereich der Landwirtschaft ist in vielen Regionen ein wichtiger Wirtschaftsfaktor. Der Anbau erfolgt in Form von Plantagen. Hochwertige Obstsorten werden meist durch Okulation oder Pfropfen veredelt. Dies erfolgt durch den Einsatz ausgewählter Obstsorten, wobei die bekannten und gewollten Eigenschaften der Früchte einer Obstsorte auf einen jungen Baum übertragen werden. Zurückgegangen ist dagegen die Nutzung von Streuobstwiesen, die früher in vielen Gebieten Mitteleuropas landschaftsprägend waren.
Als große Kohlenstoffsenke leisten Bäume einen wichtigen Beitrag gegen die derzeitige globale Erwärmung. Außerdem wird der wichtige Beitrag der Straßenbäume zur Verbesserung der Luftqualität im Rahmen des Stadtklimas zunehmend als Teil der Städteplanung mit berücksichtigt, denn Bäume verbessern die Stadtluft durch Sauerstoffproduktion, Staubfilterung und kühlende Verdunstung.[14] Dabei steigen die Ansprüche an die Stadtbäume durch den Klimawandel, der an vielen Orten u. a. für häufigere und längere Hitzewellen sorgt. Zu den am besten geeigneten Baumarten zählen, wenn man Faktoren wie den Wasserbedarf und den Kühlungseffekt betrachtet Robinie und Linde.[15] Auf der Suche nach Stadtbäumen die höhere Temperaturen, Schadstoffbelastung und Schädlinge besser verkraften als andere Sorten, erwiesen sich insbesondere Baumarten gut, die bisher noch nicht zum typischen Stadtbild zählen. Als besonders geeignet für den Einsatz im städtischen Bereich erwiesen sich – anhand von in Bayern durchgeführten Versuchsbepflanzungen – Silber-Linde, Europäische Hopfenbuche, Amberbäume, Ginkgos, Zürgelbäume, der Französische Ahorn sowie die zu den Ulmengewächsen gehörenden Zelkoven.[16]
Dieser Bedeutung entsprechend ist ein vielfältiges Brauchtum mit dem Baum verknüpft. Das reicht vom Baum, der zur Geburt eines Kindes zu pflanzen ist, über den Maibaum, der in manchen Regionen in der Nacht zum ersten Mai der Liebsten verehrt wird, über Kirmesbaum und Weihnachtsbaum, unter denen man feiert, und über den Richtbaum auf dem Dachstuhl eines neu errichteten Hauses bis zum Baum, der auf dem Grab gepflanzt wird. Nationen und Völkern werden bestimmte, für sie charakteristische Bäume zugeordnet. Eiche und Linde gelten als typisch „deutsche“ Bäume. Die Birke symbolisiert Russland, und der Baobab gilt als der typische Baum der afrikanischen Savanne. Unter der Gerichtslinde wurde Recht gesprochen (siehe auch → Thing) und unter der Tanzlinde gefeiert.
Seit 1989 wird jedes Jahr im Oktober für das darauffolgende Jahr der Baum des Jahres bestimmt, zunächst vom „Verein Baum des Jahres e. V.“, seit 2008 von der „Dr. Silvius Wodarz Stiftung“ und durch deren Fachbeirat, das „Kuratorium Baum des Jahres“ (KBJ).[17] Im Jahr 2000 wählte die Stiftung den Ginkgo-Baum (Ginkgo biloba) zum Baum des Jahrtausends als Mahnmal für Umweltschutz und Frieden.[18]
Bäume, die als Risiko- oder Gefahrenquellen in Erscheinung treten, werden mitunter als Gefahrenbaum klassifiziert.
Zahlreiche Mythen erzählen von einem Lebens- oder Weltenbaum, der die Weltachse im Zentrum des Kosmos darstellt. Bei den nordischen Völkern war es zum Beispiel die Weltesche Yggdrasil, unter deren Krone die Asen ihr Gericht abhielten. So spielt der Baum in den Mythen der Völker als Lebensbaum wie die Sykomore bei den Ägyptern oder in der jüdischen Mythologie eine Rolle. Kelten, Slawen, Germanen und Balten haben einst in Götterhainen Bäume verehrt, und das Fällen solcher Götzenbäume ist der Stoff zahlreicher Legenden, die von der Missionierung Nord- und Mitteleuropas berichten.
In vielen alten Kulturen und Religionen wurden Bäume oder Haine als Sitz der Götter oder anderer übernatürlicher Wesen verehrt. Solche Vorstellungen haben sich als abgesunkenes religiöses Gut bis in die heutige Zeit erhalten. Als Baum der Unsterblichkeit gilt der Pfirsichbaum in China. Der Bodhibaum, unter dem Buddha Erleuchtung fand, ist im Buddhismus ein Symbol des Erwachens.
Auch in der Bibel werden Bäume immer wieder erwähnt. Tanach wie auch das Neue Testament nennen unterschiedliche Baumarten, wie zum Beispiel den Olivenbaum oder den Feigenbaum, mit dessen relativ großen Blättern das erste Menschenpaar Adam und Eva laut 1. Mose/Genesis 3:7 nach ihrem Sündenfall ihre Blöße bedeckte. Im 1. Buch Mose, der Genesis, wird in Kapitel 1 in den Versen 11 und 12 berichtet, dass Gott die Bäume und insbesondere die fruchttragenden Bäume in seiner Schöpfung der Welt hervorbrachte. Zwei Bäume spielen in der Schöpfungsgeschichte eine entscheidende Rolle: Der Baum des Lebens und der Baum der Erkenntnis (von Gut und Böse).
So hat der Baum auch in der christlichen Ikonographie eine besondere Bedeutung. Dem Baum als Symbol des Sündenfalls, um dessen Stamm sich eine Schlange windet, steht häufig das hölzerne Kreuz als Symbol der Erlösung gegenüber. Ein dürrer und ein grünender Baum symbolisieren in den Dogmenallegorien der Reformationszeit den Alten und den Neuen Bund. In der Pflanzensymbolik haben verschiedene Baumarten wie auch ihre Blätter, Zweige und Früchte eine besondere Bedeutung. So weist die Akazie auf die Unsterblichkeit der menschlichen Seele hin, der Ölbaum auf den Frieden und ist ein altes marianisches Symbol für die Verkündigung an Maria. Der Zapfen der Pinie weist auf die Leben spendende Gnade und Kraft Gottes hin, die Stechpalme, aus deren Zweigen nach der Legende die Dornenkrone gefertigt war, auf die Passion Christi.
Der Arbre de Diane (Dianes Baum) ist eine Platane in Les Clayes-sous-Bois, Frankreich, die 1556 von Diana von Poitiers, der Mätresse Heinrichs II., gepflanzt worden sein soll.
Gedenkbäume sind Bäume, die zum Gedenken an ein Ereignis oder zum Gedenken an eine Person gepflanzt wurden.

Informationen über verschiedene Baumarten:
Informationen über seltene mitteleuropäische Baumarten:


Zinn ist ein chemisches Element mit dem Elementsymbol Sn (lateinisch stannum) und der Ordnungszahl 50. Im Periodensystem steht es in der 5. Periode und in der 4. Hauptgruppe, bzw. 14. IUPAC-Gruppe oder Kohlenstoffgruppe. Das silberweiß glänzende und sehr weiche Schwermetall lässt sich mit dem Fingernagel ritzen. Zinn hat einen für Metalle sehr niedrigen Schmelzpunkt. Seine Hauptverwendung lag früher im Bereich der Herstellung von Geschirr, das von Zinngießern innerhalb der städtischen Handwerkszünfte bis ins 19. Jahrhundert als weit verbreitete Gebrauchs- und Ziergegenstände als Bestandteile der bürgerlichen Haushalte hergestellt wurde. Im Orgelbau ist Zinn unverzichtbarer Bestandteil bei der Herstellung von Metallpfeifen. Moderne Nutzung erfolgt im Bereich von Elektrolöten sowie im Verzinnen von lebensmittelechten Konserven oder auch in der Medizin. Historisch hat der Mensch Zinn zuerst als Beimengung zum Kupfer als Legierungsmittel zur Herstellung der Bronze genutzt.[12]
Das Wort Zinn (ahd., mhd. zin) ist vielleicht verwandt mit ahd. zein „Stab“, „Stäbchen“, „Zweig“ (siehe Zain). Der Duden weist in diesem Zusammenhang darauf hin, dass das Metall früher in Stabform gegossen wurde.[13] Eine andere Erklärung geht davon aus, dass das Hauptzinnerz Kassiterit (Zinnstein) auch in Form von Nadeln oder „Stäbchen“ auftritt.
Die metallurgische Verarbeitung von Zinn begann etwas später als die von Kupfer. Während die Erschmelzung von Kupfer für die Vinča-Kultur auf 5400–4800 v. Chr. auf dem Balkan datiert wurde, ist diese für den Vorderen Orient auf dem Gebiet des heutigen Iran und der Türkei zwischen 5200 und 5000 v. Chr. erfolgt.[14] Die älteste datierte Legierung von Zinnbronze aus dem Zinnmineral Stannit wurde in der Ausgrabungsstätte Pločnik auf dem Gebiet des heutigen Serbien auf ca. 4650 v. Chr. datiert.[15][16]
Im südtürkischen Taurusgebirge, wo auch Zinnerz abgebaut worden sein könnte, wurden das Bergwerk Kestel und die Verarbeitungsstätte Göltepe entdeckt und auf etwa 3000 v. Chr. datiert. Ob es sich hier um die Quelle des großen vorderasiatischen Zinnverbrauches handelte, bleibt vorläufig offen. Zinnbronzen, Gold und Kupfer wurden zuerst nur wegen ihrer Farbigkeit als Schmuck verwendet. Die ersten Metallschmiede der Vinča-Kultur wählten die zinnhaltigen Mineralien mutmaßlich wegen deren schwarz-grüner Färbung aus, die Ähnlichkeit zu manganreichen Kupfererzen besaßen. Den Metallschmieden der Zinnbronzen waren die spezifischen Eigenschaften des neuen Metalls bewusst, was aus den angewendeten Techniken bei der Verarbeitung der zinnreichen Erze abgeleitet werden kann.[17]
Am Ende des 3. Jahrtausends v. Chr. (botanische Datierungen auf 2021 und 2016 v. Chr.) ließen im Elbtal ansässige Eliten Jahr für Jahr in den Sommermonaten Zinngraupen an der Roten Weißeritz bei Schellerhau durchgraben. Die Arbeiter lebten in der Saison in einfachen Laubhütten, das Zinn wurde in die festen Siedlungen im Elbtal geschafft, welche dadurch prosperierten und zu Reichtum und Ansehen kamen. Das Erzgebirge entwickelte sich damals zu einem zentralen Lieferanten für ganz Europa. Zinn war für die Bronzeherstellung wesentlich. Die in Schellerhau vom Forschungsprojekt Archeo Montan entdeckten Spuren des Bergbaus sind die derzeit ältesten in Europa.[18]
Durch die Legierung Bronze, deren Bestandteile Kupfer und Zinn sind, gelangte Zinn zu größerer Bedeutung (Bronzezeit). Für Ägypten bestätigt sich die Verwendung von Zinn durch Funde kleiner Bronzestatuetten aus der Zeit der Pyramiden (4. Dynastie, um 2500 v. Chr.).[12] Auch in einem ägyptischen Grabmal aus der 18. Dynastie (um 1500 v. Chr.) wurden Gegenstände aus Zinn gefunden. In Indien war die Bronzeherstellung bereits um 3000 v. Chr. bekannt.
Seit dem 2. Jahrtausend v. Chr. wurde Zinn in Mittelasien an der Route der späteren Seidenstraße nachweislich in größerem Maße in Bergwerken abgebaut. Ab etwa 1800 v. Chr. (Shang-Dynastie) ist Zinn in China bekannt. Ein Schriftwerk über die Künste jener Zeit, das Kaogong ji (Zhou-Dynastie, ab 1122 v. Chr.), beschreibt eingehend die Mischungsverhältnisse von Kupfer und Zinn, die je nach Art der für sakrale Gefäße, Gongs, Schwerter und Pfeilspitzen, Äxte oder Ackerbaugerät zu verwendenden Bronze verschieden waren.
Bereits früher dürfte es in den eigentlichen asiatischen Lagerstätten in Yunnan und auf der Halbinsel Malakka bekannt gewesen sein. Im Tal des Euphrat wurden seit 2000 v. Chr. Bronzegeräte und deren Herstellung zu einem bedeutenden Kulturfaktor; die Technik wurde dann von Griechen und Römern weiterentwickelt.
Die Ausbreitung des Handels mit Zinn bestätigt ebenfalls seine frühe und weitreichende Nutzbarmachung. Es wurde zunächst aus Zentralasien mit Karawanen in die Gebiete des heutigen Nahen und Mittleren Ostens gebracht. Dort holte man sich das Zinnerz ab dem 3. Jahrtausend v. Chr. aus den Lagerstätten des alten Reiches Elam östlich des Tigris und aus den Bergen von Chorasan an der persischen Grenze zu Turkmenistan und Afghanistan. Von dort scheint man es in das Land der Pharaonen weitergeliefert zu haben. In der Bibel wird Zinn im 4. Buch Mose erstmals erwähnt (Numeri 31,22 EU).
Die Phönizier hatten wahrscheinlich auf dem Seeweg Verbindungen mit den zinnreichen indischen Inseln Malakka und Bangka, ohne dass dazu genaue Angaben zu machen sind. Später transportierten die Phöniker das Zinnerz mit ihren Schiffen entlang der spanischen und französischen Küstengebiete bis zu den Inseln in der Nordsee. Auf diesen Fahrten entdeckten sie auf den sogenannten Zinninseln, zu denen möglicherweise die Insel Wight gehörte, und in den Bergen von Cornwall zinnreiche Gebiete, bauten dort das Erz ab und führten es in andere Länder aus. In kleinerem Maße begann der Zinnerzabbau in handelsmöglichen Ausmaßen auch in Frankreich (u. a. am Cap de l'Etain), in Spanien (Galicien) und in Etrurien (Cento Camerelle bei Campiglia Marittima).
In den Epen Homers sowie bei Hesiod tauchen Zinneinlagen als Schmuckornament an Streitwagen und Wehrschilden des Agamemnon sowie des Herakles auf; für Achilles werden zinnerne (wohl »verzinnte«) Beinschienen beschrieben.[19] Durch Plautus wird Zinn erstmals als Geschirr für Speisen erwähnt.[20] Als Gebrauchsmetall für Geschirr war es bei den Griechen wohl nicht bekannt.[20] Das Zinn, das die Griechen für den Bronzeguss benutzten, stammte nach Herodot von den Kassiteriden, deren geographische Lage diesem aber unbekannt war. Diese Inseln werden auch von Strabon erwähnt und beschrieben, der sie weit nördlich von Spanien lokalisiert, in der Nähe Britanniens.[21]
Der römische Schriftsteller Plinius der Ältere nannte Zinn in seiner Naturgeschichte plumbum album („weißes Blei“); Blei hingegen war plumbum nigrum („schwarzes Blei“). Er beschreibt daneben auch das Verzinnen von Kupfermünzen und berichtet von zinnernen Spiegeln und Ampullen und beschreibt, dass Bleiwasserrohre mit Zinnlegierung verlötet wurden.[20] Die hohe Nachfrage nach dem in der Alchemie dem Jupiter allegorisch bzw. als Schöpfer[22] zugeordneten Zinn[23] wird sogar als ein Grund für die römische Besetzung Britanniens angeführt. In der südwestlichen Region Cornwall wurde von 2100 v. Chr. bis 1998 Zinnerz gefördert, in der Antike ein wichtiger Zinnlieferant des Mittelmeerraums und bis ins späte 19. Jahrhundert der größte der Welt. Im Lateinischen heißt Zinn stannum, daher rührt auch das chemische Symbol (Sn).
Während der Völkerwanderung erlahmte der bergmännische Abbau von Zinnerzen völlig. Nur wenige Kultgegenstände wurden noch gefertigt. Im Konzil von Reims (813) wird neben Gold und Silber ausdrücklich nur Zinn für die Herstellung solcher Gegenstände gestattet. Die Gräberfunde von Capetiennes bestätigen dies insofern, als es zur Zeit der ersten Kreuzzüge üblich war, Priester mit Zinnkelchen und Bischöfe wie auch Äbte mit Zinnkrummstäben beizusetzen.
Der Brauch, kleine Bildnisse aus Zinnlegierung, sogenannte Pilgerzeichen, auf der Brust zu tragen, stammt vermutlich ebenfalls aus der Zeit der Kreuzzüge. Je nach Region waren dies in Mittel- und Südfrankreich St. Denis bzw. St Nicolas, in England der Heilige Thomas von Canterbury. Die von den palästinischen Pilgerorten heimgebrachten religiösen Münzen und Ampullen, kleinen Glöckchen und Pfeifen waren aus Zinn. Sie mussten nach anerkanntem Vollzug der Pilgerfahrt zur Abwendung eventuellen Missbrauchs in Flüsse und Seen geworfen werden.[24]
Ab 1100 begann die Bevölkerung in Europa nach und nach das bisher aus Ton und Holz bestehende Essgeschirr durch solches aus dem stabileren Zinn zu ersetzen. Um 1200 begann in den größeren Städten die handwerkliche Verarbeitung des Zinns in Zinngießereien.
Die Venezianer pflegten damals Handelsbeziehungen zu den zinnreichen indischen Inseln Malakka und Bangka.
Lange nachdem Bronze durch Eisen verdrängt worden war (Eisenzeit), erlangte Zinn Mitte des 19. Jahrhunderts durch die industrielle Herstellung von Weißblech von Neuem große Bedeutung.
Primäre Zinnvorkommen umfassen Greisen-, hydrothermale Gang- und seltener auch Skarn- und Vulkanisch-exhalative-Lagerstätten (VHMS). Da das wirtschaftlich bedeutendste Zinnmineral Kassiterit SnO2, auch Zinnstein genannt, ein sehr stabiles Schwermineral ist, kommt ein großer Teil der Zinnproduktion auch aus sekundären Seifenlagerstätten. In einigen primären Lagerstätten besitzt auch das Sulfidmineral Stannit Cu2FeSnS4 Bedeutung für die Zinnproduktion. Auf primären Zinnlagerstätten kommt das Element oft mit Arsen, Wolfram, Bismut, Silber, Zink, Kupfer und Lithium vergesellschaftet vor.
Zur Gewinnung von Zinn wird das Erz zuerst zerkleinert und dann durch verschiedene Verfahren (Aufschlämmen, elektrische/magnetische Scheidung) angereichert. Nach der Reduktion mit Kohlenstoff wird das Zinn knapp über seine Schmelztemperatur erhitzt, so dass es ohne höher schmelzende Verunreinigungen abfließen kann. Heute gewinnt man einen Großteil durch Recycling und hier durch Elektrolyse.
In der kontinentalen Erdkruste ist es mit einem Anteil von etwa 2,3 ppm vorhanden.[25]
Die aktuellen Reserven für Zinn werden mit 4,7 Millionen Tonnen angegeben, bei einer Jahresproduktion von 289.000 Tonnen im Jahr 2015.[26] Zu über 80 % kommt die Produktion derzeit aus Seifenlagerstätten (Sekundärlagerstätten) an Flüssen sowie im Küstenbereich, vornehmlich aus einer Region beginnend in Zentralchina über Thailand bis nach Indonesien. Die größten Zinnvorkommen der Erde wurden 1876 im Kinta Valley (Malaysia) entdeckt. Dort wurden bis heute etwa 2 Millionen Tonnen geschürft.[27] Das Material in den Schwemmlandlagerstätten hat einen Metallanteil von etwa 5 %. Erst nach verschiedenen Schritten zur Konzentrierung auf etwa 75 % wird ein Schmelzprozess eingesetzt.
In Deutschland sind größere Ressourcen im Erzgebirge vorhanden, wo das Metall vom 13. Jahrhundert an bis 1990 gewonnen wurde. Beispiele sind die Greisenlagerstätte Altenberg und die Skarnlagerstätte Pöhla. Durch verschiedene Firmen findet derzeit auch Exploration auf Zinn im Erzgebirge statt. Im August 2012 veröffentlichte erste Untersuchungsergebnisse für die Orte Geyer und Gottesberg, einen Ortsteil von Muldenhammer, lassen Vorkommen in Höhe von rund 160.000 Tonnen Zinn für beide Orte insgesamt vermuten. Diese Zahlen bestätigen prinzipiell auch Angaben, wie sie nach zu DDR-Zeiten vorgenommenen Prospektionen geschätzt wurden. Nach Aussage der Deutschen Rohstoff AG handelt es sich um das weltweit größte noch unerschlossene Zinnvorkommen. Da einerseits der Erzgehalt mit 0,27 Prozent für Gottesberg und 0,37 Prozent für Geyer verhältnismäßig gering ist, andererseits das Erz verhältnismäßig schwer aus dem Gestein zu lösen ist, ist offen, ob sich der Abbau wirtschaftlich lohnen würde. Sollte es dazu kommen, würden als Nebenprodukt auch Zink, Kupfer und Indium anfallen.[28]
Die bedeutendste Fördernation für Zinn ist China, gefolgt von Indonesien und Myanmar. In Europa war 2009 Portugal der größte Produzent, wo es als Beiprodukt der VHMS-Lagerstätte Neves Corvo gefördert wird (VHMS: volcanic-hosted massive sulfide).
Der Jahresweltverbrauch an Zinn liegt bei etwa 300.000 t. Davon werden etwa 35 % für Lote, etwa 30 % für Weißblech und etwa 30 % für Chemikalien und Pigmente eingesetzt. Durch die Umstellung der Zinn-Blei-Lote auf bleifreie Lote mit Zinnanteilen > 95 % wird der jährliche Bedarf um etwa 10 % wachsen. Die Weltmarktpreise steigen in den letzten Jahren kontinuierlich. So wurden an der LME (London Metal Exchange) 2003 noch etwa 5000 US-Dollar pro Tonne bezahlt, im Oktober 2021 jedoch mehr als 35.000 US-Dollar pro Tonne.[29][30] Die zehn größten Zinnverbraucher (2003) weltweit sind nach China auf Platz 1 die Länder USA, Japan, Deutschland, übriges Europa, Korea, übriges Asien, Taiwan, Großbritannien und Frankreich.
Die weltweite Finanzkrise ab 2007 sowie ein schwaches Wirtschaftswachstum in den Schwellen- und Entwicklungsländern setzte den Preis unter Druck. Im August 2015 sank der Preis je Tonne kurzfristig auf unter 14.000 US-Dollar. Im Oktober 2015 hatte der Preis sich wieder leicht auf rund 16.000 US-Dollar erholt. Durch den starken US-Dollar kommt der günstige Preis nur teilweise in vielen Verbraucherländern an.[31] Die weltweite Produktion lag 2020 bei rund 264.000 Tonnen, von denen alleine 84.000 Tonnen in China gefördert worden sind; weitere 53.000 Tonnen stammten aus Indonesien. 
Kassiterit wurde von der US-amerikanischen Börsenaufsicht SEC als sogenanntes „conflict mineral“ eingestuft,[32] dessen Verwendung für Unternehmen gegenüber der SEC berichtspflichtig ist. Als Grund hierfür werden die Produktionsorte im Osten des Kongo angeführt, die von Rebellen kontrolliert werden und so im Verdacht stehen, bewaffnete Konflikte mitzufinanzieren.[33] Zinn steht außerdem auf der Liste kritischer Rohstoffe der USA[34], während die EU Zinn nicht als kritischen Rohstoff einstuft.[35]
Zinn wird auch – erneut seit 2019 – bei Uis in Namibia abgebaut. Die Vorkommen gelten als eine der größten weltweit.[36]
Zinn kann drei Modifikationen mit verschiedener Kristallstruktur und Dichte annehmen:
Daneben kann noch eine zweidimensionale Modifikation namens Stanen (ähnlich der Kohlenstoffmodifikation Graphen) synthetisiert werden.
Natürliches Zinn besteht aus zehn verschiedenen stabilen Isotopen; das ist die größte Anzahl unter allen Elementen. Außerdem sind noch 28 radioaktive Isotope bekannt.
Die Rekristallisation von β-Zinn zu α-Zinn bei niedrigen Temperaturen äußert sich als die sogenannte Zinnpest.
Beim Verbiegen des relativ weichen Zinns, beispielsweise von Zinnstangen, tritt ein charakteristisches Geräusch, das Zinngeschrei (auch Zinnschrei), auf. Es entsteht durch die Reibung der β-Kristallite aneinander. Das Geräusch tritt jedoch nur bei reinem Zinn auf. Bereits niedrig legiertes Zinn zeigt diese Eigenschaft nicht; z. B. verhindern geringe Beimengungen von Blei oder auch Antimon das Zinngeschrei. Das β-Zinn hat einen abgeflachten Tetraeder als Raumzellenstruktur, aus dem sich zusätzlich zwei Verbindungen ausbilden.
Durch die Oxidschicht, mit der Zinn sich überzieht, ist es sehr beständig. Von konzentrierten Säuren und Basen wird es allerdings unter Entwicklung von Wasserstoffgas zersetzt. Jedoch ist Zinn(IV)-oxid ähnlich inert wie Titan(IV)-oxid. Zinn wird von unedleren Metallen (z. B. Zink) reduziert; dabei scheidet sich elementares Zinn schwammig oder am Zink haftend ab.
Zinn besitzt insgesamt zehn natürlich vorkommende Isotope. Diese sind 112Sn, 114Sn, 115Sn, 116Sn, 117Sn, 118Sn, 119Sn, 120Sn, 122Sn und 124Sn. 120Sn ist dabei mit 32,4 % Anteil an natürlichem Zinn das häufigste Isotop. Von den instabilen Isotopen ist 126Sn mit einer Halbwertszeit von 230.000 Jahren das langlebigste.[38] Alle anderen Isotope haben eine Halbwertzeit von nur maximal 129 Tagen, jedoch existiert bei 121Sn ein Kernisomer mit 44 Jahren Halbwertzeit.[38] Als Tracer werden am häufigsten die Isotope 113Sn, 121Sn, 123Sn und 125Sn verwendet. Zinn hat als einziges Element drei stabile Isotope mit ungerader Massenzahl und mit zehn stabilen Isotopen die meisten stabilen Isotope von allen Elementen überhaupt.
Als qualitative Nachweisreaktion für Zinnsalze wird die Leuchtprobe durchgeführt: Die Lösung wird mit ca. 20%iger Salzsäure und Zinkpulver versetzt, wobei naszierender Wasserstoff frei wird. Der naszierende, atomare Wasserstoff reduziert einen Teil des Zinns bis zum Stannan SnH4. In diese Lösung wird ein Reagenzglas eingetaucht, das mit kaltem Wasser und Kaliumpermanganatlösung gefüllt ist; das Kaliumpermanganat dient hier nur als Kontrastmittel. Dieses Reagenzglas wird im Dunkeln in die nichtleuchtende Bunsenbrennerflamme gehalten. Bei Anwesenheit von Zinn entsteht sofort eine typisch blaue Fluoreszenz, hervorgerufen durch SnH4.[1][39]
Zur quantitativen Bestimmung von Zinn eignet sich die Polarographie. In 1 ᴍ Schwefelsäure ergibt Zinn(II) eine Stufe bei −0,46 V (gegen Kalomelelektrode, Reduktion zum Element). Stannat(II) lässt sich in 1 ᴍ Natronlauge zum Stannat(IV) oxidieren (−0,73 V) oder zum Element reduzieren (−1,22 V).[40] Im Ultraspurenbereich bieten sich die Graphitrohr- und Hydridtechnik der Atomspektroskopie an. Bei der Graphitrohr-AAS werden Nachweisgrenzen von 0,2 µg/l erreicht. In der Hydridtechnik werden die Zinnverbindungen der Probelösung mittels Natriumborhydrid als gasförmiges Stannan in die Quarzküvette überführt. Dort zerfällt das Stannan bei ca. 1000 °C in die Elemente, wobei der atomare Zinndampf spezifisch die Sn-Linien einer Zinn-Hohlkathodenlampe absorbiert. Hier sind 0,5 µg/l als Nachweisgrenze angegeben worden.[41]
Weitere qualitative Nachweisreagenzien sind Diacetyldioxim, Kakothelin, Morin und 4-Methylbenzol-1,2-dithiol. Zinn kann auch mikroanalytisch über die Bildung von Goldpurpur nachgewiesen werden.[42]
Metallisches Zinn ist auch in größeren Mengen an sich ungiftig. Die Giftwirkung einfacher Zinnverbindungen und Salze ist gering. Einige organische Zinnverbindungen dagegen sind hochtoxisch. Die Trialkyl-Zinnverbindungen (insbesondere TBT, engl. „Tributyltin“, Tributylzinn) und Triphenylzinn wurden mehrere Jahrzehnte in Anstrichfarben für Schiffe verwendet, um die sich an den Schiffsrümpfen festsetzenden Mikroorganismen und Muscheln abzutöten. Dadurch kam es in der Umgebung von großen Hafenstädten zu hohen Konzentrationen an TBT im Meerwasser, die die Population diverser Meereslebewesen bis heute beeinträchtigen. Die toxische Wirkung beruht auf der Denaturierung einiger Proteine durch die Wechselwirkung mit dem Schwefel aus Aminosäuren wie beispielsweise Cystein.
In der EU werden die Höchstmengen an verschiedenen Metallen wie Zinn in Lebensmitteln durch die Verordnung (EG) Nr. 1881/2006 geregelt. Die jeweiligen Höchstgrenzen hängen dabei vom Erzeugnis ab und orientieren sich auch daran, was durch gute Herstellungspraxis oder gute landwirtschaftliche Praxis erreichbar ist. Für anorganisches Zinn gibt es verschiedene Grenzwerte für Erzeugnisse in Dosen: Lebensmittelkonserven (200 mg/kg, außer Dosengetränke 100 mg/kg), Beikost für Säuglinge und Kleinkinder, Säuglingsanfangs- und -folgenahrung, sowie diätetische Lebensmittel für Säuglinge für medizinische Zwecke in Dosen (50 µg/kg).[43]
Seit Jahrhunderten wird reines Zinnblech großflächig zur Herstellung von Orgelmetall im Sichtbereich verwendet. Diese behalten ihre silbrige Farbe über viele Jahrzehnte. Das weiche Metall wird aber in der Regel in einer Legierung mit Blei, dem sogenannten Orgelmetall, verwendet und hat für die Klangentfaltung sehr gute vibrationsdämpfende Eigenschaften. Zu tiefe Temperaturen sind wegen der Umwandlung in α-Zinn für Orgelpfeifen schädlich; siehe Zinnpest. Viele Haushaltsgegenstände, Zinngerät (Geschirr), Tuben, Dosen und auch Zinnfiguren wurden früher ganz aus Zinn gefertigt, rundweg der einfacheren Verarbeitungstechnologie der Zeit entsprechend. Mittlerweile jedoch wurde das relativ kostbare Material meist durch preiswertere Alternativen ersetzt. Ziergegenstände und Modeschmuck werden weiterhin aus Zinnlegierungen, Hartzinn bzw. Britanniametall hergestellt.
Seit dem Mittelalter war Zinngießer ein spezieller Handwerksberuf, der sich bis heute, allerdings in ganz geringem Umfang, erhalten hat. Er ist heute rechtlich in der Berufsbezeichnung Metall- und Glockengießer/-in aufgegangen. Aufgabe des Zinnputzers war die Reinigung von vor allem oxidierten, aus Zinn gefertigten Gegenständen mit einem Kaltwasserauszug des Ackerschachtelhalms, der volkstümlich deshalb auch Zinnkraut genannt wurde. Es war ein eher wenig angesehenes Wandergewerbe und wurde in den Häusern bürgerlicher oder großbäuerlicher Haushalte ausgeübt.
Als Legierungsbestandteil wird Zinn vielfältig verwendet, mit Kupfer zu Bronze oder anderen Werkstoffen legiert. Nordisches Gold, die Legierung der goldfarbigen Euromünzen, beinhaltet unter anderem 1 % Zinn. Algerisches Metall enthält 94,5 % Zinn.
Als Bestandteil von Metall-Legierungen mit niedrigem Schmelzpunkt ist Zinn unersetzlich. Weichlot (sogenanntes Lötzinn) zur Verbindung elektronischer Bauteile (beispielsweise auf Leiterplatten) wird mit Blei (eine typische Mischung ist etwa 63 % Sn und 37 % Pb) und anderen Metallen in geringerem Anteil legiert. Die Mischung schmilzt bei etwa 183 °C. Seit Juli 2006 darf jedoch kein bleihaltiges Lötzinn in elektronischen Geräten mehr verwendet werden (siehe RoHS); man setzt nun bleifreie Zinnlegierungen mit Kupfer und Silber ein, z. B. Sn95.5Ag3.8Cu0.7 (Schmelztemperatur ca. 220 °C).
Da man aber diesen Legierungen nicht traut (Zinnpest und „Tin whiskers“), ist bei der Fertigung elektronischer Baugruppen für Medizintechnik, Sicherheitstechnik, Messgeräte, Luft- u. Raumfahrt sowie für militärische/polizeiliche Verwendung weiterhin die Verwendung bleihaltiger Lote zulässig. Im Gegenteil ist der Einsatz bleifreien Lotes in diesen sensiblen Bereichen trotz RoHS verboten.
Hochreine Zinn-Einkristalle eignen sich auch zur Herstellung von elektronischen Bauteilen.
In der Floatglasherstellung schwimmt die zähflüssige Glasmasse bis zur Erstarrung auf einer spiegelglatten flüssigen Zinnschmelze.
Zinnverbindungen werden dem Kunststoff PVC als Stabilisatoren beigemischt. Tributylzinn dient als sog. Antifouling-Zusatz in Anstrichstoffen für Schiffe und verhindert den Bewuchs der Schiffskörper, es ist mittlerweile jedoch umstritten und weitgehend verboten.
In Form einer transparenten Zinnoxid-Indiumoxid-Verbindung ist es elektrischer Leiter in Anzeigegeräten wie LC-Displays. Das reine, weiße, nicht sehr harte Zinndioxid besitzt eine hohe Lichtbrechung und wird im optischen Bereich und als mildes Poliermittel eingesetzt. In der Zahnheilkunde wird Zinn auch als Bestandteil von Amalgamen zur Zahnfüllung eingesetzt. Die sehr toxischen organischen Zinnverbindungen finden als Fungizide oder Desinfektionsmittel Verwendung.
Zinn wird anstelle von Blei auch zum Bleigießen verwendet.
Stannum metallicum („metallisches Zinn“) findet auch bei der Herstellung von homöopathischen Arzneimitteln sowie als Bandwurmgegenmittel Verwendung.
Unter der Bezeichnung Argentin wurde Zinnpulver früher zur Herstellung von unechtem Silberpapier und unechter Silberfolie verwendet.
Weißblech ist verzinntes Eisenblech, es wird beispielsweise für Konservendosen oder Backformen verwendet. Tin, das englische Wort für Zinn, ist gleichzeitig ein englisches Wort für Dose bzw. Konservenbüchse.
Zu dünner Folie gewalzt nennt man es auch Stanniol, das beispielsweise für Lametta Verwendung findet. Jedoch ist Zinn im 20. Jahrhundert durch das viel preiswertere Aluminium verdrängt worden. Bei manchen Farbtuben und Weinflaschenverschlüssen findet Zinn ebenfalls Verwendung.
Zinn wird in der EUV-Lithografie zur Herstellung von integrierten Schaltkreisen („Chips“) – als notwendiger Bestandteil bei der Erzeugung von EUV-Strahlung durch Zinn-Plasma – eingesetzt.[44]
Zinn wird auch für Diabolos als ungiftige Alternative zu Blei verwendet.
Zinnverbindungen kommen in den Oxidationsstufen +II und +IV vor. Zinn(IV)-Verbindungen sind stabiler, da Zinn ein Element der 4. Hauptgruppe ist und zudem der Effekt des inerten Elektronenpaares noch nicht so stark ausgeprägt ist wie bei den schwereren Elementen dieser Gruppe, z. B. dem Blei. Zinn(II)-Verbindungen lassen sich deshalb leicht in Zinn(IV)-Verbindungen umsetzen. Viele Zinnverbindungen sind anorganischer Natur, es ist aber auch eine Reihe von zinnorganischen Verbindungen (Zinnorganylen) bekannt.
Eine Übersicht über weitere Zinnverbindungen bietet die Kategorie:Zinnverbindung.

Als Wetter (von althochdeutsch wetar „Wind, Wehen“) bezeichnet man den spürbaren, kurzfristigen Zustand der Atmosphäre (auch: messbarer Zustand der Troposphäre) an einem bestimmten Ort der Erdoberfläche, der unter anderem als Sonnenschein, Bewölkung, Regen, Wind, Hitze oder Kälte in Erscheinung tritt.
Die Meteorologie klassifiziert das örtliche Wetter einer bestimmten Zeit anhand der verschiedenen Phänomene in der Troposphäre, dem unteren Teil der Atmosphäre. Den Verlauf des Wetters bestimmt die von Sonnenstrahlung und regionaler Energiebilanz geprägte atmosphärische Zirkulation.
Physikalisch lässt sich ein Wetter durch thermodynamische Zustandsgrößen wie etwa Druck, Temperatur, Dichte beschreiben. Ein „Wetter“ in diesem Sinne kann auch in einem Labor erzeugt werden. Darüber hinaus gibt es solche Zustände und Wetterphänomene (zum Beispiel Winde) auch auf anderen Planeten, die eine Atmosphäre haben.
Das Wetter charakterisiert den Zustand der Atmosphäre an einem bestimmten Ort und zu einem bestimmten Zeitpunkt. Kennzeichnend sind die meteorologischen Elemente Strahlung, Luftdruck, Lufttemperatur, Luftfeuchtigkeit und Wind, sowie die daraus ableitbaren Elemente Bewölkung, Niederschlag, Sichtweite etc. Das Wetter ist das augenblickliche Bild eines Vorganges (Wettergeschehen), das sich hauptsächlich in der Troposphäre abspielt. Es kann sich – im Gegensatz zur Wetterlage und Witterung – mehrmals täglich ändern.
Das Wetter kann man als ein System betrachten, das vor allem von den Elementen Temperatur, Niederschlag, Bewölkung, Wind und Luftdruck geprägt wird. Zwischen einigen der Elemente bestehen Zusammenhänge (Korrelation oder Kausalität), zwischen anderen nicht.
Die Meteorologen erfassen die einzelnen Elemente des Wetters mit Messgeräten und die Wetterlage mit Begriffen wie stabil oder wechselhaft, heiter oder wolkenfrei, 3/8 bewölkt, bedeckt oder trüb, Nebeltendenz, regnerisch, Regenschauer oder stürmisch.
Umgangssprachlich sind sehr unscharfe Begriffe üblich:
Die Meteorologie untersucht das Wetter, quantifiziert seine einzelnen Elemente und charakterisiert sie durch eine Reihe fundamentaler sowie spezieller Größen (Wetterelemente):
Diese Grundgrößen werden in Wetterstationen, auf Wetterschiffen und Leuchttürmen, mit Wetterballons oder Radiosonden, mit Flugzeugen und Bojen gemessen. Wettersatelliten, andere Erdbeobachtungssatelliten und Spionagesatelliten (letztere liefern Wetterinformationen als 'Nebenprodukt') beobachten die Troposphäre aus dem Weltall und sammeln besonders viele Informationen zur Bewölkung (auch zu großflächigen Wolkensystemen), zu Wellenhöhen und Wasseroberflächentemperaturen auf Meeren und zu Luftströmungen.
Messinstrumente die der Messung von Wetterelemente dienen nennt man Wettermessgerät (siehe auch Wetterstation, Wetterhäuschen) bzw. danach was sie messen (z. B. Windmesser, Regenmesser, Hygrometer, Thermometer).
Das Wetter findet fast ausschließlich in den unteren 10 Kilometern der irdischen Lufthülle statt, der Troposphäre. Nur hier gibt es merkliche Bewölkung, weil der Wasserdampf als entscheidender Faktor nicht über die Tropopause (je nach Ort und Jahreszeit etwa 8 bis 15 km hoch) hinaus gelangen kann.
Überwiegend prägen die unteren 2 km der Peplosphäre das Wetter. Hier findet sich oft Dunst durch Anreicherung von Aerosolen, und die nächtliche Abkühlung durch Wärmestrahlung. Die Bodenreibung bremst den geostrophischen Wind, weshalb er mehr in Richtung zum tieferen Druck weht als in größerer Höhe.
Der primäre Motor des Wetters ist die Energieeinstrahlung der Sonne und die Abstrahlung (Licht und Infrarot) zu den Wolken bzw. in den Weltraum. Das erfassen heute neben terrestrischen Messungen auch großräumig Satelliten und Wetterschiffe, Radiosonden und andere moderne Methoden gut.
Für den Verlauf des Wetters sind jedoch die Strömungs-Verhältnisse in der Atmosphäre entscheidend, die von ihrer wechselnden Feuchtigkeit und den globalen Windsystemen abhängen, ferner von der regional unterschiedlichen Wärmereflexion der Erdoberfläche (Albedo), vom Gelände (insbesondere den Gebirgen, Küsten und Wüsten) und von starken lokalen Einflüssen (zyklische Winde, Neigung und Bewuchs von Berghängen …), und vom Widerstand gegen Winde, über den die Rauheit der Oberfläche (Wälder, Windschneisen, große Gebäude usw.) entscheidet.
Daher sind in Mitteleuropa nur dann lokal exakte Wetterprognosen möglich, wenn alle diese Einzelheiten einer Modellierung oder verlässlichen Erfahrung zugänglich sind. Letztere wissen auch Laien zu nutzen – siehe die vielfach bewährten Bauernregeln mit „wetterzeigenden“ Bergen (Wetterstein, Wolkenstein usw.) oder typischen Wolken-Formationen wie Schönwetter- und Schäfchenwolken, Nebel, Regen- und Fetzenwolken, Cirren, Föhnmauern usw.
Hauptartikel: Wettervorhersage
Ausgehend vom durch großflächige Messungen erfassten Wetter und damit dem Zustand der Atmosphäre werden in der Meteorologie Wettermodelle genutzt, um die weitere Entwicklung des Wetters zu prognostizieren. Davon abgesehen ist es jedoch auch möglich, auf lokaler Ebene und mit vergleichsweise wenig Hilfsmitteln gute Vorhersagen zu geben, wozu jedoch auch mehr oder weniger umfangreiche Kenntnisse notwendig sind.
Für eine Reihe von Unternehmen hat das Wetter Auswirkungen auf die betrieblichen Erfolgsgrößen. Klassische Beispiele dafür sind die Landwirtschaft und die Getränkeindustrie, bei denen Wetter sich stark auf den Umsatz auswirken kann. Während bei der Landwirtschaft überwiegend die Erntemengen betroffen sind, schwankt bei den Abfüllern von Mineralwasser und Erfrischungsgetränken der Absatz in Abhängigkeit zur Temperatur. Zu den weiteren Branchen, bei denen sich das Wetter stark auswirken kann, gehören die Baubranche sowie die Tourismus- und Freizeitindustrie. Für einige Unternehmen kann das Wetterrisiko so signifikant sein, dass es gezielt im Risikomanagement des Unternehmens beobachtet und beispielsweise über so genannte Wetterderivate abgesichert wird.
Das Landgericht Cottbus beurteilte 2012 Wetter als höhere Gewalt. Demnach geht schlechtes Wetter nicht zu Lasten des Auftraggebers; es gehört nicht zur Risikosphäre eines Bestellers von Bauleistungen.[1]
Die Wetterlage spielt bei vielen kriegerischen Auseinandersetzungen eine wichtige Rolle. Beispiele:
Seit Anfang der 1950er Jahre forscht auch das Militär über Möglichkeiten, das Wetter lokal zu beeinflussen. Eine Anwendung solcher Techniken wäre jedoch ein Verstoß gegen die ENMOD-Konvention.
Wettergeschehen haben in verschiedenster Weise Auswirkungen auf das körperliche Befinden von Lebewesen. So schrieb Herman Boerhaaves Schüler Thomas Schwenke, Verfasser der Schrift Haematologia, dem Wetter einen besonderen Einfluss auf die Blutgerinnung zu.[2] Beim Menschen spricht man u. a. von „Wetterfühligkeit“, womit sich die Disziplin der Meteorotropie genauer befasst.
Für allgemein meteorologische Literatur siehe Meteorologie.
Deutschland
Österreich
Schweiz
Belgien
Luxemburg

Der Magnetismus ist eine physikalische Erscheinung, die sich unter anderem als Kraftwirkung zwischen Magneten, magnetisierten bzw. magnetisierbaren Gegenständen und bewegten elektrischen Ladungen äußert. Er lässt sich beschreiben durch ein Feld (Magnetfeld), das einerseits von diesen Objekten erzeugt wird und andererseits auf sie wirkt.
Magnetfelder entstehen zum einen bei jeder Bewegung von elektrischen Ladungen. Das ist Grundlage von Elektromagneten und wegen des Induktionsgesetzes auch der induktiven elektronischen Bauelemente. Zum anderen existiert das magnetische (Dipol-)moment von Elementarteilchen als Folge ihres Spins, was zu Dauermagneten und anderen magnetischen Eigenschaften von Festkörpern, aber auch Flüssigkeiten und Gasen führt.
Der Magnetismus ist ein Teilgebiet des Elektromagnetismus. Die zugrundeliegende Grundkraft heißt elektromagnetische Wechselwirkung.
Um die Erscheinungen des Magnetismus zu beschreiben, führte man den Begriff des Magnetfelds ein. Magnetfelder können verursacht werden durch
Magnetische Feldlinien veranschaulichen in jedem Punkt des Feldes Richtung und Richtungssinn des Magnetfeldes bzw. des magnetischen Flusses. Diese Richtung wird dahin festgelegt, wie sich der Nordpol eines Probemagneten ausrichten würde. Die Stärke des Magnetfeldes ist proportional zum Drehmoment, das dieser Probemagnet erfahren würde, wenn man ihn um einen bestimmten Winkel aus dieser Richtung auslenkt. Der Abstand der Feldlinien zeigt die Stärke des Magnetfeldes an: Je dichter die Feldlinien, desto stärker das Feld.
In der Magnetostatik gibt es im Gegensatz zur Elektrostatik keine Ladungen – echte magnetische Monopole sind zwar denkbar, alle experimentellen Tatsachen sprechen aber gegen ihre Existenz. Somit ist das Magnetfeld quellenfrei.[1] Magnetische Feldlinien haben daher keinen Anfang und kein Ende.[2][3]
Der Verlauf magnetischer Feldlinien kann durch die Ausrichtung von Eisenfeilspänen oder einer Kompassnadel sichtbar gemacht werden; für dreidimensionale Demonstrationen kann man die Eisenfeilspäne zum Beispiel in Silikonöl suspendieren.
Hall-Sonden sind elektronische Sensoren auf Basis des Hall-Effektes, die Stärke und oft auch Richtung der Magnetfelder messen können.
Ein Stabmagnet an der Erdoberfläche richtet sich bei Fehlen anderer Kräfte so aus, dass eines seiner Enden in Richtung Norden, zum arktischen Magnetpol, und das andere in Richtung des antarktischen Magnetpols zeigt. Das nach Norden zeigende Ende wird Nordpol des Magneten genannt. Durch Definition wurde festgelegt, dass am Nordpol eines Magneten die Feldlinien aus dem Magneten aus- und an seinem Südpol in ihn eintreten. Deshalb bezeichnet man allgemein bei Elektromagneten oder Permanentmagneten Gebiete, aus denen die Feldlinien austreten, als Nordpol und Gebiete, in die sie eintreten, als Südpol.
Da der Nordpol des Magneten vom arktischen Magnetpol angezogen wird, ist der arktische Magnetpol ein magnetischer Südpol. Gleiches gilt umgekehrt für den Südpol des Magneten und den antarktischen Magnetpol.
Das magnetische Feld übt auf bewegte elektrische Ladungen  die sogenannte Lorentzkraft  aus. Sie ist proportional zur Geschwindigkeit , wirkt senkrecht zu den Feldlinien des Magnetfeldes und senkrecht zur Bewegungsrichtung der Ladung. Sie ist die Grundlage von Elektromotoren und Generatoren sowie der Ablenkung bewegter geladener Teilchen (z. B. mit Ablenkspulen). Mit einem statischen Magnetfeld wird dabei keine Energie ausgetauscht.
Das magnetische Feld übt ferner Kräfte auf Magnete und magnetisierbare Körper (Ferrimagnetismus bestimmter nichtmetallischer Festkörper, sog. Ferrite, und Ferromagnetismus von Metallen wie Eisen) aus. Magnete und gestreckte Probekörper aus magnetisierbaren Materialien richten sich immer längs der Feldlinien beziehungsweise antiparallel zu diesen aus, das heißt, der magnetische Südpol eines Probemagneten richtet sich entlang der Feldlinien zum Nordpol des erzeugenden Feldes aus. Dieser Effekt wird zum Beispiel beim magnetischen Kompass ausgenutzt, bei dem sich die Kompassnadel, ein magnetischer Dipol, nach dem Erdmagnetfeld ausrichtet. Zudem werden in inhomogenen Feldern magnetisierbare Körper in Richtung steigender Feldstärke gezogen, siehe Gradient, Anwendungen sind Elektromagnete und der Reluktanzmotor. Das gilt auch für Magnete, die sich frei ausrichten können. In entgegengesetzter Richtung orientierte Magnete werden dagegen abgestoßen.
Ursache für diese Beobachtungen ist, dass ein energieärmerer Zustand eingenommen wird – die Kräfte und Drehmomente wirken stets so, dass die Gesamtenergie des Feldes abnimmt, wenn die Körper ihnen folgen, wobei die Bindungsenergie als mechanische Arbeit frei wird. Umgekehrt wird an den Körpern Arbeit verrichtet, wenn sie gegen die Kräfte bewegt werden. Die Arbeit senkt bzw. erhöht die Energie des Feldes. Sind Spulen beteiligt, so kann auch Elektroenergie zu- oder abgeführt werden.
Die Stärke eines Magnetfeldes kann durch zwei verschiedene physikalische Größen ausgedrückt werden, die magnetische Feldstärke  (Einheit: A/m, also Ampere pro Meter; im CGS-Einheitensystem gibt es den Namen Oersted für die entsprechende Einheit) und die magnetische Flussdichte (die sog. „magnetische Induktion“)  (Einheit Tesla). Diese unterscheiden sich im Vakuum nur durch einen konstanten Faktor, die magnetische Feldkonstante :
In Materie, z. B. in Permanentmagneten, ist der Zusammenhang komplizierter: In diesem Fall ist  über einen Querspalt hinweg stetig,  über einen Längsspalt. Messungen mit einer Magnetfeld-Sonde in Quer- und Längsspalt können wesentlich verschieden ausfallen. Die Größe  ist immer quellenfrei, während das Gleiche für  nicht gilt (s. u.). Während die magnetische Feldstärke  bei Berechnungen mit elektrischen Strömen oder bei ferromagnetischem oder ferrimagnetischem Material von Vorteil ist, verwendet man die magnetische Flussdichte zum Berechnen von induzierten Spannungen oder der Lorentzkraft. Die beiden Feldgrößen sind über die Materialgleichungen der Elektrodynamik miteinander verknüpft, welche sich im einfachsten Fall über einen Faktor, die magnetische Permeabilität, ausdrücken lässt; im allgemeinen Fall gilt stattdessen  wobei der Vektor  als die Magnetisierung des Materials bezeichnet wird. Quellenfreiheit von  und Wirbelfreiheit von  – letzteres nur im Falle der Abwesenheit elektrischer Ströme,  – drücken sich mathematisch durch die Gleichungen   bzw.  aus. Dabei sind   und  die Differentialoperatoren für die Divergenz bzw. die Rotation, also für die Quellen- bzw. die Wirbeldichte eines Feldes.
Das intergalaktische Magnetfeld, ausgedrückt als magnetische Flussdichte in der Einheit Tesla (T), schätzt man auf weniger als 0,1 nT (10−10 T), das der Milchstraße auf 30 nT. Das Magnetfeld der Erde hat an der Oberfläche eine Stärke um 40 µT, dies entspricht im Gaußschen Einheitensystem 0,4 Gauss. Die magnetische Flussdichte der Sonnenflecken liegt unter 1 mT. Die Sättigungsmagnetisierung von Eisen beträgt ca. 2 Tesla.
Auf der Oberfläche von Neutronensternen, wie z. B. Pulsaren, herrschen dagegen typischerweise Flussdichten von 108 Tesla, bei Magnetaren, einer speziellen Sorte von Neutronensternen, sogar 1011 Tesla.
Das mit 1 nT derzeit (2009) schwächste Magnetfeld auf der Erde findet man in einem speziell abgeschirmten kubischen Gebäude der Physikalisch-Technischen Bundesanstalt in Berlin.[4] Zweck des Kubus ist die Messung der schwachen Hirnströme und der Herzsignale von Menschen.
Am National High Magnetic Field Laboratory in Tallahassee (Florida) wird das mit 45 T derzeit stärkste zeitlich konstante Magnetfeld auf der Erde erzeugt. Noch höhere Magnetfelder sind mit Elektromagneten in kurzen Pulsen erreichbar. Den Weltrekord für zerstörungsfreie Magnetbauweisen hält derzeit (2012) das National High Magnetic Field Laboratory in Los Alamos, USA mit 100,75 T.[5]
Mittels intensiver Laserstrahlung lassen sich Flussdichten von bis zu 34 Kilotesla erzeugen – allerdings nur während etwa 10 ps.
Hohe Magnetfelder von beispielsweise 2800 T[6] lassen sich mit Stromimpulsen erzeugen, wenn in Kauf genommen wird, dass die Spule dabei zerstört wird (bzw. sich selbst zerstört). Eine zusätzliche Steigerung der Flussdichte kann bei gleichzeitiger Komprimierung der Spule bzw. des Feldes mittels Sprengladungen erreicht werden; siehe auch Flusskompressionsgenerator.
Jedes Magnetfeld enthält Energie. Die Energiedichte  an einem beliebigen Punkt eines Magnetfelds im Vakuum ist gegeben durch
Dabei ist  der Betrag der magnetischen Feldstärke,  der Betrag der magnetischen Flussdichte am gegebenen Punkt und  die magnetische Feldkonstante oder Permeabilität des Vakuums.
Die Gesamtenergie des Magnetfelds einer stromdurchflossenen Spule beträgt
Hier steht  für die Induktivität der Spule und  für die Stromstärke.
Unter Elektromagnetismus versteht man die vielfältigen Beziehungen zwischen Magnetismus und rein elektrischen Phänomenen. Magnetfelder, die durch elektrische Ströme entstehen, können durch die Spezielle Relativitätstheorie als Folge der elektrostatischen Kräfte zwischen den Ladungen gedeutet werden. Die Erklärung beruht darauf, dass ein elektrischer Strom eine Relativbewegung entgegengesetzt geladener Teilchen darstellt, deren Ladungsdichten durch die Lorentzkontraktion verschieden beeinflusst werden. Geladene Elementarteilchen, die einen Eigendrehimpuls (Spin) haben, besitzen auch ein magnetisches Moment und sind damit u. a. verantwortlich für den Ferromagnetismus. Dies wird durch die relativistische Quantenmechanik gedeutet.
Auch bei der Wirkung eines Magnetfelds auf ein bewegtes geladenes Teilchen gibt es einen nur durch die Quantenmechanik erklärbaren Effekt (siehe Aharonov-Bohm-Effekt). Hierbei beeinflusst ein räumlich begrenztes magnetisches Feld die Dynamik eines geladenen Teilchens, auch wenn dieses sich ausschließlich in einem Bereich mit verschwindendem Magnetfeld bewegt.
Bewegungen von Ladungsträgern bewirken Veränderungen im elektrostatischen und magnetischen Feld ihrer Umgebung. Da sich diese Veränderungen gegenseitig beeinflussen und im Raum ausbreiten, spricht man von elektromagnetischen Wellen. Licht (ob sichtbar oder unsichtbar) und Rundfunk sind die bekanntesten Formen dieses Phänomens, aber auch in der Metallverarbeitung (Induktionsöfen) und zum Erhitzen sogar nichtleitender Substanzen kommt diese Form des Elektromagnetismus zur Anwendung (Mikrowellenherd).
Für eine vertiefte Darstellung und Einordnung des Elektromagnetismus siehe den Artikel Elektromagnetische Wechselwirkung.
Betrag und Vorzeichen der bewegten Ladungen sowie Betrag und Richtung ihrer Geschwindigkeit bestimmen die Stärke und Richtung der magnetischen Kräfte sowie der ihnen zugrundeliegenden magnetischen Felder, deren exakte Messung heute u. a. mit Hall-Sonden möglich ist.
Für den Zusammenhang zwischen Stromrichtung und Richtung der magnetischen Kräfte bzw. der ihnen zugrundeliegenden magnetischen Felder ist dabei eine Reihe unterschiedlich bezeichneter Regeln und Merkhilfen im Umlauf, die sich zunächst einmal danach unterscheiden, ob bei ihnen von der „konventionellen“ bzw. „technischen“ Stromrichtung (entgegen dem Elektronenfluss) oder aber der Richtung des Elektronenflusses (umgangssprachlich auch „physikalische“ Stromrichtung genannt) ausgegangen wird. Ist ersteres der Fall, spricht man von Rechte-Hand- oder Rechte-Faust-Regeln, ansonsten von Linke-Hand- oder Linke-Faust-Regeln, wobei die zuerst genannten traditionell vorherrschen.
Die nächste Unterscheidung ist die danach, ob man sich bei der betreffenden Regel außer dem Daumen auch des jeweils im rechten Winkel zum Vorgänger abgespreizten Zeige- und Mittelfingers bedient oder aber sich alle Finger außer dem Daumen zu einer Faust geschlossen vorstellt.
Während die zuerst genannten Regeln damit als eigentliche Linke- bzw. Rechte-Hand-Regeln – auch Drei-Finger-Regel, UVW-Regel oder IBF- bzw. FBI-Regel genannt – die Richtung der Lorentzkraft auf einen bewegten Ladungsträger in einem (vorgegebenen) äußeren Magnetfeld anzeigen, dienen die an zweiter Stelle genannten Regeln – Linke- bzw. Rechte-Faust-Regel, Umfassungsregel, populär auch Schraubenregel bzw. Korkenzieherregel genannt – in erster Linie dazu, die Richtung des Magnetfeldes anzuzeigen, das der bewegte Ladungsträger durch seine Bewegung selbst erzeugt, sei es frei fliegend oder aber in einem geradlinigen bzw. ringförmigen elektrischen Leiter, z. B. einer Spule.
Beispiele: Rechte-Hand-Regel bzw. (rechtshändige) UVW-Regel
Beispiele: Rechte-Faust-Regel bzw. Korkenzieherregel
Beim Magnetismus handelt es sich (ähnlich wie bei der Supraleitung) um spezifisch-quantenmechanische Effekte, die nicht einfach darzustellen sind.
Ein erfolgreiches Modell wurde schon 1927 mit der Heitler-London-Theorie der Bildung von Wasserstoff-Molekülen entwickelt, obwohl diese Theorie zunächst nichts mit „Magnetismus“ zu tun zu haben schien. Nach dieser Theorie entstehen σ-Molekülorbitale, das heißt, aus den zwei atomaren Wasserstoff-Funktionen ui(…) bildet sich ein orbitaler σ-Molekülzustand:
Das letzte Produkt ergibt sich aus dem ersten wegen des quantenmechanischen Prinzips der Ununterscheidbarkeit identischer Teilchen. Es bedeutet: Das erste Elektron r1 kann sich nicht nur beim ersten Atomkern befinden, sondern ebenso gut in einem atomaren Wasserstoff-Orbital beim zweiten Atomkern, während sich das zweite Elektron beim ersten Atomkern befindet. Dies ergibt die „Austauschwechselwirkung“, die für das Zustandekommen des Magnetismus eine fundamentale Rolle spielt und um Faktoren von 100 bis 1000 stärker ist als die durch die Elektrodynamik beschriebenen phänomenologischen Terme.
Bei der Spinfunktion χ(s1, s2), welche für den Magnetismus verantwortlich ist, gilt dann wegen des Pauli-Prinzips das komplementäre Verhalten[7]
d. h., es müssen nicht nur die ui ersetzt werden durch α und β (Ersteres bedeutet spin up, Letzteres spin down), sondern auch + durch − sowie z. B. r1 durch die beiden diskreten Werte von s1, nämlich durch ±½. Und zwar gilt:
Mit dem Minuszeichen in (1b) ergibt sich also eine Singulett-Spinfunktion. Das besagt: Die Spins sind antiparallel; beim Festkörper bedeutet das Antiferromagnetismus und bei zweiatomigen Molekülen Diamagnetismus.
Die Tendenz zur Molekülbindung, entsprechend der oben angegebenen Ortsfunktion, ergibt also wegen des Pauli-Prinzips automatisch die schon erwähnte Singulettsymmetrie des Spinzustandes; wogegen die Coulomb-Abstoßung der beiden Elektronen zu einer Singulett-Ortsfunktion und komplementär dazu zu einer Triplett-Spinfunktion führen würde, d. h., „die Spins würden jetzt parallel stehen“.
Der letztgenannte Effekt überwiegt bei Eisen, Kobalt und Nickel; diese Metalle sind ferromagnetisch. Bei den zweiatomigen Molekülen überwiegt er auch beim Sauerstoff, das im Gegensatz zu den anderen zweiatomigen Molekülen nicht diamagnetisch, sondern paramagnetisch ist. Der zuerst genannte Effekt überwiegt dagegen bei den anderen Metallen wie Natrium, Kalium, Magnesium oder Strontium, die nichtmagnetisch sind, oder bei Mangan, das antiferromagnetisch ist.
Aus dem Heitler-London-Modell entstand durch Verallgemeinerung das grundlegende Heisenberg-Modell des Magnetismus[8] (Heisenberg 1928).
Die Erklärung des Phänomens beruht also letztlich auf allen Subtilitäten der Quantenmechanik, einschließlich ihrer mathematischen Struktur, insbesondere auf dem dort beschriebenen Spin und dem Pauli-Prinzip, während die Elektrodynamik eher die Phänomenologie beschreibt.
Alle fundamentalen geladenen Elementarteilchen besitzen ein charakteristisches magnetisches Moment . Es ist über das gyromagnetische Verhältnis mit ihrem Spin verknüpft.
Das magnetische Moment eines Atoms setzt sich zusammen aus dem Beitrag der Elektronenhülle (Hüllenmoment) und dem im Allgemeinen viel schwächeren Kernbeitrag (Kernmoment).
Zum Hüllenmoment tragen das Bahnmoment, das mit dem Bahndrehimpuls der Elektronen verknüpft ist, und das durch den Elektronenspin bestimmte Spinmoment bei. Die Summe der magnetischen Momente der Elektronen eines doppelt besetzten Atomorbitals ergibt jeweils null, sodass Atome, die keine halbbesetzten Orbitale besitzen, kein permanentes Hüllenmoment aufweisen.
Das Kernmoment ist zwar sehr klein, es lässt sich aber dennoch nicht nur nachweisen (Zeeman-Effekt, Stern-Gerlach-Versuch), sondern auch praktisch anwenden [z. B. NMR-Spektroskopie (Nuclear Magnetic Resonance, kernmagnetische Resonanz], MR-Tomographie).
Magnetismus von Festkörpern ist ein kooperatives Phänomen.
Die makroskopische Magnetisierung setzt sich additiv zusammen aus den Beiträgen der einzelnen Bausteine (Atome, Ionen, quasifreie Elektronen), aus denen der Festkörper aufgebaut ist. Bei vielen Materialien haben bereits die einzelnen Bausteine ein magnetisches Moment. Allerdings weisen selbst von den Materialien, deren Bausteine solche magnetische Momente tragen, nur wenige eine makroskopische Magnetisierung auf. In der Regel addieren sich nämlich die verschiedenen Momente zum Gesamtmoment Null. Nur wenn das nicht geschieht, wenn sich also ihre Beiträge nicht in der Summe aufheben, ist eine makroskopische Magnetisierung das Ergebnis.
In Festkörpern können fünf Typen von Magnetismus auftreten.[13] Ihre Namensgebung erfolgt beim magnetischen wie auch beim elektrischen Feld durch die Verwendung der entsprechenden Vorsilbe ganz analog:
Darüber hinaus gibt es noch Formen des Magnetismus, die durch nicht magnetisches oder nichtlineares Verhalten der fünf Magnetismustypen geprägt sind:
Mit Amagnetismus, amagnetisch, nichtmagnetisch oder unmagnetisch ist meist „nicht ferromagnetisch“ gemeint, beispielsweise als Eigenschaft von austenitischem Stahl im Gegensatz zu gewöhnlichem Baustahl. Stoffe, auf die ein Magnetfeld überhaupt keine Wirkung hat, gibt es nicht. Bei sehr hohen Magnetfeldstärken kann es auch bei „amagnetischen“ Materialien zu Anziehungs- oder im noch geringeren Maße zu Abstoßungseffekten kommen, wenn auch wesentlich schwächer als bei ferromagnetischen Stoffen. Die Bezeichnung amagnetisch wird nicht einheitlich gebraucht.
Um die verschiedenen Arten des Magnetismus und deren Temperaturabhängigkeit zu untersuchen, werden sowohl verschiedene makroskopische als auch atomar-mikroskopische Methoden verwendet. Eine der empfindlichsten makroskopischen Methoden beruht auf dem Josephson-Effekt und wird im SQUID verwendet, der in der Materialforschung meist mit einem geregelten Kryostaten kombiniert wird. Der Hall-Effekt ist ebenfalls eine makroskopische Methode und findet auch Anwendung in vielen einfachen technischen Anwendungen, z. B. im Automotor.
Auf atomarer Skala werden Atomkerne unter Verwendung der Hyperfeinwechselwirkung genutzt, um mit Atomkernen im Kristallgitter am Ort des jeweiligen Kerns die Größe des Magnetfeldes zu messen. Bekannte Methoden sind Mößbauer-Spektroskopie, Gestörte Gamma-Gamma-Winkelkorrelation und NMR.
Weil jede Nervenaktivität auch aus elektrischen Strömen besteht, produziert unser Nervengewebe und insbesondere unser Gehirn ständig Magnetfelder, die mit empfindlichen Detektoren empfangen werden können.
Magnetische Wechselfelder können über Induktion elektrische Ströme im Gewebe auslösen und können so einen (schwachen) Einfluss auf das Nervensystem haben. So kann der motorische Cortex derartig mit Hilfe der Transkraniellen Magnetstimulation (TMS) stimuliert werden, dass es zu unwillkürlichen Muskelkontraktionen kommt. Auch die Nerven in den Muskeln selbst können auf diese Weise stimuliert werden.
In entsprechend starken Feldern (zum Beispiel in einem Kernspintomografen) treten sogenannte Magnetophosphene (optische Sinneswahrnehmungen) auf. Des Weiteren ist seit langem bekannt, dass magnetische Wechselfelder die Sekretion von Hormonen (Beispiel Melatonin) beeinflussen können.[15][16] Langzeitfolgen für den Menschen konnten dabei jedoch nicht beobachtet werden.[17][18]
Viele Vögel, Meeresschildkröten und weit ziehende Fische verfügen über einen Magnetsinn und können sich mittels des Erdmagnetfelds orientieren.

Der Arzt Franz Anton Mesmer entwickelte eine Theorie, die 1784 von der französischen Akademie der Wissenschaften geprüft und verworfen wurde, nach der ein Fluid, das Mesmer als Magnetismus animalis bezeichnete, von Mensch zu Mensch übertragbar sei und bei der Hypnose und bestimmten Heilverfahren (Mesmersche Streichungen) eine Rolle spielen sollte. Über den seinerzeit populären animalischen Magnetismus hinaus befasste sich in Orléans der Arzt und Magnetiseur Louis Joseph Jules Charpignon um 1845[19] auch mit Magnetismus in einer allgemeineren Bedeutung.[20]Wirkungen oder Gefahren magnetischer Gleichfelder auf den Menschen sind nicht bekannt. Auch die gepulsten Felder bei der Kernspintomografie sind im Allgemeinen ungefährlich. Dagegen kommt es bei starken Feldern in folgenden Fällen zu Gefahren:
Daher gelten in Magnetfeldlaboren und an Kernspintomografen Sicherheitsregeln, die gewährleisten, dass keinerlei ferromagnetische Teile in die Nähe geraten.
Folgende Schäden sind weiterhin relevant:
Siehe auch: Magnetotaxis, Magnetospirillum gryphiswaldense, Magnetospirillum magnetotacticum, Magnetosom
Gepulste Felder können durch elektromagnetische Induktion sämtliche elektronischen und elektrischen Einrichtungen beeinflussen oder zerstören, vgl. auch Elektromagnetischer Puls.
Unregelmäßigkeiten des Teilchenstroms von der Sonne (Sonnenwind) führen auf der Erde zu sogenannten magnetischen Stürmen, die durch Induktion Telefon- und Überlandleitungen, Kabelsysteme und auch metallene Rohrleitungen gefährden können.
Magnetische Felder können Aufzeichnungen auf magnetischen Datenträgern wie etwa Tonband, Videoband oder Festplatte löschen.
Wenn ein Magnetfeld als Folge eines Zwischenfalls – Leitungsunterbrechung beim konventionellen Elektromagneten oder Quenchen beim Supraleitungsmagneten – schlagartig zusammenbricht, können durch Induktion sehr hohe elektrische Spannungsimpulse entstehen. Führen diese zu Stromfluss, können die dadurch wiederum erzeugten Magnetfelder z. B. Gegenstände gewaltsam in den Magneten hineinziehen. Daher dürfen Experimentieraufbauten in direkter Nähe des Magneten keine geschlossenen Leiterschleifen – beispielsweise in irgendwelchen Gestellen – enthalten; dies wird durch Einfügen isolierender Zwischenstücke erreicht.
Die zwei ringförmigen Permanentmagnete aus dem Magnetron eines Mikrowellenherdes ziehen sich so stark an, dass man sich eine feine Fingerhautfalte dazwischen schmerzhaft einklemmen und verletzen kann.
Zu Missverständnissen kommt es öfter durch die Verwechslung der Begriffe „magnetisch“ (im Sinne von ferromagnetisch), „magnetisiert“ und „magnetisierbar“.
In der Umgangssprache wird unter Magnetismus praktisch ausschließlich der Ferromagnetismus verstanden, denn dieser ist im Alltag häufig und vertraut: Haftmagnete an einer Blechtafel, die Wirkungsweise eines Kompasses usw. Die anderen Arten des Magnetismus (Diamagnetismus, Paramagnetismus usw.) sind dagegen in der alltäglichen Umwelt unauffällig. Mit „magnetisch“ ist also meist „ferromagnetisch“ gemeint.
Die meisten Menschen verbinden den Begriff Magnetismus richtigerweise sehr stark mit den Werkstoffen Eisen und Stahl. Weniger bekannt ist, dass auch Nickel und Cobalt ferromagnetisch sind.
Falsche Vorstellungen über die Magnetisierbarkeit sind verbreitet und finden sich auch in einigen Büchern und sonstigen Quellen. Beispielsweise ist ein Gegenstand aus einfachem Stahl ferromagnetisch und somit magnetisierbar, aber nur magnetisch „weich“, das heißt, er verliert seine Magnetisierung sehr schnell wieder. Nicht aus jedem beliebigen Stahldraht lässt sich durch Überstreichen mit einem Dauermagneten eine provisorische Kompassnadel herstellen, ein magnetisch weicher Stahldraht ist nicht dazu geeignet. Wenn man einen magnetisch weichen Stahldraht mit einem Dauermagneten berührt, dann wird er zwar angezogen, aber nicht dauerhaft magnetisiert. Eine magnetisch „harte“ Stahlnadel lässt sich dagegen dauerhaft magnetisieren, das heißt, sie verliert ihre Magnetisierung erst über einen langen Zeitraum und könnte damit als Behelfskompass funktionieren.
Ob ein Gegenstand „magnetisch“ (im Sinne von ferromagnetisch) ist, kann man leicht prüfen, indem man ihn mit einem Dauermagneten berührt. Spürt man dabei eine Kraft, dann ist der Gegenstand ferromagnetisch. Ob ein Gegenstand „magnetisiert“ ist – das heißt, selbst ein Dauermagnet ist – kann man entsprechend an einem sehr leichten Teil aus unmagnetisiertem Stahl (z. B. einer Heft- oder Büroklammer) prüfen: Bleibt die Heftklammer an dem Gegenstand hängen, dann ist er magnetisiert.
Eine Magnetisierung beispielsweise bei Werkzeugen kann in der Praxis erwünscht sein (z. B. sind manche Schraubendreher absichtlich magnetisiert, damit die Handhabung kleiner Eisenschrauben vereinfacht wird). Die Magnetisierung kann aber auch unerwünscht sein, weil dadurch ständig kleine Eisenfeilspäne o. Ä. am Gerät haften.
Notebooks von Apple weisen mitunter eine magnetisch koppelnde Stromversorgungsbuchse auf. Diese MagSafe-Verbindung löst sich in den meisten Fällen, wenn versehentlich am Kabel gezogen wird, und kann so Stürze des Geräts vom Tisch auf den Boden vermeiden. Der flache Stecker hat einen ferromagnetischen Rahmen, der von dem Magneten in der etwas vertieft sitzenden Fläche der Buchse angezogen wird. Wird das Gerät ohne Hülle in einen Rucksack gesteckt, mit dem gelegentlich auch Handwerkszeug transportiert wird, kann die Buchse Eisen- und Rostpartikel anziehen und sich damit verstopfen. Diese Partikel können mithilfe starken Klebebands herausgezogen werden.
Die Abschirmung elektrotechnischer Geräte, Einrichtungen und Räume dient dazu, elektrische und/oder magnetische Felder von diesen fernzuhalten oder umgekehrt die Umgebung vor den von der Einrichtung ausgehenden Feldern zu schützen. Magnetische Abschirmungen werden z. B. in Röhrenmonitoren und Oszilloskopen mit Kathodenstrahlröhre eingesetzt, da es aufgrund magnetischer Störquellen zu Bildstörungen kommen kann. Dauermagnete von Lautsprechern in Fernsehgeräten mit Bildröhre werden oft magnetisch abgeschirmt.
Zur Abschirmung von statischen Magnetfeldern und von Magnetfeldern geringer Frequenz dienen weichmagnetische Werkstoffe, d. h. ferromagnetische Materialien hoher Permeabilität und geringer Remanenz. Eine magnetische Abschirmung wirkt zugleich elektrisch abschirmend, wenn sie hinreichend leitfähig ist. Hochfrequente, elektromagnetische Wechselfelder (elektromagnetische Wellen) können nur mit elektrisch leitfähigen, allseitig geschlossenen Hüllen ausreichender Dicke vollständig abgeschirmt werden. Spalte oder Öffnungen verringern die Schirmdämpfung und machen diese unmöglich, wenn ihre größte Abmessung die Größenordnung der abzuschirmenden Wellenlänge erreicht oder überschreitet.

Die Lichtgeschwindigkeit  ( nach lat. celeritas: Schnelligkeit) ist eine fundamentale Naturkonstante. Sie ist eine der Konstanten, über die seit der SI-Reform von 2019 der Meter und andere Maßeinheiten definiert sind.[1] Die Lichtgeschwindigkeit spielt eine zentrale Rolle in der speziellen und allgemeinen Relativitätstheorie und stellt einen Zusammenhang zwischen Raum und Zeit her. Sie ist die absolute Grenzgeschwindigkeit im Universum; kausale Zusammenhänge (Ursache-Wirkung-Beziehungen) können sich nicht schneller ausbreiten. Licht und andere elektromagnetischen Wellen breiten sich im Vakuum mit dieser Geschwindigkeit aus, ebenso Gravitationswellen. Die Geschwindigkeit materieller Körper (z. B. Elementarteilchen mit Masse) kann sich bei hoher Energiezufuhr der Lichtgeschwindigkeit nähern, sie aber nicht erreichen.
In einem materiellen Medium wie Luft oder Glas ist die Ausbreitungsgeschwindigkeit des Lichts kleiner. Wenn es sich nicht aus dem Zusammenhang ergibt, wird durch Wortzusätze deutlich gemacht, ob die Lichtgeschwindigkeit im Vakuum oder im Material gemeint ist. In beiden Fällen verwendet man das Formelzeichen ; zur Unterscheidung wird für die Lichtgeschwindigkeit im Vakuum auch  geschrieben.
Der Wert der Lichtgeschwindigkeit beträgt
Der Wert von 299792458 m/s gilt exakt, weil die Maßeinheit „Meter“ seit 1983 implizit dadurch definiert ist, dass der Lichtgeschwindigkeit dieser Wert zugewiesen wurde.[2][1]
Zuvor war der Meter als Vielfaches der Wellenlänge eines bestimmten atomaren Übergangs definiert gewesen, und die Lichtgeschwindigkeit war eine experimentell zu bestimmende Größe. Mit dem messtechnischen Fortschritt konnte aber die Lichtgeschwindigkeit präziser bestimmt werden als diese Wellenlänge und damit der Meter selbst. Deshalb beschloss man 1983 diese neue Definition des Meters.
Dass es eine universelle Grenzgeschwindigkeit geben muss, ergibt sich aus einem fundamentalen Grundprinzip der Physik, dem Relativitätsprinzip: Die physikalischen Gesetze sind unabhängig vom Bewegungszustand bei gleichförmiger Bewegung. Die ursprüngliche mathematische Beschreibung dieses Prinzips (Galilei-Transformation: Addition von Relativgeschwindigkeiten) führt in der Elektrodynamik zu unauflöslichen Widersprüchen und muss durch die Lorentz-Transformation ersetzt werden. Diese weicht bei hohen Geschwindigkeiten von der einfacheren Galilei-Transformation ab und erfordert, dass es eine Grenzgeschwindigkeit c gibt, die niemals überschritten werden kann. Albert Einstein erkannte, dass durch diese Grenzgeschwindigkeit Raum und Zeit untrennbar zur Raumzeit verknüpft sind und dass dadurch c die maximale Geschwindigkeit für kausale Zusammenhänge (Ursache-Wirkung-Beziehungen) ist. Kein Signal, keine Information kann schneller übertragen werden. Dies ist die Grundlage seiner speziellen Relativitätstheorie.
Das Relativitätsprinzip erzwingt, dass die maxwellschen Gleichungen der Elektrodynamik genau dieses c als Parameter enthalten, und als Konsequenz pflanzen sich elektromagnetische Wellen mit genau dieser Geschwindigkeit fort. Ihre Geschwindigkeit hängt dabei nicht von der Geschwindigkeit der Lichtquelle ab, und unabhängig vom Bewegungszustand des zu ihrer Messung verwendeten Empfängers wird stets derselbe Wert der Lichtgeschwindigkeit gemessen. Massebehaftete Teilchen können sich nur mit geringerer Geschwindigkeit v < c bewegen; für masselose Teilchen hingegen ist c die einzig mögliche Geschwindigkeit.
Aus der Relativitätstheorie ergibt sich weiterhin, dass Energie und Masse über die Beziehung E0 = mc2 verknüpft sind. Wenn es keine Grenzgeschwindigkeit gäbe ("c = ∞"), könnte es demnach keine Masse geben, weil hierfür unendlich viel Energie vonnöten wäre.
In Einsteins Allgemeiner Relativitätstheorie spielt c ebenfalls eine zentrale Rolle. Die Wirkung der Gravitation breitet sich mit dieser Geschwindigkeit aus.
Die Bezeichnung „Lichtgeschwindigkeit“ ist insofern unglücklich gewählt, als sie von der fundamentalen Bedeutung dieser Naturkonstante für Raum, Zeit und Kausalität ablenkt. Die Ausbreitungsgeschwindigkeit des Lichts ist letztlich nur eine der Konsequenzen daraus. Überdies wird als „Lichtgeschwindigkeit“ auch die Geschwindigkeit des Lichts in materiellen Medien bezeichnet, die geringer ist. Zur Vermeidung von Missverständnissen bezeichnet man die Naturkonstante daher auch als „Vakuumlichtgeschwindigkeit“, mit dem Symbol c0.
Informationen in Telekommunikationsanlagen breiten sich mit 70 Prozent (Glasfasern) bis 100 Prozent (Vakuum, Weltraum, praktisch auch Luft) der Lichtgeschwindigkeit aus. Dadurch entstehen Verzögerungszeiten, die sich nicht vermeiden lassen. Entlang der Erdoberfläche beträgt der maximale Abstand zweier Orte etwa 20.000 km. Dies entspräche bei Vakuum-Lichtgeschwindigkeit 67 ms Laufzeit. Die tatsächliche Übertragungszeit ist stets länger. Bei atmosphärischer Übertragung wird die Welle in den verschiedenen Schichten der Atmosphäre und am Erdboden reflektiert und hat so einen längeren Weg zurückzulegen.
Mikroprozessoren arbeiten heute mit Taktfrequenzen in der Größenordnung von 1 bis 5 GHz. Während eines Taktes legen elektrische Signale in Schaltkreisen mit Low-k-Dielektrikum zwischen 5 und 20 cm zurück. Beim Entwerfen von Schaltkreisen sind diese Laufzeiten nicht vernachlässigbar.
Geostationäre Satelliten befinden sich 35.786 km über dem Äquator. Um auf Telefon- oder Fernsehsignale auf diesem Weg eine Antwort zu erhalten, muss das Signal mindestens 144.000 km zurückgelegt haben: vom Sender zum Satelliten, dann zum Empfänger, anschließend den gleichen Weg zurück. Diese Laufzeit beträgt etwa 480 ms und bedeutet damit eine spürbare Verzögerung.
Raumsonden befinden sich an ihren Zielorten oft viele Millionen oder sogar Milliarden Kilometer von der Erde entfernt. Selbst mit Lichtgeschwindigkeit sind die Funksignale mehrere Minuten bis Stunden zu ihnen unterwegs. Die Antwort zurück zur Erde braucht noch einmal die gleiche Zeit. Extraterrestrische Fahrzeuge wie zum Beispiel der Mars-Rover Opportunity müssen daher selbsttätig steuern und Gefahren erkennen können, denn die Bodenstation kann erst Minuten später auf Zwischenfälle reagieren.
Aus den Maxwell-Gleichungen folgt, dass elektrische und magnetische Felder schwingen können und dabei Energie durch den leeren Raum transportieren. Dabei gehorchen die Felder einer Wellengleichung, ähnlich der für mechanische Wellen und für Wasserwellen. Die elektromagnetischen Wellen übertragen Energie und Information, was in technischen Anwendungen beispielsweise für Beleuchtung, Lichtsignale, Radio, Radar oder Laser genutzt wird.
Die Geschwindigkeit von ebenen oder kugelförmigen elektromagnetischen Wellen im Vakuum ist den Maxwell-Gleichungen zufolge der Kehrwert der Wurzel des Produkts der elektrischen Feldkonstanten  und der magnetischen Feldkonstanten 

Aus dieser Formel berechnete Maxwell 1865 mit den damals bekannten Werten für  und  den Wert von  und folgerte:[3] „Diese Geschwindigkeit ist so nahe an der Lichtgeschwindigkeit, sodass wir einen starken Grund zu der Annahme haben, dass das Licht selbst (einschließlich Wärmestrahlung und anderer Strahlung, falls es sie gibt), eine elektromagnetische Welle ist.“
 Maxwells Annahme ist in allen Beobachtungen an elektromagnetischer Strahlung bestätigt worden.
In einem Medium werden die beiden Feldkonstanten durch das Material geändert, was durch die Faktoren relative Permittivität  und relative Permeabilität  berücksichtigt wird. Beide hängen von der Frequenz  ab. Die Lichtgeschwindigkeit im Medium ist dementsprechend
Das Verhältnis der Lichtgeschwindigkeit in Vakuum zu der in einem Medium ist der (frequenzabhängige) Brechungsindex  des Mediums. Der Zusammenhang des Brechungsindex mit der relativen Permittivität und der relativen Permeabilität heißt auch maxwellsche Relation:
Wegen der im Allgemeinen gegebenen Abhängigkeit von  und  von der Frequenz der Welle ist zu beachten, dass  die Phasengeschwindigkeit im Medium bezeichnet, mit der Punkte gleicher Phase (z. B. Minima oder Maxima) einer ebenen Welle mit konstanter Amplitude fortschreiten. Die Hüllkurve eines räumlich begrenzten Wellenpakets pflanzt sich hingegen mit der Gruppengeschwindigkeit fort. In Medien weichen diese beiden Geschwindigkeiten mehr oder weniger voneinander ab. Insbesondere bedeutet ein Brechungsindex  lediglich, dass sich die Wellenberge schneller als  ausbreiten. Wellenpakete, mit denen Information und Energie transportiert werden, sind weiterhin langsamer als .[4]
Nach den Maxwell-Gleichungen ergibt sich die von der Wellenlänge unabhängige Lichtgeschwindigkeit  u. a. für den Fall einer im Vakuum unendlich ausgedehnten ebenen Welle mit einer wohldefinierten Fortpflanzungsrichtung. Demgegenüber hat jede praktisch realisierbare Lichtwelle immer ein gewisses Strahlprofil. Wird dies als Überlagerung von ebenen Wellen mit leicht veränderten Fortpflanzungsrichtungen dargestellt, haben die einzelnen ebenen Wellen zwar alle die Vakuumlichtgeschwindigkeit , jedoch gilt dies nicht notwendig für die durch die Überlagerung entstehende Welle. Es resultiert eine leicht verlangsamte Welle. Das konnte an speziell geformten Bessel-Strahlen von Mikrowellen und sichtbarem Licht auch nachgewiesen werden, sogar für die Geschwindigkeit einzelner Photonen.[5][6] Bei allen praktisch realisierbaren Lichtwellen, auch bei scharf gebündelten Laserstrahlen, ist dieser Effekt aber vernachlässigbar klein.
In Materie ist Licht langsamer als im Vakuum, und zwar gilt dort, wie oben hergeleitet wurde,  mit einem Brechungsindex , der größer als 1 ist.[7]
In bodennaher Luft ist die Lichtgeschwindigkeit etwa 0,28 ‰ geringer als im Vakuum (also ca. 299.710 km/s), in Wasser beträgt sie etwa 225.000 km/s (− 25 %) und in Gläsern mit hohem Brechungsindex bis hinab zu 160.000 km/s (− 47 %).
In manchen Medien wie Bose-Einstein-Kondensaten oder photonischen Kristallen herrscht für bestimmte Wellenlängen eine sehr große Dispersion. Licht breitet sich in ihnen deutlich verlangsamt aus.[8] So konnte die Forschungsgruppe der dänischen Physikerin Lene Hau im Jahr 1999 Licht auf eine Gruppengeschwindigkeit von ungefähr 17 m/s bringen.[9]
Grenzen zwei durchsichtige Medien aneinander, so bewirkt die unterschiedliche Lichtgeschwindigkeit in beiden Medien die Brechung des Lichts an der Grenzfläche. Da die Lichtgeschwindigkeit im Medium auch von der Wellenlänge des Lichtes abhängt, wird Licht unterschiedlicher Farbe unterschiedlich gebrochen, und weißes Licht spaltet sich in seine unterschiedlichen Farbanteile auf. Dieser Effekt lässt sich z. B. mit Hilfe eines Prismas direkt beobachten.
In einem Medium können Teilchen schneller sein als das Licht im gleichen Medium. Wenn sie elektrisch geladen sind, wie etwa Elektronen oder Protonen, tritt dabei der Tscherenkow-Effekt auf: Die Teilchen strahlen Licht ab, so wie ein überschallschnelles Flugzeug den Überschallknall hinter sich her schleppt. Dies ist beispielsweise in Schwimmbadreaktoren beobachtbar. In ihnen befindet sich Wasser zwischen den Brennelementen. Die Betastrahlung der Spaltprodukte besteht aus Elektronen, die schneller sind als die Lichtgeschwindigkeit im Wasser. Das von ihnen abgegebene Tscherenkow-Licht lässt das Wasser blau leuchten.
Der Tscherenkow-Effekt wird in Teilchendetektoren zum Nachweis schneller geladener Teilchen verwendet.
Teilchen ohne Masse bewegen sich immer und in jedem Inertialsystem mit Lichtgeschwindigkeit. Das bekannteste masselose Teilchen, das diese Eigenschaft zeigt, ist das Photon. Es vermittelt die elektromagnetische Wechselwirkung, die einen großen Teil der Physik des Alltags bestimmt. Weitere masselose Teilchen sind im Standardmodell der Teilchenphysik die Gluonen, die Vermittlerteilchen der starken Wechselwirkung. Teilchen mit einer von Null abweichenden Masse sind stets langsamer als das Licht. Wenn man sie beschleunigt, wächst ihre Energie  wegen der relativistischen Energie-Impuls-Beziehung gemäß
Dabei ist  die Geschwindigkeit des Teilchens in Bezug auf das Inertialsystem, das für die Beschreibung des Vorgangs gewählt wird. Je näher der Betrag der Teilchengeschwindigkeit  an der Lichtgeschwindigkeit  ist, desto mehr nähert sich der Quotient  dem Wert 1 an, und desto kleiner wird die Wurzel im Nenner. Je mehr sich die Teilchengeschwindigkeit der Lichtgeschwindigkeit nähert, desto größer wird die dafür benötigte Energie. Mit endlich hoher Energie kann man also ein Teilchen zwar beliebig nahe an die Lichtgeschwindigkeit beschleunigen, man kann diese jedoch nicht erreichen.
Der von der Relativitätstheorie vorhergesagte Zusammenhang von Energie und Geschwindigkeit wurde in verschiedenen Experimenten belegt.
Er hat u. a. Auswirkungen auf die Technik von Teilchenbeschleunigern. Die Umlaufzeit eines z. B. in einem Synchrotron kreisenden Pakets von Elektronen ändert sich bei weiterer Beschleunigung kaum noch; die Synchronisation der einzelnen beschleunigenden Wechselfelder kann daher konstant sein. Dagegen muss sie bei schwereren Teilchen, die mit geringerer Geschwindigkeit zugeführt werden, laufend der zunehmenden Geschwindigkeit angepasst werden.
In Physik und Astronomie treten in einigen Fällen Geschwindigkeiten > c auf. Dabei handelt es sich aber nicht um echte Überlichtgeschwindigkeit im Sinne von Informationsübertragung: 
Die Gleichungen der speziellen Relativitätstheorie könnten auch von Teilchen erfüllt werden, die sich stets schneller als mit Lichtgeschwindigkeit bewegen, so genannten Tachyonen. Diese sind aber rein hypothetisch; es gibt keine experimentellen Hinweise auf ihre Existenz.
Die Frage, ob das Licht sich unendlich schnell oder mit endlicher Geschwindigkeit ausbreitet, war bereits in der Philosophie der Antike von Interesse. Licht legt einen Kilometer in nur drei Mikrosekunden zurück. Mit den Beobachtungsmöglichkeiten der Antike ist somit unweigerlich ein Lichtstrahl in dem Moment seines Entstehens scheinbar gleichzeitig bereits an seinem Ziel.
Trotzdem glaubte bereits Empedokles (um 450 v. Chr.), Licht sei etwas, das sich in Bewegung befinde und daher Zeit brauche, um Entfernungen zurückzulegen. Aristoteles meinte dagegen, Licht komme von der bloßen Anwesenheit von Objekten her, sei aber nicht in Bewegung. Er führte an, dass die Geschwindigkeit andernfalls so enorm groß sein müsse, dass sie jenseits der menschlichen Vorstellungskraft liege. Aufgrund seines Ansehens und Einflusses fand diese Theorie allgemeine Akzeptanz.
Eine altertümliche Vorstellung vom Sehen ging davon aus, dass „Sehstrahlen“ vom Auge emittiert werden. Ein Objekt sollte demnach dann zu sehen sein, wenn diese Lichtstrahlen aus dem Auge darauf träfen. Aufbauend auf dieser Vorstellung, befürwortete auch Heron von Alexandria die aristotelische Theorie. Er führte an, dass die Lichtgeschwindigkeit unendlich groß sein müsse, da man selbst die weit entfernten Sterne sofort sehen kann, sobald man die Augen öffnet.
In der orientalischen Welt war dagegen auch die Idee einer endlichen Lichtgeschwindigkeit verbreitet. Insbesondere folgten die persischen Philosophen und Wissenschaftler Avicenna und Alhazen (beide um das Jahr 1000) dieser Idee, blieben damit aber gegenüber der Anhängerschaft der aristotelischen Theorie in der Minderheit.
Zu Beginn des 17. Jahrhunderts glaubte der Astronom Johannes Kepler, dass die Lichtgeschwindigkeit zumindest im Vakuum unendlich sei, da der leere Raum für Licht kein Hindernis darstelle. Hier scheint schon die Idee auf, dass die Geschwindigkeit eines Lichtstrahls vom durchquerten Medium abhängig sein könnte.
Francis Bacon argumentierte, dass das Licht nicht notwendigerweise unendlich schnell sein müsse, sondern vielleicht nur schneller als wahrnehmbar.
René Descartes ging von einer unendlich großen Lichtgeschwindigkeit aus. Sonne, Mond und Erde liegen während einer Sonnenfinsternis in einer Linie. Descartes argumentierte, dass diese Himmelskörper für einen Beobachter zu diesem Zeitpunkt scheinbar nicht in Reihe stünden, wenn die Lichtgeschwindigkeit endlich sei. Da ein solcher Effekt nie beobachtet wurde, sah er sich in seiner Annahme bestätigt. Descartes glaubte derart stark an eine unendlich große Lichtgeschwindigkeit, dass er überzeugt war, sein Weltbild würde zusammenbrechen, wenn sie endlich wäre.
Dem standen um das Jahr 1700 die Theorien von Isaac Newton und Christiaan Huygens mit endlicher Lichtgeschwindigkeit gegenüber. Newton sah Licht als einen Strom von Teilchen an, während Huygens Licht als eine Welle deutete. Beide konnten das Brechungsgesetz erklären, indem sie die Lichtgeschwindigkeit proportional (Newton) bzw. umgekehrt proportional (Huygens) zum Brechungsindex ansetzten. Newtons Vorstellung galt als widerlegt, seitdem im 19. Jahrhundert Interferenz und Beugung beobachtet und die Geschwindigkeit in Medien gemessen werden konnten.
Da es zu Huygens’ Zeit die erste Messung der Lichtgeschwindigkeit gab, die seiner Meinung nach viel zu hoch war, als dass Körper mit Masse diese erreichen könnten, schlug er mit dem Äther ein elastisches (weder sicht- noch messbares) Hintergrundmedium vor, das die Ausbreitung von Wellen gestatte, ähnlich dem Schall in der Luft.
Galileo Galilei versuchte um 1600 als Erster, die Geschwindigkeit des Lichts mit wissenschaftlichen Methoden zu messen, indem er sich und einen Gehilfen mit je einer Signallaterne auf zwei Hügel mit bekannter Entfernung postierte. Der Gehilfe sollte Galileis Signal unverzüglich zurückgeben. Mit einer vergleichbaren Methode hatte er bereits erfolgreich die Schallgeschwindigkeit bestimmt. Zu seinem Erstaunen verblieb nach Abzug der Reaktionszeit des Gehilfen keine wiederholbar messbare Zeit. Dies änderte sich auch nicht, als die Distanz bis auf maximal mögliche Sichtweite der Laternen erhöht wurde. Isaac Beeckman schlug 1629 eine abgewandelte Version des Versuchs vor, bei der das Licht von einem Spiegel reflektiert werden sollte. Descartes kritisierte solche Experimente als überflüssig, da bereits exaktere Beobachtungen mit Hilfe von Sonnenfinsternissen durchgeführt worden seien und ein negatives Ergebnis geliefert hätten. Dennoch wiederholte die Accademia del Cimento das Experiment. Dabei standen die Lampen etwa eine Meile voneinander entfernt. Wieder konnte keine Verzögerung beobachtet werden. Das schien Descartes’ Annahme einer unendlich schnellen Ausbreitung des Lichts zu bestätigen. Galilei und Robert Hooke deuteten das Ergebnis dagegen so, dass die Lichtgeschwindigkeit so hoch sei, dass sie mit diesem Experiment nicht bestimmt werden konnte.
Der erste Nachweis, dass die Lichtgeschwindigkeit endlich ist, gelang dem dänischen Astronomen Ole Rømer im Jahr 1676. Er fand jahreszeitlich schwankende Laufzeiten für Taktsignale vom Jupiter (Eintritt des Jupitermonds Io in Jupiters Schatten), während diesseitig die Erdrotation als stabile Zeitreferenz diente. Er gab für den Erdbahndurchmesser eine Laufzeit des Lichtes von 22 min an. Der richtige Wert ist kürzer (16 min 38 s). Da Rømer den Durchmesser der Erdbahn nicht kannte, hat er für die Geschwindigkeit des Lichtes keinen Wert angegeben. Dies tat zwei Jahre später Christiaan Huygens. Er bezog die Laufzeitangabe von Rømer auf den von Cassini 1673 zufällig fast richtig angegebenen Durchmesser der Bahn der Erde um die Sonne (siehe Sonnenparallaxe für die schrittweise Verbesserung dieses Wertes) und kam auf eine Lichtgeschwindigkeit von 213.000 km/s.
James Bradley fand 1728 eine andere astronomische Methode, indem er die Schwankungen der Sternpositionen um einen Winkel von 20″ während des Umlaufs der Erde um die Sonne (Aberration) bestimmte. Seine Messungen waren der Versuch, die Parallaxe von Fixsternen zu beobachten, um damit deren Entfernungen zu bestimmen. Daraus berechnete Bradley, dass das Licht -mal schneller als die Erde bei ihrem Umlauf ist (Messfehler 2 %). Seine Messung (veröffentlicht im Jahr 1729) wurde damals als weiterer Beweis für eine endliche Lichtgeschwindigkeit und – gleichzeitig – für das kopernikanische Weltsystem angesehen. Für die Berechnung der Lichtgeschwindigkeit benötigte er jedoch ebenfalls den Erdbahnradius.
Die erste irdische Bestimmung der Lichtgeschwindigkeit gelang Armand Fizeau mit der Zahnradmethode. Er sandte 1849 Licht durch ein rotierendes Zahnrad auf einen mehrere Kilometer entfernten Spiegel, der es wieder zurück durch das Zahnrad reflektierte. Je nachdem, wie schnell sich das Zahnrad dreht, fällt das reflektierte Licht, das auf dem Hinweg eine Lücke des Zahnrads passiert hat, entweder auf einen Zahn, oder es gelangt wieder durch eine Lücke, und nur dann sieht man es. Fizeau kam damals auf einen um 5 % zu großen Wert.
Léon Foucault verbesserte 1850 die Methode weiter, indem er mit der Drehspiegelmethode die Messstrecken deutlich verkürzte. Damit konnte er erstmals die Materialabhängigkeit der Lichtgeschwindigkeit nachweisen: Licht breitet sich in anderen Medien langsamer aus als in Luft. Im Experiment fällt Licht auf einen rotierenden Spiegel. Von diesem wird es auf einen festen Spiegel abgelenkt, wo es zurück auf den rotierenden Spiegel reflektiert wird. Da sich der Drehspiegel aber inzwischen weiter gedreht hat, wird der Lichtstrahl nun nicht mehr auf den Ausgangspunkt reflektiert. Durch Messung der Verschiebung des Punktes ist es bei bekannter Drehfrequenz und bekannten Abständen möglich, die Lichtgeschwindigkeit zu bestimmen. Foucault veröffentlichte sein Ergebnis 1862 und gab  zu  Kilometer pro Sekunde an.
Simon Newcomb und Albert A. Michelson bauten wiederum auf Foucaults Apparatur auf und verbesserten das Prinzip nochmals. 1926 benutzte Michelson in Kalifornien ebenfalls rotierende Prismenspiegel, um einen Lichtstrahl vom Mount Wilson zum Mount San Antonio und zurückzuschicken. Er erhielt , nur 12 ppm über dem heutigen Wert.
James Bradley konnte mit seinen Untersuchungen zur Aberration von 1728 nicht nur die Lichtgeschwindigkeit selbst bestimmen, sondern auch erstmals Aussagen über ihre Konstanz treffen. Er beobachtete, dass die Aberration für alle Sterne in der gleichen Blickrichtung während eines Jahres in identischer Weise variiert. Daraus schloss er, dass die Geschwindigkeit, mit der Sternenlicht auf der Erde eintrifft, im Rahmen seiner Messgenauigkeit von etwa einem Prozent für alle Sterne gleich ist.
Um zu klären, ob diese Eintreffgeschwindigkeit davon abhängt, ob sich die Erde auf ihrem Weg um die Sonne auf einen Stern zu oder von ihm weg bewegt, reichte diese Messgenauigkeit allerdings nicht aus. Diese Frage untersuchte zuerst François Arago 1810 anhand der Messung des Ablenkwinkels von Sternenlicht in einem Glasprisma. Nach der damals akzeptierten Korpuskulartheorie des Lichtes erwartete er eine Veränderung dieses Winkels in einer messbaren Größenordnung, da sich die Geschwindigkeit des einfallenden Sternenlichts zu der Geschwindigkeit der Erde auf ihrem Weg um die Sonne addieren sollte. Es zeigten sich jedoch im Jahresverlauf keine messbaren Schwankungen des Ablenkwinkels. Arago erklärte dieses Ergebnis mit der These, dass Sternenlicht ein Gemisch aus verschiedenen Geschwindigkeiten sei, während das menschliche Auge daraus nur eine einzige wahrnehmen könne. Aus heutiger Sicht kann seine Messung jedoch als erster experimenteller Nachweis der Konstanz der Lichtgeschwindigkeit betrachtet werden.
Mit dem Aufkommen der Vorstellung von Licht als Wellenphänomen formulierte Augustin Fresnel 1818 eine andere Interpretation des Arago-Experiments. Danach schloss die Analogie zwischen mechanischen Wellen und Lichtwellen die Vorstellung ein, dass sich Lichtwellen in einem gewissen Medium ausbreiten müssen, dem sogenannten Äther, so wie sich auch Wasserwellen im Wasser ausbreiten. Der Äther sollte dabei den Bezugspunkt für ein bevorzugtes Inertialsystem darstellen. Fresnel erklärte das Ergebnis von Arago durch die Annahme, dass dieser Äther im Inneren von Materie teilweise mitgeführt werde, in diesem Fall im verwendeten Prisma. Dabei würde der Grad der Mitführung in geeigneter Weise vom Brechungsindex abhängen.
1887 führten Albert A. Michelson und Edward W. Morley ein bedeutsames Experiment zur Bestimmung der Geschwindigkeit der Erde relativ zu diesem angenommenen Äther durch. Dazu wurde die Abhängigkeit der Lichtlaufzeiten vom Bewegungszustand des Äthers untersucht. Das Experiment ergab wider Erwarten stets die gleichen Laufzeiten. Auch Wiederholungen des Experiments zu verschiedenen Phasen des Erdumlaufs um die Sonne führten stets zu dem gleichen Ergebnis. Eine Erklärung anhand einer weiträumigen Äthermitführung durch die Erde als Ganzes scheiterte daran, dass es in diesem Fall keine Aberration bei Sternen senkrecht zur Bewegungsrichtung der Erde gäbe.
Eine mit der maxwellschen Elektrodynamik verträgliche Lösung wurde mit der von George FitzGerald und Hendrik Lorentz vorgeschlagenen Längenkontraktion erreicht. Lorentz und Henri Poincaré entwickelten diese Hypothese durch Einführung der Zeitdilatation weiter, wobei sie dies jedoch mit der Annahme eines hypothetischen Äthers kombinierten, dessen Bewegungszustand prinzipiell nicht ermittelbar gewesen wäre. Das bedeutet, dass in dieser Theorie die Lichtgeschwindigkeit „real“ nur im Äthersystem konstant ist, unabhängig von der Bewegung der Quelle und des Beobachters. Das heißt unter anderem, dass die maxwellschen Gleichungen nur im Äthersystem die gewohnte Form annehmen sollten. Dies wurde von Lorentz und Poincaré jedoch durch die Einführung der Lorentz-Transformation so berücksichtigt, dass die „scheinbare“ Lichtgeschwindigkeit auch in allen anderen Bezugssystemen konstant ist und somit jeder von sich behaupten kann, im Äther zu ruhen. (Die Lorentz-Transformation wurde also nur als mathematische Konstruktion interpretiert, während Einstein (1905) auf ihrer Grundlage alle bisherigen Vorstellungen über die Struktur der Raumzeit revolutionieren sollte, siehe unten). Poincaré stellte noch 1904 fest, das Hauptmerkmal der lorentzschen Theorie sei die Unüberschreitbarkeit der Lichtgeschwindigkeit für alle Beobachter, unabhängig von ihrem Bewegungszustand relativ zum Äther (siehe lorentzsche Äthertheorie). Das bedeutet, auch für Poincaré existierte der Äther.
Jedoch war eine Theorie, in der das Äthersystem zwar als existent angenommen wurde, aber unentdeckbar blieb, sehr unbefriedigend. Eine Lösung des Dilemmas fand Einstein (1905) mit der Speziellen Relativitätstheorie, indem er die konventionellen Vorstellungen von Raum und Zeit aufgab und durch das Relativitätsprinzip und die Lichtkonstanz als Ausgangspunkte bzw. Postulate seiner Theorie ersetzte. Diese Lösung war formal identisch mit der Theorie von H. A. Lorentz, jedoch kam sie wie bei einer Emissionstheorie ganz ohne „Äther“ aus. Die Lichtkonstanz entnahm er dem lorentzschen Äther, wie er 1910 ausführte, wobei er im Gegensatz zu Poincaré und Lorentz erklärte, dass gerade wegen der Gleichberechtigung der Bezugssysteme und damit der Unentdeckbarkeit des Äthers der Ätherbegriff überhaupt sinnlos sei.[10] 1912 fasste er dies so zusammen:[11]
„Es ist allgemein bekannt, dass auf das Relativitätsprinzip allein eine Theorie der Transformationsgesetze von Raum und Zeit nicht gegründet werden kann. Es hängt dies bekanntlich mit der Relativität der Begriffe ‚Gleichzeitigkeit‘ und ‚Gestalt bewegter Körper‘ zusammen. Um diese Lücke auszufüllen, führte ich das der H. A. Lorentzschen Theorie des ruhenden Lichtäthers entlehnte Prinzip der Konstanz der Lichtgeschwindigkeit ein, das ebenso wie das Relativitätsprinzip eine physikalische Voraussetzung enthält, die nur durch die einschlägigen Erfahrungen gerechtfertigt erschien (Versuche von Fizeau, Rowland usw.).“
Die Unabhängigkeit der Lichtgeschwindigkeit von der Geschwindigkeit des gleichförmig bewegten Beobachters ist also Grundlage der Relativitätstheorie. Diese Theorie ist seit Jahrzehnten aufgrund vieler sehr genauer Experimente allgemein akzeptiert.
Mit dem Michelson-Morley-Experiment wurde zwar die Konstanz der Lichtgeschwindigkeit für einen mit der Lichtquelle mitbewegten Beobachter bestätigt, jedoch keineswegs für einen nicht mit der Quelle mitbewegten Beobachter. Denn das Experiment kann auch mit einer Emissionstheorie erklärt werden, wonach die Lichtgeschwindigkeit in allen Bezugssystemen lediglich konstant relativ zur Emissionsquelle ist (das heißt, in Systemen, wo sich die Quelle mit ±v bewegt, würde sich das Licht folglich mit c ± v ausbreiten). Auch Albert Einstein zog vor 1905 eine solche Hypothese kurz in Betracht,[12] was auch der Grund war, dass er in seinen Schriften das MM-Experiment zwar immer als Bestätigung des Relativitätsprinzips, aber nicht als Bestätigung der Lichtkonstanz verwendete.[13]
Jedoch würde eine Emissionstheorie eine völlige Reformulierung der Elektrodynamik erfordern, wogegen der große Erfolg von Maxwells Theorie sprach. Die Emissionstheorie wurde auch experimentell widerlegt. Beispielsweise müssten die von der Erde aus beobachteten Bahnen von Doppelsternen bei unterschiedlichen Lichtgeschwindigkeiten verzerrt ausfallen, was jedoch nicht beobachtet wurde. Beim Zerfall von sich mit annähernd  bewegenden π0-Mesonen hätten die dabei entstehenden Photonen die Geschwindigkeit der Mesonen übernehmen und sich annähernd mit doppelter Lichtgeschwindigkeit bewegen sollen, was jedoch nicht der Fall war. Auch der Sagnac-Effekt demonstriert die Unabhängigkeit der Lichtgeschwindigkeit von der Bewegung der Quelle. Alle diese Experimente finden ihre Erklärung in der Speziellen Relativitätstheorie, die u. a. aussagt: Licht überholt nicht Licht.
Obwohl die Konstanz der Lichtgeschwindigkeit experimentell nachgewiesen wurde, gibt es bis jetzt keine ausreichend überzeugende Erklärung für ihre Konstanz und ihren speziellen Wert. Die Schleifenquantengravitation beispielsweise diktiert, dass die Geschwindigkeit eines Photons nicht als Konstante definiert werden kann, sondern dass ihr Wert von der Photonfrequenz abhängt.[14] Tatsächlich gibt es Theorien, dass die Lichtgeschwindigkeit sich mit dem Alter des Universums ändert und dass sie im frühen Universum nicht konstant war. Albrecht und Magueijo[15] zeigen, dass die kosmologischen Evolutionsgleichungen zusammen mit einer variablen Lichtgeschwindigkeit die Probleme des Horizonts, der Flachheit und der kosmologischen Konstante lösen können. Die Annahme einer Raumzeit mit drei Raum- und zwei Zeitdimensionen gibt eine natürliche Erklärung für die Konstanz der Lichtgeschwindigkeit im beobachtbaren Universum und auch dafür, dass die Lichtgeschwindigkeit im frühen Universum variierte.[16]
Mit der Entwicklung der Relativitätstheorie zu Beginn des 20. Jahrhunderts wurde klar, dass  von fundamentaler Bedeutung für die Struktur von Raum und Zeit ist – weit über elektrodynamische Phänomene hinausgehend. Auch die Gravitation sollte sich mit dieser Geschwindigkeit ausbreiten. Nach dem direkten Nachweis von Gravitationswellen konnte dies 2017 mit hoher Präzision bestätigt werden. Eine Analyse des Ereignisses GW170817 zeigte, dass die relative Abweichung höchstens zwischen -3e-15 und +7e-16 liegen kann.[17]
Originalarbeiten:
Sonst:

Nahrung dient der Ernährung von Lebewesen. Sie ist Grundlage für Stoffwechsel und damit für das Leben. Bei Heterotrophen (meist Tiere) sind für Nahrung energiereiche organische Verbindungen zentral. Bei Autotrophen (meist Pflanzen) spricht man auch von Nährstoffen – diese enthalten dann keine energiereichen organischen Substanzen.
Bei Tieren mit Verdauungstrakt wird Nahrung nach der Aufnahme in den Körper meist mechanisch (z. B. durch Kauen) und chemisch (z. B. durch Magensäure) in ihre Bestandteile zerlegt. Die in bestimmten Nahrungsbestandteilen gespeicherte Energie wird im Energiestoffwechsel verwendet, um z. B. bei Warmblütern die Körpertemperatur konstant zu halten. Des Weiteren wird die Energie aus der Nahrung im anabolen Stoffwechsel (Anabolismus bzw. Baustoffwechsel) für Erhalt und Aufbau des Körpers (z. B. Wachstum bei Kindern oder Muskelaufbau bei Erwachsenen) eingesetzt. Aber auch Wasser und Salze gehören zur Nahrung, auch wenn sie nicht direkt im Energiestoffwechsel genutzt werden können.
Der Mangel an Nahrung wird als Hunger bezeichnet und kann zum Tod eines Lebewesens führen (Hungertod).
Nahrung für den Menschen wird als Lebensmittel bezeichnet.
Feste Lebensmittel werden als „Nahrungsmittel“ bezeichnet. Als Gegenstück werden manchmal sogenannte Genussmittel abgegrenzt, die Grenze ist aber unscharf. In der Vergangenheit wurde versucht, definitorisch anhand des stofflichen Nutzens für den Körper (Eiweiß, Kohlenhydrate, Fette, Vitamine etc.) oder auch des physiologischen Brennwertes zwischen Nahrungsmitteln und Genussmitteln zu unterscheiden.[1] Mit der zunehmenden Kenntnis über Inhaltsstoffe, zum Beispiel über sekundäre Pflanzenstoffe, ist diese tradierte Trennung noch schwieriger geworden. Beispielsweise lässt sich heute bei Gewürzen unter Umständen ein konkreter stofflicher Nutzen darlegen, wo zuvor nur der Genuss als Nutzen feststellbar war. Heute wird die Unterscheidung zumeist anhand der Betrachtung als Konsumgut vorgenommen. Damit unterliegt die Grenzziehung dem gesellschaftlichen Wandel. Die regional und kulturell jeweils bedeutendsten Nahrungsmittel werden auch als Grundnahrungsmittel, die Ablehnung aus kulturellen Gründen als Nahrungstabu bezeichnet.
Feste Nahrung für gehaltene Tiere wird als Futtermittel bezeichnet.
Das Nahrungsangebot beschreibt das Vorhandensein von entsprechender Nahrung, welche der jeweilige Organismus benötigt. Nahrung ist oft ein limitierender Faktor und hat bei einem Überangebot für Lebewesen mit hoher Reproduktionsrate meist eine explosionsartige Vermehrung zur Folge.
Bestes Beispiel ist Bierhefe, welche für das Bierbrauen verwendet wird. Die Bierwürze stellt das Nährmedium dar, in dem sich die Pilze exponentiell vermehren. Solange bis sie an ihren eigenen Stoffwechselprodukten, dem Ethanol, zugrunde gehen und schlagartig absterben. Das ist bei Bakterien­kulturen zu beobachten.
Im Tierreich, wie z. B. bei Säugetieren, wirkt das Angebot regulierend auf die Bestandsentwicklung einzelner Tierarten. Das bedeutet, dass bei zunehmenden Nahrungsangebot eines Fleischfressers, dessen Bestand nach und nach wächst. Dadurch wächst der Druck auf die Population der Beute, deren Bestand wieder schrumpft und somit zeitversetzt, auch die des Jägers. Um die durch die Räuber-Beute-Beziehung bedingten Schwankungen der Populationszahlen nachzuvollziehen, wurden die wichtigsten Zusammenhänge in den Lotka-Volterra-Regeln zusammengefasst.
Da der Mensch an oberster Stelle der Nahrungskette steht, greift er am massivsten in die Gleichgewichte der Natur ein. Durch die synthetische Herstellung von Dünger und Pflanzenschutzmitteln (Haber-Bosch-Verfahren) wird eine Weltbevölkerung von heute mehr als acht Milliarden Menschen erst ermöglicht. Einen weiteren wichtigen Faktor der Nahrungssicherung bietet die Konservierung.[2] Da der Mensch am Ende der Nahrungskette steht, ist er durch Schadstoffbelastungen am meisten gefährdet. Deshalb wird oft davor gewarnt, Seefische zu verzehren,[3] da sie die Schadstoffe des Meeres in ihrem Körper besonders anreichern.
Mikroorganismen sorgen für eine Schließung der Nahrungskette, indem sie abgestorbene Pflanzen und Tiere zersetzen und somit wieder Nahrung für Pflanzen bereitstellen. Sie können aber auch das Nahrungsangebot anderer Lebewesen erweitern, wie z. B. Joghurt oder Käsekulturen. Oder sie schaden anderen Organismen, wie z. B. Krankheitserreger, welche sich von Gewebe und Blutzellen ernähren. Sie sorgen damit für eine natürliche Auslese.
Mit dem Begriff „geistige Nahrung“ werden oft Herausforderungen für das Denken umschrieben, beispielsweise Denksport­aufgaben oder „Lesestoff“, als „Nahrung für die Seele“ hingegen die Weitergabe von Emotionen und die Psyche schmeichelnde menschliche Reaktionen wie Anerkennung, Akzeptanz, Aufmerksamkeit, Bestätigung, Fürsorge, Lob, Komplimente, Respekt, Verständnis und Wertschätzung.

Als Zucker wird neben verschiedenen anderen Zuckerarten ein süß schmeckendes, kristallines Lebensmittel bezeichnet, das aus Pflanzen gewonnen wird und hauptsächlich aus Saccharose besteht.
Hauptquellen sind Zuckerrohr (ca. 80 % der weltweiten Zuckerproduktion, Anbau in den Tropen und Subtropen[1][2]) und Zuckerrübe (ca. 20 % der Zuckerproduktion weltweit, Anbau in den gemäßigten Zonen, z. B. Mitteleuropa, in den USA auch als transgene Zuckerrübe H7-1). 2018 wurden weltweit etwa 1,907 Mrd. Tonnen Zuckerrohr und 275 Mio. Tonnen Zuckerrüben produziert; die daraus jährlich gewonnene Menge an Rohzucker lag 2016 bei ca. 176 Millionen Tonnen.[2] Hauptanbauländer für Zuckerrohr sind Brasilien, Indien, und China, für Zuckerrüben sind es Russland, Frankreich und die USA.[3][4] Der durchschnittliche Zuckerkonsum lag 2021 in Deutschland bei 32,5 kg pro Kopf, was einer täglichen Menge von rund 89 Gramm entspricht.[5]
Der Konsum von Zucker gilt als eine der Ursachen vieler Zivilisationskrankheiten wie Fettleibigkeit, Diabetes, Krebs, Karies sowie Darm-, Herz- und Gefäßkrankheiten. Um das Risiko für diese Erkrankungen zu minimieren sollten nicht mehr als 25 g Zucker pro Tag konsumiert werden.[6]
Sein physiologischer Brennwert beträgt 16,8 kJ oder 4,0 kcal pro Gramm (zum Vergleich: Alkohol liefert 29,8 kJ pro Gramm, Fette etwa 39 kJ pro Gramm), mit einer Dichte von 1,6 g/cm³ ist er schwerer als Wasser (1 g/cm³). (Für die Messung von Zuckermengen im Haushalt muss nicht von der Dichteangabe ausgegangen werden, sondern von der Schüttgutdichte, die geringer ausfällt. Sie liegt für Kristallzucker bzw. gekörnten Zucker zwischen 0,67[7] und 1,02 g/cm³.[8]) Bei 20 °C sind 203,9 g Zucker in 100 ml Wasser löslich, bei 100 °C 487,2 g in 100 ml.[9]
Das Wort „Zucker“ (von althochdeutsch zuccer, seit dem 12. Jahrhundert von mittellateinisch zuccarum[10][11]) geht auf das Altindische zurück (Sanskrit शर्करा śarkarā, eigentlich „Grieß, Geröll, Kies“, aber auch „Sandzucker“), aus dem es ins Griechische (altgriechisch σάκχαρον sákcharon, daraus auch lateinisch saccharum[12] und ins Arabische (arabisch سكر sukkar) gelangt ist (vgl. deutsch Saccharin). Das Deutsche entlehnte das Wort wohl aus dem Italienischen (italienisch zucchero, von mittellateinisch zuccarum), der älteste Nachweis datiert auf das 13. Jahrhundert.[13]
Nach einer Wachstumszeit von 15 Monaten wurden die bis zu vier Meter hohen Pflanzen geerntet, indem die Stängel wenige Zentimeter über dem Boden gekappt und direkt von den Blättern befreit wurden. Anschließend wurden die Stängel direkt gepresst, um den hohen Zuckergehalt in ihnen zu erhalten. Der so entstehende Saft wurde dann in großen Kupferkesseln zum Kochen gebracht, wodurch eine aus braunem Rohzucker und Melasse bestehende Masse entstand. Aus dieser Masse musste die Melasse über Wochen ablaufen, bevor der Zucker in der Sonne trocknete und in Fässern verschifft wurde.[4]
Da bei diesem Herstellungsprozess viel manuelle Arbeit anfiel, waren Sklaven, die unter brutalen Bedingungen auf den Plantagen 12 Stunden täglich arbeiteten, für den Produktionsprozess essentiell. 
Andreas Sigismund Marggraf hatte 1747 nachgewiesen, dass im Rübensaft Zucker enthalten ist. Die Fabrikationsverfahren, die sein Schüler Franz Karl Achard um 1800 entwickelte, führten 1825 zur Entstehung der Rübenzuckerindustrie, die Ende des 19. Jahrhunderts im Weltmaßstab ebenso viel Zucker erzeugte wie die traditionelle Rohrzuckerindustrie.
Der Landwirtschaft war es gelungen, Rüben mit hohem Zuckergehalt zu züchten. Landstriche wie die Magdeburger Börde stellten sich auf den Anbau von Rüben ein. Diese Monokulturen, die viel Dünger benötigten, stimulierten ihrerseits die Entwicklung der Düngemittelindustrie.
Chemiker und Techniker sorgten durch Rationalisierungen und Automatisierungen dafür, trotz der saisonbedingten geringen Auslastung der Fabriken (der sogenannten Kampagne), dass die Rübenzuckerindustrie rentabel wurde. Zu den Pionieren der Rübenzuckerindustrie gehört Adolph Frank, der 1858 ein Patent zur Scheidung und Reinigung von Rübensäften erhielt.
Die Zuckerrüben werden nach der Ernte gereinigt und zerkleinert. Die entstehenden Zuckerrübenschnitzel werden in Extraktionstürmen mit heißem Wasser versetzt. Der enthaltene Zucker wird herausgelöst (Rohsaft). Mit Kalkmilch werden Nichtzuckerstoffe im Saft gebunden. Der so geklärte Dünnsaft enthält etwa 16 % Saccharose und ist hellgelb. Durch Verdampfungsapparate wird so lange Wasser entzogen, bis der Zuckergehalt im nun goldbraunen zähflüssigen Dicksaft ungefähr 75 % beträgt. Die weitere Eindickung geschieht mit so viel Unterdruck, dass das Wasser bereits bei 65–80 °C verdampft und der Zucker noch nicht karamellisiert. Nach Zusatz von Impfkristallen beginnt die Kristallisation, die bis zur gewünschten Kristallgröße läuft. In Zentrifugen wird der anhaftende Sirup (Melasse) von den Kristallen getrennt. Der weiße Zucker wird nun nochmals in Wasser gelöst und danach kristallisiert. Dadurch erhält man einen besonders reinen und weißen Zucker (Raffinade). Je stärker der Zucker raffiniert wird, desto länger ist er haltbar.[26]
Im Jahr 2019 wurden weltweit 169.630.291 Tonnen Rohzucker hergestellt, in Europa waren es 28.631.921 t. Die wichtigsten europäischen Herstellerländer sind Frankreich, Ukraine und Tschechien.[27]
Folgende Tabelle gibt eine Übersicht über die zehn größten Produzenten von Zucker weltweit, die insgesamt 73,3 % der Erntemenge produzierten.
Laut Statistik der FAO wies im Jahr 2018 Barbados die höchste Pro-Kopf-Versorgung an Zucker auf (48,18 kg/Jahr), Kiribati steht an 2. Stelle (47,45 kg/Jahr), gefolgt von Kuba (46,72 kg/Jahr). Zum Vergleich: In Österreich wurden rechnerisch pro Kopf und Jahr durchschnittlich 37,96 kg und in Deutschland 33,58 kg ermittelt. Aus den Zahlen geht nicht hervor, wie das Produkt in dem betreffenden Land verwendet wird.[28]
In der Europäischen Union wurde der Zuckerpreis bisher (Stand August 2017) möglichst konstant gehalten.
Die EU veröffentlicht regelmäßig einen Zuckerpreis-Report. Der Preis lag von 2006 bis 2009 um 600 Euro pro Tonne, fiel dann 2010 auf unter 500 Euro und stieg zwischen Herbst 2011 und Herbst 2013 auf über 700 Euro.[29]
Die Erntemengen in der EU und in anderen Staaten, speziell in denen der AKP-Gruppe aufgrund deren Importprivileg, beeinflussen den Zuckerpreis.
Der Weltmarktpreis liegt normalerweise deutlich unter dem EU-Preis. Nur im Jahr 2011 lag der Weltmarktpreis kurzzeitig darüber.
Ende September 2017 fiel – nach der Milchquotenregelung – auch die Europäische Zuckermarktordnung. Davor war festgelegt, dass mindestens 85 % des in der EU vermarkteten Zucker in der EU erzeugt sein müssen, und dass die europaweite Produktion mit 13,5 Millionen Tonnen pro Jahr begrenzt war. Weiter mussten Zuckerproduzenten ihren zuliefernden Landwirten laut EU-Recht pro Tonne Zuckerrüben einen gewissen Mindestpreis zahlen. Es wurde erwartet, dass die Preise für abgelieferte Rüben und Zucker ab Fabrik niedriger werden.[30][31]
Zucker wird in vielen verschiedenen Darreichungsformen angeboten. Diese unterscheiden sich je nach verwendetem Rohstoff, äußerer Form, Zusammensetzung und Art der Verarbeitung. Außerdem gibt es Zuckerprodukte mit verschiedenen Zusätzen.
Einige Zuckerbezeichnungen sind in Deutschland durch Verordnung geschützt.[32][33]
Zuckerrübensirup
Brauner Zucker (feinkörnig)
Brauner Zucker (großkörnig)
Raffinierter Zucker
Kandiszucker
Puderzucker
Der jährliche Zuckerkonsum lag 1997 in Österreich bei 40,4 Kilogramm pro Person und hat sich damit innerhalb der letzten 150 Jahre auf das Zwanzigfache gesteigert, was eine bedeutende Rolle als Ursache vermehrter Adipositas spielen dürfte. Leichtverdauliche Kohlenhydrate wie Zucker haben zudem größere Schwankungen des Insulinspiegels zur Folge, man spricht von einer höheren glykämischen Last, welches sich diesbezüglich ebenfalls negativ auswirkt.
Im Jahr 2003 erstellte ein Gremium internationaler Experten im Auftrag der Weltgesundheitsorganisation (WHO) und der Ernährungs- und Landwirtschaftsorganisation der Vereinten Nationen (FAO) einen Report. Er konstatierte, dass, wer sich gesund ernähren wolle, nicht mehr als 10 % seiner Nährstoffe aus sogenanntem freien Zuckern (englisch: free sugars) beziehen sollte (entspricht etwa 40–50 g pro Tag). Mit freien Zuckern sind Zucker gemeint, die den Lebensmitteln vom Hersteller, Koch oder Verbraucher zugesetzt werden, sowie Zucker, die natürlicherweise in Honig, Sirup und Fruchtsäften enthalten sind.[38][39]
Im Jahr 2009 gab die American Heart Association die Empfehlung heraus, dass die tägliche Aufnahme von Zucker bei maximal 45 g pro Tag (Männer) bzw. 30 g pro Tag (Frauen) liegen sollte.[40]
2015 hat die WHO die 10-%-Grenze als „strong recommendation“ (starke Empfehlung) bekräftigt, sich jedoch im Hinblick auf die Vermeidung von Karies für eine zusätzliche Halbierung auf 5 % als „conditional recommendation“ (bedingte Empfehlung) ausgesprochen.[41] Die neue Richtlinie der WHO hat Besorgnis bei den Allgemeinen Ortskrankenkassen (AOK) ausgelöst. Kai Kolpatzik, Präventionsexperte des AOK-Bundesverbands, fordert nun die Bundesregierung dazu auf, ähnlich wie auch für Alkohol und Nikotin, Maßnahmen gegen den erhöhten Zuckerkonsum der Deutschen zu treffen.[42] Das Bundesministerium für Ernährung und Landwirtschaft empfiehlt weiterhin eine maximale Menge von 10 % und bezieht sich dabei ausdrücklich auf die 2015 erschienene Richtlinie der WHO.[43]
Die auf vielen Lebensmitteln angegebene Nährwertkennzeichnung beruht auf einer Referenzmenge von 90 g pro Tag und liegt damit fast doppelt so hoch wie der von der WHO empfohlene Wert.[44]
Ernährungsphysiologisch bedenklich ist der erhöhte oder regelmäßige Konsum zuckerhaltiger Getränke, der zu Zivilisationskrankheiten wie dem Metabolischen Syndrom, Übergewicht, Adipositas und Diabetes mellitus – immer häufiger schon im Kindesalter – führt. Deshalb riet die Weltgesundheitsorganisation 2016 zur Zuckersteuer, um einen spürbaren Rückgang des Zuckerkonsums zu und daraus resultierenden Erkrankungen zu erreichen.[45][46] Zuckersteuern wurden von Frankreich, Ungarn, Finnland und Mexiko eingeführt.[47]
Es wird diskutiert, ob regelmäßiger Zuckerkonsum auch als ein Risikofaktor für Osteoporose gilt[48].
Ob Zucker ein Vitaminräuber sei, konnte bisher nicht bestätigt werden[49].
Es wird diskutiert, ob Zucker die Entstehung von Krebs fördert und ob eine zuckerfreie Nahrung das Wachstum von Krebs behindern kann. Diese These (vgl. Warburg-Hypothese) hatte einige Anhänger auch unter Ärzten, wird aktiv erforscht, und es gibt Initiativen für eine „Krebsdiät“, die auf zuckerfreier oder zuckerarmer Ernährung basiert.[50][51][52][53][54]
Die ursächliche Mitwirkung von Zucker bei der Entstehung von Zahnkaries ist heute unumstritten. Die bedeutendste Bakterienart ist Streptococcus mutans. Nahrungszucker gelangt durch Diffusion in die bakteriellen Zahnbeläge, wo sie zu intermediären Säuren abgebaut werden, welche unter einer hinreichend dicken Plaque lokal zur Entkalkung des Zahnschmelzes und dadurch zu Karies führen. Ebenfalls von Bedeutung sind Speichelzusammensetzung (Pufferkapazität, Lysozym-Gehalt), Zahnschmelzlöslichkeit (Fluoridierungsgrad) und Mundhygiene. Ob Zucker in Form von Haushaltszucker, Honig, leicht verdaulicher Stärke o. ä. aufgenommen wird, ist dabei bedeutungslos.[55]
Die Zuckerkrankheit beruht entweder auf einer Autoimmunreaktion, die die Inselzellen der Bauchspeicheldrüse schädigt (Typ 1), oder auf einer entwickelten Insulinresistenz (Typ 2). Folgen sind ein chronisch erhöhter Blutzuckerspiegel und, daraus folgend, ein erhöhter Insulinspiegel im Blut mit Heißhunger, Schlafproblemen, Stoffwechselstörungen mit Hypertriglyzeridämie und erniedrigtem HDL-Cholesterin, Übergewicht und Fettleibigkeit und ein meist sich entwickelndes Metabolisches Syndrom. Die WHO empfahl 2015 daher erstmals eine deutlich reduzierte Zuckeraufnahme mit der Nahrung für Kinder und Erwachsene.[56]
Eine weit verbreitete Annahme – vor allem in den USA – ist, dass Zucker hyperaktives Verhalten fördere, ADHS-Symptome verschlimmere bzw. ADHS verursachen könne, insbesondere bei Kindern. Das National Institute of Mental Health der USA kommt jedoch zu dem Schluss, dass die Mehrzahl der verfügbaren Studien dieser Theorie widersprechen.[57]
So wurden in einer Studie 35 Jungen im Alter von fünf bis sieben Jahren ausgewählt, deren Mütter angaben, dass ihre Söhne „zuckersensitiv“ seien. Die Mütter der Jungen wurden in zwei Gruppen geteilt. Denjenigen der einen Gruppe wurde gesagt, ihre Söhne hätten eine große Menge Zucker bekommen, während denen der anderen Gruppe (der Kontrollgruppe) gesagt wurde, ihre Söhne hätten ein Placebo bekommen. Tatsächlich hatten jedoch alle Kinder das Placebo (Aspartam) bekommen. Mütter, denen gesagt worden war, dass ihre Kinder Zucker bekommen hatten, schätzten das Verhalten ihrer Söhne signifikant stärker als hyperaktiv ein als die Mütter in der Kontrollgruppe. Auch wurde bei diesen Müttern ein anderes Verhalten beobachtet. So befanden sich diese Mütter öfter in der Nähe ihrer Söhne, kritisierten diese eher, sahen öfter nach und sprachen mehr zu ihnen, als es in der Kontrollgruppe der Fall war.[58]
In zwei anderen Studien wurde der Effekt von Zucker auf das Verhalten und das Lernen hyperaktiver Jungen untersucht. Die Forscher gaben den Kindern Lebensmittel, die entweder Zucker oder ein Placebo (Aspartam) enthielten. Die Kinder, die Zucker erhalten hatten, zeigten kein anderes Verhalten oder andere Lernfähigkeiten als diejenigen, denen man das Placebo gegeben hatte.[59] Eine ähnliche Studie mit höheren Mengen Saccharose und einer zusätzlichen Saccharin-Kontrollgruppe kam zu Ergebnissen.[60]
Zahlreiche Studien haben den Zusammenhang von Zuckerkonsum und Suchterscheinungen („Abhängigkeitssyndromen“) untersucht; die Übertragbarkeit der meist in Laborexperimenten an Ratten vorgenommenen Ergebnisse ist wissenschaftlich umstritten, ebenso die Frage nach der Einordnung von Zucker als Droge.
Zucker hat als nachwachsender Rohstoff eine zunehmende Bedeutung. Dieser sogenannte Industriezucker wird vor allem als Disaccharid Saccharose aus Zuckerrohr oder Zuckerrüben gewonnen. Das Zuckerpolymer Stärke (ein Polysaccharid) besteht aus dem Monomer Glucose (ein Monosaccharid) und wird beispielsweise aus Getreide, Mais und Stärkekartoffeln gewonnen. Ein weiteres häufig vorkommendes Glucosepolymer ist Cellulose, die vor allem aus Holz gewonnen wird.
Eine wichtige Verwendung ist die energetische Verwertung, wie die Herstellung von Bioethanol und anderen Biokraftstoffen aus Zucker oder Stärke[61] oder die thermische Verwendung (Verbrennung) von Cellulose als Bestandteil von Brennholz. Eine große Bedeutung hat auch die stoffliche Nutzung von Zucker. Zum einen dienen sie in der Biotechnologie als Energie- und Kohlenstoffquelle in Fermentationsansätzen zur Herstellung von organischen Lösungsmitteln, verschiedenen Rohstoffen (z. B. zur Herstellung von Bioplastik) und anderem. In chemischen Verfahren werden Zucker als Rohstoff zur Herstellung von Tensiden,[62] Polyolen und anderen Produkten eingesetzt.
Der Industriezucker-Markt wird für die Europäische Union im Jahre 2016 auf 2 Millionen Tonnen geschätzt, die Verwendung zur Ethanol-Herstellung als Kraftstoff wird in der EU auf 1,5 Millionen Tonnen geschätzt,[63] hier ist Brasilien führend (die USA erzeugen Ethanol vorwiegend nicht aus Zucker), das 2019 hauptsächlich aus Zuckerrohr 33,14 Millionen m³ Ethanol als Kraftstoffbestandteil erzeugte.[64]
Die Braunfärbung beim Erhitzen (> 140 °C) beruht auf einer nichtenzymatischen chemischen Reaktion, der Karamellisierungsreaktion.
Haushaltszucker schmilzt bei 186 °C. Die Braunfärbung kann daher schon unterhalb des Schmelzpunkts erfolgen, steigert sich aber ab 190 °C rapide. Der Schmelzpunkt des Zuckers eignet sich auch zur einfachen Temperatur-Kalibrierung eines Backofens.
Neben dem hier beschriebenen Zucker aus Saccharose gibt es weitere Zuckerarten, die aus anderen Sacchariden (siehe dort für eine ausführlichere Übersicht und die chemischen Hintergründe) bestehen:

Die Geometrie (altgriechisch γεωμετρία geometria, ionisch γεωμετρίη geometriē, ‚Erdmaße‘, ‚Erdmessung‘, ‚Landmessung‘) ist ein Teilgebiet der Mathematik.
Einerseits versteht man unter Geometrie die zwei- und dreidimensionale euklidische Geometrie, die Elementargeometrie, die auch im Mathematikunterricht – früher unter dem Begriff Raumlehre – gelehrt wird und die sich mit Punkten, Geraden, Ebenen, Abständen, Winkeln usw. beschäftigt, sowie diejenigen Begriffsbildungen und Methoden, die im Zuge einer systematischen und mathematischen Behandlung dieses Themas entwickelt wurden.
Andererseits umfasst der Begriff Geometrie eine Reihe von großen Teilgebieten der Mathematik, deren Bezug zur Elementargeometrie für Laien nur mehr schwer erkennbar ist. Dies gilt insbesondere für den modernen Begriff der Geometrie, der im Allgemeinen die Untersuchung invarianter Größen bezeichnet.
Die älteste erhaltene Geometrieabhandlung in deutscher Sprache stammt vom Beginn des 15. Jahrhunderts. Es handelt sich dabei um die sogenannte Geometria Culmensis, welche im Auftrag des Deutschorden-Hochmeisters Konrad von Jungingen im Raum Culm verfasst worden ist und neben dem, im Wesentlichen auf der Practica geometriae[1] des Dominicus de Calvasio beruhenden, lateinischen Text auch dessen deutsche Übersetzung enthält.[2] Als erstes gedrucktes und eigenständiges Geometriebuch in deutscher Sprache gilt Albrecht Dürers Underweysung der messung mit dem zirckel und richtscheyt in Linien ebnen unnd gantzen corporen aus dem Jahre 1525.[3]
Die Verwendung des Plurals weist darauf hin, dass der Begriff Geometrie in einem ganz bestimmten Sinn gebraucht wird, nämlich Geometrie als mathematische Struktur, deren Elemente traditionellerweise Punkte, Geraden, Ebenen … heißen und deren Beziehungen untereinander durch Axiome geregelt sind. Dieser Standpunkt geht zurück auf Euklid, der versucht hat, die Sätze der ebenen euklidischen Elementargeometrie auf einige wenige Postulate (d. h. Axiome) zurückzuführen. Die folgende Liste soll einen Überblick über verschiedene Typen von Geometrien, die in dieses Schema passen, geben:
In jeder Geometrie interessiert man sich für diejenigen Transformationen, die bestimmte Eigenschaften nicht zerstören (also ihre Automorphismen): Zum Beispiel ändern weder eine Parallelverschiebung noch eine Drehung oder Spiegelung in einer zweidimensionalen euklidischen Geometrie die Abstände von Punkten. Umgekehrt ist jede Transformation, die die Abstände von Punkten nicht ändert, eine Zusammensetzung von Parallelverschiebungen, Drehungen und Spiegelungen. Man sagt, dass diese Abbildungen die Transformationsgruppe bilden, die zu einer ebenen euklidischen Geometrie gehört, und dass der Abstand zweier Punkte eine euklidische Invariante darstellt. Felix Klein hat in seinem Erlanger Programm Geometrie allgemein als die Theorie der Transformationsgruppen und ihrer Invarianten definiert (vgl. Abbildungsgeometrie); jedoch ist das keineswegs die einzig mögliche Definition. Im Folgenden sind Geometrien und prominente Invarianten aufgezählt:
Die folgende Liste umfasst sehr große und weitreichende Gebiete mathematischer Forschung:
Üblicherweise werden im Geometrieunterricht Geräte wie Zirkel, Lineal und Geodreieck, aber auch der Computer (siehe auch: Dynamische Geometrie) verwendet. Die Anfangsgründe des Geometrieunterrichts befassen sich etwa mit geometrischen Transformationen oder dem Messen von geometrischen Größen wie Länge, Winkel, Fläche, Volumen, Verhältnisse usw. Auch komplexere Objekte wie spezielle Kurven oder Kegelschnitte kommen vor. Darstellende Geometrie ist die zeichnerische Darstellung der dreidimensionalen euklidischen Geometrie in der (zweidimensionalen) Ebene.
Die Aussagen werden in Sätzen formuliert.
Grundlegende Sätze:
Nach der Geometrie wurde der Asteroid (376) Geometria benannt. 

Eine Schraube ist ein zylindrischer oder leicht kegeliger (konischer) Körper, in dessen Oberfläche ein Gewinde eingeschnitten oder -gewalzt ist. Bei Schaftschrauben (Schrauben mit Teilgewinde) ist nur ein Teil des Schafts mit einem Gewinde versehen. An einem ihrer Enden (bei der Schaftschraube am gewindefreien Ende) befindet sich in der Regel ein Schraubenkopf. Über das »Kinn« des Schraubenkopfes (die untere Ringfläche) erfolgt die Kraftübertragung auf das zu befestigende Bauteil. An einem Ende der Schraube (bei der Kopfschraube am Kopf) befindet sich der sogenannte Drehantrieb. Dieser besitzt eine passende Kontur für den Formschluss mit dem Werkzeug, meist ein Schraubendreher, ein Schraubenschlüssel oder auch Bit, mit dem die Schraube ein- und ausgedreht werden kann. Nebst dessen existieren auch Schrauben, die per Hand gedreht werden und am Kopf über eine Rändelung, Flügel oder einen Sterngriff oder vergleichbares (vgl. Blattschraube) verfügen.
Die beiden Hauptgruppen sind selbstschneidende Schrauben (umgangssprachlich „Holzschrauben“: leicht konisch, das Gegengewinde wird beim Eindrehen in das Werkstück aus Holz, Kunststoff, Metall oder Stein geschnitten) und Schrauben mit Regelgewinden (umgangssprachlich „Metall-“ oder „Maschinenschrauben“: zylindrisch, das Gegengewinde wird bereits vor der Verschraubung in das Werkstück, meist Metall oder Kunststoff, geschnitten). Schrauben sind in vielen Formen genormte Maschinenelemente.
Eine mit einer Schraube hergestellte Verbindung ist in der Regel reversibel und lösbar. Ausnahmen stellen hier z. B. Schrauben-Antriebe dar, die nur in Einschraubrichtung einen Kraftschluss ermöglichen (z. B. bei manipulationssicheren Geräten) - diese können ohne spezielle Werkzeuge (Ausdreher) nicht wieder gelöst werden.
Im ersten Jahrhundert v. Chr. waren im Mittelmeerraum Spindelpressen aus Holz zum Auspressen von Öl- und Weinfrüchten bekannt. Diese Pressen geben eine frühe Auskunft von der Umwandlung einer Drehbewegung zwischen Schraube und Mutter in eine Längsbewegung zwischen beiden und den ältesten Hinweis auf die Kraftverstärkung (Keil-Wirkung zwischen den Gewindegängen; Prinzip der schiefen Ebene) mittels Schraube (und Mutter).
Erste Metallschrauben wurden vereinzelt in der römischen Antike angefertigt, wie beispielsweise die Nadelhalter an mehreren Zwiebelknopffibeln aus dem 4. Jahrhundert, die als von den Römischen Kaisern vergebene prunkvolle Rangabzeichen nicht nur aus wertvollen Metallen, sondern auch technisch besonders aufwändig gestaltet waren.[1] Ebenso wurden zu Beginn des 15. Jahrhunderts Metallschrauben in Europa gefertigt, die sich aber wegen ihres hohen Preises ebenfalls nicht allgemein durchsetzen konnten. Erst die Industrialisierung im 18. Jahrhundert ermöglichte die preiswerte und massenhafte Herstellung und weite Verbreitung von Schrauben. Nachfolgend eine Zeittafel der neuzeitlichen Errungenschaften:
Schrauben ohne Kopf besitzen keine Verdickung am Ende, die als Antrieb dienen kann. Gewindestangen als Meterware können beliebig abgelängt werden und werden dann auch als Gewindestift oder Gewindebolzen bezeichnet. Stockschrauben sind Gewindestangen, die sowohl ein feines (Maschinen-)Gewinde, als auch ein grobes (Holz-)Gewinde auf der gegenüberliegenden Seite besitzen. 
Schrauben unterscheiden sich unter anderem durch das Gewinde und ihre Anwendung.
Die Kopfform wird nach folgenden Kriterien unterschieden:
In der Regel sind mehrere dieser Form-Kriterien für einen bestimmten Schraubenkopf anzuwenden. Beispielsweise hat der Kopf einer Linsensenkkopfschraube (Abbildung 12) üblicherweise eine flachkugelige Oberseite und eine kegelige Unterseite. Es gibt auch Fälle mit mehreren Formen nur eines Kriteriums. Der Schraubenkopf in Abbildung 13 hat zwei Randformen: Im unteren Teil ist er zylindrisch, im oberen Teil sechskantig. In der Oberseite des Kopfes befindet sich i. d. R. (außer bei Sechskantschrauben u. ä.) eine Aussparung für den Drehantrieb (siehe unten), was die Zahl seiner Erscheinungsformen weiter vergrößert.
1. rund
2. sechskantig
3. vierkantig
4. rund, gerändelt
5. ebene Oberseite
6. halbkugelige Oberseite
7. flach-kugelige (linsenförmige) Oberseite
8. zylindrischer Rand
9. kegeliger Rand
10. Panhead-Schraubenkopf, auch Flachkopf genannt, mit flacher Unterseite
11. Senkkopfschraube mit kegeliger Unterseite
12. Linsen­senk­kopf
13. Sechskant-Kopf mit zy­lin­dri­schem Flansch und meist einer Un­ter­kopf­ver­zah­nung, teil­wei­se als Schei­ben­kopf,[4] Sper­rzahn­schrau­be, Ripp­schrau­be oder Rah­men­schrau­be be­zeichnet.
Die häufigsten Profile am Schraubenkopf zum Ansatz eines Schraubendrehers oder Schraubenschlüssels sind:
Die Längenangabe bei Schrauben bezieht sich in der Regel auf den Teil der Schraube, der nach dem vollständigen Einschrauben nicht mehr zu sehen ist. Dies ist also die Länge des Gewindes zuzüglich des eventuell gewindelosen Teils des Schafts sowie des eventuell konischen Übergangs zum Schraubenkopf. Bei Schrauben mit Senkkopf wird die Länge ab der Oberseite des Kopfes gemessen.
Die benötigte Länge ergibt sich aus der vorgesehenen Einschraubtiefe im Grundmaterial bzw. der Höhe einer Standardmutter zuzüglich der Stärke der zu befestigenden Bauteile. Die Länge einer Schraube wird meist in Millimetern angegeben.
Schrauben werden meist aus Stahl gefertigt. Weitere Werkstoffe sind Titan, Aluminium, Messing, Kunststoff und historisch sowie für Kinderbaukästen auch Holz.
Stahlschrauben erhalten heute meist eine Korrosionsschutz-Beschichtung. In besonders korrosiven Umgebungen werden Messing-, Edelstahl- oder bei geringer Belastung auch Kunststoffschrauben eingesetzt.
In Frage kommen grundsätzlich folgende Materialien:
Korrosionsarme bzw. rostfreie Schrauben bestehen aus nichtrostendem Stahl, Nickellegierungen, Kupferlegierungen (Messing), Kunststoffen, gelegentlich Aluminium, Titan oder neuerdings auch aus kohlenstofffaserverstärktem Kunststoff („Carbon“).
Einfache Stahlschrauben dagegen benötigen eine Schutzbeschichtung, wenn sie nicht unter korrosionsfreien oder -armen Bedingungen verwendet werden. Mögliche Verfahren der Oberflächenbehandlung sind:
Untersuchungen haben ergeben, dass Korrosionsschutzschichten, die sechswertiges Chrom enthalten – z. B. chromatierte und galvanisch verzinkte Schichten – krebserregend sein können. Der EU-Altauto-Verordnung 2000/53/EG entsprechend müssen daher alle Neufahrzeuge ab dem 1. Juli 2007 frei von sechswertigem Chrom (Cr-VI) sein. Daher werden immer mehr Beschichtungen zum Beispiel auf den Zinklamellenüberzug umgestellt. Neben dem Automobilsektor ist davon z. B. auch die Elektronikindustrie betroffen, die ebenfalls auf Cr-VI-freie Verfahren umstellen muss (siehe RoHS-Richtlinie).
Die Korrosionsbeständigkeit von Beschichtungen wird häufig mit dem Salzsprühtest geprüft. Bei diesem Kurzzeittest werden beschichtete Teile (z. B. Schrauben) in einer Prüfkammer über eine definierte Stundenzahl einem ständigen Salznebel ausgesetzt. Bei der Bewertung der Ergebnisse von Salzsprühprüfungen ist zu berücksichtigen, dass diese selten mit dem Verhalten in natürlichen Umgebungen übereinstimmen.[7]
Weitere Oberflächenbehandlungen, die neben dem Korrosionsschutz auch zur Dekoration oder der besseren elektrischen Kontaktgabe dienen, sind das Versilbern, das Verkupfern, die Messingbeschichtung, das Verchromen, das Vernickeln und das Vergolden.
Für die Produktion von Kopfschrauben gibt es heute hauptsächlich zwei Herstellverfahren:
Kleinere Stückzahlen von Schrauben und Muttern werden auf Drehmaschinen meist aus Stangenmaterial gedreht. Das Gewinde kann dabei geschnitten oder gerollt werden. Bei kleinen Durchmessern werden die Innengewinde mit Hilfe von Gewindebohrern und die Außengewinde mit Schneideisen hergestellt. Diese beiden Werkzeuge mit mehreren gleichzeitig schneidenden Schneiden werden auch für die Herstellung kleinerer Gewindedurchmesser von Hand gebraucht. Für große Durchmesser oder Spezialgewinde wird ein Gewindedrehmeißel verwendet.
Es gibt Schrauben mit weniger als einem halben Millimeter Durchmesser für Uhrwerke und auch mannshohe, schenkeldicke Verbindungselemente an Großmaschinen und Bauwerken.
Schraubverbindungen sind lösbar, sie begünstigen die Wiederverwertung von Geräten, Maschinen und Anlagen. Sie erleichtern Reparaturen, Umbauten und das Demontieren ausgedienter Geräte zum sortenreinen Trennen der Komponenten, um sie gegebenenfalls wiederzuverwenden. Dem steht entgegen, dass Gerätehersteller mehr und mehr die Lösbarkeit und damit zum Beispiel Reparaturen absichtlich erschweren, um diese Werkstätten vorzubehalten. Es werden zum Beispiel ausgefallene Schraubenköpfe gewählt, für die Anschluss-Werkzeug vom Hersteller der Baugruppe nur an Vertragswerkstätten geliefert wird.
Die Schraubenköpfe sind mit einer Anschlussgeometrie, dem Antriebsprofil (z. B. Schlitz, Sechskant u. a.) zum Kontakt mit dem Abtriebsprofil eines Montagewerkzeug (Schraubendreher, Schraubenschlüssel) versehen, mit dem das Anzugsdrehmoment übertragen wird. Um gezielt einen bestimmten Wert des Anzugsdrehmoments zu erzeugen, wird ein Drehmomentschlüssel verwendet. Der optimale Wert ist besonders bei Dehnschraubenverbindungen erforderlich. Auch die Schrauben zur Befestigung einer Autofelge aus Stahl werden mittels Drehmomentschlüssel mit einem bestimmten Drehmoment angezogen. Die Radschrauben sind keine Dehnschrauben, aber die Umgebungen der Felgenlöcher geben elastisch nach, so dass selbstsichernde Schraubenverbindungen gleich wie mit Dehnschrauben entstehen. Eine zusätzliche Sicherung gegen Lösen besteht durch die Reibungskräfte zwischen Schraube und Felge, die infolge der Kegelform der Kontaktfläche vergrößert sind.
In der Industrie werden verschiedene Montageverfahren angewendet:
Die Schraube sollte so angezogen werden, dass die zu übertragende Betriebskraft (z. B. die Antriebs- oder Bremsmomente einer Radverschraubung) über die Reibung in der Verbindung sicher übertragen werden können. Ist das nicht der Fall und die Schraube wird Scherkräften ausgesetzt, besteht die Gefahr des selbsttätigen Losdrehens oder des Schraubenbruchs.
Die VDI-Richtlinie VDI 2230-1 „Systematische Berechnung hochbeanspruchter Schraubenverbindungen – Zylindrische Einschraubenverbindungen“ behandelt die Berechnung und Auslegung von Schraubenverbindungen.
Metallische Gewindeverbindungen ohne vorgegebenes Anzugsmoment sollten in korrosiver Umgebung durch Fett, Montagepaste, Schraubensicherung- oder Dichtungsmittel geschützt werden, um das spätere Lösen der Verbindung zu erleichtern. Ist das Anzugsmoment vom Hersteller vorgegeben, so kann das durch die Schmierung erleichterte Eindrehen der Schraube zur Überdehnung des Gewindes führen. Hier sollte der Vorgabe des Herstellers gefolgt oder gegebenenfalls eine spezielle, nichtschmierende Montagepaste verwendet werden.
Der Auftrag von Seife, zur Not auch Wachs, auf das Gewinde von Holzschrauben erleichtert das Eindrehen und kann das Abscheren der Schraube bei Überlastung verhindern; Öl oder Fett ist bei Holz ungeeignet, da dieses bei eventuell nachfolgender Oberflächenbehandlung die Haftung der Oberflächenschutzschicht verhindern kann. Verschraubungen an Rohren oder Behältern für reinen Sauerstoff dürfen nicht gefettet werden, da der Sauerstoff zur Selbstentzündung des Fettes führen kann.
Idealerweise bestehen die Hauptelemente einer Schraubverbindung aus Materialien mit gleichem thermischen Ausdehnungskoeffizienten, um eine mechanische Wechselbelastung infolge von Temperaturschwankungen zu vermeiden.
Für leichtgewichtige Verbindungen mit geringem Raumbedarf werden anstelle von Schrauben auch Niete verwendet.
Bei sehr großen Schrauben ist das Anziehen mittels Drehmomentschlüssel kaum mehr möglich, so dass bspw. thermische Verfahren zum Einsatz kommen. Dabei wird die Schraube im erwärmten Zustand in das Durchgangsloch der zu verbindenden Bauteile eingeführt und die Mutter bis zur Anlage aufgeschraubt. Beim Erkalten zieht sich die Schraube zusammen und erzeugt so die gewünschte Vorspannkraft.[8]
Moderne Verfahren ermöglichen die Fertigung von Gewindegängen, die einen deutlich größeren Durchmesser haben als der Schaft. Hierdurch reduziert sich das notwendige Eindrehmoment. Zur weiteren Reduktion erhalten die Schrauben mancher Hersteller am Übergang zwischen Gewinde und Schaft kurze Fräsrippen, welche die Reibung am Schaft durch Zerfaserung des Holzes herabsetzen. Zu beachten ist, dass als Durchmesser der Schrauben immer der äußerste Gewindedurchmesser angegeben wird. Der Schaft kann also deutlich schlanker ausfallen als der angegebene Schraubendurchmesser. Dies ist von Nachteil, wenn der Schaft auf Biegung oder Scherung beansprucht wird. Letzteres ist insbesondere der Fall, wenn Stahlbauteile auf Holzelementen befestigt werden, die durch Zugkräfte parallel zur Holzoberfläche belastet werden. Für diesen Belastungsfall werden Schrauben angeboten, deren Schaft unterhalb des Kopfes verstärkt ist.
Die Gefahr der Spaltung des Holzes ist insbesondere beim Eindrehen von Schrauben nahe dem Rand von Holzbauteilen gegeben. Reduziert wird die Neigung zum Spalten durch:
Moderne Holzschrauben können in Nadelhölzern in der Regel ohne Vorbohren eingedreht werden. Bei Schraubverbindungen nahe dem Rand des Werkstücks sollte generell vorgebohrt werden, ebenso bei der Verwendung von Harthölzern, um das Abscheren des Schraubenkopfes durch den erhöhten Widerstand beim Eindrehen zu vermeiden. Bei der Verwendung moderner Holzbauschrauben wird das Vorbohren mit folgenden Durchmessern empfohlen:[9]
Traditionell ergab sich beim Einwalzen des Gewindes bei Holzschrauben ein Gewindedurchmesser, der ungefähr dem Durchmesser des verbliebenen Schafts entsprach. Zur Reduzierung der zum Einschrauben erforderlichen Kraft kann es bei längeren Schrauben erforderlich sein, zusätzlich oder anstelle der Bohrung zur Aufnahme des Gewindes eine Bohrung in der Länge und mit dem Durchmesser des Schafts vorzunehmen.
Die Kennzeichnung der Sechskant- und Innensechskantschrauben ab M5 erfolgt auf dem Schraubenkopf, auf dem das Herstellerkurzzeichen und die Festigkeitsklasse angegeben sind, bei Schrauben aus nichtrostendem Stahl zusätzlich A2 oder A4.
Bei der vollständigen Bezeichnung werden alle relevanten Daten angegeben, ein Beispiel ist:
Aus der Festigkeitsklasse bei Stahlschrauben lassen sich die Zugfestigkeit Rm und die Streckgrenze Re errechnen.
Als Beispiel die Festigkeitsklasse 8.8:
Gemäß der Normung für mechanische und physikalische Eigenschaften (EN ISO 898-1) sind die Festigkeitsklassen 4.6, 4.8, 5.6, 5.8, 6.8, 8.8, 10.9 und 12.9 gebräuchlich.
Bei Schraubentypen mit – aufgrund der Geometrie – reduzierter Belastbarkeit (Schwachkopfschrauben) wird seit 2009 der Festigkeitsklasse ein „0“ vorangestellt, also z. B. „08.8“ oder „010.9“. Betroffen sind u. a. Zylinderschrauben mit Innensechskant und niedrigem Kopf, Flachkopfschrauben und Senkschrauben[11].
In der Industrie kommt sehr häufig die Klasse 8.8 zur Verwendung. Die Klassen 4.6, 4.8, 5.6 und 5.8 werden hauptsächlich bei Massenwaren und gering beanspruchten Verbindungen eingesetzt. Ausnahme sind Flanschverbindungen im Rohrleitungsbau, hier werden die Klassen 4.6 und 5.6 auf Grund ihrer erhöhten Bruchdehnung häufig vorgeschrieben (z. B. Druckgeräterichtlinie 97/23/EU oder AD 2000-Merkblatt W7:2008-05). 10.9 und 12.9 werden vor allem für berechnete und definiert vorgespannte Schraubverbindungen verwendet.
In Baumärkten hingegen wird vielfach die Festigkeitsklasse 4.6 angeboten.
Für Schrauben aus nichtrostendem Stahl wird die Qualität und Festigkeitsklasse auf dem Schraubenkopf angegeben. Diese sind z. B. A (für austenitischen Stahl), 1 bis 5 (Sorte) sowie 50 (weich), 70 (kaltverfestigt) oder 80 (hochfest), zum Beispiel A2-70 oder A5-80. Weitere mechanische Eigenschaften von nichtrostenden Schrauben sind in ISO 3506-1 beschrieben.
Vorwiegend werden die Qualitäten A2 allgemein und A4 für erhöhte Korrosionsbeanspruchungen verwendet. Diese Qualitäten werden umgangssprachlich auch heute noch mit den von Krupp geprägten Werksbezeichnungen „V2A“ und „V4A“ benannt. In besonderen Fällen kommen auch Schrauben aus den Werkstoffen mit den Werkstoffnummern 1.4439 oder 1.4462 zum Einsatz, beispielsweise im Offshore-Bereich. „Nichtrostende“ (eigentlich: korrosionsarme) Schrauben haben einen silbrig-matten Glanz und sind oft (sofern austenitisch) nicht ferromagnetisch.
Die Flanken des Schraubengewindes bilden mit dem Mutterngewinde eine senkrecht zu den Flanken wirkende formschlüssige Verbindung. In Richtung der Flanken sind beide Teile gegeneinander beweglich bzw. um ihre gemeinsame Mittenachse drehbar. Werden die Teile gegeneinander „angezogen“, so entsteht eine in Normalenrichtung zwischen den einander zugekehrten Flankenflächen wirkende Kraft (N, siehe rechts stehende Abbildung) und infolge der Reibung zwischen beiden ein Kraftschluss gegen das Drehen. Die Normalkraft N wird durch die Längskraft L und die Umfangskraft U erzeugt. Im Gleichgewichtszustand nach dem Anziehen wird die Rolle der notwendigen Umfangskraft U von der (Ruhe-)Reibung (Kraft R) zwischen den mit der Normalkraft N aufeinander gepressten Flanken übernommen. Die Komponente Ru von R ist bei üblicher Gewindesteigung (in nebenstehender Abbildung übertrieben steil dargestellt) immer größer als U, und der bestehende Kraftschluss gegen Losdrehen ist ausreichend. Es besteht Selbsthemmung.
Die Längskraft L berechnet sich zu:
mit
Das Anzugsdrehmoment , welches oft bei Schraubverbindungen vorgeschrieben ist, beträgt
Die Längskraft L bleibt infolge einer kleinen elastischen Verformung des Schraubenschaftes und der beteiligten Werkstücke als elastische Kraft (Vorspannkraft) in der Schraubenverbindung gespeichert. Der Schraubenschaft wird dabei gedehnt und die Werkstücke gestaucht. Die Schraubenverbindung wirkt wie eine gespannte harte Feder, die den Kraftschluss erzeugt. Dieser kann durch Kriechen (langsame plastische Verformung der Materialien, vor allem bei Kunststoff) verloren gehen. Durch Vibrationen kann es auch zum selbsttätigen Lösen kommen, weshalb in solchen Fällen meistens eine Schraubensicherung (formschlüssig gegen Drehen) nötig ist.
Eine ausreichend dauerhaft sichere Verbindung nach dem Kraftschluss-Prinzip ist nur die besonders weiche Dehnschrauben-Verbindung (mit Metallschraube), die zum Beispiel zwischen Zylinderkopf und Motorblock in Verbrennungsmotoren angewendet wird und ohne zusätzliche (formschlüssige) Schraubensicherung auskommt. Der Schaft einer solchen Schraube ist extra lang, um als weiche Feder zu wirken und Kriechen unwirksam zu machen.
Die in den Schraubenschaft über die Ringfläche unter dem Kopf eingeleitete Kraft und die über das Gewinde eingeleitete Gegenkraft erzeugen in ihm Zugspannungen. Wegen des Reibungswiderstandes gegen das Verdrehen im Gewinde werden im Schaft auch Torsionspannungen erzeugt. Scherbeanspruchung kann entstehen, wenn sich zwei zusammengeschraubte Teile quer bewegen, Biegebeanspruchung, wenn die Fläche unter dem Kopf (oder/und der Mutter) nicht rechtwinklig zum Schaft ist. Das Gewinde stellt eine Kerbung der Oberfläche dar, wodurch die Belastbarkeit des Schafts auf Zug, Scherung und Torsion gegenüber einem glatten Bolzen herabgesetzt ist. Im Maschinenbau gibt die VDI-Richtlinie 2230 aus dem Jahre 2003 grundlegende Hinweise zur Dimensionierung.
Die Kraftübertragung auf den Gewindeflanken ist nicht gleichmäßig über die ganze in Kontakt befindliche Gewindelänge verteilt:Schraube und Mutter werden in normalen Verschraubungen entgegengesetzt elastisch verformt. Die Mutter wird gestaucht, der Schraubenschaft gedehnt. Dadurch nehmen beide Teile bei niedrigen Lasten unterschiedliche Gewindesteigung an, wodurch die ersten Gänge den größten Teil der Kraft übertragen (der erste Gang etwa ein Drittel).[12][13] Mit zunehmender Belastung beginnen die hoch belasteten Gewindegänge zu plastifizieren und die niedrig belasteten Gewindegänge übernehmen einen höheren Anteil der auf die Schraube wirkenden Kraft. Bei niedrigen Einschraubtiefen erfolgt vor dem Gewindeversagen eine gleichmäßige Lastverteilung über die Gewindegänge. Bei hohen Einschraubtiefen versagt die Schraube vor dem Gewinde, noch bevor sich eine gleichmäßige Lastverteilung über die Gewindegänge eingestellt hat.[14] Man verwendet bei besonders belasteten Schraubverbindungen besonders kerbzähes Material und erreicht durch leicht geringere Steigung der Schraube im Einschraubbereich dort eine gleichmäßigere Lastverteilung im elastischen Bereich.[15]
Um unerwünschten Zugriff zu erschweren, werden Schrauben mit außergewöhnlichen Antrieben am Kopf verwendet:
Da unabhängige Hersteller entsprechende Schraubendreher nach einiger Zeit auf den Markt bringen, besteht immer wieder Anlass für die Einführung einer neuen außergewöhnlichen Form.
Schraubenköpfe mit Schlitzen, deren linke Fläche schräg ist, lassen sich nur einschrauben, jedoch nicht lösen (Beispiel: Einweg-Schlitz).
Gewindestangen, Gewindestifte, Gewindebolzen und Stockschrauben besitzen häufig keine besondere Profilierung, die zum Ansetzen eines Werkzeugs dient, und werden dann mit der Hand, mit einem Schlüssel mit entsprechendem Innengewinde oder mit einer Zange mit gezahnten Backen eingedreht. Beim Ansetzen einer Zange verformen sich die Gewindegänge, wodurch die spätere Verwendung erschwert werden kann.
Viele Schrauben ohne Kopf, insbesondere Stift- bzw. Madenschrauben, besitzen ein einem Ende ein Senkloch zur Aufnahme eines Innensechskant- oder Innensechsrund-Antriebs.
Gewindestifte, -bolzen sowie Stockschrauben können zusätzlich oder alternativ seitliche Abflachungen zum Ansetzen eines Maulschlüssels oder einer Mutternzange aufweisen.
Stift- bzw. Madenschrauben werden auch mit einem Schlitz zur Aufnahme eines gewöhnlichen Schraubendrehers gefertigt. Soweit die neben dem Schlitz verbleibenden Segmente des Gewindes noch nicht vollständig in ein Aussengewinde eingedreht wurden, können sie nur eine geringe Kraft übertragen, ohne zu brechen.
Gewindestifte bzw. Gewindebolzen können einen mittigen Absatz besitzen, der als Anschlag oder zur Betätigung dient. 
Gewindestangen, die je zur Hälfte ein Links- und ein Rechtsgewinde besitzen, können in Spanschlössern oder etwa zwischen den beiden Schenkeln eines Zirkels zur Feinjustage eingesetzt werden. 
Madenschrauben bzw. Stiftschrauben werden häufig zum Fixieren von Türgriffen auf dem als Achse dienenden Vierkant eingesetzt. 
Kürzere Gewindestangen werden meist als Gewindestift oder Gewindebolzen bezeichnet, wobei Gewindestifte tendenziell einen geringeren Durchmesser besitzen als Gewindebolzen.
Gewindebolzen dienen zur Verbindung des Zylinderkopfes mit dem Motorblock bei Kolbenmotoren. 
Stockschrauben sind Gewindestangen, die sowohl ein metrisches (Maschinen-)Gewinde als auch ein grobes (Holz-)Gewinde besitzen.
Sie werden häufig zur Befestigung von Objekten und haustechnischen Installationen im Baubereich eingesetzt, indem das Grobgewinde entweder in Holz oder in einen Metall- oder Kunststoffdübel geschraubt wird. Der Vorteil gegenüber einer gewöhnlich Schraube besteht darin, dass die Stockschraube vorinstalliert werden kann, um anschließend das metrische Gewinde nach Bedarf zu verlängern oder zu verkürzen. Auch können schwere Objekte bei der Installation bereits auf der Stockschraube abgestützt werden, während die Befestigungsmutter auf das metrische Gewinde der Stockschraube gedreht wird.
sind Schrauben zum Befestigen von Einsätzen in Elektro-Installationsdosen („Gerätedosen“), z. B. Unterputzdosen. Sie ähneln gängigen Holzschrauben, haben aber oft ein etwas feineres Gewinde (etwa 7 Gewindegänge pro cm statt 5,5).  Gängige Geräteschrauben haben einen Durchmesser von 3,2 mm und sind in Längen bis 40 mm erhältlich. Sie sind häufig aus relativ weichem Metall gefertigt, so dass sie einfach mit einem Seitenschneider gekürzt werden können.

Gewindeprägende Schrauben nach DIN 7500 bilden wie Gewindeformer selbsttätig ein Gewinde in einer Bohrung oder einem Sackloch aus, ohne einen Span zu produzieren. Sie sind meist zur Verwendung in plastisch verformbaren Materialien bis zur Brinellhärte 135 HB bzw. Zugfestigkeit von Rm = 450 N/mm² geeignet. Zum Einschrauben ist ein erhöhtes und linear ansteigendes Drehmoment notwendig. Schrauben zur Verwendung in harten Werkstoffen werden meist an der Oberfläche vergütet und sind dadurch teurer als metrische Normschrauben. Insgesamt können Kosten eingespart werden, da das gesonderte Schneiden des Gewindes entfällt und auch eine Sicherung gegen Vibration nicht notwendig ist. In vielen Materialien ist ein geformtes Gewinde haltbarer als ein geschnittenes, da es sich der Schraube anpasst und gegebenenfalls eine Kaltverfestigung eintritt. Um das Ansetzen der Schraube zu erleichtern, haben selbstformende Schrauben oft eine konische Spitze, manche besitzen ein leicht dreikantiges (trilobulares) Profil, zur Reduzierung der Reibung.[16]

besitzen Nuten ähnlich einem Gewindebohrer und können in weichen Materialien wie Aluminium eingesetzt werden.
und Schrauben für Kunststoff formen ebenfalls ein Gewinde. Schrauben für weiche Kunststoffe weisen eine ähnlich grobe Gewindesteigung auf wie Holzschrauben. Gewinde für Blech sowie für harte Kunststoffe haben in der Regel eine Steigung, die zwischen derjenigen von Holz- und der von Metallschrauben liegt. Im Gegensatz zu Holzschrauben besitzen sie jedoch oft keine ausgeprägte Spitze und werden selten mit Senkkopf gefertigt.

Bohrschrauben sind an ihrer Spitze mit zwei Bohrschneiden ausgestattet, die geeignet sind, in Werkstücke ein Loch zu schneiden, in welchem sich das nachfolgende Gewinde selbsttätig verankert. Ebenso wie Blechschrauben besitzen Bohrschrauben ein grobes Gewinde, denn sie eignen sich überwiegend zur Verwendung in dünnen oder weichen Materialien. In dickerem Material ist eine ausreichende Spanabfuhr beim Bohren nicht möglich. In zähen Materialien können Gewinde nur mit spezialisierten Werkzeugen geschnitten werden. Bohrschrauben beschleunigen die Arbeit, da das vorherige Bohren eines Lochs entfällt. Beim Verbinden von mehreren Werkstücken aus zähem Material ist darauf zu achten, dass die Länge des Bohrkopfes mindestens der Gesamtdicke der zu verbindenden Materialien entspricht. Sonst greifen die Gewindegänge bereits, bevor die Bohrung fertiggestellt ist, wodurch die Werkstücke auseinandergetrieben würden. Bei erhöhtem Korrosionsrisiko sind Schrauben aus rostfreiem Stahl zu verwenden. Rostfreier Stahl lässt sich weniger gut härten. Daher eignen sich diese Bohrschrauben nur zur Verwendung mit Aluminiumwerkstoffen[17] (wobei jedoch zu beachten ist, dass dadurch galvanische Korrosion begünstigt wird).

Metallschrauben haben meist ein metrisches ISO-Gewinde; im englischsprachigen Raum ist auch das Whitworth-Gewinde noch verbreitet.
Für bestimmte Schraubenformen und -Kopfformen gibt es detaillierte Normen:
Es gibt noch zahlreiche Spezialschrauben für spezielle Einsatzzwecke, bei denen keine Normschrauben verwendet werden können. Sie unterscheiden sich meist durch ihre Kopf- bzw. Gewindeform (z. B. Blattschraube oder Bohrschraube mit Blechschraubengewinde DIN 7504).
Je nach der Kopfform braucht man den entsprechenden Schraubenschlüssel oder Schraubendreher zum Drehen der Schraube. Bei vielen Verbindungen soll oder muss ein Drehmomentschlüssel verwendet werden.
Damit sich eine Mutter beim Anziehen nicht ins Material gräbt, sollte sie immer mit Unterlegscheibe verwendet werden. Bei normalfesten Schrauben der Festigkeitsklasse 4.6 (Rohschrauben nach DIN 7990) ist eine Unterlegscheibe am Schraubenkopf nicht nötig. Bei hochfesten Schrauben nach DIN 6914 ist eine Unterlegscheibe auch am Schraubenkopf erforderlich, wenn diese entweder als Passschraube eingesetzt oder planmäßig vorgespannt werden.[18]

Ein Transistor ist ein elektronisches Halbleiter-Bauelement zum Steuern oder Verstärken meistens niedriger elektrischer Spannungen und Ströme. Er ist der weitaus wichtigste „aktive“ Bestandteil elektronischer Schaltungen, der beispielsweise in der Nachrichtentechnik, der Leistungselektronik und in Computersystemen eingesetzt wird. Besondere Bedeutung haben Transistoren – zumeist als Ein/Aus-Schalter – in integrierten Schaltkreisen, was die weit verbreitete Mikroelektronik ermöglicht.
Die Bezeichnung „Transistor“ ist ein Kofferwort des englischen transfer resistor,[1][2] was in der Funktion einem durch eine angelegte elektrische Spannung oder einen elektrischen Strom steuerbaren elektrischen Widerstand entspricht. Die Wirkungsweise ähnelt der einer entsprechenden Elektronenröhre, nämlich der Triode.
Die ersten Patente auf das Prinzip des Transistors meldete Julius Edgar Lilienfeld im Jahr 1925 an.[3] Lilienfeld beschreibt in seiner Arbeit ein elektronisches Bauelement, das Eigenschaften einer Elektronenröhre aufweist und im weitesten Sinne mit dem heute als Feldeffekttransistor (FET) bezeichneten Bauelement vergleichbar ist. Zu dieser Zeit war es technisch noch nicht möglich, Feldeffekttransistoren praktisch zu realisieren.[4]
Im Jahr 1934 ließ der Physiker Oskar Heil den Aufbau eines Feldeffekttransistors patentieren, bei dem es sich um einen Halbleiter-FET mit isoliertem Gate handelt.[5] Die ersten praktisch realisierten Sperrschicht-Feldeffekttransistoren JFETs mit einem p-n-Übergang (positiv-negativ) und einem Gate als Steuerelektrode gehen auf Herbert F. Mataré, Heinrich Welker sowie parallel dazu William Shockley und Walter H. Brattain aus dem Jahr 1945 zurück.[6] Der Feldeffekttransistor wurde somit historisch vor dem Bipolartransistor realisiert, konnte sich damals aber noch nicht praktisch durchsetzen. Damals wurden diese Bauelemente noch nicht als Transistor bezeichnet; den Begriff „Transistor“ prägte John R. Pierce im Jahr 1948.[2]
Ab 1942 experimentierte Herbert Mataré bei Telefunken mit dem von ihm als Duodiode (Doppelspitzendiode) bezeichneten Bauelement im Rahmen der Entwicklung eines Detektors für Doppler-Funkmess-Systeme. Die von Mataré dazu aufgebauten Duodioden waren Punktkontakt-Dioden auf Halbleiterbasis mit zwei sehr nahe beieinanderstehenden Metallkontakten auf dem Halbleitersubstrat. Mataré experimentierte dabei mit polykristallinem Silizium (kurz: Polysilizium), das er von Karl Seiler aus dem Telefunken-Labor in Breslau bezog, und mit Germanium, das er von einem Forschungsteam der Luftwaffe bei München (in dem auch Heinrich Welker mitwirkte) erhielt. Bei den Experimenten mit Germanium entdeckte er Effekte, die sich nicht als zwei unabhängig arbeitende Dioden erklären ließen: Die Spannung an der einen Diode konnte den Strom durch die andere Diode beeinflussen. Diese Beobachtung bildete die Grundidee für die späteren Spitzentransistoren, eine frühe Bauform des Bipolartransistors.
In den Bell Laboratories in den Vereinigten Staaten entwickelte die Gruppe um John Bardeen, William Shockley und Walter Brattain den ersten funktionierenden Bipolartransistor in Form eines Spitzentransistors, der am 23. Dezember 1947 erstmals firmenintern präsentiert werden konnte.[7][8][9] Für die Erfindung des Bipolartransistors erhielten John Bardeen, William Shockley und Walter Brattain 1956 den Nobelpreis für Physik. Da Shockley mit seinem Team einen Bipolartransistor realisiert hatte, der nicht auf dem Funktionsprinzip eines Feldeffekttransistors basiert, finden sich in dem US-Patent auch keine Referenzen auf die theoretischen Vorarbeiten von Lilienfeld und Heil aus den 1920er Jahren.[10][11]
Unabhängig von den Arbeiten in den USA entwickelten die beiden Wissenschaftler Herbert Mataré und Heinrich Welker in Frankreich ebenfalls einen funktionsfähigen Bipolartransistor. Sie waren einige Monate später erfolgreich und meldeten dafür am 13. August 1948 in Paris ein Patent an.[12][13][14] Am 18. Mai 1949 wurde diese Entwicklung unter dem Kunstwort „Transistron“ der Öffentlichkeit vorgestellt, der neue Begriff „Transistron“ fand aber in Folge keine wesentliche Verbreitung.[15]
In den Folgejahren folgten weitere technologische Verbesserungen. So gelang der Gruppe um Gordon Teal, Morgan Sparks und William Shockley bei den Bell Labs im Jahr 1951 die Herstellung eines Flächentransistors, der aus nur einem Kristall besteht. Bis dahin waren Bipolartransistoren als Spitzentransistoren aufgebaut.[16]
In den 1950er-Jahren gab es einen Wettlauf zwischen der Elektronenröhre und den damals üblichen Bipolartransistoren, in dem die Chancen des Bipolartransistors wegen der vergleichsweise niedrigen Transitfrequenzen häufig eher skeptisch beurteilt wurden. Die geringe Größe, der geringe Energiebedarf und später die zunehmenden Transitfrequenzen der Transistoren führten jedoch dazu, dass in den 1960er Jahren die Elektronenröhren als Signalverstärker auf fast allen technischen Gebieten abgelöst wurden.
Feldeffekttransistoren spielten im praktischen Einsatz, im Gegensatz zu den ersten Bipolartransistoren, in den 1950er bis in die späten 1960er Jahre noch kaum eine Rolle, obwohl deren theoretische Grundlagen länger bekannt waren. Feldeffekttransistoren ließen sich mit den damaligen Kenntnissen nicht wirtschaftlich fertigen und waren wegen der Durchschlagsgefahr des Gates durch unbeabsichtigte elektrostatische Entladung umständlich zu handhaben. Zur Lösung der bei bipolaren Transistoren auftretenden Probleme wie Leistungsbedarf und Anforderungen für integrierte Schaltungen beschäftigten sich Entwickler ab etwa 1955 eingehender mit den Halbleiteroberflächen und fanden Fertigungsverfahren wie die Planartechnik, die die Feldeffekttransistoren im Folgejahrzehnt zur Serienreife brachten.
Die ersten handelsüblichen Bipolartransistoren wurden aus dem Halbleitermaterial Germanium hergestellt und ähnlich wie Elektronenröhren in winzige Glasröhrchen eingeschmolzen. Die verschiedenen dotierten Zonen entstanden mit einem zentralen Germaniumplättchen, in das von beiden Seiten „Indiumpillen“ anlegiert waren.[17][18] Letztere drangen damit tief in das Grundmaterial ein, in der Mitte blieb aber eine Basisstrecke gewünschter Dicke frei. Im Jahr 1954 kamen Bipolartransistoren aus Silizium auf den Markt (Gordon Teal bei Texas Instruments und Morris Tanenbaum an den Bell Labs). Dieses Grundmaterial war einfacher verfügbar und preisgünstiger. Seit den späten 1960er Jahren kamen großteils Metall- oder Kunststoffgehäuse zur Anwendung. Einsatzbereiche lagen zunächst in der analogen Schaltungstechnik wie den damals aufkommenden Transistorradios. Das Basismaterial Germanium wurde in Folge verstärkt durch das technisch vorteilhaftere Silizium ersetzt, das einen größeren Arbeitstemperaturbereich bei wesentlich geringeren Restströmen abdeckte und durch die Siliziumdioxid-Passivierung langzeitstabiler in den elektrischen Kennwerten gegenüber Germanium ist.
Der erste auf Galliumarsenid basierende Feldeffekttransistor, der sogenannte MESFET, wurde 1966 von Carver Mead entwickelt.[19] Dünnschichttransistoren (engl. thin film transistor, abgekürzt TFT) wurden bereits 1962 von Paul K. Weimer entwickelt, konnten aber erst rund 30 Jahre später im Bereich heute üblicher farbiger TFT-Displays einen Anwendungsbereich finden.[20]
Werden alle Transistoren in sämtlichen bislang hergestellten Schaltkreisen wie Arbeitsspeicher, Prozessoren usw. zusammengezählt, ist der Transistor inzwischen diejenige technische Funktionseinheit, die von der Menschheit in den höchsten Gesamtstückzahlen produziert wurde und wird. Moderne integrierte Schaltungen, wie die in Personal Computern eingesetzten Mikroprozessoren, bestehen aus vielen Millionen bis Milliarden Transistoren, so besitzt die 2022 veröffentlichte Grafikkarte RTX 4090 76,3 Milliarden Transistoren.[21]
Es gibt zwei wichtige Gruppen von Transistoren, nämlich Bipolartransistoren und Feldeffekttransistoren (FET), die sich durch die Art der Ansteuerung voneinander unterscheiden.
Eine Liste mit einer groben Einordnung bzw. Gruppierung der Transistoren sowie weiteren Transistorenvarianten findet sich unter Liste elektrischer Bauelemente.
Bei bipolaren Transistoren tragen sowohl bewegliche negative Ladungsträger, die Elektronen, als auch positive Ladungsträger, sogenannte Defektelektronen, zur Funktion bzw. zum Ladungstransport bei. Defektelektronen, auch als Löcher bezeichnet, sind unbesetzte Zustände im Valenzband, die sich durch Generation und Rekombination von Elektronen im Kristall bewegen. Zu den bipolaren Transistoren gehören unter anderem der IGBT und der HJBT. Der wichtigste Vertreter ist jedoch der Bipolartransistor (engl.: bipolar junction transistor, BJT).
Der Bipolartransistor wird durch einen elektrischen Strom angesteuert. Die Anschlüsse werden mit Basis, Emitter, Kollektor bezeichnet (im Schaltbild abgekürzt durch die Buchstaben B, E, C). Ein kleiner Steuerstrom auf der Basis-Emitter-Strecke führt zu Veränderungen der Raumladungszonen im Innern des Bipolartransistors und kann dadurch einen großen Strom auf der Kollektor-Emitter-Strecke steuern. Je nach Dotierungsfolge im Aufbau unterscheidet man zwischen npn- (negativ-positiv-negativ) und pnp-Transistoren (positiv-negativ-positiv). Dotierung bedeutet in diesem Zusammenhang das Einbringen von Fremdatomen bei dem Herstellungsprozess in eine Schicht des hochreinen Halbleitermaterials, um die Kristallstruktur zu verändern.
Bipolartransistoren sind grundsätzlich immer selbstsperrend: Ohne Ansteuerung mittels eines kleinen Stromes durch die Basis-Emitter-Strecke sperrt der Transistor auf der Kollektor-Emitter-Strecke.
Im Schaltsymbol ist der Anschluss Emitter (E) in beiden Fällen mit einem kleinen Pfeil versehen: Bei einem npn-Transistor zeigt dieser vom Bauelement weg, beim pnp-Transistor weist er zu dem Bauelement hin.[22] Der Pfeil beschreibt die technische Stromrichtung (Bewegung gedachter positiver Ladungsträger) am Emitter. In frühen Jahren wurde in Schaltplänen bei den damals oft eingesetzten diskreten Transistoren zur Kennzeichnung des Transistorgehäuses ein Kreis um das jeweilige Symbol gezeichnet. Die Kreissymbole sind durch den heutigen vorherrschenden Einsatz integrierter Schaltungen unüblich geworden.
Die Verknüpfung zweier Bipolartransistoren mit Vor- und Hauptverstärkung zu einer Einheit wird als Darlington-Transistor oder als Darlington-Schaltung bezeichnet. Durch diese Verschaltung kann eine deutlich höhere Stromverstärkung erreicht werden als mit einem einzelnen Transistor. Weitere Details zu den Besonderheiten und Ansteuerungen finden sich in dem eigenen Artikel über Bipolartransistoren und in der mathematischen Beschreibung des Bipolartransistors. Einfache Schaltungsbeispiele finden sich in dem Artikel über Transistorgrundschaltungen und bei den Ersatzschaltungen des Bipolartransistors.
Feldeffekttransistoren, abgekürzt FET, oder auch als unipolare Transistoren bezeichnet, werden durch eine Spannung gesteuert. Besonders für FETs ist ein sehr hoher Eingangswiderstand im statischen Betrieb und die daher fast leistungslose Ansteuerung typisch.
Die drei Anschlüsse werden als Gate (dt. Tor, Gatter), das ist der Steueranschluss, Drain (dt. Senke, Abfluss) und Source (dt. Quelle, Zufluss) bezeichnet. Bei MOSFETs (Metalloxidschicht) kommt noch ein weiterer Anschluss, das Bulk oder Body (dt. Substrat), hinzu, das meist mit dem Source-Anschluss verbunden wird. Der Widerstand und somit der Strom der Drain-Source-Strecke wird durch die Spannung zwischen Gate und Source und das dadurch entstehende elektrische Feld gesteuert. Die Steuerung ist im statischen Fall fast stromlos. Der gesteuerte Strom im Drain-Source-Kanal kann, im Gegensatz zum Kollektorstrom von Bipolartransistoren, in beiden Richtungen fließen.
Die Klasse der Feldeffekttransistoren unterteilt sich in Sperrschicht-FETs (JFETs) und in die FETs, die mit einem durch einen Isolator getrennten Gate (MISFET, MOSFET) versehen sind. Unterschieden wird bei Feldeffekttransistoren darüber hinaus je nach Dotierung des Halbleiters zwischen n- und p-FETs, die sich bei den MOSFETs weiter in selbstleitende und selbstsperrende Typen aufteilen.
Bei den Unipolartransistoren ist immer nur eine Ladungsträgerart, negativ geladene Elektronen oder positiv geladene Defektelektronen, am Ladungsträgertransport durch den Transistor beteiligt.
Bei Sperrschicht-FETs (engl. junction FET, JFET) wird die elektrisch isolierende Schicht zum Gate durch eine in Sperrrichtung betriebene Diode und deren unterschiedlich große Raumladungszone gebildet. Sperrschicht-FETs sind in der Grundform immer selbstleitende Transistoren: Ohne Spannung am Gate sind sie zwischen Source und Drain leitend. Durch das Anlegen einer Gate-Spannung geeigneter Polarität wird die Leitfähigkeit zwischen Source und Drain reduziert. 
Es gibt allerdings auch spezielle Varianten, die ohne Gate-Spannung keinen Source-Drain-Strom aufweisen (selbstsperrende JFET, engl. normally-off JFET).[23]
Auch JFETs gibt es in zwei Arten: n-Kanal und p-Kanal. Im Schaltsymbol wird bei einem n-Kanal der Pfeil zu dem Transistor gezeichnet und auf dem Gate-Anschluss eingezeichnet, wie in nebenstehender Abbildung dargestellt. Beim p-Kanal-Typ ist die Pfeilrichtung umgekehrt. Sperrschicht-FETs finden wegen der etwas komplizierteren Ansteuerung nur in speziellen Anwendungen, wie beispielsweise Mikrofonverstärkern, Anwendung.
Der Überbegriff MISFET leitet sich von der englischen Bezeichnung metal insulator semiconductor field-effect transistor (Metall-Isolator-Halbleiter-Feldeffekttransistor) ab. Sie stellen die andere große Gruppe, die Feldeffekttransistoren mit einem durch einen Isolator getrennten Gate (engl.: isolated gate field-effect transistor, IGFET), dar.
Aus historischen Gründen wird statt MISFET oder IGFET meist die Bezeichnung MOSFET synonym verwendet. MOSFET steht für englisch Metal Oxide Semiconductor Field-Effect Transistor (Metall-Oxid-Halbleiter-Feldeffekttransistor) und geht auf die Ursprünge der Halbleitertechnik zurück; damals wurde als Gate-Material Aluminium und als Isolator Siliziumdioxid verwendet.
Wie der Name schon andeutet, wird ein MOSFET vor allem durch den Aufbau des Gate-Schichtstapels definiert. Dabei ist ein „metallisches“ Gate durch ein Oxid (Isolator) vom stromführenden Kanal (Halbleiter) zwischen Source und Drain elektrisch isoliert. Mit Technologiestand im Jahr 2008 wurde vornehmlich hochdotiertes Polysilizium als Gate-Material eingesetzt, womit die Bezeichnung MISFET bzw. MOSFET nicht korrekt ist. In Verbindung mit dem Substratmaterial Silizium bietet sich Siliziumdioxid als Isolationsmaterial an, da es sich technologisch einfach in den Herstellungsprozess integrieren lässt und gute elektrische Eigenschaften aufweist. Eine Ausnahme stellt die High-k+Metal-Gate-Technik dar, bei der ein metallisches Gate in Verbindung mit High-k-Materialien aus Metalloxiden eingesetzt wird.
Ein Vorteil der MOSFET-Technik ist, dass durch den Einsatz eines Isolators im Betrieb keine Raumladungszone als Trennschicht, wie beim Sperrschicht-FET mit entsprechender Ansteuerungspolarität, gebildet werden muss. Der Gate-Anschluss kann somit in bestimmten Bereichen mit sowohl positiven als auch negativen Spannungen gegen den Source-Anschluss beaufschlagt werden.
Je nach Dotierung des Grundmaterials lassen sich sowohl n- als auch p-Kanal-MOSFETs herstellen. Diese können auch in Form selbstleitender oder selbstsperrender Typen im Rahmen der Herstellungsprozesse konfiguriert werden. Die Schaltsymbole umfassen damit vier mögliche Variationen wie in nebenstehender Abbildung dargestellt. Dabei ist erkennbar, dass die selbstleitenden MOSFETs, auch als Verarmungstyp bezeichnet, eine durchgezogene Linie zwischen den Anschlüssen Drain und Source aufweisen. Diese Linie ist bei den selbstsperrenden Typen, auch als Anreicherungstyp bezeichnet, unterbrochen. Der Pfeil wird bei diesen Transistoren am Bulk-Anschluss eingezeichnet und bei einem n-Kanal-Typ zu dem Transistorsymbol orientiert, bei einem p-Kanal vom Transistor weg gezeichnet. Der Bulk-Anschluss ist oft fest mit dem Source-Anschluss direkt am Halbleiter verbunden.
Wegen der größeren Vielfalt und der leichteren elektrischen Steuerbarkeit sind MOSFETs die heute mit großem Abstand am meisten produzierten Transistoren. Möglich wurde dies vor allem durch die CMOS-Technologie, bei der n- und p-MOSFETs kombiniert werden. Erst der Einsatz dieser Technologie erlaubte die Realisierung hochkomplexer, integrierter Schaltungen mit einer deutlich reduzierten Leistungsaufnahme, die mit anderen Transistortypen nicht möglich wäre.
Neben den Transistorgrundtypen gibt es einige weitere Varianten für spezielle Anwendungsbereiche wie den Bipolartransistor mit isolierter Gateelektrode, abgekürzt IGBT. Diese Transistoren finden seit Ende der 1990er Jahre vor allem in der Leistungselektronik Anwendung und stellen eine Kombination aus MOS- und Bipolartechnologie in einem gemeinsamen Gehäuse dar. Da diese Leistungstransistoren Sperrspannungen bis zu 6 kV aufweisen und Ströme bis zu 3 kA schalten können, ersetzen sie in der Leistungselektronik zunehmend Thyristoren.
Fototransistoren sind optisch empfindliche bipolare Transistoren, wie sie unter anderem in Optokopplern Verwendung finden. Die Steuerung dieser Transistoren erfolgt nicht durch einen kleinen Basis-Emitter-Strom – mitunter wird der Basisanschluss auch weggelassen –, sondern ausschließlich durch den Einfall von Licht (beispielsweise angewendet in Lichtschranken). Licht hat in der Raumladungszone des p-n-Überganges des Bipolartransistors eine ähnliche Wirkung wie der Basisstrom, der normalerweise an der Basis(B), auf engl. Gate(G), geschaltet wird. Deswegen sollten herkömmliche Transistoren, bei denen dieser Effekt unerwünscht ist, in einem lichtundurchlässigen Gehäuse untergebracht sein.
Ein heute kaum noch verwendeter Transistor ist der Unijunctiontransistor, abgekürzt UJT. Er ähnelt in seiner Funktion eher Thyristoren bzw. den Diacs, wird historisch aber zu den Transistoren gezählt. Seine Funktion, beispielsweise in Sägezahngeneratoren, wird heute großteils durch integrierte Schaltungen realisiert.
In manchen Flüssigkristallbildschirmen, den meist farbfähigen TFT-Displays, kommen pro Pixel im aktiven Bildbereich bis zu drei Dünnschichttransistoren (engl. Thin Film Transistor, TFT) zu Anwendung. Diese Feldeffekttransistoren sind praktisch durchsichtig. Sie werden zur Ansteuerung der einzelnen Pixel verwendet und ermöglichen im Vergleich zu den transistorlosen, farbfähigen LC-Displays einen höheren Kontrast. Je nach Größe des TFT-Display können pro Bildschirm bis zu einigen Millionen Dünnfilmtransistoren eingesetzt werden.
In elektrisch programmierbaren Festwertspeichern wie EPROMs und EEPROMs finden spezielle MOSFET mit einem sogenannten Floating Gate als primäres Speicherelement Anwendung. Durch die im Floating Gate gespeicherte elektrische Ladung ist der Transistor permanent ein- bzw. ausgeschaltet und kann den Informationsgehalt eines Bits speichern. Das Beschreiben, und bei einigen Typen auch das Löschen, wird mittels des quantenmechanischen Tunneleffektes ermöglicht.
In integrierten Schaltungen werden weitere spezielle Formen wie der Multiemitter-Transistor eingesetzt, der bei Logikgattern in der Transistor-Transistor-Logik die eigentliche logische Verknüpfung der Eingangssignale durchführt.
Im Laufe der Geschichte der Mikroelektronik wurde – im Hinblick auf den funktionalen inneren Aufbau – eine Vielzahl von Transistorbauformen entwickelt, die sich vor allem in der Herstellung der pn-Übergänge und der Anordnung der dotierten Bereiche unterscheiden. Der erste praktisch realisierte Transistor war 1947 der Spitzentransistor. Darauf folgten zahlreiche Versuche, die Herstellung einfacher und somit auch günstiger zu machen. Wichtige Bauformen bipolarer Einzel-Transistoren sind: der gezogene Transistor, der Legierungstransistor, der Drifttransistor, der Diffusionstransistor, der diffundiert-legierte Mesatransistor, der Epitaxialtransistor und der Overlay-Transistor. Die wohl wichtigste Bauform ist jedoch der 1960 von Jean Hoerni entwickelte Planartransistor, der sowohl einen wirksamen Schutz des sensiblen pn-Übergangs als auch eine parallele Massenfertigung auf einem Substrat (Wafer) erlaubte – was die Entwicklung von integrierten Schaltkreisen (ICs) wesentlich beeinflusste.
Für u. a. Differenzverstärker ist es wichtig, dass deren beide Eingangstransistoren möglichst isotherm betrieben werden. Unter anderem dafür werden Doppeltransistoren hergestellt, zwei Transistoren in einem Gehäuse. Auf dem nebenstehenden Bild deutlich erkennbar sind die einzelnen Transistoren auf einem kleinen Messingplättchen, die wiederum auf einem keramischen und elektrisch isolierenden Bock liegen. Moderne Typen in SO-Gehäusen basieren teilweise auf zwei Transistoren auf einem Die, auch gibt es integrierte Transistorarrays (z. B. CA 3086) oder vollkommen integrierte Differenzverstärker in Form von Operationsverstärkern und Komparatoren.
Die erst später praktisch realisierten Feldeffekttransistoren können in ähnlich vielen Bauformen realisiert werden. Die wichtigsten Formen sind der planare Metall-Oxid-Halbleiter-Feldeffekttransistor, der Nanodrahttransistor sowie der FinFET.
Ging es in der Anfangsphase der Mikroelektronik noch darum, überhaupt funktionsfähige Transistoren mit guten elektrischen Eigenschaften herzustellen, so wurden später zunehmend Bauformen für spezielle Anwendungen und Anforderungen entwickelt, beispielsweise Hochfrequenz-, Leistungs- und Hochspannungstransistoren. Diese Unterteilung gilt sowohl für Bipolar- als auch für Feldeffekttransistoren. Für einige Anwendungen wurden auch spezielle Transistortypen entwickelt, die typische Eigenschaften der beiden Haupttypen vereinen, z. B. der Bipolartransistor mit isolierter Gate-Elektrode (IGBT).
Bipolare Transistoren wurden in der Anfangszeit aus dem Halbleiter Germanium gefertigt, während heute überwiegend der Halbleiter Silizium sowohl bei Feldeffekttransistoren als auch Bipolartransistoren verwendet wird. Der schrittweise Ersatz des Germaniums durch Silizium im Laufe der 1960er und 1970er Jahre geschah unter anderem aus folgenden Gründen (vgl. Thermische Oxidation von Silizium):[24]
Für Spezialanwendungen werden weitere Materialien eingesetzt. So besitzen einige Verbindungshalbleiter wie das giftige Galliumarsenid bessere Eigenschaften für hochfrequente Anwendungen, sind aber teurer zu fertigen und benötigen andere Fertigungseinrichtungen. Um diese praktischen Nachteile des Galliumarsenids zu umgehen, existieren verschiedene Halbleiterkombinationen wie Siliziumgermanium, die für höhere Frequenzen verwendbar sind. Für Hochtemperaturanwendungen kommen für die Herstellung von Transistoren spezielle Halbleitermaterialien wie Siliziumcarbid (SiC) zur Anwendung. Diese Transistoren können beispielsweise direkt an einem Verbrennungsmotor bei Temperaturen bis zu 600 °C eingesetzt werden.[25][26] Bei siliziumbasierenden Halbleitern liegt die maximale Betriebstemperatur im Bereich von 150 °C.
Transistoren werden heutzutage in nahezu allen elektronischen Schaltungen verwendet. Der Einsatz als einzelnes (diskretes) Bauelement spielt dabei eine nebensächliche Rolle. Sogar in der Leistungselektronik werden zunehmend mehrere Transistoren auf einem Substrat gefertigt; dies geschieht hauptsächlich aus Kostengründen.
Eine ältere Typisierung von Transistoren erfolgte nach den Einsatzgebieten:
Differenziert wird inzwischen noch mehr nach dem Anwendungsgebiet. Die Maßstäbe haben sich ebenfalls verschoben, die Grenze von 100 kHz für HF-Transistoren würde heute ca. um den Faktor 1000 höher angesetzt werden.
Ausgehend von der Zahl der gefertigten Bauelemente ist das Hauptanwendungsgebiet der Transistoren in der Digitaltechnik der Einsatz in integrierten Schaltungen, wie beispielsweise RAM-Speichern, Flash-Speichern, Mikrocontrollern, Mikroprozessoren und Logikgattern. Dabei befinden sich in hochintegrierten Schaltungen über eine Milliarde Transistoren auf einem Substrat, das meistens aus Silizium besteht und eine Fläche von einigen Quadratmillimetern aufweist. Die im Jahr 2009 noch exponentiell wachsende Steigerungsrate bei der Bauelementeanzahl pro integriertem Schaltkreis wird auch als Mooresches Gesetz bezeichnet. Jeder dieser Transistoren wird dabei als eine Art elektronischer Schalter eingesetzt, um einen Teilstrom in der Schaltung ein- oder auszuschalten. Mit dieser immer höheren Transistoranzahl je Chip wird dessen Speicherkapazität größer oder seine Funktionsvielfalt, indem bei modernen Mikroprozessoren beispielsweise immer mehr Aktivitäten in mehreren Prozessorkernen parallel abgearbeitet werden können. Alles dies steigert in erster Linie die Arbeitsgeschwindigkeit; weil die einzelnen Transistoren innerhalb der Chips dabei aber auch immer kleiner werden, sinkt auch deren jeweiliger Energieverbrauch, so dass die Chips insgesamt auch immer energiesparender (bezogen auf die Arbeitsleistung) werden.
Die Größe der Transistoren (Gate-Länge) bei hochintegrierten Chips beträgt im Jahr 2009 oft nur noch wenige Nanometer. So beträgt beispielsweise die Gate-Länge der Prozessoren, die in der sogenannten 45-nm-Technik gefertigt wurden, nur rund 21 nm; Die 45 nm bei der 45-nm-Technik beziehen sich auf die Größe der kleinsten lithographisch fertigbaren Struktur, die sogenannte Feature Size, was in der Regel der unterste Metallkontakt mit den Drain-Source-Gebieten ist. Die Halbleiterunternehmen treiben diese Verkleinerung voran; so stellte Intel im Dezember 2009 die neuen 32-nm-Testchips vor.[27] Neben dem Bereich der Mikroprozessoren und Speicher sind an der Spitze der immer kleineren Strukturgrößen auch Grafikprozessoren und Field Programmable Gate Arrays (FPGAs).[28]
In nachfolgender Tabelle ist beispielhaft die Anzahl der auf einigen verschiedenen Mikrochips eingesetzten Transistoren und Technologieknoten angegeben:
In der analogen Schaltungstechnik finden sowohl Bipolartransistoren als auch Feldeffekttransistoren in Schaltungen wie dem Operationsverstärker, Signalgeneratoren oder als hochgenaue Referenzspannungsquelle Anwendung. Als Schnittstelle zu digitalen Anwendungen fungieren Analog-Digital-Umsetzer und Digital-Analog-Umsetzer. Die Schaltungen sind dabei im Umfang wesentlich kleiner. Die Anzahl der Transistoren pro Chip bewegen sich im Bereich von einigen 100 bis zu einigen 10.000 Transistoren.
In Transistorschaltungen zur Signalverarbeitung wie Vorverstärker ist das Rauschen eine wesentliche Störgröße. Es spielt dabei vor allem das thermische Rauschen, das Schrotrauschen sowie das 1/f-Rauschen eine Rolle. Bei dem MOS-Feldeffekttransistor ist das 1/f-Rauschen bereits unter ca. 1 MHz besonders groß. Das unterschiedliche Rauschverhalten bestimmt ebenfalls die möglichen Einsatzbereiche der Transistortypen, beispielsweise in Niederfrequenzverstärkern oder in speziellen rauscharmen Hochfrequenzumsetzern.
In der analogen Schaltungstechnik werden auch heute noch diskrete Transistoren unterschiedlichen Typs eingesetzt und mit anderen elektronischen Bauelementen auf Leiterplatten verbunden, so es für diese Anforderungen noch keine fertigen integrierten Schaltungen bzw. Schaltungsteile gibt. Ein weiterer Einsatzbereich für den Einsatz diskreter Transistorschaltungen liegt im qualitativ höheren Segment der Audiotechnik.
Transistoren werden in unterschiedlichen Bereichen der Leistungselektronik eingesetzt. Im Bereich von Leistungsverstärkern finden sie sich in Endstufen. Im Bereich der geregelten Stromversorgungen wie bei Schaltnetzteilen finden Leistungs-MOSFETs oder IGBTs Anwendung – sie werden dort als Wechselrichter und synchroner Gleichrichter verwendet. IGBT und Leistungs-MOSFETs dringen zunehmend in Bereiche vor, die bisher größeren Thyristoren vorbehalten waren, bspw. in Wechselrichtern oder Motorsteuerungen. Der Vorteil der Leistungstransistoren gegenüber Thyristoren ist die Möglichkeit, Transistoren jederzeit ein- oder ausschalten zu können. Herkömmliche Thyristoren können zwar jederzeit eingeschaltet (gezündet) werden, aber nicht bzw. nur mit zusätzlichem Schaltungsaufwand wieder ausgeschaltet werden. Ein Umstand, der vor allem bei Gleichspannungsanwendungen von Nachteil ist.
Aufgrund der in der Leistungselektronik auftretenden Verlustleistungen kommen meist größere Transistorgehäuse wie TO-220 oder TO-3 zur Anwendung, die zusätzlich eine gute thermische Verbindung zu Kühlkörpern ermöglichen.
Transistoren haben normalerweise drei Anschlüsse, die als Drähte, Stifte, Bleche typisch nur an einer Seite des Gehäuses parallel herausgeführt werden. Die Lötflächen an SMD-Gehäusen liegen jedoch zumindest an zwei Seiten der Kontur. Insbesondere bei Leistungstransistoren, die fest mit einer Kühlfläche verschraubt werden, kommt es vor, dass der zu verschraubende Metallteil auch einen der drei Transitorenpole elektrisch herausführt, sodass nur zwei (weitere) Pole als Stifte o. Ä. zu finden sind. Kommen hingegen vier Drähte aus dem Gehäuse, kann einer die Funktion „S“ Schirm/Abschirmung haben. Enthält ein Gehäuse mehrere Transistoren, können – vgl. Darlingtontransistor – entsprechend viele Kontakte herausführen.
Es gibt individuell ausgesuchte Paare von Exemplaren mit möglichst ähnlichen Eigenschaften zum Einbau in entsprechend anspruchsvolle Schaltungen. Zudem gibt es sogenannte Komplementär-Paare (Typen) mit ähnlichen Eigenschaften, jedoch vertauschter Polarität, also ein npn- und ein pnp-Typ.
Der im Inneren unter Umständen filigrane Aufbau des Bauteils wird von einem vergleichsweise robusten Gehäuse gehaltert und zugleich umschlossen.
Aufgaben des Gehäuses und der Zuleitungen im Allgemeinen:
Im Sonderfall des Fototransistors als Sensor soll Licht in den Halbleiter selbst eindringen können.
Materialien der Gehäuseschale:
Einbettung der Kontakte:
Dieser Artikel ist als Audiodatei verfügbar:
Mehr Informationen zur gesprochenen Wikipedia

Ein Verbrennungsmotor, in der Patentliteratur auch als Brennkraftmaschine[1] bezeichnet, ist eine Wärmekraftmaschine nach dem Prinzip der Verbrennungskraftmaschine, die chemische Energie durch Treibstoff-Verbrennung in mechanische Arbeit umwandelt. Dazu wird in einem Brennraum ein zündfähiges Gemisch aus Kraftstoff und Luft (Sauerstoff) verbrannt. Kennzeichen aller Verbrennungsmotoren ist die innere Verbrennung, also die Erzeugung der Verbrennungswärme im Motor, weshalb in der englischen Sprache das Akronym ICE (Internal Combustion Engine) für Verbrennungsmotoren in Fahrzeugen verwendet wird. Die Wärmeausdehnung des so entstehenden Heißgases wird genutzt, um Kolben (beim Wankelmotor Rotationskolben, sog. Läufer) in Bewegung zu versetzen. Die häufigsten Arten von Verbrennungsmotoren sind Otto- (Fremdzünder) und Dieselmotoren (Selbstzünder). Eine typische Anwendung dieser Motoren ist der Antrieb von Kraftfahrzeugen (kurz Kfz) wie Automobilen oder Motorrädern, Schiffen und Flugzeugen.
Die kontinuierlich arbeitenden Strahl- und Raketentriebwerke sowie Gasturbinen zählen üblicherweise nicht zu den Verbrennungsmotoren, obwohl auch dort der Kraftstoff innerhalb der Maschine verbrannt wird. Dampfturbinen, Dampfmaschinen oder der Stirlingmotor sind keine Verbrennungsmotoren, da die für ihren Betrieb nötige Wärme außerhalb und nicht zwingend durch Verbrennung erzeugt wird.
Erste Verbrennungsmotoren waren bereits in den 1850er-Jahren bekannt. Christian Reithmann betrieb in den 1850er Jahren Gasmotoren, die er selbst entwickelt und gebaut hatte. Étienne Lenoir konnte ab 1859 einen Gasmotor betreiben. Im Jahr 1860 wurde damit das Hippomobile betrieben und ging damit in die Geschichte des Automobils ein.
Nach gut 150 Jahren weiterer Entwicklungsarbeit an Verbrennungsmotoren zeichnete sich ein Ende der Möglichkeiten ab. Die kohlenstoffbasierten Verbrennungsmotoren gelten im 21. Jahrhundert zunehmend als unerwünscht. Im November 2021 beschlossen rund 2 Dutzend Staaten auf der Weltklimakonferenz von Glasgow eine Erklärung zum Verbot von Verbrennungsmotoren im Bereich der automobilen Nutzung. Deutschland unterzeichnete die Erklärung mit Hinweis auf nicht ausgeschöpftes Potential alternativer Brennstoffe nicht.[2] Das Entwicklungs- und Anwendungspotential von Verbrennungsmotoren für alternative Energien war bis zum Ende der 2010er-Jahre nicht abschließend erforscht. Anfang der 2020er-Jahre zeichneten sich Möglichkeiten für die Zukunftsfähigkeit von Wasserstoffverbrennungsmotoren ab.
Am 8. Juni 2022 beschloss das EU-Parlament auf Vorschlag der EU-Kommission, dass ab 2035 in der Europäischen Union nur noch emissionsfreie Autos zugelassen werden dürfen. Die Hersteller wurden verpflichtet, ihre Flottenemissionen um 100 Prozent reduzieren. Das Europaparlament nahm den Vorschlag der EU-Kommission mit 339 Ja- zu 249 Neinstimmen bei 24 Enthaltungen an. Bei der Abstimmung stimmten die europäischen Konservativen (inkl. CDU/CSU) und Rechten (inkl. AfD) gegen den Beschluss. Die Fraktion der konservativen Europäischen Volkspartei (EVP) hatte zuvor vergeblich versucht, einen Kompromiss durchzusetzen, nachdem auch Fahrzeuge mit Hybridantrieb oder mit synthetischen Kraftstoffen betriebene Verbrennungsmotoren erlaubt gewesen wären. Die Grüne Fraktion im Europaparlament war mit ihrem Vorschlag eines Verbots von Verbrennungsmotoren schon im Jahr 2030 ebenfalls nicht durchgedrungen.[3][4]
Bei allen Motoren mit innerer Verbrennung wird nach jedem Arbeitstakt das beteiligte Gas gewechselt, also Abgas ausgestoßen und frisches Gemisch (Frischgas) zugeführt. Die nicht genutzte Verbrennungswärme, die mit dem Abgas entweicht, geht in die Verlustleistung ein.
Moderne Motoren verdichten das dem Arbeitsraum zugeführte Gas, dann wird unter Druck die Verbrennung eingeleitet. Das Gas erwärmt sich stark und der Druck steigt. Der Motor entspannt das heiße Gas (zum Beispiel mit einem zurückweichenden Kolben), Druck und Temperatur des Gases sinken und das Volumen nimmt zu. Dabei verrichtet es mechanische Arbeit. Je nach Bau- und Funktionsweise des Motors werden diese Vorgänge unterschiedlich verwirklicht. Grundlegend für die Funktion als Motor ist, dass wegen der Verbrennung des Kraftstoff-Luft-Gemischs die Ausdehnung des Gemischs bei höherem Druck geschieht als das Verdichten. Der maximal mögliche Wirkungsgrad hängt von den Temperaturniveaus ab, auf dem die Verbrennungswärme zu- und abgeführt wird, und ist vom Verdichtungsverhältnis und dem Kreisprozess abhängig. Große Zweitakt-Dieselmotoren erreichen Wirkungsgrade von knapp über 50 %. Moderne Fahrzeug-Ottomotoren erreichen im besten Arbeitspunkt (etwa in der Mitte des Drehzahlbandes und knapp unter der Volllastkurve) einen effektiven Wirkungsgrad von 40 %. Bei Kraftfahrzeug-Dieselmotoren liegt er bei 43 %.[5] Der Wirkungsgrad ist bei hohen Drehzahlen niedriger und fällt bei sinkender Last stark ab, weil sich die mechanischen Verluste im Motor über die Last kaum ändern. Sie betragen ungefähr 10 % der Volllastleistung und sind fast nur von der Drehzahl abhängig. (siehe Verbrauchskennfeld). Das ist besonders bei Kraftfahrzeugmotoren im Straßenverkehr von Bedeutung, da sie vor allem im unteren Teillastbereich betrieben werden.[6] Der durchschnittliche Wirkungsgrad eines Kfz-Motors liegt daher sehr viel niedriger als die Maximalwerte. Crastan gibt zum Beispiel für ein herkömmliches Fahrzeug mit Ottomotor einen durchschnittlichen Wirkungsgrad von 20 % an.[7] Dieselelektrische Antriebe dienen unter anderem dazu, die Drehzahl auch dann näher am optimalen Bereich zu halten, wenn nur mit niedriger Geschwindigkeit gefahren wird.[8][9] Aufgrund des höheren Gewichts kommen sie bevorzugt in ohnehin schweren Fahrzeugen wie Zügen oder Schiffen zum Einsatz. Prinzipiell denkbar sind auch Otto-elektrische oder Wankel-elektrische Antriebe, jedoch ist aufgrund der Vorteile des Dieselmotors bei besonders großen Motoren kaum ein Einsatzgebiet für derartige Motoren denkbar, bei denen dieselelektrische Motoren nicht besser geeignet sind.
Der Allgemeine Deutsche Sprachverein unternahm in der ersten Hälfte des 20. Jahrhunderts Versuche das zusammengesetzte Fremdwort Explosionsmotor einzudeutschen. Aus „Explosion“ wurde „Zerknall“ (wie heute noch in „Kesselzerknall“) und aus „Motor“ wurde unter anderem „Treiber“. So lautete die Vorgeschlagene deutsche Bezeichnung für einen Verbrennungsmotor „Zerknalltreibling“ die sich heute nur noch als scherzhafte Bezeichnung erhalten hat.
In der Geschichte des Motorenbaus sind viele Konzepte erdacht und realisiert worden, die nicht unbedingt in das folgende Raster passen, zum Beispiel Ottomotoren mit Direkteinspritzung oder Vielstoffmotoren. Zugunsten der Übersichtlichkeit werden diese Sonderfälle hier nicht betrachtet.
Dazu gehören der Wankelmotor (Ottomotor mit Rotationskolben und Schlitzsteuerung) oder Schiffsdieselmotoren, die oft als Zweitakt-Dieselmotor mit Auslassventilen konzipiert sind.
Ottomotoren arbeiten in der Regel mit ungefähr konstantem Verbrennungsluftverhältnis, das heißt pro eine Masseeinheit Kraftstoff werden 13 bis 15 Masseeinheiten Luft hinzugemischt. Zur Verbrennung von 1 kg Benzin werden 14,5 kg Luft benötigt; ein solches Kraftstoff-Luft-Gemisch wird als stöchiometrisch (Luftzahl ) bezeichnet. Ist mehr Luft als nötig im Brennraum, so ist das Gemisch überstöchiometrisch, mager (), ist zu wenig Luft im Brennraum, so ist das Gemisch unterstöchiometrisch, fett (). Um die Abgase in einem Katalysator mit maximaler Wirkung zu entgiften, ist eine Luftzahl von 1 erforderlich.
Dieselmotoren arbeiten mit variabler Luftzahl, etwa von 10 bis 1,3.
Die Gemischbildung kann sowohl innerhalb, als auch außerhalb des Brennraums stattfinden, wobei der bedeutendste Selbstzündermotor, der Dieselmotor, nur mit Gemischbildung innerhalb des Brennraumes funktioniert.
Die Fremdzündung ist Merkmal verschiedener Motoren, unter anderem des Ottomotors. Dabei wird das Entzünden des Kraftstoffluftgemisches durch eine Zündhilfe eingeleitet, in der Regel kurz vor dem oberen Totpunkt. Ottomotoren haben dafür Zündkerzen. Gibt es keine Zündkerze und ist die Fremdzündung unkontrolliert, so spricht man von Glühzündung. Die ersten Motoren von Gottlieb Daimler arbeiteten mit Glühzündung. Ein früher verbreiteter Glühzündermotor ist der Glühkopfmotor, nach seinem Erfinder auch Akroydmotor genannt. In Deutschland ist er insbesondere aus Ackerschleppern der Marke Lanz Bulldog bekannt, in Skandinavien als Motor von Fischerbooten, unter anderem von Bolinders. Bei diesen Motoren muss vor dem Start ein Glühkopf genannter Teil des Zylinderkopfs erhitzt werden, etwa mit einer Lötlampe, ehe die Zündung einsetzen kann. In den Glühkopf wird der Treibstoff während des Verdichtungstaktes eingespritzt. Heute werden Glühzündermotoren (die allerdings nicht nach dem Akroydverfahren arbeiten) vorwiegend im Modellbau verwendet. Beim Ottomotor können in seltenen Fällen Glühzündungen nach dem Abstellen des Motors vorkommen, sie wirken sich aber schädlich auf das Triebwerk aus und sind daher unerwünscht.
Die Selbstzündung ist Merkmal verschiedener Motoren, bekanntester Selbstzünder ist der Dieselmotor. Bei einem Selbstzündermotor werden keine Zündhilfen eingesetzt, die Zündung wird stattdessen ausschließlich durch Kompressionswärme eingeleitet. Die Arbeitsweise der Selbstzündermotoren ist von ihrem Funktionsprinzip abhängig: bei einem Dieselmotor wird zuerst reine Luft stark verdichtet und dadurch erhitzt. Kurz vor dem oberen Totpunkt (OT) wird der Dieselkraftstoff eingespritzt, der sich durch die Hitze von selbst entzündet. Da sich der Kraftstoff im Dieselmotor aufgrund der späten Einspritzung entzündet, bevor sich ein homogenes Gemisch bilden kann, spricht man beim Dieselmotor von heterogenem Gemisch. Bei sogenannten HCCI-Motoren wird hingegen ein homogenes Gemisch gebildet, das sich nur durch die Kompressionswärme entzünden soll. Anders als beim Dieselmotor muss daher die Einspritzung des Kraftstoffes früh erfolgen, damit das Gemisch bis zur Zündung gut durchmischt (homogen) ist. Dadurch werden bessere Emissionswerte erreicht. Einige Modellbaumotoren arbeiten ebenfalls mit homogener Kompressionszündung, das Gemisch wird hier mit einem Vergaser gebildet, das Verdichtungsverhältnis kann mit einer Schraube verstellt werden.
Mit Brennverfahren bzw. Verbrennungsverfahren bezeichnet man bei Verbrennungsmotoren den Ablauf, in dem der Treibstoff im Motor verbrennt.
In früheren Jahren wurde der Grad der Schnellläufigkeit aufgrund der Kolbengeschwindigkeit, später vermehrt durch die Drehzahl bestimmt.
Bei Großmotoren[11] (Schiffe, Bahn, Stromerzeuger) unterscheidet man drei Klassen:
Nach weiteren Definitionen gibt es
Hans Schrön unterscheidet im 1942 erschienenen Werk Die Verbrennungskraftmaschine zwischen drei verschiedenen Typen, den Langsamläufern, den Mittelläufern und den Schnellläufern. Als Unterscheidungsmerkmal zieht Schrön die Kolbengeschwindigkeit heran. Den Umstand, dass noch nicht alle Motoren als Schnellläufer konstruiert sind, sieht er in Punkten, die bei der Konzeption eine wichtigere Rolle als hohe Drehzahlen spielen. Die Langsamläufer und Mittelläufer sollen möglichst eine hohe Lebensdauer und Störungsfreiheit haben, dazu zählen Stationärmotoren und Schiffsmotoren. Bei Schiffsmotoren weist Schrön ebenfalls auf den Vorteil des hohen Wirkungsgrades hin. Weitere Mittelläufer sind unter anderem Triebwagen-, Lastkraftwagen-, Traktoren- und Kampffahrzeugmotoren. Schnellläufer sollen eine niedrige Masse, wenig Volumen und gute Einbaufähigkeiten haben, gegebenenfalls spielt die größtmögliche Leistung noch eine Rolle. Als Anwendungsbereich kommen Schnellboote, Flugzeuge und Leichtfahrzeuge in Betracht. Schnellläufer können sowohl Diesel- als auch Ottomotoren sein.[13]
Günter Mau unterscheidet den Grad der Schnellläufigkeit bei Dieselmotoren wie folgt:[14]
Abhängig von der Anzahl der Zylinder werden/wurden Otto- und Dieselmotoren bzw. Viertakt- und Zweitakt-Motoren gebaut als:
Die fettgedruckten Bauformen und Zylinderzahlen sind heute in Kraftfahrzeugen gebräuchlich.
Der Verbrennungsmotor mit der höchsten Zahl an Zylindern, der je gebaut wurde, ist der Reihensternmotor Swesda M520 mit 56 Zylindern in sieben Zylinderbänken zu jeweils acht Zylindern.
Viertakt-Sternmotoren haben immer eine ungerade Zylinderzahl pro Stern. Der Grund dafür ist, dass beim Viertaktmotor jeder Zylinder nur in jeder zweiten Umdrehung gezündet wird, sodass eine durchgängige Zündfolge, die für den ruhigen, vibrationsfreien Lauf des Motors erforderlich ist, nur mit ungeraden Zylinderzahlen erzielt werden kann. Mehrfachsternmotoren wie die 14-Zylinder-Doppelsternmotoren BMW 801 und Wright R-2600 oder auch der P & W R-4360 (28 Zylinder in vier Sternen zu je sieben) haben jedoch eine gerade Zylinderzahl.
Davon sind die Reihensternmotoren zu unterscheiden, bei denen mehrere Zylinderbänke sternförmig um die Kurbelwelle angeordnet sind. Dies waren zum Beispiel der Daimler-Benz DB 604, Rolls-Royce Vulture und Allison X-4520 (X-Motoren mit vier Zylinderbänken zu je sechs Zylindern = 24 Zylinder), Junkers Jumo 222 und Dobrynin WD-4K (ebenfalls 24 Zylinder, jedoch als Hexagon mit sechs Zylinderbänken zu je vier Zylindern) und der Zwölfzylindermotor Curtiss H-1640 Chieftain mit sechs Zylinderbänken zu je zwei Zylindern.
Im Motorsport werden vereinzelt trotz der höheren Unwucht auch V-Motoren mit ungeraden Zylinderzahlen (drei oder fünf) gebaut.
Als langsam laufende Schiffsdiesel gibt es Reihenmotoren mit bis zu 14 Zylindern sowie V-Motoren mit 20 oder 24 Zylindern.
Der Wankelmotor ist ein Drehkolbenmotor, der von Felix Wankel erfunden und nach ihm benannt ist. Beim Wankelmotor sind zwei kinematische Formen möglich: Zum einen der Kreiskolbenmotor, bei dem ein bogig-dreieckiger Kolben (Gleichdick) in einem oval-scheibenförmigen Gehäuse auf einer von der Exzenterwelle bestimmten Kreisbahn umläuft. Zum anderen der Drehkolbenmotor, bei dem sowohl der bogig-dreieckige Läufer als auch die oval-scheibenförmige Hüllfigur (Trochoide) auf leicht versetzten Achsen um ihre Schwerpunkte rotieren.
Der Stelzer-Motor, benannt nach seinem Erfinder Frank Stelzer, ist ein Zweitakt-Freikolbenmotor mit einem beweglichen Teil, das „Stufenkolben“ genannt wird. Es besteht aus drei starr durch eine Kolbenstange verbundenen Kolben. Der mittlere ist ein doppelt wirkender Scheibenkolben als Spülpumpe für die beiden äußeren Arbeitskolben, die als Kolben schlitzgesteuerter, gleichstromgespülter Zweitakter arbeiten. Wegen der Kolbenstange sind die Brennräume dieser Zweitakter ringförmig. Die äußeren Enden des Stufenkolbens bewegen sich aus dem Motorblock heraus und können Teil einer Arbeitsmaschine sein, zum Beispiel eines Verdichters oder elektrischen Generators.
Der Mederer-Motor und der Kreuzschleifenmotor haben einen etwas anderen Bewegungsablauf des Kolbens.
Der Kugelmotor: Der erste patentierte Kugelmotor wurde von Frank Berry 1961 in den USA entwickelt. Es folgte ein weiteres Modell, das von dem Diplom-Physiker Wolfhart Willimczik nach 1974 entwickelt wurde und nach dem Zweitakt-Prinzip arbeitet.
Herbert Hüttlin entwickelte einen Kugelmotor, der mit gekrümmten Kolben arbeitet, die sich gegeneinander bewegen. Dieser Motor wird im Schrifttum unter dem Oberbegriff Rotationskolbenmaschine genannt.
Von Arnold Wagner wird der Hiteng-Kugelmotor entwickelt. Der Hiteng-Kugelmotor arbeitet mit zwei Doppelkolben, die sich in einem kugelförmigen Gehäuse drehen. Der Erfinder bezeichnet diesen Motor als Schwenkkolbenmaschine.
In der ersten Hälfte des 20. Jahrhunderts wurde eine Reihe exotischer Konstruktionen entworfen, die jedoch das Prototypstadium nicht überschritten. Durch Fortschritte der Werkstoffforschung sind Lösungen für Probleme alter Konstruktionen möglich.
Partikel im Abgas von Verbrennungsmotoren (10–1000 nm) sind kleiner als andere, etwa durch Reifenabrieb (15.000 nm) verursachte. Wie jene bestehen sie aber aus Ruß und Kohlenwasserstoffen (zum Beispiel PAK). Ihre für den Menschen vermutete Gesundheitsrelevanz erhalten die Abgasnanopartikel aufgrund ihrer Oberfläche und Größe. Sie können Zellmembranen verletzen (Ruß) oder mit ihnen reagieren (PAK).[15] Durch ihre Größe (als Nanopartikel bezeichnet man alles unter 100 nm) gelingt ihnen die Überwindung der oberen Atemwege und der Lungenwand und damit der Eintritt in den Blutkreislauf (vgl.). Dosis, Einwirkzeit, Projizierbarkeit von Tierversuchen auf den Menschen und Begleitumstände wie das Rauchen von Studienteilnehmern bilden die Zielsetzungen derzeitiger Forschung.[16] Dem vorgreifend begrenzt die Euro 6 Abgasnorm für 2014 erstmals die Partikelmenge (6×1011 Stück pro km) und nicht mehr nur ihre Masse.[17] Die Masse wird durch die entscheidenden Nanopartikel nur zu 20 % beeinflusst, beim Diesel die Gesamtmasse aber durch geschlossene Partikelfilter bereits um 97 % reduziert.[18][19] Das zeigt, dass die dortige Ansammlung von Filtrat auch relevante Mengen von Nanopartikeln weit unter der eigentlichen Filterporengröße von 1000 nm abfängt.[20][21] Mit dieser Reduktion minimiert der Filter zudem die Klimawirksamkeit der Partikel. Die dunkle Rußfarbe macht die Partikel zu Wärmeabsorbern. Damit erwärmen sie direkt die rußbelastete Luft und nach Ablagerung auch Schneeflächen in der Arktis, die sie durch Luftströmungen etwa von Europa her erreichen.[22]
Benzin- und Dieselmotoren produzieren während Volllast- und Kaltstartphasen vergleichbare Mengen und Größen an Partikeln.[23][24] In beiden Phasen wird mehr Kraftstoff eingespritzt, als der Sauerstoff im Zylinder verbrennen kann („angefettetes Gemisch“). In Kaltstartphasen geschieht dies zur Katalysatorerwärmung, unter Volllast zur Motorkühlung. Während Benzinmotoren nur im angefetteten Betrieb Partikel durch Sauerstoffmangel erzeugen, oder bei den ab den 2000er Jahren eingesetzten Direkteinspritzern entstehen diese beim Diesel selbst im Magerbetrieb und damit während aller Betriebsphasen.[25][26] Daher liegt die Partikelmenge des Benziners insgesamt dennoch auf dem niedrigen Niveau eines Diesels mit geschlossenem Filtersystem.[27]
Ursächlich für den Dieselruß sind seine doppelt so langkettigen Aromate (vgl. Benzin). Sie weisen einen deutlich höheren Siedepunkt auf (von 170 bis 390 °C anstatt 25 bis 210 °C). Gleichzeitig liegt die Verbrennungstemperatur des Diesels aber 500 °C unterhalb der des Benzinmotors.[28] Benzin verdampft daher vollständiger als Diesel. Dessen früher siedende Bestandteile verdampfen zuerst, was den Resttropfen aus Aromaten höherer Siedepunkte zusätzlich auf niederer Temperatur hält (vgl.).
Die nicht verdampften Aromate werden während der Selbstzündungsphase temperaturbedingt in ihre Bestandteile gecrackt. Zu diesen zählt der Kohlenstoff, also Ruß.
Die Partikelzusammensetzung unterscheidet sich aufgrund der Chemie beider Kraftstoffe. So überwiegen beim Benzinmotor die PAK-Partikel, beim Dieselmotor sind es die Rußpartikel.[29] Sichtbar werden die Partikel erst durch Aneinanderlagerung. Sichtbare Partikel sind nicht mehr lungengängig und werden meist schon im oberen Atemweg ausgefiltert und abgebaut. Anlagerungen finden im Auspuff und besonders im Partikelfilter statt. Die dortige Ansammlung des Filtrats fängt auch Partikel weit unter der eigentlichen Filterporengröße (1 µm) ab. Damit sinkt die Partikelanzahl auf das Niveau eines Benzinmotors.[27]
Erkennbar wird die Partikelanlagerung im Auspuff. Fehlt diese, verfügt ein Diesel über ein geschlossenes Filtersystem und ein Benziner über wenige Anteile von Kaltstart- und Volllastphasen oder über einen Ottopartikelfilter.
Der Verbrennungsmotor verbrennt in der Regel Kohlenwasserstoffe (Diesel, Benzin), die aus fossilem Erdöl gewonnen werden. Diese Verbrennung führt zu Kohlendioxidemissionen, die zur Klimaerwärmung beitragen.
Die Kohlendioxid-Emission, die bei der Verbrennung von einem Liter Kraftstoff entsteht, lässt sich gut abschätzen[30]. Als gute Näherung kann man für Diesel die chemische Formel  annehmen. Tatsächlich ist Diesel eine Mischung verschiedener Moleküle. Kohlenstoff C hat eine molare Masse von 12 g/mol und Wasserstoff H (atomar) hat eine molare Masse von ca. 1 g/mol. Damit ist der Anteil von Kohlenstoff an der Gesamtmasse von Diesel ungefähr 12/14. Die Verbrennungsreaktion von Diesel ist:
Kohlendioxid hat eine molare Masse von 44 g/mol, da es aus 2 Sauerstoffatomen (16 g/mol) und einem Kohlenstoffatom (12 g/mol) besteht. 12 g Kohlenstoff ergeben 44 g Kohlendioxid.
Mit einer Dichte des Diesels von 0.838 kg pro Liter berechnet sich die Masse Kohlendioxid, die bei der Verbrennung von einem Liter Diesel entsteht folgendermaßen:

Diese Abschätzung ergibt einen Zahlenwert, der in guter Übereinstimmung mit den Werten ist, die man in der Literatur findet.
Für Benzin mit einer Dichte von 0,75 kg/l und einem Verhältnis von Kohlenstoff zu Wasserstoffatomen von ungefähr 6 zu 14, ergibt sich:

1 Liter Benzin ergibt rund 2,3 kg Kohlendioxid.
Beim Einsatz von Kohlenstoffverbindungen als Kraftstoff emittieren Verbrennungsmotoren Kohlenstoffdioxid (CO2). Aufgrund der üblicherweise fossilen Herkunft dieser Kraftstoffe steigt damit die CO2-Konzentration in der Atmosphäre, was zur globalen Erwärmung beiträgt. Viele Länder fördern deshalb den Verkauf von Elektroautos und/oder von Hybridelektrokraftfahrzeugen (siehe auch Elektromobilitätsgesetz und Elektromobilität #Förderwürdigkeit). Eine zunehmende Zahl von Ländern plant, den Verkauf von Neuwagen mit Verbrennungsmotor ab einem bestimmten Stichtag zu verbieten.[31]
Im Juni 2022 beschloss das EU-Parlament, dass Neuwagen ab dem Jahr 2035 kein CO2 mehr ausstoßen dürfen[32][33] und gab dazu im Februar 2023 die endgültige Zustimmung. Demnach dürfen in der EU im Bereich der Personenkraftwagen und leichten Nutzfahrzeugen ab 2035 nur noch emissionsfreie Fahrzeuge neu zugelassen werden.[34][35][36] Die Zustimmung des Rates der Europäischen Union steht noch aus (Stand 14. Februar 2023).[37]
Berlin hat sich zunächst wieder anders überlegt. „Auch Italien, Polen und Bulgarien wollen dem Verbrenner-Aus nicht zustimmen, .. Zusammen mit Deutschland hätten diese Länder eine Sperrminorität.“[38] Später wurde eine Einigung zwischen Deutschland und der EU-Kommission erzielt.[39]
Eine Alternative zu der Umstellung auf Elektromotoren, insbesondere für Bereiche, in denen diese schwierig umzusetzen ist, stellt der klimaneutrale Betrieb von Verbrennungsmotoren mit Kraftstoffen regenerativer Herkunft (z. B. Biokraftstoffe oder E-Fuels) dar. Auch dafür existieren Förderprogramme, beispielsweise in Deutschland.[40]
Einzylindermotor |
Reihenmotor |
Boxermotor |
V-Motor |
U-Motor |
W-Motor |
Y-Motor |
VR-Motor |
X-Motor |
H-Motor |
Δ-Motor
Sternmotor |
Reihensternmotor |
Mehrfachsternmotor |
Umlaufmotor |
Taumelscheibenmotor
Doppelkolbenmotor |
Gegenkolbenmotor |
Freikolbenmotor |
Rotationskolbenmotor |
Kugelmotor |
Schiebermotor |
Wankelmotor |
Glühkopfmotor |
Direkteinspritzender Glühkopfmotor

Schwarzpulver war als Büchsenpulver der erste Explosivstoff, der als Schießpulver für Treibladungen von Schusswaffen verwendet wurde. Als Sprengpulver ist es ein Sprengmittel. Heute wird es als Korn- und Mehlpulver hauptsächlich in der Pyrotechnik – insbesondere bei der Feuerwerkherstellung – sowie beim Schießen mit Vorderladern und Böllern verwendet.
Schwarzpulver ist eine pyrotechnische Mischung, die aus Salpeter (meist Kalisalpeter = Kaliumnitrat), fein gemahlener Holzkohle (wegen des geringen Ascheanteils früher vornehmlich aus dem Holz des Faulbaums, auch Pulverholz genannt, gewonnen, heute oft auch aus Erlenholz)[3][4] und Schwefel besteht. Schwarzpulver besteht im Mittel aus 75 % Salpeter, 10 % Schwefel und 15 % Holzkohle (Angaben in Gewichtsprozent). Dieses Mischungsverhältnis kann je nach Verwendungszweck leicht abweichen.
Pulver auf der Basis von Natriumnitrat, das billiger, aber sehr hygroskopisch ist, wurde in Form von Presslingen hergestellt und mit Bitumen gegen Feuchtigkeit imprägniert. Diese Presslinge waren als Geschützpulver wenig geeignet, sie wurden vornehmlich im Bergbau verwendet, die Bezeichnung lautet Sprengsalpeter.
In der frühen Geschichte des Schwarzpulvers wurde statt Kalisalpeter auch Calciumnitrat (zunächst als Mauersalpeter) und Magnesiumnitrat verwendet, die aber wegen hygroskopischer Eigenschaften das Pulver schnell unbrauchbar machten. Aus diesem Grund wurden Umlösungsprozesse entwickelt, die mit Hilfe von Pottasche aus gelöstem Calcium- und Magnesiumnitrat eine Lösung mit Kaliumnitrat lieferten (Calcium und Magnesium wurden als Karbonate ausgefällt).[5] Die Gewinnung der Nitrate für Schwarzpulver geschah später durch bakterielle Nitrifikation (siehe Kalisalpeter).
Salpeter dient als Oxidationsmittel, wobei auch andere Salze (z. B. Chlorate, jedoch wegen hoher Brisanz nicht für Treibladungspulver) verwendet werden können. Das Kohlepulver dient als Brennstoff und der Schwefel kann sowohl als Brennstoff als auch als Zündmittel verwendet werden, damit die Schwarzpulvermischung bei kleinster Berührung mit Funken zu brennen beginnt.
Zur Erzielung von Flammenfärbungen für pyrotechnische Erzeugnisse werden bestimmte Nitrate verwendet, deren Kation eine entsprechende Flammenfärbung liefert. Es wurden im sogenannten Feuerwerkbuch von 1420 Rezepte für weißes (mit Zusatz von „Felberbaumholz“), rotes (mit Sandelholz), blaues (mit Kornblumen) und gelbes Pulver (mit Indischer Narde) verwendet.[6]
Die Bestandteile müssen fein zermahlen und gleichmäßig vermischt werden. Das geschieht meistens in einer Pulvermühle. Danach wird das Gemisch in Kuchen feucht verpresst und getrocknet, die wiederum zermahlen und entweder gekörnt oder als Mehlpulver belassen werden. Beim Körnen, das schon im 15. Jahrhundert bekannt war,[7] wird das Pulver angefeuchtet und wieder in Bewegung zu Kügelchen geformt. Damit wird ein Entmischen der Bestandteile verhindert und über die Größe der Kügelchen kann die Abbrandgeschwindigkeit in gewissen Grenzen reguliert werden. Außerdem dringen beim Anfeuchten Salpeter und Schwefel in die Mikroporen der Kohlepartikel.[8] Das fertige Pulver wird noch getrocknet und kann dann abgefüllt und verpackt werden.
Deutsche Schwarzpulvermühlen gibt es in Harzgerode (Sachsen-Anhalt) und im Dörntener Ortsteil[9] Kunigunde[10] der Gemeinde Liebenburg (Niedersachsen). Die letzte in Betrieb befindliche Schwarzpulvermühle in der Schweiz befindet sich in Aubonne am Genfersee, wo das von Sportschützen weltweit wegen seiner hervorragenden Gleichmäßigkeit und geringen Abbrandrückstände bevorzugte Schweizer Schwarzpulver hergestellt wird.[4][11]
Beim Verbrennen des Schwarzpulvers entstehen Kohlendioxid, Kohlenmonoxid, Kaliumcarbonat, Kaliumsulfit, Stickstoff und Feinstaub. Es handelt sich um eine unvollständige Verbrennung. Die folgende Reaktionsgleichung ist vereinfacht und von der prozentualen Zusammensetzung des Schwarzpulvers abhängig. Nicht berücksichtigt wurde dabei die Restfeuchtigkeit sowie der Sauerstoff-, Wasserstoff- und Ascheanteil in der Holzkohle.
Die Mischung verbrennt rasch, die innerstoffliche Schallgeschwindigkeit wird dabei jedoch nicht überschritten, weswegen statt von einer Detonation von einer Deflagration gesprochen wird. Bei der Verbrennung entsteht eine Temperatur von ungefähr 2000 °C.
Schwarzpulver deflagriert mit einer Abbrandgeschwindigkeit von 300 bis 600 m/s, dabei spielen die Restfeuchtigkeit, die Gründlichkeit der Mahlung und Vermischung der Bestandteile, die Größe und Dichte der Ladung sowie die Körnung eine große Rolle:
Während bei Handwaffen feinkörniges Pulver verwendet wurde um überhaupt eine akzeptable Schussleistung zu erreichen, musste bei großkalibrigen Geschützen entsprechend grobkörniges Pulver verwendet werden um den Enddruck zu begrenzen und damit Rohrsprengungen zu vermeiden. Bei Feuerwerkskörpern wird eine Verdämmung aus Karton, Kunststoff und ähnlichem verwendet.
Das Schwadenvolumen (bei Normalbedingungen) liegt um 337 l/kg, außerdem entstehen etwa 0,58 kg feste Kaliumsalze.
Die Nachteile von Schwarzpulver sind die recht niedrige Leistung, durch die brennbaren Gase bedingtes starkes Mündungsfeuer und starke Rauchentwicklung durch die großen Mengen der festen Nitratsalze. Aus diesem Grund wurde es ab etwa 1891 weitgehend durch rauchschwaches Schießpulver auf der Basis von Nitrozellulose[12] verdrängt.
Schwarzpulver ist wenig schlag- und reibungsempfindlich. Statische Elektrizität (Funkenschlag) kann es nur äußerst schwer entzünden, da die enthaltene Holzkohle ein guter elektrischer Leiter ist und der Strom abfließen kann. Zudem sind moderne Schwarzpulver aus Sicherheitsgründen und als Rieselhilfe mit einer dünnen Graphitschicht versehen.[13] Die Zündtemperatur liegt sehr niedrig (ca. 170 °C). Schwarzpulver ist massenexplosiv. Ab einer Menge von ca. einem Kilogramm ist keine Verdämmung mehr erforderlich, damit das Pulver nicht mehr nur abbrennt, sondern in jedem Fall explodiert.
Schwarzpulver wird in der Pyrotechnik, bei frei erhältlichen Knallkörpern, unter anderem bei Modellraketenantrieben verwendet, sowie beim Sportschießen und Böllern.
Schwarzpulver wurde im Kaiserreich China erfunden.[14] Die erste schriftliche Erwähnung salpeterhaltiger Brandsätze findet sich im Song-zeitlichen Wu Jing Zong Yao um 1044. Das Buch ist aber nur in seiner jüngsten Kopie von 1550 aus der Ming-Zeit überliefert, daher ist nicht mehr erkennbar, ob die Vermerke zu den Brandsätzen nicht später hinzugefügt wurden. In dieser Zeit wurden auch Feuerpfeile (Raketen) entwickelt. Der chinesische Kriegsmandarin Yu Yunwen nutzte im Jahr 1161 derartige Feuerpfeile auch zur Abschreckung von Feinden. Im Jahr 1232 kam bei der Belagerung der Stadt Kaifeng Schießpulver zum Einsatz. Die älteste noch erhaltene Handfeuerwaffe in China stammt aus der Zeit um 1288 (Heilongjiang-Büchse).[15] In China und Japan diente jedoch das Schießpulver vornehmlich zu rituellen Zwecken, und zwar zu Ehren Verstorbener.[16] Es ist jedoch nachgewiesen, dass mit Schwarzpulver gefüllte Bomben durch die Chinesen spätestens im 13. Jahrhundert als Waffe eingesetzt wurden.[17]
Die Kenntnis über Schwarzpulver kam möglicherweise über den Mongolensturm oder Handelskontakte entlang der Seidenstraße in den arabischen Raum und (direkt oder über arabische Vermittlung) nach Europa. Dschingis Khan stellte 1214 eine chinesische Katapulteinheit für seine Feldzüge in Transoxanien auf, die auch wie schon zuvor in China üblich Bomben mit Schießpulver verschoss, und es gibt Berichte über einen Einsatz bei der Schlacht bei Muhi in Ungarn 1241.[18] Nach Kenneth Chase könnten Nachrichten davon über die Gesandtschaft (1252 bis 1255) des Franziskaners Wilhelm von Rubruck zu den Mongolen nach Europa gelangt sein, unter anderem zu Roger Bacon, der ebenfalls Franziskaner war, sich sehr für den Bericht der Gesandtschaft interessierte und 1267 eine der frühesten Erwähnungen der Verwendung von Schwarzpulver in Europa verfasste.[19]
Im arabischen Raum beschreibt der syrische Autor Hasan al-Rammah in einem Buch über berittenen Kampf und den Einsatz von Kriegsmaschinen (Al-Furusiyya wa al-Manasib al-Harbiyya) von etwa 1285 die Herstellung von Schwarzpulver, insbesondere die erforderliche Reinigung des Kaliumnitrats.
Ebenfalls erwähnt wird Schwarzpulver im Liber Ignium („Buch der Feuer“) des fiktiven Marcus Graecus. Diese Rezeptsammlung aus unterschiedlichen, teilweise antiken Quellen – nach J. R. Partington zum Großteil entstanden um 1225 mit späteren Ergänzungen bis Ende des 13. Jahrhunderts (insbesondere was das Schießpulver-Rezept betrifft) – enthält ein Rezept in der Zusammensetzung 6 Teile Salpeter, 2 Teile Holzkohle und 1 Teil Schwefel, das sich auch in einem Albertus Magnus zugeschriebenen Werk findet, dessen Zuschreibung aber sehr zweifelhaft ist. Auch Roger Bacon erwähnt in mehreren Schriften von 1242 bis 1267 mehrmals das Pulver, unter anderem als Kinder-Feuerwerkspielzeug. Ob er darüber hinaus genaue Angaben zur Herstellung und Zusammensetzung von Schwarzpulver machte, ist umstritten. J. R. Partington folgt in seiner Geschichte der Pyrotechnik einer Rekonstruktion eines Anagramms durch den Artillerieoberst Henry Hime (1904), das dieser in einer unklaren Stelle bei Bacon gelesen haben will (in einem Buch von Bacon, dessen Zuschreibung umstritten ist).[20] Die zweifelhafte Rekonstruktion liefert eine vom Liber Ignium und späteren Rezepten abweichende Zusammensetzung von fast gleichen Anteilen (7 Teile Salpeter, 5 Teile Haselholz-Kohle und 5 Teile Schwefel).
Die früheste Erwähnung von Feuerwaffen (Schießpulverwaffen) in Europa ist die Abbildung einer primitiven Kanone in einem englischen Manuskript von 1326 (Walter de Milemete) und in der Bestellung von Feuerwaffen durch den Magistrat von Florenz im gleichen Jahr.[21] In Frankreich ist die Herstellung und Verwendung von Schießpulver für Geschütze 1338 belegt.[22] Eine der ältesten europäischen Darstellungen über die Anfangszeit des Geschützwesens und die Kunst der Büchsenmeister findet sich in einer auch das „Schießpulver“ behandelnden Bilderhandschrift[23] aus der zweiten Hälfte des 14. Jahrhunderts.
Erste militärische Anwendung soll das Schießpulver in Europa 1331 bei der Belagerung von Cividale durch deutsche Ritter[24] und bei der Schlacht bei Crécy im Hundertjährigen Krieg[25] im Jahr 1346 erhalten haben, wo es allerdings noch keine entscheidende Rolle spielte. Um 1354 nutzten die Dänen das Schießpulver bei einer Seeschlacht.[16] Auch bei der Belagerung von Saint-Sauveur-le-Vicomte 1374 sollen Kanonen eine wesentliche Rolle gespielt haben und möglicherweise wurden dabei erstmals in Europa Stadtmauern mit Kanonen bezwungen.[26] In der Endphase des Hundertjährigen Kriegs spielte Feldartillerie schon eine entscheidende Rolle (Schlacht von Gerberoy 1435), und Belagerungsartillerie spielte eine entscheidende Rolle bei der Eroberung von Konstantinopel (1453) durch die Osmanen.
Im Mittelalter wurde Schwarzpulver im niederdeutschen Sprachraum als krud oder krut (Kraut) auch „Donnerkraut“ und im hochdeutschen Sprachraum als Büchsenpulver[27] (z. B. 1432)[28] und Pulver (frühneuhochdeutsch) bezeichnet. Die heutige Bezeichnung Schwarzpulver geht wohl nicht auf den Franziskaner Berthold Schwarz aus Freiburg im Breisgau zurück, der – einer Legende zufolge – im 14. Jahrhundert die treibende Wirkung der Pulvergase auf Geschosse fand, sondern auf das schwarze Aussehen des Pulvers. Erstmals als Folge der plötzlichen Entwicklung einer Dampfmenge, die ein über tausendfach größeres Volumen als das zur Explosion gebrachte Schwarzpulver hat, formulierte Vannoccio Biringuccio in seiner „Pirotechnia“ diese Treibwirkung. Dass die Luft im Pulver bzw. im Salpeter um das 800fache im Vergleich zur Atmosphäre verdichtet wird, schrieb 1772 Johann Samuel Halle in seiner Werkstätte der heutigen Künste.[29] Gegen Ende des 19. Jahrhunderts unterschied man Schwarzpulver von den neuen weißen Cellulosenitratpulvern.
Schwarzpulver blieb bis zur Erfindung der modernen Sprengstoffe der einzige militärische und zivile Explosivstoff und einziges Treibmittel für Artillerie- und Handfeuerwaffen. Im 17. Jahrhundert wurde seine Handhabung als Treibmittel für Musketen durch die Papierpatrone mit abgemessener Füllmenge einschließlich Kugel erleichtert. In der ersten Hälfte des 19. Jahrhunderts machte die Entwicklung des Hinterladers die noch einfachere Einheitspatrone möglich. Seit Mitte des 19. Jahrhunderts verdrängten brisantere Sprengstoffe – wie das Nitroglyzerin, das darauf basierende Dynamit, die Nitrozellulose (Schießbaumwolle), Nitroaromaten, Nitramine usw. – das Schwarzpulver weitgehend als Explosivstoff und Treibmittel.
Die friedliche Nutzung des Schwarzpulvers in Europa ist zunächst als Lustfeuerwerk in Vicenza Pfingsten 1379 historisch verbürgt. Italienische Spezialisten entwickelten durch Zumischung von neuen Substanzen farbige Spektakel bei Theater- und Operninszenierungen; mit originären Feuerwerken wurden Schlösser, Schlachten und Naturereignisse dargestellt. 1660 wurde der Sonnenkönig Ludwig XIV. bei seinem Einzug nach Paris von Zehntausend Feuerwerkskörpern empfangen. 1749 ließ Georg II. zur Feier des Aachener Friedens von Georg Friedrich Händel die Feuerwerksmusik komponieren; dabei brannten Teile der für das Spektakel gebauten Schlosskulisse nieder.[30]
Die begrenzte Aufbewahrung von Schwarzpulver fordern das Kurfürstentum Trier und weitere Kurfürstentümer des Heiligen Römischen Reiches im 18. Jahrhundert durch Erlass entsprechender Anordnungen zur Brandverhütung. Krämer durften hierdurch nur bis zu drei Pfund Pulver im Laden vorhalten.[31]
Heute wird Schwarzpulver vor allem für Feuerwerke verwendet. Es dient dabei als Antriebsmittel für einfache Raketen, als Ladung von Knallkörpern  und als Ausstoß- und Zerlegerladung für größere Effektträger wie beispielsweise Bomben und Bombetten.
Im Schießsport wird Schwarzpulver nur noch als Reminiszenz an die Geschichte des Schützenwesens verwendet, wo es in verschiedenen Disziplinen des Vorderlader- und Westernschießens oder zum Böller- und Salutschießen (Böllerpulver) zum Einsatz kommt.
Erhältlich ist Schwarzpulver für den sportlichen oder jagdlichen Einsatz (als Jagdschwarzpulver) in verschiedenen Korngrößen die mit dem Buchstaben F (ersatzweise auch P) gekennzeichnet werden (Körnung in mm):
Mehlpulver (englisch meal) ist die Bezeichnung für nichtgekörntes Schwarzpulver.
Mehlpulver ist Schwarzpulver, welches nicht gekörnt wurde und sich so wenig für die Verwendung in Schusswaffen eignet. Wird es zusammengedrückt, verbrennt es nur langsam an der Oberfläche (wie z. B. in einer Rakete), ist es zu lose, kann es sich so schnell umsetzen, dass durch den rapiden Druckanstieg der Lauf gesprengt wird. Zudem gelangt das feine Mehlpulver oft nicht durch Einschütten bis zum Pulversack herunter, sondern bildet vorher einen Pfropfen, so dass die Waffe nicht funktionieren kann. Hinzu kommt, dass Mehlpulver die Eigenschaft hatte, sich beim Transport in den Fässern zu entmischen. Gerade auf den ruckeligen Pferdekarren kam es oft dazu, dass nach dem Transport die drei Grundbestandteile in Schichten vorlagen.
Mehlpulver wurde früher oft als Sprengpulver in Mörsern, in Brandkugeln oder als sogenanntes Zündkraut in Steinschloss-, Radschloss- oder Luntenschlosswaffen benutzt. Heute wird es in der Feuerwerkerei verwendet, um den Abbrand einzustellen und damit den Effekt passend zur Geltung zu bringen.
Schwarzpulver wird als Sprengpulver, je nach Verwendung, den Sprengstoffen oder auch den Schießstoffen bzw. den pyrotechnischen Chemikalien zugeordnet. Die sprengtechnischen Eigenschaften sind jedoch abhängig von der Restfeuchte, der Körnigkeit, der Durchmischung und der Zusammensetzung des Pulvers, sowie von der Ladungsmenge, der Verdämmung und der Einbringung der Ladung (Bohrloch oder aufgelegte Ladung).
Ein wichtiger Einsatzort ist im Steinbruch zur Gewinnung wertvoller Werksteine wie Marmor oder Granit. Aufgrund der stark zerstörenden Wirkung von Detonationssprengstoffen kommen diese dort nicht zum Einsatz. Da Sprengpulver nicht brisant ist, sondern schiebende Wirkung hat, wird das Gestein relativ schonend losgebrochen, man erhält Bruchstücke in verwendbarer Größe und es entstehen keine Haarrisse. Nach dem Aufkommen moderner Sägemethoden verliert dieses Verfahren jedoch zunehmend an Bedeutung.
In der Artillerietechnik als Verstärkerladung (Booster) in der Zündkette. Der Anzünder zündet primär eine Schwarzpulverladung, die die weiteren Ladungsbeutel mit NC-Pulver entzündet.
Dem Schwarz- und Schießpulver wurden diverse wundersame Eigenschaften nachgesagt. So gab es im Jägeraberglauben die Vorstellung, die Zumischung pulverisierter Tierbestandteile, z. B. von Schlangen, Würmern oder Vögeln, erhöhe die Kraft des Pulvers. In Wein gemischt, so ein Aberglaube unter Soldaten, der für Teile der Schweiz aus dem Jahr 1914 nachgewiesen ist, mache das Pulver mutig. Es wurde als wundärztliches Ätzmittel[27] verwendet, und in Flüssigkeiten gelöst und eingenommen oder aufgelegt, sollte es gegen Halsschmerz, Wechselfieber, Verstopfung, Krämpfe, oder Schnittwunden in der Human- und Tiermedizin helfen.[32]
Schwarzpulver unterliegt den allgemeinen rechtlichen Regelungen für pyrotechnische Gegenstände, da dieses Stoffgemisch als pyrotechnischer Satz gilt. Spezielle Regelungen für offenes und verbautes Schwarzpulver sind:
Erwerb, Besitz und Umgang sind dem geprüften Pyrotechniker oder Sprengberechtigten prinzipiell gestattet.

Das Kolosseum (antiker Name: Amphitheatrum Novum oder Amphitheatrum Flavium, italienisch: Colosseo, Anfiteatro Flavio) ist das größte der im antiken Rom erbauten Amphitheater, der größte geschlossene Bau der römischen Antike und weiterhin das größte je gebaute Amphitheater der Welt. Zwischen 72 und 80 n. Chr. errichtet, diente das Kolosseum als Austragungsort zumeist höchst grausamer und brutaler Veranstaltungen, die von Mitgliedern des Kaiserhauses zur Unterhaltung und Belustigung der freien Bewohner Roms und des römischen Reichs bei kostenlosem Eintritt ausgerichtet wurden. Heute ist die Ruine des Bauwerks eines der Wahrzeichen der Stadt und zugleich ein Zeugnis für die hochstehende Baukunst der Römer in der Antike.
Das erste steinerne Amphitheater Roms war das 29 v. Chr. eingeweihte Amphitheater des Statilius Taurus. Bis zu seiner Vernichtung durch den Großen Brand von Rom im Jahre 64 n. Chr. stand es auf dem Marsfeld und dürfte sich nicht wesentlich von den Amphitheatern außerhalb Roms unterschieden haben. Nach dem Brand errichtete Kaiser Nero am selben Standort nicht nur einen hölzernen Ersatz, sondern begann auch am Südhang des Hügels Esquilin eine neue Palastanlage, die Domus Aurea. Die Gärten der Domus Aurea umfassten auch den späteren Standort des Kolosseums in der Talsenke zwischen den Hügeln Oppius (Teil des Esquilin) und Palatin.
Um 72, wenige Jahre nach Neros Sturz, gab sein Nachfolger Vespasian, der die Macht in einem blutigen Bürgerkrieg errungen hatte, das Gebiet demonstrativ der römischen Öffentlichkeit zurück. Dort ließ er binnen weniger Jahre ein neues steinernes Amphitheater errichten, das nicht nur Neros Vorgängerbau auf dem Marsfeld, sondern alle bisherigen Arenen übertreffen sollte, um den Ruhm der neuen Herrscherdynastie der Flavier zu mehren. Nach einer Rekonstruktion der Bauinschrift des Kolosseums wurde seine Errichtung insbesondere aus der Beute des Jüdischen Krieges finanziert, unter anderem mit dem im Jahr 70 geplünderten Tempelschatz von Jerusalem.[1]
Das Gebäude, das ursprünglich dreigeschossig sein sollte, war beim Tod Vespasians im Jahr 79 fast vollendet. Es bestand aus drei übereinander angeordneten Arkadenreihen zu je 80 Bögen. Die Arkaden wurden durch Halbsäulen gegliedert: die zu ebener Erde in dorischer, die des zweiten Geschosses im ionischer und die des dritten Geschosses in korinthischer Ordnung. Angeblich auf Wunsch von Vespasians Sohn und Nachfolger Titus wurde den drei Rundbogengeschossen noch ein viertes Geschoss hinzugefügt, das nicht von Arkaden durchbrochen, sondern massiv gestaltet und nur von rechteckigen Fensternischen durchbrochen wurde. Die Außenmauern des Kolosseums wurden in römischem Travertin ausgeführt, im Inneren wurden jedoch die billigeren Ziegel und Tuff verwendet.
Nach seiner Fertigstellung im Jahr 80 wurde das Kolosseum dem Geschichtsschreiber Cassius Dio zufolge mit hunderttägigen Spielen eröffnet, unter anderem mit Gladiatorenkämpfen, nachgestellten Seeschlachten und Tierhetzen, bei denen 5000 Tiere in der Arena getötet wurden.
Das Kolosseum ist nicht nur eine architektonische Meisterleistung, sondern wird auch den logistischen Problemen eines derart riesigen Veranstaltungsareals gerecht.
80 Eingänge rund um die Arena ermöglichten den Zuschauern, auf direktem Weg zu ihren Plätzen zu gelangen. Vier von diesen Eingängen waren der obersten Schicht vorbehalten. Unter diesen befanden sich unter anderem der Kaiser, Senatoren, Vestalinnen und die männlichen Priester. Für diese bedeutenden Personen wurde ein eigens abgesichertes Podium am Rande der Arena errichtet.
Equites und normale Bürger benutzten die verbleibenden 76 der als Bögen gestalteten Eingänge. Die umlaufenden Korridore und die zahlreichen Treppen, die meist aus Marmor angefertigt wurden, führten das Publikum bis zur Höhe des dritten Geschosses, von wo aus sie ihre Plätze auf den Sitzreihen erreichten.
Auch heute noch bedient man sich beim Stadionbau dieses ausgeklügelten Systems, das es den Zuschauern möglich machte, die Arena in nur fünf Minuten zu räumen oder in 15 Minuten zu füllen. Die Erbauer gaben daher diesem System den Namen Vomitorium (von lateinisch vomere „erbrechen“).
Im Kolosseum konnten nach heutigen Berechnungen ca. 50.000 Zuschauer Platz finden. Das podium, die erste Reihe der Sitzplätze, war den römischen Senatoren vorbehalten. Auch die kaiserliche Loge (pulvinar) befand sich hier. Spezielle Plätze gab es auch für die Vestalinnen, die traditionsgemäß öffentlichen Schauspielen beiwohnten. Darüber lag das Maenianum primum, das dem Stand der Equites (Ritter) vorbehalten war. Die darüber befindlichen Reihen maenianum secundum waren in drei Sektoren unterteilt. Der unterste Sektor (imum) diente den wohlhabenden Bürgern, während der oberste Sektor (summum) den ärmsten Bewohnern Roms vorbehalten war. Schlechter waren nur noch die Frauen der untersten Schichten untergebracht. Für sie gab es Stehplätze auf einer Holzkonstruktion auf dem obersten Geschoss (maenianum summum in ligneis), das Titus anbauen ließ.[3]
Das Kolosseum ist ellipsenförmig gebaut. Seine Breite beträgt 156 Meter, die Länge 188 Meter, der Umfang 527 Meter, die Höhe 48 Meter. Auch der Boden der Arena war elliptisch mit einer Breite von 54 Metern und einer Länge von 86 Metern. Die runde Form sollte verhindern, dass Gladiatoren, zum Tode Verurteilte oder gejagte Tiere in einer Ecke Schutz suchen konnten. Den Boden der Arena bildeten Holzbohlen, die sich nach Bedarf entfernen ließen. Darunter befanden sich die Kellerräume und das 7 Meter dicke Fundament.
Am Außenrand des Obergeschosses wurden 240 senkrecht stehende Masten befestigt, an denen ein riesiges Velarium aufgezogen werden konnte, um den Innenraum zu beschatten. Dazu wurden Seesoldaten der bei Misenum (am Golf von Neapel) stationierten römischen Flotteneinheiten herangezogen.
Der Raum unterhalb des Arenabodens war ursprünglich nicht bebaut. Nach Entfernung der Holzbohlen konnte er geflutet werden, etwa für die Naumachien (Seeschlachten), wie sie Titus nachweislich zur Einweihung des Kolosseums aufführen ließ.
Man vermutet, dass die Arena bereits unter Titus’ Bruder und Nachfolger Domitian in verschiedene Kellerräume untergliedert wurde. Damit entstand das sogenannte hypogeum, ein System aus Räumen, Gängen und Versorgungsschächten. Hier befanden sich Kerker für die zum Tode Verurteilten, der unterirdische Zugang von der benachbarten Gladiatorenkaserne (Ludus Magnus), Käfige für wilde Tiere und die Einrichtungen der höchst komplizierten Bühnenmaschinerie wie Falltüren, Rampen und Aufzüge. Mit Hilfe eines komplexen Systems von Winden und Flaschenzügen konnten aufwändige Dekorationen und Bühnenbilder in die Arena befördert werden. Innerhalb weniger Minuten konnte sich zur Überraschung der Zuschauer beispielsweise eine komplette Wald- oder eine Wüstenlandschaft aus dem Boden erheben. Unklar ist, ob die Arena nun immer noch geflutet werden konnte.
Die Unterkellerung des Kolosseums
Seilwindenführungen mit Kontergewichten von einer Hebebühne aus dem 3. Jahrhundert.
Querschnittmodell vom Hypogäum des Kolosseums zur Funktionsweise der Bühnenmaschinerie.
Detailansicht zur Konstruktion der Seilwinden- und Aufzugtechnik mit den Kontergewichten.
Blick in die Unterkellerung des Kolosseums
Als Arena war das Kolosseum fast 450 Jahre lang in Betrieb, unterbrochen nur in den Jahren von 217 bis 238, als es nach einem durch Blitzschlag am 23. August 217[4] verursachten Brand renoviert werden musste.
Das Kolosseum war der Veranstaltungsort von in aller Regel höchst grausamen Spielen, die von Mitgliedern des Kaiserhauses ausgerichtet wurden und zu denen jeder freie Bewohner Roms kostenlos Zutritt hatte.
Üblich waren vor allem Gladiatorenkämpfe (munera) und Tierhetzen (venationes), wobei Kämpfe zwischen besonders exotischen Tieren am beliebtesten waren.
Umstritten ist, ob im Kolosseum auch die Exekution von Verurteilten durchgeführt wurde, vor allem jener, über die die damnatio ad bestias, der Tod durch wilde Tiere, verhängt worden war. Die Verurteilten wurden auch gezwungen, mit Waffen gegeneinander anzutreten, was einer damnatio ad ferrum entsprach. Die verbreitete Annahme, dass im Rahmen von Christenverfolgungen zahlreiche Märtyrer im Kolosseum auf diese Weise den Tod gefunden hätten, ist nicht durch antike Quellen belegt, und viele Forscher vermuten, dass die Hinrichtungen an anderer Stelle stattfanden (vgl. Sinn 2006).
Zu Beginn fanden Schiffskämpfe (Naumachiae) im Kolosseum statt, was aber nach der Unterkellerung der Arena nicht mehr möglich war.
Einige Historiker schätzen, dass im Laufe der Jahrhunderte etwa 300.000 bis 500.000 Menschen und noch lange nach ihnen viele Millionen Tiere im Kolosseum starben. Viele Gelehrte halten diese Zahlen aber für viel zu hoch gegriffen, da Gladiatorenkämpfe seltener tödlich ausgingen als oft vermutet.
Auch nach der Christianisierung des Römischen Reichs wurden in Rom, das die Rolle als Hauptresidenz zunächst an Trier, dann an Mailand und schließlich Ravenna (Weströmisches Reich) beziehungsweise Konstantinopel (Oströmisches Reich) verloren hatte, aber immer wieder von Kaisern aufgesucht wurde, zunächst weiter Gladiatorenspiele veranstaltet, die nun oft von reichen Senatoren finanziert wurden, zuletzt vermutlich 434/435. Denn Rom blieb weiterhin der Sitz des Senats, und von den Aristokraten wurde erwartet, dem Volk Unterhaltung zu bieten.
Bereits unter Kaiser Honorius (395–423) war die Spieltätigkeit eingeschränkt worden. Die Tierhetzen (venationes) blieben hingegen gestattet und wurden auch nach dem Ende des weströmischen Kaisertums unter der Herrschaft der Ostgoten fortgesetzt. Die letzte Hetze im Kolosseum, von der die Quellen berichten, fand 523 unter der Herrschaft Theoderichs des Großen statt.
Das Christentum lehnte die Spiele zwar ab, doch gab dies nicht den Ausschlag: erst wegen des rapiden Bevölkerungsrückgangs Roms während des 5. und 6. Jahrhunderts lohnte sich der Aufwand zuletzt nicht mehr. Zur Zeit der letzten Tierhetze war das Kolosseum bereits durch Erdbeben beschädigt worden, doch hatten Odoaker und die Ostgoten noch umfangreiche Reparaturen durchführen lassen. Spätestens nach den schweren Zerstörungen, die Rom während der Rückeroberungskriege des oströmischen Kaisers Justinian erlitten hatte, verfiel das Kolosseum. Da fortan endgültig kein Herrscher mehr in der Stadt residierte und auch der Senat bald nicht mehr existierte, wurde es auch nicht wieder renoviert.
Seit dem späteren 6. Jahrhundert nutzten die verbliebenen Bewohner der verfallenden Stadt die Arkaden und Gänge des Kolosseums, um Wohnräume darin einzurichten. Schwere Schäden entstanden durch zwei Erdbeben in den Jahren 847 und 1349. Im 12. Jahrhundert wurde die Arena zum Teil in die Stadtfestung des Adelsgeschlechts der Frangipani einbezogen. Während des ganzen Mittelalters bis in die Zeit der Renaissance und des Barocks wurde das Kolosseum aber von den herrschenden Familien Roms und den Päpsten immer wieder als Steinbruch für ihre Bauten genutzt. So blieb vom äußeren Ring der monumentalen, viergeschossigen Fassade nur die nördliche Hälfte erhalten.
Die allmähliche Zerstörung wurde erst beendet, nachdem Papst Benedikt XIV. das Kolosseum im 18. Jahrhundert zur geweihten Märtyrer-Stätte erklärte, einen Kreuzweg mit Kapellen darin einrichtete und durch Edikt von 1744 den Erhalt des Kolosseums anordnete. Hintergrund war die heute umstrittene Annahme (siehe oben), dass im Kolosseum zahllose Christen für ihren Glauben gestorben seien.
Inzwischen war das Monument überdies längst zur Sehenswürdigkeit für nordeuropäische Bildungsreisende geworden, die in ihm ein erhabenes Exempel für den Verfall einstiger Größe sahen. Im Mittelalter war das Wissen über den Bau so gering gewesen, dass man vielfach glaubte, es habe sich um einen überkuppelten Tempel für den Sonnengott gehandelt; doch in der Renaissance erkannte man den wahren Zweck des Gebäudes und bewunderte seither die Alten Römer für ihre Baukunst. Im 19. Jahrhundert wurde schließlich damit begonnen, den verfallenen Bau zu sichern und archäologisch zu erforschen. Durch Untersuchungen im Kellergeschoss unter der Arena konnte erst kürzlich die Funktionsweise der antiken Bühnentechnik geklärt werden. Von der historischen Forschung konnte der tatsächliche Tod von Christen im Kolosseum jedoch nicht bestätigt werden, die antiken Überlieferungen beziehen sich auf andere Orte wie z. B. den Circus des Nero.
Seit 1964 findet im Kolosseum an jedem Karfreitagabend ein Kreuzweg mit dem Papst statt.
Die von Mussolini verwirklichte Via dell’Impero führt direkt auf das Kolosseum zu und erzeugt so eine Sichtachse, die vielen Romfotos eine Bildachse mit dem Kolosseum in der Mitte liefert. Diese bereits in den römischen Regulierungsplänen von 1873 und 1883 geplante Straßenverbindung erscheint in ebener Lage unauffälliger als in den Plänen des 19. Jahrhunderts, die eine Führung als Viadukt vorsahen. Dem steht gegenüber, dass beim Bau (1924–1932) neben zahlreichen Wohngebäuden auch wertvolle antike Relikte einfach zerstört wurden – etwa die Basis von Neros Kolossalstatue und die Meta Sudans, der Rest einer antiken Brunnenanlage.
Seit Anfang der 1980er Jahre wurde in Rom diskutiert, die als Aufmarschstraße der faschistischen Bataillone bekannte ehemalige Via dell’Impero, heute die Via dei Fori Imperiali zu sperren oder sogar abzubauen. Der römische Straßenverkehr rund um das antike Monument setzte mit seinen Abgasen jahrzehntelang der Bausubstanz des Kolosseums ganz erheblich zu. Seit 2014 ist die Straße nördlich des Kolosseums für den privaten Autoverkehr gesperrt.
Die antike Bezeichnung Amphitheatrum Flavium leitet sich von den Kaisern der flavischen Dynastie her, in deren Herrschaftszeit das Kolosseum errichtet wurde.
Frühestens im 8. Jahrhundert lässt sich die Bezeichnung Kolosseum (vgl. das urspr. altgriechische kolossos) historisch belegen. Die überwiegend anerkannte Deutung dieses Namens geht von einer durch Zenodoros geschaffene Kolossalstatue des Kaisers Nero aus, die nach dessen Tod in eine Statue des Sonnengottes Sol umgewandelt und neben dem Amphitheater aufgestellt wurde. Dieser Colossus, der mindestens bis zum 4. Jahrhundert gestanden hat, dürfte der Arena den Namen gegeben haben.
Auszuschließen ist, dass die römische Bevölkerung des Mittelalters den Bau einfach wegen seiner kolossalen Ausmaße Colosseo genannt hat, da das italienische Wort colosso für „Koloss“ erst seit dem 15. Jahrhundert in Gebrauch ist.
Das Kolosseum dient seit dem Jahr 1999 als Monument gegen die Todesstrafe. Immer wenn ein Todesurteil ausgesetzt wird oder ein Staat dieser Welt die Todesstrafe abschafft, wird das Kolosseum 48 Stunden lang in bunten Farben angestrahlt. Getragen wird die Aktion von der italienischen Regierung und verschiedenen Menschenrechtsgruppen, darunter Amnesty International und der katholischen Gemeinschaft Sant’Egidio.[5]
Das Kolosseum hat seit jeher großen Eindruck auf die Menschen gemacht. Beda Venerabilis prägte bereits im 8. Jahrhundert den Satz dum colosseum stabit, Roma stabit; dum Roma stabit, mundus stabit (dt. solange das Kolosseum steht, wird Rom stehen, solange Rom steht, wird die Welt bestehen).
Beispiele für die moderne Verwendung des Kolosseums als Filmkulisse sind etwa der Endkampf zwischen Bruce Lee und Chuck Norris in dem Kampfsportfilm Die Todeskralle schlägt wieder zu von 1972 oder der Film Gladiator aus dem Jahr 2000, für den das Bauwerk in einem Teilausschnitt auf Malta rekonstruiert und später computergestützt vervollständigt wurde. Im Jahr 2006 fanden im Kolosseum Aufnahmen zum Film Jumper statt.
In Asterix – Sieg über Cäsar zerstört Obelix Teile des Kolosseums. Das anschließende Aussehen danach erinnert an die heutigen Überreste des Kolosseums. Die Differenz zwischen den Handlungszeitraum der Asterix-und-Obelix-Geschichten (50 v. Chr.) und der Eröffnung des Kolosseum (80 n. Chr.) beträgt allerdings 130 Jahre.
Die markante Ruine des Amphitheaters ist zum Wahrzeichen Roms geworden und wird in symbolhaft abgekürzter Darstellung meist als Symbol für die ganze Stadt aufgefasst. Die Verwendung des Icons brennendes Kolosseum für die Software Nero Burning ROM ist allerdings historisch nicht korrekt, denn dem römischen Kaiser Nero wurde zwar von seinen Feinden fälschlich vorgeworfen, der Verursacher des Großen Brandes von Rom (im Jahr 64) gewesen zu sein, das Kolosseum in seiner heutigen Größe und Form wurde jedoch erst gegen 79 von einem seiner Nachfolger Vespasian auf dem Areal von Neros einstigem Palast erbaut.
Dieser Artikel ist als Audiodatei verfügbar:
Mehr Informationen zur gesprochenen Wikipedia
41.8912.491944444444Koordinaten: 41° 53′ 24″ N, 12° 29′ 31″ O

Als Oper (von italienisch opera in musica, „musikalisches Werk“) bezeichnet man seit 1639[1] eine um 1600 (mit Beginn des Barockzeitalters) entstandene musikalische Gattung des Theaters. Ferner werden auch das Opernhaus (die Aufführungsstätte oder produzierende Institution) oder die aufführende Kompagnie als Oper bezeichnet.
Eine Oper besteht aus der Vertonung einer dramatischen Dichtung, die von einem Sängerensemble, einem begleitenden Orchester sowie manchmal von einem Chor und einem Ballettensemble ausgeführt wird. Neben dem Gesang führen die Darsteller Schauspiel und Tanz auf einer Theaterbühne aus, die mit den Mitteln von Malerei, Architektur, Requisite, Beleuchtung und Bühnentechnik gestaltet ist. Die Rollen der Darsteller werden durch Maske und Kostüme optisch verdeutlicht. Als künstlerische Leitung betätigen sich Dirigenten für das Musikalische, Regisseure für die Personenführung sowie Bühnen– und Kostümbildner für die Ausstattung. Im Hintergrund unterstützt sie die Dramaturgie.
Die Oper wird mit Tanz, Musical und Operette unter dem Begriff Musiktheater zusammengefasst.[2] Die Grenzen zu verwandten Kunstwerken sind fließend und definieren sich in jeder Epoche, meist auch im Hinblick auf bestimmte nationale Vorlieben, immer wieder neu. Auf diese Art bleibt die Oper als Gattung lebendig und erhält immer wieder neue Anregungen aus den verschiedensten Bereichen des Theaters.
Schauspiele in dem strengen Sinne, dass auf der Bühne nur gesprochen würde, sind in der Theatergeschichte selten. Mischformen aus Musik, Rezitation und Tanz waren die Regel, auch wenn sich zu manchen Zeiten Literaten und Theaterleute um eine Rettung oder Reform des Schauspiels bemüht haben. Seit dem 18. Jahrhundert sind Mischformen zwischen Schauspiel und Oper aus den verschiedenen Spielarten der Opéra-comique hervorgegangen, wie Ballad Opera, Singspiel oder Posse mit Gesang. Die Singspiele Mozarts werden der Oper zugerechnet, diejenigen Nestroys gelten als Schauspiele. Auf der Grenze bewegen sich z. B. auch die Werke von Brecht/Weill, deren Dreigroschenoper dem Schauspiel näher steht, während Aufstieg und Fall der Stadt Mahagonny eine Oper ist. Sich dem Schauspiel völlig unterordnende Musik bezeichnet man als Schauspielmusik.
Eine verbreitete, dem Schauspiel verwandte Theaterform seit dem Beginn des 19. Jahrhunderts war das Melodram, das heute nur noch im populären Film gegenwärtig ist. Es hatte mit seinen Abenteuerstoffen großen Einfluss auf die Oper in jener Zeit. Stellenweise enthielt es Hintergrundmusik als Untermalung der Bühnenhandlung (weniger des gesprochenen Texts). Darauf bezieht sich der heute noch bekannte Begriff Melodram. Eine solche Untermalung findet sich zum Beispiel in Mozarts Idomeneo, Ludwig van Beethovens Fidelio, in Webers Der Freischütz (in der Wolfsschluchtszene) und in Humperdincks Königskinder.
In französischer Tradition war der Tanz seit dem Barock in die Oper integriert. Das klassische Ballett löste sich im 19. Jahrhundert mühevoll aus dieser Verbindung, aber in neoklassizistischen Werken des 20. Jahrhunderts, beispielsweise bei Igor Strawinsky oder Bohuslav Martinů, bestätigt sich die Verwandtschaft von Oper und Ballett erneut. Auch die italienische Oper war nicht frei von Tanz, wenn auch der Tanz nicht im gleichen Maß dominierte. Heute werden die Ballette und Divertissements der Repertoirewerke meist aus den Partituren gestrichen, sodass der Eindruck einer Spartentrennung entsteht.
Das Genre der Operette und verwandter Formen wie der Zarzuela grenzt sich als Weiterentwicklung aus dem Singspiel durch die gesprochenen Dialoge, aber auch durch dessen vorherrschenden Unterhaltungsanspruch und das vorrangige Bemühen um Popularität oder kommerziellen Erfolg von der ab der Mitte des 19. Jahrhunderts zunehmend durchkomponierten Oper ab. Diese Abgrenzung entstand erst im ausgehenden 19. Jahrhundert: Als die „komische Oper“ vom „niederen“ zum „hohen“ Genre geworden war, bildete sich die Operette als neues „niederes“ Genre. Ähnliches gilt für das Musical, die Weiterentwicklung des populären Musiktheaters in den Vereinigten Staaten. Operette und Musical sind gleichwohl in nicht geringerem Maße Kunstformen als die Oper.
Bereits im Theater der griechischen Antike verband man szenische Aktion mit Musik. Die Oper der Neuzeit berief sich immer wieder auf dieses Vorbild und konnte es, weil von der Aufführungspraxis wenig überliefert ist, auf unterschiedlichste Weise deuten. Ein Chor, der sang und tanzte, hatte eine tragende Rolle, indem er das Drama in Episoden gliederte oder auch die Aufgabe hatte, die Handlung zu kommentieren. Die Römer pflegten eher die Komödie als die Tragödie. Mimus und später Pantomimus hatten einen hohen Musikanteil. Durch die Zerstörung der römischen Theater im 6. Jahrhundert und die Bücherverluste in der Spätantike sind viele Quellen darüber verloren gegangen.
Jedoch werden seit Beginn des 20. Jahrhunderts zahlreiche antike Bauten, insbesondere Amphitheater und Theaterbauten, für Opernaufführungen genutzt. Die bekanntesten sind das Théâtre Antique in Orange (mit Unterbrechungen seit 1869), die Arena di Verona (seit 1913), das Odeon des Herodes Atticus in Athen (seit den 1930er Jahren), die Thermen des Caracalla in Rom (seit 1937) und der Römersteinbruch St. Margarethen (seit 1996).
Im Hochmittelalter entstand ausgehend vom Gottesdienst der Ostermesse eine neue Tradition gesungener Handlung. Das geistliche Spiel fand zunächst in der Kirche, im 13. Jahrhundert dann als Passionsspiel oder Prozessionsspiel außerhalb der Kirche statt. Beliebte Themen waren das biblische Oster- und Weihnachtsgeschehen, auch mit komödiantischen Einlagen. Die Melodien sind oft überliefert, der Einsatz von Musikinstrumenten ist wahrscheinlich, aber selten belegbar. Im höfischen Bereich gab es weltliche Stücke wie Adam de la Halles melodienreiches Jeu de Robin et de Marion (1280).
Die Zeit des Karnevals, die später zur traditionellen Opernsaison wurde, bot seit dem 15. Jahrhundert Gelegenheit zu musikalisch-theatralischen Aktionen, die von den damals größten europäischen Städten in Italien ausgingen: Intermedien, Tanzspiele, Masken- und Triumphaufzüge gehören zur städtischen Repräsentation in der italienischen Renaissance. Das Madrigal war die wichtigste Gattung der Vokalmusik und verband sich oft mit Tänzen.
Der Königshof in Frankreich gewann im 16. Jahrhundert gegenüber Italien an Bedeutung. Das Ballet comique de la reine 1581 war eine getanzte und gesungene Handlung und gilt als bedeutender Vorläufer der Oper.
Ein früher Versuch in Deutschland, eine dramatische Handlung mit singenden Protagonisten in einem Bühnenbild aufzuführen, ist die Aufführung von Orpheus und Amphion auf einer Simultanbühne anlässlich der Jülichschen Hochzeit von Johann Wilhelm von Jülich-Kleve-Berg mit Markgräfin Jakobe von Baden in Düsseldorf 1585. Als möglicher Komponist der nicht überlieferten Musik wird Andrea Gabrieli genannt. Die Musik sei so schön gewesen, „daß es denselben / so dazumahl nit zugegen gewesen / und solchen Musicum concentum & Symphoniam gehört haben / onmüglich zu glauben.“ Die Handlung war freilich primär eine Allegorese im Sinne eines Fürstenspiegels.
Die Oper im heutigen Sinn entstand Ende des 16. Jahrhunderts in Florenz. Eine wichtige Rolle in der Entstehungsgeschichte spielte die Florentiner Camerata, ein akademischer Gesprächskreis, in dem sich Dichter (z. B. Ottavio Rinuccini), Musiker, Philosophen, Adelige und ein Kunstmäzen – zunächst übernahm Graf Bardi diese Rolle, später Graf Corsi – zusammenfanden. Diese Humanisten versuchten, das antike Drama wiederzubeleben, an dem ihrer Meinung nach Gesangssolisten, Chor und Orchester beteiligt waren. Nach den Pastoraldramen des 16. Jahrhunderts wurde das Libretto gestaltet und mit den musikalischen Mitteln der Zeit (→ Barockmusik, Barockoper) in Musik gesetzt.
Vincenzo Galilei gehörte dieser Gruppe an. Er entdeckte (heute verlorene) Hymnen des Mesomedes und schrieb ein Traktat gegen die niederländische Polyphonie. Dies war ein deutlicher Beweis für den gewünschten musikalischen Stil, den damals neuen Sologesang mit Instrumentalbegleitung.
Textverständlichkeit der Vokalmusik war für die Florentiner Camerata das Wichtigste. Eine klare, einfache Gesangslinie wurde zum Ideal erklärt, der sich die sparsame Generalbass-Begleitung mit wenigen und leisen Instrumenten wie Laute oder Cembalo unterzuordnen hatte. Großartig ausgearbeitete melodische Einfälle waren unerwünscht, um den Inhalt der Worte nicht durch den Gesang zu verschleiern. Man sprach sogar von einer „nobile sprezzatura del canto“ (Giulio Caccini: Le nuove musiche, 1601), einer „noblen Verachtung des Gesangs“. Diese Art des Singens nannte man recitar cantando, rezitierenden Gesang. Die Schlichtheit und Beschränkung des recitar cantando steht im Gegensatz zur vorherrschenden Polyphonie mit ihren komplexen Ton- und Textschichtungen. Mit der Monodie, wie man diesen neuen Stil in Anlehnung an die Antike nannte, sollte das Wort wieder zu seinem vollen Recht kommen. Es entwickelte sich eine Theorie der Affekte, die durch den gesungenen Text transportiert werden konnten. Zur Monodie der einzelnen Gesangsstimme gesellten sich Chöre in Madrigalform oder als Motette. Das Orchester spielte dazwischen Ritornelle und Tänze.
Als erstes Werk der Gattung Oper gilt La Dafne von Jacopo Peri (Uraufführung 1598) mit einem Text von Ottavio Rinuccini, von der nur einzelne Fragmente erhalten geblieben sind. Weitere bedeutende Werke aus der Anfangszeit sind Peris Euridice (1600) als älteste erhaltene Oper, sowie Euridice (1602) und Il Rapimento di Cefalo (1602) von Giulio Caccini. Stoffe dieser frühen Opern entnahm man der Schäferdichtung und vor allem der griechischen Mythologie. Wunder, Zauber und Überraschungen, dargestellt durch aufwändige Bühnenmaschinerie, wurden zu beliebten Bestandteilen.
Besondere Beachtung fand Claudio Monteverdis erste Oper L’Orfeo (1607). Sie wurde anlässlich des Geburtstags von Francesco IV. Gonzaga am 24. Februar 1607 in Mantua uraufgeführt. Hier sind im Vergleich zu seinen Vorgängern erstmals ein reicheres Instrumentarium (wenngleich es in der Partitur meist nur angedeutet ist), ausgebaute Harmonik, tonmalerisch-psychologische und bildhafte Ausdeutung von Worten und Figuren sowie eine die Personen charakterisierende Instrumentation zu hören. Posaunen werden zum Beispiel für die Unterwelt- und Todesszenen eingesetzt, Streicher bei Schlafszenen, für die Hauptfigur Orfeo kommt eine Orgel mit Holzregistern (organo di legno) zum Einsatz.
Monteverdi erweitert die Gesangslinie des recitar cantando zu einem mehr arienhaften Stil und gibt den Chören größeres Gewicht. Seine Spätwerke Il ritorno d’Ulisse in patria (1640) und L’incoronazione di Poppea (1643) sind in Hinblick auf ihre Dramatik Höhepunkte der Operngeschichte. Noch in dieser letzten Oper Monteverdis, L’incoronazione di Poppea, findet man den Prolog durch drei allegorische Figuren dargestellt, in der Fortuna die Virtù (Tugend) verspottet. Die übrige Handlung spielt in der irdischen Welt um den römischen Kaiser Nero, dessen ungeliebte Gattin Ottavia und Poppea, die Gattin des Prätors Ottone. Diese wird Neros Gattin und Kaiserin. Neros brutaler Charakter wird von einem Kastraten und entsprechend virtuoser Musik dargestellt, Ottone wirkt dagegen weich, und Neros würdiger Lehrer und Berater Seneca bekommt die Bassstimme zugewiesen. Belcanto-Gesang und Koloraturreichtum werden für den Adel und für Göttergestalten eingesetzt, für die übrigen Personen schlichtere Ariosi und Lieder.
1637 wurde das Teatro San Cassiano in Venedig als erstes öffentliches Opernhaus eröffnet. In schneller Folge entstanden neue Spielstätten, und Venedig wurde mit seiner „venezianischen Oper“ zum Opernzentrum Norditaliens. Historische Darstellungen verdrängten bald die mythischen Stoffe, wie in der Oper L’incoronazione di Poppea (1642), die noch den Namen Claudio Monteverdis trägt, wobei die Forschung seit Alan Curtis darüber diskutiert, ob es sich vielmehr um ein Pasticcio handle, das sich den berühmten Namen zu Nutze machte.[3]
Das Publikum dieser Opern setzte sich vornehmlich aus Angehörigen der nichtadeligen Stände zusammen. Den Spielplan bestimmte der geldgebende Adel auf Grund des Publikumsgeschmacks. Die aus den Akademien hervorgegangene Oper wurde in diesem Zusammenhang kommerzialisiert und vereinfacht, das Orchester reduziert. Die Da-capo-Arie mit vorangestelltem Rezitativ prägte für lange Zeit den Sologesang, Chöre und Ensembles wurden gekürzt. Verwechslungen und Intrigen bildeten das Grundgerüst der Handlungen, die mit komischen Szenen der beliebten Nebenfiguren angereichert wurden. Francesco Cavalli und Antonio Cesti waren die bekanntesten venezianischen Opernkomponisten in der auf Monteverdi folgenden Generation. Die Schriftsteller Giovanni Francesco Busenello und Giovanni Faustini galten als stilbildend und wurden häufig nachgeahmt.
Zum zweiten, stärker vom Geschmack der Aristokratie geprägten Opernzentrum Italiens wurde seit den 1650er Jahren die Großstadt Neapel. Als Begründer der neapolitanischen Oper gilt der Komponist Francesco Provenzale. In der folgenden Generation wurde Alessandro Scarlatti zum Vorreiter der neapolitanischen Schule.
Die Librettisten erhielten ihr Geld durch den Verkauf von Textbüchern, die zusammen mit Wachskerzen zum Mitlesen vor der Vorstellung verteilt wurden. Lange Zeit blieb die Literatur des Renaissance-Humanismus Vorbild der italienischen Operntexte.
Opern wurden nur zu bestimmten Spielzeiten (ital.: stagione) gegeben: während des Karnevals, von Ostern bis zur Sommerpause sowie vom Herbst bis zum Advent. In der Passions- und Adventszeit wurden stattdessen Oratorien gespielt. In Rom erhielten nicht nur Maschineneffekte und Chöre ein größeres Gewicht, sondern auch geistliche Stoffe.
In Paris entwickelte Jean-Baptiste Lully zusammen mit seinem Librettisten Philippe Quinault eine französische Variante der Oper, deren herausragendstes Merkmal neben den Chören das Ballett ist. Lully verfasste eine französische Version von Cavallis L’ercole amante (1662), in die er Ballette einfügte, die größeren Beifall fanden als die Oper. Cadmus et Hermione (1673) wird als erste Tragédie lyrique angesehen und blieb modellhaft für die nachfolgenden französischen Opern.
Die aus Italien importierte Oper wurde von der Tragédie lyrique zurückgedrängt. Dennoch versuchten Lullys Nachfolger Marc-Antoine Charpentier und André Campra, französische und italienische Stilmittel zu verbinden.
Ausgehend von italienischen Vorbildern, entwickelte sich bereits gegen Mitte des 17. Jahrhunderts eine eigenständige Operntradition innerhalb des deutschen Sprachgebietes, die auch die Verwendung deutschsprachiger Libretti mit einschließt.
Die erste Oper eines „deutschen“ Komponisten war 1627 die (verschollene) Dafne von Heinrich Schütz, der die Musikform der Oper bei seinem Studienaufenthalt 1609–1613 in Italien kennengelernt hatte. 1644 entstand die erste erhaltene deutschsprachige Oper von Sigmund Theophil Staden nach einem Libretto von Georg Philipp Harsdörffer Das geistlich Waldgedicht oder Freudenspiel, genannt Seelewig, ein pastorales Lehrstück in starker Nähe zum moralisierenden Schuldrama der Renaissance.
Kurz nach dem Dreißigjährigen Krieg etablierten sich auch im deutschsprachigen Raum Opernhäuser zunehmend als zentrale Versammlungs- und Repräsentationsorte der führenden Gesellschaftsschichten. Eine zentrale Rolle spielten dabei die führenden Fürsten- und Königshäuser, die sich zunehmend eigene Hoftheater samt der zugehörigen Künstler leisteten, die in der Regel auch für die (wohlhabende) Öffentlichkeit zugänglich waren. So erhielt München sein erstes Opernhaus 1657, Dresden 1667.
Bürgerliche, d. h. durch Städte und/oder private bürgerliche Akteure finanzierte „öffentliche und populäre“ Opernhäuser wie in Venedig existierten hingegen lediglich in Hamburg (1678), Hannover (1689) und Leipzig (1693). Im bewussten Gegensatz zum durch italienischsprachige Opern dominierten Betrieb an den „adligen“ Häusern, setzte insbesondere die Hamburger Oper am Gänsemarkt als ältestes bürgerliches Opernhaus Deutschlands bewusst auf deutschsprachige Werke und Autoren. So Händel, Keiser, Mattheson und Telemann. Jene etablierten bereits ab Beginn des 18. Jahrhunderts unter Verwendung deutschsprachiger Libretti von Dichtern wie Elmenhorst, Feind, Hunold und Postel eine eigenständige deutschsprachige Opern- und Singspieltradition. Die Bedeutung Hamburgs für die Entwicklung einer eigenständigen deutschsprachigen Operntradition unterstreichen auch die beiden zeitgenössischen Schriften zur Theorie der Oper: Heinrich Elmenhorsts Dramatologia (1688) und Barthold Feinds Gedancken von der Opera (1708).
In England verbreitete sich die Oper erst relativ spät. Die vorherrschende musikalische Theaterform in der Zeit des Elisabethanischen Theaters war die Masque, eine Kombination aus Tanz, Pantomime, Sprechtheater und musikalischen Einlagen, bei denen der vertonte Text meist nicht in unmittelbarem Zusammenhang mit der Handlung stand. Im Anschluss an das puritanische Verbot von Musik- und Theateraufführungen von 1642 begründete erst die Stuart-Restauration ab 1660 wiederum ein Theaterleben, in das die Oper integriert wurde.
Ein in jeder Hinsicht singuläres Werk ist Henry Purcells knapp einstündige Oper Dido and Aeneas (Uraufführung vermutlich 1689, Libretto: Nahum Tate). Der Komponist greift darin Elemente der französischen und der italienischen Oper auf, entwickelt jedoch eine eigene Tonsprache, die sich vor allem dadurch auszeichnet, dass sie sehr eng am Text bleibt. Chorpassagen und tänzerische Abschnitte stehen den ariosen Passagen der Hauptfiguren gegenüber, die fast ohne arienartige Formen auskommen. Die wechselnden Stimmungen und Situationen werden mit musikalischen Mitteln genau wiedergegeben; die Schlussszene, wenn die karthagische Königin Dido aus unglücklicher Liebe zu dem trojanischen Helden Aeneas an gebrochenem Herzen stirbt, gehört zum Bewegendsten der Opernliteratur.
Im Laufe des 18. Jahrhunderts bilden sich zwei Operntypen heraus: Neben der etablierten Opera seria als vorwiegend vom Repräsentations- und Legitimationsbedürfnis des Adels getragene Form, die mehrheitlich auf mythologischen oder historischen Stoffen basiert und deren Personal aus Göttern, Halbgöttern, Heroen, Fürsten sowie deren Geliebten und ihrer Dienerschaft besteht, entwickelt sich um 1720 die Opera buffa mit zunächst grobschlächtig komischen Handlungen, die sich zu bürgerlich-sentimentalen entwickeln.
Eine Konkurrenz zu den italienischen Opern bilden in Frankreich einerseits die höfische Tragédie lyrique, mit ihrem im Vergleich zu älteren italienischen Opern volleren Instrumentarium, und andererseits die Opéra-comique, die vom Pariser Jahrmarktstheater herstammt. Diese Gattungen regen auch außerhalb Frankreichs Opernaufführungen in der eigenen Landessprache an, als einheimisches Gegengewicht zu den allgegenwärtigen italienischen Gesangsvirtuosen.
Stilprägend wurde die im zweiten Viertel des 18. Jahrhunderts von Italien ausgehende Tendenz, aus dem ursprünglichen Dramma per musica ein Arienkonzert bzw. eine Nummernoper mit festgelegtem Inhalt und Musik zu machen. Eine weitere zentrale Entwicklung während der ersten Hälfte des 18. Jahrhunderts ist die Einteilung der auf fünf Teile angewachsenen Da-capo-Arien mit der Abfolge AA'–B–AA' in spezifische Untergruppen:
Der Star des Abends konnte zudem eine virtuose Aria baule („Koffer-Arie“) einschieben, die mit der Handlung nichts zu tun hatte. Solche Arien konnten leicht vertauscht oder mehrfach eingesetzt werden. Der Belcanto-Gesang wurde zu einer Präsentation virtuoser Gesangstechniken, die extreme Spitzentöne, geschmeidige Triller und weite Sprünge umfassten.
Weil im 18. Jahrhundert das Konzept der Werktreue noch nicht etabliert war und Auftraggeber und Publikum stets neue, noch nie gehörte Opern wünschten, und weil vielen Opernkompanien häufig nur begrenzte Ressourcen an Instrumentalisten und Sängern zur Verfügung standen, bestand eine weitverbreitete Aufführungspraxis des 18. Jahrhunderts darin, Arien und Ensembles aus verschiedenen Werken je nach vorhandener Besetzung möglichst wirkungsvoll zusammenzustellen und eine solche Abfolge musikalischer Nummern mit neuen Texten und einer neuen Handlung zu unterlegen. Diese Art von Opern nannte man Pasticcio; ein Opernpasticcio konnte sowohl aus der Feder eines einzigen Komponisten stammen, der vorhandene Nummern aus früheren Werken wiederverwendete, als auch aus Werken verschiedener Komponisten zusammengesetzt sein. Diese Praxis führte dazu, dass Handlung und Stimmung einer Opernaufführung bis zum Ende des 18. Jahrhunderts – an einigen Aufführungsorten auch bis in die 1830er Jahre hinein – nicht festgelegt waren und ständigen Anpassungen, Wandlungen und Veränderungen unterlagen. Die Praxis des Pasticcio bedeutete, dass bis zum Beginn des 19. Jahrhunderts kaum eine Aufführung des gleichen Werks musikalisch oder inhaltlich einer vorhergehenden glich.
Das daraus folgende Handlungschaos – erzeugt von der Strategie, unterschiedlichen Erwartungen zugleich gerecht zu werden – stieß die italienischen Librettisten Apostolo Zeno und Pietro Metastasio ab. Als Gegenmaßnahme verzichteten sie ab den späten 1730er Jahren zunehmend auf überflüssige Seitenhandlungen, mythische Allegorien und Nebenfiguren und bevorzugten stattdessen eine klare, nachvollziehbare Handlung und Sprache. Damit schufen sie die Grundlage für einen „ernsteren“ Operntypus jenseits der bis dahin üblichen Aufführungspraxis der Opera seria. Das zu diesem Zweck entwickelte Handlungsschema verwickelt die Hauptfiguren nach und nach in ein scheinbar unlösbares Dilemma, das sich zum Schluss durch einen unverhofften Einfall zum Guten wendet (lieto fine). Auch dichterisch leiteten beide Autoren eine Erneuerung der Oper ein. Gegen die Beliebigkeit des Pasticcio nummerierten sie die musikalischen Teile, wodurch deren Austausch erschwert wurde. So trugen sie wesentlich zur Herausbildung der Nummernoper mit ihrer festgelegten Abfolge bei. Als in sich geschlossenes Werk mit stringenter Handlung konnte sich die Oper nunmehr gegenüber dem Schauspiel behaupten.
Die Gattung der Opera buffa entstand gleichzeitig in Neapel und Venedig als zumeist heiterer und lebensnaher Operntypus. Einerseits gab es selbstständige musikalische Komödien, andererseits die komischen Intermezzi zur Opera seria anfangs der 1730er Jahre, aus der Apostolo Zeno und Pietro Metastasio die komischen Elemente ausgeschlossen hatten, sodass sie auf Einlagen zwischen den Akten beschränkt werden mussten. Als stilprägende Werke gelten die Oper Lo frate ’nnamorato von Giovanni Battista Pergolesi, uraufgeführt am 28. September 1732 im Teatro dei Fiorentini in Neapel, und die ab Mitte der 1740er Jahre in Venedig uraufgeführten Werke Baldassare Galuppis, die in enger Zusammenarbeit mit Carlo Goldoni entstanden.
Inhaltlich schöpfte die Opera buffa aus dem reichen Fundus der Commedia dell’arte. Die Handlungen waren oft Verwechslungskomödien, deren Personal aus einem adligen Liebespaar und zwei Untergebenen, oft Magd und Diener, bestand. Letztere können im Unterschied zur Opera seria als Hauptakteure auftreten, womit sich ein bürgerliches und subbürgerliches Publikum identifizieren konnte. Die Opera buffa wurde aber auch von der Aristokratie geschätzt, die ihre Provokationen kaum ernst nahm.
Seit Mitte des 18. Jahrhunderts begann eine Verlagerung der Komik in der Opera buffa auf alltagsweltliche und gegenwartsbezogene Handlungen, in denen Adlige nicht mehr unangreifbar waren. Mozarts Don Giovanni (1787) wurde zunächst als Opera buffa angesehen und erst im 19. Jahrhundert uminterpretiert, als das Schicksal der bürgerlichen Verführten ernst genommen und der adlige Verführer als Schurke betrachtet werden konnte.
Ausdruck dieser Veränderungen ist die Weiterentwicklung der Opera buffa zum Typus der Opera semiseria Ende des 18. Jahrhunderts, weil ein bürgerliches Publikum sich auf der Bühne nicht mehr verlacht sehen wollte. Die Alltagsnähe der Opera buffa und ihres französischen Gegenstücks, der Opéra-comique, besaß in der zweiten Hälfte des 18. Jahrhunderts soziale Sprengkraft. Damit im Zusammenhang stand der von 1752 bis 1754 in Frankreich ausgetragene Buffonistenstreit. Jean-Jacques Rousseau schätzte den bürgerlich geprägten „heiteren“ Operntypus mehr als die Tragédie lyrique der Hocharistokratie. Seine Verurteilung der französischen Oper zu Gunsten der italienischen führte zu wütenden Reaktionen.
Im englischen Sprachraum wurde Georg Friedrich Händel (anglisiert George Frideric Handel) zu einem der produktivsten Opernkomponisten (mehr als 45 Opern). Sein Wirken in London hatte nicht den gewünschten geschäftlichen Erfolg, u. a. wegen der starken Konkurrenz des berühmten Kastraten Farinelli, der in der rivalisierenden Operntruppe sang, und ruinöser Gagen für die engagierten Primadonnen. Im 20. Jahrhundert sind vor allem Alcina, Giulio Cesare und Serse wieder in die Spielpläne gekommen, in den letzten Jahrzehnten auch viele andere Händel-Opern (u. a. Ariodante, Rodelinda, Giustino). Nachdem im Zuge der Alte-Musik-Bewegung die historische Aufführungspraxis immer besser erforscht worden war, entstanden auch an den großen Opernhäusern stilbildende Produktionen unter Mitwirkung von Barock-Spezialisten.
Frankreichs Pendant zur in Paris umstrittenen Opera buffa wurde die Opéra-comique. Die Rezitative wurden durch gesprochene Dialoge ersetzt. Auch dieses Modell fand im Ausland Erfolg. Die neue Einfachheit und Lebensnähe schlägt sich auch in kleineren Arietten und nouveaux airs, die im Unterschied zu den allseits bekannten Vaudevilles neu komponiert wurden, nieder.
1752 erlebte Frankreich eine neue Konfrontation zwischen der französischen und der italienischen Oper, die unter dem Namen Buffonistenstreit in die Geschichte einging. Giovanni Battista Pergolesis Oper La serva padrona (deutsch: Die Magd als Herrin) war der Anlass dafür. Gegen die Künstlichkeit und Stilisierung der herkömmlichen französischen Adelsoper waren vor allem Jean-Jacques Rousseau und Denis Diderot, die sich gegen die Kunst und Stilisierung Rameaus zur Wehr setzten. Rousseau verfasste neben der bewusst einfach gestalteten Oper Le devin du village (deutsch: Der Dorfwahrsager) auch ein preisgekröntes Traktat mit dem Titel Discours sur les sciences et les arts (1750), in dem er ein von Wissenschaft und Kultur unverdorbenes Leben zum Ideal erklärt. Weitere Musikartikel schrieb er für die berühmte umfassende Encyclopédie der französischen Aufklärung. Der Buffonistenstreit ging schließlich zu Ungunsten der italienischen Operntruppe aus, die aus der Stadt vertrieben wurde. Somit war der Streit zwar vorläufig beendet, an Beliebtheit stand die Grand opéra aber immer noch hinter der Opéra comique zurück.
Die Schließung der Oper am Gänsemarkt im Jahr 1738 führte zu einer weiteren Stärkung des zu diesem Zeitpunkt bereits dominanten italienischsprachigen Opernbetriebs im deutschen Sprachraum. Dennoch etablierte sich – ausgehend vom Hamburger Vorbild – ab Mitte des 18. Jahrhunderts zunehmend die Praxis bei Aufführungen französischer und italienischer Opern die Rezitative ins Deutsche zu übersetzen und – aus vorwiegend musikalischen Gründen – lediglich bei den Arien die Originalsprache beizubehalten. Auch wurden ab Mitte des 18. Jahrhunderts der Verkauf oder die Verteilung gedruckter Erläuterungen und Übersetzungen nicht-deutschsprachiger Werke in deutscher Sprache an das Publikum mehr und mehr üblich.
Um 1780 setzt mit dem Werk Wolfgang Amadeus Mozarts schließlich eine bis weit ins 19. Jahrhundert reichende Entwicklung ein, die zur zunehmenden Verdrängung des bis dahin dominierenden Italienischen zugunsten deutschsprachiger Werke und Aufführungen in deutscher Übersetzung führte. Dabei fand Mozart seinen ganz eigenen Weg, mit der Tradition der italienischen Oper umzugehen. Er reüssierte bereits in jugendlichen Jahren mehrfach in Italien (u. a. mit Lucio Silla und Mitridate, re di Ponto) und komponierte mit Idomeneo (1781), einer ebenfalls auf Italienisch geschriebenen Opera seria, für München sein erstes Meisterwerk. Auf diese Form sollte er mit La clemenza di Tito (1791) kurz vor seinem Tod nochmals zurückkommen. Nach den Singspielen Bastien und Bastienne, Zaide (Fragment) und Die Entführung aus dem Serail (mit dieser 1782 uraufgeführten Oper gelang es ihm, sich in Wien als freier Komponist zu etablieren) schaffte er es in seinem Figaro (1786) und mehr noch im Don Giovanni (1787), Opera seria und Opera buffa einander wieder anzunähern. Neben den zuletzt Genannten entstand 1790 als drittes Werk in kongenialer Zusammenarbeit mit dem Librettisten Lorenzo Da Ponte Così fan tutte. In der Zauberflöte (1791) verband Mozart Elemente der Oper mit jenen des Singspiels und des lokal vorherrschenden Alt-Wiener Zaubertheaters, das seine Wirkung besonders aus spektakulären Bühneneffekten und einer märchenhaften Handlung bezog. Dazu kamen Ideen und Symbole aus der Freimaurerei (Mozart war selbst Logenmitglied). Mozart-Opern (und insbesondere die Zauberflöte) gehören bis heute zum Standardrepertoire eines jeden Opernhauses. Er selbst bezeichnete die Oper als „Große Oper in 2 Akten“.
Der ebenfalls sowohl in Italien wie auch in Wien tätige Christoph Willibald Gluck leitete mit seinen Opern Orfeo ed Euridice (1762) und Alceste (1767) in denen er Elemente der ernsten Oper aus Italien und Frankreich mit der realistischeren Handlungsebene der Opera buffa kombinierte eine umfassende Opernreform ein. Der konsequent klar und logisch aufgebaute Handlungsablauf, gestaltet von Ranieri de’ Calzabigi, kommt dabei ohne komplexe Intrigen oder Verwechslungsdramen aus. Die Zahl der Protagonisten schrumpft. Oberstes Ziel ist eine größere Einfachheit und Nachvollziehbarkeit der Handlung.
Dabei ordnet sich Glucks Musik vollständig Dramaturgie und Text unter, charakterisierte Situationen und Personen und stand nicht für den belcanto-Gesang an sich. Durchkomponierte oder strophisch gestaltete Lieder ersetzten die Da-capo-Arie. Dadurch wurde eine neue Natürlichkeit und Einfachheit erreicht, die hohlem Pathos und Sängermanierismen entgegenwirkte. Der Chor schaltete sich getreu dem antiken Vorbild aktiv in die Handlung ein. Die Ouvertüre bezieht sich auf die Handlung und steht nicht mehr als abgelöstes Instrumentalstück vor der Oper. Italienisches Arioso, französisches Ballett und Pantomime, englisches und deutsches Lied sowie Vaudeville wurden in die Oper integriert, nicht als nebeneinanderstehende Einzelstücke, sondern als neuer klassischer Stil. Glucks ästhetische Ideen wurden von seinem Schüler Antonio Salieri im späten 18. Jahrhundert zu einer neuen Blüte gebracht. Besonders bedeutend sind die Opern Les Danaïdes, Tarare und Axur, re d’Ormus.
Weiterer Ausdruck der größeren Alltagsnähe der Opera buffa und der durch Christoph Willibald Gluck angeregten Neuerungen der Opernreform ist die in der 2. Hälfte des 18. Jahrhunderts einsetzende Praxis auf hohe Kastratenpartien für Männerpartien zugunsten realistischerer Stimmlagen zu verzichten. Neben der bewussten Abgrenzung von der stark durch das Virtuosentum der Kastraten geprägten Opernkultur der Opera seria des Adels, spielten hierfür nicht zuletzt Kostengründe eine entscheidende Rolle. Da Impresarios mit der Opera buffa auf ein weniger zahlungskräftiges bürgerliches und sub-bürgerliches Publikum zielten, waren die horrenden Kosten für die Gage eines bekannten Kastraten kaum zu erwirtschaften. Die hieraus folgende Identifikation der Virtuosenkultur der Kastratenpartien mit der durch den Adel geprägten kostspieligen Tradition der Opera seria erklärt auch das Verschwinden der Kastraten aus dem Opernbetrieb nach dem Ende des Ancien Régime und dem hierdurch bedingten Aufstieg der durch die „natürlichere“ Stimmbesetzung der Opera buffa und Opera semiseria geprägten bürgerlichen Schichten zur auch in Sachen Oper führenden Gesellschaftsschicht des 19. Jahrhunderts.
Im ersten Viertel des 19. Jahrhunderts verschwinden zunehmend die durch den Generalbass begleiteten Rezitative zugunsten einer ausnotierten Orchesterfassung. Neben der bis dahin noch führenden italienischen Oper und den französischen Operntypen treten nach und nach andere nationale Opernformen auf, so zuerst in Deutschland. Die Französische Revolution und der Aufstieg Napoleons zeigten ihre Auswirkungen auf die Oper am deutlichsten bei Ludwig van Beethovens einziger Oper Fidelio bzw. Leonore (1805, 1806 und 1814). Dramaturgie und musikalische Sprache orientierten sich deutlich an Luigi Cherubinis Médée (1797). Die Handlung beruht auf einem „fait historique“ von Jean-Nicolas Bouilly, das 1798 von Pierre Gaveaux unter dem Titel Léonore, ou L’amour conjugal komponiert worden war; die Ideale der französischen Revolution bilden daher auch den Hintergrund von Beethovens Oper. Fidelio kann zum Typus der „Rettungsoper“ gezählt werden, in der die dramatische Errettung eines Menschen aus großer Gefahr der Gegenstand ist. Formal ist das Werk uneinheitlich: der erste Teil ist singspielhaft, der zweite mit dem groß angelegten Chorfinale erreicht symphonische Durchschlagskraft und nähert sich dem Oratorium. Nach der Zauberflöte und dem Fidelio brauchte die deutsche Produktion mehrere Anläufe, um schließlich in der Romantik eine eigene Opernsprache zu entwickeln. Eine der wichtigsten Vorstufen hierzu lieferten E. T. A. Hoffmann mit seiner romantischen Oper Undine und Louis Spohr mit seiner Vertonung des Faust (beide 1816).
Carl Maria von Weber war es schließlich, der aus der Tradition des Singspiels mit viel dramatischem Farbenreichtum im Orchester die deutsche Oper in Gestalt des Freischütz im Jahr 1821 gebührend aufleben ließ. Sein wegen des schlechten Textbuches kaum gespieltes Werk Oberon (London 1826) maß dem Orchester so viel Bedeutung zu, dass sich später namhafte Komponisten wie Gustav Mahler, Claude Debussy und Igor Strawinsky auf ihn beriefen.
Weitere Komponisten der deutschen Romantik waren die als Opernkomponisten kaum bekannten Hochromantiker Franz Schubert (Fierrabras, komponiert 1823, UA 1897), dessen Freunde ihm keine kongeniale Textvorlage liefern konnten, und Robert Schumann, der mit der Vertonung des unter Romantikern beliebten Genoveva-Stoffs nur eine Oper (1850) vorlegte. Ferner zu nennen sind Heinrich Marschner, der mit seinen Opern um übernatürliche Ereignisse und Naturschilderungen (Hans Heiling, 1833) großen Einfluss auf Richard Wagner ausübte, Albert Lortzing mit seinen Spielopern (u. a. Zar und Zimmermann, 1837, sowie Der Wildschütz, 1842), Friedrich von Flotow mit seiner komischen Oper Martha (1847) und schließlich Otto Nicolai, der mit den Lustigen Weibern von Windsor (1849) etwas „italianità“ in die deutsche Oper trug.
Richard Wagner schließlich formte die Oper so grundlegend nach seinen Ideen um, dass die oben genannten deutschen Komponisten neben ihm schlagartig verblassten. Mit Rienzi (1842) erlebte der bis dahin eher glücklose Wagner seinen ersten Erfolg in Dresden; er wurde später von Der fliegende Holländer (1843) noch übertroffen. Wegen seiner Verwicklung in die Märzrevolution von 1848 in Dresden musste Wagner für viele Jahre ins Exil in die Schweiz. Sein späterer Schwiegervater Franz Liszt trug durch die Uraufführung des Lohengrin (1850) in Weimar dazu bei, dass Wagner trotzdem weiterhin in Deutschland präsent war. Mit der Unterstützung des jungen bayerischen Königs Ludwig II. konnte Wagner schließlich den lang gehegten Plan des Ring des Nibelungen verwirklichen, für den er  eigens das Bayreuther Festspielhaus erbauen ließ, in dem bis heute nur seine Werke gespielt werden.
Die grundlegende Neuerung Wagners bestand in der vollständigen Auflösung der Nummernoper. Tendenzen zur durchkomponierten Oper zeigten sich schon in Webers Freischütz oder in Robert Schumanns selten gespielter Genoveva (1850). Konsequent vollendet wurde diese Entwicklung erst durch Wagner. Daneben behandelte er Singstimmen und Orchesterpart grundsätzlich gleichberechtigt. Das Orchester begleitet also nicht mehr den Sänger, sondern tritt als „mystischer Abgrund“ in vielfältige Beziehung zum Gesungenen. Die Länge von Wagners Opern verlangt Sängern und Zuhörern viel Konzentration und Ausdauer ab. Die Themen seiner – mit Ausnahme einiger Frühwerke sowie der Meistersinger – durchweg ernsten Opern, deren Libretti er sämtlich selbst verfasste, sind häufig Erlösung durch Liebe, Entsagung oder Tod.
In Tristan und Isolde (1865) verlegte Wagner das Drama weitgehend in den psychischen Innenraum der Hauptfiguren, den er dann mit seiner Musik ausleuchten konnte – die äußere Handlung der Oper ist dagegen ungewöhnlich ereignisarm. Der Gestaltung dieses „ozeanischen“ Innenraums diente auch die Harmonik, die mit dem „Tristan-Akkord“ die bis dahin gültigen harmonischen Regeln in den Hintergrund rückte und damit in die Musikgeschichte einging. Musikalisch zeichnen sich Wagners Opern sowohl durch seine geniale Behandlung des Orchestersatzes, die auch auf die symphonische Musik der Zeit bis hin zu Gustav Mahler starken Einfluss ausübte, aus, als auch durch den Einsatz wiederkehrender Motive, der sogenannten Leitmotive, die sich mit Figuren, Situationen, einzelnen Begriffen oder auch mit bestimmten Ideengehalten verbinden. Mit dem Ring des Nibelungen (komponiert 1853–1876), dem wohl bekanntesten Opernzyklus in vier Teilen (daher auch schlicht „die Tetralogie“ genannt) mit etwa 16 Stunden Aufführungszeit insgesamt, schuf Wagner eine monumentale musikdramatische Verwirklichung seiner in der Schrift Oper und Drama (1852) entwickelten Reform der überkommenen Oper. Das Bühnenweihfestspiel Parsifal war die letzte seiner Opern, die die Musikwelt in zwei Lager spalteten und sowohl Nachahmer (Engelbert Humperdinck, Richard Strauss vor seiner Salome) als auch Skeptiker – insbesondere in Frankreich – hervorriefen.
In Frankreich herrschte zunächst die in der zweiten Hälfte des 18. Jahrhunderts entwickelte Form der Opéra-comique vor. Daniel-François-Esprit Auber gelang mit seiner Oper La muette de Portici (1828) deren Titelheldin von einer stumm bleibenden Ballerina dargestellt wurde, der Anschluss an die Grand opéra („große Oper“). Der Dramatiker Eugène Scribe wurde zu deren maßgeblichem Librettisten. In der Grand opéra traten neben den Verwicklungen der operntypischen Liebesgeschichte vor allem historisch-politische Motive in den Vordergrund, wie es deutlich in Rossinis letzter Oper Guillaume Tell (1829) vorgeprägt ist. Der erfolgreichste Vertreter der Grand Opéra war Giacomo Meyerbeer, mit seinen Werken Robert le diable (1831), Les Huguenots (1836) und Le prophète (1849), die jahrzehntelang und noch bis ins beginnende 20. Jahrhundert hinein, im internationalen Repertoire gespielt wurden. Andere bedeutende Beispiele sind La Juive („Die Jüdin“, 1835) von Halévy, Donizettis Dom Sébastien (1843), oder Verdis Don Carlos (1867).
Etwa ab 1850 vermischten sich Opéra comique und Grand opéra zu einer neuen Opernform ohne Dialoge. Georges Bizet schrieb 1875 sein bekanntestes Bühnenwerk Carmen noch als Opéra comique, deren Rezitative erst postum von Ernest Guiraud hinzugefügt wurden. Wenn die „realistische“ Handlung und der Ton des Werks nicht zu einer Grand opéra passen, so steht wiederum das tragische Ende, das bei der Uraufführung zunächst für einen Misserfolg sorgte, im Widerspruch zur Opéra comique. Weitere Beispiele für die Vermischung von Opéra comique und Grand opéra sind Charles Gounods Faust (1859) – hier wird zum ersten Mal der Begriff Drame lyrique verwendet – und Jacques Offenbachs Les contes d’Hoffmann (Hoffmanns Erzählungen, 1871–1880).
Schließlich trat auch Russland mit seinen ersten Nationalopern auf den Plan, genährt durch den Import anderer Erfolge aus dem Westen. Michail Glinka komponierte 1836 die Oper Жизнь за царя (Schisn sa zarja, deutsch: Ein Leben für den Zaren, in der Sowjetunion zu Iwan Sussanin  umbenannt). Das Werk hat ein russisches Sujet, ist aber musikalisch noch stark in westlichen Einflüssen verhaftet. Seine bekannteste Oper Ruslan und Ljudmila übte großen Einfluss auf die folgenden Generationen russischer Komponisten aus. Modest Mussorgski löste sich mit Boris Godunow (1874) nach einem Drama von Alexander Puschkin endgültig von westlichen Einflüssen. Auch Borodins Fürst Igor (1890) führte Glinkas Erbe weiter. Pjotr Tschaikowski stand zwischen den russischen Traditionen und denen der westlichen Welt und entwarf mit Eugen Onegin (1879) und Pique Dame (1890) Liebesdramen mit bürgerlichem Personal, die beide ebenfalls auf einer Vorlage von Puschkin beruhen.
In Böhmen waren Bedřich Smetana und Antonín Dvořák die meistgespielten Komponisten der Prager Nationaloper, die mit Smetanas Libuše (1881) im neuen Nationaltheater in Prag ihren Anfang nahm. Die verkaufte Braut (1866) desselben Komponisten wurde zum Exportschlager. Dvořaks Oper Rusalka (1901) verknüpfte volkstümliche Sagen und deutsche Märchenquellen zu einer lyrischen Märchenoper. Bohuslav Martinů und Leoš Janáček führten ihre Bestrebungen weiter. Letztgenannter Komponist ist in seiner Modernität in den letzten Jahrzehnten wiederentdeckt worden und hat vermehrt die Spielpläne erobert. Während Das schlaue Füchslein (1924) noch immer meist in der deutschen Übersetzung von Max Brod gegeben wird, werden andere Werke wie Jenůfa (1904), Káťa Kabanová (1921) oder Věc Makropulos (1926) immer häufiger in der tschechischsprachigen Originalversion aufgeführt; das ist insofern wichtig, da Janáčeks Tonsprache sich eng an die Phonetik und Prosodie seiner Muttersprache anlehnt.
Italien verfiel ab dem Jahr 1813, in dem seine Opern Tancredi und L’italiana in Algeri aufgeführt wurden, dem jungen und überaus produktiven Belcanto-Komponisten Gioachino Rossini. Il barbiere di Siviglia (1816), La gazza ladra (dt. Die diebische Elster) und La Cenerentola (beide 1817) nach dem Aschenputtel-Märchen von Charles Perrault sind bis heute im Standardrepertoire der Opernhäuser zu finden. Federnder Rhythmus und eine geistreich-brillante Orchestrierung sowie eine virtuose Behandlung der Singstimme ließen Rossini zu einem der beliebtesten und verehrtesten Komponisten Europas werden. Die bis dato noch üblichen improvisierten Verzierungen der Sänger schrieb Rossini dezidiert in seine Partien hinein und unterband damit ausufernde Improvisationen. Eine neue formale Idee verwirklichte er mit seiner scena ed aria, die den starren Wechsel von Rezitativ und Arie auflockerte und doch das Prinzip der Nummernoper aufrechterhielt. Daneben hat Rossini auch eine ganze Reihe von Opere serie geschrieben (z. B. seinen Otello, 1816, oder Semiramide, 1823). 1824 ging er nach Paris und schrieb wichtige Werke für die Opéra. Eine politische Grand opéra verfasste er über Wilhelm Tell (Guillaume Tell, 1829), die in Österreich verboten und an verschiedenen europäischen Orten in entschärfter Fassung mit anderen Haupthelden aufgeführt wurde.
Rossinis jüngere Zeitgenossen und Nachfolger kopierten zunächst seinen koloraturenreichen Stil, bis vor allem Vincenzo Bellini und Gaetano Donizetti es schafften, sich mit einem eigenen, etwas schlichteren, ausdrucksvollen und romantischeren Stil von dem übermächtigen Vorbild zu emanzipieren. Bellini war berühmt für die ausdrucksvolle und ausgefeilte Deklamation seiner Rezitative und die „unendlich“ langen und ausdrucksvollen Melodien seiner Opern, wie Il pirata (1827), I Capuleti e i Montecchi (1830), I puritani (1835), La sonnambula (1831), und vor allem Norma (1831). Die Titelpartie dieser Oper mit der berühmten Arie „Casta diva“ schrieb Bellini, genau wie die Amina in La sonnambula, der großen Sängerin Giuditta Pasta auf den Leib. Die Norma ist so anspruchsvoll, dass sie nur von ganz wenigen großen Sängerinnen gesungen und interpretiert werden kann, sie wurde durch die historische Interpretation von Maria Callas wieder der Vergessenheit entrissen.
Der wenige Jahre ältere Donizetti war ein ungemein fleißiger Komponist, der neben Bellini und vor allem nach dessen frühzeitigem Tode (1835) zum erfolgreichsten italienischen Opernkomponisten aufstieg. Seinen ersten großen Durchbruch hatte er mit Anna Bolena (1830), deren Titelpartie ebenfalls von der Pasta kreiert und von der Callas wiederentdeckt wurde. Dagegen ist Lucia di Lammermoor (1835) mit der berühmten koloraturreichen Wahnsinnsszene nie ganz aus dem Repertoire verschwunden und hält sich neben den heiteren Opern L’elisir d’amore (1832), Don Pasquale (1843), und La fille du régiment (1840) konsequent auf den Spielplänen der Opernhäuser.
Die weit gespannten Melodiebögen Bellinis machten starken Eindruck auf den jungen Giuseppe Verdi. Seit seiner dritten Oper Nabucco galt er als Nationalkomponist für das immer noch von den Habsburgern beherrschte Italien. Der Chor „Va, pensiero, sull’ ali dorate“ entwickelte sich zur heimlichen Nationalhymne des Landes. Musikalisch zeichnet Verdis Musik eine stark betonte, deutliche Rhythmik aus, über der sich einfache, oft extrem ausdrucksstarke Melodien entwickeln. In seinen Opern, bei denen Verdi mit untrüglichem Theaterinstinkt auch oft selbst am Textbuch mitwirkte, nehmen Chorszenen zunächst eine wichtige Stellung ein. Verdi verließ zunehmend die traditionelle Nummernoper; ständige emotionale Spannung verlangte nach einer abwechslungsreichen Durchmischung der einzelnen Szenen und Arien. Mit Macbeth wandte sich Verdi endgültig von der Nummernoper ab und ging seinen Weg der intimen Charakterschilderung von Individuen weiter. Mit La traviata (1853, nach dem 1848 erschienenen Roman Die Kameliendame von Alexandre Dumas d. J., der um die authentische Figur der Kurtisane Marie Duplessis kreist) brachte er erstmals einen Gegenwartsstoff auf die Opernbühne, wurde von der Zensur jedoch gezwungen, die Handlung aus der Jetztzeit zu verlegen. Verdi vertonte häufig literarische Vorlagen, etwa von Friedrich Schiller (z. B. Luisa Miller nach Kabale und Liebe oder I masnadieri nach Die Räuber), Shakespeare oder Victor Hugo (Rigoletto). Mit seinen für Paris geschriebenen Beiträgen zur Grand Opéra (z. B. Don Carlos, 1867) erneuerte er auch diese Form und nahm mit dem späten Otello Elemente von Richard Wagners Musikdrama auf, bis er mit der überraschenden Komödie Falstaff (1893; Dichtung in beiden Fällen von Arrigo Boito) im Alter von 80 Jahren seine letzte von fast 30 Opern komponierte. Wahrscheinlich seine populärste Oper ist Aida, geschrieben 1871.
Nach dem Abtreten Verdis eroberten die jungen Veristen (ital. vero = wahr) in Italien die Szene. Ungeschönter Naturalismus war eines ihrer höchsten ästhetischen Ideale – dementsprechend wurde von säuberlich verfassten Versen Abstand genommen. Pietro Mascagni (Cavalleria rusticana, 1890) und Ruggero Leoncavallo (Pagliacci, 1892) waren die typischsten Komponisten aus dieser Zeit. Giacomo Puccini wuchs hingegen an Ruhm weit über sie hinaus und ist bis heute einer der meistgespielten Opernkomponisten überhaupt. La Bohème (1896), ein Sittengemälde aus dem Paris der Jahrhundertwende, der „Politkrimi“ Tosca (1900, nach dem gleichnamigen Drama von Victorien Sardou) und die fernöstliche Madama Butterfly (1904), mit der unvollendeten Turandot (Uraufführung posthum 1926) noch um ein weiteres an Exotismus gesteigert, sind vor allem wegen ihrer Melodien zu Schlagern geworden. Puccini war ein eminenter Theatraliker und wusste genau für die Stimme zu schreiben; die Instrumentierung seiner meist für großes Orchester gesetzten Partituren ist sehr differenziert und meisterhaft.[4] Zurzeit wird der damals sehr populäre italienisch-deutsche Komponist Alberto Franchetti, trotz dreier Welterfolge (Asrael, Christoforo Colombo und Germania) zwischendurch fast vergessen, zaghaft wiederentdeckt.
Einem anderen musikdramatischen Ideal verpflichtet als die Veristen war der gleichzeitig tätige Alfredo Catalani, dessen beim Publikum sehr beliebten Werke auch mit fantastischen Elementen durchsetzt sind. Seine letzte und heute bekannteste Oper, La Wally nach dem Roman Die Geier-Wally von Wilhelmine von Hillern, wurde am 20. Januar 1892 im Teatro alla Scala in Mailand uraufgeführt.
Claude Debussy gelang es schließlich, sich vom Einfluss des Deutschen zu befreien, und schuf mit Pelléas et Mélisande 1902 eines der nuanciertesten Beispiele für die von Wagner übernommene Leitmotivtechnik. Maurice Maeterlincks Textvorlage bot viel an mehrdeutigen Symbolismen an, die Debussy in die Orchestersprache übernahm. Die Gesangspartien wurden fast durchweg rezitativisch gestaltet und boten der „unendlichen Melodie“ Wagners mit dem „unendlichen Rezitativ“ ein Gegenbeispiel. Eine der raren Ausnahmen, die dem Hörer eine gesangliche Linie darbieten, ist das schlichte Lied der Mélisande, das wegen seiner Kürze und Schmucklosigkeit kaum als echte Arie angesehen werden kann.
Nach Richard Strauss, der mit Salome und Elektra zunächst zum spätromantischen Expressionisten wurde, sich dann allerdings mit Der Rosenkavalier wieder früheren Kompositionsstilen zuwendete und mit einer Reihe von Werken bis heute viel gespielt wird (z. B. Ariadne auf Naxos, Arabella, Die Frau ohne Schatten und Die schweigsame Frau), schafften es nur noch wenige Komponisten, einen festen Platz im Repertoire der Opernhäuser zu finden. Stattdessen wurden (und werden) eher die Werke der Vergangenheit gepflegt. Die Aufnahme eines zeitgenössischen Werkes in das Standardrepertoire bleibt die Ausnahme.
Alban Berg gelang dies dennoch mit seinen Opern Wozzeck, der freitonal angelegt wurde, und Lulu, die sich ganz der Zwölftonmusik bedient. Die zuerst Fragment gebliebene Lulu wurde von Friedrich Cerha für die Pariser Aufführung unter Pierre Boulez und Patrice Chéreau in ihrer dreiaktigen Gestalt vollendet. Von beiden Opern hat insbesondere Wozzeck, bei dem Gehalt des Stücks und musikalische Vision zu einer Einheit finden, inzwischen weltweit in unzähligen Inszenierungen an großen wie kleineren Bühnen Eingang in das vertraute Opernrepertoire gefunden und eine unbestrittene Stellung erobert. Durchaus ähnlich verhält es sich mit Lulu, die jedoch wegen ihres im Werk angelegten Aufwands oft nur von größeren Bühnen bewältigt werden kann. Sie inspiriert allerdings regelmäßig wichtige Interpretinnen wie Anja Silja, Evelyn Lear, Teresa Stratas oder Julia Migenes.
Von Arnold Schönberg werden regelmäßig das Monodram Erwartung – die erste Oper für eine einzige Sängerin – sowie das vom Komponisten bewusst unvollendet hinterlassene, höchste Ansprüche an den Chor stellende Werk Moses und Aron aufgeführt. Erwartung, bereits 1909 entstanden, doch erst 1924 in Prag mit Marie Gutheil-Schoder unter der Leitung von Alexander von Zemlinsky uraufgeführt, bewies in den dem Zweiten Weltkrieg folgenden Jahren eine spezifische Faszination gleichermaßen für Sängerinnen (besonders Anja Silja und Jessye Norman) wie für Regisseure (z. B. Klaus Michael Grüber mit Silja 1974 in Frankfurt; Robert Wilson mit Norman 1995 bei den Salzburger Festspielen). 1930 begann Schönberg die Arbeit an Moses und Aron, die er 1937 abbrach; nach der szenischen Uraufführung in Zürich 1957 hat diese Oper international zumal seit den 1970er Jahren in zahlreichen Aufführungen seine besondere Bühnentauglichkeit bewiesen. Interessant ist ferner, dass Moses sich über die gesamte Oper hinweg eines Sprechgesangs bedient, dessen Tonhöhe vorgezeichnet ist, Aron dagegen singt.
Ansonsten hinterließ die Wiener Schule keine weiteren Spuren im Standardrepertoire. Musikalisch musste sich allerdings jeder moderne Komponist mit der Zwölftonmusik auseinandersetzen und entscheiden, ob er auf ihrer Grundlage weiter arbeitete oder eher in tonalen Bahnen dachte.
Hans Pfitzner gehörte zu den bedeutendsten Komponisten der ersten Jahrhunderthälfte, die bewusst an den tonalen Traditionen festhielten. Sein Opernschaffen zeigt gleichermaßen Einflüsse Richard Wagners und frühromantischer Komponisten, wie Weber und Marschner. Pfitzners Musik wird zum großen Teil von linear-polyfonem Denken bestimmt, die Harmonik bewegt sich zwischen schlichter Diatonik und bis an die Grenzen der Tonalität gehender Chromatik. Von Pfitzners Opern ist die 1917 uraufgeführte Musikalische Legende Palestrina am bekanntesten geworden. Er schrieb außerdem: Der arme Heinrich, Die Rose vom Liebesgarten, Das Christ-Elflein und Das Herz.
Franz Schreker schuf 1912 mit Der ferne Klang einen der großen Opernerfolge vor dem Zweiten Weltkrieg, geriet jedoch später in Vergessenheit, als der Nationalsozialismus seine Werke aus den Spielplänen verdrängte. Nach vielen früheren Versuchen begann erst in den 1980er Jahren die wirklich tief greifende Wiederentdeckung dieses Komponisten, die neben Neuinszenierungen von Der ferne Klang (Teatro La Fenice 1984, Wiener Staatsoper 1991) auch Aufführungen von Die Gezeichneten, Der Schatzgräber oder Irrelohe zeitigte. Eine wesentliche Rolle in Schrekers Musik spielen stark ausdifferenzierte Klangfarben. Die chromatische Harmonik Wagners erfährt bei Schreker eine nochmalige Intensivierung, die nicht selten die tonalen Bindungen bis zur Unkenntlichkeit verwischt.
Ähnlich wie Schreker erging es dem Wiener Alexander von Zemlinsky und dem Brünner Erich Wolfgang Korngold, deren Werke es nach 1945 ebenfalls schwer hatte. Seit den 1980er Jahren gelang es beiden Komponisten, wieder einen Platz im internationalen Repertoire zu erlangen, Zemlinsky mit Kleider machen Leute, besonders aber Eine florentinische Tragödie, Der Zwerg und Der König Kandaules, Korngold mit Die tote Stadt.
Auch das Schaffen von Walter Braunfels wurde von den Nationalsozialisten verboten und erfährt erst seit Ende des 20. Jahrhunderts wieder verstärkte Aufmerksamkeit. Mit seiner Oper Die Vögel war Braunfels in den 1920er Jahren einer der meistgespielten Komponisten auf deutschen Opernbühnen. An seinen Werken fällt ihre stilistische Vielseitigkeit auf: Bietet Prinzessin Brambilla einen auf die Commedia dell’arte zurückgreifenden Gegenentwurf zum Musikdrama der Wagnernachfolge, zeigen Die Vögel den Einfluss Pfitzners. Mit den späteren Opern Verkündigung, Der Traum ein Leben und Jeanne d’Arc – Szenen aus dem Leben der Heiligen Johanna nähert Braunfels sich der Tonsprache des späteren Hindemith an.
Zu den in den 1920ern erfolgreichsten Komponisten der jungen Generation zählte Ernst Krenek, ein Schüler Schrekers, der zunächst mit in freier Atonalität gehaltenen, expressionistischen Werken für Aufsehen sorgte. Ein Skandalerfolg wurde 1927 seine Oper Jonny spielt auf, die Elemente des Jazz aufgreift. Sie ist ein typisches Beispiel für die damals entstandene Gattung der „Zeitoper“, die ihre Handlungen dem stark vom Wechsel unterschiedlicher Moden bestimmten Alltag der damaligen Zeit entnahm. Kreneks Musik wurde von den Nationalsozialisten später als „entartet“ abgelehnt und verboten. Der Komponist emigrierte in die USA und brachte es bis 1973 auf über 20 Opern, in denen sich die wechselvolle Entwicklung der Musik des 20. Jahrhunderts exemplarisch widerspiegelt.
Der Zweite Weltkrieg bezeichnete einen großen Einschnitt in der Geschichte Europas und Amerikas, der sich auch auf die musikalische Welt auswirkte. In Deutschland wurden kaum noch Opern mit modernen Klängen gespielt und gerieten immer mehr ins Abseits. Ein bezeichnendes Beispiel hierfür bildet Paul Hindemith, der in den 1920ern mit Werken wie der Oper Cardillac als musikalischer „Bürgerschreck“ galt, nach 1930 aber schließlich zu einem gemäßigt modernen Stil neoklassizistischer Prägung gefunden hatte, dem u. a. Mathis der Maler (aus Teilen dieser Oper stellte der Komponist eine viel gespielte Sinfonie zusammen) zuzurechnen ist. Trotz des Stilwandels bekam Hindemith die Ablehnung deutlich zu spüren, da Adolf Hitler persönlichen Anstoß an seiner 1929 vollendeten Oper Neues vom Tage genommen hatte. Schließlich wurden auch Hindemiths Werke mit dem Etikett „entartet“ versehen und ihre Aufführung verboten. Hindemith ging, wie andere Künstler und Komponisten vor und nach ihm, 1938 ins Exil.
Die Zeit nach 1945 ist durch eine deutliche Internationalisierung und Individualisierung des Opernbetriebes gekennzeichnet, welche die bis dahin übliche Unterteilung in nationale Traditionen kaum mehr sinnvoll erscheinen lässt.
Die Oper wurde immer stärker von individuellen Einflüssen der Komponisten abhängig als von allgemeinen Strömungen. Die ständige Präsenz der „Klassiker“ des Opernrepertoires ließ die Ansprüche an moderne Opern steigen, und jeder Komponist musste seinen eigenen Weg finden, um mit der Vergangenheit umzugehen, sie fortzuführen, zu verfremden oder mit ihr zu brechen.
Im Folgenden entstanden immer wieder Opern, die die Grenzen der Gattung sprengten und zu überwinden trachteten. Auf musikalischer wie textlicher Ebene verließen die Komponisten zunehmend bekanntes Terrain und bezogen die Bühne und die szenische Aktion in den – oft genug abstrakten – musikalischen Ablauf mit ein. Kennzeichen für die Erweiterung der visuellen Mittel im 20. Jahrhundert sind die zunächst handlungsbegleitenden, später selbstständigeren Videoprojektionen.
In der zunehmenden Individualisierung der Musiksprache lassen sich in der Oper der zweiten Hälfte des 20. Jahrhunderts dennoch Strömungen erkennen: zum einen die Literaturopern, deren Dramaturgie sich zu großen Teilen an der Tradition ausrichtet. Dazu werden aber mehr und mehr aktuelle Stoffe und Libretti verwendet. Dennoch sind zwei wegweisende Werke dieser Zeit ausgerechnet Opern, die Klassiker der Literatur als Grundlage verwenden, nämlich Bernd Alois Zimmermanns Oper Die Soldaten nach Jakob Michael Reinhold Lenz und Aribert Reimanns Lear nach William Shakespeare. Weitere Beispiele für die Literaturoper wären Reimanns Das Schloss (nach Kafka) und Bernarda Albas Haus (nach Lorca). Zunehmend werden auch politische Stoffe vertont, beginnend mit Luigi Nono und Hans Werner Henze; ein jüngeres Beispiel ist Gerhard Rosenfelds Oper Kniefall in Warschau über Willy Brandt, deren Uraufführung 1997 in Dortmund allerdings bei Publikum wie Presse gleichermaßen wenig Wirkung zeigte und keine Folgeproduktionen zeitigte.
Können schon Luigi Nonos Werke aufgrund ihrer experimentellen Musiksprache nicht mehr als Literaturoper kategorisiert werden, so wird auch die Dramaturgie der Opernvorlage auf ihre experimentellen Möglichkeiten hin ausgelotet. Der Begriff Oper erfährt daher eine Wandlung in der zweiten Hälfte des 20. Jahrhunderts, viele Komponisten ersetzen ihn durch Musiktheater oder musikalische Szenen und verwenden den Begriff Oper nur für explizit mit der Tradition verbundene Werke. In den Werken experimenteller Komponisten ist nicht nur ein kreativer Umgang mit Text und Dramaturgie zu entdecken, auch die Bühne, die Orchesterbesetzung und nicht zuletzt die Musik selbst überwindet konservative Muster, das Genre ist hier nicht mehr klar eingrenzbar. Zudem werden neue Medien wie Video, Elektronik eingesetzt, aber auch das Schauspiel, Tanz und Performance halten Einzug in die Oper.
Eine ganz eigene Stimme im zeitgenössischen Musiktheater verkörpert ein anderer italienischer Komponist: Salvatore Sciarrino. Er schafft mit seinem Interesse an Klangfarben oder auch der Stille in der Musik, z. T. im Rückgriff auf Kompositionstechniken der Renaissance (z. B. in seiner Oper Luci mie traditrici von 1998 über das Leben des Madrigal-Komponisten Carlo Gesualdo) unverwechselbare Werke.
Benjamin Britten ließ das moderne England auf den internationalen Opernbühnen Einzug halten. Von seinen überwiegend tonalen Opern sind A Midsummer Night’s Dream, basierend auf dem Schauspiel William Shakespeares, Albert Herring, Billy Budd und Peter Grimes am bekanntesten. Immer wieder zeigte sich Brittens Vorliebe und Talent zur Klangmalerei insbesondere in der Darstellung des Meeres.
Die Dialogues des Carmélites (Gespräche der Karmelitinnen, Uraufführung 1957) von Francis Poulenc gelten als eines der bedeutendsten Werke des modernen Musiktheaters. Grundlage bildet der historische Stoff der Märtyrinnen von Compiègne, die 1794 unter den Augen des Revolutionstribunals singend zum Schafott schritten, nachdem sie sich geweigert hatten, ihre Ordensgelübde zu brechen. Auf Poulenc geht auch die zweite bekannt gewordene Oper für eine einzige Sängerin zurück: In La voix humaine zerbricht die schlicht als „Frau“ bezeichnete Person an der Untreue ihres Geliebten, der ihr per Telefon den Laufpass gibt. Luciano Berio verwendete in Passaggio zu der weiblichen Hauptfigur „Sie“ auch einen kommentierenden Chor.
Der Komponist Philip Glass, der Minimal Music verhaftet, verwendete für Einstein on the Beach keine zusammenhängenden Sätze mehr, sondern Zahlen, Solfège-Silben, Nonsens-Worte. Entscheidend war die Darstellung der Geschehnisse auf der Bühne. 1976 entstand Einstein on the Beach, der erste Teil einer Trilogie, in der auch Satyagraha und Akhnaten vertreten sind – Hommagen an Persönlichkeiten, die die Weltgeschichte veränderten: Albert Einstein, Mahatma Gandhi und den ägyptischen Pharao Echnaton. Glass’ Arbeiten haben besonders in Verbindung mit den als kongenial empfundenen Inszenierungen von Robert Wilson oder Achim Freyer große Publikumswirksamkeit bewiesen.
Mauricio Kagels Bühnenwerke sind ebenso oft Werke über Musik oder Theater an sich, die am ehesten als „Szenisch-musikalische Aktion“ zu klassifizieren ist – die Musik ist kaum festgelegt, da Kagel sich der freien Improvisation seiner Interpreten überlässt, die auf Nicht-Instrumenten (Reißverschlüssen, Babyflaschen etc.) spielen oder sie ungewöhnlich benutzen, bedeutungslose Silben singen oder Handlung und/oder Musik per Zufall oder durch improvisierte Lesart entstehen lassen. Mit Witz übte Kagel dabei hintersinnige Kritik an Staat und Theater, Militär, Kunstbetrieb usw. Skandale erregte sein berühmtestes Werk Staatstheater, in dem die verborgenen Mechanismen desselben an die Oberfläche gekehrt werden.
Luigi Nono verwendete seine Musik dagegen, um politische und soziale Missstände anzuklagen. Besonders deutlich wird dies in Intolleranza 1960, wo ein Mann auf einer Reise zu seiner Heimat Demonstrationen, Proteste, Folterungen, Konzentrationslager, Gefängnishaft und Missbrauch bis hin zu einer Überschwemmung erlebt und schließlich feststellt, dass seine Heimat dort ist, wo er gebraucht wird.
Ein sehr produktiver Komponist war der 2003 mit dem Premium Imperiale der Japan Art Foundation (sog. Nobelpreis der Kunst) ausgezeichnete Hans Werner Henze. Er stand von Anfang an im Konflikt mit den teilweise dogmatisch ausgerichteten herrschenden Strömungen der zeitgenössischen Musik in Deutschland (Stichwort Darmstadt bzw. Donaueschingen, s. o.), griff serielle Techniken auf, wandte jedoch auch ganz andere Kompositionstechniken bis hin zur Aleatorik an. Am Beginn seines Opernschaffens stand seine Zusammenarbeit mit der Dichterin Ingeborg Bachmann (Der junge Lord, 1952, und die Kleist-Adaption Der Prinz von Homburg, 1961). Die Elegie für junge Liebende (1961) entstand mit W. H. Auden und Chester Kallman, den Librettisten von Strawinskys Oper The Rake’s Progress. Später vertonte er Libretti von Edward Bond (The Bassarides, 1966, und The English Cat, 1980). Sein Werk L’Upupa und der Triumph der Sohnesliebe wurde 2003 bei den Salzburger Festspielen uraufgeführt. Henze, der seit vielen Jahrzehnten in Italien lebte, hat viele jüngere Komponisten nachhaltig gefördert und beeinflusst. Seit 1988 gibt es in München die von ihm gegründete Biennale für Neues Musiktheater.
Karlheinz Stockhausen vollendete 2005 seine 1978 begonnene Heptalogie LICHT. Mit seinem Hauptwerk hinterlässt er ein religiöse Themen behandelndes, monumentales Opus, bestehend aus sieben Opern, die jeweils für einen Wochentag stehen. Die ersten Opern erlebten in Mailand ihre Uraufführung (Donnerstag, Samstag, Montag), in Leipzig wurden Dienstag und Freitag zum ersten Mal gespielt. In seiner Gesamtheit wurde das insgesamt 29 Stunden Musik umfassende komplexe Werk nicht zuletzt wegen der immensen organisatorischen Schwierigkeiten noch nicht aufgeführt.
Aufmerksamkeit erregte in Deutschland 1996 die Oper Das Mädchen mit den Schwefelhölzern von Helmut Lachenmann. Sie basiert auf der bekannten Weihnachtsgeschichte von Hans Christian Andersen. Auf eigenwillige Weise und mit teilweise neuartigen Instrumentaltechniken setzt Lachenmann hier das Gefühl der Kälte in Klang um.
Nach der Statistik von Operabase sind die fünf meistaufgeführten lebenden Opernkomponisten in den fünf Spielzeiten von 2013/14 bis 2017/18 die Amerikaner Philip Glass, Jake Heggie, der Engländer Jonathan Dove, der Niederländer Leonard Evers, und der Engländer Thomas Adès. Als meistaufgeführte deutsche Komponisten nennt Operabase Peter Lund an 8., Marius Felix Lange an 11., Wolfgang Rihm an 14., Ludger Vollmer an 17., und Aribert Reimann an 23. Stelle.[5]
Seit Humperdincks Märchenoper Hänsel und Gretel haben Opernkomponisten immer wieder Kinderopern geschrieben, wie z. B. Henze (Pollicino, 1980), Oliver Knussen (Wo die wilden Kerle wohnen, 1980 und 1984) und Wilfried Hiller (Tranquilla Trampeltreu, Norbert Nackendick, Der Rattenfänger, Eduard auf dem Seil, Wolkenstein und Der Goggolori).
Opern sind von einer Formenvielfalt geprägt, die durch konventionelle Kompositionsstile ebenso wie durch individuelle Lösungen der Komponisten bestimmt wird. Deshalb gibt es keine allgemeingültige Formel für ihre Struktur. Grob gesehen, kann man jedoch eine Entwicklung von der Nummernoper über viele verschiedene Mischformen bis hin zur durchkomponierten Oper gegen 1900 feststellen.
Von der Barockzeit bis in die Romantik hinein ist die Oper eine Aneinanderreihung in sich geschlossener Musikstücke („Nummern“), die durch Rezitative oder (im Singspiel) gesprochene Dialoge miteinander verbunden werden und eine durchgängige Handlung darstellen. Wie auch das Schauspiel kann eine Oper in Akte, in Bilder, Szenen bzw. Auftritte gegliedert sein. Die musikalischen Bestandteile der Oper sind vielfältig:
Die Trennung der Nummern und die Abgrenzung zwischen Rezitativ und Arie wurden im 19. Jahrhundert in Frage gestellt. Ab 1825 verschwand allmählich das Secco-Rezitativ, an seine Stelle trat in der italienischen Literatur das Prinzip von scena ed aria, das bei Giuseppe Verdi die Akte zu einem größeren musikalischen Ganzen formt. Richard Wagner propagierte ab der Mitte des Jahrhunderts den Verzicht auf die Nummernstruktur zugunsten eines durchkomponierten, auf der Grundlage von Leitmotiven geformten Ganzen. Für Wagners Opern hat sich der Begriff Musikdrama durchgesetzt, das Stichwort „Unendliche Melodie“ steht für ein kontinuierliches Fortschreiten der musikalischen und emotionalen Entwicklung, das sich nach seiner Auffassung gegen musikalische Tanzformen durchsetzen sollte. Seine Oper Tristan und Isolde (1865) bezeichnete Wagner als „Handlung in Musik“, was an die ursprünglichen Opernbegriffe „favola in musica“ oder „dramma per musica“ erinnern sollte.
Die durchkomponierte Form wurde im späten 19. Jahrhundert allgemein bevorzugt, auch bei Jules Massenet oder Giacomo Puccini, und blieb das vorherrschende Modell der frühen Moderne bis zum Neoklassizismus, der mit brüchigen Strukturen und mit Rückbezügen auf Formen der frühen Operngeschichte experimentierte. Auch in sich abgeschlossene Teile aus durchkomponierten Opern werden in Konzerten aufgeführt, wie etwa viele Arien aus Puccini-Opern. Als Meister der durchkomponierten Großform gilt Richard Strauss, der dies insbesondere in den Einaktern Salome, Elektra und Ariadne auf Naxos unter Beweis stellte.
Im 20. Jahrhundert griffen viele Komponisten wieder auf das Nummernprinzip zurück, zum Beispiel Zoltán Kodály, Igor Strawinski oder Kurt Weill. Die Nummernoper besteht außerdem in Operette und Musical weiter.
In der Geschichte der Oper gab es zumeist einen „hohen“ und einen „niederen“ Stil, frei nach der antiken Unterscheidung zwischen Tragödie und Komödie. Nicht immer bedeutet dies jedoch eine Grenze zwischen ernst und lustig. Der „hohe“ Stil kann sich über den „niederen“ auch einfach durch antike Stoffe erheben oder durch adlige Figuren oder durch eine „literarisch“ ernst zu nehmende Vorlage oder durch „schwierige“ (bzw. bloß durchkomponierte) Musik. All diese Anhaltspunkte für das Wertvollere wurden im Lauf der Geschichte angegriffen. Dabei gab es Gattungen, die den Gegensatz abzuschwächen versuchten wie die Opera semiseria.
Solange die Oper noch im Stadium des Experiments war, wie bis zu Beginn des 17. Jahrhunderts, war eine Trennung noch nicht nötig. Sie ergab sich erst, als Opernaufführungen zur Gewohnheit wurden, und zwar aus sozialen Gründen: Die ernste Oper enthielt aristokratisches Personal und „hohe“ politische Symbolik, die komische hatte bürgerliche Figuren und „unwesentliche“ alltägliche Handlungen zum Thema. Allmählich trennten sich Opera seria und Tragédie lyrique von ihren komischen Intermezzi, aus denen Opera buffa und Opéra-comique hervorgingen. Diese Trennung wurde erst am Ende des 18. Jahrhunderts aufgebrochen: Weil die Bürger in der für sie bestimmten „niederen“ Operngattung nicht mehr komisch (also lächerlich) dargestellt werden wollten, wurde das Komische oft ins Sentimentale abgebogen und aufgewertet. Daher sind „komische Opern“ oft nicht lustig. Nach der Französischen Revolution löst sich die Ständeklausel auf, und auch bürgerliche Opern durften „ernst“ sein. Somit ergaben sich im 19. Jahrhundert andere Abgrenzungen zwischen Tragödie und Komödie als im 18. Jahrhundert.
Ein Sammelbegriff sowohl für tragische als auch für komische Werke ist das italienische Dramma per musica, wie die Oper in ihrer Anfangszeit betitelt wurde. Ein Beispiel für eine frühe ernste Oper ist Il ritorno d’Ulisse in patria von Claudio Monteverdi. Der seriöse Anspruch resultiert aus dem Rückgriff auf antike Theaterstoffe – insbesondere Tragödien – und epische Heldendichtungen. Sie wurden seit dem späteren 18. Jahrhundert von jüngeren historischen Sujets verdrängt. Im Italien des 19. Jahrhunderts wurde der Begriff Dramma in der Zusammensetzung Melodramma verwendet und nicht mehr auf das antike Drama bezogen. Sowohl Bellinis tragische Oper Norma als auch die komödiantische Oper L’elisir d’amore von Gaetano Donizetti wurden so genannt.
Als fester Begriff etablierte sich die Opera seria erst im 18. Jahrhundert. Mischformen oder tragikomische Inhalte waren mit dieser Titelbezeichnung ausgeschlossen. Händels Oper Radamisto ist ein typisches Werk. Als Antipode zu Italien verlieh Frankreich seiner eigenen Form der Opera seria den Titel Tragédie lyrique, wesentlich geprägt durch Jean-Baptiste Lully und das Ballett am Hofe Louis’ XIV., später durch Jean-Philippe Rameau. Nach der Französischen Revolution etablierte sich allmählich die Grand opéra als bürgerliche ernste Oper. Dazu zählen Les Huguenots von Giacomo Meyerbeer, auch weniger erfolgreiche Werke wie Les Troyens von Hector Berlioz.
Das durchkomponierte Musikdrama des reiferen Richard Wagner (Der Ring des Nibelungen) hatte großen internationalen Einfluss. Französische Komponisten jener Zeit wie Massenet setzten dagegen eher auf einen durchsichtigen und gesanglichen Opernstil, für den die Bezeichnung Drame lyrique verwendet wurde. Noch Debussy verwendete diesen Begriff für seine Oper Pelléas et Mélisande.
Schon immer konnten Opernstoffe von Romanen, Novellen oder Bühnenwerken herstammen. Die italienische Oper des 18. Jahrhunderts verstand sich als in Musik gekleidete Literatur. Seitdem die Musik die absolute Vorherrschaft erlangt hat, also seit dem späten 19. Jahrhundert, nennt man ausgesprochen literarische Opern Literaturoper. Death in Venice von Benjamin Britten nach der Vorlage von Thomas Mann ist eine recht getreue Umsetzung des literarischen Stoffes in Musik.
Die Opera buffa ist die Urform der heiteren Oper. Pergolesis La serva padrona galt um die Mitte des 18. Jahrhunderts als das maßgebliche Beispiel. Ein spätes Beispiel ist Il barbiere di Siviglia von Gioachino Rossini. Die ausnehmend heiteren Opern waren oft geringer angesehen als die sentimentalen. Ihre Stoffe stammen aus dem Volkstheater und von der Posse, stark beeinflusst durch die italienische Commedia dell’arte.
Aus der frühen Opera buffa geht die französische Opéra-comique (Werkgattung) hervor, die vor der Revolution zur Oper eines zunehmend selbstbewussten Bürgertums wird. Zunächst verstand man hierunter eher ein Liederspiel (Vaudeville). Doch der musikalische Anteil wurde immer größer und begann zu überwiegen. Aus der Opéra-comique ist das deutschsprachige Singspiel entstanden. Das Singspiel trägt oft volkstümlich-bürgerlichen Charakter, ist geprägt von einfachen Lied- bzw. Rondo-Formen und verwendet statt Rezitativen gesprochene Dialoge, gelegentlich auch Melodramen zwischen den musikalischen Nummern.
Der Hof sprach Französisch. Das Problem der deutschen Oper war im 18. und zum Teil noch im 19. Jahrhundert, dass sie als volkssprachliche Oper zur „niederen“ Gattung gehörte und sich behaupten und emanzipieren musste. Die Entführung aus dem Serail von Wolfgang Amadeus Mozart ist eines der bekanntesten Singspiele mit dieser Zielsetzung. Mozart bedient sich für die Arien auch komplexerer musikalischer Formen. Das im Auftrag von Kaiser Joseph II. zur Etablierung eines Nationalsingspiels geschaffene, 1782 am Wiener Burgtheater uraufgeführte Werk war für die Entwicklung der deutschen Oper von entscheidender Bedeutung.
Paris war im 19. Jahrhundert führend für die Operngeschichte, und auch die Italiener wie Rossini und Verdi kamen hierher. Die Opéra-comique, die im Haus der Opéra-Comique aufgeführt wurde, blieb auch gegenüber der neu entstandenen, durchkomponierten Grand opéra, die in der Opéra zur Aufführung kam, zweitrangig – weniger von ihrer musikalischen als von ihrer sozialen Bedeutung her. Aus den erwähnten Gründen musste sie nicht unbedingt einen heiteren Inhalt haben. Ein auch im deutschen Sprachgebiet bekanntes Beispiel einer komisch-rührseligen Opéra-comique ist Der Postillon von Lonjumeau von Adolphe Adam. Eine Gruppe von formal noch als Opéra-comique zu bezeichnenden Werken nach 1860 verstärkte den sentimentalen Grundcharakter (etwa Mignon von Ambroise Thomas). Ein sentimentaler Einschlag findet sich auch in einigen komischen Opern von Rossini (La Cenerentola).
Eine Erneuerung der Opéra-comique gelang mit Carmen von Georges Bizet, deren Dramatik in die Richtung der Verismo-Oper weist. Bei ihr war – abgesehen von den proletarischen Figuren – das Reißerische ein Merkmal des „niederen“ Stils.
Auch die „Größe“ kann ein Zeichen für hohen oder niederen Stil sein. Zuweilen findet sich der Begriff „Große Oper“ als Untertitel eines Werkes. Damit wird zum Beispiel gesagt, dass das Orchester und der Chor in großer Besetzung spielen und singen sollten, oder dass die Oper ein abendfüllendes Werk mit integriertem Ballett ist. Dies sind Opern, die nur in einem größeren Theater zur Aufführung kommen und sich vom Repertoire der fahrenden Truppen unterscheiden konnten. Als Beispiel für eine „Große Oper“ ist Manon von Jules Massenet zu nennen.
Der Begriff Kammeroper bezieht sich dagegen auf ein mit geringem Personal realisierbares Werk. Die Anzahl der Sänger ist in der Regel nicht mehr als fünf, das Orchester wird auf ein Kammerorchester begrenzt. Dies konnte aus der Not materielle Armut hervorgehen und damit auf das „niedere“ Genre verweisen oder im Gegenteil die größere Exklusivität und Konzentration eines „höheren“ Genres bedeuten. Auch die Bühne ist oftmals kleiner, was zu einer intimeren Atmosphäre beitragen kann, die für die Wirkung des Werkes von Vorteil ist. Beispiele dafür wären Albert Herring von Benjamin Britten oder „Les Larmes de couteau“ von Bohuslav Martinů.
Manche Opernkomponisten wehrten sich auch gegen die Einordnung in Gattungstraditionen oder bezeichneten ihre Werke in bewusster Relation zu diesen mit bestimmten Untertiteln. Wagners Tristan und Isolde trägt zum Beispiel die Bezeichnung „Handlung in Musik“, Luciano Berio verwendete für sein Werk Passaggio etwa den Begriff „messa in scena“ (‚Inszenierung‘). George Gershwin beschrieb sein Werk Porgy and Bess als „An American Folk Opera“. Um sich von klischeehaften Vorstellungen abzugrenzen, bevorzugen moderne Komponisten oft alternative Bezeichnungen wie etwa „azione scenica“ (Al gran sole carico d’amore von Luigi Nono) oder „azione musicale“ (‚musikalische Handlung‘, Un re in ascolto von Luciano Berio). Auch Peter Tschaikowskis bekannte Oper Eugen Onegin wurde vom Komponisten „Lyrische Szenen“ genannt.
Richard Geppert schrieb 2016 die deutsche Rockoper Freiheit mit den musikalischen Ausdrucksmitteln und Instrumenten der Rockmusik.[6]
Vereinzelt gibt es Beispiele für Opern – darunter John Coriglianos 1991 uraufgeführtes Werk The Ghosts of Versailles –, die bezogen auf die Form selbstreferenziell sind, indem sie selbst wiederum Schauspiel oder Oper enthalten.[7]
Aufgrund der nicht immer leichten Abgrenzbarkeit der Gattung Oper von anderen musikalischen Gattungen und Genres und der Praxis des Pasticcios ist eine Aussage zum Gesamtumfang des Opern-Repertoires mit zahlreichen Schwierigkeiten behaftet. Aktuelle Auflistungen gehen von ca. 5800 bis 6000 bekannten Werken aus. Rechnet man die nicht unerhebliche Anzahl verschollener und verlorener Werke, insbesondere des 18. und frühen 19. Jahrhunderts mit ein, dürfte eine Gesamtzahl von ca. 60.000 Opern realistisch sein.[8]
Die große Menge an Werken macht es Theatern und Opernhäusern nicht einfach, eine Auswahl zu treffen, die einem hohen Anspruch genügt und auch genügend Publikum findet. Abhängig von der Größe des Theaters und dem vorhandenen Budget wird von Intendant und Dramaturgie für jede Sparte des Theaters (Schauspiel, Musiktheater, Ballett, Kinder- und Jugendtheater, Puppentheater etc.) ein Spielplan erarbeitet, der dem Haus und seinen Mitarbeitern angepasst ist. Der Spielplan geht auf die regionalen Eigenheiten und Aufführungstraditionen des Ortes ein – zum Beispiel durch open air-Festspiele, Weihnachts- oder Neujahrskonzerte – weist aber auch auf aktuelle Strömungen des Musiktheaters hin, indem auch zeitgenössische Werke aufgeführt werden. Je nach Größe des Hauses werden verschiedene Opern in einer Spielzeit neu inszeniert. Die erste öffentliche Darbietung einer neuen Oper nennt man Uraufführung, die erste öffentliche Darbietung einer Oper in einer neuen Inszenierung Premiere.
Nach und nach hat sich ein praxiserprobter, mehr oder weniger enger Kanon an Opern herausgebildet, die regelmäßig auf dem Spielplan stehen. Etwa 150 Opern bilden diesen nicht festgeschriebenen Kanon im Kern. Entsprechend hat sich das Interesse vor allem des Feuilletons von den vielfach bereits bekannten Werken hin zu deren Interpretation verlagert, wobei vor allem die Inszenierung in den Vordergrund rückt. Das Publikum verbindet seine Lieblingsopern oft mit bestimmten Traditionen, die zum Teil auch in Konventionen erstarrt sind, und reagiert auf radikale Deutungsansätze (Regietheater) kontrovers.
Bis zur Mitte der 1960er Jahre wurden Opern zumeist in der jeweiligen Landessprache des Aufführungsortes aufgeführt. So wurden Verdi-Opern in Deutschland in deutscher Sprache und Wagner-Opern in Italien in italienischer Sprache gesungen, wie auch Radio- und Fernsehaufzeichnungen belegen. Bereits zuvor gab es jedoch Theater, die Opern in der jeweiligen Originalsprache aufführten, etwa die Metropolitan Opera in New York. Auch die Salzburger Festspiele zeigten Opern stets ausschließlich in der Originalsprache. Aufgrund eines Vertrages mit der Mailänder Scala, bei dem sich italienische Sänger verpflichteten, auch an der Wiener Staatsoper zu singen, führte Herbert von Karajan 1956 an der Wiener Staatsoper das Prinzip ein, Opern in der Originalsprache aufzuführen. Mit seiner Begründung, die Einheit von Wort und Musik gehe bei Übersetzungen in eine andere Sprache verloren, wurden Opern allmählich immer mehr in ihrer ursprünglichen Form aufgeführt. Auch der Schallplatten und Sänger-Markt, der sich zunehmend internationalisierte, trug entscheidend zu dieser Entwicklung bei. In der DDR gab es hingegen weiterhin eine große Tradition von Übersetzungen, jedoch wurde mit neuen Übertragungen (z. B. Walter Felsenstein, Siegfried Schoenbohm) versucht, den Inhalt des Originals genauer, sprachlich gelungener und vor allem musikalisch passender umzusetzen. Heute werden in fast allen großen Opernhäusern Opern in der Originalsprache aufgeführt und dazu simultan Übertitel eingeblendet.
An vielen kleineren Theatern, vor allem im Osten Deutschlands, gibt es noch Aufführungen in deutscher Sprache. Auch gibt es in einigen Städten (z. B. Berlin, München, Wien) mehrere Opernhäuser, von denen eines Opern in Übersetzungen aufführt, wie etwa die Volksoper Wien, die Komische Oper Berlin, das Staatstheater am Gärtnerplatz in München, oder in London die English National Opera. Hin und wieder gibt es auch eine autorisierte Übersetzung (wie im Falle der Opern Leoš Janáčeks, deren deutscher Text von Janáčeks Freund Max Brod stammt, so dass auch der deutsche Text als original gelten darf). Schwierig gestaltet sich die Aufführung in Originalsprache auch immer dann, wenn Dialoge in dem Werk vorkommen. Hier gibt es auch Mischformen, das heißt, gesprochene Texte werden übersetzt, gesungene erklingen jedoch in Originalsprache. Im Bereich Singspiel, Operette, Musical ist daher die übersetzte Musiktheateraufführung weit verbreitet. Für die exakte Übersetzung aus einer Fremdsprache ist am Theater die Dramaturgie zuständig. Wenn die Sprachkenntnisse der Korrepetitoren vertieft werden sollen, werden auch spezialisierte Coaches für eine Fremdsprache hinzugezogen.

Karate [kaɺate]  anhören?/i (japanisch 空手, dt. „leere Hand“) ist eine Kampfkunst, deren Geschichte sich sicher bis ins Okinawa des 19. Jahrhunderts zurückverfolgen lässt, wo einheimische okinawanische Traditionen (okinawa Ti, 手) mit chinesischen Einflüssen (jap. Shorin Kempō / Kenpō; chin. Shàolín Quánfǎ) zum historischen Tōde (okin. Tōdi, 唐手) verschmolzen. Zu Beginn des 20. Jahrhunderts fand dieses seinen Weg nach Japan und wurde nach dem Zweiten Weltkrieg von dort als Karate über die ganze Welt verbreitet.
Inhaltlich wird Karate vor allem durch Schlag-, Stoß-, Tritt- und Blocktechniken sowie Fußfegetechniken als Kern des Trainings charakterisiert. Einige wenige Hebel und Würfe werden (nach ausreichender Beherrschung der Grundtechniken) ebenfalls gelehrt, im fortgeschrittenen Training werden auch Würgegriffe und Nervenpunkttechniken geübt. Manchmal wird die Anwendung von Techniken unter Zuhilfenahme von Kobudōwaffen geübt, wobei das Waffentraining kein integraler Bestandteil des Karate ist.
Recht hoher Wert wird meistens auf die körperliche Kondition gelegt, die heutzutage insbesondere Beweglichkeit, Schnellkraft und anaerobe Belastbarkeit zum Ziel hat. Die Abhärtung der Gliedmaßen u. a. mit dem Ziel des Bruchtests (jap. Tameshiwari, 試し割り), also des Zerschlagens von Brettern oder Ziegeln, ist heute weniger populär, wird aber von einzelnen Stilen (Beispielsweise: Okinawan Goju Ryu) immer noch betrieben.
Das moderne Karate-Training ist häufig eher sportlich orientiert. Das heißt, dass dem Wettkampf eine große Bedeutung zukommt. Diese Orientierung wird häufig kritisiert, da man glaubt, dass dadurch die Vermittlung effektiver Selbstverteidigungstechniken, die durchaus zum Karate gehören, eingeschränkt und das Karate verwässert wird.
Karate-„dō“ (japanisch 空手道  ‚Weg der leeren Hand‘) wurde früher meist nur als Karate bezeichnet und ist unter dieser Bezeichnung noch heute am häufigsten geführt. Der Zusatz „dō“ wird verwendet, um den philosophischen Hintergrund der Kunst und ihre Bedeutung als Lebensweg zu unterstreichen.
Bis in die 1930er-Jahre hinein war die Schreibweise „唐手“ gebräuchlich, was wörtlich „chinesische Hand“ oder „fremdländische Hand“ bedeutet.[Anm. 1][1][2][3][4] Das Schriftzeichen „唐“ mit der sino-japanischen Lesung tō und der japanischen Lesung kara bezog sich auf das China der Tang-Dynastie (618 bis 907 n. Chr.). Damit waren die chinesischen Ursprünge bereits im Namen der Kampfkunst manifestiert. Vermutlich aus politischen Gründen – Japanischer Nationalismus – ging man zu Beginn des 20. Jahrhunderts, initialisiert von Funakoshi Gichin, in Japan dazu über, die homophone Schreibung kara „空“, mit der Bedeutung für „leer, Leere“ zu verwenden. Aus dem historischen „chinesische Hand“ oder „fremdländische Hand“ (karate, 唐手) wurde das heutige „Karate“ (空手) mit der Bedeutung für „leere Hand“. Das neue Zeichen wurde wie das alte kara gelesen und war auch von der Bedeutung her insofern passend, als im Karate meist mit leeren Händen, also ohne Waffen, gekämpft wird (vgl. Tang Soo Do).
Im Deutschen ist bei der Aussprache des Wortes „Karate“ eine Betonung der zweiten Silbe verbreitet. Oft wird sogar wie in mehreren romanischen Sprachen, zum Beispiel im Französischen oder Portugiesischen, auf „te“ betont; im Spanischen hingegen auf der ersten Silbe „Ká“.
Nach der japanischen Aussprache des Wortes dagegen ist eine gleichwertige Akzentuierung jeder Silbe üblich.
Die Legende erzählt, dass der buddhistische Mönch Daruma Taishi (japanisch 達磨大師（ダルマ・たいし）, dt. Meister Bodhidharma, in chinesischen Chroniken als „blauäugiger Mönch“ bekannt[5]) aus Persien[6][7] oder Kanchipuram (Südindien) im 6. Jahrhundert das Kloster Shaolin (jap. Shōrinji, 少林寺) erreicht und dort nicht nur den Chán (Zen-Buddhismus) begründet, sondern die Mönche auch in körperlichen Übungen unterwiesen habe, damit sie das lange Meditieren aushalten konnten. So sei das Shaolin Kung Fu (korrekt Shaolin-Quánfǎ, jap. Shōrin Kempō / Kenpō) entstanden, aus dem sich dann viele andere chinesische Kampfkunststile (Wushu) entwickelt hätten.
Da Karate um seine chinesischen Wurzeln weiß, betrachtet es sich ebenfalls gerne als Nachfahre jener Tradition (Chan, Bodhidharma, Shaolin), deren Historizität im Dunkeln liegt und unter Historikern umstritten ist. Trotzdem ziert das Bildnis von Daruma so manches Dōjō.
Karate in seiner heutigen Form entwickelte sich auf der pazifischen Kette der Ryūkyū-Inseln, insbesondere auf der Hauptinsel Okinawa. Diese liegt ca. 500 Kilometer südlich der japanischen Hauptinsel Kyūshū zwischen Südchinesischem Meer und Pazifischem Ozean. Heute ist die Insel Okinawa ein Teil der gleichnamigen Präfektur Japans. Bereits im 14. Jahrhundert unterhielt Okinawa, damals Zentrum des unabhängigen Inselkönigreichs Ryūkyū, rege Handelskontakte zu Japan, China, Korea und Südostasien.
Die urbanen Zentren der Insel, Naha, Shuri und Tomari, waren damals wichtige Umschlagplätze für Waren und boten damit ein Forum für den kulturellen Austausch mit dem chinesischen Festland. Dadurch gelangten erste Eindrücke chinesischer Kampftechniken des Kempō / Kenpō (chinesisch 拳法, Pinyin Quánfǎ[Anm. 2][8][9], veraltet nach W.G. Ch'üan-Fa, wörtlich „Methode der Faust“, korrekt „Kampftechnik, Technik der Kampfkunst, Technik des Faustkampfs“)[10][11][12][13] nach Okinawa, wo sie sich mit dem einheimischen Kampfsystem des Te / De (okin. Tī, 手) vermischten und sich so zum Tōde (okin. Tōdī, 唐手) oder Okinawa-Te (okin. Uchinādī – „Hand aus Okinawa“, 沖縄手) weiterentwickelten. Te bedeutet wörtlich „Hand“, im übertragenen Sinne auch „Technik“ bzw. „Handtechnik“. Der ursprüngliche Begriff für Tōde oder Karate (japanisch 唐手) kann daher frei als „Handtechnik aus dem Land der Tang“ (China) übersetzt werden (meint aber natürlich die verschiedenen Techniken als Ganzes).
Die unterschiedliche wirtschaftliche Bedeutung der Inseln führte dazu, dass sie ständig von Unruhen und Aufständen heimgesucht wurden. Im Jahre 1422 gelang es schließlich König Sho Hashi, die Inseln zu einen. Zur Erhaltung des Friedens in der aufständischen Bevölkerung verbot er daraufhin das Tragen jeglicher Waffen. Seit 1477 regierte sein Nachfolger Shō Shin und bekräftigte die Politik des Waffenverbotes seines Vorgängers. Um die einzelnen Regionen zu kontrollieren, verpflichtete er sämtliche Fürsten zum dauerhaften Aufenthalt an seinem Hof in Shuri – eine Kontrollmöglichkeit, die später von den Tokugawa-Shōgunen kopiert wurde. Durch das Waffenverbot erfreute sich die waffenlose Kampfkunst des Okinawa-Te erstmals wachsender Beliebtheit, und viele ihrer Meister reisten nach China, um sich dort durch das Training des chinesischen Quánfǎ fortzubilden.
1609 besetzten die Shimazu aus Satsuma die Inselkette und verschärften das Waffenverbot dahingehend, dass sogar der Besitz jeglicher Waffen, selbst Zeremonienwaffen, unter schwere Strafe gestellt wurde. Dieses Waffenverbot wurde als Katanagari („Jagd nach Schwertern“, 刀狩) bezeichnet. Schwerter, Dolche, Messer und jegliche Klingenwerkzeuge wurden systematisch eingesammelt. Dies ging sogar so weit, dass einem Dorf nur ein Küchenmesser zugestanden wurde, das mit einem Seil an den Dorfbrunnen (oder an einer anderen zentralen Stelle) befestigt und streng bewacht wurde.
Das verschärfte Waffenverbot sollte Unruhen und bewaffnete Widerstände gegen die neuen Machthaber unterbinden. Jedoch hatten japanische Samurai das Recht der sogenannten „Schwertprobe“, dem zufolge sie die Schärfe ihrer Schwertklinge an Leichen, Verwundeten oder auch willkürlich an einem Bauern erproben konnten, was auch vorkam. Die Annexion führte somit zu einer gesteigerten Notwendigkeit zur Selbstverteidigung, zumal damals auf dem feudalen Okinawa Polizeiwesen und Rechtsschutz fehlten, die den Einzelnen vor solchen Eingriffen schützen konnten. Der Mangel an staatlichen Rechtsschutzinstitutionen und die gesteigerte Wehrnotwendigkeit vor Willkürakten der neuen Machthaber begründeten also einen Intensivierungs- und Subtilisierungsprozess des Kampfsystems Te zur Kampfkunst Karate.
Ungefähr zwanzig Jahre dauerte es, bis sich die großen Meister des Okinawa-Te zu einem geheimen oppositionellen Bund zusammenschlossen und festlegten, dass Okinawa-Te nur noch im Geheimen an ausgesuchte Personen weitergegeben werden sollte.
Währenddessen entwickelte sich in der bäuerlich geprägten Bevölkerung das Kobudō, das Werkzeuge und Alltagsgegenstände mit seinen speziellen Techniken zu Waffen verwandelte. Dabei gingen spirituelle, mentale und gesundheitliche Aspekte, wie sie im Quánfǎ gelehrt wurden, verloren. Auf Effizienz ausgelegt, wurden Techniken, die unnötiges Risiko bargen, wie beispielsweise Fußtritte im Kopfbereich, nicht trainiert. So lässt sich in diesem Zusammenhang von einer Auslese der Techniken sprechen. Kobudō und seine aus Alltagsgegenständen und Werkzeugen hergestellten Waffen konnten schon aus wirtschaftlichen Gründen nicht verboten werden, da sie für die Versorgung der Bevölkerung sowie der Besatzer schlicht notwendig waren.
Allerdings war es sehr schwer, mit diesen Waffen einem ausgebildeten und gut bewaffneten Krieger im Kampf gegenüberzutreten. Deshalb entwickelte sich in Okinawa-Te und Kobudō, die damals noch eng miteinander verknüpft gelehrt wurden, die Maxime, möglichst nicht getroffen zu werden und gleichzeitig die wenigen Gelegenheiten, die sich boten, zu nutzen, den Gegner mit einem einzigen Schlag zu töten. Dieses für das Karate spezifische Prinzip heißt Ikken hissatsu. Die Auslese von möglichst effizienten Kampftechniken und das Ikken-Hissatsu-Prinzip brachten dem Karate den ungerechtfertigten Ruf ein, ein aggressives Kampfsystem, ja sogar die „Härteste aller Kampfsportarten“ zu sein (siehe dazu weiter unten Film und Medien).
Die tödliche Wirkung dieser Kampfkunst führte dazu, dass die japanischen Besatzer erneut das Verbot ausdehnten, und das Lehren von Okinawa-Te ebenfalls unter drakonische Strafe stellten. Allerdings wurde es weiterhin im Geheimen unterrichtet. Damit wurde die Kenntnis des Te für lange Zeit auf kleine elitäre Schulen oder einzelne Familien beschränkt, da die Möglichkeit zum Studium der Kampfkünste auf dem chinesischen Festland nur wenigen begüterten Bürgern zur Verfügung stand.
Weil die Kunst des Schreibens in der Bevölkerung damals kaum verbreitet war, und man aus Geheimhaltungsgründen dazu gezwungen war, wurden keinerlei schriftliche Aufzeichnungen angefertigt, wie das in chinesischen Kung-Fu-Stilen manchmal der Fall war (siehe Bubishi). Man verließ sich auf die mündliche Überlieferung und die direkte Weitergabe. Zu diesem Zweck bündelten die Meister die zu lehrenden Kampftechniken in didaktischen zusammenhängenden Einheiten zu festgelegten Abläufen oder Formen. Diese genau vorgegebenen Abläufe werden als Kata bezeichnet. Um dem Geheimhaltungszweck der Okinawa-Te Rechnung zu tragen, mussten diese Abläufe vor Nicht-Eingeweihten der Kampfschule (also vor potenziellen Ausspähern) chiffriert werden. Dabei bediente man sich als Chiffrierungscode der traditionellen Stammestänze (odori), die den systematischen Aufbau der Kata beeinflussten. So besitzt jede Kata noch bis heute ein strenges Schrittdiagramm (Embusen). Die Effizienz der Chiffrierung der Techniken in Form einer Kata zeigt sich bei der Kata-Demonstration vor Laien: Für den Laien und in den ungeübten Augen des Karate-Anfängers muten die Bewegungen befremdlich oder nichtssagend an. Die eigentliche Bedeutung der Kampfhandlungen erschließt sich einem erst durch intensives Kata-Studium und der „Dechiffrierung“ des Kata. Dies erfolgt im Bunkai-Training. Eine Kata ist also ein traditionelles, systematisches Kampfhandlungsprogramm und das hauptsächliche Medium der Tradition des Karate.
Der erste noch namentlich bekannte Meister des Tōde war vermutlich Chatan Yara, der etliche Jahre in China lebte und dort die Kampfkunst seines Meisters erlernte. Der Legende nach unterrichtete er wohl „Tōde“ Sakugawa, einen Schüler von Peichin Takahara. Auf Sakugawa geht eine Variante der Kata Kushanku, benannt nach einem chinesischen Diplomaten, zurück. Der bekannteste Schüler Sakugawas war „Bushi“ Matsumura Sōkon, der später sogar den Herrscher von Okinawa unterrichtete.
Bis zum Ende des 19. Jahrhunderts wurde Karate stets im Geheimen geübt und ausschließlich von Meister zu Schüler weitergegeben. Während der Meiji-Restauration wurde Okinawa im Jahre 1875 offiziell zu einer japanischen Präfektur erklärt. In dieser Zeit des gesellschaftlichen Umbruchs, in der sich die okinawanische Bevölkerung den japanischen Lebensgewohnheiten anpasste und Japan sich nach jahrhundertelanger Isolierung wieder der Welt öffnete, begann Karate wieder stärker in die Öffentlichkeit zu drängen.
Der Kommissar für Erziehung in der Präfektur Okinawa, Ogawa Shintaro, wurde 1890 während der Musterung junger Männer für den Wehrdienst auf die besonders gute körperliche Verfassung einer Gruppe junger Männer aufmerksam. Diese gaben an, auf der Jinjo Koto Shogakko (Jinjo-Koto-Grundschule) im Karate unterrichtet zu werden. Daraufhin beauftragte die Lokalregierung den Meister Yasutsune Itosu damit, einen Lehrplan zu erstellen, der unter anderem einfache und grundlegende Kata (Pinan oder Heian) enthielt, aus denen er Taktik und Methodik des Kämpfens weitgehend entfernte und den gesundheitlichen Aspekt wie Haltung, Beweglichkeit, Gelenkigkeit, Atmung, Spannung und Entspannung in den Vordergrund stellte. Karate wurde dann 1902 offiziell Schulsport auf Okinawa. Dieses einschneidende Ereignis in der Entwicklung des Karate markiert den Punkt, an dem das Erlernen und Üben der Kampftechnik nicht mehr länger nur der Selbstverteidigung diente, sondern auch als eine Art Leibesertüchtigung angesehen wurde.
Nach Beginn des Jahres 1900 begann von Okinawa aus eine Auswanderungswelle nach Hawaii. Dadurch kam Karate erstmals in die USA, die Hawaii 1898 annektiert hatten.
Funakoshi Gichin, ein Schüler der Meister Yasutsune Itosu und Ankō Asato, tat sich bei der Reform des Karate besonders hervor: Auf der Grundlage des Shōrin-Ryū (auch Shuri-Te nach der Ursprungsstadt) und des Shōrei-Ryū (Naha-Te) begann er, Karate zu systematisieren. Er verstand es neben der reinen körperlichen Ertüchtigung auch als Mittel zur Charakterbildung.
Neben den genannten drei Meistern war Kanryo Higashionna ein weiterer einflussreicher Reformer. Sein Stil integrierte weiche, ausweichende Defensivtechniken und harte, direkte Kontertechniken. Seine Schüler Chōjun Miyagi und Kenwa Mabuni entwickelten auf dieser Basis die eigenen Stilrichtungen Gōjū-Ryū bzw. Shitō-Ryū, die später große Verbreitung finden sollten.
In den Jahren von 1906 bis 1915 bereiste Funakoshi mit einer Auswahl seiner besten Schüler ganz Okinawa und hielt öffentliche Karate-Vorführungen ab. In den darauffolgenden Jahren wurde der damalige Kronprinz und spätere Kaiser Hirohito Zeuge einer solchen Aufführung und lud Funakoshi, der bereits Präsident des Ryukyu-Ryu Budokan – einer okinawanischen Kampfkunstvereinigung – war, ein, bei einer nationalen Budō-Veranstaltung 1922 in Tōkyō sein Karate in einem Vortrag zu präsentieren. Dieser Vortrag erfuhr großes Interesse, und Funakoshi wurde eingeladen, seine Kunst im Kōdōkan praktisch vorzuführen. Die begeisterten Zuschauer, allen voran der Begründer des Judo, Kanō Jigorō, überredeten Funakoshi, am Kōdōkan zu bleiben und zu lehren. Zwei Jahre später, 1924, gründete Funakoshi sein erstes Dōjō.
Über die Schulen kam Karate auch bald zur sportlichen Ertüchtigung an die Universitäten, wo damals zum Zwecke der militärischen Ausbildung bereits Judo und Kendō gelehrt wurden. Diese Entwicklung, die die okinawanischen Meister zur Verbreitung des Karate billigend in Kauf nehmen mussten, führte zur Anerkennung von Karate als „nationale Kampfkunst“; Karate war damit endgültig japanisiert.
Nach dem Vorbild des bereits im Judo etablierten Systems wurde im Laufe der dreißiger Jahre dann der Karate-Gi sowie die hierarchische Einteilung in Schüler- und Meistergrade, erkennbar an Gürtelfarben, im Karate eingeführt; mit der auch politisch motivierten Absicht eine stärkere Gruppenidentität und hierarchische Struktur zu etablieren.
Aufgrund seiner Bemühungen wurde daraufhin Karate an der Shoka-Universität, der Takushoku-Universität, der Waseda-Universität und an der Japanischen Medizinischen Hochschule eingeführt. Das erste offizielle Buch über Karate wurde von Gichin Funakoshi unter dem Namen Ryu Kyu Kempo Karate im Jahre 1922 veröffentlicht. Es folgte 1925 die überarbeitete Version Rentan Goshin Karate Jutsu. Sein Hauptwerk erschien unter dem Titel Karate Do Kyohan 1935 (diese Version wurde 1958 noch einmal um die karatespezifischen Entwicklungen der letzten 25 Jahre erweitert). Seine Biographie erschien unter dem Namen Karate-dō Ichi-ro (Karate-dō – mein Weg), in dem er sein Leben mit Karate schildert.
Nach dem Zweiten Weltkrieg wurde Karate durch Funakoshis Beziehungen zum Ausbildungsministerium als Leibeserziehung und nicht als kriegerische Kunst eingestuft, was es ermöglichte, Karate auch nach dem Zweiten Weltkrieg zur Zeit der Besatzung in Japan zu lehren.
Über Hawaii sowie die amerikanische Besatzung Japans und insbesondere Okinawas fand Karate im Laufe der 1950er und 1960er Jahre als Sportart zunächst in den USA und dann auch in Europa eine immer stärkere Verbreitung.
Aus der nach Funakoshi beziehungsweise dessen schriftstellerischen Pseudonym Shōtō benannten Schule Shōtōkan („Haus des Shōtō“) ging die erste international agierende Karate-Organisation, die JKA hervor, die noch heute einer der einflussreichsten Karateverbände der Welt ist. Funakoshi und die übrigen alten Meister lehnten die Institutionalisierung und Versportlichung sowie die damit einhergehende Aufspaltung in verschiedene Stilrichtungen gänzlich ab.
1954 gründete Henry Plée in Paris das erste europäische Budō-Dōjō. Der deutsche Judoka Jürgen Seydel kam auf einem Judo-Lehrgang in Frankreich erstmals bei Meister Murakami mit Karate in Kontakt, den er begeistert einlud, auch in Deutschland zu lehren. Aus den Teilnehmern dieser Lehrgänge entwickelte sich zunächst innerhalb der Judo-Verbände eine Unterorganisation, die Karate lehrte und aus der schließlich im Jahre 1961 der erste deutsche Dachverband der Karateka, der Deutsche Karate Bund, hervorging.
Den ersten Karateverein in Deutschland gründete schließlich Jürgen Seydel im Jahr 1957 unter dem Namen „Budokan Bad Homburg“ in Bad Homburg vor der Höhe, in dem Elvis Presley während seiner Armeezeit in Deutschland trainierte.
Die größte Ausbreitung des Karate in Deutschland gab es in den 1970er, 1980er und 1990er Jahren unter Hideo Ochi (bis dieser 1993 den DJKB, den deutschen Ableger der JKA gründete) als Bundestrainer des DKB und der Nachfolgeorganisation DKV als Zusammenschluss verschiedener Stilrichtungen. Ochi hat somit das Karate in Deutschland Ende des 20. Jahrhunderts maßgeblich verbreitet und aufgebaut.
In der DDR spielte Karate offiziell nur innerhalb der Sicherheitsorgane eine Rolle: Als junger Sportstudent beschäftigte sich Karl-Heinz Ruffert Mitte der 1970er Jahre in seiner Diplomarbeit an der Martin-Luther-Universität Halle-Wittenberg mit Karate – dadurch wurde das Ministerium für Staatssicherheit auf ihn aufmerksam. Als Offizier des MfS schließlich führte Ruffert Karate in die Ausbildung des Inlandsgeheimdienstes ein.[14] Unter der Führung des Rektors der DHfK, Gerhard Lehmann, wurde Karate in der DDR ab 1989 offiziell als Kampfsport anerkannt und in den Deutschen Judo-Verband aufgenommen.
Shōtōkan ist heute der mit Abstand am weitesten verbreitete Karatestil in Deutschland, gefolgt von Gōjū-Ryū. Seit der Jahrtausendwende gibt es auch zunehmend einzelne Dōjō in Deutschland, bei denen verschiedene Okinawa-Stile trainiert werden, beispielsweise Matsubayashi-Ryū.
Das japanische Karate teilt sich heute in vier große Stilrichtungen, nämlich Gōjū-Ryū, Shōtōkan, Shitō-Ryū und Wadō-Ryū auf, die ihrerseits auf zwei ebenfalls recht verbreitete okinawanische Stile, Shōrei-Ryū und Shōrin-Ryū, zurückgehen. Viele kleinere neuere Stilrichtungen begründen sich aus einer oder mehreren dieser sechs Schulen.[15][16]
Aber auch ursprüngliche Stile wie z. B. Uechi-ryū werden heute noch betrieben.
Es gibt im Karatetraining eine hierarchische Unterscheidung: Neben dem Sensei, dem Lehrer, gibt es die Senpai und Kōhai.
Jedes Karatetraining beginnt und endet traditionell mit einer kurzen Meditation (Mokuso). Dies soll auch den friedfertigen Zweck der Übungen zum Ausdruck bringen. Die kurze Meditation lässt auf die Tradition des Karate als Weglehre schließen, auch wenn das heutige Training nach modernen sportlichen Gesichtspunkten (so z. B. als Fitness- oder Wettkampftraining), und nicht als Übung des Weges (im Sinne des klassischen Karatedō) ausgerichtet ist.
Auch beginnt und endet jedes Karatetraining, jede Übung und jede Kata mit einem Gruß. Dadurch wird das erste Prinzip der 20 Regeln von Gichin Funakoshi zum Ausdruck gebracht: „karate wa rei ni hajimari rei ni owaru koto“ – „Karate beginnt und endet mit Respekt!“
Die herausragende Respekterweisung gegenüber dem Meister äußert sich mitunter in kurios anmutenden Regeln. So wird es etwa als unhöflich angesehen, hinter dem Rücken des Meisters zu gehen. Dies wurzelt keineswegs in der Vorstellung, hinterrücks angegriffen zu werden, sondern im Gedanken, dass ein „Vorbei-Schleichen“ auf eine mangelhafte Lehrer-Schüler-Beziehung (wegen mangelnder Würdigung) schließen lässt.
In vielen Dōjōs ist es üblich, vor Betreten und Verlassen der Halle die darin Versammelten mit einer kurzen Verbeugung zu begrüßen, eventuell wird auch der Shōmen des Dōjō mit einer weiteren kurzen Verbeugung beim Betreten und Verlassen gegrüßt.
Danach wird gemeinsam ein Grußritus (Rei) zelebriert, in der sich Schüler und Meister voreinander und vor den alten Meistern und Vorfahren (im Geiste, repräsentiert an der Stirnseite, dem Shōmen des Dōjō) verneigen.
Während der Begrüßungszeremonie gelten ungeschriebene Regeln:
Die im Folgenden beschriebene Zeremonie ist als Beispiel zu verstehen, denn sie variiert zwischen Stilrichtungen oder auch Dōjōs. Sie macht aber das Prinzip deutlich.
Die vorigen Punkte beschreiben den Ablauf einer Begrüßung, wie sie im Shōtōkan Ryū üblich (erkennbar durch den dort stark verbreiteten Ausdruck Ossu!) ist. Neben der anderen Art und Weise, wie man Seiza einnimmt und wie die Hände geführt werden, erfolgt bei Begrüßungen im Wadō-Ryū beispielsweise zuerst je nach den vor Ort herrschenden Bedingungen eine Begrüßung zur Stirnseite des Dōjō entweder mit „shōmen ni!“ oder bei Vorhandensein eines Altars mit „shinzen ni rei!“, bei der alle, auch der Sensei, gerade nach vorn ausgerichtet sind. Darauf wendet sich der Sensei seinen Schülern zu, und es folgt die Begrüßung des Sensei. Hierfür richten sich alle Schüler für gewöhnlich zu diesem aus und verbeugen sich stumm. Schließlich richten sich die Schüler beim Kommando „otagai ni rei!“ wieder frontal aus und begrüßen sich untereinander mit den Worten „Onegai shimasu!“.
In manch traditionellen Schulen und Vereinen ist es auch üblich, an der Stelle nach der Begrüßung im Knien und vor dem Aufstehen die Dōjōkun oder die 20 Paragraphen des Karate von den gelehrigsten Schülern (stellvertretend für alle) rezitieren zu lassen.
Die traditionelle Verabschiedung im Training erfolgt nach dem gleichen Muster wie die Begrüßung.
Wie in allen anderen Dō-Künsten üblich werden im Umgang der strenge Kodex des Reishiki und die Dōjōkun beachtet.
Jeder Karateka trägt einen Karate-Gi, bestehend aus einer einfachen an der Hüfte geschnürten weißen Hose, Zubon, früher bestehend aus Leinen, heute aus Baumwolle und einer Jacke, Uwagi genannt, aus dem gleichen Material. Gehalten wird die Jacke (meist neben einer leichten Schnürung) durch einen gefärbten Gürtel, dem Obi. Es wird grundsätzlich barfuß trainiert.
Dass Karateka überhaupt uniforme Trainingskleidung trugen, war nicht selbstverständlich. Das Okinawa-Te wurde von jeher in robuster Alltagskleidung trainiert. Ebenso existierte in der Zeit, da Karate noch eine insulane Kampfkunst war, kein Graduierungssystem. Der Meister wusste über den jeweiligen Fortschritt seines Schülers ohnehin Bescheid. Die Einführung einheitlicher Trainingskleidung und eines Graduierungssystems erfolgte erst nach Funakoshi Gichins Begegnung mit dem Kōdōkan-Gründer Kanō Jigorō, der eben jenes im Judo veranlasste.
Die Einführung einheitlicher Kleidung und eines Graduierungssystems ist nur im sozio-historischen Kontext zu verstehen.
Nach der Meiji-Restauration, der Auflösung des Samurai-Standes und der Einführung von Faustfeuerwaffen war die Bedeutung der traditionellen Kriegskünste zurückgegangen. Mit dem aufkeimenden Nationalismus in Japan gewannen die klassischen Kampfkünste wieder an Bedeutung, die am Verlauf der japanischen Geschichte einen entscheidenden Anteil hatten. Man sah die Kampfkünste als Bestandteil der kulturellen und nationalen Identität an. Die Kampfkünste – so auch das Karate – erhielten den Stempel der nationalistischen Politik jener Zeit.
Die Kampfkünste durchliefen eine Militarisierung westlicher Prägung. Aus diesem Blickwinkel sind die einheitliche Kleidung als Uniform, und das Graduierungssystem nach Gürtelfarben als Hierarchie nach militärischen Dienstgraden zu verstehen. Die Aufstellung in einer Reihe gleicht der militärischen Formation. Auch gewisse Stände ähneln militärischen Ständen: So sieht der Stand Musubi-Dachi aus wie die Grundstellung beim Kommando „Stillgestanden!“ bzw. „Achtung!“, und der Shizen-Tai wie der erleichterte Stand bei „Rührt Euch!“.
Die Graduierung durch farbige Gürtel[18] wurde wahrscheinlich aus dem Judo[19] übernommen. Kanō Jigorō, Gründer des Kōdōkan Judo, hat dieses System im 19. Jahrhundert erstmals verwendet. Vorher gab es kein Graduierungssystem nach Gürtelfarben in den Kampfkünsten aus Okinawa und Japan.
In Graduierungen wird zwischen den Schülergraden, den sogenannten Kyū, und den Meisterschülern bzw. Meistergraden, den sogenannten Dan, unterschieden. Jeder dieser Stufen wird eine Gürtelfarbe zugeordnet. In dem in Deutschland gebräuchlichsten Graduierungssystem existieren 9 Kyū- und 10 Dan-Grade. Der 9. Kyū ist hierbei die unterste Stufe, der 10. Dan die höchste.
Die Gürtelfarben sind eine Erfindung des modernen Budō. Viele Verbände verfolgen damit neben der beabsichtigten Motivation der Mitglieder auch finanzielle Interessen, denn für jede abzulegende Prüfung wird eine Gebühr erhoben.
Bis zum Jahre 1981 existierte im Deutschen Karate Verband eine Abstufung über fünf Schülergrade (5. bis 1. Kyū), wobei für jeden Kyū-Grad eine Farbe in der vorgenannten Reihenfolge stand. Diese Abstufung wurde zugunsten einer feineren Differenzierung durch vorstehende Graduierungen ersetzt.
Zum Erlangen des nächsthöheren Schüler- bzw. Meistergrades werden Prüfungen nach einem festen Programm und einer Wartezeit, je nach Kyū- und Dan-Graden verschieden, abgelegt. Die Programme der Prüfungen unterscheiden sich von Verband zu Verband, gelegentlich gibt es sogar Unterschiede in einzelnen Dōjō.
Das Ablegen der Prüfungen dient als Ansporn und Bestätigung des Erreichten, ähnlich wie in unserem Schulsystem.
In den Prüfungen wird auf Technikausführung, Haltung, Aufmerksamkeit, Kampfgeist, Konzentration und Willen geachtet. Der Gesamteindruck entscheidet.
Bei höheren Meistergraden (meist ab dem 5. Dan) erhöht sich der theoretische Prüfungsanteil erheblich. In einigen wenigen Organisationen werden diese Dangrade gar nur aufgrund besonderer Leistungen und Verdienste verliehen. Im Shōtōkai ist der 5. Dan (Godan) die höchste Auszeichnung.
Karate hat als Budōdisziplin, zu denen zum Beispiel auch Kendō und Judo gehören, einen spirituellen Kern aus weltanschaulichen Elementen des Zen und des Taoismus. Diese Weltanschauungen dienen dazu, die Systeme des Budō zu erklären und bilden nicht die Basis dieser Kampfkünste.
Einen guten Einblick in die Grundsätze der Karate-Philosophie bieten die 20 Paragraphen des Karate von Gichin Funakoshi.
In Japan werden die von Gichin Funakoshi aufgestellten 20 Regeln des für Karateka angemessenen Verhaltens als Shōtō Nijū Kun (japanisch 松濤二十訓, wörtlich die 20 Regeln von Shōtō, wobei Shōtō der Künstlername Funakoshis war) oder als Karate Nijū Kajō (japanisch 空手二十箇条, wörtlich die 20 Paragraphen des Karate) bezeichnet. Im deutschen Karate vermischt sich der Begriff häufig mit dem der Dōjōkun, die eigentlich nur fünf zentrale Regeln umfassen und lange vor Funakoshi und mit Bezug auf alle Kampfkünste vermutlich von buddhistischen Mönchen in Indien aufgestellt wurden.
Zum besseren Verständnis des spirituellen Wesens des Karate kann u. a. auch das Studium des Zen geeignet sein.
Die Wiederholung der Bewegungen, in Kihon (jap. „Grundschule“) und Kata (jap. „Form“) wird von manchen Meistern als Meditation betrachtet. Das Ki, also die Energie des Körpers, das Bewusstsein, das sich beispielsweise in Koordinations- und Reaktionsvermögen äußert, sollen durch körperlich anstrengende, konzentrierte und dynamische Bewegungen gestärkt werden. Da während einer Kata Konzentration gefordert ist, und gleichzeitig die Lebensenergie (Ki) unbeeinflusst vom Bewusstsein im Körper fließt, gilt Kata als „aktive Meditation“. Kata als Meditationsform ist sozusagen das Gegenteil von Zazen: Letzterer ist Versenkung im Verharren, erstere Versenkung in der Bewegung. Bloßes Üben von Techniken in einer Kata allein heißt noch lange nicht, dass die Kata als Meditationsform praktiziert wird. Erst die richtige Geisteshaltung, mit welcher der Karateka die Kata füllt, macht aus einem traditionellen Kampfhandlungsprogramm einen Weg zur spirituellen Selbstfindung und meditativen Übung.
Das Prinzip des Dō (道) findet sich in allen japanischen Kampfkünsten wieder und ist unmöglich umfassend zu beschreiben. Dō ist die japanische Lesart des chinesischen Tao (Dao), das mit dem gleichen Zeichen geschrieben wird. Es bedeutet wörtlich „Weg“ und steht dabei nicht nur für „Weg“ oder „Straße“ im engeren Sinn, sondern auch für „Mittel“ oder „Methode“ im Verständnis eines „Lebensweges“, einer „Lebenseinstellung“.
Dahinter stehen einerseits das taoistisch-schicksalhafte Prinzip, dass das Tao, der Weg, vorgezeichnet ist und die Dinge in ihrer Richtigkeit vorbestimmt; sowie die Einstellung des Nichtanhaftens und der Nichtabhängigkeit von allen Dingen, Gegebenheiten und Bedürfnissen, die im Zen-Buddhismus gelehrt wird.
Der Kodex des Bushidō geht noch weiter: Der bushi (jap. „Krieger“), der Bushidō verinnerlicht hat, befreit sich damit nicht nur von allen materiellen Bedürfnissen, sondern von dem Begehren um jeden Preis zu leben. Das Ende des eigenen Lebens wird damit nicht unbedingt erstrebenswert, aber auf jeden Fall eine zu akzeptierende Tatsache, und der Tod birgt keinen Schrecken mehr.
Diese Haltung war im alten Japan eine hochangesehene geistige Einstellung, die sich in vielen martialischen Verhaltensweisen wie dem Seppuku manifestierte.
Dies darf jedoch auf keinen Fall als Geringschätzung gegenüber dem eigenen Leben oder dem eines anderen aufgefasst werden. Im Gegenteil: Die Aufopferung des eigenen wertvollen Lebens wog vielmehr jede Schmach auf, die ein Krieger zu Lebzeiten auf sich geladen hatte. Das Seppuku, also der rituelle Selbstmord, befreit den Krieger von Schuld und Schande und stellte seine Ehre wieder her.
Das Dō-Prinzip impliziert nun viele verschiedene Konzepte und Verhaltensweisen, die nicht abschließend aufgezählt werden könnten. Deshalb hier nur einige wenige Aspekte: siehe auch Dōjōkun, Die 20 Regeln des Karate
Das Training des Geistes, des Charakters und der inneren Einstellung sind Hauptziele im Karate. Dies wird auch durch den Leitspruch der Japan Karate Association (JKA) dargelegt:
Eine weitere Grundregel im Karate lautet
Damit ist nicht das Training oder der Wettkampf gemeint, da ernsthafte Angriffs-Simulationen zu allen Budō-Künsten gehören. Der Satz verdeutlicht vielmehr den Kodex des Karatedō im täglichen Leben. Gemeint ist, dass sich der Karateka zu einer friedlichen Person entwickeln und nicht auf Streit aus sein soll. Ein Karateka führt also, bildlich gesprochen, niemals den ersten Schlag, was ebenso jegliche Provokation anderer ausschließt.
Das Karatetraining baut auf drei großen Säulen auf, dem Kihon, dem Kumite und der Kata.
Kihon (japanisch 基本) heißt „Grundlage“, „Basis“, „Fundament“ (des Könnens) und wird häufig auch als Grundschule des Karate bezeichnet.
Es umfasst die grundlegenden Techniken, die das Fundament des Karate bilden. Die einzelnen Techniken werden immer wiederholt, entweder langsam oder schnell, kraftvoll oder leicht/locker. Der Bewegungsablauf der einzelnen Technik wird in alle Bestandteile zerlegt und es wird versucht die Ideallinie der Bewegung zu finden, wobei es immer etwas zu optimieren gibt. Der Bewegungsablauf muss optimal verinnerlicht werden – reflexartig abrufbar, da für Denken, Planen und Handeln in einem realen Kampf zu wenig Zeit ist.
Einatmung, Ausatmung, maximale Anspannung des ganzen Körpers im Zielpunkt sind grundlegende Ziele dieses Trainings. Nach asiatischer Vorstellung liegt das Zentrum des Körpers und damit das Kraftzentrum dort, wo idealerweise auch der Körperschwerpunkt liegen sollte. Diesem oft bedeutungsverengend mit Hara (腹, „Bauch“) bezeichneten ideellen Punkt (ca. 2 cm unter dem Bauchnabel) kommt beim Atemtraining besondere Aufmerksamkeit zu (Bauchatmung). Eine gute Balance ist darüber hinaus erstrebenswert und wird oft umschrieben mit dem Finden des „inneren Schwerpunktes“.
Kumite (japanisch 組み手 oder 組手) bedeutet wörtlich „verbundene Hände“ und meint das Üben bzw. den Kampf mit einem, selten mehreren Gegnern (siehe Bunkai).
Das Kumite stellt innerhalb des Trainings eine Form dar, die es dem Trainierenden nach ausreichender Übung ermöglicht, sich in ernsten Situationen angemessen verteidigen zu können. Voraussetzung ist das richtige Verstehen und Einüben elementarer Grundtechniken aus dem Kihon und der Kata. Wenn die Ausführung der Technik in ihrer Grundform begriffen wurde, wendet man sie im Kumite an. Die Anwendung im Kumite ist sehr wichtig, da die Ausführung von Techniken im Freikampf nicht der vorgeschriebenen Form entsprechen müssen, da man oftmals bei überraschenden Angriffen sofort von der Kampfhaltung zur Endstellung der Abwehr gelangen muss.
Es gibt verschiedene Formen des Kumite, die mit steigendem Anspruch von einer einzigen, abgesprochenen, mehrfach ausgeführten Technik bis hin zum freien Kampf in ihrer Gestaltung immer offener werden.
Bei Verteidigungstechniken werden hauptsächlich die Arme zu Blocktechniken verwendet. Würfe, Hebel, harte, weiche Blockbewegungen oder auch nur Ausweichen, meist in Kombination mit Schritt- oder Gleitbewegungen. Eine Blockbewegung kann auch als Angriffstechnik ausgeführt werden, was ein sehr gutes Auge voraussetzt; der Angriff des Gegners wird im Ansatz mit einer Abwehrbewegung oder einem Gegenangriff (出会い, deai, „Begegnung, Aufeinandertreffen“) gestoppt.
Beim Angriff wird versucht, die ungedeckten Bereiche bzw. durch die Deckung hindurch den Gegner zu treffen. Es soll möglichst mit absoluter Schnelligkeit ohne vorzeitiges Anspannen der Muskeln konzentriert angegriffen werden, denn erhöhter Krafteinsatz führt während der Bewegung zu Schnelligkeitsverlusten. Der Kraftpunkt liegt am Zielpunkt der Bewegung. Das Prinzip der Angriffstechnik gleicht dem des Pfeiles eines Bogenschützen bei Schlag- und Stoßtechniken und dem einer Peitsche bei geschnappten Techniken.
Das Yakusoku-Kumite (japanisch 約束組手, „abgesprochenes Kumite“) ist die erste Stufe der am Partner/Gegner angewandten Technik. Dabei folgen beide Partner einem vorher festgelegten Ablauf von Angriff- und Verteidigungstechniken, die in der Regel im Wechsel ausgeführt werden. Ziel dieser Übung ist es, die Bewegungen des Partners/Gegners einschätzen zu lernen, sowie die eigenen Grundschul-Techniken in erste Anwendung zu bringen, ein Gefühl für Distanz und Intensität zu erhalten. Diese Form der Übung ist wiederum nach Schwierigkeitsgrad unterteilt.
Beim Jiyū-Kumite (japanisch 自由組手, „freies Kumite“) werden Verteidigung und Angriff frei gewählt, teilweise ohne Ansage oder Bekanntgabe.
Jiyū bedeutet „Freiheit“ oder „Wahlfreiheit“. Allgemein gilt: Man muss, egal ob man die Initiative im Angriff oder in der Abwehr ergreift, aus jeder beliebigen Position heraus reagieren können, ungehindert aller einschränkenden Gedanken, da man in überraschenden Situationen nicht sofort in eine Kampfstellung gehen kann. Es ist also egal, ob man einen Angriff blockt, sperrt, in diesen hineingeht oder selbst zum Angriff übergeht. Wichtig ist nur, all seine Aktionen in der Weise auszuführen, dass man dabei nicht von ablenkenden Gedanken erfasst wird. Der Kopf muss kühl bleiben. Ebenso wie in allen anderen Kampfkünsten hemmen die „Bewegungen im Kopf“ letztlich die Bewegungen des Körpers. Der Geist muss sozusagen ungehindert fließen können, um jede Bewegung des Gegners aufnehmen zu können.
Diese Form des Kampfes stellt die Höchstform des klassischen Kumite dar. Timing, Distanzgefühl, ein selbstbewusstes Auftreten, eine sichere Kampfhaltung, schnelle und geschmeidige Techniken, gehärtete Gliedmaßen, intuitives Erfassen, ein geschultes Auge, Sicherheit in Abwehr, Angriff und Konter … das alles sollte hinführend zum Jiyū-Kumite bereits vorher in den anderen Kumite-Formen sowie im Kihon und in der Kata eingeübt werden. Letzteres wird sich jedoch erst im Jiyū-Kumite sowie im Randori vollends ausbilden: Spontaneität.
Randori (japanisch 乱取り, „freies Üben“, wörtlich „Unruhen/Ungeordnetes abfangen“) ist eine freie Form des Partnertrainings, bei der es darum geht, ein Gespür für den Fluss eines Kampfes, der Bewegungen und der eingesetzten Energie zu bekommen. Dabei ist es nicht zielführend, wie im Kampf Treffer um jeden Preis zu vermeiden, sondern es ist ausdrücklich erwünscht, dass die Trainierenden Treffer bei gut ausgeführten Angriffen auch zulassen. Es sind keine Vorgaben bezüglich der einzusetzenden Techniken gemacht. Die Übenden sollen vielmehr das spontane Handeln aus den sich ergebenden Situationen erlernen. Das Randori sollte locker und gelassen sein, einen freien Fluss der Techniken ermöglichen und keinen Wettkampfcharakter annehmen.
Der Freikampf imitiert entweder reale Selbstverteidigungssituationen oder dient dem Wettkampf (Shiai) bzw. dessen Vorbereitung.
Kennzeichnend im traditionellen Karate ist der beabsichtigte Verzicht auf Trefferwirkung am Gegner.
Absolut notwendig ist die Fähigkeit, Angriffstechniken vor dem Ziel, dem Körper des Gegners, mit einer „starken“ Technik zu arretieren, da ohne Hand- und Kopfschutz geübt wird. Während eines Wettkampfes wäre Trefferwirkung ein Regelverstoß, der je nach Schwere zu einer Verwarnung oder zur Disqualifikation führt. „Schwache“ Techniken führen zu keiner Wertung.
Vollkontakt-Karate-Kampfsysteme gestatten und beabsichtigen in der Wettkampfordnung die Trefferwirkung. Viele dieser Stilrichtungen verwenden dazu auch Schutzausrüstungen wie Kopf- und Gebissschutz sowie einen speziellen Handschuh, der die Fingerknöchel und den Handrücken polstert. Wird der Freikampf als Wettkampf durchgeführt, so gibt es feste Regularien die beispielsweise Würfe über Hüfthöhe, Tritte zum Kopf, sowie Techniken gegen den Genitalbereich oder mit offener Hand zum Hals geführte Schläge aus Sicherheitsgründen verbieten. Ohne Handschuhe sind Angriffe mit den Händen oder Fäusten zum Kopf verboten, wie im Kyokushin-Kai, oder es wird komplette Schutzausrüstung mit Helm, Weste, Tiefschutz, Unterarm- und Schienbeinschoner und evtl. ein Spannschutz verwendet, wie auch im Taekwondo.
Kata (japanisch 型, 形) bedeutet „Form“, „Formstück“, „Schablone“. Eine Kata ist ein stilisierter und choreographierter Kampf gegen einen oder mehrere imaginäre Gegner, der einem festgelegten Muster im Raum, Embusen genannt, folgt. Verschiedene Stilrichtungen üben im Allgemeinen verschiedene Kata, jedoch gibt es auch viele Überschneidungen, Varianten und unterschiedliche Namensgebungen.
Kata entwickelten sich, wie bereits im Abschnitt Geschichte erwähnt, zur komprimierten Weitergabe der Techniken einer Schule oder eines einzelnen Meisters ohne die Notwendigkeit schriftlicher Aufzeichnung.
Bunkai (japanisch 分解, dt. „Analyse, Zerlegung“) bezeichnet die Analyse der einzelnen fest vorgeschriebenen Bewegungen einer Kata, wie sie in der entsprechenden Schule gelehrt werden. Die dabei betrachtete Form der Kata bezeichnet man als das Genkyo- (原拠) oder Basis-Modell. Dieses bezeichnet die Urform bzw. den Ursprung der Kata.
Während die Kata frei und meist öffentlich vermittelt wird, ist das Bunkai die persönliche Interpretation des (lehrenden) Meisters, seines Systems/Schule. Üblicherweise ist das (traditionelle) Bunkai damit an den persönlichen Kontakt zwischen Meister und Schüler gebunden.
Ōyō (japanisch 応用, dt. „Anwendung“) Individuelle Interpretationen durch die Schüler werden ōyō („frei“) genannt. Dabei wird der Leistungsstand wie auch körperliche oder andere individuelle Merkmale berücksichtigt. Manche Bunkai-Techniken berücksichtigen so z. B. nicht den Größenunterschied zwischen Tori und Uke.
Leider ist mit der Verallgemeinerung des Karate oft dieser Bezug verloren gegangen, weswegen vielfach freie Ōyō-Varianten in Umlauf sind, deren Urheber nicht mehr nachvollziehbar, bzw. deren Authentizität dann zweifelhaft sind. Daraus resultiert oft auch eine Unklarheit in der formalen Ausführung der Kata, da die Form wiederum ohne die ursprünglichen Bedeutungen leicht zu einem rein akrobatischen Leistungsvergleich (Wettkampf) zu verkommen droht.
Henka (japanisch 変化, dt. „Veränderung“, „Variation“). Die Ausführung der Kata und ihr Ausdruck werden trotz der gleichen Bewegungsabläufe der Ausführenden niemals gleich aussehen. Die Akzentuierungen innerhalb der Bewegungsabläufe, die eingesetzte Kraft in den Einzeltechniken, die individuelle koordinative Befähigung, die Gesamtkonstitution und viele weitere Aspekte bewirken, dass eine Kata von zwei Karatekas vorgetragen niemals gleich sein kann. Henka beschreibt, wie der Ausführende die Kata präsentiert und auch wie er sie sieht.
Kakushi (japanisch 隠し, dt. „verborgen“, „versteckt“). Jede Kata enthält Omote (表, „äußerlich“, „Oberfläche“), die offensichtlich enthaltenen Techniken, und Okuden (奥伝), den unterschwelligen oder unsichtbaren Teil. Kakushi beschäftigt sich mit letzteren Techniken, die zwar potentiell im Ablauf der Kata vorhanden sind, aber sich dem Betrachter und auch dem Praktizierenden nicht von selbst erschließen. Daher ist es meist notwendig, von einem Meister in diese unterschwelligen Kniffe und Techniken eingewiesen zu werden. In traditionell ausgerichteten Dōjō werden diese Techniken nur den Uchi-Deshi (内弟子, Privat-, Haus- oder Meisterschüler, wörtlich „Hausschüler, interner Schüler“) vermittelt. Kakushi wird traditionell ab dem 4. Dan vermittelt, da dieser auch als Dan des technischen Experten bezeichnet wird.
Ein Makiwara ist ein im Boden oder an der Wand fest verankertes Brett aus elastischem Holz, z. B. Esche oder Hickory, mit Stoff, Leder o. ä. umwickelt, auf das man schlägt und tritt. Die Elastizität des Holzes verhindert dabei einen zu harten Rückstoß in die Gelenke. Die Verletzungsgefahr (Hautabschürfungen und Gelenkverletzungen) ist am Anfang trotzdem recht hoch.
Dieses Training fördert den Knochenaufbau der Unterarme. Die Armknochen bestehen aus fast hohlen Knochen, die durch diese Trainingsform gestärkt werden. Durch die Belastung des zurückfedernden Makiwara, bei einem Schlag oder Tritt, werden diese Stellen vom Körper „verdickt“, es lagert sich also mehr Kalzium in dem Knochen an. Dieser wird dadurch härter.
Um das Kime zu trainieren, ist allgemein keine andere Übungsmethode außer dem normalen Karatetraining vonnöten (Kihon, Kata, Kumite, Makiwara), da jede Karatetechnik mit dieser Atmung ausgeführt wird. Es ist jedoch auch üblich, im Training Schwerpunkte zu setzen, doch auch dann werden Karatetechniken benutzt, um das Kime zu stärken. Kimetraining ist also Bestandteil eines umfangreichen Techniktrainings. Will man die Techniken durch das Kime stärken, so muss man zu Beginn einer Technik den Gliedmaßen jegliche Spannung nehmen. Erst beim Auftreffen einer Technik im Ziel wird die Muskulatur angespannt, gleichzeitig der Atem herausgestoßen, um die Technik zu arretieren. Um diesen Vorgang zu perfektionieren, werden meist nur einzelne Techniken geübt, hauptsächlich der gerade Fauststoß aus dem natürlichen, schulterbreiten Stand (shizentai). Isometrische Übungen stellen auch gute Übungsformen dar. Hierbei wird eine einzelne Technik ausgeführt und in der Endstellung gehalten. Dann wird Gegendruck auf diese Technik ausgeübt. Die Spannung wird ca. 4 Sekunden gehalten, die Atmung während dieser 4 Sekunden ist eine lange Kime-Atmung. Diese Übung wird mehrmals wiederholt.
Andere Übungsformen wären zum Beispiel:
Im Zuge der modernen Entwicklung mancher Karate-Schulen von Kampfkunst hin zu Kampfsport werden in einigen Stilrichtungen Karate-Turniere (sowohl Kumite- als auch Kata-Turniere) praktiziert.
Da beim Freikampf wegen der hohen Effektivität vieler Techniken bei „echtem“ Kampf hohe Verletzungs- und sogar Todesgefahr droht, herrschen einerseits sehr strenge Regeln, die u. a. den Schutz der Teilnehmer gewährleisten sollen, und andererseits wird nur ein eingeschränktes Repertoire an Techniken im Wettkampf verwendet.
Turnierkämpfe werden mit Zahnschutz und, je nach Geschlecht, mit Brust- oder Tiefschutz ausgeführt. Weitere Schutzmaßnahmen hängen stark von der Verbandsphilosophie ab. So werden etwa beim größten Verband DKV (Deutscher Karate Verband) außerdem Faust- und Fußschützer sowie Schienbeinschoner verwendet, während beim DJKB (Deutscher JKA Karate Bund) keine weiteren Protektoren erlaubt waren (ab 2013 sind auch Faustschützer vorgeschrieben).
Befürworter von Karate-Wettkämpfen betonen den sportlichen Charakter von Karate und führen die sportlich-praktische Anwendbarkeit an. Kritiker der Karate-Wettkämpfe vertreten die Meinung, dass Wettkämpfe dem wahren Charakter und Geist des Karate-Do widersprechen, und dass durch die stark reduzierte Anzahl verwendeter Techniken das Karate verflacht und degeneriert.
Es handelt sich hierbei im Grunde genommen um verschiedene Sichtweisen: einerseits die traditionelle, die Karate als Kampfkunst sieht, deren letztendliches Ziel die Vervollkommnung der Persönlichkeit ist, und andererseits die moderne sportliche, in der Karate als Kampfsport zu sehen ist, und in der die praktische Anwendung mit sportlichem Charakter erwünscht ist. Eine mögliche Sicht ist, dass der Sportgedanke das Karate bereichert hat. Die Kampfkunst Karate könne mit dem Sport leben, doch der Sport nicht ohne die Kampfkunst Karate.
Karate war bei den Olympischen Spielen 2020 in Tokio erstmals olympische Disziplin. Am 3. August 2016 stimmten die IOC Delegierten im Rahmen der 129. IOC-Session in Rio de Janeiro dem Vorschlag der IOC Exekutive zu und nahmen neben Karate auch Sportklettern, Skateboarding, Baseball und Surfing in die Liste der vom IOC anerkannten internationalen Verbände auf (siehe Karate bei den Olympischen Spielen). Viele Verbände, u. a. der DKV oder das Kampfkunst Kollegium, haben begonnen, alte Wettkampfformen und das Punktesystem zu verändern, um so den Karatewettkampf populärer und für die Olympischen Spiele geeigneter zu machen.
Qualifizierte Karatekas können an den alle vier Jahre stattfindenden World Games teilnehmen. Die World Games sind den Olympischen Spielen gleichgestellt. Deutschland hatte bereits mehrfach Goldmedaillengewinner in der Sparte Karate.
Siehe Hauptartikel: Martial-Arts-Film
alphabetisch aufsteigend

Das antike Griechenland hat die Entwicklung der europäischen Zivilisation maßgeblich mitgeprägt. Es umfasst im Kern den Zeitraum von ca. 800 v. Chr. bis zur Einbeziehung des letzten der hellenistischen Reiche 30 v. Chr. ins Römische Reich. Kulturgeschichtlich wirkten diverse Erscheinungsformen, Entwicklungen und Hervorbringungen aber weit darüber hinaus und teils bis in die Gegenwart nach. Die antike griechische Geschichte wird dabei traditionell unterteilt in die drei Epochen Archaik, Klassik und Hellenismus.
Die archaische Epoche Griechenlands folgte dabei dem Zerfall der mykenischen Kultur und den sogenannten „dunklen Jahrhunderten“ (von ca. 1050 bis ca. 800 v. Chr.). Bald nach der Entstehung des griechischen Alphabets wurden bereits grundlegende Werke der abendländischen Dichtkunst, wie vor allen die Ilias und die Odyssee, schriftlich festgehalten. Im Zeitraum von 800 bis 500 v. Chr. etablierte sich die Polis als Staatsform, und es kam zur Gründung vieler griechischer Kolonien im Mittelmeerraum und am Schwarzen Meer. In der Archaik entstanden zudem erste Formen friedlichen sportlichen Wettstreits für alle Hellenen, wie die Olympischen Spiele.
In der folgenden klassischen Periode (ca. 480–336 v. Chr.), die unter anderem die Selbstbehauptung der Griechen in den Perserkriegen sowie die Entwicklung und Ausgestaltung der attischen Demokratie, aber auch zerstörerische Kriege griechischer Poleis untereinander wie den Peloponnesischen Krieg beinhaltete, kam es verschiedentlich zu einer politischen, wirtschaftlichen und kulturellen Entfaltung, die ihresgleichen in vormodernen Zeiten sucht und die ein Fundament für das Abendland legte. Prägend war dabei vor allem Athen, das im Mittelpunkt der schriftlichen Überlieferung zu dieser Zeit steht. Zu den exemplarischen Leistungen der antiken griechischen Kultur zählen:
Mit dem makedonischen König Alexander dem Großen begann die letzte Epoche der eigenständigen griechischen Geschichte, der Hellenismus (ca. 336–27 v. Chr.). Diese Zeit war durch das Ende der Sonderrolle Athens, die Gründung zahlreicher neuer Poleis und die Verbreitung griechischer Sprache und Kultur bis nach Vorderindien, durch die gegenseitige Durchdringung von östlicher und westlicher Zivilisation und Religion sowie insbesondere durch die Etablierung von Großreichen, die von makedonischen Königen beherrscht wurden, gekennzeichnet, bis der östliche Mittelmeerraum seit 200 v. Chr. in einem gut 150 Jahre dauernden Prozess schrittweise unter römische Herrschaft geriet und schließlich Teil des Imperium Romanum wurde. 27 v. Chr. wurde der größere Teil Griechenlands zur römischen Provinz Achaea. Auch der Hellenismus brachte bedeutende intellektuelle und künstlerische Leistungen hervor; so wirkten damals Denker wie Archimedes und Eratosthenes, die bis heute wirkenden Denktraditionen der Stoa und des Epikureismus wurden begründet und monumentale Kunstwerke wie der Pergamonaltar geschaffen.
In der archaischen Epoche entwickelten die Alten Griechen mit der Polis einen spezifischen Staatsverband oder Stadtstaat, der zu ihrer typischen politisch-sozialen Organisationsform werden sollte. Zudem bildeten sie ihr eigenes Alphabet samt Schriftkultur aus, wurden zu Kolonisten an weiten Küstenstreifen von Mittelmeer und Schwarzem Meer und leiteten den Übergang von der Natural- zur Geldwirtschaft ein. Sie führten in diesem Zeitraum zu Lande wie auf dem Wasser neue Kriegstechnik ein und begründeten eine viele Wissensbereiche umfassende Literatur.[1]
Ab 700 v. Chr. verstärkte sich der Einfluss orientalischer Elemente auf die Griechische Kunst, wobei zunächst Städte auf Euböa, bald darauf das mächtige Korinth eine wichtige Vermittlerrolle spielten. In dem sich weitenden Horizont des archaischen Griechenland entstand schließlich auch die ionische Philosophie. Zu ihren herausragenden Vertretern zählen u. a. der Naturphilosoph Thales von Milet, der Mathematiker Pythagoras von Samos und der Dialektiker Heraklit von Ephesos.
In der jüngeren Forschung werden die Anfänge der Polis-Organisation hauptsächlich für das 8. Jahrhundert v. Chr. debattiert;[2] vereinzelt werden frühere Wurzeln reflektiert.[3] In der ursprünglichen Bedeutung stand Polis für die Burg als Zentrum der jeweiligen Siedlungsgemeinschaft, dann auch für umliegende besiedelte Flächen innerhalb und außerhalb der Stadtmauern, soweit vorhanden. Wirtschaftliches und politisches Zentrum der Polis war der Marktplatz, die Agora. Hier übten die versammelten Vollbürger ihre politischen Rechte aus.[4] Das Ausmaß an Mitbestimmung und Machtteilhabe der Politen variierte allerdings unter den verschiedenen Poleis. Der Adel, der zunächst noch kein Geburtsadel war, gewann an Einfluss; dadurch bedingt wurde die Königsherrschaft immer mehr zurückgedrängt und verschwand größtenteils.
Kernkriterium für die Zugehörigkeit zu dem Personalverband, der die Polis bildete – „Die Männer, nicht die Mauern machen die Stadt aus“, hieß es bei Aristoteles[5] – war individueller Landbesitz: Die Poleis waren Ackerbürgerstädte überschaubarer Größe; die Anzahl der wehrfähigen Männer lag meist zwischen 500 und 1500.[6] Oft hatten Poleis nur ein eng begrenztes Umland (Chora). Große Poleis mit weitläufiger Chora wie Athen und Sparta waren die Ausnahme.
Mit der Zeit wurde die Polis zur vorherrschenden Staatsform im antiken Griechenland (außer in Teilen Nordgriechenlands und in manchen Regionen der Peloponnes). Trotz häufiger Kleinräumigkeit und geringer militärischer Stärke legten die einzelnen Poleis Wert auf ihre Freiheit, die sie mit Selbstgenügsamkeit und Autarkie verbanden. Bedroht waren diese Errungenschaften aber nicht nur durch äußere Machtkämpfe, sondern auch durch gewaltsame innere Auseinandersetzungen (Staseis). Die intern nach Besitz und Wehrkraft abgestuften Rechte der Bürger bestimmten die jeweilige Verfassung; Strafrecht und Privatrecht gründeten sich auf vom Volk beschlossene oder von beauftragten Gesetzgebern erlassene Gesetze. Allgemein verbreitete Polis-Institutionen waren die Heeres- oder Volksversammlung, ein der Volksversammlung Vorschläge unterbreitender Rat sowie auf Zeit gewählte Amtsträger für bestimmte Aufgabenbereiche.[7]
Nicht zuletzt bildete jede Polis auch eine gesonderte Schwur- und Kultgemeinschaft mit eigener bevorzugter Schutzgottheit, der zu Ehren man regelmäßig für Opfer und Feste zusammenkam.[8] Die zentralen Kultstätten dienten auch als Sammelplatz und als Verwahrorte von Vermögenswerten, die zu rauben als schwerster Frevel bestraft wurde.[9]
Bereits Ende des 2. Jahrtausends v. Chr. sollen Griechen an Orte an der kleinasiatischen Küste übergesiedelt sein.[10] Im Zeitraum von ca. 750–550 v. Chr. kam es dann zur Großen Kolonisation, in deren Verlauf in den Küstenbereichen des Mittelmeers und des Schwarzen Meers eine Vielzahl von Apoikien (Tochterstädten) gegründet wurden.
Als Gründe dafür, dass sich Siedler aus einer Reihe von Poleis unter der Leitung eines Oikistes mit Schiffen auf den Weg zu einer Neuansiedlung weit entfernt von der Mutterstadt machten,[11] werden steigende Bevölkerungszahlen in den oft kleinräumigen Poleis erwogen, die zu Nahrungsmittelknappheit geführt haben könnten. Auch in wohlhabenderen Familien stellte sich bei Landknappheit und Erbteilung das Problem, dass der aufzuteilende Grund für mehrere Söhne keine allen genügende Existenzgrundlage bot.[12] Als alleiniges Motiv ist Landnot jedoch nicht anzusehen. So waren auch Handwerker unter den Aussiedlern; und es mochten teils auch Handelsinteressen eine Rolle spielen, beispielsweise bei der Erschließung der Seewege an der Ostküste der Adria, wo Eretria und Korinth als Mutterstädte (Metropoleis) fungierten.[13]
An der Gründung einer Apoikie waren anfangs selten mehr als 200 Menschen beteiligt. Gesucht waren Örtlichkeiten, die bebaubares Ackerland boten und die bei der Ansiedlung kaum Widerstand von Einheimischen erwarten ließen. Die Landaufteilung oblag dem das gesamte Kolonisationsunternehmen leitenden Oikisten, der auf vergleichbare Größe und Qualität der Feldstücke zu achten hatte. Nach der Verlosung an die Fahrtbeteiligten galt die neue Eigentumsordnung als dauerhaft fixiert. Die Apoikien waren von den Mutterstädten, aus denen die Kolonisten stammten, von vornherein unabhängig, übernahmen aber die Organisationsstruktur und die kultischen Gepflogenheiten ihrer jeweiligen Metropolen, sodass kulturelle und Herkunftsbindungen auf Dauer erhalten blieben.[14]
Etwa 200 bis 230 griechische Neugründungen mögen in archaischer Zeit entstanden sein, teils wiederum als Apoikien, die aus bereits bestehenden Kolonien angelegt wurden. Aus manchen von ihnen wurden bedeutende Städte mit großer Bevölkerung. Akragas dürfte im 5. Jahrhundert v. Chr. etwa 80.000 Einwohner gehabt haben, Sybaris sogar weit mehr als 100.000.[15] Als die Kolonisationsfahrten endeten, da womöglich alle günstigen Küstenlagen besetzt waren, hatte sich die Polis als stadtstaatliches Organisationsprinzip über die ursprünglichen Kernräume griechischer Besiedlung weit ausgreifend verbreitet. Am Ende des 7. Jahrhunderts saßen nach einem Bild Platons die Griechen um das Mittelmeer „wie die Frösche um den Teich“.[16]
Im Vergleich zu anderen Poleis wies Sparta bereits in archaischer Zeit eine Reihe von Besonderheiten auf, die die Spartiaten aus ihrer dorischen Stammestradition beibehielten: die gemeinsamen Mahlzeiten aller grundbesitzenden und wehrfähigen Bürger; die Jugenderziehung nicht im Elternhaus, sondern in Gemeinschaftseinrichtungen; die monarchische Spitze in der allerdings ungewöhnlichen Form des kollegialen Doppelkönigtums, dem das militärische Aufgebot und die Kriegführung oblag. Den Lebensunterhalt der ganz auf ihr Gemeinschaftsleben und ihr militärisches Dasein ausgerichteten Spartiaten[17] sicherten die von ihnen in Lakonien und Messenien unterworfenen Heloten nichtdorischer Herkunft.[18]
Nach dem Sieg der Spartiaten im Zweiten Messenischen Krieg, der die gebündelte Schlagkraft der Hoplitenphalanx gegenüber dem Heldentum von Vorkämpfern zur Geltung gebracht hatte, nahm die spartanische Ordnung in sozialer und politischer Hinsicht dauerhafte Gestalt an. Als einflussreichste griechische Militärmacht wird Sparta in den Quellen um die Mitte des 6. Jahrhunderts v. Chr. erkennbar. Höchste Anerkennung als „Beschützer und Anwalt Griechenlands“[19] wurde den Spartanern (Lakedaimoniern) in der Folge zuteil.[20]
Beim Kampf um die Vormachtstellung auf der Peloponnes stellte sich Sparta hauptsächlich Argos anhaltend in den Weg. Die Argiver entscheidend zu besiegen, gelang den Lakedaimoniern trotz mehrerer Kriege nicht. Schließlich verlegten sie sich darauf, Argos durch eine militärische Bündnispolitik zu isolieren und damit als Machtrivalen zu schwächen. Zuerst mit Tegea und danach mit weiteren peloponnesischen Poleis schloss Sparta Verträge, die die Zusicherung enthielten, die gleichen Freunde und Feinde zu haben und einander im Fall von Angriffen militärisch zu unterstützen. „Die Lakedaimonier und ihre Mitkämpfer“ nannten die Zeitgenossen den so entstandenen Peloponnesischen Bund.[21]
Zu einer markanten Form der Alleinherrschaft in manchen griechischen Poleis der archaischen Zeit kam es zwischen der Mitte des 7. Jahrhunderts v. Chr. und den Perserkriegen. Jeweils nur in lokalem oder regionalem Rahmen und unter diversen Voraussetzungen eingeführt, war die Tyrannis für diese Epoche des antiken Griechenlands jedoch nicht im Ganzen prägend.[22]  Der Begriff Tyrannis für einen Herrn oder Herrscher war in der Archaik – anders als seit dem 5. Jahrhundert v. Chr. – noch nicht negativ besetzt. Bis in die klassische Zeit war es für viele Griechen gar nicht ausgemacht, dass die Tyrannenherrschaft eines Adelssprösslings sich nachteiliger darstellte als das Regime einer Adelsclique oder gar als die Machtkämpfe von Adelsgeschlechtern untereinander und zu Lasten des Polisverbands.[23] Zum Entstehungsumfeld tyrannischer Regime gehörten Krisen, die einige Poleis in der zweiten Hälfte des 7. Jahrhunderts, andere in der ersten Hälfte des sechsten Jahrhunderts v. Chr. durchmachten. Sie waren bedingt durch hemmungslose Bereicherung von Adelsgeschlechtern unter Missachtung von Regeln und Gerechtigkeitsempfinden sowie durch die daraus resultierenden Konflikte und Gegnerschaften, die der Stasis Vorschub leisteten, der Entzweiung und dem Bürgerkrieg.[24]
Bekanntere Tyrannenherrschaften in der archaischen Epoche gab es zunächst in Korinth, wo die Kypseliden um 660 v. Chr. die Macht übernahmen, sowie in Sikyon (Kleisthenes von Sikyon), in Athen (Peisistratiden-Tyrannis) und auf Samos (Polykrates).[25] Auf die Machtusurpation durch den Tyrannen folgte meist die Verbannung seiner adligen Gegner oder Rivalen, oft verbunden mit der Enteignung und Verteilung von deren Gütern an Landlose. Leibwachen und Söldner sowie mitunter Bündnisse mit anderen Tyrannen oder Poleis dienten der Machtabsicherung.[26] Ihre Herrschaft nutzten die Tyrannen zur Mehrung des eigenen Reichtums und Ansehehens. Als Mittel zur Stützung ihrer Machtstellung kombinierten sie wahlweise die Stärkung der bedrohten Bauernschaft, die Zentralisierung der Rechtsprechung, den Bau von Tempeln, Häfen und Wasserleitungen und die Ausgestaltung von Polis-Heiligtümern und kultischen Festumzügen.[27]
Außenpolitische Ambitionen in Form von territorialen Eroberungen entwickelten die Tyrannen, von einzelnen Koloniegründungen abgesehen, kaum. Mehr mochte auch die Polisstruktur weder militärisch noch finanziell hergeben. Folglich strebten die Tyrannen „kein Imperium an, waren nur Aristokraten, welche in ihrer Stadt die Möglichkeiten ihrer Schicht allein genießen wollten, ganz egoistisch, stark unter agonalen Gesichtspunkten.“[28] Eine rechtliche Fixierung der Tyrannis, falls es Anläufe dazu überhaupt gegeben haben sollte, ist nirgends überliefert. Auch deshalb dürfte sie die Nachfolgegeneration des jeweiligen Initiators nur selten überstanden haben. „Zu heftig war der Widerstand der von der Macht verdrängten adligen Familien, und zu unbeugsam artikulierte sich der Freiheitswille der übrigen Bevölkerung, die die Willkür des tyrannischen Systems nach der Beseitigung einer akuten Bedrohung ebenso ablehnte wie den Bruch mit der Tradition, in die sich der Tyrann nicht einordnen konnte.“[29]
Die antike griechische Welt kannte kein den neuzeitlichen Erscheinungsformen entsprechendes „Nationalbewusstsein“, wie auch die griechische Kolonisation nicht auf eine zentral gesteuerte Ausweitung des griechischen Herrschaftsgebiets zielte.[30] Jede Polis, mochte sie noch so klein sein, wachte streng über die eigene Autonomie und war nicht bereit, diese freiwillig aufzugeben. Dadurch bedingt war der Krieg im antiken Griechenland eher der Normalzustand (siehe die Kämpfe zwischen Sparta und Argos oder zwischen Athen und Ägina).
Während der frühen Formierungsprozesse der Poleis wiesen diese in ihren Führungs- und Sozialstrukturen kaum Unterschiede auf. Die Entwicklung eines institutionellen Gefüges in einigen Zentren politischer Vereinigungen jedoch fand bei zunehmender Kommunikation in der näheren und weiteren Umgebung Nachahmung und Verbreitung in der Institutionalisierung öffentlicher Organe zur Gewährleistung von Ordnung und Sicherheit. Daraus ergaben sich zudem bessere Voraussetzungen für die Durchführung von Gemeinschaftsaktionen. Ebenfalls gemeinschaftsbildend waren nicht nur übereinstimmende Bräuche, sondern war auch ein Zusammengehörigkeitsgefühl im Rahmen bestimmter Sprachdialekte, etwa bei Dorern und Ioniern.[31] Im Zuge der griechischen Kolonisation wuchs angesichts der unterschiedlichen nichtgriechischen Nachbarn in den Apoikien das Bewusstsein und womöglich die Pflege griechischer Eigenart. Zu dieser Zeit kam die gemeinsame Eigenbezeichnung der Griechen als „Hellenen“ auf.[32] Ein engeres Gemeinschaftsgefühl, das auch politisch zum Ausdruck kam, entwickelte sich erst im Zuge der Perserkriege.[33]
Von elementarer Bedeutung für den kulturellen Zusammenhalt und das Selbstverständnis der Griechen bereits in archaischer Zeit waren die homerischen Epen, die Ilias und die (etwas später entstandene) Odyssee, die wohl um 700 v. Chr. in schriftlicher Form niedergelegt wurden.[34] Zu dem nachhaltig wirksamen Schrifttum der Archaik gehörten auch die für Mythologie und Weltanschauung bedeutsamen Dichtungen des Hesiod. Diese Werke bildeten in der Folgezeit einen wichtigen Kanon der antiken griechischen Kultur. Auf den aus den homerischen Epen bekannten Götterkanon bezogen sich in archaischer Zeit auch die ersten Tempel­bauten. Die antiken griechischen Poleis waren stark religiös geprägt. Zwar handelte es sich um keine Buchreligion – die Religion wurde durch Mythen und Heroengeschichten bestimmt –, doch wurden fast alle öffentlichen und privaten Handlungen von Anrufungen an die Götter begleitet.
Großereignisse, zu denen Griechen aus den verschiedenen Poleis zusammenströmten und bei denen sie ihr Zusammengehörigkeitsbewusstsein zum Ausdruck brachten, gab es vor allem in Gestalt der Panhellenischen Spiele, deren berühmteste die Olympischen Spiele waren. Hieran nahmen beispielsweise auch Griechen aus Unteritalien teil. Von ähnlicher panhellenischer Bedeutung war außerdem das Orakel von Delphi.
In der Landwirtschaft wurden die anfangs dominierenden Getreidesorte Emmer und Einkorn während der Archaik weitgehend durch die Gerste verdrängt. Ebenfalls in dieser Epoche trat der Granatapfel als neues Obst auf.[35]
Die ursprünglich aus der kunst- und architekturgeschichtlichen Betrachtung hervorgegangene Epochenbezeichnung Klassik im Rahmen der griechischen Antike bezog sich im Kern auf das Erscheinungsbild Athens zur Zeit der entwickelten Attischen Demokratie im 5. Jahrhundert v. Chr. Sie zielte unter anderem auf die Akropolis-Bauten, auf die Bildhauerkunst des Phidias und auf die Tragödiendichter Aischylos, Sophokles und Euripides, denen bereits ihr Zeitgenosse Aristophanes eine überragende Stellung bescheinigt hatte.[36] Die neuere althistorische Forschung bezieht jedoch auch das 4. Jahrhundert v. Chr. zumeist ein in das klassische Zeitalter Griechenlands: Das Wiederaufleben der Demokratie in Athen war von einem ungebrochenen Kultur- und Geistesleben begleitet, für das keinesfalls allein die Werke Platons und des Aristoteles stehen.[37] Zu beachten ist, dass der größte Teil der schriftlichen Überlieferung zur Klassik aus Athen stammt, über dessen Geschichte man daher weitaus besser informiert ist als über die der übrigen Griechen, die vorwiegend aus athenischer Perspektive geschildert werden (Athenozentrismus).
Die Perserkriege und der damit verbundene siegreiche Abwehrkampf der Griechen unter Führung Spartas und Athens stärkten um 480 nicht nur das Bewusstsein der Zusammengehörigkeit unter den Hellenen, sondern stellten auch die Weichen für die Entwicklungsbedingungen Griechenlands in der klassischen Epoche. Das galt besonders für Athen, das vordem außenpolitisch kaum eine Rolle gespielt hatte, nun aber als Vormacht im eigenen Seebund einen tiefgreifenden inneren und äußeren Strukturwandel erlebte. „Binnen fünf Jahren“, heißt es bei Werner Dahlheim, „sah sich eine Bürgerschaft, die in ihrer bisherigen Geschichte nur selten über die engere Nachbarschaft hinausgeblickt hatte, mit der Politik des gesamten östlichen Mittelmeerraums konfrontiert.“[38] Das im Werden begriffene attische Seereich und die Ausformung der Attischen Demokratie bildeten einen engen wechselseitigen Wirkungszusammenhang.[39]
Die Verbreitung der Demokratie unter griechischen Poleis könnte gegen Ende des 4. Jahrhunderts v. Chr. den Höhepunkt erreicht haben. Einen anhaltenden Aufschwung verzeichneten im 4. Jahrhundert auch die demographische und sozioökonomische Entwicklung Griechenlands.[40] Insgesamt erscheint es somit plausibel, das klassische Zeitalter Griechenlands an das Entstehen und die Existenz der Attischen Demokratie zu koppeln, also von den Kleisthenischen Reformen an der Schwelle zum „langen“ 5. Jahrhundert bis zum erzwungenen Ende im „kurzen“ 4. Jahrhundert v. Chr.[41]
Mögen neuere Forschungsergebnisse und -ansätze in Archäologie, Wirtschafts- und Geschichtswissenschaft die Erkenntnisgrundlagen auch verdichtet haben,[42] bleiben Angaben zu antiken Bevölkerungszahlen, Wirtschafts- und Wohlstandsindikatoren doch weiterhin grobe Schätzwerte.[43] Vom Tiefpunkt um 1000 v. Chr. bis zum Höhepunkt der Bevölkerungsentwicklung am Ende des 4. Jahrhunderts wird ein Anstieg der Gesamtbevölkerung in „Kerngriechenland“ (die Halbinsel südlich Makedoniens mit den Kykladen und den Ionischen Inseln) von 330.000 auf etwa 3 Millionen geschätzt, die Gesamtzahl der Polisbewohner in der damaligen griechischsprachigen Welt auf 8,25 Millionen.
Als durchschnittliche Bevölkerungsdichte im griechischen Siedlungsraum sind zu dieser Zeit 44 Einwohner pro Quadratkilometer anzusetzen.[44] Die Region Attika hatte die höchste Bevölkerungsdichte Griechenlands, nämlich zwischen 45 und 80 Einwohner pro Quadratkilometer[45] Insgesamt kann man von ca. 1000 griechischen Poleis im Mittelmeerraum und am Schwarzen Meer ausgehen, von denen weniger als 50 Prozent mehr als 2000 Einwohner hatten und nur 15 Prozent mehr als 5000.[46] Doch annähernd ein Drittel der griechischen Bevölkerung könnte Ende des 4. Jahrhunderts in städtischen Siedlungen mit mehr als 5000 Einwohnern gelebt haben.[47]
Zu den grundlegenden Strukturmerkmalen auch der griechischen antiken Gesellschaft gehören Sklaverei und Sklavenarbeit in Hauswirtschaft, Bergwerken, Landwirtschaft und Handwerk sowie die politische Rechtlosigkeit und zurückgesetzte Stellung der Frauen. Von ihnen haben auch nur wenige bleibende Bekanntheit erlangt, wie zum Beispiel die Dichterin Sappho, die Philosophin Aspasia oder die Hetäre und Aulosspielerin Lamia. Neben den ihre politischen Rechte ausübenden männlichen Bürgern gab es in den Poleis häufig zugezogene andere Mitbewohner, etwa die Metöken. Sie gingen oft wirtschaftlichen Tätigkeiten in Handel und Handwerk nach, waren von der politischen Mitwirkung aber ausgeschlossen. In klassischer Zeit wurden die Vorrechte der Adligen beschnitten, sodass der ihnen verbliebene Einfluss nun stärker in überkommenem Besitz und Reichtum gründete. Dabei dürften Privatvermögen und Landbesitz im spätklassischen Athen – für vormoderne Verhältnisse ungewöhnlich – relativ gleichmäßig verteilt gewesen sein: 60 bis 65 Prozent des Bodens könnten 70 bis 75 Prozent der Bürger gehört haben.[48] Hatte das Einkommensniveau griechischer Familien zu Beginn der archaischen Epoche um 800 v. Chr. kaum über dem Existenzminimum gelegen, so lassen Schätzwerte darauf schließen, dass der Pro-Kopf-Verbrauch am Ende des 4. Jahrhunderts durchschnittlich etwa doppelt so hoch lag.[49]
Die durchschnittliche Lebenserwartung in der Antike war im Vergleich zur Gegenwart sehr niedrig. Nur knapp über 50 Prozent der Menschen überlebten ihr fünftes Lebensjahr; nur ca. 40 Prozent wurden mindestens 30 Jahre alt, und nur gut 20 Prozent wurden 50 Jahre alt oder älter. Das 75. Lebensjahr erreichten weniger als fünf Prozent der antiken Bevölkerung. Die hohe Sterblichkeit vor allem der Jüngsten ging Hand in Hand mit einer hohen Geburtenrate. Schätzungen zufolge brachte jede Frau im Durchschnitt fünf bis sechs Kinder zur Welt.[50] Studien an Knochen aus Grabungsfunden lassen allerdings einen deutlichen Anstieg der Lebensdauer von den Dunklen Jahrhunderten Griechenlands bis zum 4. Jahrhundert v. Chr. erkennen. Die durchschnittliche Lebensdauer nach Überstehen der Kindheit verlängerte sich für beide Geschlechter um jeweils zehn Jahre: für Frauen von 26 auf 36, für Männer von unter 30 auf etwa 40 Jahre.[51]
Die antike griechische Gesellschaft war – an den Maßstäben entwickelter Industriegesellschaften gemessen – keine wohlhabende Gesellschaft. In klassischer Zeit jedoch waren nach neueren Erkenntnissen die materiellen Lebensbedingungen und Wohnverhältnisse besser als sonst im Vergleich zu anderen vormodernen Gesellschaften. Auch hatten die wirtschaftlichen Erträge und Pro-Kopf-Verbräuche seit der archaischen Zeit vergleichsweise rasant zugenommen.[52] Andererseits wurde, was über das Selbstversorgungsniveau hinaus erwirtschaftet werden konnte, nicht selten von gesellschaftlichen Eliten konsumiert und nicht investiert.[53]
Zum Nahrungsmittelangebot gehörte hauptsächlich die „mediterrane Trias“ aus Getreide (vor allem Gerste, daneben auch Weizenarten), Oliven (meist in Form von Öl, das außer zur Ernährung auch als Lampenbrennstoff und zur Körperpflege genutzt wurde), Feigen, Granatapfel, Trauben (aus denen auch der Wein gekeltert wurde) und der während der Klassik neu aufkommende Pfirsich. Je nach Jahreszeit konnte die Nahrung durch verschiedene Gemüse und gelegentlich durch Fisch ergänzt werden. In besser gestellten Haushalten mit Jagdhunden gab es auch Wildbret, vorwiegend Kaninchen.[54][55]
Die antike Gesellschaft war unumstritten eine Agrargesellschaft.[56] Man schätzt die Bauern auf 67 %[57] bis 80 %[58] aller Erwerbstätigen. Es besteht ein breiter Konsens, dass die Technik allgemein, also auch die landwirtschaftliche, während der klassischen Periode auf einem niedrigen Niveau war und das – trotz leichter Fortschritte – auch blieb.[59] Die Landwirtschaft war kleinteilig organisiert, das gilt sowohl für die Landparzellierung wie auch für die Betriebsstruktur. So gab es hauptsächlich Kleinbauern mit kleinen Äckern und – im Gegensatz zur römischen Antike – nur sehr selten Großgrundbesitzer. Die Kleinbauern waren Selbständige (auturgoi), die meist kaum mehr erwirtschafteten, als sie selbst verbrauchten (Subsistenzwirtschaft), die wenigen Großgrundbesitzer waren Aristokraten, die oft in Städten lebten und ihre Güter von Aufsehern verwalten ließen.[60] Die Arbeit auf dem Feld war den Männern vorbehalten. Frauen durften nur im Haus aktiv sein. Zumindest ist dies das Bild, das literarische Quellen (aus Sicht der Oberschicht) vermitteln. Es ist schwer, sich ein Bild davon zu machen, ob in der Praxis Frauen auch auf dem Feld mithalfen, wie das in anderen Kulturen und Epochen gut bezeugt ist.[61]
Aufgrund der angeführten Faktoren, zu denen noch die relativ schlechten geographisch-klimatischen Bedingungen für die Landwirtschaft kommen, ist anzunehmen, dass die landwirtschaftlichen Erträge vor allem mit anstrengender körperlicher Tätigkeit erwirtschaftet wurden. Dazu zählte die Kultivierung des Bodens, die Weinlese, die Ernte des Getreides und die der Oliven.[62]
Pollenanalysen lassen darauf schließen, dass die Entwaldung und Zunahme von Krautvegetation als Folge menschlicher Aktivitäten, die bereits in der Frühen Bronzezeit eingesetzt hatte, sich bis in die Archaik hinein steigerte und dann auf einem konstanten Niveau blieb.[63]
Ein wichtiger Wirtschaftszweig war die Textilindustrie. Sie ist vor allem bemerkenswert, als hier Frauen jeglicher sozialer Schicht in fast alle Produktionsprozesse involviert waren. Vor der industriellen Revolution im 19. Jahrhundert waren Textilien wegen des hohen Arbeitsaufwands sehr teuer. Deshalb war das Weben selbst für Frauen aus hohen sozialen Schichten eine ehrenvolle Aufgabe.[64] In Sagen, aber auch in zeitgenössischen Texten wird immer wieder die Bedeutung der Textilherstellung, wobei es vor allem um Wolle geht, im häuslichen Rahmen betont. Im idealen Haushalt (Oikonomikos 7.20–23), den Xenophon vorstellt, sind Frauen für Sklaven verantwortlich, die weben. Dagegen galt es für einen Mann als unwürdig, Frauenarbeiten, inklusive Weben, auszuführen.[65] Die Textilien sind meist für den häuslichen Eigenbedarf produziert worden, doch gibt es auch Belege, dass Stoffe für den Verkauf hergestellt wurden.[66] Neben Wolle wurde auch Leinen verarbeitet und getragen. Ein Großteil wurde anscheinend aus Ägypten importiert, doch gibt es Belege, dass Flachs (aus dem Leinen gefertigt wurde) in Griechenland angebaut und verarbeitet wurde.[67]
Bis auf Schmiede, Töpfer und ähnliche Hersteller von erstens nachgefragten und zweitens Spezialisierung erfordernden Produkten waren Handwerker hauptsächlich in den Städten angesiedelt. Dort existierte allerdings teilweise eine stark ausgeprägte Spezialisierung bei den beruflichen Tätigkeiten: 170 verschiedene athenische Berufe im nichtöffentlichen Bereich, meist außerhalb des landwirtschaftlichen Sektors, sind in Quellen dokumentiert.[68]
Wie die Landwirtschaft bestanden auch das Bauwesen und vor allem das Handwerk aus vielen kleinen und selbständigen Betrieben, die kaum technische Neuerungen hervorbrachten und nur selten über den lokalen Bedarf hinaus produzierten.[57] Größere Arbeitsstätten kamen selten vor, noch seltener waren Unternehmer, die von Einkünften aus Manufakturen leben und vielleicht auch noch ein Vermögen anlegen konnten.[69] Der Bergbau (in Attika vor allem Silber und Eisen) nahm in mancherlei Hinsicht (Massensklaverei, Masseneinsatz von Arbeitskräften) eine Sonderstellung ein.
Aus der Tatsache, dass im dominierenden Wirtschaftszweig, der Landwirtschaft, kaum Überschüsse erwirtschaftet wurden (Subsistenzwirtschaft), ergibt sich schon, dass der Handel mit landwirtschaftlichen Produkten ebenfalls beschränkt blieb. Diese wurden überwiegend auf lokalen Märkten verkauft und nur selten über weitere Strecken transportiert. Eine Ausnahme bildete wegen der geografisch-klimatischen Verhältnisse und zunehmenden städtischen Bevölkerung Attika. Für Athen wurde ständiger Getreideimport aus Sizilien, Ägypten und dem Schwarzmeergebiet notwendig, finanziert unter anderem über den Silberabbau in den Bergwerken bei Laureion. Über weite Strecken gehandelt wurden neben Getreide, Edelmetallen und anderen Rohstoffen auch seltene oder wertvolle Güter wie Wein, Gewürze, Olivenöl und Vasen.[70] Fernhandel wurde selten über Land, sondern meist, was um ein Vielfaches billiger war, über das Meer betrieben. Groß- und Zwischenhandel gab es höchstens in städtischen Zentren.
Mit der Zeit entwickelte sich in Athen ein regelrechtes Handelszentrum. Auf dieser Grundlage und infolge der sogenannten Seedarlehen (verzinste Darlehen, mit denen kostenintensiver Seehandel vorfinanziert wurde) wurde Athen außerdem – soweit man in der Antike von so etwas sprechen kann – zum Bankenzentrum.[57] Das Münzwesen entstand im 6. Jh. v. Chr. und breitete sich in den folgenden Jahrhunderten vor allem in den Städten weiter aus.[71]
Über mehr als die Hälfte der 1035 erfassten griechischen Poleis weiß man nur sehr wenig. 148 von den 672 Poleis, deren Staatsgebiet einschätzbar ist, umfassten nur 25 Quadratkilometer und bis zu 1000 Einwohner; nur Athen, Sparta und Syrakus kamen in klassischer Zeit auf mehr als 2000 Quadratkilometer und über 250.000 Einwohner.[72]
Von 158 Verfassungsgeschichten griechischer Poleis, die in der Schule des Aristoteles zusammengetragen wurden, ist allein die Athenaion politeia (Der Staat der Athener) erhalten geblieben. Ungeachtet der Besonderheiten in den diversen Einzelverfassungen lassen sich für die klassische Epoche drei Grundtypen unterscheiden: die speziell auf Sizilien ausgeprägte „jüngere Tyrannis“ als Einzelherrschaft, die Oligarchie, in der eine Minderheit der Bürger die Herrschaft ausübte, und die Demokratie, die alle Inhaber des Bürgerrechts politisch partizipieren ließ. Während des 4. Jahrhunderts v. Chr. schloss sich etwa die Hälfte der Poleis auf dem griechischen Festland zu Bünden (Koina) zusammen, die auch den kleineren Gemeinwesen Schutz und gewissen politischen Einfluss bieten konnten.[73]
Wichtigstes formales Beschlussorgan in griechischen Poleis war die Volksversammlung (Ekklesia), deren Zusammensetzung und tatsächliche politische Gestaltungsmacht innerhalb des Polisspektrums und im Zeitverlauf allerdings weit auseinanderfielen: von der Tyrannis, in der sie praktisch keine Rolle spielte, über die diversen aristokratisch-oligarchischen Konstellationen bis zur entwickelten Demokratie, in der sie sämtliche freien Bürger einschloss und eine umfassende Zuständigkeit besaß. Mitwirkungsrechte in der Volksversammlung, die ursprünglich mit der Heeresversammlung identisch und aus ihr hervorgegangen war, hingen wesentlich von der individuellen Fähigkeit ab, einen Wehrbeitrag für die Polis zu erbringen, indem man für die eigne militärische Ausrüstung als Hoplit aufkam (Prinzip der Selbstequipierung). Die landlosen Theten kamen zu Einfluss in der attischen Volksversammlung erst, nachdem ein Flottenbauprogramm im Zuge der Perserkriege ihren Einsatz auf den Ruderbänken der Trieren erfordert hatte.
Bei Oligarchien richtete sich die Benennung der Versammlung oft nach der Zahl der teilnahmeberechtigten Vollbürger, neben weit kleineren Zahlen beispielsweise „die Tausend“, „die Viertausend“ oder „die Zehntausend“. In anderen oligarchischen Verfassungen gab es neben der kompetenzarmen Volksversammlung einen diese dominierenden Rat oder gar keine Volksversammlung. Unter demokratischen Vorzeichen hingegen hatte jeder Stimmberechtigte auch Rederecht (mit begrenzter Zeit). Abstimmungen wurden oft durch Handaufheben entschieden. Für die Feststellung der Mehrheit war das Volksversammlungspräsidium zuständig; teilweise wurden Stimmenzähler („Händeschauer“) eingesetzt. Bei wichtigen Angelegenheiten konnten geheime Abstimmungen etwa mit Stimmsteinen angesetzt werden.[74]
Die Einrichtung von Räten (Bule) gab es von alters her im antiken griechischen Gemeinwesen. Sie lassen sich auf den Beirat der „Ältesten“ in der homerischen Königszeit zurückführen und wurden jenseits des Königtums zu Räten der Adelsgesellschaft. Mitgliederzahl und Zugehörigkeit zum jeweiligen Rat variierten wiederum beträchtlich, teils bestimmt von Familienzugehörigkeit, Altersregelungen für die Eintrittsberechtigung und Zensusbestimmungen. Ein solcher Rat bildete in Oligarchien das Regierungsorgan.
Beim Übergang zu demokratischen Verfassungsformen existierte der alte Rat teilweise mit Restbefugnissen weiter, während daneben ein neuer Rat gebildet wurde. Zu dessen Obliegenheiten gehörten üblicherweise die Vorbereitung, Einberufung und Leitung der Volksversammlungen sowie die Kontrolle der laufenden Staatsangelegenheiten einschließlich Finanzverwaltung und Verhandlungen mit auswärtigen Gesandtschaften. Eine wichtige Funktion hatte der Ratsschreiber, der Protokoll führte, den Schriftverkehr erledigte, die Volksbeschlüsse formulierte und veröffentlichte sowie für deren Aufbewahrung sorgte. In vielen Poleis war er eponym: Sein Name bezeichnete das jeweilige Jahr.[75]
Aus der Ablösung des Königtums ergab sich die Schaffung zeitlich beschränkter Ämter mit in der Regel einjähriger Amtsdauer. Handelte es sich anfänglich um Titel allgemeiner Art wie „Vorsteher“, „Ordner“ oder „die im Amt“, so wurden später spezielle Zuständigkeiten geschaffen, beispielsweise in den Bereichen Militär, Finanzverwaltung, Gerichtswesen oder öffentliche Bauten und Anlagen. Im Staat der Athener werden über 30 Ämterkollegien oder Einzelämter genannt. Eine besondere Amtstracht war nicht üblich; ebenso gab es keine fachliche Ausbildung für Ämter. Die ständige Teilnahme der Amtsträger an den Staatsangelegenheiten schien das zu erübrigen.
Die Bestellung der Beamten geschah entweder auf eher aristokratischer Grundlage durch Wahl oder nach demokratischer Lesart durch das Los. Im klassischen Athen konnte gegen den Entscheid eines Beamten an das Volksgericht (Heliaia) appelliert werden, und die Amtsträger waren dem Volk generell rechenschaftspflichtig. Eine vergleichsweise große Machtstellung besaßen Beamte hingegen im Rahmen von Oligarchien. Hier war es auch möglich, dass das Kollegium der Beamten einen engeren Regierungsausschuss neben dem Rat bildete.[76]
Rechtsschutz und eigene Verfahrensinitiative besaßen nur die Vollbürger in ihrer jeweiligen Polis. Fremde bedurften, wollten sie vor Gericht klagen, des Rechtsbeistands eines ansässigen Bürgers. Um der überflüssigen Inanspruchnahme von Gerichten vorbeugen, hatte der Kläger ein Gerichtsgeld zu hinterlegen, das er nur im Fall der Verurteilung des Beklagten von diesem zurückerstattet erhielt. Mord und Totschlag wurden nicht als Offizialdelikte verfolgt, sondern mussten privat angeklagt werden, und zwar von Angehörigen aus der Verwandtschaft des Getöteten. Die Gerichts- und Strafbefugnis von Rat und Beamten war in Oligarchien zumeist größer als in Demokratien.
Staatsanwälte oder einen Juristenstand gab es nicht. Rechtspflege war als wichtiger Teil des Gemeinschaftslebens der Polis eine Aufgabe der Bürger in ihrer Gesamtheit. Die Prozesse wurden von Laienrichtern nach Anhören der Parteien und Prüfung der beigebrachten Beweismittel nebst Eidesleistungen in einem einzigen Verfahren entschieden. Bei politischen Prozessen, in denen leidenschaftliche Parteilichkeit unter den heimischen Richtern eine gerechte Urteilsfindung erfahrungsgemäß erschwerten, wurde mit der Zeit zunehmend in anderen Poleis um die Entsendung von Richtern gebeten, von denen man ein objektiveres Urteil erwartete.[77]
Die Gesamtheit der griechischen Staatenwelt wurde von den Alten Griechen mit der Bezeichnung „Städte und Stämme“ erfasst, wobei der Begriff „Stämme“ einen staatlichen Verbund von Siedlungsgemeinschaften meinte, teils mit Einschluss von Poleis, in etwa Bundesstaaten ähnlich. Dazu gehörten in klassischer Zeit unter anderem die Zusammenschlüsse der Achäer, Ätoler, Böotier, Phoker und Thessalier. Sie bildeten Wehr- und Kultgemeinschaften, hatten eine Bürgerversammlung als beschließende Instanz, die sich aus Repräsentanten der einzelnen Untergliederungen des Bundes zusammensetzte, sowie einen Rat und Beamte des Bundes, zum Beispiel mit den Aufgaben von Archonten oder Strategen.
Das Bürgerrecht der jeweiligen bundesstaatlichen Untereinheit bestand fort, ergänzt um das gemeinsame Bürgerrecht im Bund. Gemeinsame Politikfelder für den Bund, über die zentral entschieden wurde, waren Außenpolitik und Kriegführung, äußere Vertragsschlüsse, Finanzpolitik und Münzprägung. Auf der Gemeinschaftsebene organisiert wurden auch Kulte, Bundesfeste und die damit verbundenen Wettkämpfe.[78]
Der Ionische Aufstand (ca. 500–494 v. Chr.) der seit Jahrzehnten unter persischer Oberherrschaft stehenden kleinasiatischen und zyprischen Griechen gegen das Achämenidenreich war von Athen nur halbherzig unterstützt worden. Dennoch nutzte der persische Großkönig Dareios I. diesen Anlass zur Rechtfertigung der bereits länger ins Auge gefassten Expansion seines Reiches, die er als „Vergeltungsfeldzug“ bezeichnete. Mit diesem Feldzug begannen für Griechenland die Perserkriege. Herodot, der Vater der Geschichtsschreibung, hat über diese Ereignisse in seinem Werk umfänglich Auskunft gegeben.
Athen siegte zwar bei Marathon 490 v. Chr., doch kam es zehn Jahre später zu einem erneuten Feldzug unter Führung von Dareios’ Sohn Xerxes I. 481 v. Chr. wurde daher der Hellenenbund gegründet, dem neben Sparta und Athen auch mehrere andere, aber keineswegs alle Stadtstaaten des Mutterlandes angehörten; manche waren sogar eher bereit, sich den Persern zu unterwerfen. Nach dem Hinhaltegefecht bei den Thermopylen kam es bei Salamis zur Entscheidungsschlacht. Die Griechen vernichteten die zahlenmäßig überlegene persische Flotte (480 v. Chr.). Ein Jahr später wurde auch das persische Landheer in der Schlacht von Plataiai geschlagen. 478 v. Chr. begann die Eroberung Ioniens. Sparta weigerte sich jedoch, den Schutz der Griechen fern der Heimat zu übernehmen. Athen hingegen, bisher der Juniorpartner, nahm sich der Aufgabe an und gründete 478/477 v. Chr. den Attischen Seebund.
Auf den Grundlagen der Reformen Solons und des Kleisthenes sowie der Seeherrschaft Athens in der Ägäis entstand Mitte des 5. Jahrhunderts v. Chr. die entwickelte Attische Demokratie mit Perikles als leitendem Staatsmann. Zeitgleich entwickelte sich nach dem Zeugnis des Historikers Thukydides der Dualismus zwischen der Seemacht Athen und der Landmacht Sparta, der schließlich zum Peloponnesischen Krieg führen sollte.
Während Theben in Böotien die Errichtung einer Hegemonie über die anderen böotischen Gemeinden betrieb, verfolgte Athen unter Perikles eine ähnlich aggressive Politik. Der Seebund, inzwischen längst ein Instrument zur Verfolgung athenischer Interessen, entwickelte sich mehr und mehr zum attischen Reich. 460–457 v. Chr. wurden die sogenannten Langen Mauern errichtet, die Athen mit dem Hafen Piräus verbanden und Athen selbst zur uneinnehmbaren Festung machten. Gestützt auf die Finanzmittel des Bundes, in dem die Bundesgenossen zu Tributpflichtigen Athens geworden waren, wurde die Athener Akropolis durch ein ebenso aufwendiges wie glanzvolles Bauprogramm zu einem repräsentativen Zentrum der neuen Großmacht, die sich nun kulturell als die „Schule Griechenlands“ darzustellen wusste.
Athen entwickelte sich von der Mitte des 5. Jahrhunderts ab auch zum geistigen Magneten und Zentrum Griechenlands, in das die Sophisten mit ihren Lehren und der Einführung der paideia strebten und in dem die Philosophie eines Sokrates, Platon und Aristoteles jeweils Schule machte. Im 5. Jahrhundert entstanden die Tragödien von Aischylos, Sophokles und Euripides, dann auch die Komödien des Aristophanes. Von dem großen Bildhauer Phidias, der das Bauprogramm auf der Akropolis leitete, ist anders als für seinen mit Menschenbildnissen befassten Kollegen Polyklet oder den berühmten Arzt Hippokrates eine längere Anwesenheit in Athen verbürgt. Im 4. Jahrhundert widersetzte sich der Redner Demosthenes wortmächtig, aber machtlos der anhebenden makedonischen Vorherrschaft und blieb damit bis in die Zeit Ciceros ein unerreichtes rhetorisches Vorbild.
Die Attische Demokratie, die allen Vollbürgern vermögensunabhängig eine gleichberechtigte Beteiligung sicherte und sie annähernd eineinhalb Jahrhunderte zu intensiver politischer Mitwirkung anhielt, hatte die Kehrseite, dass Frauen und Sklaven vollständig davon ausgeschlossen waren, wobei die Sklaven auch wirtschaftlich eine wichtige Rolle spielten. Die direkte Demokratie schützte zudem durchaus nicht vor so manchen Auswüchsen äußerer Machtpolitik. Mit der modernen repräsentativen und gewaltenteiligen Demokratie ist sie in vieler Hinsicht nur bedingt vergleichbar.
Athen setzte nach den Perserkriegen als Hegemon im Attischen Seebund den Kampf gegen das Perserreich im östlichen Mittelmeerraum fort. Es unterstützte schließlich sogar eine antipersische Erhebung in Ägypten, wo sich für die Athener in einer sehr verlustreichen Niederlage dann aber die Grenzen der eigenen Machtmittel und Möglichkeiten zeigten. Zu einem Ausgleich mit Persien kam es 449 v. Chr. im Zusammenhang mit dem historisch umstrittenen sogenannten Kalliasfrieden.
In Süditalien und auf Sizilien erwehrten sich währenddessen die seit der großen Kolonisation dort angesiedelten Westgriechen der Bedrohung durch die Etrusker und das mächtige Karthago. In der Schlacht von Kyme 474 v. Chr. wurden die Etrusker vernichtend geschlagen. Auf Sizilien ging der Konflikt mit Karthago weiter, auch wenn die Karthager 480 v. Chr. bei Himera geschlagen worden waren. Dort konnten sich in zahlreichen Poleis auch weiterhin Tyrannen an der Macht halten, wie beispielsweise Gelon, der zeitweise als der mächtigste Mann der griechischen Welt galt. Insgesamt blieben Staseis, also bürgerkriegsartige Konflikte innerhalb der Bürgerschaft, in vielen Poleis jahrhundertelang ein großes Problem; nicht selten führten sie zur Tyrannis der siegreichen Partei.
Zwischen Athen und Sparta kam es 460–446 v. Chr. zum Ersten Peloponnesischen Krieg. Grund war der vorläufige Austritt Megaras aus der spartanischen Allianz und dessen Überwechseln zu Athen. Während der athenischen Flottenexpedition nach Ägypten (460–454 v. Chr.) kam es 457 v. Chr. zu der für Athen verlustreichen Schlacht von Tanagra gegen die Spartaner, aber im Gegenzug zur Bezwingung Aiginas, das ungeachtet seiner geografischen Nähe zu Piräus Mitglied im Peloponnesischen Bund gewesen war, nun aber dem Attischen Seebund beitreten musste.[79] Bei schließlich unentschiedenem Ausgang des Krieges zwischen den beiden griechischen Großmächten wurde 446 v. Chr. ein dreißigjähriger Frieden Athens mit Sparta geschlossen, wobei die latenten Spannungen freilich bestehen blieben.
Über den Streit Korinths mit Korkyra bezüglich der Einmischung Athens in den Bürgerkrieg in Epidamnos, der Furcht Athens vor einem Engagement Korinths im Norden und über einen Handelsstreit mit dem mit Sparta verbündeten Megara, aber auch aus der Furcht Spartas vor einem weiteren Machtzuwachs Athens, kam es schließlich zum Peloponnesischen Krieg (mit Unterbrechungen von 431–404 v. Chr.), über dessen Verlauf bis zum Jahr 411 v. Chr. Thukydides in seinem berühmten Geschichtswerk ausführlich berichtete; an ihn schloss dann Xenophon mit den Hellenika an.
432 v. Chr. forderten Megara und Korinth Sparta ultimativ zum Eingreifen auf, doch begann der Krieg eher ungeplant mit einem Überfall der mit Sparta verbündeten Thebaner auf die Stadt Plataiai. Sparta fiel 431 v. Chr. in Attika ein, doch hatte Perikles die Bevölkerung in den Schutz der Langen Mauern zurückgezogen. Währenddessen plünderte die athenische Flotte die Peloponnes. Perikles rechnete mit der Erschöpfung des Gegners, während die Spartaner jedes Jahr in Attika einfielen.
Nach dem Tod des Perikles 429 v. Chr. kam eine neue Generation von Politikern ans Ruder, wobei Kleon für eine aggressive, Nikias hingegen für einen ausgleichende Politik gegenüber Sparta standen. 425 v. Chr. schien Sparta aufgrund der Gefangennahme mehrerer Spartiaten zum Frieden bereit, doch wurde dies von Kleon abgewiesen. Sparta reagierte jedoch und marschierte unter Führung des Brasidas 424 v. Chr. in Thrakien ein und bedrohte so die athenische Versorgung mit Getreide. 421 v. Chr. kam es zu einem Friedensvertrag (Nikiasfrieden), der jedoch nicht alle Streitigkeiten ausräumte. Sparta bekämpfte seine Erzrivalin Argos, während Athen unter dem Einfluss des Alkibiades die folgenschwere Sizilienexpedition unternahm (415–413 v. Chr.). Diese endete in einem Desaster für Athen. Die Einnahme von Syrakus misslang, und das athenische Heer wurde vernichtet, während in Griechenland Alkibiades, der zu den Spartanern übergelaufen war, diese zu einer neuen Taktik gegen Athen überredete. In Dekeleia wurde nun auf Dauer ein lakedaimonischer Stützpunkt errichtet, und zudem gewann Sparta die Unterstützung Persiens. Mit Hilfe persischen Goldes baute Sparta eine leistungsstarke Flotte auf. Immer mehr Seebundmitglieder, die von Athen wie Kolonien behandelt wurden, fielen vom Attischen Seebund ab. Im Zuge der Bestrafung abgefallener Bündner und in dem Bestreben, das Seereich als Herrschaftsinstrument auszubauen, kam es von athenischer Seite im Verlauf des Peloponnesischen Krieges vermehrt zu Gräueltaten und Übergriffen, wofür insbesondere das Beispiel der kleinen Ägäis-Insel Melos steht. Auch die Demokratie wurde zu Zwecken der Herrschaftsstabilisierung nach dem Muster Athens innerhalb des Seebunds verbreitet und als Mittel zum Erreichen politischer Ziele der Führungsmacht eingesetzt. 411 v. Chr. kam es wegen der durch den Krieg angespannten Situation in Athen selbst zu einem oligarchischen Verfassungsumsturz, der aber schon 410 v. Chr. rückgängig gemacht wurde – mit Hilfe des wieder zu Athen übergelaufenen Alkibiades.
Spartas neue Flotte unter dem fähigen Lysander bedrohte jedoch weiterhin Athens Lebensnerv. 406 v. Chr. siegten die Athener noch bei den Arginusen, doch unterlag die Flotte im folgenden Jahr in der Schlacht bei Aigospotamoi. Athen kapitulierte 404 v. Chr. vor Sparta, wurde aber nicht zerstört, da Sparta ein Gleichgewicht der Kräfte aufrechterhalten wollte. Korinth und Theben fühlten sich jedoch um die Erfüllung ihrer Kriegsziele betrogen und verfolgten nun eigene Ziele, auch und vor allem gegen Sparta.
Sparta konnte nach dem Sieg von 404 v. Chr. trotz einiger Anstrengungen die Führungsrolle Athens nicht übernehmen; dafür fehlten ihm sowohl die Ressourcen als auch der institutionelle Rahmen. Zudem kam es zwischen Sparta und Persien zum Krieg um Kleinasien (400–394 v. Chr.), da Sparta sich weigerte, die dortigen griechischen Städte den Persern auszuliefern, wie es der Vertrag von 412 v. Chr. vorgesehen hatte. Aber auch in Griechenland brachen die Kampfhandlungen nicht ab. Im Korinthischen Krieg (395–387 v. Chr.) kämpften Argos, Athen, Korinth und Theben gegen die Spartaner. 387/386 v. Chr. kam es schließlich zum sogenannten Königsfrieden, der in Wirklichkeit ein persischer Diktatfrieden war, der den Krieg im griechischen Mutterland aber wenigstens zu einem vorläufigen Ende brachte. Persien erhielt Kleinasien und Zypern, während Athen nur einige seiner alten Kleruchien behalten durfte. Alle anderen Poleis sollten autonom sein.
Auf dem Prinzip von Autonomie und Gleichberechtigung basierte die Idee der Koine Eirene, des Allgemeinen Friedens, die in den Folgejahren starke politische Wirkung entfaltete und neben dem Panhellenismus der prägende politische Gedanke dieser Zeit war. Am Ende scheiterte aber auch diese Friedensidee immer wieder an der Unmöglichkeit, sie ohne die Garantie einer starken Hegemonialmacht durchzusetzen. Der Königsfriede wird von einigen Forschern als erste Verwirklichung einer Koine Eirene angesehen.
Zum Wächter über den Königsfrieden warf sich zunächst Sparta auf, um seine eigene Position zu verteidigen. Es geriet aber zusehends in die Defensive. Athen, welches sich von der Niederlage im Peloponnesischen Krieg langsam erholt hatte, begründete 378/77 v. Chr. den Attischen Seebund neu, allerdings verkleinert und weniger von der athenischen Vormachtstellung geprägt. Tatsächlich waren aber sowohl Sparta als auch Athen über das Anwachsen der thebanischen Machtstellung besorgt und versuchten, den thebanischen Einfluss einzudämmen. Doch während sich die beiden alten Feinde annäherten, kam es 371 v. Chr. zur Schlacht von Leuktra, in welcher das spartanische Heer in offener Feldschlacht von den Thebanern vernichtend geschlagen wurde. Dies bedeutete das endgültige Ende der spartanischen Hegemonie. Auch der Höhenflug Thebens endete bereits nach wenigen Jahren, als 362 v. Chr. der wichtigste thebanische Stratege Epameinondas fiel. Sparta verlor jedoch Messenien und wurde somit zu einer Macht zweiten Ranges, zumal die dringend notwendigen inneren Reformen auch danach nicht verwirklicht wurden.
Auf Sizilien blühte währenddessen die reiche Polis Syrakus und erreichte eine quasi-hegemoniale Stellung unter Dionysios I. Im Laufe des 4. Jahrhunderts v. Chr. jedoch wurde Syrakus von schweren Bürgerkriegen heimgesucht. Bereits seit dem frühen 5. Jahrhundert lieferten sich Karthago und die sizilischen Griechen teils heftige Kämpfe (siehe oben), wobei sich beide Seiten in etwa die Waage hielten. Tatsächlich waren es gerade die Randgebiete – das sogenannte Dritte Griechenland abseits von Athen und Sparta –, die nach dem Peloponnesischen Krieg eine Blütezeit erlebten, wie eben Böotien mit Theben, aber auch Thessalien, Korinth und Megara, die sich vom Krieg erholten und vom Handel profitierten.
Im Norden Griechenlands bestieg 359 v. Chr. Philipp II. den Thron von Makedonien. Ihm gelang es, den größten Nutzen aus den Vormachtkämpfen der griechischen Poleis zu ziehen. Die streitenden makedonischen Adelsfamilien vermochte er stärker als zuvor an das Königshaus zu binden. Vor allem aber schuf er ein stehendes und professionell geschultes Heer, wodurch Makedonien zur führenden Militärmacht in Griechenland wurde. In den 50er Jahren kämpfte er gegen die Phoker und erwarb 352 v. Chr. die Vorherrschaft in Thessalien. 343 v. Chr. folgte die Eroberung Thrakiens samt den dortigen Goldbergwerken, die den wirtschaftlichen Grundstock für den weiteren Machtzuwachs legten. Athen fühlte sich von der expansiven Politik Philipps ernsthaft bedroht. Vor allem Demosthenes versuchte die Athener davon zu überzeugen, dass Philipp sie unterjochen wollte, hatte zunächst jedoch keinen Erfolg. 340 v. Chr. kam es endlich zur Bildung eines Abwehrbundes, doch unterlag das Heer 338 v. Chr. bei Chaironeia dem Heer Philipps. Dieser gründete 337 v. Chr. den Korinthischen Bund, ließ sich zum Hegemon ernennen und wurde de facto zum Beherrscher Griechenlands. Seine Pläne zu einem Feldzug gegen Persien konnte er jedoch nicht mehr verwirklichen: Er wurde 336 v. Chr. ermordet.
Sein Sohn Alexander, später der Große genannt, setzte Philipps ehrgeizige Pläne jedoch in die Tat um: Er zwang die aufständischen griechischen Städte in die Knie und zerstörte Theben. Mit seinem legendären Alexanderzug (ab 334 v. Chr.) öffnete er zugleich den Griechen das Tor zu einer neuen Welt: Er besiegte die persischen Armeen und stieß bis nach Indien vor. Damit endete das klassische Zeitalter Griechenlands.
In der Zeit um 500 v. Chr. entwickelte sich die antike griechische Geschichtsschreibung.[80] Impulse gingen sowohl von dem erweiterten geografischen Horizont als auch von der ionischen Philosophie aus. Hekataios von Milet, die sogenannten Logographen sowie die Historien Herodots stehen am Beginn der überaus reichen griechischen Geschichtsschreibung, die bedeutende Prosawerke hervorbrachte und thematisch äußerst vielfältig war. Herodot und Thukydides stellten den Maßstab dar, an dem sich viele folgende antike griechische Geschichtsschreiber bis in die ausgehende Spätantike orientierten. Die von diesen Autoren behandelten Bereiche der Historiographie reichten von der Universal- und Zeitgeschichte über spezialisierte Schriften (wie die Persika und Indika), historisch-geographischen Werken bis zu Lokalhistorien.
Allerdings ist ein Großteil der antiken Literatur und damit auch der griechischen Geschichtsschreibung verloren gegangen und oft nur in Zitaten und Auszügen erhalten (Die Fragmente der griechischen Historiker).
Bereits in der präarchaischen kykladischen Kunst des 3. Jahrtausends v. Chr. kam das Interesse an der Darstellung des menschlichen Körpers zum Ausdruck, das sich in der Kunst der Alten Griechen bei allem Formenwandel bis zur hellenistischen Plastik erhielt. Bildhauer und Maler der klassischen Periode regte das Studium der menschlichen Beschaffenheit zu realistischer Wiedergabe anatomischer Merkmale sowie von Altersunterschieden, Charaktereigenschaften und Stimmungslagen bei Menschen an. Architekten und Konstrukteure befassten sich mit Maßverhältnissen und Maßstäben, um Tempel und andere Bauten harmonisch zu proportionieren, um Plätze zu schaffen und den angemessenen Rahmen für Opferhandlungen und rituelle Begängnisse.[81]
Im 7. Jahrhundert v. Chr. waren Einflüsse aus den Kulturen des Alten Orients für Griechenland sehr wichtig, wie der Begriff Orientalisierende Periode zeigt. Mit neuen Techniken zur Bearbeitung von Rohstoffen gingen neue Arten von plastischer Kunst, Architektur und Metallurgie einher. Äußere Einflüsse veränderten die Keramik, auf stilbildende Weise zunächst im für Kommunikation und Handel besonders günstig gelegenen isthmischen Korinth. Auch in der Architektur setzten sich neue Ideen zum Schutz sakraler Plätze bzw. von Göttern zunächst in der Umgebung Korinths durch, im Poseidonheiligtum von Isthmia.[82] Frühe Beispiele dorischer Tempelanlagen finden sich im 6. Jahrhundert v. Chr. auf der Peloponnes mit dem Hera-Tempel von Olympia (um 590 v. Chr.) und dem Apollon-Tempel von Korinth (um 560 v. Chr.). Großtempel der ionischen Ordnung entstanden um 550 v. Chr. mit dem Apollonheiligtum von Didyma und um 530 v. Chr. mit dem Hera-Tempel auf Samos.[83] Kouros und Kore waren die beiden rundplastischen Haupttypen der archaischen Epoche, in der sie von überlebensgroßen abstrakten Figuren zu Bildnissen mit menschlichen Proportionen und Ausmaßen entwickelt wurden. Während die Koren mit Gewändern versehen waren, blieben die Kouroi nackt – ein Alleinstellungsmerkmal der griechischen Kunst im Vergleich mit den benachbarten Kulturen.[84]
In klassischer Zeit änderte man die Gewandung der Koren, deren Kleidung tiefere Falten erhielt, enger am Körper lag und die darunter liegenden Glieder erkennen ließ. Die Bildhauer präsentierten ein Ideal von Jugend und Schönheit, das in der Verschmelzung von Realem und Idealem auch den Göttern gefallen sollte. Die Bildhauerwerkstatt des Hagelades in Argos war laut Plinius dem Älteren die Ausbildungsstätte sowohl des Phidias als vermutlich auch des Polyklet.[85] Eine beispiellose Konzentration berühmter Zeugnisse klassischer Architektur hat Athen aufzuweisen, wo nach Perserkriegen Macht und Mittel für ein ausgreifendes Bauprogramm zur Verfügung standen und genutzt wurden. Auf der Akropolis wurde anstelle des zerstörten Vorgängerbaus Mitte des 5. Jahrhunderts v. Chr. der Parthenon errichtet. Zwischen 437 und 432 v. Chr. entstanden die Propyläen. Der bereits 499 beauftragte Bau des Nike-Tempels wurde trotz des Peloponnesischen Krieges zwischen 430 und 420 v. Chr. realisiert; an dem 440 v. Chr. begonnenen Erechtheion hingegen wurde noch Ende des 5. Jahrhunderts v. Chr. intensiv gearbeitet. Auch der Marktbereich, die Athener Agora, wurde in der klassischen Periode um markante Bauten erweitert, darunter das Hephaistaion auf dem Markthügel (Kolonos Agoraios) zwischen 450 und 415 v. Chr., zwei Säulenhallen zwischen 430 und 420 v. Chr., die Stoa des Zeus sowie die Südstoa, und ein neues Buleuterion zwischen 415 und 406 v. Chr.[86]
Die Alleinherrscher der hellenistischen Epoche setzten neue Akzente in Architektur und Kunst. Die Zentren der architektonischen Innovationen lagen nun außerhalb der bisherigen griechischen Metropolen. So bauten die Attaliden die Hauptstadt ihres Königreiches Pergamon zu einem repräsentativen Machtzentrum aus, das es an Gebäuden und Skulpturenschmuck mit Athen und Alexandria aufnahm. Bei der auf dem Burghügel gelegenen Oberstadt wurde der steile Hang dazu genutzt, um die Bauten wirkungsvoll in Szene zu setzen und die Aussicht von den einzelnen Anlagen über die Region zur Geltung zu bringen.[87] In der hellenistischen Plastik wurden Ansätze von Bildhauern des 4. Jahrhunderts wie Praxiteles, Skopas und Lysipp fortgeführt, die neben größerem Realismus nach expressiven Porträtzügen strebten. Als erstes überliefertes Porträt in der griechischen Kunst, das auf individuelle Charakterzüge angelegt war, ist die posthume lebensnahe Statue des Demosthenes anzusehen, die als Werk des Polyeuktos auf der Athener Agora aufgestellt wurde.[88]
Mit dem Alexanderzug begann das Zeitalter des Hellenismus, in dem die griechischen Poleis gegenüber den hellenistischen Großreichen, die sich nach Alexanders Tod 323 v. Chr. bildeten (siehe auch Diadochen), sowie den sich formierenden Bundesstaaten (siehe etwa Achaiischer Bund) in politischer Hinsicht nur eine untergeordnete Rolle spielten, während die griechische Kultur sich bis nach Indien verbreitete.
Griechenland blieb das Schlachtfeld der hellenistischen Großmächte. Vor allem die Antigoniden versuchten, die alte makedonische Hegemonie zu erneuern. Athens Versuch, nach dem Tod Alexanders wieder eine Macht zu werden, scheiterte kläglich (Lamischer Krieg, 323–322 v. Chr.). An die Stelle der Polis traten als Machtfaktor die griechischen Bundesstaaten. Die beiden wichtigsten waren der Aitolische Bund und der Achaiische Bund. In kultureller Hinsicht verlagerte sich der Schwerpunkt mehr in den Osten, wo vor allem Alexandria in Ägypten, später auch Pergamon in Kleinasien eine bedeutende Rolle spielten (siehe auch Diadochen). Die Westgriechen gelangten derweil bereits im Verlauf des 3. Jahrhunderts unter römische Herrschaft.
Ob nach 300 – bedingt durch die Auswanderung von Griechen und Makedoniern und die Anwerbung griechischer und makedonischer Söldner durch die Diadochenreiche – eine teilweise Entvölkerung überbevölkerter Regionen Griechenlands, verbunden mit einem wirtschaftlichen Abschwung, einsetzte, die erst in der römischen Kaiserzeit zum Stillstand kam, ist in der neueren Forschung umstritten. Inzwischen haben archäologische Untersuchungen gezeigt, dass viele griechische Städte im Hellenismus eine ökonomische Blüte erlebten.
Infolge der Kämpfe zwischen den griechischen Klein- und Mittelmächten untereinander sowie mit und gegen Makedonien kam es zum Eingreifen des Römischen Reiches gegen Philipp V. von Makedonien. Im Zweiten Makedonisch-Römischen Krieg (200–197 v. Chr.) wurde Makedonien vernichtend geschlagen. 196 v. Chr. verkündete der römische General Titus Quinctius Flamininus die Freiheit Griechenlands, Rom blieb aber faktisch Protektoratsmacht. Da die Lage weiterhin instabil war, sah sich Rom in der Folgezeit immer wieder gezwungen, insbesondere in Staseis zwischen griechischen „Romfreunden“ und „Romfeinden“ einzugreifen. Nach der Schlacht von Pydna 168 v. Chr. war Makedonien, welches unter König Perseus noch einmal versucht hatte, sich in Griechenland gegen Rom zu behaupten, als Machtfaktor ausgeschaltet. Rom engagierte sich nun dauerhaft in Griechenland. Dies führte nach der Zerstörung Korinths zur endgültigen Unterwerfung Griechenlands: 146 v. Chr. wurde die Provinz Macedonia eingerichtet, 133 v. Chr. die Provinz Asia, die die meisten kleinasiatischen Griechenstädte umfasste, und 27 v. Chr. wurde dann auch der größte Teil Zentral- und Südgriechenlands als Achaea direkter römischer Herrschaft unterworfen. In der Folge siedelten sich immer mehr Italiker in Griechenland und Kleinasien an, die dort wirtschaftliche Interessen verfolgten. Ein letztes Mal wurde die römische Herrschaft über die Hellenen um 88 v. Chr. durch König Mithridates VI. herausgefordert, doch blieb dies Episode.
Nachdem 133 v. Chr. das Reich von Pergamon durch Rom annektiert worden war, folgte 64/63 v. Chr. das Reich der Seleukiden in Syrien (welches aber bereits seit dem späten 2. Jahrhundert nur noch von regionaler Bedeutung gewesen war und seine reichsten Provinzen längst verloren hatte) und 30 v. Chr. schließlich die letzte hellenistische Macht, das Ägypten der Ptolemäer. Damit endete die Epoche des Hellenismus in politischer Hinsicht.
Die politische Geschichte des unabhängigen Griechenland war im Grunde seit 146 v. Chr., spätestens aber seit 27 v. Chr. für fast zwei Jahrtausende beendet; auf die römisch-byzantinische Herrschaft folgte die osmanische. Erst im 19. Jahrhundert sollte das Land wieder ein unabhängiger Staat werden. Doch lebte die griechische Kultur im Rahmen des Römischen Reiches fort und prägte seit dem 2. vorchristlichen Jahrhundert zunehmend auch die römische Zivilisation. Wirtschaftlich gedieh das Land während der langen, weitgehend ungestörten Friedenszeit in den ersten beiden Jahrhunderten n. Chr. (Pax Romana). Kaiser Nero (54–68 n. Chr.) war zudem ein großer Philhellene und gewährte Griechenland zahlreiche Privilegien, die seine Nachfolger aber wieder zurücknahmen. Auch Kaiser Hadrian förderte Hellas und ließ insbesondere Athen besondere Wohltaten angedeihen. Griechisch blieb derweil die lingua franca im gesamten östlichen Mittelmeerraum, und bis in die Spätantike war es für die Eliten Roms auch im Westen nahezu selbstverständlich, neben Latein auch Griechisch zu beherrschen. Die klassische griechische Bildung (paideia) blieb zumindest in der östlichen Reichshälfte auch nach dem Sieg des Christentums noch lange lebendig.[89]
Die sogenannte Reichskrise des 3. Jahrhunderts betraf dann auch Griechenland, das insbesondere in den 260er Jahren unter Barbareneinfällen zu leiden hatte, sich aber wieder einigermaßen erholen konnte und zudem mit dem Neuplatonismus die letzte bedeutende philosophische Strömung der Antike hervorbrachte. Gerade Athen blieb bis ins 6. Jahrhundert nach Christus ein bedeutendes Zentrum antiker Bildung. Seit etwa 580 n. Chr. drangen dann slawische Völker in die oströmischen Balkanprovinzen ein; um 650 n. Chr. war Griechenland bis zur Peloponnes zu großen Teilen slawisch besiedelt und konnte erst im Frühmittelalter wieder für das griechischsprachige Byzantinische Reich zurückerobert werden. In Byzanz lebten die griechische Sprache und griechisches Denken, wiewohl christlich überformt, noch jahrhundertelang fort.
Die Eroberung der römischen Orientprovinzen durch die Araber im Verlauf der islamischen Expansion (seit 636 n. Chr.) besiegelte das Ende der Antike, da Ägypten und Syrien nun dem griechischen Sprach- und Kulturraum entzogen wurden. 698 n. Chr. wurde im gesamten Reich der Kalifen die griechische Amtssprache durch das Arabische ersetzt. Allerdings zeigten sich die Eroberer offen für viele Errungenschaften der griechischen Zivilisation; so wurde nicht weniges davon nur durch die Araber für die Nachwelt bewahrt und schließlich über Sizilien und Spanien wieder dem Abendland vermittelt.[90]
Die vielfältigen Fortwirkungen altgriechischer Sprache Kultur, Kunst und Politik verteilen sich auf den Zeitraum von der Abfassung der homerischen Epen bis zu den Denkschulen der Epikureer und Stoiker, auf staatspolitischer Ebene von der Entstehung der Poleis bis zu den Diadochenreichen in der Nachfolge Alexanders des Großen. Der Mythos der von Zeus entführten Europa, dem der Kontinent seinen Namen verdankt, steht zeichenhaft für eine Reihe von Wirkungslinien, die vor allem in die westliche Zivilisation eingegangen sind.
Mittels zweier Traditionsstränge sieht Thomas A. Szlezák die europäische Zivilisation von der altgriechischen Kultur beeinflusst: zum einen von den durch das Römische Reich und das lateinische Mittelalter vermittelten kulturellen Errungenschaften der Alten Griechen sowie zum anderen durch das, was im Zuge des gezielten kulturellen Zugriffs seit der Renaissance wiedergewonnen wurde.[91] Vielfältige Rückbezüge auf Sprache, Kunst und Kultur des antiken Griechenlands waren auch für den Neuhumanismus und den Klassizismus kennzeichnend.
Erstmals in der athenischen Demokratie des 5. Jahrhunderts v. Chr., so Szlezák, sei der Begriff der Freiheit umfassend reflektiert worden, sowohl als außenpolitische wie als innenpolitische Freiheit. Die geistesgeschichtlichen Voraussetzungen des Freiheitsdenkens sieht Szlezák in der Selbstbehauptung des Individuums gegenüber dem Kollektiv, wie sie bereits mehrfach in der archaischen Lyrik auftauche und sich in Athen zu einem „Programm der individuellen Freiheit, verbunden mit Toleranz gegenüber der Freiheit des Anderen,“ verdichtet habe. Auch der altgriechische Wissenschafts- und Philosophiebegriff habe damit zu tun.[92]
Der Althistoriker Werner Dahlheim bescheinigt den Alten Griechen eine spezifische welthistorische Rolle: „Sie erprobten im Raum der Politik die Möglichkeiten einer vom Willen der ganzen Bürgerschaft getragenen politischen Ordnung und durchmaßen im Raum der geistigen Auseinandersetzungen alle Regungen und Ausdrucksformen, deren der menschliche Geist fähig ist. Sie sind damit zum wesentlichen Ausgangspunkt und zugleich integralen Bestandteil der Geschichte Europas geworden.“[93]
Geerbt hat das moderne Europa von den Alten Griechen laut Szlezák auch die Neugier für Fremde, die stets deren Neugier für die Griechen bei weitem übertroffen habe. Die den Griechen seit dem 4. Jahrhundert v. Chr. vertraute Idee einer Universalgeschichte zeige dies ebenso wie die ethnographische Überlieferung. Die griechische Kultur habe früh eine Tendenz zum Überschreiten der eigenen Grenzen in sich getragen, auch in der religiösen Ausrichtung, wie unter anderem Herodots Herleitung griechischer Götter und Kulte aus Ägypten zeige. Mit der Erweiterung des kulturellen Bezugsrahmens über die eigene Polis hinaus auf andere Poleis und Völker bis hin zum ganzen Kosmos haben die Alten Griechen für Szlezák universalistisches Denken weit stärker vertreten als jede andere alte Hochkultur.[94]
Ein polisübergreifendes religiöses Zentrum und ein Auskunftsort für alle Griechen war das Orakel von Delphi. Zu den in kultischem Rahmen abgehaltenen panhellenischen Spielen fanden sich am jeweiligen Veranstaltungsort Angehörige zahlreicher Poleis zum friedlichen Wettstreit ein. Teils waren die Spiele auch mit Musikwettbewerben verbunden, nicht jedoch in Olympia, wo solche Spiele zuerst ausgerichtet worden waren. Als reine Sportspiele mit wechselnden Austragungsorten sind die Olympischen Spiele in völkerverbindender Absicht Ende des 19. Jahrhunderts wieder aufgenommen worden und bilden seither die bekannteste Verknüpfung mit dem antiken Griechenland.
Gegenüber den Schriftkulturen in Ägypten, Mesopotamien und China, die jener der Alten Griechen vorausgingen und die zu kompliziert waren, um von der jeweiligen Bevölkerung mehrheitlich beherrscht zu werden, bot das griechische Alphabet als erste phonetische Schrift mit Konsonanten und Vokalen den Vorteil der leichteren Erlernbarkeit ohne jahrelange Schulung für breitere Bevölkerungskreise. Die unmittelbare Entsprechung zwischen gesprochener und geschriebener Sprache bewirkte beispielsweise in Athen die allgemeine Rezeption der öffentlich ausgestellten Solonischen Gesetze und die Durchführbarkeit des Ostrakismos, bei dem in der Volksversammlung der Name eines ins Exil zu Verbannenden auf einer Tonscherbe vermerkt war.[95]
Mit ihrem betonten Anknüpfen an antike Vorbilder haben Renaissance und Humanismus sprachliche Übernahmen aus dem (Alt-)Griechischen in die europäische Moderne gefördert. Sei es der romanische, der slawische oder der germanische Sprachbereich: Durchgängig finden sich dieselben griechischen Wörter wieder. „Von Polen bis Portugal, von Skandinavien bis Sizilien wird ein und dieselbe griechische Wurzel verwendet, um Musik oder Philosophie zu bezeichnen; um Tyrannei zu geißeln und Demokratie zu bejahen; um weltweit die olympischen Spiele zu feiern.“ Was einst im hellenischen Kulturkreis entdeckt, erschlossen und benannt worden ist, zieht sich im gegenwärtigen Sprachgebrauch durch eine Vielzahl unterschiedlicher Bereiche, unter anderem von der Mathematik und Geologie, über die Pädagogik und Historiographie bis zu Poesie und Theater.[96]
Die beiden dem Dichter Homer zugeschriebenen Epen Ilias und Odyssee bildeten für die Alten Griechen den literarischen Grundtext schlechthin, der die in den diversen Poleis getrennt voneinander lebenden Bürgerschaften sprachlich einte, weil der von Homer hauptsächlich gebrauchte ionische Dialekt auch dort verstanden wurde, wo man anders sprach. Als wegweisend erwies sich die literarische Qualität beider Versdichtungen, die zur Hochschätzung des Vollkommenen in der Kunst und zur Herausbildung einer Philosophie des Schönen und der ästhetischen Urteilskraft beitrugen. Ilias und Odyssee stehen laut Szlezák auch für eine bestimmte Erzähltechnik und „bildeten die Paradigmen der typisch westlich-europäischen Art, ein Geschehnis in der Dichtung zu gestalten, also die ‚Fabel‘ oder den ‚Plot‘ zu konstruieren.“ Seither sei die griechisch-europäische Tradition zudem darauf ausgerichtet, Werken von Rang eine ethisch hochstehende Orientierung abzuverlangen.[97]
Die Entstehung einer neuen Form von Dichtung in den beiden Grundformen Tragödie und Komödie sieht Szlezák in geistesgeschichtlich einzigartiger Weise verbunden mit der gleichzeitigen Entstehung der Demokratie als Staatsform in Athen. Sowohl Tragödie als auch Komödie hätten, indem sie auch die demokratischen Verfahren der Polis reflektierten, zur Bildung einer politischen Öffentlichkeit beigetragen. „Beides, die Demokratie wie das Drama, bestimmen bis heute nicht nur unser politisches und literarisches Leben im engeren Sinn, sondern auch die Art und Weise, wie wir unsere Gegenwart wahrnehmen, unsere Vergangenheit deuten und unsere Zukunft antizipieren.“[98]
Der fortwirkende Einfluss der antiken griechischen Architektur und bildenden Künste auf die europäische Baukunst und auf die Abbildung des menschlichen Körpers reicht von der Renaissance bis ins 20. Jahrhundert. Bereits die Begüterten unter den alten Römern hatten mit dem Ankauf oder der Kopie griechischer Kunstwerke des 5. und 4. Jahrhunderts v. Chr. für deren Erhalt bzw. Überlieferung gesorgt und so auch diesbezüglich Voraussetzungen geschaffen, an die in der europäischen Neuzeit angeknüpft wurde. So wirkte beispielsweise die Wertschätzung des dorischen Stils als „reine Funktionalität“ im Einklang von Statik und Form in der avantgardistischen Architektur des frühen 20. Jahrhunderts nach.[99]
Der dem antiken Griechenland verpflichtete Klassizismus und Neuhumanismus übten speziell in Deutschland einen nachhaltigen Einfluss auf das zeitgenössische Bildungsideal in Kunst und Literatur aus. Johann Joachim Winckelmann wurde zum Begründer der kunstorientierten klassischen Archäologie. Er lehrte zwischen griechischer und römischer Hinterlassenschaft bei der antiken Kunst zu unterscheiden und setzte mehrere Stilepochen der antiken griechischen Kunst an. Seine Formel von der zeitübergreifend mustergültigen „edlen Einfalt“ und „stillen Größe“ von Kunst und Kultur im antiken Griechenland prägte die Kunstauffassung vor allem in Deutschland nachhaltig, zumal prominente Geistesgrößen wie Gotthold Ephraim Lessing, Johann Gottfried Herder, Johann Wolfgang von Goethe, Friedrich Schiller und Wilhelm von Humboldt sich lobend über sein Wirken äußerten und letzterer in seiner Bildungsreform die Vermittlung des Antikenideals institutionalisierte.[100]
Insbesondere Vertreter der auf Winckelmann basierenden klassischen Archäologie suchten unter den wechselnden historischen Rahmenbedingungen des 19. Und 20. Jahrhunderts eine Leitbildfunktion der altgriechischen Kunst zu erhalten und die diversen modernen Kunstrichtungen und -formen demgegenüber als minderwertig darzustellen. Zu einer politisch stark aufgeladenen Inanspruchnahme Winckelmanns kam es in der NS-Zeit, nachdem Hitler für die Kunstausrichtung des Regimes einen neuen Klassizismus propagiert hatte. Dem breiten Publikum präsentierte der Olympia-Film Leni Riefenstahls zu den Olympischen Spielen 1936 neben Sequenzen zu den Bauten der Athener Akropolis die NS-Körperideologie in Athletengestalt.[101]
Das vielfältige, auf mehrere Jahrhunderte verteilte Gesamtspektrum der antiken griechischen Philosophie hat nachhaltigen Einfluss auf die Entwicklung des philosophischen Denkens in diversen Epochen der europäischen Geistes- und westlichen Kulturgeschichte genommen. Zu den bis in die Gegenwart viel beobachteten, teils aufeinander aufbauenden und Bezug nehmenden altgriechischen Philosophen und Philosophieschulen gehören im 6. Jahrhundert v. Chr. die Vertreter der ionischen Naturphilosophie sowie weitere Vorsokratiker, im 5. Jahrhundert v. Chr. Platon und Aristoteles sowie am Übergang zum 3. Jahrhundert v. Chr. die Epikureer und Stoiker.
Mit Thales von Milet, der eine Sonnenfinsternis korrekt vorherzubestimmen in der Lage war, setzt eine Vorstellung der Weltentstehung ein, die ohne mythische Herleitung auskommt: Als Urstoff fungiert bei ihm Wasser.[102] Von dem Sophisten Protagoras ist der Satz überliefert, wonach der Mensch das Maß aller Dinge sei, Gefolgert wird daraus unter anderem, dass die Wahrheit des Menschen so beschaffen ist, wie sie ihm erscheint. Die Entdeckung und Anerkennung der Relativität menschlicher Wertvorstellungen, so Szlezák, gehört zu den Errungenschaften des griechischen Geisteslebens im 5. Jahrhundert v. Chr. Das systematische Argumentieren Pro und Contra sei in der Sophistik aufgekommen. Pluralismus und Relativismus wiederum seien die „nicht hinterfragbaren Grundlagen des dominierenden politischen Glaubens der europäisch-amerikanischen Moderne.“[103]
Für die Entfaltung des abendländisch-philosophischen Denkens über die Jahrhunderte bedeutsam aber waren laut Szlezák vor allem Platon und Aristoteles. Beide hätten die ihnen noch vorliegenden Werke der Vorsokratiker auf kreative Weise neu gedeutet und das metaphysische Denken entwickelt. Sie seien die einzigen Philosophen, die auf fast allen Teilgebieten der Philosophie Bahnbrechendes geleistet hätten. Mit der Metaphysik sei die Reflexion auf die „Gesamtheit unserer geistigen Bemühungen“ verbunden wie auch auf die Unterscheidung von Wissenschaft und Philosophie. „Kurzum, mit der Metaphysik entsteht auch die Wissenschaftstheorie, und mit ihr die formale Logik, deren Ausarbeitung Aristoteles als erstem gelingt.“[104]
Der Entstehung des Politischen bei den Alten Griechen ist Christian Meier in dem Bewusstsein nachgegangen, dass diese – anders als beispielsweise alle neuzeitlichen politischen Denker und Akteure – sich das grundlegende begriffliche Instrumentarium zur Gestaltung und Erfassung politischer Konstellationen und Entwicklungen im historischen Geschehen überhaupt erst schaffen mussten. Im Falle Athens reichen die markanten Stationen von den Solonischen Reformen im Zeichen der Eunomie über die auf Isonomie zielenden Kleisthenischen Reformen bis zur Demokratie in klassischer Zeit.[105]
Als „erste Zivilgesellschaft der Menschheit“ bezeichnet Szlezák Athen zur Zeit der entwickelten Demokratie. Viele Tausende von Bürgern seien sich ihrer fundamentalen Freiheits- und Bürgerrechte bewusst gewesen, etwa des Rechts auf freie Rede und der Klageerhebung, und hätten sie in der politischen Auseinandersetzung auch zur Geltung gebracht. Das Prinzip der Rechenschaftslegung von Amtsträgern gegenüber dem Volk habe ein vergleichsweise wirksames Kontrollinstrument dargestellt.[106] Mit einem universalistischen und kosmopolitischen Geist sieht Szlezák das Athener Bildungswesen verbunden. Bereits für Isokrates habe die Bezeichnung Grieche mehr für ein Denken als für eine Abstammung gestanden. Grieche sei man eher durch die Teilhabe an attischer Bildung als durch Geburt. In diesem Sinne habe dann auch Goethe die Devise ausgegeben, jeder solle versuchen, auf seine Art ein Grieche zu sein.[107]
Während in einem politischen System mit zentralisierter Herrschaft laut Josiah Ober die Macht von „Spezialisten im Herrschen“ ausgeübt wird, bildete die Hinwendung „zu kollektiver Selbstregierung durch Amateure“ eine wichtige Grundlage für die außergewöhnliche Wohlfahrt und historische Sonderstellung des klassisch-antiken Griechenlands. „Die historisch einmalige griechische Form von Bürgerschaft und politischer Ordnung und ihre Rolle bei der Förderung von Spezialisierung und ständiger Innovation durch die Einführung von Bürgerrechten, bei der Koordinierung einer zahlenmäßig starken Gruppe von Menschen, die gleichzeitig herrschten und beherrscht wurden, und bei freiem Informationsaustausch war der entscheidende Faktor, durch den sich die Blüte des klassischen Griechenland in der Geschichte der Vormoderne auszeichnet.“[108]
Unter dem Eindruck des mannigfaltigen Nachwirkungsspektrums der Alten Griechen wurde deren Geschichte im Urteil der Nachwelt nicht selten als höchst bedeutsam herausgestellt. Das gilt speziell für den Krieg der griechischen Poleis gegen die Perser in den Anfängen des 5. Jahrhunderts v. Chr., der unter anderem als Existenzkampf des Griechentums im Hinblick auf seine politischen und geistigen Entwicklungspotenziale angesehen wurde. So erklärte beispielsweise der Brite John Stuart Mill die Schlacht bei Marathon zu einem Ereignis auch der englischen Geschichte und in dieser für wichtiger als die Schlacht bei Hastings.[109] Für Christian Meier wiederum bildet die Schlacht von Salamis „ein Nadelöhr, durch das die Geschichte hindurch mußte“, wenn in ihr statt großer, monarchisch regierter Reiche die in vielen selbständigen Poleis – oft unter weitgehender politischer Mitsprache breiter Schichten – lebenden alten Griechen eine wegweisende Rolle spielen sollten.[110]
Mit ihren Werken haben Herodot und Thukydides laut Szlezák das kulturelle, historische und politische Bewusstsein Europas um Erfahrungen und Denkformen bis in die Gegenwart höchst wirksam bereichert. Herodot stehe für die ungewöhnliche Botschaft und Fähigkeit der Griechen zum Ernstnehmen und Verstehen des Fremden bzw. zu kultureller „Übersetzung“. Trotz novellistischer Buntheit und vieler Unzulänglichkeiten verdiene er den Titel „Vater der Geschichtsschreibung“ mit seiner Darstellung eines welthistorischen Geschehens aus breit streuendem Material, mit dem er die Vielfalt des Lebens erschließe. Herodot sei nicht nur der erste große historische Erzähler, sondern auch „der bis heute mit Abstand am vergnüglichsten zu lesende Geschichtsschreiber.“[111]
Von Thykydides heißt es bei Dahlheim, dass er den Willen der Götter aus der Geschichte verbannte und stattdessen das Streben des Menschen nach Macht, Besitz und Geltung als bestimmende Wirkkräfte im historischen Geschehen ansetzte. Die Entdeckung der politischen Geschichte durch Thukydides markiere einen Erkenntnisfortschritt, wie er vergleichbar erst in den neuzeitlichen Schriften Macchiavellis wieder erreicht worden sei.[112] Nicht von Moralismus und „Empörungslust“ sei die Geschichtsdarstellung des Thukydides geprägt, sondern vo „kühler Distanziertheit, strikter Faktennähe, leidenschaftsloser Objektivität“, so Szlezák. „Mit diesen Mitteln schuf Thukydides ein Werk, das einerseits literarisch in Stil und Komposition höchste Meisterschaft erreicht, andererseits für Geschichtsschreibung und für illusionslose politische Analyse bis in die Neuzeit den Maßstab gesetzt hat.“[113]
In Forschungs- und Wissenschaftsgeschichte sei der Einfluss des Alten Orients auf das antike Griechenland lange ignoriert, wenn nicht entschieden zurückgewiesen worden, findet Linda-Marie Günther.[114] Unterdessen aber hätten alle altertumskundlichen Disziplinen die Bedeutung eines intensiven Kulturtransfers für wesentliche Entwicklungsprozesse der griechischen Antike erkannt. Nur in einem hoben sich die Alten Griechen laut Günther in vorklassischer Zeit von anderen älteren Kulturen ab: in der beständigen Pflege des Wettstreits untereinander, in der allgemein erwarteten Bereitschaft, sich mit anderen zu messen und so den bzw. die Besten zu ermitteln. Diese Wettbewerbsmentalität (Agonalität) habe eine soziale Mobilität mit Aufstiegschancen und Abstiegsrisiken mit sich gebracht, die der Alte Orient so nicht gekannt habe.[115]
Als Vorläufer eines Sonderweges, den Europa im Vergleich mit anderen Kulturen eingeschlagen und über weite Strecken zurückgelegt habe, sieht Christian Meier die Alten Griechen. „Bei allen Übernahmen aus dem Orient: Sie haben sich der unendlich vielen Güter, die sie dort fanden, nur zu ihren eigenen, kühnen, waghalsigen Zwecken bedient. Im wesentlichen war es alles andere als eine Fortsetzung, sondern ein Neuanfang, den wir bei ihnen finden.“ Die von ihnen ins Werk gesetzte politische Revolution verdiene „neben den langgestreckten Revolutionen der Weltgeschichte“, der Neolithischen und der Industriellen, einen eigenen herausgehobenen Platz. Allerdings sei das nachhaltige Fortwirken altgriechischer Errungenschaften auch ein Verdienst der römischen Aristokratie, die sich davon im Zuge der Schaffung eines antiken Weltreiches habe faszinieren lassen. Die so entstandene griechisch-römische Kultur habe sich dann weit nach Westen ausgebreitet, sodass sie „in eine andere Sprache, in einen anderen Kontext übersetzt werden konnte, aus dem sie dann aber neue Kräfte zu ziehen vermochte.“[116]
Auf die anhaltend starke Wirtschaftsleistung auch in nachklassischer Zeit führt Josiah Ober die „Unsterblichkeit“ des antiken Griechenlands zurück. „Hellas war groß aufgrund einer kulturellen Leistung, die durch dauerhaftes Wirtschaftswachstum getragen wurde. Dieses Wachstum wiederum wurde durch einen besonderen Politikstil ermöglicht.“[117] Statt in wirtschaftlichen und kulturellen Niedergang zu münden, folgte auf die klassische eine hellenistische Blüte. Viele griechische Poleis der hellenistischen Zeit „übernahmen prägende Institutionen und kulturelle Normen des klassischen Athen. Tatsächlich waren gegen Ende des 4. Jahrhunderts v. Chr. wohl mehr griechische Poleis Demokratien als je zuvor.“[118] Indem der politische Niedergang der großen, unabhängigen Stadtstaaten nicht zugleich das Ende von Hellas mit sich brachte, „hat sich das Andenken an die politische Ausnahmestellung Griechenlands als Teil des Welterbes erhalten“.[119] Zwar seien Autokratie und absolute Herrschaft seitdem weit verbreitet geblieben; doch gebe es eben auch das Wissen um die Alternative – dass nämlich „ein glänzendes Zeitalter bürgerzentrierter Politik und hoher Kultur möglich ist.“[120]
Geschichte
Leben
Kunst
Archäologie
Einführend (weitere Angaben sind in der Bibliografie Antike zu finden sowie vor allem den Bibliografien der Bände der Cambridge Ancient History, 2., grundlegend veränderte Auflage. Für die Zeit nach 30 v. Chr. vgl. die Artikel Römisches Reich, Byzantinisches Reich und Spätantike):

Als Apartheid (wörtlich „Getrenntheit“) wird eine geschichtliche Periode der staatlich festgelegten und organisierten sogenannten Rassentrennung in Südafrika und Südwestafrika bezeichnet. Sie war vor allem durch die autoritäre, selbsterklärte Vorherrschaft der „weißen“, europäischstämmigen Bevölkerungsgruppe über alle anderen gekennzeichnet. Bereits Anfang des 20. Jahrhunderts begonnen, hatte sie ihre Hochphase von den 1940er bis zu den 1980er Jahren und endete 1994 nach einer Phase der Verständigung mit einem demokratischen Regierungswechsel, bei dem Nelson Mandela der erste schwarze Präsident des Landes wurde. Heute wird der Begriff manchmal auch als Synonym für rassistische Segregation im Allgemeinen verwendet.[1] Zudem wurde das politische Handeln mit solchen Bestrebungen als Straftatbestand ins internationale Recht aufgenommen (→ Apartheid (Recht)).
Apartheid bedeutet ‚Getrenntheit‘, gebildet aus dem Afrikaans- oder niederländischen Adjektiv apart für ‚getrennt, einzeln, besonders, anders‘, was ursprünglich aus dem Lateinischen stammt: pars ‚der Teil‘, ad partem ‚(nur) zu einem Teil‘.[2] In weiteren Sprachen:
Bei der Entwicklung von Theorie und Praxis der Apartheid waren viele historische, gesellschaftlich-soziologische, religiöse und psychologische Faktoren wirksam. Die Relevanz und Bedeutung der einzelnen Komponenten wird von der Forschung kontrovers diskutiert. Im engeren Sinne wird nur die seit 1948 praktizierte gesetzlich verankerte Politik der Rassentrennung als Apartheid bezeichnet. In Südafrika wird der Apartheidsbegriff von offiziellen Stellen bereits für die politisch-legislativen Maßnahmen zur Rassentrennung vor 1948 verwendet, da die Grundlagen der Apartheid bereits ab 1908 schrittweise entstanden. Mit dem Sieg der Nationalen Partei – sie gewann zwar keine Mehrheit der Stimmen, siegte aber aufgrund des Wahlsystems – bei den Parlamentswahlen 1948 und der sich anschließenden Regierungsbildung unter Führung von Daniel François Malan erreichte die Ideologie der Apartheid eine Dynamik hin zu einer noch strengeren und autoritären Ausprägung als die Rassentrennungspolitik vorangegangener Regierungen.[3][4] Die Geschichte der Apartheid in Südafrika wurde vor allem durch die Konflikte zwischen zugewanderten Bevölkerungsgruppen der Bantu, niederländischstämmigen Buren, Briten und später auch den als Coloured bezeichneten „Mischlingen“ sowie Indischstämmigen geprägt. Dementsprechend war die demographische Struktur Südafrikas eine Basis zur Herausbildung des Apartheidsystems.
Ursprünglich war die Region südlich des Sambesi von den San besiedelt. Im 16. und 17. Jahrhundert stießen bantusprachige Gruppen aus dem Norden in deren Siedlungsgebiet vor und verdrängten die indigene Bevölkerung teilweise. Mitte des 16. Jahrhunderts errichteten portugiesische Seefahrer als erste Europäer kleine Niederlassungen an der Küste. 1652 gründete Jan van Riebeeck am Kap der Guten Hoffnung im Namen der Niederländischen Ostindien-Kompanie eine Station zur Versorgung von Schiffen mit Lebensmitteln, aus der in der Folge Kapstadt entstand. Die Niederländer, ab dem 18. Jahrhundert als Buren bekannt, betrieben Landwirtschaft und begannen mit den Einheimischen Handel zu treiben. Die Briten erlangten die Kontrolle über die Kapprovinz. Bei ihrem Vordringen Richtung Osten stießen sie auf die Xhosa; von 1779 bis 1879 kam es zu neun Kriegen (Xhosa- oder Kap-Grenzkriege), bei denen die Xhosa den weißen Truppen unterlagen.
Die niederländischstämmigen Buren waren durch den Calvinismus geprägt, der Johannes Calvins Prädestinationslehre weiterentwickelte. In der neo-calvinistischen Nederduitse Gereformeerde Kerk (NGK) auf dem Gebiet des heutigen Südafrikas, der auch heute noch die Mehrzahl aller weißen Afrikaaner angehören, war es bis 1857 selbstverständlich, dass Weiße und Nichtweiße gemeinsam beteten und kommunizierten. Erst 1857 beschloss diese, dass Nichtweiße „ihre christlichen Privilegien in einem separaten Gebäude oder Institute genießen“ sollten. Zur religiösen Legitimation der Apartheid wurden Stellen aus dem Alten Testament wie Dtn 23,3 EU oder Jos 23,9-13 EU herangezogen.[5]
Mit zentralen Aussagen Calvins, für den eine Unterscheidung zwischen arm und reich, Freien und Sklaven, Frauen und Männern sowie Rassen bzw. Nationalitäten in der Kirche undenkbar war (siehe Gal 3,26-29 EU), ist eine theologische Rechtfertigung der Apartheid wie etwa durch die NGK nicht vereinbar.[6] Wiederholt wurde in der Forschung (beispielsweise F. A. van Jaarsfeld, Edward A. Tiryakian und T. Dunbar Moodie) ab den 1950er Jahren die Meinung vertreten, dass einige Aspekte des Calvinismus eine wichtige Rolle bei der Ausbildung des Apartheidssystems gespielt hätten. Diesen Sichtweisen wurde ab den 1980er Jahren (beispielsweise von André du Toit oder Norman Etherington) vermehrt widersprochen.[7][8]
Unter der britischen Herrschaft zu Beginn des 20. Jahrhunderts bildeten sich die ersten umfassend geplanten Apartheidsstrukturen in Südafrika heraus.
Von 1903 bis 1905 sollte die South African Native Affairs Commission (SANAC) eine gemeinsame Ethnienpolitik für alle vier südafrikanischen Provinzen (Natal, Kapkolonie, Oranje-Freistaat und Transvaal) festlegen. Die Kommission schlug die Errichtung im Sinne der in Natal herrschenden Praxis der Native Administration vor. Diese separate Zuständigkeit einer mächtigen Verwaltungsbehörde wurde ab 1958 in Form der Bantu Administration das organisatorische Zentrum im Apartheidregime.
1910 wurde die Südafrikanische Union durch den Zusammenschluss der vier Provinzen gegründet. Die Union war von Anfang an unter Kontrolle der Weißen. Schwarze wie auch Farbige und Asiaten erhielten kein Wahlrecht, obwohl es Bemühungen dieser Art durch den Missionar James Stewart gegeben hatte. Nur an den Provinzregierungen durften sie partizipieren. Des Weiteren war jeglicher sexuelle Kontakt zwischen den unterschiedlichen als „Rassen“ bezeichneten Bevölkerungsgruppen verboten. Die Segregationspolitik wurde durch die weißen Machthaber mit einer wachsenden Zahl von Gesetzen untermauert.
Der Mines and Works Act legte 1911 die ungleiche Behandlung der Weißen und Schwarzen in der Wirtschaft fest. Das wohl wesentlichste Gesetz der räumlichen Trennung, der Natives Land Act, wurde 1913 in Kraft gesetzt. In der Folge durfte die schwarze Bevölkerung nur noch in den ihnen zugewiesenen Reservaten Land erwerben. Diese Areale umfassten rund 7,3 Prozent des südafrikanischen Territoriums. Zehn Jahre später vollzog der Natives Urban Areas Act die räumliche Trennung auch in städtischen Gebieten. Gegen die wachsende Ungleichheit der Bevölkerungsgruppen erklärte sich im Jahre 1923 eine interkirchliche Missionskonferenz, die sich unter der Leitung von Johannes Du Plessis mit einem diesbezüglichen Forderungspapier an die damalige Regierung Südafrikas wandte.
Im Jahr 1924 schränkte der Industrial Conciliation Act das Zusammenwirken der möglichen Tarifpartner ein. Mit diesem Gesetz schuf man sogenannte Industrieräte (englisch: Industrial Councils), um die bisherigen Auseinandersetzungen zwischen Arbeiterkommandos und dem Militär zu verhindern. Diese Industrieräte arbeiteten ähnlich wie Tarifkommissionen und hatten Beschlussfassungskompetenz, die jedoch im Einzelnen durch den Arbeitsminister bestätigt werden mussten. Von der Seite der Arbeiter konnten nur Personen mit dem vom Gesetz definierten Status als employees (Arbeitnehmer) daran mitwirken. Schwarze Arbeitnehmer waren jedoch davon ausgeschlossen und galten demzufolge auch nicht als tariffähig. Die Regierung des 1924 gewählten Bündnisses zwischen der National Party und der South African Labour Party unter dem gemeinsamen Ministerpräsidenten James Barry Munnick Hertzog entwickelte eine Civilized Labour Policy (zivilisierte Arbeitspolitik), nach der alle öffentlichen Arbeitgeber nur noch weiße Arbeitskräfte einzustellen hatten. Demnach verloren beispielsweise im staatlichen Eisenbahnbereich tausende schwarze Arbeitnehmer ihren Arbeitsplatz. Der damalige sozialdemokratische Arbeitsminister Frederic Creswell definierte „unzivilisierte Arbeit“ als eine Tätigkeit von Personen, die sich auf einen Lebensstil mit den nur allernötigsten Verpflichtungen beschränken, wie es unter „barbarischen und unentwickelten Menschen“ üblich sei.[9]
Weitere gesetzliche Maßnahmen, die zu Einschränkungen für die nichtweiße Bevölkerung führten, gab es in den 1930er und 1940er Jahren. Im Jahr 1948 gewann die Nationale Partei die Parlamentswahlen, wobei zwar ihre Gegner die Mehrheit der Wählerstimmen gewannen, sie aber aufgrund des Wahlsystems die Mehrheit der Sitze bekam. Zwar gewannen bei der nachfolgenden Parlamentswahl 1953 erneut Apartheidsgegner (äußerst knapp) die Mehrheit der Wählerstimmen, die NP konnte sich aber wegen des Wahlsystems wieder die Mehrheit der Sitze sichern. Danach hatte die NP das Wahlsystem ausreichend geändert und Minderheiten konsequent vom Wahlrecht ausgeschlossen[10], sodass sie durch Wahlen nicht mehr ernsthaft in ihrer Macht bedroht war und bis 1994 an der Regierung blieb.
Aus dieser Tradition heraus wurde die Rassentrennung gerechtfertigt, mit Empfehlungen aus einer mit Wissenschaftlern besetzten Kommission unterstützt und schließlich mit Gesetzen und einer speziellen Behörde institutionalisiert. In den Gesetzestexten wurde die Apartheid dabei mit dem Euphemismus „Getrennte Entwicklung“ (afrikaans: Afsonderlike Ontwikkeling) bezeichnet.
Der Sieg der burischen Nationalisten war eng verknüpft mit dem Zweiten Weltkrieg. Unter dem zuvor amtierenden Premierminister Jan Christiaan Smuts beteiligte sich Südafrika an der Seite der Briten an militärischen Auseinandersetzungen. Die Nationalisten hingegen waren gegen eine Einmischung in das kriegerische Geschehen und sympathisierten offen mit dem deutschen nationalsozialistischen Regime. Das wahlberechtigte Volk stimmte mehrheitlich mit den Nationalisten überein.
Der Regierungswechsel stellte für viele Buren, die zuvor unter britischer Herrschaft kaum Anschluss an die führende Spitze des Landes gefunden hatten, den Ausstieg aus der Armut dar. Viele zogen in urbane Gebiete und fanden dort in der aufstrebenden Wirtschaft Arbeit. Die Nationalisten, die sich im Übrigen von den Briten abzugrenzen versuchten, lenkten nun auch die Rassenpolitik in neue Bahnen. Dabei verfolgten sie drei Ziele: Erstens wollten sie die politische Macht konsolidieren, zweitens ihre Vision der Rassenbeziehungen umsetzen und drittens sollte der Bildungsstand und wirtschaftliche Status der Buren angehoben werden.
Vor 1948 waren die Schwarzen schrittweise von der selbstbestimmten politischen Teilhabe und hohen Positionen in der Wirtschaft ausgeschlossen. Die Rassentrennung war zum Teil durch das Gesetz und zum Teil durch den inoffiziellen Brauch gegeben. Die Ordnung war jedoch nicht sehr strikt. Es gab durchaus Farbige, die neben Weißen wohnten, indische Geschäftsleute, welche im Stadtzentrum ihren Aufgaben nachgingen, oder Schwarze, die außerhalb ihrer Reservate ihre Farmen bewirtschafteten.
Diese „Löcher“ in der Rassentrennung schlossen die Nationalisten mit diversen Maßnahmen. Als erstes teilten sie die ganze südafrikanische Bevölkerung in vier ethnisch differenzierte Klassen ein: Weiße, Farbige, Asiaten und Schwarze bzw. auf Englisch White, Coloured, Asiatic oder Indian und Native oder später Bantu und African.[11] (Siehe auch: Bevölkerung Südafrikas). Die Zuordnung zu einer dieser Gruppen geschah nach bestimmten Kriterien. Die Interpretation der Testergebnisse lag oft im Ermessen des Versuchsleiters. Dies betraf besonders die Einteilung in Schwarze und Farbige. Es kamen dabei verschiedene Tests zum Einsatz, wie zum Beispiel, ob ein in die Haare gesteckter Stift herunterfällt, wenn der Proband den Kopf schüttelt. Fiel der Stift heraus, so galt der Proband als Farbiger, blieb der Stift stecken, galt er als Schwarzer. Dies hatte zur Folge, dass Kurzhaarfrisuren populär wurden. Ein anderer dieser Tests bestand darin, dass der Testleiter mit Kraft eine Fingerkuppe der zu testenden Person zusammendrückte. Aus der Farbe des nach dem Loslassen verfärbten – weil blutleeren – Fingernagelbetts wurde auf die Rassenzugehörigkeit geschlossen.
Die Rassenordnung bestimmte fortan das gesamte Leben. An öffentlichen Orten war eine strikte Trennung von Weißen und Nicht-Weißen vorgeschrieben. Mischehen waren verboten. Mit dem Group Areas Act vom 13. Juni 1950 wurde die Trennung der Wohngebiete festgeschrieben. In städtischen Gebieten wurden getrennte Wohnbereiche für die verschiedenen Rassen geschaffen; die Ausbildung richtete sich ebenfalls nach der entsprechenden Rasse. Schwarze mussten außerhalb ihrer Reservate einen Pass bei sich tragen. Damit sollten in städtischen Gebieten nur jene Schwarzen geduldet werden, die dafür eine Arbeitserlaubnis vorweisen konnten. Alle übrigen Schwarzen wurden als Ausländer angesehen. Die in den Städten arbeitenden Schwarzen wurden als Gastarbeiter akzeptiert. Sie lebten überwiegend in sogenannten Townships am Stadtrand. Nichtstädtische Schwarze durften sich gemäß dem Native Laws Amendment Act von 1952 ohne Genehmigung nur 72 Stunden in Städten aufhalten. Damit war die Apartheid legalistisch vollständig. Dennoch war der Lebensstandard, die Bildungsmöglichkeiten in Schulen und den wenigen für sie zugelassenen Universitäten sowie die medizinische Versorgung und somit die Lebenserwartung der Schwarzen höher als in allen anderen afrikanischen Ländern, weswegen Südafrika auch während der Apartheid mit illegaler Einwanderung aus den nördlichen Anrainerstaaten konfrontiert war.
1953 wurde der Bantu Education Act verabschiedet, der am 1. April 1955 die Kontrolle über die Bildung der Schwarzen vom Bildungsministerium auf das Native Affairs Department übertrug. Hintergrund war es, die Afrikaner zu körperlicher Arbeit auszubilden; anstelle von Mathematik und Englisch sollte Landwirtschaft gelehrt werden. Gleichzeitig sollten alle afrikanischen Grund- und Oberschulen, die von Kirchen und Missionen betrieben wurden, von der Regierung übernommen werden, ansonsten würden diese Schulen keine staatlichen Mittel mehr als Unterstützung erhalten. Aus Protest rief der African National Congress (ANC) einen einwöchigen Schulboykott aus, der am 1. April 1955 beginnen sollte. Daraufhin wurde das Gesetz dahingehend geändert, dass die Erziehung für alle gleich sein solle.
Die Politik in Südafrika schuf eine Reihe von verschiedenen Gesetzen, Verordnungen und administrativen Strukturen, welche den Regierungen weitgehende Vollmachten ermöglichten, die Benachteiligung großer Bevölkerungsgruppen durchzusetzen und die Macht der Weißen über die anderen Gruppen zu untermauern.
Nach dem Ende des Zweiten Burenkriegs beauftragte der Gouverneur der Kapkolonie, Lord Milner, 1903 eine Kommission (South African Native Affairs Commission) mit der Untersuchung aller Lebensverhältnisse in der Eingeborenenbevölkerung von den vier „Südafrikanischen Kolonien“, also in der Kapkolonie einschließlich Natal, in Transvaal sowie in der Oranjefluss-Kolonie.
Diese von 1903 bis 1905 tätige Kommission war auf Grund ihrer personellen Zusammensetzung von europäischen Interessen geprägt und stand unter der Leitung von Sir Godfrey Lagden. In der Öffentlichkeit trug die Kommission und deren Report seinen Namen.[12] Der sogenannte Lagden-Report und die aus seinen Empfehlungen folgende Gesetzgebung wird heute als Reaktion auf die Minderheitssituation der weißen Bevölkerung mit ihren fortschreitenden wirtschaftlichen Problemen interpretiert.
Der zunehmende Landbesitz (treuhänderisch oder Gewohnheitsrecht) unter der einheimischen Bevölkerung, eine damalige zentrale Frage, sollte nach den Empfehlungen des Reports so gestaltet werden, dass er von den Gebieten der weißen Bevölkerung sowohl räumlich als auch strikt rechtlich abgetrennt war. Die weiße Bevölkerung begann sich mit Landbesitz zu bevorraten. Die dadurch eintretende Landverknappung minderte den sozialen Aufstieg der schwarzen Bevölkerung durch eigene landwirtschaftliche Betätigung und hemmte auf diese Weise nicht nur deren Gesamtentwicklung, sondern erzeugte eine Bevölkerungswanderung in Südafrika mit Konzentration an neuen Orten. Diese institutionelle Benachteiligung der einheimischen Bevölkerung zu Ungunsten ihrer Erwerbsgrundlagen in den Heimatgebieten schuf eine wachsende Zahl von Wanderarbeitern, die sich zunehmend in den Bergbauzentren konzentrierten oder sie in einem Abhängigkeitssystem zu Lohnarbeit auf „weißen“ Farmen zwang.[12]
Mit dem Natives Land Act (Act No. 27) von 1913 wurde versucht, den Landerwerb durch die schwarze Bevölkerung außerhalb jener Gebiete zu stoppen, die von der Regierung für ihre Ansiedlung vorgesehen waren. Somit betrieb man eine Konzentration der einheimischen afrikanischen Bevölkerung in den African Reserves genannten neuen Siedlungsarealen, indem man sie dort durch Grundstückskäufe und Pachtverträge gezielt zu binden versuchte. Auf diese Weise waren die Bewohner jener Areale als niedrigentlohnter Arbeitskräftepool zu Gunsten der Farmen und den Industrien der Weißen in den Städten gesteuert verfügbar gehalten. An dieser damaligen Strukturentwicklungspolitik wird der ökonomische Charakter des Apartheidkonzeptes erkennbar. Der Natives Land Act wird demzufolge als erster legislativer Meilenstein für die gesamte Apartheidpolitik angesehen. Auf seiner Grundlage schuf man 1916 die Beaumont-Kommission (Beaumont Commission), deren Aufgabe es war, eine nähere räumliche Definition für die neuen „schwarzen“ Siedlungsgebiete festzulegen. Sie erhielt ihren Namen nach William Henry Beaumont, einem ehemaligen Administrator von Natal.[13]
Der Vertreibungsprozess der schwarzen Bevölkerung aus den Städten in die African Reserves begann in Transvaal, wo die Transvaal Local Government Commission (Stallard Commission) nach der neuen Ansiedlungspolitik zielstrebig vorging. Sie argumentierte dabei mit ihrer grundsätzlichen Auffassung, wonach die Städte von der weißen Bevölkerung angelegt wurden und deshalb der schwarzen Bevölkerung darin nur ein zeitweiliger Aufenthalt erlaubt wäre.[14]
Der Native (Urban Area) Act (Act No 21) von 1923 stellt den zweiten Meilenstein in der frühen Apartheidsphase dar. Diesem Gesetz folgte 1945 in der Sache der Native (Urban Areas) Consolidation Act (Act No 25), der 1986 wiederum durch den Abolition of Influx Control Act '(Act No 68) aufgehoben wurde.[15]
Im Jahr 1927 beschloss das Südafrikanische Parlament den Native Administration Act (Act No 38). Dieses Gesetz gestaltete alle Fragen der einheimischen Bevölkerung in der Weise neu, dass die Zuständigkeit von der parlamentarischen Ebene der Südafrikanischen Union in die Verantwortung der Regierung und ihre regionalen Verwaltungen verschoben wurde. Damit festigte man mittels der Gesetzgebung ein Zweiklassen-Staatsbürgerrecht, das die Grundlage für das 1951 in Kraft getretene Gesetz Bantu Authorities Act bildete, mit dem später eine Selbstverwaltung unter weißer Oberaufsicht geschaffen wurde.[16][17]
Während der weltwirtschaftlichen Depression in den 1930er Jahren verstärkte sich auf dem Gebiet der Südafrikanischen Union die Überweidung landwirtschaftlicher Flächen und es entstand eine Überbevölkerung in den betroffenen Regionen des Landes, was eine fortschreitende Bodenerosion und sinkende Nahrungsmittelproduktion verursachte. Die naturräumlichen Veränderungen nutzte man zu weiteren reglementierenden Eingriffen in den Grundstücksverkehr. Dazu beschloss das Parlament 1936 auf Empfehlung der Beaumont-Kommission den Development Trust and Land Act (Act No 18). Das Gesetz stellte eine Reaktion auf die zunehmenden Konflikte zwischen „illegalem“ Landbesitz durch schwarze Farmer und den gesetzlich begünstigten weißen Farmern dar. Mittels dieser Rechtsvorschrift schuf die Südafrikanische Union ein System zur Registrierung der Farmwirtschaft sowie eine Kontrolle der Viehhaltung und Zuteilung der Landverpachtung an Schwarze. Zudem verbot man für die schwarze Bevölkerung den Grundbesitz und dessen Erwerb außerhalb der angewiesenen Siedlungsgebiete. Im Jahr 1936 wurde mit dem Representation of Natives Act das Wahlrecht der schwarzen Bevölkerung für eine Parlamentsvertretung stark eingeschränkt.
Der Natives Laws Amendment Act (Act No 46 / 1937) von 1937 reduzierte die Rechte von schwarzen Arbeitnehmern. Arbeitssuchende aus ländlichen Gebieten hatten nun in den Städten nur noch ein Aufenthaltsrecht für 14 Tage einschließlich ihrer Rückkehr zum Heimatort. Der industrielle Aufschwung im Verlauf des Zweiten Weltkriegs beförderte im Parlament und in der Regierung Haltungen, diese Einschränkungen wieder zu lockern. Jedoch griff 1948 die Regierung der burischen Nasionalen Party den Stand von 1937 auf und machte ihn zur Grundlage weiterer Restriktionen in ihrer Apartheidpolitik.
Für die administrative Umsetzung der erdachten Kontrollsysteme errichtete man eine staatliche Verwaltungsstruktur, den South African Native Trust (SANT). Mit diesem Instrument wurde in den ländlichen Arealen eine restriktive Umverteilungspolitik von Landvermögen unter Nutzung verfügbarer staatlicher Planungs- und Siedlungspolitik begonnen. In deren Folge setzte eine Vertreibung nicht registrierter Landwirte ein, sofern sie nicht offiziell auf den weißen Farmen als Arbeiter angemeldet waren. Die Enteignung eines erheblichen Teiles der dort lebenden einheimischen Bevölkerung war die beabsichtigte ökonomische Wirkung dieser parlamentarischen Reformbestrebungen. Man bezeichnete das als „Besserungsplanung“ (Betterment planning) und setzte diese Vorgaben in den späten 1930er und 1940er Jahren strikt um. Das führte zu einer Ausweitung der Befugnisse von den damit befassten Regierungsbeamten, den Native Commissioners und Agricultural Officers.[18][19][20]
Noch im selben Jahr ihres Sieges bei den Parlamentswahlen 1948 – bei der die Mehrheit der Bevölkerung sie nicht gewählt hatte, die sie aber aufgrund des Wahlsystems gewann – begann die Nationale Partei (Nasionale Party) unter dem neuen Premierminister Daniel François Malan Gesetze zu verabschieden, die die Segregation verschiedener Bevölkerungsgruppen schärfer definieren und weiter durchsetzen sollten. Mit der Verabschiedung dieser Gesetze wurde die Rassendiskriminierung in Südafrika, die Apartheid, auf systematische Art und Weise institutionalisiert und gesetzlich festgeschrieben.
Ideologische Voraussetzung dieser Gesetzgebung war die Einteilung der Bevölkerung nach Zugehörigkeit zu einer „Rasse“, wobei zunächst die Hautfarbe bis in die 1950er Jahre als Maßstab galt. Ziel war die Errichtung von unabhängigen, sogenannten Homelands und die Einrichtung melderechtlicher Nationalitäteneinheiten (national unit). Es wurden zunächst 8 solcher „national unit“ eingerichtet, die später um zwei weitere ergänzt wurden.[21][22]
Die Gesetze (englisch: Acts) zur systematischen Umsetzung des Apartheidskonzeptes wurden nach der Wahl 1948 und der anschließenden Erklärung der „Grand Apartheid“ in Kraft gesetzt. Wichtige Beispiele von Rechtsvorschriften zur Durchsetzung der Apartheid waren folgende:[23]
Die Auswirkungen der Apartheidpolitik werden von manchen Forschern in zwei Aspekte eingeteilt: die kleine Apartheid, auch Petty Apartheid genannt, und die große Apartheid oder Grand Apartheid. Mit große Apartheid ist die räumliche Trennung im großen Maßstab gemeint, die eigentliche Segregations- oder Homeland-Politik. Andere wissenschaftliche Darstellungen greifen diese Zweiteilung nicht auf, da das System systematischer Benachteiligungen miteinander sehr komplex verknüpft war.
Im Alltag der Nicht-Weißen waren die Formen der kleinen Apartheid unmittelbar spürbar. Sie beinhaltete die rassistisch motivierte Trennung im Dienstleistungsbereich und im öffentlichen Raum, wie auch etwa das Verbot des Betretens von öffentlichen Parkanlagen oder Badestränden und Schwimmbädern für Schwarze, separate Abteile in öffentlichen Verkehrsmitteln oder eigene Schulen. Unmissverständliche Regelungen und Verbote zur Trennung im öffentlichen Raum wurden durch Schilder erreicht. Unternehmen mussten getrennte Toiletten und Kantinen errichten. Einige Einrichtungen waren nur für Weiße zugänglich, wie hochklassige Hotels. Der Einzelhandel wickelte seinen Kundenverkehr entweder über zwei Türensysteme ab oder nahm Bestellungen von Nicht-Weißen an der Hintertür an und lieferte sie dort ebenso aus.[24][25]
Krankenhäuser, Postgebäude, Rathäuser, Banken und Toiletten hatten meist zwei, durch Schilder gekennzeichnete Eingänge. Andere Lebensbereiche waren weniger klar definiert. Durch Mundpropaganda wurden Restaurants und Bars unter Nicht-Weißen genannt, in denen man nicht bedient wurde bzw. nicht erwünscht war. Manche Nicht-Weiße testeten die Grenzen der Akzeptanz durch die Weißen. Andere scheuten sich, ihren sicheren Bereich zu verlassen. Dadurch lebten sie ruhiger und setzten sich der Diskriminierung in geringerem Umfang aus.
Manche dieser Trennungsmaßnahmen besaßen eine unmittelbare Wirkung, erzeugten aber weniger langfristige Auswirkungen für die von der Segregationspolitik betroffenen Bevölkerungsgruppen.
Viele Segregationsmaßnahmen im öffentlichen Bereich wurde auf Veranlassung von Staatspräsident Frederik Willem de Klerk in den Jahren 1989 und 1990 aufgehoben, beispielsweise:[26]
In einigen Städten, in denen die Konserwatiewe Party die stärkste kommunalpolitische Kraft bildete, wurde die Wiederherstellung von Segregationsverhältnissen in öffentlichen Einrichtungen versucht. Daraufhin kam es in Boksburg und Carletonville zu Protesten unter der schwarzen und der Coloured-Bevölkerung, die sich in Form eines Konsumentenboykotts gegen ansässige Unternehmen abspielten. Die Auswirkungen waren sehr wirkungsvoll, da es in diesem Zuge zu Geschäftsschließungen und bei anderen Unternehmen zu temporären Umsatzverlusten bis 80 Prozent kam. Die Boykotts endeten im November 1989, nachdem die De-Klerk-Regierung die Aufhebung des Gesetzes Reservation of Separate Amenities Act (deutsch etwa: „Gesetz zur Bereitstellung von getrennten Einrichtungen“) aus dem Jahre 1953 für 1990 ankündigte. Die ökonomischen Auswirkungen beschäftigen auch Gerichtsinstanzen. Richter C. F. Eloff von der Transvaal Provincial Division des Supreme Court in Pretoria bescheinigte dem Stadtrat von Carletonville durch einen Urteilsspruch im September 1989 rechtsmissbräuchliches Verhalten. Auch die restaurativen Verhältnisse in Boksburg kamen vor das Supreme Court, wo Richter S. W. McCreath die Entscheidung des Stadtrats dieser Gemeinde als „grossly unreasonable“ (deutsch etwa: „grob unangemessen“) bezeichnete und in seiner Begründung darauf verwies, dass eine Lokalverwaltung ihre Machtausübung im Interesse des Gemeindegebietes in Gänze auszuüben hätte und sie keine unangemessenen Entscheidungen in „treuwidriger Absicht“ zu treffen habe.[26]
Das damalige südafrikanische Dreikammerparlament beschloss im Juli 1990 mit überwältigender Mehrheit den Discriminatory Legislation Regarding Public Amenities Act, mit dem das Separationsgesetz von 1953 gänzlich aufgehoben wurde. Gegenstimmen kamen nur aus der Fraktion der Konserwatiewe Party, die darin eine „Zerstörung ‚weißen‘ Rechts auf Selbstbestimmung“ (destroy whites’ right to self-determination) sahen. Der Minister of Planning and Provicial Service Hernus Kriel konterte im Parlament, dass das alte Gesetz von seinem Grundsatz her diskriminierend sei und Südafrika sei es nun möglich, wieder in die internationale Gemeinschaft zurückzukehren. Für den Parlamentsabgeordneten Desmond Lockey von der Labour Party sei nun eine Stufe genommen, um in Richtung Wiederherstellung der Menschenwürde und Bürgerrechte für alle Südafrikaner weiterzugehen. Zach de Beer von der Democratic Party kommentierte: das neue Gesetz „leiste einen signifikanten Beitrag für die Gestaltung eines geeigneten Klimas zu Verhandlungen“.[26]
Der Ausschluss aller Nicht-Weißen, vorrangig jedoch der Schwarzen, vom aktiven und passiven Wahlrecht in den Landesteilen außerhalb der Reservate bzw. späteren Homelands wirkte bis in den kommunalen Bereich. Damit schufen die politischen Entscheidungsträger im parlamentarischen Vertretungssystem Südafrikas bewusst ein absolutes Defizit demokratischer Rechte für eine Bevölkerungsmehrheit. Mit der Verfassungsreform von 1984 unter Pieter Willem Botha sollte diese Lücke mit einem Dreikammersystem wieder relativiert werden, ohne der schwarzen Bevölkerungsmehrheit dabei die politische Willensbildung und Mitgestaltung in Südafrika einzuräumen. Damit konnten aus ihrem Kreis keine demokratisch legitimierten Korrekturen oder Entwicklungen in der südafrikanischen Gesellschaft angestoßen werden.
Der historische Verlauf des Stimmrechtsabbaus für die nichteuropäischstämmige Bevölkerung vollzog sich seit der Gründung der Südafrikanischen Union über mehrere Jahrzehnte und nach gruppenspezifischen (Coloured, Inder, Schwarze) Handlungsmustern. Eine wirkungsvolle Gestaltung gesellschaftlicher Fragen über die verfassungsgemäßen Strukturen der parlamentarischen und kommunalen Wahlkörperschaften war nur den europäischstämmigen Bürgern gewährt. Politische Mitgestaltung für Nichtweiße organisierte der Apartheidstaat ausschließlich aus der eigenen Herrschaftsperspektive. Raum boten dafür die Regierungen der Homelands oder weitgehend unwirksame Gremien, da sie nicht mit ausreichend Kompetenzen ausgestattet waren. Zu den letzteren gehörten vorbestimmte Institutionen mit Beratungscharakter, wie das Coloured Persons’ Representative Council und das South African Indian Council.[27]
Die Freizügigkeit war durch mehrere gesetzliche Regelungen eingeschränkt. Mit dem Natives Laws Amendment Act (Act No 54 / 1952) von 1952, einem Änderungsgesetz für den Native Labour Regulation Act von 1911 und den Natives Consolidation Act (Act No 25 / 1945) schränkte die Apartheidsregierung die bereits begrenzten Wohn- und Aufenthaltsrechte der schwarzen Bevölkerung weiter ein. Von besonderer Bedeutung sind dabei die Regelungen in section 10 (deutsch sinngemäß: Paragraph 10) dieses Gesetzes, die existenziell bedeutende Ausnahmen vom 72-Stunden-Aufenthaltsrecht außerhalb der zugewiesenen Wohngebiete in den Reservaten oder Homelands definierten. Kein Schwarzer durfte sich länger als 72 Stunden in den prescribed areas der Weißen aufhalten. Unter die Sektion-10-Rechte fielen Aufenthaltsgenehmigungen für schwarze Arbeitnehmer in den „weißen“ Regionen. Sie wurden für einen zugewiesenen Arbeitsplatz mit regionaler Beschreibung definiert und entfielen bei Verlust der Arbeit. In den stets mitzuführenden Passbüchern war diese Genehmigung und eine sich monatlich wiederholende Bestätigung des Arbeitgebers eingetragen. Bei Kontrollen konnte der legale Aufenthalt dadurch sofort festgestellt werden. Besonders Frauen und die Kinder von männlichen Wanderarbeitern waren von diesen Einschränkungen massiv betroffen, da es für sie keine familiären Zuzugsrechte gab. Der Minister für Bantu-Verwaltung, Hendrik Frensch Verwoerd, erklärte 1955 in Anlehnung an die Ergebnisse der Stallard-Kommission, dass die schwarzen Arbeitnehmer nur „auf Geheiß und durch die Gunst der Weißen“ und nicht durch gesetzlich garantierte eigene Rechte in den „weißen Gebieten“ nutzbringende Arbeiten erfüllten, weshalb sie „höchstens Besucher“ seien. Das wirtschaftspolitische Ziel dieser Regelungen bestand darin, alle schwarzen Beschäftigten in die Rolle von Kontrakt-Wanderarbeitern zu bringen und deren Sesshaftigkeit am Arbeitsort zu verhindern.[28][29]
Durch die Einordnung der Bevölkerung in „rassisch“ definierte Gruppen entstand eine Klassifizierung, die eine Unterscheidung im gesamten gesellschaftlichen Leben für jede Person von Anderen ermöglichte. Die „Rassenkategorie“ wurde in die Ausweisdokumente durch Buchstabencodes, zum Beispiel -C- für Coloureds, eingetragen. Die schwarze Bevölkerung erhielt ein besonderes Ausweisdokument, das reference book.[30] Der Population Registration Act (Act No 30 / 1950) teilte die Bevölkerung Südafrikas in drei Hauptgruppen ein:
Zur Umsetzung dieser Maßgaben schuf man ein „Amt für Rassenklassifizierung“ (Race Classification Board). Alle Südafrikaner wurden von dieser Behörde erfasst und waren zur Einsendung eines Passbildes verpflichtet. Auf dieser Grundlage entstand ein zentrales „Rassenregister“.
Seit 1951 war der Begriff Bantu als Terminus für die einheimische schwarze Bevölkerung bei der Regierung üblich und seit 1962 offizieller Begriff. Im Jahr 1978 führte man als offizielle Bezeichnung für Personen das Wort Black ein. Mit dem Jahr 1973 war in den Personaldokumenten der Schwarzen eine ethnische Untergruppe (National unit) vermerkt. Ein Ergänzungsgesetz von 1982, der Population Registration Amendment Act (Act No 101 / 1982), bewirkte eine Vereinheitlichung der Personalausweise für alle Bevölkerungsgruppen, die nun die Möglichkeit zur Aufnahme biometrischer Merkmale vorsahen. Alle Personendaten wurden in einem zentralen Computersystem des Staates gespeichert.[31]
Da das System bei Zuordnung der asiatischstämmigen Bevölkerung bereits an seine Grenzen stieß, wurde mit der Verfassung von 1983 eine vierte Kategorie der Asiaten oder Inder geschaffen, deren Rechte etwa denen der Coloureds entsprachen. Ihnen standen jetzt 5 Mitglieder im Präsidentenrat und 45 Sitze im House of Delegates zu, zuvor waren sie von der politischen Teilhabe ausgeschlossen. Kapmalaien galten aber weiter als Coloureds[32] sowie Japaner, Koreaner und Taiwan-Chinesen als weiß ehrenhalber.[33]
Die Wohngebiete der weißen Bevölkerung, auch Europeans genannt, lagen durchweg in den geographisch und strukturell vorteilhaftesten Arealen der Siedlungsgebiete. Wurden die festgelegten Bereiche für die Weißen zu eng, mussten andere Bevölkerungsgruppen Teile ihrer Wohngebiete räumen und in neu zugewiesene Bereiche umsiedeln. Ein bekanntes Beispiel war die Räumung des District Six im Zentrum von Kapstadt und die Zwangsumsiedlung von etwa 60.000 Menschen in das etwa 30 Kilometer entfernte, sandige Khayelitsha. Die schwarze Bevölkerung war in ihrem abgelegenen Wohngebiet so weit außerhalb der Gemeinden, oft hinter natürlichen oder künstlichen Hügeln sowie Müllkippen verbannt, dass sie nicht als Teil der Gemeinde angesehen werden konnte.
Mit der Konzipierung der Homelands versuchten die Apartheidsideologen eine hauptsächlich ökonomisch begründete Raumordnungspolitik umzusetzen.
Zwischen 1960 und 1980 mussten etwa 3,4 Millionen Menschen im Zuge der Homelandpolitik ihre bisherigen Wohnstätten in urbanen und ländlichen Regionen zwangsweise aufgeben. Darunter befanden sich etwa 2,8 Millionen Schwarze, 600.000 Coloureds und Inder sowie 16.000 Weiße. Auf diese Weise zerstörte man das traditionelle labour tenant system, was den Landarbeiterfamilien ein unbestrittenes Wohnrecht auf den „weißen“ Farmen garantierte, wenn sich ihr Familienoberhaupt dort für eine jährliche Mindestzeit (90 Tage in Transval, 180 Tage in Natal) zur bezahlten Arbeit verpflichtete. Den Rest des Jahres konnten sie anderen Beschäftigungen nachgehen.[34] Dies erfolgte nicht ohne Proteste, die zu unzähligen Verhaftungen führten. Auf die Zwangsumsiedlungen, besonders auf deren Ausmaß und die Leiden der Bevölkerung, machten die Bürgerrechtsorganisation Black Sash, der Südafrikanische Kirchenrat und das Surplus Peoples Project aufmerksam.[35]
Die Regierungen zerstörten ganze Siedlungen in den Townships, um so die Schwarzen zur Umsiedlung, welche beispielsweise auf dem Native Resettlement Act von 1952 basierte, zu zwingen. Lediglich vorübergehende Aufenthalte der genehmigten Arbeitskräfte in den Unterkünften der Townships waren gewollt und geduldet. Nach Auffassung der herrschenden Politik waren diese Personen nur Gäste in den „weißen“ Gebieten mit einer patriarchalisch gewährten Arbeitserlaubnis. Dahingehend orientierte die Bantu Administration 1967 in einer Direktive die Lokalbehörden darauf, dass keine „größere, bessere, attraktivere und luxuriöse Bedingung“ zu schaffen sei. Es müsse „bedacht werden, dass ein städtisches Bantu-Wohngebiet kein Heimatland, sondern Teil eines weißen Gebietes ist. Wenn diese Bedingungen zur Folge haben, den Bantu nicht nur an einen fremden Geschmack zu gewöhnen, sondern ihm auch einen Luxus aufzwingen, den sein Heimatland nicht bieten kann, und ihn so von dem entfremdet, was das Seinige ist“.[36]
Besonders die städtischen Ballungsräume waren von Zwangsumsiedlungen (urban relocation) betroffen. Nach im Jahre 1977 veröffentlichten Forschungsergebnissen gab es fünf Phasen der mit staatlichen Maßnahmen betriebenen Aussiedlung der schwarzen Bevölkerung aus den Städten. Der Natives (Urban Areas) Act von 1923 beförderte die Auflösung von Slums und sah die vollständige Aussiedlung der schwarzen Bevölkerung aus den „weißen“ Gebieten vor. In diesem Zusammenhang wurde dafür das „Prinzip der Unbeständigkeit“ proklamiert. Eine weitere Phase bildete das Compoundsystem im Bergwerkssektor und für die Zuckerrohrplantagen in der Provinz Natal. Die dritte Phase entwickelte sich nach dem Erlass des Group Areas Act von 1950, wodurch eine forcierte Umsiedlung und Slumauflösung im Umfeld der städtischen Ballungsräume betrieben wurde. Die nächste Phase ereignete sich in den 1960er Jahren. In diesem Zeitraum baute die Regierung kleine Siedlungshäuser in den Homelands. In den „weißen“ Gebieten sollte durch einen limitierten und separaten Hausbau für Schwarze die Anwesenheit einer schwarzen Arbeitsbevölkerung gezielt reguliert werden. Im Sprachgebrauch der Bantu Administration als „unproduktiv“ angesehene Personen, wie Ältere, Witwen u. a., waren ab 1967 in die vorgesehenen Schwarzengebiete zu deportieren. Im Verlauf der fünften und letzten Phase in den 1970er Jahren begann die Urbanisierung der Homelands unter der Kontrolle des South African Bantu Trust, die sich im Gleichschritt mit der Entwicklung von Industriestandorten in denselben Regionen vollzog.[37] Im Zuge der Umsiedlungen gab es in wenigen Fällen Entschädigungsleistungen. Spezielle Gesetze, wie der Slums Act, legitimierten solche Vorgehensweisen.
Die auch inhaltlich unterschiedlichen Schulsysteme, mit jeweils abgestufter Ausstattung und Qualifikation des Lehrkörpers, waren mitverantwortlich für ungleiche Zukunftschancen in Beruf, Kultur und sozialen Zusammenhängen. Das Gesetz Bantu Education Act von 1953 setzte die Rahmenbedingungen für eine einheitlich staatlich kontrollierte und geringwertige Schulbildung. Die für eine Hochschulausbildung erforderlichen Voraussetzungen erreichte nur eine ganz geringe Zahl nichtweißer Personen. Das Ziel der sogenannten „Bantubildung“ bestand in der systematisch geplanten und statisch verankerten Entwicklung einer großen, wenig gebildeten Bevölkerungsschicht, die als Niedriglohnkräfte der weißen privilegierten Minderheitsbevölkerung Südafrikas im Arbeitsmarkt nicht zur Konkurrenz erwachsen konnten. Die freien Schulen der zumeist kirchlichen Träger, einst die alternative Chance zu einer besseren Bildung für Schwarze und Farbige, wurden mit dem Bantu Education Act in dieser Eigenschaft liquidiert und einer staatlichen Aufsichtsverwaltung unterstellt.
Schon vor dem Ende der Apartheid formierten sich im Land Positionen und Aktivitäten zu einer bildungspolitischen und pädagogischen Alternative zum herrschenden und repressiv kontrollierten Staats-Bildungssystem. Die sich auf diesem Feld abzeichnenden Veränderungen gingen mit dem Erstarken der Black-Consciousness-Bewegung einher. Als 1977 der Pädagoge Es’kia Mphahlele aus dem Exil nach Südafrika zurückkehrte, befasste dieser sich mit dem Konzept der alternative education. Seine an der Witwatersrand-Universität aufgenommene Lehrtätigkeit ließ ihm dazu den erforderlichen Spielraum. Dabei bezog er sich beispielsweise auf Arbeiten von Paulo Freire. Im Jahre 1981 formulierte Mphahlele im Verlauf eines Interviews eine kritische Bestandsaufnahme des staatlichen Bildungssystems[38]:
„[Eine neue Erziehungstheorie] muß Wege finden, wie der Kolonialismus in den Köpfen abgebaut werden kann [...]. Dies wird zur Befreiung des Ichs führen, was wiederum eine Neuentdeckung des Ichs nach sich zieht. Alle kolonialisierten Völker der Welt haben zwei Ichs: das ursprüngliche (indigenous) Ich, dem die westliche Kultur übergestülpt wurde. Diese Kultur ist aggressiv, sie kommt daher mit Technologie, Ökonomie, christlicher Erziehung. Wenn sie mit uns fertig ist, stellen wir fest, daß sie uns gespalten hat in eine gebildete Elite und in die Massen, in das ursprüngliche Ich und die neue Empfänglichkeit des Individuums, das sich von der kollektiven Empfindung losgelöst hat und den Individualismus glorifiziert. Das ist die ihrem Erbe entfremdete Persönlichkeit.“
Damit wurde der neue Ansatz einer Befreiungs-Pädagogik in den politischen Diskurs um die „getrennte Entwicklung“ innerhalb Südafrikas Bildungssystem eingebracht, die dabei als zentrales Ziel den Abbau des „Kolonialismus in den Köpfen“ verfolgte.
Auf einem Kongress des National Education Crisis Committee (NECC) in Durban am 29. März 1986 verbreitete sich die Sichtweise von Mphahlele weiter. Zwelakhe Sisulu erklärte: „Wir fordern nicht mehr die gleiche Erziehung, wie sie die Weißen haben; denn das ist Erziehung zur Herrschaft. 'People’s education' dient dem Volke als ganzen, ist Erziehung, die befreit, ist Erziehung, die das Volk in die Lage versetzt, sein Leben selbst in die Hände zu nehmen. [...] Wir sind nicht willens, irgendeine Alternative zur 'Bantu Education' zu akzeptieren, die dem Volke von oben auferlegt wird. [...] Alternativen, [...] die sicher stellen sollen, daß die Ausbeutung durch ausländische Monopole weitergeht.“[39]
Das Bildungssystem für die schwarze Bevölkerung (für Coloureds und Inder gab es gesonderte Regelungen) sah keine einheitliche Pädagogenausbildung vor. Im Jahr 1985 beschäftigte das staatliche (Bantu-)Schulsystem 45.059 Lehrer, von ihnen waren 42.000 unterqualifiziert. Nur 3,6 Prozent verfügten über einen fachbezogenen Universitätsabschluss und 70 Prozent hatten nicht einmal einen eigenen Schulabschluss auf Standard 10 oder höher (Gymnasium umfasst Grade 8 bis Grade 12). Die Quote für unterqualifizierte Lehrer an Schulen für weiße Schüler lag überwiegend im einstelligen Prozentbereich.[40]
Der Spro-cas-Bericht von 1971 fasste die politisch in Kauf genommenen Schwächen des staatlichen Bildungssystems für die schwarze Bevölkerung am Beispiel des Homelands Bophuthatswana mittels markanter Punkte zusammen:[41]
Mit dem Extension of University Education Act (Act No 45 of 1959) wurde die Trennung der Hochschulbildung für „Weiße“ und „Nichtweiße“ herbeigeführt. Das Gesetz sah die Errichtung von university colleges für „non-white persons“ vor. Die Finanzierung der für die Bantubevölkerung vorgesehenen Einrichtungen kam demnach aus dem Bantu Education Account und für die Coloured- und indischstämmige Bevölkerung aus dem General Revenue Account. Weiterhin war vorgeschrieben, dass jedes university college ein (White) Council und ein (Black) Advisory Council zu wählen hatte, gleiches galt für den Senat der Hochschuleinrichtung.[43]
Die starken Einschränkungen eines freien Hochschulzuganges für Schwarze führten im Rahmen eines Sonderweges schließlich zu einer mit internationalen Hilfsmitteln und Lehrkräften seit 1978 arbeitenden Bildungseinrichtung des ANC in Tansania, die Studiengänge mit international anerkannten Abschlüssen anbot.Im Jahre 1983 begann die Vista University in verschiedenen südafrikanischen Städten ihre akademische Ausbildungstätigkeit für Schwarze, jedoch als eine Einrichtung der rassenpolitisch konzipierten Bildungspolitik im Apartheidsstaat. Für die indischstämmige Bevölkerungsgruppe gab es in Durban seit 1962 das University College for Indians und später die daraus entstandene Universität von Durban-Westville. Das Hochschulstudium für Coloureds war seit 1959 am University College of the Western Cape möglich.
Die Apartheidpolitik war hauptsächlich ein Mittel zur Sicherung wirtschaftlichen Interessen der weißen Bevölkerungsminderheit. Gesetzliche Einschränkungen und im Lande verteilte Arbeitsagenturen erzielten eine wirkungsvolle Lenkungswirkung, die den Interessen der Industrie diente. Die weitgehend ohne grundhafte Berufsausbildung mit Zertifikat abgeschlossene versehene schwarze Bevölkerung war in ein komplexes System der Wanderarbeit eingebunden, das ihnen ein Leben auf nur geringsten Standards ermöglichte, beispielsweise in den Compoundsiedlungen der Bergbauunternehmen. Gesetzlich ausgeschlossene Streik- und Tarifverhandlungsrechte machten sie zu einer beliebig verfügbaren und im Sinne der Arbeitgeber effizient einsetzbaren Masse von Billiglohnempfängern. Die Bildung von Gewerkschaften war zwar nicht verboten, aber in der Praxis unterlagen solche Aktivitäten starken Repressionen. Im Jahr 1972 wandte sich der South African Congress of Trade Unions (SACTU) mit einem umfassenden Themenkatalog an die internationale Gewerkschaftsbewegung, ihn bei seinen Bemühungen um Herstellung grundlegender Arbeitnehmerrechte zu unterstützen. Aktive Mitglieder des SACTU erlitten Verfolgung mit allen Repressionsmitteln des Apartheidsstaates.[44] Auf der Grundlage des Industrial Conciliation Amendment Act (Act No 94 / 1979) ließ die Apartheidregierung 1979 erstmals Lehrlingsausbildungsgänge für Schwarze zu. Zudem erhielten nun schwarze Arbeiter den Status von Angestellten, was ihnen zugleich Arbeitnehmerrechte verlieh. Ausgenommen davon waren Wanderarbeiter und ausländische Arbeitsmigranten, die vorrangig aus Mosambik kamen.[45]
Um die Ziele der Apartheid umsetzen zu können, war ein riesiger Verwaltungsapparat notwendig. Dieser ging aus der Native Administration der ehemaligen Staatsverwaltung nach britischem Muster hervor und erlangte als Bantu Administration zeitweilig einen großen Einfluss. Diese Eingeborenenverwaltung bildete eine weitgehend autarke Parallelstruktur zu allen anderen öffentlichen Verwaltungen.
Das Justizsystem von Südafrika wurde in der Apartheidsperiode mit Handlungsmöglichkeiten versehen, die rechtsstaatlich fragwürdig sind. Beispielsweise ermöglichte eine sogenannte Sobukwe-Klausel aus dem Jahre 1963 die Haftfortsetzung auf alleinige ministerielle Anordnung hin, ohne eine erneute richterliche Entscheidung einholen zu müssen. Im Jahr 1976 reaktivierte man dieses Instrument mit verschärften Möglichkeiten, wodurch auf der Grundlage des Internal Security Amendment Act (Act No 79 / 1976) die zeitlich unbegrenzte Ingewahrsamnahme (preventive detention) ohne Richterentscheidung nun nicht nur bei Häftlingen, sondern auch bei jeder anderen Person möglich wurde, falls sie nach subjektiver Sicht des Justizministers eine „Gefahr“ für die Sicherheit und öffentliche Ordnung darstellte. Die Unterrichtung der Betroffenen über die Gründe ihrer Vorbeugehaft war hierbei nicht zwingend vorgeschrieben. Ein mit dem Gesetz geschaffenes Review-Committee konnte Empfehlungen auf Entlassung aus dieser Internierung aussprechen, die es aber nur in wenigen Fällen formulierte. Zur Anwendung der präventiven Ingewahrsamnahme kam es im Juli 1976 in Transvaal und im August im gesamten Staat, so dass im Oktober desselben Jahres bereits 123 Apartheidkritiker präventiv in Gefängnissen interniert waren. Einige setzte man später unter die Bannungsverfügung und andere verurteilte man auf der Basis des Terrorism Act (General Laws Amendment Act, Act No 83 / 1967) und weiterer Sicherheitsgesetze zu Haftstrafen.[46]
Die Security Branch genannte Sonderpolizei war Teil der South African Police; ihre einzelnen Dienststellen wurden bedarfsweise bis in die zivilen Gemeindestrukturen aufgegliedert. Zur Ausweitung der repressiven Sicherungsmaßnahmen der Apartheidsdoktrin in der südafrikanischen Innen- und Außenpolitik entwickelte sich unter dem 1972 geschaffenen State Security Council (deutsch etwa: Staatssicherheitsrat) ein sich immer weiter verzweigendes System von Substrukturen, die im National Security Management System (NSMS) zusammengefasst waren. Neben der geheimdienstlich organisierten Beobachtung von Antiapartheidsaktivitäten in zivilen und paramilitärischen Zusammenhängen sowie der Sammlung von Informationen über ihre Netzwerke ergriffen die damit verbundenen Dienststellen und Einsatzgruppen viele operative Maßnahmen, teilweise mit dem Ziel einer Strategie der Spannung. Als spektakuläre Fälle können beispielsweise Mordanschläge im Ausland auf prominente Aktivisten der Antiapartheidsbewegung gelten, wie in den Fällen von Albie Sachs oder Ruth First sowie die systematische Bedrohung von Familienangehörigen und Personen aus dem Umfeld der Zielpersonen. Die dafür häufig genutzte Organisationsstruktur war die Sondereinheit C1, die nach ihrem Sitz als Vlakplaas bekannt wurde und unter der Führung des Offiziers Eugene de Kock stand.[47][48] Das Civil Cooperation Bureau war seitens des Militärs mit verdeckten Destabilisierungsaktionen befasst. Dabei induzierten geheime „Sicherheitskräfte“ Konflikte zwischen organisierten Bevölkerungsgruppen. Personelle und operative Kompetenz konnte dabei auch aus der Eingliederung ehemaliger Rhodesier aus den Selous Scouts in südafrikanische Strukturen gewonnen werden.[49]Eine permanent angespannte Lage unter der schwarzen Bevölkerung in den Ballungszentren kam durch undifferenzierte Großaktionen der Polizei zustande, die mit taktischen „Bürgerkriegsübungen“, vorzugsweise in der Nacht und unter Einsatz von hunderten bis über tausend Polizisten, ganze Stadtviertel abriegelten und rasterartig Hausdurchsuchungen praktizierten.[50][51]
Infolge der zunehmenden Militarisierung der gesamten Gesellschaft Südafrikas und den zunehmenden Kriegsaktivitäten im benachbarten Ausland gründete sich nach jahrelangen informellen Aktivitäten kleinerer Gruppen 1984 eine offizielle Vereinigung zur Abschaffung der Wehrpflicht. Diese End Conscription Campaign fasste das Apartheidregime im Widerspruch zu seiner total strategy der 1980er Jahre als eine feindliche Organisation auf und bannte sie im August 1988.
Zur Ausdehnung des rechtsfreien Raumes innerhalb der Apartheidpolitik nahm man mehrere einschränkende Eingriffe in die Pressefreiheit vor. Das 1959 erlassene Gefängnis-Gesetz (Prison Act, Act No 8 / 1959) und das Änderungs-Polizeigesetz (Police Amendment Act, Act No 64 / 1979) von 1979 untersagten eine unabhängige Berichterstattung, sofern sie nicht von den betroffenen Behörden selbst bestätigt wurde. Die Steyn-Kommission erarbeitete Vorschläge zur „Neuordnung“ des Mediensektors und leistete damit einen fundamentalen Beitrag zur Einschränkung der Pressefreiheit. Auf diesem Wege war nun eine unzensierte öffentliche Wahrnehmung des polizeilichen Handelns schrittweise erschwert, letztendlich unmöglich geworden. Mit dem Zweiten Änderungsgesetz zum Polizeigesetz (Second Police Amendment Act) im Jahr 1980 wurde sogar jegliche Berichterstattung über die als „terroristisch“ eingestuften Handlungen verboten. Darunter fielen auch die Namen der Inhaftierten. Vorgänge von Misshandlungen, Folter oder Mord konnten nun kaum noch von der Presse aufgegriffen werden und der ungeklärte Verbleib zahlreicher Personen nahm zu. Zugleich konnte niemand mehr den Umfang widerrechtlicher Ingewahrsamnahmen durch die Behörden abschätzen. John Dugard kritisierte bereits 1980 als Professor an der Witwatersrand-Universität diese Rechtspraxis, in dem er auf die dadurch geschaffenen Verhältnisse verwies, die beispielsweise eine Aufklärung der Todesumstände von Steve Biko unmöglich machen könnten. Der damalige Anwalt am Supreme Court of South Africa, Albie Sachs, war selbst über fünf Monate das Opfer eines dieser repressiven Gesetze, wonach ein Inhaftierter bis zu einer Dauer von 90 Tagen (definiert in section 17 des General Laws Amendment Act, Act No 37 / 1963) ohne richterliche Entscheidung im Gewahrsam der Sicherheitspolizei und dabei deren unkontrollierten Folterungen ausgesetzt sein konnte.[52][53] Über die Misshandlungen und Folterungen von Gefangenen in Südafrika informierte ein UN-Bericht aus dem Jahre 1973.[54]
In Südafrika gab es seit 1931 eine öffentliche Dienststelle, die zur Kontrolle von frei zugänglichen Unterhaltungs- und Vergnügungseinrichtungen geschaffen wurde. Diese erhielt mit dem Entertainments (Censorship) Act (Act No. 28/1931) ihre gesetzliche Grundlage. Zudem kontrollierten Zollbehörden den Import unerwünschter Druckerzeugnisse. In den 1960er Jahren begann sich der staatliche Umgang mit Medienerzeugnissen entscheidend zu wandeln. Im Jahre 1971 wird ein Änderungsgesetz beschlossen, was nun der inzwischen zur umfänglichen Behörde angewachsenen Zensurinstitution das Recht zu Hausdurchsuchungen einräumte. Wesentliche Änderungen ergeben sich 1974 mit dem Publications Act (Act No. 42/1974), der nicht nur die bisherigen Vorschriften aufgriff, sondern nun den Weg in eine lückenlose Zensur des öffentlichen und privaten Lebens eröffnete. In der Präambel dieses Gesetzes wird erklärt, dass „Bei der Anwendung des Gesetzes [...] das ständige Bemühen der Bevölkerung der Republik Südafrika anerkannt werden [soll], eine christliche Lebenssicht aufrechtzuerhalten.“ (englisch: „In the application of this Act the constant endeavour of the population of the Republic of South Africa to uphold a Christian view of life shall be recognized.“) Mit dieser Gesetzesnovelle war auch der Neuaufbau der Zensurbehörde verbunden. Dem ging eine aufwendige Vorbereitung voraus, die von einer parlamentarischen Arbeitsgruppe unter Leitung des Vizeministers des Inneren, J. T. Kruger, geleitet wurde und aus 8 weiteren NP-Mitgliedern und 4 UP-Mitgliedern bestand. Das Ergebnis wurde als Regierungspaper mit der Nummer R.P. 17/1974 veröffentlicht und enthielt u. a. einen Gesetzesentwurf, der im August 1974 mit kleinen Änderungen beschlossen wurde. Die neue Behörde stand unter der Leitung des Directorate of Publications mit ihrem Direktor, seinem Stellvertreter und weiteren drei Assistenzdirektoren. Zur Erfüllung der Zensuraufgaben gab es das „Committee“, deren Mitglieder vom Innenminister ernannt und deren Namen zunächst nicht bekanntgegeben wurden. Diese Strukturen erstreckten sich bis auf alle regionale Ebenen des Landes.[55][56][57] Im Mai 1976 gab im Zuge einer parlamentarischen Anfrage der Innenminister doch die Namen der Mitglieder im Directorate of Publications bekannt. An der Spitze des Gremiums standen J. L. Pretorius (director) und dessen Stellvertreter Professor R. E. Lighton sowie die als assistent director berufenen Beisitzer: J. T. Kruger, S. F. du Toit und M. J. van der Westhuizen.[58]
Im Jahre 1976 errichtete die Behörde ein Sonderkomitee zur Untersuchung von Bibliotheken an den Universitäten auf vermutete subversive Literatur. Unerwünschte Literatur durfte zu wissenschaftlichen Zwecken im Bestand verbleiben und von Lehrkräften unter definierten Bedingungen genutzt werden. Ferner gab es Literatur, deren Besitz verboten war, insbesondere als kommunistisch eingestufte Druckwerke durften nur mit Sondergenehmigung eingesehen und nicht ausgeliehen werden. Das Directorate of Publications war im Wesentlichen der Initiator für zensorische Ermittlungen; jedoch auch Bürgern war es möglich, die Behörde gebührenpflichtig zu einer Untersuchung aufzufordern, was geeignet war, der willkürlichen Denunziation Vorschub zu leisten. Die Zensur beschränkte sich nicht nur auf die Einschränkung der Verbreitung unerwünschter Medienwerke, sondern auch darauf, ihren Besitz selbst zu verbieten. Die umfassende Arbeit der Zensurbehörde spiegelte sich direkt in der Presse wider, weil hier die aktuellen Listungen wöchentlich veröffentlicht wurden. Im Jahresdurchschnitt ergaben sich 2000 Untersuchungsfälle, von denen etwa die Hälfte von einem Verbot betroffen waren.[57]
Das Gebaren der Zensurbehörde setzte parallel zu ihrem Wirken einen Prozess der Selbstzensur unter den Verlagen in Gang. Viele weiße Journalisten, Verleger und Autoren passten sich schnell der strengeren Lage an. Eine zentrale Rolle spielte dabei der Zusammenschluss der Zeitungsverleger, die National Press Union (NPU). Deren Pressekodex war eine Unterwerfung unter die der Regierung genehmen Berichterstattungsziele. Die so erzeugten Denk- und Schreibbarrieren bewirkten die freiwillige Aufrechterhaltung des Mythos von einer freien und nicht unter Kontrolle stehenden Presse in Südafrika und SWA/Namibia. Die ersten Versuche zur gesteuerten Selbstzensur gehen auf einen Gesetzesentwurf im Jahre 1960 zurück, den die Regierung nach vehementer Kritik aus der Medienlandschaft zurückzog und 1963 in abgeschwächter Form zum Beschluss bringen ließ. Die Regierung übte zuvor Druck auf die Verleger aus, um über die Newspaper Press Union einen genehmen Verhaltenskodex der Presse zu erzwingen. Das gelang ihr und im Kodex war nun neben anderen Bestimmungen folgende Passage untergebracht: „In Zeitungskommentaren sollen die komplexen Rassenprobleme Südafrikas in geeigneter Weise gewürdigt und ebenso das allgemeine Wohl und die Sicherheit des Landes und seiner Menschen in Betracht gezogen werden.“ Solche Eingriffe in die journalistische Arbeit erzeugte auch unter der „weißen“ Presse Südafrikas wachsenden Widerspruch. Vom Herausgeber der Sunday Times ist die Position überliefert, dass bei Befolgung solcher Richtlinien die Informationspflicht der Presse über Hauptthemen der gemeinsamen Zukunft des Landes nicht mehr nachgekommen werden kann.[57]
Das 1929 gegründete South African Institute of Race Relations untersucht und dokumentiert die Entwicklung des südafrikanischen Rassismus und der institutionellen Apartheid mit vielen Einzelpublikationen und Periodika. An der Arbeit des Instituts beteiligten sich zahlreiche Apartheidskritiker.[59]
Mehrere Kommissionen erarbeiteten im Auftrag der südafrikanischen Regierungen in den Jahren der Apartheidsperiode Empfehlungen und Konzepte, die zu konkreten Ausgestaltung der Kabinettspolitik genutzt wurden. Dazu zählten die Tomlinson-Kommission, die Native Laws Commission und weitere Gremien.
Die Gegenbewegungen an der Basis der Bevölkerung zum politischen motivierten Rassismus und den Apartheidsverhältnissen in Südafrika entstanden nicht erst mit der Machtübernahme der Nationalen Partei im Jahre 1948. Sie waren zu diesem Zeitpunkt bereits in vielfacher Ausprägung existent, weil die seit Jahrzehnten praktizierte staatliche Ausgrenzung der schwarzen, indischstämmigen und farbigen Bevölkerungsgruppe spürbare nachteilige Wirkungen auf diese ausübte.
Im Wesentlichen hatten die gesellschaftskritischen Positionen im politischen Emanzipationsprozess des ausgehenden 19. Jahrhunderts ihre Ursprünge an verschiedenen Missionsschulen, besonders im Wirkungsbereich der Anglikanischen Kirche. Diese Entwicklung leitet sich aus den aufklärerischen Impulsen hier tätiger Theologen und Missionare ab, wie James Stewart und Jane Elizabeth Waterston, sowie in dem daraus erwachsenen politischen Selbstverständnis führender schwarzer und indischstämmiger Persönlichkeiten. Internationale Einflüsse und Vorbilder wirkten als verstärkende Faktoren auf die Emanzipationsentwicklung innerhalb der schwarzen Bevölkerung, zu denen das US-amerikanische Tuskegee Institute zählte. Diese Einrichtung übte auf die Missionare in der damaligen Kapkolonie bei der Weiterentwicklung der Bildungskonzepte für die „nichtweißen“ Bevölkerungsgruppen eine Vorbildwirkung aus.
In den ausgehenden 1920er und den 1930er Jahren formierte sich durch die Wahrnehmung wachsender sozialer Differenzierungsprozesse innerhalb der südafrikanischen Gesellschaft unter manchen Theologen und Sozialwissenschaftlern die Bereitschaft zur kritischen Systemanalyse. Die Gründung des South African Institute of Race Relations im Jahre 1929 war ein Resultat dieser sich wandelnden Lage. Im zweiten Drittel des 20. Jahrhunderts etablierten sich in der schwarzen und indischstämmigen Bevölkerung selbstorganisierte Proteststrukturen. Das wird an der Gründung neuer politischer Organisationen, vermehrten Forderungen nach Angleichung der Bürgerrechte an die Standards der europäischstämmigen Oberschicht und in der wachsenden Bedeutung eigener Zeitungen erkennbar. Der ehemalige ANC-Präsident Zaccheus Richard Mahabane wandte sich in den 1930er Jahren gegen die zunehmende Gesetzgebung der Rassentrennung und setzte sich dazu für den gemeinsamen politischen Weg verschiedener Oppositionsgruppierungen ein. Die südafrikanische Regierung verschärfte in den 1930er und 1940er Jahren ihre rassistische Repressionspolitik. 1938 gründete sich in Johannesburg die Non-European United Front, zu deren führenden Mitgliedern Yusuf Dadoo gehörte. Er organisierte Massenproteste gegen die zunehmende Ausgrenzung „nichtweißer“ Bevölkerungsteile.
In der Folge dieser wachsenden innenpolitischen Spannung kam es 1949 zu einem folgenreichen Wechsel an der Spitze des ANC. Junge Mitglieder erzwangen den Rücktritt des Vorsitzenden Alfred Bitini Xuma zugunsten von James Moroka und beeinflussten damit die politische Wirkung ihrer Organisation. Trotzdem galt immer noch das Primat des gewaltfreien Widerstandes, das sich noch einmal mit dem nächsten Vorsitzenden Albert Luthuli manifestierte.
Inzwischen hatte sich in Natal der Einfluss des sich an Gandhis Prinzipien orientierende South African Indian Congress (SAIC) ausbauen können und war zu einer mächtigen Kraft in Südafrika angewachsen. Die Regierung von Jan Christiaan Smuts wollte das Wahl- und Grundstücksrecht für die Inder einschränkend regeln und erregte daraufhin heftigen Widerspruch. Eine Delegation des SAIC reiste deshalb zur indischen Regierung und erreichte dort Sanktionen gegen Südafrika. Zwischen 1946 und 1948 machte die Indian Passive Resistance Campaign auf die ungerechten Lebensverhältnisse der indischstämmigen Bevölkerung aufmerksam.
Die Defiance Campaign zwischen 1952 und 1953 war eine von ANC, SAIC und Coloureds gemeinsam angelegte Aktion zur Einforderung von Bürgerrechten und rechtlicher Gleichbehandlung. Es folgte 1956 der international beachtete Protestmarsch von 20.000 Frauen auf die Regierungszentrale in Pretoria wegen der unbeliebten Pass-Gesetze und der sich aus weiterer Zuspitzung (Anti-Pass Campaigns) entwickelnde Protest im Jahre 1960 nach Vorbild von Mahatma Gandhi in Sharpeville, der durch bewaffneten Eingriff von Polizeikräften jedoch als Massaker von Sharpeville in die südafrikanische Geschichte einging.
Die Politik des gewaltfreien Widerstandes wurde während der gesamten Apartheidsperiode von den Betroffenen nicht aufgegeben, konnte jedoch im Inland nur noch sehr eingeschränkt ausgeübt werden und verlagerte sich auf Aktionen im Rahmen der internationalen Öffentlichkeit.[60][61][62][63]
Bereits 1912, zwei Jahre nach der Errichtung der Südafrikanischen Union, gründeten der Anwalt Pixley Seme, die Geistlichen John L. Dube, Walter B. Rubusana sowie der Autor Sol Plaatje den Afrikanischen Nationalkongress (ANC). Obwohl von Männern aus der elitären Gesellschaft gegründet, verstand sich der ANC durchaus nicht als elitäre Organisation. Er stand grundsätzlich allen offen, egal welcher Hautfarbe, und akzeptierte sowohl das Christentum wie auch die englische Sprache. Der ANC verstand sich als schwarze Widerstandspartei, die volle Bürgerrechte forderte. Lange Zeit opponierte er friedfertig durch Boykotte und Streiks. So organisierte er in den 1920er Jahren Streiks der Minenarbeiter, um die schlechten Arbeitsbedingungen der Schwarzen zu verbessern.
Der ANC wurde immer mehr zur Massenorganisation. Hunderttausende befolgten die Aufrufe zu Demonstrationen oder Streiks. Beispielsweise im Jahre 1946, zwei Jahre vor dem Beginn der Apartheid, streikten rund 70.000 schwarze Minenarbeiter. Insbesondere gegen Passgesetze, wonach die städtischen Schwarzen jederzeit ein persönliches Dokument mit sich tragen mussten, um sich als Arbeitnehmer ausweisen zu können, protestierte der ANC durch Demonstrationen und durch das Verbrennen der umstrittenen Personaldokumente. Trotzdem standen keineswegs alle Nicht-Weißen, nicht einmal alle Schwarzen, hinter dem ANC. Etliche Schwarze sahen die Homeland-Politik der Regierung als Chance, den Rassismus endlich zu beenden und ihre Traditionen wieder zu leben.
In späteren Jahren sollten diese Meinungsverschiedenheiten insbesondere zwischen städtischen und ländlichen Schwarzen zu bewaffneten Auseinandersetzungen führen. So forderten Unruhen bei Pietermaritzburg zwischen 1987 und 1990 rund 4000 Todesopfer. Bei diesem Konflikt handelte es sich um Streitigkeiten innerhalb der Zulu. Städtische Zulu vertraten andere Ansichten als die in der Inkatha Freedom Party vereinten ländlichen Zulu. In den frühen 1990er Jahren, also bereits nach dem offiziellen Ende der Apartheid, wendeten sich die Inkatha-Anhänger dann im Besonderen gegen die Xhosa. Menschen beider Seiten verloren dabei ihr Leben.
Die Regierung versuchte, die Menschenrechtsaktivisten des ANC und anderer Gruppen immer wieder an ihrer Arbeit zu hindern, indem sie diese bannten. Gebannte waren eingeschränkt in ihrer Bewegungsfreiheit, sie durften ein genau definiertes Territorium nicht verlassen. Des Weiteren löste die Regierung häufig Treffen des ANC auf. Das geschah auf der Grundlage mehrerer Gesetze, im Zentrum dieser Jurisdiktion der Suppression of Communism Act von 1950.
Einigen Mitgliedern gingen die meist friedlichen Aktionen des ANC nicht weit genug. Sie gründeten 1959 eine weitere Widerstandsorganisation, den Pan Africanist Congress (PAC). Im Gegensatz zum ANC verwarf der PAC die offene Haltung gegenüber allen Rassen. Er positionierte sich als reine Schwarzen-Organisation und lehnte jegliche Zusammenarbeit mit den Weißen ab. Auf einer vom PAC organisierten Demonstration im Township Sharpeville 1960 gab ein Polizeioffizier seinen Polizisten den Befehl, mit Maschinenpistolen in die unbewaffnete Menge zu schießen. 69 Afrikaner starben, Hunderte wurden verletzt.
Dieses Ereignis löste nationale Unruhen aus, welche die südafrikanische Regierung mit eiserner Faust bekämpfte. Rund 20.000 Demonstranten wurden verhaftet. In der Folge wurden sowohl der PAC als auch der ANC verboten. Daraufhin gründete 1961 auch der ANC einen bewaffneten Flügel. Nelson Mandela selbst leitete diesen Flügel mit dem Namen Umkhonto we Sizwe, was übersetzt so viel wie Speer der Nation bedeutet. Umkhonto we Sizwe tat sich in den folgenden Jahren insbesondere durch Sabotageakte hervor.
Beide Organisationen operierten fortan aus dem Untergrund. Führende opponierende Köpfe wie Nelson Mandela oder Walter Sisulu wurden 1964 im sogenannten Rivonia-Prozess zu lebenslanger Haft verurteilt. Das Gericht warf ihnen vor allem Beteiligung an Sabotageakten vor.
In den späten 1960er Jahren entstand in Kirchen und Schulen, beeinflusst durch die Black-Power-Bewegung in den USA, die sogenannte Black-Consciousness-Bewegung. Steve Biko gilt als Begründer dieser Bewegung. Hervorgerufen durch das neue Selbstbewusstsein der Schwarzen sahen sie die Kultur der Weißen nicht mehr als übermächtig. Vielmehr lehnten sie die weiße Kultur nun ab; ihre eigenen Werte hingegen hoben sie heraus. Kunstschaffende wie Miriam Makeba engagierten sich für einen weltweiten Boykott des Apartheidregimes.
Die Folgen des neuen Bewusstseins waren zum Teil heftige Studentenunruhen. Am 16. Juni 1976 boykottierten Schüler in Soweto den Unterricht. Dies stand im Zusammenhang mit der versuchten, zwangsweise durchgeführten Einführung der bei Schwarzen verhassten Sprache Afrikaans. Mit dem Boykott begann der Aufstand in Soweto. Durch brutale Polizeieinsätze verloren in wenigen Tagen 500 bis 1000 Schwarze ihr Leben und viele Kinder und Jugendliche wurden inhaftiert. Weltbekannt ist das Foto des sterbenden 12-jährigen Hector Pieterson in den Armen eines Mitschülers. Danach nahm der bewaffnete Widerstand sprunghaft zu. Die in den nächsten zwei Jahren folgenden Unruhen verunsicherten das Land. Hunderte von Schwarzen wurden von der Polizei getötet. Die Schüler und Studenten fanden Unterstützung bei Hunderttausenden von schwarzen Arbeitern. Für die südafrikanische Wirtschaft nahm dies verheerende Ausmaße an. Einige unbedeutendere Gesetze der Apartheid wurden gelockert, um dem Unmut der Schwarzen zu begegnen.
Einige Länder unterstützten das Apartheidregime in bestimmten Teilbereichen.
Die USA setzten 21 Mal im Sicherheitsrat ihr Veto ein, um Resolutionen gegen Südafrika zu verhindern, die zumeist eine totale Wirtschaftsblockade gegen das Land zum Inhalt hatten, das waren 13 Prozent der Gesamtanzahl ihrer Vetos.[64] Auch Firmen wie IBM haben mit logistischen und technologischen Mitteln das Regime unterstützt.[65] Die Bedeutung Südafrikas für die USA lag unter anderem in den Uranvorkommen des Landes.
Allerdings waren die USA aber auch die treibende Kraft hinter der Verabschiedung des ersten Waffenembargos gegen Südafrika durch die UN im Jahr 1963.[66]
Auch die Bundesrepublik unterhielt während der Apartheid Wirtschaftskontakte zu Südafrika. Der damalige Außenminister Willy Brandt, in dessen Partei die Beziehungen zu Südafrika höchst umstritten waren, begründete dies damit, „daß man Handel und Politik nicht ohne Not koppeln soll“.[67] Einer der führenden deutschen Politiker, der durch seine Nähe zur südafrikanischen Regierung in der Zeit der Apartheid auffiel, war Franz Josef Strauß.[68] Er befürwortete die Apartheid und soll bei einem Besuch in Südafrika gesagt haben: „Die Politik der Apartheid beruht auf einem positiven religiösen Verantwortungsbewußtsein für die Entwicklung der nichtweißen Bevölkerungsschichten. Es ist deshalb falsch, von der Unterdrückung der Nicht-Weißen durch eine weiße Herrenrasse zu sprechen.“[69] Deutschen Konzernen wird vorgeworfen, sich an der Apartheid in Südafrika beteiligt zu haben. In einem seit 2002 bei Bundesgerichten in den USA anhängigen Prozess, der von Apartheid-Opfern angestoßen und u. a. von Desmond Tutu unterstützt wurde, wurden 50 internationale Konzerne, darunter auch die Daimler AG und mehrere deutsche Banken, beschuldigt, durch ihre Geschäfte die Verbrechen des Apartheid-Regimes unterstützt zu haben. Die Kläger beriefen sich auf ein Gesetz von 1789, nach dem ausländische Bürger in den USA Klagen einreichen können, wenn internationales Recht verletzt wurde. General Motors einigte sich 2012 mit den Klägern auf einen Vergleich ohne Schuldeingeständnis.[70] Ein Berufungsgericht verwarf die Klage im August 2013 einstimmig mit einer Berufung auf eine Entscheidung des Obersten Gerichtshofs der Vereinigten Staaten, nach der das Gesetz in dem Fall nicht anwendbar sei. Die Verteidigung kann nun die Einstellung des Verfahrens beantragen.[71]
Eine Studie von 1999 kam zu dem Ergebnis, dass Deutschland mit 27,3 Prozent aller Auslandsschulden des öffentlichen Sektors der wichtigste Direktfinanzier des Apartheidregimes war und „[…] in herausragender Weise den Apartheidstaat direkt, ebenso wie die strategisch wichtigen Staatskonzerne der Apartheid mit Finanzkapital bedient hat“.[72][73]
Die tatsächlichen Apartheidsverhältnisse in Südafrika waren in Deutschland bekannt und in Teilen der Bevölkerung ein Diskussionsthema, wie nach Unterstützungsnoten aus dem Kreis der Evangelischen Frauenarbeit und dem damit verbundenen Früchteboykott zu schließen ist.[74] Andererseits fand Südafrika in Mitteleuropa auch Unterstützer seiner Politik. Eine 1974 in deutscher Sprache herausgegebene Schrift des Informationsministeriums in Pretoria wandte sich an deutschsprachige Leser und setzte sich rechtfertigend mit der internationalen und inneren Kritik an der Apartheid auseinander. Darin wurden die „Anti-Apartheid-Bewegung“ und die „Vertreter der Terroristenorganisationen und der Weltkirchenrat“ zu Staatsfeinden erklärt. Dem Weltkirchenrat bescheinigt die Propagandaschrift, „den terroristischen Bewegungen in Afrika sowohl geistige Unterstützung als auch Gelder“ zu liefern. Ferner meinten die ungenannten Autoren unter den Apartheid-Kritikern „bornierte Geister“ zu finden und dass „viele selbsternannte Experten“ prophezeiten, „dass die südafrikanische Regierungspolitik in einer Katastrophe enden würde“. Gleichzeitig gaben sie einen Einblick in ihre Auffassung von Pressefreiheit, indem sie in Hinblick auf kritische Berichterstattungen „von den alten Dickschädeln, die in Presse, Rundfunk und Fernsehen immer wieder das gleiche tun“ sprachen.[75]
Positive Haltungen zu den Apartheidsverhältnissen, insbesondere zu den damit beabsichtigt herbeigeführten sozio-ökonomischen Segregationsprozessen, drangen bis in wissenschaftliche Arbeiten Deutschlands ein und wurden als „räumliche Auswirkungen einer politischen Idee“ gekennzeichnet.[76] Das geschah in der Weise, dass beispielsweise die Etablierung der Homelands als „Hinführung zur innenpolitischen Autonomie“ bezeichnet wurde oder die dort geplanten Ortsgründungen als „[…] eingerichtet als Ansatzpunkte städtischer Entwicklung (s. Smit and Boysen 1977[77])“, um „im Laufe der Zeit eine solche Attraktivität zu entwickeln, dass aus den weißen Gebieten eine Rückwanderung in diese neuen Städte einsetzt, sowie als Ansatzpunkte einer industriellen Entwicklung innerhalb der Homelands zu dienen“.[78]
Auch in Großbritannien fand das Apartheidregime Unterstützung für seine Politik. Margaret Thatcher bezeichnete den ANC in einer Pressekonferenz auf der Commonwealth-Konferenz in Vancouver im Jahre 1987 als „terroristische Organisation“ und bediente im selben Statement antikommunistische Stereotype des Kalten Kriegs.[79] Im selben Jahr erschienen Mitglieder der Young Conservatives, der Jugendorganisation der Conservative Party, auf einem Parteitag mit Hang Nelson Mandela!-Abzeichen (deutsch: „Erhängt Nelson Mandela!“).[80]
Schweizer Banken und Industrieunternehmen ignorierten wiederholt und massiv die UN-Sanktionen (da sich die Schweiz als damaliges nicht UNO-Mitglied an UN-Sanktionen nicht halten musste[81][82]) und erleichterten dadurch die Praxis des Apartheidregimes. Die Schweizer Regierung äußerte, wenn überhaupt, nur halbherzig Kritik. Dagegen gab es sogar enge Kontakte auf diplomatischer Ebene. Seit 1980 hatte der südafrikanische Militärattaché seinen Dienstsitz in Bern, zuvor noch in Rom, Köln und Wien; bereits zu dieser Zeit verweigerten andere Staaten dessen Akkreditierung.[83]
Durch die internationale Isolation Israels nach dem Sechstagekrieg verstärkten sich die Beziehungen zu Südafrika.[84] Vor allem auf militärischem Gebiet entwickelte sich eine enge Zusammenarbeit. Dazu gehörten neben konventionellen Waffenlieferungen auch lange geheim gehaltene Kooperationsprojekte zu Atomwaffen.[85][86]
In vielen Ländern gab es Unterstützung für die Bevölkerungsmehrheit Südafrikas im Kampf gegen die Apartheid. Sowohl der ANC, die Black Consciousness Movement als auch kirchliche Organisationen hatten viele Kontakte, zum Beispiel zum Weltkirchenrat, den Vereinten Nationen und kleineren Organisation wie der Anti-Apartheid-Bewegung in Deutschland und der Evangelischen Frauenarbeit in Deutschland. Dazu kamen viele lokale Gruppierungen, die oft mit Dritte-Welt-Läden zusammenarbeiteten. Unterstützt wurden diese Gruppen auch aus der SPD. So forderten die Bundestagsabgeordneten Lenelotte von Bothmer und Hans-Jürgen Wischnewski zum Beispiel 1973 eine Einschränkung der wirtschaftlichen Beziehungen Deutschlands zu Südafrika.[87]
Um auf die Situation in Südafrika aufmerksam zu machen, wurde insbesondere zum Boykott südafrikanischer Produkte aufgerufen. Die in Großbritannien sehr aktive Anti-Apartheid Movement, woran auch Ambrose Reeves und Trevor Huddleston maßgeblich beteiligt waren, erzielte damit erhebliche Erfolge. Deren Wirkungen waren so deutlich, dass der britische Premierminister Harold Macmillan in seiner sogenannten Wind-of-Change-Rede vor beiden Kammern des südafrikanischen Parlaments am 3. Februar 1960 in Kapstadt darauf hinwies.[88] Zur Unterstützung von politisch Verfolgten und ihren Familien entstanden bereits 1956 finanzielle Hilfsstrukturen zwischen Südafrika und dem Vereinigten Königreich, die sich später mit dem International Defence and Aid Fund for Southern Africa weltweit ausbreiteten.
Im Zuge dieser internationalen Protestentwicklung entstanden viele kleinere Aktionen unter anderem auf den Deutschen Evangelischen Kirchentagen. Der Früchteboykott wurde von Südafrikanern angeregt und dann von den lokalen Gruppen in ihren jeweiligen Ländern propagiert. Neben dem Boykott der Früchte aus Südafrika wurde auch gegen die die Apartheid unterstützenden Geschäfte deutscher Großbanken protestiert.
Die Bemühungen des ANC im Ausland zur Verdeutlichung der Apartheidsverhältnisse im damaligen Südafrika bewirkten an vielen Orten der Welt Reaktionen von der Gewährung seiner Aktivitäten auf fremden Territorien bis zur aktiven Unterstützung konkreter Projekte. Beispielsweise unterhielt der ANC in London seine wichtigste Auslandsvertretung und sammelte auf diese Weise politische, wissenschaftliche, logistische und finanzielle Unterstützung für zahlreiche Vorhaben. Eines dieser Projekte bestand in einer umfangreich gegliederten Bildungseinrichtung auf dem Staatsgebiet von Tansania. Zwischen 1978 und 1992 wurde dort im Solomon Mahlangu Freedom College eine Schul- und Hochschulbildung durch einen international zusammengesetzten Lehrkörper für ausgewählte Südafrikaner gewährleistet.
Die von der indischstämmigen und farbigen Bevölkerungsgruppe Südafrikas initiierten Antiapartheidsbestrebungen ermöglichten ihrerseits weitere Unterstützeraktivitäten, wie beispielsweise Studiermöglichkeiten in Indien durch direkte Protektion der Staatspräsidentin Indira Gandhi oder neue Schulprojekte in Slumsiedlungen der damaligen Provinz Natal. Eine zentrale Rolle spielte innerhalb der Organisation dieses politischen Prozesses die südafrikanische Soziologieprofessorin Fatima Meer.
Der Iran versah die Reisepässe seiner Bürger mit einem Stempel, welcher die Einreise iranischer Bürger in Südafrika untersagte. Länder wie Tansania untersagten die Einreise, wenn im Pass ersichtlich war, dass der Inhaber sich in Südafrika aufgehalten hatte.
Die Vereinten Nationen haben seit ihrer Gründung die Apartheid als gravierendes Beispiel einer systematischen Rassentrennung verurteilt. Die Mehrheiten in den Organen der Vereinten Nationen haben sich vor allem durch das Wachstum der Vereinten Nationen durch den Beitritt vieler Staaten der Dritten Welt auf der XV. Sitzung der Generalversammlung der UN (1959) zuungunsten der Politik der Apartheid verschoben.[89] Die Veränderung der Mehrheitsverhältnisse beeinflusste auch die Haltung der westlichen Staaten, inklusive der Bundesrepublik, die ab den 1970er Jahren vermehrt Resolutionen der Generalversammlung gegen die Apartheid unterstützten, sofern diese nicht zu Gewalt aufriefen oder Anti-Apartheidsorganisationen erwähnten, die als marxistisch eingeschätzt wurden.[90]
Zu den wichtigsten Reaktionen zählt die Resolution 1761 aus der XVII. Sitzung der UN-Generalversammlung vom 6. November 1962 unter Leitung von Muhammad Zafrullah Khan bezüglich der Apartheidpolitik der Südafrikanischen Regierung, die mit dieser Erklärung unter Aufruf zu Sanktionen verurteilt wurde.[91]
Von den Vereinten Nationen wurde die Entwicklung der Apartheidpolitik kontinuierlich beobachtet. Auf dem 6. Kongress der Vereinten Nationen für Verbrechensverhütung und die Behandlung Straffälliger zwischen dem 25. August und 5. September 1980 in Caracas wurde über den Fortschritt der am 18. Juli 1976 in Kraft getretenen Internationale Konvention über die Bekämpfung und Bestrafung des Verbrechens der Apartheid berichtet. Bis zum 1. Mai 1980 hatten sie 56 Staaten ratifiziert oder waren ihr beigetreten. Die UN-Menschenrechtskommission forderte die UN-Sonderkommission gegen die Apartheid (Special Committee on the Policies of Apartheid of the Government of the Republic of South Africa) auf, zusammen mit aus Südafrika stammenden Experten eine Liste zu erstellen, worin Personen, Institutionen, Organisationen und offizielle Repräsentanten der Republik Südafrika erfasst werden sollten, die für Verbrechen nach Artikel 2 der internationalen Konvention als verantwortlich angesehen wurden.[92][93]
Initiiert durch die Vereinten Nationen, gab es einen weitgehenden Boykott kulturellen Austauschs mit Südafrika. Paul Simon machte mit seinem 1986 erschienenen Album Graceland, an dem zahlreiche südafrikanische Musiker mitwirkten, auf die Apartheid aufmerksam. Er wurde aber gleichzeitig kritisiert, weil er dem Boykott nicht gefolgt war.
Die Europäische Gemeinschaft (EG) hatte sich 1985 im Rahmen der europäischen politischen Zusammenarbeit auf eine abgestimmte Haltung zu Südafrika festgelegt und ein Sonderprogramm zugunsten von Opfern der Apartheidpolitik entwickelt, das man ab 1986 praktizierte.[94] Am 16. September 1986 beschlossen die Außenminister der EG gemeinsame Sanktionen, die unter anderem Investitionen in Südafrika sowie den Import von südafrikanischem Stahl, Eisen und Goldmünzen (Krugerrand) verboten. Das im Entwurf vorgesehene Verbot des Imports von Kohle – zu jenem Zeitpunkt gingen zwei Drittel der Kohleexporte Südafrikas in EG-Länder – wurde auf Betreiben der deutschen und unterstützt von der portugiesischen Regierung nicht in den beschlossenen Text aufgenommen.[95]
Im Jahr 2019 aufgetauchte Dokumente enthüllen, dass ab den 1970er Jahren die PR-Agentur Hennenhofer damit beauftragt war, eine deutsche Boykott-Beteiligung zu verhindern. Unter anderem mit bezahlten „Informationsreisen“ nach Südafrika wurden verschiedene Politiker und Journalisten zu diesem Zweck wirkungsvoll eingespannt.[96]
Die Proteste der Schwarzen sowie andere Faktoren ließen die Apartheid ab 1974 immer mehr bröckeln. Die Vollversammlung der UN nahm im Dezember 1973 die „Konvention zur Bekämpfung und Ahndung des Verbrechens der Apartheid“ an, die 1976 in Kraft trat. Die Präambel dieser Konvention betonte, dass Apartheid als Verbrechen gegen die Menschlichkeit einzustufen ist. Straftatbestände wurden benannt, so dass mit dieser Konvention eine Strafbarkeit nach internationalem Völkerrecht begründet wurde. Die burische Regierung näherte sich in langsamen Schritten den schwarzen Vorstellungen an. Die schwarze Opposition wurde immer stärker, obwohl ihre bekanntesten Führer im Gefängnis saßen. Höhepunkte des Widerstandes in den 1970er Jahren waren Streiks in Natal (1973) sowie der Aufstand in Soweto 1976. Dem schwarzen Widerstand begegnete die Regierung mit Notmaßnahmen, die allerdings die staatlichen Kapazitäten sprengten. Die Kosten der Apartheid waren nicht mehr länger tragbar.
Der ANC wurde vom Westen während des Kalten Krieges als revolutionär und prokommunistisch angesehen. Trotz gewisser Sanktionen stützten die USA und Westeuropa das weiße Apartheidregime als Bollwerk gegen den Kommunismus, auch weil Südafrika bedeutende Uranvorkommen hat. Nachdem die portugiesischen Kolonien Moçambique und Angola unabhängig und zum Schauplatz blutiger Kriege geworden waren, erschien die Unterstützung Südafrikas noch wichtiger. Mit dem Ende des Kalten Krieges verlor dieses Element freilich seine Bedeutung, und das alte Regime Südafrikas wurde vom Westen fallen gelassen.
Wirtschaftlich geriet Südafrika schon seit 1983 in Schwierigkeiten, als der Goldpreis auf dem Weltmarkt zu verfallen begann. Die durch die europäischen und amerikanischen Sanktionen geschwächte ökonomische Situation verschärfte sich damit weiter.
Der Reformierte Weltbund schloss die niederländisch-reformierte Kirche Südafrikas aus und erhöhte so den moralischen Druck auf einen Wandel.
Die zunehmend verbesserte Organisation der nichtweißen Opposition, die in den 1980er Jahren faktisch die Verwaltung der Townships übernahm, führte zum permanenten Ausnahmezustand von 1985 bis 1990. Angestoßen durch die Dakar-Konferenz im Juli 1987, bei der sich Vertreter des ANC im Exil mit einer Gruppe weißer Oppositioneller aus Südafrika über Möglichkeiten einer friedlichen Überwindung der Apartheid ausgetauscht hatten, begann ein teilweise geheimer Dialog mit den Führern des ANC im Exil über die Zukunft Südafrikas nach der Apartheid.[97]
1989 trat Frederik Willem de Klerk die Nachfolge von Pieter Willem Botha als südafrikanischer Staatspräsident an. De Klerk übernahm sogleich die geheimen Verhandlungen mit dem noch immer inhaftierten ANC-Führer Mandela. Er stellte Mandela die sofortige Freilassung in Aussicht, wenn dieser gewisse Konditionen, wie beispielsweise die Abkehr vom bewaffneten Widerstand, annähme, worauf Mandela jedoch nicht einging. De Klerk ließ Mandela aufgrund des steigenden Druckes zusammen mit den übrigen politischen Gefangenen im Jahre 1990 frei. Die beiden Widerstandsparteien ANC und PAC wurden wieder legalisiert.
Aufgrund dieser in ihrer Summe bedeutsamen Faktoren, also des Widerstandes der Schwarzen, des internationalen Druckes, der ökonomischen Krise, des Wechsels der Regierungsführung von Botha zu de Klerk sowie der Standhaftigkeit Mandelas bei den Verhandlungen mit de Klerk, brach die weiße Autorität in den frühen 1990er Jahren Schritt für Schritt zusammen. Bei einem Referendum im März 1992 sprachen sich 68,7 Prozent der Weißen für die Abschaffung der Rassentrennungspolitik aus.
De Klerk hob wesentliche Gesetze auf, die als Pfeiler der Apartheid galten. Darunter waren der Population Registration Act, der Group Areas Act und der Land Act. Die Homelands existierten allerdings weiter; diesbezüglich änderte sich nur wenig.
Die Übergangsphase von der Apartheid zur angestrebten rechtlichen und ökonomischen Gleichstellung aller Einwohner Südafrikas dauerte von 1990 bis 1994. Während dieser Zeit wurde die Gesetzgebung der Rassentrennung verändert. Alle in Südafrika lebenden Menschen konnten sich nun frei und ohne Restriktionen bewegen. Viele Schwarze nutzten diese Chance und zogen in Städte. Seit November 1993 gab es eine plural zusammengesetzte Regierung, das Transitional Executive Council. Des Weiteren war die Übergangsphase geprägt von blutigen Konflikten zwischen der Inkatha-Partei Mangosuthu Buthelezis und dem ANC. Buthelezi, Führer des Homelands KwaZulu, sah durch das neue Staatssystem seine Macht bedroht und bekämpfte die Arbeiten an einer neuen Verfassung sowie die Wahlvorbereitungen. Erst durch den Einfluss von Washington Okumu, einem Freund aus Kenia, lenkte Buthelezi ein und erklärte eine Woche vor dem Wahltermin die Teilnahme seiner politischen Bewegung Inkatha. In kürzester Zeit mussten die Stimmzettel mit Aufklebern ergänzt werden. Die vorausgegangenen politischen Unruhen, nicht nur der zwischen ANC und Inkatha, dauerten von 1990 bis 1994 und forderten mehrere tausend Todesopfer. Nebst Buthelezi standen auch Lucas Mangope und Oupa Gqozo, die Führer der Homelands Bophuthatswana und Ciskei, den sich abzeichnenden Veränderungen ablehnend gegenüber. Die Angst vor persönlichen Verlusten förderte in dieser Situation ein Festhalten am alten System. Andere Homeland-Verantwortliche kooperierten mit den Plänen des ANC und versuchten durch Anpassung eine günstige Position in den künftigen Machtverhältnissen zu erlangen.[98]
Im März 1995 wurde im südafrikanischen Parlament die Frage nach der Zahl der Opfer während dieser Unruhen durch den Polizeiminister beantwortet. Nach den Unterlagen der Regierung sollen es ohne die Homelands 5007 Personen gewesen sein, die im Verlaufe der zahlenmäßigen angestiegenen politischen Konflikte zwischen 1992 und 1994 den Tod gefunden hatten. Das South African Institute of Race Relations veröffentlichte unter Einbeziehung der damaligen Homelands folgende Zahlen: 3347 Tote im Jahr 1992, 3794 Tote 1993 und 2476 Tote 1994. Zudem sind noch im Jahr 1995 im Verlaufe politischer Unruhen 1044 Menschen getötet worden.[99]
Die ausgehandelte Übergangsverfassung trat 1994 in Kraft. Danach würden alle fünf Jahre Regierungswahlen stattfinden. Ferner wurde das Land in neun statt in bisher vier Provinzen unterteilt. So kam es 1994 zu den ersten allgemeinen, gleichen und geheimen Wahlen Südafrikas. Der ANC gewann mit 62,6 Prozent überragend, es folgte die Nasionale Party (NP) mit 20,4 Prozent und die Inkatha Freedom Party mit 10,5 Prozent. Mandela wurde zum ersten Präsidenten unter der neuen Verfassungsordnung ernannt. Ihm zur Seite standen zwei populäre Vizepräsidenten, de Klerk von der NP und Thabo Mbeki vom ANC. Buthelezi wurde Premier der Provinz Kwazulu-Natal, er konnte seine Macht also über die bisherige Homelandgrenze ausdehnen. Mandela und de Klerk erhielten 1993 den Friedensnobelpreis.
Die Wahrheits- und Versöhnungskommission (Truth and Reconciliation Commission, TRC) wurde eingerichtet, um politisch motivierte Verbrechen zu verhandeln, die während der Zeit der Apartheid begangen worden waren. Sie geht in ihrer Entstehung zurück auf eine Initiative des ANC und des damaligen Justizministers Abdullah Omar im Jahr 1994 und wurde im Januar 1996 durch Präsident Nelson Mandela eingesetzt. Vorsitzender war Desmond Tutu. Die Wahrheits- und Versöhnungskommission bestand aus drei Ausschüssen, die jeweils unterschiedliche Aufgaben übernahmen:
Wesentliches historisches Vorbild für ihre Errichtung war die Rettig–Kommission (Comisión Nacional de Verdad y Reconciliación) in Chile mit ihrem Bericht von 1991 über die Menschenrechtsverletzungen der Regierung unter Augusto Pinochet.[100][101]
Die Kommission wurde für 18 Monate einberufen und ihre Arbeit konnte um ein halbes Jahr verlängert werden. Der relativ kurze Zeitraum ihres Wirkens war bereits zur Einberufung umstritten, da die Fülle der zu behandelnden Fälle in dieser Zeit kaum zu bearbeiten schien. Allerdings galt es auch, die Folgen des Apartheidsystems schnell öffentlich zu machen, sowohl um gegebenenfalls Entschädigungen nicht erst nach vielen Jahren zu zahlen, als auch, um den schmerzhaften Prozess der Aufklärung nicht unnötig in die Länge zu ziehen.
Ihr Ziel war es, Opfer und Täter in einen „Dialog“ zu bringen und somit eine Grundlage für die Versöhnung der zerstrittenen Bevölkerungsgruppen zu schaffen. Vorrangig hierbei war die Anhörung beziehungsweise die Wahrnehmung des Erlebens des jeweils anderen.
Den Angeklagten wurde Amnestie zugesagt, wenn sie ihre Taten zugaben, den Opfern wurde finanzielle Hilfe versprochen. Ziel war die Versöhnung mit den Tätern sowie ein möglichst vollständiges Bild von den Verbrechen, die während der Apartheid verübt worden waren, zu bekommen. Sämtliche Anhörungen waren deshalb öffentlich. Am 29. Oktober 1998 präsentierte die Wahrheits- und Versöhnungskommission ihren Abschlussbericht.[102] Vor allem von Seiten der Schwarzen wurde kritisiert, dass die Gedanken der Versöhnung und Amnestie Vorrang vor der Gerechtigkeitsfindung hatten.
Die mit der Apartheid verbundenen Diskriminierungen und Menschenrechtsverstöße sind mittlerweile auch im internationalen Recht – losgelöst von der mittlerweile überwundenen Apartheid in Südafrika – als Verbrechen gegen die Menschlichkeit definiert. Durch das Römische Statut über die Schaffung eines Internationalen Strafgerichtshofs wurde die Apartheid der Zuständigkeit dieses Gerichtshofs unterworfen. Das Statut wurde auf einer Staatenkonferenz in Rom im Jahre 1998 angenommen und seither von 139 Staaten unterzeichnet und von 114 Staaten ratifiziert. Es ist seit dem Jahre 2002 in Kraft. Somit können derartige Vorgänge mittlerweile international strafrechtlich verfolgt werden. Diese Entwicklung wurde maßgeblich dadurch motiviert, dass es früher keine derartige Rechtsgrundlage gab, so dass die Apartheid in Südafrika bzw. die Verantwortlichen juristisch praktisch nicht belangt werden konnten.
Adriaan Vlok war der erste Minister des früheren Apartheidregimes, der sich in einem Prozess gegen frühere Mitglieder der Sicherheitsbehörden vor einem Gericht für Verbrechen, die er während seiner Amtszeit begangen hatte, verantworten musste und dafür rechtskräftig verurteilt wurde.
Die über Jahre anhaltenden Unruhen hatten Südafrika in eine ökonomische Krise gestürzt. Diese brachte eine hohe Staatsverschuldung mit sich. Im Weiteren sollten die Ungleichheiten zwischen den Bevölkerungsgruppen beseitigt werden. Dies würde unter anderem bessere Schulen und eine bessere Gesundheitsversorgung für Schwarze bedeuten. Beides war jedoch mit hohen Kosten verbunden. Unterschiedlichste Interessen führten zu verschiedenen Landstreitigkeiten. Schwarze, die während der Apartheid ihr Land aufgeben mussten und gezwungen worden waren, in die Homelands zu ziehen, forderten ihr Land zurück. Die nun dort ansässigen Weißen oder Industriebetriebe machten ihre jüngeren Rechte geltend.
1999 stieg Mbeki vom Vizepräsidenten zum Präsidenten auf. Er intensivierte in der Folge die Privatisierung von Staatsbetrieben. Dies führte zu Stellenabbau und zu steigenden Strom- und Wassertarifen. Immer mehr schwarze Arbeiter, die vor allem unter diesen Maßnahmen zu leiden haben, wurden zunehmend unzufrieden mit der Politik des ANC. Sie werfen ihm vor, dass der ANC zwar von der linken Arbeiterklasse gewählt worden sei, jedoch im Interesse der rechten Bourgeoisie regiere.
in der Reihenfolge des Erscheinens
in der Reihenfolge des Erscheinens

Der Nordpol ist – als einer von zwei geografischen Polen − im allgemeinen Sprachgebrauch der nördlichste Punkt der Erde. Er entspricht dem nördlichen Drehpunkt der Erdachse und wird auch als geographischer Nordpol bezeichnet. Daneben gibt es den arktischen Magnetpol und den arktischen geomagnetischen Pol.
Derzeit liegen die drei verschiedenen Pole der nördlichen Hemisphäre bei jeder Definition im Arktischen Ozean (auch Nordpolarmeer genannt) bzw. auf dessen Inseln. Mit Verschiebung des Erdmagnetfeldes ändert sich die Lage des arktischen Magnetpols und des arktischen geomagnetischen Pols.

Der geographische Nordpol (1) ist der nördlichste Punkt der Erde und gemäß der Definition des geographischen Poles der Schnittpunkt der Erdachse mit der nördlichen Erdoberfläche. Der geographische Nordpol ist der Antipode des geographischen Südpols und hat eine feste Position bei der geographischen Breite von 90° 0′ N900Koordinaten: 90° 0′ 0″ N, was generell für Himmelskörper mit einer Rotationsachse gilt. Somit richtet sich der Blick von hier aus in der Waagrechten nur nach einer Himmelsrichtung: nach Süden. Der Nordpol hat keine eindeutige geographische Länge. Das gilt jedoch nur für den Punkt und nicht für die Blickrichtung, die eindeutig ist. Der geographische Nordpol liegt auf der Nordamerikanischen Platte, jedoch nicht auf Festland, sondern auf einer 2 bis 3 m dicken schwimmenden Eisdecke.[2] Darunter liegt das Nordpolarmeer, welches an dieser Stelle 4087 m tief ist.Nahe dem Zenit steht über dem geographischen Nordpol der Polarstern in nur 42' Abstand (2008) und die Sonne geht hier vom 21. März bis zum 23. September nicht unter (Polartag). Es folgen langsamer Sonnenuntergang, mehrwöchige Dämmerung, mehrere Monate Polarnacht, mehrwöchige Morgendämmerung und langsamer Sonnenaufgang.
Da sich die Lage der Erdachse langfristig verlagert, ist der geografische Nordpol keineswegs ortsgebunden. Zwischen dieser langfristigen Bewegung des geografischen Nordpols und der Massenumverteilung im Rahmen der globalen Erwärmung besteht dabei ein enger Zusammenhang.[3] Geodätische Beobachtungen der Polbewegung aus dem Weltraum zeigen, dass die durchschnittliche jährliche Polposition nach einer längeren Bewegung in Richtung des westlichen Grönlands ab etwa 2005 nach Osten zu driften begann, was eine abrupte Abweichung von der im letzten Jahrhundert beobachteten Driftrichtung darstellt.[3] Schwerkraftmessungen mithilfe von Satelliten ergeben, dass etwa 90 % dieser Änderung auf das beschleunigte Abschmelzen der Eisdecken und Berggletscher und den damit verbundenen Anstieg des Meeresspiegels zurückzuführen sind.[3] Der Effekt ist allerdings sehr klein und keinesfalls mit der weiter unten beschriebenen Verschiebung des Magnetpols vergleichbar: Die Polverschiebung beträgt nur etwa 3 Millimeter pro Jahr.[4]
Der arktische Magnetpol (2) ist jener Punkt der nördlichen Hemisphäre, an dem die magnetischen Feldlinien des Erdmagnetfelds vertikal zur Erdoberfläche in die Erde eintreten. Es handelt sich im physikalischen Sinne um einen magnetischen Südpol.
Die üblichen Kompasse, deren Nadel nur um die vertikale Achse drehbar ist, können sich innerhalb des Radius von circa 2000 km um den Magnetpol nicht mehr nach Norden ausrichten, weil dort die waagerechte Komponente des Erdmagnetfelds für die Anzeige zu schwach ist. Ein speziell für solche Messungen konstruierter Kompass – ähnlich einem Schiffskompass – zeigt am arktischen Magnetpol gemäß den Gesetzen des Magnetismus mit seiner „Norden“-Markierung senkrecht nach unten.
Erstmals aufgesucht wurde der arktische Magnetpol am 1. Juni 1831 von James Clark Ross nahe Kap Adelaide, Boothia-Halbinsel, Kanada; seinerzeit wurden 70° 5′ N, 96° 28′ W70.083333333333-96.466666666667 als Koordinaten ermittelt.
Bei einer Expedition des Geological Survey of Canada im Jahre 2001 wurde seine Position ermittelt und die jährliche Wanderung bis zum Jahr 2005 hochgerechnet.[5] Im April 2007 wurde die Position erneut bestimmt:[6]
Der arktische Magnetpol ist also nicht ortsfest, sondern er verlagert sich ständig nach einem mehrschichtigen Muster:
Von Jahr zu Jahr verlagert sich der Magnetpol in einer grob vorhersagbaren Art. Derzeit wandert er jährlich um etwa 40 Kilometer nordwestwärts und hat die kanadischen Inseln im Nordpolarmeer verlassen. Betrag und Richtung der jährlichen Verlagerung sind aber nicht langfristig konstant.  Laut einer Studie aus dem Jahr 2020 bewegt sich der magnetische Nordpol innerhalb des nächsten Jahrzehnts aufgrund von Änderungen an der Kern-Mantel-Grenze wahrscheinlich um ca. 500 km (390–660 km) weiter in Richtung Sibirien.[7][8]
Die sich stetig verändernde Abweichung von geographischem Nordpol und arktischem Magnetpol (Deklination) macht für Zwecke der Navigation regelmäßige Anpassungen der Umrechnung für Navigationshilfsmittel wie Karten und Funkfeuer notwendig (Kursbeschickung).
Der arktische Magnetpol wandert täglich auf einer elliptischen Bahn um seine mittlere Position.
Störungen des Erdmagnetfeldes können den Magnetpol – solange die Störung dauert – um bis zu 50 km von seiner mittleren Position verlagern.
Im Lauf der Erdgeschichte hat sich das Magnetfeld der Erde mehrmals umgepolt. Diese Polsprünge konnten durch die Ausrichtung von Eisenablagerungen in Sedimenten verschiedener Tiefen nachgewiesen werden. Einige Wissenschaftler sehen in der oben genannten Beschleunigung der jährlichen Verlagerung ein Indiz für eine langfristig bevorstehende Umpolung der Erde.
Der arktische geomagnetische Pol (3) auf der nördlichen Halbkugel ist ein theoretischer Pol des unregelmäßigen Erdmagnetfeldes, dem die Annahme entspricht, dass sich im Erdmittelpunkt ein Stabmagnet befände. Er lag 2010 bei etwa 80° 1′ N, 72° 13′ W80.02-72.21 auf der Darling-Halbinsel der zu Kanada gehörenden Ellesmere-Insel.
Beim Vergleich der Koordinaten von arktischem Magnetpol und antarktischem Magnetpol fällt auf, dass sich die beiden Magnetpole weniger genau gegenüberliegen als die geomagnetischen Pole. Der eigentliche Unterschied zwischen den Magnetpolen und den geomagnetischen Polen besteht darin, dass die Magnetpole durch Messungen bestimmt werden und die geomagnetischen Pole durch Berechnungen. Die geomagnetischen Pole sind nicht ortsfest; sie folgen einem ähnlichen Bewegungsmuster wie die Magnetpole.
Ursprünglich wurde dasjenige Ende einer Magnetit­nadel, das in Richtung geographischer Norden zeigte, Nordpol der Nadel genannt. Damals hatte man noch keine Kenntnis von dem dahinter liegenden Mechanismus. Erst sehr viel später wurde man sich dessen bewusst, dass diese von der Physik übernommene Benennung dazu führte, dass die Erde in Richtung des geographischen Nordpols physikalisch gesehen einen magnetischen Südpol hat, und in Richtung geographischer Südpol den magnetischen Nordpol.
Allerdings wird die Stelle an der Erdoberfläche, an der die Feldlinien des Erdmagnetfeldes senkrecht eintreten (physikalisch verstanden als "der Pol, der der magnetische Südpol der Erde ist"), in geographischen Zusammenhängen fast immer als "magnetischer Nordpol" bezeichnet (geographisch verstanden als: "der magnetische Pol, der im Norden liegt"). Um Missverständnisse zu vermeiden, könnten zwar die eindeutigen geographischen Begriffe „Arktischer Magnetpol“ und „Antarktischer Magnetpol“ verwendet werden. Diese Bezeichnungen werden aber nur sehr selten genutzt (obwohl sie angesichts der physikalischen Polaritätswechsel über geologische Zeiträume hinweg sinnvoll sind). In der Regel wird mit „magnetischer Nordpol“ in einem geographischen Zusammenhang immer der magnetische Pol nahe dem geographischen Nordpol bezeichnet.
Erstmals wurde der geographische Nordpol nach eigenen Angaben von den US-amerikanischen Forschern Robert Edwin Peary und Matthew Henson sowie den Inughuit Iggiánguaĸ (1883–1918, in Pearys Aufzeichnungen Egingwah), Sigdluk (1883–1927, Seeglo) und Uvkujâĸ (1880–1921, Ooqueah) unter der Führung von Iggiánguaĸs Bruder Ôdâĸ (1880–1955, Ootah) am 6. April 1909 erreicht. Es gilt jedoch nicht als wissenschaftlich gesichert, dass diese Gruppe den Pol tatsächlich erreicht hat. Pearys Aufzeichnungen sind hierfür nicht ausreichend genau und Matthew Henson berichtet in seinen Memoiren, er sei kurz vor Expeditionsleiter Peary am Nordpol gewesen und habe diesen dort getroffen. Gemeinsam habe man die Frage klären wollen, wer wohl als Erster am Pol gewesen sei. Zu dieser Klärung ist es jedoch offenbar nie gekommen. Die grönländischen Begleiter Pearys nannten den Nordpol qimmersoriartorfissuaq, den Ort, an dem man gezwungen ist, seine Hunde zu essen.[10]
Außer den Vorgenannten nahm auch Frederick Cook für sich in Anspruch, als Erster den Nordpol erreicht zu haben, und zwar bereits am 21. April 1908, also ein Jahr vor Peary. Peary startete daraufhin eine Kampagne, um die Glaubwürdigkeit von Cook zu untergraben. Ein zentraler Teil dieser Kampagne war es, Cooks angebliche Erstbesteigung des Denali als Lüge anzuprangern (Cook erstieg den Berg tatsächlich nicht). Auch andere Ungereimtheiten lassen darauf schließen, dass Cook nie in der Nähe des Nordpols war.
Pearys Aufzeichnungen sprechen nicht dafür, dass er am Nordpol war: Als er sich von seinen Begleitern trennte, war er mindestens 120 km vom Pol entfernt; nach nur 56 Stunden traf er wieder bei ihnen ein. Alle Polarforscher jener Zeit, zum Beispiel Größen wie Fridtjof Nansen, hielten ein solches Pensum für völlig unmöglich und bestritten auch die angeblichen vorherigen Tagesleistungen Pearys von über 50 km durch das Packeis.[11] Pearys Glaubwürdigkeit sank noch weiter, als das US Navy Hydrographic Office fünf der von Peary in diesem Zusammenhang gemeldeten geografischen Entdeckungen als nicht existent streichen musste.[11]
Die Überfliegung des Nordpols im Jahr 1926 durch Umberto Nobile, Roald Amundsen und Lincoln Ellsworth an Bord der Norge ist zweifelsfrei nachgewiesen und unbestritten; ebenso, dass 1937 eine Gruppe sowjetischer Wissenschaftler unter der Leitung von Iwan Papanin in die Nähe des Nordpols flog (Entfernung ca. 20 km), um die erste Polarstation Nordpol-1 zu errichten.
Am 23. April 1948 flog die sowjetische Forschungsexpedition Nord-2 unter Alexander Kusnezow bis zum Nordpol und erreichte ihn anschließend zu Fuß. Damit wurden sie die ersten Menschen, die erwiesenermaßen den Nordpol betraten.[12][13]
Der erste Mensch, der den Pol nachweislich auf dem Weg über das Eis erreichte, war der US-Amerikaner Ralph Plaisted (1927–2008), der 1968 eine vierköpfige Expedition auf Schneemobilen zum nördlichsten Punkt der Erde führte.[14] Ein Jahr später erreichte der Brite Sir Walter William Herbert mit Hundeschlitten den Nordpol.
Das nukleargetriebene U-Boot USS Nautilus erreichte am 3. August 1958 als erstes Schiff den geographischen Nordpol. Am 17. August 1977 gegen 04:00 Uhr Moskauer Zeit erreichte der sowjetische Atomeisbrecher Arktika als erstes über Wasser fahrendes Schiff den Nordpol. Hunderte Besatzungsmitglieder, Wissenschaftler und Fahrgäste betraten feierlich das direkte Polumfeld.
Im April 1990 erreichte eine deutsch-schweizerische Expedition unter Leitung eines Teams der Universität Gießen den geographischen Nordpol, um Untersuchungen zur Verschmutzung von Eis, Schnee und Luft durchzuführen.[15] Die entnommenen Proben wurden in Zusammenarbeit mit dem Geological Survey of Canada und dem Alfred-Wegener-Institut für Polar- und Meeresforschung analysiert. Weitere Stationen für Probenentnahmen waren in mehrjährigem Packeis auf 86° N, am Kap Columbia und bei Ward Hunt Island. Im Jahr 1991 konnten nach schwerer Eisfahrt die beiden ersten konventionell angetriebenen Schiffe bis zum Nordpol vordringen: Der schwedische Eisbrecher Oden und das deutsche Forschungsschiff Polarstern erreichten den Pol am 7. September während einer dreimonatigen Expedition. Der Meeresgrund in 4087 m Tiefe wurde hier erstmals 2007 von einer russischen Forschungsexpedition erreicht.
Die beiden Pole der Erde sind von polaren Eiskappen bedeckt; unter den Eismassen befinden sich z. B. subglaziale Seen und Vulkane. Die polaren Eisschelfe sind infolge der menschengemachten globalen Erwärmung von sich verstärkender Eisschmelze betroffen.[16]
Beide Pole sind im Rahmen der globalen Telekonnektion Zentren des globalen Wettergeschehens, z. B. mit den Polarwirbeln im Rahmen der arktischen Oszillation oder ihrem Einfluss auf den Jetstream und ihrer Rolle bei der thermohalinen Zirkulation der Ozeane.
Am 2. August 2007 landeten zwei russische Mir-Tauchboote auf dem Meeresgrund am Nordpol in 4261 m Tiefe und setzten dort eine Kapsel aus Titan mit der russischen Flagge ab. Die Expedition hatte das Ziel, Bodenproben zu sammeln, um die russischen Territorialansprüche mit Beweisen zu stützen, dass der Nordpol zum sibirischen Festlandsockel gehört.
Auch Dänemark, Kanada und Norwegen könnten territoriale Ansprüche erheben. In einem FAZ-Gespräch 2007 bezeichnete der Polarforscher Arved Fuchs einen politisch-wirtschaftlichen Wettlauf um den Nordpol als wahrscheinlich, es gehe schließlich um fossile Brennstoffe.[17]
2007 waren die britischen TV-Reporter Jeremy Clarkson und James May sowie die Mitglieder ihres Support-Teams im Rahmen des Top Gear: Polar Special die ersten Menschen, die den in der Polar Challenge festgelegten arktischen Magnetpol von 1996 bei 78° 35,7′ N, 104° 11,9′ W (78° 35′ 42″ N, 104° 11′ 54″ W78.595-104.19833333333) mit einer Abweichung von weniger als einem Kilometer mit einem Auto erreichten. Sie stoppten bereits bei 78° 35′ 7″ N, 104° 11′ 9″ W78.585277777778-104.18583333333, weil das Ziel in ihrem Gerät zur Positionsmessung ohne Umrechnung von Zehntelminuten in Sekunden programmiert wurde. Für die Expedition wurden stark modifizierte Varianten des Toyota Hilux sowie Toyota Land Cruiser benutzt.[18]
Nordpol der Unzugänglichkeit ist ein Begriff zur Benennung des küstenfernsten Punktes im Nordpolarmeer. Ermittelt wird er als Inkreismittelpunkt der umliegenden Meeresufer. Er befindet sich bei 84° 3′ N, 174° 51′ W84.05-174.85 und damit etwa 660 km vom geographischen Nordpol entfernt. Seine Position kann sich mit steigendem Meeresspiegel etwas verändern. Er wurde das erste Mal 1927 erreicht. Unterhalb des Nordpols der Unzugänglichkeit befindet sich kein Festland, sondern nur Eis und Wasser des hier etwa 3000 m tiefen Nordpolarmeeres.
Nordpol der Unzugänglichkeit ist ein irreführender Ausdruck, weil er nahelegt, dass Expeditionen in die Arktis hauptsächlich von den benachbarten Küsten aus unternommen würden, was nicht der Fall ist. Unzugänglichkeitspole haben keine praktische Bedeutung. Siehe auch: Liste geographischer Mittelpunkte von Ländern.

Kanada (englisch und französisch Canada) ist ein Staat in Nordamerika, der zwischen dem Atlantik im Osten und dem Pazifik im Westen liegt und nordwärts bis zum Arktischen Ozean reicht. Bundeshauptstadt ist Ottawa, die bevölkerungsreichste Stadt ist Toronto. Die einzigen Staatsgrenzen sind jene zu den Vereinigten Staaten im Süden und im Nordwesten sowie die 2022 geschaffene Grenze über die Hans-Insel zu Grönland. Kanada ist gemessen an der Fläche nach Russland der zweitgrößte Staat der Erde, hat etwa 37 Millionen Einwohner und eine Bevölkerungsdichte von nur vier Personen pro Quadratkilometer.[3]
Die Besiedlung durch die First Nations begann spätestens vor 12.000 Jahren, die Inuit folgten vor rund 5000 Jahren. Spätestens im 11. Jahrhundert und erneut ab dem späten 15. Jahrhundert erreichten Europäer das heutige Gebiet des Staates und begannen um 1600 mit der Kolonisierung. Dabei setzten sich zunächst Franzosen und Briten fest. Damals breitete sich die Bezeichnung „Canada“ aus, ursprünglich der Name eines Irokesendorfes. Frankreich trat 1763 seine Kolonie Neufrankreich an Großbritannien ab (siehe unten). 1867 gründeten drei britische Kolonien die Kanadische Konföderation. Mit dem Statut von Westminster erhielt der Staat 1931 gesetzgeberische Unabhängigkeit. Weitere verfassungsrechtliche Bindungen zum Vereinigten Königreich wurden 1982 aufgehoben.
Kanada ist ein Königreich innerhalb des Commonwealth of Nations. Nominelles Staatsoberhaupt ist somit König Charles III., der durch den Generalgouverneur von Kanada vertreten wird. Kanada ist ein auf dem britischen Westminster-System basierender parlamentarisch-demokratischer Bundesstaat und eine parlamentarische Monarchie. Amtssprachen sind Englisch und Französisch.
Die Unabhängigkeitsbestrebungen Québecs, die Stellung der frankophonen Kanadier und die Rechte der indigenen Völker (neben den First Nations und Inuit die Métis) sind wichtige Konfliktlinien in Staat und Gesellschaft. Die Themen Klimawandel und Umweltschutz, Einwanderungspolitik und Rohstoffabhängigkeit sowie das Verhältnis zu den Vereinigten Staaten[7] – von dem kulturell und historisch bedingt ein ambivalentes Bild besteht – kennzeichnen die öffentlichen Debatten.
Kanada ist mit einer Fläche von 9.984.670 km² nach Russland der zweitgrößte Staat der Erde und fast so groß wie Europa. Der Staat nimmt rund 41 % Nordamerikas ein. Im Süden und Nordwesten hat Kanada die längste Landgrenze der Welt zu den Vereinigten Staaten. Ein weiterer Nachbar ist das dänische Autonomiegebiet Grönland, das durch die rund 30 Kilometer breite Meerenge Kennedy-Kanal von der nördlichsten kanadischen Insel, Ellesmere Island, getrennt wird. Die winzige Hans-Insel war bis 2022 zwischen beiden Staaten umstritten, ehe sie durch das Ziehen einer etwa 1,2 Kilometer langen Staatsgrenze quer über das Eiland aufgeteilt wurde. Schließlich existiert mit der Inselgruppe Saint-Pierre und Miquelon südlich von Neufundland ein Überbleibsel der französischen Kolonie Neufrankreich.
Die Nord-Süd-Ausdehnung erstreckt sich von 83,11° nördlicher Breite am Kap Columbia auf Ellesmere Island in Nunavut bis zur Insel Middle Island im Eriesee bei 41,68° (etwa die Breite von Rom) und beträgt somit 41,43° oder 4634 Kilometer. Die größte Ost-West-Entfernung beträgt 5514 Kilometer von Cape Spear auf Neufundland (52,62° W) bis zur Grenze des Yukon-Territoriums mit Alaska (141° W). Die Gesamtlänge der Grenze zwischen Kanada und den USA beträgt 8890 Kilometer. Kanada hat mit 243.042 Kilometern zugleich die längste Küstenlinie der Welt. Die größte Insel ist die Baffininsel im Nordosten, welche mit einer Fläche von 507.451 km² zugleich die fünftgrößte Insel der Welt ist. Die nördlichste Halbinsel ist Boothia. 9.093.507 km² Kanadas sind Land- und 891.163 km² Wasserfläche.[2]
Kanada hat Anteil an sechs Zeitzonen, siehe hierzu Zeitzonen in Kanada.
Das geologische Grundgebirge der östlichen Provinzen sind alte, abgetragene Berge neben noch älteren Abschnitten des Kanadischen Schildes, die bis zu 4,03 Milliarden Jahre alt sind.[8] Dieser umfasst eine ausgedehnte Region mit einigen der ältesten Gesteine. Um die Hudson Bay gelegen, nimmt er fast die Hälfte des Staatsgebiets ein. Abgesehen von einigen niedrigen Bergen im östlichen Québec und in Labrador ist die Landschaft flach und hügelig. Das Gewässernetz ist dicht, die Entwässerung der Region erfolgt über eine Vielzahl von Flüssen. Die südliche Hälfte des Schildes ist mit borealen Wäldern bedeckt, während die nördliche Hälfte einschließlich der Inseln des arktischen Archipels jenseits der arktischen Baumgrenze liegt und mit Felsen, Eis und Tundrenvegetation bedeckt ist. Die östlichen Inseln des Archipels sind gebirgig, die westlichen dagegen flach.
Westlich und südlich des Kanadischen Schildes liegen die Ebenen um den Sankt-Lorenz-Strom und die Großen Seen. Die natürliche Vegetation des südlichen Teils der dort liegenden Prärieprovinzen Saskatchewan, Manitoba und Alberta ist das Präriegras. Der nördliche Teil dagegen ist bis zur Tundra-Zone bewaldet.
Die teils vulkanisch aktiven Gebirgszüge der Coast Range und der Rocky Mountains, wie der Mount Edziza oder die Northern Cordilleran Volcanic Province im Norden British Columbias, dominieren das westliche Kanada. Sie verlaufen in Nord-Süd-Richtung durch Yukon und British-Columbia, die dortige Küstenlinie wird tief von Fjorden durchschnitten. Vor der Küste liegt Vancouver Island, ein Ausläufer des Küstengebirges.
Die höchsten kanadischen Gebirgsregionen liegen im Westen mit den Rocky Mountains – höchster Berg ist der 5959 m hohe Mount Logan im Territorium Yukon – und der Kette der Küstengebirge am Pazifischen Ozean (Coast Mountains und Kaskadenkette). Ein weiteres wichtiges System verläuft entlang der Nordostküste von Ellesmere Island (Arktische Kordillere) bis zu den Torngatbergen in Québec sowie in Neufundland und Labrador. Im Osten Kanadas liegen die nördlichen Appalachen und die Laurentinischen Berge.
Der wichtigste Fluss Kanadas ist der 3058 Kilometer lange Sankt-Lorenz-Strom. Er dient als Wasserstraße zwischen den Großen Seen und dem Atlantik. Kanadas zweitlängster Fluss ist der Mackenzie River (1903 Kilometer) in den Nordwest-Territorien. Weitere bedeutende Flüsse sind der Yukon River und der Columbia River, die teilweise auch in den Vereinigten Staaten verlaufen, der Fraser, der Nelson, der Churchill und der Manicouagan sowie Nebenflüsse wie der Saskatchewan River, der Peace River, der Ottawa und der Athabasca.
Kanada ist zudem ein überaus seenreiches Land. 7,6 % seiner Landmasse sind mit insgesamt rund zwei Millionen Seen bedeckt. 563 Seen sind größer als 100 km².[9] Zu den größten Seen gehören der Große Bärensee (31.153 km²), der Große Sklavensee (27.048 km²), der Winnipegsee (24.420 km²), der Athabascasee (7.850 km²) sowie die Großen Seen (zusammen rund 245.000 km²), durch die mit Ausnahme des Michigansees die Grenze zum südlichen Nachbarland verläuft. Der größte gänzlich in Kanada gelegene See ist der Große Bärensee in den Nordwest-Territorien.
Kanada umfasst unterschiedliche Klimazonen (vom Polarklima bis zum gemäßigten Klima). Überwiegend bestimmt das boreale Klima mit langen, kalten Wintern und kurzen, heißen Sommern den größeren Teil Kanadas. Im Winter 2004/2005 wurden Temperaturen von −58 °C in Burwash Landing des Territoriums Yukon gemessen;[10] die tiefste je gemessene Temperatur wurde mit −63 °C in Snag im selben Territorium am 3. Februar 1947 aufgezeichnet.[11] Die höchste Temperatur wurde in Lytton (British Columbia) mit 49,6 °C am 28. Juni 2021 ermittelt.[12]
An der Westküste findet man maritimes Klima mit hohen Niederschlägen, da sich die feuchte, vom Ozean kommende Luft am Westrand des Küstengebirges abregnet. Den Niederschlagsrekord hält Ucluelet in British Columbia mit 489,2 mm an einem einzigen Tag (6. Oktober 1967). Die Jahreszeiten sind in den Provinzen Québec und Ontario am deutlichsten ausgeprägt, mit kalten Wintern, milden Frühjahren und Herbstmonaten und von Juli bis September oft sehr schwül-heißen Sommern mit Durchschnittstemperaturen um 25 °C.
Am häufigsten leiden die Prärieprovinzen Alberta, Saskatchewan und Manitoba unter Trockenheit. Eines der trockensten Jahre war das Jahr 1936, das trockenste jedoch 1961. Regina erhielt 45 % weniger Regen als im Durchschnitt. 1988 war so trocken, dass jeder zehnte Farmer aufgeben musste. Das wärmste Jahr in Kanada war das Jahr 1998.[13]
Große Naturgebiete, vor allem in den Tundra- und Bergregionen, bedecken 70 % Kanadas. Das entspricht 20 % der weltweit verbleibenden Wildnisgebiete (ohne Antarktis). Noch ist mehr als die Hälfte der ausgedehnten Wälder Urwald.[14] Die nördliche Waldgrenze verläuft von der Ostküste Labradors über die Ungava-Halbinsel Richtung Süden entlang des Ostufers der Hudson Bay und setzt sich anschließend schlangenlinienförmig Richtung Nordwesten zum Unterlauf des Mackenzie und weiter nach Alaska fort. Nördlich der Baumgrenze gibt es kaum oder gar keinen fruchtbaren Boden (Tundra). Die Vegetation der südlichsten Tundragebiete besteht aus niedrigem Buschwerk, Gräsern und Riedgras. Die nördlichsten Gebiete sind zu weniger als einem Zehntel mit den für die Polarregion typischen Moosen bedeckt.
Südlich der Baumgrenze, von Alaska bis Neufundland, schließt sich eines der größten Nadelwaldgebiete der Welt an. Im Osten, von den Großen Seen bis zu den Küsten, wachsen hauptsächlich Mischwälder mit Zuckerahorn, Buchen, Birken, Kiefern und Hemlocktannen. Die Tiefebenen im äußersten Süden sind mit reinen Laubwäldern bedeckt. Hier gedeihen neben Hickorybäumen, Eichen und Ulmen, Kastanien, Ahorn und Walnussbäume. In den westlichen Berggebieten sind die Fichte, Douglasie und Lodgepole-Kiefer am weitesten verbreitet, in Hochebenen wachsen außerdem Zitterpappel und Gelb-Kiefer. Die Vegetation der niederschlagsreichen Pazifikküste wird von Wäldern aus dichten, hohen Douglasfichten, westlichen Rot-Zedern und Hemlocktannen beherrscht. Das Prärieland ist zu trocken, um mehr als vereinzelte Baumgruppen hervorzubringen. Vom ursprünglich weiten, hügeligen Grasland ist heute nur noch wenig übrig; es ist dem heute berühmten Weizengürtel Kanadas gewichen.
Die arktischen Gewässer bieten Nahrung für Wale, Walrosse, Seehunde und für Eisbären. In den Tundren leben Moschusochsen, Karibus, Polarwölfe, Polarfüchse, Polarhasen und Lemminge, vereinzelt auch Vielfraße; viele Zugvögel verbringen hier den Sommer, darunter Alke, Enten, Möwen, Seeschwalben und andere Seevögel. Die Wälder im Norden sind ein idealer Lebensraum für Karibus und Elche, Luchse, Schwarz- und Braunbären. Doch gehen die Bestände der riesigen Karibuherden aufgrund von Industrialisierung und winterlichen Freizeitaktivitäten, vor allem aufgrund der Störungen durch motorisierte Schlitten, zurück. Die Bedeutung der Jagd ist hierbei rückläufig.[15]
Fünf Milliarden Vögel kommen jeden Sommer in die borealen Wälder. Daher hat Kanada 1917 zusammen mit den USA angefangen, Schutzgebiete für Zugvögel einzurichten. Heute bestehen 92 solcher Gebiete mit einer Gesamtfläche von etwa 110.000 km².[16] Zur artenreichen Vogelwelt zählen der Kardinal, der Waldsänger, der Weißkopfseeadler und die Spottdrossel sowie der seltene Marmelalk, der nur in alten Wäldern überleben kann.
Biber, Marder, Bisamratten, Nerze sind auch heute noch Grundlage des inzwischen unbedeutenden Pelzhandels. Weiter im Süden findet man Wapitis, während es in dichter besiedelten Landstrichen vor allem kleinere Säugetiere, wie Grau- und Backenhörnchen, Wiesel und Otter gibt. In den Präriegebieten leben kleinere Tiere, wie Präriehasen, Taschenratten und das Spitzschwanzhuhn sowie Bisons und Gabelböcke. In den westlichen Bergen gibt es Dickhornschafe und Schneeziegen.
Die einheimische Tier- und Pflanzenwelt steht in 44 Nationalparks, weit über tausend Provinzparks und Naturreservaten unter Schutz. Größtes Schutzgebiet ist der 44.802 km² große Wood-Buffalo-Nationalpark im nördlichen Teil von Alberta und den Nordwest-Territorien, in dem zahlreiche vom Aussterben bedrohte Arten vertreten sind. Bemerkenswert ist der dortige, mit etwa 6000 Tieren größte Bestand frei lebender Bisons der Welt. In vielen Seengebieten braucht der Mensch besonders im Sommer strenge Vorkehrungen gegen Insektenbisse, da Stech- und Kriebelmücken in sehr hoher Dichte leben.
→ Siehe auch: Liste der Städte in Kanada
Von den über 38 Millionen Einwohnern lebt mehr als die Hälfte der Bevölkerung in den 30 größten Städten. Geht man von den Ballungsräumen (census metropolitan areas) aus, steigt diese Zahl auf über 70 %. Toronto ist das bedeutendste Produktionszentrum und mit 5.928.040 Einwohnern (Stand: 2016) der größte Ballungsraum.[17] Die Handelsmetropole Montreal zählte 4.098.927, Vancouver 2.463.431 Einwohner. Weitere Ballungsräume sind die Bundeshauptstadt Ottawa-Gatineau (1.323.783), Calgary (1.392.609), Edmonton (1.321.426), Québec (800.296), Winnipeg (778.489) und Hamilton (747.545).
Der Name Kanada ist mit hoher Wahrscheinlichkeit vom Wort kanata abgeleitet, das in der Sprache der Sankt-Lorenz-Irokesen „Dorf“ oder besser „Siedlung“ bedeutete.[18] 1535 gaben Bewohner der Region um die heutige Stadt Québec dem französischen Entdecker Jacques Cartier eine Wegbeschreibung zum Dorf Stadacona.[19] Cartier verwendete daraufhin die Bezeichnung Canada nicht nur für dieses Dorf, sondern für das ganze Gebiet, das von dem in Stadacona lebenden Häuptling Donnacona beherrscht wurde. Ab 1545 war auf Karten und in Büchern die Bezeichnung Canada für diese Region üblich. Cartier nannte außerdem den Sankt-Lorenz-Strom Rivière de Canada, ein Name, der bis zum frühen 17. Jahrhundert in Gebrauch war. Forscher und Pelzhändler zogen in Richtung Westen und Süden, wodurch das als „Kanada“ bezeichnete Gebiet wuchs. Im frühen 18. Jahrhundert wurde der Name für den gesamten heutigen mittleren Westen bis Louisiana benutzt. Die seit 1763 britische Kolonie Québec wurde 1791 in Oberkanada und Niederkanada aufgeteilt, was etwa den späteren Provinzen Ontario und Québec entsprach. Sie wurden 1841 wieder zur neuen Provinz Kanada vereinigt.
1867 erhielten die neu gegründeten Bundesstaaten der Kolonien in Britisch-Nordamerika den Namen „Kanada“ und den formellen Titel Dominion. Bis in die 1950er-Jahre war die amtliche Bezeichnung Dominion of Canada üblich.[20]
Mit der zunehmenden politischen Autonomie gegenüber Großbritannien verwendete die Regierung mehr und mehr die Bezeichnung Canada in rechtlich bindenden Dokumenten und Verträgen. Das Kanada-Gesetz 1982 bezieht sich nur noch auf Canada, die inzwischen einzige amtliche (zweisprachige) Bezeichnung.
Indianer (in Kanada First Nations genannt) besiedelten Nordamerika vor mindestens 12.000 Jahren, was den Anfang der paläoindianischen Periode markiert. Vor rund 5000 Jahren folgten die Inuit.[21][22] In den Bluefish-Höhlen im nördlichen Yukon fand man die ältesten menschlichen Spuren in Kanada; in der Charlie-Lake-Höhle fanden sich Werkzeuge aus der Zeit ab etwa 10.500 v. Chr. Aus der Zeit ab etwa 9000 v. Chr. stammen Funde bei Banff und in Saskatchewan, aber auch bereits in Québec.[23]
Ab etwa 8000 v. Chr. folgte die archaische Phase. Gruppen aus dem Westen erreichten um 7500 v. Chr. das südliche Ontario. Dort fanden sich Speerschleudern.[24] Siedlungsschwerpunkte waren im Osten der untere Sankt-Lorenz-Strom und die Großen Seen sowie die Küste Labradors (L’Anse Amour Site) an der im 6. Jahrtausend die ersten größeren Grabstätten entstanden, später Burial Mounds.
Auf den Great Plains entstanden neue Waffentechnologien und weitläufiger Handel, etwa mit Chalzedon aus Oregon und Obsidian aus Wyoming.[25] In einigen Gebieten wurden noch um 8000 v. Chr. Pferde gejagt; sie verschwanden ebenso wie die Megafauna. Erst später teilte sich der riesige Kulturraum erkennbar in zwei Großräume auf, die Frühe Shield- und die Frühe Plains-Kultur, wobei sich Kupferbearbeitung bereits um 4800 v. Chr. zeigen lässt.
Im Westen reichen die Spuren bis vor 8000 v. Chr. zurück, vielfach ohne erkennbaren kulturellen Bruch. So besteht die Kultur der Haida auf Haida Gwaii seit über 9500 Jahren. Der Handel mit Obsidian vom Mount Edziza reicht über 10.000 Jahre zurück.[26]
Vor 2500 v. Chr. bestanden im Westen Siedlungen, dazu Anzeichen sozialer Differenzierung. Hausverbände bestanden, die sich saisonal zur Jagd in großen Gruppen zusammenfanden. Auch in den Plains lassen sich Dörfer nachweisen.
Die Cree, Ojibwa, Algonkin, Innu und Beothuk, die in den frühen europäischen Quellen fassbar sind, gehen wohl auf Gruppen der Shield-Kultur zurück. Die Plainskulturen waren durch Bisons gekennzeichnet, Hunde wurden als Trage- und Zugtiere eingesetzt, das Tipi setzte sich durch sowie die Herstellung von Pemmikan.
Als wichtigste kulturelle Veränderung der Plateaukultur im westlichen Binnenland gilt der Übergang von der Nichtsesshaftigkeit zur Halbsesshaftigkeit mit Winterdörfern und sommerlichen Wanderzyklen um 2000 v. Chr. Eine ähnliche Entwicklung vollzog sich früher an der Küste, deren Kulturen sich mit den Küsten-Salish in Beziehung bringen lassen. Gegen Ende der Epoche lassen sich erstmals Plankenhäuser nachweisen. Einige Salish waren bereits vor 1600 v. Chr. Bauern – wie man von den Katzie weiß.[27] Die Nuu-chah-nulth auf Vancouver Island entwickelten hochseetüchtige Kanus, mit denen sie (als einzige) auf Walfang gingen.
Die Herstellung von Tongefäßen erreichte das Gebiet des heutigen Kanada wohl von Südamerika, Pfeil und Bogen kamen um 3000 v. Chr. aus Asien und wurden wahrscheinlich erstmals von Paläo-Eskimos eingesetzt. Er erreichte die Ostküste, kam aber erst rund drei Jahrtausende später in den Westen.[28]
Mit den Keramikgefäßen ab etwa 500 v. Chr. endete an der Ostküste die archaische Phase, die von den Woodland-Perioden abgelöst wurde. Manche Dörfer, meist aus Langhäusern bestehend, waren wohl schon ganzjährig bewohnt. Auf die Frühe Woodland-Periode an den Großen Seen und dem Sankt-Lorenz-Strom (etwa 1000 v. Chr. bis 500 n. Chr.) gehen wohl die Irokesen zurück, aber auch einige der Algonkin-Gruppen.
Bis nach Zentral-Labrador zeigen sich auf dem kanadischen Schild die Einflüsse der Adena-Kultur. Ihre typischen Mounds erscheinen auch in der westlichen Schild-Kultur, beispielsweise im südlichen Ontario. Wahrscheinlich kam es infolge der Domestizierung von Wildreis zu einer herausgehobenen Schicht von Landbesitzern (Psinomani-Kultur). Der Süden Ontarios war in die Fernhandels-Beziehungen der Hopewell-Kultur eingebunden. Kupfer wurde im ganzen Osten Nordamerikas verbreitet.
Die späte Plains-Kultur lebte in hohem Maße von Bisons. Fernhandel war weit verbreitet und reichte westwärts bis zum Pazifik. Im Norden überwogen kleinere nomadische Gruppen, während sich im Süden ein Zyklus saisonaler Wanderungen durchsetzte, deren Mittelpunkt feste Dörfer waren.
Der späten Plateau-Kultur lieferten die Laichzüge der Lachse die Nahrung, ähnlich wie an der Pazifikküste. Ab 2500 v. Chr. lässt sich das so genannte Pit House („Grubenhaus“) nachweisen, das teilweise in die Erde gegraben wurde und eine bessere Bevorratung ermöglichte.
Die Küstenkultur wurde zwischen 500 v. und 500 n. Chr. als Ranggesellschaft von Süden nach Norden strenger. Eine Schicht führender Familien beherrschte den Handel sowie den Zugang zu Ressourcen und hatte die politische und spirituelle Macht. Auch hier tauchen erstmals Begräbnishügel auf. In einigen Regionen herrschten Steinhaufengräbern (cairns) vor, wie etwa um Victoria. Die Dörfer wurden zahlreicher und vielfach größer, bald stärker befestigt. Die Kultur war von Plankenhäusern, oftmals monumentalen Schnitzwerken (Totempfählen), komplexen Zeremonien und Clanstrukturen gekennzeichnet. Nirgendwo war die Bevölkerungsdichte so groß, wie an der Westküste.
Im Gegensatz dazu gestatteten die Klimabedingungen und starke vulkanische Aktivität im Nordwesten keine dauerhafte Ansiedlung.[29] Mit den Athabasken verbinden sich Fundstellen im Einzugsgebiet des Mackenzie Rivers ab 1000 v. Chr. bis etwa 700 n. Chr.[30]
Gegen 2500 v. Chr. wanderte ein Teil der Paläo-Eskimos von Alaska nach Grönland; es entwickelte sich die Prä-Dorset-Kultur. Um 500 v. Chr. bis 1000 n. Chr. folgte die „Dorset-Kultur“ (nach Cape Dorset auf einer Baffin Island vorgelagerten Insel benannt). Um 2000 v. Chr. bis 1000 n. Chr. bestand die Neo-Eskimo-Kultur. Um 1000 setzte sich eine erneute Wanderung von Alaska nach Grönland in Bewegung. Aus der Vermischung der Kulturen ging wohl die Thule-Kultur hervor, die bis etwa 1800 bestand. Ihre Angehörigen sind die Vorfahren der heutigen Inuit.
Europäische Siedler erreichten Nordamerika spätestens um das Jahr 1000, als Wikinger für kurze Zeit in L’Anse aux Meadows am nördlichsten Ende von Neufundland lebten. Als „Entdecker“ Nordamerikas gilt Giovanni Caboto, ein italienischer Seefahrer in englischen Diensten. Er landete am 24. Juni 1497 auf Neufundland und nahm das Land für England in Besitz. Baskische Walfänger und Fischer kamen ab etwa 1525 regelmäßig an die Küste Labradors und beuteten ein Jahrhundert lang die Ressourcen in der Region zwischen der Neufundlandbank und Tadoussac aus.[31] Eine Expedition unter der Leitung von Jacques Cartier erkundete 1534/35 das Gebiet um den Sankt-Lorenz-Golf und den Sankt-Lorenz-Strom und erklärte es zu französischem Besitz.
Samuel de Champlain gründete 1605 mit Port Royal (heute Annapolis Royal) und 1608 mit Québec die ersten dauerhaften Ansiedlungen in Neufrankreich. Die französischen Kolonisten teilten sich in zwei Hauptgruppen: Die Canadiens besiedelten das Tal des Sankt-Lorenz-Stroms, die Akadier (Acadiens) die heutigen Seeprovinzen. Französische Pelzhändler und katholische Missionare erforschten die Großen Seen, die Hudson Bay und den Mississippi bis nach Louisiana. Engländer gründeten ab 1610 Siedlungen auf Neufundland und besiedelten die weiter südlich gelegenen Dreizehn Kolonien. Cupids Plantation ist damit die zweitälteste angloamerikanische Siedlung in Nordamerika und war erfolgreicher als Jamestown in Virginia.
Zwischen 1689 und 1763 kam es in Nordamerika zu vier bewaffneten Konflikten zwischen Engländern (bzw. Briten) und Franzosen, die jeweils Teil von Erbfolgekriegen in Europa waren. Der King William’s War (1689–1697) brachte keine territorialen Veränderungen, doch nach Ende des Queen Anne’s War (1702–1713) gelangte Großbritannien durch den Frieden von Utrecht in den Besitz von Akadien, Neufundland und der Hudson-Bay-Region. Die Briten eroberten 1745 im King George’s War die französische Festung Louisbourg auf der Kap-Breton-Insel, gaben diese aber 1748 gemäß dem Frieden von Aachen wieder zurück. Der Siebenjährige Krieg (in Nordamerika von 1754 bis 1760 bzw. 1763) brachte schließlich die Entscheidung: Mit dem Pariser Frieden musste Frankreich 1763 fast alle seine Besitzungen in Nordamerika abtreten.
Mit der Königlichen Proklamation von 1763 entstand aus dem ehemaligen Neufrankreich die britische Provinz Québec, im selben Jahr gelangte die Kap-Breton-Insel zur Kolonie Nova Scotia. Auch wurden Rechte der französischen Kanadier eingeschränkt. 1769 wurde eine weitere Kolonie namens St. John’s Island (seit 1798 Prince Edward Island) gegründet. Um Konflikte in Québec abzuwenden, verabschiedete das britische Parlament 1774 den Quebec Act. Das Gebiet Québecs wurde zu den Großen Seen und zum Ohiotal ausgedehnt. Für die französischsprachige Bevölkerungsmehrheit galt das französische Zivilrecht und Französisch war als Sprache in der Öffentlichkeit anerkannt; durch die Zusicherung der freien Religionsausübung konnte die Römisch-katholische Kirche in der Kolonie verbleiben.
Das Gesetz verärgerte jedoch die Bewohner der Dreizehn Kolonien, die darin eine unzulässige Beschränkung ihrer nach Westen gerichteten Expansion sahen. Der Quebec Act war eines jener „unerträglichen Gesetze“ (Intolerable Acts), die schließlich zur Unabhängigkeitserklärung der Vereinigten Staaten und zum Amerikanischen Unabhängigkeitskrieg führten. Der Frieden von Paris erkannte die amerikanische Unabhängigkeit an und die Gebiete südlich der Großen Seen fielen an die Vereinigten Staaten. Etwa 50.000 Loyalisten flohen in das heutige Kanada, dazu kamen mit den Briten verbündete Indianerstämme, wie die Mohawk.[32] New Brunswick wurde 1784 von Nova Scotia abgetrennt, um die Ansiedlung der Loyalisten an der Atlantikküste besser organisieren zu können. Um den nach Québec geflohenen Loyalisten entgegenzukommen, verabschiedete das britische Parlament das Verfassungsgesetz von 1791, das die Provinz Québec in das französischsprachige Niederkanada und das englischsprachige Oberkanada teilte und beiden Kolonien ein gewähltes Parlament gewährte.
Die Spannungen zwischen den Vereinigten Staaten und Großbritannien entluden sich im Britisch-Amerikanischen Krieg (Juni 1812 bis Februar 1815). Der Friede von Gent stellte weitgehend den status quo ante bellum wieder her. In Kanada gilt der Krieg bis heute als erfolgreiche Abwehr amerikanischer Invasionsversuche. Die britisch- und französischstämmige Bevölkerung entwickelte durch den Kampf gegen einen gemeinsamen Feind ein kanadisches Nationalgefühl; die Loyalität der britischen Krone gegenüber wurde gestärkt.
Der Wunsch nach Selbstverwaltung und der Widerstand gegen die wirtschaftliche und politische Vorherrschaft einer kleinen Elite führten zu den Rebellionen von 1837, die rasch niedergeschlagen wurden. Lord Durham empfahl daraufhin in seinem Untersuchungsbericht die Einsetzung einer selbstverantwortlichen Regierung und die allmähliche Assimilierung der französischen Kanadier in die britische Kultur.[33] Der Act of Union 1840 verschmolz Nieder- und Oberkanada zur Provinz Kanada und erhob das Englische zur alleinigen Amtssprache. Bis 1849 erhielten auch die weiteren Kolonien in Britisch-Nordamerika eine eigene Regierung.
Zwei Handelsgesellschaften, die Hudson’s Bay Company (HBC) und die North West Company (NWC), kontrollierten den Handel in den weiten, nur von wenigen Ureinwohnern besiedelten Gebieten der Prärien und der Subarktis. Die HBC hatte 1670 Ruperts Land als Pachtgebiet erhalten und besaß dort das Handelsmonopol mit Pelzen. Da aber auch die NWC dort Fuß zu fassen versuchte, kam es wiederholt zu bewaffneten Auseinandersetzungen. Nach dem Pemmikan-Krieg in der Red-River-Kolonie (heute Manitoba) wurde die NWC 1821 zwangsliquidiert, und die HBC dehnte ihr Monopol auf fast den gesamten Nordwesten des Kontinents aus. 1846 schlossen die Vereinigten Staaten und Großbritannien den Oregon-Kompromiss, der westlich der Großen Seen den 49. Breitengrad als gemeinsame Grenze festlegte. Daraufhin folgte die Gründung der an der Pazifikküste gelegenen Kolonien Vancouver Island (1849) und British Columbia (1858).
Während des Sezessionskriegs in den Vereinigten Staaten erkannten führende Politiker die Notwendigkeit, möglichen amerikanischen Expansionsbestrebungen einen starken Bundesstaat entgegenzustellen, und berieten in drei Verfassungskonferenzen über die Schaffung einer Kanadischen Konföderation. Daraus resultierte das Verfassungsgesetz von 1867, das am 1. Juli 1867 in Kraft trat und das Dominion Kanada schuf, das über eine gewisse Eigenständigkeit gegenüber der Kolonialmacht Großbritannien verfügte. Die Provinz Kanada wurde in Ontario und Québec aufgeteilt, hinzu kamen New Brunswick und Nova Scotia.
Der neue Bundesstaat kaufte 1869 der Hudson’s Bay Company das Nordwestliche Territorium und Ruperts Land ab und vereinigte diese zu den Nordwest-Territorien. Nach der Niederschlagung der Red-River-Rebellion der Métis schuf der Manitoba Act 1870 im Unruhegebiet die Provinz Manitoba. British Columbia und Vancouver Island (die sich 1866 vereinigt hatten) traten 1871 der Konföderation bei, zwei Jahre später folgte Prince Edward Island.
Um den Westen für die Besiedlung durch Einwanderer zu erschließen, beteiligte sich die Regierung an der Finanzierung von transkontinentalen Eisenbahnen und gründete die North-West Mounted Police (heute Royal Canadian Mounted Police), um die staatliche Kontrolle über die Prärien und subarktischen Regionen durchzusetzen. Die Nordwest-Rebellion und die darauf folgende Hinrichtung des Métis-Führers Louis Riel 1885 führten zu einem tiefen Zerwürfnis zwischen den beiden Sprachgruppen. Als direkte Folge des Klondike-Goldrauschs wurde 1898 das Yukon-Territorium geschaffen. Aufgrund der zunehmenden Besiedlung der Prärie entstanden 1905 aus dem südlichen Teil der Nordwest-Territorien die Provinzen Alberta und Saskatchewan. Mit den Indianern schloss Kanada zwischen 1871 und 1921 elf Verträge ab, die ihnen gegen geringe Kompensationen Reservate zuwiesen, ihnen aber ihre gewohnte Lebensweise garantierten. Bis in die 1960er-Jahre versuchte man sie zwangsweise zu assimilieren und verbot den Schülern den Gebrauch ihrer Muttersprachen. Die Ureinwohner durften bis 1960 nicht an Parlamentswahlen auf nationaler Ebene teilnehmen.
An der Seite Großbritanniens nahm Kanada ab 1914 am Ersten Weltkrieg teil und entsandte Freiwillige an die Westfront. Als die Regierung versuchte, gegen den Widerstand des französischsprachigen Bevölkerungsteils den obligatorischen Wehrdienst einzuführen, kam es zur Wehrpflichtkrise von 1917.
Bei den Verhandlungen zum Versailler Vertrag trat Kanada als eigenständiger Staat auf. Es trat 1919 unabhängig von Großbritannien dem Völkerbund bei. Das Statut von Westminster von 1931 garantierte die gesetzgeberische Unabhängigkeit; einige verfassungsrechtliche Bindungen blieben bestehen. Das Land war besonders stark von der Weltwirtschaftskrise betroffen; als Reaktion darauf entwickelte sich in den folgenden Jahrzehnten ein gut ausgebauter Sozialstaat.
Kanada erklärte 1939 dem Deutschen Reich den Krieg. Trotz einer weiteren Wehrpflichtkrise spielten kanadische Truppen während des Zweiten Weltkriegs eine wichtige Rolle, insbesondere in der Atlantikschlacht, der Operation Jubilee, der Invasion Italiens, der Operation Overlord (Landung am Juno Beach) und der Schlacht an der Scheldemündung. Die Regierung von Mackenzie King wagte es nicht, Soldaten gegen deren Willen in einen Kriegseinsatz im Ausland zu schicken. So blieben Männer im Umfang von fünf Divisionen in Kanada, wo sie deutsche Kriegsgefangene bewachten. Unter den kanadischen Freiwilligen, die in Europa gegen Deutschland kämpften, rief das großen Unmut hervor.[34] 1945 wurden kanadische Soldaten maßgeblich während der Kämpfe um die Niederlande eingesetzt.
Die britische Kolonie Neufundland, die sich 1867 nicht dem Bundesstaat angeschlossen hatte und von 1907 bis 1934 ein unabhängiges Dominion gewesen war, trat 1949 nach einer langen politischen und wirtschaftlichen Krise als letzte Provinz der kanadischen Konföderation bei. 1965 wurde die neue Ahornblattflagge eingeführt und seit dem Inkrafttreten des Amtssprachengesetzes 1969 ist Kanada offiziell ein zweisprachiger Staat. Premierminister Pierre Trudeau strebte die vollständige formale Unabhängigkeit von Großbritannien an; diese wurde mit dem Verfassungsgesetz von 1982 und der Charta der Rechte und Freiheiten erreicht.
Während der 1960er-Jahre fand in Québec eine tiefgreifende gesellschaftliche und wirtschaftliche Umwälzung statt, die als „Stille Revolution“ bekannt ist. Québecer Nationalisten begannen, mehr Autonomie oder gar die Unabhängigkeit zu fordern. Nachdem die Front de libération du Québec Entführungen und Anschläge verübt hatte, wurde während der Oktoberkrise 1970 kurzzeitig ein Ausnahmezustand ausgerufen. Moderate Nationalisten stellten ab 1976 die Provinzregierung, 1980 wurde ein erstes Unabhängigkeitsreferendum mit 59,6 % der Stimmen abgelehnt. Ein weiteres Kennzeichen dieser Umwälzung ist die Ablösung der frankophonen Bevölkerung von der katholischen Kirche.
Der Constitution Act / Loi constitutionelle vom 17. April 1982, mit dem auch Verfassungsänderungen nicht mehr vom britischen Parlament abgesegnet werden müssen, gilt als Datum der formalen Unabhängigkeit (vollen Souveränität) Kanadas. 1989 scheiterten Bemühungen der Bundesregierung, Québec mit dem Meech Lake Accord als „sich unterscheidende Gesellschaft“ anzuerkennen. Die vom separatistischen Parti Québécois geführte Provinzregierung setzte 1995 das zweite Unabhängigkeitsreferendum an, das mit 49,4 % Zustimmung knapp scheiterte. 1999 wurde Nunavut geschaffen, das erste kanadische Territorium mit mehrheitlich indigener Bevölkerung.
Die letzte Volkszählung von 2021 ergab eine Einwohnerzahl von 37,0 Millionen.[3] Daraus errechnet sich eine Bevölkerungsdichte von etwa 4,2 Einwohner/km², eine der geringsten der Welt. Die Bevölkerung konzentriert sich zu einem großen Teil auf einem bis zu 350 km breiten Streifen entlang der Grenze zu den USA. Weite Teile des Nordens sind nahezu unbesiedelt. Fast vier Fünftel der Kanadier leben in Städten. Die größten Städte sind Toronto, Montreal, Calgary, Ottawa, Edmonton und Vancouver.
Der Großteil der Bevölkerung lebt in den Provinzen Ontario (14,2 Mio.) und Québec (8,5 Mio.) entlang des St.-Lorenz-Stromes, das heißt rund um Toronto, Montreal, Québec, Ottawa, London und Hamilton (Québec-Windsor-Korridor).[3] 5,0 Mio. Menschen leben in British Columbia, 4,3 Mio. in Alberta, in Manitoba 1,3 Mio. und in Saskatchewan weitere 1,1 Mio. Menschen.[3] Die vier Atlantik-Provinzen haben alle weniger als 1 Million Einwohner. Die bevölkerungsärmsten Territorien Kanadas sind Nunavut, das Yukon-Territorium und das Nordwest-Territorien, die zwischen rund 37.000 und 41.000 Einwohner haben.[3]
Kanada ist ein Einwanderungsland. 2020 waren rund 21 % der Bevölkerung im Ausland geboren.[35] Große Einwanderergruppen kamen in der Vergangenheit aus dem Vereinigten Königreich, Frankreich, Deutschland, Italien, Irland, den Niederlanden, Ungarn, der Ukraine, Polen, Kroatien und aus den USA. Heutzutage wächst die Bedeutung der Einwanderer aus Ostasien, vor allem aus der Volksrepublik China, aus Südasien (Indien und Pakistan), von den Philippinen und aus der Karibik (vor allem Jamaika und Haiti). Von den etwa sechs Millionen deutschen Auswanderern der Jahre 1820 bis 1914 gingen nur 1,3 % nach Kanada, von den 605.000 der Jahre 1919 bis 1933 gingen 5 %, von den 1,2 Millionen der Jahre 1950 bis 1969 bereits 25 % dorthin. 2006 gaben rund 3,2 Millionen Kanadier an, deutscher Herkunft zu sein. Damit sind die Deutschkanadier nach den Einwohnern mit Wurzeln im Raum Großbritannien/Irland und denen mit Wurzeln im heutigen Frankreich die drittgrößte Bevölkerungsgruppe des Landes.[36][37]
Das Bevölkerungswachstum Kanadas von 2016 bis 2021 war mit 5,2 % das höchste unter den G7-Staaten.[38]
Die Lebenserwartung eines neugeborenen Kanadiers lag 2020 bei 82,5 Jahren[39] (Frauen: 84,5[40], Männer: 80,6[41]). 26 % der Kanadier sind 19 Jahre oder jünger, 13 % 65 Jahre oder älter. Der Median des Alters der Bevölkerung lag im Jahr 2020 bei 41,1 Jahren.[42] 2006 waren 4635 Kanadier über 100 Jahre alt.[43]
In Kanada unterscheidet man drei Gruppen indigener oder autochthoner Völker: Die First Nations (auch „Indianer“ genannt), die Inuit und die Métis, Nachfahren von Europäern, die mit indianischen Frauen eine Verbindung eingegangen waren, aber auch NunatuKavummiut, Nachkommen von Inuit und Europäern im Osten Labradors, sowie Nunatsiavut im Norden der Provinz. Zahlreiche weitere Kanadier haben indianische Vorfahren. Deren Ehen wurden sehr häufig nach der „Sitte des Landes“ (custom of the country) geschlossen, also ohne kirchliche oder staatliche Mitwirkung – wie es bei Ehen zwischen Männern der Hudson’s Bay Company und Indianerinnen üblich war. Ehen dieser Art waren erst ab 1867 vollgültig.
Bei der Volkszählung im Jahr 2006 gaben 1.172.790 Kanadier an, Angehörige einer indigenen Gruppe zu sein. Das entsprach 3,8 % der Bevölkerung, wobei dieser Anteil regional sehr stark schwankt. Die Indigenen verteilten sich auf folgende Gruppen:
Im Schnitt sind die Ureinwohner erheblich jünger als die übrige Bevölkerung. So sind 50 % der indianischen Bevölkerung unter 23,5 Jahre alt, im übrigen Kanada liegt dieser als Median bezeichnete Wert bei 39,5 Jahren.
185.960 Kanadier sprachen 2001 eine der 50 indigenen Sprachen, diese umfassen die Sprachen der First Nations[44] sowie Inuktitut, die Sprache der Inuit.
Die Interessen der indigenen Bevölkerung werden staatlicherseits vom „Department of Indian Affairs and Northern Development“/„Affaires indiennes et du Nord“ vertreten, dem das Indianergesetz von 1876 zugrunde liegt. Sie selbst sehen sich allerdings eher in eigenen Organisationen, wie der Versammlung der First Nations oder anderen Organisationen vertreten. Sie berufen sich auf die Verträge, die mit Kanada und Großbritannien geschlossen worden sind, wie die Numbered Treaties, auf allgemeine Menschenrechte und auf Entscheidungen der oberen Gerichtshöfe in Großbritannien und Kanada. Die Indianer besitzen erst seit 1960 das volle Wahlrecht. Ein Teil des besonderen Lebensraumes der Inuit wurde 1999 in ein eigenes Territorium namens Nunavut zusammengefasst.
Seit 1996 wird der 21. Juni als „National Aboriginal Day“ bzw. „Journée nationale des Autochtones“ gefeiert. Zugleich kommt es nach wie vor zu Auseinandersetzungen um Landrechte und den Abbau von Bodenschätzen, wie die Grassy-Narrows-Blockade, der Streit um die Urwälder am Clayoquot Sound an der Westküste oder der Widerstand der Kitchenuhmaykoosib Inninuwug in Ontario zeigen.
Kanadas Amtssprachen sind Englisch und Französisch, wobei 20,1 % der Bevölkerung weder die eine noch die andere als Muttersprache angeben. In der Kanadischen Charta der Rechte und Freiheiten, im Amtssprachengesetz und in den Amtssprachenverordnungen ist die offizielle Zweisprachigkeit festgeschrieben, die vom Amtssprachenkommissariat durchgesetzt wird. In den Bundesgerichten, im Parlament und in allen Institutionen des Bundes sind Englisch und Französisch gleichberechtigt. Die Bürger haben das Recht, Dienstleistungen des Bundes in englischer oder französischer Sprache wahrzunehmen. In allen Provinzen und Territorien wird den sprachlichen Minderheiten der Schulunterricht in eigenen Schulen garantiert – ein Anrecht, das lange umstritten war.[45] Die Ursachen reichen bis in die französische und britische Kolonialisierungsphase Nordamerikas zurück und standen zugleich mit kulturellen und religiösen Gegensätzen in Zusammenhang.
Englisch und Französisch sind die Muttersprachen von 56,9 % bzw. 21,3 % der Bevölkerung,[46] bei 68,3 % bzw. 22,3 % sind es die zu Hause am meisten gesprochenen Sprachen (2006).[47] 98,5 % aller Einwohner sprechen Englisch oder Französisch (67,5 % sprechen nur Englisch, 13,3 % nur Französisch und 17,7 % beides).[48]
Zwar leben 85 % aller französischsprachigen Kanadier in Québec, doch gibt es bedeutende frankophone Bevölkerungsgruppen in Ontario und in Alberta, im Süden von Manitoba, im Norden und Südosten von New Brunswick (Akadier; insgesamt 35 % der Bevölkerung dieser Provinz) sowie im südwestlichen Nova Scotia und auf der Kap-Breton-Insel. Ontario hat die zahlenmäßig größte französischsprachige Bevölkerung außerhalb Québecs. Die Charta der französischen Sprache erklärt Französisch zur alleinigen Amtssprache in Québec, und New Brunswick ist die einzige Provinz, deren Verfassung die Zweisprachigkeit garantiert.[49] Andere Provinzen haben keine Amtssprache als solche definiert; jedoch wird Französisch zusätzlich zu Englisch in Schulen, Gerichten und für Dienstleistungen der Regierung verwendet. Manitoba, Ontario und Québec erlauben das gleichberechtigte Sprechen von Englisch und Französisch in den Provinzparlamenten, und Gesetze werden in beiden Sprachen erlassen. In Ontario kennen einzelne Gemeinden Französisch als zweite Amtssprache. Die Wahl der Hauptstadt des seinerzeitigen Britisch-Nordamerika durch Königin Victoria (1857) fiel möglicherweise deshalb auf Ottawa, weil es etwa an der Grenze zwischen franko- und anglophonem Gebiet lag.
Alle Regionen haben nicht-englisch- oder französischsprachige Minderheiten, hauptsächlich Nachkommen der Ureinwohner. Offiziellen Status besitzen mehrere Sprachen der First Nations in den Nordwest-Territorien. Im hauptsächlich von Inuit bevölkerten Territorium Nunavut ist Inuktitut die Mehrheitssprache und eine von drei Amtssprachen. Mehr als 6,1 Millionen Einwohner bezeichnen weder Englisch noch Französisch als ihre Erstsprache. Am weitesten verbreitet sind Chinesisch (1,012 Millionen Sprecher), Italienisch (etwa 455.000), Deutsch (etwa 450.000), Panjabi (etwa 367.000) und Spanisch (etwa 345.000).[46] Das Kanadisch-Gälische, um die Mitte des 19. Jahrhunderts noch die dritthäufigste Sprache Kanadas, ist mit etwa 500 bis 1000 vorwiegend älteren Sprechern mittlerweile fast ausgestorben,[50]
jedoch bestehen Kontakte zu schottischen Hochschulen, die Kanadiern Sprachkurse anbieten. Mehrere Schulen unterrichten die Sprache, ebenso drei Hochschulen sowie die 2006 gegründete Atlantic Gaelic Academy.[51] Erst ab 1973 wurden in Ontario deutsche Schulen vom Staat wieder unterstützt. Zwischen 1977 und 1990 erhielten die Schulen Mittel aus dem Multikulturalismusprogramm der Regierung.
Mit der Kolonialisierung kamen zunächst vor allem französische Katholiken und anglikanische Engländer nach Kanada. Darüber hinaus förderte Großbritannien die Einwanderung protestantischer Gruppen vom Mittelrhein und aus Württemberg, in geringerem Maße auch aus der Schweiz, Frankreich und den Niederlanden, sodass der Süden von Nova Scotia bis heute protestantisch ist.
Doch gab die Kolonialmacht 1774 mit dem Quebec Act jeden Versuch auf, die Katholiken zur Konversion zu bewegen. Nach der Unabhängigkeit der USA kamen zahlreiche protestantische Loyalisten in das heutige Ontario und bildeten dort die Mehrheit. In späteren Einwanderungswellen kamen wiederum katholische Iren und Italiener, aber auch ukrainische Duchoborzen hinzu. Die Einwanderung aus Schottland sorgte wiederum für eine Beseitigung des Vorrangs der Anglikanischen Kirche im Osten durch zahlreiche Presbyterianer. In Toronto setzten sich die Methodisten durch.
In Opposition zu den Katholiken, die eher dem Ultramontanismus zugeneigt waren (les bleus), aber auch zu den dominierenden Anglikanern, die vom Oranier-Orden unterstützt wurden, bildeten sich antiklerikale Gruppen (vor allem les rouges). Mit dem Lord’s Day Act von 1906 wurde ein weitgehendes Arbeitsverbot am Sonntag durchgesetzt, das bis in die 1960er-Jahre Gültigkeit beanspruchte und das der Oberste Gerichtshof erst 1985 endgültig abschaffte.[52] Eine ähnliche Bedeutungsminderung des Religiösen im Alltag fand in Québec statt. Dennoch gibt es bedeutende Gruppen, insbesondere im Süden Manitobas und Ontarios, in Alberta und im Binnenland von British Columbia. Dazu zählen die Mennoniten im Süden Manitobas, die ukrainischen Orthodoxen und Katholiken in Manitoba und Saskatchewan, die Mormonen bilden einen Schwerpunkt in Alberta. Hinzu kommen die Zeugen Jehovas und zahlreiche andere Gruppen.[53]
Die katholischen Missionare waren unter den Ureinwohnern erfolgreicher als die protestantischen, und so überwiegt dort der katholische Anteil. Dazu kommen indigene Glaubensorganisationen, wie die Shaker Church.
Mit den jüngsten Einwanderungswellen verstärkten sich nichtchristliche Religionsgemeinschaften wie Hindus, Muslime, Juden, Sikhs und Buddhisten. Sie konzentrieren sich in Großstädten, insbesondere im Großraum Toronto. Die älteste Synagoge, Congregation Emanu-El, entstand 1863 in Victoria, die erste Moschee 1938 mit der Al Rashid Mosque in Edmonton.
Etwa 67,3 % der kanadischen Bevölkerung gehörten 2011 einer christlichen Konfession an (39,0 % katholisch, etwa 24,1 % protestantisch). Die beiden größten protestantischen Glaubensgemeinschaften sind mit 6,1 % die United Church of Canada und mit 6,9 % die Anglikanische Kirche von Kanada, dazu kommen 1,9 % Baptisten, 1,4 % Lutheraner, etwa 1,7 % Orthodoxe sowie etwa 3,0 % andere christliche Glaubensgemeinschaften. Muslime stellen etwa 3,2 % der Bevölkerung, mehr als die Hälfte von ihnen lebt in Ontario. Etwa 1,0 % sind Juden, von denen wiederum knapp 60 % in Ontario leben, und etwa 1,1 % Buddhisten, 1,5 % Hindus sowie 1,4 % Sikhs. Etwa 23,9 % gaben an, keiner Glaubensgemeinschaft anzugehören.[54][55]
Die Volkszählungen von 2011, 2001 und 1991 ergaben:[55][56][57][58][59] Zu beachten bei der Prozentzahl „Veränderung 1991–2011“ (rechte Spalte) ist, dass die Gesamtbevölkerung des Staates in diesen 20 Jahren erheblich zugenommen hat; der „Zuwachs“ etwa bei den Katholiken relativiert sich damit erheblich.
Besonders schnell wachsen durch Zuwanderung die nicht-christlichen Gruppen, aber auch zahlreiche christliche Gruppen, die außerhalb der großen Kirchen stehen. Nach einer Umfrage von 2007 fühlten sich die Muslime in Kanada deutlich stärker integriert als in europäischen Staaten.[60] Insgesamt setzt die kanadische Politik im Rahmen ihrer Integrationspolitik stärker auf Erhalt und Nutzung der ethnischen und religiösen Besonderheiten als auf Anpassung.
Seit den 1960er-Jahren begann ein Wandel der Schulpolitik, die bis dahin auf Segregation basierte. Mit dem Canadian Multiculturalism Act von 1988 wurde diese formal beendet.[61][62][63]
Kanada hat, gemessen an der Bevölkerung, eine der höchsten Einwanderungsraten unter den Flächenstaaten der Welt.[2] Die Einwanderung wird über definierte Ziele gesteuert, die in einem Programm festgelegt worden sind. Hierbei gibt es etwa Programme für Flüchtlinge, zur Zuwanderung in den Arbeitsmarkt, für Existenzgründer und zum Familiennachzug.[64] Die Einwanderungskriterien sind öffentlich einsehbar und können bereits vor Antragstellung selbst überprüft werden. Für Menschen mit Berufen, die in Kanada gefragt sind, existiert zum Beispiel das Skilled Worker-Programm. Je nach Lage des Arbeitsmarkts wird eine Mindestpunktzahl festgelegt, die ein Einwanderungsinteressierter erreichen muss. Die persönliche Punktzahl setzt sich aus Punkten für den aktuellen Bildungsstand und die Berufserfahrung zusammen, aus Punkten für die vorhandenen Sprachkenntnisse in Englisch und Französisch sowie für das Alter, für Verwandte und frühere Aufenthalte in Kanada. Ein verbindliches Arbeitsangebot eines kanadischen Arbeitgebers erhöht die Punktzahl nochmals maßgeblich. Das Immigrations-Programm wurde am 1. Juli 2011 dahingehend angepasst, dass ohne ein bestehendes Arbeitsangebot nur noch Personen zum Skilled Worker-Programm zugelassen werden, die Erfahrung in einem von 29 festgelegten Berufen nachweisen können.[65] Daneben muss ein Interessent am Skilled Worker-Programm nachweisen, dass er sich für eine gewisse Zeit finanziell selbst versorgen kann. Die notwendige Summe beläuft sich derzeit (September 2011) für eine alleinstehende Person auf 11.115 CAD, für eine vierköpfige Familie auf 20.654 CAD.[66] Außerdem werden polizeiliche Führungszeugnisse aus allen Ländern benötigt, in denen der Kandidat nach dem 18. Geburtstag für sechs Monate oder länger gelebt hat.
Bei Fachkräften, die nach Kanada einwandern wollen, wird vor allem auf gute Sprachkenntnisse, eine Jobzusage und ein geringes Alter geachtet.[67] Die Einwanderung erfolgt in zwei Stufen. Zunächst wird eine unbefristete Aufenthalts- und Arbeitsgenehmigung erteilt.[67] Nach drei Jahren als „Permanent Resident“ und entsprechendem Aufenthalt im Land kann der Einbürgerungsantrag gestellt werden. Einwanderer, die noch nicht eingebürgert sind, haben Residenzpflicht. Dies bedeutet, dass man Nachweise für die vorgegebene Zeit in Kanada erbringen, oder mit jemandem verheiratet sein muss, die oder der die kanadische Staatsbürgerschaft besitzt. Bei Verstößen kann der „Permanent Resident“-Status entzogen und der Einwanderer in sein Herkunftsland zurückgeschickt werden.
Neben dem Programm für qualifizierte Einwanderungswillige steht eine gesonderte Regelung für Gastarbeiter, die keine Perspektive für eine Einbürgerung bekommen.[68] Die Zahl der nur zeitweilig in Kanada zugelassenen Arbeitskräfte übersteigt seit etwa 2006 die der Einwanderer. Die Gastarbeiter erhalten Arbeitsgenehmigungen, die in der Regel für einige Monate gelten und nur selten die Dauer eines Jahres übersteigen. Sie gelten nur für den Arbeitgeber, der die Arbeitskräfte ins Land holt, eine Kündigung ist mit dem Verlust der Aufenthaltsgenehmigung verbunden. Während das Programm für Gastarbeiter ursprünglich für Pflegekräfte in Haushalten, Kindermädchen und Arbeiter in der Landwirtschaft eingeführt wurde, wird es inzwischen für alle Tätigkeiten des Niedriglohnbereichs eingesetzt.
Außer den Programmen zur Einwanderung in den Arbeitsmarkt gibt es in Kanada auch humanitäre Aufnahmeprogramme zum Resettlement von Menschen, die vom Flüchtlingshilfswerk der Vereinten Nationen (UNHCR) als Flüchtlinge anerkannt wurden.[67][69] Noch vor der Einreise werden diese sogenannten Kontingentflüchtlinge einem Gesundheits- und Sicherheitscheck unterzogen, inklusive Iris-Scan zur eindeutigen Identifizierung. Unbegleitete Minderjährige bekommen keine Plätze, dafür bevorzugt Familien und Frauen.[67] 2018 war Kanada der Staat mit dem weltweit größten Aufnahmeprogramm von Resettlement-Flüchtlingen.[70] Jedes Jahr legt die kanadische Regierung genaue Kontingente für die Resettlement-Programme fest. Rund ein Drittel der gut 30.000 Plätze im Jahr 2019 wurden vom Staat finanziert, die restlichen Kontingentflüchtlinge wurden ganz oder teilweise von Organisationen und Privatleuten unterstützt.[67]
Mehr als 90 Prozent der Arbeitsmigrantinnen- und migranten sprechen bereits vor der Einreise Englisch, Französisch oder beides. Unter den Kontingentflüchtlingen und ihren Familien sind es 54 Prozent, auf die eines der drei Dinge zutrifft. Der weitaus größte der Teil der 341.180 Menschen, die im Jahr 2019 eine dauerhafte Aufenthaltserlaubnis für Kanada erhielten, waren Fachkräfte. neben guten Sprachkenntnisse, eine Jobzusage und ein geringes Alter vorwiesen.[67]
Menschen, die – über Grenze der USA – Kanada betreten und Asyl beantragen und keine Qualifikationen mitbringen, werden meist in die USA abgeschoben. Wird ein Asylbewerber jedoch anerkannt, bekommt er grundsätzlich eine dauerhafte Aufenthaltserlaubnis – bekommt aber keine Wohnung vom Staat zugewiesen. Notfalls erfolgt die Unterbringung in einem Gefängnis. Geflüchtete machen 14 Prozent aller Zuwanderer in Kanada aus.[67]
Kanada ist formal eine konstitutionelle Monarchie innerhalb des Commonwealth of Nations mit König Charles III. als Staatsoberhaupt. Er trägt den Titel König von Kanada und wird durch den Generalgouverneur vertreten.[72][73] Der Staat ist auch eine repräsentative parlamentarische Demokratie, die in Form eines Bundesstaates organisiert ist. Die Verfassung Kanadas besteht aus schriftlichen Rechtsquellen und ungeschriebenem Gewohnheitsrecht.[74] Das Verfassungsgesetz von 1867 enthält das Staatsorganisationsrecht, begründete ein auf dem Westminster-System des Vereinigten Königreichs basierendes parlamentarisches Regierungssystem und teilte die Macht zwischen Bund und Provinzen auf. Das Statut von Westminster von 1931 gewährte die vollständige gesetzgeberische Autonomie, und mit dem Verfassungsgesetz von 1982 wurden die letzten verfassungsrechtlichen Bindungen zum britischen Mutterstaat gelöst. Letzteres enthält einen Grundrechtskatalog (die Kanadische Charta der Rechte und Freiheiten) sowie Bestimmungen betreffend das Vorgehen bei Verfassungsänderungen.[74] Einhergehend mit dem Status als Monarchie gibt es eine Reihe von Titeln und Orden, die in Kanada verliehen werden.
Theoretisch liegt die exekutive Staatsgewalt beim Monarchen, wird aber in der Praxis durch das Kabinett (formal ein Komitee des kanadischen Kronrates) und durch den Vertreter des Monarchen, den Generalgouverneur, ausgeübt. Der Monarch und dessen Vertreter sind unpolitisch und üben überwiegend zeremonielle Funktionen aus, um die Stabilität der Regierung zu garantieren. Gemäß Gewohnheitsrecht übergeben sie alle politischen Geschäfte ihren Ministern im Kabinett, die ihrerseits gegenüber dem gewählten Unterhaus verantwortlich sind. Die exekutive Staatsgewalt liegt somit de facto beim Kabinett, jedoch können Monarch und Generalgouverneur im Falle einer außergewöhnlichen Verfassungskrise ihre Hoheitsrechte wahrnehmen.
Der Premierminister ist üblicherweise der Vorsitzende jener Partei, die im Unterhaus die meisten Sitze hält und das Vertrauen der Mehrheit der Abgeordneten besitzt. Er wird vom Generalgouverneur eingesetzt und führt als Regierungschef das Kabinett an. Da er über weitgehende Befugnisse verfügt, gilt er als mächtigste Person des Staates. Er ernennt die übrigen Kabinettsmitglieder, Senatoren, Richter des Obersten Gerichtshofes, Vorsitzende von Staatsbetrieben und Behörden und kann den Generalgouverneur sowie die Vizegouverneure der Provinzen vorschlagen. Die Bundesregierung ist unter anderem zuständig für Außenpolitik, Verteidigung, Handel, Geldwesen, Verkehr und Post sowie die Aufsicht über die Administration der drei bundesabhängigen Territorien. Aktuell ist der Vorsitzende der Liberalen Partei, Justin Trudeau, seit dem 4. November 2015 Premierminister und leitet das 29. Kanadische Kabinett.
Vom 2. Oktober 2017 bis zum 21. Januar 2021 war Julie Payette die 29. Generalgouverneurin von Kanada. Sie trat nach Abschluss einer unabhängigen Untersuchung über das von ihr geschaffene Arbeitsumfeld als Generalgouverneurin zurück.[75] Während der Zeit bis zur Ernennung einer Nachfolgerin wurden die Aufgaben durch Richard Wagner, Vorsitzender des Obersten Gerichtshofes von Kanada, als „Administrator of the Government of Canada“ wahrgenommen.[76] Am 6. Juli 2021 wurde die Ernennung von Mary Simon zur neuen Generalgouverneurin von Kanada bekannt gegeben.[1] Simon ist die erste Inuk die zum Vertreter des Königs ernannt wurde.[77]
Das kanadische Bundesparlament besteht aus dem Monarchen und zwei Kammern, dem demokratisch gewählten Unterhaus (englisch House of Commons, frz. Chambre des communes) und dem ernannten Senat von Kanada (Senate of Canada, Sénat de Canada).
Jedes Mitglied des Unterhauses wird im relativen Mehrheitswahlrecht in einem von 338 Wahlkreisen gewählt. Allgemeine Wahlen werden vom Generalgouverneur angesetzt, wenn der Premierminister dies so vorschlägt oder wenn ein Misstrauensvotum gegen die Regierung die benötigte Mehrheit erreicht. Gemäß einem 2006 verabschiedeten Gesetz beträgt die Dauer der Legislaturperiode vier Jahre. Zuvor konnte der Premierminister den Wahltermin nach Belieben festsetzen, doch musste eine Neuwahl spätestens nach fünf Jahren erfolgen. Die Regierung stellt zurzeit die Liberale Partei, während die Konservative Partei die Rolle der „offiziellen Opposition“ innehat. Weitere im Parlament vertretene Parteien werden als „Drittparteien“ bezeichnet. Es sind dies die Neue Demokratische Partei, der Bloc Québécois und die Grüne Partei.
Im Senat von Kanada, auch „Oberhaus“ (englisch upper house, frz. chambre haute) genannt, sitzen 105 Abgeordnete, die der Generalgouverneur auf Empfehlung des Premierministers ernennt. Die Sitze sind nach Regionen aufgeteilt, wobei diese seit 1867 nicht mehr angepasst wurden und deshalb große Disproportionalitäten in der Repräsentation im Verhältnis zur Einwohnerzahl bestehen. Die Senatoren haben keine feste Amtszeit, sondern können ihr Amt bis zum 75. Lebensjahr wahrnehmen. Der Einfluss des Senats ist bedeutend geringer als jener des Unterhauses.
Die Bundesstaaten führten das Frauenwahlrecht ab 1916 nacheinander und zum Teil früher ein, als dies auf Bundesebene der Fall war.[78][79][80] Schlusslicht war Québec: Das Gesetz, das auch Indianern das Wahlrecht verschaffte, wurde erst am 9. April 1949 ins Parlament eingebracht und trat am 25. April 1949 in Kraft.[81][82]
1917 wurde das aktive Wahlrecht auf nationaler Ebene vor dem Hintergrund des Krieges durch den Wartime Elections Act bestimmten Gruppen von Frauen zugestanden, über deren genaue Zusammensetzung in der Literatur Unterschiedliches zu finden ist: Krankenschwestern, die im Krieg Dienst taten;[83] euroamerikanische Frauen, die in der Armee arbeiteten oder dort nahe Angehörige (Vater, Ehemann oder Sohn) hatten oder deren Väter, Männer oder Söhne im Krieg getötet oder verwundet worden waren;[84] Frauen, deren Ehemänner, Söhne oder Väter im Krieg getötet oder verwundet worden waren;[83] eine weitere Quelle[85] nennt zusätzlich die Anforderung, dass die zugelassenen Frauen auf der Ebene der ihres Bundesstaates wahlrechtlich Männern gleichgestellt waren.
Am 24. Mai 1918 wurde das aktive nationale Wahlrecht auf alle Frauen britischer und französischer Abstammung ab 21 Jahren ausgedehnt, womit gleiche Kriterien für Frauen und Männer galten.[79][83][84] Indianer waren ausgeschlossen.[82]
1919 erhielten Frauen das passive Wahlrecht.[86] Zwar nennen andere Quellen hierfür spätere Daten[84] und sprechen von einem beschränkten Wahlrecht;[87] doch beruht dies vermutlich darauf, dass erst 1929 in einem von The Famous Five angestrengten Gerichtsverfahren endgültig geklärt wurde, dass das passive Wahlrecht in der Verfassung auch für den Senat galt, nicht nur für das House of Commons.[88]
1920 wurden die Eigentumsbeschränkungen aufgehoben.[86]
1950 und 1951 wurde durch Änderungen am Indian Act und am Canada Elections Act das aktive Wahlrecht auf nationaler Ebene auf Veteranen aus dem Kreis der Indianer und ihre Ehefrauen sowie Indianer, die normalerweise außerhalb der Reservate lebten, ausgedehnt, wenn sie auf die Steuerbefreiungen verzichteten, die ihnen der Indian Act gewährte.[89] 1950 hatten die Inuit das Wahlrecht erhalten, 1951 alle Bewohner der Nordwest-Territorien. Wahlurnen für die Inuit wurden in der östlichen Arktis erst 1962 aufgestellt.[90]
Erst im August 1960 wurde das Wahlrecht mit dem Act to Amend the Canada Elections Act auf alle Kanadier ausgedehnt.[91][92] Frauen das passive Wahlrecht.[86] Zwar nennen andere Quellen hierfür spätere Daten[84] und sprechen von einem beschränkten Wahlrecht;[87] doch beruht dies vermutlich darauf, dass erst 1929 in einem von The Famous Five angestrengten Gerichtsverfahren endgültig geklärt wurde, dass das passive Wahlrecht in der Verfassung auch für den Senat galt, nicht nur für das House of Commons.[88]
Kanadas Rechtssystem spielt eine wichtige Rolle bei der Interpretation von Gesetzen. Es berücksichtigt die sich verändernden gesellschaftlichen Gegebenheiten und hat die Macht, Gesetze zu widerrufen, die gegen die Verfassung verstoßen. Der Oberste Gerichtshof ist das höchste Gericht und die letzte Instanz. Die neun Mitglieder werden auf Vorschlag des Premierministers und des Justizministers vom Generalgouverneur ernannt. Vorsitzender des Obersten Gerichtshofes (Chief Justice of Canada, Juge en chef du Canada) ist seit 2017 Richard Wagner. Die Bundesregierung ernennt auch Richter der Obersten Gerichte der Provinzen und Territorien. Die Besetzung von Richterämtern auf unteren Stufen fällt in die Zuständigkeit der Provinz- und Territorialregierungen.
In den Provinzen sind die obersten Gerichte die Courts of Appeal. Ihre Urteile sind allerdings, im Gegensatz zu denen des Obersten Gerichtshofs in Ottawa, in den anderen Provinzen nicht bindend, wenn sie auch nicht ohne Einfluss sind. Als weitere Rechtsquelle gelten gelegentlich noch immer der Londoner Court of Appeal und das britische House of Lords. Deren Entscheidungen aus der Zeit vor 1867 sind immer noch bindend, es sei denn, der kanadische Oberste Gerichtshof hat sie aufgehoben. Das Gleiche gilt für Entscheidungen bis 1949 für den Rechtsprechungsausschuss des Privy Council. Dies ist für die Rechtsstellung der indigenen und der frankophonen Bevölkerung von erheblicher Bedeutung, da ältere Verträge mit der britischen Krone weiterhin gültig sind.
Kanada ist ein in zehn Provinzen und drei Territorien gegliederter Bundesstaat. Diese subnationalen Einheiten können in geographische Regionen gegliedert werden. Westkanada besteht aus British Columbia und den drei Prärieprovinzen Alberta, Saskatchewan und Manitoba. Zentralkanada umfasst die zwei bevölkerungsreichsten Provinzen Ontario und Québec. Als Seeprovinzen werden New Brunswick, Prince Edward Island und Nova Scotia bezeichnet; zusammen mit Neufundland und Labrador bilden sie die Atlantischen Provinzen. Die drei Territorien Yukon, Nordwest-Territorien und Nunavut umfassen sämtliche Gebiete nördlich des 60. Breitengrades und westlich der Hudson Bay.
Die Provinzen verfügen über einen hohen Grad an Autonomie, wogegen in den Territorien die Bundesregierung zahlreiche Verwaltungsaufgaben selbst übernimmt. Alle Provinzen und Territorien besitzen ein Einkammerparlament und einen Premierminister als Regierungschef. Der kanadische Monarch wird in allen Provinzen durch einen Vizegouverneur[98] vertreten, der gleichrangig mit dem Generalgouverneur ist und überwiegend zeremonielle Aufgaben wahrnimmt. In den Territorien übernimmt ein von der Bundesregierung ernannter Kommissar die Aufgaben eines Vizegouverneurs.
Während in den meisten Bundesverfassungen föderaler Staaten allein die Gesetzgebungskompetenzen des Bundes explizit aufgezählt werden, führt das Verfassungsgesetz von 1867 (englisch Constitution Act, 1867, frz. Loi constitutionnelle de 1867) nicht nur in Art. 91 die ausschließlichen Kompetenzen des Bundes, sondern in den Artikeln 92, 92A und 93 auch die ausschließlichen Kompetenzen der Provinzen auf. Hiernach verfügen die Provinzen über das Gesetzgebungsrecht u. a. in den Bereichen direkte Steuern, Beamtenbesoldung, öffentliche Einrichtungen, Gemeindewesen, Schulwesen, Gast- und sonstiges lokales Gewerbe, Eigentum und bürgerliches Recht, Gerichtsverfassungsrecht, Zivilprozessrecht, Bergbau, Forstwirtschaft und Energie.
1974 gab es Bestrebungen im kanadischen Parlament, das britische Überseegebiet der Turks- und Caicosinseln in der Karibik als elfte Provinz in den kanadischen Staatsverband aufzunehmen.[100] Der Gesetzesvorschlag fand jedoch keine Mehrheit und wurde somit abgelehnt. Seit 2003 gibt es jedoch erneute Bestrebungen in diese Richtung. Dafür müsste jedoch erstens Großbritannien die Inseln in die Unabhängigkeit entlassen und zweitens jede einzelne kanadische Provinz zustimmen. Insbesondere Letzteres ist infolge der sehr komplizierten kanadischen Verfassungsprozeduren indes wenig wahrscheinlich.
Zwar ist Kanada ein relativ junger Staat, die Rechtsordnung hat jedoch eine lange Tradition. Das in allen Provinzen mit Ausnahme Québecs geltende Common Law basiert auf Grundsätzen, die sich während Jahrhunderten in England entwickelten und ein Erbe der britischen Kolonialzeit sind. Der in Québec im Bereich des Privatrechts geltende Code civil spiegelt Prinzipien des französischen Rechtssystems wider. Das Strafrecht hingegen ist Sache des Bundesstaates und in allen Provinzen einheitlich. Im Laufe der Zeit wurden beide Rechtssysteme den Erfordernissen in Kanada angepasst.
Beide Rechtssysteme sind in die Verfassung eingeflossen. Deren Kern entstand 1867 mit der Gründung Kanadas und wurde zuletzt 1982 grundlegend durch das Verfassungsgesetz von 1982 und die Kanadische Charta der Rechte und Freiheiten ergänzt.
Kanada schaffte 1976 die Todesstrafe für Verbrechen in Friedenszeiten ab, 1998 auch im Kriegsstrafrecht. Auslöser war die 1959 erfolgte Verurteilung des damals 14-jährigen Steven Truscott zum Tode. Er wurde nach zehn Jahren Haft auf Bewährung entlassen und 2007 freigesprochen.[101]
Die Strafverfolgung fällt in die Verantwortung der Provinzen. Die Polizeibehörden sind mehrstufig aufgebaut. Die Royal Canadian Mounted Police (Abkürzung RCMP, umgangssprachliche Kurzbezeichnung Mounties, französisch Gendarmerie royale du Canada, GRC) ist die nationale Polizei. Die beiden größten Provinzen verfügen mit der Ontario Provincial Police (OPP) bzw. der Sûreté du Québec über eigene Provinzpolizeien, dort beschränkt sich der Auftrag der RCMP auf den Schutz von Bundeseinrichtungen.
Daneben gibt es weitere Polizeibehörden auf Provinzebene (z. B. British Columbia Sheriff Service, Royal Newfoundland Constabulary) und auf regionaler oder örtlicher Ebene (z. B. Toronto Police Service, York Regional Police). Ferner gibt es auf Bundesebene Polizeibehörden mit speziellen Aufgaben (z. B. Parks Canada Warden). Ähnlich den Vereinigten Staaten existieren für die Gebiete von Indianerstämmen und anderen Ureinwohnern eigene Polizeibehörden. Die beiden großen privaten Eisenbahngesellschaften (CP und CN) verfügen über je eine eigene Polizei zur Sicherung ihrer Einrichtungen. Einige Nahverkehrbetreiber sowie manche Universitäten haben eigene Hilfspolizeien (sogenannte Special Constables) eingerichtet.
Die Vereinigten Staaten und Kanada teilen sich die längste nicht verteidigte Staatsgrenze der Welt. Die Kooperation auf militärischem und wirtschaftlichem Gebiet ist eng; so sind beide Länder im Rahmen des Nordamerikanischen Freihandelsabkommens jeweils der größte Handelspartner des anderen.
Dennoch betreibt Kanada eine eigenständige Außenpolitik. Es unterhält diplomatische Beziehungen zu Kuba und beteiligte sich nicht am Vietnam- oder am Irakkrieg. Enge Beziehungen unterhält der Staat traditionell zum Vereinigten Königreich und zu Frankreich, über die Mitgliedschaft im Commonwealth of Nations und in der internationalen Organisation der Frankophonie auch zu anderen ehemaligen britischen und französischen Kolonien. Ein weiterer Schwerpunkt der außenpolitischen Beziehungen sind die Staaten der Karibischen Gemeinschaft. Im 2005 veröffentlichten International Policy Statement legte die Regierung die Leitlinien der Außenpolitik fest. Kanada sieht die Europäische Union als strategischen Partner in den Bereichen Klimawandel, Energieversorgung, Handel und Umweltschutz sowie bei außen- und sicherheitspolitischen Themen. Die Beziehungen zu Deutschland sind gut und von gemeinsamen Werten und Grundüberzeugungen geprägt.[102] Seit 2022 finden jährliche bilaterale Treffen auf der Regierungsebene in Form der German-Canadian High Level Steering Group on Bilateral Cooperation (HLSG).[103]
Einen wichtigen Teil der kanadischen Identität bildet die Unterstützung der Multilateralität.[104] 1945 gehörte Kanada zu den Gründungsmitgliedern der Vereinten Nationen. Der spätere Premierminister Lester Pearson trug wesentlich zur Beilegung der Sueskrise bei und wurde 1957 dafür mit dem Friedensnobelpreis ausgezeichnet. Sprach man bis dahin von der „Geburt der kanadischen Nation auf den Schlachtfeldern Europas“,[105] so entwickelte sich unter dem Eindruck zahlreicher UN-Blauhelmeinsätze ein Peacekeeping-Mythos, der Kanadas Rolle in Abgrenzung zu den USA begreift.
„Kanadier waren Mittelsmänner, ehrliche Makler, nützliche Helfer in einer Welt, in der diese Eigenschaften rar waren. Die Friedenssicherung veränderte uns, machte uns auf eine gewisse Weise besser.“
Kanada ist Mitglied zahlreicher internationaler Organisationen wie der OSZE, der Welthandelsorganisation, der OECD, der OAS, der APEC und der Gruppe der Sieben (G7).
Verschiedene internationale Vereinbarungen entstanden auf kanadische Initiative und wurden in diesem Land verabschiedet. Dazu gehören die Ottawa-Konvention zum Verbot von Antipersonenminen und das Montreal-Protokoll zum Schutz der Ozonschicht.
Die kanadischen Streitkräfte (englisch Canadian Forces, frz. Forces canadiennes) entstanden in ihrer jetzigen Form 1968, als Heer, Marine und Luftwaffe organisatorisch zusammengeführt wurden. Die Truppen umfassten 2020 rund 67.490 freiwillige Berufssoldaten und rund 31.000 Reservisten.[107] Hinzu kamen (Stand 2012) 5000 Canadian Rangers, deren Hauptaufgabe es ist, in entlegenen arktischen Gebieten militärische Präsenz zu zeigen.[108] Die Streitkräfte verfügen über rund 1400 gepanzerte Fahrzeuge, 34 Kriegsschiffe und 300 Kampfflugzeuge. Kanada gab 2017 knapp 1,3 Prozent seiner Wirtschaftsleistung oder 20,6 Mrd. US-Dollar für seine Streitkräfte aus und lag damit weltweit auf Platz 14.[109]
Aufgrund der engen Bindungen an das britische Mutterland waren kanadische Truppen am Burenkrieg, am Ersten Weltkrieg und am Zweiten Weltkrieg beteiligt. Seit 1948 stellt Kanada einen bedeutenden Teil der Friedenstruppen der Vereinten Nationen und war an mehr Friedensmissionen beteiligt als jede andere Nation (seit 1989 ohne Ausnahme).[110] Der Staat beteiligt sich grundsätzlich nur an kriegerischen Handlungen, die von den Vereinten Nationen sanktioniert wurden, wie etwa am Krieg in Korea, am Persischen Golf, in Afghanistan, jedoch ohne UN-Mandat im Kosovo. Kanada ist Gründungsmitglied der NATO und Vertragspartner des nordamerikanischen Luftraumverteidigungsbündnisses NORAD.
Im föderalistischen Kanada gibt es kein einheitliches nationales Bildungssystem, jedoch unterliegt der tertiäre Bildungsbereich einer einheitlichen staatlichen Qualitätskontrolle und die meisten kanadischen Universitäten sind Mitglied in der Association of Universities and Colleges of Canada (AUCC), weshalb der Standard allgemein als ausgeglichen gilt.[111]
Für das Schulwesen sind ausschließlich die Provinzen und Territorien zuständig; es gibt kein landesweites Bildungsministerium. Daher unterscheiden sich in einigen Provinzen Schuleintrittsalter (fünftes oder sechstes Lebensjahr) und Dauer der Grundschulzeit (bis Klasse 6 oder 7). Die Sekundarstufe (in Québec École polyvalente genannt) umfasst in Form einer Gesamtschule die dreijährige Junior Highschool (Sekundarbereich I) und die zwei- bis vierjährige Senior Highschool (Sekundarbereich II). Da das Bildungssystem Chancengleichheit anstrebt, erfolgt der Übergang von einer Schulstufe in die andere ohne Leistungsprüfung. Erst innerhalb der Senior High School ist der Erwerb des Abschlusszeugnisses (High School Diploma bzw. Diplôme d’Études Secondaire) davon abhängig, ob eine bestimmte Zahl von Bewertungspunkten (Creditpoints) erreicht wird. Zwei Prozent der Schulen liegen in privater, überwiegend kirchlicher Hand. Etwa zehn Prozent der Schüler besuchen eine Privatschule.[112] Das Leistungsniveau der Privatschulen galt 2006 als sehr hoch und Kanada war der einzige OECD-Staat, in dem deren Schüler selbst nach Abgleich des familiären und sozioökonomischen Hintergrundes mehr lernten, als die Schüler an öffentlichen Schulen.[113]
Während der Schulbesuch kostenfrei ist, werden an den Hochschulen Studiengebühren unterschiedlicher Höhe fällig.[114] Von den über 80 Universitäten zählen die University of Toronto und die Universität Montreal zu den größten. Die ältesten sind die Universität Laval in Québec von 1663, eine jesuitische Institution, die nach Bischof Laval benannt wurde. Dies berührt einen Grundzug der kanadischen Hochschulentwicklung, denn die frühen Institutionen waren fast alle kirchlichen Ursprungs. Erst 1818 entstand die erste säkulare Hochschule und die zweite Kanadas, die Dalhousie University in Halifax. Ihr folgten die beiden englischsprachigen Institute, die McGill University in Montreal (1821) und die University of Toronto (1827). Ihnen folgten in den 1840er-Jahren die Queen’s University in Kingston (1841) und die Universität Ottawa (1848). Letztere geht wie die Laval-Universität auf einen Missionsorden zurück, in diesem Falle auf die Oblaten der Unbefleckten Jungfrau Maria. Nach der Unabhängigkeit im Jahr 1867 folgten die von einem anglikanischen Bischof gegründete University of Western Ontario in London (1878) und die im selben Jahr gegründete Universität Montreal (die zweite von vier Hochschulen in der Stadt) sowie die McMaster University in Hamilton in Ontario. Letztere wurde ursprünglich in Toronto gegründet und zog erst 1930 nach Hamilton um. Sie geht auf die Baptist Convention of Ontario zurück.
Colleges verleihen meist nur 3- bis 4-jährige Bachelor-Abschlüsse (z. B. Minors, Majors, Spezialication, Honours), Universitäten auch 1-jährige konsekutive „post-bachelor“ Bachelor mit Honours-/Baccalaureatus Cum Honore-, 1- bis 3-jährige Master- und 3- bis 5-jährige Ph.D.-Abschlüsse. In diversen Hochschulrankings nehmen einige kanadische Universitäten Spitzenpositionen ein: Beispielsweise war in der langjährigen Durchschnittsbewertung des in Nordamerika am weitesten verbreiteten Rankings, der QS World University Rankings, im Jahr 2018 die McGill University innerhalb Kanadas auf Platz 1 und weltweit auf Platz 28. Laut dem Academic Ranking of World Universities (Shanghai-Ranking) aus dem Jahr 2018 (Jiaotong-Universität Shanghai) zählen die University of Toronto auf Platz 23 und die University of British Columbia in Vancouver auf Platz 43 zu den besten Hochschulen. Die First Nations besitzen seit 2003 eine eigene Universität, die First Nations University of Canada in Regina, der Hauptstadt der Provinz Saskatchewan. 1989 begannen die bedeutendsten Universitäten sich zusammenzuschließen, um Forschungsvorhaben zu koordinieren. Seit 2011 besteht die Gruppe als U15 Group of Canadian Research Universities, zu der ein nunmehr geschlossener Kreis von 15 Universitäten zählt. 2016 studierten über eine halbe Million ausländische Studenten an kanadischen Bildungseinrichtungen. Die größte Gruppe davon kam aus der Volksrepublik China.[115]
Im PISA-Ranking von 2015 erreichen Kanadas Schüler Platz 10 von 72 Ländern in Mathematik, Platz 7 in Naturwissenschaften und den zweiten Platz beim Leseverständnis. Kanadische Schüler gehörten damit zu den besten von allen teilnehmenden Ländern und schnitten deutlich besser ab als die aus den benachbarten Vereinigten Staaten. Die Studie stellte zudem fest, dass Schüler aus Ontario und British Columbia die besten Leistungen erbrachten.[116]
→ Siehe auch: Klimapolitische Maßnahmen Kanadas
Die Umweltpolitik Kanadas hat ungewöhnliche naturräumliche Grundlagen, vor allem ist aber die Gemengelage der Interessen eine spezifisch kanadische. Kanadas Natur ist zum bedeutendsten Faktor für den Tourismus geworden. Dazu tragen 43 National- und weit über 1500 Provinzparks sowie weitere Schutzgebiete bei, die vor allem riesige Waldgebiete beinhalten. Der älteste von ihnen ist der Banff-Nationalpark von 1885, der inzwischen über autobahnartige Straßen dem Massentourismus erschlossen wird. 1911 entstand Parks Canada (gleichberechtigt auch Parcs Canada) als älteste Nationalparkverwaltung der Welt. Doch kollidieren touristische, Erhaltungs-, Erholungs- und wissenschaftliche Interessen mit den Verwertungsinteressen der Rohstoffindustrie und gelegentlich den Interessen der Ureinwohner.
Intakte Urwälder (old growth) existieren in Kanada auch nach drei Jahrhunderten des Raubbaus aufgrund der geringen Besiedlungsdichte noch auf enorm großen Flächen. Nach Global Forest Watch Canada sind noch 62 % der borealen Wälder und 30 % der gemäßigten Wälder intakt (natürliche Ökosysteme, die im Wesentlichen vom Menschen unbeeinflusst sind).[14] Der Raubbau an der Grenze zu den besiedelten Gebieten ist jedoch immens und hat dort nur noch kleine Urwaldreste zurückgelassen. Ohne den Widerstand von Umweltschutzorganisationen wie Greenpeace, die in Vancouver gegründet wurde, oder dem Western Canada Wilderness Committee sowie den lokalen Indianern würden auch diese Urwälder sicherlich nicht mehr existieren. Die Unternehmen der Holzindustrie sind so eng mit den politischen Eliten der Provinzen verbunden, dass erst internationaler Druck und häufig Zwang der Bundesregierung und der Gerichtshöfe die Bestände in einigen Fällen retten konnten (vgl. Clayoquot Sound). Dagegen haben sich Wissenschaftler und zahlreiche Umweltverbände zusammengeschlossen, und die lange unbedeutende Green Party of Canada konnte bei der Wahl von 2008 knapp sieben Prozent der Wähler gewinnen.[117]
Nach einer Studie der Simon Fraser University, die auf Betreiben der David Suzuki Foundation durchgeführt wurde, liegt Kanada bei dreißig untersuchten Staaten bei der Produktion von Atommüll und Kohlenstoffmonoxid auf dem hintersten Rang. Zudem nimmt es beim Wasserverbrauch den 29. Platz ein. Insgesamt rangieren Kanada, Belgien und die USA am unteren Ende der Staatengruppe.
Im Oktober 2008 versuchten sich mehrere hundert Wissenschaftler gegen die Diskreditierung ihrer Arbeit durch die Regierung zur Wehr zu setzen. Gleichzeitig fanden in Victoria die größten Demonstrationen der letzten 15 Jahre gegen die Abholzung der letzten Urwälder auf Vancouver Island statt.[118]
Eine weitere Gefahr für die Urwälder, aber ebenso sehr für die riesigen nachgewachsenen Wälder stellte der in Kanada Mountain Pine Beetle genannte Bergkiefernkäfer dar. Er vernichtete mehrere Millionen Hektar Wald.[119]
Die über 250 Staudämme, die rund 58 % der in Kanada 2007 produzierten Strommenge von 612,6 Milliarden Kilowattstunden produzieren halfen (wovon Kanada über 2016 73 Milliarden Kilowattstunden exportierte)[2], werden inzwischen ebenso kritisch mit Blick auf ihre Umweltbilanz betrachtet wie der Abbau der Bodenschätze. In beiden Fällen kam es nicht nur zu häufigen Zwangsumsiedlungen der Ureinwohner wie der Innu in Labrador, sondern auch zu erheblichen Umwelt- und Gesundheitsbelastungen wie beim Abbau der Athabasca-Ölsande in Alberta. Am 14. Oktober 2008 lehnten die Cree, denen die rechtlich privilegierte Rolle der Provinzen gegenüber der Bundesregierung in Fragen der Bodenschätze und der Stromgewinnung und gegenüber den indianischen Nationen bewusst ist, den „Grünen Plan“ der Quebecer Provinzregierung daher ab.[120] Er hätte zudem Québec erneut die Verwaltung des riesigen James-Bay-Gebiets zurückgegeben, die die Cree nach langen Verhandlungen erst 2002 errungen hatten. Seit 2009 kämpfen drei lokale Cree-Gruppen mit internationaler Unterstützung um den Wald im Broadback-Tal, einen großen zusammenhängenden borealen Urwald am Rand der Holzeinschlagszone.
Im Nordosten British Columbias kam es allein 2005 bis 2008 zu sieben von der Polizei als höchst gefährlich eingeschätzten Anschlägen auf Gasleitungen der Encana Corporation, in denen stark giftiger Schwefelwasserstoff transportiert wird.[121]
Am 29. April 1998 unterzeichnete die Regierung das Kyoto-Protokoll und verpflichtete sich, die Treibhausgas-Emissionen bis 2012 um sechs Prozent zu senken. Stattdessen stiegen die Emissionen von 1990 bis 2004 um mehr als ein Viertel. Beim Klimaschutz-Index 2008 lag Kanada auf Platz 53 von 56 untersuchten Staaten, womit das Land beim Kohlenstoffdioxid-Ausstoß nur noch vor Saudi-Arabien, den USA und Australien rangiert.[122] Im Dezember 2011 erklärte der Staat kurz nach der UN-Klimakonferenz in Durban seinen Rückzug vom Kyoto-Protokoll. Damit sparte Kanada 14 Milliarden Dollar (10,5 Milliarden Euro) an Strafzahlungen für das Nichteinhalten der im Protokoll gesetzten Ziele. Unter anderem trägt die Ölsandindustrie erheblich zum steigenden Treibhausgasausstoß des Landes bei.[123]
Rechtlich liegt der Umweltpolitik vor allem der Canadian Environmental Protection Act von 1999 zugrunde. Das zuständige Ministerium ist das Department of the Environment unter Leitung von Jim Prentice (seit 2008). Ihm unterstehen neben anderen Organisationen Parks Canada und der Canadian Wildlife Service. Jede Provinz hat zudem ein eigenes Umweltministerium.
Kanada gehört zu den wohlhabendsten Ländern der Welt. Gemessen am nominalen Bruttoinlandsprodukt lag es 2020 mit umgerechnet 1,6 Billionen US-Dollar auf dem 9. Platz, bei der Kaufkraftparität mit 1,9 Billionen internationalen Dollar auf Platz 15. Beim Bruttoinlandsprodukt pro Kopf liegt der Staat 2020 mit 43.295 US-Dollar auf Platz 20, sowie kaufkraftbereinigt mit 48.759 US-Dollar auf Platz 25. Das Entwicklungsprogramm der Vereinten Nationen schätzt Kanada aufgrund seines Index der menschlichen Entwicklung als Land mit „sehr hoher menschlicher Entwicklung“ ein. Der Staat gilt zugleich als soziale Marktwirtschaft. Im Global Competitiveness Index, der die Wettbewerbsfähigkeit eines Staates misst, belegt Kanada Platz 14 von 141 Staaten (Stand: 2019).[124] Im Index für wirtschaftliche Freiheit belegt das Land 2022 den 15. Platz von 161 Ländern.[125]
Kanada war, laut einer Studie der Bank Credit Suisse aus dem Jahre 2017, der Staat mit dem achtgrößten nationalen Gesamtvermögen weltweit. Der Gesamtbesitz der Kanadier an Immobilien, Aktien und Bargeld belief sich auf insgesamt 7.407 Milliarden US-Dollar. Das Vermögen pro erwachsene Person beträgt 259.271 Dollar im Durchschnitt und 91.058 Dollar im Median (Deutschland: 203.946 bzw. 47.091 Dollar). Der Gini-Koeffizient bei der Vermögensverteilung lag 2016 bei 73,0 was auf eine mittlere Vermögensungleichheit hindeutet. Sowohl Einkommen als auch Vermögen sind in Kanada gleichmäßiger verteilt als in den benachbarten USA.[126]
Der Mindestlohn unterscheidet sich in jeder Provinz und wird von den einzelnen Provinzen selber festgelegt. Beschäftigte des Staates Kanada erhalten mindestens den Mindestlohn, der in der Provinz gilt, in der sie beschäftigt werden. 2017 lag er zwischen 10,72 (Saskatchewan) und 13,00 Dollar (Nunavut).[127] Von diesem Mindestlohn kann in einigen Bundesstaaten für verschiedene Beschäftigungsgruppen (z. B. für Beschäftigte die Trinkgelder erhalten oder für Jugendliche) abgewichen werden. Ebenfalls haben einigen Bundesstaaten jährliche automatische Anpassungen (z. B. Anpassung an Teuerungsraten) eingeführt.
Überdurchschnittlich hoch ist der Anteil der Urproduktion, also des primären Wirtschaftssektors, was auf den Reichtum an natürlichen Ressourcen zurückzuführen ist. Die in der Provinz Ontario abgebauten Mengen an Nickel decken etwa 20 % des Weltbedarfs, Kanada besitzt mit rund 28 Milliarden Tonnen die drittgrößten Erdölreserven nach Venezuela und Saudi-Arabien (Stand 2017), verfügt über zehn Prozent des weltweiten Waldbestands, dazu bedeutende Vorkommen von Schwefel, Asbest, Aluminium, Gold, Blei, Kohle, Kupfer, Eisenerz, Kaliumcarbonat, Tantal, Uran und Zink.[128] Vor der Küste der Atlantischen Provinzen liegen umfangreiche Vorkommen an Erdgas, in Alberta die Athabasca-Ölsande.[129] Wald und Wasserkraft bilden die Grundlage für die Zellstoff- und Papierindustrie.
Zahlreiche Stauseen liefern Strom und bilden damit das Rückgrat der Energieproduktion. Allein 360.000 GWh stammten aus Wasserkraft, womit Kanada knapp hinter China der zweitwichtigste Stromproduzent auf diesem Sektor ist.[130] In Kanada werden über elf Prozent des Weltstrombedarfs gedeckt, und es ist eines der wenigen Industrieländer, die Netto-Exporteure von Energie sind.[131] Die Verbindung innerhalb Nordamerikas ist dabei inzwischen so eng, dass sich riesige, grenzüberschreitende Versorgungsverbünde entwickelt haben, wie die Western Interconnection, die bis nach Mexiko reicht. Weitere Energielieferanten sind Gas, Öl, Uran (18 produzierende Kernkraftwerke) und regenerative Energien. Kernkraftwerke lieferten 2010 genau 85.219,889 von insgesamt 565.519,793 GWh Strom, also rund 15 % des Stroms.[132] Insgesamt waren in Kanada Ende 2020 Windkraftanlagen mit einer Leistung von 13,58 GW[133] installiert (2017: 12,24 GW[134], 2018: 12,82 GW[135], 2019: 13,41 GW[133]). Damit lag der Staat weltweit auf Rang 9.[133][134][135] Der größte Windpark mit 364 MW befindet sich in der Provinz Québec im Gemeindeverband La Côte-de-Beaupré.[136]
Kanada ist aufgrund seiner hohen Überschüsse einer der größten Lieferanten von landwirtschaftlichen Erzeugnissen, doch ist das Produktspektrum in den Prärieprovinzen sehr eng; im Mittelpunkt steht dabei ganz überwiegend Weizen, bei dessen Produktion Kanada 2003 mit 50,168 Millionen Tonnen an achter Stelle nach Frankreich stand.[137] Hinzu kommt Viehwirtschaft, vor allem Rinderzucht, in den letzten Jahren auch wieder die kommerzielle Zucht von Bisons. An den Küsten wird Fischzucht betrieben, die jedoch mit dem Fang von Wildfischen in Konflikt steht. Dabei ist British Columbia der größte Exporteur von Lachs und Heilbutt.
Die Zentren der Industrie liegen im Süden der Provinzen Ontario und Québec, vor allem in den Großräumen von Toronto und Montreal. Dabei spielen die Automobil- und die Luftfahrtindustrie eine bedeutende Rolle, hinzu kommen Metallindustrie, Nahrungsmittelverarbeitung sowie Holz- und Papierindustrie. Ebenfalls eine bedeutende Rolle spielen die chemische und die elektrotechnische Industrie, vor allem aber der Hightech-Bereich. Dies hängt mit dem Niedergang der großen Automobilkonzerne in den USA zusammen, der vor allem die Zulieferer und Dépendancen im Ballungsraum Toronto trifft. Alle Industrien, die sich dem Sektor der Gas- und Ölförderung anlagern, konzentrieren sich hingegen im Großraum Calgary, doch leidet diese prosperierende Industrie jüngst unter rapidem Preisverfall bei steigenden Explorationskosten. Dies hängt zum Teil mit geologisch bedingten Hemmnissen zusammen, mit dem inzwischen sehr hohen Lohnniveau und dem wachsenden Widerstand gegen die Zerstörungen der Umwelt. Dennoch entwickelte sich Kanada 2018 zum weltweit viertgrößten Förderer von Rohöl.[138]
Die Exporte betrugen 2007 36,7 % und die Importe 32,8 % des BIP. Bei weitem wichtigster Handelspartner waren dabei die USA mit 76,4 % der Exporte und 65,0 % der Importe.[139] Kanada belegt nach der EU, den USA, Japan und der Volksrepublik China den fünften Platz in der Weltaußenhandelsstatistik.[140] Der Außenhandel ist weitgehend frei, nur in wenigen Schlüsselbereichen sind ausländische Investitionen auf Minderheitsbeteiligungen beschränkt.
Mit Abstand am meisten Bedeutung besitzt der Dienstleistungssektor mit 66 % (2008) Anteil am Bruttoinlandsprodukt, gefolgt von der Industrie mit 32 % und der Landwirtschaft mit knapp 2 %.[141] Sieben der zehn größten kanadischen Unternehmen – wenn man den Umsatz zugrunde legt – sind allein im Banken- und Versicherungsbereich tätig. War die Wirtschaft in der ersten Hälfte des 20. Jahrhunderts noch weitgehend auf den Export nach Europa orientiert, vor allem in das Britische Empire, so wurden die Handelsbarrieren zum Nachbarstaat USA nach dem Zweiten Weltkrieg allmählich weitgehend abgebaut. Ein erster wichtiger Schritt war das 1965 vereinbarte Canada-United States Automotive Agreement (auch Auto Pact genannt), das die Grenzen für die Automobilindustrie vollständig öffnete. Das Kanadisch-Amerikanische Freihandelsabkommen von 1988 schaffte die Zölle zwischen beiden Ländern ab und führte zu einem deutlichen Anstieg des Handelsvolumens und der US-Investitionen in Kanada. Mit dem Nordamerikanischen Freihandelsabkommen wurde diese Freihandelszone 1994 auf Mexiko ausgedehnt. Weitere Freihandelsabkommen bestehen unter anderem mit der EFTA.[142] Kanada ist Mitglied zahlreicher wirtschaftspolitischer Organisationen, wie der Welthandelsorganisation, der OECD, des Internationalen Währungsfonds, der Weltbank und der G7.
Als eine der größten Schwächen der kanadischen Wirtschaft hat die OECD die mangelnde Umsetzung von Erfindungen in verwertbare Patente eingeschätzt.[143] Daher stieß die Regierung 2007 ein Programm namens Mobilizing Science and Technology to Canada’s Advantage an. Es soll die geringe Zahl der Patente erhöhen und zu mehr Investitionen im Forschungs- und Entwicklungsbereich anregen. Es soll zugleich die Zusammenarbeit von staatlichen Bildungseinrichtungen und industriellen Komplexen fördern. Zudem wurden Centres of Excellence in Commercialisation and Research eingerichtet sowie ein College and Community Innovation Program.
Die größte Arbeitnehmervertretung bildet der Canadian Labour Congress (CLC) oder französisch der Congrès du travail du Canada (CTC) mit seinen rund hundert Einzelgewerkschaften in 136 Distrikten, die nach eigenen Angaben drei Millionen Mitglieder haben.[144] Er ist 1956 aus dem Zusammenschluss von Trades and Labour Congress of Canada (TLC) und Canadian Congress of Labour (CCL) hervorgegangen. Während die TLC ähnlich wie in Europa nach Branchen organisiert war, war die CCL nach Orten organisiert und umfasste dort alle Gewerbe. Zudem hatte der TLC die Liberalen unterstützt, während bei der CCL Anhänger der sozialistischen Co-operative Commonwealth Federation vertreten waren. Zugleich integrierte sie die kommunistische Workers Unity League (WUL), als sie 1939 ein Bündnis gegen den Faschismus bildeten. Auch die in British Columbia ansässigen International Woodworkers of America galten als kommunistisch, wurden aber 1948 integriert. Wenig später wurden die Kommunisten ausgeschlossen. Die CLC spielte eine wichtige Rolle bei der 1962 erfolgten Gründung der New Democratic Party und bekämpfte gemeinsam mit ihr das Freihandelsabkommen mit den USA. Vorsitzender des CLC ist seit 1999 Kenneth V. Georgetti. Closed Shops sind rechtlich zulässig und in vielen Branchen üblich.
Die Finanzkrise ab 2007 blieb nicht ohne Wirkungen auf die kanadische Wirtschaft. Betroffen waren zunächst die Finanzdienstleister, die sich in Toronto ballen, wo die Toronto Stock Exchange (TSX) die drittgrößte Börse Amerikas darstellt, aber auch die Immobilienindustrie, und mit der Insolvenz von Nortel im Januar 2009 auch die Ausrüster für Telekommunikationsunternehmen.[145] Unter diesen Unternehmen ist BCE (Bell Canada Enterprises) das älteste und größte. Im 4. Quartal 2008 gingen die Exporte um 17,5 % zurück.[146]
Die Arbeitslosigkeit lag im August 2009 jeweils bei 8,7 % (September 2007 5,9 %).[147]
Im April 2022 lag sie bei 5,2 %.[148]
Der Staatshaushalt umfasste 2016 Ausgaben von umgerechnet 594,0 Mrd. US-Dollar, dem standen Einnahmen von umgerechnet 514,5 Mrd. US-Dollar gegenüber. Daraus ergibt sich ein Haushaltsdefizit in Höhe von 2,4 % des BIP.[150]
Die Staatsverschuldung betrug 2016 1.406 Mrd. US-Dollar oder 92 % des BIP.[150] Trotz der hohen Staatsverschuldung werden kanadische Staatsanleihen von der Ratingagentur Standard & Poor’s mit der Bestnote AAA bewertet (Stand 2018).[151]
Der Staatshaushalt finanziert das funktionierende System des kanadischen Finanzausgleichs.
Die erste Zeitung auf dem Gebiet Kanadas war John Bushells Halifax Gazette, die 1752 erschien.[152][153]
In Neufrankreich existierten keine Zeitungen, doch gründeten William Brown und Thomas Gilmore aus Philadelphia die zweisprachige Quebec Gazette in Québec. 1785 entstand durch Fleury Mesplet, den die Briten wegen seiner Aufforderung zum Anschluss an die USA inhaftiert hatten, das heute älteste Blatt, die Montreal Gazette. 1793 folgte in Niagara-on-the-Lake die erste Zeitung in Ontario, die Upper Canada Gazette. Diese frühen Blätter hingen weitgehend von Zuwendungen der Regierung und von Anzeigenerträgen ab, kaum von Käufern und Abonnenten. Dies sollte sich in Kanada als Dauerzustand erweisen.
In Québec entstanden 1805 und 1811 der City Mercury und in Montreal der Herald als Sprachrohre der dortigen Händlereliten, während Le Canadien (1806) und La Minerve (1826) die Frankophonen vertraten. Gegen diese Kolonial- und Händlereliten wandte sich in Ober-Kanada der Colonial Advocate, den William Lyon Mackenzie herausbrachte und der die Reform- und Farmergruppen vertrat. Ähnliches galt für Joseph Howes Novascotian (1824) in Halifax.
Die meisten Zeitungen hingen von Parteien ab, insbesondere den Reformern (den heutigen Liberalen) und den Konservativen, und zwar meist als Organe bestimmter politischer Führer. So war der Toronto Globe (1844) die Stimme des Reformers George Brown, die Toronto Mail (1872) hingegen wurde bald zur Stimme von John Macdonald, dem ersten Premier Kanadas. Ähnlich organisierten 1899 Geschäftsleute den Toronto Star zugunsten von Wilfrid Laurier um. Dagegen kauften wiederum die dortigen Konservativen die Toronto News 1908 als Parteiorgan. Jede größere Stadt hatte folglich ein liberales und ein konservatives Blatt, das die jeweilige Klientel versorgte. Bis in die 1930er-Jahre hinein blieben die Quebecer Blätter dabei von der jeweiligen Regierung abhängig.
Blätter, die nicht einer der Führungsgruppen angehörten, wie die kommunistische Presse, wurden immer wieder verboten. Der von streikenden Druckern 1892 gegründete Toronto Star ging – wie die meisten Arbeiterzeitungen – ein. In Québec erließ die Regierung Maurice Duplessis den Padlock Act, der ihre Zeitungen traf. Noch 1970 übte die Regierung eine Art Zensur aus, als es in der Oktoberkrise zu Entführungen kam.
Der erste Versuch einer Tageszeitung, der Montreal Daily Advertiser, ging nach einem Jahr 1834 in den Konkurs. Doch 1873 gab es bereits 47 Tageszeitungen, 1913 gar 138. Im äußersten Westen erschien der British Colonist ab 1858, die Manitoba Free Press 1872, der Saskatchewan Herald 1878 und das Edmonton Bulletin 1880. Die Verbreitung des Radios ab den 30er-Jahren und des Fernsehens ab den 50er-Jahren kostete die Zeitungen viele Werbekunden, so dass 1953 nur noch 89 Tageszeitungen existierten. 1986 erholte sich die Zahl wieder auf 110, doch nur noch acht Städte hatten zwei oder mehr Tageszeitungen.
Heute gehören die meisten Zeitungen zu großen Konglomeraten der Medienindustrie. Die Erlaubnis, in beiden Bereichen der Medien, Fernsehen und Printmedien, Unternehmen zu erwerben, war lange umstritten, doch seit Brian Mulroney gibt es darin keine Begrenzung mehr. Im englischen Sprachraum ist Postmedia Network führend, sie bieten in den meisten Provinzhauptstädten die führende Tageszeitung an. 90 % der frankophonen Zeitungen gehören drei Medienunternehmen: Pierre Karl Péladeaus Quebecor Inc., der allein die Hälfte der Gesamtauflage liefert, Paul Desmarais’ Gesca und Jacques Francœurs UniMédia. Schon 1950 beherrschten die vier größten Medienunternehmen 37,2 % des Gesamtmarktes, 1970 waren dies 52,9 %, 1986 gar 67 %. 80 % der Einnahmen stammen dabei aus Werbung, nur 20 % aus Verkaufserlösen.
Mit dem Radio experimentierte zunächst Guglielmo Marconi ab 1896, 1901 gelang ihm die erste drahtlose Signalübertragung über den Atlantik von Cornwall nach Neufundland.[154] Weil die Radiotechnik zunächst eher der Kontaktaufnahme zu Schiffen diente, unterstand die Aufsicht über den Radiotelegraph Act von 1913 dem Minister für Marine und Fischerei. Die Überlebenden der Titanic verdankten ihre Rettung den von Marconi gesendeten Radiowellen. Er war auch der erste, der 1919 eine private Sendelizenz in Kanada erhielt. 1928 bestanden bereits 60 Radiostationen.
Dennoch stellte eine Kommission unter Leitung von John Aird in diesem Jahr fest, dass viele Kanadier US-Stationen lauschten. Erst 1932 entschied das britische Judicial Committee of the Privy Council, dass der Staat die Oberaufsicht über die Radiokommunikation zu Recht beanspruche. 1936 begann die öffentliche Canadian Broadcasting Corporation (CBC) ihren Sendebetrieb, der seit 1932 von der Radio Commission begonnen worden war. Bis dahin hatte sich die Zahl der Radioempfänger binnen fünf Jahren auf eine Million verdoppelt.
Die heutige Struktur der CBC ist ein Produkt der Weltwirtschaftskrise: Es entstanden nur fünf zentrale Sender, deren Sendungen von privaten Distributoren weitergeleitet wurden. So entstand ein gemischtes System staatlicher und privater Sender, in dem den privaten Sendern nur eine regionale Ausstrahlung gestattet wurde. Kanada wurde eines der Länder mit den meisten Radiostationen, und eines der ersten mit Satellitensendern. Dennoch ist die US-amerikanische Konkurrenz stark vertreten.
Seit 1952 gibt es Fernsehen in Kanada, wobei die CBC die Regulierungsaufgaben wahrnahm und zugleich der bedeutendste Sender wurde. Auch hier dienten private Netzwerke als Distributoren für CBC-TV. Einer Kampagne der Privatsender gegen das CBC-Monopol folgte der Broadcasting Act von 1958 unter John Diefenbaker. Es entstand ein 15-köpfiger Board of Broadcast Governors (BBG), der die Anträge für neue Sender annahm und eher Privatsender förderte. Das TV expandierte schnell, und 1961 entstand ein zweites Netzwerk, CTV. Zwischen BBG und CBC kam es zu heftigen Streitigkeiten, so dass 1968 die Lizenzvergabe an die Canadian Radio-Television Commission (heute Canadian Radio-Television and Telecommunications Commission, CRTC) vergeben wurde, die auch das 1968 etablierte Kabel-TV an sich zog. Der Anspruch auf „Schutz, Bereicherung und Stärkung der kulturellen, politischen, sozialen und ökonomischen Struktur Kanadas“, wie es im Gesetz heißt, sollte dabei gewahrt werden. Dennoch führten Sparmaßnahmen in den letzten vier Jahrzehnten zu einer zunehmenden Abhängigkeit von Werbeetats und Einschaltquoten.
Dabei sind US-Sender über Kabel praktisch überall zu empfangen. Folglich besetzen sie im englischsprachigen Kanada rund 75 % der besten Sendezeit, während dieser Anteil in Québec nur bei 40 % liegt. Hier spielt TVA die wichtigste Rolle.
Inwiefern das Internet die entstandene Medienmacht relativieren kann, ist noch offen, zumal alle etablierten Medien in diesem neuen Markt zunehmend engagiert sind. Die Interessen der unabhängigen Medienunternehmen vertritt seit 1948 die Assoziation der kanadischen Film- und Fernsehproduktion.
Die Hauptverkehrsachse des Ostens verläuft entlang dem Sankt-Lorenz-Strom durch Ontario und Québec und verbindet Toronto, Montreal, Québec und Ottawa miteinander. Der gesamte Norden des Landes ist verkehrsmäßig wenig erschlossen, da hier, außer in den Gebieten der Rohstoffförderung, kaum Bedarf besteht. Die Ballungsräume des Westens sind, wie im Osten, hauptsächlich nahe der amerikanischen Grenze durch Verkehrssysteme verbunden, sieht man einmal von der Anbindung Edmontons ab. Dies ist vor allem dem politischen Willen der kanadischen Regierung zu verdanken, die allein durch drei transkontinentale Eisenbahnlinien und diverse Stichbahnen die weit auseinander liegenden Provinzen miteinander verbinden wollte. Davor war dies durch Kanäle geschehen, nach der Eisenbahnepoche folgten Straßenbauten, schließlich Fluglinien.
Das Straßensystem Kanadas hatte 2011 eine Gesamtlänge von 1.042.300 km und ist damit das siebt-längste der Welt. Asphaltierte Straßen hatten eine Länge von 415.600 km, wovon 17.000 km Autobahnen waren. Nach China und den Vereinigten Staaten hatte Kanada damit das drittlängste Autobahnnetz.
Das dichteste Straßennetz befindet sich im Bereich der höchsten Bevölkerungsdichten in den Atlantikprovinzen, in Süd-Ontario, in Québec entlang des St. Lorenz, in den südlichen Prärieprovinzen und im Bereich der Frasermündung um Vancouver. Als ein alle Provinzen verbindendes Element wurde von Victoria am Pazifik bis St. John’s am Atlantik der Trans-Canada-Highway gebaut, mit 8000 km eine der längsten Straßen der Welt. In den Ballungsräumen und als Verbindung zwischen größeren Zentren ist diese Straße als Autobahn ausgebaut. Durch Ontario führen zwei Routen dieser Straße, eine nördlichere und eine südlichere. Der Trans-Canada-Highway ist die einzige Bundesstraße Kanadas.
Die übrigen Landstraßen, auch die Autobahnen, werden von den Provinzen gebaut und unterhalten. Die verkehrsreichste Autobahn Kanadas bildet das Rückgrat des Québec-Windsor-Korridors, in Ontario mit der Straßennummer „401“. Mit 16 Spuren durch den Ballungsraum Toronto gehört der 401 zu den breitesten Autobahnen der Welt. Nach Norden führen nur wenige Straßen, von denen die meisten wegen großer Baumaßnahmen (Staudämme, Bergbau etc.) gebaut wurden, oder aus militärischen Gründen entstanden (zum Beispiel der Alaska Highway).
In Kanada von Bedeutung sind Überlandbusse. Jede Region verfügt über ein ausgedehntes Busnetz; die größte Busgesellschaft Greyhound Canada stellte jedoch im Mai 2021 im Nachgang zu den wirtschaftlichen Folgen der Corona-Krise den nationalen Betrieb ein. Aufrechterhalten wird lediglich der grenzüberschreitende Verkehr in die U.S.A.[155]
In Kanada herrscht Rechtsverkehr und die Geschwindigkeiten sind in km/h angegeben. Das Nationalitätskennzeichen ist CDN (nicht CND für Canada) und steht für Canadian Dominion. Dieses wird auch als Abkürzung in Herkunftsangaben z. B. bei Spielfilmen verwendet.
Der Straßenverkehr des Landes gilt als weitestgehend sicher. 2013 kamen in Kanada insgesamt 6,1 Verkehrstote auf 100.000 Einwohner. Zum Vergleich: In Deutschland waren es im selben Jahr 4,3 Tote. Das Land hat eine im weltweiten Vergleich hohe Motorisierungsrate. 2016 kamen im Land 662 Kraftfahrzeuge auf 1000 Einwohner.[156]
Zur Überwindung der großen Entfernungen ist der Inlandsflugverkehr von erheblicher Bedeutung. Etwa 75 Fluggesellschaften, darunter Air Canada, die mit 34 Millionen transportierten Passagieren größte Fluggesellschaft Kanadas, Westjet Airlines und Porter Airlines sorgen für regionale Flugverbindungen. In Westkanada fliegen Air BC, die inzwischen zu Jazz Aviation gehören und Horizon Air, in Ostkanada Air Alliance (Sitz in Québec) und Air Ontario (Ontario). Im Norden fliegen Gesellschaften wie Air Creebec (im Besitz der Cree), Air North (Whitehorse), Bearskin Airlines, Canadian North (Yellowknife) oder Air Inuit (Dorval) sowie First Air (Ottawa), die im Besitz von Inuit sind.
Air Transat und Air Canada fliegen auf internationalen und innerkanadischen Strecken, wobei Air Canada 1937 aus einer Eisenbahngesellschaft hervorging. Flughäfen mit interkontinentalen Verbindungen befinden sich in Toronto, Montreal, Calgary, Ottawa, Edmonton, Vancouver, Québec, Halifax sowie Winnipeg.
1909 flog das erste kanadische Flugzeug 800 m weit (in Baddeck), 1915 entstand mit der Curtiss JN-3 das erste Serienflugzeug. Im Ersten Weltkrieg stellte Kanada bereits 22.000 Mitarbeiter bei den Luftstreitkräften, obwohl die Canadian Air Force erst 1920 entstand. In den 30er-Jahren erfolgte ein massiver Ausbau der Flughäfen, so dass mehr als die Hälfte der gesamten Luftfracht in Kanada bewegt wurde und das Land 1945 587 Flugplätze aufwies. 1937 wurde Trans-Canada Airlines gegründet, aus der 1964 Air Canada hervorging. 2009 wurde der 23. Februar zum National Aviation Day erklärt.[157]
Die Stadt Montreal ist Sitz der zwei weltweiten Zivilluftfahrtorganisationen, der IATA und der ICAO.
Die Eisenbahn ist im 19. Jahrhundert vom kanadischen Staat umfassend gefördert worden, um die Besiedlungspolitik zu unterstützen und die nationale Einheit zu sichern. Dazu sollten die Distanzen zwischen den Provinzmetropolen durch transkontinentale Eisenbahnlinien überwunden werden. Doch seit den 1930er-Jahren ging ihre Bedeutung zugunsten des Straßenverkehrs erheblich zurück und besitzt seither nur noch innerhalb des Québec-Windsor-Korridors große Bedeutung im Personen(nah)- und Güterverkehr.
Außerhalb dieses Gebietes beschränkt sich die Bedeutung auf den Massengüterverkehr und den Tourismus, vergleichbar den Schienenkreuzfahrten in Europa. Der überregionale transkontinentale Güterverkehr wird von den beiden Bahngesellschaften Canadian Pacific Railway und Canadian National Railway durchgeführt. Betreiberin des öffentlichen Schienenpersonenverkehrs ist die VIA Rail Canada, der regionale Güterverkehr wird von vielen privaten Gesellschaften betrieben. Zu diesen Hauptlinien kommen zahlreiche Nebenlinien, die zum Teil in privater Initiative wiederbelebt worden sind, wie die Esquimalt and Nanaimo Railway auf Vancouver Island.
Im Gegensatz zu den Vereinigten Staaten verfügen kanadische Großstädte über eine Vielfalt sehr gut ausgebauter Nahverkehrssysteme. Während in den Metropolen Toronto und Montreal seit den 1950er-Jahren gebaute, klassische U-Bahnen das Rückgrat des innerstädtischen Nahverkehrs bilden, werden in kleineren Großstädten wie Calgary und Edmonton seit den 1980er-Jahren Stadtbahnsysteme (Light Rail) aufgebaut. In den übrigen Städten werden vornehmlich Diesel- und teilweise Oberleitungsbusse eingesetzt; in Ottawa gibt es ein Bus-Rapid-Transit-Netz.
Die beiden größten Nahverkehrsnetze liegen in Toronto mit der Toronto Transit Commission und Montreal mit je vier Schnellbahnstrecken und je etwa 150 Buslinien. In Toronto ist außerdem noch ein größeres Straßenbahnnetz mit elf Linien in Betrieb. Der im Zuge der Weltausstellung Expo 86 eröffnete, vollautomatische SkyTrain in Vancouver war lange das längste automatische Transportsystem der Welt.
Wichtige Seehäfen befinden sich in den Städten am Sankt-Lorenz-Strom und in Vancouver. Zudem besteht auf den Großen Seen eine bedeutende Binnenschifffahrt. Wo keine natürlichen Wasserwege bestanden, baute man ab Anfang des 19. Jahrhunderts Kanäle. Für die wirtschaftliche Entwicklung Kanadas ab 1821 war der Lachine-Kanal von entscheidender Bedeutung. In Zentralkanada war das Kanu schon seit jeher das gegebene Transportmittel, und auch heute noch sind viele Seen mit Fährschiffen ausgestattet und der Warenverkehr folgt dem Wasser.
Manche Orte sind nur über See zu erreichen, wie entlang der Westküste von Vancouver nach Port Hardy auf Vancouver Island oder Prince Rupert gegenüber von Haida Gwaii.
Die frühe Erschließung des Landes erfolgte durch das Kanu und durch den Kanalbau, der einen weitläufigen Binnenverkehr ermöglichte. Bis in die 1950er-Jahre trugen Schiffe einen erheblichen Teil der Passagiere, vor allem in abgelegenen Gebieten, doch stellten die meisten Linien, ähnlich wie zahlreiche Eisenbahnstrecken, den Verkehr ein, als die großen Überlandstraßen wie der Alaska Highway entstanden.
Im Jahr 2020 nutzten 97 Prozent der Einwohner Kanadas das Internet.[158] Die digitale Infrastruktur gilt insbesondere in den Städten als sehr leistungsstark und eine der besten der Welt.[159]
Das heutige Kanada wird überwiegend durch die europäischen Einflüsse der Pioniere, Forscher, Händler und Fischer aus Großbritannien, Frankreich und Irland, regional auch aus Deutschland und Osteuropa geprägt. In jüngerer Zeit wird das Bild in größeren Städten auch von Asiaten (zum Beispiel Vancouver, Toronto) und von Schwarzen aus der Karibik und aus Afrika ergänzt. Viele ihrer Traditionen bleiben weiterhin Teil von Kanada, etwa ihre Nahrung, Sprache, Erzählungen, Geschichte, Feiertage und Sport. Die kulturellen Feste dieser Einwanderer sind ein fester Bestandteil des kanadischen Lebens, zum Beispiel das chinesische Neujahrsfest in Vancouver oder der Caribana-Umzug in Toronto. Viele Kanadier können noch heute ihre Wurzeln zurück zu diesen Ländern verfolgen und sind stolz auf ihre Herkunft. Der in vielen Städten ursprünglich vorherrschende britische Geist wurde mit der zunehmenden Einwanderung aus anderen Ländern weitgehend verwischt. Am deutlichsten ist er noch in Victoria zu erkennen. Dies gilt auch für das frankophone Kanada, das ebenfalls starken Einflüssen durch die Einwanderung ausgesetzt ist.
Kanada und Großbritannien teilen einen Abschnitt ihrer Geschichte und Kanada ist Mitglied des Commonwealth of Nations. Beide Länder sind in Personalunion verbunden. Großbritannien ist Kanadas drittgrößter Handelspartner, und von dort kommen nach den USA die meisten ausländischen Touristen. Die Verbindungen Kanadas zu anderen frankophonen Ländern sind in der Organisation internationale de la Francophonie institutionalisiert und es gibt einen regen kulturellen Austausch mit Frankreich. So ist Kanada beispielsweise am französischsprachigen Fernsehkanal TV5 Monde beteiligt.
Deutsche Einflüsse sind vor allem in Südontario um die Stadt Kitchener (ehemals Berlin) präsent. In ganz Südontario, besonders im Gebiet von Kitchener sind Orte mit deutschen Namen verstreut. Kitchener wirbt damit, dass dort das größte Oktoberfest außerhalb Münchens gefeiert wird.
Seit den 1970er-Jahren sind in Kanada viele Asiaten eingewandert, vorwiegend aus Hongkong, China und Korea. Insbesondere in Vancouver (spöttischer Name: Hongcouver)[160] und Toronto bilden sie starke ethnische Minderheiten und die Chinatowns mit ihren chinesischen Straßen- und Werbeschildern gehören zu den Sehenswürdigkeiten.
Die Schaffung und der Schutz einer eigenständigen kanadischen Kultur wird durch Programme, Gesetze und Einrichtungen der Bundesregierung, zum Beispiel der CBC/Radio-Canada, dem NFB (National Film Board of Canada/Office national du film du Canada) und der CRTC (Canadian Radio-Television and Telecommunications Commission/Conseil de la radiodiffusion et des télécommunications canadiennes) unterstützt.[161]
Die Kulturformen der weit über 600 First Nations, wie die Indianer sich ganz überwiegend selbst bezeichnen, sind nicht einheitlich. Innerhalb des Landes, zwischen Stadt und Land, zwischen den ethnischen Gruppen sind die Unterschiede denkbar groß. Die verschiedenen Gruppen entwickelten eigene Identitäten und kulturelle Strukturen. Dabei lassen sich große Kulturareale unterscheiden. An der Pazifikküste war die Kultur von Fischfang dominiert, vor allem vom Lachs, oder vom Walfang, wie bei den Nuu-chah-nulth auf Vancouver Island. Dort finden sich auch die gewaltigen Totempfähle, deren größter über 50 m hoch ist. Im Binnenland dominierten Jagd, Sammeln und Flussfischerei. In den großen Ebenen, den Plains, war die Bisonjagd von zentraler Bedeutung, in anderen der Elch. Durch die Verbreitung des Pferdes entwickelte sich nach 1700 ein Reiternomadismus. An den Großen Seen hingegen dominierte eine agrarische Kultur mit Großdörfern.
Die nicht mit den Indianern verwandten Inuit im Norden des Landes, von denen man 2006 genau 50.485 zählte,[162] entwickelten eine überwiegend von den arktischen Lebensumständen geprägte Kultur, die sich in vielerlei Hinsicht auf das ganze Kanada auswirkt. Ein Beispiel dafür stellt das Emblem der Olympischen Winterspiele 2010 in Vancouver dar, ein Inuksuk, das aus aufeinander gestapelten Steinen besteht und eine menschliche Gestalt symbolisiert.
Die frühesten kommerziellen Erfolge feierten jedoch die bildenden Künste der Inuit schon seit den späten 1940er-Jahren. Serpentin- und Marmorskulpturen, Arbeiten in Knochen und Karibugeweih, aber auch Kunstgrafik, Wandbehänge und -teppiche, Schmuck, Keramiken und Puppen standen dabei im Mittelpunkt. Ihre Motive und Materialien gingen auf die natürlichen Umgebungen und vorhandene Traditionen zurück, wobei die erzwungene Sesshaftigkeit nun erheblich größere Werke zuließ. Zudem waren die rund 25 Gemeinden, deren Bewohner nicht mehr autark-nomadisch lebten, nun auf Geldeinnahmen angewiesen, zu denen ihnen der Kunsthandel verhalf.
Zu den bekanntesten Inuit-Autoren zählen der ehemalige „Commissioner of Nunavut“ Peter Irniq, der Schriftsteller, Dichter, Cartoonist und Fotograf Alootook Ipellie (1951–2007) und Zebedee Nungak (geb. 1951). Aus der Verbindung von Inuit-Musik und amerikanisch-kanadischer Popmusik formten die Inuit eine eigene Musik. Daneben bestehen weiterhin einfache Gesangsformen und der Kehlgesang (Throat singing). Die in Kanada erfolgreichste Sängerin ist die 1967 in Churchill geborene Susan Aglukark.
Die Erfolge der Inuit und die der US-amerikanischen Indianer inspirierten die indianischen Künstler Kanadas, eigenständig an eine außerindianische Öffentlichkeit zu treten. Früh bekannt waren dabei die Masken und Totempfähle der Pazifikküste, die noch heute eine wichtige Rolle im Selbstverständnis, aber auch auf dem Kunstmarkt spielen. Ähnlich wie die Literatur verfolgt die indianische Kunstszene aber nicht nur traditionelle Elemente, sondern verbindet sie mit euro-kanadischen Mitteln. Andere Indianerkünstler produzieren losgelöst von diesen Traditionen in deren Genres und mit deren Mitteln. Dabei sind dennoch Künstler mit einem spezifisch indianischen Weg, wie Norval Morrisseau, oder der Bildhauer und Schnitzkünstler Bill Reid, der das Werk Charles Edenshaws fortführte, erst seit den 60er-Jahren anerkannt worden. Meist stehen in der Literatur ökologische Probleme, Armut und Gewalt, entmenschte Technik oder Spiritualität im Vordergrund. Dabei lassen sich die meisten ungern als „Indianerkünstler“ etikettieren.
Seit der Kolonisierung ab dem frühen 17. Jahrhundert brachten die Einwanderer, je nach ethnischer Zusammensetzung, verschiedene europäische Musiktraditionen nach Kanada.[163][164]
Die Parallelentwicklung zur europäischen Musik ist vom Barock über die Klassik und Romantik bis hin zur Gegenwartsmusik nie abgerissen. Doch fehlten in der Neuen Welt lange die nötigen Ressourcen, um große Aufführungen wie Opern in nennenswertem Umfang durchführen zu können. Erst die Anpassung von Texten, aber auch der Austausch von Elementen zwischen den Einwanderergruppen brachte kanadische Eigenheiten hervor, zu denen Einflüsse aus den USA kamen.
John Braham war einer der ersten Sänger, die im ganzen Land bekannt wurden (ab 1841), ähnlich Jenny Lind. Zudem bestanden zahlreiche Kirchenchöre und philharmonische Gesellschaften. Die ersten Gesellschaften dieser Art waren die New Union Singing Society aus Halifax (1809) und die Québec Harmonic Society (1820). Populär waren Balladen, Tanzmusik und patriotische Hymnen. Deutsche brachten erstmals den Klavierbau nach Kanada (Thomas Heintzman), ihm folgte der Orgelbau (Joseph Casavant). 1903 organisierte C. A. E. Harriss den Cycle of Musical Festivals of the Dominion of Canada, an dem sich landesweit über 4.000 Sänger und Musiker in 15 Städten beteiligten. Mit dem Ersten Weltkrieg und der danach anwachsenden Schallplattenindustrie war der Höhepunkt selbst gemachter Musik, aber auch der Operngesellschaften überschritten. Dennoch entstanden vor und nach der Weltwirtschaftskrise Symphonieorchester, insbesondere in den drei größten Städten Montreal, Toronto und Vancouver. Sir Ernest MacMillan war der erste und einzige kanadische Musiker, der zum Ritter geschlagen wurde, und weitere Sänger sangen auf den wichtigsten Bühnen.
Erst Feldforscher wie Marius Barbeau, W. Roy Mackenzie, Helen Creighton und zahlreiche andere entdeckten die Volksmusik und die Musik der Indigenen. Wenn man von kanadischer Musik sprach, so war es nun die Gesamtheit der Folkmusik, die man im Land antraf. Doch blieb die Musikausbildung konservativ, d. h. stark angebunden an Großbritannien und Frankreich. Dennoch entstanden in den 1930er-Jahren Musikerverbände, die nach dem Krieg die Suche nach kanadischer Identität auch in der Musik stärkten. Auch wurde diese Musik vom Staat gefördert, Sammlungen traditioneller und indianischer Musik inspirierten die aufgeschlossenere Generation. Publikationen wie The Canadian Music Journal (1956–1962), Opera Canada (seit 1960) und The Canada Music Book (1970–1976) untermauerten diese Entwicklung. Die Abkopplung der kanadischen Musik von der ausländischen Avantgarde endete.
Kanadische Musiker beeinflussten die westliche Musik, wie etwa Rock- und Popmusik, in erheblichem Ausmaß, wofür Namen wie Bryan Adams, Paul Anka, Michael Bublé, Leonard Cohen, Céline Dion, Nelly Furtado, Avril Lavigne, Joni Mitchell, Alanis Morissette, Shania Twain oder Justin Bieber stehen.
Bekannte Vertreter der Rockmusik sind Rush, Alannah Myles, Billy Talent, die Crash Test Dummies, Nickelback, Saga, Steppenwolf und Neil Young.
Zu den bedeutenden Jazzmusikern zählen Paul Bley, Maynard Ferguson, Diana Krall, Moe Koffman und Oscar Peterson.
Avril Lavigne, Sarah McLachlan, Sloan und weitere Musiker haben sich der Initiative Canadian Music Creators Coalition (CMCC)[165] angeschlossen und kündigten in einer Grundsatzerklärung[166] an, künftig wieder für sich selbst sprechen zu wollen. Prozesse und das Digital Rights Management (DRM), vor allem aber die staatliche Förderung seien zu verbessern. Die CMCC forderte die Regierung auf, die Künstler gegen die Vermarktungspolitik meist ausländischer und auf einen ausländischen Markt gerichteter Musikkonzerne zu unterstützen.
Immer noch von großer Bedeutung ist die Country-Musik, die auch von zahlreichen Indianern gespielt wird. Die Canadian Country Music Association ehrt jährlich die bedeutendsten Künstler mit der Aufnahme in die Canadian Country Music Hall of Fame. Wichtige Interpreten sind bzw. waren etwa Wilf Carter, Hank Snow und Gordon Lightfoot.
Auf dem Gebiet der klassischen Musik ist der bekannteste Kanadier sicherlich Glenn Gould (1932–1982), der einer breiteren Öffentlichkeit als begnadeter Interpret vor allem der Werke Johann Sebastian Bachs bekannt ist. Berühmtheit erlangte der damals 22-Jährige im Jahr 1955 mit einer aufsehenerregenden Einspielung der Goldberg-Variationen. Seit 1987 vergibt eine nach dem Musiker benannte Stiftung[167] den Glenn-Gould-Preis.
Auch die Symphonieorchester in Montreal und Toronto haben Weltruf, die Kammermusik hat einen erstklassigen Rang: Tafelmusik und das St. Lawrence String Quartet haben verschiedene Preise gewonnen. Sänger wie Jon Vickers, Russell Braun und Michael Schade, der Flötist Robert Aitken sowie der Pianist Marc-André Hamelin und die Liedbegleiterin Céline Dutilly sind bekannte Interpreten. Auch Werke der Komponisten R. Murray Schafer und Claude Vivier werden regelmäßig aufgeführt.
Als erster Filmemacher gilt James Freer (1855–1933), ein Farmer, der ab 1897 Dokumentationen vorführte.[168][169] 1917 richtete die Provinz Ontario das Ontario Motion Picture Bureau ein, um Filme zu Unterrichtszwecken drehen zu lassen. Bereits im folgenden Jahr entstand das Canadian Government Motion Picture Bureau.
Auf Anraten von John Grierson, der als Vater des britischen und kanadischen Dokumentarfilms gilt, wurde 1939 der National Film Act verabschiedet, ein Gesetz, das es gestattete, Propagandafilme für Kriegszwecke zu drehen. 1950 wurde das Aufgabenspektrum des dazu gegründeten National Film Board of Canada beauftragt, Kanada den Kanadiern zu erklären, aber auch Nichtkanadiern. Mit der Canadian Film Development Corporation, aus der später Telefilm Canada hervorging, förderte der Staat Filmproduktionen. Das für das Kulturerbe verantwortliche Department of Canadian Heritage stockte 2001 die Mittel für Telefilm Canada auf. Den gleichen Zielen dient die Auszeichnung mit dem Genie Award, die jedes Jahr für die besten kanadischen Filme erfolgt.
Kanada ist auch als Hollywood des Nordens bekannt. Wichtigste Produktionsstätten kanadischer und US-amerikanischer Filme sind heute Vancouver, gefolgt von Montreal und Toronto. Dabei ist Alliance Films das einst größte Medienunternehmen, heute nur noch ein Rechtehändler. Der französische Film ist innerhalb von Kanada häufig erfolgreicher als der englische, weil der Quebecer Filmmarkt von US-Produktionen kaum direkt erreicht wird.
Das kanadische Autorenkino gewinnt dank erfahrener Cineasten wie Atom Egoyan (der bei der Berlinale 2002 Präsident der Jury war), David Cronenberg, Denys Arcand und Léa Pool, aber auch durch junge Filmemacher wie Jean-François Pouliot, Denis Villeneuve, Don McKellar, Keith Behrman und Guy Maddin immer mehr an Bedeutung.
Filmregisseure wie Jean-Claude Lauzon („Night Zoo“ (1987), Léolo (1992)) und Denys Arcand (unter anderem „Der Untergang des amerikanischen Imperiums“ (1986), „Jesus von Montreal“ (1989) und „Joyeux Calvaire“ (1996), „Die Invasion der Barbaren“ (2003)) haben dem kanadischen Film zu internationaler Geltung verholfen.
Bekannte kanadische Schauspieler sind: Mary Pickford, Glenn Ford, Lorne Greene, Raymond Massey, Walter Huston, Jack Carson, Raymond Burr, Christopher Plummer, Donald Sutherland, Kiefer Sutherland, Geneviève Bujold, Keanu Reeves, Dan Aykroyd, Pamela Anderson, Hayden Christensen, Leslie Nielsen, John Candy, Jim Carrey, Michael J. Fox, Mike Myers, William Shatner, Bruce Greenwood, Ryan Gosling, Ryan Reynolds, Carrie-Anne Moss und Sandra Oh. Wie man durch diese Aufzählung erkennen kann, sind viele kanadische Schauspieler häufig in Hollywood-Produktionen tätig und genießen internationales Ansehen.
Das kanadische Theater, das aus einer starken mündlichen Tradition hervorgeht, hat nicht nur weltweit bekannte Regisseure wie Robert Lepage oder Denis Marleau hervorgebracht, sondern auch eine große Anzahl von Theaterautoren, die in verschiedene Sprachen – unter anderem auch ins Deutsche – übersetzt werden. So sind in jüngster Zeit zum Beispiel Texte von Michel Marc Bouchard, Daniel Danis, Michel Tremblay, George F. Walker, David Young und Colleen Wagner von deutschen Ensembles aufgeführt worden.
Die kanadische Literatur ist anfangs dadurch gekennzeichnet, dass sie häufig von Autoren stammt, die entsprechend ihrer ethnischen Herkunft bestimmte Erwartungen an das Land herantrugen.[170] Daher erscheint das Land oft als abweisend mit Blick auf seine Natur, als kulturelle Wüste, die von außen belebt wird, und als Rohstoff für Karriere und Investitionen. Dabei spielten auch Erwartungen und Stereotype des Publikums von der Wildnis, unvorstellbarer Weite, von der Einführung der Zivilisation vor allem durch Europäer eine große Rolle. Doch überwiegt inzwischen der Drang, die eigene Kultur, die sich entwickelt hat, in ihrem Reichtum zu erfassen, ohne die Wurzeln abzuschneiden.
Historisch gesehen flossen vor allem französische, englische und irische Stile zusammen, die in ihren Heimatländern en vogue waren. Doch schon in den Reiseberichten entwickelte sich ein kanadisch geprägtes Genre, wie bei Samuel Hearne (1745–1792), Alexander MacKenzie, David Thompson, Catharine Parr Traill (1802–1899) oder Anna Jameson (1794–1860), wobei das Spektrum vom romantisierenden Abenteuerbericht (John R. Jewitt, 1783–1821) bis zur präzisen Analyse reicht (Susanna Moodie: Roughing It in The Bush, oder Forest Life in Canada, 1852). Mit der Konföderation (1867) stellte sich die Frage nach der nationalen Kultur. Ab Ende des 19. Jahrhunderts dominierten vier Figuren die literarische Szene: Duncan Campbell Scott (1862–1947), Charles G. D. Roberts (1860–1943), Archibald Lampman (1861–1899) und Bliss Carman (1861–1929), die auch als Confederation Poets (oder auch „Confederation Group“) bekannt waren.
Während des 19. Jahrhunderts drangen indigene (igloo) und lokale Wortschöpfungen (moose) in die Literatur ein, aber auch französische (gopher) in die englische und umgekehrt. Dennoch wird die englische Sprache im ganzen Land verstanden und von übergreifenden Sprachstandards dominiert. In der französischen Literatur kommt als weiteres Element eine starke Anbindung an Frankreich und seinen Lebensstil hinzu, woraus sich eine Skepsis gegenüber dem als britisch aufgefassten Rest-Kanada partiell erklärt.
Ein hervorstechendes Merkmal kanadischer Literatur ist der Humor, der allerdings eher untergründig, zuweilen schwarz, und oft als Understatement eingesetzt wird. Dabei spielen regionale Traditionen des Erzählens und des Anekdotischen eine wichtige Rolle, weniger die Themenwahl – es sei denn, es handelt sich um lokale Besonderheiten oder Unterschiede zwischen den ethnischen Gruppen. Zu den häufig anzutreffenden Motiven zählt die „garrison mentality“ (Bunkermentalität), die Entfremdung von der Heimat, in die man zurückkehrt, die Fremdheit im eigenen Land oder der spezifischen Kultur, aber auch das Zelebrieren der Wildnis, die für spirituelle Gesundung sorgt.
Kanadier sind besonders ausgeprägt an der Geschichte ihrer Vorfahren interessiert, und so existiert eine große Zahl von biographischen Versuchen zu den historisch bedeutsamen Männern und Frauen. Doch auch dort sind Klischees fast unausweichlich. So gilt das katholische Québec als mysteriös, Ontario als zwischen moralischer Klarheit und Lavieren hin- und hergerissen, die Prärien als isolierend und besitzergreifend, die Westküste als Projektionsfläche für Hoffnungen und Erwartungen, die man selbst entlarven muss. Dabei steht das Landleben überproportional im Vordergrund, während die Städte lange beinahe ignoriert wurden. Dagegen waren Autoren wie Frances Brooke (1724–1789), Susanna Moodie (1803–1885), Sara Jeannette Duncan (1861–1922) und Nellie McClung (1873–1951) die Analytikerinnen des politischen Lebens, das sich in den Städten ballt.
Ein Gegensatz besteht zwischen der Wahrnehmung Europas und der des Nachbarn USA. Europa gilt als Hort der Verfeinerung, aber auch der extremen Regionalisierung, der Nachbar als Land der sozialen Härte und der Fixierung auf ökonomischen Erfolg.
Der Erste Weltkrieg brachte die Außenwelt wieder stärker in den Blick, und zugleich schärfte die Einwanderung die Aufmerksamkeit auf die zahlreichen Kulturen, auch die der Indianer, die nun selbst begannen, sich auszudrücken. Die Malerin und Autorin Emily Carr (1871–1945) war hier für den Westen von größter Bedeutung, wenn sie auch in British Columbia lange auf Ablehnung stieß. Die Weltwirtschaftskrise brachte eine zunehmende Beschäftigung mit sozialen Problemen mit sich, der Zweite Weltkrieg wiederum zwang zur Beschäftigung mit Fragen der Macht, der Not, des Todes und wiederum der Heimkehr. Nach dem Krieg unterwarf Merrill Denison (1893–1975) den übertriebenen Nationalismus einer satirischen Betrachtung, und auch Autoren der Linken kritisierten den politischen und wirtschaftlichen Weg und die zunehmende Dominanz der USA. Zugleich machten sich in Québec antiklerikale Autoren deutlicher bemerkbar. Unter dem öffentlichen Optimismus der 1950er und 1960er Jahre entdeckten Malcolm Lowry (1909–1957) (Under the Volcano, 1947) und Ethel Wilson (1888–1980) (Swamp Angel, 1954) Alkoholprobleme und die Enge des Frauenlebens in dieser Zeit.
Materielle Unterstützung und ein größeres Publikum sorgten in den 60er-Jahren für ein Anwachsen des literarischen Marktes, Zeitschriften wie Canadian Literature und Journal of Canadian Studies erschienen, dazu kamen Paperbackausgaben, die erschwinglicher waren. Nischenmärkte entstanden, deren Publikum dennoch Autoren ernähren konnte. Sowohl die einzelnen Kulturen, als auch Frauen meldeten sich verstärkt zu Wort, wie etwa Margaret Atwood.
Seit den 70er-Jahren hat sich das Interesse an kanadischer Literatur verstetigt. So sind Autoren wie Leonard Cohen, Pierre Vallières, Margaret Atwood, Michel Tremblay und Michael Ondaatje auch außerhalb der Staatsgrenzen bekannt. Zugleich entstand ein riesiger Markt für populäre Literatur innerhalb des Landes, wie die von Joy Fielding oder Douglas Coupland (Generation X).
Nach etwa 1985 wurden staatliche Mittel in einer konservativeren Phase zurückgefahren. Verlage wie Coach House Press, Deneau, Williams-Wallace mussten schließen. Zudem ließ Kanada stärkere ausländische Konkurrenz zu, vor allem aus den USA. Autoren wie Timothy Findley (1930–2002) versuchten sich gegen Restriktionen zu wehren, indianische Literatur fand Vertreter in Eden Robinson (Haisla, geb. 1968), Jeannette C. Armstrong (Okanagan), die das Schulsystem kritisierte, der Satiriker Thomas King (Cherokee) oder der Dramatiker Tomson Highway (Cree). Daneben traten eher poetische Autoren wie Rita Joe (Mi’kmaq), Marilyn Dumont (Métis) oder Alootook Ipellie (Inuit).
2013 erhielt Alice Munro den Nobelpreis für Literatur als „Virtuosin der zeitgenössischen Kurzgeschichte“.
Wie in den meisten Künsten, so ignorierten die ersten Zuwanderer aus Europa weitgehend die Kunst der Ureinwohner. Sie brachten schon in ihren ersten Wohngebäuden und befestigten Hofanlagen sowie naturgemäß im Festungsbau (zum Beispiel Louisbourg) und in Stadtanlagen europäische Traditionen mit. Auch die Dörfer des frankophonen Kanada lagern sich wie in Frankreich um die Kirche, wobei die Missionskirchen und die Kirchen von Québec meist als Vorbilder dienten. Als Material herrschten Stein und Holz vor, Ziegel sind selten. Ähnlich wie in der Bildhauerei kamen die in Frankreich und England vorherrschenden Stile jedoch, bedingt durch die Kommunikationsverhältnisse, mit deutlicher Verspätung an. Das galt auch für die Übernahme der Klassik, nachdem die Briten Kanada erobert hatten.
Dennoch nahm die Malerei zwangsläufig die Ureinwohner auf, denn sie sollten für die Berichterstattung bei Hof dargestellt werden. Sie waren zum Teil von großer Genauigkeit, wie die Indianer- und Inuit-Porträts von John White (etwa 1540 bis etwa 1593), oder die Zeichnungen von Louis Nicolas (Codex canadiensis). Ende des 18. Jahrhunderts brachten Briten und die aus den USA geflohenen Loyalisten neue Einflüsse, die sich vor allem in den neuen Siedlungen, wie Toronto, dominierend bemerkbar machten. Es kam sogar zu einem Goldenen Zeitalter der Québecer Malerei, wobei der Stil europäisch blieb, doch die Motive wurden kanadischer. Der Schweizer Peter Rindisbacher dokumentierte etwa seine Reise durch die Hudson Bay in die Red-River-Kolonie, Paul Kane reiste durch den halben Kontinent.
In der Architektur bevorzugte man neo-klassische und neo-gotische Motive, wie in Europa, doch erhielt der britische Einfluss immer mehr Übergewicht. Mit dem repräsentativen Ausbau Ottawas und jeder Provinzhauptstadt versuchte man eine spezifisch kanadische Tradition auszudrücken. Zwischen 1873 und 1914 herrschten historisierende Stile vor, wobei sich die mitgebrachten Stile anderer europäischer Völker, wie der Italiener bemerkbar machten. Mit der Industrialisierung drangen neue Bautypen, wie Stahlbrücken oder Bahnhöfe vor, neue Materialien, vor allem Metalle dominierten. Dazu kamen Glas und schließlich Beton. James Wilson Morrice gilt als Vater des Modernismus in der Malerei. In der Skulptur herrschten historische Monumente auf Plätzen vor, vor allem Kriegsdenkmäler nach dem Ersten Weltkrieg. Doch weiterhin herrschte hierin Europa vor, bis hin zum Art déco.
Die Group of Seven versuchte eine kanadische Malerei zu entwickeln; sie bezog ihre Inspiration aus der Landschaft. Als eine der ersten nahm Emily Carr dabei nicht nur die spezifische Landschaft des Westens auf, sondern auch die grandiose Kunst der Indianer der Pazifikküste.
John Lyman gründete 1939 die Contemporary Arts Society, und über Quebec kamen kubistische Einflüsse, dort entstand die Gruppe der Automatistes. Gegen sie und den Surrealismus entstanden die Plasticiens, allen voran Guido Molinari und Claude Tousignant, Struktur- und Farbfragen traten stärker in den Vordergrund. Ähnlich in Toronto, wo sich Jack Bush und Harold Town gegen den abstrakten Expressionismus wandten. Dabei versuchten diese Gruppen sich zugleich gegen den Einfluss der USA abzusetzen. Ähnliches galt für Bildhauer wie Robert Murray oder Armand Vaillancourt. Hingegen unterscheidet sich die Architektur kaum von der internationalen. Der Fotograf Yousuf Karsh gehörte zu den bedeutendsten Porträtfotografen des 20. Jahrhunderts.
In der Bildenden Kunst hat sich Kanada in Europa durch innovative Künstler einen Namen gemacht. Jeff Wall, Rodney Graham, Ken Lum und Geneviève Cadieux haben fotografische Techniken auf neuartige Weise für sich genutzt.
Jana Sterbak hat außergewöhnliche konzeptuelle Environments geschaffen.
Die Produktion von Nahrungsmitteln hängt stark von den natürlichen Bedingungen ab. Daher weisen die Regionalküchen, wie etwa die der Küstensäume und der Graslandschaften der Prärieprovinzen, entsprechende Schwerpunkte auf. Während etwa an der Atlantikküste der Fang von Hummern, genauer von Hummerartigen (Lobster) einen wichtigen Wirtschaftszweig darstellt, war es an der Westküste der von Wildlachs; letzter wurde allerdings von Lachszuchten fast vollständig verdrängt, so dass einige Lachsarten, die noch vor wenigen Jahren in riesigen Laichzügen zu bewundern waren, inzwischen zu den bedrohten Tierarten gerechnet werden müssen.
Neben dem Umgang mit den natürlichen Ressourcen spielen aber auch kulturelle Unterschiede eine beträchtliche Rolle. Der französische Einfluss in Québec ist nicht zu übersehen, es gibt zahlreiche Restaurants mit der entsprechenden Küche. Die Prärieprovinzen sind hierin sehr stark vom mittleren Westen der USA beeinflusst, während sich im äußersten Westen ein starker britischer Einfluss bemerkbar macht, wo der englische Tee im Alltag immer noch seinen Platz hat.
Im Süden Kanadas, vor allem auf der Niagara-Halbinsel und im Okanagan-Gebiet sowie im Südosten von Vancouver Island in British Columbia wird Wein angebaut. Der über 200 Jahre alte Weinanbau nahm einen neuen Aufschwung, da ab 1974 erstmals neue Weinbaulizenzen ausgegeben wurden, und weil die Weinbauverbände (Vintners Quality Alliance) auf höhere Qualitäten drängten. Kanadische Weine tragen etwa die Hälfte zum Gesamtkonsum des Landes bei, wobei bis 2006 Vincor International und Andres Wines dominierten. Vincor wurde allerdings vom US-Weinproduzenten Constellation Brands aufgekauft.
Spirituosen können nur in besonderen Geschäften oder in Restaurants gekauft werden, die die Bezeichnung Licensed Premises tragen. Viele Restaurants gestatten ihren Gästen, eigenen Wein, Bier oder Ahornsirup mitzubringen. Das Mindestalter für den Alkoholkauf liegt zwischen 18 und 19 Jahren.
Die vorherrschende Kaffee- und Fast-Food-Kette ist Tim Hortons, kurz Tim's oder Timmies. Das Unternehmen wurde 1964 in Hamilton, Ontario gegründet und 2014 von Burger King Worldwide Inc. übernommen und gehört damit mehrheitlich zur brasilianische Investmentgesellschaft 3G Capital. 2016 gab es über 3.800 Niederlassungen in Kanada.[171] Der schärfste Konkurrent beim Fast Food ist McDonald’s[172], im Kaffeesektor das US-Unternehmen Starbucks.
Der Sport in Kanada ist vielfältig und umfasst zahlreiche Winter- und Sommersportarten.[173] Als Nationalsportart seit 1859 offiziell anerkannt war bis 1994 nur das auf indianische Wurzeln zurückgehende Lacrosse. Es gilt seit 1994 als nationale Sommersportart. Seit 1994 ist Eishockey die nationale Wintersportart. Kanada gilt nicht nur als Mutterland des Eishockeys, sondern gehört auch zu den weltweit erfolgreichsten Ländern. Sieben kanadische Mannschaften sind in der NHL, der bedeutendsten Profiliga der Welt, vertreten. Auch im Lacrosse ist Kanada überaus erfolgreich und besiegte beim World Lacrosse Championship von 2006 in London die USA.
Die bei Zuschauern beliebteste Sportart im Sommer ist neben Lacrosse Canadian Football, das große Ähnlichkeiten mit dem American Football aufweist. Das Meisterschaftsendspiel, der Grey Cup, weist bei im Fernsehen übertragenen Sportereignissen die höchste Einschaltquote auf. Ebenfalls auf Interesse stoßen Baseball, Basketball, Cricket, Curling, Fußball, Rugby Union und Softball. Die am häufigsten ausgeübten Einzelsportarten sind Eislaufen, Golf, Leichtathletik, Ringen, Schwimmen, Skateboarden, Skifahren, Snowboarden und Tennis. Da das Land überwiegend ein kühles Klima besitzt, sind die Erfolge bei Wintersportarten tendenziell zahlreicher als bei Sommersportarten.
Kanada war Gastgeber zahlreicher internationaler Sportveranstaltungen, darunter die Olympischen Sommerspiele 1976 in Montreal und die Olympischen Winterspiele 1988 in Calgary. Die Olympischen Winterspiele 2010 wurden in Vancouver ausgerichtet. Zudem waren kanadische Städte Ausrichter von vier Commonwealth Games und zahlreichen Weltmeisterschaften.
Special Olympics Kanada wurde 1969 gegründet und nahm mehrmals an Special Olympics Weltspielen teil. Der Verband hat seine Teilnahme an den Special Olympics World Summer Games 2023 in Berlin angekündigt. Die Delegation wird vor den Spielen im Rahmen des Host Town Programs von München betreut.[174][175]
Darüber hinaus gibt es bewegliche Feiertage, wie z. B. den Tag der Familie oder den Louis Riel Day.[176][177]
G6:
Deutschland |
Frankreich |
Vereinigtes Königreich |
Italien |
Japan |
Vereinigte Staaten
G7: Kanada |
(G8: Russland – Mitgliedschaft suspendiert)
Australien |
Belgien |
Chile |
Costa Rica |
Dänemark |
Deutschland |
Estland |
Finnland |
Frankreich |
Griechenland |
Irland |
Island |
Israel |
Italien |
Japan |
Kanada |
Kolumbien |
Lettland |
Litauen |
Luxemburg |
Mexiko |
Neuseeland |
Niederlande |
Norwegen |
Österreich |
Polen |
Portugal |
Schweden |
Schweiz |
Südkorea |
Slowakei |
Slowenien |
Spanien |
Tschechien |
Türkei |
Ungarn |
Vereinigtes Königreich |
Vereinigte Staaten
Kanada |
Mexiko |
Vereinigte Staaten
Albanien Albanien |
Belgien Belgien |
Bulgarien Bulgarien |
Danemark Dänemark |
Deutschland Deutschland |
Estland Estland |
Finnland Finnland |
Frankreich Frankreich |
Griechenland Griechenland |
Island Island |
Italien Italien |
Kanada Kanada |
Kroatien Kroatien |
Lettland Lettland |
Litauen Litauen |
Luxemburg Luxemburg |
Montenegro Montenegro |
Niederlande Niederlande |
Nordmazedonien Nordmazedonien |
Norwegen Norwegen |
Polen Polen |
Portugal Portugal |
Rumänien Rumänien |
Slowakei Slowakei |
Slowenien Slowenien |
Spanien Spanien |
Tschechien Tschechien |
Turkei Türkei |
Ungarn Ungarn |
Vereinigtes Konigreich Vereinigtes Königreich |
Vereinigte Staaten Vereinigte Staaten
Beobachter:
Schweden Schweden
56-109Koordinaten: 56° N, 109° W

Russland (russisch  Россия?/i Rossija [rɐˈsʲijə]), amtlich die Russische Föderation (russisch  Российская Федерация?/i Rossijskaja Federazija ‚Russländische Föderation‘[A 2]), ist ein Bundesstaat in Osteuropa und Nordasien, mit der Exklave Kaliningrad in Mitteleuropa. Mit etwa 17 Millionen Quadratkilometern ist Russland der nach der Fläche größte Staat der Welt und umfasst etwa ein Neuntel der Landmasse der Erde. Mit 144,5 Millionen Einwohnern (2019) steht es an neunter Stelle der bevölkerungsreichsten Staaten und ist zugleich einer der am dünnsten besiedelten.
Der europäische Teil des Staatsgebiets ist viel dichter besiedelt und verstädtert als der über dreimal so große asiatische Teil: Etwa 77 % der Bevölkerung (110 Millionen Einwohner) leben westlich des Urals. Die Hauptstadt Moskau ist eine der größten Städte und Metropolregionen der Welt. Das zweitwichtigste Zentrum ist Sankt Petersburg, das von 1712 bis 1918 Hauptstadt war und heute vor allem ein wichtiges Kulturzentrum bildet. Die nächstgrößten Millionenstädte sind Nowosibirsk, Jekaterinburg, Kasan und Nischni Nowgorod. Insgesamt gibt es in Russland 15 Millionenstädte und fast 70 Agglomerationen mit über 500.000 Einwohnern.
Die föderale Gliederung Russlands besteht aus acht Föderationskreisen und 85 Föderationssubjekten. Russland ist ein über 100 Ethnien zählender Vielvölkerstaat, wobei ethnische Russen fast 80 % der Bevölkerung ausmachen.
Russland ist heute ein Schwellenland im Bereich des oberen mittleren Einkommens.[7] Nach der Erholung von der postkommunistischen Transformationskrise der 1990er-Jahre wurde das Land die heute elftgrößte,[8] nach Kaufkraftparität sechstgrößte Volkswirtschaft der Welt, direkt hinter Deutschland.[9] seine Rohstoffreserven sind mit etwa 20 bis 30 % die wahrscheinlich größten der Welt,[10][11][12] mit erheblichen Vorkommen von Primärenergieträgern – vor allem Erdgas. Russland ist seit Mitte der 1980er-Jahre, damals noch als Teil der Sowjetunion, einem stetigen wirtschaftlichen, demografischen und militärischen Leistungsverfall ausgesetzt. 1991 mit dem Zerfall der Sowjetunion noch als Supermacht klassifiziert, entfaltet es nur noch das Potenzial als Regionalmacht – wie die Atommächte Israel, Pakistan und Nordkorea.[13]
Russland ist seit 1946 ständiges Mitglied des Weltsicherheitsrates und zudem Mitglied von WTO, OSZE, APEC und der SCO sowie führendes Mitglied der regionalen Organisationen GUS, OVKS und EAEU.
Die Russische Föderation ist „Fortsetzerstaat“[14] der Sowjetunion in internationalen Organisationen. Dem sowjetischen Bundesstaat voraus gingen das Russische Kaiserreich, das Zarentum Russland und ursprünglich das Großfürstentum Moskau, ein Teilfürstentum des früheren ostslawischen Reiches Kiewer Rus. Um 1990 endete der Kalte Krieg, Russland wurde zwischenzeitlich ein wenig demokratischer und näherte sich leicht „dem Westen“ an. Die damalige Verfassung sieht für Russland eine semipräsidentielle Demokratie vor, die Verfassungswirklichkeit entspricht gemäß vielen Demokratie-Indizes heute jedoch der einer Autokratie, zum Teil auch den Modellen defekter Demokratien bzw. Postdemokratien.[15][16][17] Von russischer Seite wird hierfür gelegentlich selbst der Begriff „gelenkte Demokratie“ gebraucht.[18] Auch Korruption und Menschenrechtsverletzungen sind bis heute weit verbreitet.
Russlands Anteil am globalen Bruttoinlandsprodukt ist seit Beginn der Sanktionen 2014 als Folge der Annexion der Krim und verschärft mit dem Überfall auf die Ukraine 2022 von 4 auf 2,85 % (2022) gefallen.[19] Vor allem seit dem Angriff auf das Nachbarland sind die wirtschaftlichen und politischen Beziehungen zum Westen sehr stark belastet.[A 3]
Russland ist mit 17.075.020 km² das mit Abstand flächengrößte Land der Erde. Es umfasst 11 % der Weltlandfläche, das entspricht in etwa der Fläche Australiens und Europas zusammen. Bis auf die Tropen sind alle Klimazonen vertreten.
Von Westen nach Osten erstreckt sich Russland auf einer Gesamtlänge von 9000 km, von 19° östlicher bis 169° westlicher Länge über zwei Kontinente. Auf Europa entfallen 23 % der Landfläche, auf Asien 77 %. Von Süden nach Norden beträgt die Ausdehnung bis zu 4000 km, vom 41. bis zum 81. Grad nördlicher Breite.
Auf dem Gebiet Russlands befinden sich einige der längsten Flüsse sowie der älteste und tiefste Binnensee der Welt (Baikalsee). Wenn man die Reliefstruktur und die Flusssysteme Russlands miteinander vergleicht, so entsteht ein Gitternetz aus breitenparallel verlaufenden Wasserscheiden bzw. dem Steppengürtel im Süden und den meridional ausgerichteten Stromwegen.
Russland hat neben der Volksrepublik China mit 14 die größte Anzahl Nachbarstaaten mit einer gemeinsamen Landgrenze. Die Gesamtlänge der Landesgrenzen beträgt 20.027 km. Russland grenzt des Weiteren an fünf Meere, wobei die Küstenlinie 37.653 km umfasst.
Das russische Kernland grenzt an die Staaten Norwegen (196 km) und Finnland (1340 km), gefolgt von einem kurzen Küstenstreifen zur Ostsee. Zudem teilt sich Russland eine Grenze mit den baltischen Ländern Estland (334 km) und Lettland (217 km), weiter südlich gefolgt von Belarus (959 km) und der Ukraine (1586 km, mit Landgrenze der Krim). Das Schwarze Meer trennt die europäischen Grenzen Russlands von den asiatischen. Im Kaukasus grenzen Georgien (723 km) und Aserbaidschan (284 km) an. Es folgt ein Küstenstreifen am Kaspischen Meer und eine lange gemeinsame Grenze mit Kasachstan (6846 km). In Ostasien grenzt Russland erstmals an die Volksrepublik China (etwa 40 km) und dann an die Mongolei (3485 km). Danach trifft das russische Hoheitsgebiet zum zweiten Mal mit chinesischem zusammen (3605 km). Mit Nordkorea (19 km) besteht die letzte Landverbindung zu einem anderen Staat.
Danach folgen die Küstenlinien zum Japanischen Meer, dem Ochotskischen Meer, zum Pazifischen Ozean und schließlich zur Beringsee. Über die nur etwa 85 km schmale und 30 bis 50 m tiefe Beringstraße ist Russland im äußersten Osten von Alaska getrennt. Die inmitten der Beringstraße befindliche russische Große Diomedes-Insel liegt nur 4 km von der US-amerikanischen Kleinen Diomedes-Insel entfernt. Der gesamte nördliche Teil des Landes grenzt an den Arktischen Ozean. Dort liegen verschiedene zu Russland gehörende Inseln, als nördlichste Franz-Josef-Land. Russland betrachtet zudem noch weitere Gebiete des Arktischen Ozeans und der Eisfläche als Teil seines Hoheitsgebietes.
Neben dem Kernland besitzt Russland noch eine Exklave, den nördlichen Teil des ehemaligen Ostpreußen, die heutige Oblast Kaliningrad. Dieses Gebiet, über das 1945 die Sowjetunion die territoriale Souveränität beanspruchte, grenzt an Litauen (227 km) und den südlichen Teil des früheren Ostpreußen, der jetzt zu Polen gehört (206 km). Es ist somit vollständig von EU-Ländern umgeben.
Russland ist in elf Zeitzonen eingeteilt (von UTC+2 bis UTC+12), wobei mit der Abschaffung der Zeitumstellung im Jahr 2011 bis 2014 überall ganzjährig die Sommerzeit galt. Nach anhaltender Kritik aus der Bevölkerung kehrte Russland am 26. Oktober 2014 zur Normalzeit zurück.
Russland umfasst eine Vielzahl unterschiedlicher Naturräume, die vielfältige Potenziale, aber auch sehr verschiedenartige Nutzungen aufweisen. Russland gliedert sich geographisch betrachtet hauptsächlich in die acht Großlandschaften (etwa in West-Ost-Richtung):
Mit 120.000 Flüssen und Strömen und fast zwei Millionen Seen ist Russland sehr wasserreich. Der Waldgürtel, der zwei Drittel der Fläche einnimmt, wirkt zusammen mit dem Niederschlagsüberschuss als riesiger Wasserspeicher, der ein ganzes Netz an Wasserläufen speist.
Im europäischen Teil Russlands ist der wichtigste Fluss die Wolga. Sie ist der längste Fluss Europas und verläuft ausschließlich in Russland. Zusammen mit ihren beiden Nebenflüssen Kama und Oka entwässert sie einen großen Teil der Osteuropäischen Ebene nach 3534 km zum Kaspischen Meer im Südosten. Als Wasserweg hat die Wolga besondere Bedeutung, da sie Osteuropa mit Zentralasien verbindet. Der Nordrussische Landrücken bildet die Wasserscheide zwischen Wolgabecken und Weißem Meer bzw. Barentssee im Norden. Eine große Bedeutung für die slawischen Staaten besitzt der Dnepr (auch Dnjepr genannt). Der Strom entsteht westlich von Moskau und fließt anschließend durch Belarus und die Ukraine, wo er ins Schwarze Meer mündet. Über den Dnepr-Bug-Kanal ist er mit den polnischen Flüssen Bug und Weichsel sowie mittelbar über das Oginskische Kanalsystem mit der Memel verbunden, was den Dnepr zu einer wichtigen Wasserstraße macht.
Die längsten Flüsse Russlands liegen in Sibirien und dem fernöstlichen Russland. Der Ob entspringt im südsibirischen Altai und mündet in das Nordpolarmeer. Der mit seinem Quellfluss Katun über 4300 km lange Fluss bildet – zusammen mit dem Irtysch – eines der längsten Flusssysteme Asiens mit einer Gesamtlänge von über 5400 km. Eine noch etwas längere Fließstrecke hat das Flusssystem des Jenissei, dessen Wasser (teilweise) aus der Mongolei nach Norden durch Westsibirien zum Nordpolarmeer fließt. Sein Hauptzufluss, die Angara, stellt den einzigen Abfluss des Baikalsees dar. Der Jenissei führt dem Nordpolarmeer jährlich etwa 600 km³ Wasser zu. Damit verzeichnet er die höchste Durchflussmenge aller russischen Flüsse. Die rund 4300 km lange Lena, der längste Strom, der ausschließlich in Russland verläuft und dessen Einzugsgebiet sich ausschließlich in Russland befindet, entspringt nur 5 km vom Baikalsee entfernt. Sie fließt zunächst in nordöstliche Richtung, biegt nach dem Einmünden des Aldan nach Norden und mündet in einem ausgedehnten Delta in die Laptewsee, ein Nebenmeer des Nordpolarmeers. Weitere wichtige Flüsse, die ins Nordpolarmeer münden, sind die Petschora, die Nördliche Dwina, die Chatanga sowie die Kolyma und die Indigirka.
Ein weiteres wichtiges Flusssystem bildet der Amur mit seinem Zufluss Schilka. Mit dessen Quellfluss Onon hat es eine Gesamtlänge von etwa 4400 km und führt vom Nordosten der Mongolei in östlicher Richtung entlang der chinesischen Grenze zur Pazifikküste. Amur und Anadyr sind die größten russischen Flüsse, die in den Pazifischen Ozean fließen.
Viele andere Ströme sind als Verkehrswege und als Energiequellen bedeutend, oder sie dienen in trockenen Regionen der Bewässerung. Der Don nimmt dabei eine herausragende Stellung ein. Er liegt im bevölkerungsreichen Osteuropäischen Tiefland und entwässert nach Süden in das Asowsche Meer. Andere wichtige Flüsse sind Moskwa, Selenga, Tobol, Steinige Tunguska, Untere Tunguska, Ural und Ussuri.
In Russland gibt es, besonders im ehemals vergletscherten nordwestlichen Teil des Landes, viele natürliche Seen. Das Kaspische Meer ist mit 386.400 km² der weltgrößte Binnensee. Der Seespiegel des Salzwassersees befindet sich etwa 28 m unterhalb des Meeresniveaus. Da das Kaspische Meer keinen Abfluss hat, entweicht Wasser nur durch Verdunstung, wodurch es bei dem hier herrschenden trockenen Klima zur Auskristallisation von Salzen kommt. Der Baikalsee hat als ältester Süßwassersee eine Tiefe von 1642 m, womit er nicht nur der tiefste See, sondern zugleich auch das größte Reservoir flüssigen Süßwassers weltweit (ca. ein Fünftel aller flüssigen Süßwasserreserven) ist. Weitere wichtige und große Seen sind Ladogasee (größter Binnensee Europas), Onegasee und Taimyrsee.
Rund 40 % der Fläche Russlands ist von Gebirgen überzogen. Dabei bildet der Ural die Trennlinie zwischen dem europäischen und asiatischen Teil des Landes; er stellt allerdings wegen seiner geringen Höhe von knapp 2000 m (Narodnaja, 1895 m) keine wirkliche Barriere dar.[20] Östlich des Ural erstreckt sich das sehr flache Westsibirische Tiefland, das bis zum Fluss Jenissej reicht und von weiträumigen Sumpflandschaften durchzogen ist. Südöstlich wird das Westsibirische Tiefland durch das Mittelsibirische Bergland abgeschlossen, das sich bis zum Fluss Lena erstreckt und im Norden zum schmalen Nordsibirischen Tiefland abfällt. Zum Mittelsibirischen Bergland gehören die Gebirge Sajan (Munku Sardyk, 3491 m) und das höchste Gebirge Sibiriens, der Altai (Belucha, 4506 m), im russisch-kasachisch-chinesisch-mongolischen Grenzgebiet. Östlich der Lena erhebt sich das Ostsibirische Bergland, das sich in verschiedene Gebirgsketten, wie das Werchojansker Gebirge (2389 m in Orlugan) und Tscherskigebirge (Pobeda, 3003 m), verzweigt und Höhen bis gut 3000 m erreicht. Die Halbinsel Kamtschatka ist durch ihre 160 Vulkane mit Höhen bis zu 4688 m geprägt, von denen 29 noch aktiv sind.
Weitere Gebirge in Russland sind: Baikalgebirge, Chibinen, Kaukasus, Kolymagebirge, Putorana-Gebirge, Stanowoigebirge, Stanowoihochland, Tannu-ola-Gebirge. Der höchste Berg in Russland ist der Elbrus (5642 m) im Kaukasus. Neben weiteren 5000ern im Kaukasus sind der Kasbek mit 5047 m und die Kljutschewskaja Sopka mit 4750 m bekannte Gipfel.
Russland besitzt ein ausgeprägtes Naturschutzsystem mit einer langen Tradition. Zu den klassischen russischen Schutzgebietskategorien wie den streng geschützten Sapowedniki oder den Sakasniki kamen seit den 1980er-Jahren die nach internationalen Kriterien errichteten Nationalparks und andere internationale Schutzgebietsklassen hinzu. Russland besitzt flächenmäßig eines der größten Schutzgebietssysteme der Welt:
Die Jahresdurchschnittstemperatur für Russland wird mit −5,5 °C angegeben.[21] Große Teile des Landes sind vom Kontinentalklima mit heißen Sommern und sehr kalten Wintern geprägt. Je weiter man in Richtung Osten des Landes reist, desto deutlicher spürt man die prägenden Temperaturen zu den verschiedenen Jahreszeiten, das heißt, der Sommer ist extrem heiß und die Temperaturen in den Wintermonaten mitunter eisig kalt. Kaum ein anderes Land bietet solche Temperaturunterschiede wie Russland. Die südliche Hälfte des Fernen Ostens hat Monsunklima. Die durchschnittlichen Januartemperaturen liegen mit Ausnahme der Schwarzmeerküste überall unter dem Gefrierpunkt. In Ostsibirien sinken sie bis auf −35 bis −60 °C ab, sind aufgrund der meist sehr niedrigen Luftfeuchtigkeit jedoch leichter auszuhalten. Die Sommertemperaturen sind sehr unterschiedlich. Die Durchschnittstemperaturen im hohen Norden liegen bei +1 bis +2 °C, in den Halbsteppen und Steppengebieten des Südens hingegen bei +24 bis +25 °C.
Die Klima-, Vegetations- und Ökozonen verlaufen in Russland weitgehend breitenkreisparallel, so dass eine Nord-Süd-Abfolge entsteht:
Im Nördlichen Eismeer herrscht die lebensfeindliche Kältewüste. Dies betrifft unter anderen den nördlichen Teil der Taimyrhalbinsel und weitere dort befindliche Inseln. Es herrscht ein ausgeprägtes Eisklima, in dem es kaum Pflanzen gibt. In dieser Zone gibt es nur wenige ständige Siedlungen. Die Durchschnittstemperaturen steigen nur für drei Monate knapp über den Gefrierpunkt und in den kältesten Monaten Januar und Februar erreichen sie bis −30 °C. Die jährlichen Niederschlagsmengen in Form von Schnee steigen selten über 250 Millimeter.
Beginnend vom nördlichsten Eurasischen Festland schließt sich ein baumloser und durch Permafrost gekennzeichneter Landschaftsgürtel an, der eine Nord-Süd-Ausdehnung zwischen 200 und 800 km aufweist und sich etwa bis zum Polarkreis, im Mittelsibirischen Bergland bis 70° nördlicher Breite erstreckt. Die Küstenlandschaft im Norden ist mit Ausnahme der Bucht um das Weiße Meer von der Tundra geprägt. Die Sommer sind dort zu kurz und zu kühl, als dass sich Wald ausbilden könnte. Die Durchschnittstemperaturen liegen nur vier bis fünf Monate im Jahr über dem Gefrierpunkt, wobei die wärmsten Monate in den Randgebieten ein Mittel über 10 °C aufweisen. Daher taut auch der Boden nur an der Oberfläche auf, sodass sich die reichlichen Niederschläge auf dem gefrorenen Unterboden stauen und die Tundra im Sommer in ein Meer von Sümpfen und Mooren mit einer Vegetation aus Flechten, Gräsern und Zwergsträuchern verwandeln. Landwirtschaft ist nicht möglich, nur die indigenen Rentiernomaden finden dort ihr Auskommen. Daher gibt es nur wenige menschliche Siedlungen. Weiter südwärts der Kältesteppe beginnen Fichten zunächst einzeln zu wachsen, um dann zusammen mit Moor-Birken und Espen von Sümpfen durchsetzte Waldtundra zu bilden. An ihrer Südgrenze geht die Waldtundra dann fließend in die Waldzone über.
Diese 1000 bis 2000 km breite Zone verläuft nördlich entlang der Linie St. Petersburg–Ufa–Irkutsk–Sachalin und bildet die boreale Zone bzw. die Taiga. Die Waldzone durchzieht ganz Nordeurasien. Wegen dieser gewaltigen Ausdehnung gliedert sie sich in mehrere breitenparallele Unterzonen: In den der Fläche nach bei weitem dominierenden Nadelwaldgürtel (eigentliche Taiga) im Norden, in Mittelsibirien weiter in die Sub-Taiga als Übergangszone zur Steppe sowie in einen Mischwaldgürtel, der sich jedoch lediglich im europäischen Russland südlich anschließt. Die Taiga ihrerseits bildet drei breitenparallel hintereinander geschaltete Unterzonen:
Die Waldzone ist durch kontinentales Klima mit einem starken Temperaturgefälle zwischen heißen Sommern und kalten Wintern geprägt. Die mittlere Jahrestemperatur nimmt von Westen nach Osten deutlich ab. In Pskow beträgt sie noch 5,1 °C, sinkt aber bis zum Ural auf 2,3 °C ab und erreicht im westsibirischen Tomsk nur noch 0,1 °C. Im ostsibirischen Jakutsk liegt sie dann bei −10 °C. Die niedrigen Jahresmittel sind auf den langen und sehr kalten Winter in Sibirien zurückzuführen. Dagegen entsprechen die durchschnittlichen Sommertemperaturen dem mitteleuropäischen Mittel.
In den von kühlgemäßigten Klimaten beherrschten Gebieten, die sich der Taiga südlich anschließen, wächst sommergrüner Laub- und Mischwald. Diese Zone verläuft innerhalb Europas im Dreieck St. Petersburg–Odessa–Ufa, in Westsibirien in einem Streifen von Tscheljabinsk bis Krasnojarsk sowie im Amur-Gebiet. Die Mischwaldzone verläuft damit in einem nach Osten hin sich verjüngenden Dreieck von den mittleren Karpaten und von der baltischen Küste bis an den Südural. Die Vegetation besteht primär aus Fichten, Kiefern und Eichen, ehe sie weiter südwärts in reinen Laubwald übergeht. Leithölzer bilden dort die Eiche sowie in der Westukraine Buche und Hainbuche. Kiefern wachsen, wie auch im Mischwaldbereich, vor allem in sandigen Senken wie im Pripjetbecken. Östlich des Urals gibt es aus klimatischen Gründen keinen Mischwald. Stattdessen leiten in Westsibirien Birkenhaine unmittelbar von der Taiga in die Waldsteppe über. Der Mischwald tritt dann wieder im Fernen Osten auf. Die Mischwaldzone bietet für die Landwirtschaft im Allgemeinen akzeptable, die Laubwaldzone gute Existenzbedingungen.
Weiter südlich folgt ein Steppengürtel, der am Unterlauf von Don und Wolga, Nordkaukasus, Kaspische Senke und Tuwa verläuft. Der Steppengürtel untergliedert sich im Norden in die Waldsteppe und im Süden in die eigentliche Steppe. Der Wald löst sich von Norden nach Süden in Inseln auf und verschwindet schließlich fast ganz. Das hängt mit dem nach Südosten abnehmenden Niederschlag bei gleichzeitig wachsender Verdunstungsintensität zusammen. Außer in Flusstälern (als Auwald) oder in Senken mit günstigen Grundwasserverhältnissen reicht das im Lössboden gespeicherte Wasser nicht aus, um den Flüssigkeitsbedarf von Laubhölzern zu decken. Daher bilden in der Waldsteppe Wiesen-, in der eigentlichen Steppe Federgrasformationen die Pflanzendecke. Der Steppengürtel ist aufgrund der fruchtbaren Schwarzerdeschicht ideal für den Getreideanbau.
An der Schwarzmeerküste zwischen Noworossijsk und Sotschi folgt eine Hartlaubwaldzone. An der Schwarzmeerküste herrschen im Durchschnitt um die 20 Grad Celsius. Dieser subtropische Teil Russlands ist geprägt von dichten Wäldern.
Russland beherbergt nach Kanada die größten noch verbliebenen nordischen Wildnisregionen. Nach Global Forest Watch sind rund 26 % der Wälder noch intakte Urwälder. Sie liegen zum allergrößten Teil in Sibirien. Im europäischen Teil haben noch 9 % der Wälder diesen Status.[22]
Das polare Klima an der Nordküste Russlands ist Lebensraum für Polarbären, Robben, Walrosse und Seevögel. In der sich südwärts anschließenden Tundra leben Polarfüchse, Eulen, Schneehasen und Lemminge. Im Sommer wandern große Herden von Rentieren und Wölfen in die Tundra ein. Diese Tiere sind an die lebensunfreundlichen Umstände dieser Zone perfekt angepasst. In den Wäldern von Russland nimmt die Artenvielfalt in der Tierwelt zu. So leben in der Taiga und den borealen Nadelwäldern Russlands Elche, Rentiere, Wölfe, Bären, Zobel, Eichhörnchen, Füchse und der Vielfraß. Weiter südlich haben sich Wildschweine, Nerze und Hirsche ausgebreitet. Vereinzelt gibt es auch Sibirische Tiger. Die Steppenzone Russlands ist der Lebensraum für Hamster, Ziesel sowie für den Iltis und den Steppenfuchs.
Die Bevölkerung Russlands ist sehr ungleichmäßig verteilt. 85 % der Einwohner (etwa 123 Millionen Menschen) leben im europäischen Teil, der dabei lediglich 23 % des russischen Territoriums umfasst. Deshalb leben nur 15 % (etwa 22 Millionen Menschen) im flächenmäßig weit größeren asiatischen Teil, der 77 % der Gesamtfläche ausmacht. Die Bevölkerungsdichte variiert von 362 Einwohner/km² in der Hauptstadt und ihrer Umgebung (Gebiet Moskau) und unter 1 Einwohner/km² im Nordosten und im russischen Fernen Osten. Im Schnitt beträgt sie 8,3 Einwohner/km². Da in vielen Fällen ein beträchtlicher Bevölkerungsanteil im jeweiligen Gebietshauptort lebt, liegt die Bevölkerungsdichte im ländlichen Raum auch in den relativ dicht besiedelten zentralrussischen Verwaltungsgebieten selten höher als 40 bis 50 Einwohner/km².
Russlands Bevölkerungszahl sank von 147,0 Millionen bei der Volkszählung im Januar 1989 bis 2007 auf 142,2 Mio.[24] Danach verlangsamte sich der Bevölkerungsrückgang, so dass die Einwohnerzahl 2010 bei 141,9 Mio. lag.[25] Durch die Ergebnisse der Volkszählung 2010 wurde die Bevölkerungszahl korrigiert. Die Fertilitätsrate sank zwischen 1988 und 1999 von zwei auf 1,16 Geburten pro Frau. Gleichzeitig verdoppelte sich bei den Männern die Sterblichkeitsrate von 9,4 (1970) auf 18,7 pro 1.000 Einwohner (2005). Die Durchschnittslebenserwartung der Männer sank von 63,9 Jahren 1986 auf 57,5 Jahre (1994). Bis 2004 stieg sie auf 58,9 Jahre an; 2011 lag sie bei 64,3 Jahren, 2014 bei 70,36 Jahren.[26] Die höhere männliche Sterberate führt zu einem Frauenüberschuss. 2010 gab es in Russland 10,7 Millionen mehr Frauen als Männer. Hauptursache: Ungesunde Lebensweise durch Alkohol, Rauchen sowie Verkehrsunfälle, Suizid und Mord.[27][28] Als häufigste Todesursache gelten mit 56,7 % diverse Herzkrankheiten,[29] sehr häufig sind auch Krebserkrankungen. Die Zahlen von Todesfällen infolge Drogenkonsums, Tuberkulose und HIV sind seit dem Ende der Sowjetunion merklich gestiegen. 2015 war von einer jährlichen Zunahme von 10 % bei den HIV-Ansteckungen die Rede, vor allem durch Drogenkonsum. Der Leiter der Föderalen Zentrale für die Prävention und Kontrolle von AIDS, Wadim Pokrowski, sprach Mitte 2015 von fünfzehn Regionen Russlands mit einer generalisierten Epidemie mit mehr als 1 % angesteckter Bevölkerung, ähnlich wie in Südafrika.[30] Laut Angaben zu Beginn der Welt-Aids-Konferenz 2018 nahmen die Neuinfektionen in Osteuropa und Zentralasien als einziger Weltregion zwischen 2010 und 2016 zu, zu 80 % betreffe dies Russland,[31] wo die Anzahl der Neuinfektionen 2017 laut UNAIDS doppelt so hoch lag wie 2005.[32] Im Jahr 2019 zählte die Verbraucherschutzbehörde knapp über eine Million Infizierte und zirka 80 tägliche Neuansteckungen, so Wadim Pokrowski.[33]
Die russische Regierung hat mehrere nationale Programme eingeleitet, die helfen sollen, die Geburtenrate zu steigern. Seit 2007 erhielten Eltern ab ihrem zweiten neugeborenen Kind eine einmalige staatliche Beihilfe (Mutterschaftskapital) in Höhe von fast 10.000 Euro (2012).[34] So hatten sich die Geburtenzahlen in Russland von 1,48 Mio. (2006) auf 1,9 Mio. (2012) erhöht.[35] 2018 erhielten Familien vergünstigte Hypotheken und Zuschüsse teils schon ab dem ersten Kind; für 3 Jahre wurden 9 Milliarden Dollar budgetiert.[36] Im Februar 2019 erklärte Präsident Wladimir Putin, sich nicht mit der sinkenden Geburtenrate abzufinden, und kündigte weitere Erleichterungen für Familien mit Kindern an.[37]
Der Anteil der Stadtbevölkerung blieb konstant bei 73 %.[38]
Zur Auswanderung neigten besonders höher Gebildete, teilweise wegen der herrschenden Rechtsunsicherheit.[39] Auch infolge der demografiepolitischen Anstrengungen der Regierung verlangsamte sich dieser Trend zeitweise.[40] Nach der Annexion der Krim 2014 verließen während des folgenden Wirtschaftseinbruchs wieder deutlich mehr Hochqualifizierte das Land. Im Frühjahr 2018 beklagte der Chef der Russischen Akademie der Wissenschaften eine Zahl von 44.000 Auswanderern, welche der russischen Forschung fehlten.[41]
Russland ist das zweitwichtigste Einwanderungsland der Welt. 2017 waren 8,1 % der Bevölkerung Migranten.[42] Herkunftsregionen sind hierbei vor allem die ärmeren, südlichen ehemaligen Sowjetrepubliken Zentralasiens und des Kaukasus, aber in zunehmender Zahl auch Afrika und Südostasien. Die Mehrheit der Einwanderer stellen bisher jedoch die Nachkommen von Russen, die im Kaiserreich und der Sowjetzeit in anderen Teilrepubliken angesiedelt wurden und meist mit ihren Familien gemeinsam nach Russland zurückkehrten. Der Zustrom dämpfte sich nach der Annexion der Krim durch die Wirtschaftsflaute, aber auch durch Protektionismus und Nationalismus – im ersten Halbjahr 2017 glich die Immigration die Sterblichkeit nicht mehr aus.[36]
Die Bevölkerung Russlands wird ähnlich wie in anderen Ländern Europas in den nächsten Jahrzehnten voraussichtlich weiter abnehmen, die ILO erwartet bis 2050 einen Rückgang auf 130 Millionen Einwohner.[36] Unter Annahme einer Nettozuwanderung von jährlich 300.000 Personen wäre der Rückgang nur schwach ausgeprägt. Bis 2012 stabilisierte sich die Lage etwas, die Einwohnerzahl stieg leicht an und lag bei etwa 143,5 Millionen.[43] Für die Zeit ab 2015 war aufgrund der geburtenschwachen Jahrgänge der 1990er-Jahre eine Verschlechterung der demografischen Situation erwartet worden. Dieses leichte Bevölkerungswachstum schwenkte im weiteren Verlauf der 2010er-Jahre wieder zu einer negativen demografischen Entwicklung um.[44] 2020 umfasste der Rückgang der russischen Bevölkerung laut Rosstat erstmals seit 2005 wieder mehr als 500.000 Menschen in einem Jahr. 2021 rechneten die russischen Behörden mit einem Bevölkerungsrückgang von 1,2 Millionen Menschen bis 2024.[45]
„Sankt Petersburg ist der Kopf, Moskau das Herz, Nowgorod ist der Vater, Kiew die Mutter Russlands.“
Schon ab 800 war die Kiewer Rus von vielen städteähnlichen Siedlungen gekennzeichnet, weshalb die skandinavischen Waräger das Gebiet Gardarike („Reich der Städte“) nannten. Zu den ältesten erhaltenen Städten in diesem Bereich zählen Nowgorod, Smolensk, Pskow, Rostow, Murom und Beloosero, die alle noch im ersten Jahrtausend nach Christus gegründet wurden. Im 11. und 12. Jahrhundert wurden weitere Städte im Zentrum Russlands von slawischen Siedlern gegründet. In dieser Zeit entstanden Moskau, Jaroslawl, Twer, Wladimir, Wologda, Kirow, Tula, Kursk, Kostroma, Rjasan und etwas später Nischni Nowgorod. Aufgrund der Landesgröße war eine Vielzahl großer Städte als Stützpunkte notwendig. Mit der Eroberung Kasans und Astrachans zur Mitte des 16. Jahrhunderts gründeten russische Kolonisten weitere Städte im Osten, Südosten und Süden. Zahlreiche Städte wurden zunächst als Grenzfestungen gegründet. Im Süden waren dies Stützpunkte der Verhaulinie gegen die Krimtataren, wie Orjol (1566) und das heutige Woronesch (1586). Weiter östlich, an der Wolga entstanden in dieser Zeit weitere Städte wie Samara (1586), Zarizyn (1589) und Saratow (1590). In Sibirien entstanden nach dessen Eroberung zahlreiche Kosakenforts, sogenannte Ostrogs. Aus ihnen wuchsen später Städte wie Tobolsk, Irkutsk, Bratsk, Tomsk und Jakutsk heran. Städte im Ural- und Altai-Gebirge wie Perm (1723), Jekaterinburg (1723) oder Barnaul (1730) entstanden in der Epoche Peters des Großen im Zusammenhang mit den dort vorhandenen Erzen und kostbaren Mineralen. Mit dem Niedergang der Krimtataren und dem weiteren Vorstoßen Russlands in den Kaukasus entstanden im 18. Jahrhundert neue Festungen und Städte. 1784 wurden Stawropol und Wladikawkas gegründet, 1793 Krasnodar, 1805 Nowotscherkassk, 1818 Grosny, 1844 Port Petrowsk.
Trotz der Gründungen behielten große Teilräume ihren ländlichen Charakter. Der Bauer gehörte einem Mir (Bauerngemeinde) an. Städte stellten außerhalb der Agglomerationen isolierte Erscheinungen dar und bildeten ein nur weitmaschiges Netz. Bis 1712 fungierte Moskau als Hauptstadt und wurde dann nach dem Willen Peters I. vom 1703 neugegründeten Sankt Petersburg abgelöst, um 1918 wieder offiziell den Status der Hauptstadt anzunehmen. Im 19. Jahrhundert war sogar häufig von den beiden Hauptstädten die Rede. Die Industrialisierung Ende des 19. Jahrhunderts brachte in allen Landesteilen einen bedeutenden Impuls für die nachfolgende Urbanisierung. Sie führte zur Entstehung zahlreicher neuer Städte und zum raschen Wachstum alter Städte. Viele russische Städte entstanden als Folge einer administrativen Umstrukturierung mehrerer benachbarter Dorfsiedlungen zu einer Stadtsiedlung. Neugründungen von Städten und Stadterhebungen sind bis heute ein Charakteristikum der russischen Urbanisierung.
Mehr als die Hälfte aller russischen Städte sind erst in den letzten 90 Jahren, besonders in den 1960er-Jahren gegründet worden. Deshalb gibt es unter den 160 russischen Großstädten, in denen die Hälfte der russischen Bevölkerung lebt, viele neue Städte (etwa ein Viertel). Die russischen Großstädte sind in erster Linie Industrie- und Verwaltungszentren, besitzen aber auch andere hochrangige Funktionen. Beispiele neuer Großstädte sind Magnitogorsk, Nowokusnezk oder Bratsk, zu den gewachsenen zählen unter anderem Samara und Tambow.
Zu Zeiten der Sowjetunion wurde die städtische Entwicklung zentral geplant und gesteuert. Es herrschte der Typus der Sozialistischen Stadt vor. Dazu zählt beispielsweise die Herausbildung neuer Stadttypen, etwa der Hauptstädte kleiner nationaler Republiken (u. a. Tscheboksary, Naltschik) oder der Wissenschaftsstädte (z. B. Dubna). Die in der Sowjetzeit betriebene massive Verstädterungspolitik führte dazu, dass heute 73 % der Bevölkerung in städtischen Siedlungen leben. Aus den politischen und wirtschaftlichen Umbrüchen im Russland der 1990er-Jahre gingen die Städte als eigenständige und selbstverantwortliche kommunale Einheiten hervor. Dazu erhielten sie lokale und regionale Steuerungsinstanzen. Mit den neuen Staatsgrenzen brachen aber auch stark arbeitsteilig organisierte, spezialisierte Produktions- und Distributionsabläufe zusammen. Viele Städte waren plötzlich von den bisherigen Netzwerken abgeschnitten. Ehemals zentral gelegene Städte stellten plötzlich Grenzstädte dar und waren geopolitisch peripher gelegen. Dadurch veränderten sich grundlegend die funktionale Struktur und die wirtschaftliche Entwicklungsbasis der russischen Städte und führte zu Verschiebungen im Städtesystem Russlands, mit Auf- und Absteigern. Zu den Gewinnern der Transformation gehören bisher vor allem die Metropolen, allen voran Moskau. Weil Kapital zur Gewinnung und zum Transport von Rohstoffen unter extremen Bedingungen fehlte, gerieten viele Bergbaustädte des Nordens in eine Überlebenskrise.
Die zehn größten Städte Russlands (ehemalige Namen aus sowjetischer Zeit in Klammern):
Das Zentrum von Sankt Petersburg
Die Innenstadt von Jekaterinburg
Stadtzentrum und das „Goldene Horn“ (Solotoi Rog), die Hafenbucht von Wladiwostok an der Pazifikküste
Russische Kinder in Nordrussland, 1909
Streng genommen würde Rossijskaja Federazija wörtlich übersetzt „Russländische Föderation“ (von Rossija „Russland“) und nicht „Russische Föderation“ heißen. Man hat bewusst nicht Russkaja Federazija („Russische Föderation“) als Staatsbezeichnung gewählt, um auch die nichtrussischen Nationalitäten mit einzubeziehen. Ist von dem russischen Volk oder der russischsprachigen Kultur die Rede, spricht man daher im Russischen von russkij (russisch), für den russischen Staat hingegen verwendet man das Adjektiv rossijskij (russländisch). Trotzdem wird im Deutschen in beiden Fällen zumeist das Adjektiv „russisch“ verwendet. Der Gebrauch des Wortes „russländisch“ beschränkt sich weitgehend auf Fachpublikationen. Auch die amtliche Übersetzung der Verfassung Russlands verwendet diese Variante.
Die Russische Föderation begreift sich auch heute noch als Vielvölkerstaat. Die größte Gruppe sind die Russen, die mit 79,8 % die Mehrheit der Bevölkerung stellen, doch leben nahezu 100 weitere Völker auf dem Gebiet des Landes. Trotz der Heterogenität ist die russische Bevölkerung in allen städtischen und industriell geprägten Räumen landesweit dominant und die Titularnationen bilden auch in ihren „eigenen“ Territorien häufig die Minderheit.[46] So zählen nur 23 Völker bzw. Titularnationen mehr als 400.000 Personen. Der Grad der ethnischen Identifikation variiert.
Größere Minderheiten sind die Tataren (4,0 %), die Ukrainer (2,2 %), die Armenier (1,9 %), die Tschuwaschen (1,5 %), die Baschkiren (1,4 %), die Deutschen (0,8 %) und andere. Zu den kleineren Minderheiten zählen beispielsweise die Mescheten und verschiedene Minderheiten jüdischen Glaubens. Die nichtrussischen Minderheiten sprechen überwiegend Sprachen aus dem Kreis der Turksprachen, kaukasische Sprachen, uralische Sprachen (samojedische Sprachen), altaische oder paläosibirische Sprachen. Für viele nichtrussische Völker wurden Republiken mit weitgehender Autonomie errichtet. Während manche Minderheiten, wie etwa Armenier, Koreaner und Deutsche, auf die verschiedensten Regionen Russlands verteilt sind, gibt es auch im europäischen Russland mehrere indigene Völker. Groß ist die Zahl der Nationalitäten im Kaukasusgebiet, das erst im letzten Drittel des 18. Jahrhunderts zu Russland kam.

Russisch ist die einzige überall geltende Amtssprache, parallel dazu wird in den einzelnen autonomen Republiken jedoch häufig die jeweilige Volkssprache als zweite Amtssprache verwendet. In einigen Republiken existieren auch drei oder mehr offizielle Sprachen; in Dagestan, wo mehr als 30 einheimische Volksgruppen leben, gibt es 14 offizielle Sprachen.
Die Verwendung der regionalen Sprachen wird im Unterricht, in den Massenmedien und in der Kulturpolitik gefördert. Die Regierungen und Parlamente der Republiken betrachten dies als unabdingbare Voraussetzung, um ein Aussterben von Volksgruppen zu verhindern. Allerdings nimmt die Beherrschung der indigenen Muttersprache unter vielen nicht-russischen Volksgruppen ab.
Russisch wird, ebenso wie fast alle regionalen Amtssprachen in Russland, im kyrillischen Alphabet geschrieben. Es besteht die Richtlinie, dass alle jeweiligen Sprachen kyrillisch zu schreiben sind. Ausnahmen bilden das Jiddische in der Jüdischen Autonomen Oblast, welches dort aber bereits seit Jahrzehnten kaum noch gesprochen wird, sowie das Karelische, Finnische und Wepsische in Karelien, die dort jedoch nur einen untergeordneten offiziellen Status besitzen.
In Tatarstan wurde Tatarisch als einzige Ausnahme von 2001 bis 2004 gegen den Widerstand der in Tatarstan ansässigen russischsprachigen Bevölkerung ausschließlich in lateinischer Schrift geschrieben. Diese Praxis verbot das russische Verfassungsgericht im November 2004 mit der Begründung, dass für die Einigkeit Russlands eine einheitliche Schrift notwendig sei.[48]
Nach dem Zerfall der Sowjetunion und dem damit verbundenen Verschwinden der atheistischen Staatsideologie des Marxismus-Leninismus fand eine Rückbesinnung auf religiöse Werte statt. Die in Russland am weitesten verbreiteten Religionen sind das Christentum – vor allem der russisch-orthodoxe Glaube – sowie der Islam (→ Islam in Russland). Vertreten sind darüber hinaus zahlreiche andere Konfessionen wie der römisch-katholische Glauben, der Protestantismus, das Judentum, der Buddhismus sowie traditionelle Glaubensrichtungen einiger Volksgruppen. Etwa ein Drittel der Bevölkerung bezeichnet sich als Atheisten oder Konfessionslose.[A 4]
Was die Zugehörigkeit zu einzelnen Religionsgruppen angeht, gibt es keine zuverlässigen Zahlen, da die Mitglieder von Kirchen und Gemeinden in Russland nicht registriert werden und keine Kirchensteuer erhoben wird. Umfragen weichen oft erheblich voneinander ab. So hat die Stiftung für öffentliche Meinung (FOM) 2012 nur noch 41 Prozent Orthodoxe festgestellt, gegenüber 13 Prozent Atheisten und nur 6,5 Prozent Muslimen. Weitere 25 Prozent aber bezeichneten sich als Agnostiker bzw. gaben an, an eine höhere gottähnliche Macht zu glauben.[49] Das Gesamtrussische Zentrum für Meinungsforschung (VCIOM) ging hingegen 2010 von 75 Prozent Orthodoxen und nur 8 Prozent Atheisten aus, seine Zahlen werden auch von der Russischen Botschaft in Deutschland zitiert.[50]
Abweichend von den genannten Umfragen wird der Anteil der Orthodoxen meist zwischen 51[51][52] und 72 %[A 5][53][54] angegeben, die der anderen Christen mit zusammen kaum 2 %, die der Buddhisten mit knapp 1 % und die der Juden mit etwa 0,35 %.[55] Der Fischer Weltalmanach und der Religious Freedom Report des US-Außenministeriums geben 14 % Muslime an.[54][56]
Das CIA World Factbook ging 2006 von folgenden groben Schätzungen für praktizierende Gläubige aus, also von solchen, die ihren Glauben aktiv ausüben: 15 bis 20 % Russisch-Orthodoxe, 10 bis 15 % Muslime, 2 % andere christliche Konfessionen.[57]
Der russisch-orthodoxe Glaube reicht bis ins frühe Mittelalter zurück. Die engen Kontakte zu dieser Glaubensrichtung resultierten aus dem hauptsächlich auf Konstantinopel ausgerichteten Handel und den damit engen Kontakten mit Byzanz. Die Fürstin Olga von Kiew (893–924) ließ sich als erste Herrscherin aus der rurikidischen Dynastie taufen, konnte den christlichen Glauben im Reich aber nicht durchsetzen. Nach der Belagerung von Konstantinopel (860) kamen ab 911 verstärkt orthodoxe Missionare ins Land, angeblich sollen bereits Waräger und Russen, die am Angriff von 860 teilgenommen hatten, getauft zurückgekehrt sein. Unter Olgas Enkel, Wladimir dem Heiligen, begann 988/989 die Christianisierung der Rus, wobei die Kiewer Bevölkerung in Massentaufen bekehrt wurde. Nach Wladimirs Tod 1015 wurden die bisher heidnischen Völker noch jahrzehntelang weiter christianisiert. Byzanz betrieb zu dieser Zeit seine Kirchenpolitik im bewussten Gegensatz zu Rom und vermittelte den Ostslawen bei ihrer Bekehrung antirömische Tendenzen.[58] Die Kirche Kiews wurde als Teilkirche des Patriarchates von Konstantinopel zunächst von Exarchen verwaltet, was keine Auswirkungen auf die politische Selbständigkeit der Kiewer Großfürsten hatte. Die Orthodoxe Kirche und ihre Werte bilden bis heute eine tragende gesellschaftliche Säule des russischen Reiches.
Nach der Vernichtung der Kiewer Rus im Mongolensturm und unter der nachfolgenden Goldenen Horde übersiedelte der Kiewer Metropolit im 14. Jahrhundert zunächst nach Wladimir, dann 1328 nach Moskau. Im 15. Jahrhundert löste sich die Russisch-Orthodoxe Kirche endgültig vom griechisch-orthodoxen Patriarchat in Konstantinopel, nachdem sich dieses infolge des politischen Niedergangs von Byzanz zu Zugeständnissen an den Papst bereit erklärt hatte. Die Konzeption von Moskau als Drittem Rom, das als einziges den „wahren christlichen Glauben“ aufrechterhalte, war geboren. 1589 wurde ein eigenes Patriarchat gegründet. Peter I. hob dieses auf und setzte 1721 stattdessen an die Spitze der Kirche den Heiligsten regierenden Synod, der 1918 in Sowjetrussland abgeschafft wurde. Die Sowjets stellten zunächst das Patriarchat wieder her, ehe 1988 ein Heiliger Synod der Russisch-Orthodoxen Kirche wiedererrichtet wurde.
Im Russland vor 1917 durften Anhänger der Russisch-Orthodoxen Kirche nicht zu einer anderen Konfession, auch wenn sie christlich war, übertreten und durften keine „Nichtchristen“ heiraten. Dieser Kirche war es als einziger Religion erlaubt, zu missionieren; Kinder aus „gemischten“ Ehen mit Nicht-Orthodoxen galten als orthodox. Erst mit der Revolution von 1905 wurden die Gesetze gelockert. Nach der Herrschaftsübernahme der Kommunisten wurden hauptsächlich Mitglieder dieser Kirche unterdrückt, da sie als Symbol der Autokratie galt. Zwischen 1918 und 1939 wurden ca. 40.000 orthodoxe Geistliche hingerichtet. Die 77.800 Gemeinden von 1917 wurden bis 1941 auf etwa 3100 reduziert.
Heute erlebt die Russisch-Orthodoxe Kirche eine Wiederbelebung, insbesondere in ländlichen Gebieten. Viele Klöster wurden gegründet oder wiedererrichtet. Die Kirche zählt gegenwärtig etwa 100 Millionen Mitglieder, von denen jedoch nur 5 bis 10 % regelmäßige Gottesdienstbesucher sind. Religionsunterricht an Schulen wurde 2006 wieder eingeführt. Die Russisch-Orthodoxe Kirche sieht sich als Vertreter der Interessen des Volkes, ohne im Gegensatz zur Regierung zu stehen. Der Staat selbst hingegen sieht die Kirche als Garant für den Zusammenhalt der Gesellschaft. Die Mehrheit der Bevölkerung vertraut der Kirche und sieht in ihr eine Institution, die Werte vermittelt und den inneren Zusammenhalt in der Gesellschaft stärkt.[59]
Daneben haben sich im Verlauf der Geschichte Abspaltungen vom orthodoxen Glauben vollzogen. Die älteste Abspaltung sind die Altorthodoxen oder Altgläubigen. Weitere aus der Orthodoxie hervorgegangene Glaubensrichtungen sind die Molokanen. Aus ihnen gingen wiederum die Duchoborzen hervor. Beide Religionsgemeinschaften lehnen Reichtum ab, versuchen ein Leben in Bescheidenheit zu führen und suchen nach einer wahrhaft biblischen Gemeinschaft. Von einigen Leibeigenen wurde die Gemeinschaft der Subbotniki gegründet. Diese berufen sich in erster Linie auf das Alte Testament. Viele dieser Sekten oder Gruppierungen waren im Zarenreich willkürlichen Verfolgungen ausgesetzt.
In Russland gibt es neben der russisch-orthodoxen Ausrichtung weitere christliche Konfessionen:
Der Islam in Russland ist im Nordkaukasus schon seit dem 7. Jahrhundert verbreitet und damit auf dem heutigen russischen Staatsgebiet älter als die erste russische Staatsgründung und die Christianisierung des Landes. Im Jahr 922 traten auch die Wolgabulgaren zum Islam über und gaben ihn im 13. Jahrhundert an die Tataren weiter. Die einheimischen Völker des Kaukasus und die Turkvölker sind zumeist sunnitische Gläubige. Bereits Ende des 19. Jahrhunderts waren im Russischen Reich 11,1 % der Gesamtbevölkerung muslimischer Herkunft. Im heutigen Russland ist der Anteil der Muslime mit rund 14 % etwa ebenso groß wie einst in der Sowjetunion. Von 1990 bis 1994 bestand in Russland die „Islamische Partei der Wiedergeburt“. Daneben gibt es auch eine „Islamische Partei der Wiedergeburt Tadschikistans“ sowie zahlreiche weitere Organisationen und Abspaltungen. Zentren des Islam in Russland sind heute neben Kasan und Moskau auch Ufa und Dagestan. Die zunehmende Bedeutung des Islam im Kaukasus gehe gemäß Recherchen der Nowaja gaseta im Jahr 2018 einher mit dem Vertrauensverlust in den Staat.[66]
Die Geschichte der Juden in Russland lässt sich seit dem 4. Jahrhundert nachweisen, als Juden aus Armenien und von der Krim sich auch in Tmutarakan niederließen. Im späten 8. oder frühen 9. Jahrhundert konvertierte ein Großteil der Chasaren zum Judentum. Nach der Vernichtung des Chasaren-Reiches durch Swjatoslaw I. (969) beschränkte sich das Judentum im Wesentlichen auf Kiew, die Krim und den Kaukasus. Im Großfürstentum Moskau wurden Juden 1471 das erste Mal erwähnt. Bis zur Zeit Iwans des Schrecklichen (1533–1584) wurden Juden bis auf einige gegen sie gerichtete Gesetze toleriert. Ab 1721 wurden sie aus dem Russischen Kaiserreich ausgewiesen, bis dies durch die Eingliederung der östlichen Teile Polens (1793 und 1795) unmöglich wurde. Die Juden mussten ab 1791 innerhalb des Ansiedlungsrayons leben, das sich auf dem heutigen Gebiet der Ukraine, Belarus’ und des Baltikums befand.
Im 19. Jahrhundert unterstützten führende Beamte wie Konstantin Pobedonoszew antisemitische Strömungen in der Bevölkerung. So kam es im südlichen Russland 1881 zu vielen Pogromen, nachdem den Juden fälschlich der Anschlag auf Alexander II. unterstellt wurde. Die Maigesetze von 1882 vertrieben die Juden selbst im Ansiedlungsrayon aus den ländlichen Gebieten; mit Quoten begrenzte man die Anzahl der Juden, die zu höherer Bildung zugelassen wurden, auf 3–10 %. Zwischen 1880 und 1920 flohen mehr als zwei Millionen Juden aus Russland, besonders nach Amerika. 1903 brachen neue Pogrome aus, die sich in der Russischen Revolution nochmals verstärkten und zu zwischen 70.000 und 250.000 Opfern in der jüdischen Zivilbevölkerung führten. Während des Stalinismus wurde in Russisch-Fernost die Jüdische Autonome Oblast mit dem Hauptort Birobidschan gegründet, wo sich nur wenige Juden ansiedelten. Im Vergleich zu den Jahrzehnten davor gibt es heute nur noch wenige Juden, da viele von ihnen nach Deutschland oder nach Amerika, die meisten aber nach Israel ausgewandert sind. Heute gibt es in Russland 87 Synagogen, die meisten davon in Sankt Petersburg und in Moskau, darunter die Moskauer Gedenksynagoge. Die Juden im europäischen Russland sind meist Aschkenasim, östlich davon leben daneben auch einige Bergjuden und Bucharische Juden, die zu den Mizrachim gezählt werden.
In Russland ist auch die tibetische Form des Buddhismus verbreitet, wobei er sich ursprünglich auf die asiatischen Völker (Kalmücken, Tuwiner) beschränkte. Ebenso wie Geistliche und Anhänger praktisch aller anderen Religionen wurden in der Sowjetunion während der kommunistischen Herrschaft auch buddhistische Mönche verfolgt und unterdrückt. Seit der politischen Wende in Russland und den Nachfolgestaaten der Sowjetunion verzeichnen die buddhistischen Gemeinschaften hingegen wieder Mitgliederzuwachs unter den Angehörigen der traditionell buddhistischen Völker, aber auch seitens der Russen und anderen Nationalitäten.
Der Schamanismus ist unter der indigenen Bevölkerung in Sibirien wieder weit verbreitet; insbesondere bei den kleinen Völkern des russischen Nordens. Zwar sind heute die meisten Bewohner Sibiriens Christen, dennoch sehen sie es nicht als Widerspruch, die Rituale ihrer Vorfahren zu praktizieren.
Die Sowjetunion war ein imperial geeinter Nationalitätenstaat, d. h., Nationalität war dabei ein politisches Instrument zur Konsolidierung der Sowjetmacht,[67] und auch im heutigen Russland treffen sich viele unterschiedliche Mentalitäten. Die Verschmelzung dieser Völker und Konfessionen sowie Einflüsse westlicher wie östlicher Prägungen schufen aber auch markante Eigenarten, die sich im Stereotyp der „russischen Seele“ manifestieren. Dieser Begriff prägt bis heute das Russlandbild; im westlichen Ausland diente der Begriff Russophilen und Kritikern der westlichen Lebensweise als Projektion zu der als gefühlskalt empfundenen eigenen Zivilisation.[68] Die „russische Seele“ wird als ein Hang zu extremen Gegensätzen beschrieben, der sich aus der geschichtlichen Entwicklung der russischen Volkskultur ergeben hat. Diese Extreme äußern sich z. B. in dem Streben nach dem absolut Äußersten, verbunden mit der Bereitschaft zu einer plötzlichen Richtungsänderung;[69] dazu kommen eine ausgeprägte Schicksalsergebenheit, ein Hang zur Geduld, Neigung zum Aberglauben, Leidensfähigkeit oder auch eine sehr starke Heimatverbundenheit. Die bereits erwähnte Alles-oder-nichts-Mentalität kennt keinen Kompromiss und keine goldene Mitte. Bekannt ist auch die Offenheit von Gefühlsäußerungen, positiven wie negativen, denen im Vergleich mit rationalen Erwägungen häufig mehr Gewicht zugemessen wird, was westliche Ausländer oft irritiert. Wichtig ist zudem weiterhin ein starkes Solidaritäts- und Gemeinschaftsgefühl.
Die russische Gesellschaft ist traditionell kollektivistisch geprägt, die Zugehörigkeit zu einer Gruppe sehr wichtig. Dieses Wertesystem beruht ursprünglich auf der Lebensweise der bäuerlichen Dorfgemeinschaft, dem Mir. Da auch Grund und Boden lange Zeit Gemeingut waren, definiert man sich in Russland seit jeher über die Gemeinschaft und achtet auf die Stimmigkeit von eigenem Verhalten und eigener Meinungsäußerung mit denen des Kollektivs.
Die Familie ist für viele Russen eine wichtige Bezugsgruppe, besonders auf dem Land lebt man in jeder Beziehung eng zusammen. Dort wohnen oft mehrere Generationen in einer Wohnung oder in einem Haus. Die traditionelle Familie unterstützt sich finanziell und hilft einander bei der Kinderbetreuung und Seniorenpflege. Die Kollektivorientierung zeigt sich bisweilen auch heute noch im Berufsalltag. Das Kollegium wird als Gemeinschaft erlebt und es ist sehr wichtig, diese Gruppenorientierung zu stärken. Vetternwirtschaft (Nepotismus) bei der Stellen- oder Auftragsvergabe ist dabei eine Nebenwirkung.
Seit der Auflösung der Sowjetunion orientieren sich nun aber insbesondere gebildete Bevölkerungsschichten in den Großstädten, die von der neu gewonnenen Reisefreiheit profitieren können, an Prinzipien des Individualismus, was inzwischen ein massives innergesellschaftliches Spannungsverhältnis zur Folge hat und zu einem zentralen Thema im zeitgenössischen Literatur- und Filmschaffen geworden ist. Bildungsnahe, ehrgeizige und kritische Menschen suchten nach dem markanten Bruch mit der westlichen Welt im Jahr 2014 vermehrt Lebensmöglichkeiten im Ausland; die Duma diskutierte 2015 gar ein Verbot von Fremdsprachenunterricht, weil dieser die Abwanderung fördere.[70][71][72] Im Jahr 2019 berichtete das Lewada-Zentrum, dass 53 % der Befragten im Alter von 18 bis 24 Jahren ins Ausland ziehen möchten.[73]
Im Jahr 2014 waren 43 % aller Managerposten in Russland weiblich besetzt; prozentual mehr als in jedem anderen Land der Welt.[74][75]
Artikel 41 der Verfassung Russlands garantiert allen Bürgern das Recht auf kostenlose medizinische Grundversorgung. Dieser seit Sowjetzeiten bestehende Grundsatz ist zum Teil die Ursache dafür, dass Russland im internationalen Vergleich eine vergleichsweise hohe Anzahl Ärzte und Krankenhäuser pro Kopf der Bevölkerung aufweist.[77][78] Dennoch ist der gesundheitliche Zustand der russischen Bevölkerung schlecht. Gerade beim wirtschaftlichen Niedergang der 1990er-Jahre in Russland wurde das Gesundheitswesen stark getroffen.[79] Infolge äußerst niedriger Entlohnungen der Ärzte und Krankenschwestern wurde die medizinische Versorgung der breiten Öffentlichkeit massiv verschlechtert. So ist inzwischen jede dritte Klinik der 7000 Krankenhäuser im Land dringend renovierungsbedürftig. Schrittweise werden in letzter Zeit die Gehälter für das medizinische Personal angehoben sowie staatliche Mittel in die Einrichtung neuer und Modernisierung bestehender Kliniken investiert. Zwischen 1999 und 2003 betrugen die Gesamtausgaben für den Gesundheitssektor in Russland im Verhältnis zum BIP durchschnittlich 5,70 %.
In Russland ist der Gesundheitssektor dezentral organisiert. Das Gesundheitsministerium ist auf föderaler Ebene für den gesamten Sektor zuständig. Konkrete medizinische Leistungen (inklusive die Bereitstellung von Krankenhäusern) obliegen aber den Föderationssubjekten und Gemeinden, die rund zwei Drittel der gesamten Budgetausgaben bestreiten. Das russische Gesundheitssystem wird durch einen Mix aus Budgetmitteln und Mitteln aus der Sozialversicherung finanziert. Auf die Verschlechterung der Beziehungen zum Westen folgten ab 2015 Zulassungsbeschränkungen für medizinische Geräte aus dem Ausland.[80]
Nach dem Zerfall der UdSSR stieg die Armut bis 1999 auf über 40 % Bevölkerungsanteil und sank danach spürbar. 2002 betrug der Anteil 19,6 % und reduzierte sich bis 2011 auf 12,8 % der Bevölkerung oder 18 Millionen Russen. Offiziell lag dabei das Existenzminimum bei 170 Euro für einen Menschen im arbeitsfähigen Alter; bei Kindern liegt der Wert unwesentlich niedriger, bei Rentnern beträgt er 125 Euro.[81] Der Lebensstandard verbesserte sich regional sehr unterschiedlich. Während besonders in Moskau und St. Petersburg einige Viertel in neuem Glanz zu erstrahlen begannen, war in manchen Regionen die Armut nach wie vor groß. In Tschetschenien und Dagestan lebten mehr als die Hälfte der Menschen in Armut; weitere arme Regionen sind Inguschetien, Tuwa und Kabardino-Balkarien, Mari El, Kalmückien, Burjatien und Altai und Mordwinien. 2011 betrug der Durchschnittslohn 576 € pro Monat. Die großen Einkommensdifferenzen konnten ab 2005 verringert werden, insbesondere die mittlere Einkommensschicht nahm prozentual erheblich zu. Die Renten lagen 2010 das erste Mal seit vielen Jahren über dem Existenzminimum und sollten gemäß Prognosen bis 2014 auf 268 Euro steigen. 2012 zählte etwa die Hälfte der Bevölkerung zu der einkommensschwachen Schicht, die zentrale soziale Bedürfnisse wie Wohnraum oder zusätzliche Ausbildung nicht finanzieren kann.[82] Tatsächlich betrug im Jahr 2014 die durchschnittliche Rente 10.000 Rubel, was 160 Euro entsprach.[83] Renten und Gehälter mussten eingefroren werden. Seit 2014 wurden Gelder der zweiten, kapitalgedeckten Säule der Altersvorsorge zur Deckung des Finanzbedarfs herangezogen.[84] Die Gebiete mit den höchsten Arbeitslosenzahlen in Russland waren um 2021 Dagestan, Inguschetien und Nordossetien.[85]
Die Verringerung der Armut zählte im Frühjahr 2019 zu einem der Fünfjahresziele des Präsidenten Putin: Fast 19 Millionen Russen galten als arm, das entsprach 12,9 % der Bevölkerung.[86][87]
Die ärmeren Bevölkerungsschichten litten bis 2009 unter zweistellig steigenden Verbraucherpreisen, die sich bis 2012 wieder verringerten. Von 2014 bis 2019 verringerte sich das Realeinkommen.[87] Zur Bekämpfung der Armut wurde im Herbst 2021 eine neue Berechnungsgrundlage eingeführt, womit die Zahl der Armen schlagartig um 2,8 Millionen sank. Zwar wurden Anfang 2022 die Sozialleistungen um die Inflation von 8 % erhöht, jedoch lagen die Preissteigerungen bei Lebensmitteln weit höher.[88] In besonders armen Regionen gelten die russischen Streitkräfte als einzige Möglichkeit für junge Männer, der Armut zu entkommen und jemals eine Familie versorgen zu können.[89]
Die Arbeitslosenquote hatte mit der Überwindung der Finanzkrise 2008 zu sinken begonnen. In Wachstumsregionen wie Moskau, Kaluga und St. Petersburg tendierte die Erwerbslosigkeit gegen Null. Die Arbeitslosigkeit betrug nach Berechnung nach Standards der Internationalen Arbeitsorganisation 2005 7,1 %, 2010 7,6 % und 2011 6,6 %. Bis 2014 sank sie auf 5,2 % und begann wieder zu steigen. Das Arbeitslosengeld betrug zwischen 60 und 70 Euro im Monat.[90] Die Arbeitslosigkeit ist aber aufgrund einer Besonderheit des russischen Arbeitsrechts ein problematischer Indikator für die Konjunkturlage: Betriebsbedingte Kündigungen sind in Russland zumeist unzulässig, stattdessen dürfen Arbeitgeber einseitig Arbeitsentgelte reduzieren. Daher verbleiben russische Arbeitnehmer auch bei Auftragsmangel lieber in ihrem Betrieb und nehmen hohe Lohneinbußen in Kauf, anstatt die mit 20 bis 110 Euro im Jahr 2019 eher symbolische Arbeitslosenunterstützung in Anspruch zu nehmen.[91]
Das Entwicklungsprogramm der Vereinten Nationen zählt die Russische Föderation zu den Staaten mit sehr hoher menschlicher Entwicklung.[4] Der Gini-Koeffizient lag 2016 bei 37,7.
Zur Zeit der Sowjetunion wurde die russische Natur schwer belastet: von Fabrikabfällen vermüllt, chemisch und radioaktiv verunreinigt. Auch heute gibt es ernsthafte Umweltprobleme in Russland – aber auch ein wachsendes Umweltbewusstsein in der Bevölkerung. Das Recht des Bürgers auf gesunde Umwelt und auf verlässliche Informationen über ihren Zustand ist im Artikel 42 der russischen Verfassung verankert. Allerdings hat der Umweltschutz in der russischen Politik eine vergleichsweise niedrige Priorität, was von internationalen Umweltorganisationen wie WWF oder Greenpeace immer wieder kritisiert wird.[92] So wurden in der Vergangenheit oft gängige Umweltstandards bei der Erschließung neuer Erdöl- oder Erdgasvorkommen nur unzureichend eingehalten. Ein bekanntes Beispiel der jüngsten Zeit ist die Erschließung der Fördergebiete Sachalin II, bei der in höherem Maße gegen Umweltauflagen verstoßen worden sein soll.[93] Hinzu kommt eine verbreitete Korruption[94] innerhalb staatlicher Umweltbehörden, die mehrfache Verstöße gegen Umweltauflagen beim Bau von Häusern oder massenhaften illegalen Holzeinschlag ermöglicht. Auch eine Vielzahl von Altlasten aus den Sowjetzeiten, darunter marode Fabriken, die die heutigen Umweltstandards nicht einhalten können, belasten die Umwelt in Teilen des Landes erheblich. Einige Städte mit solchen Fabriken, wie Norilsk oder Dserschinsk, gelten als ökologisches Notstandsgebiet.[95]
Je stärker die Lebensqualität stieg, umso wichtiger und dringlicher wurden Umweltfragen in Russlands Öffentlichkeit und Politik diskutiert. Seit 2004 wurden vereinzelte Bemühungen der russischen Staatsmacht zum Vorantreiben des Umwelt- und Klimaschutzes sichtbar. So wurde in Russland die Ratifizierung des Kyoto-Abkommens am 5. November 2004 mit der Zustimmung des Präsidenten zum Beschluss der Staatsduma abgeschlossen.[96] Am 30. Januar 2008 äußerte sich der designierte Präsident Dmitri Medwedew für eine schnelle Entwicklung des einheimischen Marktes für Innovationstechnik im Umweltschutz.[97] Inzwischen gibt es Pläne der Regierung, die Energieeffizienz in Russland zu steigern, um den erheblichen Verlust an Wärmeenergie für den Wohnungssektor zu begrenzen.
Russlands Geschichte erlebte seit ihrem Beginn im 9. Jahrhundert vielfältige Brüche. So ist die russische Geschichte eine Eigenentwicklung, die sich von der Entwicklung seiner Nachbarn in Europa deutlich unterscheidet. Ursächlich dafür ist ein ständiges In- und Gegeneinanderspiel typisch russischer Merkmale aus sozialen Begebenheiten und geographischen Einflüssen, die seine Geschichte auf weiten Strecken begleiteten. So gab die erdräumliche Lage Russland eine Brückenstellung zwischen Europa und Asien, die je nach Kräftelage die Aggression fremder Mächte (größere Einfälle u. a. 1240, 1242, 1609, 1709, 1812, 1917, 1941) oder die eigene Expansion begünstigte. Dazu trug das Fehlen natürlicher Grenzen bei, was Russland im Wechselspiel mit der Erfahrung fremder Einfälle dazu veranlasste, die Grenzen so weit auszudehnen, bis natürliche Grenzen einen wirksamen Schutz bilden konnten (vgl. Russische Kolonisation).[98] Dieses starke, aus historischen Einfällen resultierende Sicherheitsbedürfnis Russlands setzt sich bis heute fort.
Die Spannung zwischen wirtschaftlichen Notwendigkeiten und der Bewältigung bzw. Nichtbewältigung durch die jeweils herrschenden Gruppen gehört ebenso zu den Konstanten der russischen Geschichte. Beispielhaft zu nennen sind die Nichtbewältigung der sozialen Unruhen im Zuge des Industriezeitalters mit ihren Höhepunkten in der Revolution 1905, der Februar- und der Oktoberrevolution 1917 oder die postkommunistische Systemtransformation der 1990er-Jahre.
Die aus der byzantinischen Orthodoxie übernommenen Denkweisen führten zu Spannungen mit modernistischen Tendenzen und begründeten das markante Spannungsverhältnis zwischen Beharrung und Fortschritt, das sich z. B. bei der Kirchenspaltung 1666/1667 oder den Petrinischen Reformen 1700–1720 deutlich zeigte. Aufgrund der fehlenden römischen Rechtstradition fehlte lange Zeit ein Widerstandsrecht gegen herrscherliche Übergriffe, so dass die Beziehung zwischen Staatsgewalt und der wirtschaftlichen und politischen Freiheit des Einzelnen belastet blieb. Dies zeigte sich besonders im 19. Jahrhundert, als liberale Ideen in Russland vermehrt Anhänger fanden und sich in mehreren Attentaten gegen den russischen Selbstherrscher äußerten (z. B. Dekabristenaufstand).
Die bis zum Ende der Sowjetunion ausgeprägte Verbindung von genossenschaftlichen mit herrschaftlichen Elementen liegt ursprünglich in der orthodoxen Kirche begründet, wo die Gemeinschaft der Gläubigen eine viel größere Rolle spielte als das Gott gegenüber verantwortliche Individuum. An diese Vorstellungen des Kollektivs knüpften im 19. und 20. Jahrhundert Marxisten und Sozialisten an und setzte diese in der Sowjetunion fort. Der Ausgleich zwischen zentralistischer und dezentraler Herrschaft war in der Geschichte Russlands ein konstantes Problem. Insbesondere in Übergangszeiten (z. B. zwischen 1240 und 1480, nach 1917 und nach 1994) nahmen separatistische Strömungen an den Rändern des Landes zu.
Der alte ostslawische Name für das Gebiet des von Slawen bewohnten Teils des europäischen Russlands, Belarus’ und der Ukraine war Rus (siehe Kiewer Rus), auf Griechisch Rossia. Auf diese Form geht der heutige russische Landesname Rossija zurück. Die früheste Geschichte des europäischen Russlands (zur Geschichte des asiatischen Teils siehe Geschichte Sibiriens) ist im Norden geprägt von finno-ugrischen Völkern und Balten, im Süden von den indogermanischen Steppenvölkern des Kurganvolks, der Kimmerer, Skythen, Sarmaten und Alanen; später kamen hier noch Griechen, Goten, Hunnen und Awaren hinzu. In die Mitte, zwischen Dnepr und Bug, kamen die slawischen Völker, die sich ab dem 6. Jahrhundert auch nach Norden und Osten auszudehnen begannen.
Ab dem 8. Jahrhundert befuhren skandinavische Wikinger die osteuropäischen Flüsse und vermischten sich später mit der slawischen Mehrheitsbevölkerung. Diese auch Waräger oder Rus genannten Kriegerkaufleute waren maßgeblich an der Gründung des ersten ostslawischen Staates, der Kiewer Rus mit Zentren in Kiew und Nowgorod, beteiligt. Im südlichen Steppengebiet und an der Wolga waren hingegen Reiche der aus Asien eingeströmten Turkvölker der Chasaren und Wolgabulgaren entstanden, mit denen die Rus Handel trieben, aber auch Kriege führten. Intensive Kontakte mit dem Byzantinischen Reich führten schließlich 988 zur orthodoxen Christianisierung der Kiewer Rus.
Das mangelhafte Senioratsprinzip zur Regelung der Erbfolge förderte die Zersplitterung der Kiewer Rus im 12. Jahrhundert und erleichterte die Unterwerfung der zerstrittenen russischen Fürstentümer im Mongolensturm. Die mongolische Invasion der Rus begann 1223 mit der Schlacht an der Kalka; die Übergangsphase bis zur Mitte des 14. Jahrhunderts wird als „dunkles“ Zeitalter bezeichnet.[99] Die russische Nationalhistoriographie spricht vom „Tatarenjoch“ dieser Zeit. Die mongolische Fremdherrschaft führte demnach für zwei Jahrhunderte zu einem Abbruch der Beziehungen zum Westen und förderte die Abkapselung des orthodoxen Russlands.[100] Die russischen Fürstentümer lagen im Machtbereich der Goldenen Horde, konnten jedoch eine gewisse innere Autonomie bewahren. Derweil mussten die russischen Fürstentümer im Norden und Westen Angriffe von Schweden, Ordensrittern und Litauern abwehren. Unter den zersplitterten und verfeindeten russischen Fürstentümern erwies sich das kleine und unbedeutende Fürstentum Moskau als das durchsetzungsstärkste. Dmitri Donskoi, der verschiedene russische Fürstentümer einen konnte, besiegte im Jahre 1380 die Goldene Horde in der Schlacht auf dem Schnepfenfeld.
Der Moskauer Großfürst Iwan der Große beendete die Mongolenherrschaft und wurde de facto zum Begründer eines zentralisierten russischen Staates, indem er Schritt für Schritt die umliegenden russischen Länder „einsammelte“ (russisch собирание земель, sobiranije semel), darunter die Republik Nowgorod. Sein Titel „Herrscher der ganzen Rus“ drückte auch den Anspruch auf den vom Großfürstentum Litauen im 14. Jahrhundert beherrschten westlichen Teil der Rus aus. Dies führte zu langanhaltenden Kriegen im 16. und 17. Jahrhundert mit Polen und Litauen (vgl. Russisch-Litauische Kriege). Unter Iwan dem Großen wurde die russische Gesetzgebung reformiert und der Großteil des heutigen Moskauer Kremls erbaut. Sein Enkel Iwan IV. begründete 1547 das Zarentum Russland. Unter seiner Herrschaft begann nach der Einnahme der Tatarenhauptstadt Kasan auch die Eroberung Sibiriens, die russische Kosaken erstmals im 17. Jahrhundert bis an den Pazifik brachte.
An der Wende zum 18. Jahrhundert öffnete Zar Peter der Große das in den alten Strukturen erstarrte Zarentum Russland westeuropäischen Einflüssen und förderte Wissenschaft und Kultur. 1703 gründet er die Stadt Sankt Petersburg, die – seit 1712 als neue Hauptstadt – das Symbol für den russischen Fortschritt werden sollte. Mit dem Sieg gegen Schweden im über 20 Jahre währenden Großen Nordischen Krieg erlangte Russland nach mehr als 150 Jahren der Auseinandersetzung mit Schweden die Vormachtstellung im Ostseeraum (vgl. Nordische Kriege). Russland übernahm die Position Schwedens als nordische Großmacht in Europa. Zur Unterstreichung des neuen Status im diplomatischen Ranggefüge Europas ließ Zar Peter das Russische Zarentum in „Russisches Kaiserreich“ umbenennen und änderte den Monarchentitel offiziell von „Zar“ in „Kaiser“ (russisch Император, Imperator).
Katharina die Große führte Peters Expansionspolitik weiter. Unter ihrer Regierung wurde das Krimkhanat („Neurussland“) erobert. Durch die Beteiligung an den drei Teilungen Polens wurde die Westgrenze Russlands weit in Richtung Mitteleuropa vorgeschoben. 1812 fielen Napoleons Truppen in Russland ein und eroberten Moskau, wurden schließlich jedoch vernichtend geschlagen. Dies gab den Auftakt zu den Befreiungskriegen, bei denen russische Truppen mit ihren Verbündeten (Preußen, Österreich, Vereinigtes Königreich u. a.) Napoleon endgültig besiegen und zur Abdankung zwingen konnten. Alexander I. zog als „Befreier Europas“ in Paris ein. Nach dem Wiener Kongress 1814/15 erlangte Russland eine dominierende Rolle auf dem europäischen Festland, die bis zum Krimkrieg 1853–1856 andauerte. Aufgrund der festgefahrenen gesellschaftlichen Strukturen wie der Autokratie und der Leibeigenschaft konnte das agrarisch geprägte Reich jedoch mit den sich rasant entwickelnden Industriestaaten immer weniger Schritt halten. Der verlorene Krimkrieg gegen die Westmächte legte die inneren Schwächen des Reiches offen und gab Anstoß zu einer Phase der inneren Reformen. Diese beschleunigten Russlands wirtschaftliche Entwicklung, doch das Land wurde immer wieder von inneren Unruhen destabilisiert, da die politischen Veränderungen nicht weitreichend genug waren und große Teile der Bevölkerung ausgeklammert wurden. Den „Westlern“, die eine Übernahme westeuropäischer Lebensformen und politischer Institutionen propagierten, standen aber immer auch die nationalromantisch geprägten „Russophilen“ oder „Slawophilen“ gegenüber, die einen eigenen, spezifisch russischen Weg in die Moderne forderten und die pauschale Übernahme westlicher Werte ganz oder zum großen Teil ablehnten.
In den großen Städten entstand um die Jahrhundertwende ein Industrieproletariat, aber sehr rasch auch eine bürgerliche Mittelschicht. Diese forderte ihren Anteil an der Verfügung über die Staatseinnahmen und die Mitverantwortung für die öffentlichen Angelegenheiten. Die Angehörigen der Mittelschicht besaßen aber kein gemeinsames politisches Bewusstsein. Sie verstanden unter politischer Freiheit kein moralisches Ziel, sondern meinten damit die Freiheit der materiellen Entfaltung und gerechte Besteuerung.[101] So ließ sich die Mittelschicht auch nicht auf Dauer von den utopischen Entwürfen der Intelligenzija leiten. Eine Anpassung der Verfassungswirklichkeit des Staates, der die Mittelschicht näher eingebunden hätte, fand aber nicht statt. Stattdessen flammte der Terror wieder auf. Die Niederlage im Russisch-Japanischen Krieg führte letztlich zur Russischen Revolution von 1905. Der russische Kaiser Nikolaus II. war jedoch nicht bereit, grundlegende Reformen einzuleiten und ließ ein weitgehend funktionsloses Parlament, die Duma, das er notgedrungen genehmigt hatte, nur kurze Zeit später wieder auflösen.
Als im Jahre 1914 der Erste Weltkrieg ausbrach, erfasste Russland als Mitglied der Entente eine patriotische Welle – eine Stimmung, die anfänglich alle Kriegsparteien bestimmte, einschließlich des Deutschen Kaiserreichs und dessen Verbündeten (Mittelmächte). Die anfänglichen Erfolge, vor allem gegen Österreich-Ungarn und das Osmanische Reich, wurden bald abgelöst von einem Stellungskrieg, bis 1917 die Moral der russischen Soldaten nachgab und die Front zusammenbrach. Die Unzufriedenheit der Bevölkerung und die trostlose Versorgungslage führten in der Hauptstadt Petrograd zu Demonstrationen der Arbeiter und Bauern. Nach blutiger Niederschlagung der Demonstranten stürmten diese den Winterpalast und Kaiser Nikolaus II. wurde zum Abdanken gezwungen.
In Folge kam im Februar 1917 eine provisorische Regierung (unter Beteiligung der Menschewiki und von Sozialrevolutionären) an die Macht, die als Doppelregierung mit Arbeiter- und Soldatensowjets amtierte. Die radikalrevolutionären Bolschewiki stellten hier zunächst eine Minderheit dar. Da die provisorische Regierung zur Enttäuschung weiter Teile der Bevölkerung den Krieg nicht beendete und nötige innenpolitische Reformen nicht in Angriff nahm, gewannen die Bolschewiki unter dem im April aus dem Exil zurückgekehrten Wladimir Iljitsch Lenin an Zulauf und stürzten diese im Oktober 1917.
Nach der Februarrevolution 1917 erlangten die Frauen in Russland das aktive und passive Wahlrecht.[102] Sie waren sowohl an den Wahlen zu den Sowjets als auch zu den Stadtdumas zugelassen. Im Mai 1917 wurde ein Gesetz beschlossen, das russischen Staatsbürgerinnen und Staatsbürgern über 20 das Recht verschaffte, die Konstituierende Versammlung zu wählen. Nach der Oktoberrevolution wurde das aktive und passive Frauenwahlrecht in der Verfassung der Russischen Sowjetrepublik vom 10. Juli 1918 festgeschrieben.[103][104][105]
Aus dem der Oktoberrevolution folgenden Bürgerkrieg zwischen den sozialistischen „Roten“ und den gegenrevolutionären „Weißen“ gingen die Bolschewiki als Sieger hervor. Die drei baltischen Staaten Estland, Lettland und Litauen, wie auch Finnland, errangen dagegen durch Abwehr der Roten Armee bzw. durch längere Bürgerkriege ihre Unabhängigkeit von Russland. Im Laufe des Bürgerkriegs sowie des darauf folgenden Polnisch-Russischen Kriegs verlor Russland 1920 Teile Belarus’ und der Ukraine („Ostpolen“) an Polen. 1921 wurde die russische Sowjetrepublik als Russische Sozialistische Föderative Sowjetrepublik (RSFSR) ausgerufen, die den wichtigsten Teil der späteren Sowjetunion darstellte.
Am 30. Dezember 1922 wurde aus dem bisher bestehenden Sowjetrussland die Sowjetunion gegründet und eine staatlich kontrollierte Wirtschaftspolitik ausgerufen. Die Sowjets wurden als Eigentümer von Boden und Produktionsmitteln erklärt. Lenins Tod am 21. Januar 1924 führte zu einem erbitterten Nachfolgekampf, in dem sich Josef Stalin gegen Leo Trotzki durchsetzte. Der Stalinismus zeichnete sich durch gezielten Terror aus. Seit 1928 wurde die staatliche Wirtschaft Fünfjahresplänen unterworfen und die Industrialisierung der Sowjetunion vorangetrieben. Die Zwangskollektivierung in der Sowjetunion wurde von der Kampagne der „Entkulakisierung“ begleitet.
Im August 1939 schloss die Sowjetunion einen Nichtangriffspakt mit dem NS-Staat, wobei in einem geheimen Zusatz auch eine einvernehmliche Aufteilung Osteuropas aufgenommen wurde. Dies ermöglichte Hitler Anfang September 1939 den geplanten Angriffskrieg gegen Polen, der mit einem sowjetischen Angriff gegen Ostpolen Mitte September abgestimmt war. Im Winterkrieg überfiel die Sowjetunion Finnland und gewann kleinere Teile des Landes. 1940 wurden Litauen, Lettland und Estland besetzt.
Nach dem deutschen Überfall auf die Sowjetunion am 22. Juni 1941, der zum Deutsch-Sowjetischen Krieg führte (in der Sowjetunion Großer Vaterländischer Krieg genannt), trat das Land der Anti-Hitler-Koalition bei. Allein während der Leningrader Blockade verhungerten über eine Million Menschen in Leningrad. Insgesamt starben in diesem Krieg geschätzt 27 Millionen Sowjetbürger, davon 14 Millionen Zivilisten.[106] Sie konnte aber im Kriegsverlauf den deutschen Truppen schwere Niederlagen zufügen und siegte im Mai 1945 in der abschließenden Schlacht um Berlin. Nach dem Krieg sicherte sich die Sowjetunion großen Einfluss in den angrenzenden Ländern Polen, Tschechoslowakei, Ungarn, Rumänien, Bulgarien, Albanien und in der DDR. In diesen Ländern blieben Hunderttausende sowjetischer Soldaten stationiert. Der Kalte Krieg dominierte bis 1989 die Weltpolitik.
Der letzte sowjetische Präsident Michail Gorbatschow leitete ab 1987 mit der „Perestroika“ einen Umbau des politischen und wirtschaftlichen Systems in der Sowjetunion ein und förderte mit der Politik der „Glasnost“ die Transparenz und Offenheit der Staatsführung gegenüber der Bevölkerung, worauf einzelne Unionsrepubliken die Unabhängigkeit von der Sowjetunion anstrebten. Nach dem misslungenen Augustputsch in Moskau 1991 konservativer Kommunisten beschlossen der Präsident Russlands Boris Jelzin und Vertreter der Sowjetrepubliken die Auflösung der UdSSR zum 31. Dezember 1991.
Die Russische Föderation übt seit 1992 als größte ehemalige Sowjetrepublik (Russische SFSR) die völkerrechtlichen Rechte und Pflichten der UdSSR aus.[107] In den ersten Jahren ergaben sich innenpolitische Konflikte über den einzuschlagenden Kurs: In der russischen Verfassungskrise 1993 löste Jelzin per Ukas den Volksdeputiertenkongress sowie den Obersten Sowjet Russlands auf, die sich seinen Bemühungen und den Resultaten einer Volksbefragung am 25. April 1993 widersetzt hatten, Wirtschaftsreformen durchzusetzen. Jelzin ordnete eine gewaltsame Stürmung des Parlamentsgebäudes (Weißes Haus) an, in dem sich etwa 100 Parlamentarier und weitere Anhänger verbarrikadiert hatten. Bei der gewaltsamen Niederschlagung eines weiteren Aufstandes gegen ihn am 3. und 4. Oktober gab es in Moskau 190 Tote. Im Dezember billigte die russische Bevölkerung per Volksabstimmung die neue Verfassung der Russischen Föderation (Zweikammersystem, Präsidialverwaltung).
Unter Jelzin wurden in Russland Teile der Wirtschaft privatisiert und Reformen versucht. Dabei gelangten wertvolle Unternehmen, Banken und Rohstoffvorkommen, u. a. Mineralöl, bei Versteigerungen weit unter ihrem Wert in den Besitz von Oligarchen wie beispielsweise Sergey Grishin und Roman Abramowitsch, die gute Beziehungen zu Herrschenden hatten bzw. diesen Schmiergelder und Schutzgelder zahlten.[108] Durch lukrative Geschäfte mit dem Staat konnten die Oligarchen ihren Profit zum Nachteil des Volkes noch steigern.
1991/92 gab es eine Rubelkrise. Das Bruttoinlandprodukt (BIP) lag 1993 um 12 % unter dem von 1992 und um 29 % unter dem von 1991. Die Industrieproduktion war 1993 um 31,3 %, die Konsumgüterproduktion um 24,8 % und die Nahrungsmittelproduktion um 27,3 % niedriger als 1991. Im Oktober 1993 waren 2400 Produktionsbetriebe vorübergehend stillgelegt, im Februar 1994 4280. Wegen Nichtzahlung von Löhnen und Gehältern kam es zu gesamtwirtschaftlich folgenschweren Streiks, z. B. in den Kohlerevieren.[109]
Die Inflation war jahrelang hoch und große Teile der Bevölkerung verarmten. 1998 rutschte das Land in die Zahlungsunfähigkeit (→ Russlandkrise). Insbesondere in der Übergangszeit nahmen aufgrund des Erstarkens regionaler Autonomien nach dem Ende der stark zentralistischen Sowjetzeit zentrifugale Strömungen an den Rändern des Landes zu. So sah sich seit Mitte der 1990er-Jahre die russische Regierung mit Unabhängigkeitsbewegungen und Machtkämpfen in zahlreichen Teilrepubliken konfrontiert, besonders im Ersten Tschetschenienkrieg 1994/96, bei dem zehntausende Menschen starben. Von Frühherbst 1999 bis Anfang 2000 brachten russische Truppen den Großteil Tschetscheniens wieder unter ihre Kontrolle (vgl. Zweiter Tschetschenienkrieg).
Die chaotischen Jahre unter Jelzin verunsicherten viele Menschen. Die Geburtenrate war niedrig; Kriminalität, Alkoholismus etc. waren verbreitet. In der Endphase von Jelzins Herrschaft bestand die russische Außenpolitik fast nur noch aus leeren Drohungen und Reaktionen. Dies betraf z. B. die NATO-Osterweiterung und den Kosovokrieg. Auch einige markante Ereignisse wie der Untergang der Kursk im August 2000, der tagelange Brand des Moskauer Fernsehturms Ostankino und das Ende der Mir im März 2001 förderten bei vielen Russen das Gefühl, Russland sei von der Rolle einer Supermacht auf die eines Schwellenlands zurückgefallen.[110]
Hohe Rohstoffpreise (Öl, Gas, Stahl), eine Steuerreform und Kapitalrückfluss förderten die wirtschaftliche Erholung nach dem Amtsantritt Wladimir Putins. Nach der Geiselnahme von Beslan im September 2004 leitete Putin einen grundlegenden Umbau des Staatswesens ein, der Macht und Kontrolle in noch stärkerem Maß als bisher in den Händen des Präsidenten konzentrierte. „Für Putin ging es später darum, mit Hilfe einer ‚Machtvertikale‘ der Exekutive auf allen staatlichen Ebenen die Alleinherrschaft des Kreml zu sichern.“ Die Machtvertikale wird von westlichen Beobachtern wie z. B. Margareta Mommsen (2012) als in jeder Hinsicht unvereinbar mit Vorstellungen einer eigenständigen Rolle des Parlaments, von wechselnden parlamentarischen Mehrheiten sowie vom freien Wettbewerb politischer Parteien gesehen.[111] Selbst die höchsten politischen Amtsträger verfügten über kein klares Verfassungsverständnis; mit diesem Ansatz könne weder eine Verfassungslegitimität noch eine Verfassungskultur entstehen. „Unterdessen wird der praktizierte Autoritarismus als ein notwendiges Provisorium gerechtfertigt. So beruft sich Putin gerne auf eine ‚Herrschaft per Handsteuerung‘. […] Damit gab er sich überzeugt, dass der politische Prozess weiterhin der persönlichen Lenkung und der ad hoc-Arrangements anstatt der Verfassung folgen müsse.“[111]
Nach den Fälschungen der Parlamentswahlen 2011 kam es zu Großdemonstrationen mit Hunderttausenden von Teilnehmern. Darauf und auf die Proteste bei der Präsidentenwahl reagierte die Staatsmacht mit noch mehr Repression; es wurde bereits verhaftet, wer sich mit einer anderen „protestierenden“ Person traf; jede andere Protestform als ein Einzelprotest wurde verboten,[112] Anmeldungen von Demonstrationen zur Bewilligung waren stets willkürlichen Regeln unterworfen. Auch wurde ein Gesetz über „ausländische Agenten“ in Russland eingeführt.
Ab 2013 begann die Stagnation der Wirtschaft.[113]
Am 20. Februar 2014 kam es in einem nur halb verdeckten Militäreinsatz prorussischer Kämpfer zu Angriffen auf die durch einen Freundschaftsvertrag mit Russland verbundene Ukraine. Deren vertraglich garantierte Souveränität wurde vor allem durch die russische Annexion der Halbinsel Krim verletzt. Am 21. März 2014 wurde der Föderationskreis Krim gegründet. Die völkerrechtliche Legitimität dieser Schritte ist außerhalb Russlands, aber auch in Russland selbst umstritten.[114][115]
Der von Russland 2014 angestoßene Hybridkrieg in der östlichen Ukraine währte nach einer internationalen Eindämmung über mehrere Jahre. Am 24. Februar 2022 folgte darauf der russische Überfall auf die Ukraine.
In der Gesellschaft war es in der Zwischenzeit zu mehreren Demonstrationswellen gekommen. 2018 demonstrierten die Menschen wochenlang gegen die Erhöhung des Rentenalters, 2019 kam es neben einer bewilligten Großdemonstration[116] trotz Demonstrationsverboten zu wochenlangen Protesten gegen den Ausschluss von Kandidaten bei den Kommunalwahlen.[117] Diese Proteste wären moralisch, nicht politisch, so Leonid Gosman, daher vereinten sie Menschen verschiedener politischer Ansichten gegen die Arroganz und Unzulänglichkeit der Behörden mit ihren Lügen und ihrer Verachtung der Menschen.[118] Weitere Proteste gab es in Chabarowsk 2020 nach der offensichtlich politisch motivierten Festnahme des Gouverneurs Sergei Furgal. Im Januar 2021 protestierten Zehntausende gegen die Festnahme von Alexei Nawalny.
Nachdem die russische Propaganda auf die Menschen eingewirkt hatte, erreichte eine mäßige Zustimmung zu Putins Krieg gegen die Ukraine 2022 laut einer Umfrage rund 58 %, während 23 % diesen klar ablehnten. Die Zustimmung war unter den über 66-Jährigen mit 75 % am höchsten, während sie bei den unter 24-Jährigen noch 29 % betrug.[119]
Am 21. Oktober 2022 wurde nach Anordnung des Präsidenten ein Koordinierungsrat „für die Erfüllung der Bedürfnisse der russischen Streitkräfte“ geschaffen, in welchem verschiedene Minister einsitzen und welcher die Aufgabe hat, die Lieferung und Reparatur von Waffen, militärischer Ausrüstung sowie die Versorgung der Armee mit materiellen Ressourcen und medizinischer Versorgung sicherzustellen.[120] Russische Soldaten hatten sich monatelang über mangelhafte Versorgung und Waffen beklagt, nach der Mobilmachung Ende September waren diese Mängel verstärkt ersichtlich geworden. Durch den Koordinationsrat wurden zivile Strukturen für die Versorgung der Armee verantwortlich.[121]
Der russische Föderalismus ist sehr asymmetrisch geprägt, da das föderale System eine Kombination aus ethnoföderalen Republiken und territorial-föderalen Gebieten darstellt. Die Einteilung des Landes wurde im Wesentlichen aus der Sowjetzeit übernommen, sieht man von der Statusanhebung der meisten Autonomen Gebiete zu Republiken und der Aufteilung der vormaligen Tschetscheno-Inguschetischen ASSR in zwei Republiken ab. Russland gliedert sich laut Artikel 65 der russischen Verfassung in 85 Föderationssubjekte. Dazu zählen 22 Republiken, neun Regionen (Krai), 46 Gebiete (Oblast), drei Städte föderalen Ranges (Moskau, Sankt Petersburg und Sewastopol), ein Autonomes Gebiet und vier Autonome Kreise. Dass die Völkerrechtssubjekte Krim und Sewastopol zu Russland gehören, ist international nicht anerkannt. Die Republiken wurden nach den jeweils dominierenden nichtrussischen Volksgruppen definiert, wenngleich ihre Grenzen nicht immer mit den ethnischen übereinstimmen, während die Gebiete in den übrigen, mehrheitlich von Russen bewohnten Teilen des Landes nach rein administrativen Gesichtspunkten gebildet wurden. Territorien, in denen kleinere nichtrussische Minderheiten leben, erhalten den niedrigeren Rang eines Autonomen Gebietes, beziehungsweise Autonomen Kreises. Bezogen auf Bevölkerung, Fläche und relativen Wohlstand unterscheiden sich die Föderationssubjekte mitunter erheblich.
Obwohl alle Föderationssubjekte formal gleichgestellt sind, sind nur die Republiken berechtigt, eine eigene Verfassung zu erlassen. Sie können zudem internationale Verträge unterzeichnen, solange sich diese an die russische Verfassung halten. Besonderheiten der Republiken bestehen zudem in der traditionellen Namensgebung, der Anzahl der Abgeordneten in Regionalparlamenten und spezifischen Gesetzgebungskompetenzen.
Die Oblaste und Kraje sind im Unterschied zu den Republiken keine Staaten. Sie verfügen nur über Statuten anstelle von Verfassungen. An der Spitze der Republiken steht meist ein Präsident. Die übrigen Föderationssubjekte werden von dem Leiter der Administration geführt, dem Gouverneur. Die gesetzgebenden Körperschaften in den Republiken sind sowohl Einkammer- als auch Zweikammersysteme. In den Gebieten besteht die parlamentarische Vertretung nur aus einer Kammer.
Seit 2005 werden die Republikpräsidenten und Gouverneure nicht mehr von der Bevölkerung, sondern vom regionalen Parlament gewählt. Die Kandidaten schlägt der Präsident vor.
Im Jahr 2000 schuf Präsident Putin per Dekret sieben Föderationskreise, die jeweils mehrere Föderationssubjekte zu einer größeren Einheit zusammenfassen. Ziel dieser Reform war die Stärkung der vertikalen Machtverteilung und eine Verschärfung der Kontrolle über die regionalen Machthaber. Die Einwohnerzahlen in der folgenden Tabelle beziehen sich auf die Volkszählung vom 9. Oktober 2002. Im Jahr 2010 wurde zudem der Föderationskreis Nordkaukasus, durch Ausgliederung aus dem Föderationskreis Südrussland, als achter Föderationskreis geschaffen.
Nach der gewaltsamen und widerrechtlichen Aneignung der Krim durch die Russische Föderation[122] bildete die Krim ab dem 21. März 2014 einen eigenen (neunten) Föderationskreis, der per 28. Juli 2016 aufgelöst und dem Föderationskreis Südrussland angeschlossen wurde.
Neben den genannten zwei hierarchischen föderalen Ebenen (1. Föderationskreis, 2. Föderationssubjekt) gibt es noch eine dritte eigenständige Verwaltungsebene, die der kommunalen Selbstverwaltung (Rajon). Deren administrative Leiter werden von der Bevölkerung direkt gewählt. Die Regionen sind gegenüber den kommunalen Selbstverwaltungsorganen administrativ höherstehend und weisungsberechtigt.
Mit dem Untergang der Sowjetunion Ende 1991 kam die Chance für demokratische und liberale Reformen. Diese wurden durch den kommunistisch dominierten Volksdeputiertenkongress blockiert. Präsident Boris Jelzin griff deswegen zu harten und verfassungswidrigen Mitteln und löste den Volksdeputiertenkongress im Herbst 1993 durch den Einsatz des Militärs auf. Es wurde eine Verfassung geschaffen, die den Präsidenten weitgehend der Kontrolle von Volk und Parlament entzog. Die gültige Verfassung der Russischen Föderation wurde am 12. Dezember 1993 durch eine Volksabstimmung angenommen und trat am 25. Dezember 1993 in Kraft. Sie stellt einen Bruch mit der sowjetischen Vergangenheit dar. Im Mittelpunkt steht gemäß der Verfassung der Mensch: Menschenrechte und Freiheitsrechte wie Rede-, die Presse- und die Reisefreiheit sind die höchsten Werte. In der seither umgesetzten Praxis wird Russland wegen der Einschränkung von Grundrechten als gelenkte Demokratie bezeichnet oder aber mit dem Fachbegriff Autoritarismus umschrieben. Die Kluft zwischen Rhetorik und Handlungen in diesen Sphären ist eklatant.[123]
Die Bilanz der Ära Jelzin war gespalten: Zwar konnten in Russland demokratische und liberale Reformen eingeführt werden. Während der Liberalisierung und Privatisierung schnellten die Verbraucherpreise in die Höhe und eine neue Oberschicht von Oligarchen entstand, die aktiv politische Macht ausübten.[124] In der Bevölkerung wurde diese Demokratisierungs- und Liberalisierungsphase darum noch verstärkt als Auflösung einer gesicherten und berechenbaren staatlichen, gesellschaftlichen und wirtschaftlichen Ordnung empfunden, in welcher viele Leistungen gratis gewesen waren. Hinzu kam, dass der mit der Privatisierung verbundene Gang an die Börse von der internationalen Finanzkrise ab 2007 überschattet wurde. Während 2007 neun Aktiengesellschaften an der internationalen Londoner Börse neues Kapital einwerben konnten, gelang 2009 auf dem Höhepunkt der Krise mit RusHydro nur noch einem russischen Unternehmen der Gang an die Börse.[125]
2008 kam es zum Kaukasuskrieg gegen Georgien.
Ab 2010 stabilisierten sich die politischen Verhältnisse nach und nach, nicht zuletzt aufgrund der fortschreitenden Konzentration der Staatsmacht auf einen starken Präsidenten, die allerdings auch zu Lasten von Pluralismus und demokratischen Freiheiten ging.
Russland ist nach der Verfassung vom 12. Dezember 1993 ein „demokratischer föderativer Rechtsstaat mit republikanischer Regierungsform“[126] und einem semipräsidentiellen Regierungssystem. So ist das Staatsoberhaupt der Präsident Russlands, der vom Volk für jeweils sechs Jahre direkt gewählt wird. Der Präsident gehört unmittelbar keiner der drei Staatsgewalten an, sichert aber ihr Funktionieren und Zusammenwirken. Die Haupteinwirkungsform des Präsidenten ist das Dekret, mit dem er jeden Sachverhalt mit unmittelbarer Rechtswirkung regeln kann. Der Präsident bestimmt die Hauptrichtungen der Außenpolitik und kann internationale Verträge unterzeichnen. Er ist der Oberste Befehlshaber der Streitkräfte Russlands, ernennt und entlässt das Oberkommando der Streitkräfte.
De facto stellt das politische System Russlands eine Mischung aus instabilen demokratischen Institutionen und autoritären Praktiken dar. Seit der Jahrtausendwende lässt sich dabei eine deutliche „Ent-Demokratisierung“ dieses Systems und eine Zentralisierung der politischen Macht beim Präsidenten und seiner Verwaltung beobachten.[128]
Unter Präsident Putin (2000 bis 2008 und erneut seit 2012) wurde die Macht des Staatsoberhaupts durch die Schaffung einer „Machtvertikalen“ ausgebaut: Der Präsident Russlands schlug ab 2005 bis Mai 2012 die Gouverneure vor – die Regionalparlamente konnten diese nur noch bestätigen. Diese von Russland „souveräne Demokratie“ genannte Variante beschnitt politische Rechte[129] der Regionen, die unter Präsident Jelzin ein politisches Gegengewicht aufgebaut hatten. Die Gouverneure wiederum ernannten (seit 2002 anstelle der regionalen Parlamente) die Vertreter für den Föderationsrat und auch lokale Vertreter wie Bürgermeister.[130] Kritische Beobachter sprachen nach der Entmachtung der Regionen auch von einer „Surrogatsföderation“ anstelle einer richtigen Föderation.[131]
Nach Protesten wegen der Parlamentswahlen im Dezember 2011 wurde das Gesetz geändert. Die Gouverneure werden seit Oktober 2012 wieder gewählt. „Im Ergebnis entstand“, so Margareta Mommsen, „ein autoritäres System mit der Besonderheit förmlich fortbestehender demokratischer Einrichtungen. Diese gaukeln demokratische Verhältnisse lediglich vor. Nicht zufällig sprechen kritische Beobachter von einer ‚simulierten Demokratie‘.“[132] So enden polizeiliche und staatsanwaltliche Ermittlungen dort bzw. werden erst gar nicht begonnen, wo sie einflussreiche Politiker berühren.[133]
Im Demokratieindex der britischen Zeitschrift The Economist belegt Russland unter den 167 untersuchten Staaten den 124. Rang und wird als „autoritäres Regime“ eingestuft (Stand 2020).[134] Im Jahr 2007 wurde es noch als hybrides System eingestuft.[135] Etwas weniger negativ ist die Einstufung im Transformationsindex der Bertelsmann-Stiftung, wo Russland 2017 (bezogen auf Demokratie) auf Platz 84 von 137 Ländern zwischen Mali und Bangladesch liegt.[136]
Seit dem Verzicht der KPdSU auf ihre verfassungsmäßige Führungsrolle 1990 vollzog sich ein Wandel von einem totalitären Einparteienstaat zu einer Mehrparteiendemokratie. Es bildeten sich Hunderte von politischen Gruppierungen, Splittergruppen, Bewegungen und Parteien, die ein breites politisches Spektrum von Monarchisten bis hin zu Kommunisten abdecken. Die russischen Parteien sind eher schwach und verfügten selten über eine stabile Identität.
Seit der Parlamentswahl in Russland 1995 unterstützt die Regierung jeweils eine neue, eigene Hausmacht. Diese administrativen, von oben gegründeten „Parteien der Macht“ (партии власти, partii wlasti) sind lose Ad-hoc-Bündnisse, die sich auf Bürokraten stützen, die dem Präsidenten loyal ergeben sind.[137]
Seit der Jahrtausendwende funktionieren einige wenige Parteien als gesellschaftliche Netzwerke, die spezifische Wählergruppen mobilisieren können. Von 2008 bis 2011 bestanden in Russland nur sieben Parteien. Im Zuge der Demonstrationen zur Parlamentswahl im Dezember 2011 wurde ein neues Parteiengesetz verabschiedet, das die Zulassung neuer Parteien ab einer Mitgliederzahl von 500 Personen vorsieht (bisher 40.000). Nach einer Entscheidung des EGMR zugunsten der regierungskritischen Partei der Volksfreiheit stieg die Zahl der russischen Parteien bis Jahresende 2012 auf 48 an.
Gegenwärtig wird die Politik Russlands von einer einzigen Partei, Einiges Russland (Единая Россия, Jedinaja Rossija), dominiert. Einiges Russland entstand 2001 aus den Parteien Einheit (Единство, Jedinstwo) und Vaterland – ganz Russland (Отечество – Вся Россия, Otetschestwo – wsja Rossija), die sich wiederum zum Teil aus der untergegangenen Partei Unser Haus Russland (Наш дом – Россия, Nasch dom – Rossija) rekrutierten, der Partei von Putins Vorgänger Boris Jelzin.
Neben dieser großen Partei existieren weitere und Splitterparteien. Zu ihnen zählen die Kommunistische Partei der Russischen Föderation, die Liberal-Demokratische Partei Russlands und die sozialdemokratische Partei Gerechtes Russland. Daneben gibt es noch außerhalb der Duma die Partei Jabloko, die Patrioten Russlands und Rechte Sache.
Bis zum Amtsantritt des neuen Präsidenten Wladimir Putin hatten sich die russischen NGOs weitgehend frei von staatlichen Einflüssen entwickeln können. Wahrscheinlich war ihr Einfluss auf den Staat größer als umgekehrt. Das sollte sich schnell ändern. Putin ging sofort daran, die bis dahin zwar nicht autonom agierenden, aber von unterschiedlichen Machtzentren kontrollierten Bereiche der russischen politischen Öffentlichkeit systematisch der Regierung zu unterwerfen. Er nannte das, die „Machtvertikale stärken“ und eine „Diktatur des Rechts“ aufbauen. Hinter diesem Vorgehen steckt die Überzeugung, dass der russische Staat in den 1990er-Jahren kurz vor dem Zerfall gestanden habe und dass das ursächlich mit der Schwäche der Zentralmacht zusammengehangen habe.
Der erste Versuch, die NGOs einzubinden, war die Initiative zu einer großen Bürgerversammlung 2001 im Kreml. Bei dieser Versammlung wurden ausgewählte Themen diskutiert. Allerdings wurden aus Regierungssicht nicht konstruktive NGOs, die sich nicht einfach unterordnen wollten, ausgeschlossen. Dies sollte eine Art „Burgfrieden“ zwischen NGOs und der russischen Regierung darstellen. Jedoch wurde Anfang 2002, trotz Protesten und Verhandlungen, die steuerliche Gleichsetzung von kommerziellen und nichtkommerziellen Unternehmen verabschiedet. Endgültig brach der Frieden, als Michail Chodorkowski verhaftet wurde. Dieser hatte mit seiner Stiftung Offenes Russland begonnen, in großem Maße Projekte von NGOs zu finanzieren, und war somit die letzte Hoffnung auf langfristige und nachhaltige Finanzierung von NGOs im Inland gewesen. Der zweite Bruch war die Rosenrevolution in Georgien, die als Misserfolg der russischen Politik gewertet wurde und in der Wahrnehmung der russischen Regierung ein Werk der vom Westen finanzierten NGOs war. Dies wurde auch beim Machtwechsel in der Ukraine vermutet.
Putin drückte das am 26. Mai 2004 in seiner alljährlichen Ansprache vor beiden Parlamentskammern so aus:
„Es gibt Tausende konstruktiv arbeitende zivilgesellschaftliche Vereinigungen in unserem Land. Aber längst nicht alle orientieren sich daran, die wirklichen Interessen der Menschen zu verteidigen. Für einen Teil dieser Organisationen ist es zu vorrangigen Aufgabe geworden, Finanzierung von einflussreichen ausländischen Stiftungen zu bekommen, für andere, zweifelhafte Gruppen und kommerzielle Interessen zu bedienen. Gleichzeitig interessieren sie die dringendsten Probleme des Landes und seiner Bürger nicht.“
Letztlich blieb das Verhältnis zwischen Regierung und NGOs ambivalent in Putins erster Amtszeit, was aus der Tatsache resultiert, dass marktwirtschaftliche Systeme ein gewisses Maß an Freiheit erfordern. Das Taktieren der Regierung mit den NGOs ist Ausdruck dessen, dass man ein Übergreifen dieser Freiheit ins Politisch-Gesellschaftliche verhindern möchte.
Die zweite Amtszeit war in Bezug auf die NGOs in erster Linie geprägt durch das NGO-Gesetz, mit dem der russischen Regierung weitreichende Kontroll- und Sanktionsinstrumente in die Hand gegeben wurde. Die Rosregistracija überwacht nun die Tätigkeiten der NGOs. Sich dagegen zu beschweren, ist in einer hoch korrupten Gesellschaft wie der russischen, in der Beschwerde- und Berufungsinstanzen insbesondere gegen staatliches Handeln, etwa Gerichte, nur sehr eingeschränkt funktionieren, mit hohem administrativen Aufwand verbunden.[138][139] Die Registrierungsbehörden setzen verstärkt auf Bestimmungen des Arbeitsrechts, Steuerrechts, Arbeitsschutzes oder Brandschutzes, um staatliches Vorgehen gegen die NGOs zumindest teilweise zu kaschieren.[138]
Am 23. Mai 2015 unterschrieb Präsident Putin ein Gesetz, dank dem es russischen Behörden ohne Vorwarnung möglich ist, internationale NGOs auf eine schwarze Liste zu setzen. Hohe Strafen drohen jedermann, der mit solchen „unerwünschten Organisationen“ in Kontakt tritt.[140] Das Gesetz schränkt die Arbeit der Medien und der Zivilgesellschaft ein. Als ein Fall der Anwendung dieses Gesetzes wurde der Entzug des Abgeordnetenmandates des Jabloko-Politikers Lew Schlosberg bekannt, der 2014 von den Beisetzungen wohl in der Ukraine gefallener russischer Soldaten berichtet hatte.[141][142]
Im April 2022 wurde deutschen Stiftungen wie auch der Deutschen Forschungsgemeinschaft, die zuvor von einer Art „besonderem Verhältnis“ zwischen Deutschland und Russland profitiert hatten, die Registrierung entzogen. Ebenso betraf dies Amnesty International und Human Rights Watch sowie die Carnegie-Stiftung.[143]
Von internationalen Bürgerrechtsorganisationen und dem Auswärtigen Amt der Bundesrepublik Deutschland werden die Einschränkungen der Pressefreiheit seit dem Jahr 2001 kritisiert. Die staatliche Einflussnahme im Bereich des Fernsehens ist komplett; alle landesweit sendenden TV-Stationen sind entweder direkt in staatlichem Besitz oder unter staatlicher Kontrolle. Im Radiobereich ist die Situation ähnlich. Offiziell gibt es zwar keine Zensur durch die Regierung, durch Repressionen und Verbote von regimekritischen Sendern sowie die Eigentumsverhältnisse und teilweise Selbstzensur findet diese aber faktisch statt.[144] Drei von insgesamt sechs Voten beim Treffen des Menschenrechtsrates des Präsidenten im Oktober 2017 hatten den durch die staatlichen Medien und deren Propaganda geschürten Hass in der Gesellschaft beklagt.[145][146]
Die Tötungsrate in Russland unterlag zwischen 1990 und 2017 ausgeprägten Schwankungen zwischen 30,5 Tötungen (im Jahr 1995) und 9,2 Tötungen (im Jahr 2017) je 100.000 Einwohner. Der Staat schütze die Bürger nicht, klagten 2017 die Nowaja gaseta sowie die geflüchtete Julija Latynina.[147] Auch ist häusliche Gewalt in Russland ein gesellschaftliches Problem. 40 % aller Gewaltverbrechen in Russland werden in den eigenen vier Wänden, innerhalb der Familie, begangen.[148] Diese Gewalt richtet sich insbesondere gegen Frauen. So sterben dadurch in Russland laut Angaben des Innenministeriums 12.000 bis 14.000 Frauen jährlich.[148][149]
Wiederholt kommt es zu Anschlägen auf Oppositionelle oder Brandanschlägen auf deren Eigentum. Besondere Aufmerksamkeit erregten die Sprengstoffanschläge auf Wohnhäuser 1999, hinter denen man staatliche Täter vermutet. Auch kursierten Listen mit Adressangaben von Oppositionellen im Internet.[150] Polizeiliche und staatsanwaltliche Ermittlungen enden hingegen dort bzw. werden erst gar nicht begonnen, wo sie einflussreiche Politiker berühren.[133] Seit 2015 drohen auch jeder Einzelperson, die sich mit einem improvisierten (oder gar leeren) Protestplakat auf die Straße stellt, bis zu fünf Jahre Haft.[151] In Russland saßen im Jahr 2013 geschätzte 600.000 Menschen in „strenger Lagerhaft“,[152] darunter nicht nur nach Meinung der Menschenrechtsorganisation Memorial auch etliche politische Gefangene.[153] Etwa 140.000 Gefangene waren im Frühjahr 2019 in Haft aufgrund des Paragraphen 228.2 zu Drogen, dessen Missbrauchsmöglichkeiten schon länger bekannt waren[154][155] und der durch den Skandal um den Journalisten Iwan Golunow international bekannt wurde.[156] Im August 2020 ist die Zahl der inhaftierten Sträflinge, Verdächtigen und Angeklagten in russischen Straf- und Untersuchungshaftanstalten laut dem Bundesgefängnisdienst (FSIN) erstmals auf weniger als 500.000 gesunken. Dies ist den Angaben der FSIN zufolge auf den Einsatz alternativer, nicht-inhaftierender Strafen sowie die allgemeine Liberalisierung des Strafvollzugssystems zurückzuführen.[157]
Im Dezember 2015 unterschrieb Putin ein Gesetz, wonach das russische Verfassungsgericht auf Antrag der Regierung Urteile internationaler Gerichte außer Kraft setzen kann, was in erster Linie Urteile des Europäischen Gerichtshofs für Menschenrechte (EGMR) betreffen könnte.[158] Auch für den Kulturbereich wurde eine „nicht greifbare Zensur“ beschrieben.[159]
Homosexualität in Russland ist weitgehend tabuisiert. Die gesetzlichen Regelungen beinhalten unter anderem ein Verbot „homosexueller Propaganda“ (etwa der Regenbogenflagge), was von Kritikern als Verstoß gegen die Europäische Menschenrechtskonvention, das Recht auf sexuelle Selbstbestimmung, die Versammlungsfreiheit und Meinungsäußerungsfreiheit gewertet wird.
Unter dem Vorwand der Extremismusbekämpfung wurden die Freiheiten religiöser Minderheiten stark eingeschränkt.[160] 2016 wurde es Angehörigen nicht registrierter Religionsgemeinschaften verboten, mit anderen über ihre religiöse Überzeugung zu sprechen.[161] Im März 2017 beantragte das russische Justizministerium ein Verbot der Religionsgemeinschaft der Zeugen Jehovas und all ihrer Aktivitäten,[162][163] das im April 2017 umgesetzt wurde.[164]
Auf der Krim hat sich die Menschenrechtslage seit der Besetzung durch Russland erheblich verschlechtert. Laut einem Bericht des UNHCHR kommt es immer wieder zu willkürlichen Verhaftungen und Folter, auch eine außergerichtliche Hinrichtung ist dokumentiert.[165] Am brisantesten ist die Menschenrechtslage seit Jahren im Kaukasus, namentlich in Tschetschenien. Die Überprüfung von Bürgerrechten, z. B. bei Verstößen gegen die Europäische Menschenrechtskonvention, findet nach dem Gesetz vor dem Obersten Gerichtshof Russlands statt.
Im Korruptionswahrnehmungsindex von Transparency International lag Russland mit 28 von möglichen 100 Punkten im 2022er-Ranking weltweit auf Platz 137 unter 180 Staaten und an letzter Stelle aller europäischen Staaten.[166]
Im Jahr 2016 ordnete Präsident Putin persönlich für Kontrollbehörden eine „Kontrollpause“ an. Die angeblichen Sicherheitskontrollen hatten kaum je der Sicherheit gedient, sondern zum größeren Umfang der Bereicherung. Ein Durchbrechen der Korruptionsketten sei auch deshalb kaum möglich, weil saubere Beamten kein Geld nach oben abgeben können und deshalb aus dem Amt gedrängt werden oder Posten für ehrliche Beamte wegen Ablösesummen erst gar nicht zugänglich seien, schreibt Jens Siegert, langjähriger Leiter des Moskauer Büros der Heinrich-Böll-Stiftung.[167] Die Nähe zur Staatsmacht ermöglicht Geld und Privilegien:[168] Jelena Tschischowa beschreibt denn auch nicht nur die alltägliche Korruption, sondern auch, wie der Umfang mit der Nähe zur Macht im Kreml zunimmt, und nennt die Gemeinsamkeit: „In einem autoritären Land ist «Freund» ein Schlüsselbegriff.“[169] Alexei Nawalny und seine Anti-Korruptions-Organisation deckten zahlreiche Fälle von persönlicher Bereicherung und Nepotismus auf höchster Ebene auf, darunter auch die Existenz des sogenannten Palast Putins.

Im Frühjahr 2022 vermehrte sich der bestehende Diskurs, ob Russland unter Putin faschistisch zu nennen sei. Alexander J. Motyl hatte schon 2009 geschrieben, dass sich Russland seit dem Machtantritt Wladimir Putins in Richtung Faschismus bewege. Bis in jenes Jahr sei das System erst mal „faschistoid“ und noch nicht konsolidiert.[170] Die 2009 noch offene Entwicklung des Systems verlief bis 2022 derart, dass Motyl Russland mittlerweile einen faschistischen Staat nannte.[171] Michail Jampolski schrieb 2015 von nationalem (russischem) Exzeptionalismus (vgl. Exzeptionalismus) und Intoleranz auch gegenüber Demokratie; die russische Gesellschaft kultiviere „Gruppen von Grundvoraussetzungen“ gemäß Wilfred Bion. Die zusätzliche projektive Identifikation einer sich postimperial gedemütigt fühlenden Nation verglich er mit dem Aufkommen des Nationalsozialismus in Österreich. Individualität werde abgelehnt, vorherrschend sei eine Ablehnung der Differenzierung von Persönlichkeiten, dies führe zu noch mehr Intoleranz gegenüber Abweichungen von einem einzigen Denk- und Verhaltensmuster.[172]
Bereits im Jahr 2014 wies Timothy Snyder darauf hin, dass die Ideologie Putins faschistische Wurzeln habe[173] im Bezug auf den von Putin oft zitierten Iwan Iljin, den „Philosophen des russischen Faschismus“ russisch-christlicher Ausprägung. Iljin erklärte den Faschismus einer „auserwählten“ Nation als einzige mögliche Erlösung aus einer seit der Schöpfung andauernden Schande.[174] 2022 schrieb Snyder, der Glaube, dass Politik mit der Wahl des „richtigen“ Feindes beginne, und die Rede von „heilender Gewalt“ sei zweifellos faschistisch. Die maximale Selbstbezogenheit und der groteske Widerspruch von Putins Kriegs-Rechtfertigungen bestätigten nur den offen vorliegenden russischen Faschismus.[175]
Auch der russische Politologe Wladislaw Leonidowitsch Inosemzew hielt Putin für einen faschistischen Herrscher; Russland erfülle nun im 2022 „mustergültig den Katalog dessen, was Faschismus ausmacht“. Man könne Putin nur verstehen, wenn man davon ausginge, dass er weder Politiker noch Militär ist, sondern ein Geheimdienstler, dem Loyalität, Vertrauen und Netzwerke wichtiger sind als Institutionen. Beim KGB galt, wie in der organisierten Kriminalität, zu der Putin in seiner Leningrader Zeit enge Verbindung gehabt habe, ein „Kult von Macht und persönlicher Loyalität“. Die Kluft zwischen Putins Russland und dem demokratischen Westen sei um das Jahr 2006 entstanden, „als er feststellte, dass es in der atlantischen Welt keine Staatsoberhäupter gab, mit denen er von starkem Mann zu starkem Mann reden konnte“, der Westen aber „andererseits Russland Werte und Verfahren ‹aufzwingen› wollte, welche die Macht Putins selbst hätten vernichten können.“[176][177][178]
Auch der polnische Schriftsteller Szczepan Twardoch bezeichnete Russland 2022 als eine „faschistische Diktatur … mit allen Attributen einer solchen: staatlichem Nationalismus, fehlender Opposition, militärischer Indoktrinierung der Jugend schon im frühesten Alter, Arbeitslagern und Mord an politischen Gegnern.“[179] Der russische Dichter Dmitri Lwowitsch Bykow hatte schon Ende 2019 festgestellt, dass die russische Gesellschaft in der ihr innewohnenden Trägheit entlang der herrschenden Propaganda in „rasendsten Faschismus“ abgleite.[180]
Laut Jason Stanley verwendet Putin „antisemitische Schlüsselelemente einer weltweit vernetzten Rechten, die in Putin ihren Führer“ sehe; er sei „selbst ein faschistischer Autokrat, der demokratische Oppositionsführer und Kritiker inhaftiert“.[181]
Stefan Meister nannte das Regime um Putin „zunehmend faschistisch“ und ging davon aus, dass Angst die russische Gesellschaft vermehrt prägen werde.[182] Faschismus sei ein aufgeladenes Wort, fand Robert Gellately. Man könne natürlich Putin „in die eine oder andere Definition von Faschismus hineinzwängen“. Er würde ihn nicht als Faschisten sehen, sondern als jemand, der in einer Zeit voller Gewalt sozialisiert wurde; Gulag, Geheimpolizei, Repression; stets sei sowjetische Gewalt „extrem brutal“ gewesen. Irgendwann würden die Russen die Lügen erkennen und der Polizeistaat werde ihr Alibi für ihr Nichtwissen sein.[183] Ulrich Schmid sagt, der Gedanke des Faschismus liege zwar nahe, aber „das jetzige russische System einfach als faschistisch zu bezeichnen, ebnet wohl mehr ein, als dass wir Konturen erkennen können“.[184] Von einer starken Annäherung in Rhetorik und Denken sprach Lettlands Präsident Egils Levits.[185]
Die Definition von Robert Paxton, wonach „obsessive Beschäftigung mit dem Niedergang der eigenen Gemeinschaft, ihrer Demütigung oder Opferrolle sowie durch kompensatorische Kulte von Einheit, Stärke und Reinheit“ ein Ausdruck des Faschismus sei, treffe auf Russland zu, so der BR-Kulturredakteur Martin Zeyn.[186]
Die russische Währung ist der russische Rubel (Рубль; Kürzel RUB) zu 100 Kopeken (Копейка). Ein Euro entspricht gegenwärtig 117,2 Rubel. Nach starker Inflation in den 1990er-Jahren wurde im Jahr 1998 eine Währungsreform durchgeführt, bei der 1000 alte Rubel (RUR) durch je einen neuen Rubel (RUB) ersetzt wurden. Seitdem war der Rubel bis 2008 gegenüber US-Dollar und Euro im Wesentlichen stabil, die Inflation betrug 2006 8,2 %. Dazu hat bisher vor allem die Wechselkurspolitik der russischen Zentralbank beigetragen. Um eine rasche Aufwertung des Rubels mit einer Verschlechterung der preislichen Wettbewerbsfähigkeit russischer Produzenten zu verhindern, intervenierte sie am Devisenmarkt. Sie kaufte die Russland mit den hohen Leistungsbilanzüberschüssen zufließenden Devisen gegen Rubel auf. Die umlaufende Rubelgeldmenge stieg stark an. Das Inflationspotential wuchs. Im Zuge der Internationalen Wirtschaftskrise verlor der Rubel im zweiten Halbjahr 2008 rund 20 % seines Wertes gegenüber dem Euro.[187] Seit der Annexion der Krim verlor der Rubel mehr als die Hälfte seines Wertes gegenüber Euro, US-Dollar oder Renminbi.
Neben dem Rubel finden im Alltag auch US-Dollar und Euro Verwendung. Bis zum Januar 2007 wurden Preise auch oft in Verrechnungseinheiten angegeben, die je einem US-Dollar entsprachen. Da die Verwendung von Drittwährung in Russland nicht erlaubt ist, wurde dennoch in Rubel gezahlt. Diese Praxis ist aber seit Januar 2007 verboten. Wegen häufiger Bankeninsolvenzen und Finanzkrisen sind viele Russen dazu übergegangen, ihre Ersparnisse als Bargeld in Euro- und Dollar-Scheinen oder in Immobilien anzulegen.
Der Staatshaushalt umfasste 2016 Ausgaben von umgerechnet 236,6 Mrd. Dollar, dem standen Einnahmen von umgerechnet 186,5 Mrd. Dollar gegenüber. Damit hatte das Land ein Haushaltsdefizit in Höhe von 3,9 % des BIP.[188] Der Abschluss der Duma- und Präsidentenwahl gibt ab Mitte 2012 Anlass zu neuen umfangreichen Modernisierungsausgaben zugunsten der Infrastruktur, Wirtschaft und der Landesverteidigung. Angekündigt ist auch eine weitere Steigerung der Sozialausgaben. Somit werden die Ausgaben tendenziell weiter steigen, was aufgrund einer geringen Verschuldungsquote kein Problem darstellt. Die Staatsverschuldung betrug 2016 17,0 % des BIP.[188]
2006 betrug der Anteil der Staatsausgaben vom BIP folgender Bereiche:
Nach dem Ende der Sowjetunion ist Russland darum bemüht, seinen Einfluss in der Welt, aber insbesondere in seiner direkten Nachbarschaft zu konsolidieren. Hierbei verfolgt Russland die Idee einer multipolaren Weltordnung, in der Großmächte eigenverantwortlich ihre nationalen Interessen vertreten. Russland ist in eine Anzahl regionaler Konflikte verstrickt, von denen viele kriegerischen Charakter haben und nur teilweise oder noch gar nicht gelöst wurden – darunter die Tschetschenienkriege (1994 bis 2009), die Kriege um Abchasien und Südossetien (Georgienkrieg), der Konflikt in Transnistrien und zuletzt der russische Krieg in der Ukraine und die Annexion der Krim.
Im außenpolitischen Konzept sieht sich Russland als Großmacht, die selbstständig nationale Interessen verfolgt. Der Großmachtanspruch leitet sich in erster Linie aus Russlands imperialem Erbe und zweitens aus seinem bedeutenden Arsenal an Atomwaffen ab. Seinen Einfluss generiert Russland daneben über die militärischen Streitkräfte (derzeit ca. 1.000.000 Soldaten, Militärbasen in verschiedenen ehemaligen Sowjetrepubliken und in Syrien (Marinebasis Tartus)), Rüstungsexporte, die Vollmitgliedschaft mit Vetorecht im UN-Sicherheitsrat und die Stellung als bedeutender Energielieferant. Darüber hinaus bestehen jedoch enorme Schwierigkeiten, dem eigenen Anspruch gerecht zu werden. Dies rührt insbesondere aus der ökonomischen Schwäche her. Daneben verfügt es im Gegensatz zur Sowjetunion nicht mehr über ein attraktives Herrschafts- und Kultursystem. Die Möglichkeit, militärische Macht in politischen Einfluss umzuwandeln, ist auf Russlands unmittelbare Umgebung beschränkt. Es fehlt Russland an verlässlichen Verbündeten, wie die Nichtanerkennung Abchasiens und Südossetiens durch die restlichen GUS-Staaten zeigt.
Die politische Führung in Moskau drängt auf die Prärogative des UN-Sicherheitsrats. Ein Beispiel hierfür ist die Forderung, dass die NATO nur mit Zustimmung des UN-Sicherheitsrats tätig werden soll. Selbst besteht die Führung Russlands aber auf dem Recht, unilateral handeln zu dürfen, was das Verhalten im Georgienkrieg belegt. Um seinem Ziel näher zu kommen, sieht sich Russland nach Gegenpolen zu den USA um. Besonders Asien gewinnt dabei eine stetig wachsende Bedeutung. Die BRICS werden im außenpolitischen Konzept als strategische Partner betrachtet. Während Russland und Indien traditionell gute Beziehungen pflegen und diese weiter ausgebaut haben, hat sich das russisch-chinesische Verhältnis durch die Lösung alter Spannungen stetig verbessert. Abgesehen vom gemeinsamen Ziel, der weltpolitischen Dominanz des Westens etwas entgegenzusetzen, stehen vor allem Wirtschafts- und Rüstungsprojekte sowie russische Rohstofflieferungen im Vordergrund der Kooperation.[190][191][192][193]
Generell sieht sich Russland seit etwa 2004 durch die NATO-Osterweiterung und einen zunehmenden Einfluss der USA auf die eigene geostrategische Interessensphäre bedroht.[194] Dabei wird Russland vorgeworfen, destabilisierende Methoden zur außenpolitischen Einflussnahme einzusetzen. Dazu gehören beispielsweise Cyberangriffe, Beeinflussung von Wahlkämpfen und die Untergrabung von Beistandsverpflichtungen.[195]
Russland gewährte 2013 dem US-amerikanischen Whistleblower Edward Snowden eine Aufenthaltserlaubnis.
Russland ist ständiges Mitglied des UN-Sicherheitsrates, aller UN-Nebenorganisationen, der OSZE und zudem Mitglied der EBRD sowie des IWF und der Weltbank. Beim G8-Gipfel im Mai 1998 wurde Russland formal in die damalige Gruppe der Sieben (G7) aufgenommen; diese wurde dadurch zur G8. Im März 2014 schlossen diese Sieben Russland wegen des Krieges in der Ukraine wieder aus der G8 aus. Am 15. März 2022 kam Russland einem Ausschluss aus dem Europarat zuvor, indem es seinen Austritt ankündigte.[196]
Unter Putin gewannen zwei Sicherheitsorganisationen besonderes Gewicht – die Organisation des Vertrages über Kollektive Sicherheit (OVKS) und die Shanghai-Organisation für Zusammenarbeit (SOZ):
Die Auflösung der Sowjetunion stellte Russland zunächst vor die Aufgabe, das Verhältnis zu den aus Russlands Sicht oft als „Nahes Ausland“ (ближнее зарубежье) bezeichneten Nachfolgerepubliken neu zu gestalten. Die aus der Sowjetzeit geerbten wirtschaftlichen Beziehungen zwischen den einzelnen Republiken erforderten eine neue rechtliche Form der Kooperation und der Integration. Zugleich waren für Russland zahlreiche Objekte von strategischem Interesse, die nun außerhalb der Russischen Föderation lagen. Hierzu zählten u. a. der Weltraumbahnhof Baikonur, militärstrategische Einrichtungen in Aserbaidschan und Belarus sowie der Flottenstützpunkt der Schwarzmeerflotte in Sewastopol.
Zur Nachfolgeorganisation der Sowjetunion wurde die Gemeinschaft Unabhängiger Staaten (GUS), der zunächst 12 der 15 ehemaligen Sowjetrepubliken beitraten. Dieser eher lockere Staatenbund hat jedoch bis zur heutigen Zeit seine Bedeutung weitgehend eingebüßt. Mit Belarus hat sich Russland in der Russisch-Belarussischen Union zusammengeschlossen, auf die sich Boris Jelzin mit Aljaksandr Lukaschenka (belarussischer Staatspräsident seit 1994) verständigte. Nach Einschätzung von Politologen hing ihre Entwicklung jedoch stark mit persönlichen Ambitionen Lukaschenkas zusammen, der Nachfolger Jelzins in einem künftigen Unionsstaat zu werden. Als nach Jelzin 1999 Wladimir Putin russischer Präsident wurde, kühlte sich das Verhältnis zu Belarus ab, dem Putin einen Beitritt zur Russischen Föderation vorschlug. Bis 2011 verlief die weitere Integration sehr schleppend, viele Projekte wie die gemeinsame Währung wurden nicht umgesetzt. Die Beziehungen waren vielmehr von Energiekonflikten überschattet. 2011 trat Belarus jedoch der gemeinsamen Zollunion mit Russland und Kasachstan bei, die bereits seit 2000 im Rahmen der Eurasischen Wirtschaftsgemeinschaft in Planung war. Zu den weiteren Zielen dieser Gemeinschaft zählt ein gemeinsamer Wirtschaftsraum und die Schaffung einer politischen Union, die für weitere Staaten des postsowjetischen Raumes offensteht.
Russland hatte schon immer ein ambivalentes, ab der Annexion der Krim 2014 gar ein gespanntes Verhältnis zur Ukraine. Trotz enger historischer und kultureller Verbindungen und einer fortbestehenden wechselseitigen Abhängigkeit, besonders in Energiefragen, haben geschichtsbezogene Meinungsverschiedenheiten (vgl. Holodomor) sowie der erklärte Westkurs der Ukraine das Verhältnis schwer belastet. Vor allem westlich orientierte Regierungen der Ukraine wurden von Russland wiederholt unter Druck gesetzt, so zum Beispiel nach der Präsidentschaftswahl in der Ukraine 2004, als es zum russisch-ukrainischen Gasstreit kam. Nach der Abwahl des russlandfreundlichen Politikers Wiktor Janukowytsch und dem Euromaidan, bei dem sich die Demonstranten für eine Westorientierung der Ukraine aussprachen, kam es zur Annexion der Krim durch Russland und zum russisch-ukrainischen Krieg seit 2022, wobei sogenannte Separatisten ab 2014 für eine Autonomie des Donbass kämpften. Diese wurden durch Russland personell und militärisch unterstützt. Bereits im Jahr 2009 war in ukrainischen Medien offen über die Möglichkeit eines militärischen Angriffs durch Russland diskutiert worden.[198] In diesem Konflikt kam es auch zum Abschuss des Fluges MH17.
Im Februar 2022 startete Russland einen Angriffskrieg auf die gesamte Ukraine. Am 30. September 2022 annektierte Russland die Süd- und Ostukraine.
Als Antwort auf die Annexion der Krim 2014 wurden von der Europäischen Union Sanktionen gegen Russland ergriffen. Dabei geht es vorwiegend um bestimmte Ausrüstungen für die russische Öl- und Gasindustrie, zudem wird verschiedenen russischen Finanzinstituten der Zugang zum Finanzmarkt erschwert. Der Beschluss dieser Sanktionen erfolgt jeweils befristet für ein halbes Jahr (letztmals bis Januar 2019) und bedarf der Einstimmigkeit des Rates der Europäischen Union.[199]
Im Zuge des Angriffskriegs gegen die Ukraine verhängte die EU massive Sanktionen gegen Russland, infolgedessen kam es auch zu Gegenreaktionen seitens Russlands.
Deutsche waren die ersten „westlichen“ Europäer, mit denen Russland intensiver in Kontakt kam. Seit Mitte des 13. Jahrhunderts bestand der Peterhof in Nowgorod als Handelsniederlassung der Hanse. Zu militärischen Auseinandersetzungen kam es seit dem 12. Jahrhundert mit dem Schwertbrüderorden in Livland.
Die kulturellen Beziehungen zwischen Deutschen und Russen waren besonders eng unter Peter dem Großen. Russische Deutsche haben einen großen Beitrag zur Entwicklung der russischen Kultur geleistet, beispielsweise Kaiserin Katharina II., Admiral Adam Johann von Krusenstern, der Militäringenieur Graf Eduard Iwanowitsch Totleben, der Musiker Swjatoslaw Teofilowitsch Richter und viele andere. Der historische Beitrag Deutschlands wird daher bis heute in Russland anerkannt und geschätzt. Auch politisch blickten Deutschland und Russland bis zum Ende des 19. Jahrhunderts auf lange Bündnistraditionen zurück. Insbesondere das Königreich Preußen lehnte sich seit dem Ende des Siebenjährigen Krieges 1763 bis zur Deutschen Reichsgründung von 1871 eng an das russische Zarenreich an, da es zweimal in seiner Geschichte letztlich durch Russland vor der fast völligen Vernichtung bewahrt worden war – 1762 durch den Seitenwechsel Zar Peters III. im Siebenjährigen Krieg und 1807 durch die Fürsprache Zar Alexanders I. bei Napoleon im Frieden von Tilsit. Während der Befreiungskriege kämpften Russen und Deutsche gemeinsam gegen die französische Fremdherrschaft. So waren russische Soldaten maßgeblich an der Befreiung Deutschlands beteiligt. Die „Allianz der drei Schwarzen Adler“ – Russland, Österreich und Preußen –, die bereits in der ersten Hälfte des 18. Jahrhunderts bestanden hatte, setzte sich in der Folge nach dem Wiener Kongress als Heilige Allianz fort. Die schweren kriegerischen Auseinandersetzungen im 20. Jahrhundert haben bis heute Nachwirkungen. Die rechtliche Grundlage der Beziehungen des wiedervereinigten Deutschlands und der Russischen Föderation bilden der Vertrag über die abschließende Regelung in bezug auf Deutschland vom 12. September 1990, der Vertrag über gute Nachbarschaft, Partnerschaft und Zusammenarbeit vom 9. November 1990 sowie die Gemeinsame Erklärung des Präsidenten der Russischen Sozialistischen Föderativen Sowjetrepublik und des Bundeskanzlers der Bundesrepublik Deutschland vom 21. November 1991. Im Zeichen der friedlichen deutschen Wiedervereinigung war die deutsche Seite einerseits dankbar für die problemlose Abwicklung der Folgeauswirkungen, andererseits fühlte sich Deutschland als Impulsgeber und Motor für eine stärkere Integration Russlands in europäische Strukturen und warb für Kredite und Investitionen in Russland.[201] Ab der Kanzlerschaft Gerhard Schröders und dem Wirtschaftsaufschwung in Russland unter Wladimir Putin intensivierten sich die deutsch-russischen Beziehungen insbesondere im Bereich der Wirtschaft, aber auch beim politischen Dialog. Ab 1998 fanden jährlich bilaterale Regierungskonsultationen auf höchster Ebene unter Beteiligung beider Regierungen statt.
Es gab in Russland zwischenzeitlich mehr als 6.000 Unternehmen mit deutscher Beteiligung, einschließlich mehr als 1.350 russisch-deutscher Joint Ventures.
Zwischen Deutschland und Russland entwickelte sich ein enger kultureller und bildungspolitischer Austausch. 2003 wurde ein Regierungsabkommen zur Förderung des gegenseitigen Erlernens der Partnersprache abgeschlossen. Rund 12.000 junge russische Staatsbürger studierten an deutschen Hochschulen. Im April 2005 wurde eine gemeinsame Erklärung für eine strategische Partnerschaft auf dem Gebiet der Bildung, Forschung und Innovation unterzeichnet. Ab 2006 gab es Koordinierungsbüros in Hamburg und Moskau für den bilateralen Schüler- und Jugendaustausch. Das Goethe-Institut ist an vielen Orten in Russland präsent, in Moskau, St. Petersburg und seit Frühjahr 2009 in Nowosibirsk. Daneben sind zahlreiche weitere deutsche Kulturmittler in Russland vertreten.
Bundesaußenminister Frank-Walter Steinmeier stellte 2014 klar, dass eine im Jahre 2008 vorgeschlagene Modernisierungspartnerschaft aufgrund der formulierten Voraussetzungen von der russischen Seite ausgeschlagen worden war.[202]
Obwohl die Tendenz stieg, hatten 2011 trotz starker Wirtschaftsbeziehungen und eines bedeutenden Austausches zwischen den Zivilgesellschaften nur ein Drittel der Deutschen Russland als Partnerland vertraut. Dies lässt sich auf die Rolle der Medien zurückführen, die einen entscheidenden Einfluss bei der Wahrnehmung Russlands haben (vgl. Russlandberichterstattung in Deutschland). Bis zum Amtsantritt Wladimir Putins herrschte in den deutschen Medien das Bild eines „armen“ und „unberechenbaren“ Russlands vor. Durch die wirtschaftliche Stabilisierung nach der Jahrtausendwende und hohe Einkommen aus den Ölvorkommen verschwand dieses Bild allmählich. An seine Stelle rückte die Angst vor Putins Energie-Imperium und der Abhängigkeit von ihm. Die Berichterstattung der politischen Situation in Russland wurde durch die Stagnation der Medien und deren Personalabbau zuweilen als zu wenig differenziert wahrgenommen; Präsident Medwedew galt den Einen als „liberal“, den anderen als Präsident eines Landes, welchem ein Umsturz bevorstand.[203]
Die Abkühlung der russisch-deutschen Beziehungen begann schon im Herbst 2012, als der Bundestag eine Resolution mit Kritik an Russlands Innenpolitik verabschiedete.[204] Die Putin-Regierung betreibt seit Mai 2012 eine „nationalpatriotische und gegen westliche Einflüsse gerichtete Politik“.[205]
Im Februar 2014 kritisierte Russland die deutsche Rolle beim Euromaidan in der Ukraine. Im Verlauf der Ukraine- und Krimkrise zeigte sich, dass russische Geheimdienste zunehmend versuchen, mittels gezielter Infiltration sozialer Netzwerke wie Facebook sowie der Kommentarbereiche westlicher, auch deutscher Onlinemedien (betroffen sind etwa die Deutsche Welle und die Süddeutsche Zeitung), die öffentliche Meinung im Ausland zu Gunsten Russlands zu manipulieren. Wie die Süddeutsche berichtet, sind zu diesem Zweck hunderte bezahlte Manipulatoren im Einsatz.[206][207]
Unmittelbar nach der Annexion der Krim 2014 wurden in der Europäischen Union Wirtschaftssanktionen gegen Russland verhängt.[208] Als Folge brach der deutsch-russische Handel binnen Monaten um rund ein Drittel ein. Im Sommer 2017 wurden die Sanktionen wiederum verschärft.[209]
Im Februar 2020 warf der deutsche Außenminister Heiko Maas der russischen Regierung angesichts des russischen Militäreinsatzes im Rahmen des syrischen Bürgerkriegs vor, das humanitäre Völkerrecht gebrochen und Kriegsverbrechen im Gouvernement Idlib begangen zu haben.[210]
Der Syrienkonflikt ist einer der wenigen internationalen Konflikte, in denen die russische Regierung eine zentrale Rolle spielt. Dabei brachte ihre Verweigerungshaltung gegenüber jeglichen Versuchen, im Rahmen des UN-Sicherheitsrats internationalen Druck auf die Regierung Assad auszuüben, der russischen Regierung scharfe Kritik westlicher und regionaler Akteure ein und beschädigte das Ansehen Russlands in der arabischen Welt. Russland nahm von Anfang an die klare Haltung ein, dass die Kämpfe zwischen Regierung und Opposition nur innersyrisch zu lösen sei. Dies sei erstens durch ergebnisoffene Verhandlungen zwischen beiden Seiten zu erreichen und sollte zweitens ohne externe Einmischung geschehen, sei es durch Waffenlieferung an die Rebellen oder durch militärische Intervention. Deswegen blockierte Russland nicht nur Resolutionsentwürfe im UN-Sicherheitsrat, die Sanktionen vorgesehen hätten (Oktober 2011, Juli 2012), sondern auch solche, die lediglich die Gewaltanwendung durch die syrische Regierung verurteilt hätten, ohne dass zugleich die Regimegegner ebenfalls verurteilt und zum Gewaltverzicht aufgerufen würden (Februar 2012).[211]
Die Führung Russlands gibt vor, damit eine neutrale Haltung einzunehmen. Mehrmals betonten Präsident Putin, Außenminister Lawrow und Ministerpräsident Medwedew, dass ihr Land – im Gegensatz zu den westlichen Staaten oder den Golfmonarchien – nicht einseitig Partei ergreife.[211]
Jedoch unterstützt die russische Regierung die Regierung Assads auf vielfältige Weise. Erstens stützt man auf internationaler Bühne die Legitimationsstrategie der syrischen Führung. Durch eine Darstellung der Opposition primär als einer Gruppe von „Fanatikern“, Islamisten oder Terroristen wird die Schuld am Gewaltausbruch implizit ihr zugewiesen. Zweitens liefert Russland weiterhin Waffen an die syrische Regierung, darunter Luftabwehrsysteme (Buk-M2 [Nato-Code: SA-17 Grizzly] und Panzir-S1 [Nato-Code: SA-22 Greyhound]) und Helikopter. Russland verweist darauf, dass die Exporte nach internationalem Recht zulässig seien. Schließlich hat der UN-Sicherheitsrat – aufgrund russischer und chinesischer Weigerung – bislang kein Waffenembargo verhängen können. Als verlässlicher Exporteur – so die russische Rechtfertigung – sei die russische Regierung daher verpflichtet, bestehende Verträge zu erfüllen. „Neue Lieferungen“ seien aber suspendiert worden, erklärte Wjatscheslaw Dsirkaln vom Föderalen Dienst für Militärtechnische Zusammenarbeit im Juli 2012. Drittens hilft die russische Regierung der Regierung Assad auch, indem sie Banknoten für die syrische Regierung druckt.[211]
Die Motive der russischen Syrienpolitik gehen über materielle Interessen hinaus. Sie betreffen grundlegende Fragen der internationalen Ordnung und regionalen Machtbalance, aber auch konkrete sicherheitspolitische Risiken für Russland selbst. Der „arabische Frühling“ warf für die internationale Gemeinschaft erneut die Frage auf, wie mit dem Spannungsverhältnis zwischen staatlicher Souveränität und Schutzverantwortung („responsibility to protect“ – „R2P“) umzugehen ist. Es geht um konträre Ansichten zur Ausgestaltung der internationalen Ordnung und den Anspruch Russlands, diese mitzubestimmen. Die russische Regierung lehnt die „R2P“ nicht prinzipiell ab, will es aber an enge Grenzen gebunden wissen, ohne das Ziel eines „Regime Change“ von außen. Dahinter steht eine traditionelle Interpretation staatlicher Souveränität. Diese hat auch eine innenpolitische Begründung. Schließlich stellt eine Aufweichung des Nichteinmischungsgebots für die autoritäre Führung in Moskau auch aus Gründen des eigenen Machterhalts ein Gefahrenszenario dar.[212]
Nach den Giftgas-Angriffen von Ghuta und der Drohung der US-amerikanischen Regierung mit einem Militärschlag gelang es Russland, zwischen der US-amerikanischen und der syrischen Regierung zu vermitteln. Am 14. September 2013 wurde vereinbart, dass die syrische Regierung zunächst binnen einer Woche das gesamte Giftgasarsenal offenlegen und den UN-Inspektoren uneingeschränkten Zugang zu den Lagerstätten gewähren muss. Mitte November sollen die UN-Inspekteure die Arbeit aufnehmen. Die Chemiewaffen sollen außerhalb von Syrien vernichtet werden.[213] Am 16. September sprach sich Russland erneut gegen eine UN-Resolution aus, die eine Drohung im Falle einer Nichterfüllung der Vereinbarung gegen die syrische Regierung vorsah.[214]
Humanitäre Hilfe leistet Russland in dem Konflikt hingegen kaum,[215] so stellte die Regierung im Jahr 2015 für das UN-Hilfsprogramm zur Versorgung der rund 4 Millionen Syrer, die vor dem Krieg in die Nachbarländer geflohen sind, bislang einen Betrag von 300.000 US-Dollar zur Verfügung, was 0,02 % der für die Hilfsmaßnahmen veranschlagten Gesamtkosten deckt.[216] In Russland selbst halten sich Schätzungen zufolge zwischen 8000 und 12.000 syrische Flüchtlinge auf, viele davon illegal. Im Jahr 2015 wurde kein einziger Syrer in Russland offiziell als Flüchtling anerkannt, 482 Asylsuchende wurden geduldet.[217]
Durch den russischen Militäreinsatz sind bis Ende September 2019 laut der syrischen Beobachtungsstelle für Menschenrechte etwa 19.000 Menschen (davon ca. 8300 Zivilisten) ums Leben gekommen.[218] Insbesondere im Gouvernement Idlib sind durch die Offensiven der russischen und syrischen Streitkräfte hunderttausende Menschen zur Flucht genötigt worden. Auch hinterließ die Offensive einen immensen Schaden der lokalen Infrastruktur. So sind laut einem Bericht von Amnesty International zwischen Mai 2019 und Februar 2020 mindestens 18 Angriffe auf Krankenhäuser und Schulen in Syrien durch die russischen und syrischen Streitkräfte verübt worden.[219] In der Folge haben fünf Kliniken darum schließen müssen.[219] Im Juli 2020 blockierte die russische Regierung mit einem Veto im UN-Sicherheitsrat den Fortbestand eines Großteils der UN-Hilfslieferungen von medizinischen Gütern und Nahrungsmitteln nach Syrien,[220] sodass das UN-Hilfsprogramm für Syrien nur noch eingeschränkt fortgesetzt wurde.[221]
Durch die Präsenz der paramilitärischen Organisation Gruppe Wagner in mehreren afrikanischen Staaten (u. a. Angola, Guinea, Guinea-Bissau, Kongo, Libyen, Madagaskar, Mali, Mosambik, Simbabwe, Sudan, Zentralafrikanische Republik)[222][223][224][225] versucht der russische Staat, seinen Einfluss dort inoffiziell zu mehren.[226][227] So ist die Wagner-Gruppe eine Kriegspartei im seit 2014 bestehendem Bürgerkrieg in Libyen[228] und im Konflikt in Mali[229]. Auch durch den Auslandssender RT führt Russland in Afrika geschickte Desinformationskampagnen.[227][230]
Recherchen der European Investigative Collaboration ergaben, dass Russland vom Handel mit Konfliktdiamanten aus Afrika profitiert und das Schürfen von Diamanten aus Gebieten, aus denen offiziell keine Diamanten exportiert werden dürfen, durch die Gruppe Wagner überwachen lässt. Als Mitglied des Kimberley-Prozesses macht Russland von seinem Vetorecht Gebrauch, um Bemühungen zur Unterbindung des Handels mit Konfliktdiamanten zu untergraben.[225][223]
Nach dem Militärputsch in Myanmar 2021 durch die dortige Militärregierung hat Russland laut den Vereinten Nationen Militärtechnik im Wert von 406 Millionen Dollar an die Streitkräfte von Myanmar verkauft.[231]
Mit der Unterschrift Präsident Putins trat am 31. Dezember 2015 Ukas 683 und damit eine neue Militärdoktrin in Kraft, welche erstmals die USA sowie deren Alliierte, die NATO und die EU als Bedrohung für Russland und seine Nachbarn benannte.[232][233] Im März 2018 widmete Präsident Putin ein Drittel seiner Rede an die Nation der Präsentation angeblich unbesiegbarer Nuklearwaffen.[234]
In Russland gilt eine allgemeine Wehrpflicht für wehrfähige Männer ab 18 bis maximal 27 Jahren. 2007 wurde sie von 24 auf 18, 2008 dann auf 12 Monate verkürzt. Da die wehrpflichtigen Soldaten früher auch in Krisengebieten wie Tschetschenien eingesetzt wurden und es im Rahmen der Dedowschtschina nicht selten zu Misshandlungen von jungen Rekruten durch Vorgesetzte kommt, gibt es in der Bevölkerung, besonders durch die Mütter Wehrpflichtiger, immer wieder Kritik an der Wehrpflicht.
Im Jahr 2018 gab Russland 61,4 Mrd. Dollar für sein Militär aus. Es liegt damit im internationalen Vergleich hinter den Vereinigten Staaten mit 649 Mrd. Dollar, der Volksrepublik China mit 250 Mrd. Dollar, Saudi-Arabien mit 67,6 Mrd. Dollar, Indien mit 66,5 Mrd. Dollar und Frankreich mit 63,8 Mrd. Dollar auf Platz 6, gefolgt vom Vereinigten Königreich und Deutschland.[235] Die schon ab 2000 massiv gestiegenen[236] Rüstungsausgaben Russlands hatten sich noch von 2004 bis 2014 verdoppelt[237] und sollen ab 2014 rund ein Fünftel der gesamten Staatsausgaben betragen.[238]
Die Informationslage über die Zahlen zum militärischen Personal ist weitestgehend unklar. Bis zum Ukrainekrieg im Februar 2022 verfügten die Streitkräfte über ca. 850.000 Mann. Davon entfielen 300.000 Mann auf das Heer, 40.000 auf die Luftlandetruppen, 150.000 Mann auf die Marine, 160.000 Mann auf die Luftwaffe, 70.000 Mann auf die Strategischen Raketentruppen, 20.000 Mann auf die Spezialkräfte und 100.000 weitere Soldaten für Stabsaufgaben, Cybereinsätze, Unterstützung und Logistik.[239]
Zusätzlich verfügt die Nationalgarde über schätzungsweise 200.000 bis 250.000 Mann.
Der russische Staat besitzt den 1949 noch als Sowjetunion erlangten Status einer anerkannten Atommacht und verfügt mit 5977 Stück über das weltweit größte Arsenal an nuklearen Sprengköpfen, vor den Vereinigten Staaten mit 5428 (Stand: Januar 2022).[240]
Russland verfügte nach westlichen Informationen im Jahr 2021 über 6.255 Atomsprengköpfe,[241] wovon (Stand 2009) 4.830 operativ nutzbar waren.[242] Im Jahr 2015 wurden neue Raketen für die Nuklearstreitkräfte angekündigt.[243] Die „stationierten“ Atomsprengköpfe stiegen von 1.400 im Jahr 2013 auf 1.796 im Jahr 2016. Die Zahl der stationierten Sprengköpfe stieg damit aufgrund neu eingeflotteter U-Boote gegenüber dem Inkrafttreten des New-START-Abkommens im Jahr 2011.[244]
Es gibt in Russland eine Reihe von Spezialeinheiten (SpezNas), die dem Innenministerium (MWD) unterstellt sind. Die Streitkräfte des MWD umfassten im Jahre 2007 insgesamt 170.000 Mann. Ihr Oberbefehlshaber, ein Armeegeneral, ist gleichzeitig Stellvertreter des Innenministers. Die Inneren Truppen gliederten sich 2007 in fünf Divisionen (ODON), zehn Brigaden (OBRON) und eine Anzahl selbständiger Einheiten. Sie sind mit Schützenpanzern und eigener Artillerie ausgerüstet. Dem MWD unterstehen mit der Polizija (полиция) außerdem die regulären Polizeikräfte, die bis März 2011 als Miliz bezeichnet wurden. Diese sind z. B. für die Aufsicht über die Staatsstraßen zuständig. Daneben gibt es die rund 20.000 Mann der Polizei-Spezialeinheit OMON (ОМОН), die für Notfälle, Großlagen und den Schutz des Nukleararsenals zuständig sind. Dem MWD untersteht schließlich auch der russische Inlandsgeheimdienst, FSB. Dem Föderalen Sicherheitsdienst FSB wurden unter Präsident Putin die von Jelzin geschaffenen selbstständigen Sicherheitsdienste – die Grenztruppen Russlands – untergeordnet, die etwa 160.000 Mann ausmachen.[245]
In der Feuerwehr in Russland waren im Jahr 2019 landesweit 271.000 Berufs- und 956.600 freiwillige Feuerwehrleute organisiert, die in 18.322 Feuerwachen und Feuerwehrhäusern, in denen 22.735 Löschfahrzeuge und 1.326 Drehleitern bzw. Teleskopmasten bereitstehen, tätig sind.[246] Der Frauenanteil beträgt 14 %.[247] In den Jugendfeuerwehren sind 262.354 Kinder und Jugendliche organisiert.[248] Die russischen Feuerwehren wurden im selben Jahr zu 1.161.581 Einsätzen alarmiert, dabei waren 471.426 Brände zu löschen. Hierbei wurden 8.559 Tote von den Feuerwehren bei Bränden geborgen und 9.461 Verletzte gerettet.[249] Die Landesbrandaufsicht Федеральный государственный пожарный надзор, vertreten durch den staatlichen Brandinspektor (auch Chief State Inspector der Russischen Föderation für Brandüberwachung, russisch Российская Федерация по пожарному надзору), die dem Ministerium für Notsituationen МЧС роии unterstellt ist, repräsentiert die russischen Feuerwehren im Weltfeuerwehrverband CTIF.[250]
Russland ist ein entwickeltes Industrie- und Agrarland. Das Land ist zudem Gründungsmitglied der seit dem 1. Januar 2015 existierenden Eurasischen Wirtschaftsunion. Die führenden Industriebranchen sind Maschinenbau sowie die Eisen- und Nichteisenmetallverarbeitung. Gut entwickelt sind auch die chemische und petrolchemische Industrie sowie die Holz-, Leicht- und Nahrungsmittelindustrie.
Das russische Bruttoinlandsprodukt betrug im Jahr 2015 ca. 1192 Mrd. EUR. Das Bruttoinlandsprodukt pro Kopf betrug im selben Jahr 8137 Euro.[254] Der Dienstleistungssektor steuert 62,6 % zum Bruttoinlandsprodukt bei. Auf den industriellen Sekundärsektor entfallen rund 32,7 %, auf den Agrarsektor (Bauwirtschaft und Landwirtschaft) 4,7 %.[255] Die Weltbank schätzte, dass rund ein Viertel der gesamtwirtschaftlichen Produktion von der Rohstoffproduktion gestellt wird.
Laut einer Studie der Bank Credit Suisse beträgt der durchschnittliche Vermögensbesitz je erwachsene Person in Russland 16.773 US-Dollar. Im Median liegt er jedoch bei nur 3.919 US-Dollar (Weltdurchschnitt: 3.582 US-Dollar), was auf eine hohe Vermögensungleichheit hindeutet. Mehr als 70 % der russischen Bevölkerung besitzen weniger als 10.000 US-Dollar an Vermögen. Russland belegte Platz 19 in der Rangliste der Länder nach totalem Privatvermögen, einen Platz vor Indonesien und einen hinter Schweden. Russland war 2017 das Land mit der fünfthöchsten Anzahl an Milliardären (insgesamt 96). Die sogenannten Oligarchen im Land sind teilweise zum Symbol für korrupte Strukturen und Ungleichheit geworden.[256]
Die Gesamtzahl der Beschäftigten beträgt 73,5 Millionen (2006). 30 % der Erwerbstätigen arbeiteten 2005 in der Industrie. In der Landwirtschaft waren 10 %, im Dienstleistungsbereich 22 % und im öffentlichen Sektor nochmals 22 % aller Erwerbstätigen beschäftigt. Im Jahr 2013 sagte die russische Vize-Ministerpräsidentin Olga Golodez, nur 48 Millionen (statt 86 Millionen) Arbeitsfähige seien für die Regierung sichtbar,[257] je nach Schätzung macht die Schattenwirtschaft die Hälfte der Wirtschaftsleistung aus. Kleine und mittlere Betriebe leisteten ein Fünftel, wohingegen die staatlichen Konzerne 70 % beitrugen.[258] Auch aufgrund der minimalen Renten weiterhin arbeitstätige Rentner gehörten zum Heer der selbständig erwerbenden Kleinverdiener, die ihr Einkommen kaum je deklarierten: Die Steuermoral lag angesichts der bekannten korrupten Ausschweifungen der Politiker darnieder.[259]
Nach Jahren des Aufschwungs steckte die russische Wirtschaft um die Jahre 2015/16 in der Rezession. Nachdem das russische Bruttoinlandsprodukt im Jahr 2014 noch um 0,6 % gewachsen war[260], schrumpfte die russische Wirtschaft 2015 um 3,7 %.[254] Für das Jahr 2016 wurde offiziell ein Rückgang der Wirtschaftsleistung um 0,2 % vermeldet.[261] Als Hauptgründe für die Rezession wurden zumeist der sehr niedrige Ölpreis, der Verfall des Rubels sowie die westlichen Sanktionen im Zuge des Krieges in der Ukraine genannt. Allerdings werden der russischen Wirtschaft auch grundsätzliche strukturelle Probleme bescheinigt. Des Weiteren hatte Russland mit erhöhten Inflationsraten im Falle des Jahres 2015 von bis zu 15 % zu kämpfen.[262] Die Inflation fiel 2018 wieder auf um 3 %.[263] Im Global Competitiveness Index, der die Wettbewerbsfähigkeit eines Landes misst, belegt Russland Platz 38 von 137 Ländern (Stand 2017/18).[264] Im Index für wirtschaftliche Freiheit belegt das Land 2017 Platz 114 von 180 Ländern.[265]
Die gesamtwirtschaftliche Entwicklung Russlands nach der Auflösung der Sowjetunion war zunächst von einem drastischen Einbruch der Produktion geprägt. Dazu trug der Wegfall eingespielter Handelsbeziehungen im Verbund der Sowjetunion bei. Der Übergang von der Planwirtschaft zu einer marktwirtschaftlichen Ordnung war schwierig und gelang nur in Teilbereichen. Insgesamt verringerte sich das Bruttoinlandsprodukt um gut 40 %. Kurz nach Beginn der Asienkrise begann im Herbst 1997 die Russlandkrise. Am 17. August 1998 erklärte Russland den Staatsbankrott und musste die Dollarbindung des Rubel aufgeben. Die „Politik des Minimalstaates“ unter Jelzin führte dazu, dass die föderale Regierung nicht imstande war, Steuern einzutreiben und für Rechtssicherheit zu sorgen. Dies änderte sich unter der Präsidentschaft von Wladimir Putin ab dem Jahr 2000. Um die politische Kontrolle im Staat wieder zu erlangen, stärkte er den Staatsapparat auf Kosten des Einflusses der Oligarchen.
Putin führte in Russland bis 2008 eine staatlich geführte korporatistische Wirtschaft. Im Jahr 2007 führte er per Gesetz sechs Institutionen zur Bündelung von Staatsaktivitäten in strategisch wichtigen Bereichen ein, unter alleiniger Führung des Präsidenten. Darunter fallen die Nukleartechnik bei Rosatom, die Bank für Außenwirtschaft VEB, der Reformfonds für Immobilien[266], Rusnano oder das Rüstungsgüter-Konglomerat Rostec, dazu Olimpstroi, die 2014 aufgelöste Staatsgesellschaft für Bauten der Olympischen Spiele in Sotschi 2014.[267] Die VEB war aus der Außenhandelsbank der UdSSR hervorgegangen. An diesen durch Gesetz geschaffenen Staatskonglomeraten kritisierte unter anderem Ministerpräsident Medwedew die Verwendung von Staatseigentum oder Staatsmitteln zur Gründung, was zu einer versteckten Privatisierung führe.[268][269] Bei einer Prüfung der Korporationen im Jahr 2009 durch Medwedew wurden Missbrauch und Ineffizienz festgestellt.[270] Präsident Medwedew nannte in seiner Rede an die Nation im November 2009 die Organisationsform der Korporationen „ohne Perspektive“.[271]
Wenige Tage später erwiderte Ministerpräsident Putin, Staatskorporationen seien schlicht eine Notwendigkeit, und betonte, dass darüber in der Staatsführung Einigkeit herrsche.[272]
In den ersten vier Jahren von Putins Präsidentschaft folgte die Einführung einer Flatrate bei der Einkommensteuer (vgl. Steuerrecht (Russland)), der vollen Konvertibilität des Rubels und eines Drei-Jahres-Budgets (dies bis zu den Finanzproblemen im Jahr 2015[273]). Um von den Einnahmen des Energiesektors zu profitieren, wurden private Unternehmen aus diesem Bereich zurückgedrängt. Auch außerhalb des Energiesektors baute der Staat seinen Einfluss aus. Die Regierung förderte die Bildung staatlicher Großkonzerne, die strategische Branchen dominieren sollen. So wurden beispielsweise private Unternehmen für Maschinen- und Automobilbau von Staatsbetrieben übernommen und durch Subventionen gestützt, um modernisiert werden zu können.
Große Produktionskapazitäten aus der Zeit der UdSSR waren nicht ausgelastet, so dass sich die russische Regierung daran orientierte, durch eine nachfrageorientierte Wirtschaftspolitik mittels expansiver, wachstumsorientierter Geldpolitik diese Kapazitäten wieder voll auszulasten. Dies brachte eine zweistellige Inflationsrate mit sich. Das von Präsident Putin gesetzte Ziel, das Bruttoinlandsprodukt binnen zehn Jahren zu verdoppeln, sollte mittels staatlichem Ausgabenprogramm erreicht werden. Dafür wurden Gehälter im öffentlichen Dienst sowie Renten, sonstige Sozialleistungen und Ausgaben für den Wohnungsbau erhöht. Möglich wurde das Sozialprogramm durch den Ölboom, der neben hohen Mehreinnahmen für den Staat eine Reduzierung der Auslandsverschuldung ermöglichte, die 2000 noch 166 Mrd. Dollar betrug. Ein Teil der Öleinnahmen floss in den 2004 errichteten Stabilisierungsfonds, der sinkende Staatseinnahmen abfedern und eine mögliche Inflation abschwächen sollte. Dieser Stabilisierungsfonds wurde 2008 in einen Reservefonds und einen Wohlstandsfonds (zur Rentensicherung) aufgegliedert. Der Wohlstandsfonds betrug 2011 68,4 Mrd. Euro, der Reservefonds 19,9 Mrd. Euro.[274]
Die russische Wirtschaft hatte sich vom Produktionseinbruch im Zuge der Finanzkrise des Jahres 1998 rasch erholt, da die 1998 eingetretene deutliche Abwertung des Rubels der russischen Wirtschaft Auftrieb verschaffte und ausländische Güter verteuerte, so dass Produkte aus Russland dort wettbewerbsfähiger wurden. Außenwirtschaftlich verstärkte sich die Abhängigkeit der russischen Wirtschaft vom Energiesektor allerdings weiter. Trotz kräftig gestiegener Investitionen wurde in Russland im internationalen Vergleich zu wenig investiert. Investoren kritisierten fehlende Rechtssicherheit, weit verbreitete Korruption, eine überbordende Bürokratie und die geringe Leistungsfähigkeit des russischen Bankensystems.
Im Zuge der Internationalen Wirtschaftskrise wies die russische Wirtschaft seit Mitte 2008 deutlich negative Entwicklungen auf, was in hohem Maße auf ihre große Abhängigkeit vom Rohstoffsektor zurückzuführen war. Aufgrund des drastischen Preisverfalls beim Erdöl und Erdgas sanken die Staatseinnahmen. Die weltweite Finanzkrise hatte Russland 2009 hart getroffen. Russland konnte durch seine Antikrisenpolitik größere Bankenzusammenbrüche verhindern, so dass das russische Finanzsystem wieder als stabil gilt. Die Pflichteinlagen bei der Zentralbank wurden hochgeschraubt, Banken bekamen staatliche Hilfen. Die Russische Zentralbank verwendete fast 300 Milliarden Dollar an Reserven, um den als Folge des ausländischen Kapitalabzugs unter Abwertungsdruck gekommenen Rubel zu stützen. 2010 und 2011 setzte eine wirtschaftliche Erholung in Russland ein.
Durch diese Krise wurde sichtbar, dass die Fixierung auf den Rohstoffreichtum das Land in eine Sackgasse führt und die Abhängigkeit von den Weltmarktpreisen für Erdöl, Erdgas oder Metalle zu hoch ist. Bereits zu Beginn des 21. Jahrhunderts hatte in Russland eine intensive Diskussion über Sonderwirtschaftszonen eingesetzt. Unter Wladimir Putin wurde 2005 ein entsprechendes Gesetz über Sonderwirtschaftszonen in der Russischen Föderation verabschiedet. Bis Ende 2009 wurden 15 dieser Zonen konzipiert und bestätigt, darunter unter anderem zwei Industrie-Sonderwirtschaftszonen (Jelabuga, Lipezk), vier technikorientierte Sonderwirtschaftszonen (Moskau, St. Petersburg, Dubna, Tomsk) sowie sieben Zonen für Tourismus und Erholung.
Zinsen wurden gesenkt, um Investitionen in die Produktion zu ermöglichen. Die Inflationsrate erreichte 2011 ihren niedrigsten Stand seit 20 Jahren. Die Regierung war bemüht, preistreibende Faktoren wie die Verteuerung von Treibstoffen und Strom über Quartalsvereinbarungen mit den Anbietern unter Kontrolle zu halten.
Während das Land 1999 noch auf Platz 22 der größten Wirtschaftsnationen lag, hatte es 2012 den 9. Platz in der Welt nach nominalen BIP inne. Lag der Wert des russischen BIP in Relation zum deutschen im Jahr 2004 bei 21,7 %, waren es 2011 bereits 51,7 %. Der Beitritt zur Welthandelsorganisation (WTO) erfolgte 2012 nach 18 Verhandlungsjahren, wodurch die Importzölle sanken und der Modernisierungsdruck der heimischen Wirtschaft stieg. Im Jahr 2015 lag Russlands Wirtschaftsleistung wieder hinter der Italiens auf Rang 10 oder 11.[275] Die Regierung hatte es bis 2018 nie gewagt, das Renteneintrittsalter, welches Stalin im Jahr 1932 festgelegt hatte, zu erhöhen – die Renten, welche Frauen ab 55 Jahren, Männer ab 60 Jahren erhalten, sind jedoch so niedrig, dass sich viele in der Schattenwirtschaft Geld dazuverdienen. Gleichzeitig fehlten dem Arbeitsmarkt Arbeitskräfte.[36]
Durch die Sanktionen des Westens aufgrund der russischen Annexion der Krim sowie des von Russland gefütterten Krieges in der Ukraine seit 2014 stagnierte die wirtschaftliche Entwicklung in Verbindung mit einem Einbruch des Erdölpreises. Es akzentuierten sich die strukturellen Probleme der russischen Wirtschaft, die über Jahre auf den Rohstoffexport ausgerichtet war. Die NZZ schrieb im August 2015 in einem Vergleich mit der Rubelkrise von 1997: „Heute ist die Lage weniger bedrohlich, aber die Besserungschancen sind geringer“;[276] so konnte die Rubelschwäche wegen der Finanzrestriktionen nicht dazu genutzt werden, die Wirtschaft zu modernisieren und zu diversifizieren.[277] Das russische Haushaltseinkommen 2015 sank durchschnittlich um 8,5 %, während die Lebensmittelpreise bis 25 % anstiegen. Die Jahresinflation 2015 betrug 12,9 %.[278][279][280] Eine Kapital-Amnestie sollte ab Dezember 2014 Geld nach Russland zurückbringen. Während bei Präsidentensprecher Peskow bei der Einführung von einem absolut einmaligen, für ein Jahr gültigen Angebot die Rede war, wurde die Amnestie im Dezember 2015 bis Juni 2016 verlängert und Anfang 2018 nach neuen amerikanischen Sanktionen erneuert.[281][282][283]
Alle staatlichen Ausgaben mussten gekürzt werden, nur die Rüstung war nicht davon betroffen.[275] Der russische Ministerpräsident Medwedew hatte wiederholt erklärt, das Land werde „unbefristet“ mit den westlichen Sanktionen leben müssen.[284] Die Wirtschaftsentwicklung blieb gelähmt, weil die Techniken des Machterhalts des Putin-Regimes nicht nur politische, sondern auch wirtschaftliche Reformen verhinderten. Der Anteil der Staatswirtschaft stieg an, die Schattenwirtschaft blühte, die Realeinkommen waren zwischen 2014 und 2018 mehrmals gesunken.[285] Ein Steuersatz von 0 % für die Jahre 2017/2018 hätte Selbständigerwerbende zur Registrierung ihrer Tätigkeit animieren sollen; von den vermutlich rund neun Millionen derart Werktätigen hatten sich gerade mal 936 registrieren lassen. Nach einem erneuten Gesetzesvorschlag von 2018 sollte diesen Kleinverdienern beim Auffliegen der Tätigkeit der gesamte Ertrag abgenommen werden, also eine härtere Strafe, als sie Gutverdienende zu befürchten hätten.[286] Eine Geschäftseröffnung war für die Mehrzahl befragter Russen im Februar 2019 nicht erstrebenswert, da es nicht möglich sei, ohne Mogeleien zu wirtschaften.[287] Die ausländischen Direktinvestitionen, die 2013 noch 69 Milliarden Dollar umfasst hatten, waren laut Le Monde bis 2018 auf weit unter 5 Milliarden gefallen.[288]
Im Juli 2018 wurde entschieden, die Mehrwertsteuer um 2 % zu erhöhen, womit sie ab 1. Januar 2019 20 % betrug.[289][290]
Mehrere Wochen demonstrierten die Menschen in ganz Russland im Sommer 2018 gegen die Erhöhung des Rentenalters.[291] Die Zustimmungsraten Putins stürzten ab wie 2012,[292][293] das übliche System „schlechte Bojaren, guter Zar“[294] funktionierte also nicht. Zwar könne Putins Beliebtheit dank der umfassenden Propaganda kaum unter 60 % fallen, aber die große Mehrheit der Befragten sei doch überzeugt, dass Putin für den Machtmissbrauch verantwortlich sei, den die Opposition den Regierenden vorwirft; die Erhebungen des Lewada-Zentrums unterschieden in „Zustimmung“ zur Politik und in „Vertrauen“.[295]
Nachdem schon im Vorkriegsjahr 2021 die Preise aufgrund der Kartellisierung der Wirtschaft wieder merklich gestiegen waren, wurden nach dem russischen Angriff auf die Ukraine 2022 durch die freie Welt beispiellose Sanktionen gegen Russland verhängt.[296] Dadurch könnte der Dienstleistungssektor sich wieder verkleinern; der Staat kontrollierte Anfang 2022 bereits 60 bis 75 % der Wirtschaft direkt oder indirekt.[297]
Die Holzindustrie ist hauptsächlich im Nordwesten des europäischen Teiles, im zentralen Uralgebirge, in Südsibirien und im Süden des fernöstlichen Russlands vertreten. Russland verfügt über etwa ein Fünftel des Waldbestandes der Erde und über rund ein Drittel des Weltbestandes an Nadelwald; der größte Teil der russischen Nutzholzproduktion besteht aus Weichholz, hauptsächlich von Kiefern, Tannen und Lärchen. Wichtigstes Laubholz für den Handel ist Birke.
Die Landwirtschaft ist nach wie vor eine wichtige Branche der russischen Wirtschaft. Einst die Kornkammer Europas, erlitt die russische Landwirtschaft in den 1990er-Jahren einen drastischen Einbruch der Agrarproduktion – jedoch schon in den 1980er-Jahren war Russland der weltweit bedeutendste Weizenexporteur. Der Produktionswert der russischen Landwirtschaft lag 2009 wieder bei umgerechnet 38 Milliarden Euro. Im Jahr 2016 unterstrich Präsident Putin den Willen, eine Agrar-Exportnation zu sein.[298] Von der Rekordernte von 75 Millionen Tonnen Weizen im Jahr 2016 könnten knapp 7 Millionen Tonnen (ähnlich wie 2015) exportiert werden. Für den Transport ist die staatliche Agrar-Transportbehörde Rusagrotrans zuständig.[299] Der Wert der exportierten Landwirtschaftsgüter lag 2016 bei 17 Milliarden Dollar.[300] Die Bedingungen für die Landwirtschaft sind vor allem im europäischen Teil Russlands sowie in Südrussland gut, das russische Schwarzerdegebiet ist das größte der Welt. Die landwirtschaftliche Nutzfläche beträgt 219 Millionen Hektar, das sind 13 % der Landfläche Russlands. Davon sind 122 Millionen Hektar Ackerfläche, was 9 % des weltweiten Ackerlandes entspricht.[301] Mehr als 80 % der Saatflächen liegen an der Wolga, im Nordkaukasus, am Ural und in Westsibirien innerhalb des sogenannten Agrardreiecks. Der Ackerbau macht 36 % der landwirtschaftlichen Bruttoerzeugung Russlands aus, die Tierzucht über 60 %. Die wichtigsten landwirtschaftlichen Erzeugnisse in Russland sind Getreide, Zuckerrüben, Sonnenblumen, Kartoffeln und Flachs. Die Binnenfischerei liefert mit dem Stör den begehrten russischen Kaviar. In der Transformationsphase zwischen 1990 und 1997 gingen die Schweine- und Geflügelbestände fast um die Hälfte zurück. Russland importierte seitdem einen Teil seiner Nahrungsmittel. Es war schon zuvor, aber insbesondere seit seinen Gegen-Sanktionen gegen den Westen nach der Annexion der Krim im Jahr 2014 das Ziel der russischen Regierung, die Fähigkeit zur Eigenversorgung zu steigern und die Importabhängigkeit zu reduzieren.[302] Der Bestand an Rindern beträgt 12,1 Millionen Tiere, an Schweinen 7 Mio. sowie an Schafen und Ziegen 4,6 Mio. Rinderzucht wird vorwiegend im Wolgagebiet, in Westsibirien und dem europäischen Zentrum betrieben, Schweinezucht findet sich ebenfalls im Wolgagebiet, aber auch in Nordkaukasien und im zentralen Schwarzerdegebiet. Schafzucht weist Schwerpunkte in den Regionen Ostsibirien, Nordkaukasiens und dem Wolgagebiet auf.
Die Naturreichtümer Russlands sind eine wichtige Basis für die Wirtschaft des Landes. In Russland befinden sich 16 % aller mineralischen Naturressourcen der Welt, davon 32 % aller Erdgasvorräte (erster Platz in der Welt), 12 % aller Vorräte an Erdöl, die sich insbesondere in Westsibirien, auf der Insel Sachalin, in Nordkaukasien, der Republik Komi und den Erdölgebieten im Wolga-Ural-Bereich (Kaspische Senke) finden. Mit der kräftigen Zunahme der Ölexporte bei steigenden Ölpreisen von 2002 bis 2011 war die Bedeutung der Förderung besonders von Öl und Gas in Russland angewachsen und spielte eine wichtige Rolle für die Wirtschaft auch außerhalb Russlands. Russische Unternehmen wie Gazprom, Rosneft oder Lukoil sind an der Erdöl- und Erdgasförderung beteiligt, die hauptsächlich in den nördlichen und östlichen Landesteilen stattfindet.
Mit seinen Goldvorräten belegt Russland den dritten Platz in der Welt. Weltbekannt sind die Diamantenvorkommen im nordostsibirischen Jakutien. Seit 1996 werden hier Diamanten in einer der weltweit größten Kimberlit-Lagerstätten, in Mirny, gewonnen.
Russlands Anteil an den Weltvorräten an Eisen und Zinn beträgt über 27 %, an Nickel 36 %, an Kupfer 11 %, an Kobalt 20 %, an Blei 12 %, an Zink 16 % und an Metallen der Platingruppe 40 %. 50 % der weltweit bekannten Kohlevorkommen finden sich in Russland.[303] Entsprechend den mineralischen Vorkommen spielt die Steinkohle- und Eisenerzförderung eine sehr wichtige Rolle in der Wirtschaft Russlands. Größere Erzvorkommen finden sich vor allem in den altgefalteten Gebirgen (Chibinen auf der Kola-Halbinsel, Ural, Altai, Sajangebirge sowie andere sibirische Gebirgszüge). Lagerstätten von Steinkohle finden sich in einigen Vorsenken dieser Gebirge, vor allem am Ural (u. a. Kohlelagerstätten von Workuta) sowie im Donezbecken an der Grenze zur Ukraine. Die Kohlenförderung litt an fehlenden Investitionen und hat im Vergleich zur Sowjetzeit an Bedeutung verloren.
Mit Öl, Erdgas oder Kohle betriebene Wärmekraftwerke erzeugten 2003 rund 63 % der gesamten Stromproduktion von rund 851 Mrd. Kilowattstunden. Auf Wasserkraftwerke entfielen 21 %, auf Kernkraftwerke 16  %. Die russische Regierung plant, den Anteil der Kernenergie an der Stromerzeugung bis 2020 auf etwa ein Drittel zu verdoppeln, um noch mehr Erdöl und Erdgas exportieren zu können. Das Stromnetz und die meisten Großkraftwerke sind nach wie vor unter staatlicher Kontrolle. Um von den Einnahmen des Energiesektors zu profitieren, war die russische Politik darauf ausgerichtet, die staatliche Kontrolle über die Energiewirtschaft wieder zu verstärken und private Unternehmen aus diesem Bereich zurückzudrängen. Das wurde durch die Zerschlagung des Erdölkonzerns Yukos und die Übernahme des Ölkonzerns Sibneft durch die halbstaatliche Erdgasgesellschaft Gazprom erreicht. Zu den größten Gas- und Ölförderungskonzernen gehört heute Surgutneftegas, wo Präsident Wladimir Putin 37 % der Aktien kontrolliert.[304] Alle russischen Kernkraftwerke sind Eigentum des staatlichen Unternehmens Rosatom und werden vom ebenfalls staatlichen Unternehmen Rosenergoatom betrieben. Den größten Anteil an der Stromproduktion hatte bis zum Jahr 2008 Unified Energy System, das zu über 50 % dem russischen Staat gehörte und inzwischen in kleinere Unternehmen aufgeteilt wurde.
Das Programm der Gasversorgung der russischen Regionen läuft seit 2005. Es war geplant, in zehn Jahren, also bis 2015, Gas in jedes Dorf zu führen. 2019 wurde das Programm bis 2030 verlängert. Das neue Ziel ist nicht mehr alle, sondern 85 % der Siedlungen des Landes zu versorgen. In Russland wurden auch im Jahr 2022 noch Schulen und Krankenhäuser mit Brennholz beheizt, das laut Nowaja Gaseta teurer sei als in Deutschland.[305]
Neben den alten Industriegebieten Moskau, Nischni Nowgorod, Sankt Petersburg, Saratow, Rostow und Wolgograd sind seit dem Zweiten Weltkrieg weitere Industriestandorte vorzugsweise im asiatischen Teil des Landes entstanden. Die Schwerindustrie konzentriert sich im Ural um Jekaterinburg. Russland nimmt eine führende Rolle in der weltweiten Produktion von Stahl und Aluminium ein. In den letzten Jahren haben sich in Russland weltbekannte Stahlkonzerne mit hoher Finanzkraft gebildet. Dies sind zum Beispiel Evraz, Severstal, Magnitogorsk Iron and Steel Works und Novolipetsk Steel, die zu den weltweit 30 größten Stahlkonzernen gehören. Wichtige Zentren der Schwerindustrie sind Magnitogorsk, Tscheljabinsk, Nischni Tagil, Nowokusnezk, Tscherepowez und Lipetsk.
An den alten Hauptindustriestandorten Moskau, dem Wolgagebiet, dem Nordwesten und dem Ural produzieren zahlreiche Maschinen- und Fahrzeugindustrien, aber auch Geräte- und Anlagenbauherstellung ist hier angesiedelt. Mehrere Zweige des Verarbeitenden Gewerbes wie Maschinenbau, Autoindustrie und Rüstungsindustrie einschließlich Luftfahrtindustrie fielen nach dem Ende der Sowjetunion in eine tiefe Krise. Die Produktion ging stark zurück. In den 2000er-Jahren ging es aber auch in der verarbeitenden Industrie wieder bergauf. Vor allem auf Märkten in der GUS konnten Marktanteile zurückgewonnen und neue Märkte in Asien gefunden werden, weil sich einige russische Erzeugnisse als einfacher und preiswerter als westliche Konkurrenzprodukte profilieren konnten. Die Inlandsproduktion von Maschinen und Ausrüstungen erreichte 2006 ein Volumen von rund 63 Milliarden Euro.[308] Um die notwendige Modernisierung im Maschinenbau zu forcieren, steuert der Staat die weitere Entwicklung des Maschinenbaus von oben. Dazu gehörte die Gründung der Staatsholding Rostechnologii, in die Staatsanteile von fast 500 Unternehmen (Rüstungsbetriebe, Fluggesellschaften, Lkw- und Waggonhersteller und Maschinenbauer) eingebracht wurden.
Der Flugzeugbau war eine der wichtigsten und technisch am höchsten entwickelten Branchen der russischen Industrie. Nach dem Zerfall der Sowjetunion wurden die Produktionsketten zwischen den ehemaligen Unionsrepubliken unterbrochen. Das hatte tiefgreifende negative Auswirkungen auf den russischen Flugzeugbau. Die wichtigsten Entwickler und Produzenten von Flugzeugen in Russland wurden 2006 in der OAK zusammengefasst. 2010 lieferte die OAK 75 Flugzeuge aus bei einem Erlös von vier Milliarden US-Dollar.[309] Die bekanntesten russischen Autohersteller sind AwtoWAS, KAMAZ, Ischmasch oder die GAZ-Gruppe. Sehr oft sieht man noch die in Russland hergestellten Automarken Schiguli, Moskwitsch, Lada Niva und Oka sowie die Lkw KAMAZ, Ural und andere. Inzwischen kooperieren die russischen Autohersteller mit ausländischen Konzernen. Aktuell arbeiten die Volkswagen Group Rus mit GAZ, Ford mit Sollers, Renault-Nissan und AwtoWAZ sowie General Motors (GM) mit Avtotor zusammen. Dadurch entstanden und entstehen derzeit neue Montagewerke in Kaluga, Nischni Nowgorod, Togliatti, St. Petersburg und Kaliningrad.[310] Koordiniert wird Russlands Rüstungsindustrie vom staatlichen Rüstungsexporteur Rosoboronexport. Rosoboronexport koordiniert die Arbeit der verschiedenen Rüstungsunternehmen und schließt diese über Beteiligungen zu einem Konzern zusammen.
Die chemische Industrie Russlands ist eine der Hauptbranchen der Volkswirtschaft Russlands, deren Anteil am Umfang der Warenproduktion 6 % erreicht. Der chemische Komplex Russlands schließt 15 große Industriegruppen ein, die sich auf den Ausstoß einer vielfältigen Produktion spezialisiert haben.[311] Die führenden Unternehmen in diesem Bereich sind die hochrentablen, erdölverarbeitenden Unternehmen und Produzenten von chemischen Düngemitteln. Darüber hinaus sind in Russland die Herstellung von Chemiefasern, Kunststoffen und Autoreifen stark entwickelt. Die Wirtschaft Russlands wird auch durch die Herstellung von Baustoffen, die Leichtindustrie (hauptsächlich Textilindustrie) und die Nahrungsmittelindustrie geprägt.
Zu den führenden lokalen Einzelhandelsketten gehören mit großem Abstand die X5 Retail Group (zu dem u. a. die Ketten Pjatjorotschka und Perekrjostok gehören), Magnit, bei den internationalen Ketten führen die Metro Group und Auchan. Den Bankenmarkt dominieren Staatsinstitute wie Sberbank, WTB, Rosselchosbank und Wneschekonombank. Allein die Sberbank, die frühere Werktätigensparkasse der Sowjetunion, hält etwa die Hälfte aller Spareinlagen. Über ein landesweites Filialnetz verfügt nur die Sberbank. Der Anteil der staatlich kontrollierten Banken am Gesamtmarkt beträgt im Schnitt etwa 50 %. Die größten russischen Privatbanken (Gazprombank, Alfa Group, MDM Bank, Rosbank) sind Teil von Industrieholdings und nehmen hauptsächlich Aufgaben im Rahmen der Holding wahr.[312]
Von der Lieferstruktur her wichtigster Handelspartner Russlands ist Deutschland, das vor allem industrielle Fertigerzeugnisse wie Maschinen, Anlagen und Spitzentechnik nach Russland liefert. Russland ist im Gegenzug Deutschlands größter Rohöllieferant und deckt rund ein Drittel des deutschen Erdgasbedarfs. Der deutsch-russische Handel stieg 2018 um 8,4 % auf 61,9 Mrd. Euro. Die deutschen Importe aus Russland legten im Vorjahresvergleich um 14,7 % zu und betrugen rund 36 Mrd. Euro. Auch die Exporte nach Russland sind um 0,6 % auf 25,9 Mrd. Euro gestiegen.[313] Die Volksrepublik China hat 2010 Deutschland als wichtigsten Außenhandelspartner abgelöst, ebenfalls von Bedeutung für Russland sind die Niederlande, Ukraine, Italien, Belarus und die Türkei. Schon heute ist Russland weltweit zweitgrößter Exporteur von Rohöl und weltweit größter Exporteur von Erdgas. Der Export von Energieträgern und Elektrizität hat einen Anteil von 62,8 % an den Gesamtausfuhren (Metalle, Metallprodukte: 9,9 %, Chemikalien: 4,1 %).[255] Russlands Anteil am weltweiten Warenhandel ist trotz seiner bedeutenden Stellung als Rohstofflieferant jedoch vergleichsweise gering. Er beträgt 2 %, knapp ein Drittel des Anteils Deutschlands.
Russlands Warenaustausch mit dem Ausland war 2019 rückläufig. Auf US-Dollarbasis sank der Handelsumsatz im Vergleich zum Vorjahr um 3,1 %, er belief sich auf umgerechnet rund 595 Milliarden Euro. Die Einfuhren von Waren und Dienstleistungen legten um 2,2 % zu, die Ausfuhren gingen hingegen um 6 % zurück. Erstmals seit zehn Jahren bremste der Export somit das BIP-Wachstum.[314]
Das Land verfügt über sehenswerte Naturlandschaften, darunter UNESCO-Weltnaturerbe, sowie Sehenswürdigkeiten von hohem kulturellen Wert. 2010 besuchten 2,4 Millionen ausländische Touristen Russland, wohingegen 13,1 Mio. Russen zur Erholung ins Ausland reisten.[315] Der Binnentourismus brachte es auf 29,1 Mio. Reisende. Obwohl der Touristenstrom aus Asien und Südamerika zunimmt, machen Gäste aus Europa – mit Deutschland an der Spitze – den Großteil der Besucher in Russland aus. So waren auch die Einreisezahlen von Urlaubs- und Geschäftsreisenden kontinuierlich gestiegen; waren es 2002 rund 360.000 Deutsche, die das Land bereisten, so kamen 2008 558.000 deutsche Besucher. Allerdings waren davon nur 66.000 Urlaubsreisen Deutscher und der Rest Geschäftsreisen sowie Familien- und Freundschaftsbesuche.[316] 2017 besuchten 580.000 Deutsche die Russische Föderation.[317] Individualtouristen wurden häufig durch Visa-Beschaffung und sprachliche Hürden abgeschreckt, während das Land bei Reisegruppen beliebter ist.
Touristen waren lange durch ein unattraktives Markenimage abgeschreckt,[318] wonach „Russland ein unbehagliches Land“ und „nicht bereit dazu sei, Touristen aufzunehmen. Dass die Menschen dort unfreundlich seien und dass überall rundherum die Gefahr lauere“, meinte Alexander Radkow, Chef der staatlichen Tourismusagentur Rostourismus, im Jahr 2012.[319] Trotz vermehrter Aktivitäten durch die Föderale Tourismusagentur fehlt bislang eine wirksame PR- und Marketingstrategie, die das schlechte Image des Landes im Westen, verursacht u. a. durch mediale Berichterstattung,[203][320] welche vor allem Nachrichten über Anschläge, Korruption und Unfreiheit enthält, beeinflussen könnte.[321]
Der Tourismus in Russland konzentriert sich vor allem auf die beiden Metropolen Moskau und Sankt Petersburg. Sankt Petersburg gilt als Venedig des Nordens und besitzt ein reiches kulturelles Angebot und eine historische Innenstadt, die vollständig UNESCO-Weltkulturerbe ist. Typisch für St. Petersburg sind die Weißen Nächte mit den hochgeklappten Newa-Brücken von Ende Mai bis Mitte Juli. Darüber hinaus werden Schifffahrten auf der Wolga sowie Besichtigungen von altrussischen Städten nordöstlich von Moskau, dem sogenannten Goldenen Ring mit mehr als 20 Städten, angeboten. Natururlaub ist vor allem in Karelien und dem Altai-Gebirge (Weltnaturerbe) möglich. Die Transsibirische Eisenbahn (Transsib) führt auf rund 9300 km von Moskau über Jekaterinburg, Nowosibirsk, die Hauptstadt Sibiriens, Irkutsk, das auch „Paris“ Sibiriens genannt wird, sowie die Region um den Baikalsee, ebenfalls ein UNESCO-Weltnaturerbe, bis nach Wladiwostok. Die Transsib wird sowohl von Individualtouristen in den Regelzügen der russischen Eisenbahn befahren als auch von Gruppenreisenden, die Fahrten in Sonderzügen buchen.
Auch Kaliningrad, das frühere Königsberg, zieht immer mehr deutsche Besucher an. Die Kurische Nehrung, eine schmale Landzunge, 2000 zum UNESCO-Weltkulturerbe erklärt, liegt teils in der Oblast Kaliningrad, teils in Litauen.
Im innerrussischen Fremdenverkehr sind die Badeorte der Schwarzmeerküste sowie eine Reihe von nordkaukasischen Thermalquellen-Kurorten wie Kislowodsk oder Pjatigorsk von Bedeutung. 400 km liegen zwischen dem nördlichsten und dem südlichsten Punkt der russischen Schwarzmeerküste. Auf diesem relativ kleinen Küstenabschnitt, der auf dem gleichen Breitengrad gelegen ist wie die Badeorte der Adria und der italienischen und französischen Mittelmeerküste, konzentriert sich innerhalb der Saison von Mai bis Oktober der Großteil des Seebadbetriebes Russlands.
Zunehmender Beliebtheit erfreut sich der Skitourismus im Nordkaukasus. Vor allem für die Olympischen Winterspiele 2014 in Sotschi wurde die entsprechende Infrastruktur ausgebaut.
Mit einer Größe von 17.075.400 km² liegt das besondere Augenmerk des Landes auf einer möglichst breit gefächerten und funktionierenden Infrastruktur. Nach der politischen Wende Russlands hatte sich das Verkehrsaufkommen zunächst aufgrund des Wirtschaftsabbaus überwiegend reduziert, erlebte dann aber ein starkes Wachstum. Die derzeitige Infrastruktur stammt noch zu einem größeren Teil aus den Zeiten der Sowjetunion und ist inzwischen modernisierungsbedürftig, und die bestehenden Verkehrssysteme erzeugen kaum Netzwerkeffekte. Die Erweiterung und Modernisierung der Transport-Infrastruktur besitzt für die russische Regierung daher hohe Priorität. 2005 beschloss die Regierung eine Strategie zur Erneuerung der Verkehrswege, mit Schwerpunkt auf fortgesetzten Modernisierungen und Verbesserungen im Schienen-, Straßen- und Luftverkehr sowie der Sanierung der Häfen des Landes. Zudem sollen Konzessionen und andere öffentlich-private Partnerschaftsmodelle im Transportsektor forciert werden, um auch in diesem Sektor Finanzierungsmittel privater Investoren zu mobilisieren.
Trotz schwieriger Bedingungen will sich Russland programmatisch als ein wichtiges Drehkreuz im Asien-Europa-Verkehr und zum Teil auch auf der Nord-Süd-Achse von Nordeuropa Richtung Indien etablieren. Die Logistikinfrastruktur soll dazu vor allem an den Knotenpunkten Moskau und Sankt Petersburg ausgebaut werden.
Während die Verkehrsinfrastruktur Russlands westlich des Urals insgesamt gut ausgebaut ist, ist die Infrastruktur von Straßen- und Eisenbahnen im Trans-Ural und in Sibirien technisch bestenfalls veraltet und nicht wettbewerbsfähig. Größtes verkehrstechnisches Hindernis zur wirtschaftlichen Anbindung der riesigen Territorien Sibiriens an die boomenden süd- und südostasiatischen Staaten sind fehlende Verkehrswege in Nord-Süd-Richtung.[322] Demzufolge vereinbarten Wladimir Putin und Xi Jinping 2015, die respektive von Russland und China initiierte Eurasische Wirtschaftsunion und die Silk Road Belt Initiative in ein Projekt, die Central Eurasia Initiative, zu integrieren. Darin soll eine logistische Strategie zu einem neuen Transportgerüst für Sibirien und den Fernen Osten Russlands ausgearbeitet werden.
Im Logistics Performance Index, der von der Weltbank erstellt wird und die Qualität der Infrastruktur misst, belegte Russland 2018 den 75. Platz unter 160 Ländern.[323]
Seit 2000 ist in Russland der Trend zur Straße deutlich zu erkennen. Die Straßendichte ist mit 40 m Straße pro km² sehr gering. Dies ist unter anderem auf die in großen Teilen des Landes sehr geringe Bevölkerungsdichte zurückzuführen. Das Straßennetz in Russland ist von sehr unterschiedlicher Qualität, sein Ausbau kann mit dem immer stärker werdenden Straßenverkehr nicht Schritt halten. Die Dichte des Netzes nimmt von West nach Ost stark ab: Je weiter man sich von Moskau nach Osten entfernt, desto mehr verschlechtern sich die Straßenverhältnisse. Trotzdem wird der Großteil des Güterverkehrs zwischen Westeuropa und Russland über die Straße abgewickelt – im Transit über Polen und Belarus oder über die Nordroute via Polen und die baltischen Republiken sowie über Finnland. Dazu trägt auch der Spurweitenunterschied der Eisenbahnen bei.
Das russische Autobahn- und Fernstraßennetz umfasst zusammen etwa 540.000 km (2001), davon sind zwei Drittel befestigt. Erst seit 2003 existiert eine räumlich und saisonal durchgehende Straßenverbindung von der Ostsee zum Pazifik. Die Fernstraßen sind außerhalb der Ballungsgebiete in der Regel nicht als Autobahnen oder Schnellstraßen ausgebaut und auch bei größeren breiten Straßen sind die Richtungsfahrbahnen nicht durch Leitplanken voneinander getrennt. Die wichtigste Fernstraße in Russland ist die Europastraße 30, die in Sibirien endet.
Der Anteil der Transportkosten an den Produktionskosten liegt aufgrund der schlechten Straßen bei bis zu 20 %. Die schlechte Infrastruktur kostet das Land bis zu 9 % seiner Wirtschaftsleistung; Verkehrsexperten schätzen, dass jährlich umgerechnet mindestens 32 Milliarden Euro in den Ausbau der Straßen investiert werden müssten.[324]
Im Straßenverkehr passieren relativ viele tödliche Unfälle. Im Jahr 2013 kamen in Russland insgesamt 18,9 Verkehrstote auf 100.000 Einwohner. Zum Vergleich: In Deutschland waren es im selben Jahr 4,3 Tote. Insgesamt kamen damit 27.000 Personen im Straßenverkehr ums Leben. Die Motorisierungsrate des Landes liegt weltweit im oberen Mittelfeld. 2017 kamen im Land 324 Kraftfahrzeuge auf 1000 Einwohner. Mit ungefähr 46,9 Millionen Fahrzeugen verfügt Russland über den fünftgrößten Fuhrpark aller Länder.[325]
Fast die Hälfte der Passagierbeförderung findet im Nahverkehr statt, vorwiegend über das Busnetz, das in 120 Städten existiert. Darüber hinaus verfügen 90 russische Städte über ein Obusnetz, in 66 Städten gibt es Straßenbahnen und Vorortzüge und in sieben Städten auch eine U-Bahn sowie in vier weiteren S-Bahnlinien.
In den 1990er-Jahren verfielen viele der guten Nahverkehrsnetze und wurden zunehmend durch private Bus- oder Linientaxibetriebe ergänzt oder ersetzt. Auch in jüngster Zeit wurden in mehreren Großstädten Straßenbahn- oder Obussysteme zugunsten von Bussen stillgelegt (so 2008 der Obus in Archangelsk und die Straßenbahn in Iwanowo, oder 2009 die Straßenbahn in Woronesch).
Als Massentransportmittel über lange Distanzen nimmt die Eisenbahn in Russland einen wichtigen Teil des Verkehrsmarktes ein. Aufgrund der großen Entfernungen bildete im frühen 20. Jahrhundert die Anbindung des Fernen Ostens eine große Herausforderung, die das Land mit der berühmten Transsibirischen Eisenbahn herstellen konnte. Parallel dazu wurde Ende des 20. Jahrhunderts zur Erschließung des fernen Ostens Sibiriens die Baikal-Amur-Magistrale vom Baikalsee zum Fluss Amur gebaut. Durch diese beiden und die abzweigenden Strecken wird das Land in west-östlicher Richtung erschlossen. Durch sie kann beispielsweise die Beförderung von Gütern zwischen Pusan und Helsinki von etwa 47 Tagen auf dem Seeweg auf ca. 16 Tage reduziert werden.
Im Mai 2001 beschloss die russische Regierung die Umsetzung der Bahnreform. Die Hauptziele war die Liberalisierung des Eisenbahnmarktes und Freigabe der Tarife im Eisenbahnverkehr. Im Rahmen der Bahnreform wurde im Oktober 2003 das ehemalige Bahnministerium (MPS) aufgelöst und Russlands zweitgrößtes staatliches Unternehmen, die Rossijskije schelesnyje dorogi (RZhD) gegründet. In den letzten Jahren sind in Russland auch 85 Privatbahnunternehmen entstanden, die heute mehr als 25 % der Güter transportieren und rund 30 % (etwa 200.000 Güterwagen) des gesamten Güterwagen-Bestands in Russland besitzen. Das Streckennetz in Russland wird von der RZhD betrieben. Insgesamt umfasst das gut entwickelte Eisenbahnnetz (Breitspur mit 1520 mm Spurweite) rund 87.000 km, davon ist knapp die Hälfte (40.000 km) elektrifiziert. Auf der Insel Sachalin existieren fast 1000 km in 1067 mm Breite. Daneben gibt es zusätzlich 30.000 km nicht öffentlicher Industriebahnen (alle Angaben 2004). Während in Westeuropa schon seit Jahrzehnten der Straßengüterverkehr der dominierende Verkehrsträger ist und die Bahn eine nachrangige Bedeutung hat, konnte der Lkw in Russland erst seit 2000 aufholen. Daher besitzt die Bahn in Russland mit 83 % einen überdurchschnittlich hohen Marktanteil am Güterverkehr.
Russland verfügt über eine beträchtliche Anzahl von Häfen und befahrbaren Wasserstraßen. 72.000 km Binnenwasserwege verbinden im europäischen Teil Russlands die Ostsee, das Schwarze Meer, die Binnenseen und das Weiße Meer miteinander. Wichtige Wasserstraßen dabei sind die Wolga, die Kama, die Nischni Nowgoroder Oka, die Wjatka, der Don und die Kanäle, die diese Flüsse miteinander verbinden.
In Sibirien sind 24.000 km schiffbar. Durch die Entwässerung der großen Flüsse Ob, Jenissei und Lena in das Polarmeer fehlt eine Ost-West Erschließung auf dem Wasserweg; durch Eisbildung ist die Polarroute nur wenige Monate im Sommer möglich, diese Periode verlängert sich aber durch den Klimawandel. Die Schiffbarkeit der Flüsse und Kanäle wird durch meteorologische Einflüsse (Wasserstand) und mangelhaften Ausbau stark beeinträchtigt. Seit 1990 ist in Russland ein Abbau des Bestands der Binnenschiffsflotte zu beobachten. Die Zahl der Binnenschiffe betrug 2002 noch etwa 8800, davon waren 8000 Güterschiffe und 800 Passagierschiffe. Die wichtigsten russischen Binnenhäfen sind Archangelsk, Perm, Jaroslaw, Saratow und Tscheboksary.
Die Seeschifffahrt gehört zu den stark wachsenden Verkehrsbranchen in Russland. Wesentlicher Grund dafür ist das steigende Exportaufkommen an Rohöl und Mineralölerzeugnissen. Die wichtigsten Seehäfen befinden sich in St. Petersburg und Kaliningrad an der Ostsee, Noworossijsk und Sotschi am Schwarzen Meer sowie Wladiwostok, Nachodka, Magadan und Petropawlowsk-Kamtschatski am Pazifischen Ozean; Murmansk ist der einzige ganzjährig eisfrei gehaltene (Nord-)Atlantikhafen. Im Jahr 2003 betrug der Güterumschlag in den russischen Häfen 285,7 Millionen Tonnen. Für den Güterverkehr zwischen dem russischen Kernland und der Exklave Kaliningrad ist der Fährverkehr von Bedeutung.
In Russland und der Sowjetunion kam der Luftfahrt aufgrund der Fläche des Landes schon früh eine große Bedeutung zu. Der nationale Flugverkehr verbindet entlegene Gebiete, deren Erschließung auf dem Landweg sich nie lohnte. Zu Zeiten der Sowjetunion war die staatliche Aeroflot die größte Fluggesellschaft der Welt und ihre Preise teils günstiger als die der Eisenbahn.
Die Flugscheine in den fernen Osten Russlands werden auch heute vom Staat subventioniert.[326]
Neben der weiterhin halbstaatlichen Aeroflot fliegen als größere Gesellschaften die ebenfalls mit dem Staat verbundene Rossija, S7 Airlines oder UTair. Die Zahl von Flughäfen in Russland verringerte sich zwischen 1992 und 2011 von 1302 auf 496, wobei die Zahl internationaler Flughäfen von 19 auf 70 gestiegen war und 55 Flugplätze über eine befestigte Piste von mehr als 3000 m Länge verfügten. Mehrere internationale Fluggesellschaften fliegen außer Moskau auch andere russische Städte an. Die größten und wichtigsten Flughäfen sind Scheremetjewo-2 und Domodedowo in der Nähe von Moskau. Die Flugzeugflotte Russlands umfasste im Jahr 2011 rund 6000 Flugzeuge, davon knapp 2000 Frachtflugzeuge. Zur Belebung der russischen Luftfahrtindustrie dienen staatliche Förderung und Regulierungen. Im Herbst 2018 erteilte die Regierung den Banken Sberbank und VTB den Auftrag zur Gründung einer großen Regionalfluglinie,[327] Mit deren Hilfe sollte eine Aufwertung der Regionalflughäfen zur Entlastung des Drehkreuzes Moskau erreicht werden.[328] Im Januar 2020 erteilte Präsident Putin der Regierung die Anweisung, eine Gesellschaft für die Erschließung der abgelegenen östlichen Regionen zu bilden mit einer rein aus russischen Flugzeugen bestehenden Flotte.[326] Diese Gesellschaft wurde auf Basis der Red Wings geschaffen.[329] Nach dem russischen Einmarsch in die Ukraine 2022 und den westlichen Sanktionen erteilten die russischen Behörden 21 Fluggesellschaften die Erlaubnis, ausländische Luftfahrzeuge ohne gültiges Lufttüchtigkeitszeugnis zu betreiben, was zu einem Flugverbot über der EU führte.[330] Russland selbst schloss den Luftraum und elf Flughäfen (Anapa, Belgorod, Brjansk, Woronesch, Gelendschik, Krasnodar, Kursk, Lipezk, Rostow am Don, Simferopol und Elista) entlang des Kriegsgebietes zunächst für sieben Tage, danach wurden die Maßnahmen dutzendfach verlängert.[331] China verweigerte den doppelt registrierten Flugzeugen ebenfalls die Benutzung seines Luftraums.[332]
In den 1990er-Jahren litt die russische Raumfahrt unter großen Finanzierungsproblemen, so dass viele Programme stillstanden. Durch die Verbesserung der wirtschaftlichen Situation konnte sich die russische Raumfahrt erholen. Das Staatsunternehmen Roskosmos ist als nationale Weltraumorganisation für das zivile Raumfahrtprogramm des Landes zuständig; sein Sitz befindet sich im Sternenstädtchen nahe Moskau. Es wurde 1992 als Behörde gegründet und übernahm die wesentlichen Ressourcen der sowjetischen Raumfahrt. Roskosmos nutzt aktuell drei Raumfahrtbahnhöfe: das Kosmodrom Plessezk bei Archangelsk, das Kosmodrom Wostotschny im Amur-Gebiet sowie das Kosmodrom Baikonur in Kasachstan, die Hauptbasis der sowjetischen und russischen Raumfahrt. Russland ist seit Jahrzehnten einer der erfolgreichsten Anbieter von kommerziellen Raketenstarts.
Im Juli 2005 wurde ein neues Raumfahrtprogramm für die Jahre 2005 bis 2015 von der russischen Regierung genehmigt. Ziel war es, das Weltniveau der russischen Raumfahrt zu sichern und die Position Russlands unter den weltweit führenden Raumfahrtmächten zu festigen. Priorität hatten dabei die Entwicklung und Nutzung der Raumfahrttechnik und -dienstleistungen sowie der Bau von Raumschiffen für bemannte Flüge, Transport- und interplanetare Missionen, darunter auch ein wiederverwendbares Raumfahrtsystem. Russland beteiligt sich maßgeblich an der ISS, zu deren Versorgung, seit der Einstellung des Space-Shuttle-Programms, vermehrt die Sojus-Rakete mit dem Sojus-Raumschiff und dem Progress-Raumtransporter eingesetzt werden.
Weiterhin sollen die wissenschaftlich-technischen Grundlagen für einen bemannten Flug zum Mars und eine Raumstation der neuen Generation geschaffen werden. In einem ersten Schritt wollte Russland dazu bis 2015 seine Satellitenflotte vorrangig mithilfe westlicher Elemente an den Weltstandard heranführen. Zudem sollten zu diesem Zeitpunkt vom neuen Kosmodrom Wostotschny im Amur-Gebiet die ersten unbemannten Starts mit modernisierten Versionen der bisherigen Trägerraketen erfolgen. Tatsächlich startet dort seit 2016 das ältere Modell Sojus-2.1. Für 2020 waren von Wostotschny erste bemannte Starts von Raumschiffen mit der neuen Trägerrakete Angara A5 geplant; dies verschiebt sich auf Mitte der 2020er-Jahre. Zugleich sind für die 2020er-Jahre Missionen zur vertieften Erforschung des Mondes sowie des Planeten Venus  vorgesehen. 
Die russische Raumfahrtindustrie war seit Sowjetzeiten mit der der Ukraine verwoben; mehrere Raketen wie die Dnepr und die Zenit wurden gemeinsam entwickelt und produziert. Durch den Krieg mit der Ukraine zerbrach diese Zusammenarbeit, sodass Russland etwa die Hälfte seiner Auswahl an Trägerraketen verlor. Neue Eigenentwicklungen wie die Sojus-5 und -6 sollen dies im Laufe der 2020er-Jahre kompensieren.
Der überwiegende Teil des russischen Postwesens wird vom staatlichen Unternehmen Potschta Rossii abgewickelt. Dieses wurde 2002 aus dem zugleich aufgelösten föderalen Post- und Telekommunikationsministerium ausgegliedert, das auch zu Sowjetzeiten für den Postverkehr zuständig war. Heute bietet die Potschta Rossii ihre Dienstleistungen in insgesamt über 42.000 Postämtern an, die flächendeckend über ganz Russland verteilt sind. Die Zahl der Beschäftigten im Unternehmen beläuft sich russlandweit auf rund 415.000.[333] In vielen Städten bieten Postfilialen seit Anfang des 21. Jahrhunderts neben grundlegenden Postdienstleistungen – wie etwa dem Versenden und Empfangen von Briefen, Paketen und Telegrammen sowie dem Postgiro – auch ergänzende Dienste an, darunter öffentliche Computerarbeitsplätze mit Internetzugang.
Im Briefzustellungsbereich ist Potschta Rossii in Russland Monopolist. Im Bereich der Paketpost sind seit den 1990er-Jahren auch international tätige Kurierunternehmen wie DHL oder TNT Express in Russland tätig.
Das gesamtrussische Telekommunikationsunternehmen Rostelekom ist das größte Unternehmen dieser Branche in Russland. Seit dem 1. April 2011 gehören zu ihm die Regionalfilialen Dalny Wostok (Ferner Osten), Sibir, Ural, Wolga, Jug (Süden), Sewero-Sapad (Nord-West) und Zentr (Zentrum). Den Mobilfunkmarkt teilen sich landesweit im Wesentlichen die drei größten Anbieter des Landes Mobile TeleSystems, Beeline und MegaFon, ferner einige kleinere regionale Anbieter. Diese Branche erlebte in Russland ab dem Jahr 2000 einen rasanten Wachstum: Besaß noch im Jahr 2000 weniger als 1 % der russischen Bevölkerung ein Mobiltelefon, überschritt 2006 die landesweite Anzahl von Handys bereits die Bevölkerungszahl und betrug mit dem Stand vom 31. März 2007 gut 155 Millionen.
Im Jahr 2019 wurde per Gesetz verfügt, dass der Internet-Datenverkehr über eigene Server zu laufen hat, sodass fortan eine Unabhängigkeit gegenüber dem Ausland gewährleistet ist.[334][335]
Die Geschichte des Internets in Russland beginnt im September 1990, als die Top-Level-Domain „.su“ für die damalige Sowjetunion angemeldet wurde. Diese Domain wird von russischen Websites teilweise bis heute benutzt. Im März 1994 wurde die offizielle Top-Level-Domain „.ru“ für russische Internet-Adressen angemeldet. Websites unter dieser Domain machen einen beträchtlichen Teil des russischen Internets – oft kurz Runet genannt – aus. Inzwischen hat das Land auch eine kyrillische Top-Level-Domain (.рф). Das russische Internet-Segment rangierte um 2012 mit insgesamt mehr als 3,6 Millionen Domainnamen auf Platz vier weltweit.
In den 2000er-Jahren stieg die Anzahl der Internetnutzer in ganz Russland kontinuierlich an: Gab es im Jahre 2000 nur 3,1 Millionen Nutzer (2,1 % der Bevölkerung) landesweit, betrug ihre Anzahl 2007 bereits 28 Mio. (19,5 %).[336] Mit mehr als 50 Millionen Internet-Usern wurde Russland 2011 zum europäischen Spitzenreiter.[337] 2016 nutzten 102 Millionen Russen oder 71,3 % der Bevölkerung das Internet.[338] Zu den bedeutendsten Internet-Projekten des Runet gehören die Suchmaschinen Rambler und Yandex, das Online-Netzwerk W Kontakte sowie die Informations- und Nachrichtenportale RBC Informations Systems, Lenta.ru und Gazeta.ru. Zu den bekanntesten Providern gehören größere Telekommunikationsunternehmen wie CenterTelekom, MGTS, North-West Telecom oder WolgaTelekom.[339] Im Zuge einer staatlichen Förderung des Internet-Ausbaus verzeichneten die Social-Media-Aktivitäten in Russland einen außergewöhnlich starken Auftrieb, entsprechende Plattformen spielen in Russland eine bedeutende Rolle. Besonders populär sind die in Russland entstandenen Plattformen Vkontakte.ru und Odnoklassniki.ru, die höhere Wachstumsraten auswiesen als internationale, wie etwa Facebook. Auch LiveJournal wurde in Russland im internationalen Vergleich überdurchschnittlich genutzt und schließlich russisch. Die Bruttoreichweite der Social Networks betrug im Jahr 2010 rund 49,2 Millionen der in Russland lebenden Personen.[340] Seither wurden viele Regulierungen mit schwammigen Formulierungen erlassen, welche den Behörden ein Durchgreifen gegen Dienste und Nutzer erlauben. Ab 2018 müssten sämtliche Kommunikationsinhalte gespeichert (und dem Staat zur Verfügung gestellt) werden, eine Verschiebung dieser Pflicht um 5 Jahre musste wegen des Aufwandes im Jahr 2017 erwogen werden.[341]
Seit dem Zusammenbruch des Sowjetsystems gab es viele Umstrukturierungsphasen im russischen Mediensektor. Staatliche Reformen haben den Medienmarkt zu Beginn der 1990er-Jahre privatisiert. Viele Zeitungen, Verlage und Fernsehsender gingen seitdem Allianzen mit Oligarchen ein, um ihr Überleben zu sichern. Dabei gerieten sie aber unter deren Kontrolle, die durch Manipulationen politischen Einfluss über die Medien ausüben.[342] Die Präsident Putin widersprechenden Medienimperien von Boris Beresowski und Wladimir Gussinski (Media Most) wurden durch Gerichtsbeschluss zerschlagen. Die größten russischen Medienholdings sind die Gazprom-Media und die WGTRK, die Allrussische Staatsgesellschaft für Fernsehen und Radio. Obwohl die Medienzensur durch Roskomnadsor (Aufsichtsbehörde für Massenmedien, Kommunikation und den Schutz des kulturellen Erbes) praktiziert wird[343], ist laut der russischen Verfassung, Kapitel 2, Artikel 29 die Freiheit der Meinung und des Wortes garantiert. Propaganda und Agitation, die soziale, rassische, nationale und religiöse Feindschaft schürt, ist verboten, ebenso die Relativierung der Rolle der Streitkräfte im Zweiten Weltkrieg. Die meisten Russen bevorzugen das Fernsehen als Informationsquelle Nummer eins, gefolgt von Zeitungen. Nach Angaben von Roskomnadsor sind in Russland (Stand: Jahr 2012) 66.032 Medien gelistet. Darunter finden sich 5254 TV-Sender, 3769 Radiosender, 28.449 Zeitungen und 21.572 Zeitschriften.[344] Die Kanäle des Staatsfernsehens sind keine Massenmedien im westlichen Sinne.[345]
Die tagesaktuelle Presse der UdSSR wurde jahrzehntelang vor allem durch die halbamtliche Presseagentur TASS mit Informationen versorgt. Nach dem Zusammenbruch der UdSSR entwickelte sich in Russland eine freie Presse, die sich jedoch heute wieder zunehmenden Repressionen durch die Regierung ausgesetzt sieht. Freedom House bewertet die Pressefreiheit als „nicht frei“ und mit einem generellen Abwärtstrend (2002 war das Land noch als „teilweise frei“ verzeichnet).[346] In der Rangliste der Pressefreiheit der Reporter ohne Grenzen rangiert Russland im Jahr 2023 auf dem 164. Platz; in Europa schnitt nur die Türkei (Rang 165) schlechter ab.[347]
Im Frühjahr 2017 wurde der Journalist Nikolai Andruschtschenko getötet.[348] Laut dem Bericht von Reporter ohne Grenzen steht der Tod des Opfers in direktem Zusammenhang mit seiner journalistischen Tätigkeit.[349]
Unter den Printmedien gilt die Boulevardzeitung Moskowski Komsomolez als die beliebteste im Land. Nach eigenen Angaben erreicht die Boulevardzeitung etwa 1,3 Millionen Leser. Sie ist auch die günstigste. Wichtigste Tageszeitung ist die Komsomolskaja Prawda, mit einer Auflage von heute 830.000 Exemplaren. Die Tageszeitung Rossijskaja gaseta (Auflage: 430.000 Exemplare) ist ein Verlautbarungsblatt der russischen Regierung mit Sitz in Moskau. Russische Gesetze und Erlasse treten erst mit der Veröffentlichung in der Rossijskaja Gaseta in Kraft. Eine staatliche Informations- und Analyseagentur ist seit 1993 die RIA Novosti mit eigenen Korrespondenten in mehr als 40 Ländern.
Neben dem staatlichen Radio Rossii gibt es zahlreiche private Hörfunksender – meist Lokalsender. Einige Moskauer Stationen haben auch Lizenzen in den Regionen. Der Sender Echo Moskwy galt bis zu seiner erzwungenen Unterbrechung 2022 als einziger verbliebener Vertreter regierungskritischer Medien. Russische Radiosender nutzen heutzutage die auch in Deutschland üblichen UKW-Frequenzen (87,5 MHz bis 108,0 MHz) unter der englischen Bezeichnung „FM“. Zu Sowjetzeiten wurde das so genannte OIRT-Band (65,9 bis 73,1 MHz) genutzt, wo heute unter dem Namen UKW noch einzelne Sender laufen. Viele russische Wohnungen haben einen Radiostecker, mit dem man in der Art des Drahtfunks ein bis drei Sender empfangen kann. Die simplen Geräte benötigen keine weitere Stromversorgung und haben oftmals als einziges Bedienelement einen Lautstärkeregler. Unter der Bezeichnung Stimme Russlands wird der umfangreiche Rundfunk-Auslandsdienst betrieben.
Das Fernsehen ist für 85 % der russischen Bevölkerung die hauptsächliche und oft einzige Informationsquelle und eignet sich daher besonders als Propaganda-Instrument der Regierung, die die inhaltliche Ausrichtung der Programme sorgfältig steuert.[351] In den meisten Teilen Russlands können drei landesweite und ein bis zwei regionale Fernsehsender empfangen werden. In Moskau sind je nach Lage mehr als ein Dutzend Fernsehanbieter terrestrisch empfangbar. Der Perwy kanal, dt. Erster Kanal, ist landesweit der Sender mit der größten Reichweite und kann von 99,8 % der russischen Bevölkerung empfangen werden, die wöchentliche Zuschauerschaft beim Sender erreicht über 80 % der Bevölkerung. Ein Teil der russischen Fernsehsender wird vom staatlichen Medienkonzern WGTRK betrieben. Zu dessen Angebot gehört der Kanal Rossija 1, der laut eigenen Angaben von ca. 98,8 % der russischen Bevölkerung empfangen wird. Auch ein Sportsender namens Sport (russisch: Спорт) und ein Kultursender namens Rossija K werden von WGTRK betrieben. Daneben gibt es seit 2005 den international ausgerichteten, englischsprachigen Sender Russia Today mit Sitz in Moskau, dessen erklärte Ziele sind, alte Vorurteile und Klischees über Russland abzubauen und dem Publikum die russische Sichtweise auf das internationale Geschehen vorzustellen. Auch Entwicklungen innerhalb Russlands sollen hier aus russischer Perspektive beleuchtet werden. Vesti ist einer der wichtigsten Nachrichtenkanäle Russlands. Er ist ein Teil von Telekanal Rossija und RTR. Der TV-Sender Russian TV international wird speziell für die im Ausland lebenden Russen produziert.
In den 1990er-Jahren entwickelten sich in Russland mehrere teils landesweite private Fernsehsender, die auch unabhängige und auch regierungskritische Informationssendungen im Programm hatten. Zu Beginn der 2000er-Jahre gerieten jedoch die landesweit empfangbaren Sender unter die indirekte Kontrolle des Staates oder wurden geschlossen und durch staatliche Sender ersetzt. So sendet Sport heute auf der Frequenz von TW-6. Russland sendet mit der Fernsehnorm SECAM (Variante Osteuropa). Russland plant langfristig (in den 2010er-Jahren) DVB-T einzuführen. Angeblich sollen derartige Geräte subventioniert werden, damit sich die Bevölkerung das verhältnismäßig teure Gerät anschaffen kann.
Das Bildungssystem in Russland gliedert sich in vier Abschnitte: allgemeine Schulausbildung, Berufsausbildung, Hochschulausbildung und die Postgraduierten-Ausbildung. Die allgemeine Schulausbildung bedeutet nicht, dass das Kind eine Schule besuchen muss.[352] Auf Wunsch der Eltern kann ein Kind eine häusliche Ausbildung erhalten, wenn sein Kenntnisstand dem Schulprogramm entspricht, was zweimal jährlich geprüft wird. Dieses Recht ist in Russland durch die Staatsverfassung (der Artikel 43) sowie durch das Bundesgesetz №273-ФЗ (das Föderalgesetz über Bildung in der Russischen Föderation) garantiert.[353]
Der Staat wendete um 2017 4 % des Zentralhaushalts für Bildung auf.[36] Im PISA-Ranking von 2015 erreichen russische Schüler Platz 23 von 72 Ländern in Mathematik, Platz 32 in Naturwissenschaften und Platz 26 beim Leseverständnis.[354]
Die Allgemeine Schulausbildung untergliedert sich wiederum in die Abschnitte Grundstufe, Hauptstufe und Oberstufe.
Für die Hochschulausbildung steht den Studierenden in Russland ein vielfältiges Hochschulwesen zur Verfügung. Außer der klassischen Universität mit einem breiten Fächerangebot gibt es verschiedene Hochschulen und Akademien mit einer speziellen technischen, pädagogischen oder ökonomischen Ausrichtung. Das Abitur ist zwar Voraussetzung für den Hochschulbesuch, es muss jedoch zusätzlich eine Aufnahmeprüfung bestanden werden. Die Studienfinanzierung gibt es für leistungsstarke Schüler kostenfrei, für einen immer größer werdenden Teil der Bevölkerung aber nur gebührenfinanziert. Die Hochschulen haben nach 1992 größere Rechte zur Selbstverwaltung erhalten. Hochschulen werden neu aufgestellt; altehrwürdige Einrichtungen erhalten neue Namen und moderne Strukturen.
Die Dauer der meisten Studienprogramme beträgt fünf Jahre, wobei die ersten zwei Jahre wie in Deutschland auch, einem allgemeinen Grundstudium dienen, dem dann die fachliche Spezialisierung im Hauptstudium folgt. Bis 1991 gab es als einzigen Abschluss nur das Diplom. Mit der schrittweisen Einführung neuer Studiengänge sind neben dem Diplom auch der Bachelor und Master als Abschlüsse möglich, den die meisten Studenten auch anstreben.
Insgesamt lassen sich vier Kategorien von Hochschuleinrichtungen in folgender Hierarchie aufstellen:
Zu den bekanntesten russischen Universitäten gehören die Staatliche Moskauer Lomonossow-Universität, die Staatliche Universität Sankt Petersburg, die Staatliche Universität Kasan und die Staatliche Technische Universität Nowosibirsk. Inzwischen ist in Russland die Gründung von privaten Schulen und Hochschulen erlaubt. Ihr Besuch ist nicht kostenlos und meist nur für eine kleine Schicht erschwinglich. In Russland gab es 2005 1061 Universitäten und Hochschulen, wovon 413 private Hochschulen waren.
Erste Anfänge der wissenschaftlichen Tätigkeiten gab es in Russland bereits zu Zeiten der Kiewer Rus. So stammen die ersten überlieferten Chroniken, die Nestorchroniken, aus dem Jahr 1070. Dort wurden vor allem historische Ereignisse und auch meteorologische Beobachtungen festgehalten.
Wissenschaft als soziale Einrichtung entstand in Russland aber erst Anfang des 18. Jahrhunderts unter der Herrschaft Peter des Großen. Zu dieser Zeit wurden die ersten wissenschaftlichen Einrichtungen des Russischen Reichs gegründet, vor allem 1724 die Akademie der Wissenschaften. 1755 wurde in Moskau mit der heutigen Lomonossow-Universität die erste Universität Russlands gegründet. Im Jahre 1916 gab es in ganz Russland rund 100 Hochschulen, davon 10 Universitäten, sowie einige Dutzend Forschungseinrichtungen. Damit befand sich die Wissenschaft des Russischen Reichs im Vergleich zu vielen anderen europäischen Ländern auf einem niedrigen Entwicklungsniveau. Dennoch genossen schon damals bestimmte Bereiche der russischen Wissenschaft internationales Ansehen. So waren unter den ersten Nobelpreisträgern zwei russische Akademiker, Iwan Pawlow (1904) und Ilja Metschnikow (1908).
Einen erheblichen Entwicklungsschub bekam die russische Wissenschaft zu Sowjetzeiten. Die Sowjetunion besaß insgesamt ein gut ausgebautes Forschungs- und Entwicklungssystem. Charakteristisch für diese Zeit war der hohe Zentralisierungsgrad der Forschung. So waren die meisten Wissenschaftler bei der Akademie der Wissenschaften oder in ihren regionalen Abteilungen angestellt. Zentrale Merkmale waren die Trennung von Forschung und Produktion, die Dominanz der Akademie der Wissenschaften der UdSSR in der Grundlagenforschung und in der anwendungsbezogenen Forschung und die geringe Bedeutung des Hochschulbereichs in der Forschung. Alle Unternehmen im Wirtschaftsbereich waren in Staatsbesitz und führten selbst wenig Forschung durch. Ein Großteil der Forschung wurde durch spezialisierte Forschungsinstitute vorgenommen, die im Allgemeinen organisatorisch von den staatlichen Unternehmen getrennt waren. Da der Sowjetstaat der Industrialisierung und militärischer Überlegenheit eine sehr hohe Priorität einräumte, förderte er die Forschung und Entwicklung auf diesen Gebieten besonders großzügig. Nach dem Ende des Zweiten Weltkriegs förderte der Staat die Entwicklung der sowjetischen Raumfahrt sehr intensiv. Dies alles führte dazu, dass die Sowjetunion in der zweiten Hälfte des 20. Jahrhunderts zu einem Industrieland aufgestiegen war. Die Forschung und Entwicklung galt auf bestimmten Gebieten, wie der Rüstungsindustrie und der Raumfahrt, als weltweit führend.
Die Wissenschaft erlebte in der Russischen Föderation in den 1990er-Jahren eine schwere Krise, da es permanent an finanziellen Mitteln fehlte, um die vorhandenen Forschungseinrichtungen zu unterstützen. Das führte zu Entwicklungsstopps auf vielen Gebieten und zur Abwanderung qualifizierter Forschungs- und Lehrkräfte ins europäische Ausland oder in die USA. Die Institutionen und Arbeitsweisen in der russischen Forschung und Entwicklung haben viele Merkmale des ehemaligen sowjetischen Systems beibehalten, die Mehrheit der Forschungsorganisationen sind vom Wirtschaftssektor getrennt. Forschungseinrichtungen in Unternehmen sind in der Regel gering ausgebildet. Die Russische Akademie der Wissenschaften hat eine dominierende Stellung inne. Fast zwei Drittel aller Forschungseinrichtungen waren (Stand April 2012) in Staatsbesitz und beschäftigen 78 % des Forschungspersonals. Dagegen sind 14 % der Einrichtungen privatwirtschaftlich organisiert. Aufgrund dieser Übermacht des Staates wird die russische Forschung vorrangig durch große Forschungsinstitute angeführt, kleine Organisationen haben nur eine geringe Bedeutung. Dementsprechend beschäftigten 2008 die größten aller russischen Forschungseinrichtungen insgesamt 53 % des Forschungspersonals und waren für 44 % der gesamten Forschungsausgaben verantwortlich. Bei der Finanzierung von Forschung und Entwicklung überwiegt die Förderung durch den Staatshaushalt. Anfang der 2010er-Jahre versuchte die Regierung, den Forschungsbeitrag der Universitäten zu erhöhen. Am gesamten Finanzierungsumfang der Forschung macht der Hochschulsektor gerade 6–7 % aus. 12 % des Lehrpersonals werden als Forscher eingestuft. Fast die Hälfte aller Universitäten und anderen Hochschuleinrichtungen beteiligt sich überhaupt nicht an Forschungsaktivitäten.
Trotz Krisen in den 1990er-Jahren nehmen einige Bereiche der Wissenschaft Russlands nach wie vor im internationalen Vergleich obere Positionen ein. So wurden fünf russische Physiker mit dem Nobelpreis ausgezeichnet: Schores Alfjorow im Jahr 2000, Alexei Abrikossow und Witali Ginsburg im Jahr 2003 sowie Andrei Geim und Konstantin Nowosjolow im Jahr 2010.
Zur Förderung der einheimischen Forschung und Entwicklung ab 2000 wurden spezielle nationale Zielprogramme entworfen, die unter anderem eine Erhöhung der Gehälter für Angestellte in der Wissenschaft, die Förderung von Nachwuchsakademikern und die landesweite Einrichtung von Technologieparks vorsahen. Dabei wurde besonders auf die Weiterentwicklung in den Bereichen Wert gelegt, in denen Russland früher Spitzenergebnisse erzielte, also vor allem in Naturwissenschaften und der Rüstungsindustrie. Präsident Medwedew startete eine Modernisierungsoffensive durch Förderung von Schlüsselprojekten, so die Stadt der Innovationen (Innograd), in Skolkowo. Dort sollen künftig neue Techniken erforscht und bis zur Marktreife entwickelt werden. Der neue Forschungs- und Entwicklungskomplex sollte vorrangig in fünf Bereichen arbeiten: Energie, Informationstechnik, Telekommunikation, Biomedizin und Kerntechnik. Die russische Regierung plante weiterhin den Einstieg in die Produktion von Mikroelektronik. Auch bei der Satellitennavigation will Russland seinen Markt stärker auf die Nutzung des einheimischen Systems GLONASS trimmen.
Die Aussichten für die Wissenschaft verdüsterten sich nach dem russischen Angriffskrieg 2022, alle Spitzen des Wissenschaftsbetriebs wurden auf Regierungskurs eingeschworen, während umgekehrt 8000 Wissenschaftler einen Protestbrief unterschrieben. Das MIT wie auch das CERN stellte die Zusammenarbeit ein, die Russen am CERN hofften auf einen Verbleib als „staatenlose“ Mitarbeiter, ähnlich russischer Olympiateilnehmer. Es wurde eine Kürzung der Mittel ebenso befürchtet wie Probleme bei der Labor-Ausrüstung für die Grundlagenforschung.[355] Schon seit längerer Zeit konnten Wissenschaftler mit Kontakten ins Ausland willkürlich belangt und als „ausländische Agenten“ registriert werden.[356]
Die russische Kultur besteht aus einer europäischen Hochkultur und einer gewachsenen russischen Volkskultur. Zeitweise verstand sich Russland als das radikale Andere des Westens, auch weil die russische Kultur im Vergleich zu der westeuropäischen über lange Zeit eine andere Entwicklung nahm, bedingt durch ihren Standort an der Peripherie der westlichen Kulturentwicklung. Weiterhin führte das Schisma von 1054 zu einem sich völlig anders entfaltenden orthodoxen Christentum mit einer wachsenden Ablehnung des Katholizismus. Die russische Staats- und Rechtsauffassung, die dem byzantinischen Cäsaropapismus entstammt, im Unterschied zur römischen Rechtstradition im Westen, trug ebenso zu der Abgrenzung der russischen Kultur zu der westeuropäischen bei (vgl. Rechtsgeschichte Russlands). Im Gegensatz zu der Entwicklung von Nationalstaaten im restlichen Europa vollzog sich in Russland ab 1550 der Wandel zu einem Vielvölkerreich, der die kulturelle Entwicklung mitprägte.
Die russische Kultur ist weiterhin durch zeitlich verschiedene Entwicklungsphasen zur westeuropäischen Kultur geprägt. Dies lässt sich durch die geokulturelle Randlage und gleichzeitige Ausdehnung Russlands nach Osten erklären, die ein unterschiedliches Evolutionstempo im Wechselspiel verlangsamter und beschleunigter Nachhol- und Entwicklungsphasen hervorrufen, wodurch es in der russischen Geschichte wiederholt zu gesellschaftlichen Umbrüchen und politischen Radikalisierungen kam. Demnach kann Russland als eine Übersetzungskultur angesehen werden, allerdings nicht in passiver Nachahmung, sondern aus dem Bedürfnis des Nachholens und Überbietens. Dies erzeugt produktive Wechselwirkungen, indem Eigenes nach dem imitierten Fremden modelliert wird und so Neues hervorbringt.
Russlands Kulturgeschichte beginnt weitgehend mit seiner Christianisierung (988/989) am Ende des 10. Jahrhunderts, wobei auf Ersuchen des Kiewer Fürsten Wladimir I. die byzantinische Kultur in ihren slawisierten Formen für die nächsten sieben Jahrhunderte bei den Russen die Vorherrschaft gewann. Es folgte ein rasches Aufblühen ihres Schrifttums, ihrer Kunst und Architektur nach der Einführung des Christentums.
Gerade die Orthodoxie bedingte ein anderes, auf Beharrung und Traditionen basierendes Kulturverständnis. Die religiöse Weltanschauung und kirchliche Textauffassung bestimmte und verlangsamte im Moskauer Reich die kulturelle Entwicklung. Eine Erstarrung der russisch-orthodoxen Kultur setzte ab 1500 ein, nachdem der Impulsgeber Byzanz durch den Fall Konstantinopels unter osmanische Herrschaft gelangt war. Unter Peter I. begann ab dem 17. Jahrhundert eine forcierte Säkularisierung und Europäisierung des gesellschaftlichen Lebens. Der erste Kaiser des Russischen Reiches holte westeuropäische Architekten und Künstler ins Land und wollte durch die äußere Europäisierung – z. B. Ablegung der Bärte und Annahme der europäischen Kleiderordnung – eine Änderung der inneren Einstellung erreichen. Die Europäisierung Russlands erreichte aber nur eine kleine Oberschicht. Russland fand im 19. Jahrhundert den Anschluss an die europäische Kultur und gehörte um 1900 zu ihrer Avantgarde.[357] Neben einer verwestlichten Hochkultur der Oberschicht bestand die traditionelle russische Volkskultur im Volk fort, so dass bis 1914 immer noch zwei Kulturen nebeneinander bestanden. In der Sowjetunion wurde dann unter Stalin der Sozialistische Realismus zur einzigen verbindlichen Kulturnorm erklärt. Nicht systemkonforme schriftliche oder gesungene Ausdrucksformen von Kultur konnten nur im Untergrund als Samisdat erscheinen. Im neuen russischen Staat erlebte die russische Kultur in den 1990er-Jahren eine erneute Krise. So mussten die russischen Kunstschaffenden mit den wegfallenden staatlichen Förderungen und der Konkurrenz in der kapitalistischen Massenkultur in den 1990er-Jahren zuerst den daraus resultierenden Stillstand überwinden.
Die Wohnhäuser in Russland wurden lange in Blockbauweise (Isba) errichtet. Diese Blockhäuser findet man heute noch auf den Dörfern. Sie sind meist in blauen oder grünen Farbtönen gestrichen und besitzen phantasievolle geschnitzte, meist weiße Fensterrahmen. Blau und Grün sollen als Farben der Orthodoxie böse Geister vertreiben.[358]
Russisches traditionelles Handwerk bildet einen wichtigen Aspekt der russischen Volkskultur. In der Waldzone der Nordost-Rus entwickelten sich das Drechslerhandwerk und die Holzschnitzerei. An Orten, an denen Lehm vorhanden war, entwickelte sich das Keramikhandwerk. In den nördlichen Regionen Russlands mit seinen ausgedehnten Flachsfeldern wurden Spitzen geklöppelt. Der Ural mit seinen reichen Vorkommen von Eisenerz sowie von Halbedel- und Schmucksteinen ist für seine Gießkunst, den Waffenschmuck und Schmuckartikel berühmt. Berühmt ist das Dymkowo Keramik-Spielzeug (siehe Anna Afanassjewna Mesrina), Chochloma, Keramik aus Gschel und Lackminiaturen aus Palech. Matrjoschka ist das beliebteste russische Souvenir. Schon ein paar Jahre nach ihrem Aufkommen wurde die Matrjoschka auf der Pariser Weltausstellung von 1900 demonstriert, wo sie eine Medaille verdiente und weltweiten Ruhm erlangte.
Zur traditionellen russischen Kleidung gehörten Kaftan, Kossoworotka und Uschanka für Männer, Sarafan und Kokoschnik für Frauen, mit Lapti aus Bast und Walenki (Filzstiefel) als übliches Schuhwerk. Zur traditionellen Kleidung der Kosaken aus dem südlichen Russland gehören Burka und Papacha.
Die russische Küche, ursprünglich eine typische Bauernküche, verwendet viele Zutaten aus Fisch, Geflügel, Pilzen, Beeren und Honig. Gegessen wird Brot, Pfannkuchen, getrunken wird Kwas, Bier und Wodka. Wodka ist ein Teil der russischen Kultur. Laut russischen Chroniken entstanden im Russland des 12. Jahrhunderts erste Brennereien. Zunächst wurde Wodka für medizinische Zwecke verwendet. Russischer Wodka wird aus Getreide hergestellt. Traditionell bevorzugt man in Russland einen reinen, nicht aromatisierten Wodka, der bei Zimmertemperatur meist in Gesellschaft getrunken wird. Zu Wodka wird oft etwas Salziges (beispielsweise Salzgurken, Salzpilze oder Salzhering) serviert. Schmackhafte Suppen und Eintöpfe wie Schtschi, Borschtsch, Rassolnik, Ucha, Soljanka und Okroschka kennzeichnen die russische Küche. Berühmt sind auch russische Teigspeisen wie Piroschki, Blini und Syrniki. Kiewer Kotelett, Bœuf Stroganoff, Pelmeni und Schaschlik sind beliebte Fleischgerichte, die letzten beiden sind tatarischen und kaukasischen Ursprungs. Weitere verbreitete Fleischgerichte sind Kohlrouladen (russ. Голубцы) in der Regel mit Fleisch gefüllt. Typisch russisches Salate sind Vinaigrette (russ. винегрет), Oliviersalat und Hering im Pelzmantel (russ. Сельдь под шубой). Tee wird in Russland bereits seit dem 17. Jahrhundert in jedem Haushalt getrunken, sodass sich in Russland eine richtige Teekultur entwickelte. Zur Zubereitung des Tees wird in Russland traditionell ein Samowar verwendet, er gilt in Russland als eine Art Nationalsymbol. Neben den traditionellen russischen Desserts, wie Baranki, Prjaniki, Warenje und Pastila (bzw. Sefir), werden zum Tee auch gerne orientalische Süßigkeiten, wie Halva, Gosinaki und Lokum, sowie diverse Schokoladen und Torten serviert.
Russlands große Anzahl von ethnischen Gruppen verfügt über ausgeprägte Traditionen der Volksmusik. Typische russische Musikinstrumente sind Gusli, Balalaika, Schaleika und Garmon. Das russische Volk besitzt eine reiche Tanzfolklore. Berichte über russische Tänze finden sich seit dem 11. Jahrhundert. Tänze spielen für das russische Volk eine große Rolle. In vielen Tänzen kommen die nationalen Züge des russischen Charakters sehr klar zum Ausdruck. Die älteste Art des russischen Tanzes ist der so genannte Chorowod, ein Reigentanz einer Gruppe von Teilnehmern, die sich an den Händen halten. Die zweite Art von Tänzen, die für die russische Tanzkunst charakteristisch ist, sind die Improvisationstänze. Sie werden als Solotänze (Mann oder Frau), in Paaren oder von mehreren Tanzenden aufgeführt. In diesen Tänzen kommt die Individualität des Tanzenden besonders stark zum Ausdruck. Der Perepljas ist eine Art Tanz um die Wette, wobei jeder der Reihe nach auftretende Tänzer bestrebt ist, den anderen durch seine Tanzmeisterschaft, Phantasie und bessere Ausführung der Bewegungen zu übertrumpfen.
Russland besitzt eine ausgeprägte Dampfbadkultur, die Banja. Der Besuch der Banja ist ein Ritual. Dort finden bis heute wichtige Gespräche, Geschäftsverhandlungen und politische Besprechungen statt. Auch im Kreml gibt es eine Banja. Nach alter russischer Tradition klopft man sich vorsichtig mit Weniks ab – in warmes Wasser getauchte, getrocknete Birkenzweigbündel.
Zur Erholung und zum Entspannen verbringen russische Stadtbewohner die Wochenenden oder ihren Urlaub gerne in einer Datscha, einem Land- bzw. Ferienhaus mit Garten. Seit drei Jahrhunderten gehören die Datschen zur russischen Geschichte und Kultur. Auch in vielen russischen Balladen und in der russischen Literatur findet die Datscha oftmals Erwähnung. Ab Mitte Mai beginnt die Datscha-Saison. Rund um St. Petersburg und Moskau gibt es sehr viele Datschen-Vororte, die sich im Laufe ihrer Geschichte immer weiter von der Stadt entfernt haben.
Bekannt sind auch die russischen Märchen, die ihre Ursprünge in der heidnischen Zeit der Rus haben. Sie bildeten die Grundlage für die berühmten sowjetischen Märchenfilme. Sie haben Märchengestalten wie „Väterchen Frost“, das „Schneeflöckchen“ oder die „Hexe Baba Jaga“ auch nach Mitteleuropa gebracht.
Die russische Gastfreundschaft selbst in wirtschaftlich schwierigsten Zeiten ist sprichwörtlich. Bei einer Einladung versucht der Gastgeber bewusst, so viele verschiedene Gerichte wie möglich zuzubereiten. Das zeigt, dass für die Gäste an nichts gespart wird. Bis heute lebt der Brauch, bei offiziellen Anlässen ein rundes Brot mit einem Salzgefäß in der Mitte an den wichtigsten Gast auszuhändigen. Brot war lange Zeit das Hauptnahrungsmittel in Russland. Salz war rar und deswegen sehr teuer.[359]
Ein im 19. Jahrhundert sehr verbreitetes Straßenbild im Winter war die Troika, das typisch russische Dreigespann. Dazu werden drei Pferde vor einer Kutsche oder einem Schlitten nebeneinander angeschirrt. Am Bogen hängt ein Glöckchen, das während der Fahrt ständig bimmelt und die Pferde in Trab hält. Die Troika stammt von den Waldai-Höhen, einer Hügellandschaft zwischen Moskau und St. Petersburg, und wird heute als Folklore gepflegt.
Als Nationalfeiertage gelten in Russland der sogenannte Tag der Einheit des Volkes am 4. November, der an die Befreiung Moskaus im Jahre 1612 von polnisch-litauischen Fremdherrschern erinnert, sowie der Tag Russlands am 12. Juni anlässlich der Erklärung der Staatssouveränität der Russischen SFSR an diesem Tag im Jahr 1990. Daneben gibt es jährlich mehrere gesetzliche Feiertage, von denen vor allem das Neujahrsfest (durchgehend vom 1. bis 5. Januar) gefeiert wird. Das Neujahrsfest wurde 2005 verlängert, dafür aber der für die Kommunisten wichtigste Nationalfeiertag, der Tag der Oktoberrevolution am 7. November, abgeschafft. Die russisch-orthodoxen Christen feiern Weihnachten nicht wie bei den Christen anderer Konfessionen am 24. Dezember. Sie feiern nach dem Julianischen Kalender am 7. Januar das Fest der Erscheinung des Herrn. Während der Sowjetzeit waren religiöse Feste nicht erlaubt. Doch seitdem im Jahr 1991 der 7. Januar zu einem offiziellen Feiertag erklärt wurde, wird Weihnachten in Russland wieder richtig gefeiert. Den Heiligen Abend am 6. Januar nennt man in Russland Sotschelnik.
Jedes Jahr begeht die russisch-orthodoxe Kirche das Epiphaniasfest. Es ist einer der ältesten orthodoxen Feiertage und geht zurück auf die Taufe Jesu im Jordan. Trotz Frost zieht es Jahr für Jahr Millionen Russen in der Nacht vom 18. auf den 19. Januar ans Eisloch. An diesem einen Tag im Jahr ist das Wasser aller Flüsse und Seen Russlands heilig, besonders wenn es zuvor von einem orthodoxen Priester gesegnet wurde. Dreimal müssen Teilnehmende vollständig untertauchen. Vor jedem Eintauchen des Kopfes bekreuzigen sie sich. Die Prozedur soll die Gläubigen von Sünden reinigen und ihnen neue Kraft verleihen.
Der „Tag des Sieges“ über das nationalsozialistische Deutschland (am 9. Mai) besitzt nach wie vor einen hohen Stellenwert in der Bevölkerung. Anfang Mai kommen dazu überall in Russland festlich gekleidete Kriegsveteranen zusammen und gedenken der gefallenen Kameraden. Oft fängt so ein Treffen an einem Grab oder Grabmal des unbekannten Soldaten oder an einem Ewigen Feuer an. Danach wird die Gedenkfeier entweder bei einem offiziellen Empfang oder privat an einer Festtafel fortgesetzt. Zum Siegestag schenkt man Kriegsveteranen Nelken. Jedes Jahr finden am Siegestag in vielen Städten Russlands (2011: 23) Militärparaden statt.
Fällt ein gesetzlicher Feiertag auf einen Dienstag oder einen Donnerstag, ist die Einrichtung eines arbeitsfreien Brückentags am Montag bzw. Freitag üblich, indem der vorhergehende Samstag bzw. der nachfolgende Sonntag im Gegenzug zu Arbeitstagen erklärt werden.
Moskau und St. Petersburg sind die kulturellen Zentren Russlands mit einer großen Anzahl kultureller Einrichtungen. Allein Moskau weist mehr als 120 Theater, fünf Opernhäuser, sechs professionelle Symphonieorchester sowie zahlreiche Museen und Galerien auf. Das Moskauer Bolschoi-Theater genießt Weltruf, die Eremitage in St. Petersburg und die Staatliche Tretjakow-Galerie in Moskau beherbergen weltbedeutende Kunstsammlungen. In anderen regionalen Zentren haben sich auch kulturelle Szenen entwickelt, so etwa in Nowosibirsk (Theater, Oper), Jekaterinburg (Theater, zeitgenössischer Tanz) und Nischni Nowgorod (zeitgenössische Kunst).
In Russland genießt die Literatur eine sehr große Wertschätzung. Die in Westeuropa üblichen und gültigen Ordnungsmuster der Poetik und Gattungslehre, wie auch literarische Epochenbezeichnungen werden aber in Russland anders, weil zeitversetzt und in anderer Funktion, verwendet. Der Romanik entsprach in der Kiewer Rus die „Periode der stilistischen Einfachheit“ (11. Jh.), der Gotik das „Zeitalter des ornamentalen Stils“ (12. und 13. Jh.), für die folgenden Jahrhunderte vom 14. bis zum 16. gibt es gebräuchliche ideologische und geopolitische Epochennamen („Periode der geistigen Auseinandersetzungen“ und „Moskauer Literatur“). Im 17. und 18. Jahrhundert führte die Nachahmung barocker Stilverfahren zu einem späten Gleichklang mit dem westeuropäischen Zeitstil.
Der aus der byzantinischen Geschichtsschreibung übernommene Grundbestand an geistlichen Texten und Gattungen legte die Grundlage der kirchenslawische Tradition fest, was im slawischen Mittelalter als Literatur und als literarischer Text galt. Es herrschte die Dominanz eines geistlich-kirchlichen Literaturbegriffs (d. h. Lesen und Schreiben – ähnlich wie in der Ikonenmalerei – zum Nutzen der Seele). Andererseits fehlten die ästhetische Funktion, Individualstil, Fiktionalität (Trennung von Wahrheit und Dichtung), literarische Gattungen im neuzeitlichen Sinn und ein moderner Autorenbegriff. Literatur mit nicht vorherrschend geistlicher Funktion im alten Russland (vor 1700) ist vergleichsweise wenig vertreten. Der literarische Übergang zur Neuzeit vollzog sich im Namen einer möglichst festen und unmittelbaren Anbindung Russlands an Westeuropa unter Peter dem Großen.
Zu Beginn des 18. Jahrhunderts erfüllte die Literatur vorrangig Erziehungs- und Repräsentationsfunktionen für den Staat. Gegen 1800 emanzipierte sich die literarische Kommunikation von den Ansprüchen des Hofes, der Bildungsinstitute sowie des Mäzenatentums. Russische Autoren konnten ihre Werke erstmals auf einem eigenen Buchmarkt veröffentlichen. Für Jahrzehnte dominierte nun das Genre des realistischen Gesellschaftsromans, der die Leser in Europa nachhaltig beeindruckte. Der russische realistische Roman entwickelte seine eigenen Verfahren zur Abbildung der Wirklichkeit und bildete Metastandpunkte bezüglich der destabilisierenden Wirkung westlicher Modernisierung auf traditionelle Lebensformen und gesellschaftliche Strukturen heraus.
Puschkin gilt als Begründer der modernen russischen Literatur. Weitere russische Schriftsteller von Weltrang sind: Michail Bulgakow, Fjodor Dostojewski, Nikolai Gogol, Maxim Gorki, Boris Pasternak, Alexander Solschenizyn, Lew Tolstoi, Anton Tschechow, Iwan Turgenew, der Exilant Vladimir Nabokov und Iwan Bunin, der erste russische Schriftsteller, der mit dem Nobelpreis für Literatur ausgezeichnet wurde.
1990 verzeichneten Bücher in Russland eine Auflagenstärke von insgesamt 1,6 Milliarden Büchern. 2004 waren es nur noch 562 Millionen. Auflagenstärkste Autorin war dabei Darja Donzowa mit 99 Bänden und einer Auflagenstärke von 18,1 Mio. Büchern.
Der russische Buchhändler-Verband beklagte im Jahr 2016 die gestiegenen Preise sowohl für die Produktion als auch für den Verkauf durch kleine Buchhändler mit Handelsgebühren. So gebe es in Moskau nur noch eine Buchhandlung pro 58.000 Einwohner; die 12 Millionen Einwohner Moskaus teilten sich 199 Buchhandlungen im Vergleich zu den 3 Millionen Einwohnern von Paris mit deren 700 Buchhandlungen.[360]
Auch auf dem Gebiet der Malerei leistete Russland einen großen Beitrag. Die Porträtmalerei war im 18. Jahrhundert sehr populär. Aber auch andere Stilrichtungen, wie Historienmalerei und religiöse Malerei wurden häufig verwendet. Gegen Ende des 19. Jahrhunderts kam die europäische Moderne, wie Impressionismus und Jugendstil, in abgeleiteter Form nach Russland.
Im Zusammenhang mit dem Impressionismus und der Russische Avantgarde sind Namen wie Wassily Kandinsky, Kasimir Malewitsch, Alexej von Jawlensky, Wladimir Tatlin, Michail Larionow und Natalja Gontscharowa zu erwähnen. Zu den großen russische Malern zählen außerdem Andrei Rubljow, Ilja Repin, Marc Chagall, Michail Wrubel, Walentin Serow, Wassili Surikow, Iwan Aiwasowski, Isaak Lewitan, zu den bedeutenden Landschaftsmalern gehören Nikolai von Astudin und viele mehr. In neuerer Zeit machen vor allem provokative Künstler und Künstlergruppen wie „Die blauen Nasen“ Furore, die international ausgezeichnet, von der russisch-orthodoxen Kirche und den Behörden aber immer wieder in die Schranken verwiesen werden.
Siehe auch: Liste russischer Maler, Peredwischniki, Mir Iskusstwa, Russische Avantgarde, Suprematismus, Kubofuturismus, Konstruktivismus (Kunst)
In Russland befinden sich 25 Welterbestätten, davon 14 als UNESCO-Weltkulturerbe (Stand 2013); darunter befinden sich die Altstädte und historische Zentren von Derbent, Jaroslawl, Sankt Petersburg, Weliki Nowgorod, Wladimir oder die Kreml von Kasan und Moskau sowie die Holzkirchen von Kischi Pogost.
Die frühe Architektur Russlands orientiert sich an der des Byzantinischen Reichs: frühe Sakralbauten orientieren sich wie die byzantinischen am Griechischen Kreuz, das von fünf Kuppeln gekrönt wird. Beispiele hierfür sind die Sophienkathedrale in Nowgorod, oder die Kirche Sankt Demetrios in Wladimir. Westeuropäische Einflüsse breiteten sich mit dem Barock aus. Barockeinflüsse (Russischer Barock) begannen sich Ende des 17. Jahrhunderts in Russland zu zeigen (Kirche der Gottesmutter-Ikone von Wladimir zu Kurkino in Moskau).
Ein eigenständiger russischer Stil hatte sich wahrscheinlich ursprünglich nur im Bereich der Holzbauten entwickelt, von denen aufgrund des Baumaterials aber keine Bauten erhalten sind, die älter als das 17. Jahrhundert sind. Die Kirchen, die daraus entstanden, zeichnen sich durch eine einfachere zentrale Anlage und einen großen oktogonalen Mittelturm aus. Diese wurden im Laufe der Zeit immer dekorativer ausgestaltet. Ein berühmtes Beispiel ist die Basilius-Kathedrale auf dem Moskauer Roten Platz von 1555. Ihren Durchbruch erreichte sie jedoch im von Zar Peter I. gegründeten Sankt Petersburg. Europäische Architekten wie Andreas Schlüter oder Domenico Trezzini kamen nach Russland, sie bauten Gebäude wie das Menschikow-Palais oder die Peter-und-Paul-Festung.
Architektur von Weltniveau erreichten die Baumeister unter Katharina II. (Bartolomeo Francesco Rastrelli). Die Paläste wie der Winterpalast in St. Petersburg, das Schloss Peterhof oder der Katharinenpalast zeigen an den Fassaden einen großen und gewaltigen Rokoko-Stil und sind im Inneren exorbitant luxuriös ausgestattet.
Mit dem Klassizismus, der in Russland ungefähr zur selben Zeit einsetzte wie im restlichen Europa, begannen erstmals originär russische Baumeister wie Iwan Jegorowitsch Starow eine herausragende Stellung einzunehmen. Die meisten Gebäude der Petersburger Innenstadt sind bis heute klassizistisch geprägt. Ein Paradebeispiel dafür ist die Rossistraße in Sankt Petersburg, benannt nach dem Architekten Carlo Rossi, deren Gesamtanlage einschließlich der Häuser einem streng geometrischen Gesamtmuster folgt. In den Sakralbauten wie der Isaakskathedrale allerdings mischen sich klassizistische und historistische Stilelemente.
Anfang des 20. Jahrhunderts waren avantgardistische Strömungen in der gesamten russischen Kultur stark. Nach der Oktoberrevolution konnten ihre Verfechter diese einige Jahre lang umsetzen. Beispielgebend ist hier El Lissitzky oder neuartige Prototypen für Wohnungsbau, Industriebau und für die öffentliche Verwaltung. Internationale Architekten wie Le Corbusier, Walter Gropius, Peter Behrens und Ludwig Mies van der Rohe konnten in Moskau bauen. Unter Stalins Herrschaft erfolgte jedoch schnell ein Rückschlag auf monumental gesteigerte klassische Muster. Der Zuckerbäckerstil begann vorherrschend zu werden, die Repräsentativität stand gegenüber künstlerischen Entwürfen klar im Vordergrund. In der spätsowjetischen Phase der 1970er-Jahre bis zum Zusammenbruch des Sowjetreiches entstanden in allen Teilrepubliken einzigartige, teils futuristische Bauwerke,[361] deren radikale Ästhetik und eigenwillige Formensprache im Gegensatz zur konformistischen Staatsarchitektur stand. Seit dem Zusammenbruch der Sowjetunion wird zunehmend ein historisierender Baustil modern, der Anknüpfungspunkte in der traditionellen russischen Architektur sucht. Beispiele hierfür sind neben vielen anderen Gebäuden die wiederaufgebaute Christ-Erlöser-Kathedrale in Moskau, oder die gleichnamige Kathedrale in Kaliningrad.
Die russische Musik reicht weit zurück. Ihre Ursprünge liegen im heidnischen Brauchtum der Ostslawen. Nach der Annahme des Christentums entwickelte sich zuerst die kirchliche Musik. Ursprünglich aus Byzanz gekommen, gewann sie schnell nationale russische Merkmale. Im 11. Jahrhundert bildete sich ein besonderer Typ des orthodoxen Kirchengesangs, der sogenannte Snamenny raspew heraus. Erst im 16. bis 17. Jahrhundert verbreitete sich das lyrische Volkslied. Einige Lieder sind weltberühmt, wie z. B. Lied der Wolgaschlepper, Kalinka, Katjuscha, Kosakenwiegenlied, Dubinuschka, Korobeiniki, Schwarze Augen.
Die Anfänge der russischen Kunstmusik begannen sich im 18. Jahrhundert zu entwickeln und standen seit Peter dem Großen unter Beeinflussung westeuropäischer Musik. Der wichtigste Komponist dieser Zeit war Dmytro Bortnjanskyj, in dessen Schaffen sowohl Kunstmusik wie auch die typisch russisch anzusehenden A-cappella-Gesänge der orthodoxen Kirchenmusik vertreten sind. Jewstignei Ipatowitsch Fomin, der bedeutendste Opernkomponist Russlands des späten 18. Jahrhunderts, war immer noch westlich geprägt. Wendungen aus der russischen Volksmusik tauchen erstmals verstärkt in den Opern und Orchesterstücken Michail Glinkas und Alexander Dargomyschskis auf, wodurch sie den Weg zu einer nationalrussischen Komponistenschule ebneten. Im Anschluss daran formierte sich aus fünf jungen Komponisten die sogenannte Gruppe der Fünf (Alexander Borodin, César Cui, Mili Balakirew, Modest Mussorgski, Nikolai Rimski-Korsakow), die es sich zur Aufgabe machte, gezielt die Eigentümlichkeiten russischer Volksmusik für Symphonien, Opern, Tondichtungen und Kammermusik nutzbar zu machen.
In Kontrast dazu entwickelte sich eine eher an westlicher Musik (besonders der deutschen Romantik) orientierte Gegenströmung, die durch Anton Rubinstein begründet wurde. Ihr gehörte auch der bedeutendste russische Komponist des 19. Jahrhunderts, Pjotr Tschaikowski, an, dessen Werke (Symphonien, Opern, Ballette, Kammermusikwerke) der russischen Musik erstmals auch im Ausland zu größerem Ansehen verhalfen. Die nachfolgenden Komponisten wie Anatoli Ljadow, Sergei Tanejew, Anton Arenski, Alexander Gretschaninow, Alexander Glasunow und Wassili Kalinnikow setzten in ihren Kompositionen vor allem auf eine aussöhnende Vereinigung des westlich-internationalen und des russisch-nationalen Stiles. Während Sergei Rachmaninow in seinen Klavierkonzerten und Symphonien den Stil Tschaikowskis eigenständig weiterentwickelte, hielt mit Alexander Skrjabin, Schöpfer eines eigenwilligen harmonischen Systems, erstmals die musikalische Moderne in Russland Einzug.
Der Expressionismus ist in der russischen Musik durch das Frühwerk Igor Strawinskis und Sergei Prokofjews repräsentiert. In den 1920er-Jahren experimentierten viele Komponisten mit neuartigen musikalischen Gestaltungsmitteln, so auch der junge Dmitri Schostakowitsch, dessen frühe Werke sich besonders durch satirischen Tonfall auszeichnen. Die meisten älteren Komponisten hielten dagegen an der Romantik fest, wie Glasunow, Reinhold Glière und Nikolai Mjaskowski, später dann auch Prokofjew. Ab Mitte der 1930er-Jahre wurde für russische Musiker auf Anordnung Stalins die Doktrin des Sozialistischen Realismus bindend, die avantgardistische Experimente verbot und eine „volksnahe“ Kunst forderte. Dieser Zwang lockerte sich erst nach Stalins Tod 1953 allmählich. Hauptrepräsentanten einer sowjetischen Musikkultur wurden im Anschluss neben Schostakowitsch vor allem Dmitri Kabalewski und der Armenier Aram Chatschaturjan. Seit etwa 1980 machen sich auch wieder die einst verpönten avantgardistische Elemente in russischen Kompositionen bemerkbar, so bei Edisson Denissow, Sofia Gubaidulina und Alfred Schnittke. Dagegen hielten Komponisten wie der gebürtige Pole Mieczysław Weinberg oder Boris Tschaikowski die Tradition in der Nachfolge Schostakowitschs aufrecht.
Neben der althergebrachten Unterhaltungsmusik aus der Zeit der Sowjetunion, der sogenannten Estrada, gibt es eine Reihe unterschiedlicher Genres russischer Popmusik. Als bedeutender russischer Liedermacher/Chansonnier des 20. Jahrhunderts wird der Dichter, Sänger und Schauspieler Wladimir Wyssozki angesehen, dessen Lieder größtenteils in den 1960er- und 1970er-Jahren entstanden. Zu Beginn der 1980er-Jahre und in der Zeit der Perestroika entwickelte sich in Russland eine vitale, russischsprachige Rockmusikszene, welche die gestandenen Bands wie Maschina Wremeni ergänzte. Als Galionsfigur dieser Jahre gilt gemeinhin der im Jahre 1990 verstorbene Frontmann von Kino, Wiktor Zoi, dessen Lieder und Texte für viele Bands der nachfolgenden Jahre prägend waren. Neben originären russischen Bands wie Kino, Ljube, Aquarium, DDT und Nautilus Pompilius, oder den Punkbands Graschdanskaja Oborona und Sektor Gasa wurde die Popkultur im Bereich der Musik stark vom internationalen Mainstream beeinflusst.
In den 1990er-Jahren etablierte sich in den kulturellen Zentren des Landes, aber insbesondere in St. Petersburg ein weitläufiger Underground, der bis heute das gesamte Spektrum der Musik abdeckt. Gegen Ende des Jahrhunderts startete auch das russische MTV. Während dieser Zeit wurde eine Vielzahl von Rockbands gegründet und aufgelöst, vor allem aber feierten die bereits in den 1980er-Jahren gegründeten Formationen große Erfolge. Auch die ersten Bands der Untergrundkultur konnten viele Zuhörer gewinnen, so z. B. Leningrad. Sehr bekannt wurde in dieser Zeit auch Semfira. Spätestens seit Beginn dieses Jahrzehnts hat auch russische Popsa bedeutende Marktanteile inne. Dabei handelt es sich um tanzbare Musik mit einem hohen Elektroanteil, die besonders Teenager zur Zielgruppe hat und sich musikalisch vollständig an international erfolgreichen Projekten orientiert (Walerija, VIA Gra). Das Duo t.A.T.u. ist die bislang einzige international erfolgreiche russische Popband. Ein weiteres, in der Zeit der Sowjetunion weitgehend an den Rand gedrängte Genre erlebt die letzten Jahre ebenfalls eine Renaissance – das russische Chanson. Ein populärer Star dieser Richtung ist der Sänger Michail Schufutinski.
Das Ballett hat in Russland eine lange Tradition und ist eine sehr beliebte Form der Unterhaltung. Peter I. lernte Ballett auf einer seiner Reisen nach Westeuropa kennen und war begeistert. An seiner Residenz gab es zwar auch Tanzvergnügungen, aber sie waren anders, folkloristischer, volksnaher. So wurden Ballettspezialisten aus Europa nach Russland engagiert. Damit begann die eindrucksvolle Entwicklung des russischen Balletts, deren Tänzer und Choreographen bald durch das Patronat der russischen Monarchie für das Bolschoi- und Mariinski-Ballett zu den führenden Europas aufstiegen. In der choreographischen Arbeit Marius Petipas, zu denen insbesondere Pjotr Iljitsch Tschaikowski die Musik lieferte, entstanden mit Der Nussknacker, Schwanensee und Dornröschen die klassischen Meisterwerke im romantischen Ballett in Russland.
Auf Initiative des Impresarios Sergei Pawlowitsch Djagilew wurden 1909 die wegweisenden Ballets Russes gegründet. Auf Tourneen in den Kulturhauptstädten Europas in Paris und London wurde die Kompagnie zum Fixpunkt der europäischen Kunstavantgarde. Das europäische Publikum geriet angesichts der, teils dem zeitgenössischen Faible für Folklore und Orientalismus, teils der revolutionären Neuerungen von Musik, Choreographie und Interpretation, wie beispielhaft in der Inszenierung von Petruschka durch Igor Strawinsky, Michel Fokine und Vaslav Nijinsky, in Begeisterungsstürme. Das russische Ballett hatte damit in seiner allgemeinen Entwicklung Frankreich als führende Ballettnation entthront. Russische Technik und russisches Repertoire waren nun allgemeine Synonyme des klassischen Balletts. Der Einfluss ging so weit, dass auch bekannte westliche Tänzerinnen (wie Alicia Markova) ihre Namen russifizierten, um Chancen auf ein Engagement zu verbessern.
Auch die weltweite Entwicklung des Balletts im 20. Jahrhundert wurde in der Emigration zahlreicher in Russland ausgebildeter Tänzer und Choreographen entscheidend geprägt. George Balanchine hatte fundamentalen Einfluss auf den choreographischen Stil im zeitgenössischen Ballett und Rudolf Nurejew initiierte mit der Wiederaufnahme des klassischen Repertoires die anhaltende Popularität der romantischen Ballette, die bis heute Standardwerke geblieben sind. Durch interpretatorischen Anspruch und technische Bravour setzen sie hier auch nach wie vor Maßstäbe.
Zwar leitete die weitere politische Entwicklung in der Sowjetunion auch im Ballett eine künstlerische Stagnation im Vergleich zu den Entwicklungen im modernen Tanz ein, jedoch blieb durch staatliche Ausbildung wie an der Waganowa-Ballettakademie und der finanziellen Förderung neuer Produktionen das hohe Niveau erhalten. Das sowjetische Repertoire wurde, wie in Sergei Prokofjews „Romeo und Julia“ und „Cinderella“ teilweise unmittelbar im Westen adaptiert. Die Entwicklung einer dramaturgischen Inszenierung eines sozialistischen Balletts wurde auf wirkungsvolle Weise in Juri Grigorowitschs Choreographie von „Spartakus“ umgesetzt, die weiterhin Höhepunkt im Ballettschaffen geblieben ist.
Russland brachte so große Tänzerpersönlichkeiten wie Anna Pawlowa, Tamara Platonowna Karsawina, Léonide Massine, Galina Ulanowa, Mikhail Baryshnikov, Natalja Romanowna Makarowa und Maja Plissezkaja hervor. Die heute wohl bekannteste Ballettgruppe ist das Russische Staatsballett mit bisher 20 Millionen Besuchern. Es wurde 1981 von Irina Tichimisowa gegründet und ist seit 1984 unter der Leitung von Wjatscheslaw Gordejew, Ex-Bolschoi-Star.
Auch in diesem Bereich gibt es staatliche Einflussnahme und regimekritische Kulturschaffende werden bedrängt: Im Juni 2017 rief der Regisseur Kirill Serebrennikow gar das Publikum auf zu bestätigen, dass es das Stück Ein Sommernachtstraum gesehen hatte; dies, um dem Irrsinn ein Ende zu bereiten, nachdem ein staatliches Komitee ihm vorgeworfen hatte, den für diese Produktion bewilligten Beitrag unterschlagen zu haben.[362][363]
Die faktisch wieder eingeführte Zensur nach dem russischen Überfall auf die Ukraine 2022 führte dazu, dass Werke und Autoren wie zum Beispiel Iwan Wyrypajew teils auf direkte Weisung nicht mehr aufgeführt, aber auch Inszenierungen abgesagt wurden, zum Beispiel Bulgakows Adam und Eva.[364]
Die russische Filmgeschichte begann bereits in der Epoche des Russischen Kaiserreichs mit Stummfilmpionieren wie Alexander Chanschonkow, Iwan Mosschuchin und Wera Cholodnaja. Sergei Eisenstein und Andrei Tarkowski, beide zur Sowjetzeit tätig, gehören zu den wichtigsten europäischen Filmregisseuren. Zahlreiche bemerkenswerte russische Filme und Regisseure blieben jedoch aufgrund des Ost-West-Konfliktes im Westen weitgehend unbekannt. In der Sowjetunion unterlag das Kino einer strengen ideologischen Zensur, innerhalb des erlaubten ideologischen Rahmens wurde jedoch eine beachtenswerte Talentförderung und staatliche Unterstützung des Kinogewerbes betrieben. Auch heute noch betrachten viele Russen die Sowjetzeit, die viele beliebte Schauspieler und Filme hervorbrachte, als den Höhepunkt der russischen Filmkunst und der Schauspielschule.
Trotz der postsowjetischen Krise der russischen Filmindustrie erreichten seit den 1990er-Jahren russische Filme gelegentlich internationale Erfolge: Zu nennen ist beispielsweise der oscarprämierte Streifen Die Sonne, die uns täuscht (1994) von Regisseur Nikita Michalkow, das Jugenddrama The Return – Die Rückkehr (2003) von Andrei Swjaginzew, der hierfür mit dem Goldenen Löwen bei den Internationalen Filmfestspielen von Venedig ausgezeichnet wurde, sowie die Fantasy-Verfilmung Wächter der Nacht – Nochnoi Dozor (2004), die zur kommerziell bislang erfolgreichsten russischen Filmproduktion wurde. Der wichtigste Filmpreis in Russland ist der Nika, welcher von der Russischen Akademie für Filmkunst verliehen wird. Zu den größten russischen Filmstudios gehören Goskino, Sowkino, Mosfilm, Lenfilm, Gorki Filmstudio (vormals Meschrabpom), sowie das Animationsstudio Sojusmultfilm.
Insgesamt ließ sich in Russland (im Gegensatz zu Europa) in den Jahren bis 2012 ein enormer Anstieg der Kinobesuche nachvollziehen. Bemerkenswert war, dass die russische Filmproduktion bei der beinahen Verdoppelung der Kinobesuche ihren – im Vergleich mit Europa überdurchschnittlich hohen – Marktanteil, der seit 2005 stets über einem Viertel an allen Kinobesuchen in Russland lag, halten konnte.
Die staatliche Aufsichtsbehörde überprüfte immer wieder Filme und der Propagandist Dmitri Kisseljow forderte schon 2018 gar eine Einschränkung der Meinungsfreiheit[365] aufgrund kritischer Filme, zudem wurden einzelne ausländische Produktionen überhaupt nicht gezeigt, bei anderen wurden Startdaten so gelegt, dass sie nicht mit patriotischen russischen Filmen konkurrieren.[366]
Video-Art ist im modernen Russland sehr beliebt. Russland ist einer der wichtigsten Märkte für YouTube.[367] Die beliebteste Episode aus der russischen Zeichentrickserie Mascha und der Bär hat über 3 Milliarden Aufrufe.[368][369][370] Besonders populär ist die Show +100500, die Video-Rezensionen für lustige Videos[371][372] und BadComedian beherbergt, der populäre Filme rezensiert.[373] Viele russische Filmtrailer wurden für Golden Trailer Awards nominiert.[374][375] Viele Videos von Nikolai Kurbatow, dem Begründer der Trailer-Poetik und der Dialog-Konstruktion des Trailers, wurden auf die großen YouTube-Kanäle hochgeladen, als Haupttrailer verwendet und ins Buch der Rekorde eingetragen.[376][377][378][379]
In Russland hat Sport einen relativ hohen Stellenwert, was man auf die umfassende sportliche Förderung in der UdSSR zurückführen kann (vgl. Sport in der Sowjetunion). 2008 besaß Russland 2687 Stadien ab 1500 Sitzplätze und mehr als 3762 Schwimmbäder und 123.200 Sportanlagen. Der Breitensport ist bedeutend, so liegt die Zahl der Mitglieder in den Sportvereinen bei 22,6 Millionen Menschen, darunter 8,1 Millionen Frauen.[380] Die beliebteste Mannschaftssportart der Russen ist Fußball (vgl. Fußball in Russland), das einen Boom erlebt – begünstigt durch starke finanzielle Sponsorenförderung aus der Wirtschaft. Eishockey (vgl. Eishockey in Russland) ist die zweitbeliebteste Mannschaftssportart. Basketball ist die drittbeliebteste Mannschaftssportart, aber auch Schach und Tennis erfreuen sich großer Beliebtheit. Russland hat bereits zahlreiche Weltklassesportler hervorgebracht. Besonders in den Sportarten Leichtathletik, Wintersport, Eiskunstlauf, Turnen/Gymnastik und Gewichtheben dominieren russische Sportlerinnen und Sportler. Aus keiner Nation stammen mehr aktuelle und ehemalige Schachweltmeister und Großmeister als aus Russland.
Russland nahm inklusive der Teilnahmen als Teil der Sowjetunion bisher 19-mal an Olympischen Sommerspielen und 17-mal an Olympischen Winterspielen teil. Bislang konnten Sportler aus Russland und der Sowjetunion 1911 olympische Medaillen bei den Sportwettbewerben erringen und belegen damit Platz zwei des ewigen Medaillenspiegels. 1980 war die damals sowjetische Hauptstadt Moskau zum ersten Mal Ausrichter der Olympischen Sommerspiele. Der Schwarzmeer-Kurort Sotschi richtete 2014 zum ersten Mal die Olympischen Winterspiele in Russland aus. Darüber hinaus ist Russland häufig Austragungsort von internationalen Wettbewerben wie Welt- und Europameisterschaften. So trug Russland 2018 zum ersten Mal die Fußball-Weltmeisterschaft aus, die unter anderem in Moskau, Sankt Petersburg, aber auch in der Exklave Kaliningrad stattfand. Im Motorsport stellt Russland mit Witali Petrow einen ehemaligen und mit Daniil Kwjat einen aktiven Formel-1-Piloten. Auch die DTM und die Superbike-WM waren schon in Moskau zu Gast.
Eine Domäne stellt Russland auch im Eisspeedwaysport dar und die russischen Eisspeedway-Piloten stellten Eisspeedway-Weltmeister in Serie. Die Städte Togliatti und Balakowo sind die Zentren des russischen Speedway-Motorrad-Rennsports.
Im Boxen zählt das Land ebenfalls weltweit zur Spitze. Seit Ende der Sowjetunion gewannen russische Amateurboxer ab 1996 bei Olympischen Spielen 10× Gold, 6× Silber und 15× Bronze. Zusammen mit 14× Gold, 19× Silber und 18× Bronze bei Olympischen Spielen aus Sowjetzeiten, liegt Russland derzeit mit insgesamt 84 Olympiamedaillen auf Platz 2 des ewigen Medaillenspiegels hinter den USA mit 114 Medaillen und vor Kuba mit 73 Medaillen (Stand nach den Olympischen Spielen 2016). Von 1993 bis 2017 gewannen russische Boxer und Boxerinnen zudem 45 Goldmedaillen bei Weltmeisterschaften.
Rugby Union erfreut sich ebenfalls zunehmender Beliebtheit. Die russische Nationalmannschaft qualifizierte sich bisher für zwei Rugby-Union-Weltmeisterschaften (2011 und 2019), erreichte jedoch noch nicht die K.O.-Phase. Russland ist einer der Teilnehmer bei der Rugby-Union-Europameisterschaft und trifft dort auf andere aufstrebende Nationalmannschaften. Vor allem Spiele gegen den politischen Rivalen Georgien stoßen auf großes Interesse und gelten als eine Art „David gegen Goliath“, auch aufgrund Russlands negativer Gewinnbilanz gegen den südlichen Nachbarn. Seit 2021 spielen Russland und Rumänien den Kiseleff Cup aus; diese Trophäe ist nach dem Herzog Pawel Kisseljow benannt, einem Russen, der bei der Ausarbeitung der ersten Verfassung für die beiden Fürstentümer Walachei und Moldau (heutiges Rumänien und Republik Moldau) entscheidend mitgewirkt hatte.[381] Als Heimatstadion dient das Zentralstadion in Sotschi.
Die Welt-Anti-Doping-Agentur (WADA) wirft Russland vor, seit Jahren systematisches, staatlich gesteuertes Doping zu betreiben; die Manipulationen würden vom Sportministerium „geleitet, kontrolliert und überwacht“, vom Inlandsgeheimdienst FSB unterstützt und beträfen fast alle Sportarten, insbesondere im russischen Leichtathletikverband herrsche eine „tief verwurzelte Betrugskultur“. Bei den Leichtathletik-Weltmeisterschaften 2013 in Moskau seien zahlreiche positive Doping-Proben russischer Sportler ausgetauscht worden, aber auch bei den Olympischen Winterspielen in Sotschi 2014 und bei den Schwimmweltmeisterschaften 2015 in Kasan.[382][383] Im November 2015 entzog die WADA der nationalen russischen Anti-Doping-Agentur RUSADA die Akkreditierung;[384] wenige Tage später schloss der Weltleichtathletikverband (IAAF) die russischen Leichtathleten bis auf weiteres von allen internationalen Wettkämpfen – also auch von den Olympischen Spielen in Rio de Janeiro – aus.[385] Auch die russischen Gewichtheber durften nach einer entsprechenden Entscheidung des Weltverbands IWF nicht in Rio antreten.[386]
Auch die russische sportwissenschaftliche Forschung ist hiervon betroffen. Während die Trainingswissenschaft lange von den Erfolgen der Sportlerinnen und Sportler durch systematische Planung und Entwicklung wie z. B. der Periodisierung des sportlichen Trainings profitierte, ist der Innovationsvorsprung in den letzten Jahren geschrumpft, da die Methoden bei gleichzeitiger Reduktion des Dopings sich als weniger erfolgreich erwiesen.[387] Eine Langzeitanalyse der führenden sowjetischen/russischen trainingswissenschaftlichen Zeitschrift Theorie und Praxis der Körperkultur (Moskau) zeigte, dass die in der Zeitschrift verwendete Literatur immer älter wurde und die Zeitschrift heute mit einem Durchschnittsalter der Literatur von 15 Jahren sich um mehr als zehn Jahre gegenüber den 1980er-Jahren verschlechtert hat.[388] Inzwischen wird auch über die Einbeziehung von verdeckten Dopingmethoden publiziert, da sich die Nanotechnologie noch immer weitgehend den Kontrollen der WADA entzieht.[389]
Die Welt-Anti-Doping-Agentur (WADA) hatte im Dezember 2019 die russische Anti-Doping-Agentur RUSADA nach diversen Dopingskandalen – unter anderem wurden Daten von Athleten manipuliert – für vier Jahre gesperrt und einen Olympia-Bann für die russische Mannschaft ausgesprochen.[390] Das Verfahren zum russischen Staatsdoping soll im Herbst 2020 vor dem Internationalen Sportgerichtshof (CAS) verhandelt werden. Der CAS setzte als Termin für die Anhörung den 2. bis 5. November an.[390] Die RUSADA hat dagegen Einspruch beim CAS eingelegt.
G6:
Deutschland |
Frankreich |
Vereinigtes Königreich |
Italien |
Japan |
Vereinigte Staaten
G7: Kanada |
(G8: Russland – Mitgliedschaft suspendiert)
Volksrepublik China |
Indien |
Kasachstan |
Kirgisistan |
Pakistan |
Russland |
Tadschikistan |
Usbekistan
Staaten mit Beobachterstatus:
Iran |
Mongolei
Dialogpartner:
Afghanistan |
ASEAN |
GUS
Interessierte Staaten:
Belarus |
Nepal |
Turkmenistan
Mitgliedstaaten:
Armenien |
Belarus |
Kasachstan |
Kirgisistan |
Russland
Ehemalige Mitgliedstaaten:
Aserbaidschan |
Georgien |
Usbekistan
Beobachterstaaten:
Afghanistan |
Serbien
Mitgliedstaaten:
Belarus |
Kasachstan |
Kirgisistan |
Russland |
Tadschikistan |
Usbekistan
Ehemaliger Mitgliedstaat:
Usbekistan
Beobachterstaaten:
Armenien |
Moldau |
Ukraine
Mitgliedstaaten:
Armenien |
Belarus |
Kasachstan |
Kirgisistan |
Russland
Beobachterstaaten:
Kuba |
Moldau |
Usbekistan
Albanien |
Armenien |
Aserbaidschan |
Bulgarien |
Georgien |
Griechenland |
Moldau |
Rumänien |
Russland |
Serbien |
Ukraine |
Türkei
Kiewer Rus (862–1169) |
Fürstentum Wladimir-Susdal (1125–1321) |
Großfürstentum Moskau (1321–1547) |
Zarentum Russland (1547–1721) |
Russisches Kaiserreich (1721–1917) |
Provisorische Regierung (Russland) (1917) |
Sowjetrussland (1917–1922) |
Russische Sozialistische Föderative Sowjetrepublik (1917–1922) |
Sowjetunion (1922–1991) |
Russland (seit 1991)
58.6570.116666666667Koordinaten: 59° N, 70° O

Bangkok (thailändisch กรุงเทพมหานคร, Krung Thep Maha Nakhon, [kruŋ tʰêːp máʔhǎː náʔkʰɔːn],  anhören?/i; kurz กรุงเทพฯ, Krung Thep, [kruŋ tʰêːp]; historische Schreibung zum Teil auch Bankok[2]) ist seit 1782 die Hauptstadt des Königreichs Thailand. Sie hat einen Sonderverwaltungsstatus und wird von einem Gouverneur regiert. Die Hauptstadt hat 8,249 Millionen Einwohner (Volkszählung 2010) und ist die mit Abstand größte Stadt des Landes. In der Bangkok Metropolitan Region (BMR), der größten Metropolregion in Thailand, leben insgesamt 14,566 Millionen Menschen (Volkszählung 2010).[1]
Die Stadt ist das politische, wirtschaftliche und kulturelle Zentrum Thailands mit Universitäten, Hochschulen, Palästen und über 400 Wats (buddhistische Tempelanlagen und Klöster) sowie wichtigster Verkehrsknotenpunkt des Landes. In Bangkok ist auch die Wirtschafts- und Sozialkommission für Asien und den Pazifik (UNESCAP) beheimatet. Mit mehr als 17 Millionen ausländischen Touristen war Bangkok im Jahr 2013 die meistbesuchte Stadt der Welt, bevor sie 2014 wieder von London abgelöst wurde und seither auf Platz 2 rangiert.[3] Seit 2016 steht Bangkok mit über 20 Millionen Touristen jährlich auf Platz 1 der meistbesuchten Städte der Welt.[4][5]
Die Zeit zu UTC beträgt +7 Stunden. Der Zeitunterschied zu Mitteleuropa beträgt +6 Stunden im Winter und +5 Stunden im Sommer, da es in Thailand keine Sommerzeit gibt.
Der zeremonielle Name der Stadt Bangkok in Thai lautet in transkribierter Form Krung Thep Maha Nakhon Amon Rattanakosin Mahinthara Yutthaya Mahadilok Phop Noppharat Ratchathani Burirom Udom Ratchaniwet Maha Sathan Amon Phiman Awatan Sathit Sakkathattiya Witsanukam Prasit ( anhören?/i). Es ist die alte Thai-Bezeichnung der Stadt und mit 169 lateinischen Buchstaben der längste Ortsname einer Hauptstadt weltweit.
In thailändischer Schrift lautet der Name (139 Zeichen ohne Leerzeichen):
„กรุงเทพมหานคร อมรรัตนโกสินทร์ มหินทรายุธยามหาดิลก ภพนพรัตน์ราชธานีบุรีรมย์ อุดมราชนิเวศน์ มหาสถานอมรพิมาน อวตารสถิต สักกะทัตติยะ วิษณุกรรมประสิทธิ์“
„Stadt der Devas, große Stadt [und] Residenz des heiligen Juwels Indras [Smaragd-Buddha], uneinnehmbare Stadt des Gottes, große Hauptstadt der Welt, geschmückt mit neun wertvollen Edelsteinen, reich an gewaltigen königlichen Palästen, die dem himmlischen Heim des wiedergeborenen Gottes gleichen, Stadt, die von Indra geschenkt und von Vishvakarman gebaut wurde.“
Devas (thailändisch Thep) sind in der hinduistischen Mythologie eine Kategorie von 33 göttlichen Wesen, die gemeinsam mit dem Gott Indra den Himmel auf der Spitze des Bergs Meru bewohnen. Weil die Devas als geflügelte Wesen dargestellt werden, wird Krung Thep in westlichen Texten oft mit „Stadt der Engel“ übersetzt.[6]
Bevor der Ort im Jahr 1782 zur Hauptstadt erhoben wurde, war sein Name schlicht Bang Kok (บางกอก,  anhören?/i). Bang bezeichnet einen Ort an einer Wasserstraße, Kok leitet sich möglicherweise von Makok ab, der thailändischen Bezeichnung für die Früchte von Spondias pinnata („Gelbe Balsampflaume“)[7][8] oder Elaeocarpus hygrophilus. Anderen Theorien zufolge kommt es von Ko ‚Insel‘ oder Khok ‚Hügel‘.[9] Dieser Name tauchte in Europa zum ersten Mal auf einer portugiesischen Landkarte von 1511 auf.
Auch nachdem sie zur Hauptstadt ausgebaut worden war und einen ihrer Bedeutung als royales, religiöses und kosmologisches Zentrum des Reichs (Mandala) entsprechenden, viel klangvolleren Namen bekommen hatte, wurde die Stadt in den europäischen Sprachen weiterhin als Bangkok bezeichnet.[10]
Thailänder verwenden dagegen gewöhnlich die Kurzform Krung Thep. Die offizielle Bezeichnung, beispielsweise auf Autokennzeichen, lautet Krung Thep Maha Nakhon.
Im Jahre 1989 vertonte die thailändische Rockgruppe Asanee-Wasan in ihrem Album Fak tong („Kürbis“) den Song Krung Thep Mahanakhon, dessen Text ausschließlich aus dem vollständigen zeremoniellen Namen Bangkoks besteht. Viele Thailänder nutzen seitdem dieses Lied, um sich den langen Namen besser merken zu können.
Das Siegel von Bangkok zeigt die Gottheit Indra auf Erawan, dem mythologischen Elefanten, der in einigen Abbildungen auch drei Köpfe haben kann. In seiner Hand hält Indra einen Blitz. Das Siegel basiert auf einer Zeichnung des Prinzen Narisara Nuwattiwong.
Die Stadt liegt an der Nahtstelle der Indochinesischen und der Malaiischen Halbinsel am Mae Nam Chao Phraya (Chao-Phraya-Fluss) und nördlich des Golfs von Thailand durchschnittlich fünf Meter über dem Meeresspiegel. Der Chao Phraya hat eine Breite von etwa 400 Metern.
Das westlich des Chao Phraya gelegene Gebiet wird Thonburi genannt und war bis 1971 eine eigenständige Stadt, während der östlich gelegene Teil vor mehr als zweihundert Jahren nur ein kleines Dorf war, hauptsächlich bewohnt von chinesischen Händlern. Das Stadtgebiet hat eine Fläche von 1.565,2 Quadratkilometern, die gesamte Metropolregion eine Bodenfläche von 7.761,5 Quadratkilometern.
Für Bangkok ist die folgende Landnutzung dokumentiert.
Bangkok ist unterteilt in 50 Distrikte (Khet, manchmal fälschlicherweise auch Amphoe genannt) und diese sind weiter in 169 Khwaeng, den Tambon vergleichbar, eingeteilt. Es folgen die Khets in deutscher Transkription und in thailändisch:
Bang Bon (บางบอน), Bang Kapi (บางกะปิ), Bang Khae (บางแค), Bang Khen (บางเขน), Bang Kho Laem (บางคอแหลม), Bang Khun Thian (บางขุนเทียน), Bang Na (บางนา), Bang Phlat (บางพลัด), Bang Rak (บางรัก), Bang Sue (บางซื่อ), Bangkok Noi (บางกอกน้อย), Bangkok Yai (บางกอกใหญ่), Bueng Kum (บึงกุ่ม), Chatuchak (จตุจักร), Chom Thong (จอมทอง), Din Daeng (ดินแดง), Don Mueang (ดอนเมือง), Dusit (ดุสิต), Huai Khwang (ห้วยขวาง), Khan Na Yao (คันนายาว), Khlong Sam Wa (คลองสามวา), Khlong San (คลองสาน), Khlong Toei (คลองเตย), Lak Si (หลักสี่), Lat Krabang (ลาดกระบัง), Lat Phrao (ลาดพร้าว), Min Buri (มีนบุรี), Nong Chok (หนองจอก), Nong Khaem (หนองแขม), Pathum Wan (ปทุมวัน), Phasi Charoen (ภาษีเจริญ), Phaya Thai (พญาไท), Phra Khanong (พระโขนง), Phra Nakhon (พระนคร), Pom Prap Sattru Phai (ป้อมปราบศัตรูพ่าย), Prawet (ประเวศ), Rat Burana (ราษฎร์บูรณะ), Ratchathewi (ราชเทวี), Samphanthawong (สัมพันธวงศ์), Sai Mai (สายไหม), Saphan Sung (สะพานสูง), Sathon (สาทร), Suan Luang (สวนหลวง), Taling Chan (ตลิ่งชัน), Thawi Watthana (ทวีวัฒนา), Thonburi (ธนบุรี), Thung Khru (ทุ่งครุ), Watthana (วัฒนา), Wang Thonglang (วังทองหลาง) und Yan Nawa (ยานนาวา).
Siehe auch: Distrikte von Bangkok
Bangkok hat weder eine klar definierte Innenstadt noch einen zentralen Geschäftsbezirk. Diese Funktion ist vielmehr auf mehrere Bezirke verteilt. Der älteste Teil der Stadt ist die Rattanakosin-Insel im Bezirk Phra Nakhon, um den Großen Palast, Sanam Luang, Wat Ratchabophit und Sao Ching Cha („Riesenschaukel“). In der zweiten Hälfte des 19. Jahrhunderts entwickelte sich dann das „Chinesenviertel“ um die Yaowarat- und die Charoen-Krung-Straße (damals „New Road“, die erste asphaltierte Straße Bangkoks) im Bezirk Samphanthawong zum zentralen Geschäftsviertel der Stadt. In den 1950er und 1960er Jahren siedelten sich dann Banken und Großkanzleien entlang der Silom-, Surawong- und Si-Phraya-Straße im Bezirk Bang Rak an. Seit den 1990er Jahren sind im Bezirk Sathon und entlang der Witthayu-Straße (Bezirk Pathum Wan) vorwiegend teure Bürohochhäuser und entlang der Rama-I- und Phloenchit-Straße (ebenfalls Pathum Wan) große Einkaufszentren und Luxushotels entstanden. Durch die Einrichtung der beiden Massenverkehrssysteme Skytrain (BTS) und U-Bahn (MRT) hat sich die Ausbreitung der Gebiete mit zentraler Geschäftsfunktion noch verstärkt. Attraktiv sind jetzt fast alle Gebiete, die mit einem der beiden Verkehrsmittel erreichbar sind. Das gilt insbesondere für das Gebiet um die Sukhumvit-Straße (Bezirke Khlong Toei und Watthana) und den mittleren Abschnitt der Ratchadaphisek-Straße (Bezirke Phaya Thai und Huai Khwang).[12]
Bangkok befindet sich in der tropischen Klimazone. Die Jahresdurchschnittstemperatur beträgt 28,4 °C, die jährliche Niederschlagsmenge 1.498 Millimeter im Mittel. Hauptregenzeit ist während des Monsuns zwischen Mai und Oktober, in einzelnen Stadtgebieten muss besonders im September und Oktober mit Überschwemmungen gerechnet werden. Der meiste Niederschlag fällt im September mit 344 Millimetern im Mittel, der wenigste im Januar mit durchschnittlich neun Millimetern.
Die durchschnittlichen Temperaturen liegen das ganze Jahr über zwischen 26,1 °C und 30,4 °C. Die mittlere Tagestemperatur beträgt maximal 34,9 °C, minimal 20,8 °C bei hoher Luftfeuchtigkeit. Laut der World Meteorological Organization ist Bangkok damit im Jahresdurchschnitt die heißeste Stadt der Welt.
Wärmster Monat ist der April mit maximal 35 °C und minimal 26 °C mittlere Tagestemperatur. Der kälteste Monat ist in der Gegend um Bangkok der Dezember mit maximal 31 °C und minimal 21 °C Tagesmitteltemperatur. Die Trockenzeit geht von Dezember bis März. März und April sind die heißesten Monate in Bangkok.
Bangkok hat mit erheblichen Umweltproblemen zu kämpfen. Eine dichte Wolke aus Abgasen liegt permanent über der Stadt. Experten haben festgestellt, dass in den Hauptverkehrsstraßen die Luftverschmutzung bereits gesundheitsschädliche Werte erreicht hat. Seit dem Bau der Hochhäuser ist die Ventilation der Straßen nicht mehr gewährleistet, so dass die Konzentration der Giftstoffe dramatisch ansteigt. Saisonal mischt sich Rauch von Brandrodung mit den Abgasen und verursacht Krankheiten bei zahlreichen Bewohnern.[15]
Über Bronchitis, Asthma oder Erschöpfung klagt bereits jeder siebte Bewohner. Besonders Belastete, wie Motorrad-Boten, Tuk-Tuk-Fahrer, Verkehrspolizisten und Straßenhändler, tragen häufig Atemschutzmasken.
Probleme bereitet Bangkok auch die Wasserversorgung. So besitzt die Stadt kein zentrales Wasserversorgungsnetz. Auch der Aufbau eines Abwassersystems steht erst am Anfang. Bis in die 1990er Jahre leiteten die Industrie und private Haushalte das Abwasser zentral ohne jegliche Säuberung in den Mae Nam Chao Phraya (Chao-Phraya-Fluss).
Die Abfälle verschmutzen die Luft und vergiften häufig jegliches Leben in den Gewässern. Zahlreiche Fabriken in der Metropolregion – darunter auch in vielen Wohngebieten – dürfen eigene Brunnen zur geregelten Wasserversorgung errichten, was zu einem kontinuierlichen Absinken des Grundwasserspiegels beiträgt.
Die Grundwasserabsenkung trug mindestens bis in die 1970er-Jahre erheblich zu einer Landsenkung bei und führte zeitweise zu einer Absenkung um mehr als 10 cm pro Jahr. Die jährliche Absenkung findet weiterhin statt, hat sich aber auf bis zu 2 cm pro Jahr verringert. Die derzeitige Senkung wird vor allem auf die hohe Gebäudelast zurückgeführt.[16]
Bangkok bildete ursprünglich nur ein kleines Fischerdorf am östlichen Ufer des Chao Phraya. Im Jahr 1511 wurde es zum ersten Mal auf einer portugiesischen Landkarte verzeichnet.[8] Um 1680 gab es südlich des Dorfes nur drei bewohnte Plätze: ein Zollhaus, die von den Holländern 1622 angelegte Faktorei namens Fort Amsterdam sowie den Ort Ban Vat.[17] Noch während der geschichtlichen Epoche des Königreichs Ayutthaya entwickelte sich der Ort zu einem ansehnlichen Handelshafen und bedeutenden Haltepunkt an der Wasserroute zur Hauptstadt.[8]
Der Ursprung des heutigen Bangkok liegt in der Kleinstadt Thonburi, heute Teil der Hauptstadt, am westlichen Ufer des Chao Praya. General Taksin machte Thonburi im Jahr 1772, nachdem die Hauptstadt des Königreiches Ayutthaya 1767 im Krieg mit Birma weitgehend zerstört worden war, zur neuen Hauptstadt. Zehn Jahre später verlegte der neue König Rama I., Begründer der bis heute regierenden Chakri-Dynastie, den Regierungssitz auf das östliche Ufer und begann damit das Gebiet namens Rattanakosin, mit dem damals vor allem von Chinesen bewohnten Dorf Bang Kok, nach dem Vorbild der früheren Residenzstadt zur Hauptstadt auszubauen.
Der offizielle Name der Stadt lautet seit damals in der Kurzform Krung Thep ( กรุงเทพฯ?/i). Das ist jedoch nur eine Kurzform des vollständigen Namens, des weltweit längsten Städtenamens (siehe oben Name der Stadt). Westliche Händler und Reisende verwendeten stattdessen den Namen des Dorfes Bangkok, woraus die heute international bekannte Bezeichnung wurde.
Rattanakosin wurde durch einen Kanal, den Khlong Lot, zu einer künstlichen Insel in einer Biegung des Chao Phraya in deren Zentrum der neue Königspalast und der königliche Tempel, der Wat Phra Kaeo mit dem Smaragd-Buddha (Phra Kaeo), dem Nationalheiligtum Thailands, errichtet wurden.
In jener Zeit war die ganze Stadt von einem dichten Netz von Kanälen (Khlongs) durchzogen. Der Verkehr spielte sich zum Großteil auf diesen Khlongs ab. Selbst die Märkte fanden auf dem Wasser statt („Schwimmende Märkte“). Straßen gab es kaum. Damals wurde Bangkok auch manchmal als das Venedig des Ostens bezeichnet. Die meisten Khlongs wurden ab der Mitte des 19. Jahrhunderts nacheinander zugeschüttet um Raum für den stetig zunehmenden Straßenverkehr und die wachsende Stadt zu schaffen.
1863 wurde die erste gepflasterte Straße der Stadt Thanon Charoen Krung (wörtlich übersetzt „Straße zur Vergrößerung der Hauptstadt“; westliche Ausländer nannten sie New Road – „Neue Straße“) an Stelle eines früheren Elefanten-Trampelpfads fertiggestellt. Während der Regentschaft König Chulalongkorn (Rama V.) (regierte 1868–1910) entstanden eine Eisenbahnlinie die Bangkok mit dem Norden des Landes verband, Straßenbahnlinien für den innerstädtischen Verkehr, eine große Anzahl neuer Straßen und die Mehrzahl der oft von europäischen Stilen beeinflussten Regierungsgebäude.
Zu Beginn des 20. Jahrhunderts wuchs die Stadt über ihre früheren Grenzen hinaus nach Norden und Osten. Einen weiteren Wachstumsschub, insbesondere für die westlich des Flusses gelegenen Stadtteile, bedeutete die Einweihung der ersten Brücke, der Memorial Bridge, über den Chao Praya im Jahr 1932. Während des Zweiten Weltkrieges war Bangkok für einige Jahre von japanischen Streitkräften besetzt und wurde ab 1944 von den Alliierten bombardiert. Nach dem Ende des Krieges erholte sich die Stadt aber rasch und wuchs beständig weiter.
Bis zur Mitte des 20. Jahrhunderts waren die meisten Khlongs bereits zugeschüttet und durch Boulevards und Straßen ersetzt worden. Während dieser Zeit entstanden auch die Fernverkehrsstraßen in alle Himmelsrichtungen, wie die Sukhumvit-Straße. Ab den 1960er und 1970er Jahren wurden so viele Häuser gebaut und Stadtautobahnen ausgebaut wie nie zuvor. In den 1970er-Jahren war Bangkok Schauplatz einschneidender politischer Ereignisse: zunächst des Volksaufstandes gegen die Militärdiktatur im Oktober 1973, drei Jahre später dann des Massakers an linken Studenten und Demonstranten auf dem Campus der Thammasat-Universität. Mit dem Wirtschaftsboom der 1980er Jahre (siehe „Tigerstaaten“) setzte eine weitere neue Entwicklung ein, die zur Errichtung einer großen Zahl von Hochhäusern führte und das Stadtbild nachhaltig veränderte. Die Zahl der Bewohner stieg zugleich rasant und machte Bangkok schließlich zu einer der größten Metropolen der Welt. Im Mai 1992 gab es in der Hauptstadt abermals Massenproteste gegen die damalige Regierung, die im sogenannten „Schwarzen Mai“ brutal niedergeschlagen wurden.
Zu Beginn des 21. Jahrhunderts leben in Bangkok über sechs Millionen Menschen, in der Metropolregion sogar über zehn Millionen. Wirtschaftlich erholt sich die Stadt zusehends vom Zusammenbruch am Ende des Booms der 1990er Jahre, was nicht zuletzt auch in neuen Bauvorhaben seinen Ausdruck findet. Eines der größten städtischen Probleme stellt der Straßenverkehr dar. Auch der Ausbau des öffentlichen Verkehrsnetzes mit Bangkok Metro und Bangkok Skytrain konnte die Situation bislang nur minimal entspannen.
Nach mehreren politischen Krisen im Land seit 2006 war Bangkok im April und Mai 2010 Schauplatz von blutigen Unruhen, die international für Aufsehen sorgten.
Ende Oktober 2011 überschwemmte die größte Flutkatastrophe seit 50 Jahren mehrere Bezirke Bangkoks, viele Bezirke mussten evakuiert werden.
Ende 2013 und Anfang 2014 war Bangkok abermals Schauplatz politisch motivierter Unruhen. Die Oppositionsbewegung gab im Januar 2014 das Ziel aus, das öffentliche Leben in Bangkok komplett lahmzulegen („Shutdown Bangkok“). Der Konflikt endete im Mai 2014 mit einem neuerlichen Militärputsch.
Den Daten der Volkszählung von 2000 zufolge waren 99 % der Einwohner von Bangkok thailändische Staatsbürger. 94,5 % waren Buddhisten, 4,1 % Muslime und 1 % Christen. 37,3 % sind nicht in Bangkok geboren, sondern aus einer anderen Provinz zugezogen.[18]
Bangkok ist ein Beispiel für ein dynamisches, jedoch nicht geplantes städtisches Wachstum. 1947 überschritt die Einwohnerzahl erstmals die Millionengrenze. 1960 lebten in Bangkok bereits 2,1 Millionen Menschen. Zwischen 1970 (3,1 Millionen) und 2000 (6,3 Millionen) hat sich die Einwohnerzahl verdoppelt. 2010 lebten in der Hauptstadt nach Angaben des Nationalen Statistikamtes (NSO) 8,2 Millionen Menschen.[1] Das entspricht einem Wachstum zwischen 2000 und 2010 von 2,6 % pro Jahr.
Die Bevölkerungsdichte betrug 5.270 Einwohner pro Quadratkilometer (in München, der am dichtesten besiedelten Gemeinde Deutschlands, waren es zum Vergleich 4.275). Die Stadt ist weitaus größer als alle anderen Städte Thailands. Sie hatte 2010 bereits 8-mal so viele Einwohner wie die nächstgrößere Stadt Chiang Mai (953.000 Einwohner), die Bangkok Metropolitan Region (BMR) sogar 15-mal so viel wie das nächstgrößere Ballungsgebiet, welches auch durch Chiang Mai gebildet wurde (953.000 Einwohner).
Da die Hauptstadt um ein Vielfaches größer ist als alle anderen Großstädte Thailands und da hier die wichtigen politischen und wirtschaftlichen Entscheidungen gefällt werden, ist Bangkok eine Primatstadt. Sie ist weltweit einer der am extremsten ausgeprägten Fälle einer Primatstadt: Im Jahr 2000 hatte sie 40-mal so viele Einwohner wie die damals zweitgrößte Stadt des Landes, Nakhon Ratchasima.[19] Das Pro-Kopf-Bruttoinlandsprodukt war 2006 zehnmal höher als in der ärmsten Provinz des Landes (Bangkok: 319.322 Baht, Mae Hong Son 33.231 Baht).[20] Während die Hauptstadtregion sich wirtschaftlich entwickelte, profitierten weite Teile des Landes nicht oder hatten gar einen Rückgang des Bruttoinlandsproduktes zu verzeichnen. Durch diese Vorrangstellung hatte die Stadt in den letzten Jahrzehnten einen immensen Wanderungszuwachs.
In der Metropolregion wurden bei der Volkszählung 2000 etwa 10,2 Millionen Menschen gezählt. Bei der Zählung im Jahr 2010 waren es bereits 14,6 Millionen. Das entspricht einem Wachstum zwischen 2000 und 2010 von 3,7 % pro Jahr. Die Bevölkerungsdichte betrug 1.877 Einwohner pro Quadratkilometer. Zur BMR gehören die Stadt Bangkok und die sie umgebenden Provinzen Nakhon Pathom, Nonthaburi, Pathum Thani, Samut Prakan und Samut Sakhon.
Die folgende Übersicht zeigt die Einwohnerzahlen nach dem jeweiligen Gebietsstand. Bis 1910 handelt es sich um Schätzungen, von 1919 bis 2010 um Volkszählungsergebnisse des Nationalen Statistikamtes Thailand. Die Einwohnerzahlen beziehen sich auf die eigentliche Stadt ohne den Vorortgürtel.
Ein großes Problem für die Metropole Bangkok ist es, die vielen in den letzten Jahren zugezogenen Menschen, vor allem Landflüchtlinge, ausreichend zu versorgen. Für zahlreiche Menschen mussten Wohnungen gebaut werden.
In Khlong Toei befindet sich der Hafen von Bangkok. Der Bezirk ist außerdem bekannt für das größte Slumgebiet Bangkoks. Der Nordwesten des Bezirks erfüllt dagegen Funktionen eines zentralen Geschäftsviertels Bangkoks. Hier sitzen die thailändische Börse, das Königin-Sirikit-Kongresszentrum, Banken, Hotels und Einkaufszentren.
Die zunehmende Nachfrage nach Bauland steigerte die Wohnungs- und Grundstückspreise erheblich. Deshalb werden viele Siedlungen für die weniger Verdienenden am Stadtrand errichtet. Die öffentlichen Einrichtungen (Krankenhäuser und Schulen) sind unzureichend. Müll und Abwasser werden nicht mehr ausreichend entsorgt, der Grundwasserspiegel sinkt durch den gestiegenen Verbrauch immer schneller.
Vor allem die Menschen in den Siedlungen sind durch Infektionserkrankungen wie Cholera, Diarrhöe und Typhus gefährdet, die durch unzureichende hygienische Bedingungen verbreitet werden. Hinzu kommen Atemwegs- und Hauterkrankungen aufgrund der giftigen Emissionen der zahlreichen Industriebetriebe und des Autoverkehrs. Einige Industriebetriebe haben jedoch in den letzten Jahren den Großraum Bangkok aus Kostengründen, da hier ein höherer Mindestlohn zu zahlen ist und die hohen Grundstückspreise sowie die Infrastruktur außerhalb der Hauptstadt verbessert wurde, verlassen.
Ein weiteres Problem in Bangkok ist der zunehmende Verkehr. Das Straßennetz ist trotz zahlreicher Autobahnen völlig überlastet. Außerdem verfügt Bangkok mit einem relativ kleinen U-Bahn- und Hochbahnnetz nur über wenige Massentransportmittel, hat aber eines der größten Busnetze weltweit. 2021 wurden zwei S-Bahn-Linien eröffnet, um die Vororte besser mit dem Stadtzentrum zu verbinden, außerdem gibt es einen neuen Hauptbahnhof um den Schienenpersonenfernverkehr und den ÖPNV besser miteinander zu verknüpfen. Durch die zahlreichen Autos kommt es in der Stadt zu sehr großer Luftverschmutzung (hohe Ozon- und Kohlenstoffmonoxidwerte).
Das heutige Bangkok entstand 1972 aus der Zusammenlegung der alten Provinz Bangkok (offiziell Phra Nakhon) und der Provinz Thonburi, die die westlich des Chao Phraya gelegenen Stadtteile des heutigen Bangkok umfasste.
Bangkok ist ein besonderes Verwaltungsgebiet. Anders als die 76 Provinzen Thailands (Changwat) hat sie einen direkt gewählten Gouverneur. Gouverneurswahlen finden im Regelfall alle vier Jahre statt. Der Gouverneur steht der hauptstädtischen Lokalverwaltung (Bangkok Metropolitan Administration, BMA) vor. Diese ist eigenverantwortlich für den öffentlichen Nahverkehr, Straßen, Stadtplanung, Wohnungsbau, Abfallwirtschaft, Umweltschutz und öffentliche Ordnung zuständig.
Ab 2009 war M.R. Sukhumbhand Paribatra von der Demokratischen Partei der Gouverneur von Bangkok. Er wurde im März 2013 mit 46,2 % gegenüber Pongsapat Pongcharoen von der Pheu-Thai-Partei mit 39,7 % wiedergewählt. Im Oktober 2016 wurde er vom Vorsitzenden der Machthabenden Militärjunta Prayut Chan-o-cha abgesetzt und durch den Polizeigeneral Aswin Kwanmuang ersetzt.[21] Seit der Gouverneurswahl am 22. Mai 2022 ist Chadchart Sittipunt amtierender Gouverneur von Bangkok.[22][23] Chadchart Sittipunt konnte die Wahl in allen 50 Distrikten Bangkoks für sich entscheiden. Er trat als Unabhängiger an und gewann die Wahl mit einem Erdrutschsieg mit 52,65 % (1,38 Millionen) aller Stimmen, ein neuer Höchstrekord.[24]
Die Stadtverwaltung wird von einem Stadtrat (Bangkok Metropolitan Council) kontrolliert. Nach der Kommunalwahl von 2022 gehören ihm 50 Mitglieder an, davon 20 Abgeordnete der Pheu-Thai-Partei, 14 der Move-Forward-Partei, 9 der Demokratischen Partei, drei der Rak-Krungthep-Partei, zwei der Phalang-Pracharat-Partei und zwei der Thai-Srang-Thai-Partei.[25]
Die Stadtverwaltung (BMA) ist nur für die eigentliche Stadt Bangkok und nicht für die über die Stadtgrenzen hinausgewachsene Agglomeration Groß-Bangkok (Greater Bangkok Area, GBA) oder gar die noch größere Metropolregion Bangkok (Bangkok Metropolitan Region, BMR) zuständig.[26]
Bangkok unterhält mit folgenden Städten Partnerschaften:[27]
Neben Städtepartnerschaften unterhält Bangkok noch zahlreiche Städtefreundschaften und Städtekontakte.
In der Stadt gibt es über 400 Wats (buddhistische Tempelanlagen). Der bedeutendste ist der Wat Phra Kaeo (Wat Phra Sri Rattana Satsadaram), der den sogenannten „Smaragd-Buddha“ beherbergt und landesweit eine hohe Verehrung genießt.
Zusammen mit dem Großen Palast, dem Wat Pho (Wat Phra Chetuphon), dem ältesten und größten Tempel Bangkoks, sowie dem Wat Mahathat, der eine der großen buddhistischen Universitäten (Mahachulalongkornrajavidyalaya-Universität) Südostasiens beherbergt, bildet der Wat Phra Kaeo auf der Rattanakosin-Insel das historische Zentrum der Stadt.
In Bangkok befinden sich zudem das thailändische Nationalmuseum, die Nationalgalerie, die Nationalbibliothek und das Nationaltheater.
Das Nationaltheater in Bangkok befindet sich auf dem Gelände des alten Wang Na, des Palastes des Zweiten Königs von Thailand (des Uparat) an der Thanon Na Phra That. Vor dem Osteingang befindet sich eine Statue von Phra Pinklao, der als jüngerer Bruder von König Mongkut von diesem zum vorletzten Uparat ernannt wurde.
Das Gebäude wurde nach einem Brand von 1960 bis 1965 in seiner gegenwärtigen Form aufgebaut: das T-förmige Gebäude zeigt eine Mischform thailändischer und westlicher Baukunst. Über dem Haupteingang des Theaters befindet sich ein Relief, das den Schutzgott der Künste, den hinduistischen Gott Ganesha darstellt. Im Gebäude finden Aufführungen des klassischen thailändischen Tanzes statt. Der Saal besitzt eine originelle Trapezform. 2006 wurden Renovierungsarbeiten durchgeführt.
Das königliche Theater Sala Chalermkrung (สาลา-เฉลิม-กรุง) war ein Geschenk von König Prajadhipok (Rama VII.) an sein Volk und wurde am 2. Juli 1933 zum 150. Rattanakosin-Jubiläum eröffnet. Es war das erste Theater Thailands mit Klimaanlage und auch das erste Kino Thailands. Seit 2006 wird hier der thailändische Maskentanz Khon regelmäßig aufgeführt, der Szenen aus dem Epos Ramakien enthält.[30]
Im neu erbauten Joe Louis Theatre auf dem Gelände des Suan-Lum-Night-Bazaar wird das Ramayana-Epos als thailändisches Puppenspiel aufgeführt. Vor der Aufführung werden die traditionellen Figuren vorgestellt. Drei Künstler agieren mit bunten, aufwändig bestickten Stabpuppen, die vom Familienoberhaupt Sakorn Yanghiawsod (Joe Louis) seit den 1950er Jahren angefertigt werden.
Das Nationalmuseum beherbergt eine beachtliche Sammlung von Artefakten und Kunstgegenständen von der thailändischen Bronzezeit bis zur neuesten Zeit. Der ganze Komplex besteht aus historischen Gebäuden im thailändischen Stil, befand sich hier doch bis zur Zeit von König Chulalongkorn der Wang Na, der Palast des Uparat, des sogenannten Vize-Königs von Siam.
Die Bootshäuser am Khlong Bangkok Noi, einer Dependance des Nationalmuseums direkt hinter der Pinklao-Brücke, beherbergen die Königsbarken (Royal barges). Diese kunstvoll geschnitzten und verzierten Barken werden nur bei der königlichen Barkenprozession benutzt.
Die Nationalgalerie (National Gallery Museum) liegt gegenüber dem Nationalmuseum an der Thanon Chao Fa. Dort sind alte und zeitgenössische Gemälde bedeutender thailändischer Künstler ausgestellt.
Jim Thompsons Thai-Haus am Khlong Saen Saep in der Soi Kasemsan 2, einer Quergasse der Rama-I.-Straße (Thanon Phra Ram 1), ist ein Ensemble aus mehreren, einander zugeordneten Holzhäusern im altthailändischen Stil. Dieses wurde von Jim Thompson (* 1906), jenem legendenumwobenen Mann geschaffen, der nach Ende des Zweiten Weltkrieges die thailändische Seidenindustrie wiederbelebt und weltbekannt gemacht hat, und der 1967 unter mysteriösen Umständen spurlos verschwunden ist. Heute ist der Komplex ein Museum, in dem Thompsons exquisite Sammlung asiatischer Kunst zu sehen ist. Nur etwa 500 Meter Fußweg entfernt liegt das Kunst- und Kulturzentrum Bangkok.
Der Wang Suan Pakkad (Suan-Pakkad-Palast) ist ein Komplex von acht Thai-Häusern inmitten eines Gartens an der Thanon Si Ayutthaya. Früher die Residenz von Prinz Chumbhot, beherbergt er eine bedeutende Sammlung asiatischer Antiquitäten. Besonders sehenswert ist dabei der „Lackpavillon“, ein kleiner Holzpavillon aus der Zeit von König Narai von Ayutthaya (1656–1688), der bis zum Jahr 1959 im Wat Ban Kling bei Ayutthaya als Kuti diente. Er ist ausgeschmückt mit Wandbildern in Lai-Rot-Nam-Technik (Schwarzgoldlack) aus der späten Ayutthaya- oder frühen Rattanakosin-Periode.
Im Bangkok Doll Museum (Puppenmuseum) in der Soi Ratchataphan hinter der Thanon Ratchaprarop werden Puppen aus einheimischer Fabrikation ausgestellt. Im Ban Kamthieng, einem 200 Jahre alten Thai-Haus im Lan-Na-Thai-Stil im Garten der Siam Society in der Thanon Asok Montri ist eine Sammlung von Arbeitsgeräten thailändischer Bauern und Fischer zu sehen.
Die Queen’s Gallery an der Thanon Ratchadamnoen nahe der Phan-Fa-Lilat-Brücke zeigt regelmäßig auf vier Etagen Ausstellungen bekannter und weniger bekannter zeitgenössischer thailändischer Künstler. Oft sind diese Ausstellungen von Königin Sirikit gesponsert.
Dieses bedeutendste Wahrzeichen Bangkoks am Ufer des Chao Phraya besteht aus über 100 Gebäuden in verschiedenen architektonischen Stilen. Das gesamte Gelände mit einer Fläche von mehr als 200.000 Quadratmetern wird von einer 1,9 Kilometer langen zinnenbewehrten Mauer umfasst.
Ein besonderes Kleinod dieses Ensembles ist der Wat Phra Kaeo (Tempel des Smaragd-Buddha), ein Glanzstück thailändischer Kunst. Zu seinen Kostbarkeiten gehört der Smaragd-Buddha, die am meisten verehrte Buddhastatue Thailands.
Weitere Attraktionen dieses großen Palastes sind die Amarin-Winichai-Thronhalle, die Dusit-Maha-Prasat-Thronhalle und der große Chakri-Palast. Weiter dazu zählen der „Royal Thai Decorations and Coin Pavillon“ mit einer ständigen Ausstellung königlicher Insignien, Preziosen, Medaillen und Münzen sowie anderer Zahlungsmittel, die Anfang des 11. Jahrhunderts in Umlauf waren.
Vor dem Großen Palast befindet sich der Sanam Luang (auch Phramen-Ground, also das „Phra-Meru“-Feld genannt), ein weitläufiger, von Tamarindenbäumen umsäumter Paradeplatz, der seit der Gründung Bangkoks für die Kremation von Mitgliedern der königlichen Familie, aber auch für andere öffentliche Zeremonien benutzt wird, wie etwa die königliche Pflugzeremonie im Monat Mai. Um den Platz herum gruppieren sich mehrere Prachtbauten: das Amt der Schönen Künste (Fine Arts Department) mit der Silpakorn-Kunsthochschule, die Thammasat-Universität und das Nationalmuseum, das Nationaltheater, das Justizministerium sowie der Lak Müang (Stadtpfeiler), das spirituelle Zentrum der historischen Stadt.
Wat Pho („Tempel des liegenden Buddha“) ist eine weitläufige Tempelanlage direkt südlich des Grand Palace. Dort befindet sich die riesige, mit Blattgold überzogene Statue eines liegenden Buddha – 46 Meter lang und 15 Meter hoch – seine Fußsohlen sind mit Perlmutt eingelegt. Wat Pho war einst auch die erste öffentliche Bildungseinrichtung des Landes. Der Tempel ist außerdem berühmt für seine traditionelle Thai-Massage.
Wat Arun („Der Tempel der Morgendämmerung“) ist ein beeindruckender Tempelbau und das Wahrzeichen Bangkoks auf dem anderen Ufer des Chao Praya, gegenüber dem Grand Palace. Seine etwa 75 Meter aufragende Pagode ist mit Porzellan-Kacheln überzogen und funkelt in der Sonne.
Wat Traimit („Tempel des Goldenen Buddha“) ist ein Heiligtum am Ende der Thanon Yaowarat in „Chinatown“, in der Nähe des Bangkoker Hauptbahnhof Hualampong. Dort thront ein drei Meter hoher Buddha aus fünfeinhalb Tonnen massivem Gold.
Wat Benchamabophit („Marmortempel“) liegt an der Thanon Si Ayutthaya, neben dem Chitralada-Palast, der Residenz von König Bhumibol Adulyadej. Dieser ist einer der neuesten Tempel Bangkoks. Er wurde während der Herrschaft von König Rama V. (1868–1910) aus weißem Carrara-Marmor errichtet. Auffällig sind Elemente der Sakralarchitektur Europas, zum Beispiel bunte Glasfenster. Im Wandelgang (Phra Rabieng) rund um den Ubosot befindet sich eine Sammlung von Buddha-Statuen aus Bronze der verschiedensten Kunststile Thailands.
Wat Suthat an der Thanon Bamrung Mueang ist bekannt für seine erlesenen Wandgemälde aus dem 19. Jahrhundert, die noch anlässlich des 200-jährigen Jubiläums der Stadt Bangkok mit finanzieller Hilfe der Bundesrepublik Deutschland restauriert worden waren. Die Riesenschaukel vor dem Tempel, Sao Ching Cha genannt, wurde vor langer Zeit für brahmanische Rituale verwendet. Einige Geschäfte in der Umgebung bieten buddhistische Devotionalien feil.
Wat Saket („Der Goldene Berg“): Das Interessanteste an diesem Tempel ist der aus dem 19. Jahrhundert stammende sogenannte „Goldene Berg“. Der goldene Chedi, der einen künstlichen Hügel mit seiner 87 Meter hohen vergoldeten Pagode krönt, beherbergt Buddha-Reliquien. Von dort kann die Altstadt von Bangkok besichtigt werden.
Weitere bedeutende Tempel im Zentrum von Bangkok sind Wat Mahathat am Rand des Sanam Luang Felds, der die Haupt-Universität der buddhistischen Mahanikai-Glaubensgemeinschaft, die Mahachulalongkornrajavidyalaya-Universität beherbergt, Wat Ratchanatdaram an der Thanon Ratchadamnoen hinter dem Rama-III-Memorial-Park mit seinem Loha Prasat (einer Pagode, die aufgrund ihres Baumaterials „Eisen-Palast“ genannt wird), Wat Ratchabopit, ein Tempel an der Thanon Ban Mo mit einer Mischung von einheimischen und europäischen Stilelementen sowie Wat Intharawihan in der Thanon Wisutkasat mit einer 32 Meter hohen stehenden Buddhafigur.
Zu einer umfangreichen Übersicht siehe auch Liste buddhistischer Tempel in Bangkok.
Wat Phra Kaeo
Wat Arun
Wat Benchamabophit
Wimanmek-Palast
Lak Müang (Schrein der Stadtsäule), das Bangkoker „Stadtheiligtum“, liegt an der Südostecke des Sanam Luang Feldes. Es beherbergt den von König Rama I. (1736–1809) gelegten Grundstein Bangkoks – eigentlich eine Säule. Diese steht im Ruf, Wünsche zu erfüllen. Den ganzen Tag lang werden von den Gläubigen Tänzerinnen dafür bezahlt, mit ihren Tänzen die dort ansässigen Stadtgeister zu betören und ihnen so Gesundheit und Wohlstand abzuhandeln.
Der Wimanmek-Palast (das himmlische Palais) – es ist das größte Teakholz-Gebäude der Welt – liegt hinter dem Parlament. Es beherbergt auf drei Stockwerken 81 Zimmer, Säle und Vorzimmer und ist mit Erinnerungsstücken des Königshauses von Ende des 19. Jahrhunderts möbliert. Das Teakholz ist an zahlreichen Stellen mit Blattgold überzogen.
Sehenswert sind auch der Chao Phraya und die noch erhalten gebliebenen Kanäle von Bangkok (Khlongs), die noch bis zur Mitte des 20. Jahrhunderts weite Teile der Stadt durchzogen. Für den Bau von Straßen wurden viele dieser alten Verkehrswege zugeschüttet. Zusammen mit dem Chao Phraya, dem „Fluss der Könige“, zeigen sie anschaulich, wie das Leben und Treiben auf dem Wasser im Wesentlichen schon seit einigen Jahrhunderten kaum verändert abläuft.
Sehenswürdigkeiten am Chao Phraya sind die Phra-Phutthayotfa-Brücke (umgangssprachlich Thai „Saphan Phut“, englisch „Memorial Bridge“), die erste Brücke zwischen Bangkok und Thonburi, eingeweiht zum 150. Jahrestag der Gründung Bangkoks, und die Rama-VIII.-Brücke, die erst im Mai 2002 eingeweiht wurde.
Seit 1941 steht das Siegesdenkmal auf einem heute sehr verkehrsreichen Platz mit Kreisverkehr.
Der Lumphini-Park ist der größte Park im Zentrum von Bangkok. Dort werden jeden Morgen „Tai-Chi“-Übungen praktiziert. Der Park ist ummauert und enthält einen künstlichen See, der mit mietbaren Ruder- und Tretbooten befahren werden kann. Der 576.000 Quadratmeter große Lumphini-Park wurde bereits in den 1920er Jahren von König Vajiravudh (Rama VI.) auf königlichem Grundbesitz geschaffen.
Im Dusit-Zoo, Bangkoks ältestem zoologischen Garten neben der Royal Plaza, sind die meisten bekannten afrikanischen und asiatischen Säugetiere und Vögel und noch viele weitere Tierarten zu sehen. Auf dem Areal befinden sich Cafés und ein künstlich angelegter See mit Flächen zur Erholung. Das Gelände wird von zwei Kanälen durchflossen, wovon einer im Herbst Austragungsort eines berühmten Floßwettbewerbs anlässlich des Loi-Krathong-Festes ist.
Sehenswert ist auch der König-Rama-IX.-Park, ein etwa 80 Hektar großer Park und botanischer Garten in der Soi 103 (Udomsuk) der Thanon Sukhumvit. Er wurde im Jahre 1987 anlässlich des 60. Geburtstages von König Bhumibol Adulyadej eröffnet.
Der Rommaninat-Park an der Thanon Maha Chai war bis Mitte der 1990er Jahre das Stadtgefängnis von Bangkok. Die Stadtverwaltung siedelte das Gefängnis vor die Tore der Stadt aus und machte einen schönen Park daraus. Er bietet für viele Anwohner eine allabendliche Sport- und Erholungsmöglichkeit.
Blick auf den Lumphini-Park
Lumphini-Park
Chatuchak-Park
Wachirabenchatat Park, Chatuchak Distrikt
In zwei großen und mehreren kleinen über Bangkok verteilten Stadien gibt es die Möglichkeit, den thailändischen Nationalsport Muay Thai, besser bekannt unter dem Namen Thaiboxen, mit all seinen Ritualen anschauen zu können. Sowohl im Lumpini- als auch im Ratchadamnoen-Stadion sind einige der besten Kämpfer des Landes zu sehen. Die Veranstaltungen finden die ganze Woche abwechselnd in beiden Stadien statt. Abgesehen vom Kampf im Ring findet zusätzliche Unterhaltung durch das Wetten auf den Ausgang der Kämpfe und durch Musikbegleitung statt, die von einem traditionellen Ensemble (Pi Phat) gespielt wird, das auf der Pi oder „Thai-Oboe“ basiert.
Eine andere beliebte Sportart ist das traditionelle „Sepak Takraw“. Hierbei handelt es sich um eine Art Ballspiel, bei dem ein geflochtener Rattanball mit allen Teilen des Körpers – außer mit den Händen – so lang wie möglich in der Luft gehalten werden muss. Professionelle Teams spielen auf dem Sanam-Luang-Feld vor dem Großen Palast in Bangkok gegeneinander. Eine weitere Version dieser Sportart ist das Netztakro, das im Nationalstadium und im Hua Mak-Stadion gespielt wird.
Ebenfalls ein traditioneller Sport in Thailand ist der Drachenkampf, bei dem ein symbolischer Kampf der Geschlechter in der Luft stattfindet. Hierbei kämpfen die großen „männlichen“ Drachen, genannt Chula, gegen die kleineren, „weiblichen“ Pakpao. Die riesigen Chulas müssen von einem ganzen Team Männer gelenkt werden. Diese bunten Veranstaltungen finden im März und April auf dem Sanam-Luang-Feld statt, weil dann jeden Nachmittag der dazu notwendige Wind aufkommt.
Neben den traditionellen, für Thailand typische Sportarten, wird in Bangkok auch Fußball gespielt. Im Rajamangala-Nationalstadion trägt die thailändische Fußballnationalmannschaft fast alle ihre Heimspiele aus. Daneben gibt es fünf Fußballvereine, die in der höchsten Liga des Landes, der Thai Premier League spielen.
Zudem sind in Bangkok Galopprennen sehr populär. Hierbei ist das Wetten auf den Ausgang der Rennen legal. Es gibt in Bangkok sogar zwei Galopprennbahnen: der „Royal Bangkok Sports Club“ und der „Royal Turf Club of Thailand“.
Staatliche Feiertage und große religiöse Feste werden in Bangkok besonders prunkvoll begangen: Zum Geburtstag der Königin oder des Königs (5. Dezember) finden Paraden in den geschmückten Straßen statt. Bei großen Festen werden sogar die Königlichen Barken zu Wasser gelassen. Auch das chinesische Neujahrsfest ist Anlass zu dreitägigen Feierlichkeiten in Chinatown.
Wisakha Bucha, das größte buddhistische Fest, wird im Wat Phra Keo und auf dem Sanam Luang begangen. Bereits ab acht Uhr ziehen 30 bis 40 liebevoll dekorierte Wagen mit Statuen, die Szenen aus dem Leben von Buddha darstellen, durch die Thanon Rachdamnoen zum Königspalast.
Während der kühlen Jahreszeit von Mitte Februar bis Ende März finden auf dem Sanam Luang Drachenwettkämpfe statt. Zur Pflugzeremonie auf dem Sanam Luang Mitte Mai strömen Bauern aus dem ganzen Land nach Bangkok. Während der Songkran-Feiern werden in Bangkoks Straßen wahre Wasserschlachten ausgetragen. Zu dieser Zeit führen zahlreiche Absperrungen zum Verkehrschaos, vor allem in der Altstadt; in Chinatown ist es hingegen ruhig.
In Bangkok gibt es Restaurants für praktisch jeden Geschmack. Alle wichtigen europäischen, nah- und fernöstlichen Geschmacksrichtungen sind in vielen Feinschmecker-Restaurants vertreten. Die höchste Restaurant-Dichte haben wohl die Gebiete, in denen sich die Touristen hauptsächlich aufhalten, allen voran die Thanon Sukhumvit mit ihren zahlreichen Nebenstraßen. Reiseführer behaupten, in diesen Gebieten sei man nie weiter als 50 Meter vom nächsten Restaurant entfernt.
Dennoch gibt es einige erwähnenswerte Besonderheiten in Bangkok:
siehe auch: Thailändische Küche
Große Einkaufszentren konzentrieren sich vor allem in der unteren Thanon Sukhumvit (Sukhumvit-Straße), der Thanon Silom (Silom-Straße) und besonders am Siam Square. Hier liegen das MBK, das Siam Discovery Center, Zen, Central World und das Siam Paragon nebeneinander, in dem es neben allen erdenklichen Luxusmarken wie Ferrari und Lamborghini auch ein riesiges begehbares Aquarium gibt.
In den Einkaufszentren sind Geschäfte, Büros, Restaurants, Kinos und Kaufhäuser untergebracht. Die Einkaufspaläste, die zu den größten der Welt zählen, liegen an den Ausfallstraßen außerhalb des Zentrums, beispielsweise Seacon Square und Seri Center in der Thanon Srinagarindra weit östlich der Innenstadt, sowie der Future Park im Stadtteil Rangsit, nördlich des Flughafens Don Mueang.
In der Millionenstadt Bangkok haben einige Märkte mit ländlichem Charakter überlebt, auf denen frisches Obst und Gemüse, Fisch und Fleisch angeboten wird. Die legendären schwimmenden Märkte gibt es allerdings nur noch außerhalb der Metropole. Auf den meisten Märkten Bangkoks werden vor allem Textilien und Drogerieartikel verkauft, aber auch Pflanzen und Souvenirs.
In Bangkok günstig angeboten werden Silber- und Niellowaren, Puppen und Masken, Holzschnitzereien, Abreibungen von Tempelreliefs auf dünnem Reispapier, Bronze-Artikel, Baumwolltextilien, Sonnenschirme, Fächer und vieles mehr. Der Handel mit Antiquitäten ist in Thailand seit 1989 verboten. Deshalb lebt eine ganze Branche von der Produktion teilweise täuschend echter Fälschungen.
Laut einer Studie aus dem Jahr 2014 erwirtschafte der Großraum Bangkok ein Bruttoinlandsprodukt von 307 Milliarden US-Dollar in Kaufkraftparität. In der Rangliste der wirtschaftsstärksten Metropolregionen weltweit belegte er damit den 35. Platz. Das BIP pro Kopf liegt bei 19.705 US-Dollar (KKP). In der Stadt waren knapp 7 Millionen Arbeitskräfte beschäftigt.[31]
Die Stadt ist eines der bedeutendsten Wirtschafts- und Transportzentren in Südostasien. Bangkok ist sowohl Verkehrsknotenpunkt als auch Hafenstadt, über diese Stadt laufen 90 Prozent des Außenhandels; dort konzentrieren sich Industrie und Verwaltung. Die Stadt erwirtschaftete 2006 nach Angaben des Nationalen Statistikamtes (NSO) 28,0 % des Bruttoinlandsproduktes (BIP) des Landes; die Bangkok Metropolitan Region (BMR) 43,7 %. Das BIP Bangkoks betrug 2,190 Billionen Baht (Thailand: 7,816 Billionen). Es lag damit um das 261-fache über dem BIP der wirtschaftlich schwächsten Provinz Thailands, Mae Hong Son (BIP 2006: 8,406 Milliarden Baht).[20]
Die Thailändische Börse (Stock Exchange of Thailand) hat ihren Sitz in Bangkok. Der Vorgänger wurde 1962 in privater Form einer limited partnership gegründet und 1963 als Bangkok Stock Exchange zur limited company. Seit dem 1. Januar 1991 trägt die Börse den heutigen Namen. Außerdem befinden sich in der Stadt das Hauptquartier der Wirtschafts- und Sozialkommission für Asien und den Pazifik der Vereinten Nationen (UN) sowie weitere UN-Stellen. Zahlreiche Weltkonzerne besitzen Büros oder Fabriken in Bangkok. Dazu gehören unter anderem Toyota, Unilever, Procter & Gamble, Philips, Sony, Compaq und Tesco.
Fertiggerichte, Holz und Textilien sind Bangkoks wichtigste Exportwaren. Die Stadt befindet sich in einer Tiefebene, in der überwiegend Reis angebaut wird, der dann in den Reismühlen Bangkoks, einem der wichtigsten Industriezweige, weiterverarbeitet wird. Weitere bedeutende industrielle Produkte sind unter anderem Lebensmittel und Kraftfahrzeuge sowie Textilien, Zement, Schmuck, Solarzellen und Solarmodule. Ferner gibt es hier eine rege Musik- und Filmindustrie.
Ölraffinerien und Werften gibt es in Bangkok zahlreiche. Die Stadt ist als Schmuck- und Juwelenhandelszentrum bekannt. Sie ist das weltweite Zentrum für die Aufarbeitung minderwertiger und die Herstellung synthetischer Steine. Relativ gering sind die im eigenen Land geförderten Saphire und Rubine, das meiste wird importiert.
Tourismus ist eine wichtige Einnahmequelle. Die Khaosan Road, in der Nähe des Palastkomplexes, ist der Treffpunkt für Rucksacktouristen aus aller Welt. Am anderen Ende des touristischen Spektrums verfügt Bangkok mit dem Hotel Oriental und dem Peninsula über Luxushotels, die seit Jahren zu den Besten der Welt gezählt werden.
Nach Angaben der Staatlichen Tourismusbehörde (TAT) besuchten 2006 Bangkok 36,2 Millionen Gäste. Das entspricht einem Wachstum von 3,83 % gegenüber 2005. Unter den Touristen waren 23,8 Millionen Thais (+5,45 %) und 12,4 Millionen Ausländer (+0,84 %). Die Zahl der Personen, die in einem Hotel, Gäste- oder Apartmenthaus übernachteten und der Polizei gemeldet wurden, lag 2006 bei 13,9 Millionen (+3,56 %). Darunter waren 3,2 Millionen Thais (+11,25 %) und 10,7 Millionen Ausländer (+1,46 %). Die meisten ausländischen Touristen in Bangkok kamen aus folgenden Ländern: Japan (1.495.912), Volksrepublik China (1.357.387), USA (682.083), Singapur (618.853), Großbritannien (494.990), Australien (442.982), Hongkong (442.632), Taiwan (433.269), Malaysia (370.188), Indien (361.156), Deutschland (343.922) und Südkorea (305.185).[32]
Obwohl Prostitution wie in vielen anderen Ländern offiziell illegal ist, stellt das Gewerbe auch in Bangkok sowohl einen wichtigen Wirtschaftsfaktor als auch ein großes Problem dar. Sextouristen kommen vor allem aus Japan, Malaysia, Singapur, Taiwan, Australien, Europa, den arabischen Staaten und den USA nach Thailand. Die bekanntesten und berüchtigtsten Rotlichtbezirke in Bangkok sind Patpong, Nana Plaza und Soi Cowboy.
In Bangkok finden sich auch viele für die Wirtschaft des Landes arbeitende Institute, wie das Rice Research Institute.
Die Stadt ist wichtigster Knotenpunkt des Straßen- und Eisenbahnnetzes in Thailand.
Bangkok ist Sitz eines für Südostasien bedeutenden internationalen Flughafens. Bis September 2006 war dies der Bangkok International Airport (Don Mueang), 22 Kilometer nördlich des Stadtzentrums.
Die Eröffnung des Suvarnabhumi Airport – der sich etwa 30 km östlich der Hauptstadt in der Gemeinde Bang Phli der Provinz Samut Prakan befindet und eine Passagierkapazität von 45 Millionen Fluggästen pro Jahr hat – fand baubedingt erst am 28. September 2006 statt. Der neue Suvarnabhumi Airport hat den Flughafencode „BKK“ vom alten Don Mueang Airport übernommen, dieser wurde vorerst nur noch von Charterflügen und Privatmaschinen angesteuert. Im Jahr 2011 hatte der Flughafen „BKK“ rund 47,5 Millionen Passagiere abgefertigt.[33] Die Fahrt von Suvarnabhumi mit dem Taxi in die Innenstadt dauert in der Regel etwa 30 bis 60 Minuten, wobei bei extremen Verkehrsaufkommen zu Hauptverkehrszeiten auch bis zu drei Stunden eingeplant werden können. Seit 2010 besteht mit dem Suvarnabhumi Airport Railway Link (SARL) auch eine Schnellbahn-Verbindung zwischen Innenstadt und Flughafen.
Am 25. März 2007 wurde der alte Flughafen Don Mueang mit dem Flughafencode „DMK“ für Inlandsflüge wiedereröffnet. Seit dem 1. Oktober 2012 fliegt die Billigfluggesellschaft AirAsia anstelle des größeren Flughafens Bangkok Suvarnabhumi ausschließlich den Flughafen Don Mueang an.[34]
Die erste Eisenbahnstrecke Thailands von Bangkok nach Pak Nam wurde am 11. April 1893 eröffnet.[35] Am 21. Dezember 1900 stellte eine englische Gesellschaft die Eisenbahnverbindung Bangkok-Ayutthaya-Khorat fertig, anschließend baute man die Linie Bangkok-Ratchaburi-Petchaburi aus, die am 19. Juni 1903 fertig gestellt wurde.[36]
Bis zur Eröffnung des neuen Hauptbahnhofs Bang Sue Grand Station im August 2021, fuhren die meisten Reisezüge von Hua Lamphong, dem Hauptbahnhof Bangkoks, Richtung Norden, Nordosten, Osten und Süden. Der alte Hauptbahnhof soll im Anschluss in ein Museum umgewandelt werden.[37] Nach Kabin Buri fahren zudem Züge ab Makkasan Station. Züge Richtung Kanchanaburi und Mae Nam Khwae („River Kwai“) und einige langsame Züge in den Süden fahren vom Bahnhof in Bangkok Noi ab. Statt vom alten Bahnhofsgebäude starten die Züge von der New Station, zu der ein kostenloser Shuttle in fünf Minuten fährt. Für den Güterverkehr besteht am nördlichen Stadtrand der mechanisierte Rangier- und Güterbahnhof Bang Sue. Hierbei handelt es sich um einen der weltweit nur wenigen Rangierbahnhöfe in Meterspur.
Bangkok hat drei Fernbusbahnhöfe, von denen die meisten Busse (außer einigen AC-Bussen und Minibussen) abfahren: Der östliche Busbahnhof (Richtung Ostküste) an der Thanon Sukhumvit, gegenüber der Einmündung der Soi Ekamai (daher auch kurz Ekamai genannt); der sehr große nördliche und nordöstliche Busbahnhof (kurz Mo Chit) an der Thanon Kamphaeng Phet 2 im Bezirk Chatuchak und der südliche Busbahnhof (kurz Sai Tai), südlich der Nationalstraße 338, an der Auffahrt der Thanon Borommaratchachonnani.
Der Hafen von Bangkok liegt am linken (östlichen) Chao-Phraya-Ufer und belegt eine Fläche von mehr als 3,6 Quadratkilometern. Im Alltagsleben wird er Khlong Toei genannt, heißt aber eigentlich „Pak Nam“ (so viel wie Mündung, wörtlich Wassermund). Der Seehafen ist einer der größten Südostasiens. 2004 wurden 15,3 Millionen Tonnen Fracht umgeschlagen (2003: 14,6 Millionen).[38] Bereits Ende des 19. Jahrhunderts bestanden regelmäßige Dampferverbindungen nach Hongkong, Singapur und Saigon.
Allgemeines
Außerhalb der Rattanakosin-Insel wird der Straßenverkehr in Bangkok von Fernstraßen bestimmt, entlang derer die Stadt seit der Aufgabe der traditionellen Wasserstraßen, der Khlongs, gewachsen ist. Die dennoch zahlreich vorhandenen Wasserwege, deren Überbrückung nicht immer erschwinglich war, führen auch heute noch zu einer weit geringeren Vernetzung der Straßen, als es in vergleichbaren Großstädten üblich ist.
Viele Nebenstraßen sind bis heute aufgrund der fehlenden Brücken Sackgassen geblieben. Dies führt zu einem geringen Durchgangsverkehr in den betroffenen Wohnvierteln, aber damit auch zu einer höheren Verkehrsdichte auf den größeren Straßen. Die Nebenstraßen, auf denen Kfz-Verkehr möglich ist, werden Soi genannt. Sois haben alle einen eigenen Namen, die jedoch bei kleineren Nebenstraßen im täglichen Sprachgebrauch nur in seltenen Fällen verwendet werden. Meist wird die Soi-Nummer benannt, beispielsweise Sukhumvit Soi 16. Größere Nebenstraßen wiederum sind häufig nur mit dem Soi-Namen bekannt, beispielsweise Soi Ekkamai (เอกมัย) statt Sukhumvit Soi 63.
Auf errichteten Radwegen nimmt Radverkehr etwas zu, wird 2019 berichtet.[39]
Verkehrsbelastung
In den 1970er Jahren haben europäische Stadtplaner ohne großen Erfolg versucht, das chronische Verstopfungs-Problem von Bangkoks Straßen mit modernen Brückenkonstruktionen zu beseitigen. In den 1980er Jahren wurde ein umfangreiches Einbahnstraßensystem eingerichtet. Busse erhielten während der Hauptverkehrszeit eine eigene Busspur, wo sie auch in Einbahnstraßen in beide Richtungen fahren können. Erst die Errichtung zahlreicher mautpflichtiger Hochstraßen und zusätzlicher Brücken (englisch fly-over), sowie die Errichtung einer elektrischen Hochbahn, der sogenannten „Bangkok Skytrain“ (BTS), und der Metro (MRT) haben eine gewisse Besserung erreicht.
Öffentlicher Nahverkehr
Der straßengebundene öffentliche Nahverkehr wird gegenwärtig überwiegend von dieselbetriebenen Stadtbussen bewältigt, die ein dichtes Busnetz bilden, das praktisch jeden Winkel der Stadt abdeckt. Es werden Stadtbusse unterschiedlicher Klassen eingesetzt: Busse mit Klimaanlage, Busse mit Ventilator und Busse mit offenen Fenstern (von den Thai spöttisch air thammachat, „Natur-Klimaanlage“, genannt). 2010 wurde zusätzlich eine neue Buslinie (BRT) eingeführt, die einen Großteil der Strecke auf extra abgetrennten Busspuren zurücklegt.
Individualverkehr wird vorwiegend durch die zahlreichen Taxis, Motorradtaxis und Tuk Tuks (offene Motorroller mit Sitzbank) abgewickelt, die Fahrgäste durch die Stadt befördern. Insbesondere die Tuk Tuks haben vor allem in der Vergangenheit das Stadtbild Bangkoks erheblich geprägt und sind noch immer eine besondere Touristenattraktion. In ihrer Funktion werden sie jedoch mittlerweile von den Motorradtaxis abgelöst.
Am 22. September 1888 fuhr die erste Pferdestraßenbahn und am 1. Februar 1893 die erste elektrische Straßenbahn in Bangkok. Der Verkehr wurde am 1. Oktober 1968 eingestellt.[36] Bis Ende der 1990er Jahre verfügte Bangkok über kein schienengebundenes Massentransportmittel. Durch den Aufbau eines Schnellbahnnetzes versucht man dem Verkehrschaos Herr zu werden, nachdem in den Jahrzehnten zuvor eher auf Ausbau des Straßenverkehrsnetzes gesetzt wurde.
Am 5. Dezember 1999 nahm die Hochbahn Bangkok Skytrain, das erste öffentliche Schnellbahnprojekt der Stadt, ihren Betrieb auf. Sie verkehrt auf zwei Linien mit 32 Kilometer Länge und 30 Stationen. Es war die erste elektrisch betriebene Bahn mit Europäischer Normalspur (1435 Millimeter Spurweite) und befördert derzeit 250.000 Fahrgäste pro Tag. Beide Linien kreuzen sich am Umsteigebahnhof Siam am belebten Siam Square.
Am 3. Juli 2004 wurde der erste Streckenabschnitt der Bangkok Metro eröffnet. Er ist 21 Kilometer lang, führt über 18 Bahnhöfe und besitzt dieselbe elektrische und technische Ausrüstung wie der Skytrain. Umsteigemöglichkeiten zur Skytrain bestehen in Si Lom/Saladaeng, Sukhumvit/Asok und Chatuchak/Mo Chit. Die Kapazität pro Fahrtrichtung beträgt circa 40.000 Fahrgäste pro Stunde. In Planung sind Erweiterungen nach Norden bis Charansanitwongse und Tha Phra, nach Süden bis nach Bang Khae, mit einer möglichen Ausführung als Ringstrecke.[40]
Am 5. Dezember 2009 wurde der Suvarnabhumi Airport Rail Link eröffnet. Die auf Stelzen geführte Hochbahn ist eine Stadtbahn im Öffentlichen Personennahverkehr von Bangkok, die den Flughafen Bangkok-Suvarnabhumi mit der Innenstadt Bangkoks verbindet. Betreiber ist die staatliche Eisenbahngesellschaft Thailands, die State Railway of Thailand (SRT).
Mit den SRT Red Lines bestehen zwei am 2. August 2021 eröffnete meterspurige Vorortbahnen, die vom Bahnhof Bang Sue, wo Anschluss an die blaue Linie der Metro, zu zahlreichen Buslinien sowie zu Fernverkehrszügen besteht und welcher den bisherigen Hauptbahnhof ersetzen soll, nach Rangsing und Taling Chan, welche elektrifiziert sind und eine Höchstgeschwindigkeit von bis zu 160 km/h erreichen. Daneben gibt es dieselbetriebene Vorortzüge von Hua Lamphong nach Lop Buri, Kaeng Koi, Prachin Buri und Suphanburi, von Thon Buri nach Ratchaburi sowie zwischen Wongwian Yai und Maekong mit Fährbetrieb zwischen Machachai und Ban Laem. 
Personenfähren (Thai: เรือยนต์ข้ามฟาก) – kleine, relativ hohe Boote mit Dach – überqueren von zahlreichen Piers aus den Mae Nam Chao Phraya. Allerdings stimmen die Anlegestellen nicht mit den Piers der Expressboote überein.
Die langen „Expressboote“ (เรือด่วนเจ้าพระยา) mit vielen Sitzplätzen verkehren auf dem Mae Nam Chao Phraya über 18 Kilometer zwischen Nonthaburi (Norden) und Krung-Thep-Brücke (Süden) sowie flussaufwärts über Nonthaburi hinaus bis Pak Kret.
Ruea Hang Yao (เรือหางยาว, wörtl.: Langschwanzboote, sogenannt, weil sie von einem Außenbordmotor an einer langen Stange angetrieben werden) – schmale Boote mit Sitzplätzen für etwa 15 Personen. Sie verkehren regelmäßig im Linienverkehr auf den Khlongs von Bangkok und Thonburi. Sie werden vor allem von Pendlern genutzt, um in die Vororte zu gelangen.
Bangkok beherbergt verschiedene technische Institute, mehrere Hoch- und Fachschulen und sechs Universitäten, von denen zwei im weltweiten Forschungsverbund eingegliedert sind: die Chulalongkorn-Universität und die Thammasat-Universität. Nahe dem Wat Phra Kaeo befindet sich die Kunstakademie, die 1921 gegründete Silpakorn-Universität.
Die Chulalongkorn-Universität (benannt nach Rama V. Chulalongkorn) ist eine der ältesten Universitäten Thailands. Sie wurde am 26. März 1917 von König Rama VI. Vajiravudh gegründet, nachdem sie seit 1899 als Verwaltungsfachschule existierte. Von Beginn an legte man sehr viel Wert auf die studentische Selbstverwaltung. Heute (2003) lernen etwa 28.000 Studenten an der Universität, die Zahl der akademischen Mitarbeiter beträgt knapp 3.000.
Die Thammasat-Universität wurde am 27. Juni 1934 als „Universität der moralischen Wissenschaft und Politik“ gegründet und wird heute noch „Universität des Volkes“ genannt, weil sie sich besonders um einen Hochschulzugang für alle Menschen des Landes bemüht. Seit 1934 haben etwa 240.000 Studenten an der Universität studiert, von denen einige Premierminister von Thailand, Präsidenten des Obersten Gerichts, Parlamentsmitglieder, Senatoren und erfolgreiche Geschäftsleute geworden sind.
Die Kasetsart-Universität (benannt nach dem Zweck der Universität) ist eine der berühmten Universitäten und war die erste thailändische landwirtschaftliche Universität. Sie wurde am 2. Februar 1943 gegründet, nachdem sie seit 1914 als pädagogische Fachhochschule existierte. Heute lernen circa 47.000 Studenten an der Universität, die Zahl der akademischen Mitarbeiter beträgt etwa 5.200.
Der Thailand Research Fund im Khet Phaya Thai organisiert die Forschungs- und Entwicklungsarbeiten des Landes.
Mitglieder des Königshauses:
Kunst und Kultur:
Politik:
Religion:
Sport:
Wirtschaft:
Wissenschaft:
Sonstiges:
Belletristik
Bang Bon |
Bang Kapi |
Bang Khae |
Bang Khen |
Bang Kho Laem |
Bang Khun Thian |
Bangkok Noi |
Bangkok Yai |
Bang Na |
Bang Phlat |
Bang Rak |
Bang Sue |
Bueng Kum |
Chatuchak |
Chom Thong |
Din Daeng |
Don Mueang |
Dusit |
Huai Khwang |
Khan Na Yao |
Khlong Sam Wa |
Khlong San |
Khlong Toei |
Lak Si |
Lat Krabang |
Lat Phrao |
Min Buri |
Nong Chok |
Nong Khaem |
Pathum Wan |
Phasi Charoen |
Phaya Thai |
Phra Khanong |
Phra Nakhon |
Pom Prap Sattru Phai |
Prawet |
Rat Burana |
Ratchathewi |
Samphanthawong |
Sai Mai |
Saphan Sung |
Sathon |
Suan Luang |
Taling Chan |
Thawi Watthana |
Thonburi |
Thung Khru |
Wang Thonglang |
Watthana |
Yan Nawa
Norden:
Chiang Mai |
Chiang Rai |
Kamphaeng Phet |
Lampang |
Lamphun |
Mae Hong Son |
Nakhon Sawan |
Nan |
Phayao |
Phetchabun |
Phichit |
Phitsanulok |
Phrae |
Sukhothai |
Tak |
Uthai Thani |
Uttaradit
Nordosten:
Amnat Charoen |
Bueng Kan |
Buri Ram |
Chaiyaphum |
Kalasin |
Khon Kaen |
Loei |
Maha Sarakham |
Mukdahan |
Nakhon Phanom |
Nakhon Ratchasima |
Nong Bua Lam Phu |
Nong Khai |
Roi Et |
Sakon Nakhon |
Si Sa Ket |
Surin |
Ubon Ratchathani |
Udon Thani |
Yasothon
Zentral:
Ang Thong |
Ayutthaya |
Bangkok |
Chachoengsao |
Chai Nat |
Chanthaburi |
Chon Buri |
Kanchanaburi |
Lop Buri |
Nakhon Nayok |
Nakhon Pathom |
Nonthaburi |
Pathum Thani |
Phetchaburi |
Prachin Buri |
Prachuap Khiri Khan |
Ratchaburi |
Rayong |
Sa Kaeo |
Samut Prakan |
Samut Sakhon |
Samut Songkhram |
Saraburi |
Sing Buri |
Suphan Buri |
Trat
Süden:
Chumphon |
Krabi |
Nakhon Si Thammarat |
Narathiwat |
Pattani |
Phang-nga |
Phatthalung |
Phuket |
Ranong |
Satun |
Songkhla |
Surat Thani |
Trang |
Yala

Mekka (arabisch مكة, DMG Makka) ist eine Stadt mit circa 2 Millionen Einwohnern (Stand 2020) im westlichen Saudi-Arabien und mit der Heiligen Moschee und der Kaaba der zentrale Wallfahrtsort des Islams.[1] Jedes Jahr pilgern rund 2,5 Millionen Muslime zur Haddsch nach Mekka, während Nicht-Muslimen das Betreten der Stadt verboten ist. Mekka ist Hauptstadt der Provinz Mekka in der Region Hedschas. Aufgrund der großen religiösen Bedeutung, die die Stadt im Islam hat, wird sie im Arabischen üblicherweise mit einem ehrenden Beinamen versehen und als مكة المكرّمة Makka al-Mukarrama ‚Mekka, die Ehrwürdige‘ bezeichnet.
Mekka gilt als die Geburtsstadt Mohammeds, des Propheten des Islam. Das wichtigste Wallfahrtsziel bildet die Kaaba, ein fensterloses, quaderförmiges Gebäude im Hof der Hauptmoschee, das nach islamischer Auffassung erstmals vom Propheten Adam erbaut und dann vom Propheten Abraham wiedererbaut wurde. Historisch gesichert ist, dass die Kaaba schon in vorislamischer Zeit ein zentrales Heiligtum der arabischen Stämme des Umlandes war. In ihrer südöstlichen Ecke befindet sich ein schwarzer Stein – möglicherweise ein Meteorit (Hadschar al-Aswad), den der Überlieferung nach der Prophet Abraham vom Engel Gabriel empfing.
Um die Stadt Mekka erstreckt sich ein heiliger Bezirk, der von Nichtmuslimen nicht betreten werden darf; seit dem Beginn des Huthi-Konflikts (2015) auch nicht mehr von Jemeniten.[2] Straßensperren schirmen die Stadt vor dem Besuch von Nichtmuslimen ab.
In den vergangenen Jahrhunderten gelang es jedoch einigen europäischen Reisenden, meist als Muslime getarnt, nach Mekka zu kommen. Dazu gehörten der versklavte Landsknecht Hans Wild (zwischen 1607 und 1609), der deutsche Forschungsreisende Ulrich Jasper Seetzen (1809), der Basler Jean Louis Burckhardt (1814), der unter anderem durch die Entdeckung der alten Nabatäerhauptstadt Petra bekannt wurde, sowie der deutsche Orientalist und Forschungsreisende Heinrich von Maltzan, der im Jahre 1860 als Muslim getarnt mit einem durch Bestechung eines Arabers erhaltenen Pass Mekka besuchte, worüber er in einem 1865 erstmals erschienenen und bis heute mehrfach neu aufgelegten Buch berichtete. 1853 lieferte der englische Abenteuerreisende Richard Francis Burton eine detaillierte Beschreibung von Mekka, nachdem er dort als Derwisch verkleidet an allen wichtigen religiösen Zeremonien teilgenommen hatte. Der niederländische Islamwissenschaftler Christiaan Snouck Hurgronje hielt sich im ausgehenden 19. Jahrhundert ebenfalls unter falscher Identität in Mekka auf. Ergebnis seiner Studienreise war ein zweibändiges Werk (Mekka, erschienen 1889), das aus einem Text- und einem Bildband mit Fotografien besteht.
1979 waren Angehörige der französischen Groupe d’intervention de la gendarmerie nationale mit einer Ausnahmegenehmigung des saudischen Militärs an der Niederschlagung der Besetzung der Großen Moschee in Mekka beteiligt.[3]
Mekka liegt ca. 90 km vom Roten Meer entfernt zwischen der Küstenebene und dem Hochland in einem wüstenartigen Becken, eingeschlossen zwischen zwei Bergketten. Der tiefer gelegene Teil der Stadt um die Kaaba ist der ältere Siedlungskern; die Oberstadt liegt im Norden. Wegen der zahlreichen in das Stadtgebiet hineinragenden Berge und Hügel war es erforderlich, mehrere Straßentunnel zu bauen.
Die Bevölkerung Mekkas wuchs in den letzten Jahren rasant an. Sie überschritt in den 1990er Jahren erstmals die Millionengrenze und liegt im Jahre 2017 bei 1,9 Millionen für die Agglomeration. Bis 2035 wird von einer weiteren Steigerung auf 2,5 Millionen Einwohner gerechnet.
Bevölkerungsentwicklung der Agglomeration laut UN[4]
In den letzten Jahren war ein deutliches Wachstum Mekkas um jährlich fast 200.000 Einwohner zu beobachten, damit geht ein Stadtumbau einher, der sich z. B. um die Heiligen Stätten herum besonders bemerkbar macht. Der Großraum Mekka wird heute geprägt von den infrastrukturellen Einrichtungen, die die Pilger beherbergen, verköstigen und transportieren.[5] Ganze bisher niedrig und locker bebaute Hügelsiedlungen wurden abgetragen und die Flächen begradigt, um Platz für Großbauten, vor allem Pilgerhotels, zu schaffen. Südlich der Großen Heiligen Moschee entstand bis Ende 2012 ein massiger Hotelkomplex, in dessen Mittelpunkt der höchste der Abraj Al Bait Towers mit 601 Metern die neue Stadtkrone bildet.
Mekka hat nur einen kleinen Flughafen ohne Linienverkehr. Der Jeddah King Abdulaziz International Airport und der Hafen von Dschidda sind daher wichtige Infrastrukturen für die Pilger. Eine Metro zwischen den Pilgerstätten wurde 2010 eröffnet, weitere Metrolinien sind geplant. Seit  11. Oktober 2018 verbindet eine Hochgeschwindigkeitsstrecke Mekka mit Dschidda und Medina.[6]
Eine Konstante in der Geschichte Mekkas sind die durch heftige Regenfälle verursachten Sturzfluten, die die Stadt in regelmäßigen Abständen heimsuchen. Der moderne Gelehrte Ruschdī as-Sālih Malhas zählt insgesamt 85 große Überschwemmungen, die von den Anfängen des Islams bis zum Jahr 1931 Mekka überflutet haben.[7] Das Wasser strömte dabei meistens von Minā her durch das Wādī Ibrāhīm in den unteren Bereich der Stadt hinab.[8] Zum Schutz der Stadt vor diesen Sturzfluten wurden im Laufe der Geschichte immer wieder verschiedene Dämme in Mekka angelegt, die das Wasser an den Heiligen Stätten vorbeileiten sollten, allerdings nie vollständigen Schutz boten.
Die Frühgeschichte Mekkas liegt im Dunkeln. Sicher ist, dass schon in vorislamischer Zeit hier ein heidnisches Heiligtum bestand, das Ziel einer arabischen Wallfahrt war.[9] Nach der islamischen Überlieferung begann die Besiedlung Mekkas, als der Stammvater Abraham seine Nebenfrau Hagar und den gemeinsamen Sohn Ismael an diesen Ort brachte. Er bat Gott darum, seine Familie zu versorgen und ihnen die Herzen der Menschen zugeneigt sein zu lassen. Hierauf wird das Koranwort in Sure 14:37 bezogen: „Unser Herr, ich habe (einige) aus meiner Nachkommenschaft in einem Tal ohne Pflanzungen bei Deinem geschützten Haus wohnen lassen, unser Herr, damit sie das Gebet verrichten. So lasse die Herzen einiger der Menschen sich ihnen zuneigen und versorge sie mit Früchten, auf dass sie dankbar sein mögen.“ Weiter erzählt die Sage: „Als die Wasservorräte zu Ende gingen, lief Hagar insgesamt sieben Mal zwischen den Hügeln Safa und Marwa hin und her, um nach Wasser oder Karawanen Ausschau zu halten.“ Nachdem sie zu ihrem Zelt zurückkehrte, fand sie neben ihrem Sohn Ismael eine sprudelnde Quelle vor, die bis heute existiert und unter dem Namen Zamzam bekannt ist.
Um die gleiche Zeit siedelten sich zwei Stämme aus dem Jemen, Dschurhum und Qatūrā, in Mekka an. Ismael heiratete später eine Frau aus dem Stamm der Dschurhum. Als Abraham später nach Mekka zurückkehrte, errichtete er gemeinsam mit Ismael die Kaaba. Ismael behielt zeit seines Lebens die Kontrolle über die Kaaba und auch über den Stamm der Dschurhum. Nach seinem Tod übernahm sein Schwiegervater Mudād ibn ʿAmr die Aufsicht über das Heiligtum und auch die Führung des Stammes. Die Dschurhum ließen sich im Gebiet oberhalb der Kaaba nieder, während die Qatūrā unter ihrem Führer as-Sumaidiʿ den unteren Teil der Stadt in Besitz nahmen.[10]
Nach einiger Zeit brachten Gruppen aus dem südarabischen Stamm der Azd die Niederung von Mekka in ihren Besitz. Während die meisten Azd-Gruppen von dort aus in andere Gebiete der arabischen Halbinsel weiterzogen, blieb alleine die Gruppe der Chuzāʿa in Mekka zurück.[11] Die Chuzāʿa, die sich selbst zu einem eigenen Stamm entwickelten, werden in der islamischen Überlieferung für die Einführung des Götzendienstes in Mekka verantwortlich gemacht. Sie sollen darüber hinaus auch den ersten Damm zum Schutz der Stadt vor Überschwemmungen errichtet haben.[12]
Der Name „Mekka“ selbst wird erst 751 n. Chr., über 100 Jahre nach Mohammeds Tod, eindeutig in islamischen Quellen genannt. Die Behauptung vieler Muslime, der alte Name Mekkas laute „Bakkah“ und der Ort sei unter diesem Namen auch im Alten Testament der Bibel zu finden, hält einer Überprüfung nicht stand. Die historisch-archäologische Forschung ist sich mittlerweile einig, dass Mekka, wenn es vor Mohammeds Zeit als Siedlung existierte, nicht früher als im 3. Jahrhundert n. Chr. entstanden sein kann. Damit ist auch der frühestmögliche Zeitpunkt der Erbauung der heidnischen Kaaba gesetzt. Hierfür spricht neben der fehlenden schriftlichen Nennung in zeitgenössischen Quellen auch das völlige Fehlen einer Ortschaft mit dem Namen oder im Bereich des heutigen Mekka auf antiken Landkarten Arabiens.
Während des frühen 6. Jahrhunderts übernahm der Stamm der Quraisch die Kontrolle über die Stadt Mekka. Die Quraisch konnten sich als erfolgreiche Händler etablieren und mit anderen arabischen Stämmen ein System von Bündnissen aufbauen. Enge Beziehungen bestanden vor allem mit dem Stamm der Banū Sulaim, dessen Hauptwohngebiet zwischen Mekka und Medina lag.[13] Im Inneren war der Stamm der Quraisch allerdings von Clanrivalitäten geprägt.
Die Kaaba war in dieser Zeit bereits Zielpunkt einer Wallfahrt und wurde von den arabischen Stämmen als Heiligtum des Gottes Hubal verehrt. Zum vorislamischen Kaaba-Kult gehört neben der Verehrung von Allah die Verehrung der altarabischen Gottheiten al-Lāt, Manāt und Uzza. Politisches und gesellschaftliches Zentrum der Stadt war die Dār an-Nadwa, ein Versammlungshaus, in dem die Ratsversammlung der Quraisch stattfand und die wichtigsten Übergangsriten gefeiert wurden.
Die Pilgerströme haben möglicherweise dazu beigetragen, dass sich Mekka zu einem Handelszentrum entwickelte, obwohl es selber wenig produzierte und nur geringen strategischen Wert hatte. Einige Historiker vertreten allerdings die Ansicht, dass Mekka seine Bedeutung vor allem wegen seiner Lage gewann. Es lag auf dem Weg der zweimonatigen Reise von Byzanz zu den jemenitischen Königreichen Saba, Ma'in, Qataban, Ausan und Hadramaut, die enge Handelskontakte mit Indien und Ostafrika hatten. Inwieweit Mekka vom Weihrauchhandel profitierte, ist strittig. Zusammen mit Ta'if und Yathrib bildete Mekka in dieser Zeit eine der drei großen Städte des Hedschas. Da die Stadt in einem trockenen und unfruchtbaren Tal lag, war sie vollständig von den Nahrungsmitteln, die in Tāʾif produziert wurden, abhängig.[14]
Um 610 begann der Begründer des Islam, Mohammed, in Mekka öffentlich zu predigen und zu einer neuen monotheistischen Religion aufzurufen. Aufgrund des hartnäckigen Widerstands der Quraisch gegen seine neue Lehre wanderte er im Sommer 622 mit seinen Anhängern in die Stadt Yathrib (später als Medina bekannt) aus, wo sich bereits zahlreiche Angehörige der Stämme Aus und Chazradsch seiner Religion angeschlossen hatten. Von Yathrib aus nahm Mohammed den Kampf gegen die nichtislamischen Mekkaner auf. Die militärische Auseinandersetzung zwischen ihm und der Stadt Mekka lässt sich in vier Phasen[15] einteilen:
Der Islam hat den Kult des Schwarzen Steins der Kaaba aus der altarabischen Religion in den Islam übernommen, ebenso die Wallfahrt nach Mekka. Die mit der Wallfahrt verbundenen Riten wurden jetzt allerdings auf Abraham zurückgeführt.[9]
Nachdem im Jahre 638 erneut ein heftiger Regen die Stadt überschwemmt hatte, ließ ʿUmar ibn al-Chattāb im oberen Teil der Stadt einen neuen Damm anlegen, der die Heilige Moschee vor weiteren Überschwemmungen schützen sollte.[21] In der Folgezeit wurde die Moschee von Mekka mehrere Male vergrößert, so zum Beispiel während der Herrschaft des abbasidischen Kalifen al-Mahdi (reg. 775–785) durch dessen Statthalter Dschaʿfar ibn Sulaimān.[22] Als im Jahre 809/810 eine große Dürre herrschte, erbaute Zubaida bint Dschaʿfar, die Ehefrau von Hārūn ar-Raschīd, eine Rohrleitung, die Wasser von ʿAin al-Muschāsch und Hunain nach Mekka leitete. Diese Leitung bildete im 9. Jahrhundert die Grundlage der mekkanischen Wasserversorgung.
Ab dem späten 10. Jahrhundert wurde die Stadt von den Scherifen von Mekka regiert. Sie unterstellten sich nacheinander verschiedenen Herrscherhäusern, zunächst den Fatimiden, dann den Ayyubiden, den Rasuliden und den Mamluken von Ägypten. 1326 stellte Amīr Tschūpān mit der Freilegung und Reparatur der Wasserleitung von ʿAin Bāzān die mekkanische Wasserversorgung wieder auf eine stabilere Grundlage. Um den Wasserfluss der Leitung in Trockenphasen zu erhöhen, wurde die Leitung im Laufe der Zeit mit weiteren Zuläufen versehen. Außerdem musste die Leitung häufig repariert und gereinigt werden, da sich die Röhren bei Überschwemmungen regelmäßig mit Erde und Geröll zusetzten.
Ab dem Jahr 1517 stand Mekka unter der Oberhoheit der Osmanen. In dieser Zeit erhielt die Stadt eine besonders große Bedeutung für die Muslime Südostasiens. Mehrere Sultane des malaiischen Archipels ließen sich Einsetzungsschreiben von den Scherifen von Mekka geben. Außerdem war hier das Motiv der Islamisierung der eigenen Dynastie durch Gesandte aus Mekka ein wichtiges Element in der Herrschaftslegitimation.[23] Muslime aus Südostasien stellten im 19. Jahrhundert das größte Kontingent der Pilger in Mekka.[24]
Der Großscherif Hussein ibn Ali, der später König des Hedschas wurde, warf 1916 die türkische Herrschaft über Mekka nieder. Im Oktober 1924 nahmen die wahhabitischen Ichwān von Sultan Abd al-Aziz ibn Saud Mekka ein, und König Husain musste fliehen. Schon kurz nach diesem Ereignis lud ʿAbd al-ʿAzīz zu einem Kongress nach Mekka ein, der die Muslime mit der saudischen Herrschaft versöhnen sollte. Nachdem ʿAbd al-ʿAzīz im Januar 1926 zum König erhoben worden war, nahmen die Planungen für diesen Kongress konkretere Formen an. ʿAbd al-ʿAzīz schickte Telegramme an verschiedene muslimische Politiker und Organisationen und forderte sie dazu auf, an der Veranstaltung teilzunehmen, um die Zukunft der Wallfahrtsstätten zu sichern und den Komfort für die Pilger zu verbessern. Im Juni/Juli 1926 fand der Kongress dann statt.[25]
Vom 20. November bis zum 5. Dezember 1979 fand in Mekka ein Terrorangriff mit Geiselnahme auf die große Moschee statt, in dessen Verlauf womöglich über 1000 Menschen getötet wurden.

Der Arktische Ozean, auch Nordpolarmeer, Nördliches Eismeer, Arktische See oder kurz Arktik genannt, ist mit 14,09 Millionen km² der kleinste Ozean der Erde. Mit einer durchschnittlichen Wassertiefe von 987 m zählt er zu den flacheren Meeren, seine größte Tiefe beträgt 5669 m[1]. Er wird manchmal auch als Nebenmeer des Atlantischen Ozeans betrachtet. Der Arktische Ozean liegt in der Arktis und ist ganzjährig von Meereis bedeckt, wobei die Ausdehnung des Eises jahreszeitlich schwankt und durch den Klimawandel massiv abgenommen hat.[2] Laut NASA verringert sich die sommerliche Meereisbedeckung derzeit um 12,6 % pro Dekade aufgrund von globaler Erwärmung.[3]
Innerhalb des Arktischen Ozeans bzw. auf dessen Inseln liegen die vier Pole der nördlichen Hemisphäre.
Der Arktische Ozean, der sich im äußersten Norden der Nordhalbkugel der Erde befindet, liegt zwischen den drei Kontinenten Asien, Europa und Nordamerika. Daher gilt er auch als ein interkontinentales Mittelmeer, obwohl er wissenschaftlich betrachtet einer der fünf Ozeane der Erde ist.
Mit dem Atlantik ist der Arktische Ozean hauptsächlich durch das etwa 1500 km breite Europäische Nordmeer zwischen Grönland und Skandinavien verbunden, darüber hinaus durch schmale Meeresstraßen in der nordkanadischen Inselwelt, die zur Davisstraße westlich von Grönland führen. Mit dem Pazifik ist er nur durch die etwa 85 km breite Beringstraße verbunden.
Der Arktische Ozean hat eine Ausdehnung von rund 14,09 Millionen km² und ist großflächig von Eis bedeckt. Dabei nimmt die Eisbedeckung mit den Jahreszeiten zu und ab. Insgesamt hat die Eisfläche seit den 1970er Jahren deutlich abgenommen.[4] Während die spät-sommerliche Eisbedeckung im September in den 1980er Jahren noch zwischen 6 und 7 Millionen km² betrug, beträgt diese in den 2010er Jahren nur noch durchschnittlich 4,13 Millionen km².[5][6] Mitte September 2020 lag dieser Wert bei 3,8 Millionen km².[7]
Im Molloytief, das sich 165 km westlich von Spitzbergen befindet, ist der Arktische Ozean bis 5669 m tief.[1]
Zwischen Grönland und Skandinavien grenzt der Arktische Ozean an das Europäische Nordmeer (das nördlichste Randmeer des Atlantiks) und zwischen Alaska und Ostsibirien an die Beringstraße, die zum Beringmeer (das nördlichste Randmeer des Pazifiks) überleitet. Grönlandsee und Europäisches Nordmeer sind auch Randmeere des Atlantiks, weil sie zu diesem Ozean überleiten. Auch weitere Nebenmeere des Arktischen Ozeans sind in folgender Tabelle gelistet:
Fünf Flüsse, die zu den längsten der Welt gehören, münden in den Arktischen Ozean. Auf der euroasiatischen Seite sind das die großen sibirischen Flüsse Ob, Jenissei und Lena, in Nordamerika der Yukon River in Alaska und der kanadische Mackenzie River. Die in den Ozean eingetragene Wassermenge durch die sibirischen Flüsse ist etwa 3-4 mal größer als die der nordamerikanischen Zuflüsse.[8] Die Forscher um Bruce Peterson behaupten,[9] dass die ins Nordpolarmeer gelangte Süßwassermenge von 1936 bis 1999 um etwa 128 km³ bzw. etwa um sieben Prozent zugenommen hat.[10]
Der Arktische Ozean grenzt jeweils an die nördlichen Gebiete folgender Länder und Landteile: Alaska (USA), Kanada und Grönland (Dänemark), Island, Norwegen sowie Russland. Die politische Zugehörigkeit verschiedener Teile des Arktischen Ozeans zu den Anrainerstaaten ist umstritten (→ Außenpolitik Russlands#Arktis).
Dänemark und Kanada haben am 14. Juni 2022 ihren jahrzehntelangen Streit („Whiskykrieg“) um die Hans-Insel offiziell beigelegt.
Außerdem grenzt er unter anderem an folgende Inseln und Inselgruppen: Banksinsel, Franz-Joseph-Land, Königin-Elisabeth-Inseln mit Ellesmere-Insel, Grönland, Island, Kolgujew, Neusibirische Inseln, Nowaja Semlja, Sewernaja Semlja, Spitzbergen und Wrangelinsel.
Innerhalb des Arktischen Ozeans bzw. auf dessen Meeresboden befinden sich Schwellen, Tiefseebecken und ein Meerestief.
Zu den Schwellen gehören der Fletcherrücken, der Gakkelrücken, der Lomonossow-Rücken (Harrisschwelle), die Ostsibirische Schwelle und die Tschuktschenschwelle. Die drei großen Tiefseebecken, die sich alle im Zentrum des Ozeans befinden, sind das im Litketief bis 5449 m tiefe Eurasische Becken, das bis 4994 m tiefe Kanadische Becken und das bis 3290 m tiefe Zentralarktische Becken.
Wiederholt in der Erdgeschichte war in Kaltzeiten der Meeresspiegel stark abgesenkt, weil auf den Kontinenten dicke Gletscher lagerten. Bei in den Weltmeeren bis zu 130 m tieferem Wasserspiegel während der Weichsel-Kaltzeit vor 60.000 bis 70.000 Jahren wurden Arktis und Europäisches Nordmeer zu einem „Mittelmeer des Nordens“, stark durch Land umschlossen. Liegt nun ein dicker Panzer aus Gletschereis auf diesem Mittelmeer, reichen bis zu acht Neuntel seiner Dicke unter die Wasseroberfläche und können seichte Verbindungsreste – etwa westlich und östlich von Island – zu umgebenden Meeren verschließen. Gibt es reichlich Niederschlag auf dem Polareisgletscher, kann er bis zu mehrere hundert Meter Dicke erreichen sowie oben wachsen und unten zugleich durch Erdwärme schmelzen, damit darunter eingeschlossenes Salzwasser verdünnen und dadurch den Wasserkörper binnen weniger tausend Jahre aussüßen. Weiters können nach Norden fließende Flüsse im Sommer Süßwasser einspeisen.
Ein Team um Walter Geibert, Geochemiker vom Alfred-Wegener-Institut, schließt von fehlendem Thorium-230 (HWZ = 75.000 a) in Sedimentschichten dieses Mittelmeeres auf Salzfreiheit seines Wassers während zumindest zweier Kaltzeiten, nämlich auch in der Saale-Kaltzeit vor 150.000 bis 130.000 Jahren.[11][12][13][14]
Der Arktische Ozean wird etwa drei bis fünf Monate im Jahr durch die Schifffahrt genutzt, die Russland in der Nordostpassage und die USA und Kanada in der Nordwestpassage betreiben. Der russische Staat unterhält zu diesem Zweck eine Atomeisbrecherflotte. In geringem Maß finden auch Expeditionskreuzfahrten statt. Ferner sind im Arktischen Ozean U-Boote mehrerer Nationen aktiv. Die wichtigsten Häfen sind Churchill, Archangelsk, Seweromorsk, Dikson, Murmansk, Pewek und Tiksi.
Am 27. August 2014 drang das Expeditionskreuzfahrtschiff Hanseatic unter Kapitän Thilo Natke bis auf eine Distanz von rund 480 Kilometern zum Nordpol vor; die Hanseatic gelangte an eine Nordposition, die bis dahin nur von Eisbrechern erreicht werden konnte und stellte damit einen Weltrekord auf.
Aufgrund der ausgeprägten Saisonalität in der Arktis von Mitternachtssonne im Sommer und Polarnacht im Winter beschränkt sich die Primärproduktion photosynthetisierender Organismen wie Eisalgen und Phytoplankton auf die Frühlings- und Sommermonate (März/April bis September).[15] Wichtige Konsumenten von Primärproduzenten im zentralen Arktischen Ozean und den angrenzenden Schelfmeeren sind Zooplankton, insbesondere Copepoden (Calanus finmarchicus, Calanus glacialis und Calanus hyperboreus) und Krill, sowie eisassoziierte Fauna (z. B. Amphipoden).[16] Diese Primärkonsumenten bilden ein wichtiges Bindeglied zwischen den Primärproduzenten und höheren trophischen Ebenen. Die Zusammensetzung der höheren trophischen Ebenen im Arktischen Ozean variiert mit der Region (atlantische Seite vs. pazifische Seite) und mit der Meereisbedeckung. Sekundäre Konsumenten in der Barentssee, einem vom Atlantik beeinflussten arktischen Schelfmeer, sind hauptsächlich subarktische Arten, darunter Hering, junger Kabeljau und Lodde. In den eisbedeckten Regionen des zentralen Arktischen Ozeans ist der Polardorsch ein zentrales Raubtier der Primärkonsumenten. Die Spitzenprädatoren im Arktischen Ozean – Meeressäugetiere wie Robben, Wale und Eisbären, machen Jagd auf Fische.
Zu den gefährdeten Meereslebewesen im Arktischen Ozean gehören Walrosse und Wale. Das Gebiet hat ein empfindliches Ökosystem und ist dem Klimawandel besonders ausgesetzt, da es sich schneller erwärmt als der Rest der Welt. Gelbe Haarquallen sind in den Gewässern der Arktis reichlich vorhanden. Der Gebänderte Butterfisch ist die einzige Butterfischart, die im Arktischen Ozean lebt.
Etwa 50 Arten von Seevögeln, darunter Alken, Sturmvögel, Kormorane, Möwen, Seeschwalben und Greifvögel, drängen sich im Juni und Juli an den Brutfelsen und Stränden. Sie alle holen ihre Nahrung aus dem Meer. In der Arktis pflanzen sich acht Robben-Arten fort, sechs davon zwischen dem Eis. Am größten werden die Walrosse, deren Bullen über vier Meter lang und über eine Tonne schwer werden. Sie leben vorwiegend entlang der Küsten Ostsibiriens, Alaskas, Kanadas, Grönlands und Nordskandinaviens, tauchen im flachen Wasser nach Muscheln und anderen wirbellosen Bodenbewohnern. Die Jungen werden im Mai auf dem Treibeis geboren. Arktische Seebären pflanzen sich nur auf den Inseln der Beringstraße fort.
Aufgrund der stärkeren Erwärmung der Arktis hat die durchschnittliche Eisbedeckung des Arktischen Ozeans in den letzten Jahrzehnten um 12,85 % pro Dekade abgenommen.[17] Besonders das dicke, mehrjährige Eis, das die Arktis früher dominiert hat, ist massiv zurückgegangen.[18] Der Rückgang der Eisbedeckung führt mit zu einer Erwärmung des Meerwassers, was wiederum den Rückgang der Eisbedeckung beschleunigt (→ Polare Verstärkung).[19]
Es besteht die Befürchtung, dass über die Grundwasser-Ströme der Flüsse Tetscha und Ob stark kontaminiertes Wasser des Karatschai-Sees (im südlichen Ural, Russland) in den Arktischen Ozean gelangen könnte.[20] Dadurch würde eine der letzten großen Wildnisse verseucht werden. Mögliche Folgen für die Tier- und Pflanzenwelt sind nicht exakt abschätzbar.[21]
Die russische Marine hat (auch) in der Arktik Atomreaktoren von Schiffen und/oder atombetriebene Schiffe, darunter U-Boote versenkt, deponiert oder verloren. Siehe: Atommüllproblematik der russischen Marine. Russland betreibt das erste schwimmende Kernkraftwerk seit 2020 im Arktikhafen Pewek.
Arktischer Ozean |
Atlantischer Ozean |
Indischer Ozean |
Pazifischer Ozean |
Südlicher Ozean

563Koordinaten: 56° 0′ 0″ N, 3° 0′ 0″ O
Die Nordsee (veraltet Westsee, Deutsches Meer[2]) ist ein Randmeer des Atlantischen Ozeans. Sie ist ein Schelfmeer und liegt im nordwestlichen Europa. Bis auf die Meerengen beim Ärmelkanal und beim Skagerrak ist sie auf drei Seiten von Land begrenzt und öffnet sich trichterförmig zum nordöstlichen Atlantik. In einem 150-Kilometer-Bereich an der Küste leben rund 80 Millionen Menschen.
Die Nordsee selbst ist ein wichtiger Handelsweg und dient als Zugang Mittel- und Nordeuropas zu den Weltmärkten. Die südliche Nordsee ist zusammen mit dem angrenzenden Ärmelkanal die am dichtesten befahrene Schifffahrtsregion der Welt. Unter dem Meeresboden befinden sich größere Erdöl- und Erdgasreserven, die seit den 1970er Jahren gefördert werden. Kommerzielle Fischerei hat den Fischbestand des Meeres in den letzten Jahrzehnten vermindert. Umweltveränderungen entstehen auch dadurch, dass die Abwässer aus Nordeuropa und Teilen Mitteleuropas direkt oder über die angrenzende Ostsee in das Meer fließen.
Die Nordsee liegt größtenteils auf dem europäischen Kontinentalschelf. Eine Ausnahme bildet lediglich ein schmales Gebiet der nördlichen Nordsee vor Norwegen. Die Nordsee wird begrenzt von der Insel Großbritannien im Westen und dem nord- und mitteleuropäischen Festland mit Norwegen (Nordost), Dänemark (Ost) sowie Deutschland (Südost), Niederlande (Süd), Belgien und Frankreich (Südwest). Im Südwesten geht die Nordsee durch die Straße von Dover in den Ärmelkanal über, im Osten hat sie über Skagerrak und Kattegat Kontakt zur Ostsee und nach Norden öffnet sie sich trichterförmig zum Europäischen Nordmeer, das im Osten des Nordatlantiks liegt. Neben den offensichtlichen Grenzen durch die Küsten der Anrainerstaaten wird die Nordsee durch eine gedachte Linie vom norwegischen Lindesnes hin zum dänischen Hanstholm in Richtung Skagerrak abgegrenzt.
Die International Hydrographic Organization definiert die Grenzen der Nordsee wie folgt:[3]
Die nördliche Grenze zum Atlantik ist naturräumlich weniger eindeutig. Während die seit langem im Entwurfsstadium befindliche 4. Edition der Limits[4] die Nordgrenze nach Süden zum Breitenkreis 60⁰ 51' N verschiebt, verläuft die Grenze des Oslo-Pariser-Abkommens von 1962 etwas weiter westlich und nördlich entlang von 5° westlicher Länge und 62° nördlicher Breite auf Höhe des norwegischen Geirangerfjords.
Die Nord-Süd-Ausdehnung beträgt 1.120 km von 50° 56′ N bis 62° N. Die maximale Breite von Osten nach Westen beträgt 1.001 km von 4° 26′ W bis 9° 50′ O.
Die Flächenausdehnung der Nordsee beträgt rund 575.000 km2 bei einer durchschnittlichen Tiefe von 94 m. Das ergibt ein Wasservolumen von 54.000 km3.
Für Fischerei- und Wettervorhersagen wie beispielsweise die des Seewetterdienstes Hamburg wird die Nordsee international in verschiedene Seegebiete untergliedert:[5]
Westliche Nordsee von Norden nach Süden:
Östliche Nordsee von Norden nach Süden:
In die Nordsee münden zahlreiche Fließgewässer, von denen einige sehr wasserreich sind:
Das heutige Nordseebecken bildete sich im Tertiär heraus. Ihre heutigen Umrisse erhielt die Nordsee jedoch erst im frühen Holozän vor etwa 8.000 Jahren. Auch der jetzige Zustand ist nur ein Stadium in der dynamischen Entwicklung der Nordsee: langfristig lässt sich weiterhin ein Anstieg des Meeresspiegels beobachten, der über die letzten 7500 Jahre gerechnet bei etwa 33 Zentimeter/Jahrhundert liegt (mittleres Tidenhochwasser an den deutschen Küsten). Im 20. Jahrhundert stieg das Wasser um etwa 20 bis 25 Zentimeter.
In der Weichseleiszeit waren, wie in den anderen Eiszeiten auch, große Wassermengen im Eis der Gletscher gebunden. Das Inlandeis Skandinaviens war bis zu drei Kilometer dick. Der Meeresspiegel lag auf dem Höhepunkt der Weichseleiszeit bis zu 120 Meter unter dem heutigen Stand, die Küstenlinien verliefen etwa 600 Kilometer nördlich des heutigen Stands. Große Teile der Nordsee lagen damals trocken. Am Ende der Weichseleiszeit lag der Meeresspiegel etwa 60 Meter unter dem heutigen Normalnull, wobei die Küstenlinie nördlich der heutigen Doggerbank verlief. Die gesamte südliche Nordsee war Festland, das sogenannte Doggerland, die britischen Inseln und das europäische Festland waren eine zusammenhängende Landmasse. In den darauf folgenden Jahrtausenden stieg das Wasser, wobei dieser Anstieg im Laufe der Zeit an Geschwindigkeit abnahm.
Vor etwa 9850 bis 7100 Jahren wurden Teile des Elbe-Urstromtals überflutet. Etwas später öffnete sich der Ärmelkanal und das Wattenmeer begann sich zu bilden. In der darauf folgenden Zeit wechselten Phasen stärkeren Wasseranstiegs (Transgression) mit solchen einer Wassersenkung (Regression). Vor etwa 5000 Jahren (3000 v. Chr.) lag der Meeresspiegel an der südlichen Küste etwa vier Meter unter dem heutigen Niveau, um den Beginn der christlichen Zeitrechnung knapp zwei Meter unter dem heutigen Meeresspiegel. Nachdem er zwischenzeitlich anstieg, sank er um das Jahr 1000 wieder auf das Niveau zu Beginn der christlichen Zeitrechnung, um schließlich in mehreren Schüben langsamer weiter zu steigen.[7]
Die Nordsee ist ein Schelfmeer mit einer durchschnittlichen Tiefe von nur 94 Metern. Der Meeresboden liegt größtenteils auf dem Schelf, und so steigt die Tiefe von 25 bis 35 Metern im südlichen Teil am Kontinentalhang zwischen Norwegen und nördlich der Shetlandinseln auf bis zu 100 bis 200 Metern. Der gesamte südliche Teil des Meeres ist dabei höchstens 50 Meter tief. Die Ausnahme bildet die Norwegische Rinne; an dieser tiefsten Stelle misst die Nordsee 725 Meter. Die flachste Stelle abseits der Küstengebiete liegt in der Doggerbank. Die südliche Nordsee wird von zahlreichen großen Sandbänken durchzogen.
Die Nordsee wird generell in die flache südliche Nordsee, die Zentralnordsee, die nördliche Nordsee und die Norwegische Rinne mit dem Übergang Skagerrak unterteilt. In der südlichen Nordsee geht der Ärmelkanal in die Straße von Dover über. Die Southern Bight liegt vor der niederländischen und belgischen Küste, die Deutsche Bucht inklusive der Helgoländer Bucht vor der deutschen Küste. Das Flachwassergebiet der Doggerbank begrenzt die deutsche Bucht hin zur Zentralnordsee. Das Wattenmeer zieht sich an der südlichen Küste von Den Helder in den Niederlanden nahezu die gesamte deutsche Nordseeküste entlang bis Esbjerg in Dänemark.
Die Flachwasserzone Doggerbank ist etwa halb so groß wie die Niederlande mit einer Tiefe zwischen nur 13 Metern bis zu höchstens 20 Metern. Sie ist als Ort zum Fischfang berühmt, bei Stürmen brechen hier sogar häufiger die Wellen.
Die Norwegische Rinne ist durchschnittlich 250 bis 300 Meter tief, wird am Übergang zum Skagerrak bis zu 725 Metern tief und spielt eine wichtige Rolle beim Wasseraustausch mit Ostsee und Atlantik. Entlang der Norwegischen Rinne fließt der Norwegische Strom, über den der größte Teil des Nordseewassers in den Atlantik fließt. Ebenso fließt hier ein Großteil des aus der Ostsee stammenden Wassers nach Norden. In der Zentralnordsee, etwa 200 Kilometer östlich der schottischen Stadt Dundee finden sich im Devil's Hole weitere Gräben. Die wenige Kilometer langen Gräben gehen in einer Umgebung, die etwa 90 Meter Wassertiefe hat, auf 230 Meter hinunter.
Die „Straße von Dover“ erreicht Meerestiefen von etwa 30 Metern, der Meeresgrund fällt nach Westen hin bis zum Ende des Ärmelkanals bis zu 100 Meter ab. Zwischen den Niederlanden und Großbritannien liegen Tiefen zwischen 20 und 30 Metern, die bis zu 45 Meter an der friesischen Front gehen.
Der Salzgehalt des Meerwassers ist orts- und jahreszeitenabhängig und liegt zwischen 15 und 25 Promille in der Nähe der Flussmündungen und bis zu 32 bis 35 Promille in der nördlichen Nordsee.
Die Temperatur kann im Sommer 25 °C erreichen und 10 °C im Winter. Die Temperatur variiert dabei stark abhängig vom Einfluss des Atlantiks und der Wassertiefe, vor allem wegen der Meeresströmungen. In der tieferen nördlichen Nordsee, in einem Gebiet südlich und östlich der Shetlands, ist die Wassertemperatur durch das einströmende Atlantikwasser das ganze Jahr über fast konstant bei 10 °C, während an der sehr flachen Wattenmeerküste die größten Temperaturunterschiede auftreten und es in sehr kalten Wintern auch zu Eisbildung kommen kann.
Von 1965 bis 2010 stieg die Durchschnittstemperatur der deutschen Nordsee um 1,67 °C, die der Ozeane stieg im Mittel um 0,74 °C.[8]
Das Austausch-Salzwasser der Nordsee fließt durch den Ärmelkanal und entlang der schottischen und englischen Küsten aus dem Atlantik in die Nordsee. Größte Süßwasserzulieferer sind die in die Ostsee mündenden Flüsse, die über das Skagerrak ihren Abfluss in die Nordsee finden. Die Nordseeflüsse entwässern etwa 841.500 Quadratkilometer und bringen pro Jahr ungefähr 296 bis 354 Kubikkilometer Süßwasser ins Meer. Die Ostseeflüsse entwässern mit 1.650.000 Quadratkilometer knapp das doppelte Gebiet und tragen 470 Kubikkilometer Süßwasser jährlich bei.
Entlang der dänischen und norwegischen Küsten fließt das Wasser im Norwegischen Strom in den Atlantik zurück. Dieser bewegt sich vor allem in einer Wassertiefe von 50 bis 100 Metern. Das Brackwasser der Ostsee und aus Nordsee- und Fjorden stammendes Süßwasser sorgen für einen relativ niedrigen Salzgehalt des Stroms. Ein Teil des wärmeren einfließenden Atlantikwassers dreht entlang des Stroms wieder nordwärts und sorgt für einen warmen Kern im Gewässer. Im Winter hat der Strom eine Temperatur von 2 bis 5 °C; die Salinität beträgt weniger als 34,8 Promille. Das durch eine Front getrennte Atlantikwasser der Nordsee ist hingegen über 6 °C warm; der Salzgehalt liegt bei mehr als 35 Promille.[9]
In etwa ein bis zwei Jahren ist das Wasser im Meer komplett ausgetauscht. Innerhalb des Meeres lassen sich anhand von Temperatur, Salzgehalt, Nährstoffen und Verschmutzung klare Wasserfronten erkennen, die im Sommer ausgeprägter sind als im Winter. Große Fronten sind die „friesische Front“, die Wasser aus dem Atlantik von Wasser aus dem Ärmelkanal trennt und die „dänische Front“, die Küstenwasser vom Wasser der Zentralnordsee trennt. Die Einmündungen aus den großen Flüssen gehen nur langsam in Nordseewasser über. Wasser aus Rhein und Elbe beispielsweise lässt sich noch bis zur nordwestlichen Küste Dänemarks klar vom Seewasser unterscheiden.
Die Auswirkung von Stoffeinträgen aus Flüssen und der Atmosphäre auf die Wasserzirkulation lassen sich als komplexe Szenarien nur mit Hilfe von modernen numerischen Verfahren berechnen.
Die Gezeiten werden durch die Gezeitenwellen aus dem Nordatlantik ausgelöst, da die Nordsee selbst zu klein und zu flach ist, um eine nennenswerte Tide auszubilden. Ebbe und Flut wechseln sich in einem Rhythmus von etwa 12 h 25 min ab, genauer: Der Zeitabstand bis zur übernächsten Tide beträgt in der Regel 24 h 50 min. Die Gezeitenwelle läuft bedingt durch die Corioliskraft an der Ostküste Schottlands und Englands in südlicher Richtung und erreicht 10 bis 11 Stunden nach Eintreffen in Schottland die Deutsche Bucht. Sie umläuft dabei zwei oder drei amphidromische Punkte. Eine Amphidromie liegt kurz vor der Straße von Dover. Sie bildet sich durch die Gezeitenwelle, die über den Ärmelkanal einläuft, und beeinflusst die Gezeiten in dem schmalen Gebiet De Hoofden in der Southern Bight zwischen Südengland und Belgien und den Niederlanden. Rechnet man diesen Punkt mit, so braucht die Gezeitenwelle von Nordschottland bis Borkum zwölf Stunden länger. Die beiden anderen amphidromischen Punkte liegen kurz vor der Küste Südnorwegens und auf einer Schnittlinie zwischen Süddänemark und Südschottland über der Jütlandbank auf 55° 25′ N, 5° 15′ O. Sie bilden ein einziges Feld, um das die Gezeiten herumlaufen.
Der Tidenhub liegt an der Küste Südnorwegens bei unter einem halben Meter, erhöht sich aber, je weiter eine Küste von der Amphidromie entfernt liegt. Flache Küsten und trichterartige Verengungen erhöhen den Tidenhub. Am größten ist er in der Wash an der englischen Küste, wo ein Tidenhub von 6,8 Metern erreicht wird. Durch Interferenzen mit den Tidenwellen aus dem Ärmelkanal gibt es an der niederländischen Küste bei Rotterdam[10][11] gespaltene Niedrigwasser und bei Den Helder[12] periodisch zwei- bis dreigipflige Hochwasser. An der deutschen Nordseeküste beträgt der Tidenhub je nach Küstenform und -lage zwischen zwei und viereinhalb Metern. Vor der jütländischen Küste lässt der Tidenhub nach und in Skagerrak und Kattegatt laufen die Gezeitenwellen aus.
In Flachwasserbereichen, also nicht zuletzt in der Deutschen Bucht, wird der tatsächliche Tidenhub jedoch stark von weiteren Faktoren wie der Küstenlage und dem herrschenden Wind oder Sturm beeinflusst (Sturmflut). In den Mündungsgebieten der Flüsse kann ein hoher Wasserstand der Flüsse den Fluteffekt maßgeblich verstärken.
Starke Gezeiten, große algen- und Seetangreiche Flachwasserbereiche, der Strukturreichtum und der große Nährstoffvorrat in der See sorgen für ein vielfältiges Leben in der Nordsee.
Die Nordsee bietet eine Reihe sehr verschiedener Lebensraumtypen, die von unterschiedlichen Biozönosen bewohnt sind. So unterscheidet man grundsätzlich in die Lebensräume der Küstengebiete, die verschiedene Küstentypen wie die Steilküsten, Felsküsten und Sandküsten beinhalten, von den tatsächlichen aquatischen Lebensräumen. Wichtige Übergangsgebiete stellen im Fall der Nordsee zudem die Salzwiesen und die Wattflächen dar, die sich durch einen Wechsel der Lebensbedingungen abhängig von Ebbe und Flut auszeichnen. In der Nordsee liegt das größte und artenreichste Wattenmeer der Welt. Auch die Bereiche der großen Flussmündungen, die Ästuare, die sich durch eine Durchmischung des in die Nordsee fließenden Süßwassers und des salzigen Nordseewassers auszeichnen, stellen einen eigenen Lebensraumtyp dar.
Die aquatischen Lebensräume lassen sich zudem in das Freiwasser, das sogenannte Pelagial, sowie den Gewässerboden, das Benthal, unterscheiden. Die benthischen Lebensräume wiederum unterscheiden sich durch ihre Tiefe sowie durch ihre Bodenbeschaffenheit. So können sie felsig, kiesig oder sandig sein, außerdem können sie mehr oder weniger bis gar keine Schlickschichten tragen.
Die Nordsee leidet durch direkte Einleitungen von Schadstoffen, durch die Schadstoffbelastungen, die die Flüsse mit sich führen, und vor allem in den Küstenregionen unter den Belastungen, die die menschliche Nutzung mit sich bringt. Der Küstenschutz hat an der gesamten südlichen Nordseeküste einen stark landschaftsverändernden Einfluss. Tourismus und Freizeitgestaltung spielen hier eine ambivalente Rolle – zum einen belasten sie die Küstengebiete stark, zum anderen aber geben sie einen direkten ökonomischen Anreiz, die Landschaft weitgehend unversehrt und „schön“ zu erhalten. Wegen Überfischung schrumpfte in den 1970er Jahren vor allem die Population des Nordseeherings. Die Kabeljau-Bestände sind trotz einer gemeinsamen EG-Regulierung aus dem Jahre 1983 in den letzten Jahren extrem zurückgegangen.
Zum Schutz der Nordsee trafen die Anliegerstaaten verschiedene Abkommen. Das Bonner Abkommen von 1969 war das erste internationale Abkommen zum Umweltschutz in der Nordsee und betraf ausschließlich die möglichen negativen Folgen der Ölförderung.
Die Abkommen von Oslo (1972) und Paris (1974) beschäftigten sich erstmals in größerem Maßstab mit Schadstoffen im Meer; in ihrer Folge verabschiedeten die Anliegerstaaten 1992 die Oslo-Paris-Konvention. Für den Umweltschutz an den Küsten sind die Anliegerländer zuständig, die zu diesem Zweck verschiedene nationale Regelungen getroffen haben. In Deutschland bilden die Nationalparks Wattenmeer in Schleswig-Holstein, Niedersachsen und Hamburg die größten deutschen Nationalparks.
Der Plastikmüll in der Nordsee hat in den letzten Jahren nicht abgenommen. 90 % des Mülls besteht aus Kunststoffen. Bei 60 % der untersuchten Eissturmvögel konnte mehr als 0,1 Gramm Kunststoffe im Magen nachgewiesen werden.[13]
Die Küstenlinie der Nordsee hat sich in der Vergangenheit durch Sturmfluten und durch Landgewinnung geändert.
Die Nordsee wird durch ihre größte Insel, Großbritannien, nach Westen begrenzt, allerdings liegt nur deren Ostküste an der Nordsee. Zu den größten Inselgruppen, die komplett in der Nordsee liegen, zählen die Shetlandinseln und Orkney.
Die nördlichen Nordseeküsten sind glazial geprägt durch die großen Gletscher, die auf ihnen zu den verschiedenen Eiszeiten lagen. Dadurch entstand eine stark gegliederte und zerklüftete Küstenlandschaft. Die Fjorde entstanden durch Gletscher, die aus dem Gebirge durch sie hindurchzogen und tiefe Rinnen in den Untergrund schnitten und schabten. Während des folgenden Anstiegs des Meeresspiegels füllten die Fjorde sich mit Wasser. Sie weisen oft steile Küstenlinien auf und sind für Nordseeverhältnisse sehr tief. Fjorde kommen insbesondere an der Küste Norwegens vor.
Fjärde sind ähnlich wie die Fjorde aufgebaut, jedoch meist flacher mit breiteren Buchten, in denen sich auch oft kleinere Inseln befinden. Die Gletscher, die zu ihrer Entstehung führten, konnten den Untergrund auf einem größeren Gebiet beeinflussen und räumten so weitere Strecken des Landes ab. Fjärde finden sich vor allem an der schottischen und nordenglischen Nordseeküste. Einzelne Inseln in den Fjärden oder Inseln und Küste sind heute oft durch Nehrungen oder Halbinseln aus Sandablagerungen verbunden. Lokal heißen diese Tombolos.
Die Fjärde gehen nach Süden in eine Kliffküste über, die vor allem aus Moränen der Eiszeitgletscher entstanden sind. Durch den horizontalen Aufprall der Nordseeküste entstehen hier Abbruchküsten; das Material, das dabei abbricht, ist wichtiger Sedimentlieferant für das Watt auf der anderen Seite der Nordsee. Große Ästuare (Trichtermündungen) mit den dazugehörigen Watt- und Marschgebieten unterbrechen diese Kliffküste. Große Mündungen im Süden Englands gehören zu den Flüssen Themse und Humber.
Sowohl in Südnorwegen als auch an der schwedischen Küste des Skagerraks finden sich Schären. Entstanden ähnlich wie Fjorde und Fjärde hatten hier die Gletscher noch größeren Einfluss auf die Landschaft, so dass diese weiträumig abgetragen wurde. Strandflaten, die sich vor allem in Südnorwegen finden, sind Gesteinsplatten, die oft mehrere Kilometer Ausdehnung haben, fast vollkommen abgeschliffen wurden und heute oft wenige Meter unter der Meeresoberfläche liegen.
Die Flachküste der südlichen und östlichen Küste bis hinauf nach Dänemark ist in ihren Grundzügen zwar ebenfalls eiszeitlich geformt, ihre Form wird jedoch vor allem durch das Meer und Sedimentablagerungen bestimmt. Der gesamte Küstenverlauf ist flach, die Tiden überschwemmen oft große Landstriche und geben diese danach wieder frei. Das Wasser lagert Sedimente ab. Im mikrotidalen Bereich (bis 1,35 Meter Tidenhub), wie etwa an der niederländischen oder der dänischen Küste, bilden sich Strandwälle mit Dünen. Im mesotidalen Bereich (1,35 bis 2,90 Meter Tidenhub) bilden sich Barriereinseln, im makrotidalen Bereich (über 2,90 Meter), wie etwa in der Elbmündung, bilden sich unterseeische Sandbänke.
Die niederländischen West- und die deutschen Ostfriesischen Inseln sind Barriereinseln. Sie entstanden an den Brandungskanten des Meeres, an denen durch die Brandung Sedimente aufgeschüttet und hinter denen durch die brechenden Wellen Sedimente abgetragen wurden. Im Laufe der Zeit sammelten sich so Sandplaten an, die schließlich nur noch von Sturmfluten überschwemmt wurden. Die ersten Pflanzen begannen auf den Sandbänken zu siedeln, das Land verfestigte sich.
Obwohl sie heute befestigte Inseln sind, befinden sich einige von ihnen auch weiterhin in Bewegung. Für die ostfriesische Insel Juist beispielsweise sind seit 1650 fünf verschiedene Kirchplätze nachweisbar, da der Ort des Kirchenbaus mit der sich verlagernden Insel Schritt halten musste. Zeitweise bestand Juist auch aus zwei Inseln, bevor es wieder zusammenwuchs. Die benachbarte Insel Wangerooge verschob sich in den letzten dreihundert Jahren einmal um ihre komplette Länge nach Osten. Aufgrund der herrschenden Umweltbedingungen wird auf den Ostfriesischen Inseln an den Westküsten Land abgetragen, während sich an den Ostküsten Sedimente ablagern. Die Westküsten werden deshalb heutzutage verstärkt von den Menschen geschützt. Der Wattstrom (auch Balje, Gatt oder Tief) zwischen den Inseln dient zum Durchfluss der Gezeiten, so dass dort die Strömung ein Zusammenwachsen der Inseln verhindert.
Die Nordfriesischen Inseln sind hingegen aus den Resten alter Geestkerninseln entstanden, die durch Sturmfluten und Wassereinwirkungen teilweise abgetragen und vom Hinterland getrennt wurden. Sie sind deshalb oft höher und in ihrem Kern weniger stark Veränderungen ausgesetzt als die südlich liegenden Inseln. Außerhalb des Kerns finden sich aber dieselben Prozesse wie an West- und Ostfriesischen Inseln, besonders ausgeprägt auf Sylt, wo ein Durchbruch der Insel im südlichen Bereich droht, während der Lister Hafen im Norden versandet.
Die Halligen sind Reste des in mittelalterlichen Sturmfluten untergegangenen Marschlandes. Ihre Gestalt war in der Vergangenheit großen Veränderungen ausgesetzt. Von einmal über hundert Halligen existieren heute nur noch zehn, die übrigen sind entweder abgetragen oder ans Festland angedeicht worden.
Die sich nördlich anschließenden Dänischen Wattenmeerinseln sind aus Sandbänken entstanden. Noch bis in das 20. Jahrhundert war die Versandung der Inseln ein großes Problem. Zum Schutz der Inseln wurden kleinere Wälder angelegt.
An der südöstlichen Küste finden sich ebenfalls viele ausgedehnte Ästuare wie die von Maas, Rhein, Weser, Elbe oder Eider.
Besonders die Southern Bight veränderte sich durch Landgewinnung, denn die Niederländer waren dabei besonders aktiv; das größte Projekt dieser Art war die Abdeichung und die Landgewinnung am IJsselmeer.
Zwischen Esbjerg (Dänemark) im Norden und Den Helder (Niederlande) im Westen erstreckt sich das Wattenmeer. Dies ist eine von Ebbe und Flut geprägte Landschaft, von der wichtige Teile mittlerweile zum Nationalpark erklärt wurden. Die Insel Helgoland bildet einen Ausnahmefall, da sie nicht durch das auflaufende Watt entstand, sondern erheblich älter ist und aus Buntsandstein besteht. Die Festlandsküste im Bereich des Wattenmeers ist bis auf kurze Abschnitte, etwa bei Schobüll und Cuxhaven-Duhnen, durch Deiche gesichert.
Sturmfluten gefährden besonders die Küsten der Niederlande, Belgiens, Deutschlands und Dänemarks. Diese sind relativ flach, so dass bereits eine relativ geringe Erhöhung des Wasserstandes ausreicht, um weite Landstriche unter Wasser zu setzen. Zudem sind Stürme aus westlichen Richtungen an der Nordsee besonders heftig, so dass die gefährdetsten Stellen die südöstlichen Küsten sind. Winde aus Nordwest treffen dabei vor allem die Niederlande und die niedersächsische Küste, Winde aus West- bis Südwest die schleswig-holsteinische Küste. Im Laufe der Geschichte kosteten Sturmfluten hunderttausenden Menschen das Leben, diese Fluten formten maßgeblich die heutige Küstengestalt mit. Bis in die frühe Neuzeit hinein lagen die Opferzahlen oft bei mehreren zehntausend oder gar hunderttausend Opfern pro Flut. Inwieweit diese Zahlen zuverlässig sind, kann aber nach heutigem Wissen nur schwer eingeschätzt werden.
Die erste aufgezeichnete Flut war die Julianenflut in den Niederlanden, deren Datumsangabe (17. Februar 1164) allerdings heute bezweifelt wird.[14] Die Erste Marcellusflut 1219 traf vor allem Westfriesland und Ostfriesland, das damals noch bis zur Weser reichte; mit ihr begann der Jadebusen zu entstehen. Bei der Sturmflut von 1228 überliefern die Chroniken 100.000 Tote. Die Zweite Marcellusflut oder Grote Mandränke von 1362 traf Süd- und Ostküste der Deutschen Bucht, wieder überliefern die Chroniken 100.000 Tote, die vordere Küstenlinie Nordfrieslands wurde weitgehend zerstört und große Landflächen dauerhaft an die See verloren. Dabei versank auch die heute sagenumwobene Stadt Rungholt. Die Insel Strand entstand. Bei der Burchardiflut (Zweite Grote Mandränke) 1634 wurde unter anderem die Insel Strand zerstört. Übrig blieben die Halligen. Bei der Neujahrsflut 1721 wurde die Düne von Helgoland getrennt.
Im 20. Jahrhundert trafen schwerwiegende Sturmfluten die Niederlande mit der Hollandsturmflut, die am 1. Februar 1953 2.000 Tote zur Folge hatte, und die Sturmflut 1962 am 16./17. Februar Hamburg, bei der 315 Personen starben. Die „Jahrhundertflut“ von 1976 und die „Nordfrieslandflut“ von 1981 brachten die höchsten bisher gemessenen Wasserstände an der Nordseeküste. Da nach der Hamburger Flut jedoch der Deichbau und der Küstenschutz erheblich verbessert worden war, kam es hier nur zu Sachschäden.
Vom 26. bis zum 28. Februar 1990 wurden innerhalb von drei Tagen fünf Fluten vom Sturm auf maximale Höhen getrieben. In Büsum wurden Windgeschwindigkeiten bis 160 km/h gemessen. Aufgrund des verbesserten Küstenschutzes kam es jedoch nur zu einigen Sachschäden.[15]
Der Übergangsbereich zwischen Land und Meer an den Gegenden mit flacher Küste war ursprünglich stark amphibisch geprägt. Das Land bestand aus zahlreichen Inseln und Halligen, die durch Flüsse, Bäche und Moore getrennt waren. Das „Festland“ wurde regelmäßig überflutet. In den besonders durch Sturmfluten bedrohten Gegenden siedelten die Menschen zuerst auf natürlichen Erhebungen wie Geestzungen, Dünen oder Uferwällen. Letztere boten aber nur in Phasen sinkender Meeresspiegel hinreichend Schutz. So wurden schon im 1. bis 4. Jahrhundert Siedlungen auf Warften errichtet – künstlichen Hügeln von teilweise mehreren Metern Höhe.[16] Die zweite Warftenperiode begann im 7. Jahrhundert und hielt bis ins 20. Jahrhundert an.
Die ersten Deiche waren kleine Ringdeiche um einzelne Felder, die im Sommerhalbjahr ausreichten, die Feldfrüchte, vor allem Hafer und Pferdebohnen, bis zur Ernte zu schützen, aber von den schweren Sturmfluten des Winterhalbjahrs überflutet wurden.[14] Ab dem Beginn des Hochmittelalters begannen die Menschen, die vereinzelten Ringdeiche zu einer Deichlinie direkt an der Küste zusammenzufassen und so langfristig den amphibischen Bereich zwischen Land und Meer in Festland zu verwandeln.
Zwar war man schon im 13. Jahrhundert stolz auf den „Goldenen Ring“, einen Deich in gleicher Höhe um ganz Friesland, aber zunächst war die Koordination noch schlecht und die Mittel der einzelnen Landgemeinden unzureichend. Zudem lag bei örtlicher Selbsthilfe die Last der Reparatur von Deichbrüchen bei denjenigen, die am stärksten von einem Meereseinbruch geschädigt worden waren. Erst staatliche Koordination und wirtschaftliche Potenz wie die der Grafschaft Oldenburg konnte die Dienste der Marschbauern und kommerzieller Unternehmer zu effektiven Deichbauten zusammenfassen.[17][18] Vorbild beim Deichbau waren jahrhundertelang die Niederlande, noch heute ist dort der Rijkswaterstaat die mächtigste Behörde im Lande. Von ihnen wurden mit technischen Errungenschaften auch Irrwege übernommen. Da Erdarbeiten ohne maschinelle Hilfe sehr aufwändig sind und mancherorts auf weichem Untergrund nicht einmal Fuhrwerke (Stürzkarren) eingesetzt werden konnten, stützte man die Flanken der Deiche mit Holzkonstruktionen, um größere Deichhöhen zu erreichen. Die so gebauten Stackdeiche erwiesen sich bei Sturmfluten als anfällig gegen überschlagende Wellen. Zudem wurde das verbaute Holz zunehmend von der Schiffsbohrmuschel zerfressen, die durch den internationalen Seeverkehr aus tropischen Gewässern eingeschleppt worden war. Mit der Verfügbarkeit von Baumaschinen konnten ab dem späten 19. Jahrhundert immer größere Erdmassen zu breiteren und höheren Deichen aufgehäuft werden. Auf besonders weichem Untergrund werden aber Deiche durch Spundwände verstärkt, weil zusätzlich aufgeschüttete Erde im Untergrund versinkt.[14]
Eine der ersten großen Maßnahmen, dem Meer durch Verkürzung der Deichlinie weniger zu bieten, war der 1593 (Vorarbeiten) bis 1615 angelegte Ellenser Damm. Als größtes Einzelbauwerk entstand 1927 bis 1932 der Abschlussdeich, der die Zuiderzee zum IJsselmeer machte. Nach der niederländischen Watersnood 1953 und der Sturmflut 1962 an der deutschen Nordseeküste und in Hamburg wurden nicht nur die Deiche noch einmal erhöht. Seither wurden vor allem im Rhein-Maas-Schelde-Delta aber auch an der deutschen Nordseeküste zahlreiche Flussmündungen und Meeresarme durch Sperrwerke gesichert. Um die Küste als natürlichen Lebensraum nicht zu sehr zu beeinträchtigen, sind diese Sperrwerke zunehmend so eingerichtet und gesteuert, dass sie normale Gezeitenströme ganz oder teilweise zulassen und nur bei Sturmflut geschlossen werden, vgl. die Renaturierung der Luneplate.
Heutiger Küstenschutz an der flachen Nordseeküste besteht aus mehreren Ebenen. Das Deichvorland nimmt dem Meer bereits einiges an Kraft, mit dem es auf den Deich treffen kann. Liegt der Deich direkt am Meer, ist ein besonders gesicherter Schardeich notwendig. Der Seedeich wurde im Laufe der Zeit immer höher (bis zu 10 Meter) und bekam ein flacheres Profil, um ebenfalls die Angriffskraft der Wellen zu schwächen. Moderne Deiche sind bis zu 100 Meter breit. Dahinter folgt ein Deichverteidigungsweg und meist weiteres dünn besiedeltes Land. Ältere Deichlinien im Hinterland werden mancherorts als zusätzlicher Schutz erhalten, vielerorts aber abgetragen, in Marschen und Poldern ist selbst Erde kostbar.
Auch Dünen tragen zum Küstenschutz bei. Mancherorts, besonders an der holländischen Küste zwischen Hoek van Holland und Den Helder, bilden sie den alleinigen Schutz. Andernorts, etwa in Zeeland und auf einigen Nordfriesischen Inseln, wurden sie durch Deiche verstärkt. Sie werden heute mit Strandhafer bepflanzt, um sowohl Erosion durch Wind und Wasser als auch das Wandern der Dünen selbst zu vermindern. Besonders aufwändige Maßnahmen des Küstenschutzes sind die Deltawerke in den Niederlanden oder Sandvorspülungen vor der deutschen Insel Sylt.
Angesichts fortschreitender Klimaänderung und einem möglichen weiteren Anstieg des Meeresspiegels gibt es erste Überlegungen, ob es in Zukunft notwendig werden könnte, die ganze Nordsee mit zwei Dämmen abzusperren.[19] Der niederländische Ozeanologe Sjoerd Groeskamp sagte zu seiner Studie, dass er diese Lösung, mit zwei Dämmen zwischen Frankreich und England und zwischen Schottland und Norwegen, zwar für technisch möglich, zugleich aber auch sehr unerwünscht hält.[20] Die Dämme würden die Nordsee für immer verändern, weil aus Salzwasser Brackwasser würde und die Fischerei sehr darunter leiden würde.[21] Zugleich warnte Groeskamp wie verheerend die Folgen sein würden, wenn sich der Meereswasserpegel wirklich um 10 Meter anheben würde. Es gilt, die Klimawanderung auszubremsen, bevor drakonische Maßnahmen notwendig werden, so betonte Groeskamp in seinen Medienauftritten. Groeskamps Projekt wurde Northern European Enclosure Dam (NEED) getauft und sollte der Bevölkerung in Nordwesteuropa als Warnung dienen. Vorher gab es bereits Entwürfe für einen Deich von Calais bis Göteborg (De Haakse Zeedijk), der in gekürzter Form nur die Niederlande schützen würde.[22]
Die südliche Nordseeküste ist sehr dicht besiedelt und wird dementsprechend stark genutzt. In einem 150-Kilometer-Bereich an der Küste leben 80 Millionen Menschen, davon fast die gesamte Bevölkerung der Niederlande und Belgiens, fast alle davon in urbanen Gegenden. In diesen Bereichen haben die Küstenregionen eine Bevölkerungsdichte von über 1.000 Einwohner pro Quadratkilometer. Der Küstenabschnitt zwischen Hamburg und Brüssel ist stark industrialisiert. Hier findet sich eine der größten Ansammlungen von Schwerindustrie weltweit.
Kanalverbindungen:
Obwohl die faktische Kontrolle der Nordsee seit der Zeit der Wikinger entscheidend für die Machtverhältnisse in Nordwesteuropa war und sich seit dem Ersten Englisch-Niederländischen Seekrieg zur Frage der Weltpolitik entwickelte, gehörte die Nordsee bis nach dem Zweiten Weltkrieg juristisch niemandem, die angrenzenden Staaten nahmen nur schmale Küstengewässer für sich in Anspruch. In den letzten Jahrzehnten hat sich dies allerdings gewandelt.
Die an die Nordsee angrenzenden Länder beanspruchen die Zwölfmeilenzone.[23] Die seewärtige Grenze dieser Zone bildet die Grenze des deutschen Hoheitsgebietes. Die Fläche der Nordsee innerhalb des Hoheitsgebietes ist als Seewasserstraße eine Bundeswasserstraße.
In der Zwölfmeilenzone nehmen die Länder beispielsweise das exklusive Recht zur Fischerei wahr. Island konnte in den sogenannten Kabeljaukriegen international eine 200-Meilen-Zone der Fischfangrechte durchsetzen, der sich die EU-Staaten anschlossen und so faktisch die Nordsee gegenüber anderen Ländern verschlossen. Der Fischfang ist auf EU-Staaten und den Anrainerstaat Norwegen begrenzt; andere Länder müssen spezielle Abkommen schließen. Die Koordination beruht auf der gemeinsamen Fischereipolitik der EU und Verträgen zwischen der EU und Norwegen.
Nachdem unter der Nordsee Bodenschätze gefunden worden waren, nahm Norwegen die Rechte des Übereinkommens über den Festlandsockel für sich in Anspruch, woraufhin die anderen Staaten ebenso verfuhren. Der Nordseeboden ist weitgehend entsprechend dem Mittellinienprinzip aufgeteilt, nach dem die Grenze zwischen zwei Küstenstaaten auf einer gedachten Mittellinie liegt. Nur zwischen den Niederlanden, Deutschland und Dänemark wurde der Boden nach langwierigen Auseinandersetzungen und einem Spruch des Internationalen Gerichtshofs[24] anders verteilt. Da Deutschland aufgrund seiner geografischen Positionen sonst nur einen sehr kleinen Teil Boden im Verhältnis zur Küstenlinie bekommen hätte, gehört nun noch ein weiteres Feld, der sogenannte Entenschnabel, zur deutschen ökonomischen Zone.
In Bezug auf Umweltschutz und Meeresverschmutzung hat die 25- bzw. 50-Meilen-Zone des MARPOL (marine pollution)-Abkommens Geltung. Die Oslo-Pariser-Abkommen beschäftigen sich ebenfalls mit Fragen des Meeresschutzes in der gesamten Nordsee. Im Wattenmeer sind jeweils die nationalen Staaten zuständig, die dieses Problem national unterschiedlich lösen; um eine gemeinsame Politik in Bezug auf das Wattenmeer zu gewährleisten, tagt die Trilaterale Wattenmeerkommission.
Für Schiffssicherheit und eine Koordinierung des Seeverkehrs soll die Europäische Agentur für die Sicherheit des Seeverkehrs sorgen, die Anfang 2003 ihre Arbeit aufnahm. Die Kommission gehört zur EU, Norwegen und Island haben als direkt betroffenen Staaten ebenfalls einen Sitz in ihr. Nach dem 1978 verabschiedeten Paris Memorandum of Understanding haben sich unter anderem alle EU-Staaten verpflichtet regelmäßig 25 Prozent der Schiffe, die einen EU-Hafen anlaufen auf die Einhaltung internationaler Sicherheitsbestimmungen zu überprüfen. Das Wattenmeer und die Küsten Großbritanniens, Belgiens und Frankreichs wurden als Particularly Sensitive Sea Area ausgezeichnet. In der Nordsee gelten ebenso wie in der Ostsee die strengsten Bestimmungen der MARPOL-Konventionen zur Abwasser- und Müllentsorgung vom Schiff aus.
1958 entdeckten Geologen bei Slochteren in der niederländischen Provinz Groningen ein Erdgasfeld. Es stand zu vermuten, dass sich weitere Felder unter der Nordsee befinden würden, jedoch waren zu diesem Zeitpunkt die Besitzrechte an der Nordsee im Hochseebereich unklar. 1966 begannen Probebohrungen, 1969 entdeckte die Phillips Petroleum Company im norwegischen Sektor das Ekofisk-Feld – damals eines der 20 größten Erdölfelder der Welt, das sich zudem durch sehr hochwertiges schwefelarmes Öl auszeichnete. Die erste kommerzielle Ausbeutung erfolgte ab 1971, das Ekofisk-Öl wurde erst mit Tankern, ab 1975 mit einer Pipeline ins englische Cleveland und seit 1977 auch mit einer weiteren Pipeline ins deutsche Emden geleitet. In größerem Maßstab beuten die Ölkonzerne die Vorräte der Nordsee jedoch erst seit der Ölkrise aus, als der international steigende Ölpreis dies wirtschaftlich attraktiv machte und die notwendigen hohen Investitionen ermöglichte. In den 1980ern und 1990ern folgten weitere große Entdeckungen von Ölfeldern. Obwohl die Produktionskosten vergleichsweise hoch sind, haben die hohe Qualität des zu fördernden Öls, die politische Stabilität der Region und die Nähe zu den Absatzmärkten Westeuropas die Nordsee zu einer wichtigen Ölregion werden lassen.
Mittlerweile gibt es im Meer 450 Bohrinseln, die Nordsee ist das wichtigste Gebiet der Offshore-Förderindustrie. Die meisten Plattformen befinden sich im britischen Sektor der Nordsee, gefolgt vom norwegischen, dem niederländischen und dem dänischen Sektor. Der britische und der norwegische Sektor enthalten dabei mit Abstand die größten Ölreserven. Schätzungen gehen davon aus, dass sich allein im norwegischen Sektor 54 Prozent der Öl- und 45 Prozent der Gasreserven befinden. Bedeutende Ölfelder sind neben dem Ekofisk-Feld auch das norwegische Statfjord-Feld, zu dessen Erschließung erstmals die Norwegische Rinne mit einer Pipeline überwunden wurde. Das norwegische Staatsunternehmen Statoil erhält, einem norwegischen Gesetz entsprechend, mindestens 50 Prozent der Anteile an Ölfeldern, die im norwegischen Sektor liegen. Das größte Erdgasfeld der Nordsee ist das Troll-Feld. Es liegt in der Norwegischen Rinne in einer Tiefe von 345 Metern, so dass große Anstrengungen unternommen werden mussten, um es überhaupt zu erschließen. Die Bohrplattform ist mit 472 Metern Höhe und 656.000 Tonnen Gewicht die größte Offshore-Bohrplattform und das größte jemals von Menschen transportierte Objekt.
Im deutschen Sektor befinden sich nur zwei Plattformen, es handelt sich bei ihm um den am wenigsten erschlossenen Sektor in dieser Hinsicht. Das größere der beiden Felder ist das Ölfeld Mittelplate.
Ihren Hochstand erreichte die Förderung 1999, als fast 6 Millionen Barrel (950.000 Kubikmeter) Erdöl und 280.000.000 Kubikmeter Erdgas täglich gefördert wurden. Mittlerweile gilt die Nordsee als erschlossenes Rohstoffgebiet, in dem kaum noch größere Entdeckungen zu erwarten sind. Alle großen Ölkonzerne sind an der Förderung beteiligt, in den letzten Jahren haben aber große Ölkonzerne wie Shell oder BP die Ölförderung in dem Gebiet bereits eingestellt und die Fördermenge geht seit 1999 aufgrund fehlender Reserven kontinuierlich zurück. Der Preis von Brent Crude, einer der ersten in der Nordsee geförderten Ölsorten, wird heute als Standard- und Vergleichspreis für Erdöl aus Europa, Afrika und dem Nahen Osten genutzt.
Neben Öl und Gas entnehmen die Anrainerstaaten dem Meeresboden jedes Jahr mehrere Millionen Kubikmeter Sand und Kies. Diese werden vor allem für Bauvorhaben, zur Sandaufschüttung an Stränden und zum Küstenschutz verwendet. Größte Entnehmer 2003 waren die Niederlande (etwa 30 Millionen Kubikmeter) und Dänemark (etwa 10 Millionen Kubikmeter im Nordseeraum). 2005 entnahm Deutschland der Nordsee etwa 740.000 Kubikmeter.[25]
Aufgrund der geologischen Entstehung befinden sich unter der Nordsee auch umfangreiche Kohleflöze. Im jüngsten Report der British Geological Survey (BGS) werden die Vorräte auf drei Billionen Tonnen bis 23 Billionen Tonne Kohle geschätzt. Um diese unterseeischen Mengen zu nutzen, plant das Unternehmen „Five-Quarter“ durch „Deep Gas Winning“ eine umweltfreundliche Variante zu finden. Dazu würden in sehr dünnen Bohrungen Sauerstoff und ultraerhitzter Wasserdampf injiziert und es wird Synthesegas, Wasserstoff und Kohlenmonoxid, sowie Methan und Kohlendioxid in den Lagerstätten unter Wasser freigesetzt. Diese Art des Abbaus erfordert keinen Zusatz weiterer Chemikalien wie beim Fracking an Land.[26]
Die Nordsee-Anrainerstaaten, allen voran Großbritannien und Dänemark, nutzen seit dem Ende der 1990er Jahre die küstennahen Bereiche der Nordsee zur windbetriebenen Stromproduktion. Erste Windkraftanlagen entstanden vor der englischen Küste (Blyth im Jahre 2000) sowie der dänischen Küste (Windpark Horns Rev im Jahre 2002).
Seit 2001 bestehen Planungen, auch in der deutschen Wirtschaftszone der Nordsee Offshore-Windparks zu errichten, welche die gegenüber Windparks an Land erheblich stärkeren und gleichmäßigeren Winde auf See nutzen können. Bisher wurden 697 Windkraftanlagen an 10 Standorten genehmigt (Stand Dezember 2005). Gegen diese Windparks werden jedoch auch Bedenken vorgetragen: Befürchtet werden beispielsweise Schiffskollisionen und eine Beeinträchtigung der Meeresökologie, vornehmlich während des Fundamentbaus. Hinzu kommt, dass die Entfernung zu den Abnehmern zu einem Transportverlust von Energie führt und der Neubau von Leitungen im Wattenmeer erforderlich sein könnte, das jedoch fast komplett als Biosphärenreservat und Nationalpark ausgewiesen ist.
Energiegewinnung aus dem Meer befindet sich noch in den Anfangsstadien. Während die südliche Nordsee nach Meinung der meisten Experten zu geringen Tidenhub, Wellen und Strömungen für derartige Versuche aufweist, könnten sich an den Küsten Norwegens und am Übergang zwischen Nordsee und Irischer See geeignete Stellen für Wellen- und Strömungskraftwerke finden. Erste Versuche mit dem Wellenkraftwerk Wave Dragon wurden von 2003 bis zum Januar 2005 an der dänischen Küste abgeschlossen. Eine Mini-Pilotanlage für ein Osmosekraftwerk existiert in der Nähe der norwegischen Stadt Trondheim.
Seit etwas über hundert Jahren wird an der südlichen Nordseeküste Fischfang in kommerziellem Ausmaß praktiziert. Fischfang in der Nordsee konzentriert sich auch heute noch auf den südlichen Teil und die Küstengewässer, wobei vor allem mit Grundschleppnetzen gearbeitet wird.
Durch stetige technische Weiterentwicklung dehnten sich die Fangmengen bis in die 1980er Jahre beständig aus, bis sie mit etwa 3 Millionen Tonnen pro Jahr einen Höchststand erreichten. Seitdem ging die Fangmenge zurück, heute werden etwa 2,3 Millionen Tonnen pro Jahr gewonnen, aber mit teilweise erheblichen Unterschieden in einzelnen Jahren. Neben dem angelandeten Fisch gehen Schätzungen zufolge jährlich in der Nordsee ca. 150.000 Tonnen nicht marktfähiger Beifangfisch und rund 85.000 Tonnen tote oder geschädigte Wirbellose als Beifang wieder über Bord.
Vom angelandeten Fisch wird etwa die Hälfte zu Fischmehl und Fischöl verarbeitet. Zu den wichtigen gefangenen Fischen gehören Makrele, Kabeljau, Schellfisch, Wittling, Seelachs, Scholle und Zungen. Ebenfalls werden Nordseegarnele, Hummer und Krabben (Kurzschwanzkrebse) gefangen. Verschiedene Muschelarten wie Miesmuscheln, Kammmuscheln oder Austern werden in Kulturen gezüchtet, so dass man bei der Ernte nicht von Fischerei im eigentlichen Sinne sprechen kann.
Der Fischfang in einer so dicht besiedelten Umgebung auf technischem Hochstand bringt die Gefahr der Überfischung mit sich.
Obwohl die Fangquoten seit 1983 von der EG/EU reguliert werden, leiden vor allem Schellfisch und Kabeljau durch den Fang. Alleine die Schleppnetzfischerei Dänemarks kostet jährlich 5.000 Schweinswale das Leben. Seit den 1960er Jahren wurde versucht, die Fischbestände durch verschiedene Regelungen wie bestimmte Fangzeiten, eine begrenzte Zahl von Fischereischiffen usw. zu schonen, diese Regeln wurden aber nicht systematisch angewandt, so dass sie kaum Entlastung brachten. Seitdem mit dem Vereinigten Königreich und Dänemark zwei wichtige Fischereinationen Mitglied der Europäischen Gemeinschaft wurden, versuchen diese mit Hilfe der Gemeinsamen Fischereipolitik das Problem in den Griff zu bekommen, Norwegen hat in der Hinsicht verschiedene Abkommen mit der EG getroffen.
Zahlen stammen von der FAO, zitiert nach der University of British Columbia. In der FAO-Fangregion „Nordsee“ sind Skagerrak und Kattegat eingeschlossen.[27]
Im Einzugsbereich der Flüsse, die in die Nordsee münden, leben auf ungefähr 850.000 Quadratkilometern etwa 160 Millionen Menschen. Die Ströme entwässern einen Großteil Westeuropas, darunter ein Viertel Frankreichs, drei Viertel Deutschlands, fast die gesamte Schweiz und Großbritannien, die Hälfte Jütlands, die gesamten Niederlande und Belgien, den Süden Norwegens sowie kleine Teile von Österreich. In diesem Bereich findet sich die größte Ansammlung weltweiter Industrie, allein 15 Prozent der Weltindustrieproduktion finden im Einzugsbereich der Nordsee statt.
Europas größte Häfen befinden sich an der Nordsee. Dabei konzentriert sich die Schifffahrt vor allem auf sechs große Häfen. Die kleineren Regionalhäfen haben an Bedeutung verloren; der Containerbetrieb in den vier größten Häfen (Rotterdam, Antwerpen, Hamburg und Bremen/Bremerhaven) hat sich von 1991 bis 2000 um etwa zwei Drittel erhöht. Mit Abstand größter und wichtigster Hafen ist Rotterdam. Nach eigener Auskunft ist das Hinterland des Hafens ganz Europa. Es gibt wöchentliche Feeder-Verbindungen in 140 andere Städte. Skandinavien und der Ostseeraum werden hauptsächlich über Bremerhaven und Hamburg bedient. Ein Sammelbegriff für (wichtige) Nordseehäfen ist Nordrange.
In der Nordsee fanden in den frühen 1990ern 27,5 Prozent der weltweiten Schiffsbewegungen statt, mit steigender Tendenz. Der größte Teil dieser Bewegungen fand in der südlichen Nordsee statt, wiederum ein größerer Teil davon auf der Schifffahrtsstraße zwischen Elbmündung und Ärmelkanal. Seit den späten 1960er Jahren gilt er in der Nordsee ein System der Zwangswege: um den Schiffsverkehr möglichst reibungslos und unfallfrei zu gestalten, werden sowohl spezielle Tiefwasserwege ausgewiesen als auch sich behindernder Schiffsverkehr systematisch getrennt. Die wichtigsten Tiefwasserwege laufen von der Straße von Dover in die Deutsche Bucht. Große Häfen haben jeweils eigene Zugangswege; im Bedarfsfall (nämlich dann wenn sich Sedimente in Fahrrinnen abgelagert haben) stellen Baggerschiffe wieder die benötigte Mindest-Wassertiefe her.
Die Nordsee ist viel befahren; auf ihr verlaufen wichtige Handels- und Verkehrswege. Unter Seefahrern ist sie berüchtigt, unter anderem wegen des „Blanken Hans“ und der Untiefen wie der „Große Vogelsand“. Grundseen und sehr schwerer Seegang zu Zeiten der Sturmfluten in Frühling und Herbst haben zu vielen Schiffsunglücken geführt, die in früheren Zeiten gelegentlich auch Strandräubern als Verdienstquelle gedient haben.
An den Küsten werden sowohl die Strände als auch die Küstengewässer touristisch genutzt. Touristisch besonders erschlossen sind dabei die belgische, niederländische, deutsche und dänische Küste. In Großbritannien gibt es einzelne Touristenorte an der Nordseeküste. Der Küstentourismus konzentriert sich in England auf die Kanalküste.
Windsurfen und Segeln sind wegen des immer vorhandenen Windes beliebte Wassersportarten. Die Nordsee gilt wegen der starken Gezeiten und der vielen Flachwassergebiete in Küstennähe als wesentlich schwieriger zu segelndes Gebiet als Ostsee oder Mittelmeer, so dass hier weit weniger Segler unterwegs sind als an den anderen Küsten.
Das Wattwandern an den nordfriesischen Inseln und Halligen, den dänischen und ostfriesischen Inseln, aber auch Angeln und Sporttauchen, beispielsweise das Wracktauchen bei Scapa Flow, ist möglich.
Die besonderen klimatischen Bedingungen an z. B. der deutschen Nordseeküste gelten als gesundheitsfördernd. Bereits im 19. Jahrhundert nutzten Reisende ihren Aufenthalt an der Küste als Kur-Urlaub. Die günstigen Klimafaktoren von Luft, Temperatur, Wasser, Wind und Sonnenstrahlung aktivieren Abwehrkräfte und Kreislauf, stärken das Immunsystem und wirken heilend insbesondere auf Haut und Atemwege. Im Sinne der Thalasso-Therapie werden neben den klimatischen Gegebenheiten dabei zur Kuranwendung auch Meerwasser, Schlick, Sole, Algen und Meersalz als Heilmittel genutzt.
Eine Besonderheit waren in Deutschland die bis in die 1990er Jahre durchgeführten Butterfahrten, die als Schiffsfahrten außerhalb der Hoheitsgewässer einen zollfreien Einkauf ermöglichten.
Der Atlas Geographike Hyphegesis des Claudius Ptolemäus aus dem 2. Jahrhundert n. Chr. führt die Nordsee unter dem griechischen Namen Γερμανικὸς Ὠκεανός Germanikòs Ōkeanós. Dieser Name gelangte durch Lehnübersetzung als Oceanus Germanicus oder Mare Germanicum ins Lateinische, von da ins Englische als German Sea und ins Deutsche als Deutsches Meer.[2]
Die im spätmittelhochdeutschen belegte Bezeichnung nordermer oder nortmer wurde im 17. Jahrhundert durch den heute geläufigen Namen Nordsee ersetzt (niederländisch Noordzee). In der niederländischen Sprache bildet die Noordzee ein Gegensatzpaar mit der Zuidersee – der ‚südlichen See‘, von Friesland und der Nordseeküste aus gesehen.[28] Bedingt durch die Verbreitung des von den Hansekaufleuten genutzten Kartenmaterials setzte sich der Name Nordsee (englisch North Sea, frz. Mer du Nord etc.) allmählich europaweit durch.
Daneben gebräuchliche Namen waren lange Zeit Mare Frisicum (Friesisches Meer) und Westsee, dessen dänische Entsprechung Vesterhav neben Nordsø heute noch üblich ist.
Karte (c. 1375 – c. 1425) aus der Geographike Hyphegesis von Claudius Ptolemäus: griechisch Γερμανικός Ὠκεανός Germanikós Ōkeanós
Karte von Gerhard Mercator, 1596: Oceanus Germanicus
Atlas von Willem Blaeu, 1631: Oceanus Germanicus oder De Noordt Zee
Karte des Herzogtums Schleswig, 1650: Mare Cimbricum (Kimbrisches Meer) oder Westsee
Französische Karte, 1771: Mer d’Allemagne (Deutsches Meer)
Französische Karte, 1780: Mer du Nord (Nordsee) für den nördlichen Abschnitt, Mer d’Allemagne für den südlichen Abschnitt
Die erste geschichtlich verbürgte intensive Nutzung der Nordsee als Verkehrsweg erfolgte durch die Römer. 55 und 54 v. Chr. drang Julius Caesar in Britannien ein (siehe Caesars Britannienfeldzüge). 12 v. Chr. ließ Drusus eine Flotte von über 1000 Schiffen bauen und über den Rhein in die Nordsee segeln. Der überlegenen Zahl, Taktik und Technik der Römer hatten die Friesen und Chauken nichts entgegenzusetzen, und als die Römer zu den Mündungen von Weser und Ems vordrangen, mussten sich die dort ansässigen Stämme ergeben.
5 v. Chr. konnten die römischen Kenntnisse über die Nordsee im Rahmen eines militärischen Vorstoßes unter Tiberius bis hin zur Elbe deutlich erweitert werden: Plinius der Ältere beschreibt, dass römische Seeverbände an Helgoland vorbeikamen und sich bis an die Nordostküste Dänemarks vorwagten.
Mit der Eroberung Britanniens durch Aulus Plautius (43 n. Chr.) begann ein reger und regelmäßiger Schiffsverkehr zwischen den Häfen in Gallien (Portus Itius) und denen in England. Die römische Ära dauerte knapp 350 Jahre und endete mit dem Rückzug der römischen Legionen um das Jahr 400.
Im verbleibenden Machtvakuum auf der britischen Insel stießen die ursprünglich aus dem heutigen Norddeutschland und Dänemark stammenden Sachsen, Angeln und Jüten mit der nächsten großen Wanderungsbewegung über die Nordsee vor. Sie waren während der römischen Besatzungszeit Britanniens bereits als Söldner während der Spätphase des Römischen Reiches eingesetzt worden, überquerten in den Jahrhunderten der Völkerwanderung zahlreich die Nordsee und siedelten sich im Süden und Osten Englands an, wobei sie die ursprünglich dort lebenden Kelten in die Gebiete des heutigen Schottlands und Wales vertrieben.
Ungefähr im 7. Jahrhundert wanderten die ursprünglich aus den heutigen Niederlanden stammenden Friesen über die Nordsee auf die nordfriesischen Inseln Sylt, Amrum und Föhr aus. In einer zweiten Einwanderungswelle im 11. Jahrhundert wurde auch das jütländische Festland zwischen Eider und Wiedau in Südjütland besiedelt, wo die Friesen auf die Jüten stießen. Das nordfriesische Siedlungsgebiet macht heute einen Großteil des Kreises Nordfriesland aus.
Die nächste größere Wanderungswelle über die Nordsee brachte die vor allem aus dem heutigen Dänemark und Norwegen stammenden Nordmannen auf die britischen Inseln. Mit dem Überfall auf Lindisfarne 793 begannen die Plünderungszüge der Wikinger, die die nächsten hundert Jahre vor allem als Piraten und Plünderer unterwegs waren. Sie überfielen küstennahe Klöster, Gehöfte und Städte und fuhren auf den Flüssen landeinwärts. Dem Anglo-Saxon Chronicle zufolge begannen sie ab 851, auch zu siedeln. Diese Wanderungsbewegungen aus Skandinavien hielten bis etwa 1050 an.
Alfred dem Großen von Wessex gelang es als erstem sächsischen König, den Wikingern Widerstand zu leisten, indem er eine eigene Flotte aufstellte. Er konnte das Gebiet von den Dänen befreien und gilt als erster englischer König. Während das Meer die britischen Angelsachsen von den germanischen Stämmen getrennt hatte, hielten die Skandinavier die gesamte Zeit über die Nordsee Kontakt zur alten Heimat. Somit gehörte der größte Teil der britischen Inseln und der nördliche Teil des Meeres fest zum Machtbereich skandinavischer Könige, den Wikingern.
Hardiknut war der letzte dänisch-britische König, nach seinem Tod zerfiel das Nordseereich aufgrund innerer Konflikte, die politische Union zwischen Skandinaviern und Briten über die Nordsee hinweg war getrennt. Nachdem diese Trennung erfolgt war, begann die Nordsee vorerst ihre Bedeutung zu verlieren. Seit dem Einfall Wilhelms des Eroberers aus der Normandie im heutigen Frankreich orientierten sich die britischen Inseln ebenso wie die westlichen Küstenregionen der Nordsee entlang der großen europäischen Flüsse nach Süden in Richtung Mittelmeer und Orient.
Die wichtigste Verbindung zur Außenwelt für Norddeutschland und Skandinavien war hingegen die Ostsee, wo die Hanse ihre Blütezeit erlebte. Der einzig bedeutendere Handelsweg über die Nordsee führte durch die deutsche Bucht von Flandern in die Häfen der Hansestädte.
Die Hanse hatte ihren Schwerpunkt zwar in der Ostsee, wichtige Kontore befanden sich aber auch im norwegischen Bergen (Bryggen), dem Stalhof im englischen London und dem flandrischen Hansekontor in Brügge.
Der Aufstieg Brügges begann für die Nordsee nicht untypisch mit einer Sturmflut, die 1134 eine tiefe Fahrrinne, den Zwin, riss, die das Anlaufen größerer Handelsschiffe in die Stadt ermöglichte. Zwischen Brügge und London begann sich ein lebhafter Handelsverkehr mit britischer Wolle und flandrischen Tüchern zu entwickeln.
Ab dem 13. Jahrhundert reisten deutsche Hanse-Kaufleute regelmäßig nach Brügge und London und begannen, eine regelmäßige Handelsroute in diese Städte aufzubauen. Brügge wurde zum Endpunkt der Ost-West-Handelslinie mit dem Peterhof in Nowgorod in Russland und war über den Schiffsverkehr zugleich mit Frankreich, Italien, Spanien und den Niederlanden verbunden.
Schon 1441 musste die Hanse die wirtschaftliche Gleichberechtigung der Niederländer anerkennen, nachdem Brügge als wichtigstem Kontor der Hanse mit Antwerpen ein mächtiger Konkurrent erwachsen war und sich die Niederlande zusätzlich mit den Dänen als den „Herren des Sunds“ verbündet hatten. Die Niederländer begannen nach der gewonnenen Grafenfehde, in die Handelsgebiete der Hanse vorzudringen und einen eigenen Ostseehandel zu betreiben.
Die vereinigten Niederlande entwickelten sich im 16. Jahrhundert zur ersten Welthandelsmacht. Für die Geschichte niederländischer Händler diente die Nordsee selbst nur mehr als Startpunkt für ihre Fahrten über die Ozeane. Sie war zum Tor zur Welt geworden, die Herrschaft über die Nordsee war ausschlaggebend dafür, einen direkten Weg zu den Märkten der Welt zu haben.
Während des Achtzigjährigen Krieges begannen die Niederlande auch mit einem groß angelegten Überseehandel – sie jagten Wale nahe Spitzbergen, betrieben Gewürzhandel mit Indien und Indonesien, gründeten Kolonien in Brasilien, Nordamerika (Nieuw Nederland), Südafrika und in der Karibik (vergleiche auch Die große Tulpenmanie). Der Reichtum, den sie aus diesem Handel anhäuften, führte im 17. Jahrhundert zum „Goldenen Zeitalter“ (de gouden eeuw) der Niederlande.
1651 verhängte England die Navigationsakte, die vielen niederländischen Handelsinteressen schadete. Der Kampf um die Akte mündete 1652 in den Ersten Englisch-Niederländischen Krieg, der 1654 mit dem Frieden von Westminster endete; die Navigationsakte wurde durch die Niederlande anerkannt.
1665 erklärten die Engländer den Niederländern erneut den Krieg: Es begann der Zweite Englisch-Niederländische Krieg. Mit Unterstützung der Franzosen, die in der Zwischenzeit in die Spanischen Niederlande – heute Belgien – einmarschiert waren, gewannen die Niederländer die Oberhand. Engländer und Niederländer schlossen 1667 den Frieden von Breda, nachdem der niederländische Admiral Michiel de Ruyter einen großen Teil der englischen Flotte auf der Themse zerstört hatte. Es wurde vereinbart, dass die Engländer die niederländischen Besitzungen in Nordamerika (das Gebiet um das heutige New York City) behalten durften, während die Niederländer Suriname von den Engländern erhielten. Auch die Navigationsakte wurde zu Gunsten der Niederlande modifiziert.
Das Jahr 1672 wurde in den Niederlanden als das Rampjaar („Katastrophenjahr“) bekannt: England erklärte der Republik den Krieg, gefolgt von Frankreich, dem Hochstift Münster und Kurköln, die eine Allianz gegen die Niederlande bildeten. Frankreich, Kurköln und das Hochstift Münster marschierten in die Republik ein, während die Landung der Engländer an der Küste nur knapp verhindert werden konnte.
Die Niederländer bezogen den südlichen Nordseeraum als Hinterland ein: In Schleswig-Holstein zeugen noch heute zahlreiche Hinterlassenschaften von Holländern, die einwanderten oder Handelsgüter mitbrachten. Die Niederländer brachten über das Meer ihre technische Meisterschaft in Deichbau und Entwässerungstechnik mit. Hausbau- und Landwirtschaftstechniken wurden von Holland beeinflusst, die Küstenstriche Schleswig-Holsteins gelangten ebenfalls zu Reichtum. Zahlreiche Bewohner der Küstengebiete heuerten auf niederländischen Schiffen an – besonders bekannt sind wohl die Walfahrer der nordfriesischen Inseln.
Englands Aufstieg zur beherrschenden Seemacht begann 1588, als der Invasionsversuch der spanischen Armada an einer Kombination von herausragenden englischen Seegefechten unter der Führung von Sir Francis Drake und dem schlechten Wetter scheiterte. Die erstarkende englische Marine lieferte sich mehrere Seekriege mit den auf der anderen Nordseeseite liegenden Niederlanden und konnte diese am Ende des 17. Jahrhunderts als weltumspannende Seemacht ablösen. Der Aufbau des Britischen Empires als Reich, „in dem die Sonne nie untergeht“, war nur möglich, weil die britische Marine die europäischen Gewässer und speziell die Nordsee uneingeschränkt beherrschte. Der einzig ernst zu nehmende Versuch, diese Vorherrschaft zu brechen wurde von Napoleon unternommen. Die von Admiral Horatio Nelson gewonnene Schlacht von Trafalgar, die die britische Vorherrschaft zur See für mehr als ein Jahrhundert sicherte, führte dann aber nur zur Kontinentalsperre, mit der Großbritannien von den Importen des europäischen Kontinents abgeschnitten werden sollte.
In diesem Krieg standen sich in der Nordsee hauptsächlich die Flotten der beiden Anrainer Deutschland (Kaiserliche Marine) und Großbritannien (Grand Fleet) gegenüber.
Aufgrund der Übermacht britischer Schiffe konnte die Grand Fleet beinahe ungestört die Seeherrschaft über die Nordsee erlangen und eine Seeblockade einleiten. Das Ziel der Blockade war es, Deutschland von den Schifffahrtswegen zu trennen, um die Versorgung mit kriegswichtigen Importen zu verhindern und das ungestörte Übersetzen des britischen Expeditionskorps zu garantieren. Aufgrund der defensiven Ausstattung Helgolands mit einer starken Küstenverteidigung war für Deutschland nur die Deutsche Bucht gesichert, während die übrige Nordsee und der Ärmelkanal während des gesamten Krieges durch die Royal Navy kontrolliert wurde.
Das erste Seegefecht fand am 28. August 1914 vor Helgoland statt und endete mit einem klaren britischen Sieg. Da die Überwasser-Streitkräfte der kaiserlichen Marine auf offenem Wasser chancenlos waren, leiteten die Deutschen den U-Boot-Krieg ein. Nach anfänglichen Misserfolgen deutscher Unterseeboote gelang es U 9 am 22. September 1914 drei britische Panzerkreuzer ca. 50 km nördlich von Hoek van Holland zu versenken.
Im November 1914 erklärte die britische Kriegsmarine die gesamte Nordsee zur Kriegszone, die daraufhin vermint wurde. Schiffe, die unter der Flagge neutraler Staaten fuhren, konnten in der Nordsee ohne Vorwarnung das Ziel britischer Angriffe werden.
Im Gefecht auf der Doggerbank erlitt Deutschland am 24. Januar 1915 eine weitere Niederlage gegen die Briten und in der Folgezeit schlugen sämtliche Versuche, die alliierte Nordseeblockade zu durchbrechen, fehl. Aufgrund dieser Fehlschläge erfolgte am 4. Februar der Beginn des uneingeschränkten U-Boot-Krieges, in dem neben alliierte auch neutrale Schiffe angegriffen werden konnten.
Am 31. Mai und 1. Juni 1916 kam es vor Jütland mit der Skagerrakschlacht zur größten Seeschlacht des Ersten Weltkriegs und gemessen an der Zahl und Größe der beteiligten Schiffe (258) zur wahrscheinlich größten Seeschlacht der Weltgeschichte. Das Ziel der Deutschen, die britische Marine entscheidend zu schwächen und damit die Aufhebung der Seeblockade zu erzwingen, wurde nicht erreicht. Letztlich endete die Schlacht ohne einen eindeutigen Sieger und Deutschland setzte wieder alle Hoffnungen auf den uneingeschränkten U-Boot-Krieg.
Als sich das Ende des Krieges anbahnte, sollte gegen den Willen der neuen deutschen Regierung am 28. Oktober 1918 noch einmal ein Großangriff auf die britische Marine stattfinden, worauf der Kieler Matrosenaufstand ausbrach und der Seekrieg sein Ende fand. Die Meuterei der Matrosen leitete auch die Entwicklung zur Novemberrevolution in Deutschland ein.
Auch der Zweite Weltkrieg war hinsichtlich des Seekrieges auf Seiten der Deutschen Marine vor allem ein U-Boot-Krieg, der allerdings kaum noch in der Nordsee, sondern vor allem im Atlantik ausgetragen wurde. Anders als im Ersten Weltkrieg war die Nordsee auch nicht mehr ausschließliches Hoheitsgebiet der Alliierten, sondern vor allem in den ersten Kriegsjahren Schauplatz einer intensiven Küstenkriegsführung mit kleinen Fahrzeugen wie U-Booten, Minensuchbooten und Schnellbooten. Doch trotz anfänglicher Erfolge, die Großbritannien zeitweise in eine Versorgungskrise brachten, gelang es nicht, den Widerstand entscheidend zu brechen. Wie im Ersten Weltkrieg beherrschten die Alliierten bald die See, speziell wegen der Luftüberlegenheit auch die Nordsee und schnitten Deutschland von überseeischer Versorgung ab. Der damit verbundene Mangel an Ressourcen für die Kriegführung war einer der Gründe dafür, dass der Krieg nicht zu gewinnen war.
Am 14. Oktober 1939 gelang es Kapitänleutnant Günther Prien mit dem Unterseeboot U 47 in die Bucht von Scapa Flow einzudringen und das Kriegsschiff HMS Royal Oak mit 1400 Mann Besatzung zu versenken.
Am 9. April 1940 lief auf deutscher Seite die Operation Weserübung an, bei der fast die gesamte deutsche Flotte mobilisiert und in Richtung Skandinavien in Fahrt gesetzt wurde. Kurze Zeit später waren die militärischen Ziele der Invasion (Besetzung der norwegischen Häfen, Sicherstellung der Eisenerz-Versorgung, Verhinderung einer zweiten Front im Norden) erreicht und Norwegen und Dänemark besetzt. Diese Besatzung dauerte bis zum Ende des Krieges und während der gesamten Zeit diente der quer über die Nordsee laufende Shetland Bus als wichtiger Flucht- und Versorgungsweg von Norwegen nach Großbritannien. Zuerst von norwegischen Fischerbooten betrieben, wurden diese im Laufe des Krieges durch drei U-Boot-Jäger der Royal Navy ersetzt.
Aufgrund der Unterlegenheit bei den größeren Kampfschiffen, deutlich sichtbar durch die frühen Verluste (Admiral Graf Spee 1939, Blücher 1940 und Bismarck 1941), verlegte sich die Kriegsmarine mehr und mehr auf die Kriegsführung mit kleinen Einheiten und die verbliebenen Großkampfschiffe wie die Tirpitz ankerten nahezu untätig in Norwegens Fjorden.
In den letzten Kriegsjahren und den ersten Nachkriegsjahren unter alliierter Aufsicht wurden große Mengen Munition in der Nordsee verklappt. Während chemische Kampfstoffe vor allem in Skagerrak und Ostsee versenkt wurden, wurde konventionelle Munition (Granaten, Minen, Panzerfäuste, Patronen etc.) in der Deutschen Bucht versenkt. Die Zahlenschätzungen gehen hier weit auseinander, klar scheint jedoch zu sein, dass mehrere hunderttausend Tonnen Munition in der See versenkt wurden.
In der Zeit nach dem Zweiten Weltkrieg trat die Nutzung der Nordsee für friedliche Zwecke in den Vordergrund; denn während sich in der Ostsee die Gegner des Kalten Kriegs direkt gegenüberstanden und beäugten, war die Nordsee ein neben Schweden nur von NATO-Mitgliedsstaaten begrenztes Meer.
Ökonomische Bedeutung gewann die Nordsee in den 1960ern, als die Anrainerstaaten begannen, gefundenes Erdöl und -gas kommerziell zu nutzen. Die größte Katastrophe in der Geschichte der Öl- und Gasförderung in der Nordsee war der Untergang der Bohrinsel Piper Alpha 1988, bei dem 167 Menschen ums Leben kamen.
Im August 2011 erteilte die Deutsche Bundesregierung einen Auftrag zur systematischen archäologischen Prospektion der südlichen Nordsee, auch außerhalb der 12-Seemeilen-Zone, an das Deutsche Schifffahrtsmuseum in Bremerhaven, da der Bestand zahlreicher archäologischer Fundplätze durch geplante Bauvorhaben bedroht ist.[29]
Laut deutschen Behörden befinden sich (Stand 2020) etwa 1,3 Millionen Tonnen Kampfmittel und 280.000 Tonnen chemische Waffen des Dritten Reichs versenkt vor der deutschen Nordseeküste.[30][31]

