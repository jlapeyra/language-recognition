Rosine Bernardt, conocida artísticamente como Sarah Bernhardt (París, 23 de octubre de 1844-París, 26 de marzo de 1923), fue una actriz francesa de teatro y cine, una de las más famosas y aclamadas a finales del siglo XIX y principios del XX. Trabajó en obras como La dama de las camelias, de Alejandro Dumas, hijo; Ruy Blas, de Victor Hugo; Fédora y La Tosca, de Victorien Sardou, y L'Aiglon, de Edmond Rostand. También interpretó papeles masculinos, incluido el del príncipe Hamlet en la obra homónima de Shakespeare. Rostand la llamó «la reina de la pose y la princesa del gesto», mientras que Hugo elogió su «voz dorada». Realizó varias giras teatrales por todo el mundo y fue una de las primeras actrices destacadas en realizar grabaciones de sonido y actuar en películas.
También está relacionada con el éxito del artista Alphonse Mucha, a quien le dio en 1894 su primer encargo de un afiche en París, con el cual ganó notoriedad y luego lo convirtió en uno de los artistas más cotizados de esta época por su estilo art nouveau.
Sarah Bernhardt nació el 23 de octubre de 1844 en el número 5 de la calle de l'École-de-Médecine en París. Su nombre real era Rosine Bernardt. Su madre era una mujer de religión judía de origen neerlandés llamada Judith-Julie Bernardt (1821-1876), alias Youle. Se ganaba la vida como cortesana junto con su hermana Rosine. Julie tuvo varias hijas más. En abril de 1843 tuvo dos niñas gemelas que fallecieron a las dos semanas. Tras Sarah, tuvo a Jeanne (fecha de nacimiento desconocida) y a Régine en 1855, que murió de tuberculosis en 1873. Todas fueron hijas de padres distintos y desconocidos. Sarah Bernhardt nunca supo quién era su padre biológico, aunque se cree que era el duque de Morny, medio hermano de Napoleón III.
Sarah pasó los primeros cuatro años de su vida en Bretaña al cuidado de un ama de cría. La primera lengua que Sarah aprendió fue el bretón y por esta razón, al iniciar su carrera teatral, adoptó la forma bretona de su apellido, «Bernhardt». En esta época sufrió un accidente que muchos años después le acarrearía graves problemas de salud. Cayó de una ventana y se rompió la rodilla derecha. Aunque sanó sin problemas, la rodilla le quedó delicada para siempre y en 1914, a causa de una dolorosa inflamación de esa misma rodilla, tuvieron que amputarle la pierna derecha. Tras el accidente, su madre la llevó consigo a París, donde permaneció dos años. A punto de cumplir siete años ingresó en la Institución Fressard, un internado para señoritas próximo a Auteuil. Permaneció allí dos años. En 1853 entró en el colegio conventual Grandchamp, cercano a Versalles. En este colegio participó en su primera obra teatral, Tobías recupera la vista, escrita por una de las monjas. También aquí fue bautizada e hizo la primera comunión. El ambiente místico del colegio le hizo plantearse el hacerse monja.
Tras abandonar Grandchamp a los 15 años, su madre trató de introducirla en el mundo galante para que se ganara la vida como cortesana, pero Sarah, influida por su educación conventual, se negó repetidamente a ello. Julie Bernard tenía un salón en su piso parisiense donde se reunían sus clientes. Entre ellos estaba el medio hermano de Napoleón III, el duque de Morny, quien aconsejó que Sarah se inscribiera en el Conservatorio de música y declamación. Gracias a los contactos del duque, Sarah entró sin dificultad en 1859. En 1861 ganó un segundo premio en tragedia y una mención honorífica en comedia. 
Finalizados sus estudios en el Conservatorio, entró, de nuevo gracias a los influyentes contactos de Morny, en la Comédie-Française. Debutó el 11 de agosto de 1862 con la obra Iphigénie, de Jean Racine. Su fuerte carácter le atrajo problemas con sus compañeros, lo que provocó que abandonara la Comédie por primera vez en 1863. Tres semanas más tarde fue contratada por el Teatro Gymnase, donde hizo siete pequeños papeles en distintas obras. Actuó por última vez el 7 de abril de 1864 con la obra Un mari qui lance sa femme. 
Ese mismo año conoció a uno de los grandes amores de su vida, Charles-Joseph Lamoral, príncipe de Ligne. Inició una apasionada relación con él, hasta que quedó embarazada y el príncipe la abandonó. El 22 de diciembre de 1864 dio a luz a su único hijo, Maurice Bernhardt. Sin oficio y habiendo fracasado momentáneamente en el mundo del teatro, siguió los pasos de su madre, convirtiéndose en prostituta de lujo. Sarah no abandonó su actividad como cortesana hasta que su carrera teatral se hubo afianzado con éxito y pudo mantenerse solo con el trabajo que le reportaba el teatro.
Tres años más tarde, en 1867 debutó en el Teatro del Odéon con Las mujeres sabias (Les femmes savantes) de Molière. Ahí empezó su verdadera carrera profesional. Participó en muchos montajes teatrales, alternando la vida teatral con la vida galante. La fama le llegó repentinamente en 1869 con Le Passant, de François Coppée, una obra en verso de un solo acto. Sarah, además, hizo por primera vez en esta obra un papel masculino, el del trovador Zanetto. Hizo papeles de hombre en varias obras más (Lorenzaccio, Hamlet y L'Aiglon).
En 1870, durante la guerra franco-prusiana, habilitó el Odeón como hospital para convalecientes, donde cuidó con dedicación a los heridos de guerra. En 1871 el improvisado hospital tuvo que ser cerrado por problemas de salubridad.
Tras la derrota francesa y la caída de Napoleón III, muchos intelectuales, exiliados por estar en contra del emperador, pudieron regresar a Francia, entre ellos Victor Hugo. El regreso de Hugo fue trascendental en la vida de Bernhardt, ya que el escritor la eligió para protagonizar el reestreno de su obra Ruy Blas. Bernhardt además protagonizó otra obra de Hugo, Hernani. Ruy Blas la encumbró a cotas de éxito inimaginables. Regresó a la Comédie-Française como una gran estrella y allí afianzó su repertorio y sus múltiples registros como actriz.
El estilo de actuación de Bernhardt se basaba en la naturalidad. Detestaba profundamente las viejas normas del teatro francés, donde los actores declamaban histriónicamente y hacían gestos exagerados. Rompió con todo lo establecido, profundizando en la psicología de los personajes. Estudiaba cada gesto y cada entonación del texto que debía decir, buscando la perfección natural sin que se notara ningún tipo de artificio. Destaca en su arte que cuando representó a grandes heroínas de tragedia o reinas, huyó de la sobreactuación y de la afectación. Son famosas sus escenas de muerte, en las que en vez de, según sus propias palabras, «ofrecer toda una retahíla de patologías» tales como estertores, toses, gemidos agónicos, profundizaba en el acto de morir desde el punto de vista psicológico y sentimental.
Aparte de su profesión de actriz, se interesó por la escultura y la pintura. Llegó a exponer varias veces en el Salón de París entre los años 1874 y 1896 y recibió premios y menciones honoríficas en ambas disciplinas. Escribió también tres libros: su autobiografía titulada Ma double vie, Petite Idole y L´art du Théâtre: la voix, la geste, la pronontiation.
Bernhardt se especializó en representar las obras en verso de Jean Racine, tales como Iphigénie, Phèdre o Andromaque. Destacó especialmente, entre muchas otras, en La Dame aux camélias, de Dumas hijo, Théodora, de Sardou, L'Aiglon, de Edmond Rostand, Izéïl, de Silvestre y Morand, Macbeth, de Shakespeare y Jeanne D'Arc, de Jules Barbier.
En 1879 realizó su primera salida de Francia, concretamente a Inglaterra, donde estuvo seis semanas haciendo dos representaciones diarias y obtuvo un éxito rotundo. Al llegar al país fue recibida espectacularmente, lo que indica que su fama había cruzado las fronteras de Francia. En esta primera visita conoció a un joven escritor llamado Oscar Wilde. Años más tarde, en 1893, Bernhardt aceptaría representar su obra Salomé. Ese mismo año, Sarah fue ascendida a Socia Plena de la Comédie-Française. Los Socios Plenos son la jerarquía más alta de esta institución.
Tras su espectacular éxito en Inglaterra decidió hacer su primera gira americana. Partió a los Estados Unidos el 15 de octubre de 1880. El éxito fue total. Bernhardt haría repetidas giras por los Estados Unidos (sus famosas «giras de despedida»). También visitó México, según dio cuenta en una crónica Manuel Gutiérrez Nájera y América del Sur (actuó en Brasil, Perú, Cuba, Argentina y Chile). Viajaba en tren y en barco y llegó a cruzar el cabo de Hornos. En Estados Unidos su éxito era tal que le habilitaron un tren con siete vagones de lujo llamado Sarah Bernhardt Special, que era de uso exclusivo de la actriz. Sus giras la llevaron a Australia y visitó las islas Hawái y las islas Sandwich. Actuó en Egipto, Turquía, Moscú, Berlín, Bucarest, Roma y Atenas. En su periplo, actuó no solo en grandes teatros, sino también en teatros de ínfima categoría. 
Bernhardt tuvo una agitada vida sentimental, en la que destacan nombres como Louise Abbèma[1]​, Gustave Doré, Victor Hugo, Jean Mounet-Sully, Jean Richepin, Philippe Garnier, Gabriele D'Annunzio y Eduardo, Príncipe de Gales, entre otros. Se casó una sola vez, con un oficial griego llamado Jacques Aristidis Damala, hijo de un rico armador —nacido en El Pireo en 1842— era adicto a la morfina. Bernhardt se casó con él el 4 de abril de 1882 y fue un matrimonio tempestuoso. Sarah intentó convertir en actor a Damala, pero fracasó. La actriz le impartió clases de actuación y le dio el papel de Armand Duval en La dama de las camelias. Se eran infieles mutuamente y un día Damala, abrumado por el éxito de su mujer, por las constantes burlas de los actores de la compañía de Bernhardt y la mala relación con Maurice Bernhardt, se alistó en la Legión y fue destinado a Argelia. Meses más tarde regresó con Sarah. Las separaciones y reconciliaciones fueron continuas hasta que Sarah decidió irse de gira por todo el continente americano en 1887 y Damala ya no la acompañó. Fue la separación definitiva. Permanecieron casados hasta la muerte de Damala por los efectos secundarios del abuso continuado de morfina, en 1889, a la edad de 42 años. Bernhardt lo enterró en Atenas y adornó la tumba con un busto tallado por ella misma.
Sarah Bernhardt fue también la primera actriz empresaria del mundo del espectáculo. A raíz de una relación muy tensa con el director de la Comédie-Française, Perrin, Bernhardt rompió su contrato y dimitió como Socia Plena el 18 de marzo de 1880. La Comédie pleiteó contra ella y le ganó el juicio. Sarah Bernhardt tuvo que renunciar a su pensión de 43 000 francos que habría tenido si hubiese permanecido un mínimo de veinte años en la Comédie y, además, fue condenada a pagar 100 000 francos de multa, que nunca pagó. Tras su esplendorosa primera gira americana, que le había hecho ganar una gran fortuna, Bernhardt arrendó el teatro Porte-Saint-Martin en 1883. En este teatro produjo y actuó en obras como Frou-Frou y La Dame aux camélias, entre otras. Durante sus giras, el teatro permanecía abierto y se estrenaban obras continuamente con distinto éxito comercial. Bernhardt no dudaba en apoyar el teatro de vanguardia, así que, además del repertorio clásico, en el Porte-Saint-Martin se estrenaban obras de nuevos autores que rompían con el teatro tradicional. Tras unos años, Bernhardt arrendó el Théatre de la Renaissance, donde representó muchas obras de éxito. En 1899 alquiló por veinticinco años el enorme Theâtre des Nations, único teatro donde actuaría en Francia durante los últimos veinticuatro años de su vida.
Su vida familiar no fue sencilla. Tuvo una relación tensa y distante con su madre. Su progenitora nunca fue una madre cariñosa e interesada, y esto hizo que Sarah siempre buscase su aprobación y su cariño. Julie Bernard sentía predilección tan solo por su hija Jeanne y descuidó totalmente la educación de su hija menor, Régine. Sarah Bernhardt sentía predilección por su hermana pequeña Régine, y cuando logró ser independiente, se la llevó a vivir consigo para alejarla de la madre y de las intenciones de esta de convertirla también en cortesana. Lamentablemente, a causa del abandono afectivo que sufrió y del ambiente del piso de su madre, Régine se prostituyó a los trece años. Falleció a los dieciocho, en 1873, de tuberculosis. Su otra hermana, Jeanne, también fue cortesana durante una época y siempre que tenía necesidad de dinero. Para apartarla de esa vida, Bernhardt se la llevó consigo, incluso en varias de sus giras por Estados Unidos y Europa. Jeanne era una actriz mediocre, pero hacía pequeños papeles y vivía una vida de lujo junto a su hermana. Se sabe que sufrió crisis de neurosis a causa de su adicción a la morfina y que estuvo ingresada en el hospital de La Pitié-Salpetrière en París, al cuidado del doctor Jean-Martin Charcot. El hijo de Sarah, Maurice, siempre estuvo muy unido a su madre. Vivió siempre a su sombra, malgastando auténticas fortunas en el juego, en viajes y en una vida regalada.
El siglo XX empezó con un gran éxito, L'Aiglon, de Edmond Rostand. La obra fue estrenada el 15 de marzo de 1900 y obtuvo un éxito sin precedentes. Sarah hizo 250 representaciones de L'Aiglon y tras esto hizo otra gira por Estados Unidos para representarla. En Nueva York representó la obra en el Metropolitan Opera House y cosechó un enorme éxito. Probó suerte también con el recién nacido cine. En 1900 filmó Le Duel d'Hamlet, haciendo ella de Hamlet. En 1906 rodó La Dame aux camélias, con Lou Tellegen, su amante de aquel momento, haciendo de Armand Duval. Bernhardt, cuando la vio, se horrorizó y mandó destruir el negativo, que afortunadamente todavía existe. Rodó también Elisabeth, reine d'Anglaterre, dirigida por Louis Mercanton. En 1913 filmó Jeanne Doré, dirigida por Tristan Bernard. Esta película se considera la mejor rodada por Bernhardt y en la que se puede observar mejor su arte interpretativo. La película se conserva en la Cinématèque de Paris.
En 1914 le fue concedida la Legión de Honor. En 1915 la rodilla derecha, la misma que se había fracturado de niña, había llegado a provocarle dolores insoportables. Para colmo, durante una de sus interpretaciones de la obra dramática Tosca —la misma que Puccini hizo triunfar en el género operístico—, en la última escena, cuando la heroína se lanza desde un barranco, no se tomaron las medidas de seguridad pertinentes; Sarah se lanzó y se hirió la pierna. Aunque hacía ya varios años que padecía molestias constantes, durante el año 1914 fue empeorando y tuvieron que amputarle la pierna en febrero de 1915. Aun con la pierna amputada, Sarah Bernhardt siguió actuando. Una vez recuperada y ya empezada la Primera Guerra Mundial, la actriz decidió hacer una gira tras las trincheras francesas haciendo actuaciones para animar a las tropas. Organizó varias giras con su compañía y recorrió toda Francia. Recitaba monólogos, poemas o representaba actos famosos de su repertorio de obras en las que no debía estar de pie. Siguió también participando en películas tras la guerra. Su salud fue empeorando y sufrió un grave ataque de uremia que estuvo a punto de matarla. En 1922 vendió su mansión en el campo de Belle-Île-en-Mer, donde había rodado años atrás una película documental sobre su vida. Cuando le llegó la muerte estaba rodando una película, La Voyante. El rodaje se estaba realizando en su casa, en el Boulevard Péreire, puesto que la actriz estaba ya muy delicada de salud. El 15 de marzo de 1923, tras rodar una escena, quedó totalmente agotada y se desmayó. Nunca se recuperó. Once días más tarde, el 23 de marzo, falleció en brazos de su hijo Maurice.
Su entierro fue multitudinario. Unos 150 000 franceses acudieron a despedirla. Fue inhumada en el cementerio parisino del Père-Lachaise.
A pesar de ser llamada «la divina Sarah» por su carácter excéntrico y caprichoso, Sarah Bernhardt trabajó en innumerables proyectos teatrales demostrando un carácter perseverante, una gran profesionalidad y dedicación a su arte.
En sus últimos años, Bernhardt escribió un libro de texto sobre el arte de actuar. Escribía siempre que tenía tiempo, normalmente entre producciones y cuando estaba de vacaciones en Belle-Île. Tras su muerte, el escritor Marcel Berger, su íntimo amigo, encontró el manuscrito inacabado entre sus pertenencias en su casa del boulevard Pereire. Editó el libro y se publicó como L'Art du Théâtre en 1923[2]​.
Prestó especial atención al uso de la voz, «el instrumento más necesario para el artista dramático». Fue el elemento, escribió, que conecta al artista con el público. «La voz debe tener todas las armonías, ... seria, quejumbrosa, vibrante y metálica». Para que una voz sea completa, escribió: «Es necesario que sea un poco nasal. Un artista que tiene una voz seca nunca puede tocar al público». También destacó la importancia de que los artistas entrenen su respiración durante los pasajes largos. Ella sugirió que una actriz debería poder recitar el siguiente pasaje de Phédre de una sola vez:
Señaló que «el arte de nuestro arte es que el público no lo note... Debemos crear un ambiente con nuestra sinceridad, para que el público, jadeante, distraído, no recupere su equilibrio y libre albedrío hasta la caída del telón. Lo que se llama obra, en nuestro arte, sólo debe ser la búsqueda de la verdad»[2]​.
También insistió en que los artistas deben expresar sus emociones con claridad y sin palabras, usando «el ojo, la mano, la posición del pecho, la inclinación de la cabeza... La forma exterior del arte es a menudo todo el arte; al menos, es lo que golpea a la audiencia con mayor eficacia». Animó a los actores a «trabajar, sobreexcitar su expresión emocional, acostumbrarse a variar sus estados psicológicos y traducirlos... La dicción, la forma de pararse, la mirada, el gesto son los predominantes en el desarrollo de la carrera de un artista»[2]​.
Explicó por qué le gustaba interpretar papeles masculinos: «Los papeles de los hombres son en general más intelectuales que los de las mujeres... Solo el papel de Phèdre me da el encanto de escarbar en un corazón que está verdaderamente angustiado... Siempre, en el teatro, los papeles de los hombres son los mejores. Y, sin embargo, el teatro es el único arte en el que las mujeres a veces pueden ser superiores a los hombres»[2]​.
Los críticos franceses de teatro elogiaron las actuaciones de Bernhardt; Francisque Sarcey, un influyente crítico de París, escribió sobre su interpretación de 1871 en Marie: «Tiene una gracia soberana, un encanto penetrante y no sé qué. Es una artista natural e incomparable». Refiriéndose a su interpretación de Ruy Blas en 1872, el crítico Théodore de Banville escribió que Bernhardt «declamó como un pájaro azul canta, como el viento suspira, como el agua murmura»[3]​. De la misma interpretación, Sarcey escribió: «Ella agregó la música de su voz a la música del verso. Cantó, sí, cantó con su voz melodiosa ...»[3]​.
Victor Hugo era un ferviente admirador de Bernhardt, alabando su «voz de oro». Al describir su actuación en su obra de teatro, Ruy Blas escribió en 1872 en sus Carnets: «¡Es la primera vez que esta obra realmente se representa! Ella es mejor que una actriz, es una mujer. Es adorable, es mejor que hermosa, tiene movimientos armoniosos y miradas de seducción irresistibles»[3]​.
Su interpretación en 1882 de Fédora fue descrita por el crítico francés Maurice Baring en los siguientes términos «Una atmósfera secreta emanaba de ella, un aroma, una atracción que era a la vez exótica y cerebral... Ella literalmente hipnotizó a la audiencia» y jugó «con tal tigre pasión y seducción felina que, ya sea buen o mal arte, nadie ha podido igualar desde entonces»[4]​.
En 1884, Sigmund Freud vio a Bernhardt interpretar a Theodora y escribió:
También tuvo sus críticos, particularmente en sus últimos años entre la nueva generación de dramaturgos que abogaban por un estilo de actuación más natural. George Bernard Shaw escribió sobre el «carácter infantilmente egoísta de su actuación, que no es el arte de hacerte pensar mejor o sentir más profundamente, sino el arte de hacerte admirarla, compadecerla, defenderla, llorar con ella, reírte de ella» sus bromas, seguir su suerte sin aliento y aplaudirla salvajemente cuando caiga el telón... Es el arte de engañarte»[5]​. Ivan Turgenev escribió: «Todo lo que tiene es una voz maravillosa. El resto es frío, falso y afectado; ¡la peor clase de parisina repulsiva y chic!» [3]​[6]​[7]​ El dramaturgo ruso Antón Chéjov, entonces un joven estudiante de medicina, pagaba sus estudios escribiendo reseñas para un periódico de Moscú. Afirmó que «Estamos lejos de admirar el talento de Sarah Bernhardt. Es una mujer que es muy inteligente y sabe producir un efecto, que tiene un gusto inmenso, que comprende el corazón humano, pero quería demasiado asombrar y abrumar a su audiencia»[3]​. Escribió que en sus papeles, «el encanto se sofoca con artificio»[6]​.
Las actuaciones de Sarah Bernhardt fueron vistas y valoradas por muchas de las principales figuras literarias y culturales de finales del siglo XIX. Mark Twain escribió: «Hay cinco tipos de actrices: malas actrices, buenas actrices, buenas actrices, grandes actrices, y luego está Sarah Bernhardt»[8]​. Oscar Wilde la llamó «la incomparable», esparció lirios en su camino y escribió una obra de teatro en francés, Salomé, especialmente para ella; fue prohibido por los censores británicos antes de que pudiera realizarse[4]​. Poco antes de morir, Wilde escribió: «Las tres mujeres que más he admirado en mi vida son Sarah Bernhardt, Lillie Langtry y la reina Victoria. Me habría casado con cualquiera de ellas con mucho gusto»[4]​.
Después de ver una actuación de Bernhardt en 1903, la actriz británica Ellen Terry escribió: «¡Qué maravillosa era Sarah Bernhardt! Tenía la transparencia de una azalea aunque con aún más delicadeza, la ligereza de una nube pero más etérea. El humo de un papel quemado es lo que mejor la describe»[3]​.
El autor británico D.H. Lawrence vio a Bernhardt interpretar La Dama de las Camelias en 1908. Después, le escribió a un amigo:

Frank Lloyd Wright (pronunciado [frank lojd rajt]; Richland Center, 8 de junio de 1867-Phoenix, 9 de abril de 1959) fue un arquitecto, diseñador de interiores, escritor, y educador estadounidense, que diseñó más de mil obras, de las cuales se completaron 532. Wright proponía el diseño de arquitecturas que estuviesen en armonía con la humanidad y el entorno que las rodeaba, una filosofía conocida como arquitectura orgánica. Fue el iniciador del movimiento Prairie School, desarrollando el concepto usoniano de la vivienda. En 2019, ocho obras de Wright fueron declaradas Patrimonio de la Humanidad por la Unesco.
Frank Lloyd Wright nació en el seno de una familia de pastores unitaristas de origen británico, y pasó buena parte de su infancia y adolescencia en una granja de Wisconsin, en pleno contacto con la naturaleza, lo que años más tarde condicionó su concepción de la arquitectura.
Ingresó para estudiar ingeniería en la universidad de Wisconsin, pero tras dos años decidió abandonar la carrera y se trasladó a Chicago, en donde comenzó a trabajar en el estudio de Joseph Lyman Silsbee, pero al considerar a este un arquitecto demasiado «convencional», Frank Lloyd Wright decidió abandonar ese puesto de trabajo y comenzar a trabajar con Louis Sullivan,[1]​ quien tenía su estudio en el Auditorium Building (Chicago) y que además forma parte de la llamada Escuela de Chicago. A este hombre, Frank lo recordaría con afecto y respeto.
Durante estos años diseñó la Casa Winslow, en River Forest, Illinois, la primera de la famosa serie de viviendas de pradera. Se trata de casas unifamiliares, fuertemente integradas en su entorno. Las cubiertas sobresalen considerablemente de las fachadas y las ventanas forman una secuencia continua horizontal. El núcleo central de las viviendas lo constituye una gran chimenea, alrededor de la cual se disponen las estancias. Otras casas diseñadas en este estilo fueron, por ejemplo, la de Willitts, en Highland Park, Illinois, y la D. Martin, en Búfalo.
Wright creó un nuevo concepto respecto a los espacios interiores de los edificios, que aplicó en sus casas de pradera, pero también en sus demás obras. Wright rechaza el criterio existente hasta entonces de los espacios interiores como estancias cerradas y aisladas de las demás, y diseña espacios en los que cada habitación o sala se abre a las demás, con lo que consigue una gran transparencia visual, una profusión de luz y una sensación de amplitud y abertura. Para diferenciar unas zonas de otras, recurre a divisiones de material ligero o a techos de altura diferente, evitando los cerramientos sólidos innecesarios. Con todo ello, Wright estableció por primera vez la diferencia entre «espacios definidos» y «espacios cerrados».
Wright además estudió con gran atención la arquitectura maya y aplicó un estilo reminiscente maya a muchas de sus viviendas, conocido como Revival Maya.
Wright abandonó a su familia en 1909 y viajó a Europa. El año siguiente presentó sus trabajos en una exposición de arquitectura y diseño en Berlín, donde obtuvo un gran reconocimiento. Una publicación que se editó sobre sus obras influyó a las nuevas generaciones de arquitectos europeos.
De regreso a los Estados Unidos diseñó su propia vivienda, Taliesin, que en el transcurso de los años se quemó en tres ocasiones, y que Wright reconstruyó siempre de nuevo.
Los años comprendidos entre 1900 y 1910 abarcan la denominada etapa clásica, durante la que escribe varios libros y dicta algunas conferencias. Este periodo es calificado por el propio arquitecto como el de las casas de la pradera, por el gran número de casas de verano que edifica en los bosques o junto a los lagos de Wisconsin y de Míchigan y en los suburbios boscosos del norte de Chicago.
El análisis de la constitución de los materiales y la relación con el entorno natural son aspectos básicos en estas construcciones; todas ellas tienen en común la planta en forma de T o de cruz, la composición por volúmenes, la cubierta a dos o a cuatro aguas y los porches cubiertos como continuación espacial del interior. Entre ellas podemos citar las casas Ward Willits (Highland Park, Chicago, 1902), Glasner (Glencoe, Illinois, 1904), Cooley (Riverside, Illinois, 1908) y la Robie (Chicago, 1908).
En 1901 dicta la conferencia Arte y artesanía de la máquina, en la Sociedad de Artes y Oficios de Chicago (se publicó en 1930). En 1904 construye la fábrica Larkin (Búfalo), de carácter monumental, estructura rectangular y fachada de ladrillo. Para este edificio diseña también el mobiliario, adecuándolo a su función laboral. La obra más famosa de este periodo es el Unity Temple (Oak Park, 1904), en el que utiliza el hormigón armado por primera vez y deja la instalación eléctrica a la vista como parte integrante de la arquitectura y el diseño.
Durante los años 1915 a 1922 Wright trabajó junto a Antonin Raymond en el proyecto del Hotel Imperial de Tokio, Japón, para el cual desarrolló un nuevo método de construcción resistente a los terremotos, que consistía en colocar sus cimientos en soportes basculantes hidráulicos cuya eficacia se vio comprobada tras permanecer intacto después del terremoto que devastó la ciudad en 1923. Este hotel, lamentablemente, fue demolido en los años 60.
Otro proyecto innovador en cuanto al método de construcción fue la casa Barnsdall, en Los Ángeles, que se realizó mediante bloques de cemento prefabricados, diseñados por Wright. Este método de construcción lo aplicó después también en otras de sus obras. Un ejemplo es la casa Millard, para cuyo diseño creó un bloque con figuras étnicas, que usó también como unidad de medida, el bloque fue fabricado con arena del lugar pretendiendo «integrar» la casa a su entorno. Otro ejemplo es la casa Ennis-Brown construida con lo que el llamaba «bloques de tela» y que es el mayor de los construidos en Los Ángeles.
Cuando atravesó un periodo en el que no tuvo muchos encargos, Wright aprovechó para escribir un libro sobre planificación urbanística, que publicó en 1932, año en el cual comenzó sus Tertulias y la escuela en Taliesin, por la cual pasaron grandes arquitectos y artistas del siglo XX como: John Lautner, E. Fay Jones y Paolo Soleri. Años más tarde creó otro centro en Arizona y en la actualidad éstos son los lugares donde están sus fundaciones.
Fijó definitivamente su estudio y residencia en la finca que construyó a tal efecto en pleno desierto de Phoenix, llamada Taliesin West (1938-59), en la que logró la integración absoluta del edificio en el paisaje, y donde se expone el modelo de una ciudad distribuida horizontalmente sobre el territorio y cuyos habitantes disponen de automóviles para desplazarse por ella.
Uno de sus proyectos más destacados y más conocidos que perpetúa su genio, lo realizó entre 1935 y 1939. Se trata de la casa Kaufmann o Fallingwater house en Bear Run, edificada sobre una enorme roca, directamente encima de una cascada sobre el riachuelo Bear con un estilo muy moderno adelantado a su época.
En los años siguientes, Wright diseñó toda clase de proyectos, y en todos introdujo criterios originales y avanzados para su época. También escribió otros libros y numerosos artículos, algunos de los cuales se han convertido en clásicos de la arquitectura de nuestro tiempo.
Frank Lloyd Wright se casó tres veces y tuvo siete hijos, cuatro hijos y tres hijas. También adoptó a Milanoff Svetlana.
Sus esposas fueron:
Uno de los hijos de Wright, Frank Lloyd Wright Jr., conocido como Lloyd Wright, fue también un notable arquitecto en Los Ángeles. Un hijo de Lloyd Wright (y nieto de Wright), Eric Lloyd Wright, es actualmente un arquitecto en Malibú, donde proyecta principalmente residencias, y también edificios civiles y comerciales.
Otro de sus hijos arquitecto, John Lloyd Wright, creó el Lincoln Logs en 1918, y trabaja principalmente en el área de San Diego. La nieta de Frank, Elizabeth Wright Ingraham, es una arquitecta en Colorado Springs, Colorado; es la madre de Christine, una diseñadora de interiores en Connecticut, y Catherine, es profesora de arquitectura en el Instituto Pratt.
Wright diseñó una casa para David Samuel Wright, hijo de su primer matrimonio con Catalina, y la esposa de David, Glady.
La actriz ganadora del Oscar Anne Baxter era nieta de Wright. Baxter era la hija de Catherine Baxter (nacida Catherine Wright), nacida del primer matrimonio de Wright. Una hija de Baxter, Melissa Galt, vive y trabaja en Atlanta, donde trabajo como diseñadora de interiores.
Su hijastra Svetlana (hija de Olgivanna) y su hijo Daniel murieron en un accidente automovilístico en 1946. Su viudo, William Wesley Peters, se casó más tarde con Svetlana Alliluyeva, la hija menor y única hija de Iósif Stalin. La pareja se divorció al no poder adaptarse ella al estilo de vida comunal de las comunidades Wright, en comparación con la vida en la Unión Soviética de su padre, y debido a la interferencia constante de la viuda de Wright. Peters desempeñó el cargo de Presidente de la Frank Lloyd Wright Foundation desde 1985 a 1991.
Un bisnieto de Wright, S. Lloyd Natof, actualmente vive y trabaja en Chicago como maestro de carpintería especializado en el diseño y creación de mobiliario de madera por encargo.
El 7 de julio de 2019 la UNESCO declaró Patrimonio de la Humanidad ocho obras de Frank Lloyd Wright: el Museo Guggenheim de Nueva York; el Unity Temple en Oak Park (Illinois); la casa Frederick C. Robie en Chicago; la casa Taliesin en Spring Green (Wisconsin); la casa Hollyhock en Los Ángeles; la casa de la cascada (Fallingwater) en Mill Run (Pensilvania); la casa Taliesin West en Scottsdale (Arizona); y la primera casa Herbert y Katherine Jacobs en Madison (Wisconsin). En su cuadragésimo tercera reunión celebrada en Bakú (Azerbaiyán), el comité de Patrimonio Mundial seleccionó ocho de las obras más emblemáticas del arquitecto bajo el epígrafe La arquitectura del siglo XX de Frank Lloyd Wright.[2]​
La Unesco lo describe así:
Wright diseñó más de 400 estructuras que fueron construidas[4]​ de las que sobreviven unas 300 en 2005. Al menos cinco se han perdido a causa de las fuerzas de la naturaleza: la casa frente al mar de W. L. Fuller en Pass Christian, destruida por el huracán Camille en agosto de 1969; el bungalow Louis Sullivan de Ocean Springs, destruido por el huracán Katrina en 2005; y la casa Arinobu Fukuhara (1918) en Hakone, Kanagawa, Japón, destruida en el Gran terremoto de Kantō de 1923. En enero de 2006, la Casa Wilbur Wynant en Gary, Indiana, fue destruida por un incendio.[5]​ En 2018 el complejo Arch Oboler en Malibú, fue destruido por el incendio Woolsey.[6]​
Edificios notables de Wright demolidos intencionadamente fueron: los Midway Gardens (construidos en 1913, demolidos en 1929), el Larkin Administration Building (construido en 1903, demolido en 1950), los Apartamentos Francis y los Apartamentos Francisco Terrace (Chicago, construidos en 1895, demolidos en 1971 y 1974, respectivamente), el Geneva Inn (Lake Geneva, construido en 1911, demolido en 1970), y el Banff National Park Pavilion (construido en 1914, demolido en 1934). El Hotel Imperial de Tokio (construido en 1923) sobrevivió al terremoto del Gran Kantō de 1923, pero fue demolido en 1968 debido a las presiones urbanísticas.[7]​ El Salón del Automóvil Hoffman de la ciudad de Nueva York (construido en 1954) fue demolido en 2013.[8]​
Frank Lloyd Wright fue la figura central de la llamada arquitectura orgánica, una tendencia que representó, de manera emblemática un punto de vista alternativo y una estética diferente, en cierta forma opuesta o complementaria a la arquitectura racionalista, cuyo  representante más conspicuo  fue el arquitecto suizo Le Corbusier.
Wright permaneció siempre fiel  a la ideología individualista del «pionerismo» americano, que devuelve a la exaltación y la profundización de las relaciones entre el individuo y el espacio arquitectónico y entre este y la natura o el espacio natural, asumido como referencia fundamental de las propias obras. Este interés lo llevó a centrarse en su tema preferido de la casa o residencia unifamiliar, las «prairie houses», las cuales constituyeron el aspecto determinante de sus inicios como arquitecto.
En su libro La Arquitectura orgánica (1939), Wright expone sus ideas sobre arquitectura; donde postula una arquitectura que rehúsa la mera investigación estética o el simple gusto superficial, puesto que no son más que imposiciones externas a la relación entre el hombre y la naturaleza. El proyecto arquitectónico,  tiene que crear una armonía entre el hombre y la natura y construir un nuevo sistema que manifieste un equilibrio real entre el entorno construido y el medio ambiente, mediante la integración de varios elementos artificiales o propios del hombre (edificios, calles, etc.) dentro de la natura. Todo esto forma parte de un único sistema interconexionado, un organismo, el espacio arquitectónico. La casa de la cascada (The Fallingwater) es el ejemplo paradigmático de este modo de hacer y entender la arquitectura.
Al final de su vida (y tras su muerte en 1959), Wright recibió muchos reconocimientos honoríficos por sus logros. En 1941 recibió la Medalla de Oro del Royal Institute of British Architects. El Instituto Americano de Arquitectos le concedió la Medalla de Oro del AIA en 1949. Esa medalla supuso un simbólico "entierro del hacha de guerra" entre Wright y el AIA. En una entrevista radiofónica, comentó: "Bueno, nunca me afilié a la AIA, y ellos saben por qué. Cuando me dieron la medalla de oro en Houston, les dije francamente por qué. Pensando que la profesión de arquitecto es lo único que le pasa a la arquitectura, ¿por qué iba a unirme a ellos?". Fue galardonado con la Medalla Frank P. Brown del Instituto Franklin en 1953. Recibió títulos honoríficos de varias universidades (incluida su alma mater, la Universidad de Wisconsin), y varias naciones le nombraron miembro honorífico de sus academias nacionales de arte y/o arquitectura. En el año 2000, Fallingwater fue nombrado "El edificio del siglo XX" en una encuesta informal de los diez mejores edificios, realizada por los miembros que asistieron a la convención anual de la AIA en Filadelfia. En esa lista, Wright aparecía junto a muchos de los mejores arquitectos de Estados Unidos, como Eero Saarinen, I.M. Pei, Louis Kahn, Philip Johnson y Ludwig Mies van der Rohe; era el único arquitecto que tenía más de un edificio en la lista. Los otros tres edificios eran el Museo Guggenheim, la casa Frederick C. Robie y el edificio Johnson Wax.
En 1992, la Ópera de Madison de Madison, encargó y estrenó la ópera Shining Brow, del compositor Daron Hagen y el libretista Paul Muldoon, basada en los primeros acontecimientos de la vida de Wright. La obra ha tenido desde entonces numerosas reposiciones, incluyendo una reposición en junio de 2013 en Fallingwater, en Bull Run, por el Opera Theater of Pittsburgh. En 2000, se estrenó en el Milwaukee Repertory Theater la obra Work Song: Three Views of Frank Lloyd Wright, basada en la relación entre los aspectos personales y profesionales de la vida de Wright.
En 1966, el Servicio Postal de los Estados Unidos honró a Wright en  una serie de estadounidenses prominentes de sellos de 2 centavos de dólar.
"So Long, Frank Lloyd Wright" es una canción escrita por Paul Simon. Art Garfunkel declaró que el origen de la canción surgió de su petición de que Simon escribiera una canción sobre el famoso arquitecto Frank Lloyd Wright. El propio Simon declaró que no sabía nada sobre Wright, pero de todas maneras escribió la canción.[9]​
En 1957, Arizona hacía planes para construir un nuevo edificio del capitolio. Siendo de la opinión que los proyectos presentados para el nuevo capitolio eran tumbas al pasado, Frank Lloyd Wright ofreció Oasis como alternativa al pueblo de Arizona.[10]​
En 2004, una de las agujas incluidas en su diseño fue erigida en Scottsdale.
La ciudad de Scottsdale, en Arizona rebautizó una parte de su Bell Road, una importante vía este-oeste del área metropolitana de Phoenix, en honor a Frank Lloyd Wright.
Ya mencionados anteriormente es la designación de sus ocho obras más notables como Patrimonio de la Humanidad por la Unesco en 2019.

Jean-Baptiste Poquelin (París; 15 de enero de 1622-ibidem; 17 de febrero de 1673),[1]​ llamado Molière, fue un dramaturgo, actor y poeta francés, ampliamente considerado como uno de los mejores escritores de la lengua francesa y la literatura universal. Sus trabajos existentes incluyen comedias, farsas, tragicomedias, comédie-ballets y más. Sus obras se han traducido a todas las lenguas vivas principales. Considerado el padre de la Comédie Française, perteneciente al Gran Siglo francés, maestro del Clasicismo imperante en la Francia del siglo XVII, sus trabajos se interpretan con más frecuencia que los de cualquier otro dramaturgo actual.
Despiadado con la pedantería de los falsos sabios, la mentira de los médicos ignorantes, la pretenciosidad de los burgueses enriquecidos, Molière exalta la juventud, a la que quiere liberar de restricciones absurdas. Muy alejado de la devoción o del ascetismo, su papel de moralista termina en el mismo lugar en el que él lo definió: «No sé si no es mejor trabajar en rectificar y suavizar las pasiones humanas que pretender eliminarlas por completo», y su principal objetivo fue el de «hacer reír a la gente honrada». Puede decirse, por tanto, que hizo suya la divisa que aparecía sobre los teatritos ambulantes italianos a partir de los años 1620 en Francia, con respecto a la comedia: Castigat ridendo mores, «Corrige las costumbres riendo», tomada del poeta neolatino francés Jean de Santeul (1630-1697).
La vida de Molière se documentó desde muy pronto; ya Voltaire le escribió una temprana biografía, que acompañó con comentarios a cada una de sus obras.[2]​ Hijo del tapicero real Jean Poquelin y de Marie Cressé, se atribuye a sus tíos (sin que esto sea seguro) la razón de su interés por el teatro, ya que a menudo lo llevaban a ver piezas representadas.
Perdió a su madre a la edad de 10 años (1632) y, aunque no parece haber sido particularmente afecto a su padre, vivió con él en un piso alto del Pavillon des Singes, en la calle Saint-Honoré, sita en un barrio pudiente de París; su padre se volvió a casar y enviudó de nuevo en 1636. Molière había iniciado estudios en 1633, a los once años, en el colegio jesuita de Clermont, actual liceo Louis-le-Grand, donde se codeó con la levantisca nobleza francesa, sometida con mano de hierro por el cardenal Richelieu, valido de Luis XIII, hasta 1639. Estudia derecho en Orleáns entre 1640 y 1642 y, una vez licenciado, se colegia, aunque solo durante seis meses.[3]​ Por entonces frecuenta el círculo libertino del filósofo epicúreo Pierre Gassendi, Claude-Emmanuel Luillier, llamado Chapelle, Cyrano de Bergerac y Charles Coypeau de Assoucy.[4]​ Sustituye entonces a su padre (1642) en el oficio de tapicero del rey Luis XIII, y se relaciona con la familia de comediantes Béjart.
Pero ansiaba consagrarse solo a la farándula y el 30 de junio de 1643 Jean-Baptiste firmó con los Béjart el acta de constitución de L'Illustre Théâtre / El Ilustre Teatro. La directora será Madeleine Béjart, de la que andaba enamorado; inician un largo periodo de giras por provincias, sobre todo por el sur de Francia e incluso se presentan en París el uno de enero de 1644; el papel de Molière no consiste solo en actuar, sino en escribir farsas y desde 1644, ya con el sobrenombre de Molière, incluso dirige la compañía, algo que volverá a hacer en 1650.
Sin embargo el éxito se hizo esperar y, con suerte, solo cosechaban medianas recaudaciones con un repertorio de tragedias de Pierre Corneille y otros autores, además de farsas y pantomimas al estilo de la Commedia dell'Arte en forma de intermedios en las tragedias, a fin de atraer a todo tipo de público. Tras una racha de varios fracasos, la compañía llegó a acumular tantas deudas que el director Molière fue encarcelado varios días en 1645. Una vez en libertad, la compañía se fusionó con la de Charles Dufresne, protegido por el duque de Epernon, e hizo una gira por Nantes, Toulouse, Albi, Poitiers, Toulouse, Narbonne, Agen y Pézenas.[5]​
Entre 1645 y 1658 se fue formando como actor y dramaturgo; escribió esbozos de farsas así como sus dos primeras comedias, El atolondrado o los contratiempos (L'Étourdi ou les Contretemps), estrenada en Lyon en 1655, y El despecho amoroso (Le Dépit Amoureux) en la que introduce al personaje de Crascarilles, representada por vez primera en Béziers en 1656. Pero lo importante fue que desde 1653 el poderoso príncipe de Conti, quien tenía un ala propia al lado de la de su hermano, el Gran Condé, en el Palacio de Versalles, y acababa de ser nombrado gobernador de la Guyena y el Languedoc, había empezado a proteger a la compañía, e incluso le había otorgado una pensión de 6000 libras en 1655. 
Poco duró esta protección: de súbito De Conti se convirtió a una religiosidad católica intransigente, y el 20 de diciembre de 1656 les retiró no solo esa asignación, sino que se volvió enemigo por motivos morales de toda forma de farándula y espectáculo teatral; la compañía pasó a depender entonces del gobernador de Normandía. En Ruan Molière conoció a Corneille.[6]​
Al volver a París en octubre de 1658, protegido por Felipe I de Orleans, hermano del rey, interpretó ante Luis XIV una tragedia de Corneille, Nicomedes,[7]​ que aburrió, y una farsa escrita e interpretada por él mismo, El doctor enamorado,[8]​ que divirtió, pero no se ha conservado. Molière lucía un gran talento cómico y su voz y mímica desencadenaron las risas. Pronto la compañía alcanzó una reputación inigualable en lo cómico con El desengaño amoroso y sobre todo con la primera de las grandes comedias de Molière, Las preciosas ridículas (Les précieuses ridicules, 1659), que consiguió un éxito enorme y confirmó el favor del joven rey Luis XIV hasta el punto de que este los instaló en un teatro estable en el Petit-Bourbon, en donde actuaban alternándose con la compañía italiana Scaramouche, así llamada por el tópico personaje cómico italiano vestido de negro.
En 1659 la compañía de Molière había sufrido una gran reestructuración: Charles Dufresne se retiró, Du Parc y Marquise se unieron a la compañía del Teatro du Marais, y entraron en la farándula La Grange, futuro editor de Molière, el célebre actor de farsas Julien Bedeau, apodado "Jodelet" por el personaje de Scarron, y su hermano L’Espy, así como Du Croisy y su mujer. Sganarelle ou le Cocu imaginaire /  Esganarel o El cornudo imaginario (1660) confirma la buena fama cómica de Molière (fue su pieza más representada en vida del autor), quien, no obstante, no alcanza a competir con las compañías especializadas en el género trágico; en esta comedia creó el famoso personaje de Sganarelle, que recuperaría muchas veces en otras obras y al que siempre interpretó él mismo.[9]​ Sin embargo, el Petit-Bourbon fue derribado en 1660 para construir la columnata del Louvre y, después de un difícil invierno sin trabajar, en que las compañías rivales del Hôtel de Bourgogne y del Marais intentaron dividir la compañía de Molière haciendo a sus comediantes distintas proposiciones, todos permanecieron fieles y unidos en torno al comediógrafo. El rey los trasladó en 1661 al Palais-Royal o Palacio Real, donde el cuatro de noviembre estrena Les facheux (Los inoportunos) con música de Lully y un éxito abrumador: 39 representaciones seguidas. Todavía intentó Molière destacar en la tragedia, y su canto del cisne en el género fue Don García de Navarra, un fracaso del que se desquitó con el éxito de la comedia La escuela de los maridos, estrenada en junio de ese año en el lujoso palacio del superintendente Fouquet, quien caerá al poco tiempo y será sustituido por Colbert.[10]​
La muerte del cardenal Mazarino vuelve más fuerte el poder del Rey. El 20 de febrero de 1662 Molière se casó con Armande Béjart, hija de Madeleine, quien tenía unos veinte años menos que él. El mismo año abordó un tema poco corriente en su época: la condición de la mujer, y La escuela de las mujeres (L'École des femmes) fue un gran éxito. Pero el partido dévote o devoto y sobre todo su sociedad secreta, la Compañía del Santo Sacramento, protegida por la reina madre Ana de Austria, consideraban a Molière un libertino o descreído y temían la influencia que podría ejercer sobre el rey, sobre todo después de que Molière representara en Versalles entre el 8 y el 14 de mayo de 1662 y el 26 de diciembre La escuela de las mujeres, por lo que declararon obscena e irreligiosa esta obra. Además, la protección del Rey había despertado los celos en otras compañías teatrales y se chismorrea interesadamente, por ejemplo, que su esposa Armande no es sino la hija que tuvo con Madeleine Béjart.[10]​ 
Edmé Boursault, amigo de los Corneille, hizo representar en el Hôtel de Bourgogne Le Portrait du peintre ou la Contre-critique de L’École des femmes / El retrato del pintor o la contracrítica de La escuela de las mujeres, seguida de la triste Chanson à la coquille, obra obscena y ordinaria dirigida contra Madeleine Béjart y el mismo Molière.
Molière contraatacó ridiculizando a sus adversarios en La crítica de la escuela de las mujeres (La Critique de l’École des femmes), representada en agosto de 1663, y en el Impromptu de Versalles (L'Impromptu de Versailles), que lo fue en octubre. La guerra entre Molière y sus detractores devotos ha empezado y el Rey, de momento, pese a su favorable inclinación hacia el cómico, no interviene todavía. Por entonces ya estaba fraguando su Tartufo, y en 1663 representó los tres primeros actos ante el rey. 
En 1664 se nombró responsable de las diversiones de la corte a Molière, e incluso el propio rey Luis y Henriette o Enriqueta de Inglaterra apadrinan a su primer hijo, Luis, que fallecerá unos meses después. Molière puso en marcha Los placeres de la Isla encantada y representó La princesa de Élide / La Princesse d’Élide, en donde mezclaba texto, música y danza y recurría a maquinaria escénica sofisticada.
Ese mismo año Molière concluyó su Tartufo / Tartuffe, donde denunciaba la hipocresía religiosa; Tartufo aparecía vestido de cura y con cilicio. El escándalo que se levantó entre los beatos fue de tal calibre que el rey prohibió durante cinco años la obra. A pesar de ello, Molière llevó a cabo algunas representaciones privadas y reescribió la obra dos veces al menos para sortear los reparos; esta pieza cuenta, pues, con un abundante subtexto.
El 15 de febrero de 1665 se estrena su obra Don Juan (Dom Juan), inspirada en El burlador de Sevilla, atribuida a Tirso de Molina, a través del Festin de Pierre ou Le fils criminel de Nicolas Drouin, llamado Dorimond (1659), pero solo llega a las quince representaciones, pues tras el cierre cuaresmal de los teatros el partido dévote presiona tan fuertemente a Molière que es él mismo el que no repone la obra. Se critica en ella la figura del libertino, que Molière conoce tan bien y al que hace decir a su criado Sganarelle lo siguiente:
Fue Dom Juan, tras Tartufo, la comedia más censurada y perseguida de Molière. La obra no se repuso hasta 1677, y solamente en una versión expurgada y versificada por Thomas Corneille. En cuanto a su edición, hizo falta esperar al año 1683 para que un librero de Ámsterdam publicara el texto íntegro. En vez de "Dios" se ponía la palabra "Cielo" y en vez de "Iglesia", "templo". Sin embargo, la compañía recibe por fin el apoyo del rey, quien concede una pensión de 7000 libras a sus cómicos y la autoriza a llamarse "Compañía Real". El 14 de diciembre de 1665 estrena una farsa tradicional, El amor médico, pero Molière cae gravemente enfermo.[12]​ 
Estrena en 1666 dos obras maestras, El misántropo (Le Misanthrope) y El médico a palos / Le Médecin malgré lui. En El misántropo expresa su amargura tras separarse de Armande e introduce un nuevo tipo de necio, un hombre de elevados principios morales que critica constantemente la debilidad y estulticia de los demás y, sin embargo, es incapaz de ver los defectos de Célimène, la muchacha de la que se ha enamorado y que encarna a esa sociedad que él condena.[13]​
En los años siguientes Molière intentó desesperadamente reponerse de su tuberculosis, enfermedad entonces casi siempre fatal, y atravesó por periodos de recuperación y recaída. Aparte de que Colbert creó en 1666 un Consejo de Policía que aumentó la presión de la censura en todos los órdenes, tenía que lidiar además con una oposición reforzada a su teatro por parte del partido devoto: se aliaron contra él el príncipe de Conti, quien en un póstumo Trait de la comédie / Tratado de la comedia (1666) lo acusa de hacer caer al teatro en el libertinismo; Jean Racine, y el erudito François Hédelin, abate D'Aubignac, a quien todo el mundo hacía caso. Molière seguía actuando irregularmente, pero no dejó de escribir, y en diciembre de 1666 volvió a Versalles para estrenar su Ballet de las Musas y representar Melicerte y El siciliano, o El Amor pintor /  Le Sicilien ou l’Amour peintre..[14]​
En 1667 sufre una recaída de seis meses, pero redacta y consigue estrenar la segunda versión del Tartufo bajo el título de Panulfo o El impostor; sin embargo el arzobispo de París Hardouin de Péréfixe convence al primer presidente del Parlamento para prohibir la obra al día siguiente de su estreno. No lo puede impedir el rey, porque está en la campaña de Flandes, y aunque Molière le pide su intercesión, no logra nada. En 1668 estrena dos obras menores con aparato: el 13 de enero y en el Palais Royal, Anfitrión / Amphitryon, que conoce 28 representaciones, y, el 18 de julio y en Versalles, Georges Dandin. Una nueva obra maestra es El avaro (L'Avare), estrenada el 9 de septiembre del mismo año en el Palais Royal. Al fin se levantó la prohibición sobre la tercera versión del Tartufo, que se estrena el 5 de febrero de 1669, en cinco actos y con final feliz, y alcanzó un enorme éxito: cuarenta y cuatro representaciones sucesivas, toda una marca o récord entonces. Se presenta ahí a la falsa devoción como la gran corruptora de las costumbres. Y así lo proclama Tartufo a la casada que pretende seducir:
En el prólogo a la edición del tercer Tartufo, hace ver Molière la lucha que ha debido mantener por estrenar su comedia. Si unos padres de la iglesia apoyan la comedia, otros denigran el teatro: "La única conclusión que se puede sacar de aquella divergencia de opiniones en unos espíritus inspirados por una misma verdad es que ambos consideraron la comedia de modo distinto, y mientras unos la contemplaron en su pureza, otros la vieron en su corrupción... De lo que se trata es de discutir de las cosas, no sobre las palabras... No hay que confundir nunca el mal uso del arte con la finalidad del mismo... Si el fin de la comedia consiste en corregir los vicios de los hombres, no veo por qué motivo ha de haber vicios privilegiados".[15]​ Y al final menciona una anécdota importante:
El padre de Molière falleció el 27 del mismo mes; el seis de octubre del mismo año Molière estrenó en el Castillo de Chambord El señor de Pourceaugnac.[14]​
No descansaban los detractores de Molière. En 1670 se estrena la comedia de un ilustre desconocido, Le Boulanger de Chalussay, tal vez un pseudónimo, destinada a difamar la vida privada de Molière y sus actores, Élomire Hypocondre, ou Les médecins vengués, donde el nombre del comediógrafo es fácilmente reconocible en forma de anagrama.[17]​ Pero, como el poder del Rey se ha fortalecido, Molière, en comandita con el compositor Jean-Baptiste Lully, pasa a ser el proveedor de espectáculos para la Corte. Los amantes magníficos se estrena el cuatro de febrero de 1670 en Saint-Germain y El burgués gentilhombre el catorce de octubre en la Sala de los guardas del primer piso del Castillo de Chambord. Se trata de una comedia-ballet con música de Lully, donde se ridiculiza al rico e ingenuo comerciante Monsieur Jourdain, que aspira a ser recibido en la corte y es timado por un desaprensivo que lo embauca con falsas promesas, de forma que el futuro e ignorante caballero se prepara para ser recibido tomando clases de música, baile, esgrima y filosofía; se hizo tópica la escena en que monsieur Jourdain averigüa que está hablando en prosa sin saberlo. Después, el 24 de mayo de 1671, estrenó Los enredos de Scapin / Les Fourberies de Scapin. Comedias intrascendentes y apresuradas, como La condesa de Escarbagnas, y Psyché (escrita a causa de las prisas en colaboración con Pierre Corneille y Philippe Quinault) preparan Las mujeres sabias, una alta comedia de las mejores del autor, estrenada en 1672. Pero fallece la que todo lo fue para Molière (amiga, amante, socia fundadora antes que resignada suegra): Madeleine Béjart, y además muere a los pocos días de nacer su segundo hijo.[18]​ 
.
Su última obra es El enfermo imaginario / Le Malade imaginaire, estrenada el diez de febrero de 1673 y lo cierto es que no tenía que esforzarse en representar el papel principal: sufrió un ataque agudo de hemoptisis en el curso de la cuarta representación, el 17 de febrero de 1673, y lo llevaron a su domicilio; su mujer no encontró un sacerdote que le diera la extremaunción y murió sin renegar de su profesión de actor, considerada inmoral por la Iglesia. Bajo la ley francesa de aquel tiempo, no estaba permitido que los actores fueran enterrados en el terreno sagrado de un cementerio. Sin embargo, la viuda de Molière, Armande, le pidió al Rey que su cónyuge pudiera tener acceso a un funeral normal "por la noche, y sin ninguna pompa ni cortejo". El rey accedió y Molière fue enterrado en la parte del cementerio de Saint Joseph reservada a los ahores o niños no bautizados. Al año siguiente su mujer se volvió a casar, y para apaciguar la codicia y la envidia de sus herederos el rey fundó el 21 de octubre de 1680 la Comédie Française o Comedia Francesa, institución que subsiste todavía hoy consagrada a representar el teatro clásico francés.[18]​ La presunta fosa donde fue enterrado el gran comediógrafo fue hallada vacía en 1792 por los patriotas revolucionarios, quienes trataban de recuperar también los restos mortales del fabulista La Fontaine.
Generalmente en las representaciones de teatro se dice que trae mala suerte vestirse de verde en Francia,[19]​ dado que Molière supuestamente habría sufrido el ataque estando en el teatro vestido de este color. Pero esto es controvertido; aunque la superstición existe, Molière iba vestido entonces de color amaranto, y cada país tiene su propio color prohibitivo en el teatro: en España es el amarillo, en Inglaterra el azul, y en Italia es el morado.[20]​
Molière era un gran lector; el inventario de su biblioteca menciona unos ciento ochenta volúmenes de historia y literatura, de los cuales cuarenta son comedias francesas, italianas y españolas.[21]​ Los argumentos de Molière son a menudo poco originales, algo de lo que también se podría acusar a Shakespeare; a menudo las toma de algún autor menor. L'Étourdi, por ejemplo, es una imitación de L'Inavertito de Niccolò Barbieri (Turin, 1628). Le Dépit amoureux se inspira en Nicolo Secchi, L'Interesse (1581). Ya se ha hablado de cómo el Dom Juan viene de Tirso de Molina a través de Le Festin de pierre ou le Fils criminel (1659) de Dorimond, pero de los Adelfos de Terencio se toman algunos elementos de L'École des maris y de su Phormion la estructura de las Fourberies de Scapin. Dom Garcie de Navarre ou le Prince jaloux es además una adaptación de Le gelosie fortunate del principe Rodrigo de Giacinto Andrea Cicognini, mientras que La Princesse d'Élide lo es de una pieza de Agustín Moreto. L'École des maris combina una comedia española de Antonio Hurtado de Mendoza con una farsa italiana; L'École des femmes contamina una novela de Paul Scarron (por cierto, gran plagiador de hispanos) con una farsa italiana; el Tartuffe toma sobre todo de Flaminio Scala, Vital d'Audiguier y Antoine Le Métel d'Ouville, así como, más accesoriamente, de otra novela de Scarron, Les Hypocrites, que contamina con los scenari italianos y, según algunos, con la pieza de Pietro Aretino Lo ipocrito. En Les Précieuses ridicules, Molière explota muy particularmente una obra de Charles Sorel sobre Les Lois de la galanterie, pero tomando la trama de L'Héritier ridicule (1649) de Scarron. Para Le Misantrophe el propio Molière admitió haberse inspirado en los fragmentos conservados de la obra homónima de Menandro, el gran autor de la comedia nueva griega. 
Entre sus influencias más generales destacan las comedias de Plauto, en especial en Anfitrión, inspirada en la comedia homónima del autor latino y El avaro, que deriva de un personaje de la Aulularia plautina. También parece haber hecho mal uso de una de las obras de Cyrano de Bergerac, El pedante burlado (Le Pédant Joué), de la cual copió una escena casi al pie de la letra.
Molière apenas tuvo amigos y sí muchos enemigos. Se ha demostrado que su filosofía de fondo procede de Gassendi, como ya advirtió Voltaire. Una de sus pocas amistades fue el poderoso tutor del rey Luis XIV, el filósofo escéptico François de La Mothe Le Vayer, y es lo cierto que hay mucho escepticismo y pirronismo lamothiano cuidadosamente encubierto en Molière, como apercibió una violenta (y anónima) Lettre sur la Comédie de l'Imposteur. 
Molière lleva a su culmen la comedia de costumbres y la comedia de carácter, aunque también utilizó los géneros de la farsa, la comedia de intriga y la comedia-ballet; para esta última trabajó solo con los músicos Jean-Baptiste Lully y más tarde Marc-Antoine Charpentier. Y aunque sus personajes están tomados del natural y son fruto de una cuidadosa observación de la realidad, son a la vez universales, tal como recomendaba Horacio; poseen siempre algún rasgo desmedrado y exagerado que constituye la raíz de su comicidad, de manera que el tema general de su teatro es moral y viene a reducirse a un ataque contra todo exceso: la demasiada franqueza de El misántropo, el desmesurado afán de quedar bien en sociedad de El burgués gentilhombre, el deseo inmoderado de atesorar de El avaro, la abundante piedad e hipocresía del Tartufo y, junto a esto, una defensa de la moderación y el equilibrio que propugnaba un sano clasicismo. Asume todas las unidades aristotélicas de acción, tiempo y lugar, y añade además la de estilo, por lo que su teatro es considerado una prolongación de la comedia clásica grecolatina.[22]​ Su concepto de lo cómico aparece expreso en la Lettre sur la comédie de l'Imposteur, publicada anónima en 1667:
Pero Molière ha sido interpretado globalmente de tres maneras: como un libertino, como un naturalista o como un maestro del sentido común, y es lo cierto que todas estas perspectivas confluyen en él y es difícil juzgarlo sin alguna de ellas. Por otra parte, su clasicismo hace que sea adaptable a cualquier ideología, estética o época antigua o moderna. Y, aunque Pierre Louÿs intentó probar ingeniosamente en 1919 que su obra la escribió en realidad Pierre Corneille,[23]​ los argumentos del editor de sus Obras completas Georges Forestier y los estudios de estilística computacional, con seis métodos distintos de análisis, han dictaminado que solo Molière es el autor de sus obras; es más, Corneille es el autor que más se aleja de su lenguaje y estilo.
A causa del influjo que sobre Molière ejercieron las farsas italianas de la Commedia dell'Arte, sus obras poseen un abundante subtexto no verbal, por lo que requieren de actores muy formados que puedan garantizar una adecuada representación.[24]​
Utiliza el verso en unas obras y la prosa en otras, y recurre a todas las formas de comicidad visual, de situación y verbal; de este último tipo, usa la ambigüedad, la repetición, los apartes, el quid pro quo, los malentendidos, el diálogo de sordos, el elogio paradójico o irónico (en el Dom Juan, el valet Sganarelle hace un elogio del tabaco, mientras que su señor lo hace de la infidelidad amorosa y de la hipocresía), la antífrasis y las parodias, y logra armonizar diferentes estilos o registros en el mismo personaje jugando sobre todo con la hipérbole (los personajes principales son a menudo afectados por una manía que lleva al extremo de los grotesco o improbable, dando lugar a la comicidad), pero también con la repetición y la simetría. También recurre a la amphigouri o bernardina.
Y aunque en su época le reprocharon haber recurrido a un género considerado bajo y vulgar como la farsa, su amigo el clasicista Nicolás Boileau asistía a sus estrenos y reía en ellos de buena gana, aunque denunciaba en su L'Art poétique las disparidades de tono y lo que juzgaba debilidades en la obra de Molière.[25]​ 
La estructura reiterada de sus comedias se centra en el fallo único de un héroe, aislado en su delirio imaginativo (enfermedad imaginaria, avaricia, devoción, esnobismo, etc.), que causa problemas dentro de una familia y se convierte, según la convención literaria, en un obstáculo para el matrimonio de los amantes; en consecuencia, otra novedad, el carácter del personaje determina directamente la trama.
Fueron tempranas las versiones que el teatro de Molière tuvo en español; ya en 1680 se llevó a
cabo en el Real Sitio del Retiro una función ante Carlos II y María Luisa de Orleáns, en la que, junto a la comedia de Pedro Calderón de la Barca Hado y divisa de Leonido y de Marfisa, se representó el sainete El labrador gentilhombre, calcado de Le Bourgeois gentilhomme de Molière.
Ya en el siglo XVIII hubo traducciones y adaptaciones múltiples; por mencionar solo algunas, las de Manuel de Iparraguirre (El avariento y El enfermo imaginario, 1753), Ramón de la Cruz (Las preciosas ridículas, en verso, aunque el original está en prosa, refundida por Manuel Catalina en 1867 con el título Las cultas latiniparlas, entre otras nueve piezas, por ejemplo, El casado por fuerza, adaptación de El matrimonio forzoso estrenada en 1767; El mal de la niña, traducción de El amor médico, 1768, etcétera),[26]​ Juan José López de Sedano (El misántropo, 1778) y Cándido María Trigueros (La hipocresía castigada / Juan de Buen Alma, o El gazmoño / Tartufo, 1768). La representación de esta pieza en Sevilla por la compañía del ilustrado Pablo de Olavide fue de hecho una de las principales acusaciones que motivaron su caída en desgracia y encierro en la cárcel de la Inquisición en 1776.[27]​ Un intento de reponer la pieza en 1777 en el teatro de la Cruz en Madrid, pese a haber pasado la censura en los términos de la Real Orden de 1753, impulsó su prohibición a las dos representaciones y su inclusión en el Index Librorum Prohibitorum / Índice de libros prohibidos del Vaticano por un edicto del 20 de junio de 1779.[28]​ Leandro Fernández de Moratín y José Marchena fueron los más asiduos en un grupo en el que también entrarían Dámaso de Isusquiza, Ramón de la Cruz, Alberto Lista y, ya en el s. XIX y en el XX, Manuel Bretón de los Herreros, Jacinto Benavente, Narciso Alonso Cortés, Enrique Llovet y Adolfo Marsillach).
El primero hizo versiones de La escuela de los maridos (1808) y El médico a palos (1814), y publicó unas Obras selectas de Moliére en francés y en español, traducidas por D. Leandro de Moratin, y continuadas por Estanislao de Cosca Vayo (Madrid: Imprenta de Repullés, 1834), y el segundo intentó traducir todo su teatro, en un proyecto auspiciado por José I, aunque solo han llegado a nosotros sus traducciones libres del Tartufo (El hipócrita, 1811) y La escuela de las mujeres (1812). Según Marcelino Menéndez Pelayo y Francisco Javier Hernández, sus versiones son casi insuperables por la fidelidad y naturalidad con que aclimata al idioma castellano el original francés, incluso cuando se trata de tecnicismos y sociolectos especiales.[29]​[30]​
En el siglo XIX pueden encontrarse entre otras las traducciones de El avaro de Dámaso de Isusquiza (Madrid, 1800) y de Juan de Dios Gil de Lara (Segovia, 1820). Más modernamente, en 1897 un joven Jacinto Benavente se atrevió a poner en escena la primera versión española del Dom Juan en el Teatro de Princesa, en Madrid.[31]​ Julio Gómez de la Serna (1895-1983), hermano del más famoso Ramón, tradujo sus Obras completas (Madrid: Aguilar, 1945; reimpresa en 1951, 1957, 1961, 1966, 1973, 1987 y 1991) con un estudio preliminar y un censo de personajes, si bien se ha discutido modernamente su versión,[32]​ y Francisco Javier Hernández tradujo Tres comedias (La escuela de los maridos, La escuela de las mujeres, Tartufo o El hipócrita), Madrid: Editora Nacional, 1977, por no hablar de las innúmeras traducciones sueltas, entre las que cabe mencionar las de Carlos Princivalle, la de Enrique Llovet que sirvió de base para el Tartufo montado por Adolfo Marsillach en 1970 y las de Mauro Armiño, José Escué, Carlos R. Dampierre, Carlos Ortega, Luis Martínez de Merlo y Encarnación García Fernández, entre muchos otros.
Las traducciones al catalán fueron muy tempranas también, y comienzan ya en el siglo XVIII con las del ilustrado y políglota Pedro Ramis.[33]​ Alfons Maseras tradujo prácticamente sus obras completas para la editorial Barcino en ocho volúmenes publicados entre 1930 y 1936.[34]​
La primera traducción de Molière al gallego fue la de Le médecin malgré lui, realizada por el sacerdote y escritor Xosé Manuel Carballo Ferreiro en los años 1960 y publicada en la colección O Moucho de Edicións Castrelos. En 1986 Manuel Guede y Eduardo Alonso Rodríguez tradujeron Le malade imaginaire, que fue publicada en tres ocasiones desde entonces. Henrique Harguindey tradujo Le Bourgeois gentilhomme en 2007 y el Tartufo en 2010.

Johannes Chrysostomus Wolfgangus Theophilus Mozart[a]​ (Salzburgo, 27 de enero de 1756-Viena, 5 de diciembre de 1791), más conocido como Wolfgang Amadeus Mozart, fue un compositor, pianista, director de orquesta y profesor del antiguo Arzobispado de Salzburgo (anteriormente parte del Sacro Imperio Romano Germánico, actualmente parte de Austria), maestro del Clasicismo, considerado como uno de los músicos más influyentes y destacados de la historia.
La obra mozartiana abarca todos los géneros musicales de su época e incluye más de seiscientas creaciones, en su mayoría reconocidas como obras maestras de la música sinfónica, concertante, de cámara, para fortepiano, operística y coral, logrando una popularidad y difusión internacional.
En su niñez más temprana en Salzburgo, Mozart mostró una capacidad prodigiosa en el dominio de instrumentos de teclado y del violín. Con tan solo cinco años ya componía obras musicales y sus interpretaciones eran del aprecio de la aristocracia y realeza europea. A los diecisiete años fue contratado como músico en la corte de Salzburgo, pero su inquietud le llevó a viajar en busca de una mejor posición, siempre componiendo de forma prolífica. Durante su visita a Viena en 1781, tras ser despedido de su puesto en la corte, decidió instalarse en esta ciudad, donde alcanzó la fama que mantuvo el resto de su vida, a pesar de pasar por situaciones financieras difíciles. En sus años finales, compuso muchas de sus sinfonías, conciertos y óperas más conocidas, así como su Réquiem. Las circunstancias de su temprana muerte han sido objeto de numerosas especulaciones y elevadas a la categoría de mito.
Según críticos de música como Nicholas Till, Mozart siempre aprendía vorazmente de otros músicos y desarrolló un esplendor y una madurez de estilo que abarcó desde la luz y la elegancia, a la oscuridad y la pasión —todo bien fundado por una visión de la humanidad «redimida por el arte, perdonada y reconciliada con la naturaleza y lo absoluto»—.[1]​ Su influencia en toda la música occidental posterior es profunda; Ludwig van Beethoven escribió sus primeras composiciones a la sombra de Mozart, de quien Joseph Haydn escribió que «la posteridad no verá tal talento otra vez en cien años».[2]​
Wolfgang Amadeus Mozart nació el 27 de enero de 1756 en Salzburgo, en la actual Austria, que en esa época era un arzobispado independiente del Sacro Imperio Romano Germánico. Fue el último hijo de Leopold Mozart, músico al servicio del príncipe arzobispo de Salzburgo. Leopold era el segundo maestro de capilla en la corte del arzobispo aunque fue un experimentado profesor. Su madre se llamaba Anna Maria Pertl. Debido a la altísima mortalidad infantil en la Europa de la época, de los siete hijos que tuvo el matrimonio solo sobrevivieron Maria Anna, apodada cariñosamente Nannerl, y Wolfgang Amadeus. Fue bautizado en la catedral de San Ruperto el día después de su nacimiento con los nombres de Joannes Chrysostomus Wolfgangus Theophilus Mozart; a lo largo de su vida firmaría con diversas variaciones sobre su nombre original, siendo una de las más recurrentes «Wolfgang Amadè Mozart».[3]​
La casa natal de Mozart se encuentra en la Getreidegasse de la ciudad de Salzburgo. Se trata de una casa que actualmente cuenta con una gran cantidad de objetos de la época e instrumentos que pertenecieron a Mozart durante su niñez. Es uno de los lugares más visitados de Salzburgo y una especie de santuario para músicos y aficionados a la música de todo el mundo.[4]​
Leopold componía y daba clases de música. El año del nacimiento de Wolfgang publicó un exitoso tratado para la interpretación del violín titulado Versuch einer gründlichen Violinschule. Después del nacimiento de Wolfgang abandonó todo, salvo las tareas propias de su cargo, para dedicarse de manera exclusiva a la formación de su hijo. Fue exigente como padre y como profesor y en todo momento estuvo al tanto de la formación de Wolfgang, para guiarlo como hombre y como artista.
Nannerl y Wolfgang Amadeus mostraron desde muy pequeños facultades para la música. Nannerl comenzó a recibir clases de teclado con su padre cuando tenía siete años, y su hermano, cuatro años y medio menor que ella, la miraba evidentemente fascinado. Años después de la muerte de su hermano, ella rememoró:
Entre esas pequeñas piezas se encuentran el Andante para teclado en do mayor, Köchel Verzeichnis (KV) 1a, y el Allegro para teclado en do mayor, KV 1b.
Cuando Wolfgang Amadeus tenía cuatro años tocaba el clavicordio y componía pequeñas obras de considerable dificultad; a los seis, tocaba con destreza el clavecín y el violín. Podía leer música a primera vista, tenía una memoria prodigiosa y una inagotable capacidad para improvisar frases musicales.
Definitivamente no era un niño común. Su progenitor era un hombre inteligente, orgulloso y religioso. Creía que los dones musicales de su hijo eran un milagro divino que él, como padre, tenía la obligación de cultivar.[5]​ Cuando el niño iba a cumplir seis años de edad, Leopold decidió exhibir las dotes musicales de sus hijos ante las principales cortes de Europa. Según los primeros biógrafos de Wolfgang, su padre «quiso compartir con el mundo el milagroso talento de su hijo...». Leopold creyó que proclamar este milagro al mundo era un deber hacia su país, su príncipe y su Dios, por lo que tenía que mostrarlo a la alta sociedad europea, ya que de otra manera él sería la criatura más ingrata.[6]​
El biógrafo Maynard Solomon afirma que mientras Leopold era un profesor fiel a sus hijos, existen evidencias de que Wolfgang trabajaba duramente para avanzar más allá de lo que le enseñaban.[7]​ Su primera composición impresa y sus esfuerzos precoces con el violín fueron por iniciativa propia y Leopold se vio fuertemente sorprendido. Padre e hijo tenían una relación muy estrecha y estos logros de niñez hicieron llorar de alegría a Leopold más de una vez.[3]​
Finalmente Leopold dejó de componer cuando el excepcional talento musical de su hijo se hizo evidente.[8]​ Él era el único profesor de Wolfgang en sus primeros años y le enseñó música, así como el resto de asignaturas académicas.[7]​
Durante los años en los que Mozart se estaba formando su familia realizó varios viajes por Europa, en los cuales mostraban a él y a su hermana Nannerl como niños prodigio. El 12 de enero de 1762 la familia entera partió hacia Múnich, comenzando con una exhibición en la corte del príncipe elector de Baviera Maximiliano III y más tarde en el mismo año en la corte imperial de José II de Habsburgo en Viena y Praga. La permanencia en la ciudad de Viena, uno de los principales centros de la música en esa época, culminó con dos recitales ante la familia imperial en el palacio de Schönbrunn. El pequeño Wolfgang causaba sensación en cada concierto, aunque el dinero recolectado en este viaje no fue tanto como los elogios recibidos. Podría decirse que este fue un viaje de prueba para Leopold. El 5 de enero de 1763 la familia Mozart retornó a Salzburgo; el viaje había durado poco menos de un año.
El 9 de junio de 1763 iniciaron una larga gira de conciertos que duró tres años y medio, en la que la familia se desplazó a las cortes de Múnich, Mannheim, París, Londres, La Haya, otra vez a París y volvieron a casa pasando por Zúrich, Donaueschingen y Múnich, cosechando grandes éxitos. Durante este viaje Mozart conoció a un gran número de músicos y las obras de otros compositores, en particular a Johann Christian Bach, a quien Mozart visitó en Londres en 1764 y 1765. Bach fue una influencia importante para el joven compositor. La familia regresó a Viena a finales de 1767 y permaneció en la ciudad hasta diciembre del año siguiente. En Viena fueron llamados al palacio por la madre del emperador, María Teresa, quien quedó encantada con el niño Wolfgang Amadeus hasta el punto de que incluso lo sentó en su regazo y lo besó.
En Versalles los Mozart tocaron ante el monarca Luis XV. La anécdota fue que en esa ocasión la amante del rey, la altiva Madame de Pompadour, no permitió que el niño Wolfgang la abrazara por temor a que se estropeara su traje.[10]​ En Londres causaron la admiración del rey Jorge III y durante este viaje el joven músico compuso su Primera Sinfonía (en mi bemol mayor, KV 16).[11]​ En los Países Bajos deslumbró tocando el órgano y compuso su primer oratorio (Die Schuldigkeit des ersten Gebotes, KV 35) a los nueve años.[12]​
A menudo estos viajes eran duros debido a los rudimentarios medios de transporte de aquel tiempo,[13]​ la necesidad de esperar pacientemente las invitaciones y el pago de las actuaciones por parte de la nobleza[14]​ y las largas enfermedades, algunas casi mortales, padecidas lejos de su hogar: en primer lugar enfermó Leopold, en el verano de 1764 durante su estancia en Londres,[15]​ y luego enfermaron ambos niños en La Haya durante el otoño de 1765.[16]​
La familia regresó a Salzburgo el 30 de noviembre de 1766. Después de un año en la ciudad, Leopold y Wolfgang viajaron a Italia, dejando en casa a la madre de Wolfgang y a su hermana. Estos viajes duraron de diciembre de 1769 a marzo de 1771 y, al igual que los primeros viajes que realizaron, tenían como objetivo mostrar las capacidades del joven como intérprete y como compositor que maduraba rápidamente. Mozart conoció en Bolonia a Giovanni Battista Martini, importante teórico de la música en aquel tiempo y por quien Mozart siempre guardó un gran afecto, y fue aceptado como miembro de la Academia Filarmónica de Bolonia, considerada el centro de erudición musical de la época.[17]​[18]​ El ingreso de Mozart en la Academia fue extraordinario, ya que aún le faltaba mucho para los veinte años, edad mínima exigida por el reglamento.
Llegaron a Roma el 11 de abril de 1770, donde escuchó el Miserere, de Gregorio Allegri, una vez durante una representación en la Capilla Sixtina. Esta obra tenía carácter secreto, pues solo podía interpretarse en dicho lugar y la publicación de su partitura estaba prohibida bajo pena de excomunión. Sin embargo, apenas llegado a la posada donde se alojaba, el joven compositor demostró poder escribir de memoria una versión muy aproximada de la partitura completa. El papa Clemente XIV, admirado del talento del músico de 14 años, no solo no lo excomulgó, sino que lo nombró Caballero de la Orden de la Espuela de Oro.[19]​[b]​[20]​
En Milán Mozart escribió la ópera Mitridate, re di Ponto (KV 87, 1770), que fue interpretada con éxito. Esto supuso el encargo de dos nuevas óperas y Wolfgang y Leopold volvieron dos veces más a Milán (desde diciembre de 1771 hasta agosto de 1772 y desde octubre de ese mismo año hasta marzo de 1773) para la composición y los estrenos de Ascanio in Alba (KV 111, 1771) y Lucio Silla (KV 135, 1772). Leopold esperaba que estas visitas consiguieran una contratación profesional para su hijo en Italia, pero sus esperanzas nunca se cumplieron.[21]​ Hacia el final del último viaje a Italia, Mozart escribió la primera de sus obras más famosas y que todavía es interpretada extensamente en la actualidad, el motete Exsultate, jubilate, KV 165.
Cada representación del joven Wolfgang Amadeus era una exhibición de su virtuosismo con el clavecín y el violín (se cuenta que ya en esa época podía tocar el teclado con los ojos vendados), y maravillaba a los espectadores improvisando sobre cualquier tema que le proponían.[22]​
Mozart y su padre volvieron definitivamente a Salzburgo el 13 de marzo de 1773. Allí se enteraron de la muerte del príncipe-arzobispo Sigismund von Schrattenbach, quien siempre los había apoyado. Comenzó entonces una nueva etapa, mucho más difícil, en la que Hieronymus von Colloredo, el nuevo príncipe-arzobispo de Salzburgo, se mostró autoritario e inflexible con el cumplimiento de las obligaciones impuestas a sus subordinados. Mozart era hijo predilecto de la ciudad, en la que tenía muchos amigos y admiradores,[23]​ y tuvo la oportunidad de trabajar en numerosos géneros musicales, incluyendo sinfonías, sonatas, cuartetos de cuerdas, serenatas, divertimentos, mucha música sacra y algunas óperas menores. Varias de estas primeras obras aún son interpretadas. Entre abril y diciembre de 1775, Mozart desarrolló un entusiasmo por los conciertos para violín, produciendo una serie de cinco conciertos (los únicos que escribiría en su vida), incrementando constantemente su sofisticación musical. Los últimos tres (KV 216, KV 218 y KV 219) son ahora básicos en el repertorio de este instrumento. En 1776 centró sus esfuerzos en los conciertos para piano y orquesta (de los cuales compondría un total de 27), culminando en el Concierto para piano y orquesta n.º 9 en mi bemol mayor (KV 271, llamado Jeunehomme) a principios de 1777, considerado por los críticos el punto de inflexión de su obra.[24]​
A pesar de estos éxitos musicales y de ser confirmado en su puesto de maestro de conciertos (Konzertmeister), Mozart estaba cada vez más descontento con su situación en Salzburgo y redobló sus esfuerzos para establecerse en cualquier otro sitio. Uno de los motivos de dicho descontento fue su bajo salario, 150 florines por año,[c]​[25]​ pero también necesitaba mucho tiempo para componer sus óperas y la ciudad en raras ocasiones se lo permitía. La situación empeoró en 1775 cuando el teatro de la corte fue clausurado, especialmente desde que el otro teatro de Salzburgo fue reservado principalmente para las compañías visitantes.[26]​
Leopold y Wolfgang realizaron dos largas expediciones en busca de trabajo durante su larga estancia en Salzburgo. Visitaron Viena desde el 14 de julio al 26 de septiembre de 1773 y Múnich desde el 6 de diciembre de 1774 hasta marzo de 1775. Estas visitas no tuvieron éxito, aunque el viaje a Múnich tuvo una gran acogida popular con el estreno de la ópera La finta giardiniera (KV 196) y el viaje a Viena fue positivo para su arte, ya que conoció el nuevo estilo vienés a través de la música de Joseph Haydn.[27]​
Mozart trabó relación con los miembros de la famosa orquesta de Mannheim, la mejor de Europa en esa época. Esta orquesta era conocida porque, de una manera muy característica y especial, exageraban de manera muy explícita la diferencia entre los pasajes suaves y los fuertes. Este estilo se difundió como «estilo de Mannheim» y pocas décadas después sería una característica principal de la música del Romanticismo. También se enamoró de Aloysia Weber, una de las cuatro hijas de la familia Weber, a la que conoció durante una escala en Múnich. En Mannheim había algunas perspectivas de conseguir empleo, pero no encontraron nada y los Mozart se marcharon a París el 14 de marzo de 1778[28]​ para continuar su búsqueda. Allí su suerte apenas mejoró. En una de sus cartas a casa insinúa la posibilidad de establecerse como organista en Versalles, pero Mozart no estaba demasiado interesado con este nombramiento.[29]​ Su situación económica era delicada hasta el punto de que debido a las deudas tuvo que empeñar objetos de valor.[30]​ El peor momento de su viaje fue cuando la madre de Mozart enfermó y falleció el 3 de julio de 1778.[31]​ Probablemente se demoraron demasiado en llamar a un médico, según Halliwell, por la falta de fondos.[32]​
Durante la estancia de Wolfgang en París, Leopold seguía buscando enérgicamente oportunidades para la vuelta de su hijo a Salzburgo[33]​ y con el apoyo de la nobleza local asegurarle una mejor posición como organista y primer violinista de la corte. El salario anual ascendía a 450 florines,[34]​ pero Wolfgang era reacio a aceptarlo[35]​ y después de marcharse de París el 26 de septiembre de 1778 se detuvo en Mannheim y Múnich, todavía con la esperanza de obtener un nombramiento fuera de Salzburgo. En Múnich se volvió a encontrar con Aloysia, convertida en una exitosa cantante, pero ella le dejó claro que no estaba interesada en él.[36]​
Finalmente, Wolfgang regresó a su hogar el 15 de enero de 1779 y aceptó el nuevo puesto, pero su descontento con Salzburgo no había disminuido. La Sonata para piano n.º 8 en la menor (KV 310) y la Sinfonía n.º 31 en re mayor (KV 297, llamada París) están entre las obras más conocidas de la estancia de Mozart en París, donde fueron ejecutadas el 12 y 18 de junio de 1778, respectivamente.[37]​
En enero de 1781, se estrenó en Múnich la ópera Idomeneo, re di Creta (KV 366) de Mozart con un «considerable éxito»[38]​ y en marzo, el compositor fue llamado a Viena, donde su patrón el arzobispo Colloredo acudió a las celebraciones del acceso al trono austriaco de José II de Habsburgo como emperador.[d]​ Mozart, fortalecido por los elogios recibidos en Múnich, se sintió ofendido cuando Colloredo lo trató como a un mero sirviente y particularmente cuando el arzobispo le prohibió tocar ante el Emperador en casa de la condesa Maria Wilhelmine Thun, actuación por la que hubiera recibido unos honorarios iguales a la mitad del salario anual que cobraba en Salzburgo.
El enfrentamiento llegó en mayo, cuando Mozart se negó a llevar un paquete enviado por Colloredo a Salzburgo. Ante su negativa de convertirse en mensajero, Mozart es insultado por su patrón y el compositor, de forma audaz, lo interrumpe en medio de su ira: «¿Su Gracia no está conforme conmigo?». La respuesta de Colloredo fueron más improperios y se cerró con un «¡vete ya!». Mozart intentó dimitir de su puesto presentando su dimisión al auxiliar del arzobispo, el conde Arco, pero el arzobispo la rechazó. Le concedieron un permiso el mes siguiente, pero de forma insultante. Días más tarde, cuando Mozart intentaba entregar personalmente a Colloredo un último «memorial», el conde Arco le cerró el paso en la antecámara del arzobispo, produciéndose otra escena violenta, y el compositor fue expulsado literalmente «con una patada en el culo».[39]​
La discusión con el arzobispo fue muy dura para Mozart porque su padre se posicionó en su contra, ya que esperaba fervientemente que siguiera obedientemente a Colloredo en su vuelta a Salzburgo. Leopold intercambió cartas con su equivocado hijo, urgiéndole a reconciliarse con su patrón, pero Wolfgang defendió apasionadamente sus intenciones de emprender una carrera independiente en Viena. El debate finalizó cuando Mozart renunció a su puesto, liberándose de las demandas de un patrón opresivo y un padre demasiado solícito. Solomon caracteriza la dimisión de Mozart como un «paso revolucionario» que alteró enormemente el curso de su vida.[40]​ En Viena, Mozart se había dado cuenta de algunas buenas oportunidades y decidió instalarse allí como intérprete y compositor independiente.[39]​
La nueva carrera de Mozart en Viena tuvo un buen comienzo. A menudo realizaba interpretaciones como pianista, destacando en una competición ante el Emperador con Muzio Clementi el 24 de diciembre de 1781 y pronto se «consolidó como el mejor intérprete de teclado de Viena».[39]​ También prosperó como compositor y en 1782 completó la ópera El rapto en el serrallo (Die Entführung aus dem Serail, KV 384), que fue estrenada el 16 de julio de ese mismo año y obtuvo una enorme aclamación. Además daría inicio al género operístico conocido como singspiel u ópera alemana, en un momento en que el italiano era el idioma más habitual para la ópera. La obra fue pronto interpretada «a través de la Europa de habla germana»[39]​ y consolidó plenamente la reputación de Mozart como compositor. Como anécdota, el emperador José II comentó al final del estreno de la ópera: «Música maravillosa para nuestros oídos, verdaderamente creo que tiene demasiadas notas», a lo que el compositor contestó: «Exactamente, ¿cuántas son menester?».
A pesar de que Mozart aún no lograba su madurez y profundidad definitiva, en esta obra se expresa quizá por primera vez la dimensión dramática que se aprecia en las posteriores óperas del compositor de Salzburgo. Esta ópera le dio a Mozart el mayor éxito teatral que conocería en vida.
En la época en la que sus disputas con el arzobispo Colloredo estaban en su punto más álgido, Mozart se trasladó con la familia Weber, que se habían mudado a Viena desde Mannheim. El padre, Fridolin, había fallecido y el resto de la familia acogía ahora huéspedes como medio para subsistir.[41]​ Tras su fracaso sentimental con Aloysa Weber, que estaba ahora casada con el actor Joseph Lange, encontró consuelo en Constanze, la hermana menor. Pero sabía que su padre Leopold no apreciaba a esa familia puesto que, no sin razones, creía que estos (fundamentalmente la madre) querían aprovecharse del éxito de su hijo. Sin embargo, hay suficientes antecedentes de que Constanze amaba verdaderamente a Mozart y nunca compartió las maquinaciones de su madre. Como el consentimiento de su padre era fundamental para Mozart, quiso viajar a Salzburgo para presentarle formalmente a la novia, pero varios eventos postergaron el temido viaje para enfrentarse a su progenitor.
Finalmente, el 4 de agosto de 1782, sin el consentimiento paterno, Wolfgang Amadeus y Constanze se casaron en Viena.[42]​ Para celebrar la unión y para calmar a su padre, Mozart compuso la inconclusa Gran misa en do menor (KV 427). Pensaba estrenarla en Salzburgo con Constanze como primera soprano solista. Solo pudo hacerlo en agosto de 1783, pero no consiguió su objetivo. Deseaba demostrar a su familia que había sabido elegir, pero Leopold y Nannerl jamás terminarían de aceptar a Constanze. En el contrato de matrimonio, Constanze «asigna a su prometido quinientos florines que [...] ha prometido aumentar después con mil florines», «para poder sobrevivir» con el total. Además, todas las adquisiciones conjuntas durante el matrimonio debían ser propiedad común de ambos.[43]​ El matrimonio tuvo seis hijos: Raimund Leopold (17 de junio de 1783-19 de agosto del mismo año), Karl Thomas Mozart (21 de septiembre de 1784-31 de octubre de 1858), Johann Thomas Leopold (18 de octubre de 1786-15 de noviembre de ese año), Theresia Constanzia Adelheid Friedericke Maria Anna (27 de diciembre de 1787-29 de junio de 1788), Anna Maria (25 de diciembre de 1789, fallecida poco después de su nacimiento) y Franz Xaver Wolfgang Mozart (26 de julio de 1791-29 de julio de 1844), de los cuales solo dos sobrevivieron, Karl Thomas y Franz Xaver Wolfgang.
Durante los años 1782 y 1783 conoció profundamente la obra de Georg Friedrich Händel y Johann Sebastian Bach a través del barón Gottfried Van Swieten, un coleccionista y aficionado musical que tenía en su poder una biblioteca con gran cantidad de obras de compositores barrocos. Entre las obras que estudió se encontraban los oratorios de Händel y El clave bien temperado de Bach. Mozart asimiló los modos de composición de ambos, fusionándolo con el propio, dando a la mayoría de las obras de este período un toque contrapuntístico, apreciable en las transcripciones que hizo de algunas fugas de El clave bien temperado KV 405, las fugas para piano KV 394, KV 401 y KV 426 (esta última transcrita luego para cuerdas con el número de catálogo KV 546). Pero, sobre todo, se puede apreciar la influencia de Händel y Bach en los pasajes de fuga de La flauta mágica y el final de la Sinfonía Júpiter. El estudio de estos autores fue para Mozart tan importante que llegó a realizar arreglos para obras como El Mesías (Der Messias, KV 572) o Alexander's Feast (KV 591), ambos oratorios de Händel.
En 1783, Mozart y Constanze visitaron a la familia de este en Salzburgo. Leopold y Nannerl fueron, a lo sumo, solamente corteses con Constanze pero la visita al menos incitó la composición de una de las grandes obras litúrgicas de Mozart, la ya mencionada Gran misa en do menor (KV 427). Aunque no completada, fue estrenada en Salzburgo con Constanze cantando las partes solistas.[44]​
Mozart conoció a Joseph Haydn en Viena. Cuando Haydn visitaba la ciudad, en ocasiones interpretaban juntos en un cuarteto de cuerdas improvisado. Los seis cuartetos de Mozart dedicados a Haydn (KV 387, KV 421, KV 428, KV 458, KV 464 y KV 465) datan del período de 1782 a 1785 y suponen una respuesta cuidadosamente considerada a los Cuartetos de cuerda rusos Opus 33 que Haydn había compuesto en 1781. Al oírlos, Haydn permaneció en pie como signo de respeto hacia Mozart y, según recordó más tarde su hermana, dijo a Leopold sobre Wolfgang: «Le digo a usted ante Dios, y como un hombre honesto, que su hijo es el mayor compositor conocido por mí en persona y por reputación, tiene gusto y, además, la mayor habilidad para la composición».[45]​
Desde 1782 hasta 1785, Mozart organizó conciertos en los que realizaba interpretaciones como solista, presentando tres o cuatro nuevos conciertos para piano en cada estación. Ya que el espacio en los teatros era escaso, reservó lugares poco convencionales para realizar sus conciertos, como un cuarto grande en el Trattnerhof (un edificio de apartamentos) y el salón de baile del Mehlgrube (un restaurante), entre otros.[46]​ Los conciertos eran muy populares y de los que él estrenó algunos todavía son obras básicas de su repertorio. Solomon escribe que durante este periodo Mozart creó «una conexión armoniosa entre un ejecutante-compositor impaciente y una audiencia encantada, que dieron la oportunidad de atestiguar la transformación y la perfección de un género musical principal».[46]​
Con las sustanciales ganancias de sus conciertos y otras actuaciones, el matrimonio Mozart adoptó un modo de vida más bien lujoso. Se trasladaron a un apartamento caro, con un alquiler anual de 460 florines.[47]​ Mozart también compró una excelente fortepiano de Anton Walter por aproximadamente 900 florines, una mesa de billar por unos 300,[47]​ envió a su hijo Karl Thomas a un internado caro[48]​[49]​ y contrataron sirvientes. Por lo tanto, con este modo de vida el ahorro era imposible y el corto período de éxito financiero no hizo nada para amortiguar las dificultades que más tarde Mozart experimentaría.[50]​[51]​
El 14 de diciembre de 1784, Mozart se convirtió en francmasón y fue admitido por la logia Zur Wohltätigkeit.[52]​ La francmasonería jugó un papel importante en el resto de la vida del compositor, ya que acudió a muchas reuniones, muchos de sus amigos eran masones y en varias ocasiones compuso música masónica.
Mozart anhelaba reformas sociales en el sentido de progreso pero no al punto de apoyar las reivindicaciones sociales que Pierre-Augustin de Beaumarchais defendía en la pieza original de Las bodas de Fígaro. Mozart tenía el espíritu del ideal masónico completamente opuesto al de los jacobinos. La condición de «doméstico» o «lacayo» no le parecía deshonrosa.[53]​
A pesar del gran éxito obtenido con El rapto en el serrallo en 1782, Mozart compuso poca literatura operística en los siguientes cuatro años, produciendo únicamente dos obras inconclusas (L'oca del Cairo, KV 422, y Lo sposo deluso, KV 430) y la comedia en un acto Der Schauspieldirektor (KV 486). Se centró fundamentalmente en su carrera como pianista solista y como compositor de conciertos. Sin embargo, alrededor de 1785, Mozart abandonó la composición de obras para teclado[54]​ y comenzó su famosa colaboración operística con el libretista Lorenzo da Ponte.
En 1786 tuvo lugar en Viena el exitoso estreno de la ópera Las bodas de Fígaro (KV 492), basada en la obra homónima de Pierre-Augustin de Beaumarchais y que no estuvo exenta de polémica debido a su contenido político. Sin embargo, Mozart y Da Ponte se las arreglaron para excluir de esta todo aquello que pudiese «poner nerviosas» a las autoridades vienesas y logró pasar la censura. La preocupación del Emperador residía en que la obra sugería la lucha de clases y en Francia ya había provocado algunos disturbios a su hermana María Antonieta. En el aria de Fígaro «Se vuol ballare» se nota parte de ese contenido que quiso minimizarse (Fígaro, con fina pero intensa ironía, entona una cavatina dirigida a su patrón el Conde de Almaviva).
Su recepción en Praga más tarde en el mismo año fue aún más cálida y esto condujo a una segunda colaboración con Da Ponte: la ópera Don Giovanni (KV 527), que fue estrenada en Praga en octubre de 1787 con un rotundo éxito, al igual que sucedió en su estreno en Viena en 1788. Esta obra, que narra las aventuras de Don Juan, había sido un tema recurrente en la literatura y el teatro y, por lo tanto, Da Ponte no se basa en un texto en particular, sino que recoge información de múltiples fuentes. La ópera fue catalogada por Mozart como un dramma giocoso y su título original era Il dissoluto punito o sia Il D. Giovanni. El contenido dramático de esta obra está presente desde el comienzo, con la muerte del comendador, hasta el final y contiene algunos de los pasajes más hermosos de la obra de Mozart.
Las dos óperas se encuentran dentro de las obras más importantes de Mozart y son básicas en el repertorio operístico actual, aunque en sus estrenos su complejidad musical causara dificultades tanto para los oyentes como para los intérpretes. El padre del compositor, Leopold, no pudo ser testigo de estos acontecimientos, ya que había fallecido el 28 de mayo de 1787. Esto sumió al hijo en una gran aflicción, ya que su padre había sido su mejor consejero y amigo (hecho documentado en la numerosa correspondencia entre ambos).
En diciembre de 1787, Mozart finalmente obtuvo un puesto estable bajo el patrocinio aristocrático. El emperador José II lo designó como su «compositor de cámara» (Kammermusicus), un puesto que había quedado vacante el mes anterior tras la muerte de Christoph Willibald Gluck. Este fue un nombramiento a tiempo parcial, recibiendo únicamente 800 florines por año y que solo requirió que Mozart compusiera obras para los bailes anuales en el palacio imperial. Mozart se quejó a Constanze de que la paga era «demasiado para lo que hago, demasiado poco para lo que yo podría hacer».[55]​ Sin embargo, a pesar de que este ingreso era modesto fue importante para Mozart cuando llegaron los tiempos duros. Los expedientes judiciales muestran que el objetivo del Emperador era impedir que su estimado compositor abandonara Viena en la búsqueda de mejores perspectivas.[55]​
En 1787, el joven Ludwig van Beethoven pasó dos semanas en Viena, esperando estudiar con Mozart. Los documentos existentes sobre este encuentro son contradictorios y existen al menos tres hipótesis en vigor: que Mozart oyó la interpretación de Beethoven y lo elogió, que Mozart rechazó a Beethoven como estudiante, y que nunca se llegaron a encontrar.
Hacia el final de la década de 1780 la situación económica de Mozart empeoró. Alrededor de 1786 dejó de aparecer frecuentemente en conciertos públicos, por lo que sus ingresos se redujeron.[56]​ Esa época fue de grandes dificultades para todos los músicos de Viena a causa de la guerra entre Austria y Turquía y que el nivel de prosperidad y estatus económico de la aristocracia, que los financiaba, se había reducido.[57]​
La ciudad de Viena iría perdiendo el interés musical por Mozart debido al advenimiento de otros pianistas con una técnica mucho más aguerrida, como en el caso de Muzio Clementi, con escalas en terceras y acordes más sonoros ideales para los pianos de construcción inglesa de una sonoridad más robusta (al contrario de los de sonoridad delicada vienesa, aptos para las escalas y sutilezas del pianismo mozartiano). Sus Academias o conciertos por suscripción, que habían sido en toda su estadía en Viena una de las mejores fuentes de ingreso (además de inspiración y motivo de composición de sus conciertos para piano y orquesta a partir del n.º 11, KV 413), comenzaron a perder audiencia, por lo que ya no le reportaban beneficios económicos.
A mediados de 1788, Mozart y su familia se trasladaron desde el centro de Viena a un alojamiento más barato en el barrio periférico de Alsergrund.[56]​ Mozart comenzó a pedir prestado dinero, cada vez más frecuentemente a Johann Michael Puchberg, un amigo y hermano de la misma logia masónica, documentados por una «lamentable secuencia de cartas suplicando préstamos».[58]​ Maynard Solomon y otros autores han sugerido que Mozart estaba sufriendo una depresión y que parecía que ralentizaba su recuperación económica.[59]​ Las principales obras de este periodo incluyen las tres últimas sinfonías (n.º 39 en mi bemol mayor, KV 543, n.º 40 en sol menor, KV 550, y n.º 41 en re mayor, KV 551 Júpiter), todas ellas de 1788, y la última de las tres óperas escritas en colaboración con Da Ponte, Così fan tutte (KV 588), estrenada en 1790.
Aproximadamente en esa época, Mozart realizó una serie de largos viajes con la esperanza de incrementar sus ingresos: a Leipzig, Dresde y Berlín en la primavera de 1789 y a Fráncfort, Mannheim y otras ciudades alemanas en 1790. Estos viajes solo produjeron éxitos aislados y no mitigaron los sufrimientos económicos de la familia.
En 1789 recibió una oferta del empresario inglés Johann Peter Salomon, quien le propuso a él y a Haydn realizar una gira de conciertos por Inglaterra. Se acordó que Haydn fuese el primero en ir, durante la temporada 1791-1792, y Mozart iría a la vuelta de este, lo que no pudo concretar por su fallecimiento.
El último año de vida de Mozart, 1791, fue, hasta su enfermedad final, un tiempo de gran productividad y, en cierto sentido, un tiempo de recuperación personal.[60]​ Realizó numerosas composiciones, incluyendo algunos de sus trabajos más admirados: la ópera La flauta mágica (Die Zauberflöte, KV 620), el último concierto para piano y orquesta (n.º 27 en si bemol mayor, KV 595), el Concierto para clarinete en la mayor KV 622, el último de su gran serie de quintetos de cuerda (KV 614 en mi bemol mayor), el motete Ave verum corpus KV 618 y el inacabado Réquiem en re menor KV 626.
La situación financiera de Mozart, una fuente de ansiedad extrema en 1790, finalmente comenzó a mejorar, ya que, aunque las evidencias no sean concluyentes[61]​ aparecieron patrocinadores ricos en Hungría y Ámsterdam prometiendo anualidades a Mozart a cambio de composiciones ocasionales. Probablemente también se benefició de la venta de música de baile compuesta en su papel como compositor de cámara imperial.[61]​ Mozart no volvió a pedir dinero prestado a Puchberg y empezó a hacer frente al pago de sus deudas.[61]​
Experimentó una gran satisfacción por el éxito público de algunos de sus trabajos, destacando La flauta mágica (representada en numerosas ocasiones en el corto período entre su estreno y la muerte del compositor)[62]​ y la Pequeña cantata masónica KV 623, estrenada el 15 de noviembre de 1791.[63]​
En marzo de 1791, Mozart ofreció en Viena uno de sus últimos conciertos públicos; tocó el concierto para piano y orquesta KV 595. Su último hijo, Franz Xaver, nació el 26 de julio.
La salud del compositor empezó a declinar y su concentración disminuía. Mozart se sintió enfermo durante su estancia en Praga el 6 de septiembre durante el estreno de su ópera La clemenza di Tito (KV 621), compuesta en ese año como un encargo para los festejos de la coronación de Leopoldo II como emperador.[64]​ La obra fue acogida con frialdad por el público. Al regresar a Viena, Mozart se puso a trabajar en el Réquiem y preparó, en compañía del empresario teatral y cantante Emanuel Schikaneder, los ensayos de La flauta mágica. Esta se estrenó con enorme éxito el 30 de septiembre, con el propio Mozart como director.
Por entonces Mozart escribió el Concierto en la mayor para clarinete (KV 622), compuesto para el clarinetista Anton Stadler. En octubre su salud empeoró; caminaba con su esposa por el Prater cuando de pronto se sentó en un banco y muy agitado comentó a Constanze que alguien lo había envenenado. El 20 de noviembre la enfermedad se intensificó y cayó postrado en cama, sufriendo hinchazón, dolores y vómitos.[65]​
Mozart recibió los cuidados de su esposa Constanze y su hermana menor Sophie durante su enfermedad final y fue atendido por el doctor Nicolaus Closset. Es un hecho probado que estaba mentalmente ocupado en la finalización de su Réquiem. Sin embargo, las evidencias de que realmente dictara pasajes a su discípulo Franz Xaver Süssmayr son muy remotas.[66]​[67]​
El 5 de diciembre de 1791, aproximadamente a la medianoche, llegó el doctor Closset de la ópera y ordenó que le pusieran compresas frías de agua y vinagre sobre la frente para bajarle la fiebre (a pesar de que Sophie se mostró reacia a hacerlo, puesto que pensaba que no sería bueno para el enfermo el cambio tan brusco de temperatura). Esto hizo tanto efecto en él que perdió el conocimiento y no volvió a recuperarse hasta su muerte. Según Sophie, los últimos suspiros de Mozart fueron «como si hubiera querido, con la boca, imitar los timbales de su Réquiem».[68]​
A las doce y cincuenta y cinco minutos de la madrugada, Mozart falleció en Viena a la edad de 35 años, 10 meses y 8 días, y su funeral tuvo lugar en la Catedral de San Esteban (donde anteriormente se había casado con Constanze), el día 6 de diciembre. Fue amortajado según el ritual masónico (manto negro con capucha).
El entierro de Mozart fue de tercera categoría, con un coste de ocho florines con cincuenta y seis kreutzer (más un suplemento de tres florines para pagar el coche fúnebre), lo usual para miembros de la burguesía media. Fue enterrado al anochecer, siendo trasladado el féretro en coche de caballos hasta el cementerio de San Marx en Viena, en el que recibió sepultura en una tumba comunitaria simple (parecida a una fosa común).[69]​ El tiempo que hacía aquella noche era suave y tranquilo, y con nieblas frecuentes, no tormentoso o ventisca como se ha pensado erróneamente. El biógrafo Otto Jahn afirmó en 1856, al entierro asistieron Antonio Salieri, Süssmayr, Gottfried Van Swieten y otros dos músicos.[70]​
La escasa afluencia de público al entierro de Mozart no reflejó su categoría como compositor, ya que los funerales y conciertos en Viena y Praga contaban con mucha afluencia.[71]​ Ciertamente, en el período inmediatamente posterior a su muerte la reputación de Mozart se incrementó considerablemente: Solomon lo describe como «una ola de entusiasmo sin precedentes»[71]​ por sus obras. Varios escritores redactaron biografías sobre el compositor, como Friedrich Schlichtegroll, Franz Xaver Niemetschek y Georg Nikolaus von Nissen, entre otros; y los editores compitieron para publicar las ediciones completas de sus obras.[71]​
La inesperada y misteriosa muerte de Mozart ha suscitado gran interés desde el principio. En el acta de defunción oficial constaba que el compositor austriaco había fallecido a causa de una hitziges Frieselfieber («fiebre miliar aguda», refiriéndose a una erupción cutánea parecida a semillas de mijo), una descripción que no basta para identificar la causa en la medicina moderna y que es demasiado amplia e inexacta, ya que no se llevó a cabo la autopsia debido al avanzado estado de descomposición en que se encontraba el cadáver.
Se han propuesto una multitud de teorías sobre la muerte del compositor, incluyendo triquinosis, gripe, envenenamiento por mercurio y un extraño achaque en el riñón. La práctica de sangrías en los pacientes era común en la época y también se cita como un posible factor que contribuyera a su muerte. Sin embargo, la versión más ampliamente aceptada es la muerte por una fiebre reumática aguda. Es conocido que tuvo tres o incluso cuatro ataques desde su infancia y esta enfermedad es recurrente, con consecuencias incrementalmente más serias en cada ataque, como una infección descontrolada o daño en las válvulas cardiacas.[65]​
El aspecto físico de Mozart fue descrito por el tenor Michael Kelly, en sus Reminiscencias como «un pequeño hombre notable, muy delgado y pálido, con una prominente cabellera de cabellos claros, por la que se mostraba muy vanidoso». Como escribió Franz Xaver Niemetschek, uno de sus primeros biógrafos, «no había nada especial en [su] físico. [...] Era pequeño y su semblante, excepto sus ojos grandes e intensos, no mostraba ningún signo de su genio». Su tez facial estaba picada, una secuela de la viruela que sufrió en su niñez. Le gustaba la ropa elegante; Kelly lo recordó en un ensayo de la siguiente forma: «Estaba sobre el escenario con su pelliza carmesí y su bicornio con encajes de oro, dando el tempo de la música a la orquesta». Por su lado, Constanze escribió más tarde que «era un tenor, bastante suave en la oratoria y delicado en el canto, pero cuando algo lo excitaba, o era necesario esforzarse, era tan poderoso como enérgico».[73]​
Por lo general Mozart trabajaba durante mucho tiempo y con energía, terminando composiciones a un gran ritmo debido a los ajustados plazos. A menudo hacía bosquejos y esbozos aunque, a diferencia de Ludwig van Beethoven, no se han conservado ya que Constanze los destruyó después de su muerte.[74]​
Fue criado según la moral católica y fue un miembro leal de la Iglesia en todas las etapas de su vida.[75]​
Mozart vivió en el centro del mundo musical vienés y conocía a un gran número y variedad de gente: compañeros músicos, intérpretes teatrales, amigos que como él se habían mudado desde Salzburgo y muchos aristócratas, incluyendo algún conocido del emperador José II. Solomon considera que sus tres amigos más cercanos pudieron haber sido Gottfried Janequin, el conde August Hatzfeld y Sigmund Barisani. Muchos otros incluyeron entre sus amistades a su viejo colega Joseph Haydn, los cantantes Franz Xaver Gerl y Benedikt Schack y el trompista Joseph Leutgeb. Leutgeb y Mozart mantuvieron un curioso tipo de burlas amistosas, a menudo con Leutgeb siendo el objeto de las bromas pesadas de Mozart.[76]​
Disfrutaba jugando al billar y el baile y tenía varios animales domésticos: un canario, un estornino, un perro y también un caballo para equitación lúdica.[77]​ En particular en su juventud, Mozart tenía una asombrosa inclinación hacia el humor escatológico (no tan insólito en su tiempo), que se aprecia en muchas de sus cartas que han sobrevivido, especialmente aquellas escritas a su prima Maria Anna Thekla Mozart alrededor de 1777-1778, pero también en su correspondencia con su hermana Nannerl y sus padres.[78]​ Mozart incluso escribió música escatológica, como el canon Leck mich im Arsch KV 231 (literalmente «Lámeme el culo», a veces idiomáticamente traducido como «Bésame el culo» o «Atáscate»).[e]​
Mozart aparece hoy como uno de los más grandes genios musicales de la historia. Fue excelente fortepianista, organista, violinista y director, destacaba por sus improvisaciones, que solía realizar en sus conciertos y recitales.
La música de Mozart, al igual que la de Joseph Haydn, es presentada como un ejemplo arquetípico del estilo clásico. En la época en la que comenzó a componer, el estilo dominante en la música europea era el estilo galante, una reacción contra la complejidad sumamente desarrollada de la música del Barroco. Pero cada vez más, y en gran parte en las manos del propio Mozart, las complejidades del contrapunto del Barroco tardío surgieron una vez más, moderado y disciplinado por nuevas formas y adaptado a un nuevo entorno estético y social. Mozart fue un compositor versátil y compuso obras para cada uno de los géneros musicales principales para la época, incluyendo la sinfonía, la ópera, el concierto para solistas y la música de cámara. Dentro de este último género, realizó composiciones para diversas agrupaciones de instrumentos, incluyendo el cuarteto y el quinteto de cuerda y la sonata para piano. Estas formas no eran nuevas, pero Mozart realizó avances en la sofisticación técnica y el alcance emocional de todas ellas. Casi sin ayuda de nadie desarrolló y popularizó el concierto para piano clásico. Compuso numerosas obras de música religiosa, incluyendo una gran cantidad de misas; pero también muchas danzas, divertimentos, serenatas y otras formas musicales ligeras de entretenimiento. También compuso para cualquier tipo de instrumento.
Los rasgos centrales del estilo clásico están todos presentes en la música de Mozart. La claridad, el equilibrio y la transparencia son los sellos de su trabajo, pero cualquier noción simplista de su delicadeza enmascara el poder excepcional de sus obras maestras más finas, como el Concierto para piano n.º 24 en do menor KV 491, la Sinfonía n.º 40 en sol menor KV 550 y la ópera bufa Don Giovanni. Charles Rosen hace hincapié en este punto:
Sobre todo durante su última década, Mozart explotó la armonía cromática hasta un extremo inusitado, con una notable seguridad y un gran efecto artístico.
Mozart siempre tuvo un don para absorber y adaptar los rasgos más valiosos de la música de otros compositores. Sus viajes seguramente le ayudaron a forjarse un lenguaje compositivo único.[f]​ En Londres siendo niño, tuvo lugar un encuentro con Johann Christian Bach y escuchó su música. En París, Mannheim y Viena encontró muchas otras influencias compositivas, así como las capacidades de vanguardia de la orquesta de Mannheim. En Italia conoció la obertura italiana y la ópera bufa, las cuales afectaron profundamente en la evolución de su propia práctica. Tanto en Londres como Italia, el estilo galante estaba en auge: música simple, brillante con una predilección por la cadencia; un énfasis en la tónica, dominante y subdominante y la exclusión de otro tipo de acordes, frases simétricas y particiones claramente articuladas en la forma total de los movimientos.[80]​ Algunas de las primeras sinfonías de Mozart son oberturas italianas, con tres movimientos que penetran unos en otros; muchas son homotonales (cada movimiento en la misma armadura de clave, con el movimiento más lento en el tono relativo menor). Otras obras imitan a las de Bach y otras muestran las simples formas binarias redondeadas escritas habitualmente por los compositores vieneses.
A medida que Mozart fue madurando, fue incorporando a sus composiciones más rasgos adaptados del Barroco. Por ejemplo, la Sinfonía n.º 29 en la mayor KV 201 tiene un tema principal de contrapunto en su primer movimiento y experimenta con longitudes de frase irregulares. Algunos de sus cuartetos a partir de 1773 tienen finales de fuga: probablemente bajo la influencia de Haydn, que había incluido tres finales en esa forma en su Opus 20 que había publicado por esa época. La influencia del movimiento Sturm und Drang (Tempestad e ímpetu) en la música, con su presagio de la llegada de la era romántica, es evidente en la música de ambos compositores en esa época y la Sinfonía n.º 25 en sol menor KV 183 de Mozart es otro buen ejemplo de ello.
Mozart a veces cambiaría su foco de interés entre la ópera y la música instrumental. Compuso óperas en cada uno de los estilos predominantes: la ópera bufa, como Las bodas de Fígaro, Don Giovanni y Così fan tutte; la ópera seria, como Idomeneo y La clemencia de Tito; y el singspiel, como El rapto en el serrallo y La flauta mágica. En sus óperas posteriores empleó cambios sutiles en la instrumentación, la textura orquestal y el timbre, para aportar una mayor profundidad emocional y destacar los movimientos dramáticos. Algunos de sus avances en el género operístico y la composición instrumental son: su empleo cada vez más sofisticado de la orquesta en las sinfonías y conciertos, que influyó en su orquestación operística y el desarrollo de su sutileza en la utilización de la orquesta al efecto psicológico en sus óperas, que fue un cambio reflejado en sus composiciones posteriores no operísticas.[81]​
La obra de Mozart fue catalogada por Ludwig von Köchel en 1862, en un catálogo que comprende 626 opus codificados con un número del 1 al 626 precedido por el sufijo KV.
La producción sinfónica e instrumental de Mozart consta de: 41 sinfonías, entre las que destacan la n.º 25 en sol menor, KV 183 (1773), la Sinfonía n.º 31, en re mayor, KV 297, París (1778), la n.º 35, en re mayor, KV 385, Haffner (1782); la n.º 36, en do mayor, KV 425, Linz (1783); la n.º 38, en re mayor, KV 504, Praga (1786); y las tres últimas (la n.º 39, en mi ♭ mayor, KV 543; la n.º 40, en sol menor, KV 550; y la n.º 41, en do mayor, KV 551, Júpiter), compuestas en 1788; varios conciertos (27 para piano, 5 para violín y varios para otros instrumentos); 18 sonatas para piano, 36 para piano y violín y para otros instrumentos, que constituyen piezas clave de la música mozartiana; música de cámara (dúos, tríos, cuartetos y quintetos); adagios, 61 divertimentos, serenatas, marchas.
Mozart empezó a escribir su primera sinfonía en 1764, cuando tenía 8 años de edad. Esta obra está influida por la música italiana, al igual que todas las sinfonías que compuso hasta mediados de la década de 1770, época en que alcanzó la plena madurez estilística. El ciclo sinfónico de Mozart concluye con una trilogía de obras maestras formado por las sinfonías n.º 39 en mi ♭ mayor, n.º 40 en sol menor y n.º 41 en do mayor, compuestas en 1788.
Con respecto a su producción operística (22 óperas), después de algunas obras «menores», sus grandes títulos llegaron a partir de 1781: Idomeneo rey de Creta (1781); El rapto en el serrallo (1782), la primera gran ópera cómica alemana; Las bodas de Fígaro (1786), Don Giovanni (1787) y Così fan tutte (1790), escritas las tres últimas en italiano con libretos de Lorenzo da Ponte; La flauta mágica (1791), en la que se reflejan los ritos e ideales masónicos, y La clemencia de Tito (1791).
El grueso de la música religiosa que escribió forma parte del periodo salzburgués, donde existe una gran cantidad de misas, como la Misa de Coronación, KV 317, 17 sonatas da chiesa y otras piezas para los diversos oficios de la Iglesia católica. En el período vienés disminuye su producción sacra. Sin embargo, las pocas obras de carácter religioso de este periodo son claros ejemplos de la madurez del estilo mozartiano. Compuso la Misa en do menor KV 427 (la cual quedó inconclusa, al igual que el Réquiem), el motete Ave verum corpus KV 618 y el Réquiem en re menor, KV 626.
También escribió bellísimas canciones, tales como Abendempfindung an Laura KV 523, entre otras. Compuso numerosas arias de concierto de gran calidad, muchas de las cuales fueron usadas en óperas de otros compositores a modo de encargo. De sus arias de concierto se pueden destacar, por su calidad y encanto: Popoli di Tessaglia...Io non chiedo, eterni dei KV 316, Vorrei spiegarvi, oh Dio! KV 418, ambas para soprano, o Per pietà KV 420, para tenor.
A principios de 2012 fue descubierta la obra Allegro Molto, de 84 compases y tres minutos de duración, en un desván del Tirol. Se estima que la obra fue compuesta en 1767.[83]​
Aunque algunas de las primeras piezas de Mozart fueron escritas para clavecín, en sus primeros años también se familiarizó con los pianos fabricados por el constructor de Ratisbona Franz Jakob Späth. Más tarde, cuando Mozart estaba de visita en Augsburgo, quedó impresionado por los pianos Stein y compartió esto en una carta a su padre.[84]​ El 22 de octubre de 1777, Mozart había estrenado su Concierto para piano triple (K.242) con instrumentos proporcionados por Stein. El organista de la catedral de Augsburgo, Demmler, estaba tocando la primera parte, Mozart la segunda y Stein la tercera.[85]​ En 1783, cuando vivía en Viena, compró un instrumento de Walter.[86]​ Leopold Mozart confirmó el vínculo que Mozart tenía con su Walter fortepiano: «Es imposible describir el ajetreo y el bullicio. El piano de su hermano se ha trasladado al menos doce veces de su casa al teatro o a la casa de otra persona».[87]​  
El discípulo más conocido de Mozart fue probablemente Johann Nepomuk Hummel, a quien Mozart tomó bajo tutela en su casa de Viena durante dos años cuando era un niño. Fue una figura de transición entre el Clasicismo y el Romanticismo.[82]​
Más importante es la influencia que Mozart ejerció sobre los compositores de generaciones posteriores. Después del aumento en su reputación después de su muerte, el estudio de sus partituras ha sido una parte común de la educación de los músicos clásicos.
Ludwig van Beethoven, catorce años más joven que Mozart, valoró y estuvo profundamente influido por las obras de este, al que conoció cuando era un adolescente. Tal y como se piensa, Beethoven interpretó en la orquesta de la corte de Bonn las óperas de Mozart[88]​ y viajó a Viena en 1787 para estudiar con Mozart. Algunas obras de Beethoven son comparables directamente con las obras de Mozart y compuso cadencias (WoO 58) del Concierto para piano n.º 20 en re menor KV 466 de Mozart.
Varios compositores han rendido homenaje a Mozart componiendo conjuntos de variaciones sobre sus temas. Beethoven escribió cuatro conjuntos (Op. 66, WoO 28, WoO 40 y WoO 46). Otros ejemplos son las Variaciones para piano y orquesta Op. 2 de Frédéric Chopin sobre «Là ci darem la mano» de Don Giovanni (1827) y las Variaciones y fuga sobre un tema de Mozart de Max Reger (1914), basado en la Sonata para piano n.º 11 KV 331.[89]​ Piotr Ilich Chaikovski compuso su Suite orquestal n.º 4 en sol, llamada «Mozartiana» (1887), como un tributo al compositor salzburgués. Existe un Vals del compositor Josef Lanner sobre temas de óperas de Mozart, llamado «Die Mozartisten» («Los Mozartistas», op. 196).
Dado que Wolfgang Amadeus Mozart tuvo una vida dramática en muchos sentidos, incluyendo su extraordinaria carrera como niño prodigio, sus luchas para alcanzar la independencia personal y desarrollar su carrera, sus problemas financieros y su muerte algo misteriosa mientras intentaba terminar su Réquiem, numerosos artistas han encontrado en Mozart una fuente de inspiración para sus obras. Tales trabajos han incluido novelas, óperas, películas —entre las que destaca Amadeus de Miloš Forman— y juegos. También se ha usado su imagen en la acuñación de monedas o en la emisión de sellos postales, en muchos casos con motivo de los aniversarios de su nacimiento o fallecimiento.
El asteroide (1034) Mozartia, descubierto el 7 de septiembre de 1924 por Vladímir Aleksándrovich Albitski,[90]​ y el piedemonte de hielo Mozart, en la isla Alejandro I de la Antártida, reciben su nombre en su honor.[91]​ Además, el nunatak Fígaro, situado cerca de este, toma su nombre del personaje Fígaro de su ópera de Las bodas de Fígaro.[92]​

Marco Polo (Venecia, c. 20 de septiembre de 1254-ib., 8 o 9 de enero de 1324)[2]​ fue un mercader y viajero italiano[3]​[4]​[5]​ (veneciano), famoso por los relatos que se le atribuyen sobre el viaje a Asia Oriental, manuscritos por Rustichello de Pisa con el título original de Il Milione, y conocido en español como Los viajes de Marco Polo, narración que dio a conocer en la Europa medieval las tierras y civilizaciones del Asia Central y China.[6]​[7]​
Existen discrepancias entre los historiadores sobre el hecho de que Marco Polo haya realizado efectivamente los viajes que se le atribuyen, en particular aquellos que lo ubican en Mongolia y China, de los que proviene su celebridad.[8]​ [7]​
Según los relatos, Marco Polo nació y aprendió a comerciar en Venecia (República de Venecia)[9]​ mientras su padre Niccolò Polo y su tío Maffeo Polo, viajaban por Asia donde habrían conocido a Kublai Kan. En 1269 ambos regresaron a Venecia y vieron por primera vez a Marco, llevándolo con ellos —según los relatos— en un nuevo viaje comercial a Asia, en el que habrían visitado Armenia, Persia y Afganistán, recorriendo toda la Ruta de la Seda, hasta llegar a Mongolia y China. Las narraciones afirman que Marco Polo permaneció 23 años al servicio de Kublai Kan, emperador de Mongolia y China, llegando a ser gobernador durante tres años de la ciudad china de Yangzhou y volviendo a Venecia en 1295.
En 1295 Venecia estaba en guerra con su rival, la República de Génova. En el transcurso del conflicto Marco fue capturado y encarcelado por los genoveses. Fue en esa situación que en 1298, durante su período en la cárcel, conoció al escritor Rustichello de Pisa, a quien relató sus fabulosos viajes, que fueron el tema del libro conocido en principio como Le divisament du monde, Livre des merveilles du monde, o Il Milione. Fue liberado en 1299, Marco Polo se convirtió en un rico mercader y miembro del Gran Consejo de la República de Venecia. Murió en 1324 y lo enterraron en la iglesia de San Lorenzo de su ciudad. El relato de sus viajes, inspiró, entre otros, a Cristóbal Colón que poseía un ejemplar del libro cuidadosamente anotado.[10]​
En la época de Marco Polo, el comercio en Europa seguía un sistema triangular, en el que los productos de lujo procedentes de Oriente (seda, especias) ocupaban un importante lugar. Estos, en la conocida como ruta de la seda atravesaban  Asia Central y las tierras controladas por los sarracenos, después de lo cual los compraban comerciantes italianos (venecianos, genoveses, pisanos...), que obtenían grandes beneficios revendiéndolos en Europa.
A causa de ello, Venecia y otros puertos italianos ganaron importancia y comenzaron una política comercial agresiva para explotar estas rutas comerciales.
Durante la Baja Edad Media, la República de Venecia comenzó a convertirse en una potencia mediterránea. Al control del interior y de la costa de Dalmacia, se unió una extensa actividad mercantil con Oriente, que le llevó a establecer consulados y colonias de comerciantes por todo el Mediterráneo Oriental. Apoyó a los cruzados como manera de contrarrestar al islam y mantuvo un largo conflicto con Génova por el predominio comercial.
Durante la Cuarta Cruzada, por sugerencia veneciana, los cruzados saquearon Constantinopla, decapitando el Imperio bizantino y conquistando numerosos territorios. Aunque el subsiguiente Imperio latino fue pronto reconquistado por los bizantinos, Venecia siguió controlando varias islas y ciudades, y se convirtió en una de las principales potencias mercantiles.
El Imperio mongol fue instituido por Genghis Kan en 1206. Tras largas luchas internas, unificó a las diversas tribus mongolas bajo su mando, involucrándolas en una expansión que les llevaría a conquistar China, Asia Central, Rusia y llegar hasta Irak, Siria y Anatolia. También les llevaría a la incursión de Europa central y del resto de la Europa Oriental para poder consolidar la conquista de Rusia. Así llegaron, entre otros sitios, temporalmente a Dalmacia, que estaba controlada por Venecia, lo que puso a los venecianos por primera vez en contacto directo con los mongoles.
A su muerte le sucedió su hijo Ogodei, quien continuó con esta expansión y consolidó la jerarquía del Gran Kan sobre los diversos reinos mongoles. En tiempos de Marco Polo este Gran Kan era Kublai Kan.[10]​
El mundo conocido por los europeos no iba mucho más allá del actual Oriente Medio. Las pocas noticias que se tenían de lo que estaba más allá eran generalmente confusas y muy mitificadas. Es de destacar la leyenda del Preste Juan, un mítico rey cristiano que se suponía existía rodeado de infieles en Asia Central. Los intercambios comerciales se encontraban casi siempre mediatizados por persas y árabes.
La expansión del Imperio mongol les llevó a las mismas puertas de Europa tras atravesar las estepas rusas y amenazar Polonia, aunque pronto se retiraron. Más al sur, sin embargo, los mongoles saquearon Bagdad (Irak) y sometieron a reinos musulmanes que se habían enfrentado en las cruzadas con los cristianos.
Es así como se despierta el interés por los mongoles en Europa. A la curiosidad por esos bárbaros, tenidos hasta entonces como seres casi mitológicos, se le suma en lo político la posibilidad de obtener un aliado contra el enemigo islámico, una forma más ventajosa de negociar con Oriente en lo económico, y un deseo evangelizador, dada la gran tolerancia religiosa de los mongoles. Antes de Marco Polo, varios misioneros, como Giovanni da Pian del Carpine, viajaron como embajadores a Oriente, aunque sin conseguir resultados concretos. Se hace referencia a los contactos entre romanos y el Imperio chino, pero este también estableció contacto con los romanos con anterioridad a la Ruta de la Seda. Uno de los primeros contactos que tuvo China con Roma fue cuando el emperador Ban Chao hizo una campaña contra los nómadas de Asia Central y envió a uno de sus colaboradores, Ga Yin, que viajó hacia occidente visitando los establecimientos comerciales romanos de la costa oriental del Mar Negro. Por tanto, el contacto entre Roma y China era recíproco, pese a que Roma tenía más información sobre China gracias a la multitud de viajes que se habían hecho hacia aquella zona.

Existe muy escasa información sobre Marco Polo y su familia aparte de la contenida en el libro que relata su viaje. Constan unos pocos documentos venecianos, pero están completamente ausentes en las fuentes chinas.[a]​ Se encuentra información adicional, que hay que tratar con precaución por tratarse en su mayor parte de rumores y leyendas populares, en el prefacio de la edición italiana de Il milione publicada en 1559 por Giovanni Battista Ramusio.[12]​
Marco Polo nació en 1254 en una familia de mercaderes.[13]​ Su padre, Niccolò, había formado una asociación comercial, o fraterna compagnia, con sus hermanos Marco y Maffeo,[b]​ cuya residencia familiar se encontraba en la parroquia de San Severo, al norte de la basílica de San Marcos.[15]​ Después de la toma de Constantinopla de 1204 Venecia poseía su propio barrio en la ciudad, además de controlar el puerto, y muchos venecianos se instalaron allí, entre ellos los Polo.[16]​ Una de las rutas comerciales que de allí partían era la del mar Negro y en ella se especializaron, hasta el punto de que disponían de una casa en el puerto de Soldaia, el principal emporio comercial italiano de Crimea.[17]​
La familia Polo entró al patriciado veneciano en el siglo XIV, bastante después de la Serrata del Consejo Mayor. Esto se consiguió en gran parte a los matrimonios convenidos con varias de las familias de la más alta nobleza de Venecia, entre las que se encontraron los Querini, Bragadín, Trevisán, Delfín, Gradénigo, Contarini y Vendramín. El mismo aventurero, Marco, se casó con Donata Badoer, también perteneciente a una familia noble, aunque perdió su posición al casarse con el célebre veneciano.
La elección del nuevo papa se retrasaba (acabaría siendo la más prolongada de la historia) así que Niccolò y Matteo iniciaron el viaje de regreso a la corte de Kublai Kan en 1271, pero esta vez acompañados por Marco, que ya tenía diecisiete años. Llegaron a Acre, donde se encontraron de nuevo con Tedaldo Visconti, que quizá estaba allí en relación con la cruzada en la que estaba embarcado el futuro rey Eduardo I de Inglaterra. Tras desviarse a Jerusalén para hacerse con el aceite de la lámpara del Santo Sepulcro, siguieron viaje hasta Ayas donde se enteraron de que la elección papal había recaído precisamente en Tedaldo Visconti, que adoptó el nombre de Gregorio X. Regresaron de inmediato a Acre, en una galera facilitada por el rey León III de Armenia Menor, donde Visconti les facilitó nuevas credenciales además de hacer que les acompañaran dos frailes dominicos, aunque estos pronto abandonaron el largo viaje.[18]​[10]​
Marco pronto se ganó el favor de Kublai Kan, quien le hizo su consejero y emisario durante diecisiete años. Gracias a ello llegó a conocer las vastas regiones de China y los numerosos logros de la civilización china, muchos de los cuales eran más avanzados que los contemporáneos europeos.
Cuando una embajada del rey de Persia le solicita a Kublai Kan una princesa para el rey, los Polo la acompañan, decidiendo regresar a Venecia.
A su regreso de China en 1295 (escoltando a una princesa china llamada Kokacín), la familia de Marco Polo se estableció en Venecia donde se convirtió en una sensación y atrajo a multitud de oyentes, que a duras penas creían sus historias sobre la lejana China.
No está claro cómo llegó Marco a la prisión genovesa, donde se relata que en 1298 conoció a «Rusticiaus» de Pisa. En la primera mitad del siglo XIV d. C. en su obra Imago Mundi seu Chronica, el fraile dominico Jacopo d'Acqui afirmó que Marco fue hecho prisionero en 1296 tras una batalla marítima entre mercaderes genoveses y venecianos en las cercanías de Layas, aunque la fecha real de la batalla corresponde a 1294, el año anterior a la llegada de Marco a Venecia. De Giambattista Ramusio, en el siglo XVI d. C., procede la versión que sitúa el origen del encarcelamiento en la batalla naval de Curzola, la gran derrota veneciana que tuvo lugar en septiembre de 1298, aunque la fecha deja escaso margen para que fuera trasladado a Génova, conociera a Rustichello y ambos comenzaran a redactar su libro. Quizá resultó capturado en algún otro enfrentamiento de menor importancia en torno a esas fechas, pero en cualquier caso debió ser liberado tras la ratificación del tratado de paz entre Génova y Venecia en julio de 1299.[c]​[20]​
Es posible que, como era habitual en Génova, la «prisión» consistiera en estar confinado en casa de alguna familia, pero en cualquier caso Marco pasó los pocos meses de su encierro dictando a Rustichello un detallado relato de sus viajes por las entonces desconocidas regiones de Extremo Oriente. El manuscrito original, probablemente escrito en francés antiguo o francoitaliano, no ha sobrevivido, pero el libro,[21]​ Il milione (‘El millón’, más conocido en español como Los viajes de Marco Polo o Libro de las maravillas) tuvo rápido éxito y fue traducido pronto a muchas lenguas europeas. Estas traducciones, incluso las más tempranas, son a menudo bastante diferentes entre sí y contienen numerosos detalles contradictorios y controvertidos.[22]​
Existe una versión alternativa y minoritaria que considera la colaboración de Marco Polo y Rustichello en la prisión de Génova como una ficción.[23]​ En este caso la versión de Rustichello sería la traducción al francés de un texto escrito originariamente por Marco en veneciano, un manuscrito que también se habría perdido cuya versión conservada más próxima sería la traducción al latín realizada entre 1310 y 1317 por el dominico Francesco Pipino. Rustichello habría escrito la obra para el rey Eduardo I de Inglaterra quien, en su condición de cruzado, tendría gran interés en datos sobre Asia y Oriente.[24]​
Marco Polo murió, después de redactar testamento, entre la puesta del sol y la medianoche del domingo 8 de enero de 1324,[25]​ fecha que podría corresponder al 9 de enero debido a la ley veneciana que fijaba el final del día en el momento de la puesta del sol.[1]​ Fue enterrado en la iglesia de San Lorenzo, donde también reposaban los restos de su padre, Niccolò,[25]​ aunque su sarcófago desapareció durante las obras de reconstrucción en 1592.[26]​
Procede de Jacopo d'Acqui la anécdota, significativa de las dudas que ya en su momento suscitaron sus relatos, de que en su lecho de muerte su familia pidió a Marco que corrigiese o eliminase de sus historias todo lo que no fuese cierto. Marco se negó, insistiendo: «¡Solo he contado la mitad de lo que verdaderamente vi!».[25]​
Desde entonces, gran número de historiadores han puesto en duda, en todo o en parte, la veracidad básica del relato:[27]​ algunos mantienen abierta la cuestión mientras otros descartan basándose en la evidencia interna la presencia de Marco Polo en China. Algunos autores afirman que Marco Polo (o también a veces incluso Maffeo y Niccoló) solo llegó a algún lugar de Asia central, como Bujará o Karakórum y que todos los detalles sobre China proceden de escritos persas o relatos de segunda mano.[28]​ Algunos hechos dudosos, omisiones o inexactitudes que señalan son:
Para responder a estos argumentos algunos historiadores han supuesto que el hecho de atribuirse importante participación en algunos acontecimientos pueda responder al deseo de Marco Polo de ocupar algún puesto de relevancia a su vuelta, o también que fueran recursos de Rustichello para incrementar el interés del relato, como podría indicar el hecho de que ambas anécdotas se omitan en manuscritos que podrían corresponder a revisiones del texto hechas con posterioridad por el propio Marco.[34]​ 
También se ha argumentado que en los testamentos de los Polo, estaban a su muerte todavía en posesión de varias paiza de oro auténticas, los salvoconductos que se describen como procedentes del Gran Kan en persona («magnifici chan tartarorum»), abriendo también la posibilidad de que la paiza de Marco proviniera de su tío Maffeo.[35]​ Además probablemente proviene del relato de Marco Polo la primera mención a la existencia de Japón (llamada Cipango por Polo) en la literatura occidental, un caso único antes del siglo XVI d. C..[36]​
Algunos historiadores han hecho notar que la redacción formularia y estereotipada de las descripciones que se ofrecen en el libro de lugares y acontecimientos ocurridos en el sur de China contrasta con la más detallada y concreta en cuanto los sucesos se producen en el norte. De esa diferencia concluyen que es posible que los Polo pasaran gran parte del tiempo en esta última zona, entre la nueva capital, Daidu, y la residencia de verano de Shangdu, y que su conocimiento del sur dependiera mucho más de testimonios de segunda mano.[37]​ Hay que tener en cuenta también que en los relatos, los Polo se mueven sobre todo en el entorno de la corte Yuan, basada en los clanes mongoles, y alejada de la base de la población china, que recién había sido conquistada y cuyas costumbres les eran extrañas[38]​, por lo cual difícilmente documentarían grupos sociales con quienes rara vez contactaban.
En los relatos de Marco Polo, también se describen otros aspectos de la vida en Asia Oriental, con mucho detalle: el papel moneda, el Gran Canal, la estructura del ejército mongol, los tigres y el sistema postal imperial.
Las muralla china tampoco es mencionada por otros viajeros, como Odorico de Pordenone o Giovanni Marignolli, que recorrieron esa zona en el siglo XIV d. C.;[31]​ también se advierte que la Gran Muralla fue construida para detener invasiones mongolas pero más de 50 años antes de la llegada de Polo los mongoles habían invadido exitosamente China e instaurado la Dinastía Yuan.[39]​
Cronistas posteriores a Marco Polo rastrearon sus orígenes hasta la isla de Curzola en el mar Adriático (actualmente Korčula, en Croacia) donde incluso se sigue conservando una vieja casa en la que se dice que nació. Sin embargo, la historiografía moderna tiene serias dudas de este origen, pues el apellido Polo (de origen veneciano) aparece mencionado varias veces en ciudades del norte de Italia. No obstante, hay quienes afirman que su verdadero nombre y apellido eran Marc Pol, apellido que, efectivamente, tuvo su primera aparición en Dalmacia. Esta última afirmación es dada con base en los registros aparecidos en el anuario veneciano Chronicon Iustiniani (1358). El escudo familiar de los Pol contiene tres pájaros de agua, aves que recibían el nombre de "pol" en Dalmacia del Sur, mientras que en Venecia se les llamaban "pola", palabra de la cual se cree se derivaron los apellidos "Polo" y "Pollo" en Italia.[cita requerida]
Aunque los Polo no fueron en forma alguna los primeros europeos en llegar a China por tierra (considérese por ejemplo a Juan de Plano Carpini así como la única delegación romana que partió a la China con objeto de establecer relaciones diplomáticas entre Roma y China), gracias al libro de Marco, su viaje fue el primero en conocerse ampliamente y el mejor documentado hasta entonces. Finalmente también fue Marco Polo el que más lejos viajó gracias a las circunstancias de su tiempo y, gracias a ello, Europa empezó a conocer a través del libro lugares que no se conocieron antes como Japón, Indonesia e Indochina.
La leyenda cuenta que Marco Polo introdujo en Italia algunos productos de China, entre ellos los helados, la piñata y la pasta, especialmente los espaguetis. Sin embargo, esta leyenda está muy cuestionada. Por ejemplo, hay pruebas de que la pasta era conocida en Grecia e Italia desde la antigüedad. En la España árabe hay referencias escritas acerca de los fideos (llamados entonces aletría) desde el siglo XII.
El libro escrito por Marco Polo, a pesar de que muchas de sus aseveraciones, en su época, se pusieron en duda, inspiró a muchos viajeros y exploradores. El mismo Cristóbal Colón tenía una copia, con anotaciones manuscritas suyas en los márgenes, que todavía se conserva.[41]​
Marco Polo aparece nombrado en el videojuego de ficción histórica Assassin's Creed II, cuando en 1321 se topó con los Asesinos mientras visitaba a Kublai Kan, y debió ser entonces cuando consiguió el códice.[44]​[45]​ Luego volvió a Italia y se lo dio a Dante Alighieri, que también se relacionaba con los Asesinos.[44]​[45]​
El videojuego Uncharted 2 se inspira en las aventuras de Marco Polo, donde Nathan Drake busca la Piedra Cintamani y sigue su rastro hasta Shambala.[46]​
Posee una serie homónima en Netflix, Lorenzo Richelmy interpreta a un joven Marco Polo que pasa de ser prisionero de Kublai Khan (Benedict Wong) a convertirse en uno de sus mejores aliados y amigos.

Michael Faraday (/ˈmaɪkəl ˈfæɹəˌdeɪ/; Newington Butt, 22 de septiembre de 1791-Hampton Court, 25 de agosto de 1867) fue un científico británico que estudió el electromagnetismo y la electroquímica. Sus principales descubrimientos incluyen la inducción electromagnética, el diamagnetismo y la electrólisis.
A pesar de la escasa educación formal recibida, Faraday es uno de los científicos más influyentes de la historia.[1]​ Mediante su estudio del campo magnético alrededor de un conductor por el que circula corriente continua, fijó las bases para el desarrollo del concepto de campo electromagnético. También estableció que el magnetismo podía afectar a los rayos de luz y que había una relación subyacente entre ambos fenómenos.[2]​[3]​ Descubrió asimismo el principio de inducción electromagnética, el diamagnetismo, las leyes de la electrólisis e inventó algo que él llamó dispositivos de rotación electromagnética, que fueron los precursores del actual motor eléctrico.[4]​
En el campo de la química, Faraday descubrió el benceno, investigó el clatrato de cloro, inventó un antecesor del mechero de Bunsen, el sistema de números de oxidación e introdujo términos como ánodo, cátodo, electrodo e ion. Finalmente, fue el primero en recibir el título de Fullerian Professor of Chemistry en la Royal Institution de Gran Bretaña, que ostentaría hasta su muerte.
Faraday fue un excelente experimentador, que transmitió sus ideas en un lenguaje claro y simple. Sus habilidades matemáticas, sin embargo, no abarcaban más allá de la trigonometría y el álgebra básica. James Clerk Maxwell tomó el trabajo de Faraday y otros y lo resumió en un grupo de ecuaciones que representan las actuales teorías del fenómeno electromagnético. El uso de líneas de fuerza por parte de Faraday llevó a Maxwell a escribir que "demuestran que Faraday ha sido en realidad un gran matemático. Del cual los matemáticos del futuro derivarán valiosos y prolíficos métodos".[5]​ La unidad de capacidad eléctrica en el Sistema Internacional de Unidades (SI), el faradio (F), se denomina así en su honor.
Albert Einstein tenía colgado en la pared de su estudio un retrato de Faraday junto a los de Isaac Newton y James Clerk Maxwell.[6]​ El físico neozelandés Ernest Rutherford declaró: "Cuando consideramos la extensión y la magnitud de sus descubrimientos y su influencia en el progreso de la ciencia y de la industria, no existen honores que puedan retribuir la memoria de Faraday, uno de los mayores descubridores científicos de todos los tiempos".[7]​[1]​ Fue miembro de la Royal Society de Londres.
Faraday nació en la aldea de Look Butt,[8]​ que es ahora parte del municipio de Southwark (prácticamente en el centro de Londres), pero que, en aquel entonces, era una zona suburbana del condado de Surrey.[9]​ No provenía de una familia rica. 
De niño, la madre de Faraday lo sacó del colegio porque los métodos y castigos para los niños eran terribles; la maestra de Faraday se burlaba y le castigaba por no pronunciar bien la "R". Faraday comenzó a estudiar por su cuenta, pero su creatividad e ingenio lo llevarían a la fama a pesar de no tener una formación rigurosa en ciertos campos de la ciencia. 
Su padre, James, se trasladó junto a su esposa y sus dos hijos a Londres durante el invierno de 1791, desde Outhgill, en Westmorland, donde trabajó como aprendiz del herrero del pueblo. Michael nació durante el otoño de ese año. El joven Michael Faraday, el tercero de cuatro hermanos, llegó a ser, a la edad de 14, aprendiz de George Riebau, encuadernador y vendedor de libros de la ciudad.[10]​
Durante los siete años que duró su aprendizaje, Faraday leyó muchos libros, entre ellos The improvement of the Mind,de Isaac Watts, estudiando con gran entusiasmo los principios y sugerencias ahí escritos. Durante esta época también desarrolló su interés por la ciencia, especialmente por el fenómeno eléctrico.
En 1812, a la edad de 20 años, y ya acabando su aprendizaje de encuadernador, Faraday comenzó a asistir a las conferencias del destacado químico inglés Humphry Davy, de la Royal Institution y de la Royal Society, y de John Tatum, fundador de la City Philosophical Society. La mayoría de las invitaciones para las conferencias fueron ofrecidas a Faraday por William Dance, uno de los fundadores de la Royal Philharmonic Society. Faraday, posteriormente, envió a Davy un libro de 300 páginas basado en notas que él mismo había tomado durante esas conferencias. La respuesta de Davy fue inmediata, amable y favorable. Davy, durante un experimento con tricloruro de nitrógeno, se dañó gravemente la vista, por lo que decidió contratar a Faraday como su secretario. Cuando uno de los asistentes de la Royal Institution, John Payne, fue despedido, Humphry Davy se vio en la necesidad de buscar un sustituto para el puesto, designando a Faraday asistente de química de la Royal Institution, el 1 de marzo de 1813.[8]​
En la clasista sociedad inglesa de la época, Faraday no era considerado un caballero. Cuando Davy decidió emprender un viaje por el continente en 1813-1815, su sirviente prefirió no ir. Faraday, que iba en calidad de asistente científico, se vio forzado a suplir las tareas del sirviente hasta que se pudiera encontrar uno nuevo en París. La esposa de Davy, Jane Apreece, se negaba a tratar a Faraday como un igual (le obligaba a viajar fuera del carruaje, comer con los sirvientes, etcétera), le hacía que su vida resultase tan miserable, que lo llevó a contemplar la idea de regresar a Inglaterra solo y abandonar la ciencia. El viaje, sin embargo, le dio acceso a la élite científica europea y sus fascinantes y estimulantes ideas.[8]​
Faraday se casó con Sarah Barnard (1800-1879) el 12 de junio de 1821.[11]​ Se conocieron a través de sus familias en la iglesia Sandemaniana, confesando su fe a esta congregación el mes siguiente a su matrimonio. No tuvieron hijos.[12]​
Faraday fue un cristiano devoto; su congregación Sandemaniana era una filial de la Iglesia de Escocia. Una vez casado, sirvió como diácono y, durante dos períodos, como presbítero. Su iglesia estaba ubicada en Paul's Alley en Barbican Estate. Este lugar de reuniones fue trasladado a Barnsbury Grove, Islington, en 1862. Aquí fue donde Faraday cumplió los últimos dos años de su segundo período de presbítero, antes de dimitir de su cargo.[13]​[14]​ Biógrafos del científico han señalado que "un fuerte sentimiento de unidad entre Dios y la naturaleza impregnó la vida y el trabajo de Faraday".[15]​
En junio de 1832, la Universidad de Oxford concedió a Faraday el grado de Doctor of Civil Law (honorario). Durante su vida, la corona británica le ofreció un título de caballero, en reconocimiento a sus servicios a la ciencia, el cual fue rechazado por motivos religiosos. Faraday creía que acumular riquezas y perseguir recompensas mundanas atentaba contra la palabra sagrada de la Biblia, prefiriendo seguir siendo llamado "simplemente Sr. Faraday, hasta el final".[16]​
Rechazó dos veces convertirse en presidente de la Royal Society.[17]​
Fue elegido miembro extranjero de la Real Academia de las Ciencias de Suecia en 1838, y fue uno de los ocho miembros extranjeros elegidos por la Academia de Ciencias de Francia en 1844.[18]​
Faraday sufrió un colapso nervioso en 1839, pero regresaría posteriormente a sus investigaciones sobre electromagnetismo.[19]​
En 1848, como resultado de las gestiones del príncipe consorte Alberto, se le concedió una casa de Gracia y Favor en Hampton Court en Middlesex, libre de gastos y costos de mantenimiento. En 1858, Faraday se retiró a vivir a ese lugar.[20]​
Al ser consultado por el gobierno británico con el fin de ayudar en la producción de armas químicas para la Guerra de Crimea (1853-1856), Faraday rechazó participar, alegando motivos éticos.[21]​
Faraday murió en su casa en Hampton Court, a 35 km al suroeste de Londres, el 25 de agosto de 1867, a la edad de 75 años.[22]​ A pesar de haber rechazado una sepultura en la Abadía de Westminster, existe ahí una placa conmemorativa en su nombre, cerca de la tumba de Isaac Newton. Faraday fue sepultado en la sección de disidentes del Cementerio de Highgate.
Desde 1935 el cráter lunar "Faraday" lleva este nombre en su memoria.[23]​
El primer trabajo de Faraday en el área de la química fue como asistente de Humphry Davy. Estaba especialmente interesado en el estudio del cloro, descubriendo dos nuevos compuestos de cloro y carbono. También condujo los primeros rudimentarios experimentos sobre difusión de gases, fenómeno que había sido previamente identificado por John Dalton. La importancia física de este fenómeno fue enteramente revelada por Thomas Graham y Johann Josef Loschmidt. Tuvo éxito al lograr licuar diversos gases, investigó la aleación del acero y produjo varios nuevos tipos de vidrio destinados a fines ópticos. Un ejemplar de estos pesados cristales tomaría posteriormente una gran importancia histórica; cuando Faraday ubicó el vidrio en un campo magnético descubrió la rotación del plano de polarización de la luz. Este ejemplar fue también la primera sustancia que se encontró que era repelida por los polos de un imán.
Faraday inventó una temprana forma del mechero de Bunsen, usado en todos los laboratorios de ciencia del mundo como una buena fuente de calor.[24]​[25]​
Trabajó ampliamente en el campo de la química, descubriendo sustancias tales como el benceno y condensando gases como el cloro. La licuefacción de gases ayudó a establecer que estos corresponden a vapores de líquidos con bajo punto de ebullición, otorgando una base más sólida al concepto de agregación molecular. En 1820, Faraday informó de la primera síntesis de compuestos de cloro y carbono, el hexacloroetano (C2Cl6) y el tetracloroetileno (C2Cl4), publicando sus resultados al año siguiente.[26]​[27]​[28]​
También descubrió la composición del clatrato hidrato de cloro, que había sido descubierto por Humphry Davy en 1810.[29]​[30]​ Asimismo, es responsable del descubrimiento de las leyes de la electrólisis y de introducir términos como ánodo, cátodo, electrodo e ion, propuestos en gran parte por William Whewell.
Faraday fue el primero en descubrir lo que posteriormente serían llamadas nanopartículas metálicas. En 1847 descubrió que las propiedades ópticas del coloide de oro diferían de las del metal macizo. Esta fue, probablemente, la primera observación registrada sobre los efectos del tamaño cuántico, y podría ser considerado como el nacimiento de la nanociencia.[31]​
Faraday es más conocido por su trabajo relacionado con la electricidad y el magnetismo. Su primer experimento registrado fue la construcción de una pila voltaica con siete monedas de medio penique, apiladas junto a siete discos chapados en zinc y seis trozos de papel humedecidos con agua salada. Con esta pila pudo descomponer el sulfato de magnesio (primera carta a Abbott, 12 de julio de 1812).
En 1821, poco después del descubrimiento del fenómeno electromagnético por parte del físico y químico danés Hans Christian Ørsted, Davy y el científico británico William Hyde Wollaston intentaron, sin éxito, diseñar un motor eléctrico.[33]​ Faraday, habiendo discutido el problema con los dos hombres, persistió y logró construir dos dispositivos que producían, lo que él denominó, "rotación electromagnética". Uno de ellos, conocido ahora como motor homopolar, producía un movimiento circular continuo ocasionado por la fuerza magnética circular en torno a un alambre que se extendía hasta un recipiente con mercurio que tenía un imán en su interior; el alambre rota alrededor del imán cuando se le suministra una corriente eléctrica desde una batería química. Estos experimentos e inventos conformaron las bases de la tecnología electromagnética moderna. La emoción debida a estos descubrimientos llevó a Faraday a publicar sus trabajos sin haberlos presentado previamente a Davy o Wollaston. La controversia resultante dentro de la Royal Society tensó la relación con su mentor Davy y pudo haber contribuido a que Faraday fuera designado para otras tareas, impidiendo su participación en investigación electromagnética durante varios años.[34]​[35]​
Desde su primer descubrimiento en 1821, Faraday continuó su trabajo de laboratorio, explorando las propiedades electromagnéticas de distintos materiales y desarrollando la experiencia requerida. En 1824, diseñó un circuito para estudiar si el campo magnético podía regular el flujo eléctrico de un cable adyacente, pero no encontró tal relación.[36]​
Durante los siguientes siete años, Faraday ocupó la mayor parte de su tiempo perfeccionando la fórmula de un cristal con cualidades ópticas, el borosilicato de plomo,[37]​ el cual utilizaría en sus posteriores experimentos que lo llevarían a relacionar el fenómeno electromagnético con la luz.[38]​
En su tiempo libre continuó publicando sus trabajos experimentales en óptica y electromagnetismo; mantuvo también correspondencia con científicos que había conocido en su viaje a través de Europa con Davy y que también se encontraban investigando el electromagnetismo.[39]​ Dos años después de la muerte de Davy, en 1831, Faraday dio inicio a la gran serie de experimentos que lo llevarían a descubrir la inducción electromagnética.
El gran descubrimiento de Faraday surgió cuando enrolló dos solenoides de alambre alrededor de un aro de hierro, y encontró que cuando hacía pasar corriente por un solenoide, en el otro solenoide se inducia temporalmente otra corriente.[33]​ Este fenómeno se conoce como inducción mutua.[40]​ Este aparato aún se expone en la Royal Institution. En experimentos posteriores, observó que si hacía pasar un imán por el interior de una espira de alambre conductor, circularía una corriente eléctrica por este alambre. La corriente también fluía si la espira se movía sobre el imán en reposo. Sus demostraciones establecieron que un campo magnético variable generaba un campo eléctrico; esta relación fue modelada matemáticamente por James Clerk Maxwell como la Ley de Faraday, que posteriormente se convertiría en una de las cuatro ecuaciones de Maxwell, y que a su vez evolucionarían a un modelo más general conocido como teoría de campos. Faraday usaría después los principios que había descubierto para construir la dínamo eléctrica, antecesor de los actuales generadores y motores eléctricos.
En 1832, realizó una serie de experimentos para estudiar la naturaleza fundamental de la electricidad. Faraday utilizó "estática", baterías y "electricidad animal" para producir el fenómeno de atracción eléctrica, electrólisis, magnetismo, etc. Concluyó que, al contrario de la opinión científica de la época, la división entre varios "tipos" de electricidad era irreal. En vez de eso, propuso que solo existe un "tipo" de electricidad, y que unos valores variables de cantidad e intensidad (corriente y voltaje) producirían diferentes grupos de fenómenos.[33]​
Cerca del final de su carrera, Faraday propuso que la fuerza electromagnética podía extenderse en el espacio vacío alrededor de un conductor. Esta idea fue rechazada por sus pares científicos, no pudiendo vivir lo suficiente como para ver la aceptación de su proposición por parte de la comunidad científica. El concepto de Faraday de líneas de flujo saliendo desde cuerpos cargados e imanes dio una forma de ver los campos eléctrico y magnético; ese modelo conceptual fue crucial para el exitoso desarrollo de dispositivos electromecánicos que dominarían la industria y la ingeniería por el resto del siglo XIX.
En 1845, Faraday descubrió que muchos materiales exhibían una débil repulsión frente a campos magnéticos: un fenómeno que denominó diamagnetismo.[42]​
También descubrió que el plano de polarización de la luz linealmente polarizada podía rotarse debido a la aplicación de un campo magnético externo alineado con la dirección de propagación de la luz. Este fenómeno es llamado en la actualidad efecto Faraday. Así lo hace constar en su libro de notas: "He, al fin, tenido éxito en iluminar una curva magnética o línea de fuerza y en magnetizar un rayo de luz".[43]​
En los últimos años de su vida, en 1862, Faraday utilizó un espectroscopio para estudiar la alteración de las líneas espectrales en presencia de un campo magnético. El equipamiento disponible, sin embargo, no fue suficiente como para mostrar una determinación precisa del cambio espectral. Posteriormente, el físico neerlandés Pieter Zeeman utilizaría un aparato mejorado para estudiar el mismo fenómeno, publicando sus resultados en 1897 y recibiendo el premio Nobel de Física en 1902. Tanto en su publicación de 1897[44]​ como en su discurso de aceptación del Nobel en 1902,[45]​
Zeeman hizo referencia al trabajo de Faraday.
En su trabajo en electricidad estática denominado La cubeta de Faraday, se demostró que la carga eléctrica se acumula solo en el exterior de un conductor cargado, sin importar lo que hubiera en su interior. Esto es debido a que las cargas se distribuyen en la superficie exterior de tal manera que los campos eléctricos internos se cancelan. Este efecto de barrera es conocido como jaula de Faraday.
De una obra de Isaac Watts titulada The Improvement of the Mind —La mejora de la mente—, leída a sus catorce años, Michael Faraday adquirió estos seis constantes principios de su disciplina científica:
Faraday llevó a cabo este descubrimiento en 1845. Consiste en la desviación del plano de polarización de la luz como efecto de un campo magnético, al atravesar un material transparente como el vidrio. Se trataba del primer caso conocido de interacción entre el magnetismo y la luz
Michael Faraday inició la primera serie de Conferencias de Navidad en 1825. Esto llegó en un momento en el que la educación organizada para jóvenes era escasa. Presentó un total de diecinueve series de conferencias.[46]​


Nikola Tesla (en cirílico serbio: Никола Тесла; Smiljan, Imperio austríaco, actual Croacia; 10 de julio de 1856-Nueva York, 7 de enero de 1943) fue un inventor, ingeniero eléctrico y mecánico serbio nacionalizado estadounidense,[1]​[2]​[3]​ célebre por sus contribuciones al diseño del moderno suministro de electricidad de corriente alterna (CA).[4]​
Tesla, que nació y se crio en el Imperio austríaco, estudió ingeniería y física en la década de 1870 sin obtener un título, aunque adquirió experiencia práctica a principios de la década de 1880 trabajando en telefonía para la empresa Continental Edison, que por entonces lideraba la nueva industria de la energía eléctrica. En 1884 emigró a Estados Unidos, donde adquirió la doble nacionalidad. Trabajó durante un corto tiempo en Edison Machine Works en Nueva York antes de emprender el camino por su cuenta. Con la ayuda de socios para financiar y comercializar sus ideas, Tesla fundó laboratorios y empresas en Nueva York para desarrollar dispositivos eléctricos y mecánicos. Su motor asíncrono de corriente alterna (CA) y las patentes relacionadas con el sistema polifásico, licenciadas por Westinghouse Electric en 1888, le reportaron grandes sumas de dinero y además se convirtieron en la piedra angular del sistema polifásico finalmente comercializado por esta empresa.
Tesla adquirió fama como inventor, mostrando en su laboratorio los logros a numerosas personalidades y patrocinadores adinerados, además de sobresalir por su talento para el espectáculo en conferencias públicas. A lo largo de la década de 1890, Tesla siguió investigando sobre iluminación inalámbrica y la distribución inalámbrica de energía eléctrica por todo el mundo a través de sus experimentos con energía de alta tensión y alta frecuencia en Nueva York y Colorado Springs. Construyó uno de los primeros barcos con control remoto inalámbrico. En 1893 anunció la posibilidad de establecer comunicación inalámbrica con sus dispositivos y trató de ponerlo en práctica en su proyecto inconcluso de la Wardenclyffe Tower, un transmisor de potencia y comunicación inalámbrica intercontinental, pero le retiraron la financiación ecónomica que recibia ante la falta de progresos.[5]​
Las invenciones realmente realizadas por Tesla son dificiles de identificar pues frecuentemente se basan en comentarios propios, sin evidencia contrastables, como es la autoatribuida, obtención de las primeras imágenes de rayos X o en la asignación de inventos que no realizó[6]​, como es el caso de la radio.  
Después de 1910 se involucró en proyectos con escaso diverso éxito. Tras gastar la mayor parte de su dinero, vivió en varios hoteles de Nueva York, en los que dejó facturas sin abonar. Murió en esa ciudad en enero de 1943.[7]​ El trabajo de Tesla cayó en un relativo olvido después de su muerte, pero en 1960 la unidad de inducción electromagnética en el Sistema Internacional de Unidades fue nombrada tesla en su honor.[8]​ Desde la década de 1990 hay un claro resurgimiento del reconocimiento de sus aportaciones a la ingeniería.[9]​
Nikola Tesla, de etnia serbia,[10]​[11]​ nació en el pueblo de Smiljan (actualmente en Croacia), en el entonces Imperio austrohúngaro, y tiempo después se nacionalizaría estadounidense.[12]​
Tras su demostración de la comunicación inalámbrica por medio de ondas de radio en 1894[13]​ y después de su victoria en la guerra de las corrientes, se le reconoció ampliamente como uno de los más grandes ingenieros eléctricos de los Estados Unidos.[14]​ Durante este periodo la fama de Tesla rivalizaba con la de cualquier inventor o científico de la historia o la cultura popular,[15]​ pero debido a su personalidad excéntrica y a sus afirmaciones increíbles —a veces totalmente inverosímiles, y en ocasiones, falsas— acerca del posible desarrollo de innovaciones científicas y tecnológicas, Tesla terminó relegado al ostracismo y considerado un científico loco.[16]​ Nunca prestó especial atención a sus finanzas y se dice que murió empobrecido a los 86 años.[17]​
Además de su trabajo en electromagnetismo e ingeniería electromecánica, el trabajo de Tesla más tarde sirvió en diferente medida al desarrollo de la robótica, el control remoto, el radar, las ciencias de la computación, la balística, la física nuclear[18]​ y la física teórica. Llevó adelante estudios que permitirían desarrollar la radio, pero nunca desarrolló este concepto debido a que no entendía del todo la física inherente a este fenómeno. Posteriormente, cuando Guillermo Marconi reclamó los derechos de uso de la radio en plena Segunda Guerra Mundial, la Corte Suprema de los Estados Unidos rechazó la solicitud, incluyendo en su decisión la restauración de ciertas patentes previas a la de Marconi, entre ellas algunas de Tesla.[19]​
La unidad de medida del campo magnético (B) del Sistema Internacional de Unidades (también denominado densidad de flujo magnético o inducción magnética), el tesla (T), fue llamado así en su honor en la Conferencia General de Pesas y Medidas de París en 1960.[20]​
Su personalidad, su carácter excéntrico y la historia de su experimento sobre transmisión inalámbrica, son utilizados por aficionados a las teorías conspirativas para justificar varias pseudociencias, atribuyéndole inventos, hechos y/o investigaciones que no se corresponden con la realidad.[21]​
Nikola Tesla era hijo de padres serbios.[22]​ Nació en el pueblo de Smiljan, en el Imperio austrohúngaro, cerca de la ciudad de Gospić, perteneciente al territorio de la actual Croacia. Su certificado de bautismo afirma que nació el 28 de junio de 1856 del calendario juliano, correspondiente al 10 de julio del calendario gregoriano en uso actualmente. Su padre fue Milutin Tesla, sacerdote de la iglesia ortodoxa serbia en la jurisdicción de Sremski Karlovci, y su madre, Đuka Mandić, ama de casa de ascendencia serbia,[22]​ que dedicaba parte de su tiempo como científica autodidacta al desarrollo de pequeños aparatos caseros.[1]​[23]​
Se cree que su origen paterno proviene de alguno de los clanes serbios del valle del río Tara, o bien del noble herzegovino Pavle Orlović.[24]​ Su madre, Đuka, provenía de una familia ortodoxa domiciliada en Lika y Banija, pero con profundos orígenes en Kosovo.[25]​ Era competente fabricando herramientas artesanales caseras y había aprendido de memoria numerosos poemas épicos serbios, pero nunca aprendió a leer.[26]​
La familia se trasladó a Gospić en 1862. Tesla asistió al gymnasium de Karlovac, donde completó el plan de estudios de cuatro años en tres.[27]​
Más tarde comenzó los estudios de ingeniería eléctrica en la Universidad de Graz, en la ciudad del mismo nombre, en 1875. Mientras estuvo allí, estudió los usos de la corriente alterna. Algunas fuentes afirman que se licenció por la Universidad de Graz,[28]​[29]​[30]​ aunque la universidad afirma que no recibió ningún grado y que no continuó más allá del segundo semestre del tercer año, durante el cual dejó de asistir a las clases.[31]​[32]​[33]​[34]​
Respecto a su época en Graz, Tesla afirmaba que "trabajaba desde las 3 a.m. a las 11 p.m., incluso domingos y días festivos".[13]​ Después de la muerte de su padre en 1879, Tesla encontró un paquete de cartas de sus profesores a su padre, advirtiéndole de que, a menos que lo sacaran de la escuela, su hijo moriría por exceso de trabajo. Al final de su segundo año perdió su beca y se volvió adicto al juego.[13]​ Durante su tercer año perdió su asignación y el dinero de su matrícula, aunque más adelante se recuperó de sus pérdidas iniciales y devolvió el saldo a su familia. Afirmaba que "pudo dominar [su] pasión en ese momento", pero más tarde en los EE. UU. fue nuevamente conocido por jugar al billar.[35]​
En diciembre de 1878 abandonó Graz y dejó de relacionarse con sus familiares. Sus amigos pensaban que se había ahogado en el río Mura. Se dirigió a Maribor (hoy Eslovenia), donde obtuvo su primer empleo como ayudante de ingeniería, trabajo que desempeñó durante un año. Durante este periodo sufrió una crisis nerviosa. Posteriormente fue persuadido por su padre para continuar sus estudios en la Universidad Carolina en Praga, a la que asistió durante el verano de 1880. Allí fue influido por Ernst Mach. Sin embargo, después de que su padre falleciera dejó la Universidad, completando solamente un curso.[36]​
Tesla pasaba el tiempo leyendo muchas obras y memorizando libros completos, ya que supuestamente poseía una memoria fotográfica.[37]​ En su autobiografía relató que en ciertas ocasiones experimentó determinados momentos de inspiración. Durante su infancia sufrió varios episodios de una enfermedad muy peculiar, que le provocaba que cegadores haces de luz apareciesen ante sus ojos, a menudo acompañados de alucinaciones. Normalmente las visiones estaban asociadas a una palabra o idea que le rondaba la cabeza. Otras veces, estas le daban la solución a problemas que se le habían planteado. Simplemente con escuchar el nombre de un objeto era capaz de visualizarlo de forma muy realista. Actualmente la sinestesia presenta síntomas similares. Tesla podía visualizar una invención en su cerebro con precisión extrema, incluyendo todas las dimensiones, antes de iniciar la etapa de construcción; una técnica algunas veces conocida como pensamiento visual. No solía dibujar esquemas; en lugar de eso concebía todas las ideas solo con la mente. También en ocasiones tenía reminiscencias de hechos que le habían sucedido previamente en su vida, fenómeno este que se inició durante su infancia.[37]​
En 1880 se trasladó a Budapest para trabajar bajo las órdenes de Tivadar Puskás en una compañía de telégrafos,[38]​ la compañía nacional de teléfonos. Allí conoció a Nebojša Petrović, un joven inventor serbio que vivía en Austria. A pesar de que su encuentro fue breve, trabajaron juntos en un proyecto usando turbinas gemelas para generar energía continua. Para cuando se produjo la apertura de la central telefónica en 1881 en Budapest, Tesla se había convertido en el jefe de electricistas de la compañía, y fue más tarde ingeniero del primer sistema telefónico del país. También desarrolló un dispositivo que, de acuerdo con ciertas fuentes, era un repetidor telefónico o amplificador, pero que, según otros, pudo haber sido el primer altavoz.[39]​
En 1882 se trasladó a París, Francia, para trabajar como ingeniero en la Continental Edison Company (una de las compañías de Thomas Alva Edison), diseñando mejoras para el equipo eléctrico traído del otro lado del océano gracias a las ideas de Edison. Según su biografía, en el mismo año concibió el motor de inducción e inició el desarrollo de varios dispositivos que usaban el campo magnético rotativo, por los cuales recibió patentes en 1888.
Poco después, Tesla despertó de un sueño en el cual su madre había muerto, «y yo supe que eso había sucedido».[41]​ Tras esto cayó enfermo. Permaneció dos o tres semanas recuperándose en Gospić y en el pueblo de Tomingaj, cerca de Gračac, el lugar de nacimiento de su madre.
En junio de 1884 llegó por primera vez a los Estados Unidos, a la ciudad de Nueva York,[42]​ con poco más que una carta de recomendación de Charles Batchelor, un antiguo empleador. En la carta de recomendación a Thomas Edison, Batchelor escribió: «conozco a dos grandes hombres, usted es uno de ellos; el otro es este joven».[43]​ Edison contrató a Tesla para trabajar en su Edison Machine Works. Empezó a trabajar para Edison como un simple ingeniero eléctrico, resolviendo algunos de los problemas de la compañía.
La compañía de Edison había instalado varias dinamos en el SS Oregon, en aquel momento uno de los transatlánticos más rápidos y el primer barco en contar con electricidad a bordo,[44]​ empleada para la iluminación de la nave.[45]​ En 1884 las dinamos se dañaron, lo que retrasó la salida del buque de Nueva York.[46]​ Tesla se presentó voluntario para realizar la reparación, y estuvo trabajando toda la noche para lograr hacer funcionar de nuevo las dinamos,[46]​ gracias a lo cual recibió las felicitaciones de Edison a la mañana siguiente.[44]​
La carrera de Tesla progresó rápidamente. Se le ofreció incluso la tarea de rediseñar completamente los generadores de corriente continua de la compañía de Edison.[47]​ Tesla afirmaba que le ofrecieron 50.000 dólares (~ 1,1 millones en 2007, ajustado por la inflación)[48]​ por rediseñar los ineficientes motores y generadores de Edison, mejorando tanto su servicio como su economía.[37]​ En 1885, cuando Tesla preguntó acerca de su remuneración, Edison replicó: "Tesla, usted no entiende nuestro humor estadounidense", rompiendo así su palabra.[49]​[50]​ Con un sueldo de solo USD 18 a la semana, tendría que haber trabajado 53 años para reunir el dinero que le fue prometido; la oferta era igual al capital inicial de la compañía. Renunció a su empleo de inmediato cuando se le denegó aumentar su salario a USD 25 semanales.[51]​
Así pues, poco después, necesitado de trabajo, se encontró a sí mismo cavando zanjas para la compañía de Edison por un corto periodo de tiempo, que aprovechó para concentrarse en su sistema polifásico de corriente alterna.[37]​
En 1886, Tesla fundó su propia compañía, la Tesla Electric Light & Manufacturing. Los primeros inversores no estuvieron de acuerdo con sus planes para el desarrollo de un motor de corriente alterna y finalmente lo relevaron de su puesto en la compañía. Trabajó como obrero en Nueva York de 1886 a 1887 para mantenerse y reunir capital para su próximo proyecto. En 1887 construyó un motor de inducción sin escobillas, alimentado con corriente alterna,[52]​ que presentó en el American Institute of Electrical Engineers (Instituto Americano de Ingenieros Eléctricos), actualmente IEEE (Instituto de Ingenieros Eléctricos y Electrónicos) en 1888. Sin embargo, Galileo Ferraris había desarrollado el mismo diseño varios meses antes de manera independiente. En el mismo año desarrolló el principio de su bobina de Tesla, y comenzó a trabajar con George Westinghouse en la Westinghouse Electric & Manufacturing Company's en los laboratorios de Pittsburgh. Westinghouse escuchó sus ideas sobre sistemas polifásicos, que podrían permitir la trasmisión de corriente alterna a larga distancia.[53]​
Conflicto comercialLa demostración de Tesla de su motor de inducción y el posterior otorgamiento de la patente por parte de Westinghouse, ambos en 1888, se produjeron en un momento de competencia extrema entre las compañías eléctricas.[54]​[55]​ Las tres grandes empresas, Westinghouse, Edison y Thompson-Houston, intentaban crecer en una economía capitalista de negocios intensivos, mientras que financieramente se socavaban unas a otras. Incluso hubo una campaña de propaganda, denominada "guerra de las corrientes", con Edison Electric tratando de afirmar que su sistema de corriente continua era mejor y más seguro que el sistema de Westinghouse.[56]​[57]​ Competir en este mercado significaba que Westinghouse no dispondría inmediatamente de los recursos en efectivo o de ingeniería para desarrollar el motor de Tesla y el sistema polifásico relacionado.[58]​
Dos años después de firmar el contrato de Tesla, Westinghouse Electric estaba en problemas. El casi colapso de Baring Brothers en Londres desencadenó el pánico financiero de 1890, lo que provocó que los inversores solicitaran sus préstamos a Westinghouse.[59]​ La repentina escasez de efectivo obligó a la compañía a refinanciar sus deudas. Los nuevos prestamistas exigieron que Westinghouse recortara lo que parecía un gasto excesivo en la adquisición de otras compañías, investigación y patentes, incluidos los derechos acordados por el motor de Tesla.[60]​[61]​ En ese punto, el motor de inducción de Tesla no había tenido éxito y estaba estancado su desarrollo.[59]​[58]​ Westinghouse estaba pagando un canon[62]​ garantizado de 15 000 dólares por año, aunque los ejemplos operativos del motor todavía no eran habituales, al igual que los sistemas de alimentación polifásicos necesarios para alimentarlos.[59]​[63]​
A principios de 1891, George Westinghouse explicó sus dificultades financieras a Tesla en términos contundentes, diciéndole que si no cumplía con las demandas de sus prestamistas ya no tendría el control de Westinghouse Electric y Tesla tendría que "tratar con los banqueros" para intentar cobrar sus futuros derechos.[64]​ Las ventajas de que Westinghouse continuara defendiendo su motor probablemente parecieron obvias a Tesla y aceptó liberar a la empresa de la cláusula de pago del canon del contrato.[64]​[65]​ Seis años después, Westinghouse compraría la patente de Tesla por un pago de 216.000 dólares como parte de un acuerdo de intercambio de patentes firmado con General Electric (una compañía creada a partir de la fusión de Edison y Thompson-Houston en 1892).[66]​[67]​[68]​
El dinero que Tesla obtuvo de la licencia de sus patentes de corriente alterna lo hizo económicamente independiente y le proporcionó el tiempo y los fondos necesarios para perseguir sus propios intereses.[69]​ En 1889, Tesla se mudó de la tienda de Liberty Street que Peck y Brown habían alquilado y durante los siguientes doce años trabajó en una serie de talleres/laboratorios en Manhattan, como un laboratorio en el 175 de Grand Street (1889-1892), el cuarto piso del 33-35 South de la Quinta Avenida (1892-1895), y el sexto y séptimo pisos del 46-48 East de Houston Street (1895-1902).[70]​[71]​ Tesla y su personal contratado realizarían parte de su trabajo más importante en estos talleres.
Ciudadano estadounidense
El 30 de julio de 1891, Tesla se convirtió en ciudadano de los Estados Unidos a la edad de 35 años. Instaló su laboratorio en la Quinta Avenida con 35 Sur, en la ciudad de Nueva York, en ese mismo año. Posteriormente lo trasladó a la calle Houston con 46 Este. En este lugar, mientras realizaba experimentos sobre resonancia mecánica con osciladores electromecánicos, generó resonancia en algunos edificios vecinos y, aunque debido a las frecuencias utilizadas no afectó al suyo, sí generó quejas ante la policía: como la velocidad del resonador creció, y siendo consciente del peligro, se vio obligado a terminar el experimento utilizando un martillo, justo en el momento en que llegaron los agentes.[72]​
También hizo funcionar lámparas eléctricas en dos lugares de Nueva York, proporcionando evidencia para el potencial de la trasmisión inalámbrica de energía.[73]​
Algunas de sus amistades más cercanas eran artistas. Se hizo amigo de Robert Underwood Johnson, editor del Century Magazine, quien adaptó algunos poemas serbios de Jovan Jovanović Zmaj (que Tesla tradujo). También en esta época, Tesla fue influido por la filosofía védica (es decir, la doctrina hinduista) según los preceptos de Swami Vivekananda; en tal medida que después de su exposición a estas enseñanzas, empezó a usar palabras en sánscrito para nombrar algunos de sus conceptos fundamentales referentes a la materia y la energía.[74]​
A los 36 años le fueron otorgadas las primeras patentes relacionadas con la alimentación polifásica y continuó con sus investigaciones sobre los principios del campo magnético rotativo. De 1892 a 1894 sirvió como vicepresidente del Instituto Americano de Ingenieros Eléctricos (American Institute of Electrical Engineers), el precursor, junto con el Institute of Radio Engineers, del actual IEEE. De 1893 a 1895 investigó la corriente alterna de alta frecuencia, generando una corriente alterna de un millón de voltios usando una bobina de Tesla cónica e investigó el efecto pelicular en conductores,[75]​ diseñó circuitos LC,[76]​ inventó una máquina para inducir el sueño,[77]​ lámparas de descarga inalámbricas,[78]​ y transmisión de energía electromagnética, construyendo el primer radiotransmisor.[79]​ En San Luis, Misuri, hizo una demostración sobre radiocomunicación en 1893.
En la Exposición Mundial Colombina de Chicago de 1893, hubo por primera vez un edificio dedicado a exposiciones eléctricas. En este evento Tesla y George Westinghouse presentaron a los visitantes la alimentación mediante corriente alterna que fue usada para iluminar la exposición. Además se exhibieron las lámparas fluorescentes y bombillas de Tesla de un solo nodo.[80]​
También explicó los principios del campo magnético rotativo y del motor de inducción, demostrando cómo mantener verticalmente un huevo de cobre mediante su dispositivo conocido como "Huevo de Colón de Tesla".[81]​
Desarrolló el llamado generador de Tesla en 1895, en conjunto con sus inventos sobre la licuefacción del aire.[82]​ Sabía, por los descubrimientos de Kelvin, que el aire en estado de licuefacción absorbía más calor del requerido teóricamente, cuando retornaba a su estado gaseoso y era usado para mover algún dispositivo.[83]​ Justo antes de finalizar su trabajo y patentar cualquier aplicación, se produjo un incendio en su laboratorio, que destruyó todo su equipo, modelos e invenciones. Poco después, Carl von Linde, en Alemania, presentó una patente de la aplicación de este mismo proceso.[84]​
En el verano de 1889, Tesla viajó a la Exposición Universal en París, donde se enteró de los experimentos de Heinrich Rudolf Hertz (realizados en 1886-1888) que demostraron la existencia de radiación electromagnética, incluidas las ondas de radio.[85]​ Encontró este nuevo descubrimiento "refrescante" y decidió explorarlo más a fondo. Al repetir, y luego expandir, estos experimentos, intentó alimentar una bobina de Ruhmkorff con un alternador de alta velocidad, que había estado desarrollando como parte de un sistema de lámpara de arco mejorado, pero descubrió que la corriente de alta frecuencia sobrecalentaba el núcleo de hierro y fundía el aislamiento entre los devanados principales y secundarios en la bobina. Para solucionar este problema, se le ocurrió su bobina de Tesla con un espacio de aire en lugar de material aislante entre los devanados primarios y secundarios, y un núcleo de hierro que se podía mover a diferentes posiciones dentro o fuera de la bobina.[86]​
Después de 1890, Tesla experimentó con la transmisión de potencia mediante acoplamiento inductivo y capacitivo, utilizando altos voltajes de corriente alterna generados con su bobina Tesla.[87]​ Intentó desarrollar un sistema de iluminación inalámbrico basado en acoplamiento inductivo y capacitivo, y realizó una serie de demostraciones públicas donde encendió tubos de Geissler e incluso bombillas incandescentes en un escenario.[88]​ Pasó la mayor parte de la década trabajando en variaciones de esta nueva forma de iluminación con la ayuda de varios inversores, pero ninguna de las empresas logró sacar un producto comercial de sus hallazgos.[89]​
En 1893, en St. Louis, Misuri, ante el Instituto Franklin de Filadelfia (Pensilvania) y ante la National Electric Light Association, Tesla dijo a los espectadores que estaba seguro de que un sistema como el suyo podría finalmente conducir "señales inteligibles o incluso energía eléctrica a cualquier distancia sin el uso de cables" al conducirlas a través de la Tierra.[90]​[91]​ Pensaba que solo era cuestión de tiempo que el hombre pudiese adaptar las máquinas al engranaje de la naturaleza, declarando: «Antes de que pasen muchas generaciones, nuestras máquinas serán impulsadas por energía obtenida en cualquier punto del universo».[92]​
Tratando de encontrar una forma mejor de generar corriente alterna, Tesla desarrolló un alternador accionado con vapor. Lo patentó en 1893 y lo presentó en la Exposición Mundial Colombina de Chicago de ese año. El vapor era forzado hacia el oscilador y se precipitaría a través de una serie de válvulas, empujando un pistón unido a una armadura hacia arriba y hacia abajo. La armadura magnética vibraba hacia arriba y hacia abajo a alta velocidad, produciendo un campo magnético alterno. Este campo inducía a su vez una corriente eléctrica alterna en las bobinas de alambre adyacentes. Eliminó las partes complicadas de una máquina/generadora de vapor, pero nunca se contempló como una solución de ingeniería factible para generar electricidad.[93]​[94]​
A principios de 1893, el ingeniero Benjamin G. Lamme de Westinghouse había progresado mucho desarrollando una versión eficiente del motor de inducción de Tesla, y Westinghouse Electric comenzó a calificar su sistema bifasico completo como el "Sistema Tesla Polifase". Creían que las patentes de Tesla les daban la prioridad sobre otros sistemas de corriente alterna.[95]​
Westinghouse Electric le pidió a Tesla que participara en el Exposición Mundial Colombina de Chicago de 1893, donde la compañía tenía un gran espacio en un edificio dedicado a exhibiciones eléctricas. Westinghouse Electric ganó la licitación para iluminar la Exposición con corriente alterna  y fue un evento clave en la historia de esta forma de electricidad, ya que la compañía demostró al público estadounidense la seguridad, fiabilidad y eficiencia de un sistema de corriente alterna completamente integrado.[96]​[97]​[98]​ Tesla mostró una serie de efectos eléctricos relacionados con la corriente alterna, así como su sistema de iluminación inalámbrico, utilizando una demostración que había realizado anteriormente en toda América y Europa; [99]​ incluyendo el uso de alto voltaje, y una corriente alterna de alta frecuencia para encender un lámpara de descarga inalámbricamente.[100]​
Un observador anotó:
Tesla también explicó los principios del campo magnético rotativo en un motor de inducción al demostrar cómo hacer que un huevo de cobre se coloque de punta, usando un dispositivo que él construyó conocido como el Huevo de Colón,[102]​ e introdujo su nuevo generador de corriente alterna con un oscilador alimentado a vapor.
En 1893, Edward Dean Adams, que encabezaba la compañía que explotaría el salto hidroeléctrico adyacente a las Cataratas del Niágara, solicitó la opinión de Tesla sobre qué sistema sería mejor para transmitir la energía generada en las cataratas. Durante varios años, hubo una serie de propuestas y concursos abiertos sobre la mejor manera de usar la energía generada por las cataratas. Entre los sistemas propuestos por varias empresas de EE. UU. y Europa se encontraban las corrientes alternas bifásicas y trifásicas, las corrientes continuas de alta tensión y sistemas de aire comprimido. Adams se dirigió a Tesla para obtener información sobre el estado actual de todos los sistemas de la competencia, y Tesla le aconsejó que un sistema de dos fases sería el más fiable, y que había un sistema de Westinghouse para encender bombillas incandescentes usando corriente alterna de dos fases. La compañía adjudicó un contrato a Westinghouse Electric para construir un sistema de generación de corriente alterna de dos fases en las Cataratas del Niágara, basado en el asesoramiento de Tesla y la demostración de Westinghouse en la Exposición Colombina de que podrían construir un sistema de corriente alterna completo. Al mismo tiempo, se otorgó un contrato adicional a General Electric para construir el sistema de distribución de la corriente generada.[103]​
Algunas personas creen falsamente que en las cataratas del Niágara se construyó la primera central hidroeléctrica gracias a los desarrollos de Tesla en 1893, consiguiendo en 1896 transmitir electricidad a la ciudad de Búfalo (Nueva York). Las primeras centrales hidroeléctricas se desarrollaron primero en Europa en 1878-1885. Tras 1885 Westinghouse contrató, entre otros, a William Stanley, Oliver B. Shallenberger y Benjamin Lamme, para construir sistemas de potencia de corriente alterna en todo EE. UU. Tesla no se unió a Westinghouse hasta 1888.[104]​
En 1895, Edward Dean Adams, impresionado con lo que vio cuando recorrió el laboratorio de Tesla, aceptó ayudar a fundar la empresa Nikola Tesla, creada para financiar, desarrollar y comercializar una variedad de patentes e invenciones anteriores de Tesla, así como otras nuevas. Alfred Brown firmó, trayendo patentes desarrolladas bajo Peck and Brown. El consejo de la empresa se completó con William Birch Rankine y Charles F. Coaney.[105]​ Encontraron pocos inversores, dado que a mediados de la década de 1890 se vivía un momento difícil desde el punto de vista financiero, y las patentes inalámbricas de iluminación y osciladores que se establecieron en el mercado nunca se materializaron. La compañía manejaría las patentes de Tesla en las décadas siguientes.
En la madrugada del 13 de marzo de 1895, el edificio de la Quinta Avenida Sur que albergaba el laboratorio de Tesla se incendió. El fuego comenzó en el sótano del inmueble y fue tan intenso que el laboratorio de Tesla situado en el cuarto piso se quemó y colapsó en el segundo piso. El incendio no solo retrasó los proyectos en curso de Tesla, sino que también destruyó una colección de notas tempranas y material de investigación, modelos y piezas de demostración, incluidas muchas que habían sido expuestas en la Exposición Colombina de 1893. Tesla dijo al The New York Times: "Estoy demasiado apenado para hablar. ¿Qué puedo decir?"[106]​ Después del incendio, Tesla se mudó al 46-48 de East Houston Street y reconstruyó su laboratorio en los pisos 6 y 7.
En 1894, Tesla empezó a investigar los que después se llamaron rayos X. En el incendio de su laboratorio en 1895 se perdió todo su trabajo, según afirmó el propio Tesla. Mientras tanto, en noviembre de ese mismo año, el científico alemán Wilhelm Röntgen concluía su extensa y sistemática investigación de los rayos X, publicando sus conclusiones en 1895. La primera publicación de Tesla sobre los "rayos de Rontgen" data de 1895.[109]​
Según el propio Tesla narra, usó su propio tubo de vacío (similar a su patente Patente USPTO n.º 514170: «#514,170»). Este dispositivo difería de otros tubos de rayos X por el hecho de no tener electrodo receptor. El término moderno para el fenómeno producido por este artefacto es Bremsstrahlung (o radiación de frenado).
En sus primeras investigaciones Tesla diseñó algunos experimentos para producir rayos X. Afirmó que con estos circuitos, «el instrumento podrá generar rayos de Roentgen de mayor potencia que la obtenida con aparatos ordinarios».[110]​
También mencionó los peligros de trabajar con sus circuitos y con los rayos X producidos por sus dispositivos de un solo nodo. De muchas de sus notas en las investigaciones preliminares de este fenómeno, se deduce que atribuyó el daño de la piel a varias causas. Inicialmente creyó que el daño no podría ser causado por los rayos de Roentgen, sino por el ozono generado al contacto con la piel y en parte también al ácido nitroso. Pensaba que se trataba de ondas longitudinales, como las producidas por las ondas en plasmas.[111]​[112]​
En 1898, Tesla mostró en público un barco que controlaba usando un radiocontrol basado en un cohesor -que denominó "telautomaton"- durante una exposición eléctrica en el Madison Square Garden.[113]​ La multitud que presenció la demostración hizo afirmaciones escandalosas sobre el funcionamiento del barco, tales como magia, telepatía o que estaba siendo pilotado por un mono entrenado oculto en su interior.[114]​ Tesla intentó vender su idea al ejército de los EE. UU. como un tipo de torpedo controlado por radio, pero la marina mostró poco interés.[115]​ El radiocontrol remoto siguió siendo una novedad hasta la Segunda Guerra Mundial, cuando varios países lo usaron en sus programas militares.[116]​ Tesla aprovechó la oportunidad para demostrar aún más la "Teleautomática" en una conferencia pronunciada en una reunión del Club Comercial de Chicago, mientras viajaba a Colorado Springs, el 13 de mayo de 1899.[106]​
Desde la década de 1890 hasta 1906, Tesla invirtió gran parte de su tiempo y fortuna en una serie de proyectos para desarrollar la transmisión inalámbrica de energía. Fue una expansión de su idea de usar bobinas para transmitir la potencia que había estado demostrando en la iluminación inalámbrica. Vio este procedimiento no solo como una forma de transmitir grandes cantidades de energía en toda la Tierra, sino también, como había señalado en sus conferencias anteriores, una forma de transmitir comunicaciones en todo el mundo.
En el momento en que Tesla estaba formulando sus ideas, no había una forma factible de transmitir de forma inalámbrica señales de comunicación a largas distancias, y mucho menos grandes cantidades de energía. Había estudiado las ondas de radio desde el principio y llegó a la conclusión de que parte del estudio existente sobre ellas, realizado por Hertz, era incorrecto.[117]​[118]​[119]​ Además, esta nueva forma de radiación era ampliamente considerada en ese momento como un fenómeno de corta distancia que parecía extinguirse en menos de una milla.[120]​ Tesla notó que, incluso si las teorías sobre ondas de radio eran verdaderas, no tenían ningún valor para los propósitos previstos, ya que esta forma de "luz invisible" disminuiría a distancia como cualquier otra radiación y la haría viajar en línea recta hacia el espacio, "perdiéndose irremediablemente".[121]​
A mediados de la década de 1890, Tesla estaba trabajando en la idea de que podría conducir electricidad a larga distancia a través de la Tierra o la atmósfera, y comenzó a desarrollar experimentos para probar esta idea, incluyendo la instalación de un gran transformador de resonancia basado en una bobina de Tesla ubicado en su laboratorio de East Houston Street.[122]​[123]​[124]​ Parece que tomó prestada la idea común en aquel momento de que la atmósfera de la Tierra era conductiva,[125]​[126]​ y propuso un sistema compuesto por globos suspendidos, transmisores y receptores, electrodos en el aire por encima de 9.000 m de altitud, donde pensó que la presión más baja le permitiría enviar altos voltajes (millones de voltios) a largas distancias.
Su «sistema mundial para la transmisión de energía eléctrica sin cables» basado en la conductividad eléctrica de la Tierra, funcionaría mediante la transmisión de energía por varios medios naturales y el uso subsiguiente de la corriente trasmitida entre los dos puntos para alimentar dispositivos eléctricos.[127]​
Tesla afirmó haber demostrado la transmisión inalámbrica de energía a principios de 1891. Sin embargo nunca pudo llevarla a la práctica de una forma eficiente.[128]​
En 1899, Tesla se traslada a un laboratorio en Colorado Springs, Estados Unidos, para iniciar sus experimentos con alta tensión y mediciones de campo eléctrico. Los objetivos trazados por Tesla en este laboratorio eran: desarrollar un transmisor de gran potencia, perfeccionar los medios para individualizar y aislar la potencia transmitida y determinar las leyes de propagación de las corrientes sobre la Tierra y su atmósfera.[129]​ Durante los ocho meses que estuvo en Colorado Springs, Tesla escribió notas con una detallada descripción de sus investigaciones día a día. Allí dedicó la mitad de su tiempo a medir y probar su enorme bobina Tesla y otro tanto a desarrollar receptores de pequeñas señales y a medir la capacidad de una antena vertical. También realizó observaciones sobre bolas de fuego, que afirmaba haber producido. Un día, notó un comportamiento inusual de un instrumento que registraba tormentas, un cohesor rotativo. Se trataba de un instrumento que realizaba registros cuando una tormenta se aproximaba y se alejaba de su laboratorio. Concluyó que se trataba de la existencia de ondas estacionarias, que podían ser creadas por su oscilador. Con equipos sensibles pudo realizar mediciones de rayos que caían a gran distancia de su laboratorio, observando que las ondas de las descargas crecían hasta un pico y luego decrecían antes de repetir el ciclo total. Tesla sugirió que esto se debía al hecho de que la Tierra y la atmósfera poseían electricidad, lo que hacía que el planeta se comportara como un conductor de dimensiones ilimitadas, en el que era posible hacer transmisión de mensajes telegráficos sin hilos, y todavía más; transmitir potencia eléctrica a cualquier distancia terrestre, casi sin pérdidas, por medio de sus conocimientos de resonancia. Tesla había descubierto que podía producir un anillo alrededor de la Tierra como una campana, con descargas cada dos horas, y también que podía hacerlo resonar eléctricamente. Encontró que la resonancia del planeta era del orden de los 10 Hz, un valor realmente exacto para su época, ya que hoy en día se sabe que es de 8 Hz. Después de que descubriera cómo crear ondas eléctricas permanentes para transmitir potencia eléctrica alrededor del mundo, el científico alemán W. O. Schumann postuló que la Tierra conductiva y la ionosfera forman una guía de onda esférica, a través de la cual se pueden propagar ondas electromagnéticas de muy baja frecuencia (conocidas como ELF por sus siglas en inglés), generadas por la actividad de los rayos a escala mundial, con valores cercanos a los 8 Hz, fenómeno que se conoce como la resonancia Schumann. Tesla realizó trabajos mucho más avanzados que los otros pioneros de la transmisión sin hilos, Hertz y Marconi, quienes usaron altas frecuencias que no resonaban con la Tierra, a diferencia de las ondas de radio de altas longitudes de onda empleadas por Tesla, que tenían la ventaja de ser recibidas en sitios remotos de la Tierra, o en las profundidades del mar, para mantener la comunicación entre naves de superficie y submarinos.[130]​
En su laboratorio de Colorado Springs, observó señales inusuales que más tarde creyó podrían ser evidencia de comunicaciones de radio extraterrestre provenientes de Venus o Marte.[131]​ Notó que eran señales repetitivas, pero con una naturaleza distinta a las observadas en tormentas y ruido terrestre. Tesla mencionó que sus invenciones podrían ser usadas para hablar con otros planetas. Y afirmó que inventó el "Teslascopio" para ese propósito. Actualmente se debate sobre el tipo de señales que Tesla pudo recibir, que podrían ser resultado de la radiación natural extraterrestre,[132]​ aunque de todas formas, ha quedado para la historia de la ciencia como el precursor de la radioastronomía.
El 7 de enero de 1900, Tesla dejó Colorado Springs, no sin antes trasladarse durante ciertos períodos de tiempo a la localidad cercana de Cripple Creek, en donde realizaba experimentos colocando bombillas sobre el terreno y, comentaban asombrados sus vecinos, estas se encendían solas. El laboratorio fue demolido y su contenido vendido para pagar las deudas. El conjunto de los experimentos allí preparados por Tesla para el establecimiento de la transmisión de telecomunicaciones inalámbricas trasatlánticas fue conocido como Wardenclyffe.
Se dice que Nikola Tesla no hacía planos, sino que lo memorizaba todo.[37]​
[133]​ .
Algunos de sus estudios nadie podía descifrarlos debido a su enorme capacidad inductiva. Para la mayoría de sus proyectos ideaba los documentos de cabeza, le bastaba con tener la imagen de dicho objeto sin saber cómo funcionaba, simplemente lo elaboraba sin saber que podía suponer un gran avance para la humanidad.[37]​ Fue un lector minucioso de la teoría física de Ruđer Bošković.
En fechas reciente se difundio[134]​ una sentencia del Tribunal Supremo de EE UU que supuestamente reconocia la radio era invención de Tesla y no Marconi.  En la sentencia no es eso lo dice[6]​, de hecho no trata sobre la invención de la radio y el propio Tesla no creia en que la radio tuviese gran alcance.
Se especula que ideó un sistema de transmisión de electricidad inalámbrico, de tal suerte que la energía podría ser llevada de un lugar a otro mediante ondas de naturaleza no hertzianas.[135]​ Dicho sistema se basaría en la capacidad de la ionosfera para conducir electricidad, la potencia se transmitiría a una frecuencia de 6 Hz con una enorme torre llamada Wardenclyffe Tower, para valerse de la resonancia Schumann como medio de transporte.
Hoy en día se sabe que esta frecuencia es de 7,83 Hz y no de 6 , aunque realmente varía desde 7,83 Hz a 12 Hz, según la actividad solar y el estado de la ionosfera. Si bien se ha creído que el fracaso del proyecto se dio por problemas financieros, otras versiones aseguran que en realidad las ideas de Tesla para la torre (planeaba utilizar la Tierra como conductora de la electricidad a todo el globo) no funcionaron y que volvió a pedirle dinero a Morgan, pero este, decepcionado ante los resultados, se negó. Algunos expertos de la actualidad han intentado estudiar cómo se suponía que funcionara la Torre Wardenclyffe, pero generalmente terminan con más preguntas que respuestas.[cita requerida] No está del todo claro qué método pretendía utilizar Tesla para transmitir electricidad y muchos creen que tal vez ni siquiera él mismo lo tenía definido.
Tesla recorrió Nueva York tratando de encontrar inversores para lo que pensó que sería un sistema viable de transmisión inalámbrica, invitándolos a cenar en el Palm Garden del Waldorf Astoria (el hotel donde vivía en ese momento), The Players Club y el restaurante Delmonico.[136]​ En marzo de 1901, obtuvo 150.000 dólares (equivalentes a unos 4,4 millones de dólares de 2018) de J. P. Morgan a cambio de una participación del 51% sobre cualquier patente inalámbrica generada, y comenzó a planear la instalación de la Torre Wardenclyffe para ser construida en Shoreham (Nueva York), unos 160 km al este de la ciudad, en la costa norte de Long Island.[137]​
En julio de 1901, había expandido sus planes para construir un transmisor más potente para pasar por delante del sistema de radio de Marconi, que Tesla pensó que era una copia de su propio sistema.[138]​ Se acercó a Morgan para pedirle más dinero y poder construir el sistema más grande, pero Morgan se negó a proporcionar más fondos.[139]​
En diciembre de 1901, Marconi transmitió con éxito la letra S de Inglaterra a Newfoundland, derrotando a Tesla en la carrera por ser el primero en completar dicha transmisión. Un mes después del éxito de Marconi, Tesla intentó hacer que Morgan respaldara un plan aún más grande para transmitir mensajes y energía mediante el control de "vibraciones en todo el mundo".[138]​ Durante los cinco años siguientes, Tesla escribió más de 50 cartas a Morgan, implorando y exigiendo fondos adicionales para completar la construcción de Wardenclyffe. Continuó el proyecto durante otros nueve meses en 1902. La torre se erigió en su totalidad, hasta alcanzar los 57 m de altura.[140]​ En junio de 1902, trasladó sus operaciones desde el laboratorio de Houston Street a Wardenclyffe.[137]​
Los inversores de Wall Street estaban poniendo su dinero en el sistema de Marconi, y algunos medios de prensa comenzaron a volverse contra el proyecto de Tesla, alegando que era un engaño.[141]​ El proyecto se detuvo en 1905, y en 1906, los problemas financieros y otros eventos pudieron llevar a Tesla a sufrir un ataque de nervios.[142]​ Tuvo que hipotecar la propiedad Wardenclyffe para cubrir sus deudas en el Waldorf Astoria, que finalmente ascendieron a 20.000 dólares (aproximadamente medio millón de dólares de 2018).[143]​ Perdió la propiedad por ejecución hipotecaria en 1915, y en 1917, la Torre fue demolida por el nuevo propietario para hacer del suelo un activo inmobiliario más viable.
Después de que Wardenclyffe cerró, Tesla continuó escribiendo a Morgan. Tras la muerte de "el gran hombre", escribió al hijo de Morgan, Jack, tratando de obtener más fondos para el proyecto. En 1906, abrió oficinas en el 165 de Broadway en Manhattan, tratando de recaudar más fondos desarrollando y comercializando sus patentes. Más adelante pasó a tener oficinas en la Metropolitan Life Tower de 1910 a 1914; a estar alquilado durante unos meses en el Woolworth Building (de donde tuvo que mudarse porque no podía pagar el alquiler); y finalmente a las oficinas de 8 West 40th Street, donde permaneció de 1915 a 1925. Cuando se mudó a esta última oficina, estaba en bancarrota: la mayoría de sus patentes se habían agotado y estaba teniendo problemas con los nuevos inventos que trataba de desarrollar.[144]​
En su 50 cumpleaños, en 1906, Tesla demostró una turbina sin álabes de 200 CV de potencia y capaz de girar a 16.000 rpm. Entre 1910 y 1911 se probaron varios de sus motores de turbina sin paletas (de entre 100 y 5000 CV) en la Central eléctrica de Waterside de Nueva York.[145]​ Colaboró en este desarrollo con varias compañías, incluyendo el período 1919-1922 trabajando con la Allis-Chalmers en Milwaukee.[146]​[147]​ Pasó la mayor parte de su tiempo tratando de perfeccionar la turbina Tesla con Hans Dahlstrand, el ingeniero jefe de la empresa, pero las dificultades de ingeniería hicieron que nunca se convirtiera en un dispositivo práctico.[148]​ Tesla licenció la idea a una empresa de instrumentos de precisión y encontró uso en forma de exactos velocímetros y de otros instrumentos.[149]​
Cuando estalló la Primera Guerra Mundial, los británicos interceptaron el cable telegráfico transatlántico que unía los EE. UU. con Alemania para controlar el flujo de información entre los dos países. También trataron de cortar la comunicación inalámbrica alemana desde y hacia los Estados Unidos, haciendo que la empresa estadounidense Marconi demandase a la compañía de radio alemana Telefunken por infracción de patente.[150]​ Telefunken convocó a los físicos Jonathan Zenneck y Carl Ferdinand Braun para su defensa, y contrató a Tesla como testigo durante dos años por 1000 dólares al mes. El caso se estancó y luego quedó sin efecto cuando los EE. UU. entraron en guerra contra Alemania en 1917.[150]​[151]​
En 1915, Tesla intentó demandar a la Compañía Marconi por infringir sus patentes de sintonización inalámbrica. La patente de radio inicial de Marconi había sido adjudicada en los EE. UU. en 1897, pero su solicitud de patente de 1900, que cubría mejoras a la transmisión de radio, había sido rechazada varias veces antes de su aprobación final en 1904, con base en que infringía otras patentes existentes, incluidas dos patentes de ajuste inalámbrico de Tesla de 1897.[152]​[118]​[153]​ El caso de 1915 de Tesla quedó en nada,[154]​ pero en un caso relacionado, donde la compañía Marconi intentó demandar al gobierno de EE. UU. por infracciones de patentes durante la Primera Guerra Mundial, una decisión de la Corte Suprema de los Estados Unidos de 1943 restauró las patentes anteriores de Oliver Joseph Lodge, John Stone y Tesla.[155]​ El tribunal declaró que su decisión no tenía relación con la reclamación de Marconi como el primero en lograr la transmisión de radio, solo que dado que la demanda de Marconi sobre ciertas mejoras patentadas era cuestionable, la compañía no podía reclamar una infracción sobre esas mismas patentes.[156]​[118]​
El 6 de noviembre de 1915, un informe de la agencia de noticias Reuters de Londres le otorgó el Premio Nobel de Física de 1915 a Thomas Edison y a Nikola Tesla; sin embargo, el 15 de noviembre, un despacho de Reuters desde Estocolmo declaró que el premio de ese año estaba siendo otorgado a Sir William Henry Bragg y a su hijo William Lawrence Bragg "por sus servicios en el análisis de la estructura cristalina por medio de rayos X".[157]​[158]​[159]​ Hubo rumores infundados en aquel momento de que tanto Tesla como Edison habían rechazado el premio. [157]​ La Fundación Nobel señaló que: "Cualquier rumor de que una persona no ha recibido un Premio Nobel porque ha hecho conocer su intención de rechazar la recompensa es ridículo"; un destinatario podría rechazar un Premio Nobel solo después de que se le anuncie como ganador.[157]​
Ha habido afirmaciones posteriores de los biógrafos de Tesla de que Edison y Tesla fueron los destinatarios originales y que ninguno recibió el premio debido a su animosidad entre ellos; que cada uno trató de minimizar los logros y el derecho a ganar el premio del otro; que ambos rechazaron aceptar el premio si el otro lo recibía primero; que ambos rechazaron cualquier posibilidad de compartirlo; e incluso que un acaudalado Edison se negó a que Tesla recibiera el premio en metálico de 20.000 dólares.[157]​[160]​
En los años posteriores a estos rumores, ni Tesla ni Edison ganaron el premio (aunque Edison recibió una de las 38 posibles candidaturas en 1915 y Tesla recibió una de las 38 candidaturas posibles en 1937).[161]​
El 7 de enero de 1943, a la edad de 86 años, Tesla murió solo en la habitación 3327 del Wyndham New Yorker Hotel. Su cuerpo fue encontrado por una doncella que entró en la habitación de Tesla, ignorando el cartel de "no molestar" que el propio Tesla había colocado en su puerta dos días antes. Un médico forense examinó el cuerpo y dictaminó que la causa de la muerte había sido una trombosis coronaria.[106]​
Dos días después, el FBI ordenó que la Custodia de Propiedades Extranjeras se apropiara de las pertenencias del difunto,[106]​ aunque Tesla era ciudadano estadounidense.[106]​ John G. Trump, profesor del M.I.T. y un conocido ingeniero eléctrico que prestaba servicios como ayudante técnico del National Defense Research Committee, fue llamado para analizar los artículos de Tesla, que estaban en custodia.[106]​ Después de una investigación de tres días, el informe de Trump concluyó que no había nada que pudiera constituir un riesgo en manos hostiles, concluyendo que:
En una caja que supuestamente contenía una parte del "rayo de la muerte" de Tesla, Trump encontró una caja de décadas (una sencilla caja conmutadora de resistencias eléctricas)[172]​ de 45 años de antigüedad.
El 10 de enero de 1943, el alcalde de la ciudad de Nueva York, Fiorello La Guardia, leyó en directo un panegírico escrito por el autor esloveno Louis Adamic en la emisora de radio WNYC, mientras que las piezas de violín "Ave María" y la canción popular serbia "Tamo daleko" se escuchaban en segundo plano.[106]​ El 12 de enero, dos mil personas asistieron a un funeral de estado en la Catedral de San Juan el Divino. Después del funeral, el cuerpo de Tesla fue llevado al Cementerio Ferncliff en Ardsley, Nueva York, donde fue incinerado. Al día siguiente, un segundo servicio fue conducido por destacados sacerdotes en la Capilla de la Trinidad (hoy Catedral serbia ortodoxa de San Java) en la ciudad de Nueva York.[106]​
En 1952, la insistencia del sobrino de Tesla, Sava Kosanović, consiguió que todas la propiedades de Tesla fueran enviadas a Belgrado en 80 baúles marcados con las letras NT.[106]​ En 1957, la secretaria de Kosanović, Charlotte Muzar, transportó las cenizas de Tesla de Estados Unidos a Belgrado.[106]​ Las cenizas se muestran en una esfera dorada sobre un pedestal de mármol en el Museo Nikola Tesla.[173]​
Desde 1900 vivía en el Hotel Waldorf Astoria de Nueva York, acumulando una gran factura.[174]​ En 1922, se trasladó al Hotel St. Regis, y seguiría un patrón a partir de entonces de mudarse a un nuevo hotel cada pocos años, dejando cuentas impagadas.[175]​[176]​
Paseaba hasta un parque todos los días para alimentar a las palomas. Se dedicó a alimentarlas en la ventana de la habitación de su hotel, atrayendo a los pichones heridos para curarlos.[176]​[177]​[178]​  Afirmaba que era visitado diariamente por una paloma blanca que había recogido cuando estaba herida. Se gastó más de 2000 dólares, incluyendo la construcción de un dispositivo que sostuvo al animal cómodamente para que sus huesos pudieran sanar, curando sus alas y patas rotas.[179]​ Tesla declaró:
Las facturas impagadas de Tesla y las quejas sobre el desastre de su alimentación a las palomas lo obligaron a abandonar el St. Regis en 1923, el Hotel Pennsylvania en 1930 y el Hotel Governor Clinton en 1934.[176]​ En un momento dado, también ocupó habitaciones en el edificio JPMorgan Chase Tower de Nueva York.[181]​
En 1934, se mudó al Hotel Wyndham New Yorker, y la Westinghouse Electric & Manufacturing Company comenzó a pagarle 125 dólares por mes, además de abonar su alquiler, gastos que la compañía pagaría durante el resto de la vida de Tesla. El relato de cómo se llegó a esta situación varían. Distintas fuentes dicen que Westinghouse estaba preocupado (o había sido advertido) por la posible mala publicidad en torno a las condiciones empobrecidas bajo las cuales vivía su ex inventor estrella.[182]​[183]​[184]​[185]​ El pago ha sido descrito como una "tarifa de consultoría" para evitar la aversión de Tesla a aceptar caridad, o como algún tipo de compensación no especificada.[184]​
En 1931, Kenneth Swezey, un joven escritor que había estado asociado con Tesla por un tiempo, organizó una celebración para el 75° cumpleaños del inventor. Tesla recibió cartas de felicitación de más de 70 pioneros de la ciencia y la ingeniería, incluido Albert Einstein,[186]​ y también apareció en la portada de Time magazine.[187]​ La leyenda de la portada "Todo el mundo es su central eléctrica" destacó su contribución a la generación de energía eléctrica. La fiesta fue tan celebrada, que Tesla la convirtió en un evento anual, una ocasión en la que sacaba una gran cantidad de comida y bebida (con platos de su propia creación) e invitaba a la prensa a ver sus inventos y escuchar historias sobre hazañas pasadas, opiniones sobre eventos actuales, o algunas veces anuncios extraños o desconcertantes.[188]​[189]​
Con ocasión de su cumpleaños en 1932, Tesla afirmó que había inventado un motor que funcionaría gracias a la radiación cósmica.[189]​ En 1933, a la edad de 77 años, dijo a los periodistas que, después de treinta y cinco años de trabajo, estaba a punto de producir pruebas de una nueva forma de energía. Afirmó que era una teoría de la energía que estaba "violentamente opuesta" a la física de Einstein, y podría ser aprovechada con un aparato que sería barato de ejecutar y duraría 500 años. También dijo a los periodistas que estaba trabajando en una forma de transmitir longitudes de onda de radio privadas individualizadas, investigando en avances de metalurgia y desarrollando una forma de fotografiar la retina para registrar el pensamiento.[190]​
En la fiesta de 1934, Tesla dijo a los periodistas que había diseñado una superarma que, según él, terminaría con todas las guerras.[191]​[192]​ La llamaría "teleforce", pero usualmente se la mencionaba como su rayo de la muerte.[193]​ La describió como un arma defensiva que se pondría en la frontera de un país para ser utilizada contra el ataque de la infantería o de aeronaves. Tesla nunca reveló durante su vida los planes detallados de cómo el arma podía funcionar, pero en 1984, salieron a la luz en el archivo del Museo Nikola Tesla de Belgrado.[194]​ El tratado, titulado "El nuevo arte de proyectar energía concentrada no dispersiva a través de los medios naturales", describe un tubo de vacío abierto en un lado con un sellado de gas que permite que las partículas salgan, un método para cargar lingotes de tungsteno o de mercurio a millones de voltios, y dirigir la corriente resultante mediante repulsión electrostática.[195]​[189]​ Tesla intentó interesar al Departamento de Defensa de los Estados Unidos,[196]​ el Reino Unido, la Unión Soviética y Yugoslavia en el dispositivo.[197]​
En 1935, en su 79 fiesta de cumpleaños, Tesla cubrió muchos temas. Afirmó haber descubierto el rayo cósmico en 1896 e inventó una forma de producir corriente directa por inducción, y realizó muchas afirmaciones sobre su oscilador mecánico.[198]​ Describiendo el dispositivo (que esperaba que le haría ganar 100 millones de dólares en dos años), dijo a los periodistas que una versión de su oscilador había causado un terremoto en su laboratorio de 46 East Houston Street y en las calles vecinas en el centro de la ciudad de Nueva York en 1898.[198]​ Siguió diciendo a los periodistas que su oscilador podría destruir el Empire State Building con 5 libras de presión de aire.[199]​ También explicó una nueva técnica que desarrolló usando sus osciladores, que llamó "telegeodinámica", utilizándola para transmitir vibraciones al suelo que, según afirmó, funcionarían a cualquier distancia para ser utilizadas para la comunicación o para localizar depósitos minerales subterráneos.[200]​
Tesla trabajaba todos los días desde las 9:00 a. m. hasta las 6:00 p. m. o más tarde, con una cena exactamente a las 8:10 p. m., en el restaurante Delmonico's y más adelante en el Waldorf Astoria. Pedía su cena por teléfono al maitre, quien también debía ser el único que la sirviera. "La comida debía estar lista a las ocho en punto ... Cenaba solo, excepto en las raras ocasiones en que daba una cena a un grupo para cumplir con sus obligaciones sociales. A continuación, Tesla reanudaba su trabajo, a menudo hasta las 3:00 a.m."[201]​
Para hacer ejercicio, caminaba entre 13 y 16 km cada día. Todas las noches flexionaba los dedos de sus pies cien veces, afirmando que estimulaba sus células cerebrales.[202]​
En una entrevista con el editor de periódicos Arthur Brisbane, dijo que no creía en la telepatía, y comentó que: "Supongamos que pienso en asesinarte", dijo, "en un segundo lo sabrías. ¿No es maravilloso? ¿Por qué proceso realizaría la mente todo esto?" En la misma entrevista, afirmó que creía que todas las leyes fundamentales podrían reducirse a una.[203]​
Tesla se hizo vegetariano en sus últimos años, viviendo de alimentarse tan solo de leche, pan, miel y jugos de vegetales.[192]​[204]​
Tesla medía 1,88 m de altura y pesaba 64 kg, con casi ninguna variación de peso desde 1888 hasta alrededor de 1926. Su aspecto fue descrito por el editor periodístico Arthur Brisbane como "casi el hombre más alto, casi el más delgado y sin duda, el más serio que va a Delmonico regularmente".[203]​[205]​ Era una figura elegante y estilizada en la ciudad de Nueva York, meticuloso en su aseo y vestimenta, y ordenado en sus actividades cotidianas, un aspecto que mantuvo para promover sus relaciones comerciales.[206]​ También se ha dicho que tenía los ojos claros, "manos muy grandes" y "enormes pulgares".[203]​
Tesla leyó muchas obras, memorizó libros completos y supuestamente tenía una memoria eidética.[207]​ Era polígloto, dominando ocho idiomas: serbocroata, checo, inglés, francés, alemán, húngaro, italiano y latín.[208]​ En su autobiografía relató que experimentaba ciertos momentos concretos de inspiración. Durante sus primeros años, padeció repetidamente episodios en los que estuvo enfermo. Sufría una aflicción peculiar, consistente en que destellos cegadores de luz aparecían ante sus ojos, a menudo acompañados de visiones. [207]​ A menudo, las visiones estaban relacionadas con una palabra o una idea que podría haber encontrado; en otras ocasiones le proporcionarían la solución a un problema particular que se había planteado. Con solo escuchar el nombre de un elemento, podría visualizarlo con detalles realistas. [207]​ Tesla visualizaría una invención en su mente con extrema precisión, incluidas todas sus dimensiones, antes de pasar a la etapa de construcción, una técnica a veces conocida como dibujo mental. Por lo general, no hacía dibujos a mano, sino que trabajaba de memoria. Desde su infancia, tuvo frecuentes ráfagas de recuerdos relativas a eventos que habían sucedido previamente en su vida.[207]​
Tesla afirmó que nunca dormía más de dos horas por noche.[209]​ Sin embargo, admitió haber estado "dormitando" de vez en cuando "para recargar sus baterías".[202]​ Durante su segundo año de estudio en Graz, desarrolló una afición apasionada por competir al billar, al ajedrez y a las cartas, a veces pasando más de 48 horas seguidas en una mesa de juego.[210]​ En una ocasión trabajó durante 84 horas sin descanso en su laboratorio.[211]​ Kenneth Swezey, un periodista del que Tesla se hizo amigo, confirmó que el inventor rara vez dormía. Swezey recordaba una madrugada, cuando Tesla lo llamó a las 3 a. m.: "Estaba durmiendo en mi habitación como un muerto ... De repente, el timbre del teléfono me despertó ... [Tesla] habló animadamente, con pausas, [como solía hacer] ... solucionó un problema, comparó una teoría con otra, lo comentó, y cuando sintió que había llegado a la solución, de repente colgó el teléfono."[202]​
Tesla nunca se casó, explicando que su castidad fue muy útil para sus capacidades científicas.[207]​ Una vez dijo que en su juventud sentía que nunca podría ser lo suficientemente digno para una mujer, considerando a las mujeres superiores en todos los sentidos. Su opinión se vio influida en los últimos años cuando contempló cómo las mujeres estaban tratando de superar a los hombres y hacerse más dominantes. Esta "nueva mujer" suscitó mucha indignación por parte de Tesla, que sentía que las mujeres estaban perdiendo su feminidad al tratar de alcanzar el poder, perdiendo lo que la hacía superior a un hombre. En una entrevista con el Galveston Daily News del 10 de agosto de 1924, declaró: "En lugar de la suave voz de mi adoración reverente, ha llegado la mujer que piensa que su principal éxito en la vida reside en hacerse a sí misma tanto como sea posible, adoptando la vestimenta, la voz y las acciones del hombre, en los deportes y en logros de todo tipo ... La tendencia de las mujeres a apartar al hombre, suplantar el antiguo espíritu de cooperación con él en todos los asuntos de la vida, es muy decepcionante para mí".[212]​ Aunque le dijo a un periodista en años posteriores que a veces sentía el no haberse casado, y que había hecho un sacrificio demasiado grande por su trabajo.[179]​ Eligió prescindir de perseguir o de entablar ninguna relación conocida, encontrando todo el estímulo que necesitaba en su trabajo.
Tesla era asocial y propenso a aislarse con su trabajo.[213]​[214]​[113]​[215]​ Sin embargo, cuando se involucró en la vida social, muchas personas hablaron muy positivamente y con admiración de él. Robert Underwood Johnson lo describió como poseedor de una "distinguida dulzura, sinceridad, modestia, refinamiento, generosidad y fuerza". [179]​ Su secretaria, Dorothy Skerrit, escribió: "Su sonrisa genial y su nobleza siempre denotaron las características caballerescas que estaban tan arraigadas en su alma".[216]​ El amigo de Tesla, Julian Hawthorne, dejó escrito que: "Rara vez uno conoce a un científico o a un ingeniero que también sea un poeta, un filósofo, un conocedor de la música refinada, un lingüista y un entendido degustador de la comida y la bebida".[217]​
Era un buen amigo de F. Marion Crawford, Robert Underwood Johnson,[218]​ Stanford White,[219]​ Fritz Lowenstein, George Scherff y Kenneth Swezey.[220]​[221]​[222]​
Siendo ya un hombre maduro, se hizo amigo íntimo de Mark Twain; pasaron mucho tiempo juntos en su laboratorio y en otros lugares.[218]​ Twain describió notablemente la invención del motor asíncrono de Tesla como "la patente más valiosa desde el teléfono".[223]​ A finales de la década de 1920, se hizo amigo de George Sylvester Viereck, poeta, escritor, místico y posteriormente, propagandista del nazismo. De vez en cuando, asistía a cenas organizadas por Viereck y su esposa.[224]​[225]​
A veces, Tesla podía ser duro y expresar su disgusto por las personas con sobrepeso, como cuando despidió a una secretaria por su peso.[226]​ Rápidamente criticaba la indumentaria de sus empleados; y en varias ocasiones, ordenó a un subordinado que se fuera a casa y cambiara su vestimenta.[207]​
Relación con Edison
Tesla había sido empleado de la Compañía Edison entre 1883 y 1885, primero en París y a continuación en Nueva York. Conocía personalmente al propio Edison, pero su prometedora carrera en la empresa se vio truncada porque no recibió el salario que pensaba que se le había prometido. Sintiéndose engañado, comenzó a trabajar por su cuenta.
Tres años después, en 1888, Tesla había pasado a colaborar con Westinghouse, y en 1893 inició en Chicago sus demostraciones públicas para evidenciar la superioridad de la corriente alterna sobre la corriente continua de Edison, entablándose entonces lo que se conoce como la "guerra de las corrientes".[227]​
Edison trató de combatir la teoría de Tesla mediante una campaña para fomentar ante el público el peligro que corrían al utilizar la corriente alterna. El ingeniero Harold P. Brown, que había iniciado una cruzada contra los peligros de la corriente alterna, fue financiado por Edison para investigar la electrocución, contribuyendo al desarrollo de la silla eléctrica y consiguiendo por medios subrepticios que fuera alimentada con corriente alterna.[228]​
Respecto a su rival, Edison llegó a decir que: "Tesla es un sujeto que siempre está a punto de hacer algo", en una despectiva alusión a las habituales y altisonantes declaraciones a la prensa de su competidor.[229]​ Por su parte, Tesla llegó a criticar abiertamente el desaliñado modo de vida de Edison, afirmando que: "Tal era su desidia que, de no haber contraído matrimonio con una mujer de sobresaliente inteligencia, que puso todo su empeño en sacarlo a flote, habría muerto hace muchos años".[230]​ 
Cuando Thomas Alva Edison murió, en 1931, Tesla aportó la única opinión negativa a The New York Times, enterrando bajo un denso manto la vida de Edison:
No estuvo de acuerdo con la teoría de que los átomos se componen de partículas subatómicas más pequeñas, indicando que no existía un electrón que creara una carga eléctrica. Creía que si existían electrones, eran un cuarto estado de la materia o "subátomo" que solo podía existir en un vacío experimental y que no tenían nada que ver con la electricidad.[232]​[233]​ Opinaba que los átomos son inmutables: no podían cambiar de estado o dividirse de ninguna manera. Creía en el concepto dominante en el siglo XIX de un «éter» omnipresente que transmitía la energía eléctrica.[234]​
Generalmente era antagónico con las teorías sobre la conversión de la materia en energía.[235]​ También criticaba la teoría de la relatividad de Einstein, diciendo:
Afirmó haber desarrollado su propio principio físico con respecto a la materia y la energía, en el que comenzó a trabajar en 1892,[216]​ y en 1937, a los 81 años, afirmó en una carta que había completado una "teoría dinámica de la gravedad" que [haría] poner fin a las especulaciones ociosas y a las concepciones falsas, como la del espacio curvo. Señaló que la teoría estaba resuelta en todos los detalles y que esperaba poder dársela pronto al mundo.[237]​ Esta elucidación de su teoría nunca ha sido encontrada en sus escritos.[238]​
Además de sus dotes como científico tecnológico, Tesla es ampliamente considerado por sus biógrafos desde el punto de vista filosófico como un humanista.[239]​[240]​[241]​ Esto no impidió que, como muchas otras personas notables de su época, se convirtiera en un defensor de una versión de selección artificial impuesta en forma de eugenesia.
Expresó la creencia de que la "piedad" humana había interferido con el "funcionamiento despiadado de la naturaleza". Aunque su argumentación no dependía del concepto de una "raza elegida" o la superioridad inherente de una persona sobre otra, abogó por la eugenesia. En una entrevista de 1937, declaró:
En 1926, Tesla hizo un comentario sobre los males de la sumisión social de las mujeres y de su lucha para obtener la igualdad de género, e indicó que el futuro de la humanidad estaría a cargo de "abejas reinas". Pensaba que las mujeres se convertirían en el sexo dominante en el futuro.[243]​
Hizo predicciones sobre temas relevantes en el entorno inmediato a la Primera Guerra Mundial, en un artículo titulado "Ciencia y Descubrimiento son las grandes Fuerzas que conducirán a la Consumación de la Guerra" (20 de diciembre de 1914).[244]​ Pensaba que la Sociedad de las Naciones no era el remedio adecuado para los problemas de su tiempo.[245]​
Tesla fue educado en la religión ortodoxa. Durante su vida adulta no se consideró a sí mismo como un "creyente en el sentido ortodoxo", oponiéndose al fanatismo religioso, y opinaba que "el budismo y el cristianismo son las religiones más grandes tanto al número de creyentes como en importancia".[246]​ También dijo que "Para mí, el universo es simplemente una gran máquina que nunca comenzó a existir y nunca terminará" y "lo que llamamos 'alma' o 'espíritu' no es más que la suma de los funcionamientos del cuerpo. Cuando este funcionamiento cesa, el 'alma' o el 'espíritu' también cesa".[246]​
En su artículo, "El problema de incrementar la energía humana", publicado en 1900, Tesla escribió:
Entre los más destacables inventos y descubrimientos que han llegado al conocimiento del público en general, se pueden destacar:
Para un listado más amplio de Patentes ver Anexo:Patentes de Tesla


Jean-Paul Charles Aymard Sartre (París, 21 de junio de 1905-París, 15 de abril de 1980), conocido comúnmente como Jean-Paul Sartre, fue un filósofo, escritor, novelista, dramaturgo, activista político, biógrafo y crítico literario francés, exponente del existencialismo y del marxismo humanista.[2]​ Fue el décimo escritor francés galardonado como Premio Nobel de Literatura, en 1964, pero lo rechazó explicando en una carta[3]​ a la Academia Sueca que él tenía por regla rechazar todo reconocimiento o distinción y que los lazos entre el hombre y la cultura debían desarrollarse directamente, sin pasar por las instituciones establecidas del sistema.[4]​ Fue pareja de la filósofa Simone de Beauvoir.  El corazón de su filosofía residía en el concepto de libertad y en su sentido concomitante de la responsabilidad personal. Insistió, en una entrevista pocos años antes de su muerte, en que nunca había dejado de creer que «El hombre se hace a sí mismo».[5]​
Era hijo de Jean-Baptiste Sartre, un oficial naval, y Anne-Marie Schweitzer, prima de Albert Schweitzer. Su padre murió de fiebre cuando él tenía apenas quince meses, y Anne-Marie lo crio con ayuda de sus abuelos maternos, Louise Guillemin y Charles Schweitzer, quien enseñaría matemáticas a Jean-Paul y le introduciría desde muy joven en la literatura clásica.
La filosofía le atrajo desde su adolescencia en los años veinte, cuando leyó Essai sur les données immédiates de la conscience (Ensayo sobre los datos inmediatos de la consciencia) de Henri Bergson. Tuvo influencias de Immanuel Kant, Georg Wilhelm Friedrich Hegel, Karl Marx, Friedrich Engels, Søren Kierkegaard, Edmund Husserl, y Martin Heidegger, entre otros.
Estudió en París en la «elitista» École Normale Supérieure (una escuela normal superior o ENS), donde se graduó en 1929 con un doctorado en Filosofía. Durante sus estudios conoció a Simone de Beauvoir y a Raymond Aron. Sartre y Beauvoir se hicieron compañeros inseparables para el resto de sus vidas.[6]​
Realizó su servicio militar en el Ejército Francés entre 1929 y 1931. Declaró posteriormente, en 1959, que cada francés era responsable colectivamente de los crímenes cometidos durante la Guerra de Independencia de Argelia (que era una colonia francesa).[8]​
En 1939 sirvió como meteorólogo en el Ejército Francés durante la Segunda Guerra Mundial.[9]​ [10]​ Fue capturado por tropas alemanas en 1940 en Padoux,[11]​ cuando pasó nueve meses como prisionero de guerra en Nancy y luego en Stalag, en la ciudad alemana de Tréveris. No abandonó la filosofía durante ese período y, según su testimonio, escribía a diario apuntes en una libreta que conservó durante su vida en prisión.
Fue durante este período de confinamiento cuando leyó Ser y tiempo, obra de Martin Heidegger, que más tarde se convertiría en una gran influencia para su propio ensayo sobre fenomenología y ontología. A causa de su mala salud (afirmaba que su mala vista y su exotropía afectaban a su equilibrio), fue liberado en abril de 1941. Según otras fuentes, se escapó tras una visita médica al oftalmólogo.[12]​
Con el estatus de civil, recupera su puesto de profesor en el Liceo Pasteur, cerca de París, y se instala en el Hotel Mistral. En octubre de 1941 se le concedió un puesto, anteriormente ocupado por un profesor judío al que se le había prohibido enseñar por la Ley de Vichy, en el Liceo Condorcet de París.
Tras regresar a París en mayo de 1941, participó en la fundación del grupo clandestino Socialismo y Libertad (Socialisme et Liberté) con otros escritores como Simone de Beauvoir, Maurice Merleau-Ponty, Jean-Toussaint Desanti, Dominique Desanti, Jean Kanapa y estudiantes de la École Normale. En la primavera de 1941, Desanti sugirió con «alegre ferocidad» en una reunión que el grupo asesinara a destacados colaboradores de la guerra como Marcel Déat, pero Beauvoir señaló que su idea fue rechazada ya que «ninguno de nosotros se sentía capacitado para fabricar bombas o lanzar granadas».[13]​ El historiador británico Ian Ousby observó que los franceses siempre tuvieron mucho más odio hacia los colaboracionistas que hacia los nazis, señalando que era a franceses como Déat a quienes Sartre quería asesinar y no al gobernador militar de Francia, el general Otto von Stülpnagel, y que el lema popular siempre fue «¡Muerte a Laval!» en lugar de «¡Muerte a Hitler!».[14]​ En agosto, Sartre y Beauvoir fueron a la Riviera francesa buscando el apoyo de André Gide y André Malraux. Sin embargo, tanto Gide como Malraux se mostraron indecisos, y esto pudo ser la causa de la decepción y el desánimo de Sartre. Socialismo y Libertad pronto se disolvió y Sartre decidió escribir en lugar de participar en la resistencia activa. Entonces escribió El ser y la nada, Las moscas y A puerta cerrada, ninguno de los cuales fue censurado por la Alemania nazi, y también colaboró en revistas literarias legales e ilegales.
En su ensayo París bajo la ocupación, Sartre escribió que el comportamiento «correcto» de los alemanes había atrapado a demasiados parisinos en la complicidad con la ocupación, aceptando como natural lo que no era natural:

Sartre observó que cuando los soldados de la Wehrmacht preguntaban educadamente a los parisinos en su francés con acento alemán por una dirección, la gente solía sentirse avergonzada y en apuros para ayudar a la Wehrmacht, lo que llevó a Sartre a comentar que «no podíamos ser naturales».[16]​ El francés era un idioma ampliamente enseñado en las escuelas alemanas y la mayoría de los alemanes podían hablar al menos algo de francés. Al propio Sartre siempre le resultaba difícil cuando un soldado de la Wehrmacht le pedía indicaciones, normalmente decía que no sabía a dónde quería ir el soldado, pero aun así se sentía incómodo ya que el mero hecho de hablar con la Wehrmacht significaba que había sido cómplice de la ocupación.[17]​ Ousby escribió: 

Sartre sostuvo que la propia «corrección» de los alemanes provocó la corrupción moral de muchas personas, que utilizaron el comportamiento «correcto» de los alemanes como excusa para la pasividad, y que el propio acto de tratar de vivir la existencia cotidiana sin desafiar la ocupación ayudó al «Nuevo Orden en Europa», que dependía de la pasividad de la gente corriente para lograr sus objetivos.[15]​
A lo largo de la ocupación, la política alemana consistió en saquear Francia, y la escasez de alimentos fue siempre un problema importante, ya que la mayoría de los alimentos del campo francés iban a parar a la Alemania nazi.[18]​ Sartre escribió sobre la «lánguida existencia» de los parisinos mientras la gente esperaba obsesivamente la única llegada semanal de camiones con alimentos del campo que permitían los alemanes, afirmando que «París se encorvaba y bostezaba de hambre bajo el cielo vacío. Aislada del resto del mundo, alimentada sólo por la piedad o algún motivo ulterior, la ciudad llevaba una vida puramente abstracta y simbólica».[18]​ El propio Sartre vivía a base de una dieta de conejos que le enviaba un amigo de Beauvoir que vivía en Anjou.[19]​ Los conejos solían estar en un avanzado estado de putrefacción llenos de gusanos, y a pesar de tener hambre, tiró una vez un conejo por no poder comerlo, diciendo que tenía más gusanos que carne.[19]​ También comentó que las conversaciones en el Café de Flore entre intelectuales habían cambiado, ya que el miedo a que uno de ellos fuera un mouche («delator») o un escritor de las corbeaux (cartas anónimas de denuncia) significaba que ya nadie decía realmente lo que quería decir, imponiendo la autocensura.[20]​ Sartre y sus amigos del Café de Flore tenían razones para temer; en septiembre de 1940, solo la Abwehr ya había reclutado a 32.000 franceses para trabajar como mouches, mientras que en 1942 la Kommandantur de París recibía una media de 1500 cartas al día enviadas por los corbeaux.[21]​
Sartre observó que, bajo la ocupación, París se había convertido en una «farsa», parecida a las botellas de vino vacías que se mostraban en los escaparates, ya que todo el vino se había exportado a Alemania, con el aspecto del antiguo París, pero vaciado, ya que lo que había hecho especial a París había desaparecido.[22]​ Durante la ocupación, casi no había coches en las calles, ya que el petróleo era trasladado a Alemania mientras los ocupantes imponían un toque de queda nocturno, lo que lo llevó a comentar que París «estaba poblada por los ausentes».[23]​ También señaló que la gente empezó a desaparecer bajo la ocupación:
Sartre opinó que los uniformes feldgrau («gris campo») de la Wehrmacht y los uniformes verdes de la Policía del Orden, que habían parecido tan extraños en 1940, se habían convertido en algo aceptado, ya que la gente estaba adormecida y aceptaba «un verde pálido y apagado, de tensión discreta, que el ojo casi esperaba encontrar entre las ropas oscuras de los civiles».[25]​ Bajo la ocupación, los franceses solían llamar a los alemanes les autres («los otros»), lo que inspiró su famoso aforismo, incluido en la obra teatral A puerta cerrada: «l'enfer, c'est les Autres» («El infierno son los otros»).[26]​ Sartre pretendía que la línea «l'enfer, c'est les Autres» fuera, al menos en parte, una indirecta a los ocupantes alemanes.[26]​
Fue un colaborador muy activo de Combat, un periódico creado durante el periodo clandestino por Albert Camus, un filósofo y escritor que tenía creencias similares. Sartre y Beauvoir mantuvieron su amistad con Camus hasta 1951, con la publicación de El hombre rebelde de Camus. Sartre escribió mucho en la posguerra sobre grupos minoritarios desatendidos, concretamente sobre los judíos franceses y los negros. En 1946, publicó Antisemita y judío, después de haber publicado la primera parte del ensayo, Portrait de l'antisémite, el año anterior en el tercer número de la revista Les Temps Modernes (Tiempos modernos). En el ensayo, al explicar la etiología del «odio» ataca el antisemitismo en Francia[27]​ durante una época en la que los judíos que volvían de los campos de concentración eran rápidamente abandonados.[28]​ En 1947, publicó varios artículos sobre la condición de los afroamericanos en Estados Unidos -específicamente sobre el racismo y la discriminación que sufrían en el país- en su segunda colección Situaciones. Luego, en 1948, para la introducción de l'Anthologie de la nouvelle poésie nègre et malgache (Antología de la nueva poesía negra y malgache) de Léopold Sédar Senghor, escribió Orfeo negro (reeditado en Situaciones III), una crítica al colonialismo y al racismo a la luz de la filosofía que había desarrollado en El ser y la nada. Más tarde, mientras algunos autores lo tachaban de resistente, el filósofo y resistente francés Vladimir Jankelevitch criticó la falta de compromiso político de Sartre durante la ocupación alemana, e interpretó sus posteriores luchas por la libertad como un intento de redención. Según Camus, Sartre era un escritor que resistía; no un resistente que escribía.[cita requerida]
En 1945, una vez terminada la guerra, se trasladó a un apartamento en la rue Bonaparte, donde produciría la mayor parte de su obra posterior y donde vivió hasta 1962. Desde allí ayudó a fundar una revista literaria y política trimestral, Les Temps modernes, en parte para popularizar su pensamiento.[29]​ Dejó de dar clases y se dedicó a la escritura y al activismo político. Se basaría en sus experiencias de guerra para su gran trilogía de novelas, Los caminos de la libertad, escrita entre 1945 y 1949.
El primer periodo de su carrera, definido en gran parte por El ser y la nada (1943), dio paso a un segundo periodo —durante la Guerra Fría, cuando el mundo se percibía dividido en los bloques Occidental (capitalista) y Oriental (socialista)— de participación política muy publicitada. Tendía a glorificar la Resistencia después de la guerra como la expresión intransigente de la moral en acción, y recordaba que los résistants eran una «banda de hermanos» que habían disfrutado de la «verdadera libertad» de una manera que no existía antes ni después de la guerra.[30]​ Fue «despiadado» al atacar a cualquiera que hubiera colaborado o permanecido pasivo durante la ocupación alemana; por ejemplo, criticando a Camus por firmar un llamamiento para evitar que el escritor colaboracionista Robert Brasillach fuera ejecutado.[30]​Las manos sucias, obra teatral escrita 1948, exploraba particularmente el problema de ser un intelectual políticamente «comprometido». Abrazó el marxismo, pero no se afilió al Partido Comunista Francés (PCF). Durante un tiempo, a finales de la década de 1940, describió el nacionalismo francés como «provinciano» y en un ensayo de 1949 pidió unos «Estados Unidos de Europa».[31]​ En un ensayo publicado en la edición de junio de 1949 de la revista Politique étrangère, Sartre expresó:
Sobre la guerra de Corea, declaró: «No dudo que los feudalistas surcoreanos y los imperialistas norteamericanos hayan promovido esta guerra. Pero tampoco dudo que la hayan iniciado los norcoreanos».[33]​ En julio de 1950, Sartre escribió en Les Temps Modernes sobre su actitud y la de Beauvoir ante la Unión Soviética:
Sartre sostenía que la Unión Soviética era un Estado «revolucionario» que trabajaba por la mejora de la humanidad y que solo podía ser criticado por no estar a la altura de sus propios ideales, pero que los críticos debían tener en cuenta que el Estado soviético necesitaba defenderse de un mundo hostil; por el contrario, Sartre sostenía que los fracasos de los Estados «burgueses» se debían a sus deficiencias innatas.[30]​ El periodista suizo François Bondy escribió que, a partir de la lectura de los numerosos ensayos, discursos y entrevistas de Sartre, «nunca deja de surgir un simple patrón básico: el cambio social debe ser integral y revolucionario» y los partidos que promueven los cargos revolucionarios «pueden ser criticados, pero sólo por aquellos que se identifican completamente con su propósito, su lucha y su camino hacia el poder», considerando la posición de Sartre como «existencialista».[30]​
Sartre creía en esta época en la superioridad moral del Bloque del Este a pesar de sus violaciones de los derechos humanos, argumentando que esta creencia era necesaria «para mantener viva la esperanza»[35]​ y se oponía a cualquier crítica a la Unión Soviética[36]​ hasta el punto de que Maurice Merleau-Ponty lo calificó de «ultrabolchevique».[37]​ La expresión de Sartre «los obreros de Billancourt no deben ser privados de sus esperanzas»[37]​ se convirtió en un latiguillo que significaba que los militantes comunistas no debían decir toda la verdad a los obreros para evitar la disminución de su entusiasmo revolucionario.[38]​
En 1954, justo después de la muerte de Stalin, visitó la Unión Soviética, de la que afirmó que encontró una «completa libertad de crítica», al tiempo que condenaba a los Estados Unidos por hundirse en el «prefascismo».[39]​ Sobre los escritores soviéticos expulsados de la Unión de Escritores Soviéticos, opinó que «todavía tenían la oportunidad de rehabilitarse escribiendo mejores libros».[40]​ Sus comentarios sobre la Revolución húngara de 1956 son bastante representativos de sus opiniones, frecuentemente contradictorias y cambiantes. Por un lado, vio en Hungría una verdadera reunificación entre intelectuales y trabajadores[41]​[42]​ solo para criticarlo por «perder la base socialista».[43]​ Condenó la invasión soviética de Hungría en noviembre de 1956.[44]​ 
En 1964 atacó El «discurso secreto» de Jruschov, que condenaba las represiones del estalinistas y la purgas, argumentando que «las masas no estaban preparadas para recibir la verdad».[45]​
En 1973 sostuvo que «la autoridad revolucionaria siempre necesita deshacerse de algunas personas que la amenazan, y su muerte es la única manera».[46]​ Varias personas, empezando por Frank Gibney en 1961, clasificaron a Sartre como un «idiota útil» debido a su posición acrítica. [47]​
Sartre llegó a admirar al líder polaco Władysław Gomułka, un hombre que estaba a favor de una «vía polaca al socialismo» y que quería más independencia para Polonia, pero que era leal a la Unión Soviética por la cuestión de la línea Óder-Neisse.[48]​ El periódico de Sartre Les Temps Modernes dedicó varios números especiales en 1957 y 1958 a la Polonia de Gomułka, elogiándola por sus reformas.[48]​ Bondy escribió sobre la notable contradicción entre el «ultrabolchevismo» de Sarte, ya que expresaba su admiración por el líder chino Mao Zedong como el hombre que llevó a las masas oprimidas del Tercer Mundo a la revolución, al tiempo que elogiaba a líderes comunistas más moderados como Gomułka.[48]​
Como anticolonialista, desempeñó un papel destacado en la lucha contra el dominio francés en Argelia y el uso de la tortura y los campos de concentración por parte de los franceses en Argelia. Fue un eminente partidario del Frente de Liberación Nacional (FLN) en la Guerra de Argelia y uno de los firmantes del Manifiesto de los 121. En consecuencia, se convirtió en objetivo interno de la Organización del Ejército Secreto (OAS; Organisation de l'Armée Secrète), escapando de dos atentados con bomba a principios de los años 60.[49]​ En 1959 afirmó que cada francés era responsable de los crímenes colectivos durante la Guerra de Independencia de Argelia.[50]​ Tuvo una amante argelina, Arlette Elkaïm, que se convirtió en su hija adoptiva en 1965.[51]​ Se opuso a la participación de Estados Unidos en la guerra de Vietnam y, junto con Bertrand Russell y otros, organizó un tribunal destinado a exponer los crímenes de guerra de Estados Unidos, que se conoció como el Tribunal Russell en 1967.
Su obra después de la muerte de Stalin, Crítica de la razón dialéctica, se publicó en 1960 (un segundo volumen apareció póstumamente). En Crítica, se propuso dar al marxismo una defensa intelectual más vigorosa que la que había recibido hasta entonces; terminó concluyendo que la noción de «clase» de Marx como entidad objetiva era falaz. Su énfasis en los valores humanistas de las primeras obras de Marx condujo a una disputa con un destacado intelectual de izquierdas en Francia en la década de 1960, Louis Althusser, que afirmaba que las ideas del joven Marx habían sido superadas decisivamente por el sistema «científico» del último Marx. A finales de la década de 1950, Sartre comenzó a argumentar que las clases trabajadoras europeas eran demasiado apolíticas para llevar a cabo la revolución predicha por Marx, e influenciado por Frantz Fanon afirmó que eran las masas empobrecidas del Tercer Mundo, los «verdaderos condenados de la tierra», quienes llevarían a cabo la revolución.[52]​ Uno de los temas principales de sus ensayos políticos en la década de 1960 era su disgusto por la «americanización» de la clase obrera francesa, que prefería ver programas de televisión estadounidenses doblados al francés que agitar una revolución.[30]​
Visitó Cuba en la década de 1960 para conocer a Fidel Castro y habló con Ernesto Che Guevara. Tras la muerte de Guevara, declaró que era «no sólo un intelectual sino también el ser humano más completo de nuestra época»[53]​ y el «hombre más perfecto de la época».[54]​ Sobre Guevera, también dijo que «vivía sus palabras, hablaba sus propias acciones y su historia y la historia del mundo corrían paralelas».[55]​ Sin embargo, se posicionó en contra de la persecución de los homosexuales por parte del gobierno castrista, que comparó con la persecución nazi a los judíos, y dijo: «En Cuba no hay judíos, pero hay homosexuales». 
Su vida se caracterizó por una actitud militante de la filosofía. Se solidarizó con los más importantes acontecimientos de su época, como el Mayo francés, la Revolución Cultural en China —en su etapa de acercamiento a los maoístas, al final de su vida— y con la Revolución cubana. A pesar de su abrumadora fama mundial, mantuvo una vida sencilla, con pocas posesiones materiales y activamente comprometido con varias causas hasta el final de su vida.
En 1964, renunció a la literatura en un relato autobiográfico, ingenioso y sardónico, sobre los primeros diez años de su vida, Las palabras. El libro es un irónico contraataque a Marcel Proust, cuya reputación había eclipsado inesperadamente la de André Gide (que había proporcionado el modelo de littérature engagée para la generación de Sartre). La literatura, concluyó, funcionaba en última instancia como un sustituto burgués del compromiso real con el mundo.[cita requerida] 
En octubre de 1964, recibió el Premio Nobel de Literatura, «por su obra que, rica en ideas y llena del espíritu de libertad y la búsqueda de la verdad, ha ejercido una influencia de gran alcance en nuestra época», pero lo rechazó, alegando que su aceptación implicaría perder su identidad de filósofo.[4]​​[56]​ En 1945, había rechazado la Legión de Honor.[4]​[57]​ Fue el primer galardonado con el Nobel que declinó voluntariamente el premio, y sigue siendo uno de los dos únicos galardonados en hacerlo.[58]​ El premio Nobel se anunció el 22 de octubre de 1964; días antes, el 14 de octubre, había escrito una carta al Instituto Nobel, en la que pedía que se le retirara de la lista de nominados, y advertía que no aceptaría el premio si se le concedía, pero la carta llegó con un mes de retraso.[59]​ El 25 de octubre, tres días después de la noticia, Le Figaro publicó una declaración de Sartre explicando su negativa. Dijo que no deseaba ser «transformado» por ese premio, y que no quería tomar partido en una lucha cultural entre Oriente y Occidente aceptando un premio de una prominente institución cultural occidental.[4]​[60]​ No obstante, fue el premiado de ese año.[61]​ Tras recibir el Nobel intentó escapar de los medios de comunicación escondiéndose en la casa de la hermana de Simone, Hélène de Beauvoir en Goxwiller, Alsacia.[9]​ Según Lars Gyllensten, en el libro Minnen, bara minnen (Recuerdos, solo recuerdos) publicado en el año 2000, el propio Sartre o alguien cercano a él se puso en contacto con la Academia Sueca en 1975 para solicitar el dinero del premio, pero se lo negaron.[62]​ 
Durante una huelga de hambre colectiva en 1974, visitó al miembro de la Facción del Ejército Rojo en la Prisión de Stammheim y criticó las duras condiciones de encarcelamiento.[63]​
En 1977 firmó, junto a numerosos filósofos franceses, una polémica petición para despenalizar las relaciones homosexuales con menores así como la reducción de la edad de consentimiento sexual como consecuencia del conocido «caso de Versalles», en el que fueron condenados tres adultos por tener relaciones con chicos menores de 15 años sin violencia[64]​.
Hacia el final de su vida, comenzó a describirse a sí mismo como un «tipo especial» de anarquista.[65]​
Falleció el 15 de abril de 1980, a los setenta y cuatro años de edad, en el hospital de Broussais tras una enfermedad que lo había apartado de la dirección del periódico Libération años antes. Fue enterrado el 20 de abril, rodeado de una inmensa multitud. Más de veinte mil personas acompañaron el féretro hasta el cementerio de Montparnasse, en París, donde descansan sus restos.
En una primera etapa desarrolló una filosofía existencialista, a la que corresponden obras como El ser y la nada (1943) y El existencialismo es un humanismo (1946). Desde que en 1945 fundó la revista Les Temps Modernes, se convirtió en uno de los principales teóricos de la izquierda. En una segunda etapa, se adscribió al marxismo, cuyo pensamiento expresó en Crítica de la razón dialéctica (1960), aunque él siempre consideró a esta obra como una continuación de El ser y la nada.[cita requerida]
Sartre consideraba que el ser humano está «condenado a ser libre», es decir, arrojado a la acción y responsable plenamente de su vida, sin excusas. Aunque admitió algunos condicionamientos (culturales, por ejemplo), no admitió determinismos. Concibió la existencia humana como existencia consciente. El ser del hombre se distingue del ser de la cosa mientras es consciente. La existencia humana es un fenómeno subjetivo, en el sentido de que es conciencia del mundo y conciencia de sí (de ahí lo subjetivo). Sartre se formó en la fenomenología de Husserl y en la filosofía de Heidegger, de quien fue discípulo. Se observa aquí la influencia que ejerce sobre Sartre el racionalismo cartesiano. En este punto se diferencia de Heidegger, quien deja fuera de juego a la conciencia.[cita requerida]
Si en Heidegger el Dasein es un «ser-ahí», arrojado al mundo como «eyecto», para Sartre el humano en cuanto «ser-para-sí» es un «pro-yecto», un ser que debe «hacer-se».
Consecuentemente, para Sartre en el ser humano «la existencia precede a la esencia», que explica con un ejemplo: si un artesano quiere realizar una obra, primero «la» piensa, la construye en su cabeza: esa prefiguración será la esencia de lo que se construirá, que luego tendrá existencia. Los seres humanos no son el resultado de un diseño inteligente y no tienen dentro de sí algo que los haga «malos por naturaleza» o «tendientes al bien» —como diversas corrientes filosóficas y políticas han creído—, y continua: «Nuestra esencia, aquello que nos definirá, es lo que construiremos nosotros mismos mediante nuestros actos», estos nos son ineludibles: no actuar es un acto en sí mismo puesto que nuestra libertad no es algo que pueda ser dejado de lado: ser es ser libres en situación, ser es ser-para, ser como «proyecto».[cita requerida]
Sartre sostuvo, con una seguridad mancilladora, que somos absolutamente libres, pero también tenemos una responsabilidad absoluta, sobre nosotros y sobre el mundo. Por eso dijo que estamos condenados a ser libres. La libertad del sujeto, entonces, tiene que ser ejercida con responsabilidad. El hombre como ser libre es su propio autor. Por eso, la existencia del hombre precede a su esencia. Argumenta, que cuando el hombre nace, no tiene esencia, a saber, no tiene significado, no hay concepto de sí mismo, y es, como lo explica muy rigorosamente en su filosofía, la cual por esencia es compleja, el mismo que da significado a su existencia. Muchos filósofos fueron inspirados por el pensamiento sartreano. Su filosofía, con un aspecto afín a la de Heidegger, pero infinitamente original, desafió a la filosofía y a los filósofos. Fue capaz, con un don único, de señalar con precisión los errores de las teorías epistemológicas, las cuales se fanfarroneaban de ser esencialmente objetivas.[cita requerida]
El periodo inicial de su carrera, definido por El ser y la nada (1943), fue seguido por un segundo periodo de activismo político e intelectual. En particular, su trabajo de 1948 Manos sucias examinaba el problema de ser un intelectual y participar en la política al mismo tiempo. Nunca llegó a afiliarse al Partido Comunista Francés, aunque fue simpatizante de la izquierda y desempeñó un papel prominente en la lucha contra el colonialismo francés en Argelia. Se podría decir que fue el simpatizante más notable de la guerra de liberación de Argelia. Tenía una ayudante doméstica argelina, Arlette Elkaïm, a quien hizo hija adoptiva en 1965. Se opuso a la guerra de Vietnam y, junto a Bertrand Russell y otras luminarias, organizó un tribunal con el propósito de exhibir los crímenes de guerra de los Estados Unidos. El tribunal se llamaba «Tribunal Russell».[cita requerida]
Es evidente que «nunca fue marxista completamente porque esa filosofía «no puede ser reconciliada con el existencialismo sartreano» (Copleston, 1994). Ello lo hubiera llevado a abandonar completamente su obra más potente: El ser y la nada, obra en la que hizo tanto énfasis en la libertad (Stumpf, 1993, pág. 570). Sartre afirmaba que la fuente de todo significado es el para-sí, es el que constituye al mundo; para el marxismo, es la realidad material la que constituye al sujeto. Sus filosofías son incompatibles: por un lado, Sartre sostenía que es la conciencia humana la que con su operar consciente va construyendo la historia y le va confiriendo sentido al mundo, por el otro, el materialismo dialéctico considera que es la infraestructura la que determina a la superestructura, en ese sentido toda actividad espiritual o ideológica es el resultado de la infraestructura o la estructura económica, ésta está determinada».[6]​ 
Agudamente crítico del estalinismo, su pensamiento político atravesó varias etapas: desde los momentos de Socialismo y Libertad, agrupación política de la resistencia francesa frente a la ocupación alemana, cuando redactó un programa basado en Saint-Simon, Proudhon y demás, cuando consideraba que el socialismo de Estado era contradictorio a la libertad del individuo, hasta su brevísima adhesión al Partido Comunista Francés, y su posterior acercamiento a los maoístas. Su principal trabajo en el intento de comunión entre el existencialismo y el marxismo fue Crítica de la razón dialéctica, publicado en 1960.[cita requerida]
Su énfasis en los valores humanistas de Marx y su resultante interés en el joven Marx lo llevaron al famoso debate con el principal intelectual comunista en Francia de los años sesenta, Louis Althusser, quien trató de redefinir el trabajo de Marx en un periodo premarxista, con generalizaciones esencialistas sobre la humanidad, y un periodo auténticamente marxista, más maduro y científico (a partir del Grundrisse y El capital). Algunos dicen que este es el único debate público que Sartre perdió en su vida, pero hasta la fecha sigue siendo un evento controvertido en algunos círculos filosóficos de Francia.[cita requerida]
Durante la guerra de los Seis Días se opuso a la política de apoyo a los árabes, pregonada por los partidos comunistas del mundo (excepto Rumanía). Y, junto con Pablo Picasso, había organizado a doscientos intelectuales franceses para oponerse al intento de destrucción del Estado de Israel, haciendo un llamado a fortalecer los sectores antiimperialistas de ambas partes como única forma de llegar a una paz justa y al socialismo. Sartre era un admirador del kibutz.[66]​
En el pensamiento de Sartre, cabe destacar las siguientes ideas:
Durante las décadas de 1940 y 1950, las ideas de Sartre eran muy populares, y el existencialismo fue la filosofía preferida de la generación beatnik en Europa y los Estados Unidos. En 1948 la Iglesia católica listó todos los libros de Sartre en el Index librorum prohibitorum. La mayoría de sus obras de teatro están llenas de símbolos que sirven de instrumento para difundir su filosofía. La más famosa, A puerta cerrada, contiene la famosa frase «El infierno son los otros». El Otro  —en francés tiene un alcance universal y casi metafísico— como otredad, como alteridad radical.[67]​
Además del impacto de La náusea, su mayor contribución literaria fue la trilogía Los caminos de la libertad (compuesta por La edad de la razón, El aplazamiento, y La muerte en el alma), que traza el impacto de los eventos de la pre-guerra en sus ideas. Se trata de una aproximación más práctica y menos teórica al existencialismo.
Sobresale también su famoso ensayo sobre Gustave Flaubert: El idiota de la familia. Es un minucioso y voluminoso texto relativo al autor de Madame Bovary, donde Sartre examina cómo brota el deseo de escribir.
En 1964 escribió una autobiografía denominada Las palabras. Ese mismo año recibió el Premio Nobel de Literatura, «por su obra que, rica en ideas y llena del espíritu de libertad y la búsqueda de la verdad, ha ejercido una influencia de gran alcance en nuestra época». Sin embargo, lo rechazó, alegando que su aceptación implicaría perder su identidad de filósofo.[4]​​[56]​
Sartre rechazó durante décadas la noción del Unbewußtsein («lo inconsciente»), particularmente la planteada por Freud. Argumentaba que lo inconsciente era un criterio «característico del irracionalismo alemán», y por tal motivo se oponía a una psicología que se basara en un «irracionalismo».
De este modo, intentó un «psicoanálisis racionalista» al cual llamó «psicoanálisis existencial», basándose en una total autocrítica del sujeto hasta profundización que eliminara la «mala fe», que es un autoengaño (basado principalmente en racionalizaciones) por las cuales el sujeto pretende tranquilizarse, y al tratarse precisamente de «fe», el individuo cree ciegamente en ellas sin cuestionarlas. Y argumenta: «Un ser humano adulto no puede ni debe estar defendiendo sus defectos en hechos ocurridos durante su infancia, eso es mala fe y falta de madurez».


Charles André Joseph Marie de Gaulle (pronunciado /ʃaʁl də ɡol/ ( escuchar); Lille; 22 de noviembre de 1890-Colombey-les-Deux-Églises, 9 de noviembre de 1970) fue un general y estadista francés que dirigió la resistencia francesa contra la Alemania nazi en la Segunda Guerra Mundial y presidió el Gobierno Provisional de la República Francesa de 1944 a 1946 para restablecer la democracia en Francia. Fue una figura predominante en Francia durante la Guerra Fría, además de ser promotor de la reconciliación franco-alemana y una de las figuras influyentes en la historia del proceso de construcción de la Unión Europea, lo cual hacen que su pensamiento continúe influyendo en la política de su nación. En 1958, abandonó su retiro de la política tras haber sido nombrado primer ministro por el presidente René Coty. Su principal obra como presidente del gobierno fue la promulgación de una nueva constitución, la cual dio paso a la Quinta República una vez que esta se aprobó mediante un referéndum. Ese mismo año fue elegido presidente y fue reelecto en dicho cargo en 1965, renunciando en 1969.
Se graduó en la Escuela Especial Militar de Saint-Cyr en 1912. Fue nombrado oficial durante la Primera Guerra Mundial, habiendo salido herido varias veces y luego hecho prisionero en Verdún. Pétain lo consideraba un muy buen oficial.[1]​ En 1921 se casó con Yvonne Vendroux, con quien tuvo tres hijos: Philippe (1921-presente), Élisabeth (1924-2013) y Anne, que tenía síndrome de Down (1928-1948).[2]​ Durante el período de entreguerras, ejerció diversos cargos militares, en particular el de secretario del Consejo de Defensa Nacional (1937-1940), bajo el mando del mariscal Philippe Pétain. Durante la invasión alemana de mayo de 1940, dirigió una división blindada que contraatacó a los invasores; posteriormente fue nombrado subsecretario de guerra. Al caer el gobierno de Paul Reynaud y establecerse el régimen de Pétain, su antiguo jefe, con el apoyo del colaboracionista nazi, Pierre Laval, constató que las nuevas autoridades no continuarían la guerra contra el Tercer Reich y por el contrario planearían la rendición francesa en lugar de luchar desde Argelia. Al negarse a aceptar el armisticio de su gobierno con Alemania, De Gaulle exhortó a la población francesa a resistir la ocupación y continuar la lucha en su llamamiento del 18 de junio desde Londres tras haber abandonado el país el 16 de junio de 1940. Allí asumió el mando de la Francia Libre o Francia Combatiente.
Durante esos años escribió el libro L'Appel (entre 1940 y 1942), donde expone su visión de la guerra: la enorme tragedia de la ocupación, el espíritu derrotista, la entrega al enemigo, el llamamiento a no claudicar desde Londres, la organización de la Francia Libre, la lucha por la dignidad de esta en defensa de toda Francia y las aportaciones que estos franceses prestaron a los aliados, mediante la organización de fuerzas armadas que participaron en combates decisivos contra la Wehrmacht.
Dirigió un gobierno en exilio (fundado en Londres) así como las fuerzas armadas de este, las cuales estaban en contra de las potencias del Eje. A pesar de las frías relaciones que poseía con el Reino Unido y especialmente con los Estados Unidos, emergió como el líder indiscutido de la resistencia francesa. Se convirtió en jefe del Gobierno Provisional de la República Francesa en junio de 1944, siendo este el gobierno interino de Francia después de su liberación. Ese mismo 1944, De Gaulle introdujo una política económica dirigista, la cual implicaba un control sustancial conducido por el Estado sobre una economía capitalista. La consecuencia de esto fueron treinta años de crecimiento económico sin precedentes en la historia francesa, por lo cual dicho periodo (1944-1974) fue denominado como los Treinta gloriosos.
Frustrado por el regreso del mezquino partidismo en la nueva Cuarta República, dimitió a principios de 1946 sin dejar de lado la actividad política al haber fundado el partido Agrupación del Pueblo Francés (Rassemblement du Peuple Français, RPF). No obstante se retiró de la política a principios de la década de 1950 y escribió un libro sobre su experiencia en la guerra titulado Memorias de guerra, el cual rápidamente se convirtió en un elemento básico de la literatura francesa moderna. Cuando la guerra de Independencia de Argelia estaba desgarrando la inestable Cuarta República, la Asamblea Nacional lo trajo nuevamente al poder durante la crisis de mayo de 1958. Fundó la Quinta República con una fuerte presidencia, y fue elegido para continuar como presidente. Logró mantener a Francia unida mientras daba los pasos para terminar la guerra, pese a la molestia de los Pieds-Noirs (franceses étnicos nacidos en Argelia) y de los militares; ambos anteriormente habían apoyado su regreso al poder para mantener el dominio colonial. Le otorgó la independencia a Argelia y progresivamente a otras colonias francesas.
En el contexto de la Guerra Fría, De Gaulle inició su «política de grandeza» afirmando que Francia, como potencia principal, no debería depender de otros países, como los Estados Unidos, para su seguridad nacional y prosperidad. Con este fin, siguió una política de «independencia nacional» que le llevó a retirar al país de la estructura militar de la OTAN, a despejar el país de bases estadounidenses[3]​ y a lanzar un programa de desarrollo nuclear independiente que convirtió a Francia en la cuarta potencia nuclear. Restauró las relaciones francoalemanas para crear un contrapeso europeo entre las esferas de influencia angloamericanas y soviéticas mediante la firma del Tratado del Elíseo el 22 de enero de 1963. Sin embargo, se opuso a cualquier desarrollo de una Europa supranacional, favoreciendo una Europa soberana. De Gaulle criticó abiertamente la intervención de los Estados Unidos en Vietnam y el «privilegio exorbitante» del dólar estadounidense. En sus últimos años, su apoyo al lema Vive le Québec libre y sus dos vetos a la entrada del Reino Unido en la Comunidad Económica Europea generaron una considerable controversia.
Pese a haber sido reelegido presidente en 1965, su permanencia en el puesto se vio en crisis por las protestas generalizadas de estudiantes y trabajadores en mayo de 1968. No obstante, logró librarse de dicha crisis y el mismo año consiguió ganar las elecciones legislativas con una considerable mayoría en la Asamblea Nacional. Sorpresivamente, De Gaulle renunció en 1969 después de perder un referéndum en el que propuso una mayor descentralización. Murió un año después en su residencia en Colombey-les-Deux-Églises, dejando sus memorias presidenciales sin terminar. Muchos partidos y figuras políticas francesas reclaman un legado denominado gaullista, siendo muchas las calles y monumentos en Francia las que están dedicadas a su memoria.
De Gaulle nació en la región industrial de Lille, ubicada en el departamento de Norte. Fue el tercero de cinco niños y fue criado en una familia devotamente católica y tradicional. Su padre, Henri de Gaulle, fue profesor de historia y literatura en un colegio jesuita y posteriormente fundó su propia escuela.[4]​
Henri de Gaulle provenía de una familia noble de Normandía y Borgoña, siendo este un funcionario público. El nombre se cree que es de origen flamenco, y bien puede derivar de van der Waulle («de la muralla»). La madre de De Gaulle, Jeanne (de apellido de soltera, Maillot), descendía de una familia de empresarios ricos de Lille. Tenía ascendencia francesa, irlandesa, escocesa, flamenca y alemana.[5]​[6]​
Como parte de la nobleza francesa, la familia De Gaulle había perdido la mayor parte de sus tierras en la Revolución Francesa, a la que se opusieron.[7]​
El padre de De Gaulle alentó el debate histórico y filosófico entre sus hijos a la hora de la comida, y con su aliento, el entonces pequeño Charles se familiarizó con la historia francesa desde una edad temprana.[4]​ Impresionado por como su madre relataba el cuento de cómo lloraba una niña cuando supo de la capitulación francesa ante los alemanes en Sedan en 1870, desarrolló un gran interés en la estrategia militar.[4]​ También fue influenciado por su tío, quien también era llamado Charles de Gaulle. Su tío era historiador y apasionado estudioso celta que escribió libros y panfletos que abogaban por la unión de los galeses, escoceses, irlandeses y bretones en un solo pueblo. Su abuelo Julien-Philippe también era historiador, y su abuela Josephine-Marie escribió poemas que apasionaron su fe cristiana.[4]​
Cuando De Gaulle tenía diez años ya era lector de historia medieval. Durante su adolescencia temprana comenzó a escribir poesía, y más tarde su familia pagó una composición, un juego de un acto en verso sobre un viajero, para ser publicado en privado.[8]​ Era un lector voraz, por lo que a raíz de ello, leyó tomos filosóficos de escritores como Henri Bergson, Charles Péguy y Maurice Barrès. Además de los filósofos alemanes Nietzsche, Kant y Goethe, leyó las obras de los antiguos griegos (especialmente Platón) y la prosa del poeta romántico Chateaubriand.[8]​
De Gaulle se educó en el Collège Stanislas de París y estudió brevemente en Bélgica, donde continuó mostrando su interés por leer y estudiar historia, compartiendo a su vez el gran orgullo que muchos de sus compatriotas sentían por los logros de su nación.[9]​ A la edad de quince años escribió un ensayo que imaginaba al «General de Gaulle» llevando al ejército francés a la victoria sobre Alemania en 1930; más tarde escribió que en su juventud había esperado con cierta anticipación ingenua la inevitable guerra futura con Alemania para vengar la derrota francesa de 1870.[10]​
La Francia en la que De Gaulle creció durante su adolescencia, era una sociedad dividida, que traía consigo acontecimientos y procesos que no eran bienvenidos en la familia De Gaulle: el crecimiento del socialismo y el sindicalismo, la separación legal de la Iglesia y el Estado en 1905 y la reducción del período del servicio militar a dos años (también en 1905). Igualmente desagradables para los De Gaulle fueron la Entente Cordiale con Gran Bretaña, la primera crisis marroquí y, sobre todo, el caso Dreyfus. Henri de Gaulle llegó a ser un partidario de Alfred Dreyfus, el general franco-judío en cuestión. Sin embargo, el apoyo del padre de Gaulle era condicional, puesto a que estaba más preocupado por la reputación del Ejército que por su inocencia per se. El mismo período también vio el resurgimiento del catolicismo evangélico, la culminación de la Basílica Sacré-Cœur y el surgimiento del culto a Juana de Arco.[11]​[9]​
De Gaulle no fue un alumno sobresaliente hasta la mitad de su adolescencia, pero a partir de julio de 1906 fue más tenaz en la escuela a medida que se concentraba en reunir los requisitos para entrar a la Escuela Especial Militar de Saint-Cyr.[12]​ De Gaulle se unió al ejército, a pesar de estar más inclinado a una carrera como historiador, aunque dichas razones podrían encontrarse en buscar complacer a su padre. No obstante, posteriormente escribió: «cuando entré en el ejército, era una de las mejores cosas del mundo»,[4]​ comentario que el periodista y biógrafo de De Gaulle, Jean Lacouture, sugiere que debe ser tratado con cautela, pues la reputación del ejército francés estaba en un punto bajo a principios del siglo XX luego del caso Dreyfus.[13]​ Prueba de la reputación del Ejército es que había menos de 700 postulantes para entrar a St. Cyr en 1908, en lugar de los más de 2000 que había para el cambio de siglo.[13]​ Inicialmente, de Gaulle fue utilizado para romper huelgas.[13]​
Finalmente, De Gaulle tuvo éxito en su postulación para entrar en Saint Cyr en 1909. Su clasificación dentro de los postulantes fue mediocre (119 de 221 participantes),[14]​ sin embargo era relativamente joven y aquel había sido su primer intento en un examen. De acuerdo con una ley del 21 de marzo de 1905, los aspirantes a oficiales del ejército debían permanecer por lo menos un año en las filas, incluido el tiempo como privado y suboficial, antes de asistir a la academia. En consecuencia, en octubre de 1909, De Gaulle se alistó en el 33.º Regimiento de Infantería del Ejército de Tierra Francés, con base en Arras. Este fue un regimiento histórico en las batallas de Austerlitz, Wagram y Borodinó. En abril de 1910 fue ascendido a cabo. El comandante de su compañía se negó a promocionarlo a sargento, el rango habitual para un posible oficial. Esta razón se debió a que de acuerdo con su superior el joven sentía claramente que nada menos que el condestable de Francia sería suficiente para él. Finalmente fue ascendido a sargento en septiembre de 1910.
De Gaulle asumió como sargento en St. Cyr en octubre de 1910. Al final de su primer año escaló hacia el puesto 45. En St Cyr, de Gaulle adquirió el sobrenombre de «el Gran Espárrago» por su altura extremadamente elevada (1,96 m), su frente alta y su nariz.[4]​ Su trayectoria en la academia fue de menos a más, por lo que en forma posterior recibió elogios por su conducta, modales, inteligencia, carácter, espíritu militar y resistencia a la fatiga. En 1912, se graduó en 13.er lugar de su clase.[15]​
Prefiriendo servir en Francia que en ultramar, en octubre de 1912, se reincorporó al 33.º Regimiento de Infantería como subteniente (segundo teniente). El regimiento ahora estaba comandado por el coronel (y futuro mariscal) Philippe Pétain, a quien De Gaulle seguiría durante los siguientes quince años. Más tarde escribió en sus memorias: «Mi primer coronel, Pétain, me enseñó el arte del mando».[16]​
En el período previo a la Primera Guerra Mundial, De Gaulle habría estado de acuerdo con Pétain en cuanto a la obsolescencia de la caballería y de las tácticas tradicionales en la era de las ametralladoras y el alambre de púas. A menudo debatía con su superior acerca de eventuales grandes batallas y el posible resultado de cualquier guerra. Lacouture es escéptico, señalando que aunque Pétain escribió evaluaciones entusiastas de De Gaulle en los primeros dos trimestres de 1913, es poco probable que se destacara entre los 19 capitanes y 32 tenientes bajo su mando. De Gaulle habría estado presente en las maniobras de Arras de 1913, en las que Pétain criticó al general Le Gallet en la cara, pero no hay evidencia en sus cuadernos que aceptara las ideas pasadas de moda de Pétain sobre la importancia del poder de fuego.
De Gaulle fue ascendido a primer teniente en octubre de 1913.
Cuando finalmente estalló la guerra en Francia a principios de agosto de 1914, el 33.° Regimiento, considerado una de las mejores unidades de combate del país, fue inmediatamente enviado a hacerle frente al avance alemán en Dinant. Sin embargo, el comandante del Quinto Ejército Francés, el general Charles Lanrezac, permaneció anclado a las tácticas de batalla del siglo XIX, arrojando sus unidades a cargas de bayoneta sin sentido con cornetas y colores completos volando contra la artillería alemana, por lo cual incurrieron en grandes pérdidas.
Como comandante de pelotón, De Gaulle estuvo involucrado en feroces combates desde el comienzo. Recibió su bautismo de fuego el 15 de agosto y fue uno de los primeros en ser herido, recibiendo una bala en la rodilla durante la masacre de Dinant.[17]​ En el hospital habría mostrado su irritación con base en las tácticas utilizadas, por lo que habló acerca de ellas con otros oficiales, criticando los métodos obsoletos del Ejército francés. Sin embargo, no hay evidencia contemporánea de que haya entendido la importancia de la artillería en la guerra moderna. En cambio, en su escrito de la época, criticó la ofensiva «superpuesta», la insuficiencia de los generales franceses y la «lentitud de las tropas inglesas».[18]​
Se reincorporó a su regimiento en octubre, como comandante de la Séptima Compañía. Muchos de sus antiguos camaradas habían perdido la vida. En diciembre se convirtió en ayudante del regimiento.
La unidad de De Gaulle ganó el reconocimiento por arrastrarse repetidas veces en tierra de nadie para escuchar las conversaciones del enemigo en sus trincheras. La información que De Gaulle brindó luego de estas maniobras fue tan valiosa que el 18 de enero de 1915 recibió la Croix de Guerre. El 10 de febrero fue ascendido a capitán, inicialmente de forma provisional. El 10 de marzo de 1915, De Gaulle recibió una bala en la mano izquierda durante la batalla del Somme, la cual inicialmente parecía trivial, pero que luego terminó infectándolo. La herida lo incapacitó durante cuatro meses y como secuela dejó el que se viese obligado a usar su anillo de bodas en la mano derecha.[19]​ En agosto, comandó la décima compañía antes de regresar al trabajo como ayudante de regimiento. El 3 de septiembre de 1915 su rango de capitán se hizo permanente. A fines de octubre, al regresar de un permiso, regresó nuevamente al mando de la décima compañía.
Como comandante de compañía en Douaumont (durante la batalla de Verdún) el 2 de marzo de 1916, mientras lideraba una carga para intentar salir de una posición que había sido rodeada por el enemigo, recibió una herida de bayoneta en el muslo izquierdo tras ser aturdido por un proyectil y ser capturado luego de desmayarse por los efectos del gas venenoso. Fue uno de los pocos sobrevivientes de su batallón. Los soldados alemanes lo sacaron de un cráter vacío y lo tomaron prisionero.[20]​
De Gaulle pasó 32 meses en un campo de prisioneros de guerra alemán en Ingolstadt (Baviera).[21]​ Su estancia allí fue relativamente cómoda, aunque en una carta escrita para su madre, definió este encierro como «un lamentable exilio».
En cautiverio, De Gaulle leía periódicos alemanes. Habilidad que había adquirido en la escuela, además de poner dicha lengua en práctica durante sus vacaciones de verano, las que solían ser en Alemania. Su fervor patriótico y su confianza en la victoria le valieron otro sobrenombre, Le Connétable («el Condestable»), título del comandante en jefe del Ejército francés en épocas medievales.[22]​ En este período entabló amistad con otro prisionero, Mijaíl Tujachevsky, quien se convertiría en uno de los más importantes generales soviéticos. En Ingolstadt, De Gaulle escribió su primer libro, el cual se titulaba Discorde chez l'ennemi (La casa del enemigo dividida), y en este se analizaban los problemas y las divisiones dentro de las fuerzas armadas alemanas. El libro fue publicado en 1924.[23]​
De Gaulle además organizaba ponencias sobre el estado de la guerra para sus compañeros cautivos. Pero, sobre todo, intentó la evasión cinco veces, sin éxito, ya que su gran estatura le hacía demasiado visible. Intentó escapar escondiéndose en una canasta de ropa, cavando un túnel, cavando un agujero en una pared e incluso haciéndose pasar por una enfermera para engañar a sus guardias. En efecto, fue trasladado a un establecimiento de mayor seguridad y castigado a su regreso con largos períodos de confinamiento solitario y con el retiro de privilegios como periódicos y tabaco. De sus dos años y medio de cautiverio guardó un recuerdo amargo, considerándose un revenant, un soldado inútil que no sirvió para nada. En sus cartas a sus padres, hablaba constantemente de su frustración de que la guerra continuaba sin él, calificando la situación de «una desgracia vergonzosa» y la comparaba con ser engañado.
Con el fin de la guerra cerca, su depresión aumentó porque no estaba participando en la victoria, y a pesar de sus esfuerzos, permaneció en cautiverio hasta después del armisticio, cuando fue liberado. El 1 de diciembre de 1918, regresó a la casa de su padre en Dordoña para reunirse con sus tres hermanos, que habían servido en el ejército y sobrevivieron a la guerra.
Después del armisticio, De Gaulle sirvió junto al personal de la Misión Militar francesa en Polonia como instructor de la infantería de dicho país durante su guerra con la Rusia comunista (1919-1921). Se distinguió en las operaciones cerca del río Zbruch, con el rango de mayor en el Ejército polaco, y adquirió la condecoración militar más alta de Polonia: la Orden Virtuti Militari.[24]​
De Gaulle regresó a Francia en 1921, donde se convirtió en profesor de historia militar en St. Cyr. Para esa fecha ya era un orador prominente,  aptitud que había adquirido durante sus años de prisionero en Ingolstadt.[25]​ Luego estudió en la École Militaire de noviembre de 1922 a octubre de 1924. Aquí se enfrentó con su instructor, el coronel Moyrand, siendo el motivo de las disputas, el empleo de tácticas de acuerdo más a las circunstancias que a la doctrina. Después de un ejercicio en el que había desempeñado el papel de comandante, se negó a responder una pregunta sobre suministros, respondiendo que «de minimis non curat pretor» («un líder no se preocupa por trivialidades»). Obtuvo calificaciones respetables, pero no sobresalientes: 15 de 20 en varios de sus exámenes. Moyrand escribió en su informe final que era un «oficial inteligente, culto y serio, brillante y talentoso». Sin embargo, lo criticó por no obtener los beneficios suficientes del curso, la arrogancia que mostraba, excesiva autoconfianza, la brusca desestimación de los puntos de vista de los demás y su actitud de «rey en el exilio». Habiendo ingresado en el lugar 33 de 129, se graduó en el lugar 52, con un grado de assez bien («lo suficientemente bueno»). Fue destinado a Maguncia para ayudar a supervisar el suministro de alimentos y equipos para el Ejército francés de ocupación en Alemania.[26]​[27]​
La carrera de De Gaulle fue rescatada por el mariscal Pétain, quien hizo los arreglos para que la calificación de su personal se cambiara a bien en lugar de assez bien.[28]​ Esta clasificación implicaba un «bueno», pero no un «excelente», que se requería para una vacante para general. A partir del 1 de julio de 1925 comenzó a trabajar para Pétain, ejerciendo como «oficial de pluma».[29]​ De Gaulle desaprobaba la decisión de Pétain con base en tomar el mando en Marruecos en 1925 y desaprobaba la adulación pública hacia dicho general y su esposa. En 1925, De Gaulle comenzó a cultivar a Joseph Paul-Boncour, su primer patrón político.[30]​ El 1 de diciembre de 1925 publicó un ensayo sobre el «papel histórico de las fortalezas francesas». Este era un tema popular debido a la Línea Maginot que entonces se estaba planificando, pero su argumento era bastante matizado: sostenía que el objetivo de las fortalezas debería ser debilitar al enemigo, no economizar en defensa.
Las diferencias entre de Gaulle y Pétain comenzaron a acrecentarse, y estas tuvieron un punto de inflexión cuando De Gaulle escribió la historia de un soldado francés titulada Le Soldat. En aquella obra registró principalmente material histórico, pero Pétain quería agregar un capítulo final donde ilustrase sus propios pensamientos. Todo esto terminó con una tensa discusión a fines de 1926, la cual terminó con De Gaulle estallando en ira en la oficina de Pétain.[31]​ En octubre de 1926 regresó a sus deberes con la sede del Ejército del Rin.
De Gaulle había jurado que nunca regresaría a la École de Guerre excepto como comandante, pero a invitación de Pétain, y presentado al escenario por su patrón, pronunció tres conferencias allí en abril de 1927: «Liderazgo en tiempo de guerra», «Personaje» y «Prestige». Estas ponencias más tarde formaron la base de su libro Le Fil de l'Épée (El borde de la espada, 1932). Muchos de los oficiales en la audiencia eran sus mayores, quienes le habían enseñado y evaluado solo unos años antes.(Lacouture, 1991, p. 88)
Luego de doce años como capitán, De Gaulle fue ascendido a comandante el 25 de septiembre de 1927.[32]​ En noviembre de ese año, fue destinado por dos años a comandar las fuerzas de ocupación de Treveris.[33]​[34]​
De Gaulle entrenó duramente a sus hombres. Una de sus primeras acciones fue coordinar un ejercicio de cruce del río Mosela durante la noche. Sin embargo, posteriormente arrestó a un soldado por intentar apelar a su adjunto para que lo transfirieran a una unidad más cómoda. Cuando se le investigó inicialmente intentó invocar su condición de miembro de la Maison Pétain, además de solicitar al mismo general el que se protegiera de una reprimenda por interferir con los derechos políticos del soldado. Un observador escribió sobre De Gaulle que, aunque alentaba a los oficiales jóvenes, «su ego brillaba desde lejos». En el invierno de la temporada 1928-1929, treinta soldados (siete de ellos del batallón de De Gaulle) murieron a causa de la llamada «gripe alemana». Después de una investigación, fue elogiado en las esferas del debate parlamentario como un comandante moral y excepcionalmente capaz. La acción que más le valió elogios fue el haber llevado una banda de luto para un soldado privado que era huérfano. Este elogio vino por parte del entonces primer ministro Raymond Poincaré.[35]​
Las disputas entre De Gaulle y Pétain por la escritura de Le Soldat se profundizaron en 1928. Pétain, por ende, buscó un nuevo escritor, el coronel Audet, quien no estaba dispuesto a asumir el cargo y escribió a De Gaulle sentirse avergonzado de hacerse cargo del proyecto. Ante estos hechos, Pétain prefirió definitivamente no publicar el libro.[31]​ En 1929, Pétain no utilizó el borrador del texto hecho por De Gaulle para elogiar al mariscal Ferdinand Foch, quien había muerto ese año.[31]​
La ocupación aliada de Renania-Palatinado estaba llegando a su fin, y el batallón que comandaba De Gaulle debía ser disuelto. Para aquella fecha, De Gaulle buscaba un puesto de catedrático en la École de Guerre.[36]​ Aparentemente hubo una amenaza de dimisión masiva por parte de miembros de la facultad en caso de que se le concediese la cátedra. Se habló de una destinación a Córcega o al norte de África, pero siguiendo los consejos de Pétain, aceptó un envío de dos años al Líbano y Siria.[37]​
En la primavera de 1931, cuando su destinación a Beirut llegaba a su fin, De Gaulle una vez más, solicitó a Pétain el interceder para conseguir un puesto de catedrático en la École de Guerre. Pétain trató de obtenerle un cupo como profesor de Historia allí, pero una vez más no lo tendría. En cambio, De Gaulle, basándose en los planes que había trazado en 1928 para la reforma de esa institución, le pidió a Pétain que le creara un puesto especial que le permitiera dar conferencias sobre «la conducta de la guerra», tanto para la École de Guerre como para el Centre des Hautes Études Militaires (CHEM; un colegio superior de personal para generales, conocido como la «escuela para mariscales»), como para los civiles en la Escuela Normal Superior de París, y para los funcionarios públicos.[38]​
En cambio, Pétain le aconsejó que solicitara un puesto en la Secrétariat Général du Conseil Supérieur de la Défense Nationale (SGDN; Secretaría General del Consejo de la Defensa Nacional, reportando al Subsecretario del primer ministro, para así luego trasladarse al Ministerio de Guerra en 1936) en París. Pétain le prometió negociar por el nombramiento, el cual creía que sería beneficioso para De Gaulle. Fue enviado a la SGDN en noviembre de 1931, partiendo como oficial de redacción.[39]​[34]​
Fue ascendido a teniente coronel en diciembre de 1932 y nombrado jefe de la Tercera Sección (operaciones). Su servicio en la SGDN le dio seis años de experiencia siendo parte de la colaboración entre el ejército y el gobierno para la planificación militar, lo que, a la larga, le permitió asumir responsabilidades ministeriales en 1940.[40]​
Después de estudiar las gestiones militares en países como los Estados Unidos, Italia y Bélgica, De Gaulle redactó un proyecto de ley para la organización de Francia en tiempos de guerra. Hizo una presentación acerca de su proyecto de ley al CHEM. El proyecto de ley fue aprobado por la Cámara de Diputados, sin embargo rechazado en el Senado.[41]​
A diferencia de Pétain, De Gaulle creía en el uso de tanques y maniobras rápidas en lugar de la guerra de trincheras.[42]​ De Gaulle se convirtió en discípulo de Émile Mayer (1851-1938), un teniente coronel retirado (su carrera había sido damnificada por el caso Dreyfus) y pensador militar. Mayer pensó que, aunque las guerras aún debían ocurrir, era «obsoleto» que los países civilizados amenazaran o se enfrentaran unos a otros como lo habían hecho en siglos anteriores. Tenía una mala opinión acerca de la calidad de los generales franceses, además de ser crítico de la Línea Maginot y un defensor de la guerra mecanizada. Lacouture sugiere que Mayer centró los pensamientos de De Gaulle en su obsesión por la mística del líder fuerte (Le Fil d'Epée: 1932) y en la lealtad a las instituciones republicanas y la reforma militar.[43]​
En 1934, De Gaulle escribió Vers l'Armée de Métier (Hacia un Ejército Profesional). Propuso la mecanización de la infantería, con énfasis en una fuerza élite de 100 000 hombres y 3000 tanques. El libro imagina tanques recorriendo el país como caballería. El mentor de De Gaulle, Emile Mayer, era algo más profético que él acerca de la importancia futura del poder aéreo en el campo de batalla. Tal ejército compensaría la escasez de población en Francia y sería una herramienta eficiente para hacer cumplir el derecho internacional, particularmente el Tratado de Versalles, que prohibía a Alemania rearmarse. También pensó que sería un precursor de una reorganización nacional más profunda, y escribió que «un maestro debe hacer su aparición [...] cuyas órdenes no pueden ser impugnadas; un hombre defendido por la opinión pública».[44]​
Solo 700 copias fueron vendidas en Francia; se cree que la afirmación de que se vendieron miles de copias en Alemania es una exageración. De Gaulle utilizó el libro para ampliar sus contactos entre los periodistas, en particular con André Pironneau, editor de L'Écho de Paris. El libro atrajo elogios en todo el espectro político, salvo de parte de la izquierda, la cual estaba comprometida con el ideal de un ejército de ciudadanos.[45]​ Las opiniones de De Gaulle atrajeron la atención del inconformista político centroderechista, Paul Reynaud, a quien frecuentemente escribía en términos obsequiosos. Reynaud lo invitó a reunirse con él, el 5 de diciembre de 1934.[46]​
La familia De Gaulle era muy reservada.[47]​ De Gaulle estaba profundamente enfocado en su carrera en este momento. No hay evidencia alguna de que haya sido tentado por el fascismo, así como también hay poca evidencia acerca de sus puntos de vista sobre crisis políticas como los disturbios del 6 de febrero de 1934.[48]​ Aprobó la campaña de rearme que inició el gobierno del Frente Popular en 1936, aunque la doctrina militar francesa seguía siendo el que los tanques deberían usarse en paquetes de penique para el apoyo de la infantería (irónicamente, en 1940 las unidades de los panzers alemanes serían quienes usarían el sistema que De Gaulle defendía).[49]​ En 1935, había escrito una carta a su madre advirtiéndole que la guerra con Alemania era inevitable y que tarde o temprano iba a librarse, además de haberle asegurado que el pacto de Pierre Laval con la Unión Soviética ese mismo año fue para bien, comparándolo con la alianza de Francisco I y los otomanos contra el emperador Carlos V.[50]​
Desde abril de 1936, mientras todavía estaba en su puesto de personal de la SGDN, de Gaulle también fue conferencista de generales en la CHEM.[41]​ Los superiores de De Gaulle desaprobaban sus puntos de vista acerca de los tanques, y no consiguió su ascenso a coronel, supuestamente debido a que su récord de servicio no era lo suficientemente bueno. Intercedió con su patrón político, Reynaud, quien terminó mostrándola su historial al ministro de Guerra, el radical-socialista Edouard Daladier. Daladier, que era un entusiasta del rearme con armamento modernas, se aseguró de que de Gaulle fuese incluido en la lista de ascensos para el año entrante.[51]​[52]​
En 1937, el general Bineau, quien fue su profesor en St. Cyr, escribió en su informe sobre su cátedra en CHEM que era altamente capaz y apto para el alto mando en el futuro, pero que habría ocultado sus atributos bajo «una actitud fría y elevada».[41]​ Lo pusieron al mando del 507.° Regimiento de Tanques (que consistía en un batallón de Char D2 medianos y un batallón de tanques ligeros R35) en Metz el 13 de julio de 1937, y su ascenso a coronel completo entró en vigencia el 24 de diciembre de ese año. De Gaulle atrajo la atención del público al encabezar un desfile de 80 tanques en la Place d'Armes en Metz, en su tanque de comando «Austerlitz».[53]​
En ese momento, De Gaulle estaba comenzando a ser conocido como «Coronel Motor».[54]​ A invitación de la editorial Plon, produjo otro libro, La France et son Armée (Francia y su Ejército) en 1938. De Gaulle incorporó gran parte del texto que había escrito para Pétain una década antes en Le Soldat, siendo esto motivo de disgusto para Pétain. Al final, De Gaulle acordó incluir una dedicación a Pétain (aunque escribió la suya en lugar de usar el borrador que le envió Pétain), la cual fue eliminada en ediciones de posguerra. Hasta 1938, Pétain había tratado a De Gaulle, como dice Lacouture, «con buena voluntad ilimitada», pero en octubre de 1938, en privado, pensaba que su antiguo protegido era «un hombre ambicioso y muy mal educado».[55]​
Para el estallido de la Segunda Guerra Mundial, de Gaulle fue puesto al mando de los tanques del Quinto Ejército Francés (cinco batallones dispersos, equipados en gran medida con tanques ligeros R35) en Alsacia. El 12 de septiembre de 1939 atacó en Bitche, simultáneamente con la ofensiva de Sarre.[56]​[57]​
Los tanques de De Gaulle fueron inspeccionados por el presidente Lebrun, quien quedó impresionado, pero lamentó que ya fuera demasiado tarde para implementar sus ideas.[58]​ Escribió un artículo titulado L'Avènement de la force mécanique (La Llegada de la Fuerza Blindada), el cual envió al general Georges (Comandante en Jefe en el Frente Noreste) y al socialista Léon Blum. Daladier, primer ministro en ese momento, no leyó dicho artículo dado que supuestamente estaba demasiado ocupado para hacerlo.[59]​
A fines de febrero de 1940, Reynaud le comunicó a de Gaulle que sería destinado para el mando de una división blindada tan pronto como fuese posible. A principios del mismo año (la fecha exacta es incierta), de Gaulle propuso a Reynaud que fuera nombrado secretario general del Consejo de Guerra. Esto, bajo todos los efectos le convertiría en consejero militar de gobierno. Sin embargo, cuando Reynaud fue nombrado primer ministro durante marzo, dependía del respaldo de Daladier, por lo que el puesto recayó en la figura del político Paul Baudouin (posterior canciller del Régimen de Vichy).
A finales de marzo de 1940, Reynaud le comunicó a de Gaulle que se le daría el mando de la 4.ª. División Blindada. El gobierno estaba en una posición de aparente purga, puesto que Daladier y el general Maurice Gamelin fueron atacados por la opinión pública a raíz de la derrota de los Aliados en Noruega. El 3 de mayo, De Gaulle, por su parte, seguía insistiendo a Reynaud para que se le diese el puesto de secretario general en el consejo para afrontar la guerra y así redirigir la organización del Ejército. El 7 de mayo se encontraba reuniendo al personal de su nueva división.
Los alemanes atacaron el oeste el 10 de mayo. De Gaulle ya estaba activo con su nueva división el 12 de mayo. Los alemanes habían penetrado la frontera en Sedán el 15 de mayo de 1940. Ese día, teniendo reunidos tres batallones de tanques y menos de un tercio de sus hombres, fue convocado al cuartel general, indicándosele que atacara para así ganar tiempo para que el Sexto Ejército del General Robert Touchon se desplegara hacia al Aisne desde la Línea Maginot. El general Georges le comentó a de Gaulle que era su oportunidad para implementar las ideas.
De Gaulle comandó algunas unidades de artillería y de caballería en retirada, y de paso, recibió una media brigada adicional, cuyos batallones incluía algunos tanques pesados Char B1 bis. El ataque a Montcornet, un cruce de carreteras clave cerca de Laon, comenzó alrededor de las 04:30 el 17 de mayo. Superado en número y sin apoyo aéreo, perdió 23 de sus 90 vehículos en minas y armas antitanque (o Stukas). El 18 de mayo fue reforzado por dos nuevos regimientos de caballería blindada, elevando su fuerza a 150 vehículos. Atacó de nuevo el 19 de mayo y sus fuerzas fueron devastadas una vez más por los Stukas alemanes y la artillería. Ignoró las órdenes de retirarse por parte del general Georges y, a primera hora de la tarde, exigió dos divisiones más a Touchon, quien rechazó su solicitud. A pesar de que los tanques de De Gaulle obligaron a la infantería alemana a retirarse a Caumont-sur-Durance, la acción solo trajo un efímero alivio temporal e hizo poco para detener el arrollador avance alemán. No obstante, fue uno de los pocos éxitos que disfrutaron los franceses mientras sufrían derrotas en otras partes del país.
Retrasó su retiro hasta el 20 de mayo. El 21 de mayo, a petición de los agentes de propaganda, dio una charla en la radio francesa sobre su reciente ataque. En reconocimiento por sus esfuerzos, de Gaulle fue ascendido al rango de brigadier general el 23 de mayo de 1940. A pesar de haber sido retirado obligatoriamente como coronel el 22 de junio (ver más abajo), usaría el uniforme de general de brigada durante el resto de su vida.
Los días 28 y 29 de mayo, de Gaulle dirigió un ataque contra los alemanes al sur del departamento de Somme en Abbeville, tomando alrededor de 400 prisioneros en un intento desesperado por tratar cortar una ruta de escape con tal de que las fuerzas aliadas nuevamente tuvieran una cabeza de playa en Dunkerque.
Se convirtió en el jefe militar más visible de la Francia liberada y, gracias a este prestigio, presidió hasta 1946 el Gobierno Provisional de la República.
Tras un largo período alejado de la vida pública, vuelve a la arena política para solucionar el enquistado problema de Argelia, colonia francesa que quería independizarse, y la endémica inestabilidad política de la IV República.
Francia, al borde de la guerra civil por las tensiones entre el gobierno central, desunido y desorganizado, y un grupo de ultraderecha, pro-colonos de Argelia, denominado Organisation de l'Armée Secrète (OAS) que exigía la represión lisa y llana del movimiento independentista argelino de Ahmed Ben Bella, y con serios desequilibrios financieros heredados de esa situación, recurrió a él momentos antes de que estallara un golpe de Estado contra el último primer ministro Pierre Pflimlin. De Gaulle asumió el cargo enseguida (1 de junio de 1958), logró del presidente Coty y de la Asamblea General plenos poderes y procedió a la creación de la V República, aprobada masivamente en un referéndum ese mismo año. Al año siguiente, de Gaulle obtendría la presidencia venciendo con el 78 % de los votos al comunista Georges Marrane, que apenas logró el 13 %, y el 9 % el independiente André Châtelet. Esta sería la única elección presidencial francesa realizada por medio de un cuerpo electoral de alrededor de 80 000 personas compuesto de diputados, consejeros generales y de representantes de los concejos. Aprovechando el impulso obtenido favoreció la creación de un movimiento (no quiso que se llamara partido) alrededor de su figura, la Unión para la Nueva República (UNR). Creó así la ideología del gaullismo (en francés Gaullisme) de tipo tercerposicionista con toques de conservadurismo.
Esta etapa se caracteriza por su firme oposición a los Estados Unidos, mediante una reafirmación de la soberanía francesa que se plasmará en la salida de las estructuras militares integradas de la OTAN (a la que volvería Francia en 2009, durante la presidencia de Nicolás Sarkozy) y en la petición de conversión en oro de las reservas francesas de dólares, lo que provocó una crisis financiera mundial, que fue uno de los factores que obligaron a Nixon a suspender la convertibilidad del dólar en oro en agosto de 1971.
Finalmente, tras una sangrienta guerra no convencional (guerrillas, atentados...) Argelia se independiza en julio de 1962, poniendo punto final al problema más sangrante del Gobierno de Gaulle. Significó un giro radical en la política exterior francesa, que abandona veleidades coloniales pretéritas y centra sus miras en la construcción europea.
El 22 de enero de 1963, Alemania y Francia se reconcilian tras la firma por Charles de Gaulle y Konrad Adenauer del Tratado del Elíseo. Las disonancias con países vecinos por la construcción del Mercado Común Europeo hicieron mermar la popularidad de De Gaulle en el frente interno, que llegó afectada a las elecciones presidenciales de 1965. En ellas, de Gaulle no logró imponerse en la primera vuelta, ya que obtuvo el 44 % de los sufragios, 34 puntos menos que seis años antes. Ante tal situación, de Gaulle estuvo a punto de renunciar a su candidatura y retirarse de la política por lo que consideraba una reprobación, pero finalmente se presentó y venció con el 54 % ante el 46 % de la Federación de la Izquierda Democrática y Socialista que encabezaba Mitterrand.
En 1964 realizó una histórica gira por varios país de América Latina (que fue conmemorada en 2014), en la cual visitó varios países como Venezuela, Uruguay, Paraguay, donde se reunió con el dictador Alfredo Stroessner; Chile donde se reunió con el presidente Jorge Alessandri y el presidente electo Eduardo Frei Montalva; México donde se reunió con el presidente Adolfo López Mateos y sus antecesores Emilio Portes Gil, Lázaro Cárdenas del Río, Miguel Alemán y Adolfo Ruiz Cortines. En este país pronunció algunos discursos en español en el Zócalo de la Ciudad de México ante la presencia de miles de personas; también visitó Brasil y Argentina, donde un grupo de partidarios de Juan Domingo Perón buscaron una posibilidad de reunir a ambas figuras políticas.
En la etapa final de su Gobierno, se enfrentó a un recrudecimiento del conflicto social que derivará en el denominado «Mayo francés» (1968, una revuelta estudiantil y obrera que fuerza la caída del gabinete del primer ministro gaullista Georges Pompidou). En 1969, de Gaulle convoca un referéndum sobre las regiones en Francia para lograr mayor legitimidad, pero pierde. Derrotado, dimitirá y se retirará de la política. Murió de un aneurisma el 9 de noviembre de 1970, dejando sus memorias inconclusas.
De Gaulle dejó una impronta indeleble en la política francesa del pasado y presente siglo, pues buena parte de sus ideas están todavía presentes en la Francia actual, bajo la corriente del denominado «gaullismo». En su honor se cambió el nombre a la parisina plaza de «L'Étoile» (lugar en el que se sitúa el Arco de Triunfo de París) por plaza Charles de Gaulle.

Solimán I, llamado el Magnífico, (entre los occidentales) o Kanuni (entre los turcos), es decir, el Legislador[1]​ (en turco moderno: I. Süleyman; en turco otomano: سليمان, Sulaymān; Trebisonda, 6 de noviembre de 1494 - Szigetvár, 6 de septiembre de 1566) fue sultán y padishá del Imperio otomano desde 1520 hasta su muerte, y uno de los monarcas más importantes de la Europa del siglo XVI. Llevó al Imperio otomano a sus mayores cotas. Bajo su administración el estado otomano gobernaba al menos a 20-25 millones de personas.[1]​
Sucedió a su padre, el sultán Selim I, en septiembre de 1520 y comenzó su reinado emprendiendo una campaña militar contra las potencias cristianas en Europa Central y el Mediterráneo. Belgrado cayó en 1521; en 1522-1523 fue el turno de Rodas, arrancada del largo dominio de los Caballeros de San Juan. En la batalla de Mohács, librada en agosto de 1526, Solimán destruyó la fuerza militar de Hungría y el propio rey húngaro Luis II perdió la vida. Durante su conflicto con los safávidas, también consiguió anexionar gran parte de Oriente Próximo, que incluía Bagdad, y amplias zonas del norte de África, hasta el oeste de Argelia. Bajo su mandato, la flota otomana dominaba los mares desde el Mediterráneo hasta el mar Rojo, que atravesaba el golfo Pérsico.
A la cabeza de un imperio en expansión, Solimán promovió importantes cambios legislativos en los ámbitos de la sociedad, la educación, la fiscalidad y el derecho penal. Sus reformas, llevadas a cabo en colaboración con el principal funcionario judicial del imperio, Ebussuud Efendi, armonizaron la relación entre las dos formas de derecho otomano: el derecho estatal (Kanun) y el derecho religioso (Shari'ah). Solimán también fue un excelente poeta y orfebre, además de un gran mecenas de la cultura. Supervisó la llamada «Edad de Oro» del Imperio otomano, y en consecuencia, fomentó su desarrollo artístico, literario y arquitectónico.
Rompiendo con la tradición otomana, Solimán se casó con Hürrem Sultan, una mujer de su harén, una cristiana de ascendencia rutena que se convirtió al islam y fue conocida en Occidente como Roxelana, presumiblemente por su pelo rojo.[2]​ Su hijo, Selim II, sucedió a su padre en 1566 a su muerte tras un reinado de cuarenta y seis años. Los otros herederos potenciales (Şehzade Mehmed y Şehzade Mustafa) ya habían muerto, el primero de viruela y el segundo de estrangulamiento. Otro hijo, Şehzade Bayezid, fue ejecutado, junto con sus cuatro hijos, en 1561, por una emboscada ordenada por Selim II, tras una rebelión que organizó. Durante mucho tiempo se creyó que a la muerte de Solimán le seguía un periodo de decadencia del imperio. Este punto de vista se ha abandonado desde entonces, pero el final del reinado de Solimán se sigue considerando con frecuencia un punto de inflexión en la historia otomana. De hecho, en las décadas que siguieron a su muerte, el imperio comenzó a experimentar importantes cambios políticos, institucionales y económicos, un fenómeno que a menudo se denomina «transformación del Imperio otomano».
Poco o nada se sabe de los primeros años de vida de Solimán; como en aquella época no había razones concretas para suponer que se convertiría en sultán en el futuro, los cronistas no se molestaron en registrar ningún hecho concreto de su infancia. Se supone que pudo haber nacido el 6 de noviembre de 1494 en Trebisonda, en la actual Turquía, durante la época en que su padre Selim gobernaba esa provincia.[3]​ Su madre, la joven de 17 años Ayşe Hafsa Sultan, era probablemente de sangre real, posiblemente una hija del kan de Crimea Meñli I Giray. Es probable que su educación comenzara a los siete años; estudió el Corán, aritmética, música, escritura, tiro con arco y, casi con toda seguridad, aprendió la lengua persa y la árabe. Como era costumbre para los hijos de los altos dignatarios, hacia los once años fue circuncidado, dejó a su madre y se fue a vivir a su propia residencia.[4]​ A la edad de quince años, su abuelo, el sultán Bayezid II, lo nombró gobernador de Karahisar y dos años más tarde gobernador de Caffa (actual Feodosia), antigua colonia genovesa y encrucijada del comercio entre Irán, India y Europa. El 6 de agosto de 1509, Solimán partió para llegar a la ciudad y tomar posesión.[5]​[6]​
En 1512, Selim obligó a su padre, quien era sultán, a abdicar y al mismo tiempo exterminó a sus hermanos y a otros posibles sucesores, una práctica habitual en la casa real otomana, poniendo así fin a la guerra civil y convirtiéndose en el nuevo sultán legítimo.[7]​ En esta época Solimán tenía 17 años y, además de gobernar Caffa, realizaba otras tareas administrativas en nombre de su padre; participó en una campaña militar en Irán, gobernó Edirne y combatió a los bandidos en Magnesia, donde posteriormente se quedaría desde 1512 para tomar las riendas.[5]​[8]​[9]​ Fue en Magnesia donde entabló una estrecha amistad con Pargalı İbrahim Paşa, un esclavo que más tarde se convertiría en uno de sus asesores más cercanos. Mientras tanto, el Imperio otomano, bajo el liderazgo de Selim, continuó su expansión, derrotando al sultanato rival egipcio de los mamelucos circasianos de la dinastía buryí. Esto llevó a los turcos a anexionarse Siria, Egipto, Palestina y Arabia, lo que hizo que se hicieran con el control de las tres ciudades sagradas de La Meca, Medina y Jerusalén.[9]​[10]​
Mientras el futuro sultán se ocupaba de la administración del imperio, su padre Selim murió mientras viajaba de Constantinopla a Edirne. Inmediatamente se prefirió mantener esta noticia en secreto, a la espera de la llegada de su hijo, para evitar posibles levantamientos en las filas del ejército. Sin embargo, una vez que Solimán llegó al féretro de su padre, los jenízaros, las tropas de infantería de élite del ejército, acogieron la sucesión y se quitaron las gorras en señal de luto sin ningún tipo de alboroto,[9]​ gracias en parte al regalo de 5000 aspers por cabeza que les dio el nuevo sultán para garantizar su lealtad.[11]​ Al regresar a Constantinopla a la cabeza del cortejo fúnebre, el 1 de noviembre Solimán pudo recibir en la sala del diván el homenaje de los altos dignatarios, los ulema y el gran muftí, y en consecuencia, ascendió oficialmente al trono como décimo sultán otomano. Entre sus primeros actos oficiales, se encuentra la orden de construir la mezquita de Yavuz Selim y realizar la habitual concesión de beneficios a los miembros del ejército. Además, decidió derogar algunas duras disposiciones ordenadas anteriormente por Selim y liberó a seiscientos notables egipcios para poner fin al despiadado régimen emprendido por su padre en los últimos años de su vida.[12]​
Sin embargo, la primera preocupación por el orden público vino de Siria, donde estaba en marcha una revuelta fomentada por Janbirdi al-Ghazali, un alto dignatario que se había apoderado de Damasco, Beirut y Trípoli. Solimán respondió rápidamente, enviando un contingente a las órdenes de Ferhad Pasha, quien dominó a los rebeldes y aplastó el levantamiento.[8]​[13]​ Había demostrado, pues, decisión y capacidad administrativa, características que le hacían, a ojos de sus súbditos, merecedor de reinar sobre un imperio que ya era vasto en aquella época, predominando sobre todo el mundo musulmán, y que seguiría ampliando a lo largo de su vida.[14]​[15]​
Una vez establecida su autoridad dentro de las fronteras otomanas, Solimán pudo ocuparse de la política exterior. En aquella época, Europa vivía un periodo de dificultades provocados por continuos conflictos y las violentas divisiones internas entre el catolicismo y el protestantismo. Para defender las fronteras del Sacro Imperio Romano, Fernando de Habsburgo, hermano del Emperador del Sacro Imperio Romano Carlos V, se situó al este.[16]​
La paz de 1503, firmada al final de la guerra turco-veneciana, preveía el pago de un tributo anual a los otomanos. El asesinato del enviado que había acudido a recoger el dinero de los húngaros fue el casus belli para que Solimán atacara a los cristianos del Danubio con el objetivo de conquistar Belgrado, ciudad estratégica desde la que podría avanzar posteriormente hacia Viena y Budapest. Durante todo el invierno de 1520, el imperio estuvo ocupado en la preparación de la expedición, la primera de Solimán.[17]​ El 6 de febrero del año siguiente, el ejército dirigido por el sultán abandonó Constantinopla en medio de fastuosas celebraciones.[18]​
Después de viajar durante varios meses en tres columnas, las tropas se reunieron bajo las puertas de Belgrado, y en consecuencia, comenzaron el asedio de la ciudad el 25 de junio; el sultán fue asistido en la dirección de las operaciones por sus más altos dignatarios, entre los que se encontraban el pashá Pargalı İbrahim, el gran visir Piri Mehmed y Mehmet Beg Mihaloglu, quien más tarde se convirtió en el gobernador de facto de Valaquia. Al principio, la defensa de la ciudad resultó eficaz, pero cuando las divisiones religiosas entre los católicos y los ortodoxos asediados se hicieron sentir, allanaron el camino a los otomanos. Así, tras un largo bombardeo, Solimán pudo entrar en la ciudad el 29 de agosto, donde recitó personalmente la oración de la Yumu'ah del viernes.[19]​[20]​[21]​
Galvanizado por su éxito en Belgrado, Solimán retomó el plan trazado por su padre Selim de atacar la fortaleza cristiana de Rodas, iniciativa que ya intentó Mehmed II sin éxito en 1480. La isla de Rodas, en ese momento en manos de los Caballeros de San Juan, representaba un peligro real para los otomanos, ya que era una base para los corsarios cristianos que desde hacía tiempo atacaban a los peregrinos musulmanes en su viaje Hajj a La Meca y saqueaban los barcos mercantes. Consciente de que los europeos no intervendrían por estar ocupados en conflictos internos, el sultán estaba decidido a acabar con ello.[20]​[22]​
Tras enviar una carta el 1 de junio de 1522 al Gran Maestre de los Caballeros Philippe Villiers de l’Isle-Adam exigiendo la rendición de la isla, Solimán ordenó al jefe de la expedición Lala Kara Mustafa Pasha que la armada otomana abandonara el puerto de Constantinopla y navegara con sus trescientos barcos hacia Rodas. Pocos días después, el 18 de junio, el propio sultán partió por tierra con un séquito de cien mil hombres desde Üsküdar para hacer la guerra a los jinetes. Al llegar a Kütahya el 2 de julio, se unió a las otras fuerzas proporcionadas por el beylerbey (gobernador) de Rumelia y Anatolia. El ejército otomano llegó a Marmaris el 28 de julio. Saludado por el disparo de más de un centenar de cañones, el sultán inició el sitio de Rodas.[23]​[24]​
Aunque Isle-Adam contaba con una pequeña fuerza para defender la ciudad, compuesta por unos siete mil soldados y setecientos caballeros, estaba bien motivada. Sin embargo, para el Gran Maestre quedó claro que la única estrategia posible para detener a los doscientos mil otomanos era la de esperar una rápida intervención de las demás fuerzas cristianas de Europa. Preocupado por la prolongación del asedio, el 23 de septiembre Solimán ordenó un asalto a las murallas de Rodas, que sin embargo rompió la extenuante defensa de los caballeros y costó a ambos bandos pérdidas muy cuantiosas.[25]​ Un nuevo intento de asalto el 12 del mes siguiente también terminó en fracaso después de herir al ağa (comandante) de los jenízaros. El 10 de diciembre, tras haber perdido más de 3000 soldados en un solo ataque unos días antes, Solimán ofreció a los cristianos negociar una rendición. Después de dar más vueltas, Isle-Adam finalmente se dio cuenta de que nunca recibiría la ayuda que esperaba de los cristianos de occidente y que no podría mantener el asedio mucho más tiempo.[26]​ Así que se llegó a un acuerdo de rendición que permitía a los caballeros abandonar la isla sin peligro y a los habitantes restantes estar exentos de impuestos y devşirme durante cinco años; sin embargo, los comandantes no pudieron detener a los jenízaros, quienes saquearon la ciudad y profanaron los lugares sagrados.[27]​[28]​
Solimán recibió personalmente al Gran Maestre antes de que abandonara la ciudad, tratandole amistosamente y consolándolo; se dice que el sultán confió al «gran visir» que estaba «verdaderamente apenado por haber expulsado a este anciano de su palacio». El 1 de enero de 1523, los caballeros supervivientes abandonaron la isla para dirigirse a Mesina en el exilio (desde donde, unos años más tarde, se establecerían en Malta); Solimán había triunfado, no sin dificultad, una vez más, y en consecuencia, sembró el terror en la población cristiana de Europa.[21]​[27]​[29]​
A su regreso a Constantinopla, Solimán nombró a su viejo amigo Pargalı İbrahim Paşa gran visir del Imperio otomano y comandante de todo el ejército a excepción de los jenízaros, como era costumbre. La relación de gran amistad y confianza entre el sultán e İbrahim siempre ha interesado mucho a los historiadores y ha escandalizado a los contemporáneos; nacido cristiano en 1494 probablemente en Parga, una antigua posesión veneciana en Epiro, İbrahim fue hecho prisionero y posteriormente ofrecido a Solimán. Enseguida destacó por su inteligencia y capacidad de aprendizaje, y se decidió que debía recibir una educación adecuada. Cuando Solimán se convirtió en sultán, İbrahim tuvo la oportunidad de ascender rápidamente en la jerarquía del imperio, por lo que se convirtió rápidamente en el «gran visir» en junio de 1523. Sin embargo, parece que el propio İbrahim, preocupado por su propia seguridad, había pedido a su amigo el sultán que no se le considerara para un puesto tan prestigioso y arriesgado, pero que Solimán se negó a acceder a su petición, asegurándole su protección incondicional.[30]​[31]​
El 18 de mayo de 1524, la haseki (concubina favorita) y esposa de Solimán, Hürrem Sultan (1502-1558), dio a luz a su hijo Selim. Poco se sabe de los orígenes de Hürrem, conocida en Occidente como Roxelana, probablemente fue una esclava tártara entregada al harén imperial. En cualquier caso, pronto se convirtió en la favorita de Solimán, que por ella abandonó a la primera haseki Mahidevran Gülbahar, quien le había dado su primogénito Şehzade Mustafa en 1515, cuando aún era gobernador de Manisa. Antes de Selim, Hürrem había tenido otros dos hijos del sultán, Şehzade Mehmed, nacido en 1521, y Şehzade Abdullah, nacido en 1522, quien murió sólo tres años después. Se ha hablado mucho de Roxelana; capaz de pasar en poco tiempo de esclava del harén a concubina y finalmente a esposa legal, contraviniendo las costumbres otomanas, tuvo una gran influencia sobre el sultán, hasta el punto de que algunos historiadores la consideran la tejedora de tramas que acabaron condicionando toda la política del imperio y en particular la sucesión al sultanato, viéndola como la iniciadora del llamado 'sultanato de las mujeres'.[32]​[33]​
A pesar de la firmeza con la que se gobernaba el imperio, las rebeliones eran bastante frecuentes. Posiblemente debido al descontento con el nombramiento de İbrahim Paşa como gran visir, su competidor Ahmed Paşa, recién nombrado gobernador de Egipto, organizó una insurrección, y en consecuencia, obtuvo inicialmente el apoyo de los dignatarios mamelucos, el maestre de los Caballeros Hospitalarios de Jerusalén, el sah Ismail I de Persia e incluso el papa. Sin embargo, cuando los mamelucos lo abandonaron, pronto fue asesinado y la revuelta sofocada.[34]​ Consciente de que Egipto era un terreno fértil para la insurrección, Solimán decidió enviar a İbrahim, la única persona en la que podía confiar plenamente, en una misión.[35]​ En un año, el leal gran visir sometió a los rebeldes, promulgó leyes y reorganizó la administración del poder; su éxito fue tan grande que no hubo más revueltas en la zona durante más de tres siglos.[36]​
Tras las victorias en el Danubio y Rodas, el imperio disfrutó de un periodo de paz, pero esta situación no era de recibo, ya que la tradición guerrera otomana quería que el sultán estuviera constantemente luchando para expandir las fronteras imperiales y extender el Islam por todo el mundo. Cuando los jenízaros se dieron cuenta de que no había planes para nuevas campañas militares, aprovecharon la ausencia del sultán, quien había ido a Edirne para la cacería, y provocaron disturbios en la capital, lo que obligó a este último a regresar inmediatamente. Al recuperar rápidamente el control, ya sea ejecutando él mismo a algunos alborotadores o distribuyendo dinero a los soldados para calmarlos, Solimán tuvo claro que había que planificar una expedición militar lo antes posible.[37]​
Así, a principios del invierno de 1525, el sultán ordenó preparar una campaña militar sin haber decidido el objetivo. Sólo en los primeros meses de 1526, por invitación también de Francisco I de Francia con quien el imperio había comenzado a tejer los primeros contactos (que pronto darían lugar a una alianza antihabsbúrgica),[38]​[39]​ optó por atacar de nuevo las fronteras cristianas; la expedición se dirigiría, por tanto, hacia Hungría.[40]​ Los preparativos diplomáticos fueron febriles: se aseguró la neutralidad de la República de Venecia mediante la concesión de ciertos privilegios, mientras que estaba claro que el emperador Carlos V no intervendría porque estaba ocupado en la guerra de la Liga de Cognac contra Francisco I. Además, Solimán consideraba que las fronteras orientales con la Persia del sah Tahmasp I eran seguras y que su intervención allí podía posponerse.[41]​
El 21 de abril de 1526, Solimán abandonó la capital al frente de un ejército con cien mil hombres y trescientos cañones. Junto a él marchaban el gran visir İbrahim, algunos visires, el dragomán de la Sublime Puerta y otros dignatarios.[41]​ La expedición llegó, no sin dificultad, a Sofía donde se dividió: el sultán se dirigió a Belgrado desde donde continuaría a Buda (la actual Budapest), mientras que el gran visir se dirigió a Petrovaradin, que tomó tras un breve asedio que sólo le costó veinticinco hombres.[42]​ Durante la marcha, se sometieron las ciudades de Ilok y Osijek, y se construyó en sólo cinco días un puente de 332 metros de longitud sobre el Drava, que fue inmediatamente destruido una vez que el ejército lo hubo cruzado para eliminar cualquier posibilidad de retirada. Los otomanos llegaron entonces a la llanura de Mohács donde les esperaba el rey Luis II de Hungría y Bohemia para impedirles continuar hacia Buda.[42]​
La batalla de Mohács tuvo lugar el 29 de agosto y comenzó favorablemente para las tropas cristianas, a pesar de su inferioridad numérica, debido también a que las peticiones de ayuda de Luis a Occidente habían sido desatendidas. El gran valor de la caballería húngara salió a relucir, hasta el punto de que treinta y dos jinetes llegaron a poner en grave peligro la vida del propio Solimán, quien se salvó gracias a su armadura y al sacrificio de sus guardaespaldas y de los jenízaros que acudieron en su defensa.[43]​ Sin embargo, la clara superioridad de la artillería otomana marcó la diferencia, y al atardecer el ejército cristiano fue derrotado; en la retirada el rey Luis cayó muerto en un río, lo que lo sumó a los treinta mil muertos en las filas cristianas. Solimán había triunfado una vez más, el camino para una entrada triunfal en Buda estaba abierto.[38]​[44]​[45]​[46]​
Al regresar a Constantinopla al final de la campaña, el sultán podía considerarse dueño del destino de Hungría. La muerte del rey Luis colapsó la autoridad central húngara y se produjo una lucha por el poder. Algunos nobles ofrecieron la corona de Hungría al archiduque de Austria Fernando I de Habsburgo, emparentado con la familia real húngara. Otros nobles, sin embargo, se decantaron por Juan Zápolya, quien contaba con el apoyo de Solimán pero que no era reconocido por las potencias de la Europa cristiana.[47]​
Hungría se dividió en tres partes: la mayor parte de la actual Hungría fue reclamada por Solimán, se creó el estado vasallo de Transilvania y se entregó a la familia Zápolya, y Fernando I obtuvo la Hungría Real. Así, la frontera entre el Imperio otomano y el Sacro Imperio Romano Germánico quedó fijada temporalmente.[48]​[49]​
Con Hungría ya sometida, Solimán pudo dirigir su mirada a una de sus mayores ambiciones: Viena. Así, el 10 de mayo de 1529, con las habituales ceremonias con gran pompa, abandonó la capital y regresó a Mohács, donde se reunió con Zápolya, quien, en el transcurso de una solemne audiencia, fue reconocido como rey de Hungría y, en consecuencia, como vasallo del Imperio otomano. Una miniatura conservada en el palacio de Topkapı recuerda el acontecimiento, mostrando el momento en que el sultán entrega a Zápolya la corona, vestido con un caftán de honor. Abandonando el lugar de la que había sido una de sus victorias más importantes, Solimán llegó a Buda en sólo tres días, ocupada de nuevo por las tropas cristianas.[50]​ Tras un breve asedio, la ciudad fue tomada y sus habitantes esclavizados, pero a diferencia de la vez anterior, los jenízaros tenían prohibido el pillaje. Pocos días después, Zápolya fue coronado oficialmente como rey; Solimán no asistió a la ceremonia, probablemente para no dar demasiada importancia a quien consideraba sólo un vasallo menor; además, se acercaba el otoño y, por tanto, era necesario dirigirse hacia Viena lo antes posible.[51]​
El sitio de Viena comenzó el 27 de septiembre de 1529 y supuso un enfrentamiento del ejército otomano de Solimán, con ciento veinte mil hombres, veitiocho mil camellos y trescientas piezas de artillería, contra los defensores cristianos, que contaban con unos veinte mil combatientes y setenta y dos cañones a las órdenes de Felipe del Palatinado-Neoburgo.[52]​
Las operaciones resultaron más difíciles de lo esperado para los otomanos. Aunque el incesante fuego de artillería había conseguido abrir una brecha en las murallas de la ciudad, los atacantes inexplicablemente no aprovecharon esto para intentar entrar.[53]​[54]​ La frustración por la continuación del asedio más allá de lo esperado y la preocupación por la proximidad del invierno llevaron a Solimán a ordenar un asalto a la Puerta de Carintia el 14 de octubre. Sin embargo, el resultado no fue concluyente y el gran sultán no tuvo más remedio que abandonar el objetivo y regresar a Constantinopla, no sin antes celebrar la campaña militar como un éxito y negar que el objetivo fuera conquistar Viena. El viaje de vuelta duró aproximadamente dos meses, durante los cuales el ejército otomano perdió muchos hombres por las enfermedades y el mal tiempo.[55]​[56]​[57]​
A pesar de sus aplastantes victorias en Hungría, Solimán aún no había establecido plenamente su autoridad en la región. Así, el 22 de abril de 1532, volvió a salir de la capital al frente de más de cien mil hombres cuyas filas incluían doce mil jenízaros,[58]​ treinta mil soldados de Anatolia, dieciséis mil de Rumelia y veinte mil spahi a los que se añadía una fuerza de artillería de trescientos cañones.[59]​
Al llegar a Belgrado y recibir nuevos refuerzos de los tártaros de Sahib I Giray, Solimán estaba dispuesto a enfrentarse definitivamente al emperador Carlos V. Sin embargo, su ambición no pudo cumplirse: los cristianos no se sentían en absoluto preparados para enfrentarse a un enemigo tan decidido y numeroso, por lo que Fernando de Habsburgo prefirió enviar dos embajadores al sultán para ofrecerle un tributo de cien mil ducados a cambio de la paz y su reconocimiento como rey de Hungría. Mientras tanto, Solimán recibió otra propuesta diplomática: el rey de Francia se ofreció a invadir Italia para luchar allí contra Carlos. El sultán rechazó ambas ofertas, pero en cuanto a esta última, prometió ayudar a los franceses a conquistar Génova y Milán, uno de los primeros actos sustanciales de la alianza franco-otomana.[59]​
Al fracasar los esfuerzos diplomáticos para frenar el avance otomano, los cristianos tuvieron que ponerse a la defensiva. El ejército de Solimán, tras numerosos éxitos,[60]​ se paralizó durante el asedio de Güns, una ciudad situada a sólo cien kilómetros de Viena y defendida por sólo ochocientos hombres al mando de Nikola Jurišić, que se prolongó durante todo el mes de agosto, lo que hizo que los otomanos perdieran un tiempo valioso.[61]​[62]​ Después de conquistar trabajosamente la ciudad, Solimán prefirió dirigirse hacia el oeste, hacia Estiria, en lugar de apuntar directamente a Viena, probablemente porque había contado con expulsar a Carlos V de la ciudad y así enfrentarse a él en campo abierto, pero el emperador de los Habsburgo prefirió evitar el contacto y permaneció dentro de las murallas.[63]​
En Estiria, Solimán conquistó varias ciudades, pero tuvo que renunciar a la toma de Graz y Maribor, que no cedieron a la fuerza otomana. El tiempo perdido en Güns no le permitió continuar con las operaciones por mucho tiempo, y regresó a Constantinopla el 18 de noviembre de 1532. El resultado de esta campaña también se celebró solemnemente: se dice que hubo cinco días de ceremonias en la capital, pero el sultán no pudo quedar plenamente satisfecho.[64]​ Al término de los acontecimientos, se firmó una tregua entre los otomanos y los cristianos, que dio lugar al tratado de Constantinopla de 1533; con ello Zápolya conservó el reino de Hungría, Carlos V salvó sus fronteras y pudo concentrar sus fuerzas para contrarrestar la Liga de Esmalcalda. Solimán, por su parte, podía ahora volver su mirada hacia Persia, consciente de la oportunidad de romper la tregua con los cristianos cuando quisiera.[65]​
Durante siglos, en el mundo musulmán, hubo un conflicto entre los otomanos y el Imperio safávida, con este último gobernando Persia y el moderno Irak, dividido por la fe religiosa: suní el primero, chiita el segundo.[66]​[67]​ Los discípulos chiíes, o kizilbash', persiguieron a los suníes en Mesopotamia, convirtieron mezquitas y, en 1508, fueron culpables de destruir la tumba de Abū Ḥanīfa al-Nuʿmān, uno de los teólogos suníes más importantes de la historia. También solían impedir las conexiones entre los otomanos y sus aliados uzbekos. Esta situación exigía que los otomanos intervinieran lo antes posible. En cuanto el sultán tuvo un pretexto para reclamar Bagdad, decidió atacar al imperio rival e İbrahim recibió la orden, en otoño de 1533, de dirigir un ejército al Azerbaiyán persa donde conquistó Tabriz el 16 de julio del año siguiente. Había comenzado lo que se conocería como la «campaña de los dos Irak».[38]​[68]​[69]​
Dos meses después, tras un viaje que le hizo pasar por varias ciudades en triunfo, Solimán se unió a su gran visir y con él se dirigió a la capital de Persia, Bagdad, que durante mucho tiempo había sido uno de los centros más importantes del Islam y la sede del califato, pero que para entonces ya estaba en decadencia. El ejército que participó en esta expedición era enorme: se dice que estaba formado por unos doscientos mil hombres, lo que provocó muchas dificultades logísticas y de aprovisionamiento, dificultades acentuadas por la inminente mala temporada.[70]​[71]​
Al acercarse el ejército otomano, el sah rechazó un enfrentamiento directo y evacuó la ciudad; el 4 de diciembre de 1543, Solimán pudo así entrar sin luchar y considerarse así el legítimo sucesor de los califas y defensor del sunismo.[72]​[73]​[74]​ Con la anexión de Bagdad al Imperio otomano, la ciudad experimentaría una nueva temporada de crecimiento y prosperidad. Tras pasar el invierno en esta, el 2 de abril de 1545 Solimán y su ejército emprendieron un difícil viaje que en tres meses le llevó de vuelta a Tabriz, donde fijó su residencia en el palacio del sah.[75]​ Al igual que en la anterior campaña húngara, cuando Solimán había buscado un enfrentamiento directo con Carlos V, también esperaba un combate con el sah; pero él también, al igual que Carlos, había rehuido causar un impacto contra un ejército tan fuerte, por lo que prefirió una estrategia más de espera. Así, las conocidas dificultades logísticas del ejército otomano, que se encontraba lejos de sus bases de aprovisionamiento, le obligaron a abandonar la idea de perseguir al ejército del sah, mucho más móvil y a gusto en la región, y a conquistar las ciudades santas de Qom y Kashan.[76]​ Solimán dio entonces órdenes de regresar a Constantinopla, donde llegó a principios de enero de 1536. La campaña había supuesto la pérdida de más de treinta mil hombres, principalmente por hambre y enfermedades, pero había consolidado al sultán a los ojos de sus correligionarios contemporáneos como un gran conquistador que había triunfado sobre la herejía suní; el imperio se extendía ahora desde las puertas de Viena hasta Bagdad.[56]​[77]​[78]​
Pocos días después del final de la expedición, se produjo un acontecimiento que todavía hoy es cuestionado por los historiadores. El 15 de marzo de 1536, el gran visir Pargalı İbrahim Paşa fue encontrado asesinado en su dormitorio del palacio de Topkapı. Sin duda fue el propio Solimán quien ordenó su ejecución; se desconocen los motivos por los que lo hizo y sólo se han formulado teorías: la sospecha de que fue el artífice de una conspiración o el resultado de una petición de Roxelana, quien veía a İbrahim como un rival por el poder, son las hipótesis más creíbles. El hecho es que Solimán tuvo que lamentar haber hecho matar a su mejor amigo, compañero de éxitos y excelente estratega.[79]​[80]​[81]​[82]​[83]​
.
Tras las campañas danubianas y persas, Solimán comenzó a mirar hacia el Mediterráneo, entonces bajo el control de las armadas de la República de Venecia y de Ragusa.[84]​ A pesar de algunos éxitos como los de Rodas o Egipto, la flota otomana no era tan temible como sus fuerzas terrestres, pero la disponibilidad de puertos estratégicamente situados y de arsenales eficientes proporcionaba a los otomanos la posibilidad de imponer su autoridad también en el mar.[85]​ La figura clave en la reorganización de la armada otomana fue, al igual que el coetáneo Andrea Doria para los españoles, el corsario Jeireddín, conocido en Europa como «Barbarroja». Hijo de un alfarero griego de Mitilene,[86]​ gracias a sus incursiones en el Mediterráneo, pronto adquirió una gran reputación que le llevó a finales de 1533 a hacer su entrada triunfal en Constantinopla donde fue nombrado kapudanpaşa (equivalente a gran almirante) de la flota otomana y gobernador (Bey) de las islas. En virtud de este prestigioso título, organizó la flota de Solimán en pocos meses, mientras este se encontraba ocupado en Persia, dispuesto a dar batalla a las potencias navales cristianas.[87]​[88]​
Como primera acción, Barbarroja conquistó Túnez, en ese momento en manos de la dinastía hafsí.[89]​ Sin embargo, en julio de 1535, la ciudad sería a su vez conquistada por Carlos V quien, preocupado por la expansión otomana en el Mediterráneo, había tomado personalmente la dirección de una contraofensiva considerada por los contemporáneos como una cruzada junto a Doria.[90]​[91]​ A pesar de la gran victoria cristiana, la flota de Barbarroja sólo sufrió pérdidas marginales y pudo, unos meses después, atacar las Islas Baleares y Valencia, saqueándolas. Convencido de las posibilidades de éxito en el mar, el propio sultán supervisó el equipamiento de una nueva flota, que continuó a lo largo de 1536 en los astilleros del Cuerno de Oro.[92]​[93]​
El 17 de mayo de 1537, Solimán, acompañado de sus hijos Selim y Mehmed, llegó a Vlora, en la actual Albania, con planes para atacar Bríndisi, mientras su aliado Francisco I de Francia se dirigía a conquistar Génova y Milán. La expedición debía incluir a Lütfi Paşa y a Barbarroja como comandantes de la flota. Sin embargo, el plan no llegó a buen puerto, ya que el rey de Francia rompió el acuerdo y atacó en Flandes y Picardía. Mientras tanto, un ataque de unas galeras venecianas a una delegación diplomática otomana en viaje dio a Solimán el pretexto para cambiar su objetivo y dirigirse a Corfú, una importante base de la Serenísima.[94]​
Las operaciones para el asedio de la ciudad comenzaron en agosto de ese mismo año y fue el primer acto de lo que se conoce como la tercera guerra turco-veneciana, parte del escenario más amplio de las guerras turco-venecianas. A pesar de que Solimán podía contar con la fuerza de veinticinco mil hombres y trescientos veinte barcos, la resistencia de los venecianos resultó ser bastante tenaz, y en particular la artillería de los defensores fue decisiva para que el sultán decidiera retirarse. La tradición cuenta que Solimán, tras ver la pérdida de dos galeras, declaró que «la vida de un solo musulmán no puede pagarse con la conquista de mil fortalezas».[95]​[96]​
A su regreso a Constantinopla, el sultán encargó a Barbarroja la misión de combatir a los venecianos en las islas del mar Egeo para expulsarlos. Durante las dos campañas militares que siguieron, el kapudanpaşa logró un éxito tras otro, por lo que contó entre sus conquistas las islas de Siros, Patmos, Egina, Paros, Andro, Scíathos, Esciros y Serifos. El 28 de septiembre de 1538, Barbarroja derrotó a la Liga Santa promovida por el papa Pablo III en la batalla de Préveza.[97]​ Al final de las operaciones, veinticinco islas venecianas habían caído o habían sido saqueadas, mientras que miles de cristianos habían sido hechos prisioneros.[98]​ El 20 de octubre de 1540, se firmó una paz en la que los embajadores venecianos acordaron con Solimán el pago de trescientos mil ducados como indemnización de guerra y el abandono definitivo de las islas conquistadas por Barbarroja, a las que se sumaron Nauplia y Malvasia.[99]​
En octubre del año siguiente, Carlos V intentó frenar los éxitos otomanos dirigiendo una expedición a Argel, que sin embargo resultó desastrosa para los cristianos.[100]​[101]​ Solimán se convirtió así en el señor absoluto del Mediterráneo, una supremacía que los otomanos mantuvieron durante más de treinta años.[102]​
En 1539, el gran visir Ayas Mehmed Bajá, quien había sucedido en el cargo más alto de la Sublime Puerta tras el asesinato de İbrahim Paşa, murió de peste, y el sultán decidió nombrar en su lugar a Luṭfī Paşa, su cuñado y general del ejército otomano, quien era de origen albanés.[103]​
El sultán no había tenido tiempo de firmar el tratado de paz con Venecia y ya tenía que volver a centrar su atención en Hungría, de donde llegaban noticias preocupantes. En julio de 1540, Zápolya había muerto quince días después de tener un hijo de Isabela Jagellón, hija del rey Segismundo I de Polonia. Aunque Solimán creía que el niño era su legítimo sucesor, salió a la luz un acuerdo secreto que Zápolya había hecho en 1538, en el que se establecía que a su muerte Hungría volvería a manos de Fernando de Habsburgo a cambio de su reconocimiento en el trono mientras viviera. Para Solimán, este acuerdo era inaceptable, ya que se hizo antes del nacimiento de su hijo, mientras que Fernando afirmaba que el niño no era hijo de Isabel y por ello invadió Hungría.[104]​
Así, Isabel y su hijo Juan abandonaron Buda para pedir ayuda a Solimán, quien tuvo que partir de nuevo en dirección a Hungría. Esta vez el ejército otomano encontró poca resistencia en su camino y, reforzado por las tropas enviadas por Isabel, sitió Buda, y en consecuencia, derrotaron a las defensas dirigidas por Wilhelm von Roggendorf y expulsaron a los imperiales, quienes dejaron dieciséis mil hombres atrás.[105]​ El 2 de septiembre de 1541, el sultán acompañado por su hijo Şehzade Bayezid y el gran visir Hadım Suleiman Paşa (quien había tomado el relevo de Luṭfī Paşa unos meses antes) entró en la ciudad con grandes honores y, como era su costumbre, se dirigió a una iglesia que entretanto se había convertido en una mezquita para rezar. Primero dio su palabra a Isabel de que su joven hijo Juan reinaría sobre Hungría en cuanto alcanzara la edad necesaria, y luego regresó a Constantinopla.[106]​[107]​
Pero no pasaron ni dos años antes de que tuviera que partir de nuevo para su octava campaña en Hungría. De hecho Fernando, fortalecido por un momento de tranquilidad en el frente interno con los protestantes, había vuelto a reclamar el trono húngaro ofreciendo a Solimán el reconocimiento como rey a cambio de un tributo anual de cien mil ducados. El sultán, quien no tenía la menor intención de aceptar esta oferta, no tuvo más remedio que abandonar la capital el 23 de abril de 1543 y emprender la guerra contra los Habsburgo.[108]​ Una vez más, la campaña comenzó de la mejor manera posible para los otomanos, quienes tomaron una fortaleza tras otra en Eslavonia y en la Hungría leal a Fernando.[108]​ Después de volver a entrar en Buda, el ejército otomano se dirigió inmediatamente al norte para el sitio de Esztergom, que cayó después de unas dos semanas, también gracias a la ayuda de la artillería francesa ofrecida por el rey Francisco I.[109]​
Este nuevo éxito contra los cristianos puso fin a las ambiciones de Fernando sobre Hungría, que permaneció en manos otomanas hasta 1686. A este éxito le siguieron largas negociaciones que desembocaron en un tratado de paz entre los otomanos y el Sacro Imperio Romano Germánico el 13 de junio de 1547. Sin embargo, Solimán no podía estar del todo satisfecho, ya que aún no había conseguido luchar contra el emperador Carlos V en campo abierto, y la muerte de Francisco I el 31 de marzo de 1547 lo hacía improbable para el futuro, ya que le faltaba un aliado clave en el escenario europeo. Era el momento de mirar de nuevo a Persia.[110]​
A principios de 1548, Solimán abandonó de nuevo Constantinopla y marchó con su ejército hacia Persia. A pesar de su éxito en su primera expedición, su autoridad estaba en peligro: varios vasallos se habían mostrado infieles y la propaganda safávida a favor del chiismo seguía ganando adeptos en Anatolia.[111]​
La primera acción, una vez que llegó al teatro de operaciones, fue el asedio de Van, que había vuelto recientemente a manos de Tahmasp I tras perderlo en 1534. Tras tomar Van a finales de agosto, el sultán decidió reparar en Alepo para pasar el invierno. Al mismo tiempo, el ejército otomano ocupó otras fortalezas locales mientras el hijo del sah Suleiman Mirza, quien se había unido a las filas de los otomanos, asolaba el oeste de Irán hasta que fue capturado y posteriormente asesinado.[112]​ Después del invierno, el ejército turco se dirigió hacia Erzurum mientras el gran visir Rüstem Bajá continuaba sometiendo las fortalezas safávidas entre Kars y Artvin.[112]​
La campaña fue un éxito que reafirmó la autoridad otomana sobre Persia, pero Solimán volvió a fracasar en su intento de cerrar definitivamente las cuentas con la dinastía safávida, lo que le obligó unos años más tarde a retomar la ruta hacia Persia para una tercera campaña.[113]​
Solimán tuvo seis hijos de sus dos esposas, Mahidevran y Hürrem, cuatro de los cuales sobrevivieron más allá de 1550: el mayor Şehzade Mustafa era hijo de Mahidevran, mientras que los más jóvenes Selim, Bayezid y Cihangir eran de Hürrem. Esta última era consciente de que si Mustafá se convertía en sultán, sus propios hijos serían asesinados por estrangulamiento, una costumbre de la dinastía otomana. De hecho, al menos hasta el reinado de Ahmed I, el imperio carecía de medios formales para regular la sucesión, por lo que a menudo recurría a la cruel práctica de dar muerte a los príncipes rivales para evitar disturbios y rebeliones. Además, el hijo de Mahidevran era unánimemente reconocido como el más talentoso de todos sus hermanos y gozaba del apoyo del poderoso Pargalı İbrahim Paşa, en ese momento el gran visir del imperio, así como del ejército y en particular de los jenízaros. El embajador austriaco Ogier Ghiselin de Busbecq señaló que «Solimán tiene entre sus hijos uno llamado Mustafá, quien es maravillosamente educado y prudente y está en edad de gobernar, ya que tiene 24 o 25 años; que Dios no permita nunca que un sarraceno de tal fuerza se acerque a nosotros», y continuó hablando de las «notables dotes naturales» de Mustafá. Se cree que Hürrem es responsable, al menos en parte, de las intrigas que se produjeron en torno al nombramiento de un sucesor en el sultanato; aunque era la esposa del sultán, no tenía ningún papel público oficial, pero esto no le impedía ejercer una fuerte influencia política. En un intento de evitar la ejecución de sus hijos, Hürrem probablemente utilizó su influencia sobre su marido para eliminar a quienes apoyaban el ascenso de Mustafá.[114]​[115]​[116]​
Así, en luchas de poder aparentemente instigadas por Hürrem, Solimán hizo asesinar a İbrahim y lo sustituyó por su yerno, Rüstem Bajá. En 1552, cuando comenzó la tercera campaña contra Persia y Rüstem fue nombrado comandante en jefe de la expedición, comenzaron las intrigas contra Mustafá. Rüstem envió a uno de los hombres de mayor confianza del sultán para que informara de que los soldados consideraban que había llegado el momento de poner a un príncipe más joven en el trono, ya que el sultán no había dirigido personalmente el ejército; al mismo tiempo, se extendieron rumores de que Mustafá había sido favorable a la idea. Irritado por ello y creyendo que Mustafá conspiraba contra él para obtener el trono, Solimán convocó a su hijo a su tienda en el valle de Ereğli el verano siguiente durante su regreso de la campaña en Persia, afirmando que «podría librarse de los crímenes de los que ha sido acusado y no tendría nada que temer si venía».[117]​
Mustafá se enfrentó a una elección: comparecer ante su padre a riesgo de ser asesinado o, si se negaba, ser acusado de traición. Al final, Mustafá optó por entrar en la tienda de su padre, confiando en que el apoyo del ejército le protegería. Busbecq, quien afirma haber recibido el relato de un testigo presencial, describió los últimos momentos de Mustafá. Cuando este entró en la tienda de su padre, los eunucos le atacaron mientras se defendía valientemente. Su padre, separado de la pelea sólo por las cortinas de lino de la tienda, se asomó a través de ella y «miró amenazantemente, con sus ojos llenos de ira y crueldad por su falta de coraje. Entonces, los verdugos, alarmados, redoblaron sus esfuerzos, arrojaron a Mustafá al suelo y, echándole la soga al cuello, lo estrangularon».[118]​[119]​ El pueblo otomano recibió negativamente la noticia del asesinato de Mustafá, los jenízaros acusaron a Solimán de haber «apagado el sol más brillante»; los poetas y escritores celebraron al joven príncipe al que dedicaron elegías y obras.[120]​[121]​[122]​
Se dice que Cihangir murió de pena pocos meses después de enterarse del asesinato de su hermanastro.[123]​ A los dos hermanos supervivientes, Selim y Bayezid, se les dio el mando en diferentes partes del imperio. Sin embargo, en pocos años estalló una guerra civil entre ellos, cada uno apoyado por fuerzas leales. Con la ayuda del ejército de su padre, Selim derrotó a Bayezid en Konya en 1559, lo que llevó a este último a refugiarse con los safávidas junto con sus cuatro hijos.[124]​ Tras los intercambios diplomáticos, el sultán exigió al sah Tahmasp I que Bayezid fuera extraditado o ejecutado. En 1561, a cambio de grandes cantidades de oro, el sah permitió que un asesino turco estrangulara a Bayezid junto con todos sus hijos, lo que allanó el camino para la sucesión de Selim al trono cinco años después.[125]​[126]​
Los trágicos acontecimientos que rodearon la sucesión se produjeron, como ya se ha dicho, con el telón de fondo de lo que se conocería como la tercera campaña persa, la última librada por Solimán. Las operaciones habían comenzado en 1552 cuando el gran visir Rüstem Bajá, al mando del ejército, se dirigió a Anatolia y llegó a Carmania donde se detuvo para pasar la mala temporada. Fue en esta ocasión cuando Rüstem comenzó su complot contra el príncipe Mustafá, quien ya se ha mencionado, lo que consiguió que el verano siguiente Solimán, para demostrar que seguía siendo el jefe del Imperio, también dirigiera un ejército para unirse al gran visir y participar en la campaña.[117]​
Tras la ejecución de su hijo Mustafá, el sultán se dirigió a Alepo para acuartelarse. La entrada en la ciudad, que ahora forma parte de Siria, fue triunfal y pomposa. El explorador inglés Anthony Jenkinson relató con gran detalle que la procesión era abierta por seis mil jinetes ligeros Sipahi, vestidos de rojo escarlata, seguidos por diez mil tributarios, vestidos de terciopelo amarillo, a su vez seguidos por cuatro capitanes, cada uno al frente de doce mil armados; luego dieciséis mil jenízaros vestidos de púrpura, mil pajes de honor y tres hombres a caballo blanco; después el sultán «en su resplandeciente majestad» seguido de los grandes dignatarios y cuatro mil jinetes armados cerrando la procesión.[127]​
En la primavera, después de enviar al sah Tahmasp I un ultimátum pidiéndole que volviera al sunismo, que fue recibido con una previsible respuesta negativa, Solimán inició operaciones militares que provocaron la devastación de toda la región de Ereván.[128]​ La violenta ofensiva otomana hizo que el sah pidiera el fin de las hostilidades; el sultán aceptó la oferta, lo que dio lugar a la paz de Amasya,[129]​ estipulada el 29 de mayo de 1555, en la que se redibujaron las fronteras entre los dos imperios con el sah reconociendo las conquistas otomanas pero en la que Solimán permitió a los peregrinos chiíes llegar con seguridad a los lugares sagrados del Islam bajo su control.[130]​ Además, el sah prometió que pondría fin a las incursiones y a la propaganda chií en Anatolia.[131]​ Como resultado de estos acuerdos, no hubo más combates en el frente persa durante más de veinte años.[78]​
Al mismo tiempo, Solimán también tuvo que ocuparse de sofocar una nueva rebelión, una de las más graves de su imperio. Un súbdito afirmó ser el príncipe Mustafá, quien había escapado del asesinato y estableció un nuevo gobierno ilegítimo en el norte de Anatolia, por lo que reclutó a muchos rebeldes. Los insurgentes fueron detenidos por la decisiva intervención del príncipe Bayezid, pero quedó claro que estos disturbios eran una muestra de la mala condición del campesinado, que estaba siendo exprimido por la inflación y los fuertes impuestos.[132]​ El imperio estaba experimentando una grave crisis monetaria causada por la llegada de dinero barato, a menudo falsificado, procedente de Europa.[133]​
En 1530, el emperador Carlos V había concedido a los Caballeros Hospitalarios, expulsados de Rodas unos años antes por Solimán, la isla de Malta donde se establecieron y tuvieron la oportunidad de restablecerse.[134]​ El diván otomano, preocupado por las incursiones cristianas contra su armada, decidió preparar una expedición para atacarlos en su nueva base. La planificación de la nueva campaña, en la que participaron todos los astilleros del Cuerno de Oro, fue larga y se prolongó hasta principios de 1565. Una vez preparada, la flota de más de doscientos barcos, incluidas 150 galeras en estado de guerra, se hizo a la mar bajo el mando de Lala Kara Mustafa Pasha. Sin embargo, los grandes preparativos no podían mantenerse en secreto y, por tanto, habían alertado a los Caballeros, quienes tuvieron tiempo de preparar sus defensas; la resistencia de la isla, dirigida por el Gran Maestre Jean Parisot de La Valette, pudo contar con ocho mil quinientos defensores de los cuales setecientos eran caballeros.[135]​
El 18 de mayo, comenzó el Gran Sitio de Malta y pronto la isla fue ocupada por los otomanos, quienes no pudieron tomar las fortalezas estratégicas de San Telmo y San Ángel.[136]​ A pesar de un incesante bombardeo, los otomanos no pudieron obtener la ventaja, y el 12 de septiembre se decidió retirar la flota a la capital. La derrota, la primera bajo el mandato de Solimán, costó a los turcos entre veinte mil y treinta y cinco mil hombres, según las fuentes.[137]​[138]​[139]​
En 1566, Solimán tenía 72 años, 46 de los cuales los había pasado gobernando el imperio; el anciano sultán sufría de gota que le obligaba a moverse en silla de ruedas. Maximiliano II de Habsburgo había sucedido a su padre Fernando I en el trono del Sacro Imperio Romano Germánico y dio nueva vida a las pretensiones de los Habsburgo sobre los países del Danubio, lo que reabrió la cuestión de Transilvania que había quedado aparcada durante algunos años tras las victorias otomanas.[140]​
El 1 de mayo de 1566, el sultán partió de Constantinopla al frente de uno de los mayores ejércitos que jamás había comandado, al parecer trescientos mil soldados equipados con una enorme artillería, para su decimotercera campaña militar. Llevaba más de una década sin participar personalmente en una expedición, y fue criticado por ello tanto por los visires como por el pueblo llano, ya que se consideraba un deber del líder otomano luchar continuamente para ampliar las fronteras del Islam contra los infieles. Al parecer, la campaña habría sido sencilla, sin necesidad probablemente de ningún combate.[141]​
Sin embargo, la enfermedad le había debilitado profundamente, ya no podía montar a caballo y se veía obligado a seguir a las tropas sólo gracias a un carruaje. Tras una marcha ralentizada por causas naturales, que duró 49 días, Solimán llegó a Belgrado y luego se dirigió a Zemun, donde fue recibido por Juan Segismundo con gran pompa.[142]​[143]​
Tras otorgarle el poder sobre los territorios entre Tisza y Transilvania, el sultán se dirigió a Szigetvár, que puso bajo asedio desde el 6 de agosto. Los combates continuaron hasta el 8 de septiembre, pero Solimán no pudo ver el final: murió en su tienda durante la noche del 5 al 6 de septiembre. Para evitar disturbios en el ejército, que seguía en campaña, el gran visir Sokollu Mehmet Bajá decidió mantener en secreto la noticia de la muerte del sultán, al menos hasta que su sucesor Selim llegara al féretro de su padre. En ese momento, el único príncipe que quedaba estaba en Kütahya, donde era gobernador, y tan pronto como fue informado, partió para unirse a Sokollu. Durante la espera, el gran visir impidió que todo el mundo entrara en la tienda donde se encontraba el cuerpo de Solimán y dio órdenes a las tropas como si vinieran del propio sultán fallecido.[144]​ El secreto se mantuvo incluso cuando el ejército marchó de vuelta a Constantinopla tras el fin de las operaciones en la región del Transdanubio Meridional. Sólo cuando la procesión estuvo cerca de Belgrado llegó Selim y entonces se pudo difundir la noticia.[145]​[146]​
El cuerpo embalsamado del sultán fue llevado a Constantinopla para ser enterrado junto a Roxelana en el mausoleo construido cerca de la Mezquita de Süleymaniye, mientras que su corazón, hígado y algunos otros órganos fueron enterrados en Turbék, a las afueras de Szigetvár, donde se erigió un cenotafio que se convirtió en lugar sagrado y de peregrinación. En una década se construyeron en las cercanías una mezquita y un hospicio sufí y se pusieron varias docenas de soldados a vigilar el lugar. Ningún historiador tiene constancia de la ceremonia fúnebre del gran sultán; debió de ser muy sencilla, como era habitual en el Islam.[147]​
El diplomático veneciano Bartolomeo Contarini dejó una de las primeras descripciones del aspecto de Solimán, en el momento en que se convirtió en sultán: «sólo tiene veinticinco [en realidad 26] años, es alto y delgado pero duro, con un rostro fino y huesudo. El vello facial es evidente, pero apenas. El sultán se muestra amable y de buen humor. Se dice que Solimán tiene un nombre apropiado, que le gusta leer, que está bien informado y que tiene sentido común».[148]​[149]​ Una imagen dibujada anterior es la de Alberto Durero en 1526 que, sin embargo, nunca conoció al sultán otomano, sino que se basó en descripciones de comerciantes venecianos que habían estado en Constantinopla. Sin embargo, la exactitud de la representación de Durero queda confirmada por las similitudes con el dibujo realizado por Hieronymus Hopfer; en ambos casos, el gran sultán aparece con un cuello alargado, nariz arqueada y orejas pequeñas; rasgos somáticos que se asemejan en gran medida al aspecto de su bisabuelo Mehmed II, tal y como lo transmitió del famoso retrato de Gentile Bellini.[148]​ Una descripción posterior, de sus años de madurez, lo describe como un «hombre juvenil, delgado y muy frágil, pero que cuenta con una mano muy fuerte y que puede disparar el arco mejor que nadie».[145]​
Por otro lado, Ogier Ghislain de Busbecq da una descripción de un viejo Solimán de los años de su entrada en Amasya (1555): «Aunque su rostro estaba triste, su expresión seguía inspirando una sensación de gran majestad. Su salud es buena si no fuera por su mala complexión, signo de alguna enfermedad secreta. Pero, al igual que las mujeres, puede soportar bien los estragos del tiempo. Se maquilla, sobre todo los días que despide a los embajadores».[131]​
El carácter del sultán es descrito por la mayoría como tranquilo y reflexivo, muy alejado del carácter irascible de su padre Selim I,[148]​ mientras que un diplomático veneciano en la corte otomana hacia 1530 afirma que «su carácter es colérico y melancólico y que no es muy bueno en el trabajo porque ha abandonado el imperio al gran visir sin cuyo consejo ni él ni otros miembros de la corte toman decisiones, mientras que İbrahim actúa sin consultar nunca al Gran Señor ni a nadie más».[145]​ De hecho, aunque sus contemporáneos europeos lo consideraron un gobernante de indudable grandeza, no dejaron de reconocer su, quizá excesiva, dependencia de su esposa Roxelana y de İbrahim Paşa, y de acusarlo de la muerte de sus propios hijos.[150]​
En cualquier caso, todos los comentaristas coinciden en describirlo como un «musulmán piadoso, inmune a todo fanatismo, tolerante con los cristianos» que garantizaba los derechos de los infieles (Dhimmi), pero también implacable contra lo que consideraba una herejía chiita.[14]​ Firmemente convencido de que contaba con el favor de Dios, recitaba fervientes oraciones antes de cada batalla importante y, en cuanto conquistaba una ciudad, acudía a una mezquita para dar gracias por la victoria. Cuando no estaba ocupado administrando, participaba en discusiones teológicas con los eruditos de la corte, estudiaba filosofía y libros sagrados, hasta el punto de que se han conservado ocho ejemplares del Corán copiados personalmente por Solimán.[145]​
Como buen musulmán, también demostró un estilo de vida sobrio, aunque no a la altura de los primeros sultanes. Los comentaristas informan con detalle de que su vajilla era estrictamente de porcelana, y sólo una vez utilizó recipientes preciosos, pero los juristas le reprendieron. En su vejez fue aún más cuidadoso, por lo que sólo utilizó loza. Incluso las bebidas alcohólicas, que había tolerado en su juventud, pasaron a estar prohibidas en su vida posterior.[150]​[151]​ No obstante, su imagen pública siempre estuvo rodeada de un aura de opulencia. Su corte observaba un ceremonial fastuoso que él mismo había desarrollado, y los embajadores que recibía contaban su asombro ante tanta riqueza. Cada vez que Solimán abandonaba la capital al mando de su ejército para embarcarse en una de las muchas campañas que marcaron su vida, se celebraban fastuosas fiestas. En tales ocasiones, el sultán era aficionado a vestirse con telas preciosas y a llevar hermosas joyas.[152]​
En la vejez, Solimán, cansado por la enfermedad y las constantes campañas militares, perdió parte del carácter guerrero que le había distinguido en su juventud; el noble Antonio Barbarigo proporciona un fresco del sultán en 1558: «Este señor tiene 66 años y ha reinado felizmente durante 32; es de estatura media, más duro que de otra manera, pálido, tiene grandes ojos negros y una nariz aguileña; es un caballero justo, benigno y muy religioso en su derecho, aunque de joven era belicoso y amante de la guerra, es claro, sin embargo, que ahora que es viejo desea la paz con todo príncipe, y nunca hará la guerra a nadie a menos que sea forzado a ello por aquellos con quienes guerrea o por las falsas persuasiones de sus ministros... Sabe que es señor de muchos países y reinos, y desea disfrutar de los que tiene en paz. Es muy aficionado a la historia, y lee continuamente las historias de Alejandro Magno y las de los persas. Este señor está muy aquejado de gota y por esta razón, por consejo de sus médicos, va todos los años a invernar a Adrianópolis...».[153]​
Aunque Solimán era conocido como «el Magnífico» en Occidente, para los otomanos era Kanuni Solimán o «el Legislador» (قانونی).[154]​[155]​ En esa época, la sharía, o Ley Sagrada, era la principal ley del imperio y, al ser considerada divina por el islam, ni siquiera el sultán tenía poder para cambiarla.[156]​ Sin embargo, un área distinta de la legislación, conocida como Kanun (قانون, derecho canónico), dependía únicamente de la voluntad de Solimán y abarcaba ámbitos básicos como el derecho penal, la tenencia de la tierra y la fiscalidad. Hizo recopilar todas las diversas sentencias emitidas por los nueve sultanes otomanos que le habían precedido y, tras eliminar las duplicidades y resolver las declaraciones contradictorias, cuidando de no violar las leyes fundamentales del Islam, publicó un único código legal. En este contexto, el sultán, apoyado por su gran muftí Ebussuud Efendi, intentó reformar la legislación para adaptarla a un imperio en rápida evolución. Cuando las leyes del Kanun alcanzaron su forma definitiva, el código de leyes pasó a llamarse kanun-i Osmani (قانون عثمانی), o «leyes otomanas». El código legal de Solimán duró más de trescientos años.[157]​[158]​[159]​
Prestó especial atención a la situación de los rayah, súbditos cristianos que trabajaban la tierra de los spahi. Su Kanune Raya, o Código de los rayas, reformó la ley que regulaba los gravámenes e impuestos a los que estaban obligados los rayas, lo que elevó su estatus por encima del de siervos, hasta el punto de que muchos siervos cristianos emigraron a territorios turcos para beneficiarse de esta reforma. El sultán también se preocupó de proporcionar protección a los judíos que residían en su imperio durante los siglos venideros: a finales de 1553 o 1554, a sugerencia de su médico y dentista favorito, el judío español Moses Hamon, el sultán emitió un firmán (فرمان) en el que denunciaba formalmente las difamaciones contra ellos. También promulgó nuevas leyes penales y policiales, en las que prescribía una serie de multas para delitos específicos, así como reducía los casos de ejecución o mutilación. En el ámbito de la fiscalidad, se cobraban impuestos, aunque juzgados como leves, sobre diversos bienes y productos, como los animales, la minería, los beneficios comerciales y los derechos de importación y exportación.[160]​
Durante su reinado, Solimán siempre se preocupó de trazar personalmente una estrategia general a seguir por el imperio, pero para la ejecución de estas directrices y la atención a los detalles se valió del visir (literalmente «el que decide»), o de altos dignatarios que actuaban como consejeros y ministros.[162]​ Estos componían el diwan (o dīvān), el órgano administrativo supremo del imperio con poderes prácticamente ilimitados. Normalmente había tres visires que administraban conjuntamente la política interior y exterior, así como el orden público, mientras que dos defterdar supervisaban una compleja organización de oficinas a las que se confiaba la gestión de las finanzas. Otro cargo importante era el nişancı, el calígrafo de la corte, quien se encargaba de autentificar los documentos poniendo la tughra del sultán y de comprobar las leyes que se promulgaban; dos kazasker administraban todo el sistema judicial. Si las circunstancias lo requerían, también podrían participar otros dignatarios. El diwan estaba presidido por el gran visir, hombre de confianza del sultán. Durante su largo reinado, hasta diez personas se turnaron en este cargo, por orden: Pîrî Mehmed Paşa (1518 - 1523), Pargalı İbrahim Paşa (1523 - 1536), Ayas Mehmed Bajá (1536 - 1539), Çelebi Lütfi Paşa (1539 - 1541), Hadım Suleiman Paşa (1541 - 1544), Rüstem Bajá (1544 - 1553) Kara Ahmed Bajá (1553 - 1555), Rüstem Bajá (1555 - 1561), Semiz Ali Paşa (1561 - 1565) y Sokollu Mehmet Bajá (1565 - 1579). Solimán, como todos los sultanes desde Mehmed II, no estaba directamente presente en la sala donde se celebraban las reuniones, pero aun así existía la posibilidad de que asistiera en secreto. En el diwan el Magnífico también solía recibir a los embajadores en ceremonias de gran pompa y circunstancia.[163]​[164]​
Para hacer valer sus derechos, tuvo que luchar contra un sinfín de adversarios. La fuerza de su sultanato se basaba en la función crucial del cuerpo de infantería de jenízaros (del turco yeni çeri, «nueva tropa»). Estos fueron reclutados a la fuerza entre los jóvenes cristianos, obligados en los primeros siglos del sultanato a ser célibes, y vinculados por la tradicional adhesión a la misma hermandad religiosa, la Bektashiyya. Los jenízaros, considerados la élite del ejército otomano (en la primera mitad del siglo XVI contaban con unos doce mil),[165]​ no podían tener otra ocupación o fuente de ingresos que los derivados de la profesión de las armas, y su inactividad en tiempos de paz aumentaba el riesgo de disturbios. La necesidad de mantenerlos ocupados puede ayudar a entender por qué las campañas militares otomanas fueron tan frecuentes y por qué la primera década de su reinado fue, en consecuencia, un período de intensa actividad bélica.[166]​
Solimán también dio un fuerte impulso al fortalecimiento de la flota militar otomana, que hasta entonces no estaba especialmente desarrollada. La disponibilidad de recursos prácticamente ilimitados, de puertos seguros en posiciones estratégicas y de arsenales eficientes permitió alcanzar resultados impensables con gran rapidez, hasta el punto de que los otomanos pronto llegaron a controlar el Mediterráneo. En Gálata, en el Cuerno de Oro, estaba el mayor centro de producción de la flota con sus más de cientoveinte refugios cubiertos con capacidad para albergar cada uno dos galeras disponibles en 1550,[167]​ mientras que diez años después había quince mil trabajadores en el puerto.[168]​ El desarrollo de la flota fue tan rápido que para el asedio de Rodas los otomanos pudieron contar con cien buques, mientras que para el Gran Sitio de Malta en 1566 se dispuso del doble.[169]​
Las primeras décadas de su sultanato coincidieron con un periodo muy favorable para la economía de sus súbditos. El modelo económico adoptado entre los otomanos en aquella época era muy diferente al occidental, acercándose este al capitalismo. En el Imperio otomano existía una gestión mucho más estricta de la economía, que se mantuvo hasta finales del siglo XIX, centrada en la satisfacción de las necesidades del Estado y de la población más que en la búsqueda del enriquecimiento personal. Para ello, el gobierno central controlaba las actividades de producción y distribución con una autoridad inflexible, a través de una serie de funcionarios, y fijaba los precios para proteger al consumidor.[170]​
Cualquier trabajador del imperio se clasificaba en esnaf (similar a las corporaciones occidentales) de modo que en el siglo XVI había hasta un millar sólo en Constantinopla. Estos, agrupados por turnos, estaban dirigidos por un jefe y un comité, todos ellos elegidos por los afiliados, y supervisaban el control de los precios, regulaban el acceso a la profesión y controlaban la distribución de las materias primas. Estas instituciones se inspiraron en las costumbres de los futuwwa (hermandad religiosa) que se remonta a la época de los abasíes y se desarrolló bajo el sultanato de Rum y entre los principados de Anatolia.[171]​[172]​
El comercio estaba muy desarrollado y las rutas de suministro se extendían por todo el imperio y más allá. La carne, los cereales, la madera, la miel y los metales de Bulgaria y Rumanía llegaban a la capital; Anatolia suministraba cereales, fruta y caballos, mientras que el arroz, el azúcar, el algodón y grandes suministros de trigo procedían de Egipto. El café, las especias y los caballos se importaban de otros países árabes orientales, las sedas y las alfombras persas se importaban de Persia, y las piedras preciosas del Lejano Oriente. Europa, y en particular la República de Venecia, exportaba al Imperio otomano productos manufactureros, tecnológicos y de lujo.[173]​[174]​ El imperio, por su parte, exportaba materias primas y especias principalmente a Europa.[175]​
Para facilitar el comercio, Solimán construyó carreteras y puentes, algunos de los cuales seguían en uso a finales del siglo XX, y fomentó la renovación y ampliación de los caravasares ya construidos por los selyúcidas.[176]​ Las campañas militares emprendidas también tenían una finalidad comercial; en particular, las expediciones a Persia que permitieron abrir nuevas rutas comerciales hacia Asia y reforzar el comercio con los aliados uzbekos.[177]​
La educación era otro ámbito considerado muy importante por el sultán. Las escuelas anexas a las mezquitas y financiadas por fundaciones religiosas ofrecían en gran medida enseñanza gratuita a los niños musulmanes antes de que se hiciera en los países cristianos de la época. En la capital del imperio, Solimán aumentó a catorce el número de mektebs (مكتب, escuelas elementales), en las que se enseñaba a los niños a leer y escribir, así como los principios del Islam. Los jóvenes que deseaban una educación superior podían entonces ingresar en una de las ocho madrasas (مدرسه) cuyo plan de estudios incluía gramática, metafísica, filosofía, astronomía y astrología. Los cursos avanzados permitían una formación de nivel universitario, cuyos graduados se convertían en imanes (امام) o profesores. Los centros educativos solían ser uno de los muchos edificios que rodeaban los patios de las mezquitas; los demás eran bibliotecas, baños, cocinas, viviendas y hospitales en beneficio del pueblo. Asistir a un curso de estudios era un requisito esencial para aspirar a cualquier puesto de prestigio en el imperio. De estas instituciones culturales surgieron hombres de ciencia de gran valor que marcaron la vida científica del imperio a lo largo del siglo, como el matemático y astrónomo Taqi ad-Din Muhammad ibn Ma'ruf, quien en 1577 construyó el Observatorio de Constantinopla, que le permitió actualizar las tablas de Ulugh Beg del siglo anterior, y el geómetra Ali Ib Veli, quien se anticipó a los europeos en el estudio de los logaritmos.[178]​[179]​
Si a finales del siglo XVI el ambiente cultural otomano parecía variado y estimulante, y sólo en la capital había más de cien madrasas, esta situación favorable se desvaneció en los años siguientes, cuando los ulema comenzaron a interferir, y en consecuencia, prohibieron el estudio de las ciencias en favor de la enseñanza de una rígida visión religiosa que rayaba en el fanatismo.[180]​
Bajo el patrocinio de Solimán, el Imperio otomano entró en la cima de su desarrollo cultural. La sede imperial supervisaba cientos de sociedades artísticas (llamadas اهل حرف Ehl-i Hiref, «Comunidad de Artesanos») que tenían su sede en el Palacio Topkapı. Tras un periodo de aprendizaje, los artistas y artesanos podían ascender de categoría en su campo y recibían un salario acorde en cuotas anuales o trimestrales. Los registros de nóminas, que han sobrevivido hasta la actualidad, dan testimonio del alcance del mecenazgo artístico de Solimán;[181]​ por ejemplo, el más antiguo de los registros, que data de 1526, enumera cuarenta sociedades con más de seiscientos miembros. El Ehl-i Hiref trajo a la corte del sultán a los artesanos más talentosos del imperio, tanto a los procedentes del mundo islámico como a los originarios de los territorios recientemente conquistados en Europa, lo que formó una mezcla de culturas árabes, turcas y europeas.[182]​ Entre los artesanos que servían a la corte se encontraban pintores, encuadernadores, peleteros, joyeros y orfebres. Mientras que los gobernantes anteriores habían estado influenciados por la cultura persa (Selim I, por ejemplo, escribía poesía en persa), durante el periodo de Solimán se afirmó la identidad artística del imperio otomano.[183]​
Su reinado se considera una edad de oro para la literatura turca;[184]​ él mismo fue un poeta que escribió en persa y turco bajo el takhallus (seudónimo o nombre de pluma) Habibi (محبی, «el amante» o «el enamorado»).[185]​ Algunos de los versos de Solimán se convirtieron en proverbios turcos, como el famoso «Todos apuntan al mismo significado, pero muchas son las versiones de la historia». Un embajador veneciano comentó que el sultán: «se deleita en componer en alabanza a Dios, haciéndose humilde y diciendo siempre que no es nada; pero para dejar constancia de su grandeza, hace una crónica de todo lo que ha hecho».[186]​ Cuando su joven hijo Mehmed murió en 1543, compuso un emotivo cronograma para conmemorar el año: «incomparable entre los príncipes, mi sultán Mehmed». Además de la obra del sultán, muchos grandes talentos animaron el mundo literario durante su reinado, como Fuzûlî y Bâkî.[187]​ El historiador literario Elias John Wilkinson Gibb observó que «en ninguna época, incluso en Turquía, se dio mayor fomento a la poesía que durante el reinado de este sultán». [188]​
Solimán también es famoso por financiar la construcción de numerosos monumentos y fomentar el desarrollo arquitectónico dentro de su imperio. A través de una serie de proyectos, el sultán trató de transformar Constantinopla en el centro de la civilización islámica haciendo construir puentes, mezquitas, palacios y diversas instituciones benéficas y sociales. El mayor de estos fue construido por Mimar Sinan, arquitecto principal del sultán, gracias al cual la arquitectura otomana alcanzó su cenit.[189]​ Sinan llegó a ser responsable de la construcción de más de trescientos monumentos en todo el imperio, que incluían sus dos obras maestras, la mezquita de Suleymaniye y la mezquita de Selimiye, esta última construida en Adrianópolis (actual Edirne) durante el reinado de su hijo Selim II. Solimán también restauró la Cúpula de la Roca en Jerusalén y las murallas (las actuales de la ciudad vieja de Jerusalén), renovó la Kaaba en La Meca y construyó un complejo en Damasco.[190]​[191]​
A la muerte de Solimán, el Imperio otomano era una de las principales potencias del mundo.[192]​ Las conquistas del Magnífico habían ampliado sus fronteras hasta incluir Bagdad, muchos territorios balcánicos, que incluían las actuales Croacia y Hungría, y la mayor parte del norte de África. La expansión del imperio en Europa había dado a los otomanos la posibilidad de influir directamente en el equilibrio de poder en el Occidente cristiano, hasta el punto de que el embajador austriaco Ogier Ghislain de Busbecq advirtió de la inminente conquista de Europa: «Por parte [de los turcos] están los recursos de un poderoso imperio, la fuerza ininterrumpida, el hábito de la victoria, la resistencia al trabajo, la unidad, la disciplina, la frugalidad y la vigilancia ... ¿Podemos dudar de cuál será el resultado? ... Cuando los turcos se asienten con Persia, se lanzarán a nuestras gargantas respaldados por el poderío de todo Oriente; no me atrevo a decir lo poco preparados que estamos».[193]​
Su legado, sin embargo, no fue sólo de carácter militar; un siglo después, el viajero francés Jean de Thévenot hablaba de la «fuerte base agrícola del país, el bienestar de los campesinos, la abundancia de alimentos básicos y la preeminencia de la organización en el gobierno de Solimán».[194]​ Treinta años después de su muerte, el famoso dramaturgo inglés William Shakespeare lo menciona como un prodigio militar en El Mercader de Venecia, donde el Príncipe de Marruecos se jacta de su destreza diciendo que derrotó a Solimán en tres batallas.
Sin embargo, la valoración de la obra del «magnífico» sultán, de sus logros en el ámbito administrativo, cultural y militar, debe tener en cuenta también la contribución fundamental de las numerosas figuras de talento que le sirvieron, como los grandes visires İbrahim Paşa y Rüstem Bajá, el gran muftí Ebussuud Efendi, quien desempeñó un importante papel en la reforma legal, y el canciller y cronista Celalzade Mustafa, quien fue decisivo en el desarrollo de la burocracia y en el establecimiento del mito del Magnífico.[195]​



El albedrío (de la deformación vulgar del vocablo latino arbitrium,[1]​ a su vez de arbiter, ‘juez’[2]​), libre albedrío o libre elección es la creencia de aquellas doctrinas filosóficas según las cuales las personas tienen el poder de elegir y tomar sus propias decisiones. Muchas autoridades religiosas han apoyado dicha creencia,[3]​ mientras que ha sido criticada como una forma de ideología individualista por pensadores tales como Baruch Spinoza, Arthur Schopenhauer, Karl Marx y Friedrich Nietzsche.
El principio del libre albedrío tiene implicaciones religiosas, éticas, psicológicas, jurídicas y científicas. Por ejemplo, la ética puede suponer que los individuos son responsables de sus propias acciones. En la psicología, implica que la mente controla algunas de las acciones del cuerpo, las cuales son conscientes.[cita requerida]
La existencia del libre albedrío ha sido un tema central a lo largo de la historia de la filosofía y de la ciencia. Se diferencia de la libertad en el sentido de que conlleva la potencialidad de obrar o no obrar.[4]​
Existen varios puntos de vista sobre si la libertad metafísica existe, eso es, si las personas tienen el poder de elegir entre alternativas genuinas.[5]​
El determinismo es el punto de vista según el cual todos los eventos son resultados inevitables de causas previas, de que todo lo que pasa tiene una razón de ser.
El incompatibilismo es el punto de vista según el cual no es posible reconciliar una creencia en un universo determinista con el verdadero libre albedrío. El determinismo duro acepta tanto el determinismo como el incompatibilismo, y rechaza la idea de que los humanos poseen un libre albedrío.
Lo contrario a esto es el libertarismo[6]​ filosófico, que mantiene que los individuos tienen libertad metafísica y por lo tanto rechaza el determinismo. El indeterminismo es una forma del libertarismo que, según su punto de vista, implica que el libre albedrío realmente existe, y esa libertad hace que las acciones sean un efecto sin causa. La teoría de la agencia es una forma del libertarismo que mantiene que la elección entre el determinismo y el indeterminismo es una dicotomía falsa. Antes que voluntad, es un efecto sin causa. La teoría de la agencia sostiene que un acto de libre albedrío es un caso de agente-causalidad, por lo cual un agente (persona, el ser) causa un acontecimiento. Es una filosofía separada de la teoría económica y política del libertarismo. El libertarismo metafísico se llama a veces voluntarismo para evitar esta confusión.
El compatibilismo[7]​ es el punto de vista que sostiene que el libre albedrío surge en el exterior de un universo determinista aun en ausencia de incertidumbre metafísica. Los compatibilistas pueden definir al libre albedrío como el surgimiento de una causa interior, tal como los pensamientos, las creencias y los deseos que uno piense que existen en uno mismo. La filosofía que acepta tanto el determinismo como el compatibilismo se llama el determinismo suave.
El determinismo sostiene que cada situación se condiciona íntegramente y así es determinada por los estados de los propósitos que la precedieron.[8]​ Una formulación del determinismo es el fatalismo, donde se afirma que existe una necesidad (destino) que rige todo, negando la libertad. Esta posición la sostenían los estoicos y puede registrarse en Parménides de Elea.[9]​[10]​[11]​ El determinismo filosófico es ilustrado a veces por la idea del demonio de Laplace, que conoce todos los hechos acerca del pasado y presente y todas las leyes naturales que gobiernan el mundo y utiliza este conocimiento para prever el futuro hasta el más mínimo detalle. El pensamiento de Laplace, sin embargo, no representa la postura científica actual acerca del tema.[12]​ 
Por otro lado, el indeterminismo especula que esta proposición es incorrecta, ya que hay acontecimientos que no son determinados enteramente por acontecimientos previos. Los pitagóricos, Sócrates y Platón intentaron conciliar la libertad con el determinismo y la ley causal. Aristóteles fue uno de los primeros en argumentar a favor del indeterminismo.[14]​ Lucrecio afirmaba que el libre albedrío surge del azar, de movimientos caóticos de átomos llamados "clinamen".[13]​[15]​ Entre indeterministas modernos se encuentran Robert Nozick,[16]​ y Robert Kane.[17]​
El incompatibilismo mantiene que el determinismo no se puede reconciliar con el libre albedrío. Los incompatibilistas generalmente afirman que una persona actúa libremente solo cuando ésta es la única que origina la causa que desencadena una acción y que podría haber terminado auténticamente de otra manera. Ellos mantienen que si el determinismo es verdad, cada elección es determinada por acontecimientos previos.[8]​
Joe Campbell en su libro Free Will, describe el problema del libre albedrío como el "dilema del libre albedrío":[18]​ 
Sam Harris expresa en su corto ensayo también titulado Free Will que cada elección que hacemos, se hace como resultado de causas que "están determinadas por causas anteriores y no somos responsables de ellas, o son producto de la casualidad y no somos responsables de ellas".[19]​[20]​
Hay un punto de vista intermedio en que las condiciones pasadas podrían tener influencia, pero no determinan las acciones futuras. Las elecciones individuales son un resultado entre muchos resultados posibles, todos los cuales son inducidos pero no son determinados por el pasado. Incluso si el agente del albedrío se esfuerza espontáneamente en escoger entre las acciones disponibles, propiamente el agente no es el que origina la causa de la acción, porque nadie puede realizar las acciones que son imposibles, tal como volar con solo batir los brazos. Aplicado a estados interiores, esta perspectiva sugiere que uno puede escoger entre las opciones en que uno piensa, pero no puede escoger una opción inverosímil de realizar. Según esta opinión, las elecciones actuales pueden iniciar, determinar o limitar las elecciones futuras.
Duns Scoto sostuvo que "nada otro que la voluntad es la causa total de la volición de la voluntad".[21]​ Dios como concibe Scoto, es la libertad absoluta.[22]​ René Descartes argumentó sobre la relación mente y cuerpo, llamada dualismo cartesiano, en donde la mente es una substancia distinta de la materia, el cuerpo, siendo inmaterial y estando fuera de toda ley física. Creía que la mente ejercía control sobre el cuerpo a través de la glándula pineal. Cómo funciona esa interacción sigue siendo un tema polémico (ver Problema mente-cuerpo y Mecanicismo). Gottfried Leibniz sostuvo que las mentes humanas son reflejos de Dios o pequeños dioses, como motores inmóviles, con independencia causal y la capacidad de pervivir el mundo. Esta doctrina se encuentra en el neoplatonismo y la doctrina del Génesis.[23]​ Por otro lado, Nicolas Malebranche sostuvo que toda acción es causa de Dios, incluso nuestras acciones (ver Ocasionalismo). 
Baruch Spinoza comparó la creencia del hombre en el libre albedrío con una piedra que piensa que escogió el sendero al cual llegó por el aire y el lugar en el cual aterrizó. En la Ética escribió: "Las decisiones de la mente no son nada salvo deseos, que varían según varias disposiciones puntuales". "No hay en la mente un absoluto libre albedrío, pero la mente es determinada por el desear esto o aquello, por una causa determinada a su vez por otra causa, y ésta a su vez por otra causa, y así hasta el infinito". "Los Hombres se creen libres porque ellos son conscientes de sus voluntades y deseos, pero son ignorantes de las causas por las cuales ellos son llevados al deseo y a la esperanza".[24]​[25]​
Arthur Schopenhauer, estando de acuerdo con Spinoza, escribió: "Todos creen a priori que son perfectamente libres, aun en sus acciones individuales, y piensan que a cada instante pueden comenzar otro capítulo de su vida... Pero a posteriori, por la experiencia, se dan cuenta —a su asombro— de que no son libres, sino sujetos a la necesidad; su conducta no cambia a pesar de todas las resoluciones y reflexiones que puedan llegar a tener. Desde el principio de sus vidas y al final de ellas, deben soportar el mismo carácter...”.[26]​ No obstante, Schopenhauer afirmó en El mundo como voluntad y representación que los en el mundo (fenómenos) no tienen libertad, pero la voluntad (como noúmeno) no está subordinada a las leyes de la necesidad (causalidad) y, por tanto, es libre.[27]​ En sus obras Los dos problemas fundamentales de la ética también critica la noción de libre albedrío:
Friedrich Schiller propuso una coyuntura a este dilema en su Educación estética del Hombre en una serie de Cartas, que fue ahondada aún más por Rudolf Steiner en su Filosofía de la Libertad. Ambos sugieren que el individuo es inicialmente 'no libre; esto se debe a que el individuo actúa con base en principios religiosos, éticos y morales, o que aún son racionales.
Los "deterministas duros", tales como d'Holbach, son esos incompatibilistas que aceptan el determinismo y rechazan el libre albedrío. Dijo: “…la Naturaleza no es más que una cadena inmensa de causas y efectos [...] Todos los movimientos que se efectúan en ella siguen leyes constantes y necesarias [...] La voluntad del hombre es movida o determinada secretamente por causas exteriores que producen un cambio en él; creemos que se mueve por sí misma porque no vemos la causa que la determina [...]”.[28]​ Los "libertarios", tales como Thomas Reid, Peter van Inwagen,[29]​ son esos incompatibilistas que aceptan el libre albedrío y niegan el determinismo, teniendo en cuenta que alguna forma del indeterminismo es verdad. El incompatibilista más conocido de la historia de la filosofía fue Immanuel Kant.[30]​
Otros filósofos sostienen que el determinismo es compatible con el libre albedrío. La noción de libre albedrío "compatibilista" se ha atribuido tanto a Aristóteles como a Epicteto.[31]​ Crísipo, influenciado por Aristóteles es considerado el primer compatiblista.[31]​[32]​ Estas personas, tales como Thomas Hobbes, generalmente aclaman que una persona actúa libremente si no encuentra un obstáculo en hacer lo que tiene en la voluntad de hacerlo por decisión propia.[33]​ Articulando esta cláusula elemental, Hume escribe que “esta libertad hipotética se aplica universalmente a cualquiera que no sea un prisionero encadenado”. Los "compatibilistas" apuntan con frecuencia a casos en donde la libertad de alguien es negada — violaciones, asesinatos, asaltos, y la lista continúa. La clave para estos casos no consiste en que el pasado esté determinando el futuro, sino en que el agresor está dominando sobre los deseos y preferencias de las acciones de la víctima. El agresor está forzando a la víctima, y, de acuerdo con los compatibilistas, esto es lo que domina sobre el libre albedrío. Además, argumentan que el determinismo no es lo que importa, sino el hecho de que las acciones de los individuos son el resultado de sus propios deseos y preferencias, sin estar dominados por alguna fuerza externa o interna. Para ser un compatibilista, uno no necesita endorsar alguna concepción particular del libre albedrío, sino aceptar que el determinismo está relacionado con este. El compatibilista más conocido de la historia de la filosofía fue David Hume.[34]​ Hoy en día esa posición es defendida, por ejemplo, por Daniel Dennett.[35]​
Otro punto de vista es que el concepto de “libre albedrío” es, como diría Hobbes, un “discurso absurdo”, porque la libertad es un poder definido en términos del albedrío, el cual es una cosa —y así la voluntad no es la clase de cosa que podría ser libre o no libre—. John Locke, en su  "Ensayo sobre el entendimiento humano",  indicó que eso de llamarse "libre" es comprometerse a un error de categoría:
Esta cuestión también se plantea si cualquier acto intencionado puede ser libre o cualquier acto sin intención puede estar relacionado con el albedrío, dejando la libertad como un oxímoron. Algunos compatibilistas argumentan que esta ambigüedad del concepto “libre albedrío” es en parte culpable de la percepción de contradicción entre el determinismo y la libertad. Así, desde un punto de vista compatibilista, el uso de “libre albedrío” en un sentido “incompatibilista" puede ser interpretado como lenguaje cargado.
La sociedad generalmente hace a la gente responsable por sus acciones y dirá que merecen premios o castigos por lo que hagan. Sin embargo, muchos creen que la responsabilidad moral requiere libre albedrío; en otras palabras, la habilidad de tomar distintas alternativas. Además, otro tema de importancia es si los individuos siempre son moralmente responsables y, de ser así, en qué sentido.
Aristóteles creía claramente que nuestras deliberaciones involucraban elecciones entre posibilidades alternativas, y esto implica tanto la posibilidad de hacer lo contrario. Su definición de la voluntad voluntaria como causada desde dentro de un agente (libertarismo) todavía hoy es válida. Aristóteles desafió a quienes dijeron que nuestras acciones están determinadas por nuestro carácter, ya que negaría la responsabilidad moral. Admitió que algunos aspectos de nuestro carácter pueden ser innatos y, por lo tanto, limitar nuestra responsabilidad, pero somos al menos parcialmente libres.[14]​ Siguiendo a Aristóteles, Epicuro pensó que los agentes humanos tienen la capacidad autónoma de trascender la necesidad y el azar. Su tertium quid es la autonomía del agente, lo que "depende de nosotros".[37]​ No obstante, Tim O'Keefe ha argumentado que Epicuro no eran libertario, sino compatibilista.[38]​
Los incompatibilistas tienden a pensar que el determinismo no está relacionado con la responsabilidad moral. Para Kant deber implica poder. Es decir, para afirmar que se debe realizar una acción, debe estar a mi alcance de realizarla, si no, no tendría sentido exigirla.[36]​ Después de todo, parece imposible que uno pueda llamar a alguien responsable por una acción que podía predecirse desde antes. Los deterministas duros pueden decir “Muy mal para la responsabilidad moral” y descartar el concepto —Clarence Darrow utilizó este argumento para defender a los asesinos Leopold y Loeb— mientras, controvertidamente, los libertinos podrían decir “Muy mal para el determinismo”. Este caso parece ser el corazón de la disputa entre los deterministas duros y los compatibilistas; los deterministas duros están forzados a aceptar que los individuos tienen con frecuencia “libre albedrío” en el sentido compatibilista, pero pueden negar que es este sentido de libertad el que realmente importa —que puede llegar en la responsabilidad moral—. Solo porque las opciones de un agente no sean coherentes, de acuerdo con los deterministas duros, no cambia el hecho de que el determinismo le quita la responsabilidad al agente.
Los compatibilistas argumentan con frecuencia que, por otro lado, el determinismo es un prerrequisito para la responsabilidad moral —la sociedad no puede considerar a alguien responsable a menos que sus acciones sean determinadas por algo—. Este argumento fue empleado por Hume y por el anarquista William Godwin. Después de todo, si el indeterminismo es cierto, entonces esos eventos no son determinados; son al azar. Una de las preguntas formuladas es si es posible culpar o castigar a una persona por llevar a cabo una acción que saltó espontáneamente a su sistema nervioso. Argumentan que uno necesita mostrar cómo la acción proviene de los deseos y las preferencias —el carácter de las personas— antes de que uno considere a la persona como responsable social. Los liberales podrán responder que las acciones indeterminadas no tienen relación con el azar y que resultan de un sustantivo albedrío en que sus decisiones serán indeterminadas. Este argumento es ampliamente considerado como no satisfactorio, ya que solo dificulta el problema y envuelve metafísica, así como el concepto Ex nihilo nihil fit.
San Pablo, en su Epístola a los romanos, plantea la siguiente cuestión sobre responsabilidad moral:
Desde esta perspectiva, los individuos todavía pueden perder su honor por medio de sus actos, aun cuando tales fueran determinados completamente por Dios.
Un punto de vista similar dice que la responsabilidad de la culpabilidad moral del individuo recae en el carácter individual. Eso significa que una persona con el carácter de un asesino no tiene otra alternativa más que matar, pero todavía puede ser castigado porque es un derecho castigar a las personas con mal carácter.
Algunas interpretaciones de responsabilidad moral también asumen que una persona es, desde que nace hasta que muere, extrínsecamente dependiente de sus cambios físicos y mentales. Así, Stanley Williams, de 52 años, fue ejecutado debido a un crimen que cometió cuando tenía 28 años.
El filósofo Isaías Berlín clamó que para tener opción de libertad el agente debería poder actuar de manera contraria. Este principio, que van Inwagen llama el “principio de las posibilidades alternativas”, dice ser un requisito para la libertad. Desde este punto, las acciones llevadas a cabo desde la influencia de una coerción irresistible no son libres y el agente no es moralmente responsable de ellas.
Sin embargo, algunos compatibilistas, así como Harry G. Frankfurt o Daniel Dennett, argumentan que hay casos en los que, aun cuando el agente no pudiera actuar de otra manera, su elección aún es libre, porque la coerción irresistible coincide con las intenciones y deseos personales del agente, así como el dicho “Ahora, pon la pistola en mi sien y oblígame a disparar”. En Elbow Room, Dennet presenta un argumento para la teoría compatibilista sobre el libre albedrío. La elaboró más adelante en el libro de 2003 Freedom Evolves. El razonamiento básico consiste en que, si los individuos no considerasen a Dios, o a un demonio infinitamente poderoso, o la capacidad de viajar en el tiempo, entonces habría caos y habría seudo-azar o un azar cuántico; el futuro se define en una enfermedad, compuesto por todos los seres finitos. Los únicos conceptos bien definidos son las “expectativas”. Además, la habilidad para hacer lo “contrario” solo tiene sentido cuando se manejan expectativas y no con un futuro totalmente desconocido. Desde que los individuos tienen la habilidad de actuar de una manera diferente a la que el resto espera, el libre albedrío puede existir. Los incompatibilistas aclaman que el problema con esta idea es que la herencia y la cantidad de coerción irresistible creada por el ambiente hacen que todas nuestras acciones sean controladas por fuerzas fuera de nosotros mismos, determinadas por el azar.
El filósofo John Locke negó que el término “libre albedrío” tenga sentido. Sin embargo, también afirmó que el determinismo era irrelevante. Creía que la capacidad de actuar voluntariamente consistía en que los individuos tienen la habilidad de posponer una decisión lo suficiente como para deliberar sobre las consecuencias de tomar o no esa alternativa. Se han ofrecido análisis más sofisticados de la libertad compatibilista, así como otras críticas. 
David Hume defendió el compatibilismo como necesario para la moral, pues necesitamos tanto la libertad como la necesidad.[34]​[39]​
William James, filósofo y psicólogo, etiquetó como determinismo leve a la posición actualmente conocida como compatibilismo, y argumentó que las formulaciones del determinismo leve eran “un dilema de evasión en el cual el verdadero asunto de importancia se ha difuminado completamente”. Pero los puntos de vista de James eran un tanto ambivalentes. Mientras creía en el libre albedrío en “campos éticos,” pensaba que no había evidencia para su existencia en campos psicológicos o científicos. Además, no creía en el incompatibilismo como se formuló anteriormente, en que el indeterminismo de las acciones humanas fuera un requisito para la responsabilidad moral. En su clásica obra Pragmatismo, publicada en 1907, escribió que “El instinto y sus utilidades pueden ser confiables para llevar los asuntos sociales de castigo y culpa” fuera de las teorías metafísicas. Creía que el indeterminismo es importante como una “doctrina de alivio” —permite creer que, aunque el mundo desde muchos puntos de vista sea un lugar malo, puede mejorar a través de las acciones de los individuos—. El determinismo, argumentó, indetermina ese meliorismo.
A lo largo de la historia, las personas han hecho intentos de responder a las preguntas del libre albedrío a través de principios científicos. La primera mentalidad científica muchas veces mostró al Universo como determinista, y muchos pensadores creían que era simplemente cuestión de recolectar suficiente información el poder predecir eventos futuros con perfecta precisión.
Esto motiva a los individuos a ver el libre albedrío como una ilusión. La ciencia moderna es una mezcla de teorías deterministas y estocásticas. Por ejemplo, la decadencia radioactiva ocurre con probabilidad predecible, pero no es posible, aún en teoría, decir exactamente cuándo un núcleo particular decaerá. La mecánica cuántica predice observaciones solo en términos de probabilidad. Esto coloca dudas sobre el determinismo del Universo. Algunos científicos deterministas como Albert Einstein creen en la teoría de la variable escondida; que por debajo de las probabilidades de la mecánica cuántica hay más variables (ver la paradoja EPR).
Esta teoría ha traído grandes dudas sobre sí misma, por las desigualdades de Bell, que sugieren que “Dios puede jugar a los dados en verdad” después de todo, quizás poniendo en duda las predicciones del demonio de Laplace. El filósofo contemporáneo más importante que ha capitalizado el éxito de la mecánica cuántica y la teoría del caos para defender la libertad incompatible es Robert Kane, en La importancia del libre albedrío y otros escritos. Los argumentos de Kane, aun así, se aplican perfectamente a cualquier entidad “impensable” que se comporte de acuerdo con la mecánica cuántica.
Como los físicos, los biólogos han cuestionado el libre albedrío. Uno de los debates más odiados de la biología es el de “lo innato y lo adquirido”. Este debate cuestiona la importancia de la genética y la biología en el comportamiento humano cuando se compara con la cultura y el medio ambiente. Los estudios de genética han identificado muchos factores genéticos que afectan la personalidad del individuo, como en casos obvios como el Síndrome de Down, a efectos más sutiles como una predisposición estadística hacia la esquizofrenia.
Aun así, no es certero que la determinación ambiental afecta menos el libre albedrío que la determinación genética. Los últimos análisis del genoma humano muestran que solo tiene veinte mil genes. Estos genes, y el reconsiderado material genético intrón, y la nueva MiRNA, permiten un nivel de complejidad análoga a la complejidad del comportamiento humano. Desmond Morris y otros antropólogos han estudiado la relación entre el comportamiento y la selección natural en humanos y otros primates.
La síntesis de estos dos campos de investigación es que la genética humana puede ser lo suficientemente compleja como para explicar tendencias del comportamiento y que los factores ambientales beneficiosos para la evolución, tales como el comportamiento de los padres y los estándares culturales, modifican estos factores genéticos. Ninguno de estos fenómenos, complejidad genética o desventajas en el comportamiento cultural, requieren del libre albedrío para explicar el comportamiento humano. Sin embargo, la presencia de los genes que juegan un papel en algunas conductas, como por ejemplo desórdenes mentales, no vuelve a un comportamiento automático, y los estudios sugieren que hay personas que sufren de una predisposición genética a ser más explosivos, pero el comportamiento violento no necesariamente se vuelve un rasgo en la conducta del individuo.
Parece que es necesario más de un gen, y un posible combustible ambiental para expresar el rasgo; esto sugiere que la naturaleza y la crianza juegan un importante papel en nuestro comportamiento. Algunos difieren y afirman que alguna forma de libre albedrío puede todavía existir, ya que el factor ambiental en el libre albedrío le permite a una persona manipular ese ambiente de manera tal que esta manipulación implique un compromiso entre su propio cuerpo y mente, porque una acción aislada no existe, una motivación parecida o comparable a ambos actos existe, y los factores genéticos permiten a esas dos o más acciones ser tomadas en cualquier situación o momento, pero solo a veces ese compromiso puede significar un evento que no es al azar, al menos en algunas instancias, el argumento tiende a implicar.
La parte de crianza aquí puede estar en conflicto con información a corto plazo, así que no necesariamente predice o explica el resultado del curso de la acción a ser tomada. Aun así, otros discuten que esos factores en solitario pueden explicar el resultado del comportamiento sin la necesidad del “libre albedrío”. Las investigaciones sobre el tema siguen en proceso.
También se ha vuelto posible el estudio del cerebro vivo y los investigadores ahora pueden observar la maquinaria de la toma de decisiones trabajando. Un experimento en este campo fue conducido por Benjamín Libet en los años 1980, en el cual él les pedía a sujetos escoger un momento cualquiera para agitar su muñeca mientras él lo asociaba con la actividad cerebral.
Libet descubrió que la actividad cerebral inconsciente que llevaba a la decisión consciente de mover su muñeca comenzaba medio segundo antes de que el sujeto conscientemente decidiera moverla. Esta masa de carga eléctrica ha sido llamada potencial de estar listo (o potencial de preparación). Los descubrimientos de Libet sugieren que las decisiones tomadas por un sujeto son primero hechas en un universo inconsciente y después son traducidas a una “decisión consciente”, y la creencia del sujeto de que esto ocurrió bajo su voluntad se debe únicamente a la visión retrospectiva del evento. Por otro lado, Libet todavía encuentra espacio en su modelo para el libre albedrío, en la noción del poder del veto: de acuerdo con este modelo, los impulsos inconscientes que ocasionarán un acto voluble pueden ser suprimidos por los esfuerzos conscientes del sujeto. Cabe acotar que esto no significa que Libet crea que las acciones inconscientemente incentivadas necesitan la ratificación de la consciencia, sino que, más bien, la consciencia retiene el poder de negar la actualización de los impulsos inconscientes.[40]​[41]​
Un experimento relacionado, realizado después por el doctor Álvaro Pascual-Leone, se basaba en preguntar a los sujetos qué mano querían mover. Encontró que, estimulando diferentes hemisferios del cerebro usando campos magnéticos, era posible influenciar fuertemente en la escogencia de la mano. Normalmente la gente que opta por la mano derecha escogería mover dicha mano el 60 % del tiempo, pero cuando el hemisferio derecho era estimulado, escogerían la mano izquierda en un 80 % de las situaciones; el hemisferio derecho del cerebro es responsable del lado izquierdo del cuerpo, y viceversa. A pesar de la influencia externa en la toma de decisiones, los sujetos continuaban reportando que creían haber tomado la decisión libremente. El mismo Libet,[42]​ sin embargo, no interpreta su experimento como experiencia de la ineficacia del libre albedrío consciente —él señala que a pesar de la tendencia que dice que al presionar un botón, y acumular por 500 milisegundos, el consciente retendrá el derecho a vetar esa acción en los últimos milisegundos—. Se puede comparar con un jugador de golf, que puede mover el putter varias veces antes de acertar. Si nos basamos en esto, la acción simplemente recibe una estampa de aprobación en el último milisegundo. También planeando las actividades del día de mañana, o para dentro de una hora, el interruptor de los milisegundos es insignificante.
Puede, o no, ser posible alcanzar una realización científica final involucrando la posibilidad del libre albedrío adentrándonos en los orígenes de nuestros pensamientos conscientes. En el punto de vista científico, toda experiencia consciente es contingente hacia las neuronas —un golpe fuerte en la cabeza puede servir como demostración a este punto, así como casos documentados de lesiones neurológicas.[43]​ El cerebro consiste en miles de millones de neuronas, con mil billones de conexiones entre ellas. En un nivel bioquímico, la tarea principal de una neurona es propagar impulsos electro-químicos a otras neuronas formando un “circuito integrado” que constantemente recibe información de los sentidos (vista, olor, tacto y gusto) y devolviendo información para controlar músculos y órganos. Solo el 10 % de las neuronas en el sistema nervioso tratan con los impulsos sensoriales y con el control de músculos; las neuronas sobrantes sirven para integrar, refinar y procesar señales de entrada o salida.
La experiencia del libre albedrío es así conceptualizada surgiendo de alguna combinación de estas neuronas, pero ¿cómo llegamos a esta acumulación de neuronas, que son finos hilos de grasa con el potencial de recibir impulsos eléctricos, pueden dar poder a nuestro consciente, emociones y sentimientos? ¿Cómo puede ser que este concepto de “yo” y nuestro libre albedrío puede controlar neuronas y nuestro comportamiento, y el cerebro es meramente una sopa tibia de grasa, colesterol y neurotransmisores? Este misterio sin resolver domina el debate moderno sobre la existencia de nuestra conciencia y la posibilidad del libre albedrío.
En los años 1970, Libet estuvo involucrado en los estudios de la actividad neural y la "sensación de umbral". Estas investigaciones trataban de determinar la secuencia de activación en sitios específicos del cerebro requerida para desencadenar acciones voluntarias tales como el pulsado de un botón, utilizando equipos electroencefalográficos. Un famoso experimento —luego reproducido muchas veces por otros grupos— demostró que eventos cerebrales inconscientes (observables como potenciales eléctricos, llamados potenciales de preparación [en inglés readiness potential]) realmente preceden en un lapso variable (de 0.3 hasta varios segundos) la sensación consciente de haber tomado una decisión voluntaria en preparación de una acción motora —como el pulsado de un botón—. 
Ahora bien conocido en neurología, el llamado 'Bereitschaftspotential' (BP en idioma alemán, 'readiness potential' en inglés), también llamado 'potencial premotor, es una medida de la actividad en el córtex motor y el área motora suplementaria en el cerebro ocupado en la preparación de un movimiento muscular voluntario. Es una manifestación de la contribución cortical al planeamiento del movimiento voluntario. Fue registrado y reportado ya en 1964 por  Hans Helmut Kornhuber y Lüder Deecke en la University of Freiburg en Alemania. La publicación completa apareció en 1965 luego de muchos experimentos usados como control.[44]​
Estas observaciones indican que los procesos neurológicos inconscientes preceden y potencialmente causan tanto la sensación de haber realizado una decisión por propia voluntad como el mismo acto motor.[45]​
Hay ciertos desórdenes relacionados con el cerebro que pueden ser denominados como desórdenes del libre albedrío: en el desorden obsesivo-compulsivo un paciente puede sentir una agobiante necesidad de hacer algo en contra de su propia voluntad. Los ejemplos incluyen lavarse las manos varias veces al día, reconociendo el deseo como su propio deseo, aunque parece estar en contra de su propia voluntad. En el síndrome de Tourette y otros parecidos, los sujetos se moverán involuntariamente, desarrollando tics y articulaciones. En el síndrome de la mano ajena, que es también llamado el síndrome del Dr. Strangelove, denominado así por la popular película, las extremidades del paciente harán actos significativos sin la intención del sujeto.
En la emergente o filosofía generativa de la ciencia cognitiva y la psicología evolucionista, el libre albedrío es la generación de posibles comportamientos infinitos de la interacción de un grupo de reglas y parámetro finitos. A pesar del impredecible carácter del comportamiento emergente de procesos deterministas que guía a la percepción del libre albedrío, este como una entidad ontológica no existe.
Como una ilustración, los juegos de mesa de estrategia como el ajedrez y el go, son rigurosamente determinados en sus reglas y parámetros expresados en términos de la oposición de las piezas en relación con las demás en el tablero. Aun así, el ajedrez y el go, con sus estrictas y simples reglas, generan una gran variedad de comportamientos impredecibles. Por analogía, los emergentes o generativos sugieren que la experiencia de libre albedrío emerge de la interacción de reglas finitas y parámetros determinados que generan comportamientos infinitos y predecibles. En la vista de la dinámica y psicología y evolución, células autómatas y las ciencias generativas el comportamiento social puede ser controlado como proceso emergente, y la percepción del libre albedrío fuera de la casualidad es esencialmente una prueba de ignorancia.
En enero de 2011 se publicó en la revista Proceedings of the Royal Society el artículo Hacia un concepto científico de la voluntad libre como un rasgo biológico: acciones espontáneas y toma de decisiones en los invertebrados,[47]​ en el que se afirma que hasta las moscas de la fruta manifiestan de alguna manera una conducta con libre albedrío. Su autor, Björn Brembs, afirma que el comportamiento de las moscas, pese a no ser completamente libre, no está completamente constreñido. El trabajo aporta evidencia obtenida de cerebros de moscas, unos cerebros considerablemente más pequeños que el nuestro que, sin embargo, parecen estar dotados de flexibilidad en la toma de decisiones. El científico se atreve a señalar que la capacidad de elegir entre diferentes opciones de comportamiento, incluso en la ausencia de diferencias en el medio ambiente, sería una capacidad común a la mayoría de los cerebros, si no de todos, por lo que los animales más simples no serían autómatas totalmente predecibles.[48]​
El pensamiento científico temprano a menudo describía el universo como determinista, por ejemplo, en el pensamiento de Demócrito o los Chárvaka, y algunos pensadores afirmaban que el simple proceso de recopilar suficiente información les permitiría predecir eventos futuros con perfecta precisión. La ciencia moderna, por otro lado, es una mezcla de teorías deterministas y estocásticas.[49]​ La mecánica cuántica predice eventos solo en términos de probabilidades, lo que arroja dudas sobre si el universo es determinista en absoluto, aunque la evolución del vector de estado universal es completamente determinista. Las teorías físicas actuales no pueden resolver la cuestión de si el determinismo es cierto en el mundo, ya que están muy lejos de una Teoría del todo potencial y están abiertas a muchas interpretaciones diferentes.[50]​[51]​
Suponiendo que una interpretación indeterminista de la mecánica cuántica es correcta, todavía se puede objetar que tal indeterminismo se limita a todos los propósitos prácticos a los fenómenos microscópicos.[52]​ Este no es siempre el caso: muchos fenómenos macroscópicos se basan en efectos cuánticos. Por ejemplo, algunos dispositivos generadores de números aleatorios por hardware funcionan amplificando los efectos cuánticos en señales prácticamente utilizables. Una pregunta más significativa es si el indeterminismo de la mecánica cuántica permite la idea tradicional de libre albedrío (basada en una percepción de libre albedrío). Sin embargo, si la acción de una persona es solo el resultado de una completa aleatoriedad cuántica, los procesos mentales experimentados no tienen influencia en los resultados probabilísticos (como la volición).[53]​ Según muchas interpretaciones, el no determinismo permite que exista el libre albedrío,[54]​ mientras que otros afirman lo contrario (porque la acción no era controlable por el ser físico que afirma poseer el libre albedrío).[55]​
Como ha sido resumido por Swami Vivekananda: «La mente es una parte integral de la naturaleza que está unida por la ley de la causalidad. Ya que la mente está unida por una ley, ésta no puede ser libre. La ley de la causa aplicada a la mente, se llama Karma». El filósofo del Advaitin Chandrashekhara Bharati Swaminah dice en un diálogo grabado en el libro Diálogos con el Gurú por R. Krishnaswami Aiyar, Chetana Limited, Bombay, 1957:[56]​
A una cuestión en la que uno debería resignarse al destino, el Swaminah responde que de hecho uno debería dedicarse al libre albedrío y elaborarlo:
En la filosofía hindú, no hay un conflicto entre el destino y libre albedrío, ya que las dos son formas del karma del individuo.
Thanissaro Bhikkhu enseñó: “Las enseñanzas de Buda sobre el Karma son interesantes porque es una combinación de causalidad y libre albedrío. Si las cosas fuesen totalmente causadas no habría manera para desarrollar una habilidad —tus acciones serían totalmente predeterminadas. Si no hubiese causalidad, todas las habilidades serían inútiles porque las cosas estarían constantemente cambiando sin rima o razón entre ellas. Pero es precisamente por la existencia de un elemento de causalidad y otro de libre albedrío, que tu puedes desarrollar habilidades en tu vida. Te preguntas: ¿Qué está involucrado en el desarrollo de una habilidad? —esto significa ser sensible a tres cosas básicamente: 1) Es un ser sensible a las causas provenientes del pasado, 2) Es un ser sensible a lo que estás haciendo en el momento presente, y 3) Es un ser sensible a los resultados de lo que estás haciendo en el momento presente —como se unen estas tres cosas”.
La doctrina teológica de la divina sabiduría se dice que está frecuentemente en conflicto con el libre albedrío. Después de todo, si Dios sabe exactamente que pasará, exactamente todas las acciones que cada uno hará, el estatus de las opciones libres se cuestionan. Dios ya sabe por adelantado la verdad sobre las opciones de uno, lo cual limita nuestra libertad. Este problema se relaciona con el problema Aristotélico de la batalla marina: mañana habrá o no una batalla marina. Si hubiese una, entonces era verdad que ayer habría una. Entonces sería necesario que la batalla ocurriera. Si no hubiese una, entonces por razonamiento similar, es necesario que no ocurriera. Esto implica que el futuro, sea lo que sea, está totalmente regido por verdades pasadas —verdaderas propuestas sobre el futuro. De todas formas, algunos filósofos sostienen que la necesidad y la posibilidad son definidas respecto a un punto en el tiempo y una matriz dada de circunstancias empíricas, entonces algo que es meramente posible desde la perspectiva de un observador pueden ser necesarias desde la perspectiva de un omnisciente. Algunos filósofos creen que el libre albedrío es equivalente a tener un alma, y por eso, de acuerdo con aquellos que afirman que los animales carecen de alma, los mismos no poseen libre albedrío. La filosofía judía remarca que el libre albedrío es un producto de la intrínseca alma humana, utilizando la palabra neshama, venida de la raíz judía nshm נשמ que significa “aliento”.
En la teología cristiana, Dios es descrito no solamente como alguien omnisciente, sino que además es omnipotente; un hecho que mucha gente, cristianos y no-cristianos también, opinan que implica que no solamente Dios siempre ha sabido qué decisiones tomará cada uno mañana, sino que además ya ha determinado esas decisiones. Eso es, creen ellos, que por la virtud de su conocimiento, Él sabe que influenciará las decisiones individuales, y con la virtud de su omnipotencia, Él controla esos factores. Esto se vuelve especialmente importante para las doctrinas relacionadas con la salvación y la predestinación. Otras ramas, como los Metodistas, creen que mientras Dios es omnipotente y conoce las decisiones que los individuos van a tomar, Él todavía da el poder a los individuos para escoger o rechazarlo todo, sin importar las condiciones externas o internas relacionadas con la decisión. Por ejemplo, cuando Jesús fue clavado a la cruz, los dos asesinos, uno de cada lado, estaban a punto de morir. Solamente uno pidió a Jesús el perdón, mientras que el otro, incluso al borde de la muerte y sin nada que perder, decide burlarse del mismo. En el punto de vista de los Metodistas y otros que creen en el albedrío, esto fue la elección entre la vida y la muerte eterna.
Quienes proponen el “libre albedrío”, defienden el hecho de que el conocimiento de un suceso por venir es enteramente diferente a causar el suceso. Quienes proponen el "determinismo" estarían de acuerdo, pero cuestionarían si el conocimiento del futuro sería posible sin la presencia de una causa determinante (ver Boettner, más abajo). Aun así, la definición de la predestinación varía entre los cristianos.
A partir de la obra de Juan Calvino, La Institución de la Religión Cristiana, los calvinistas divulgan la idea de que Dios, en su soberanía, decidió quién iba a ser salvado desde antes de la Creación como está escrito en el Sínodo de Dort, convocado por la Iglesia reformada neerlandesa en 1619.
Los calvinistas al igual que los luteranos niegan el libre albedrío concluyendo que la voluntad humana, en vez de ser amo de sus propios actos, está rígidamente predeterminada en todas sus opciones a lo largo de su vida. Como consecuencia, el hombre está predeterminado desde antes de su nacimiento al eterno premio o castigo de tal manera que no puede haber tenido nunca real libre poder sobre su propio destino. Lutero abiertamente sostenía que el libre albedrío es un mito, un nombre que no encubre ninguna realidad, pues no está en el poder del hombre concebir el bien o el mal, ya que los eventos ocurren por necesidad.[57]​
Ellos citan Efesios 1-4: «En Cristo Dios nos eligió antes de la creación del mundo, para estar en su presencia sin culpa ni mancha». Uno de sus más fuertes defensores de este punto de vista fue el predicador Puritano-Americano y teólogo Jonathan Edwards.
Edwards creía que la indeterminación era incompatible con la dependencia del individuo a Dios y su soberanía. Él pensaba que si las respuestas de los individuos eran causalmente libres, entonces su salvación depende parcialmente en ellos y la soberanía de Dios no es “absoluta y universal”. El libro de Edwards, Libertad del Albedrío, defiende la determinación teológica. En este libro, Edwards intenta demostrar que la liberalidad es incoherente. Por ejemplo, él dice que a través de la “determinación propia” el libertario manifiesta que las acciones propias son precedidas por un acto de libre albedrío o que los actos propios carecen de causas suficientes. La primera afirmación nos guía a un infinito regreso mientras que la segunda implica que los acciones propias ocurren por accidente y no puede hacer a alguien “mejor o peor, así como un árbol es mejor que otros árboles porque periódicamente es alumbrado por un cisne o una luciérnaga; o una roca más viciosa que otras rocas, porque las serpientes se han enrollado bajo ella más seguidos”.[58]​
Sin embargo, no debería ser considerado que este punto de vista niega completamente el libre albedrío. Clama que el hombre es libre de actuar de acuerdo con sus impulsos morales y deseos, pero que no es libre de actuar en su contra o cambiarlos. Quienes proponen, como John L. Girardeau, han indicado en sus creencias en que la neutralidad moral es imposible; que aún de ser posible, y que uno fuese adepto a ideas contrarias, uno no podría tomar decisión alguna; si uno, por otro lado, se inclina levemente hacia una opción, se escogerá esa sobre las otras.
Cristianos no-Calvinistas intentan una reconciliación con los conceptos duales de Predestinación y libre albedrío al señalar la situación de Dios como Cristo. Al tomar la forma de un hombre, un elemento necesario en este proceso es que Jesús vivió en la forma de un mortal. Cuando Jesús nació, no fue creado por el poder omnisciente de Dios el Creador, sino con la mente de un niño —aun así, era todavía completamente Dios—. El precedente que esto crea es que Dios es capaz de abandonar sabiduría, o ignorarla, mientras siga siendo Dios. Aunque esto no es inconcebible, y aunque la omnisciencia, y que Dios sabe cual es el futuro que le espera a los individuos, el cual está en poder de negar este conocimiento en orden de preservar el libre albedrío individual.
Sin embargo, una reconciliación más compatible con la teología no-calvinista establece que Dios es, de hecho, ignorante de los eventos futuros, pero, siendo eterno, está fuera del tiempo y del pasado, presente y futuro como una sola creación. Consecuencialmente no se cree que Dios sabría que Jeffrey Dahmer iba a ser culpable de homicidio años antes del evento, pero que Dios era consciente de ello por toda la eternidad, viendo todo el tiempo como un presente único. Esta era el punto de vista ofrecido por Boecio en el libro V de su "Consolación de la filosofía”.
Loraine Boettner difería acerca de la doctrina de pre-conocimiento divino y que este no escapaba de los alegados problemas de pre-ordenación. Escribió que “Lo que Dios más sabe de antemano, en la propia naturaleza del caso, es tan arreglado y certero como lo que está preordenado; y si uno es inconsistente con la agencia libre del hombre, el otro también lo es. La preordenación predice los eventos certeros, mientras que el preconocimiento presupone que ellos si son certeros”. Algunos cristianos teólogos, sintiendo la mordida de este comentario, optaron por limitar la doctrina del preconocimiento no haciéndolo todo junto, sino que formando una nueva escuela de pensamiento, similar al socinianismo y la teología del proceso, llamado Teísmo Abierto.
La oposición a la teología y soteriología calvinistas en Holanda se fortaleció a partir de la obra de Jacobus Arminius, profesor de la Universidad de Leiden. Después de su muerte, sus seguidores encabezados por Simón Episcopius escribieron el manifiesto Remonstrance, el cual sostenía cinco puntos:
Antes de Arminius, Menno Simons había escrito y polemizado contra la doctrina de la predestinación y sostenido que Dios ha dejado vida y muerte a nuestra elección deuteronomio 29:15-20) y no quiere que nadie perezca, sino que todos se arrepientan, vayan al conocimiento de la verdad y sean salvos (2Pedro 3:9, Ezequiel 33:10-20). Así, los menonitas holandeses encontraron afinidades con los arminianos tanto en su rechazo a la persecución religiosa, como en la oposición a la concepción calvinista de la predestinación. Los bautistas John Smyth y Thomas Helwys, exiliados en Ámsterdam entre 1606 y 1612 fueron influenciados por el arminianismo y sus seguidores son hoy conocidos como Bautistas Generales, por su convencimiento de que Jesús murió para salvar a todos los hombres que crean en Él.
Los metodistas, como su fundador John Wesley, defendieron los criterios arminianos creen que mientras Dios es omnipotente y conoce las decisiones que los individuos van a tomar, Él todavía le da el poder a los individuos para escoger o rechazarlo todo, sin importar las condiciones externas o internas relacionadas con la decisión. Por ejemplo, cuando Jesús fue clavado a la cruz, los dos asesinos, uno de cada lado, estaban a punto de morir. Solamente uno pidió a Jesús el perdón, mientras que el otro, incluso al borde de la muerte y sin nada que perder, decide burlarse del mismo. En el punto de vista de los Metodistas y otros que creen en el libre albedrío, esto fue la escogencia entre la vida y la muerte eterna.
También fue adoptada la teología arminiana en el siglo XIX por el movimiento restauracionista de los Discípulos de Cristo e Iglesias de Cristo. Actualmente las tesis arminianas han llegado a tener aceptación entre cristianos de diferentes denominaciones de varios países.
Teólogos de la Iglesia católica abrazan la idea del albedrío, pero generalmente no ven el libre albedrío existiendo aparte o contradiciendo la Gracia divina. San Agustín de Hipona y Santo Tomás de Aquino escribieron extensamente sobre el libre albedrío, con Agustín concentrándose en la importancia del libre albedrío en su respuesta a los maníqueos, y también en las limitaciones de un concepto de libre albedrío ilimitado como negación de gracia, en sus refutaciones de Pelagio. El énfasis del catolicismo cristiano en el libre albedrío y gracia es generalmente contrastado con la predestinación de la cristiandad protestante especialmente después de la contrarreforma, pero entender conceptos que difieren acerca del libre albedrío, es tan importante como entender los diversos conceptos de la naturaleza de Dios, centrándose en la idea en que Dios puede ser todopoderoso y omnisapiente, aunque la gente continúe ejercitando el libre albedrío, ya que Dios no existe en el tiempo.
Tomás de Aquino veía a los humanos como preprogramados (en virtud de ser humanos) para buscar ciertas metas, pero capaces de elegir entre rutas para lograr estas metas (nuestro telos aristotélico). Su punto de vista se ha asociado tanto con el compatibilismo como con el libertarismo.[59]​[60]​ Al enfrentar las elecciones, argumentó que los humanos están gobernados por el intelecto, la voluntad y las pasiones. La voluntad es "el motor primario de todos los poderes del alma [...] y también es la causa eficiente del movimiento en el cuerpo".[61]​ El libre albedrío entra de la siguiente manera: el libre albedrío es un "poder apetitivo", es decir, no un poder cognitivo del intelecto (el término "apetito" de la definición de Aquino "incluye todas las formas de inclinación interna").[62]​ Afirma que el juicio "concluye y termina el consejo. Ahora bien, el consejo se termina, primero, por el juicio de la razón; en segundo lugar, por la aceptación del apetito [es decir, el libre albedrío]".[63]​
El Concilio de Trento declaró que el libre albedrío del hombre, movido y animado por Dios, puede por su consentimiento cooperar con Dios, quien anima e invita su acción; y que por ello puede disponerse y prepararse para obtener la gracia de justificación. La voluntad puede resistirse a la gracia de Dios si así elige. No es como una cosa inanimada que permanece puramente pasiva. Aún debilitado y disminuido por la caída de Adán, el libre albedrío no es destruido en la carrera (Sesión VI, cap. I y V).[64]​
El concepto de libre albedrío será también muy importante en las iglesias ortodoxas, particularmente en las orientales ortodoxas, y muy especialmente en las afiliadas al Cóptico. Muy similar al concepto del judaísmo, el libre albedrío es tratado como axiomático. Todos poseen un libre albedrío que seguirá siguiendo su consciencia y arrogancia, ambas siendo parte del individuo. Mientras uno más sigue la consciencia, se obtienen mejores resultados, y mientras uno más siga la arrogancia, peores serán los resultados. Seguir la arrogancia propia es a veces comparado con los peligros de caer en un hueco al caminar en oscuridad, sin la luz de la conciencia que ilumina el camino. Doctrinas muy similares han también encontrado expresión escrita en el “manual de Disciplina” de los Manuscritos del Mar Muerto, y en algunos textos religiosos baja la posesión de los judíos Beta Israel de Etiopía.
Los miembros de la Iglesia de Jesucristo de los santos de los últimos días creen que Dios le ha dado a todos los humanos el regalo del albedrío, siendo la meta última retornar a su presencia. David O. McKay, anterior profeta y presidente de la Iglesia, comunicó: “es el propósito del Señor que el hombre se convierta a su imagen y semejanza. Para que el hombre lo logre fue necesario para el Creador hacerlo primero libre”.
Con respecto al conflicto albedrío y predestinación, los santos de los últimos días opinan que Dios preordenó al hombre en particulares estaciones de la vida, en orden de avanzar Su plan para guiar a la humanidad de vuelta a Su presencia. Estas preordenanzas no eran decretos inalterables, sino llamadas de Dios para que el hombre realizara misiones específicas en su mortalidad. Los hombres son responsables por su propio destino, a pesar de su fe y obediencia a los mandamientos de Dios.
El “albedrío” entonces no debería ser interpretado como las acciones sin consecuencias; significa que es un don de Dios y las consecuencias deben venir necesariamente como resultado de las decisiones hechas. Aun así el albedrío y la contabilidad son complementarias y no pueden ser separadas.
Una diferencia grande, y un punto de vista clave para el entendimiento del albedrío de los santos de últimos días, entre los cristianos comunes y los santos de últimos días involucra la creencia en una vida antes de la inmortalidad. Los miembros de la Iglesia de Jesucristo de los santos de los últimos días creen que antes que la tierra fuese creada toda la humanidad vivía en una vida preexistente como hijos espirituales de Dios, citando a hebreos 12:9. Aquí Dios, su Padre, nutrió, enseñó y vio los medios para su desarrollo, pero nunca los robó de su albedrío, citando a doctrina y convenios 29:35. En este estado persistente ellos podían aprender, escoger, crecer, retroceder, como en la tierra. Esta preparación les permitiría volverse los hombres y mujeres de la tierra, y ser educados posteriormente y probados en la escuela de la inmortalidad para retornar a la presencia de Dios y volverse como Él.
Aun así se cree que la vida preexistente ha sido un período infinitamente largo de probación, progresión y escuela. Algunos de los hijos espirituales de Dios ejercitaron tanto su albedrío que se conformaron con la ley de Dios y se convirtieron en “nobles y grandes”. Estos fueron preordenados antes de sus nacimientos mortales para realizar grandes visiones para el Señor en esta vida, como fue descrito en el Abraham en los versos 3:22-28. Pero incluso éstos, quienes fueron preordenados para la grandeza podían caer y transgredir las leyes de Dios. Por lo tanto, la mortalidad es simplemente un estado donde la progresión y la probación son continuados así como comenzó en la preexistencia. Sin su albedrío la mortalidad sería inútil.[cita requerida] 
La Nueva Iglesia swedenborguiana, fundada sobre los escritos de Emanuel Swedenborg, enseña que cada persona tiene completa libertad para escoger el cielo o el infierno. Swedenborg afirma que si Dios es el Amor mismo, la gente debe tener libre albedrío. Si Dios es Amor, entonces Él no desea daño a nadie: así que es imposible que Él predestine a cualquier persona al infierno. Por otro lado, si Dios es Amor, entonces Él debe amar cosas fuera de si; si la gente no tiene la libertad para escoger el mal como serán simples extensiones de Dios, y el no podrá amarlo como algo fuera de sí. Además, Swedenborg deja en claro que si una persona no tiene libre albedrío para escoger la bondad y la fe, entonces todos los mandamientos de la Biblia para amar a Dios y al prójimo no tienen valor, ya que nadie puede escoger hacerlos —y es imposible que un Dios que es Amor y sabiduría enseñe mandamientos inconclusos.[cita requerida]
La creencia del libre albedrío (hebreo: bejirá jofshith בחירה חפשית, bejirá בחירה) es axiomática en el pensamiento judío, y está conectada muy de cerca con el concepto de premio y castigo, basado en la Torá. El Versículo 30:19 del Deuteronomio dice “Yo (Dios) te he dado vida y muerte, bendición y maldición: escoge vida”. El libre albedrío es entonces discutido largamente en la filosofía judía, primariamente como el objetivo de Dios en la creación, y después resultando en una paradoja.
Las enseñanzas tradicionales sobre la creación, particularmente influenciado por el misticismo judío, son que “este mundo es como un pasillo para el Mundo Venidero” (Pirkei Avoth 4:16). “El hombre fue creado con el solo propósito de regocijar a Dios, y derivando el placer del esplendor de Su presencia… el lugar donde esta alegría se dará es en el Mundo Venidero, que fue creado expresamente para esto; pero el camino al objeto de nuestros deseos es este mundo…”. (Moshe Jaim Luzato, Mesillat Yesharim, Cap.1).
El libre albedrío es requerido en la justicia de Dios, “de otra manera, el Hombre no obtendría ni rechazaría actos de bondad sobre los cuales él no tendría control”.[65]​ Es entendido posteriormente que para que el Hombre pueda tener un libre albedrío verdadero, no debe tener solamente esto internamente, sino también un ambiente que permita una decisión entre obediencia y desobediencia. Dios, así, creó el mundo para que bien y mal puedan operar libremente;[65]​ esto es el significado de la máxima rabínica, “todo está en las manos del cielo menos el miedo al cielo” (Talmud, Berachot 33b).
En la literatura Rabínica, hay mucha discusión entre la omnisciencia de Dios y el libre albedrío. La visión representativa es que “todo está previsto; aun así, el libre albedrío es dado”. (Rabí Akiva, Pirke Avot, 3:15). Basado en el entendimiento, el problema es descrito como una paradoja, más allá de nuestro entendimiento.
“El Sagrado, Bendito sea, sabe todo lo que pasará antes de que haya pasado. ¿Sabrá Dios si una persona particular será buena o mala, o no lo sabrá?, si Él lo sabe, será imposible para esa persona no ser buena, y así demuestra que no conoce todo lo que Él ha creado… El Sagrado, Bendito sea, no tiene temperamentos y está fuera de dichos ambientes, a diferencia de la gente, cuyos seres y temperamentos son dos cosas separadas. Dios y sus temperamentos son uno, y la existencia de Dios está más allá de la comprensión del hombre… No tenemos las capacidades de comprender cómo El Sagrado, Bendito Sea, conoce todos los eventos y su creación. [Sin embargo] se sabe sin duda que la gente hace lo que quiere sin El Sagrado, Bendito Sea, forzándolos a hacer algo… Es dicho por esto que un hombre es juzgado de acuerdo a sus acciones”.
(Maimonides, Mishne Torá, Teshuva 5:5).
La paradoja es explicada, pero no resuelta, al observar que Dios existe fuera del tiempo y por lo tanto, su conocimiento del futuro es exactamente el mismo conocimiento del pasado y del presente.[65]​ Así como su conocimiento del pasado no interfiere con el libre albedrío del hombre, tampoco en un futuro. Una analogía es aquella del viaje en el tiempo: El viajero del tiempo, habiendo regresado del futuro, sabe previamente lo que alguien hará, pero mientras él sabe esto, este conocimiento no causa la acción del sujeto; el sujeto tuvo libre albedrío incluso cuando el viajero del tiempo tuvo un conocimiento previo. Esta distinción entre conocimiento previo y predestinación, es discutido por la crítica de Maimonides Abraham Ibn Daud; véase Hasagat HaRABaD ad loc.
Aunque lo previo representa la vista mayoritaria en el pensamiento rabínico, hay muchos grandes pensadores que resuelven la paradoja al excluir explícitamente el divino conocimiento previo. Ambos, Saadia Gaon y Judah ha-Levi mantienen que “las decisiones del hombre preceden al conocimiento de Dios”. Gersónides sostiene que Dios sabe, de antemano, las decisiones abiertas a cada individuo, pero no conoce que decisión el individuo en su libertad tomará.
Isaiah Horowitz toma el punto de vista de que Dios no puede saber cuáles elecciones morales tomarán las personas, pero que, sin embargo, no deteriora su perfección.
La existencia del libre albedrío y la paradoja descrita anteriormente está ligada cercanamente al concepto de Tzimtzum. Tzimtzum apoya la idea de que Dios “contrajo” su esencia infinita, para permitir la existencia de un “espacio conceptual”, en el cual un mundo finito, independiente pudiese existir. Esta “constricción” hizo posible al libre albedrío, y seguidamente el potencial para heredar el potencial y el Mundo Venidero. Más allá, de acuerdo con la primera aproximación, está entendido que la paradoja de la omnisciencia del libre albedrío provee un plano temporal paralelo a la paradoja inherente dentro de Tzimtzum.
En la garantía del libre albedrío, Dios, de alguna manera ha “disminuido” su conocimiento previo para permitir la acción independiente del hombre; Él posee su conocimiento previo y aun así el libre albedrío existe. En el caso de Tzimtzum, Dios ha contraído su esencia para permitir la existencia independiente del hombre; aun así es atractivo y trascendente.
En el pensamiento judío, el libre albedrío es todavía discutido en conexión con la Teología Negativa, la Divina Simplicidad y la Divina Providencia así como los principios judíos de la fe en general.
El islam enseña: Dios es omnisciente y omnipotente; lo ha sabido todo por la eternidad. Pero aún, hay una tradición de albedrío para que el hombre reconozca la responsabilidad de sus acciones, la cual ha sido extraída del Corán.
Así está escrito en el Corán: “Nadie cargará el peso de otro”.
El albedrío es la base sobre la cual uno puede ser castigado o recompensado en la vida posterior.
Una encuesta reciente de 2009 ha demostrado que el compatibilismo es una postura bastante popular entre quienes se especializan en filosofía (59 %). La creencia en el libertarismo ascendió al 14 %, mientras que la falta de creencia en el libre albedrío llegó al 12 %. Más de la mitad de las personas encuestadas eran estadounidenses.[66]​
El 79 % de los biólogos evolucionistas dijeron que creen en el libre albedrío según una encuesta realizada en 2007, solo el 14 % eligió no tener libre albedrío y el 7 % no respondió la pregunta.[67]​
Una de las más famosas historias del libre albedrío es el relato de Frank R. Stockton de 1882 titulado ¿La Dama o el Tigre?, en la que la protagonista se somete a una difícil decisión. En el relato de ciencia ficción de Larry Niven Todos los Caminos de la Miríada toma la teoría del libre albedrío de los universos múltiples a un "reductio ad absurdum".
Tanto en la trilogía de The Matrix como en la película The Devil's Advocate existen muchas referencias al libre albedrío y a la importancia de hacer nuestras propias elecciones. En Bruce Almighty al personaje principal, Bruce Nolan (Jim Carrey), le fueron dados los poderes de Dios por un determinado período con la advertencia de que «no podía interferir con el libre albedrío». En la película Donnie Darko, el personaje principal puede ver lo que Dios planifica que la gente haga, una implicación del pensamiento cristiano del libre albedrío.
En los videojuegos de la serie Legacy of Kain, uno de los personajes principales, Raziel, es el único con libre albedrío. Todos los otros personajes son dominados por la Rueda del Destino y a ellos sus líneas del tiempo fueron escritas desde el inicio hasta el final mientras Raziel tenía oportunidad para cambiar su línea de tiempo como él lo eligiese usando varias máquinas del tiempo.

El budismo es una religión y una doctrina filosófica y espiritual[3]​[4]​perteneciente a la familia dhármica. Es considerada como una religión «no teista», lo que significa que no tienen y no siguen a un dios; esta se centra en buscar la paz, la armonía, la tranquilidad y el equilibrio. Dentro del budismo encontramos los llamados «lamas», quienes son los guías espirituales. Comprende una variedad de tradiciones, creencias religiosas y prácticas espirituales principalmente atribuidas a Buda Gautama. El budismo es la cuarta religión más importante del mundo con más de 500 millones de adeptos o el 7 % de la población mundial.[5]​
El budismo se originó en la India, específicamente al sur de Nepal[6]​, entre los siglos VI y IV a. C., desde donde se extendió a gran parte del este de Asia y declinó su práctica en el país de origen durante la Edad Media.
La mayoría de las tradiciones del budismo y la filosofía budista comparten el objetivo de superar el sufrimiento (dukkha) y el ciclo de muerte y renacimiento (samsara), ya sea por el logro del nirvana o por el camino de la budeidad.[7]​[8]​ Las escuelas budistas varían en su interpretación del camino hacia la liberación, la importancia relativa y la canonicidad asignadas a los diversos textos budistas, y sus enseñanzas y prácticas específicas. Las prácticas ampliamente observadas incluyen refugiarse en el buda, el dharma y la sangha, la observancia de los preceptos morales, el monasticismo, la meditación y el cultivo de los pāramitās (perfecciones o virtudes).
Existen dos ramas principales del budismo, la del theravāda (‘doctrina de los ancianos’) y la del mahāyāna (‘el gran vehículo’).
El budismo theravāda es dominante en Sri Lanka y el sudeste asiático, como en Camboya, Laos, Birmania y Tailandia. El del mahayana, que incluye las tradiciones de la tierra pura, el zen, el budismo nichiren, el shingon y la escuela tiantai (tendai), se encuentra en todo el este de Asia.
Vajrayana, un conjunto de enseñanzas atribuidas a adeptos indios, puede verse como una rama separada o como un aspecto del budismo mahayana. El budismo tibetano, que conserva las enseñanzas vajrayana de la India del siglo VIII, se practica en los países de la región del Himalaya, Mongolia y Kalmukia.[9]​
A pesar de una enorme variedad en las prácticas y manifestaciones, las escuelas budistas comparten varios principios filosóficos entre sí. Todos los elementos de las enseñanzas filosóficas fundamentales se caracterizan por estar estrechamente interrelacionados y contenidos en otros, por lo que para alcanzar su entendimiento se necesita una visión holística de su conjunto. Además, se suele subrayar el hecho de que todas las enseñanzas están orientadas a guiar o señalar el dharma, es decir, la ley universal u orden cósmico, del cual debe darse cuenta el mismo practicante.
Aunque el budismo tiene un vasto número de escrituras y prácticas, el núcleo del budismo, las Cuatro nobles verdades y el Noble camino óctuple, son distinguidas en el mundo por no tener mención alguna de dioses o alguna noción de veneración deidades. Son puramente éticas meditativas y directrices basadas en las verdades del sufrimiento psicológico debido a la impermanencia.
Si bien el budismo es considerado como religión no teísta, sí aceptan la creencia en realidades espirituales, como el renacimiento, el karma y la existencia de seres espirituales y deidades en el budismo, pero no rinden culto a los dioses que son vistos como de naturaleza permanente, las deidades son seres iluminados que han alcanzado la Iluminación, como los Budas, o son considerados una "representación" de estos; su trato a estos difiere mucho del concepto tradicional occidental de deidad.
Tras el despertar de Buda Gautama, el primer discurso (sutra) que dio fue a sus antiguos compañeros de meditación, en lo que se conoce como "la puesta en marcha de la rueda del dharma" (dhammacakkappavattana). En este primer discurso, Buda Gautama establece las bases para la comprensión de la realidad del sufrimiento y su cese.
Estas bases se conocen como "las Cuatro Nobles Verdades", las cuales constatan la existencia de lo que en el budismo se llama duḥkha; una angustia de naturaleza existencial.
1. Existe duḥkha: el sufrimiento, insatisfacción o descontento existen.
Duhkha es el concepto central del budismo y se traduce como la "incapacidad de satisfacer" y sufrimiento.[10]​[11]​ La vida es imperfecta, la insatisfacción y el sufrimiento existen y son universales. Este es el punto de partida de la práctica budista. Esta verdad contiene las enseñanzas sobre las Tres Marcas de la Existencia. Como dice el Dhammacakkappavattana sutta: "El nacimiento es sufrimiento, la vejez es sufrimiento, la enfermedad es sufrimiento, la muerte es sufrimiento, asociarse con lo indeseable es sufrimiento, separarse de lo deseable es sufrimiento, no obtener lo deseado es sufrimiento. En breve, los cinco agregados de la adherencia son sufrimiento."[12]​
La orientación básica del budismo expresa que anhelamos y nos aferramos a situaciones y cosas impermanentes.[10]​ Esto nos coloca en el estado de Samsara, el ciclo de repetidos renacimientos y muerte. Queremos alcanzar la felicidad mediante situaciones y bienes materiales que no son permanentes y, por tanto, no logramos la verdadera felicidad.
2. El origen del duhkha es el tṛṣṇā (en sánscrito: el deseo, el querer, el anhelo, la sed).
El sufrimiento se origina en el ansia que causan los deseos, los sentidos o el placer sensual, cualquier situación o condición placentera, buscando la satisfacción ahora aquí y después allí. Hay tres formas de anhelo: kama-tanha, anhelo de placeres sensoriales; bhava-tanha, anhelo a continuar el ciclo de vida y muerte; y vibhava-tanha, anhelo a no experimentar el mundo y sentimientos dolorosos.[13]​ Creemos que algún acto, logro, objeto, persona o entorno nos llevarán a la satisfacción permanente del “yo”, cuando el "yo" en sí no es más que una fabricación impermanente de la mente. El apego y el anhelo producen karma, que nos ata al samsara, la ronda de la muerte y el renacimiento.
3. Existe un cese de duhkha, llamado nirvana: el sufrimiento puede extinguirse cuando se extingue su causa.
El sufrimiento se extingue con el abandono del anhelo, con la ausencia de pasión, el no albergar más. Esto es el nirvana. Esta verdad dice que es posible poner fin al sufrimiento. El nirvana es el "apagar" de nuestras vidas en samsara, tal como la llama de una vela que se extingue con el viento. Connota el fin del renacimiento.[14]​
4. Existe un Noble camino óctuple para lograr este cese: el método para extinguir al sufrimiento.
El budismo prescribe un método, o camino, con el que se intenta evitar los extremos de una búsqueda excesiva de satisfacción por un lado y de una mortificación innecesaria por el otro. Este camino comprende la sabiduría, la conducta ética y el entrenamiento o cultivo de la ‘mente y corazón‘[15]​ por medio de meditación,[16]​ atención y la plena consciencia del presente[17]​ de manera continua. Se requiere método y disciplina para eliminar la ignorancia, el anhelo y finalmente duhkha es el camino de la sabiduría, la ética y la meditación, expuesto de manera detallada en el Noble Camino.
Saṃsāra se refiere a una existencia cíclica, circular y errante.[18]​ Se refiere a la teoría del renacimiento y la "ciclicidad de toda vida, materia, existencia", una suposición fundamental del budismo, como con todas las principales religiones indias.[19]​ Samsara en el budismo se considera dukkha, insatisfactorio y doloroso, perpetuado por el deseo y avidya (ignorancia), y el karma resultante.[19]​
La liberación de este ciclo de existencia, el nirvana, ha sido la base y la justificación histórica más importante del budismo.[20]​
El renacimiento no es visto como algo deseable, ni significa un determinismo o destino. El camino budista sirve para que la persona pueda liberarse de esa cadena de causas y efectos. Mientras no exista un cese de este ciclo, nuestra vida es samsárica, es decir, dukkha. Si bien el individuo debe experimentar las circunstancias en las que le toca vivir, a la vez es el único responsable de lo que decida hacer frente a ellas.
En la India, la idea de reencarnación era ya parte del contexto en el que nació el budismo. En el pensamiento budista, este renacimiento no involucra a ninguna alma, debido a su doctrina de anattā (sánscrito: anātman, no-yo) que rechaza los conceptos de un yo permanente o un alma eterna e inmutable, como en el hinduismo.[21]​ Sin embargo no debería interpretarse el renacimiento budista como un hecho metafórico o psicológico; la reencarnación es una parte fundamental de las creencias budistas.
En el renacimiento budista, el proceso del karma hará que la existencia de seres conscientes se manifieste, sin tener un alma o espíritu eterno. La mayoría de las tradiciones budistas afirman que la vijñāna (la conciencia de una persona) aunque evoluciona y cambia, existe como un continuo y es la base de lo que experimenta el renacimiento. Por eso se prefiere el término "renacimiento" en vez de "reencarnación". Así, las acciones de cuerpo, habla y pensamiento conllevan efectos que se experimentarán con el tiempo, ya sea en la vida actual o siguiente, por un flujo de conciencia connectado causalmente con la conciencia previa.[22]​ La continuidad entre individuos la constituye esa corriente causal, que se manifiesta como tendencias y circunstancias en sus vidas.
El renacimiento puede ocurrir en uno de los cinco reinos según Theravada, o en seis según otras escuelas: reinos celestiales, semidioses, humanos, animales, fantasmas hambrientos y reinos infernales.[23]​
En el budismo, el karma (del sánscrito: "acción, trabajo") impulsa saṃsāra. Las buenas acciones (Pāli: kusala) y las malas acciones (Pāli: akusala) producen "semillas" en la conciencia que maduran más tarde en esta vida o en un renacimiento posterior.[24]​ La existencia del karma es una creencia central en el budismo, ya que con todas las principales religiones indias, no implica ni fatalismo ni que todo lo que le sucede a una persona sea causado por el karma.[25]​
Según el budismo, toda acción intencionada (karma) crea uno o varios efectos que aparecen cuando las circunstancias son proclives, a lo que se llama maduración (vipaka) o fruto (phala).[24]​ El karma en aplicación a la doctrina budista se refiere a cualquier acción de habla, cuerpo o pensamiento hecho con intención (cetana). Por tanto, los movimientos ajenos a la volición o la intencionalidad —como ocurre en el caso de actos reflejos— son neutrales kármicamente. Sin embargo, el karma bueno o malo se acumula incluso si no hay acción física, y solo tener malos o buenos pensamientos crea semillas kármicas; así, las acciones del cuerpo, el habla o la mente conducen a semillas kármicas.[25]​ En las tradiciones budistas, los aspectos de la vida afectados por la ley del karma en los nacimientos pasados y actuales de un ser incluyen la forma del renacimiento, el reino del renacimiento, y las principales circunstancias de la vida.[26]​ En el Cula-kammavibhanga Sutta Buda explica que estas cosas no existen por casualidad sino por el karma.[27]​ Funciona como las leyes de la física, sin intervención externa, en cada ser en los seis reinos de la existencia, incluidos los seres humanos y los dioses.[25]​ El "buen" y "mal" karma se distinguen de acuerdo a la raíz de las acciones. En el Kukkuravatika Sutta[28]​ Buda clasifica el karma en cuatro grupos:
La doctrina de karma budista no significa destino ni predeterminación, ya que no existe un automatismo ciego en la voluntad respecto a las tendencias mantenidas y no es posible anticipar que ocurrirá. La práctica budista además permite tomar observación y consciencia de este funcionamiento para ocasionar un distanciamiento respecto a esas tendencias.
El karma no se debe entender como castigo, ya que esta ley es impersonal y no hay intervención divina en el karma. Del mismo modo que las leyes de la naturaleza no requieren intervención divina. Algunos tipos de condicionalidad son inmutables: ni siquiera un Buda puede escapar de ser afectado una vez haya nacido y posea un cuerpo.
Pratītya-samutpāda, (el "surgimiento condicionado" o la "originación dependiente"), es la teoría budista para explicar la naturaleza y las relaciones del ser, el renacimiento y la existencia. El budismo afirma que no hay nada independiente, excepto el estado de nirvana.[29]​ Todos los estados físicos y mentales dependen y surgen de otros estados preexistentes. Todo surge y existe en un estado condicionado.[30]​ Esta teoría constituye una formulación elaborada del proceso de existir y de cómo los seres están atrapados por la ignorancia en un ciclo de sufrimiento. Este proceso es constante y supone una explicación que abarca tanto la duración de todas las vidas pasadas como de la vida actual, instante tras instante. Por lo tanto, el "ser" supone un ámbito que se crea y se destruye momento tras momento.
Pratītya-samutpāda es la creencia budista de que esta relación de dependencia es la base de la ontología, no un Dios creador ni el concepto védico ontológico llamado Ser universal (Brahman) ni ningún otro principio creativo trascendente.[31]​ En el budismo, el surgimiento dependiente se refiere a las condiciones creadas por una pluralidad de eventos que necesariamente originan a fenómenos dependientes, creando condiciones que conducen al renacimiento.[32]​
El budismo aplica la teoría del surgimiento dependiente para explicar los ciclos de renacimiento, a través de su doctrina de los Doce Nidānas o "doce eslabones". Establece que debido a que Avidyā (ignorancia) existe, Saṃskāras (formaciones kármicas) existe, porque Saṃskāras existe, por lo tanto, Vijñāna (conciencia) existe, y de manera similar vincula a Nāmarūpa (cuerpo sensible, lit. "nombre y forma"), Ṣaḍāyatana (seis sentidos), Sparśa (estimulación sensorial, lit. "contacto") , Vedanā (sensación), Taṇhā (anhelo), Upādāna (aferramiento), Bhava (devenir), Jāti (nacimiento) y Jarāmaraṇa (decaimiento, vejez, muerte).[33]​
Mientras la ignorancia no sea erradicada, de nuevo se repite el proceso sin fin. El camino budista busca erradicar la ignorancia y romper esta cadena, es lo que se conoce como nirvana (el cese) de esta cadena.[34]​
Buda Gautama afirmó que es posible el cese definitivo del círculo de la originación dependiente y el renacimiento. La meta de la práctica budista es, por tanto, el de despertar del samsāra para experimentar el cese de las emociones negativas (kleshas), el sufrimiento (dukkha) y la verdadera naturaleza de la existencia. Este logro del Nirvāņa (Pali: nibbāna), ha sido el último objetivo soteriológico del camino budista desde la época del Buda.[35]​
Nirvāņa literalmente significa "apagar, extinguirse".[36]​ En los primeros textos budistas, es el estado de moderación y autocontrol lo que lleva al "apago" y al final de los ciclos de sufrimiento.[37]​ Muchos textos budistas también describen el nirvana como asociado con la sabiduría que conoce el no-yo (anatta) y la vaciedad (sunyata).[38]​
El estado del Nirvāņa ha sido descrito en textos budistas en parte de una manera similar a otras religiones indias, como el estado de completa liberación, iluminación, felicidad suprema, dicha, libertad intrépida, permanencia, insondable e indescriptible.[39]​ El Nirvāņa se describe principalmente por lo que no es: no-nacido, no-originado, no-creado, no-compuesto. Sin embargo, no se debe confundir ni con la aniquilación o aislamiento del individuo ni con un nihilismo.
Si bien el budismo considera el Nirvāņa como el objetivo espiritual supremo, en la práctica tradicional, el enfoque principal de una gran mayoría de budistas laicos ha sido buscar y acumular el bien a través de buenas acciones, donaciones a los monjes y varios rituales. Esto ayuda a uno para obtener un mejor renacimiento.[40]​
Una doctrina relacionada en el budismo es la de anattā (Pali) o anātman (sánscrito). Es la opinión de que no existe un yo permanente, alma o esencia inmutable, permanente.[41]​ Los filósofos budistas, como Vasubandhu y Buddhaghosa, generalmente defienden esta visión a través del esquema de los cinco agregados. Ellos intentan demostrar que ninguno de estos cinco componentes de la personalidad puede ser permanente o absoluto.[42]​ Esto se puede ver en los discursos budistas como el Anattalakkhana Sutta.
"Vacío" o "vacuidad" (Skt: Śūnyatā, Pali: Suññatā), es un concepto relacionado con muchas interpretaciones diferentes a lo largo de los diversos budismos. En el budismo temprano, se decía comúnmente que los cinco agregados son vacíos (rittaka), huecos (tucchaka), sin núcleo (asāraka), por ejemplo, como en el Pheṇapiṇḍūpama Sutta (SN 22:95).[43]​ De manera similar, en el budismo Theravada, a menudo simplemente significa que los cinco agregados están vacíos de un Ser.[44]​
Śūnyatā es un concepto central en el budismo Mahāyāna, especialmente en la escuela Madhyamaka de Nagarjuna, y en los sutras Prajñāpāramitā. En la filosofía de Madhyamaka, se refiere a la visión que sostiene que todos los fenómenos (dharmas) no tienen ningún "svabhava" (literalmente "naturaleza propia") y, por lo tanto, no tienen ninguna esencia subyacente, por lo que están "vacíos" de independencia. Esta doctrina buscaba refutar las teorías heterodoxas de svabhava que circulaban en ese momento, como en la escuela Vaibhasika.[45]​
Todas las formas de budismo veneran y se refugian espiritualmente en las "tres joyas" (triratna): Buda, Dharma y Sangha.
Si bien todas las variedades de budismo veneran a "Buda" y "budeidad", tienen diferentes puntos de vista sobre lo que son.
En el budismo Theravada, un Buda es alguien que se ha despertado a través de sus propios esfuerzos y perspicacia. Han terminado a su ciclo de renacimientos y a todos los estados mentales no saludables que conducen a malas acciones.[46]​ Si bien está sujeto a las limitaciones del cuerpo humano de ciertas maneras (por ejemplo, en los primeros textos, el Buda sufre de dolores de espalda), se dice que un Buda es "profundo, inconmensurable, difícil de comprender como es el gran océano," y también tiene inmensos poderes psíquicos (abhijñā).[47]​
Theravada generalmente ve al Buda Gautama (el Buda histórico Sakyamuni) como el único Buda de la era actual. Aunque ya no está en este mundo, nos ha dejado el Dharma (Enseñanza), el Vinaya (Disciplina) y la Sangha (Comunidad).[48]​
Mientras tanto, el budismo Mahāyāna tiene una cosmología muy expandida, con varios Budas y otros seres santos (aryas) que residen en diferentes mundos. Los textos de Mahāyāna no solo reverencian a numerosos Budas además de Sakyamuni, como Amitabha y Vairochana, sino que también los ven como seres trascendentales o supramundanos (lokuttara).[49]​ El budismo Mahāyāna sostiene que estos otros Budas pueden ser contactados y pueden beneficiar a los seres en este mundo.[50]​ En Mahāyāna, un Buda es una especie de "rey espiritual", un "protector de todas las criaturas" con una vida de incontables eones. La vida y muerte de Buda Sakyamuni en la tierra generalmente se entiende como una "mera aparición" o "una manifestación hábilmente proyectada en la vida terrenal por un ser trascendente iluminado, que todavía está disponible para enseñar a los fieles a través de experiencias ."[51]​[52]​
"Dharma" (Pali: Dhamma) en el budismo se refiere a la enseñanza del Buda, que incluye todas las ideas principales descritas anteriormente. Si bien esta enseñanza refleja la verdadera naturaleza de la realidad, no es una creencia a la que aferrarse, sino una enseñanza pragmática que se debe poner en práctica. Se asemeja a una balsa que es "para cruzar" (al nirvana) no para aferrarse.[53]​
También se refiere a la ley universal que la enseñanza revela y al orden cósmico en el que se basa.[54]​ Es un principio eterno que se aplica a todos los seres y mundos. En ese sentido, también es la máxima verdad y realidad sobre el universo, por lo tanto, es "la forma en que las cosas son realmente". Los budistas creen que todos los Budas en todos los mundos, en el pasado, presente y futuro, entienden y enseñan el Dharma.
La tercera "joya" en la que los budistas se refugian es "Sangha", que se refiere a la comunidad monástica de monjes y monjas que siguen la disciplina monástica del Buda Gautama. Esta disciplina fue "diseñada para dar forma a la Sangha como una comunidad ideal, con las condiciones óptimas para el crecimiento espiritual."[55]​ La Sangha está formada por aquellos que han elegido seguir la forma de vida ideal del Buda, que es una renuncia monástica con mínimas posesiones materiales (como un tazón y una túnica).[56]​
La Sangha es importante porque preservan y transmiten el Dharma del Buda. Como afirma Rupert Gethin "la Sangha vive la enseñanza, preserva la enseñanza en las escrituras y enseña a la comunidad. Sin la Sangha no hay budismo".[57]​ La Sangha también actúa como un "campo de mérito" para los laicos, permitiéndoles hacer méritos espirituales al donar a la Sangha. A cambio, mantienen su deber de preservar y difundir el Dharma en todas partes por el bien del mundo.[58]​
También se supone que el Sangha sigue la regla monástica (Vinaya) del Buda, sirviendo así como un ejemplo espiritual para el mundo y las generaciones futuras. Las reglas de Vinaya también obligan a la Sangha a vivir en dependencia del resto de la comunidad laica (deben mendigar, etc.) y así llevan a la Sangha a una relación con la comunidad laica.[59]​
También hay una definición separada de Sangha, que se refiere a aquellos que han alcanzado cualquier etapa del despertar, sean o no monásticos. Esta sangha se llama āryasaṅgha, "la Sangha noble".[60]​ Todas las formas de budismo generalmente veneran a estos āryas (pali: ariya, "nobles" o "santos") que son seres espiritualmente elevados. Los aryas han alcanzado los frutos del camino budista.[61]​ Convertirse en un arya es un objetivo en la mayoría de las formas de budismo. El āryasaṅgha incluye seres santos como bodhisattvas, arhats y sotapannas ("quien entra a la corriente").
En el budismo temprano y en el budismo Theravada, un arhat (que literalmente significa "digno") es alguien que alcanzó el mismo despertar (bodhi) de un Buda siguiendo las enseñanzas de un Buda. Se ve que han terminado el renacimiento y todas las impurezas mentales. Mientras tanto, un bodhisattva ("un ser destinado al despertar") es simplemente alguien que está trabajando para despertar a la Budeidad. Según todas las primeras escuelas budistas, así como Theravada, para ser considerado un bodhisattva, uno debe haber hecho un voto frente a un Buda vivo y también debe haber recibido una confirmación de su futura Budeidad.[62]​ En Theravada, el futuro Buda se llama Metteya (Maitreya) y es venerado como un bodhisatta.[62]​
El budismo Mahāyāna generalmente ve el logro del arhat como uno inferior, ya que se ve como hecho solo por el bien de la liberación individual. Por lo tanto, promueve el camino del bodhisattva como el más alto y más valioso.[63]​ Mientras que en Mahāyāna, cualquiera que tenga bodhichita (el deseo de convertirse en un Buda que surge de un sentido de compasión por todos los seres) se considera un bodhisattva, algunos de estos seres santos (como Avalokiteshvara) han alcanzado un nivel espiritual muy alto y son vistos como seres supramundanos muy poderosos que brindan ayuda a innumerables seres a través de sus poderes avanzados.[64]​[65]​
El budismo Mahāyāna también difiere de Theravada y las otras escuelas del budismo temprano en la promoción de varias doctrinas únicas que están contenidas en los sutras y tratados filosóficos de Mahāyāna.
Una de ellas es la interpretación única del sunyata y la originación dependiente que se encuentra en la escuela Madhyamaka. Otra doctrina muy influyente para Mahāyāna es la visión filosófica principal de la escuela Yogācāra, denominada Vijñaptimātratā-vāda ("la doctrina de que solo hay ideas" o "impresiones mentales") o Vijñānavāda ("la doctrina de la conciencia"). Según Mark Siderits, lo que los pensadores clásicos de Yogācāra como Vasubandhu tenían en mente es que solo somos conscientes de las imágenes o impresiones mentales, que pueden aparecer como objetos externos, pero "en realidad no existe tal cosa fuera de la mente".[66]​  Hay varias interpretaciones de esta teoría principal, muchos estudiosos la ven como un tipo de idealismo, otras como un tipo de fenomenología.[67]​
Otro concepto muy influyente exclusivo de Mahāyāna es el de "naturaleza de Buda" (buddhadhātu) o "matriz de Tathagata" (tathāgatagarbha). La naturaleza de buda es un concepto que se encuentra en algunos textos budistas del primer milenio de nuestra era, como los Sūtras de Tathāgatagarbha. Según Paul Williams, estos sutras sugieren que "todos los seres sintientes contienen un Tathagata" como su "esencia, núcleo de la naturaleza interior, el Ser".[68]​ Según Karl Brunnholzl, "los primeros sutras mahayana que se basan y discuten la noción de tathāgatagarbha como el potencial de Buda que es innato en todos los seres sintientes comenzó a aparecer en forma escrita a fines del segundo siglo y principios del tercer siglo [d.C.]".[69]​ Para algunos, la doctrina parece estar en conflicto con la doctrina de anatta, lo que lleva a los estudiosos a postular que estos textos de Tathāgatagarbha fueron escritos para promover el budismo entre los no budistas.[70]​ Esto se puede ver en textos como el Laṅkāvatāra Sūtra, que establece que la naturaleza de Buda se enseña a ayudar a aquellos que tienen miedo cuando escuchan la enseñanza de anatta.[71]​ Los pensadores budistas han avanzado varias interpretaciones del concepto a lo largo de la historia del pensamiento budista y la mayoría intentan evitar algo como la doctrina hindú de atman.
Estas ideas budistas indias, en varias formas sintéticas, forman la base de la filosofía posterior de Mahāyāna en el budismo tibetano y el budismo del este asiático.
Se ha utilizado y descrito una amplia variedad de caminos y modelos de progreso espiritual en las diferentes tradiciones budistas. Sin embargo, generalmente comparten prácticas básicas como sila (ética), samadhi (meditación, dhyana) y prajña (sabiduría), que se conocen como los "tres entrenamientos".
Un principio rector importante de la práctica budista es el Camino Medio (madhyama-pratipad). Fue parte del primer sermón del Buda, donde presentó el Noble Sendero Óctuple como una "vía intermedia" entre los extremos del ascetismo excesivo y el hedonismo.[72]​[73]​
Un estilo de presentación común del camino (mārga) a la liberación en los primeros textos budistas es la "plática graduada" o "enseñanza gradual", en la cual el Buda presenta su entrenamiento paso a paso.[74]​
En los primeros textos, se pueden encontrar numerosas secuencias diferentes del camino gradual.[75]​ Una de las presentaciones más importantes y ampliamente utilizadas entre las diversas escuelas budistas es el Noble Sendero Óctuple, o "Sendero Óctuple de los Nobles" (Sct. 'Āryāṣṭāṅgamārga'). Se puede encontrar en varios discursos, el más famoso en el Dhammacakkappavattana Sutta (El discurso sobre el giro de la rueda del Dharma). Otros suttas como el Tevijja Sutta y el Cula-Hatthipadopama-sutta dan esquemas diferente del camino gradual budista, aunque con muchos elementos similares, como la ética y la meditación.[75]​

Según Rupert Gethin, en los suttas Pali, el camino budista hacia el despertar también se resume con frecuencia en otra fórmula corta: "abandono de los [cinco] obstáculos, práctica de los cuatro establecimientos de atención plena y desarrollo de los [siete] factores del despertar."[76]​El Noble Camino (Āryamārgaḥ) consiste en un conjunto de ocho factores o cualidades interconectados que, cuando se desarrollan juntos, conducen al cese de dukkha.[77]​ Estos ocho factores son: Visión correcta (o Comprensión correcta), Pensamiento correcto, Hablar correcto, Acción correcta, Medios de vida correctos, Esfuerzo correcto, Atención plena correcta y Concentración (Samadhi) correcta.
Este Óctuple Sendero es la cuarta de las Cuatro Nobles Verdades y se agrupa en tres divisiones básicas, de la siguiente manera:[78]​[79]​
El budismo theravada es una tradición diversa y, por lo tanto, incluye diferentes explicaciones del camino hacia el despertar. Sin embargo, las enseñanzas del Buda a menudo están encapsuladas por Theravadins en el marco básico de las Cuatro Nobles Verdades y el Óctuple Sendero.[87]​[88]​
Algunos budistas Theravada también siguen la presentación del camino trazado en el Visuddhimagga de Buddhaghosa. Esta presentación se conoce como las "Siete Purificaciones" (satta-visuddhi).[89]​ Este esquema y su esquema acompañante de "conocimientos de perspicacia" (vipassanā-ñāṇa) son utilizados por los estudiosos influyentes modernos de Theravadin, como Mahasi Sayadaw (en su "El progreso de la perspicacia") y Nyanatiloka Thera (en "El camino del Buda para la liberación").[90]​[91]​
El budismo Mahāyāna se basa principalmente en el camino del Bodhisattva, alguien que está en el camino a la budeidad.[92]​
En los primeros textos Mahāyāna, el camino de un bodhisattva era el despertar de la bodhichita y la práctica de los pāramitās. Entre el siglo I y III d. C., esta tradición introdujo la doctrina de los diez Bhumi, que significa diez niveles o etapas de despertar que se escalan durante muchos renacimientos.[93]​ Los eruditos de Mahāyāna luego delinearon un camino elaborado, para monjes y laicos, y el camino incluye el voto de enseñar el conocimiento budista a otros seres, para ayudarlos a liberarse, una vez que uno llegue a la Budeidad en un futuro renacimiento.[92]​
Una parte de este camino son los pāramitā (perfecciones, virtudes trascendentes). Los textos de Mahāyāna son inconsistentes en su discusión sobre los pāramitās, y algunos textos incluyen varias listas.[94]​[95]​ Las seis paramitas han sido más estudiadas, y son: Dāna (Caridad), Śīla (Ética), Kṣānti (paciencia), Vīrya (fuerza), Dhyāna (Meditación), Prajñā (Sabiduría).[95]​
En los Mahāyāna Sutras que incluyen diez pāramitā, las cuatro perfecciones adicionales son "medios hábiles, voto, poder y conocimiento". El pāramitā más discutido y la perfección mejor valorada en los textos de Mahayana es el "Prajñā-pāramitā", o la "perfección de la comprensión".[94]​ Esta percepción de la tradición Mahāyāna, afirma Shōhei Ichimura, ha sido la "percepción de la no dualidad o la ausencia de realidad en todas las cosas".[96]​
El budismo de Asia Oriental está influenciado tanto por las presentaciones clásicas budistas indias del camino, como por las presentaciones clásicas de Mahāyāna, como la que se encuentra en el Da zhidu lun.[97]​ Hay muchas presentaciones diferentes de soteriología, incluidos numerosos caminos y vehículos (yanas) en las diferentes tradiciones del budismo de Asia Oriental.[98]​ No hay una presentación dominante del camino espiritual. Por ejemplo, en Zen, se puede encontrar varias, como las "Dos entradas y las Cuatro Prácticas" de Bodhidharma y "Los cinco rangos" de Dongshan Liangjie. Mientras tanto, en Tiantai, el camino se describe en el Móhē zhǐguān de Zhiyi.
En el budismo indo-tibetano, el camino hacia la liberación se describe en el género literario conocido como Lam-rim ("Etapas del camino"). Todas las diversas escuelas tibetanas tienen sus propias presentaciones de Lam-rim. Este género se remonta al texto del maestro Indio Atiśa, llamado "La lámpara para el camino hacia la iluminación" (Bodhipathapradīpa, siglo XI).[99]​
En varios sutras que presentan el camino graduado enseñado por el Buda, como el Samaññaphala Sutta y el Cula-Hatthipadopama Sutta, el primer paso en el camino es escuchar el Dharma del Buda. Esto luego se dice que conduce a la adquisición de confianza o fe en las enseñanzas del Buda.[75]​
Los maestros budistas de Mahayana como Yin Shun también afirman que escuchar el Dharma y estudiar los discursos budistas es necesario "si uno quiere aprender y practicar el Buddha Dharma."[100]​ Del mismo modo, en el budismo indo-tibetano, los textos de "Etapas del camino" (Lamrim) generalmente colocan la actividad de escuchar las enseñanzas budistas como una práctica importante.[101]​
Tradicionalmente, el primer paso formal en la mayoría de las escuelas budistas requiere tomar los "Tres Refugios", también llamados las Tres Joyas (Sánscrito: triratna, Pali: tiratana) como la base de la práctica religiosa.[102]​ El budismo tibetano a veces agrega un cuarto refugio, en el lama. Los budistas creen que los tres refugios son protectores y una forma de reverencia.[102]​
La antigua fórmula que se repite para refugiarse afirma que "voy al Buda como refugio, voy al Dhamma como refugio, voy a la Sangha como refugio"."[103]​ Según Harvey, recitar los tres refugios no se considera un lugar para esconderse, sino un pensamiento que "purifica, eleva y fortalece el corazón".[104]​
En muchas escuelas budistas existe algún tipo de ceremonia oficiada por un monje o maestro que ofrece la toma de refugio en las Tres Joyas. Esto es una manifestación pública del compromiso pero no es algo indispensable. La persona puede por ella misma tomar refugio con sinceridad y es suficiente para determinados budistas.
Según Peter Harvey, la mayoría de las formas de budismo "consideran saddhā (Sct. śraddhā), confianza confiable o la fe, como una cualidad que debe ser equilibrada con la sabiduría, y como una preparación y acompañamiento para la meditación".[105]​ Debido a esto, la devoción (Sct. Bhakti; Pali: bhatti) es una parte importante de la práctica de la mayoría de los budistas. Las prácticas devocionales incluyen oración ritual, postración, ofrendas, peregrinación y cantar.[106]​
La devoción budista generalmente se centra en algún objeto, imagen o ubicación que se considera sagrada o espiritualmente influyente. Ejemplos de objetos de devoción incluyen pinturas o estatuas de budas y bodhisattvas, estupas y árboles bodhi.[107]​ El canto grupal público para devocionales y ceremoniales es común a todas las tradiciones budistas y se remonta a la antigua India, donde el canto ayudó a memorizar las enseñanzas transmitidas oralmente.[108]​ Los rosarios llamados malas se usan en todas las tradiciones budistas para contar el canto repetido de fórmulas o mantras comunes. El canto es, por lo tanto, un tipo de meditación grupal devocional que conduce a la tranquilidad y comunica las enseñanzas budistas.[109]​
La ética budista, llamada Śīla (sánscrito) o Sīla (Pāli), se fundamenta en los principios de ahimsa (no ocasionar daño) y el Camino medio (moderación; no reprimir ni tampoco aferrarse a nada).[110]​ Según las enseñanzas budistas, los principios éticos están determinados por el hecho de si una acción cualquiera podría tener una consecuencia dañina o perjudicial para uno mismo o para otros. Śīla consiste en el habla correcto, la acción correcta y los medios de vida correctos.[80]​
Las escrituras budistas explican los cinco preceptos (en sánscrito: pañcaśīla) como el estándar mínimo de la moral budista.[111]​ Es el sistema moral más importante del budismo, junto con las reglas monásticas.[112]​ Los cinco preceptos se aplican tanto a los devotos masculinos como femeninos, y estos son:[113]​[114]​
Los monjes y monjas budistas por su parte, siguen más de 200 normas de disciplina descritas en detalle en el Vinaya pitaka.[113]​
El Canon Pali recomienda que uno debe compararse con los demás, y sobre la base de eso, no hacer daño a los demás. La compasión y la creencia en la retribución kármica forman la base de los preceptos.[115]​ Practicar los cinco preceptos es parte de la práctica laica regular, tanto en el hogar como en el templo.[116]​
Los cinco preceptos no son mandamientos y las transgresiones no invitan a sanciones religiosas, pero su poder se ha basado en la creencia budista en las consecuencias kármicas. Por ejemplo, matar conduce al renacimiento en los reinos del infierno, y durante más tiempo en condiciones más severas si la víctima del asesinato era un monje.[117]​ Dentro de la doctrina budista, los preceptos están destinados a desarrollar la mente y el carácter para progresar en el camino hacia la iluminación.[118]​
La vida monástica en el budismo tiene preceptos adicionales como parte del Vinaya ("Disciplina") y el Patimokkha (código de reglas monásticas), y a diferencia de los laicos, las transgresiones de los monjes invitan a sanciones. La expulsión total de un sangha sigue a cualquier instancia de asesinato, participación en relaciones sexuales, robo o afirmaciones falsas sobre el conocimiento de uno. La expulsión temporal sigue a un delito menor. Las sanciones varían según la fraternidad monástica (nikaya).[119]​ El contenido preciso del Vinaya Pitaka (escrituras en el Vinaya) difiere en las diferentes escuelas y tradiciones, y los diferentes monasterios establecen sus propios estándares en su implementación. La lista de pattimokkha se recita cada quince días en una reunión ritual de todos los monjes.[120]​
Los laicos y los novatos en muchas fraternidades budistas también sostienen ocho o diez preceptos de vez en cuando. Cuatro de estos son los mismos que para el devoto laico: no matar, no robar, no mentir y no intoxicarse.[121]​ Los otros cuatro preceptos son:[122]​
Los ocho preceptos a veces son observados por los laicos en los días de uposatha. Los diez preceptos también incluyen abstenerse de aceptar dinero.[122]​
Otra práctica importante enseñada por el Buda es la restricción de los sentidos (indriya-samvara). En los diversos caminos graduados, esto generalmente se presenta como una práctica que se enseña antes de la meditación formal, y que apoya la meditación al debilitar los deseos sensoriales que son un obstáculo.[123]​ Según Bhikkhu Anālayo, la restricción de los sentidos es cuando uno "protege las puertas de los sentidos para evitar que las impresiones sensoriales conduzcan a deseos y tristeza".[123]​  Es una forma de atención consciente hacia las impresiones sensoriales que no se detiene en sus principales características o signos (nimitta). Esto evita que influencias dañinas entren en la mente. Se dice que esta práctica da lugar a una paz y felicidad interna que forma una base para la concentración y la comprensión.[124]​
Una virtud y práctica budista relacionada es la renuncia (nekkhamma).[125]​ En general, se refiere a renunciar a las acciones y deseos que se consideran poco saludables en el camino espiritual, como el deseo por sensualidad y cosas mundanas.[126]​ La renuncia puede ser cultivada de diferentes maneras. La práctica de donación, por ejemplo, es una forma de cultivar la renuncia. Otro es renunciar a la vida laica y convertirse en monástico.[127]​ Practicar el celibato (ya sea para la vida como monje o temporalmente) también es una forma de renuncia.[128]​
Una forma de cultivar la renuncia enseñada por el Buda es la contemplación (anupassana) de los "peligros" (o "consecuencias negativas") del placer sensual (kāmānaṃ ādīnava). Como parte del discurso graduado, esta contemplación se enseña después de la práctica de donación y la ética.[129]​
Otra práctica relacionada con la renuncia y la moderación sensorial enseñada por el Buda es la "moderación al comer", lo que para los monjes generalmente significa no comer después del mediodía. Los laicos devotos también siguen esta regla durante los días especiales de observancia religiosa (uposatha).[130]​ Observar el Uposatha también incluye otras prácticas relacionadas con la renuncia, principalmente los ocho preceptos.
Para los monjes budistas, la renuncia también se puede entrenar a través de varias prácticas ascéticas opcionales llamadas dhutaṅga.
La formación de la facultad llamada "atención plena" (Pali: sati, sánscrito: smṛti, que significa literalmente "recordar, memoria") es fundamental en el budismo. Según Bhikkhu Analayo, la atención plena es una plena conciencia del momento presente que mejora y fortalece la memoria.[131]​ El filósofo budista indio Asanga definió la atención plena de la siguiente manera: "Significa que la mente no olvida el objeto experimentado. Su función es la no distracción".[132]​ Según Rupert Gethin, sati también es "una conciencia de las relaciones entre las cosas y, por lo tanto, una conciencia del valor relativo de cada fenómeno".[133]​
Existen diferentes prácticas y ejercicios para entrenar la atención plena en los primeros discursos, como los cuatro Satipaṭṭhānas (sánscrito: smṛtyupasthāna, "establecimientos de atención plena") y Ānāpānasati (sánscrito: ānāpānasmṛti, "atención plena de la respiración").
Una facultad mental relacionada, que a menudo se menciona al lado de la atención plena, es sampajañña ("comprensión clara"). Esta facultad es la capacidad de comprender lo que uno está haciendo y lo que está sucediendo en la mente, y si la mente está siendo influenciada por estados no saludables o saludables.[134]​
La meditación (citta-bhavana, "cultivo de la mente") es una parte importante de la práctica budista. Hay muchas y variadas técnicas de meditación budista dependiendo de cada tradición y escuela, si bien todas se basan en dos componentes llamados samatha (calma mental, tranquilidad) y vipassana (conocimiento directo, intuición). En el núcleo central de toda meditación budista hay una observación tranquila y atenta de los procesos y fenómenos de la experiencia. En los primeros textos, citta-bhavana se refiere principalmente al logro de "samādhi" (unificación mental) y la práctica de dhyāna (Pali: jhāna).
Samādhi es un estado de conciencia tranquilo, sin distracciones, unificado y concentrado. Asanga lo define como "un enfoque mental sobre el objeto investigado. Su función es ser una base para el conocimiento (jñāna)".[132]​
Dhyāna es "un estado de perfecta ecuanimidad y conciencia plena (upekkhā-sati-parisuddhi)", alcanzado a través del entrenamiento mental.[135]​
En la meditación budista se enseñan varios temas o enfoques, como la respiración, el cuerpo físico, las sensaciones agradables y desagradables y la mente misma.
La evidencia más temprana de la meditación india se encuentra en el Rigveda.[136]​ Las metodologías meditativas encontradas en los textos budistas son algunas de las primeras descripciones que han sobrevivido a la era moderna.[137]​ Es probable que estas metodologías incorporen lo que existía antes del Buda. No existe un acuerdo académico sobre el origen de la práctica de dhyāna. Algunos eruditos, como Bronkhorst, ven a los cuatro dhyānas como un invento budista.[138]​ Alexander Wynne argumenta que el Buda aprendió dhyāna de los maestros brahmánicos.[139]​
En cualquier caso, el Buda enseñó meditación con un nuevo enfoque y una teoría única de la liberación que integró los cuatro dhyanas con la atención plena y otras prácticas.[140]​ La discusión budista sobre la meditación no tiene el concepto de un atman (un yo eterno o alma), y critica tanto la meditación excesivamente ascética del jainismo como las meditaciones hindúes que buscan acceder a un Ser eterno y universal.[141]​
Los textos budistas enseñan varios esquemas de meditación. Una de las más destacadas es la de los cuatro "rupa-jhānas" (cuatro meditaciones en el ámbito de la forma), que son "etapas de concentración progresivamente más profundas".[142]​ Según Gethin, son estados de "perfecta atención, quietud y lucidez".[143]​ Se describen en el Canon Pali como estados pacíficos de plena conciencia sin ningún deseo. En los primeros textos, se representa al Buda entrando en jhāna tanto antes de su despertar bajo el árbol bodhi como también antes de su nirvana final (ver: Mahāsaccaka-sutta y Mahāparinibbāṇa Sutta).[144]​[145]​
Las cuatro rupa-jhānas son:[146]​
Existe una amplia variedad de opiniones académicas (tanto de eruditos modernos como de budistas tradicionales) sobre la interpretación de estos estados meditativos, así como opiniones variadas sobre cómo practicarlos.[147]​
Hay otros cuatro estados meditativos, referidos en los primeros textos como "arupa samāpattis" (los logros sin forma). Son vistos como formas de conciencia más elevadas y refinadas. A menudo son agrupados en el esquema de los jhānas. También se llaman jhānas inmateriales (arūpajhānas) en los comentarios. El primer logro sin forma es un lugar o reino de espacio infinito (ākāsānañcāyatana) sin forma. El segundo se denomina reino de la conciencia infinita (viññāṇañcāyatana); el tercero es el reino de la nada (ākiñcaññāyatana), mientras que el cuarto es el reino de "ni percepción ni no percepción".[148]​
En el canon Pali, el Buda describe dos cualidades meditativas que se apoyan mutuamente: śamatha ("calma") y vipassanā (sánscrito: vipaśyanā, visión clara o visión penetrante).[149]​ El Buda compara estas cualidades mentales con un "par de mensajeros rápidos" que ayudan a transmitir el mensaje de nibbana (en SN 35.245).[150]​
Las diversas tradiciones budistas generalmente ven la meditación budista como dividida en estas dos formas principales. Samatha también se llama "meditación calmante", y se enfoca en calmar y concentrar la mente, es decir, desarrollar samadhi y los cuatro dhyānas. Según Damien Keown, vipassanā, mientras tanto, se centra en "la generación de una visión penetrante y crítica (paññā)".[151]​
Existen numerosas posiciones doctrinales y desacuerdos dentro de las diferentes tradiciones budistas con respecto a estas cualidades o formas de meditación. Por ejemplo, en el Sutta de los "cuatro formas de convertirse en un arhat" (AN 4.170), se dice que uno puede desarrollar calma y luego vipassana, o vipassana y luego calma, o ambas cualidades al mismo tiempo.[152]​ Mientras tanto, en el Abhidharmakośakārikā de Vasubandhu, se dice que uno tiene que practicar samatha y alcanzar samadhi primero y luego vipaśyanā se practica con los cuatro fundamentos de la atención plena (smṛtyupasthānas).[153]​
Comenzando con los comentarios del indologo Luis de La Vallée-Poussin, una serie de estudiosos han argumentado que estos dos tipos de meditación reflejan una tensión entre dos tradiciones budistas antiguas diferentes con respecto al uso de dhyāna. Una que se centró en la práctica de vipassana y la otra que se centró exclusivamente en dhyāna.[154]​[155]​ Sin embargo, otros eruditos como Analayo y Rupert Gethin no están de acuerdo con esta tesis de "dos caminos", en cambio ven ambas prácticas como complementarias.[155]​[156]​
Los cuatro estados inconmensurables (Sct: apramāṇa, Pāli: appamaññā) o las cuatro viviendas celestiales (Brahma-viharas) son virtudes o temas para la meditación budista. Ayudan a una persona a renacer en el reino celestial (Brahma) y también a cultivar samadhi.[157]​[158]​
Los cuatro Brahma-vihara son:[157]​[159]​
Según Peter Harvey, las escrituras budistas reconocen que las cuatro prácticas de meditación Brahmavihara "no se originaron dentro de la tradición budista".[160]​
Prajñā (sánscrito) o paññā (Pāli) es sabiduría o conocimiento trascendental de la verdadera naturaleza de la existencia. Otro término que está asociado con prajñā y que a veces es equivalente a él es vipassanā (Pāli) o vipaśyanā (sánscrito), que significa "visión especial o vista clara". En los textos budistas, a menudo se dice que la facultad de vipassana se cultiva a través de los cuatro establecimientos de atención plena.[161]​
En los primeros textos, paññā se incluye como una de las "cinco facultades" (indriya) que se enumeran comúnmente como elementos espirituales importantes para ser cultivados (ver, por ejemplo: AN I 16). Paññā junto con samadhi, también figura como uno de los "entrenamientos en los estados mentales superiores" (adhicitta-sikkha).[161]​
La tradición budista considera la ignorancia (avidyā), la incomprensión o la percepción errónea de la naturaleza de la realidad, como una de las causas básicas de dukkha. Superar esta ignorancia es parte del camino hacia el despertar. Esto incluye la contemplación de la impermanencia y la verdad del "no-yo" (anatta), que desarrolla al desapego y libera a un ser de dukkha y saṃsāra.[162]​[163]​
Prajñā es importante en todas las tradiciones budistas. Se describe de diversas maneras como sabiduría con respecto a la naturaleza no permanente de los dharmas (fenómenos), conocimiento sobre la cualidad de "no-yo", la vaciedad, el funcionamiento del karma y el renacimiento, y el conocimiento de la originación dependiente.[164]​ Del mismo modo, vipaśyanā se describe de manera similar, como en el texto Theravada lammado Paṭisambhidāmagga, donde se dice que es la contemplación de las cosas como impermanentes, insatisfactorias y no-yo.[165]​
Algunos estudiosos como Bronkhorst y Vetter han argumentado que la idea de que vipassana conduce a la liberación fue un desarrollo posterior en el budismo y que hay inconsistencias con la presentación budista temprana de samadhi y vipassana.[166]​[167]​ Sin embargo, otros como Collett Cox y Damien Keown han argumentado que vipassana es un aspecto clave del proceso de liberación budista, que coopera con el samadhi para eliminar los obstáculos a la iluminación (es decir, los āsavas).[168]​[169]​
En el budismo Theravāda, el enfoque de la meditación vipassana es saber de forma continua y completa cómo los fenómenos (dhammas) son impermanentes (annica), no-yo (anatta) y dukkha.[170]​[171]​ El método más utilizado en el Theravāda moderno para la práctica de vipassanā es el que se encuentra en el Satipatthana Sutta. Hay cierto desacuerdo en Theravāda con respecto a samatha y vipassanā. Algunos en el Movimiento Vipassana enfatizan fuertemente la práctica de vipassana sobre samatha, y otros no están de acuerdo con esto.[172]​
En el budismo Mahāyāna, también se enseña y practica el desarrollo de la comprensión (vipaśyanā) y la tranquilidad (śamatha). Las diferentes escuelas del budismo Mahāyāna tienen un gran repertorio de técnicas de meditación para cultivar estas cualidades. Estos incluyen la visualización de varios budas y mandalas, la recitación del nombre de un buda y la recitación de mantras y dharanis tántricos.[173]​[174]​ Vipaśyanā en el budismo Mahāyāna también incluye obtener una comprensión directa de ciertos puntos de vista filosóficos, como la visión de la vaciedad y la visión Yogacara de "solo conciencia". Esto se puede ver en textos de meditación como el Bhāvanākrama de Kamalaśīla ("Etapas de meditación", siglo IX), que enseña una forma de vipaśyanā basada en la filosofía de Yogācāra-Madhyamaka.[175]​
El budismo se basa en las enseñanzas de un maestro espiritual y filósofo mendicante llamado "el Buda" ("el Despierto", c. siglo V al IV siglo a. C.).[176]​ Los primeros textos budistas tienen el apellido de Buda como "Gautama" (Pali: Gotama). Los detalles de la vida de Buda se mencionan en muchos textos budistas tempranos, pero son escasos y las fechas precisas son inciertas.[177]​
La evidencia de los primeros textos sugiere que nació en Lumbini y creció en Kapilavastu, una ciudad en la llanura del Ganges, cerca de la moderna frontera entre Nepal y la India, y que pasó su vida en lo que ahora son los modernos Bihar y Uttar Pradesh.[178]​ Nació en una familia de élite de la tribu Shakya, que estaba gobernada por una pequeña oligarquía o un consejo similar a una república.[179]​
Según textos tempranos como el Pali Ariyapariyesanā-sutta ("El discurso sobre la noble búsqueda", MN 26) y su paralelo chino en MĀ 204, Gautama se conmovió por el sufrimiento de la existencia cíclica, por lo que empezó una búsqueda para la liberación del sufrimiento y el renacimiento.[180]​
Los primeros textos y biografías afirman que Gautama estudió por primera vez con dos maestros de meditación, Alara Kalama (sánscrito: Arada Kalama) y Uddaka Ramaputta (Udraka Ramaputra).[181]​ Al encontrar que estas enseñanzas eran insuficientes para lograr su objetivo, recurrió a la práctica del ascetismo severo, que incluía un estricto régimen de ayuno y varias formas de control de la respiración.[182]​ Esto tampoco alcanzó su objetivo, y luego recurrió a la práctica meditativa de "dhyana". Se sentó a meditar bajo un árbol de Ficus religiosa ahora llamado "el árbol de Bodhi" en la ciudad de Bodh Gaya y alcanzó el "Despertar" (bodhi).
Según varios textos tempranos como el Mahāsaccaka-sutta y el Samaññaphala Sutta, al despertar, el Buda adquirió una visión del funcionamiento del karma y sus vidas anteriores. También logró el fin de las impurezas mentales (asavas), el fin del sufrimiento y renacimiento en saṃsāra.[182]​ Este evento también trajo certeza sobre el Camino Medio como el camino correcto de la práctica espiritual para terminar con el sufrimiento. Como un Buda completamente iluminado, atrajo seguidores y fundó una Sangha (orden monástica).[183]​
Pasó el resto de su vida enseñando el Dharma que había descubierto en todo el norte de la India, y luego murió, logrando el "nirvana final", a la edad de 80 años en Kushinagar, India.[184]​
A pesar de no ser considerado como un dios, en la tradición Gotama fue un ser humano que logró milagros como bilocación, levitación, telepatía, etcétera; por lo que alrededor de la figura de Buda existe mucha confusión sin saberse claramente si fue una persona común y corriente o un ser trascendental.[185]​
Las raíces del budismo se encuentran en el pensamiento religioso de la Edad de Hierro India a mediados del primer milenio antes de Cristo. Este fue un período de gran fermento intelectual y cambio sociocultural conocido como "la segunda urbanización", marcado por el crecimiento de las ciudades y el comercio, la composición de los primeros Upanishads y el surgimiento histórico de las tradiciones Śramaṇa como el Jainismo.
Los primeros textos budistas incluyen los cuatro principales Nikāyas Pali, sus Agamas paralelos que se encuentran en el canon chino (y otros textos fragmentarios paralelos) junto con el cuerpo principal de las reglas monásticas, que sobreviven en las diversas "patimokkhas".[186]​[187]​ Sin embargo, estos textos fueron revisados con el tiempo, y no está claro qué constituye la primera capa textual. Un método para obtener información sobre el núcleo más antiguo del budismo es comparar las versiones existentes más antiguas de los diversos textos sobrevivientes.
Ciertas enseñanzas básicas aparecen en muchos lugares a lo largo de los primeros textos, lo que ha llevado a la mayoría de los estudiosos a concluir que Gautama Buda debe haber enseñado algo similar a las Cuatro Nobles Verdades, el Noble Sendero Óctuple, el Nirvana, las tres marcas de la existencia, los cinco agregados, la originacion dependiente, el karma y el renacimiento.[188]​ Según N. Ross Reat, todas estas doctrinas son compartidas por los textos Theravada Pali y el Śālistamba Sūtra de la escuela Mahasamghika.[189]​ Un estudio reciente de Bhikkhu Analayo concluye que Theravada Majjhima Nikaya y Sarvastivada Madhyama Agama contienen las mismas doctrinas principales.[190]​

Sin embargo, algunos estudiosos argumentan que el análisis crítico revela discrepancias entre las diversas doctrinas encontradas en estos primeros textos, y la autenticidad de ciertas enseñanzas y doctrinas como el karma y las cuatro nobles verdades han sido cuestionadas.[191]​[192]​El budismo puede haberse extendido solo lentamente por toda la India hasta la época del emperador mauria Ashoka (304–232 a. C.), quien era un defensor público de la religión. El apoyo de Ashoka y sus descendientes condujo a la construcción de más estupas (como en Sanchi y Bharhut), templos (como el Templo Mahabodhi) y se extendió por todo el imperio Mauria y en tierras vecinas como Asia Central y Sri Lanka.
Durante y después del período de Mauria (322-180 a. C.), los cismas en la comunidad budista dieron lugar a varias escuelas, debido a la geografía, así como a disputas relacionadas con la disciplina monástica y la doctrina budista.[193]​ Cada Saṅgha también comenzó a acumular su propia versión del Tripiṭaka ("canasta triple").[194]​ En su Tripiṭaka, cada escuela incluía los Suttas (discursos) del Buda, un Vinaya (regla monástica) y una canasta de Abhidharma, que eran textos sobre la clasificación escolástica detallada, resumen e interpretación de los Suttas.[194]​ Los detalles de la doctrina en los Abhidharmas de varias escuelas budistas difieren significativamente, y estos se compusieron a partir del siglo III a. C..[195]​
El imperio Kushan (30–375 d. C.) llegó a controlar el comercio de la Ruta de la seda a través de Asia Central. Esto los llevó a interactuar con el budismo de Gandhara y el Grecobudismo de Bactriana. Los Kushans patrocinaron el budismo en todas sus tierras, y muchos centros budistas fueron construidos o renovados, especialmente por el emperador Kanishka (128-151 d. C.).[196]​[197]​ El apoyo Kushan ayudó al budismo a expandirse a una religión mundial a través de las rutas comerciales.[198]​ El budismo se extendió a Khotan, a la cuenca del Tarim y a China (al menos para el siglo II EC).[197]​ 
Tras convertirse en una religión importante en China, el budismo chino se exportó al resto de Asia Oriental. Desde China, el budismo se introdujo a Corea (en siglo IV), a Japón (siglos VI-VII) y a Vietnam (siglos IX-II).[199]​[200]​En cada una de estas regiones, el budismo desarrolló sus propias características y tradiciones, convirtiéndose en el budismo japonés y el budismo coreano.
La escuela Theravada llegó a Sri Lanka en el siglo III a. C. Sri Lanka se convirtió en una base para su posterior expansión al sudeste asiático después del siglo V d. C..[201]​[202]​ El budismo Theravada fue la religión dominante en Birmania durante el Reino Hanthawaddy (1287-1552).[203]​ También se hizo dominante en el Imperio Jemer durante los siglos XIII y XIV y en el Reino Sukhothai de Tailandia durante el reinado de Ram Khamhaeng (1237 / 1247-1298).[204]​
Los orígenes del budismo Mahāyāna ("Gran Vehículo") no se comprenden bien y hay varias teorías en competencia sobre cómo y dónde surgió este movimiento nuevo. Las teorías incluyen la idea de que comenzó como varios grupos que veneraban ciertos textos o que surgió como un movimiento ascético forestal.[205]​ Las primeras obras de Mahāyāna se escribieron entre el siglo I a. C. y el siglo II d. C.[205]​ [206]​
No hay evidencia de que Mahāyāna alguna vez se haya referido a una escuela separada o secta formal con un código monástico separado, sino que existió como un cierto conjunto de ideales y doctrinas posteriores para bodhisattvas.[207]​ [208]​ Mahāyāna inicialmente parece haber seguido siendo un pequeño movimiento minoritario que estaba en tensión con otros grupos budistas, luchando por la aceptación entre la mayoría.[209]​ Sin embargo, durante los siglos V y VI d. C., parece haber habido un rápido crecimiento del budismo Mahāyāna, que se muestra por un gran aumento en la evidencia epigráfica y manuscrita. Sin embargo, seguía siendo una minoría en comparación con otras escuelas budistas.[210]​
Durante el período Gupta (siglos IV-VI) y el imperio de Harṣavardana (c. 590-647), el budismo continuó siendo influyente en la India, y las grandes instituciones de aprendizaje budista como las universidades Nalanda y Valabhi estaban en su apogeo.[211]​ El budismo también floreció bajo el apoyo del Imperio Pāla (siglos VIII-XII).
Durante este período de Mahāyāna tardío, se desarrollaron cuatro tipos principales de pensamiento: Mādhyamaka, Yogācāra, Naturaleza de buda (Tathāgatagarbha) y la escuela de Pramana de Dignaga.[212]​ Bajo las dinastías Gupta y Pala, el budismo tántrico o Vajrayana se desarrolla. Promovió nuevas prácticas, como el uso de mantras, dharanis, mudras, mandalas y la visualización de deidades y budas. También se desarrolló una nueva clase de literatura, los tantras budistas. Esta nueva forma de budismo esotérico se remonta a grupos de magos yoguis errantes llamados mahasiddhas.[213]​[214]​
Ya durante esta era posterior, el budismo estaba perdiendo el apoyo estatal en otras regiones de la India, incluidas las tierras de las siguiente dinastías: los Karkotas, los Pratiharas, los Rashtrakutas, los Pandyas y los Pallavas. Esta pérdida de apoyo a favor de las religiones hindúes como el vaishnavismo y el shaivismo, es el comienzo del largo y complejo período del declive del budismo en el subcontinente indio.[215]​ Las invasiones islámicas y la conquista de la India (siglos X al XII) dañaron y destruyeron a muchas instituciones budistas, lo que llevó a su eventual desaparición de la India por el siglo XII.[216]​
El budismo ha enfrentado varios desafíos y cambios durante la colonización de los estados budistas por parte de los países cristianos y su persecución bajo los estados modernos. Tras un primer contacto con Occidente en el siglo XIII (por medio de viajeros como Marco Polo o Rubruquis), en el siglo XVII se intensificó la llegada de misioneros europeos —principalmente jesuitas— que ofrecían del budismo una imagen negativa, interpretándola como «religión del vacío» y de «negación de Dios».[217]​
Una respuesta a algunos de estos desafíos se ha llamado el modernismo budista. Las primeras figuras modernistas budistas, como el converso estadounidense Henry Olcott (1832-1907) y Anagarika Dharmapala (1864-1933), reinterpretaron y promovieron el budismo como una religión racional que consideraban compatible con la ciencia moderna.[218]​
En el siglo XIX, el budismo comenzó a ser estudiado por eruditos occidentales. Fue el trabajo de eruditos pioneros como Eugène Burnouf, Max Müller, Hermann Oldenberg y Thomas William Rhys Davids lo que allanó el camino para los estudios budistas modernos. Este período también vio a los primeros conversos occidentales al budismo, como Helena Blavatsky y Henry Steel Olcott.[219]​
Mientras tanto, el budismo del este asiático sufrió bajo varias guerras que asolaron China durante la era moderna, como la rebelión Taiping y la Segunda Guerra Mundial (que también afectó al budismo coreano). Durante el período republicano (1912–49), figuras como Taixu (1899–1947) desarrollaron un nuevo movimiento llamado "budismo humanista", y aunque las instituciones budistas fueron destruidas durante la Revolución Cultural (1966–76), ha habido un renacimiento de la religión en China después de 1977.[220]​ El budismo japonés también pasó por un período de modernización durante el período Meiji.[221]​ Mientras tanto, en Asia Central, la llegada de la represión comunista al Tíbet (1966-1980) y Mongolia (entre 1924-1990) tuvo un fuerte impacto negativo en las instituciones budistas, aunque la situación ha mejorado un poco desde los años 80 y 90.[222]​
Después de la segunda guerra mundial, la inmigración desde Asia, la globalización, la secularización de la cultura occidental y un renovado interés en el budismo entre la contracultura de los años 60 llevaron a un mayor crecimiento en las instituciones budistas occidentales.[223]​
El budismo no está sistematizado en una organización jerárquica vertical. La autoridad religiosa descansa en los textos sagrados: los Sutras, que son discursos del Buda Gautama y sus discípulos. Además de eso, hay un numeroso material de interpretación en el que contribuyen maestros y personajes a través de la historia que los han comentado y analizado.
Un esquema común utilizado por los estudiosos se centra en tres formas geográficas o culturales: [224]​[225]​[226]​
La comunidad monástica se organiza históricamente por líneas de transmisión en el tiempo y en algunas escuelas las cadenas de relaciones entre maestros y discípulos son centrales. Los laicos tienen distinto papel dependiendo de las dos grandes ramas, Theravāda (‘escuela de los ancianos’) y Mahāyāna (‘gran vehículo’). En el budismo Mahayana, la vida laica se considera tan útil para alcanzar el Nirvana como la vida monástica, mientras que en el Theravada se da un énfasis a la vida monástica.[227]​[228]​ Otra clasificación muy común es identificar una tercera rama; el Vajrayāna (o Tántrico), que se puede considerar una parte o una división del Mahayana.[229]​
Esta organización descentralizada ha permitido una enorme flexibilidad de puntos de vista, variaciones y enfoques.[230]​ Las variantes de budismo se dieron por divisiones en el tiempo de puntos de discusión doctrinales, como a su vez, por distintos contextos sociales y geográficos, como un árbol ramificado.[231]​
En general, el budismo se fue implantando en muchos países sin entrar en conflicto directo con las religiones autóctonas, sino en muchos casos, intercambiando influencias. A diferencia de otras religiones, el budismo no conoce la noción de guerra santa, la conversión forzada, ni tampoco considera la noción de herejía como algo siempre pernicioso.[232]​ Aunque han existido algunos episodios históricos de enfrentamientos violentos por cuestiones de doctrina o de acoso a personajes disidentes o algunas minorías, estos son excepcionales para una religión que se convirtió en la mayoritaria de Asia Oriental durante un recorrido histórico de 2500 años.[233]​[234]​[235]​ La pluralidad de enfoques y la aceptación de distintos puntos de vista doctrinales ha sido históricamente algo compartido y aceptado en la comunidad budista, lo que ha dado lugar a una enorme cantidad de literatura religiosa y filosófica.[236]​
Las estimaciones sobre el número de budistas en el mundo varían significativamente, según diferentes fuentes disponibles,[237]​[238]​[239]​[240]​ siendo las más modestas entre los 200 y los 330 millones de seguidores.[241]​ La página web budista Buddhanet considera que los 350 millones podría ser la cifra más consensuada,[241]​ la cual no incluye a las personas que solo simpatizan con el budismo o que siguen el budismo al lado de otra religión como el taoísmo, el sintoísmo o el cristianismo, algo que no es poco común.[242]​[243]​[244]​[245]​[246]​ La página web Adherentes.com establece el número de budistas en 375 millones (6 % de la población mundial).[247]​ En cualquiera de estas mediciones el budismo es la cuarta religión más grande del mundo después del cristianismo, el islam y el hinduismo, seguida por la religión tradicional china. Otros cálculos menos moderados elevan la cantidad de budistas a 500 millones,[242]​ pero el número exacto en general es incierto y difícil de definir por las características propias del budismo y los países por donde se ha extendido.
En cualquier caso, esto significa que el budismo es de las mayores religiones de la humanidad en número de seguidores. Estas cifras han aumentado considerablemente tras las recogidas en el siglo XX, sobre todo porque en países como China empiezan a aparecer los datos tras su apertura política. Asimismo, en India se han dado conversiones masivas al budismo de cientos de miles de personas pertenecientes a la casta de los intocables (dalits), debido al trato que les da la religión hindú.
La mayor parte de los budistas están en Asia. Para obtener una cifra mundial más exacta, la principal dificultad es dar una cifra sobre China. El budismo posee un importante arraigo histórico en ese país, sin embargo, es oficialmente un país ateo, en el que además se practica una religión popular tradicional muy heterogénea y sincretista que, entre otros, incluye elementos budistas, que con frecuencia se lista por separado.
En los países de Occidente el número de budistas ha crecido significativamente desde los años 1960.[248]​ En Europa Occidental cuenta con 20 millones de seguidores y conforma hoy el 5 % de la población. En Estados Unidos el budismo tiene una gran implantación, con unos cuatro millones de seguidores.[249]​
En el ámbito educativo, el budismo se estudia como especialidad en algunos de los principales centros universitarios occidentales.[250]​ Algunas de las universidades más prestigiosas (Oxford, Harvard, Lausana, Berkeley, Salamanca, Milán) tienen una sección de estudios de religiones y lenguas orientales con especialidad sobre budismo.[251]​[252]​[253]​[254]​[255]​ Asimismo, en los países donde el budismo representa una mayoría o porcentaje significativo, existen centros de educación superior dedicado al estudio y formación en el budismo, tales como: el Institute of Buddhist Studies en California, la Dongguk University en Corea del Sur, la Bukkyo University y la Universidad de Sōka, ambas en Japón, el International Buddhist College en Tailandia y la Universidad de Sri Jayewardenepura en Sri Lanka, entre muchas otras instituciones.
El budismo, como todas las religiones indias, era una tradición oral en la antigüedad.[256]​ Las palabras del Buda, las primeras doctrinas, conceptos y las interpretaciones fueron transmitidas de generación en generación por el boca a boca en los monasterios y no a través de textos escritos. Los primeros textos canónicos budistas probablemente fueron escritos en Sri Lanka, unos 400 años después de la muerte de Buda.[256]​ Los textos fueron parte de los Tripitakas y muchas versiones aparecieron a partir de entonces afirmando ser las palabras del Buda. Los textos académicos de comentarios budistas, con autores nombrados, aparecieron en la India, alrededor del siglo II EC.[256]​ Estos textos fueron escritos en pali o en sánscrito, a veces en idiomas regionales, como manuscritos de hojas de palma, corteza de abedul, rollos pintados, tallados en las paredes de los templos, y más tarde en papel.[256]​
A diferencia de lo que la Biblia es para el cristianismo y el Corán es para el Islam, pero como todas las principales religiones indias antiguas, no hay consenso entre las diferentes tradiciones budistas en cuanto a lo que constituye las escrituras o un canon común en el budismo.[256]​ La creencia general entre los budistas es que el corpus canónico es vasto.[257]​[258]​ Este corpus incluye los antiguos Sutras organizados en Nikayas, la misma parte de tres canastas de textos llamadas Tripitakas.[259]​ Cada tradición budista tiene su propia colección de textos, gran parte de los cuales es la traducción de antiguos textos budistas pali y sánscrito de la India.
En el budismo Theravada, la colección estándar de textos sagrados es el Canon Pali. El Pāli Tipitaka, que significa "tres cestas", se refiere al Vinaya Pitaka, el Sutta Pitaka y el Abhidhamma Pitaka. Estos constituyen las obras canónicas completas más antiguas en una lengua indoaria del budismo. El Vinaya Pitaka contiene reglas disciplinarias para los monasterios budistas. El Sutta Pitaka contiene palabras atribuidas al Buda. El Abhidhamma Pitaka contiene exposiciones y comentarios sobre el Sutta, y estos varían significativamente entre las escuelas budistas.
El canon budista chino incluye 2184 textos en 55 volúmenes, mientras que el canon tibetano comprende 1.108 textos -todos ellos afirmados por el Buda- y otros 3461 compuestos por eruditos indios venerados en la tradición tibetana.[259]​ La historia textual budista ha sido vasta; más de 40,000 manuscritos, en su mayoría budistas, algunos no budistas, fueron descubiertos en 1900 solo en la cueva de Dunhuang en China.[259]​
La crítica al budismo, al igual que la crítica a la religión en general, puede encontrarse en quienes están en desacuerdo con las afirmaciones de preguntas, creencias u otros factores de diversas escuelas budistas. Algunas denominaciones budistas, muchos países predominantemente budistas y los líderes budistas han sido criticados de una manera u otra.[cita requerida] Fuentes de crítica pueden venir a partir de ejemplos como los agnósticos, los escépticos, la filosofía antirreligiosa, los partidarios de otras religiones (como el hinduismo) o por los budistas que abrazan la reforma o, simplemente, expresan disgusto.[cita requerida]
También se ha criticado el uso que se hace de herramientas budistas como la meditación simplemente para aliviar el estrés que producen dinámicas sociales injustas o incluso como forma de desconectarse del sufrimiento (en vez de acabar con su causa) como el baipás espiritual.[cita requerida]

El varón (del latín varo, -ōnis ‘valiente, esforzado’,[3]​ probablemente relacionado con vir ‘varón, héroe’, bajo la influencia del germánico baro ‘hombre libre’)[4]​[5]​ u hombre (homo, -ĭnis,[6]​ probablemente relacionado con humus ‘tierra, suelo’, del que también derivaría humānus ‘relativo al hombre, humano’)[7]​ es el ser humano de sexo masculino, independientemente de si es niño o adulto. La anatomía masculina se distingue de la femenina por el sistema reproductor masculino, que incluye el pene, los testículos, el conducto espermático, la próstata y el epidídimo, así como las características sexuales secundarias. 
Varón u hombre también remite a diferencias de carácter cultural y social que se le atribuyen por género.
Como la mayoría de los otros mamíferos machos, el genoma de un hombre generalmente hereda un cromosoma X de la madre y un cromosoma Y del padre. El feto masculino produce mayores cantidades de andrógenos y menores cantidades de estrógenos que un feto femenino.[8]​ Esta diferencia en las cantidades relativas de estos esteroides sexuales es responsable de las características fisiológicas que distinguen a los hombres de las mujeres. Durante la pubertad, las hormonas que estimulan la producción de andrógenos dan como resultado el desarrollo de características sexuales secundarias, exhibiendo así mayores diferencias entre los sexos. Estos incluyen una mayor masa muscular, el crecimiento del vello facial y una menor composición de grasa corporal.[9]​
El Día Internacional del Hombre se celebra el 19 de noviembre.
Después de la fecundación durante las primeras etapas celulares se define a nivel biológico si el ser futuro será masculino o femenino resultando en este caso que el cromosoma 23 sea tipo XY determinando el desarrollo futuro del infante y del adulto, generando pene y testículos externos y posteriormente desencadenando un proceso hormonal segregando principalmente testosterona a partir de la adolescencia que creará como consecuencia las características sexuales secundarias masculinas.[10]​
La testosterona es una hormona androgénica propia del macho en muchas especies, que permite desarrollar los músculos del varón con poco esfuerzo[11]​ y es determinante en parte de su desarrollo físico y de las características sexuales secundarias.[12]​
El aparato reproductor masculino garantiza que el varón tenga la capacidad de fecundar el óvulo femenino y en ello la transmisión de la información genética por medio de la célula espermatozoidal. Los órganos sexuales primarios del varón son exteriores, a diferencia de los de la mujer que son internos.[13]​ La andrología es la ciencia que estudia el aparato reproductor masculino.
Entre las características secundarias más comunes que empiezan a desarrollarse a partir de la pubertad y la edad viril (y que no necesariamente son siempre así) sin que su ausencia vaya en contra de la identidad masculina, se cuentan las siguientes[14]​[15]​[16]​[17]​[18]​[19]​[20]​[21]​[22]​
Un ser humano del sexo masculino es varón desde el momento en el cual es concebido: el espermatozoide contiene los cromosomas sexuales diferenciados X o Y, mientras la hembra tiene el cromosoma homogamético X. La combinación cromosómica entre el espermatozoide y el óvulo determina el sexo del individuo concebido, lo que da como resultado que un feto pueda ser determinado como “hembra” si la combinación cromosómica es XX y como varón si es XY. La combinación genética XY es más frecuente que la combinación genética XX, mientras que la mortalidad infantil es menor en varones recién nacidos que en niñas.
El varón infante recibe el nombre de “niño” al menos hasta el inicio de su pubertad. También es popular llamarlo “mozo”, palabra que lo determina hasta su primera juventud (aproximadamente hasta los 20 años de edad). Durante este tiempo comienza todo el proceso de desarrollo físico, psicológico y social como “varón” que le permitiría desarrollar un rol determinado por la cultura a su condición humana masculina.
Tanto varones como mujeres son víctimas del mismo tipo de enfermedades que afectan al género humano, pero cada género tiene una tendencia mayor a un determinado tipo. Las síndromes que más se manifiestan en el varón son el autismo, el daltonismo y el mal de Alzheimer, que ataca principalmente en la edad mayor, pero puede presentarse en varones jóvenes.
La esperanza de vida masculina, como la femenina, varía considerablemente de acuerdo al desarrollo de cada sociedad.
En cuanto a la tasa de mortalidad infantil a nivel global, se considera que los varones recién nacidos tienen una mayor esperanza de vida que las niñas. El desfasaje entre la población neonata masculina y femenina se equipara durante la adolescencia, tiempo en el cual aumenta en todos los continentes la morbilidad masculina por encima de la femenina debido a la mayor participación de los varones en confrontaciones armadas, guerras o simplemente en el desafío del peligro. Otros riesgos como el consumo de estupefacientes, alcohol, enfermedades de transmisión sexual y violencia urbana, mayor entre los varones que entre las muchachas, reducen la población masculina adolescente en todo el mundo.
De acuerdo con los datos del Servicio de Endocrinología y Metabolismo de la Unidad Asistencial Dr. César Milstein de Buenos Aires, en Argentina, el síndrome de Klinefelter (en adelante, SK) tiene una prevalencia de 0.2 %.[23]​ Sin embargo, esta condición raramente es diagnosticada en la infancia. Las primeras consultas en relación con el SK suelen aparecer durante la adolescencia, suscitadas fundamentalmente por los primeros atisbos de cambios en el cuerpo que conlleva la pubertad. Hasta no hace mucho tiempo, las y los niños que tenían esta particularidad crecían y se desarrollaban sin ser conscientes de ninguna anomalía y desarrollaban una vida sexual común.[24]​ Otro motivo para consultar es la preocupación en torno a problemáticas vinculadas con la infertilidad, a pesar de que en algunos casos se pudo observar que en sujetos donde había bastantes células germinales funcionando normalmente en los testículos, podrían fecundar sin inconvenientes.[25]​ En la actualidad muchos agentes de la salud e investigadores científicos de disciplinas conexas fueron dejando en desuso el término "síndrome de Klinefelter", usando en su lugar la descripción de "varones XXY".
En general no afecta al fenotipo, pero en ocasiones se tiene estatura y extremidades mayores por lo que a veces se le llama síndrome del supermacho.[26]​
La sexualidad y la atracción masculinas varían de persona a persona, y el comportamiento sexual de un hombre puede verse afectado por muchos factores, incluidas las predisposiciones evolucionadas, la personalidad, la educación y la cultura.[27]​
Si bien la mayoría de los hombres son heterosexuales, una minoría son homosexuales o bisexuales.[28]​ 
El símbolo del género masculino occidental actual es un círculo con una flecha y se conoce como "lanza de Marte" o "flecha de Ares".[29]​ Se ha sugerido que el arma es un símbolo fálico.[30]​ Es el símbolo que representa el dios y el planeta Marte, ambos considerados símbolos de la masculinidad, frente a Venus, que simboliza la feminidad. La corriente mitopoético establece cuatro arquetipos que conforman la psicología del hombre: El Rey, El Guerrero, El Mago y El Amante.
Corporalmente, en algunas sociedades, se consideran símbolos masculinos y del hombre la barba o unos genitales de tamaño grande, que se pueden resaltar o amplificar visualmente por medio de la indumentaria y accesorios.
La discusión acerca de las diferencias entre varones y mujeres, especialmente en Occidente, no es unánime. Psicológicamente, la asociación tradicional de aptitudes y actitudes a un género normalmente se basa en suposiciones consolidadas por el hábito de la observación directa, de la actividad y personalidad de las personas de ambos géneros en el contexto social. Esta asociación se arraiga principalmente en la edad infantil.[31]​
Los estereotipos masculinos varían según el nivel cultural de la sociedad, la edad y el momento histórico. Por ejemplo, estudiantes y personas adultas definen de forma diferente lo que se considera masculino. Los estudiantes elaboran unos estereotipos de rol de género más claramente definidos que las personas adultas. Los estereotipos masculinos normalmente está más definido que los estereotipos femeninos.[32]​ No obstante, esta asignación de características es cada vez más alejada de la realidad, por lo que los mismos estereotipos de género van cambiando paulatinamente, conforme al cambio de tareas tradicionalmente asignadas a uno de los dos sexos como, por ejemplo, la incorporación de la mujer al mundo laboral. Así mismo, el incremento de la actividad de las mujeres en los ámbitos deportivos propicia un cambio del estereotipo tradicional masculino.[33]​
Las sociedades y culturas orientales o más conservadoras asumen muchos de esos estereotipos como lo que es o debe ser en el varón, pero la era de la globalización poco a poco los hace entrar en el debate. Entre los "estereotipos" más comunes se pueden enumerar:
Muchos de estos paradigmas tienen fundamento científico, mientras que otros no (aunque la sociedad ha hecho que muchos de estos estereotipos sean realidad como por ejemplo, el saludo de dos mujeres puede ser beso, entre hombre y mujer también, pero entre hombres es raro sin ninguna razón, etc). Por ejemplo, no es sencillo separar los elementos innatos de la biología masculina de aquellos que han sido influenciados por la cultura. En tal caso, la agresividad puede darse tanto en el varón como en la mujer de acuerdo al ambiente en que estos se desenvuelvan. La mayor masa corporal y muscular del varón y las culturas patriarcales contribuyen a acentuar el estereotipo de la agresividad masculina. Los grupos feministas en sus estudios señalan que en la violencia intrafamiliar, el abuso infantil, el maltrato infantil y la violencia contra la mujer, tienen como principal verdugo en la mayoría de los casos al varón tanto de países industrializados como en vías de desarrollo.
Algunos de estos estereotipos se asocian, en ocasiones erróneamente y en ocasiones acertadamente con los niveles de hormonas sexuales masculinas, como la testosterona, o la menor cantidad de hormonas sexuales femeninas, como los estrógenos. En el caso de la agresividad, tradicionalmente relacionada con el nivel de testosterona, algunos estudios indican que dicha relación no corresponde con sus resultados.[38]​
Desde su nacimiento se viste a los varones de celeste y se les enseña a creer que productividad, conquista, poder, hiperactividad y penetración son sinónimos de virilidad. De pequeños se les enseña a no llorar, a no ser vulnerables, a no quejarse, a no mostrar sus debilidades ni sus sentimientos y a ser autosuficientes y no pedir ayuda. Se les enseña a confundir acción y agresión con masculinidad, a rendir en los deportes aún a expensas de su propia salud, a exponerse a peligros y a deportes de riesgo. Las consecuencias de la adecuación a este marcado estereotipo social se las puede encontrar en los servicios de terapia intensiva de los hospitales con mayoría masculina, en la población carcelaria, donde la gran mayoría de los reclusos son varones, en las estadísticas de accidentes y en los hechos delictivos que leemos en los diarios.[39]​
La educación masculina depende en gran parte de la discusión de los estereotipos masculinos en el grado en que estos sean asumidos por una sociedad. La educación entonces que parte desde el hogar dada al niño, pasa por la formal y se expresa en las relaciones sociales y en la imagen que presentan los medios de comunicación, tiene diversos matices que dependen de la cultura del país, continente o región del mundo.
La primera educación de la sexualidad y socialización del niño parte del hogar. El padre y la madre son los encargados de transmitir la primera información sobre el rol sexual que desempeñará el niño en sociedad. En general, el padre transmitirá al hijo varón las características psicológicas de su sexualidad. En ello entran en juego los paradigmas asumidos y las maneras de ser del varón en la sociedad en la que nació. La manera de vestirse, de llevar el cabello, de hablar, de modular la voz, el tipo de juegos, los juguetes, las exigencias disciplinarias diferenciadas entre el varón y la mujer, la casi ausencia de cosméticos y otros muchos elementos, determinan poco a poco la conciencia propia del ser un varón en sociedad.
Llegada la pubertad, el papel del padre adquiere un rol más activo en la educación del hijo varón. En muchas culturas este paso entre el niño y el hombre es celebrado. Entre culturas del orden natural como tribus y clanes, el muchacho debe afrontar un número determinado de desafíos que le permitirán ser respetado en su grupo social como un varón adulto. En antiguas culturas célebres por su formación militar como los griegos (Esparta por ejemplo), China, Japón (los Samurái), los Azteca, los Quechua y los Chibcha, el paso a la edad adulta del muchacho era marcado por su capacidad de prepararse como un guerrero y su aceptación y aprecio social nacían de su coraje demostrado en las luchas, artes marciales y batallas. Pero también la religión tiene un papel del primer orden en la formación masculina del muchacho. La pubertad está marcada por un rito de iniciación que da al muchacho un estatus social y religioso. Por ejemplo, en el judaísmo, este viene representado en el bar mitzvah, celebración que le da al varón adolescente el derecho de leer los libros sagrados en la Asamblea. Para el cristianismo, ese momento viene marcado por la Confirmación.
Pasada la pubertad, el muchacho comienza un camino de desarrollo final hacia la adultez en la cual compite por demostrar la capacidad de su identidad como varón. Los deportes de competencia y fuerza física, por ejemplo, adquieren una enorme importancia, el afán por tener una pareja, el ingreso en un grupo social de adolescentes (la pandilla), la búsqueda de una vocación y otros son la preocupación del muchacho, situaciones no siempre pacíficas. Resta el peligro del consumo de drogas, alcohol, fumar, delincuencia y otros males sociales en el cual el joven ingresa en muchos casos llevado por el ánimo de una búsqueda de su propia identidad e independencia.
El rol sexual del varón adquiere su máxima plenitud en el matrimonio como marido y como padre. El rol masculino ha tenido una diversidad de influencias a lo largo de la historia. La Revolución industrial, la Revolución Femenina y otros momentos, han tenido sus consecuencias en la figura del padre y marido. Obviamente partimos de una lectura de Occidente, porque en otras culturas no occidentales, este papel puede estar marcado por una concepción más tradicionalista como la llamada Familia patriarcal en la cual la figura paterna es el centro de toda autoridad. En India y otros sitios de la Tierra, se practica la dote en la cual el padre de la hija paga una cierta cantidad al padre del hijo varón. En cambio, en otros países, como Camboya, la tradición es la opuesta: es el padre del hijo varón quien da la dote al padre de la hija. Pero en ambos casos, la libertad de ambos jóvenes se ve restringida en la escogencia del cónyuge, la cual es decisión de sus padres. Casos similares se presentan entre las culturas musulmanas, muchas de las cuales todavía practican la poligamia, es decir, el varón puede casarse con varias mujeres como solución a la escasez de hombres que morían en la guerra.
Históricamente, el mundo occidental, la belleza masculina en el arte se ha visto casi siempre desde el punto de vista masculino, es decir la belleza de los hombres según hombres homosexuales o heterosexuales. En los siglos XX y XXI algunas artistas y feministas han interesado a dar nuevos modelos de belleza, sexual o no, femenina, priorizando esta investigación sobre el trabajo de la belleza masculina desde el punto de vista de las mujeres.
En la antigua Grecia, las esculturas de figuras masculinas estaban destinadas a los hombres pederastas de la aristocracia y representaban cuerpos jóvenes siempre desnudos o semidesnudos, incluso cuando realizaban tareas a las que las personas suelen llevar ropa, como ir a combate, influyendo sobre la pintura y escultura posteriores. Ya en la época arcaica, por ejemplo, los cuerpos masculinos estaban parcial o totalmente desnudos siempre mientras que las representaciones femeninas vestían xitons o peplo. El ideal masculino estaba basado en los jóvenes atletas y gimnastas. El imperio romano, un pueblo más guerrero, al atleta le añadió una armadura. La belleza masculina clásica es pues un cuerpo muy joven, adolescente o casi, de complexión atlética, sin pelo facial y a menudo con cabello ondulado, largo y abundante, aunque el corte de pelo puede depender de modas locales.[40]​[41]​ 
En la Edad Media, en Europa, el pelo largo indicaban virilidad. El ideal de belleza se asociaba a un caballero guerrero, y como tal debía ser alto, fuerte y con hombros anchos para aguantar la armadura, manos grandes como símbolo de habilidad con la espada y piernas largas y rectas. Las vestimentas favorecedoras eran las correspondientes a un caballero preparado para el combate.
El David de Miguel Ángel se considera el ideal de belleza masculina del renacimiento italiano. El ideal de belleza masculina según los hombres del renacimiento era una figura con pectorales anchos, sin pelo facial, pelo largo y brillantes, cejas pobladas y marcadas y la mandíbula ancha.
En el barroco la belleza se asocia con la artificialidad, la fastuosidad y la coquetería. Destacan los vestidos suntuosos de múltiples capas, bajo los que se adivinan cuerpos más redondos que los del renacimiento, y zapatos de tacón, joyas, encajes, pomposidad y perfumes. En cuanto a la cabeza ahora está de moda la profusión del pelo, muchas veces con peluca, la piel muy blanca y las mejillas rosadas, con mucho maquillaje, pecas y carmines.
En el siglo XXI, algunos experimentos en psicología basados en puntuaciones o comentarios de personas frente a fotografías de hombres con rasgos diferentes sostienen que parece que en los hombres se valora que sea alto, con un torso tendiendo a la forma de V, con los hombros anchos y, algunas culturas más que otras valoran los músculos muy desarrollados.[42]​
Según un informe de 2014 de la Oficina de las Naciones Unidas contra la Droga y el Delito (UNODC), de las 437.000 personas asesinadas en todo el mundo en 2012, el 95 % de los perpetradores eran hombres que también representaban el 80 % de las víctimas. Cuando los homicidios ocurren en el contexto de violencia doméstica, en el 15 % de los casos, el 70 % de las víctimas son mujeres. A diferencia de las mujeres, que corren un mayor riesgo de ser asesinadas por conocidos, los hombres son asesinados principalmente por extraños. Uno de cada siete asesinatos  en el mundo es el de un hombre muy joven.[43]​
Según un informe de 2010 de la Oficina de las Naciones Unidas contra la Droga y el Delito y un estudio de la Organización Mundial de la Salud de 2017, la mayoría de las violaciones son cometidas por hombres y la mayoría de las víctimas son mujeres, la misma proporción existe para la violencia doméstica y la agresión sexual.[44]​[45]​
El Hombre de Vitruvio, obra de Leonardo Da Vinci, considerada una de las más perfectas proporciones del varón
Obra de Miguel Ángel, el David, considerada la más grande representación artística de la figura masculina.
Fotografía de un varón desnudo, tomada por Wilhelm von Gloeden (1895).

El socialismo es una corriente filosófica política, social y económica, y una ideología, que abarca una gama de sistemas socioeconómicos caracterizados por la propiedad social de los medios de producción,[1]​[2]​[3]​ y la autogestión de empresas por parte de los trabajadores. Incluye teorías políticas y los movimientos asociados con tales sistemas. La propiedad social puede ser pública, colectiva o cooperativa.[4]​[5]​ La RAE define el término socialismo como: «Un sistema de organización social y económica basado en la propiedad y administración colectiva o estatal de los medios de producción y distribución de los bienes».[6]​ El sistema socialista implica, por tanto, una planificación y una organización colectiva consciente de la vida social y económica orientada a la satisfacción de necesidades.[7]​[8]​ No obstante, hay muchos tipos de socialismo y no existe una definición única que las englobe a todas, siendo la propiedad social el elemento común compartido por sus diversas formas[9]​ cuyo objetivo es sortear las ineficiencias y crisis tradicionalmente asociadas con la acumulación de capital y el sistema de ganancias sobre la base de la explotación laboral.[10]​[11]​
La ideología socialista critica los males y las injusticias del capitalismo (como la distribución desigual de la riqueza, la feroz competitividad en el mercado, o la incapacidad de autorrealización y desarrollo humano, etc.) trascendiéndolo por un sistema socioeconómico autodenominado moralmente superior.[8]​
Subsisten sin embargo criterios encontrados respecto a la necesidad de la centralización de la administración económica mediante el Estado como única instancia colectiva en el marco de una sociedad compleja,[12]​[13]​ frente a la posibilidad de formas diferentes de gestión descentralizada de la colectividad socialista, tanto por vías autogestionarias como de mercado, así como mediante el empleo de pequeñas unidades económicas socialistas aisladas y autosuficientes.[14]​[15]​ El primer acto en que el Estado se manifiesta efectivamente como representante de toda la sociedad: la toma de posesión de los medios de producción en nombre de la sociedad, es a la par su último acto independiente como Estado.[16]​
Los sistemas socialistas se dividen en formas de no mercado y de mercado.[17]​ El socialismo de no mercado implica reemplazar los factores de mercado y el dinero con planificación e ingeniería económica integrada o criterios técnicos basados ​​en cálculos realizados en especie, produciendo así un mecanismo económico diferente que funciona de acuerdo con leyes y dinámicas económicas diferentes a las del capitalismo.[18]​[19]​[20]​ El debate del cálculo socialista, originado por el problema del cálculo económico, se refiere a la viabilidad y los métodos de asignación de recursos para una economía planificado socialista,[21]​ ya sea de forma centralizada o participativa / democrática.[8]​ Por el contrario, el socialismo de mercado[8]​ conserva el uso de los precios monetarios, los factores de mercados y, en algunos casos, el ánimo de lucro, con respecto al funcionamiento de las empresas de propiedad social y la asignación de bienes de capital entre ellas. Las ganancias generadas por estas empresas serían controladas directamente por la fuerza laboral de cada empresa o se acumularían a la sociedad en general en forma de dividendo social.[22]​[23]​
Existen también discrepancias sobre la forma de organización política bajo el socialismo para lograr o asegurar el acceso democrático a la sociedad socialista a clases sociales o poblaciones,[24]​ frente a la posibilidad de una situación autocrática por parte de las burocracias administrativas.[25]​ La política socialista ha sido tanto de orientación internacionalista como nacionalista; organizado a través de partidos políticos y opuestos a la política de partidos a veces se superponen con los sindicatos y otras veces son independientes y críticos de ellos; y presente tanto en países industrializados como en desarrollo.[26]​
Las formas históricas de la organización social de tipo socialista pueden dividirse entre determinadas evoluciones espontáneas de ciertas civilizaciones de carácter religioso y las construcciones políticas establecidas por proyectos ideológicos deliberados. De estas se destaca el Imperio inca.[27]​ El movimiento socialista incluye un conjunto de filosofías políticas que se originaron en los movimientos revolucionarios de mediados a finales del siglo XVIII y por preocupación por los problemas sociales asociados con el capitalismo.[28]​ A finales del siglo XIX, después del trabajo de Karl Marx y Friedrich Engels, el socialismo había llegado a significar oposición al capitalismo y la defensa de un sistema poscapitalista basado en alguna forma de propiedad social de los medios de producción. El socialismo marxista fue más tarde denominado como socialismo científico, caracterizado por la dictadura del proletariado como objetivo para sentar el comunismo (sistema socioeconómico sin clases sociales), en contraposición a autores socialistas anteriores, denominados socialistas utópicos. A lo largo del siglo XIX, los términos "comunismo" y "socialismo" se usaron como sinónimos.[29]​ Por otro lado, pensadores anarquistas como Pierre-Joseph Proudhon y Mijaíl Bakunin defendieron un socialismo libertario sin Estado[30]​[31]​ en comparación al socialismo de Estado marxista.
A finales del siglo XIX se originó la socialdemocracia dentro del movimiento socialista,[32]​ apoyando las intervenciones económicas y sociales para promover la justicia social.[33]​ Mientras conserva el socialismo como un objetivo a largo plazo,[34]​[35]​[36]​ desde el período de posguerra ha llegado a abrazar a una economía mixta keynesiana dentro de una economía de mercado capitalista.[37]​ No fue sino hasta la Revolución Bolchevique con Lenin que el término socialismo llegó a referirse a una "primera fase" a la "fase superior" del comunismo.[38]​[39]​ En la década de 1920, el comunismo y la socialdemocracia se habían convertido en las dos tendencias políticas dominantes dentro del movimiento socialista internacional,[40]​ con el socialismo mismo convirtiéndose en el movimiento secular más influyente del siglo XX.[41]​ Mientras que el surgimiento de la Unión Soviética como el primer Estado socialista nominal del mundo condujo a la asociación generalizada del socialismo con el modelo económico soviético, algunos economistas e intelectuales argumentaron que en la práctica el modelo funcionaba como una forma de capitalismo de Estado[42]​[43]​[44]​ o una economía administrativa o de mando no planificada.[45]​[46]​ Tras la caída del bloque soviético, el término «socialismo del siglo XXI» de Heinz Dieterich Steffan como "producto de la reflexión sobre el socialismo soviético-oriental del siglo XX"[47]​ adquirió difusión mundial por varios líderes latinoamericanos como Hugo Chávez de Venezuela y Evo Morales de Bolivia.
Actualmente, las ideas y partidos socialistas que van desde los partidos laboristas hasta las diversas variedades del izquierdismo siguen siendo una fuerza política con diversos grados de poder e influencia en todos los continentes, encabezando gobiernos nacionales en muchos países de todo el mundo, los cuales han adoptado las causas de otros movimientos sociales como el ambientalismo, el feminismo y el progresismo.[48]​ También se afirma que todas las economías son sistemas híbridos "no simplemente como tipos ideales de todo o nada [...] sino también como variables", como dice Erik Olin Wright, que combinan la propiedad privada, social y estatal siendo más socialistas o menos capitalistas.[8]​ La mayoría de los principales partidos se reúnen, a nivel internacional, dentro de la Internacional Socialista, y a nivel europeo, dentro del Partido Socialista Europeo. Además de la diversidad vinculada a sus variaciones ideológicas, el socialismo también conoce muchos avatares vinculados a contextos geográficos y culturales, como el socialismo árabe o el socialismo africano.
Para Andrew Vincent, "la palabra 'socialismo' encuentra su raíz en el latín sociare, que significa combinar o compartir. El término relacionado, más técnico en el derecho romano y luego medieval fue societas. Esta última palabra podría significar compañerismo, así como la idea más legalista de un contrato consensuado entre hombres libres".[49]​
Al parecer la palabra socialismo fue empleada por primera vez por el monje Ferdinando Facchinei en 1766 para referirse a la doctrina de los que defendían el contrato social como el fundamento de la organización de las sociedades humanas. 20 años más tarde, otro autor italiano Appiano Buonafede volvió a utilizarla. El término socialismo también se atribuye en Francia a Pierre Leroux[50]​ y Marie Roch Louis Reybaud, mientras que en Gran Bretaña se asocia a Robert Owen,[51]​ quien sus discípulos de Robert Owen ya habían empleado el término ‘socialist’ para designar a los adeptos de la doctrina de Owen por 1827 en el Co-operative Magazine.[52]​
Sin embargo, la palabra socialismo, en el sentido moderno del término, no aparece hacia 1830 en Gran Bretaña y en Francia, casi simultáneamente, para designar las ideas de los seguidores de Robert Owen, Henri de Saint-Simon y grupos de Europa occidental que habían surgido de la Revolución Francesa.[53]​ El primer uso preciso del neologismo se suele atribuir al sansimoniano francés Pierre Leroux quien en el número de octubre-diciembre de 1833 de la Revue encyclopédique publicó un artículo titulado Del individualismo y del socialismo, aunque en él criticaba ambas doctrinas por considerarlas el resultado de la exageración de la idea de libertad, la primera, y de la idea de asociación, la segunda.[54]​ Sin embargo, en una nota añadida a la reimpresión del artículo años más tarde escribió:[55]​
Entre agosto de 1836 y abril de 1838 Louis Reybaud publicaba en la Revue des deux mondes tres estudios bajo el título de Socialistas modernos dedicados a Saint-Simon, a Charles Fourier y a Robert Owen, y en los que confirmaba que el término socialismo, en su sentido moderno, había surgido hacia 1830.[55]​
Como ha destacado Jean-Paul Thomas, toda «palabra nueva, responde a realidades nuevas. Las doctrinas sociales no surgen casualmente a principios del siglo XIX. Tienen como origen inmediato la revolución industrial y la miseria que le acompaña… Contraponen a la búsqueda egoísta del provecho la visión de una comunidad de productores ligados unos a otros por una solidaridad fraternal». Según este autor las raíces del socialismo hay que buscarlas en las propuestas igualitarias de los grupos «radicales» de la Revolución Francesa, como la del enragé Jacques Roux que escribió en 1793, denunciando los acaparamientos de los bienes de subsistencia: «los productos de la tierra, como los elementos, pertenecen a todos los hombres. El comercio y el derecho de propiedad no pueden consistir en hacer morir de miseria y de inacción a nuestros semejantes».[55]​
Según El manual de Oxford de Karl Marx, "Marx usó muchos términos para referirse a una sociedad poscapitalista: humanismo positivo, socialismo, comunismo, reino de la individualidad libre, asociación libre de productores, etc. Usó estos términos de manera completamente intercambiable. La noción de que 'socialismo' y 'comunismo' son etapas históricas distintas es ajena a su obra y solo entró en el léxico del marxismo después de su muerte".[57]​
Unos diez años después de la aparición de los términos «socialismo» y «socialista» surgieron en Francia las palabras «comunismo» y «comunista» y su uso se difundió rápidamente. Étienne Cabet y el neobabuvista Jean-Jacques Pillot las emplearon de inmediato y el adjetivo «comunista» fue usado para referirse a un banquete organizado por Pillot celebrado el 1 de julio de 1840 en las afueras de París en el que participaron más de mil comensales, en su mayoría obreros, y en el que se defendió la necesidad de aplicar reformas que no fueran meramente políticas para alcanzar una «igualdad real».[58]​ En junio de 1843 el poeta alemán Heinrich Heine, quien desde hacía más de diez años vivía en París, advirtió de su crecimiento: «Los comunistas son en Francia el único partido que merece atención».[59]​
Desde Francia los términos «comunismo» y «comunista» se difundieron por los Estados alemanes y por Suiza, gracias al libro de Lorenz von Stein publicado en 1842 en Leipzig con el título El socialismo y el comunismo en la Francia de hoy (Der Sozialismus und Communismus des heutigen Frankreichs) —Wilhelm Weitling, August Becker y otros los utilizaron enseguida—, y también por Gran Bretaña a través de otros canales. Así el término «comunismo» fue sustituyendo progresivamente al originario de «socialismo» o al menos se confundió con él.[60]​
En la década de 1830, en general, la palabra "socialismo" llegó a significar casi cualquier tipo de reforma con el propósito de mejorar la situación del proletariado y "comunismo" como más extremo que el socialismo. Una distinción generalizada era que el socialismo socializaba solo la producción, mientras que el comunismo socializaba tanto la producción como el consumo.[38]​
Según Jean Bruhat, en la década de 1840 «comunista» y «socialista» no eran términos completamente equivalentes ya que los comunistas se distinguían por unas ideas que en ellos estaban más claramente afirmadas que en los socialistas, como la realidad de la lucha de clases de la que se derivaba la necesidad de la revolución —la conquista del Estado— para alcanzar la nueva sociedad, pues para cambiar al hombre había que cambiar el régimen económico y social en el que vivía, como lo advirtió el neobabuvista Théodore Dézamy cuando criticaba a los que creían «que para modelar al hombre a su gusto bastaría proponérselo de un modo testarudo y enérgico».[61]​ Estas diferencias fueron las que motivaron que Karl Marx y Friedrich Engels adoptaran el término «comunista» y no el de «socialista» para llamar a la Liga que fundaron en 1847 y al manifiesto de la misma hecho público al año siguiente.
En Principios del comunismo, un programa de la Liga de los Comunistas que sirvió de borrador para el manifiesto, Engels escribió que había tres tipos de socialistas: los socialistas reaccionarios, los socialistas burgueses (antagónicos ambos a los objetivos comunistas) y los socialistas democráticos (que a veces pueden alinearse útilmente con los comunistas).[62]​ Engels explicó en Contribución al problema de la vivienda que: "La característica esencial del socialismo burgués es que pretende conservar la base de todos los males de la sociedad presente, queriendo al mismo tiempo poner fin a estos males".[63]​ Esta clasificación se mantuvo en el Manifiesto Comunista.
En sus críticas mutuas, tanto Marx como Pierre-Joseph Proudhon aceptaron que el "comunismo" y "socialismo" eran distintos.[38]​ En la Europa cristiana, se creía que los comunistas habían adoptado el ateísmo. En la Inglaterra protestante, el término comunismo se parecía al nombre del rito de la comunión de la Iglesia católica, por lo que socialista era el término preferido.[64]​ El filósofo británico John Stuart Mill discutió una forma de socialismo económico dentro de un contexto liberal que más tarde se conocería como socialismo liberal. En ediciones posteriores de sus Principios de economía política (1848), Mill argumentó además que "en lo que respecta a la teoría económica, no hay nada en principio en la teoría económica que excluya un orden económico basado en políticas socialistas"[65]​[66]​ y promovió la sustitución de las empresas capitalistas por cooperativas de trabajadores.[67]​
La definición y el uso del socialismo se estableció en la década de 1860, reemplazando a asociacionista, cooperativo y mutualista que se habían utilizado como sinónimos mientras el comunismo cayó en desuso durante este período.[68]​ Una de las primeras distinciones entre comunismo y socialismo fue que el último tenía como objetivo socializar únicamente la producción, mientras que el primero tenía como objetivo socializar tanto la producción como el consumo (en forma de libre acceso a los bienes finales).[69]​ En 1888, los marxistas emplearon el socialismo en lugar del comunismo, ya que este último había llegado a ser considerado un sinónimo antiguo de socialismo.[70]​
Engels explicó en el prólogo del Manifiesto de 1890 que “socialismo” designaba un movimiento burgués, el “comunismo” un movimiento obrero debido a que en aquellos años «la parte de los obreros que, convencida de la insuficiencia de las revoluciones meramente políticas, exigía una transformación radical de la sociedad, se llamaba entonces comunista» mientras que la mayoría de los que se hacían llamar «socialistas» «se hallaban fuera del movimiento obrero y buscaban apoyo más bien en las clases "instruidas"», «y como nosotros ya en aquel tiempo sosteníamos muy decididamente el criterio de que "la emancipación de la clase obrera debe ser obra de la clase obrera misma", no pudimos vacilar un instante sobre cuál de las dos denominaciones procedía elegir».[71]​
Después de 1848, los términos «socialismo» y «comunismo» se afirmaron y se superpusieron, identificándose en unos períodos y diferenciándose en otros, y también se utilizaron para caracterizar etapas de desarrollo histórico distintas.[72]​ Marx y Engels cambiaron al uso del término "socialismo", para significar exactamente lo que antes habían querido decir con "comunismo".[38]​ El sociólogo francés Émile Durkheim afirmó que en el «comunismo», a diferencia del «socialismo», la contribución a la producción común era libre y no planificada mientras que el consumo se vivía en común.[73]​
En 1888, el socialismo era de uso general entre los marxistas y fue utilizado para significar lo que antes se había entendido por el término menos popular y más restringido.[38]​ El programa de Erfurt de Karl Kautsky de 1891 explicó:[74]​

Friedrich Engels en Del socialismo utópico al socialismo científico, define el socialismo como un sistema social y económico caracterizado por el control por parte de la sociedad, organizada con todos sus integrantes, tanto de los medios de producción como de las diferentes fuerzas de trabajo aplicadas hacia los mismos.[75]​[76]​ Engels opina que una “sociedad socialista” debe concebirse en un estado de constante cambio y su diferencia con el orden actual consiste en la producción organizada sobre la base de la propiedad común de la nación de todos los medios de producción.[77]​ Según Engels, el objetivo de un partido socialista es comunista, cuyo último era vencer todo al Estado y superar la "democracia burguesa".[78]​
Mientras tanto, los socialistas no marxistas continuaron hablando de una distinción entre socialismo y comunismo, aunque no como etapas sucesivas.[38]​ En la Crítica del programa de Gotha, Marx diferencia entre una etapa comunista previa en donde el individuo compraría bienes con vales de trabajo, de una etapa superior, en la que cada persona contribuirá según sus capacidades y recibirá acorde a sus necesidades.[79]​[80]​ Poco después en 1917, Lenin en su obra El Estado y la revolución utilizó la palabra «socialismo» para referirse a la primera etapa en la consecución de la sociedad sin clases o «comunismo», caracterizada por la organización colectiva de la producción y la distribución en tanto que el consumo seguiría siendo particular.[81]​
Según el marxismo, en un sistema socialista, al establecerse la propiedad social (colectiva) de los medios de producción, desaparece cualquier forma de propiedad privada de los bienes de capital y con esta el capitalismo como forma de apropiación del trabajo asalariado, una forma de explotación por vía económica. Por lo tanto el socialismo constituye el primer paso para la extinción de las clases sociales (o comunismo) dando así por superada la lucha de clases como motor del progreso histórico.[82]​
Vladimir Lenin expresó en su escrito "Seis tesis acerca de las tareas inmediatas del poder soviético" que el Estado socialista organizado por la dictadura del proletariado tenía como objetivo sentar las bases del comunismo y se encargaría de la dirección de la economía bajo el modo de producción "socialista" centralizado. El socialismo moderno es, en primer término, por su contenido, fruto del reflejo en la inteligencia, por un lado, de los antagonismos de clase que imperan en la moderna sociedad entre poseedores y desposeídos, capitalistas y obreros asalariados, y, por otro lado, de la anarquía que reina en la producción.[75]​
No fue sino hasta 1917 después de la Revolución Bolchevique que Vladimir Lenin se apropió del término para significar una etapa entre capitalismo y comunismo bajo la dictadura del proletariado del que Marx ya hablaba en la Crítica del programa de Gotha.[38]​[83]​ Según Engels, la "forma específica" de esta etapa es la república democrática.[84]​[85]​ No fue sino hasta la Revolución Bolchevique que el término socialismo llegó a referirse a esta etapa previa.[74]​
Lenin utilizó el término para defender el programa bolchevique de la crítica marxista de que las fuerzas productivas de Rusia no estaban suficientemente desarrolladas para el comunismo.[86]​ La distinción entre comunismo y socialismo se hizo notoria en 1918 después de que el Partido Laborista Socialdemócrata Ruso se rebautizara a sí mismo como Partido Comunista de toda la Unión, interpretando comunismo específicamente para significar socialistas que apoyaron la política y las teorías del bolchevismo, leninismo y más tarde del marxismo-leninismo,[87]​ aunque los partidos comunistas continuaron describiéndose como socialistas dedicados al socialismo.[88]​
Por extensión se define como socialista a toda doctrina o movimiento que aboga por su implantación. Frecuentemente existen diferentes movimientos políticos que adoptan el título de Socialismo: desde aquella fecha existen ideas de búsqueda del bien común e igualdad social, hasta los proyectos reformistas de construcción progresiva de un Estado socialista en términos marxistas, o las variantes pre y post-marxistas de socialismo (sean obreristas o nacionalistas), o al intervencionismo, conceptos de socialismo o de sus métodos que pueden variar drásticamente según varíen los interlocutores políticos y que algunas veces se distancian en mayor o menor medida de su etimología: estatistas, nacionalistas, marxistas, cooperativistas, corporativistas gremiales clásicos, socialistas de renta, socialistas de mercado, mutualistas, socialdemócratas modernos, etc.
El socialismo continúa siendo un término de fuerte impacto político, que permanece vinculado con el establecimiento de un orden socioeconómico construido por, para, o en función de, una clase trabajadora organizada originariamente sin un orden económico propio, y para el cual debe crearse uno público (por vía del Estado o no), ya sea mediante revolución o evolución social o mediante reformas institucionales, con el propósito de construir una sociedad sin clases estratificadas o subordinadas unas a otras; idea esta última que no era originaria del ideario socialista sino del comunista y cuya asociación es deudora del marxismo-leninismo. La radicalidad del pensamiento socialista no se refiere tanto a los métodos para lograrlo sino más bien a los principios que se persiguen.
Existen diferencias entre los grupos socialistas, aunque casi todos están de acuerdo en que están unidos por una historia en común que tiene sus raíces en el siglo XIX, en las luchas de los trabajadores siguiendo los principios de solidaridad y vocación a una sociedad igualitaria, con una economía que pueda, desde su punto de vista, servir a la totalidad de la población en vez de a unos pocos.
Por otro lado el significado de facto del socialismo ha ido cambiando con el transcurso del tiempo. Así en el marxismo-leninismo el socialismo es considerado como la fase previa al comunismo, mientras que en la socialdemocracia con el término de socialismo se alude a la redistribución de la riqueza mediante la aplicación de un sistema fiscal progresivo.
Inglaterra fue una de las dos cunas del socialismo «utópico». Existieron dos causas importantes que dan al socialismo utópico inglés su carácter peculiar: la revolución industrial, con su cortejo de miserias para el desarrollo del Proletariado británico, y el desarrollo de una nueva rama de la ciencia: la economía política, concepto asociado a la búsqueda de dominio titular de las ciencias políticas.
En Francia tuvo un carácter más filosófico que en Inglaterra. Su primer representante fue el conde Henri de Saint-Simon, considerado por Engels el creador de la idea en estado embrionario que sería utilizada por todos los socialistas posteriores.[89]​ Propuso la Federación de Estados Europeos, como instrumento político para controlar el comienzo y desarrollo de guerras. Al mismo tiempo Charles Fourier, concibió los falansterios (comunidades humanas regidas por normas de libre albedrío e ideologías económicas socializadas).
Henri de Saint-Simon contrastó la doctrina liberal del individualismo que enfatizaba el valor moral del individuo mientras enfatizaba que las personas actúan o deberían actuar como si estuvieran aisladas unas de otras. Los socialistas utópicos originales condenaron esta doctrina del individualismo por no abordar las preocupaciones sociales durante la Revolución Industrial, incluida la pobreza, la opresión y las grandes desigualdades en la riqueza. Consideraban que su sociedad dañaba la vida comunitaria al basarla en la competencia. Presentaron el socialismo como una alternativa al individualismo liberal basado en la propiedad compartida de los recursos.[90]​ Saint-Simon propuso la planificación económica, la administración científica y la aplicación del conocimiento científico a la organización de la sociedad. Por el contrario, Robert Owen propuso organizar la producción y la propiedad a través de cooperativas.[90]​[91]​
Poco después aparece la teoría comunista marxista que desde una teoría crítica del comunismo, desarrolla una propuesta política: el «socialismo científico». Karl Marx postula en una de sus obras la diferenciación entre «valor de mercado» y «valor de cambio» de una mercancía y la definición de plusvalía, siendo estas sus mayores contribuciones a la economía política; no obstante, los economistas modernos no utilizan estos conceptos del mismo modo que lo hacen los seguidores de la escuela marxista del pensamiento económico, argumentando que la teoría expuesta por Marx no contempla la interacción total de la ciencia económica y se ve parcializada por el comunismo. Entre los socialistas hubo una muy pronta división entre marxistas y anarquistas los cuales eran la esencia más cercana a la ideología marxista. El marxismo como teoría recibió muchas críticas, algunas de ellas constituirán durante muchas décadas la base ideológica de la mayoría de partidos socialistas. Más tarde, a raíz de la Revolución rusa y de la interpretación que le dio Lenin, el leninismo se convertiría en foco de admiración de los partidos comunistas, agrupados bajo la III Internacional.
La teoría marxista se construye conjuntamente con el anarquismo. El anarquismo se podría inscribir dentro de los conceptos tempranos del socialismo, que como ideal busca que las personas decidan sobre sus vidas libre e independientemente; la abolición del Estado y de toda autoridad; exaltando al individuo. Poco después de que Mijaíl Bakunin y sus seguidores se unieran en 1868, la Primera Internacional se polarizó en campos encabezados por Marx y Bakunin. Las diferencias más claras entre los grupos surgieron sobre sus estrategias propuestas para lograr sus visiones. La Primera Internacional se convirtió en el primer foro internacional importante para la promulgación de ideas socialistas. «Que la libertad sin el socialismo es el privilegio, la injusticia; y que el socialismo sin la libertad es la esclavitud y la brutalidad».[92]​
La meta del socialismo es construir una sociedad basada en la igualdad, la equidad económica, la iniciativa personal, la cooperación moral de un individuo, eliminando las compensaciones estratificadas por esfuerzo, promoviendo estructuras políticas y económicas de distribución como por ejemplo el seguro social.
El socialismo alcanzó su apogeo político a finales del siglo XX en el bloque comunista de Europa, la Unión Soviética, estados comunistas de Asia y del Caribe.
Durante la segunda mitad del siglo XX fue de gran importancia para el llamado bloque socialista (conjunto de los países controlados por la Unión Soviética tras la contraofensiva en el frente oriental durante la Segunda Guerra Mundial), donde la URSS impuso sistemas de gobierno socialistas dependientes.
En la década de 1920, el comunismo y la socialdemocracia se habían convertido en las dos tendencias políticas dominantes dentro del movimiento socialista internacional,[40]​ con el socialismo mismo convirtiéndose en el movimiento secular más influyente del siglo XX.[41]​ Mientras que el surgimiento de la Unión Soviética como el primer estado socialista nominal del mundo condujo a la asociación generalizada del socialismo con el modelo económico soviético, algunos economistas e intelectuales argumentaron que en la práctica el modelo funcionaba como una forma de capitalismo de Estado[42]​[43]​[44]​ o una economía administrativa o de mando no planificada.[45]​[46]​
Tras la Segunda Guerra Mundial, la tensión militar-ideológica entre el bloque socialista, encabezado por la Unión Soviética, y el capitalista, encabezado por Estados Unidos, desembocó en un enfrentamiento político que se conocería como Guerra Fría. Se conoció de ella extraoficialmente y fue la competencia por la superioridad en todos los aspectos y lograr así el dominio completo (pero no directo) de la mayor cantidad de países. Culminó con la disolución política de la URSS, tras una crisis agravada por su situación económica y política y fuertes presiones externas, acompañada de una pronunciada crisis en los demás estados socialistas, principalmente los europeos.
El socialismo del siglo XXI es un concepto que aparece en la escena mundial en 1996, a través de Heinz Dieterich Steffan.[94]​ El término adquirió difusión mundial desde que fue mencionado en un discurso por el entonces presidente de Venezuela, Hugo Chávez, el 30 de enero de 2005, desde el V Foro Social Mundial. Dieterich argumentó que tanto el capitalismo industrial de libre mercado como el socialismo del siglo XX en forma de marxismo-leninismo no han logrado resolver problemas humanitarios urgentes como la pobreza, el hambre, la explotación del trabajo, la opresión económica, el sexismo, el racismo, la destrucción de los recursos naturales y ausencia de una democracia verdaderamente participativa. El socialismo del siglo XXI tiene elementos socialistas democráticos, pero se interpreta principalmente al revisionismo marxista.[95]​
En el marco de la Revolución Bolivariana, Chávez señaló que para llegar a este socialismo habrá una etapa de transición que denomina como Democracia Revolucionaria. Hugo Chávez expresó «Hemos asumido el compromiso de dirigir la Revolución Bolivariana hacia el socialismo y contribuir a la senda del socialismo, un socialismo del siglo XXI que se basa en la solidaridad, en la fraternidad, en el amor, en la libertad y en la igualdad» en un discurso a mediados de 2006. Además, este socialismo no está predefinido. Más bien, dijo Chávez «debemos transformar el modo del capital y avanzar hacia un nuevo socialismo que se debe construir cada día».
Los partidos socialistas y las ideas siguen siendo una fuerza política con diversos grados de poder e influencia en todos los continentes, encabezando gobiernos nacionales en muchos países de todo el mundo. Hoy, muchos socialistas también han adoptado las causas de otros movimientos sociales como el ambientalismo, el feminismo y el progresismo.[48]​

La Mancomunidad de Naciones (en inglés, Commonwealth of Nations), antes denominada Mancomunidad Británica de Naciones (British Commonwealth of Nations), es una organización compuesta por 56 países soberanos independientes y semiindependientes que, con la excepción de Togo, Gabón, Mozambique y Ruanda,[1]​ comparten lazos históricos con el Reino Unido. En el pasado, Irlanda y Zimbabue también formaron parte de ella.
Su principal objetivo es la cooperación internacional en el ámbito político y económico, y desde 1950 la pertenencia a ella no implica sumisión alguna a la Corona británica, aunque se respeta la figura del monarca del Reino Unido. Con el ingreso de Mozambique, la organización ha favorecido la denominación «Mancomunidad de Naciones» para subrayar su carácter internacionalista. Sin embargo, el adjetivo «británico» se sigue utilizando con frecuencia para diferenciarla de otras mancomunidades existentes a nivel internacional.
El rey Carlos III del Reino Unido es la cabeza de la organización, según los principios de la Mancomunidad, «símbolo de la libre asociación de sus miembros».
En 2022, Togo (ex-colonia alemana y francesa) y Gabón (ex-colonia francesa) se unieron a la Mancomunidad convirtiéndose en los nuevos miembros sin lazos previos con el Reino Unido.[cita requerida]
La organización tiene sus orígenes en la Conferencia Imperial de 1930, cuando el gobierno británico reconoció ciertos derechos de autodeterminación a sus colonias e inició los trabajos que culminaron con el Estatuto de Westminster en 1931, y que dieron origen a la Mancomunidad (en ese entonces consistente en un puñado de excolonias aún leales a la monarquía). Hacia el interior la administra una Secretaría General con sede en la ciudad de Londres que, en la actualidad, ocupa el indio Kamalesh Sharma. Otras organizaciones hermanas que colaboran con los esfuerzos de la Secretaría General son la Fundación de la Mancomunidad (en inglés, Commonwealth Foundation) y la Mancomunidad del Aprendizaje (en inglés, Commonwealth of Learning) con sede la primera en Londres y la última en la ciudad de Vancouver, Canadá.
En 1864, los representantes de las tres colonias de la Norteamérica británica comenzaron a negociar su fusión en una confederación autónoma. Los territorios (Nueva Escocia, Nuevo Brunswick y Provincia Unida de Canadá) temían una posible agresión de Estados Unidos y querían establecer sus propias fuerzas de defensa. Al mismo tiempo requerían de autonomía que permitiera el intercambio comercial con ese mismo país.
Para evitar una crisis política que derivara en algo similar a la pérdida de las Trece Colonias el siglo anterior, el Reino Unido aceptó las condiciones de sus colonos el 1 de julio de 1867 con el surgimiento de la Confederación canadiense, cuyo nuevo sistema político consistía en un Dominio mediante el cual, la confederación podía gobernarse a sí misma, en lo que se refería a su política interior, salvo que sus leyes seguirían estando sujetas a la supervisión del Parlamento del Reino Unido y un eventual veto del Monarca británico; es decir, permanecían unidas bajo la corona al Imperio británico, pero su gobierno interno estaba bajo la responsabilidad de los ciudadanos nacidos en el país. Este sistema de gobierno llevó a la independencia parcial de otras colonias británicas: Australia (1901), Nueva Zelanda (1907), Terranova (1907), Sudáfrica (1910) y el Estado Libre de Irlanda (1922).[2]​
En 1884, durante su visita a Australia, el primer ministro Archibald Primrose describió el cambio del Imperio británico, ya que alguna de sus colonias se hicieron más independientes, como una «comunidad de naciones». Periódicamente se produjeron conferencias de primeros ministros británicos y coloniales desde principios de 1887 y dieron lugar a la creación de las Conferencias Imperiales en 1911.[3]​
La Mancomunidad se desarrolló a partir de las Conferencias Imperiales. Jan Smuts presentó una propuesta concreta en 1917 cuando acuñó el término «la Mancomunidad Británica de Naciones» y previó las «relaciones y los reajustes constitucionales futuros en esencia» en la importante Conferencia de Paz de 1919, a la que también asistieron delegados de los dominios y Gran Bretaña.[4]​ El término recibió el reconocimiento imperial legal por primera vez en el Tratado Anglo-Irlandés de 1921, cuando el término «Mancomunidad Británica de Naciones» (en inglés, British Commonwealth of Nations) sustituyó a «Imperio británico» en la redacción del juramento que toman los miembros del Parlamento del Estado Libre de Irlanda.[5]​[6]​
En la Declaración Balfour en la Conferencia Imperial 1926, se estableció que Gran Bretaña y sus dominios eran «iguales en estatus, en modo alguno subordinado a otro en cualquier aspecto de sus asuntos internos o externos, aunque unidos por la lealtad común a la Corona, y libremente asociados como miembros de la Mancomunidad Británica de Naciones». Estos aspectos de la relación se formalizaron por el Estatuto de Westminster en 1931, que se aplica a Canadá sin necesidad de ratificación, pero Australia, Nueva Zelanda y Terranova tuvieron que ratificarlo para que entrara en vigor. Terranova nunca lo hizo, ya que el 16 de febrero de 1934, con el consentimiento de su parlamento, el gobierno de Terranova terminó voluntariamente y su gobierno pasó al control directo de Londres. Terranova más tarde se unió a Canadá como su décima provincia en 1949. Australia y Nueva Zelanda ratificaron el Estatuto en 1942 y 1947, respectivamente.
Después de la Segunda Guerra Mundial, el Imperio británico fue desmantelado poco a poco, salvo los catorce territorios de ultramar británicos aún en poder del Reino Unido. En abril de 1949, a raíz de la Declaración de Londres, la palabra "británico" se eliminó del título de la Mancomunidad para reflejar su naturaleza cambiante. Birmania (también conocida como Myanmar, 1948) y Aden (1967) son los únicos estados que eran colonias británicas en el momento de la guerra que no se unieron a la Mancomunidad después de la independencia.
Los antiguos protectorados británicos y mandatos que no se convirtieron en miembros de la Mancomunidad son Egipto (independiente en 1922), Irak (1932), Transjordania (1946), la Palestina británica (parte de la cual se convirtió en el estado de Israel en 1948), Sudán (1956), Somalilandia Británica (que se había unido a la antigua Somalia italiana en 1960 para formar Somalia), Kuwait (1961), Baréin (1971), Omán (1971), Catar (1971), y los Emiratos Árabes Unidos (1971).
Irlanda abandonó la Mancomunidad el 18 de abril de 1949 dando vigencia a la Ley de la República de Irlanda de 1948, que había entrado en vigor el mismo día, y a la ley británica sobre Irlanda de 1949 con efecto retroactivo. Irlanda, de hecho, no había participado activamente en la Mancomunidad desde principios de la década de 1930 y finalmente la abandonó. Otros dominios, sin embargo, deseaban convertirse en repúblicas sin cortar sus lazos con Gran Bretaña. El asunto se trató en abril de 1949 en una reunión de primeros ministros de la Mancomunidad en Londres. Bajo la Declaración de Londres, aceptada por la India, cuando se convirtió en una república en enero de 1950, accedió a aceptar al soberano británico como un "símbolo de la libre asociación de sus naciones miembros independientes y como tal, el Jefe de la Mancomunidad". Al oír esto, el rey Jorge VI dijo al político indio Krishna Menon : "Así que me he convertido 'en mí mismo' ". 
Los demás países de la Mancomunidad reconocieron la afiliación continua de la India a la asociación. Ante la insistencia de Pakistán, la India no fue considerada como un caso excepcional y se asumió que los otros Estados recibirían el mismo trato que la India.
Comúnmente se ve a la Declaración de Londres como iniciadora de la Mancomunidad moderna. Siguiendo el precedente de la India, otras naciones se convirtieron en repúblicas o en monarquías constitucionales con sus propios monarcas, mientras que algunos países adoptaron el mismo monarca del Reino Unido; pero sus monarquías se desarrollarían de manera diferente y pronto se convirtieron en independientes de la monarquía británica en su totalidad. Se considera al monarca como una entidad independiente de la entidad jurídica de cada ámbito, a pesar de que la misma persona es monarca de cada reino.
Debido al crecimiento de la Mancomunidad, el Reino Unido y los dominios previos a 1945 llegaron a conocerse, aunque de forma informal, como la Antigua Mancomunidad. Los planificadores en el período de "entreguerras", como Lord Davies, que también había tenido "un papel importante en la construcción de la Sociedad de las Naciones" en los Estados Unidos, en 1932 fundaron la Sociedad de la Nueva Mancomunidad, con Winston Churchill como su presidente.[7]​ Esta nueva sociedad tenía por objeto la creación de una fuerza aérea internacional para ser el brazo de la Liga de las Naciones, para permitir el desarme de las naciones y salvaguardar la paz.
El término Nueva Mancomunidad se ha utilizado en Gran Bretaña (sobre todo en los años 1960 y 1970) para referirse a los países recientemente descolonizados, predominantemente no blancos y en vías de desarrollo. Se utiliza a menudo en los debates sobre la inmigración de estos países.[8]​ Gran Bretaña y los dominios anteriores a 1945 se conocieron informalmente como la Vieja Mancomunidad o, más específicamente, como la «Mancomunidad blanca», en referencia a lo que se conocía como los Dominios Blancos.[9]​[10]​
Según la fórmula de la Declaración de Londres, el rey Carlos III es el jefe de la Mancomunidad, título que por ley forma parte de los títulos reales de este en cada uno de los reinos de la Mancomunidad,[11]​ los quince miembros de la Mancomunidad que lo reconocen como su monarca. Cuando el monarca muere, el sucesor de la corona no se convierte automáticamente en el nuevo jefe de la Mancomunidad.[12]​ Sin embargo, en su reunión de abril de 2018, los líderes de la Mancomunidad acordaron que el por entonces príncipe Carlos sucedería a su madre como jefe.[13]​ El cargo es simbólico y representa la libre asociación de miembros independientes,[11]​ la mayoría de los cuales (34) son repúblicas y cinco tienen monarcas de diferentes casas reales (Brunéi, Eswatini, Lesoto, Malasia y Tonga).
El principal foro de toma de decisiones de la organización es la Reunión bienal de Jefes de Gobierno de la Mancomunidad (CHOGM por sus siglas en inglés), donde los jefes de gobierno de la Mancomunidad, incluidos (entre otros) primeros ministros y presidentes, se reúnen durante varios días para discutir asuntos de interés mutuo. El CHOGM es el sucesor de las Reuniones de Primeros Ministros del Mancomunidad y, anteriormente, de las Conferencias Imperiales y de las Conferencias Coloniales, que se remontan a 1887. También hay reuniones periódicas de ministros de finanzas, de ministros de derecho, de ministros de salud, etc. miembros antes que ellos, no están invitados a enviar representantes a reuniones ministeriales o CHOGM.[11]​
El jefe de gobierno que organiza el CHOGM se denomina presidente en ejercicio (CIO) y conserva el cargo hasta el siguiente CHOGM. Desde el CHOGM más reciente, en el Reino Unido en 2018, el presidente en ejercicio ha sido el primer ministro del Reino Unido.[14]​
El próximo (26º) CHOGM debía celebrarse en Kigali, Ruanda, en junio de 2020. Debido a la pandemia de COVID-19, se reprogramó para que se celebrara allí en la semana del 21 de junio de 2021; pero, debido a que la pandemia ha continuado, la reunión se ha pospuesto indefinidamente. Cuando tenga lugar, irá acompañado de reuniones de un Foro de la Juventud de la Mancomunidad, un Foro de Mujeres de la Mancomunidad y un Foro del Pueblo de la Mancomunidad.[15]​
La Secretaría de la Mancomunidad, creada en 1965, es la principal agencia intergubernamental de la Mancomunidad, que facilita las consultas y la cooperación entre los gobiernos y países miembros. Es responsable ante los gobiernos miembros colectivamente. La Mancomunidad de Naciones está representada en la Asamblea General de las Naciones Unidas por la secretaría en calidad de observador. La secretaría organiza cumbres de la Mancomunidad, reuniones de ministros, reuniones consultivas y debates técnicos; ayuda al desarrollo de políticas y brinda asesoramiento sobre políticas, y facilita la comunicación multilateral entre los gobiernos miembros. También proporciona asistencia técnica para ayudar a los gobiernos en el desarrollo social y económico de sus países y en apoyo de los valores políticos fundamentales de la Mancomunidad.[16]​
La secretaría está dirigida por el secretario general de la Mancomunidad, quien es elegido por los jefes de gobierno de la Mancomunidad por no más de dos mandatos de cuatro años. El secretario general y dos subsecretarios generales dirigen las divisiones de la Secretaría. La actual secretaria general es Patricia Scotland, baronesa Escocia de Asthal, de Dominica, que asumió el cargo el 1 de abril de 2016, sucediendo a Kamalesh Sharma de India (2008-2016). El primer secretario general fue Arnold Smith de Canadá (1965-1975), seguido por Sir Shridath Ramphal de Guyana (1975-1990), Jefe Emeka Anyaoku de Nigeria (1990–99) y Don McKinnon de Nueva Zelanda (2000–2008).[16]​
El ingreso de Mozambique en el año 1995 desencadenó una polémica a nivel internacional, pues la antigua colonia portuguesa en África no tenía nexo alguno con la comunidad británica y la maniobra fue calificada como una suerte de neoimperialismo cultural y económico en una región afligida por la pobreza. La secretaría general de la organización justificó el hecho con la aparente cooperación de Mozambique en la cruzada de la Mancomunidad contra el racismo en África, particularmente en Sudáfrica y Zimbabue. Para evitar situaciones similares, a partir de la Cumbre de Edimburgo en 1997 la organización limitó la incorporación solo a aquellas naciones que posean algún vínculo constitucional con las naciones de la Mancomunidad y se comprometan a respetar las normas y convenciones existentes en la misma. Sin embargo, en noviembre de 2009 se repitió la situación con Ruanda, antiguo protectorado alemán y luego mandato belga y por lo tanto sin relación alguna con la comunidad británica, al ser aceptado como miembro 54 de la Mancomunidad en la LXI reunión celebrada en Puerto España, capital de Trinidad y Tobago.[17]​
Actualmente, la Mancomunidad carece de constitución pero sus miembros se comprometen voluntariamente a cumplir con la Declaración de Principios de la Mancomunidad firmada en Singapur en 1971 y ratificada en la Declaración de Harare de 1991. En términos generales la Declaración reconoce la importancia de la democracia y el buen gobierno, el respeto a los derechos humanos, la igualdad entre el hombre y la mujer, el respeto a las leyes y el desarrollo socioeconómico sostenible. La financiación de la organización proviene de los gobiernos que participan con una cuota calculada a partir del producto nacional bruto y el tamaño de la población de cada país.
Como dato anecdótico, una característica de los países de la Mancomunidad es que generalmente su sentido de circulación es por la izquierda, diferenciándose de los otros países donde es por la derecha. Las excepciones son Belice, Canadá, Gambia, Ghana, Nigeria y Sierra Leona, ya que, a pesar de ser antiguas colonias británicas y ser miembros de la Mancomunidad, establecieron su sentido de circulación por la derecha a fin de estar en línea con sus vecinos. Otro caso similar es el de Ruanda, donde al igual que en los países anteriormente mencionados, se conduce por la derecha, pero esto es debido a que Ruanda fue colonia belga en el pasado y no británica. También ocurre un caso similar con Camerún, en África, y Vanuatu, en Oceanía, que a pesar de ser miembros de la Mancomunidad de Naciones, tienen el sentido de circulación por la derecha, debido a la influencia francesa durante el pasado colonial de ambos países.
Otra característica de los países de la Mancomunidad es que las misiones diplomáticas de cada país en los otros son llamadas altas comisiones en vez de embajadas, cuyos jefes son llamados altos comisionados en vez de embajadores.
La Mancomunidad de Naciones tiene una presencia importante en los cinco continentes. Los 54 países miembros forman una comunidad que abarca unos 31 millones de kilómetros cuadrados y casi dos mil quinientos millones de personas, casi la tercera parte de la población mundial. 
Los estados miembros deben cumplir una serie de requisitos expuestos en la Declaración de Harare (1991): 
Con la inclusión de países sin pasado colonial o relación cultural (Mozambique en 1995) con el Reino Unido se incluyeron nuevos requisitos con la Declaración de Edimburgo (1995):
Desde la creación de la Mancomunidad de Naciones por el Reino Unido, los primeros miembros fueron los dominios que habían adquirido la autonomía desde finales del siglo XIX y principios del siglo XX, cuya relación fue confirmada por el Estatuto de Westminster (1931). Estas naciones fueron Nueva Zelanda, Australia, Sudáfrica y Canadá. Tras la Segunda Guerra Mundial (1939-1945) comienza el proceso descolonizador que afectó a las colonias británicas de Asia (India británica) y que supuso el ingreso de tres naciones resultantes de la descomposición de la India británica: India, Pakistán y Ceilán (Sri Lanka). En 1957 se añadieron Ghana, primera colonia del África subsahariana en independizarse, y Malasia. 
En los años 60 se entra en el periodo de plenitud del proceso descolonizador incluyéndose las colonias de África, Asia y Europa. Así, entraron un total de veinte estados incluyendo a Nigeria, Kenia, Uganda, Jamaica, Barbados, Singapur o Chipre entre otras. En los años 70 y 80 se concluyó el proceso descolonizador británico e ingresaron veintiún naciones de todos los continentes salvo Europa, incluidas Papúa Nueva Guinea, Bangladés, Santa Lucía o Brunéi. El último proceso de ampliación incluyó estados que no habían sido colonias británicas (salvo Camerún en su parte oriental) como Namibia (1990) o Ruanda (2009), la última incorporación.
A lo largo de su existencia, la Mancomunidad de Naciones ha llevado a cabo procesos de suspensión de alguno de sus miembros por incumplimiento de estos principios o la retirada voluntaria de los miembros. No obstante, estas naciones han regresado al seno de la organización salvo Irlanda y Zimbabue que la abandonaron definitivamente en 1949 y 2003 respectivamente: 
El rey Carlos III del Reino Unido es el jefe de la Mancomunidad de Naciones pero también es el jefe de Estado de algunos estados que pertenecen a la Mancomunidad de Naciones, los conocidos como monarquías de la Mancomunidad. En estos Estados el monarca está representado por un gobernador general del país correspondiente. No obstante, dentro de la Mancomunidad de Naciones existen también monarquías independientes y repúblicas: 
Entre los Estados de la Mancomunidad de Naciones se produjeron algunos cambios de gobierno que, en su mayoría, afectaron a la figura de la reina Isabel II en la jefatura del Estado. Así, de los dieciséis cambios en la forma de gobierno dentro de los miembros, quince corresponden al paso de una monarquía parlamentaria (o dominio en época postcolonial) a una república. El último fue el de Barbados en el año 2021. 
De monarquías a repúblicas (año de cambio)
Transformaciones
Era una figura por la que un país era considerado un miembro especial cuya participación estaba limitada en determinadas funciones. Originalmente, era un estatus que ostentaban unos pocos países recién incorporados, cuya participación estaba condicionada por sus limitados propios recursos financieros. Más recientemente, el nombre se ha cambiado a miembro pendiente de pagos.
Entre las características de estos miembros especiales estarían, en primer lugar, que no están obligados a realizar pagos a la Mancomunidad, pueden asistir a la mayoría de las funciones y órganos de la Mancomunidad de Naciones, pero no están invitados a asistir a las reuniones de jefes de gobierno del Mancomunidad de Naciones. Aunque están limitados en estos aspectos, todavía se les considera miembros de la Mancomunidad de Naciones.
El estatus fue creado especialmente para Nauru, que tenía una población y un área excepcionalmente pequeñas. A Nauru le siguió otro estado soberano del Pacífico, Tuvalu, y luego el país caribeño de San Vicente y las Granadinas y, el país asiático de Maldivas. Estos ganaron progresivamente la membresía de pleno derecho. Sin embargo, Nauru se retrasó en sus pagos de suscripción y volvió a ser miembro especial en julio de 2005. Finalmente, Nauru pasó a ser miembro de pleno derecho de nuevo en junio de 2011.

El racismo se basa en la exacerbación del sentido de superioridad de un grupo étnico que suele motivar la discriminación o persecución de otro u otros con los que convive. La palabra «racismo» designa también la doctrina antropológica o la ideología política basada en ese sentimiento.[1]​[2]​[3]​[4]​ Conforme a la Convención Internacional sobre la Eliminación de todas las Formas de Discriminación Racial aprobada por la Asamblea General de la Organización de las Naciones Unidas el 21 de diciembre de 1965:

El primer artículo de la convención internacional sobre la eliminación de todas las formas de discriminación racial define al racismo como:

Existen autores que proponen distinguir entre el racismo en sentido amplio del racismo en sentido restringido. En el primer caso, se trataría de una actitud etnocéntrica o «sociocéntrica» que separa el grupo propio del ajeno, y que considera que ambos están constituidos por esencias hereditarias e inmutables que hacen de los otros, de los ajenos, seres inadmisibles y amenazadores. Esta concepción de los demás conduciría a su segregación, discriminación, expulsión o exterminio y podría apoyarse en ideas científicas, religiosas o en meras leyendas o sentimientos tradicionales. Afirma también la superioridad intelectual y moral de unas razas sobre otras, superioridad que se mantiene con la pureza racial y se arruina con el mestizaje. Este tipo de racismo, cuyo modelo es el nazi y el racismo occidental en general, conduce a defender el derecho natural de las razas «superiores» a imponerse sobre las «inferiores».[7]​ El racismo en sentido restringido es una doctrina de apariencia científica que afirma la determinación biológica hereditaria de las capacidades intelectuales y morales del individuo, y la división de los grupos humanos en razas, diferenciadas por caracteres físicos asociados a los intelectuales y morales, hereditarios e inmutables.
Otorgar o retener derechos o privilegios basándose en la raza o rehusar asociarse con personas por su raza se conoce como discriminación racial.
Las actitudes, valores y sistemas racistas establecen, abierta o veladamente, un orden jerárquico entre los grupos étnicos o raciales, utilizado para justificar los privilegios o ventajas de las que goza el grupo dominante.
Buraschi y Aguilar definen el racismo como "un sistema de dominación y de inferiorización de un grupo sobre otro basado en la racialización de las diferencias, en el que se articulan las dimensiones interpersonal, institucional y cultural. Se expresa a través de un conjunto de ideas, discursos y prácticas de invisibilización, estigmatización, discriminación, exclusión, explotación, agresión y despojo”.[8]​
Para combatir el racismo, la Organización de Naciones Unidas adoptó en 1965 la Convención internacional sobre la eliminación de todas las formas de discriminación racial y estableció el día 21 de marzo como Día Internacional de la Eliminación de la Discriminación Racial.
El historiador israelí Benjamin Isaac ha propuesto una definición de racismo que permite identificar formas premodernas del mismo ya que no se sustenta en la idea del determinismo biológico.[9]​

Por su parte Philomena Essed en Understanding Everyday Racism: An Interdisciplinary Theory (Londres, 1991) ha propuesto la siguiente definición de racismo:[10]​

En 2004 el historiador israelí Benjamin Isaac publicó el libro The invention of Racism in Classical Antiquity que levantó una gran controversia porque en él afirmaba que, aunque «el racismo no existía en la Antigüedad clásica, bajo la forma moderna de un determinismo biológico», «ciertos rasgos característicos del racismo se encuentran ya en los textos de la literatura antigua, y las lecturas que han sido hechas sobre ellos en períodos ulteriores de la historia occidental les han conferido,  bajo diferentes formas, una influencia que no hay que pasarla por alto».[9]​ Isaac afirmaba la existencia de un «pensamiento racista pre-moderno» entre los griegos y los romanos antiguos por lo que la «genealogía» del racismo en Occidente se podía «rastrear» hasta «la Antigüedad clásica».[11]​
Entre los «rasgos característicos del racismo [que] se encuentran ya en los textos de la literatura antigua», Isaac ha señalado el determinismo medioambiental ampliamente admitido a partir de mediados del siglo V a. C. y que fue desarrollado por Aristóteles en el siglo siguiente en su Política.[12]​ Según Aristóteles el medio ambiente ideal en el que vivían los griegos les predisponía a gobernar a los pueblos menos favorecidos por la naturaleza.[13]​ Sin embargo, como ha destacado Maurice Sartre, las teorías que griegos y romanos desarrollaron para justificar la dominación sobre otros pueblos, «nunca desembocaron en políticas de exterminio, ni de exclusión deliberada. ¡Bien al contrario! Los griegos y los romanos permitieron muy ampliamente la integración de los “bárbaros”».[14]​
Isaac también ha señalado como otro rasgo «proto-racista» el mito de la autoctonía de la polis de Atenas, según el cual los atenienses ocupaban la tierra en la que vivían desde el inicio de los tiempos, por lo que sus linajes eran puros. Según Isaac, «esta valorización de la sangre pura mantiene con el racismo moderno una innegable proximidad».[13]​[15]​ Sin embargo, los historiadores contrarios a las tesis de Isaac citan a Isócrates que dio una definición del griego que privilegiaba la cultura en detrimento del nacimiento: «Se llama griegos más bien a las gentes que participan de nuestra educación más que a los tienen el mismo origen que nosotros».[16]​ 
Los historiadores que se oponen a las tesis de Isaac niegan que se pueda aplicar al mundo greco-romano el concepto de «raza»  y por tanto difícilmente se le puede imputar a la Antigüedad clásica el origen del racismo en Occidente. [17]​ Paulin Ismard lo afirma con rotundidad: «El pensamiento de la alteridad entre los autores griegos no ha dado lugar a la elaboración de ideologías raciales. La invocación regular del genos (el linaje, o la filiación) o de la ‘’eugenesia’’ (el buen nacimiento) para identificar grupos y comunidades no ha desembocado en la construcción de categoría raciales coherentes».[18]​ Lo mismo afirma Christian Geulen: «No puede hablarse de nacimiento del racismo a partir del espíritu de la Antigüedad». «Ver un peligro para la propia cultura en la existencia en sí de comunidades culturales ajenas dentro de las fronteras del Imperio, era extraño a la imagen propia tanto de los romanos como de los griegos».[19]​
Esto último también ha sido destacado por Maurice Sartre que pone como el «mejor ejemplo» al Imperio Romano ya que «funcionó como una formidable máquina de integrar, incluidas poblaciones que tenían una reputación detestable». «La integración a la antigua fue mucho más de lo que se cree respetuosa de las culturas indígenas: convertirse en “griego” o en “romano” nunca acarreó el abandono de tradiciones ancestrales». Sin embargo, Sartre valora positivamente el libro de Benjamin Isaac ya que, «desvelando esta cara oscura del pensamiento antiguo», «ayuda a comprender mejor los mecanismos del pensamiento racista a través del tiempo».[14]​ El mismo matiz apunta Christian Geulen: que los griegos y romanos no construyeran un pensamiento ni una praxis racista «no significa que haya que excluirlos de la historia del racismo».[20]​
El cristianismo aportó un nuevo concepto, el universalismo, hasta entonces ajeno a la Antigüedad al considerarse la verdadera religión de toda la humanidad. De esta forma la división entre griegos/romanos y «bárbaros», propia de la Antigüedad, fue sustituida por la diferenciación entre los que ya formaban parte de la comunidad cristiana, los bautizados, y «los todavía no cristianos» (los paganos).[21]​ Un grupo especial lo constituían los judíos ya que eran la cuna de la religión cristiana y por tanto no eran perseguidos, pero solo «conllevados» que no tolerados como lo demostrarían los pogromos que sufrieron sobre todo a partir del siglo XIV. Las regiones desconocidas de la Tierra en el imaginario del Occidente medieval aparecían pobladas por seres fabulosos no destinados a la salvación. «En esta cosmovisión difícilmente puede percibirse una dimensión racista», ha afirmado Christian Geulen, quien añade que en cuanto a «los conflictos políticos medievales, apenas puede hablarse de motivos racistas reconocibles».[22]​
Por otro lado, en el islam medieval la maldición de Cam, convenientemente reelaborada, fue utilizada para justificar la esclavitud de los negros al señalarlos como los descendientes de Cam que, según el relato bíblico, se había mofado de su padre Noé cuando lo encontró borracho y desnudo y Noé furioso había maldecido al hijo de Cam, Canaan, a ser «para sus hermanos el esclavo de los esclavos». En la Biblia nada se decía del color de la piel de Cam (en realidad se trataba de justificar la esclavitud de los cananeos, los grandes enemigos de Israel), pero en el siglo III el padre de la Iglesia Orígenes añadió a la maldición el prejuicio de la piel al afirmar que los hijos de Cam estaban abocados a una vida degradante marcada por la oscuridad (en sentido espiritual) y asoció a los etíopes, descendientes del hijo maldito de Noé, a los negros. En la Alta Edad Media los etíopes serán considerados el espíritu del mal que se opone al del ángel. Sin embargo, será el gran erudito árabe Al-Tabari el que en el siglo X afirmó claramente que la maldición de Cam había acarreado el ennegrecimiento de su piel por lo que sus descendientes eran los negros que estaban condenados a la esclavitud.[23]​
Los estatutos de limpieza de sangre fueron el mecanismo de discriminación legal en la Monarquía Hispánica (y el Reino de Portugal)[24]​ hacia la minoría judeoconversa (que junto con los miembros de la minoría morisca constituían los cristianos nuevos). Consistían en exigir al aspirante a ingresar en las instituciones que lo adoptaban el requisito de descender de «cristiano viejo», es decir, de no tener ningún antepasado judío, moro o condenado por la Inquisición. Causó rechazo en determinados sectores eclesiásticos por el hecho de que presuponían que ni siquiera el bautismo lavaba los pecados de los individuos, algo completamente opuesto a la doctrina cristiana. El primer estatuto de limpieza de sangre fue la "Sentencia-Estatuto" aprobada en 1449 en la ciudad de Toledo.[25]​ El más importante, y que sirvió de modelo a los posteriores, fue el aprobado en la catedral de Toledo en 1547 a propuesta del arzobispo Juan Martínez Silíceo y que fue ratificado por el papa y el rey Felipe II en 1555.[26]​
A partir del estatuto de Toledo los estatutos se difundieron rápidamente por la península:[27]​

El estatuto de limpieza de sangre de la catedral de Toledo fue confirmado sucesivamente por el Papa Paulo III en 1548, por el Papa Julio III en 1550, por el Papa Paulo IV en 1555 y por por el Rey Felipe II en 1556. Este estatuto permaneció vigente durante siglos y recién fue abolido en 1865.[27]​
Los estatutos de limpieza de sangre se basaban en «la idea de que los fluidos del cuerpo, y sobre todo la sangre, transmitían del padre y la madre a los hijos un cierto número de cualidades morales» y en la de que «los judíos, en tanto que pueblo, eran incapaces de cambiar, a pesar de su conversión».[29]​ El historiador francés Jean-Frédéric Schaub ha atribuido los estatutos de limpieza de sangre a la competencia para el acceso a los cargos y a las dignidades que para los cristianos, que pronto se llamarán a sí mismos «cristianos viejos», suponían los «cristianos nuevos», liberados por fin de las numerosas restricciones que como judíos padecían antes de la conversión. Además, «eclesiásticos y magistrados temían el debilitamiento de la ortodoxia católica romana» que podía suponer la entrada en la comunidad cristiana de estos nuevos miembros.[30]​
Sigue siendo objeto de debate si los estatutos de limpieza de sangre ibéricos son el origen del racismo europeo moderno.[31]​[32]​[33]​ Según Jean-Fréderic Schaub «la contribución de los estatutos de pureza de sangre ibéricos a la formación de las categorías raciales se sitúa en el punto de unión entre exclusión personal y estigmatización colectiva».[29]​ Según Max Sebastián Hering Torres, «por primera vez en la historia europea se utilizan los criterios "raza" y "sangre" como estrategia de marginación. Moralistas como Torrejoncillo no duda en afirmar [en Centinela contra judíos] que el judaísmo se define con base en la "sangre", sin importar que la conversión al cristianismo hubiera tenido lugar hace veintiuna generaciones».[34]​
Los estatutos de limpieza de sangre también se establecieron en el Imperio español en América como un instrumento para asegurar la preeminencia social de los «peninsulares» (los colonizadores nacidos en Europa, también llamados «gachupines» o «chapetones») y de los «criollos» (los colonizadores nacidos en América de ascendencia hispana). En este caso se trataba de demostrar que no se tenía ningún ascendiente indio o africano. Y esto era especialmente relevante en una sociedad colonial cada vez más mezclada étnicamente, hasta el punto de estructurarse según un sistema de castas determinado por el color de la piel ―lo que ha sido calificado como una «pigmentocracia»―.[35]​[36]​[37]​[37]​[38]​[39]​[40]​[41]​
Como ha destacado el hispanista británico John Elliott, «la limpieza de sangre se convirtió en la América española en un mecanismo para el mantenimiento del control por parte de la élite dominante. La acusación de sangre mezclada, que acarreaba el estigma de ilegitimidad (agravado por el de la esclavitud cuando había también ascendencia africana), se podía usar para justificar una política segregacionista que excluía a las castas de cargos públicos, desde el ingreso en corporaciones municipales y órdenes religiosas hasta la matriculación en colegios y universidades, y también de la afiliación en muchos gremios y cofradías».[42]​ Así pues, concluye Elliott, «la América colonial española se desarrolló hasta convertirse en una sociedad codificada por el color».[43]​
El sociólogo puertorriqueño Ramón Grosfoguel sostiene, por su parte, que el racismo aparece con la conquista europea de América a partir de 1492 y que es un proceso constitutivo del fenómeno de la «colonialidad»:

En la «Era de los Descubrimientos» los estudiosos se afanaron en poner orden a tanta diversidad étnica y cultural como se estaban encontrando los colonizadores y exploradores europeos y así se construyó el concepto moderno de «raza» basado en características fenotípicas.[45]​ Los religiosos fueron los que «comenzaron a describir y clasificar a los pueblos y culturas de la tierra en relación con sus características físicas y morales; así se destacaría la variedad de la creación de Dios y su orden armónico». Les siguieron eruditos, naturalistas, filósofos y médicos que al principio se esforzaron en seguir la doctrina de la Iglesia pero que pronto entraron en conflicto con la «doctrina creacionista».[46]​ El pionero en acuñar el término de «raza» en el sentido moderno fue el francés François Bernier en 1685.[45]​
La ruptura definitiva con la «doctrina creacionista» se produjo en el siglo XVIII cuando el hombre perdió su posición privilegiada, que según la Biblia le había concedido Dios, y fue situado dentro del reino animal.[47]​ En 1758 el naturalista sueco Linneo en la segunda edición de su Systema naturae situó al hombre en la clase de los mamíferos y en el orden de los primates y dividió el género Homo en dos especies: el Homo sapiens y el Homo troglodytes (en el que incluye a los orangutanes). El Homo sapiens presentaba cuatro variedades (razas) determinadas por el color de la piel: negro (niger), blanco (albus), rojo (rufus) y amarillo (luridus) y cada una de ellas habitaba en un continente diferente (Africa, Europa, América y Asia, respectivamente) y se caracterizaba por uno de los «cuatro temperamentos» (el asiático, melancólico; el americano, colérico; el europeo, sanguíneo; y el africano, flemático). Así, «la clasificación de Linneo une las características físicas, morales y culturales».[48]​[49]​
«El cuestionamiento del estatuto privilegiado del hombre en el seno de la naturaleza se acompaña también con la historización y la temporalización de las sociedades humanas y de la misma naturaleza. Ciencia del hombre y ciencia de la sociedad se construyen en paralelo en el siglo de las Luces. De Voltaire a Adam Smith, los autores diseñan una trayectoria universal desde el estado salvaje  hacia la sociedad civil, en el curso de la cual los pueblos pasan de una condición casi animal a la plena humanidad.[…] Es pues la capacidad del hombre de transformar la naturaleza la que define los estadios del salvajismo, la barbarie y la civilización».[50]​ Y esta división trinitaria de la historia sirve de fundamento para la racialización de la humanidad en cuanto que hay sociedades (los «blancos») que han alcanzado el estadio superior, la «civilización», mientras que el resto se encuentran «todavía» en los estadios inferiores de la «barbarie» o del «salvajismo».[51]​[52]​[53]​ Así pues, «se presentaba a los europeos blancos como la raza que superaba a todas las demás desde el punto de vista estético y moral».[54]​ Para explicar las múltiples variedades de seres humanos los ilustrados recurrieron en general a la vieja teoría aristotélica del determinismo medioambiental, singularmente Montesquieu.[55]​
El filósofo alemán Kant afirmó la existencia de «cuatro razas» (siguiendo a Linneo, las denominó «blanca», «amarilla», «negra» y «roja») y estableció una jerarquía entre ellas: «La humanidad existe en su mayor perfección en la raza blanca. Los hindúes amarillos poseen una menor cantidad de talento. Los negros son inferiores y en el fondo se encuentran una parte de los pueblos americanos». De los «negros» decía que únicamente podían desarrollar una «cultura de esclavos», mientras que los «blancos» eran los únicos con los talentos necesarios para la «cultura de la civilización».[56]​
La división de la humanidad en razas que se encontraban en estadios «inferiores» de la evolución histórica sirvió para «justificar» la esclavitud de millones de negros africanos llevados a América mediante la trata atlántica.[57]​ «En conjunto, la esclavitud y el tráfico de esclavos representaban la primera forma de un racismo ya plenamente definido en la Edad Moderna europea».[58]​ Pero se «hacía necesario explicar por qué se habían convertido a seres humanos en mercancías. En esta degradación real de los africanos convirtiéndolos en bestias de carga estuvo el auténtico origen histórico de por qué luego se los situó en el escalón inferior de la jerarquía racial», como hizo, por ejemplo, Kant.[59]​
Entre 1853 y 1855 el conde Arthur de Gobineau publicó Essai sur l’inegalité de races humaines (‘Ensayo sobre la desigualdad de las razas humanas’)[60]​[61]​ en el que sostenía que las civilizaciones acaban decayendo debido a la «degeneración racial» que inexorablemente se produce durante su desarrollo como resultado de la mezcla racial (solo la «pureza racial» habría evitado la decadencia, pero la propia dinámica de las civilizaciones hacía imposible mantenerla).[60]​[62]​.
«Gobineau se considera de hecho uno de los más importantes fundadores del racismo moderno… Desde la segunda mitad del siglo XIX hasta la primera del siglo XX, casi ninguna ideología o praxis racista ―ya fuera en contextos coloniales, antisemitas o totalitarios― dejó de situarse en la línea de racionalización teórica de Gobineau».[63]​ Sin embargo, el mito de la superioridad de la raza aria, base del antisemitismo, no fue obra de Gobineau sino de Ernest Renan, «el verdadero fundador del antisemitismo académico en Francia, un antisemitismo no político, estrictamente especulativo, que no hace ningún llamamiento a la persecución… Es Jules Soury quien efectúa el paso a la acción».[64]​
Otro pilar del «racismo científico»  fue el poligenismo desarrollado por el naturalista suizo Louis Agassiz. Los poligenistas se oponían al monogenismo ya que defendían que cada raza humana había tenido un origen diferente. Así se demostraba, según ellos, la existencia de razas «superiores» e «inferiores». Por otro lado Agassiz intentó hacer compatible el poligenismo con la Biblia para lo que afirmó que el relato de Adán solo se refería a la «raza caucásica».[60]​ No es casualidad que el poligenismo fuera adoptado por los antropólogos estadounidenses que defendían el sistema esclavista de los Estados del Sur.[65]​
Un tercer pilar del racismo «científico« fue la antropometría. Había aparecido a finales del siglo XVIII con la obra de Christoph Meiners y de Johann Friedrich Blumenbach ―pioneros en la craneometría―, pero su principal impulsor a nivel internacional en siglo siguiente fue el estadounidense Samuel Morton. El propósito de Morton era probar que se podía establecer «objetivamente» una jerarquía de las razas basándose en el tamaño del cerebro ―midiendo la capacidad craneal―. En Observations on the Size of the Brain in Various Races (‘Observaciones sobre el tamaño del cerebro en las diferentes razas’, 1849) dividió la humanidad jerárquicamente en seis grandes «razas»: «caucásica moderna», «caucásica antigua», «mongólica», «malaya», «americana» y, finalmente, «negra».[66]​ «Divulgar la supuesta condición inferior del indígena, del africano y del asiático permitía legitimar su conquista y su explotación, sin crear paradojas éticas con la moral de Occidente», ha subrayado Max Sebastián Hering Torres.[67]​
El vuelco definitivo en el concepto «científico»  de «raza» se produjo cuando se impuso la idea de que la naturaleza no era fija e inmutable sino que evolucionaba. El primer paso lo dio el biólogo francés Jean Baptiste Lamarck,[68]​ pero quien dio el paso definitivo fue Charles Darwin (El origen de las especies, 1859). Como ha destacado Christian Geulen, para Darwin «el impulso definitivo para la evolución como proceso de cambio era algo que nunca se hubieran atrevido a tener en cuenta los ilustrados: el azar. Este genera cambios de forma continua, que inmediatamente después son expuestos, a su vez, a la constante presión selectiva que se produce en la lucha diaria por la existencia». Los individuos con unas características con mayores posibilidades de sobrevivir en su medio ambiente tendrán mayores posibilidades para reproducirse y con ello adquirir también las nuevas características y así es como termina apareciendo una nueva especie.[69]​ Las metáforas que utilizó Darwin para explicar su teoría serían malinterpretadas como la supervivencia de los más fuertes y la adaptación al medio para sobrevivir. «Ambas teorías son falsas en tanto trastocan el azar en la teoría de Darwin», ha subrayado Christian Geulen.[70]​
La teoría de la evolución de Darwin tuvo una enorme influencia sobre el racismo.[71]​ Después de 1859 la idea de raza cambia: «se admite a partir de entonces que la humanidad ha evolucionado durante periodos de tiempo más largos de lo que se concebía anteriormente. Así se convierte en insostenible, incluso para un poligenista, pretender observar las razas humanas en su pureza original». De esta forma se alcanza el consenso entre los antropólogos de que «las razas no existen más que en el estado transformado e híbrido».[72]​ 
De la tergiversación y «adaptación» a la sociedad de la teoría de la evolución de Darwin surgió el darwinismo social inaugurado por Herbert Spencer. Otros darwinistas sociales destacados fueron Alfred Russel Wallace y Ernst Haeckel. [67]​ Una derivación del darwinismo social fue la eugenesia, concepto acuñado por el británico Francis Galton en 1883 y que Alfred Ploetz y Wilhelm Schallmayer[73]​ introdujeron en Alemania bajo el término Rassenhygiene ('higiene racial').[67]​
La «forma más clara de práctica racista» en el siglo XIX fue la esclavitud que siguió existiendo en el espacio africano y árabe y en América, tanto del Norte como del Sur. Y ello a pesar del creciente rechazo que suscitó ―ya el Congreso de Viena de 1815 la declaró ilegal― no sólo porque era contraria a los derechos del hombre sino también por la mezcla de razas que suponía. De hecho la esclavitud apenas fue legitimada por el racismo científico y sus defensores, como los dueños de las plantaciones en el Sur de Estados Unidos, recurrieron a otro tipo de argumentos, como el paternalismo sobre sus esclavos negros.[74]​
La situación cambió radicalmente con la Proclamación de Emancipación de 1863 —a la que siguió la Decimotercera Enmienda a la Constitución de los Estados Unidos— por la que los esclavos pasaron a ser hombres libres ya que fue a partir de entonces cuando se utilizaron los «argumentos» biológicos y raciales para «demostrar» la inferioridad de los negros ―además de recurrir a la intimidación y a la violencia por medio del Ku-Klux-Klan―. Al mismo tiempo se aprobaron las primeras prohibiciones explícitas de la mezcla racial y de las relaciones sexuales entre negros y blancos, que fueron castigadas para los negros con fuertes penas. En este contexto fue cuando se estableció la «regla de una gota» (One-Drop Rule), una norma «tristemente célebre para la comprobación de la pertenencia racial que estuvo vigente en los tribunales de los Estados del Sur hasta la década de 1970».[75]​ Según el One-Drop Rule era considerada negra aquella persona que llevara una sola gota de «sangre negra», es decir, quien tuviera un solo antepasado negro en las últimas cinco generaciones. Como ha señalado Christian Geulen, «de esta forma se declaró no existente, al menos jurídicamente, cualquier forma de mezcla racial; sólo había genuinos blancos o negros. Naturalmente se descartó una aplicación inversa del One-Drop-Rule para saber quién debería ser considerado jurídicamente como blanco…».[76]​
Conforme fue avanzando el siglo XIX los judíos fueron «quienes se convirtieron en objeto preferido de las ideologías racistas». El término «antisemitismo» nació en el Imperio Alemán en los años 1870-1880 «para dar nombre a una visión del mundo que veía los fundamentos de todo desarrollo cultural en la diferenciación y la lucha entre lo “ario” y lo “semita”». Así pues, como ha destacado Christian Geulen, el «antisemitismo racista» ―como él lo denomina― «no era en absoluto una simple aversión hacia los judíos. A finales del siglo XIX, y no sólo en Alemania, el antisemitismo era un programa de partido y una filosofía de la historia, un punto de vista político y una doctrina natural y social; era un medio esencial de entenderse a sí mismos…».[77]​
Según Geulen, la razón fundamental de la hostilidad «racial» hacia los judíos no residía en el tradicional antijudaísmo cristiano sino en el hecho de que, especialmente en Alemania, constituían «la única comunidad cultural minoritaria importante». «En ese estatus de “marginales establecidos” [«la integración de los judíos en la sociedad europea alcanzó su punto culminante en la segunda mitad del siglo XIX, y en especial en Alemania»] se centró buena parte de la propaganda antisemita». Para los antisemitas ultranacionalistas alemanes «el judaísmo no era sin más el enemigo de una pretendida raza alemana, sino también un enemigo del racismo como doctrina e interpretación del mundo ―y por tanto adquirió para los antisemitas cada vez más los rasgos de una raza fundamentalmente enemiga―». Uno de los primeros en expresar ese nuevo antisemitismo fue el historiador berlinés Heinrich von Treitschke quien en 1879 escribió: «Los judíos son nuestra desgracia». Pronto quedó claro, como lo demostró el affaire Dreyfus, que el nuevo antisemitismo alimentado por las teorías raciales no era sólo un fenómeno alemán.[78]​
En Francia se considera a Ernest Renan el verdadero fundador del «antisemitismo académico», pero, según Pierre-André Taguieff, «es Jules Soury quien efectúa el paso a la acción. Denuncia el dominio absoluto de los judíos sobre el aparato político, las instituciones, etc. […] La “lucha de las razas” es reinterpretada como la principal manifestación de la “lucha por la existencia” en los humanos. El combate entre el “Ario” y el “Semita” es una lucha a muerte».[64]​ Aún más radicales son las tesis antisemitas de Georges Vacher de Lapouge que recurre a la craniometría para establecer «científicamente» la jerarquía de las razas europeas en cuyo escalón inferior se encontraba «el Judío», «el único competidor peligroso del Ario en el presente», aunque está destinado a ser vencido por ser incapaz del «trabajo productivo» y estar desprovisto de «sentido político» y «espíritu militar».[79]​
La eugenesia, concepto acuñado por el británico Francis Galton en 1883 y difundido en el Imperio Alemán por Alfred Ploetz y Wilhelm Schallmayer bajo el término Rassenhygiene (‘higiene racial’),[67]​ conoció su periodo de apogeo entre finales del siglo XIX y 1945, «cuando la eugenesia quedó desacreditada por los crímenes del nacionalsocialismo».[80]​ Como ha destacado Christian Geulen, con la eugenesia, «la antigua idea de la lucha racial y el tema central de la mezcla de razas fueron completados con un tercer motivo que iba a transformar en totalitario el discurso racial, así como las prácticas que lo acompañaban: la idea de la creación artificial de la raza».[81]​
Cuando en 1912 se reunió en Londres el Primer Congreso Mundial de Eugenesia el movimiento eugenésico estaba ya muy extendido especialmente en Estados Unidos, Inglaterra, los Países Escandinavos y Alemania. Su objetivo era favorecer la reproducción de las características deseadas (eugenesia «positiva») y evitar la de las indeseadas (eugenesia «negativa»). En última instancia el proyecto eugenésico consistía en «poder controlar la evolución humana».[82]​ En algunos estados de Estados Unidos y en los Países Escandinavos, se habían decretado las primeras prohibiciones de reproducción y esterilizaciones forzosas de los «deficientes» (enfermos crónicos, discapacitados físicos y psíquicos, «criminales natos», etc.).[83]​ En lo que discrepaban los eugenistas entre sí era en si se debía primar la intervención en el medio, como sostenían los más liberales, o en la «crianza biológica», como sostenían los más reaccionarios.[84]​ 
Tras la terrible tragedia de la Gran Guerra de 1914-1918, aunque algunos teóricos raciales comenzaron a dudar de la efectividad de las medidas eugenésicas y a partir de entonces convirtieron en el centro de sus preocupaciones la posibilidad de una caída y decadencia de la propia raza, supuestamente superior ―dos obras ejemplificaron esta nueva percepción: Passing of the Great Race (‘El final de la gran raza’) del estadounidense Madison Grant y Der Untergang des Abendlandes (‘La decadencia de Occidente’) del alemán Oswald Spengler―,[85]​ la eugenesia no se cuestionó y aparecieron propuestas muy radicales, como la del jurista alemán Karl Binding y el psiquiatra forense Alfred Hoche que en un pequeño libro publicado en 1920 abogaron por la eliminación de las personas que llevaban una «vida indigna de la vida» (Vernichtung lebensunwerten Lebens) ―enfermos incurables y retrasados mentales― y que constituían una «existencia lastre» para la comunidad debido al alto coste que suponía cuidarlos y al gran número de camas de hospitales que ocupaban. Esta sería la política que aplicarían los nazis en la década siguiente por medio del programa secreto de exterminio denominado Aktion T4, disfrazado bajo el término «eutanasia».[86]​
La evolución de la ideología racista en la cultura alemana tuvo su máximo desarrollo con el movimiento nacionalsocialista (nazismo), liderado por Adolf Hitler, que obtuvo la adhesión de una gran parte de la población alemana en las décadas de 1930 y 1940, hasta que colapsó con la derrota de Alemania en la Segunda Guerra Mundial, en 1945. El nacionalsocialismo surgió como una ideología de superioridad de la llamada «raza blanca» y dentro de ella la supremacía de una hipotética «raza aria», de la cual los alemanes, eran considerados su expresión más pura. El racismo nazi estuvo dirigido contra las personas de origen judío —«el Judío» constituía la gran amenaza para la supervivencia de la nación alemana—y en segundo lugar contra otras minorías, como el pueblo gitano.
El judío internacional (1920), libro del famoso empresario estadounidense Henry Ford, fue de gran influencia en la expansión mundial del antisemitismo y en la ideología nazi, determinando la persecución y asesinato de los alemanes de origen judío, y luego de otras minorías étnicas como los gitanos. Esta clase de ideas se manifiesta en el desplazamiento, internamiento, y, más tarde, el exterminio sistemático de un número estimado de 11 millones a 12 millones de personas. En medio de la Segunda Guerra Mundial, aproximadamente la mitad de esas víctimas son judíos en lo que es históricamente recordado como el Holocausto (Shoah). Entre 100 000 y 1 000 000 de gitanos también fueron asesinados (Porraimos).
Tras la derrota de los fascismos en la Segunda Guerra Mundial el racismo perdió su poder de convicción y su legitimidad, pero los mitos y las prácticas racistas pervivieron durante un tiempo en el mundo poscolonial o que estaba en proceso de emancipación. El ejemplo más extremo lo representó el régimen sudafricano del apartheid que perduró hasta los años 1990. Según Christian Geulen, «apartheid significaba no sólo la separación de la población en una raza privilegiada y en otra que carecía de derechos en muchos aspectos. En la conciencia de los sudafricanos blancos era más bien un sistema que consideraban necesario para la estabilidad y la pervivencia de su forma de vida propia como colonizadores en un medio abiertamente hostil».[87]​ «Sudáfrica fue, sobre todo, la última de las regiones poscoloniales en las que el racismo y el mito de la lucha de razas se presentaron en la forma imperialista clásica de un conflicto entre culturas europeas y no europeas», concluye Geulen.[88]​
El apartheid fue un régimen de segregación racial implantado en Sudáfrica por colonizadores neerlandeses bóer o afrikaner, como parte de un régimen más amplio de discriminación política, económica, social y racial, de la minoría blanca de origen europeo sobre la mayoría negra aborigen, derivado a su vez del colonialismo. La palabra apartheid en afrikaner significa «segregación».
El apartheid propiamente dicho se inició en 1948 con la toma del poder por parte del Partido Nacional. Este partido decidió implantar un régimen racista que consolidara el poder de la minoría blanca e impidiera el mestizaje de la población. Con ese fin sancionó en 1949 la Ley de Prohibición de Matrimonios Mixtos n.º 55/49, que prohibió los matrimonios de personas consideradas «blancas» con personas consideradas «no blancas». Al año siguiente la separación sexual de los habitantes, según el tono de piel, se completó con la Ley de Inmoralidad n.º 21 de 1950, que reguló la vida sexual de los ciudadanos, prohibiendo la «fornicación ilegal», y «cualquier acto inmoral e indecente» entre una persona blanca y una persona africana, india, o de color. Estas normas implantaron lo que se conoció como «pequeño apartheid».
En 1955 en un congreso llevado a cabo en Kliptown, cerca de Johannesburgo, varias organizaciones opositoras, incluyendo el Congreso Nacional Africano y el Congreso Indio, formaron una coalición común que adoptó la Proclama de Libertad, con el fin de establecer un Estado sin discriminación racial. Las luchas anti racistas fueron severamente reprimidas por el régimen bóer, incluyendo matanzas y detenciones masivas. Entre los líderes negros detenidos se encontraba Nelson Mandela que permaneció preso durante 27 años (1963-1990).
Estados Unidos y los países de Europa Occidental toleraron el apartheid durante las décadas de 1950 y 1960, debido a que Sudáfrica había adoptado una posición abiertamente anticomunista, pero a partir de la década de 1970, el régimen sudafricano comenzó a ser rechazado por la opinión pública mundial y la mayor parte de la comunidad internacional, y su apoyo comenzó a limitarse a los Estados Unidos, Israel y las dictaduras iberoamericanas de ese momento (Argentina, Brasil, Chile, Uruguay, etc).[89]​ Finalmente a principios de la década de 1990 el apartheid sería abolido y en 1994 Nelson Mandela ocupaba la presidencia del país.
Tras conocerse las atrocidades cometidas por los nazis durante la Segunda Guerra Mundial, la noción biológica de «raza» quedó deslegitimada, aunque no completamente al principio,[90]​ y con ella el racismo científico.[91]​[92]​ También tuvo gran importancia ―mayor aún, según Christian Geulen―[90]​ el cambio de modelo de las ciencias humano-biológicas, ya iniciado antes de la guerra, que supuso el paso de la eugenesia a la genética y que fue definitivo cuando en 1953 se descubrió la estructura del ADN.[93]​ 
La Unesco, organismo internacional fundado en noviembre de 1945, declaró a la «raza» «la plaga del mundo» que había conducido a la humanidad a la catástrofe y en el acta de constitución condenó «el dogma de la desigualdad de las razas y los hombres». El antropólogo estadounidense de origen suizo Alfred Métraux, dirigente de la organización, le encargó a Claude Lévi-Strauss un ensayo que se convertirá en su célebre obra ‘’Raza e Historia”. Publicada en 1952 constituirá un punto de inflexión fundamental sobre el tema al hacer de la raza un concepto totalmente ilegítimo para pensar las diferencias humanas: solo existe una raza, la raza humana, con sus diferencias culturales.[94]​ A partir de entonces el término «raza», si es que se utiliza, se entenderá como una categoría social y no como un hecho natural.[95]​ Se alcanzó así un «consenso antirracista relativamente amplio», aunque no cristalizó hasta los años 1960, impulsado por los diferentes los movimientos sociales de la década entre los que destacó el movimiento por los derechos civiles en Estados Unidos que contribuyó «de forma notable a deslegitimar el racismo en la conciencia mundial». «El racismo se consideró un fenómeno anticuado, reaccionario y esencialmente premoderno».[96]​
La UNESCO puso como modelo el Brasil «mestizo», el país que habría superado las diferencias raciales y puesto fin al racismo ―una valoración que será puesta en duda años después―, mientras que el contramodelo será la Sudáfrica del apartheid, un país que iba «al revés de la historia» al aplicar una política declaradamente racista, lo que le supondrá su progresivo aislamiento internacional.[97]​
En la década de 1950 se extiende la visión optimista de que una vez deslegitimado el concepto de raza ―la ausencia de razas humanas es una verdad científica admitida y proclamada―[95]​ la extensión de la educación será suficiente para hacer desaparecer el racismo. El número de julio-agosto de 1950 de El Correo de la UNESCO así lo declara en su primera página: «Los científicos del mundo entero denuncian un mito absurdo… el racismo».[94]​ Una comisión de la UNESCO dirigida por el antropólogo estadounidense Ashley Montagu propone en 1950 que se deje de utilizar el término «raza» sustituyéndolo por la expresión «grupo étnico». El psicólogo canadiense Otto Klineberg propone a su vez como prioritario deshacer la ilusión de la «pureza racial». Siguiendo esta misma línea los historiadores franceses Lucien Febvre y François Crouzet escriben un ‘’Manual de historia de la civilización francesa” titulado «Somos mestizos» (“Nous sommes des sang-mêlés”) pero que solo será publicado muchos años más tarde, aunque algunos extractos del mismo aparecieron en una revista alemana en 1953. El libro-manual estaba destinado a los profesores y a los alumnos de secundaria con la finalidad de desarrollar la idea de que la humanidad es «una gran familia de pueblos unidos, y no un campo cerrado para batallas raciales que disfrazan (mal) horribles conflictos de interés». En uno de sus capítulos se decía: «Bienaventurada la nación que no es “pura”. Porque en la variedad extrema de tipos de individuos que la componen, podrá encontrar ciudadanos y ciudadanas capaces de hacer frente a todas las dificultades, a todas las pruebas que la vida reserva a un grupo de hombres organizados en nación. Y tanto mejor para ella».[98]​
Hubo países que fueron más allá del ámbito educativo ―como Francia mediante la ley Pleven aprobada en 1972― y consideraron el racismo, no una opinión sino un delito.[95]​ En otros países como Estados Unidos las opiniones racistas eran legales pero existía una presión social que las reducía a los círculos de los supremacistas blancos. [95]​
Sin embargo, el racismo biológico, a pesar de que estaba desacreditado, no desapareció del todo después de 1945. En 1947 el historiador francés Louis Chevalier explicaba que Francia tenía unos «valores raciales» que había que defender contra las «minorías extranjeras» que habían causado tanto mal al país. En 1950 el premio Nobel de física William Shockley proponía esterilizar a los negros por razones eugenésicas. En 1994 Richard Herrnstein y Charles Murray publicaban The Bell Curve en el que explicaban los resultados menos buenos de los escolares negros por deficiencias intelectuales innatas. En 2007 el bioquímico estadounidense James Dewey Watson afirmó que la inteligencia de los africanos era inferior a la de los occidentales. Como ha destacado Pap Ndiaye, «la llama mediocre del racismo biológico no ha sido mantenida más que por ideólogos de extrema derecha».[99]​ En una encuesta de 2020 solo el 8 % de los franceses consideraba que existían «razas superiores a otras».[97]​
En 1972 la socióloga francesa Colette Guillaumin constató que la deslegitimación del racismo biológico no había supuesto la desaparición del racismo y propuso el concepto de «racismo sin razas» para designar una forma persistente de rechazo al otro que no se basa en un discurso biológico puro y duro sino en la denuncia de costumbres y culturas tan radicalmente distintas que sería imposible que pudieran vivir juntas. Fue así como se asentó el concepto de racismo «cultural», racismo «diferencialista» o «neoracismo».[97]​ En 1984 el ensayista franco-tunecino Albert Memmi escribió:[100]​ 

El racismo cultural «no se basa en una jerarquía racial biológica (que toma la precaución de condenar en general), sino en diferencias culturales consideradas como irreductibles y antagónicas entre los grupos, y de las cuales el grupo dominante debería de protegerse, si no quiere desaparecer».[101]​ «Los racistas culturales consideran que los “otros” tienen globalmente modos de vida diferentes e inferiores a los suyos, que vienen de sociedades atrasadas, que tienen prácticas culturales heredadas de tiempos oscuros de la humanidad, de las cuales puede a veces brotar algún destello de creatividad, pero que siguen siendo inferiores a los suyos».[101]​ «En la determinación de aquello de lo que hay que protegerse, de lo que hay que mejorar y de lo que hay que defender, ya no se sitúa en primer plano la raza, sino la cultura, la sociedad, la nación o simplemente la forma de vida propia».[102]​
De acuerdo con María Dolores París Pombo, en su artículo titulado "Estudios sobre el racismo en América Latina",[103]​ publicado a través del Departamento de Relaciones Sociales, de la Universidad Autónoma Metropolitana, Unidad Xochimilco, se pueden distinguir dos tipos de ideología racista en América Latina, los cuales tienen orígenes históricos distintos:
En la actualidad, la región no cuenta con ningún grupo que se autodenomine de forma oficial como racista y difunda sus "creencias" o perspectivas con este enfoque de manera expresa; sino que este tipo de personas se manifiestan de forma más difusa en diversos estratos sociales, donde se encuentran presentes de forma cotidiana las categorías raciales como base para la valoración sociocultural.[104]​
Asimismo estas pueden verse reflejadas en elementos cotidianos y comúnmente aceptados por la población en general como en el caso de la presencia de referencias estéticas muy marcadas dentro de la publicidad, en los medios de comunicación, en las relaciones interpersonales, familiares, etc. hasta formas de referirse al aspecto físico de las personas en el mercado laboral: la "buena presentación" necesaria para ocupar puestos que implican contacto directo con clientes o el público en general. En estos casos, suele ocurrir que aquel individuo clasificado como "el indígena" o "el negro", tiene menores posibilidades de ascender en el ámbito laboral, el acceso a puestos políticos importantes en el ámbito nacional, desempeño educativo o el éxito cultural.[104]​
El origen del racismo en Ecuador se da en la época de la conquista, es decir entre los siglos XV y XVI, ya que en estos siglos se establecieron relaciones de dominación política y subordinación socio-cultural. Desde ese momento los grupos no europeos son considerados inferiores y no racionales de acuerdo a su tradición cultural y a sus características físicas.[105]​
En la sociedad ecuatoriana existe en el imaginario social, a manera de patrones de comportamiento que posibilitan la segregación de personas étnica y racialmente diferentes. Esta segregación en la mayoría de los casos se da por el prejuicio social o por el objetivo de preservar el estatus quo que es manejado por un grupo demográficamente mayoritario que maneja distintos puestos de poder dentro de la sociedad. En el Ecuador no existen legislaciones que legitimen el racismo, pero tiene prácticas que sí lo hacen y además han sido justificadas desde las instituciones estatales, gubernamentales y por los medios de comunicación.[106]​
Todo esto se debe a que el proceso de construcción de la identidad ecuatoriana se ha estructurado erróneamente a base del color de la piel y la apariencia física. Esto es respaldado gracias a que en la imagen publicitaria siempre se impone el ideal del “blanco” que no representa la realidad ecuatoriana. A manera de construcción del sentido de nación ecuatoriana ha sido un proceso contradictorio ya que por un lado se promueve la noción de ciudadanía, participación e igualdad constitucional, pero, por otro lado, en la práctica, se mantiene la intolerancia étnico-cultural hacia los grupos indígenas y afro-ecuatorianos.[107]​
La variedad socio cultural de Ecuador ha generado un sistema de clasificación en lo que se considera “normal” y que los grupos como los afros ecuatorianos o los indígenas no entran en la escala de normalidad. Esta práctica es el “etno-centrismo” que es parte de todo grupo socio cultural, sin embargo, este proceso se vuelve negativo cuando se ve las diferencias físicas del otro con intolerancia y se vuelve realmente negativo cuando se intenta eliminar estas diferencias, o, por ende, eliminar a los grupos sociales que corresponden a las características del otro.[107]​
La discriminación y el racismo contra los afroecuatorianos están relacionados con la pobreza y exclusión. Según el PRODEPINE, el 92.7 % de ellos no tienen acceso a los servicios básicos. El censo del 2001 revela que este pueblo registra un índice de NBI superior al 70 % frente al 45 % de los blancos y del 61.3 % nacional, su analfabetismo supera el 10.5 % frente al 5 % de los blancos y 9 % nacional. Mientras que la tasa de asistencia a la universidad apenas no supera 7 puntos respecto a 19 de los jóvenes blancos y de 14 del promedio nacional.
Actualmente existe un reconocimiento pluricultural y multi-étnico en Ecuador, aunque se mantienen estructuras del Estado unitario ecuatoriano que hace imposible el ejercicio del derecho a la diferencia cultural y la constitución de una sociedad intercultural. Se mantiene un discurso de pluriculturalidad que es utilizado por los grupos dominantes como una estrategia de usurpación simbólica.[108]​
En muchos países hoy en día está penalizado el racismo desde penas menores hasta mayores, considerando esta discriminación como delito, lo mismo que sucede por orientación sexual, cultural u otra característica. Algunos la penalizan con sanciones como puede ser el cobro de multas con dinero o hasta penas privativas de la libertad.
Miguel Hidalgo y Costilla, padre de la patria mexicana, emitió el Decreto contra la esclavitud, las gabelas y el papel sellado el 6 de diciembre de 1810 en la ciudad de Guadalajara, aboliendo la esclavitud.
En España no se abolió totalmente la esclavitud hasta el 7 de octubre de 1886. Si bien en el territorio peninsular se abolió en 1838, persistió y se toleró legalmente en las colonias amparada por la presión de las «Ligas Nacionales» que la defendían con argumentos patrióticos, hasta la creación de la figura transitoria del patronato y su definitiva abolición en 1886.[109]​
La esclavitud se prohíbe en los Estados Unidos de América en 1864 luego de la Guerra de Secesión mediante la Decimotercera Enmienda.
La segregación o separación racial es la separación de espacios, servicios y leyes para las personas de acuerdo a su ascendencia. Fue practicada en muchos lugares del mundo hasta mediados del siglo XX.
En 1868 que se derogaron las leyes segregacionistas que limitaban los derechos civiles de los afrocubanos bajo las antiguas Leyes de Indias, hasta entonces el código legal vigente en Cuba, con la abolición de los "Estatutos de limpieza de sangre".
En los Estados Unidos, se prohíbe en 1964 la aplicación desigual de los requisitos de registro de votantes y la segregación racial en las escuelas, en el lugar de trabajo e instalaciones que sirvan al público en general («lugares públicos») y en 1965 la Ley de derecho de voto.
La Declaración Universal de los Derechos Humanos, adoptada en 1948, indica que "toda persona tiene los derechos y libertades proclamados en esta Declaración, sin distinción alguna de raza", siendo uno de los documentos más relevantes contrarios a la discriminación y segregación racial.
Es gracias al avance de las diferentes ciencias y al retroceso progresivo del oscurantismo social, moral y religioso, que desde el último cuarto del siglo XX, existe un estigma social asociado con los que se describen a sí mismos como racistas. Las causas son varias, incluido el progreso social y tecnológico, pero principalmente la atención generada por los crímenes cometidos por británicos y españoles contra los habitantes de las Antillas y las Américas, el comercio de las naciones europeas con esclavos africanos, norteamericanos contra las naciones amerindias del continente, los turcos con el exterminio de los armenios, o con Alemania nazi contra judíos, gitanos y otros, y el horror causado por el Japón imperial en Corea, China y otros lugares, y los avances en las conquistas sociopolíticas de los afroamericanos en EE. UU.
Así que la identificación de un grupo o persona como racista tiene una carga de valor sumamente negativa. El último país en declararse oficialmente racista ha sido Sudáfrica que en 1990 modificó su sistema de apartheid por presiones internas y externas.
La Convención para la Prevención y la Sanción del Delito de Genocidio (1948) y la Convención Internacional sobre la Eliminación de todas las Formas de Discriminación Racial (1965) son los instrumentos internacionales fundamentales para comprender la aspiración humana de erradicar el racismo. Con el objeto de reafirmar el compromiso de los Estados con la eliminación de la discriminación racial y la realización efectiva del principio de igualdad en la región en el marco del Cuadragésimo Tercer Período Ordinario de Sesiones de la Asamblea General de la Organización de los Estados Americanos se lleva a cabo la Convención Interamericana contra el Racismo, la Discriminación Racial y Formas Conexas e Intolerancia (2013), este instrumento consolida estándares internacionales en la materia, y avanza en la definición legal de formas contemporáneas de racismo.
El Día Internacional de la Eliminación de la Discriminación Racial se celebra el 21 de marzo de cada año. Ese día, en 1960, la policía abrió fuego y mató a 69 personas en una manifestación pacífica contra la «ley de pases» del apartheid que se realizaba en Sharpeville, Sudáfrica. Al proclamar el Día en 1966, la Asamblea General instó a la comunidad internacional a redoblar sus esfuerzos para eliminar todas las formas de discriminación racial.[110]​[111]​ Desde entonces, el sistema del apartheid en Sudáfrica ha sido desmantelado. Leyes y prácticas racistas se han suprimido en muchos países, y se ha construido un marco internacional para luchar contra el racismo, guiado por la Convención Internacional sobre la Eliminación de todas las Formas de Discriminación Racial.
Se considera en medios científicos que el concepto actual de raza es una creación europea que se desarrolló como respuesta y justificación a su expansión imperial durante los siglos XVI al XX[cita requerida].
El concepto de raza, como demostración de la superioridad o inferioridad de ciertos grupos humanos, evolucionó progresivamente durante ese periodo hasta niveles muy sofisticados y eruditos para convertirse en una verdadera pseudociencia que más recientemente adquirió nombres como «etnografía» o «antropología física». El auge de estas doctrinas ideológicas disfrazadas de ciencia no han sido insignificantes y han resultado en regímenes tan destructivos como la Alemania Nazi o el Apartheid.
Según el terapeuta estadounidense Albert Ellis desde la psicología cognitiva, el racismo es un mecanismo de prejuicios que surgen por conveniencia, para discriminar, descartar o dominar a otras personas o aceptarlas preferentemente, sin tener remordimientos y sin reflexionar si eso bueno o malo o si es una opinión subjetivo u objetiva. Ordinariamente es una actitud hostil o favorable hacia una persona que pertenece a un grupo social. En la mayor parte de los casos se da por hecho que existe una inferioridad natural o genética en el grupo segregado, o bien, una circunstancia cualquiera que estable la inferioridad de sus integrantes. También es común se ponga un acento en las diferencias culturales, lo que explicaría la inferioridad o superioridad de los otros. Se tratan de alteraciones en la mente humana que son moderadamente difíciles de eliminar y llevan a una distorsión de la percepción o a una distorsión cognitiva, a un juicio impreciso o a una interpretación ilógica. Según la psicología cognitiva, el racismo es un apasionamiento subjetivo a favor o en contra de algo sin que existan argumentos suficientes para sustentar esta posición. Un sesgo es un error en el procesamiento de la información y hace que las personas tengan distorsiones cognitivas. Para poner a prueba las ideas racistas las personas deben someter a sus propios pensamientos a la prueba de la racionalidad, funcionalidad y objetividad.
En 1899 apareció el poema La carga del Hombre Blanco del escritor indo-inglés Rudyard Kipling, quien recibiera el premio Nobel ocho años después, en donde convoca al «Hombre Blanco» a conquistar y asumir el gobierno del mundo, como un servicio a las personas «no blancas», aun sabiendo que ello traería «el odio de aquellos que custodiáis».[112]​
En 1911 la 14.ª edición de la Enciclopedia Británica adoptó la ideología racista al sostener que «el Negro es intelectualmente inferior al caucásico».[113]​
Ya en la década de 1930, apareció en Francia la popular tira cómica Las aventuras de Tintín, de Hergé, portadora del pensamiento racista que había encarnado en el pensamiento occidental, especialmente notable en historias como Tintín en el Congo (1930-1931) o La oreja rota (1935).
En 1899 la ideología racista se consolidó con la publicación del libro Los Fundamentos del siglo XIX del inglés Houston S. Chamberlain. Profundizando las ideas de Gobineau, Chamberlain acentúa el papel de los pueblos germánico-nórdicos, como representantes auténticos de la hipotética «raza aria», y por lo tanto superiores a todos los demás. Chamberlain sostiene que la entrada de los pueblos germanos en la historia, alrededor del año 1200, significó «el ascenso de un nuevo mundo», la civilización europea, y que ese proceso histórico, aún en marcha, consiste en el «ascenso gradual de un mundo teutónico» en el que los elementos extraños no teutónicos serán hundidos como si fueran barcos piratas.[114]​
En las grandes potencias de la época aparecieron autores que intentaban demostrar que la «raza superior» eran los sajones (Gran Bretaña y los Estados Unidos),[115]​ los celtas (Francia),[116]​ y los teutones (Alemania). Varios pensadores británicos de la época utilizaron el racismo para justificar el Imperio Británico, como Thomas Henry Huxley (La Lucha por la Existencia en la Sociedad Humana,[117]​ 1888), Benjamin Kidd (Evolución Social,[118]​ 1894), P. Charles Michel (Una visión biológica de nuestra política internacional,[119]​ 1896), Charles Harvey (La Biología de la Política Británica,[120]​ 1904).
En 1902, el novelista estadounidense Thomas Dixon, Jr., publicó Las manchas del leopardo: un romance sobre la carga del Hombre Blanco - 1865-1900,[121]​ primera novela de una trilogía racista basada en la ideología de supremacismo blanco, que incluiría también The Clansmen (El hombre del Clan), en la cual se glorifica al Ku Klux Klan. Sobre esa trilogía, D. W. Griffith filmó en 1915 la película El nacimiento de una nación.
En 1885 el antropólogo haitiano Anténor Firmin publicó su tratado De l´egalité des races humaines, (De la igualdad de las razas humanas), en respuesta al famoso libro de Gobineau (De la Inegalité des races humaines),[122]​ y al colonialismo, en momentos en que los europeos se repartían África en la Conferencia de Berlín, ignorando a sus habitantes. Precursor del pensamiento antirracista y de la antropología moderna, la obra de Firmin sería ignorada por los académicos europeos durante décadas, hasta que el colapso moral del Holocausto, obligara a las potencias del mundo a asumir una posición pública contraria al racismo.
También hubo películas y telenovelas que muestran la realidad del racismo, en el caso del cine estadounidense, ciertos productores cinematográficos han sido acusados de racistas al poner a personajes antagónicos como negros, asiáticos, hispanoamericanos incluso hasta los indígenas de Norteamérica. También algunas telenovelas, sobre todo mexicanas, brasileñas, colombianas, venezolanas y entre otros, que han procurado superar el racismo como por ejemplo poniendo como protagonista principal a una mujer humilde de sector rural y que llega a la ciudad en busca de un futuro mejor y los antagonistas son los de clase social alta quienes promueven la discriminación y al fin la muchacha logra igualarse al nivel de sus contrincantes. Una de las telenovelas que ha tratado de superar estas barreras racistas es la de Niña moza, una muchacha hija de terratenientes y de clase alta quien lucha a favor de los esclavos para abolir la esclavitud y reconocer los derechos de libertad de los afroamericanos.
En primer lugar hay que hacer referencia que la educación no solo engloba a los centros educativos, sino que también a las familias ya que los padres son los primeros educadores. Cabe destacar que en 1933, la familia era la institución más importante como socializadora en la igualdad y solidaridad. Con esto, se ve reflejado que la educación en casa influye más de lo que pensamos en las creencias del niño, ya sea negativa o positivamente.
No obstante, centrándonos en los centros educativos, los inmigrantes sufren un periodo de adaptación. La mayor parte de los inmigrantes llegan al país huyendo de su situación económica del país de origen. Una de las mayores dificultades que se encuentran al llegar es el idioma o lengua, y es ahí donde las escuelas deben poner en marcha métodos útiles para su aprendizaje. No solo la lengua es uno de los factores que no conocen, sino la cultura ya que tienen otra forma de vivir o de ser, y puede provocar un choque cultural.
Tras esto, las personas tienen que socializar, y la escuela es un factor esencial y una oportunidad para que los niños o jóvenes comiencen a relacionarse. Sin embargo, en muchas ocasiones los inmigrantes tienen dificultades tanto con los compañeros, como con algunos profesores, lo que daría a lugar a una formación de un grupo de iguales del mismo lugar de origen o de inmigrantes. En las propias escuelas hay discriminación y racismo, las víctimas en muchas ocasiones prefieren acudir a colegios donde el porcentaje de inmigrantes sea alto para así no sentirse incómodo. Además, cabe destacar que prefieren acudir a centros públicos que a privados, ya que en estos encuentran más actitudes racistas. Todas estas actitudes, comentarios o acciones racistas los jóvenes las suelen realizar en grupo o no en solitario. Muchas de las víctimas del racismo prefieren callarse y otros prefieren decírselo a profesores. Además, existen prejuicios o estereotipos acerca de las posibilidades académicas y futuro de los inmigrantes.
El bullying relacionado con el color de piel aumenta en las estadísticas mientras en los colegios se sigue minimizando. La ausencia de protocolos específicos hace más dura la batalla de madres, hijos e hijas que sufren este acoso.[123]​
Además de las formas clasistas de racismo existen otras menos conocidas como el racismo aversivo, la mestizofobia y el racismo oculto.
En 1986, dos psicólogos sociales, Samuel L. Gaertner y John F. Dovidio, acuñaron el término "racismo aversivo" para definir el racismo de quienes no se consideran racistas. Se caracteriza de un racismo no explícito y que por su naturaleza sutil y ejecución no “a sabiendas” aparece diluido. Es el de aquellos que comienzan sus frases con un "Yo no soy racista, pero...". O de quienes hacen bromas racistas como si no tuvieran consecuencias. Pero las tiene: produce en sus víctimas una gran indefensión y frustración.[124]​ También se ha denominado microrracismo. Impide, por ejemplo, que en el ámbito escolar se puedan mezclar personas de ascendencias iberoamericana y española.[125]​
Una forma menos conocida de racismo es la creencia de que el mestizaje produce individuos inferiores a la «raza pura» (degeneración), defendido por Louis Agassiz, como Gobineau sostuvo.
Una forma moderna de racismo, como reacción al racismo contra los negros, los indios y asiáticos, es negar la identidad mestiza y la defensa de las poblaciones mezcladas más por su color oscuro de piel que por su condición mestiza. En este racismo, las poblaciones mestizas son tratadas como negro, indio o blanco, negando su peculiaridad.[126]​[127]​
El racismo oculto es una forma de racismo no explícita que busca la extensión y legitimación del racismo. Entre las variantes más comunes de racismo oculto se encuentran las pseudociencias sociales y médicas mencionadas arriba, la argumentación política en contra de determinados grupos humanos bajo pretextos culturales o étnicos y la manipulación de datos estadísticos con el fin de inferir indirectamente la inferioridad de unos grupos humanos sobre otros. Cabe mencionar al respecto que una de las formas más ominosas de racismo oculto es la relación post-facto y no causa-efecto entre pertenecer a una «raza» o «etnia» determinada y la pertenencia a una clase social.
La clasificación de las personas como perteneciente a una u otra raza ha sido ampliamente usada y aún lo es para mantener a grupos humanos en situación de sometimiento, a condiciones de vida de opresión, ignorancia y dependencia, y acusar a estos grupos de ser inferiores cuando sólo son víctimas y no causas del problema. Así mismo esta clasificación se usó y se utiliza para mantener la posición de mayor poder de otros grupos dentro de la escala social, estableciéndose un círculo vicioso de retroalimentación entre estatus socioeconómico y pertenecía a ciertas «razas». Este mecanismo se alimenta a sí mismo y se tiende a perpetuar ad infinitum hasta que sobrevengan cambios inevitables en la sociedad.
Algunos conceptos como la discriminación racial y la xenofobia están relacionados con el racismo aunque no lo son propiamente.
Es un concepto que suele identificarse con el de racismo y que lo abarca, aunque se trata de conceptos que no coinciden exactamente. Mientras que el racismo es una ideología basada en la superioridad de unas razas o etnias sobre otras, la discriminación racial es un acto que, aunque suele estar fundado en una ideología racista, no siempre lo está. En este sentido hay que tener en cuenta que la discriminación racial positiva (cuando se establecen discriminaciones con el fin de garantizar la igualdad de las personas afectadas), constituye una forma de discriminación destinada a combatir el racismo.
El prejuicio es una actitud social propagada entre la gente por una clase explotadora, a fin de estigmatizar a algún grupo como inferior, de modo que tanto la explotación del grupo como la de sus recursos pueda justificarse. Roger Bastide distingue el prejuicio de raza de los otros prejuicios que son: prejuicio de color, de clase y cultural.[128]​
El racismo suele estar estrechamente relacionado y ser confundido con la xenofobia, es decir el «odio, repugnancia u hostilidad hacia los extranjeros».[129]​ Sin embargo existen algunas diferencias entre ambos conceptos, ya que el racismo es una ideología de superioridad, mientras que la xenofobia es un sentimiento de rechazo; por otra parte la xenofobia está dirigida solo contra los extranjeros, a diferencia del racismo. El racismo también está relacionado con otros conceptos con los que a veces suele ser confundido, como el etnocentrismo y el colonialismo.
El Diccionario de la Real Academia Española tiene una definición para racismo.

El idioma turco ( Türkçe (?·i) o Türk dili) pertenece a la familia lingüística de las lenguas túrquicas, cuya área geográfica se extiende desde el occidente de China hasta los Balcanes. Las lenguas más próximas al turco son el azerí, el tártaro de Crimea, el gagauzo y el turcomano.
Es oficial en Turquía, donde se habla desde la época medieval, cuando los turcos procedentes de Asia Central se instalaron en Anatolia, que entonces era parte del Imperio bizantino. Es oficial también en Chipre, donde comparte cooficialidad con el griego, así como en la autoproclamada República Turca del Norte de Chipre. En algunas zonas balcánicas se habla una variedad conocida como turco otomano (Osmanlı Türkçesi), que tiene diferencias con el turco de Turquía. En varios países de la Europa occidental existen importantes comunidades de hablantes de turco, emigradas de Turquía en fechas recientes.
Es una lengua aglutinante, como lo son el quechua, el finés, el japonés o el vasco, y por tanto se basa en un sistema de afijos añadidos a la raíz de las palabras que permiten expresar gran cantidad de significados con pocas palabras. El turco usa casi exclusivamente sufijos. Su morfología no suele tener excepciones y es altamente regular. Otras importantes características son la ausencia del género gramatical, el orden sintáctico es SOV, que es una lengua de núcleo final y que usa postposiciones.
Ha tenido varios sistemas de escritura. Se escribió con caracteres árabes (alfabeto turco otomano) adaptados desde el siglo XIII hasta la reforma ortográfica emprendida en los años 1920 por el gobierno de Mustafa Kemal Atatürk, que emprendió varias iniciativas de occidentalización del país para contribuir a su modernización. La reforma ortográfica vino acompañada de un intento de "depuración" nacionalista, es decir, de sustituir la ingente cantidad de préstamos lingüísticos (sobre todo aquellos provenientes del árabe y, en menor medida, del persa) por vocablos de raíz turca, objetivo vigente hoy en día que no ha cosechado los éxitos esperados.[cita requerida] 
Está regulada por la Türk Dil Kurumu (TDK), la Sociedad de la Lengua Turca.
Ejemplo de texto: Artículo 1 de la Declaración Universal de los Derechos Humanos
El idioma turco pertenece al subgrupo de las lenguas oghuz, el cual incluye el gagauzo y el azerí. Las lenguas oghuz forman el subgrupo sur occidental de las lenguas túrquicas, una familia de lenguas que comprende más de treinta lenguas vivas habladas a través del este de Europa, Asia central y Siberia.[1]​ Algunos lingüistas ubican las lenguas túrquicas dentro de la macrofamilia de lenguas altaicas, aunque las similitudes de las lenguas que la conforman hoy se suelen considerar debidas al contacto areal. Cerca del 40% del total de hablantes de lenguas túrquicas son hablantes nativos del idioma turco. Las características del turco, tales como la armonía vocálica, aglutinación y la ausencia de género gramatical son universales dentro de las lenguas túrquicas y las lenguas propuestas como parte del grupo altaico. Existe un alto grado de mutua inteligibilidad entre el turco y otras lenguas oghuz, incluyendo el turcomano, el azerí, el kashgai, el gagauzo y el turco gagauzo de los Balcanes.
Nota: Pese a que (a) es la vocal abierta central no redondeada, se la considera como posterior debido a la armonía vocálica.

Una característica importante a la hora de analizar la estructura fonética del idioma turco, así como también otros de la familia de lenguas túrquicas, es la armonía vocálica. Esta regla consiste en que una palabra contendrá vocales anteriores (e, i, ö, ü) o vocales posteriores (a, ı, o, u) pero no ambas. Por ejemplo, en vişne (guinda ácida) /i/ es anterior cerrada no redondeada y /e/ es anterior abierta no redondeada. Esta regla se denomina «armonía palatal».
Además, las vocales no redondeadas (a, e, ı, i) tienen que ser seguidas por vocales no redondeadas; pero las vocales redondeadas (o, u, ö, ü) pueden ser seguidas por vocales abiertas y no redondeadas (a/e) o por cerradas y redondeadas (u/ü). Esta regla se llama la «armonía labial», por ejemplo: oduncu (leñador), kömürcü (carbonero), köylülerle (con los campesinos).
Según estas reglas, cada vocal puede ser seguida solo por dos vocales, que son:
Todos los sufijos también observan las mismas reglas.
La lengua turca, a diferencia de la mayoría de las lenguas europeas, es una lengua aglutinante, lo que quiere decir que con frecuencia usa afijos y específicamente sufijos o terminaciones gramaticales a la raíz de una palabra que no se modifica. [72] Una palabra puede tener muchos afijos y estos también se pueden usar para crear nuevas palabras, como crear un verbo a partir de un sustantivo o un sustantivo a partir de una raíz verbal (ver la sección sobre formación de palabras). La mayoría de los afijos indican la función gramatical de la palabra. [73] Los únicos prefijos nativos son sílabas intensificadoras aliterativas utilizadas con adjetivos o adverbios: por ejemplo, sım sıcak ("hirviendo" < sıcak ) y mas mavi ("azul brillante" < mavi ). [74] Cada sufijo tiene su propio significado gramatical.
El uso extensivo de afijos puede dar lugar a palabras largas, por ejemplo, Çekoslovakyalılaştıramadıklarımızdanmışsınızcasına, que significa "En la forma en que eres uno de esos que aparentemente no pudimos convertir al checoslovaco". Si bien este caso es artificial, las palabras largas ocurren con frecuencia en turco normal, como en este título de la columna necrológica de un periódico: Bayramlaşamadıklarımız (Bayram [festival] -Recipr-Impot-Partic-Plur-PossPl1; "Aquellos de nuestro grupo con quienes no intercambiar los saludos de la temporada "). [75] Otro ejemplo puede verse en la última palabra de este título de la Guía de ortografía turca en línea ( İmlâ Kılavuzu): Dilde birlik, ulusal birliğin vazgeçilemezlerindendir ("La unidad en el lenguaje es uno de los indispensables [dispense-Pass-Impot-Plur-PossS3-Abl-Copula] de la unidad nacional ~ La unidad lingüística es una condición sine qua non de la unidad nacional"). [76]
A la palabra puede ser añadido más de un sufijo, como en el ejemplo siguiente:
El turco no tiene género gramatical y el sexo de las personas no afecta la forma de las palabras. El pronombre en tercera persona o puede referirse a "él", "ella" o "eso". A pesar de esta falta, el turco todavía tiene formas de indicar el género en los sustantivos:

No existe un artículo definido en turco, pero la definición del objeto está implícita cuando se usa la terminación acusativo (ver más abajo). Los sustantivos turcos se declinan tomando terminaciones de declinaciones. Hay seis declinaciones o casos de sustantivos en turco, con todas las terminaciones después de la armonía de vocales (que se muestran en la tabla usando la notación de superíndice de taquigrafía ). Dado que la postposición "ile" a menudo se agrega como sufijo al sustantivo, algunos lo analizan como una declinación instrumental, aunque toma el genitivo con pronombres personales, demostrativos singulares y kim interrogativo. El marcador plural -ler ² sigue inmediatamente al sustantivo antes de cualquier declinación u otros afijos (p. Ej.köylerin "de los pueblos").
En turco hay seis declinaciones gramaticales: nominativo, genitivo, dativo, acusativo, locativo y ablativo. La vocal del sufijo depende de la armonía vocálica:
En la lengua turca no hay género gramatical, por lo que la palabra arkadaş (amigo) se puede utilizar tanto para hombres como mujeres. No hay artículo definido, pero el número bir (uno) se puede usar como un artículo indefinido. El plural se indica mediante el sufijo -lar/ler (dependiendo de la armonía vocálica), como en evler (casas) y atlar (caballos).
El marcador de caso acusativo se usa solo para objetos definidos; compare (bir) ağaç gördük "vimos un árbol" con ağacı gördük "vimos el árbol". [77] El marcador plural -ler  generalmente no se usa cuando se quiere decir una clase o categoría: ağaç gördük puede significar igualmente "vimos árboles [mientras caminábamos por el bosque]" - en contraposición a ağaçları gördük "vimos los árboles [en cuestión] ".
La declinación de ağaç ilustra dos características importantes de la fonología turca: asimilación de consonantes en sufijos ( ağaç t an, ağaç t a ) y expresión de consonantes finales antes de vocales ( ağa c ın, ağa c a, ağa c ı).
Además, los sustantivos pueden llevar sufijos que asignan a la persona: por ejemplo -imiz, "nuestro". Con la adición de la cópula (por ejemplo -im, "Yo soy") se pueden formar oraciones completas. La partícula interrogativa mi sigue inmediatamente a la palabra cuestionada, y también sigue la armonía de vocales: köye mi? "[yendo] al pueblo?", ağaç mı? "¿[es un] árbol?".
Los adjetivos turcos no se declinan. Sin embargo, la mayoría de los adjetivos también se pueden usar como sustantivos, en cuyo caso se declinan: por ejemplo, güzel ("hermosos") → güzeller ("(los) hermosos / personas"). Usados atributivamente, los adjetivos preceden a los sustantivos que modifican. Los adjetivos var ("existente") y yok (" inexistente ") se usan en muchos casos donde en español usaría "hay" o "tiene", por ejemplo , süt yok ("no hay leche", lit. "( la) leche (es) inexistente "); la construcción " sustantivo -GEN sustantivo -POSS var / yok "sustantivo "; imparatorun elbisesi yok " el emperador no tiene ropa "(" (el) emperador- de ropa- su inexistente "); kedimin ayakkabıları yoktu (" mi gato no tenía zapatos ", literalmente " gato- mi - de zapato- plur. - su tiempo pasado inexistente ").
Las primeras inscripciones túrquicas conocidas son las monumentales inscripciones Orkhon. Se encuentran en la actual Mongolia y fueron erigidas en honor al príncipe Kul Tigin y su hermano el emperador Bilge Khan. Estas inscripciones están fechadas entre los años 732 y 735. Después del descubrimiento y excavación de estos monumentos y otras losas de piedra por arqueólogos rusos en un área más amplia alrededor del valle de Orkhon entre 1889 y 1893, se estableció que el idioma de las inscripciones fue el antiguo túrquico. La escritura orkhon también se conoce como runas túrquicas, por sus formas parecidas a las runas germánicas. Sin embargo, esta lengua era diferente del turco otomano y el turco moderno.
Con la expansión turca a principios de la Edad Media europea (siglos VI al XI), los pueblos hablantes de lenguas túrquicas se extendieron a través de Asia Central, cubriendo una vasta zona geográfica desde Siberia hasta Europa y parte del Mediterráneo. La dinastía selyúcida en particular, llevó su lengua, el turco oghuz (ancestro directo del actual idioma turco) a Anatolia en el siglo XI. También durante el siglo XI, el lingüista qarajánida, Mahmud al-Kashgari, publicó el primer diccionario de una lengua túrquica y un mapa de la distribución geográfica de hablantes de lenguas túrquicas en el Compendio de dialectos túrquicos (en turco otomano: Divânü Lügati't-Türk). A principios del siglo XI d. C. en Anatolia, actualmente Turquía, apareció el llamado antiguo turco anatolio que es el antecesor medieval del moderno turco.
Seguido a la adopción del Islam alrededor del año 950 por los qarajánidas y los selyúcidas, quienes son considerados los ancestros culturales de los otomanos, la lengua administrativa de estos estados adquirió numerosos préstamos del idioma árabe y el idioma persa. La literatura turca del periodo otomano, particularmente la poesía divan o diwan, fue fuertemente influenciada por el persa, incluyendo la adopción de la métrica y una gran cantidad de palabras importadas. La lengua oficial y literaria del Imperio otomano (1299-1922) fue una mezcla de turco, árabe y persa que difería considerablemente del idioma hablado diariamente en ese periodo denominado kaba Türkçe hablado por personas con menor grado de escolaridad y por la gente de zonas rurales. Esta lengua era mucho más pura y se convirtió en la base del turco moderno.
Después de la fundación de la República de Turquía y la reforma de la escritura, la Sociedad de la Lengua Turca (TDK) se estableció en 1932 bajo el patronazgo de Mustafa Kemal Atatürk con el objetivo de llevar a cabo una investigación sobre la lengua. Uno de los objetivos de esta asociación era iniciar una reforma lingüística para reemplazar las palabras de origen árabe y persa con su equivalente de origen turco. Con la prohibición de usar palabras extranjeras en la prensa, la asociación consiguió eliminar cientos de palabras extranjeras del idioma turco. Mientras que la mayoría de las palabras introducidas en la lengua por la Türk Dili Kurumu (TDK) eran nuevos derivados de raíces turcas, esta asociación optó también por revivir palabras del antiguo túrquico que no habían sido usadas durante siglos.
Debido a este repentino cambio en el idioma, los hablantes de cierta edad y los jóvenes comenzaron a diferir en su léxico: mientras las generaciones nacidas antes de 1940 tendían a usar términos de origen persa y árabe, las nuevas generaciones favorecieron las nuevas expresiones. Es particularmente irónico que el propio Atatürk, en sus largos discursos al nuevo parlamento en 1927, usaba un estilo de dicción otomana que hoy sonaría tan extraña que necesitó tres traducciones al turco moderno: la primera en 1963, la segunda en 1986 y, nuevamente, en 1995. Existe también una dimensión política en este debate lingüístico, con grupos conservadores que emplean más arcaísmos, tanto en la prensa como en la vida diaria.[cita requerida]
Las últimas décadas han visto el trabajo continuo de la TDK para acuñar nuevas palabras, a fin de expresar nuevos conceptos y tecnologías a medida que entran en la lengua, mayormente del inglés. Muchos de esos términos, particularmente del área de la tecnología, han recibido amplia aceptación. No obstante, la TDK ha sido criticada por acuñar palabras que, según muchos, suenan artificiales. Algunos cambios tempranos, como bölem que vino a reemplazar a fırka (partido político), no logró el apoyo popular; de hecho fırka fue también reemplazada por la palabra parti tomada del francés. Algunas palabras tomadas del antiguo turco tomaron significados especializados, por ejemplo, betik (que significó originalmente libro) es usada para el llamado script de la informática.
Muchas palabras coexisten con su antigua contraparte. Esto ocurre usualmente cuando una palabra prestada de otro idioma cambia su significado original: por ejemplo, dert, derivada del persa dard (درد: dolor), significa "problema" en turco; [2]​ mientras la palabra turca nativa ağrı se usa para "dolor físico". En ocasiones, el préstamo lingüístico tiene en turco un significado ligeramente diferente de su original, lo cual crea una situación similar a la coexistencia de palabras de origen germánico y latino en el idioma inglés, como pork (carne de cerdo, cf. francés porc) y pig ("cerdo", "puerco"), to commence ("iniciar", "empezar", "comenzar", palabra de estilo formal; cf. francés commencer) y to begin ("empezar"), etc. Entre las antiguas palabras reemplazadas, hay términos de geometría, puntos cardinales, algunos nombres de meses y muchos sustantivos y adjetivos.
Hasta el 3 de noviembre de 1928, el turco se escribía en caracteres árabes. A partir de esa fecha, Mustafa Kemal Atatürk ordenó reemplazar esa escritura por una especialmente diseñada para la lengua turca por la Dil Encümeni (Comisión del Idioma), que se basó en el alfabeto latino, agregando algunos signos diacríticos, donde cada fonema es representado por un único signo. Dentro de tales signos diacríticos se cuentan las vocales «ö» y «ü» (que siguieron el modelo alemán, con similar valor, esto es, /œ/ y /y/), la vocal cerrada posterior no redondeada «I», «ı» /ɯ/ (con su contrapartida «İ», «i» para el sonido /i/), las dos consonantes con cedilla: «ç» para la «ch» /tʃ/, «ş» para la «sh» /ʃ/, y finalmente la «ğ» para /ɣ/.
Dicha política, conocida como «La revolución de los signos», tuvo el mérito de reducir considerablemente el analfabetismo y Turquía alcanzó un nivel de alfabetización de 92,2 % según una investigación de 2006 realizada por TESEV.[3]​
El idioma turco es hablado como lengua materna por el pueblo turco en Turquía y por la diáspora turca en otros treinta países. En general, las minorías turco-hablantes se encuentran en países que formaron parte, total o parcialmente, del Imperio otomano, tales como Grecia, Bulgaria, Chipre, Macedonia del Norte, Serbia y Rumanía. Más de tres millones de hablantes de turco se encuentran en Alemania y existen importantes comunidades en Francia, Países Bajos, Austria, Suiza, Bélgica y Reino Unido. Debido a la asimilación cultural de estas comunidades turcas en los países huéspedes no todos los inmigrantes hablan fluidamente el idioma turco.[cita requerida]
El número de hablantes de turco en Turquía es de alrededor de 70 millones, equivalente al 93 % de la población (91,3 % según el informe de TESEV arriba mencionado). La mayoría de las minorías lingüísticas del país son bilingües con el turco.
El turco es la lengua oficial de Turquía y es uno de los idiomas oficiales de Chipre. También tiene estatus oficial (pero no primario) en el distrito de Prizren en Kosovo y tres municipios de Macedonia del Norte, basado en la concentración de la población local turcohablante.
En Turquía, el órgano regulador del turco es la Asociación de la Lengua Turca (Türk Dil Kurumu o TDK), fundada en 1932 bajo el nombre Türk Dili Tetkik Cemiyeti (Sociedad de Investigación sobre el Lenguaje Turco). La Asociación de Lengua Turca fue influenciada por la ideología del purismo lingüístico: de hecho, una de sus tareas principales era la sustitución de préstamos y construcciones gramaticales extranjeras por equivalentes de origen turco, Estos cambios, junto con la adopción del nuevo alfabeto turco en 1928, formaron el moderno idioma turco que se habla hoy, TDK se convirtió en un organismo independiente en 1951, con el levantamiento del requisito de que fuera presidido por el Ministro de Educación. Este estatus continuó hasta agosto de 1983, cuando se convirtió de nuevo en un cuerpo gubernamental en la constitución de 1982, después del golpe militar de 1980.
En la provincia turca de Giresun, los habitantes de la aldea de Kuşköy se han comunicado utilizando una versión silbada del turco durante más de 400 años. La región está formada por una serie de valles profundos y el inusual modo de comunicación permite conversar a distancias de hasta 5 kilómetros. Las autoridades turcas estiman que todavía hay unas 10.000 personas que utilizan el idioma silbado. Sin embargo, en 2011, la UNESCO consideró que silbar turco era un idioma moribundo y lo incluyó en su lista de patrimonio cultural inmaterial . Desde entonces, la dirección de educación local lo ha introducido como un curso en las escuelas de la región, con la esperanza de reactivar su uso.
Un científico alemán de origen turco, Onur Güntürkün, de la Universidad del Ruhr, realizó un estudio en el que observó a 31 "hablantes" de kuş dili ("lengua de pájaro") de Kuşköy, y descubrió que el lenguaje silbado reflejaba la estructura léxica y sintáctica del idioma turco.[4]​
Fuentes en línea

Genji Monogatari (源氏物語, ''Genji Monogatari''?), generalmente traducida como Novela de Genji, Romance de Genji o Historia de Genji, es una novela clásica de la literatura japonesa, considerada por muchos como la novela más antigua de la historia, escrita alrededor del año 1000 por Murasaki Shikibu.[1]​[2]​[3]​
Realmente la obra no tiene título, por lo que ha recibido numerosas denominaciones, de las cuales la más común y aceptada es Genji Monogatari ( 源氏物語, La historia de Genji?). Sin embargo, ha sido publicada bajo diversos títulos:cuento o relato o historia Murasaki no Monogatari (紫の物語, El cuento/relato/historia de Murasaki?), Hikaru Genji (光源氏, 'Hikaru Genji'?), Genji (源氏, 'Genji'?), Gengo (源語, 'Gengo'?   palabra formada por el primer kanji de Genji y el segundo de monogatari), Shibun (紫文, 'Shibun'?   palabra formada por los ideogramas de Murasaki y de escrito), entre otros.
Murasaki Shikibu (紫 式部, Murasaki Shikibu?) una mujer de la realeza, que formaba parte de la corte de la Emperatriz a fines del siglo X y comienzos del siglo XI, es considerada la autora.[4]​[5]​ En la obra en sí no figura ningún autor, sin embargo en el Diario de Murasaki Shikibu, se encuentran, además de anécdotas varias y descripciones de los incidentes entre la realeza y la pleitesía, notas aclaratorias sobre La novela de Genji. Hay una teoría sobre coautoría de la novela. Yosano Akiko, la primera autora en hacer una traducción de Genji al japonés moderno, creía que Murasaki Shikibu había escrito los capítulos 1 al 33, y que los capítulos 34 al 54 fueron escritos por su hija Daini no Sanmi.[6]​
La novela de Genji —que cuenta con 54 rollos (帖, kakimono) o capítulos— se considera la obra maestra de la literatura dinástica japonesa, y la primera novela del mundo en el sentido moderno, después de los precedentes romanos de El Satiricón y El asno de oro, más los precedentes greco-bizantinos de la novela amatoria del tipo de Dafnis y Cloe.[cita requerida] Presenta varias características novedosas, como descripciones psicológicas detalladas de los personajes, y el personaje Kaoru —considerado por algunos el primer antihéroe—, entre otras. Por este rasgo puede considerarse el primer ejemplo de la novela psicológica.
Su lectura es una tarea difícil —incluso para los japoneses— debido a diversos factores. Primero, durante el período Heian, la realeza consideraba de buen gusto hablar citando o parafraseando refranes o poesías. La obra está dirigida a las mujeres de la realeza del período Heian, período en el cual la práctica general era no referirse por su nombre a una persona. Por ello, en la obra no se nombra a los personajes masculinos por su nombre, sino por rango o título, ni tampoco a los personajes femeninos, a los se introduce bajo alguna descripción de su vestimenta, citando la primera frase que hace el personaje al entrar en escena, o su relación con algún personaje importante. Con esto se daba a entender al lector —de la época— cuál era su posición social. 
Otra de las singularidades es que fue escrito utilizando el silabario kana empleado por las mujeres de la Corte (llamado onnade o «mano de mujer»), ya que el empleo de los ideogramas kanji de origen chino estaban reservados para ser escritos solo por hombres, lo que generó en el texto muchas palabras ambiguas, que no siempre son deducibles por el contexto.[7]​
A pesar de ser una novela, la autora —que era una poetisa de la realeza— introdujo numerosas poesías, por lo que es también considerado un excelente exponente de la poesía dinástica japonesa del período Heian.[8]​
Genji Monogatari es una novela de principios del siglo XI, cerca del cenit del período Heian. Cuenta la historia del príncipe Genji a través de 54 capítulos que incluyen toda su vida amorosa, su recuperación del poder imperial y la vida de su hijo y su "nieto" tras su muerte. Se ha sugerido que el personaje protagonista fue inspirado por la figura de Minamoto no Tōru. De las traducciones al inglés, las de Arthur Waley y la de Royall Tyler son las más reconocidas. 
Tanto por la extensión, los contenidos, y la calidad literaria de la obra, es considerada una de las más influyentes dentro de la literatura japonesa. Es una novela de corte moderno que narra la vida política y amorosa del príncipe Genji y de sus descendientes, reflejando la vida de la corte imperial japonesa, al tiempo que describe las emociones derivadas de la poligamia usual de la época. También refleja el carácter fugaz de la vida.[8]​
Autores como Jorge Luis Borges, Octavio Paz, Yasunari Kawabata o Marguerite Yourcenar se han hecho eco de la importancia del Genji.[9]​
La novela se ha dividido tradicionalmente en tres partes: las dos primeras tratan de la vida de Genji, y la última desarrolla los primeros años de dos de sus descendientes más destacados, Niou y Kaoru. También hay algunos capítulos más breves de transición, que se suelen agrupar por separado y de los que es puesta en duda su autoría. 
Capítulos 1-41: La historia de Genji 
Capítulos 42-44: Son de transición, episodios muy cortos que siguen a la muerte de Genji.
Capítulos 45-54: los llamado Uji, cuentan la historia de los descendientes de Genji, Niou y Kaoru.[10]​
El último, y por tanto número 54, es el capítulo llamado «El puente flotante de los sueños». Sobre él se argumenta a veces que es una parte separada de la sección de Uji. Parece que continúa la historia de los capítulos anteriores, pero tiene un título del capítulo particularmente abstracto. Es el único capítulo donde el título no tiene ninguna referencia clara en el texto, aunque esto puede ser debido a que el capítulo está sin terminar. Además existe el hecho de que desconocemos cuando los capítulos adquirieron su título.
La obra recoge la vida del hijo de un emperador japonés, conocido por los lectores como Hikaru Genji, o Shining Genji. Esta denominación no es su nombre real: Genji (源氏?) sino otra manera de leer los caracteres chinos del clan Minamoto (源 の 氏, Minamoto-no-Uji?) al que Genji perteneció. Por razones políticas, Genji es relegado a la condición de plebeyo (dándole el nombre Minamoto) para poder realizar una carrera como funcionario imperial. De esta manera, el personaje puede moverse en dos esferas, la imperial y la plebeya.[11]​  
Genji era el segundo hijo de un antiguo emperador y una concubina de bajo rango. Su madre muere cuando Genji tiene tres años, sumiendo al Emperador en una gran tristeza ya que no puede olvidarla. Cuando oye hablar de una mujer, la Señora Fujitsubo, que se parece a ella, la hará su esposa. Genji también queda fascinado por ella. Visita Kitayama, un área montañosa y rural al norte de Kioto, y allí  encuentra a una preciosa niña de diez años por la que se siente subyugado, Murasaki, que es sobrina de la Señora Fujitsubo. La secuestra, la lleva a su palacio y la educará para ser su mujer ideal. Durante este tiempo, Genji también se encuentra de forma secreta con la Señora Fujitsubo que engendrará un hijo. Todo el mundo, excepto los dos enamorados, creen que el padre de la criatura es el Emperador. El hijo del Emperador se convierte en el heredero a la corona y la Señora Fujitsubo en la Emperatriz, pero Genji y Fujitsubo juran mantener el secreto.[11]​ 
Genji se reconcilia con su esposa, la señora Aoi, y ella da a luz a un hijo, aunque muere poco después. Encuentra consuelo en Murasaki, con quien termina casándose. El padre de Genji, el Emperador, muere, y sus enemigos políticos toman el poder. Entonces otro asunto secreto de los amores de Genji sale a la luz: Genji y una concubina de su hermano, el Emperador Suzaku, son descubiertos cuando se encuentran a escondidas. Al Emperador no le molestan estos encuentros de Genji con la mujer, pero tiene el deber de castigarle, por lo que es exiliado en la ciudad de Suma. Allí, un próspero hombre de Akashi acoge a Genji, que mantiene una relación amorosa con su hija. Ella da a luz una hija, que se convertirá más tarde en Emperatriz.[11]​
Vuelve del exilio triunfante, reclamado por el Emperador. El nuevo emperador Reizei sabe que Genji es su verdadero padre y eleva el rango de Genji a lo más alto posible. Se casa con otra esposa, la llamada Tercera Princesa, que es violada y tendrá un hijo, Kaoru. Esta nueva boda de Genji afecta y modifica la relación entre él y Murasaki, quien muere. Genji la sobrevive  apenas unos años, tras alejarse de la vida mundana.[11]​
El resto de la obra es conocida como los «Capítulos Uji». En estos capítulos se sigue a Niou y Kaoru, que son grandes amigos. Niou es un príncipe imperial, el hijo de la hija de Genji, la actual Emperatriz desde que Reizei ha abdicado. Kaoru es reconocido como hijo de Genji, pero en realidad es hijo de su sobrino. Se muestra la rivalidad entre ambos personajes por las hijas de un príncipe imperial que vive en Uji, una ubicación situada a cierta distancia de la capital. El cuento termina bruscamente, mostrando a Kaoru preguntándose si la chica a la que ama ha sido escondida por Niou. A Kaoru se le ha llamado a veces como el primer antihéroe de la literatura.[12]​
Como la obra fue escrita para el entretenimiento de la corte japonesa del siglo undécimo, el trabajo presenta varias dificultades para los lectores modernos. En primer lugar, y seguramente el punto más importante, es la lengua que usa Murasaki: en la corte japonesa del periodo Heian, la lengua contenía numerosas declinaciones y una gramática compleja. Otro de los problemas radica en que, en la sociedad cortesana Heian, se consideraba rudo y descortés el hecho de mencionar los nombres de las personas y, por tanto, ninguno de los personajes es nombrado a lo largo de la obra. En lugar de ello, la narración se refiere a los hombres según su rango o el punto de la vida donde se encuentran y a las mujeres por el color de su ropa, por las palabras usadas en un encuentro, o bien por el rango de un pariente masculino prominente. Esto tiene como resultado una descripción diferente para un mismo personaje según el capítulo de la obra.[7]​
Otro aspecto es la importancia de utilizar poesía en las conversaciones. Modificar o reescribir un poema clásico resulta complicado, ya que servían para comunicar sutiles alusiones de forma oculta. Los poemas en Genji aparecen a menudo en la forma Tanka de la clásica poesía japonesa Waka.[7]​ Muchos de los poemas eran conocidos por el público y generalmente, solo al leer las primeras líneas, ya se daba por supuesto que quien lo leía sería capaz de completar el texto por sí mismo.
Como la mayoría de la literatura Heian, el Genji probablemente fue escrito en gran parte, si no en su totalidad, en kana (la escritura fonética japonesa) y no con los caracteres chinos.
Dejando de lado el vocabulario referente a la política y al budismo, el Genji contiene muy pocas palabras prestadas del chino. Esto tiene el efecto de dar a la historia, incluso de forma muy clara, mucha fluidez. Sin embargo, también lleva a la confusión: hay una serie de palabras del vocabulario en japonés «puro», que tienen muchos significados diferentes y, para los lectores modernos, el contexto no siempre es suficiente para determinar el sentido que se pretendía.
Existe una única traducción directa del japonés al castellano, la de Hiroko Izumi Shimono e Iván Pinto Román del Fondo Editorial de la Asociación Peruano Japonesa. Contiene notas que facilitan la comprensión de la obra y hermosas ilustraciones de la colección de Kuyobunko de la Universidad de Waseda. Esta versión incluye también un anexo con los personajes principales. 
Se han realizado dos traducciones indirectas del original: la de Jordi Fibla en Ediciones Atalanta parte de Royall Tyler que es la más pura filológicamente; la de Xavier Roca-Ferrer en Ediciones Destino utiliza la de Arthur Waley, la traducción canónica pero que se permite una gran adaptación para facilitar su comprensión (por ejemplo asignar nombres a los personajes). 
Genji Monogatari se ha adaptado al cine varias veces. En 1951 por el director Kōzaburō Yoshimura, en 1966 por el director Kon Ichikawa, y en 1987 por el director Gisaburo Sugii. Esta última adaptación es una película de animación. No es una versión completa del cuento, sino que básicamente, cubre los primeros 12 capítulos, añadiendo una serie de motivaciones psicológicas de más que no aparecen explícitamente en la novela. En 2001, Tonkō Horikawa realizó una adaptación con un reparto únicamente femenino. En la película Sennen no Koi - Hikaru Genji Monogatari («Genji, un amor de 1000 años»), Murasaki explica la historia de Genji a una chica como una lección del comportamiento de los hombres.
La película Yokihi (o "La Princesa Yang Kwei-fei") de Kenji Mizoguchi, del año 1955, se puede considerar como una especie de protosecuela del Genji.
A principios de 2009, se emitió en la televisión japonesa Genji Monogatari Sennenki, una serie anime de once episodios basada en el Genji Monogatari. Esta versión fue dirigida por Osamu Dezaki.
Genji Monogatari también se ha adaptado para la ópera por Minoru Miki. La obra fue compuesta durante 1999 y se hizo la primera representación en la Ópera Teatro de San Luis (Misuri), con libreto de Colin Graham, en inglés. El libreto fue posteriormente traducido al japonés por el compositor.

Mercurio es el planeta del sistema solar más cercano al Sol y el más pequeño. Forma parte de los denominados planetas interiores y carece de satélites naturales al igual que Venus. Se conocía muy poco sobre su superficie hasta que fue enviada la sonda planetaria Mariner 10 y se hicieron observaciones con radar y radiotelescopios. Posteriormente fue estudiado por la sonda MESSENGER de la NASA y actualmente la astronave de la Agencia Europea del Espacio (ESA) denominada BepiColombo, lanzada en octubre de 2018, se halla en vuelo rumbo a Mercurio a donde llegará en 2025 y se espera que aporte nuevos conocimientos sobre el origen y composición del planeta, así como de su geología y campo magnético.
Antiguamente se pensaba que Mercurio siempre presentaba la misma cara al Sol (rotación capturada), situación similar al caso de la Luna con la Tierra; es decir, que su periodo de rotación era igual a su periodo de traslación, ambos de 88 días. Sin embargo, en 1965 se mandaron impulsos de radar hacia Mercurio, con lo cual quedó definitivamente demostrado que su periodo de rotación era de 58,7 días, lo cual es ⅔ de su periodo de traslación. Esto no es coincidencia, y es una situación denominada resonancia orbital.
Al ser un planeta cuya órbita es inferior a la de la Tierra, lo observamos pasar periódicamente delante del Sol, fenómeno que se denomina tránsito astronómico. Observaciones de su órbita a través de muchos años demostraron que el perihelio gira 43" de arco más por siglo de lo predicho por la mecánica clásica de Newton. Esta discrepancia llevó a un astrónomo francés, Urbain Le Verrier, a pensar que existía un planeta aún más cerca del Sol, al cual llamaron Vulcano, que perturbaba la órbita de Mercurio. Ahora se sabe que Vulcano no existe; la explicación correcta del comportamiento del perihelio de Mercurio se encuentra en la teoría general de la relatividad de Einstein.
Mercurio es uno de los cuatro planetas rocosos o sólidos; es decir, tiene un cuerpo rocoso, como la Tierra. Este planeta es el más pequeño de los cuatro, con un diámetro de 4879 km en el ecuador. Mercurio está formado aproximadamente por un 70% de elementos metálicos y un 30% de silicatos. La densidad de este planeta es la segunda más alta de todo el sistema solar, siendo su valor de 5430 kg/m³, solo un poco menor que la densidad de la Tierra. La densidad de Mercurio se puede usar para deducir los detalles de su estructura interna. Mientras la alta densidad de la Tierra se explica considerablemente por la compresión gravitacional, particularmente en el núcleo, Mercurio es mucho más pequeño y sus regiones interiores no están tan comprimidas. Por tanto, para explicar esta gran densidad, el núcleo debe ocupar gran parte del planeta y además ser rico en hierro,[3]​ material con una alta densidad.[3]​ Los geólogos estiman que el núcleo de Mercurio ocupa un 42% de su volumen total (el núcleo de la Tierra apenas ocupa un 17%). Este núcleo estaría parcialmente fundido,[4]​[5]​ lo que explicaría el campo magnético del planeta.
Rodeando el núcleo existe un manto de unos 600 km de grosor. La creencia generalizada entre los expertos es que en los principios de Mercurio un cuerpo de varios kilómetros de diámetro (un planetesimal) impactó contra él deshaciendo la mayor parte del manto original, dando como resultado un manto relativamente delgado comparado con el gran núcleo.[6]​ (Otras teorías alternativas se discuten en la sección Formación de Mercurio).
La corteza mercuriana mide en torno a los 100-200 km de espesor. Un hecho distintivo de la corteza de Mercurio son las visibles y numerosas líneas escarpadas o escarpes que se extienden varios miles de kilómetros a lo largo del planeta. Presumiblemente se formaron cuando el núcleo y el manto se enfriaron y contrajeron al tiempo que la corteza se estaba solidificando.[7]​
La superficie de Mercurio, como la de la Luna, presenta numerosos impactos de meteoritos que oscilan entre unos metros hasta miles de kilómetros. Algunos de los cráteres son relativamente recientes, de algunos millones de años de edad, y se caracterizan por la presencia de un pico central. Parece ser que los cráteres más antiguos han tenido una erosión muy fuerte, posiblemente debida a los grandes cambios de temperatura que en un día normal oscilan entre 623 K (350 °C) por el día y 103 K (−170 °C) por la noche.
Al igual que la Luna, Mercurio parece haber sufrido un período de intenso bombardeo de meteoritos de grandes dimensiones, hace unos 4 000 000 000 (cuatro mil millones) de años. Durante este periodo de formación de cráteres, Mercurio recibió impactos en toda su superficie, facilitados por la práctica ausencia de atmósfera que pudiera desintegrar o frenar multitud de estas rocas. Durante este tiempo, Mercurio fue volcánicamente activo, formándose cuencas o depresiones con lava del interior del planeta y produciendo planicies lisas similares a los mares o marías de la Luna; una prueba de ello es el descubrimiento por parte de la sonda MESSENGER de posibles volcanes.[8]​
Las planicies o llanuras de Mercurio tienen dos edades distintas; las jóvenes llanuras están menos craterizadas y probablemente se formaron cuando los flujos de lava enterraron el terreno anterior. Un rasgo característico de la superficie de este planeta son los numerosos pliegues de compresión que entrecruzan las llanuras. Se piensa que, como el interior del planeta se enfrió, se contrajo y la superficie comenzó a deformarse. Estos pliegues se pueden apreciar por encima de cráteres y planicies, lo que indica que son mucho más recientes.[9]​ La superficie mercuriana está significativamente flexada a causa de la fuerza de marea ejercida por el Sol. Las fuerzas de marea en Mercurio son un 17 % más fuertes que las ejercidas por la Luna en la Tierra.[10]​
Destacable en la geología de Mercurio es la cuenca de Caloris, un cráter de impacto que constituye una de las mayores depresiones meteóricas de todo el sistema solar; esta formación geológica tiene un diámetro aproximado de 1550 km (antes del sobrevuelo de la sonda Messenger se creía que su tamaño era de 1300 km). Contiene, además, una formación de origen desconocido no antes vista ni en el propio Mercurio ni en la Luna, y que consiste en aproximadamente un centenar de grietas estrechas y de suelo liso conocida como La Araña; en el centro de esta se encuentra un cráter, desconociéndose si dicho cráter está relacionado con su formación o no. Interesantemente, también el albedo de la cuenca de Caloris es superior al de los terrenos circundantes (al revés de lo que ocurre en la Luna). La razón de ello se está investigando.[11]​
Justo en el lado opuesto de esta inmensa formación geológica se encuentran unas colinas o cordilleras conocidas como Terreno Extraño, o Weird Terrain. Una hipótesis sobre el origen de este complejo geomorfológico es que las ondas de choque generadas por el impacto que formó la cuenca de Caloris atravesaron toda la esfera planetaria convergiendo en las antípodas de dicha formación (180 °), fracturando la superficie[12]​ y formando esta cordillera.
Al igual que otros astros de nuestro sistema solar, como el más semejante en aspecto, la Luna, la superficie de Mercurio probablemente ha incurrido en los efectos de procesos de desgaste espaciales, o erosión espacial. El viento solar e impactos de micrometeoritos pueden oscurecer la superficie, cambiando las propiedades reflectantes de ésta y el albedo general de todo el planeta.
A pesar de las temperaturas extremadamente altas que hay generalmente en su superficie, observaciones más detalladas sugieren la existencia de hielo en Mercurio. El fondo de varios cráteres muy profundos y oscuros cercanos a los polos que nunca han quedado expuestos directamente a la luz solar tienen una temperatura muy inferior a la media global. El hielo (de agua) es extremadamente reflectante al radar, y recientes observaciones revelan imágenes muy reflectantes en el radar cerca de los polos;[13]​ el hielo no es la única causa posible de dichas regiones altamente reflectantes, pero sí la más probable. Se especula que el hielo tiene solo unos metros de profundidad en estos cráteres, conteniendo alrededor de una tonelada de esta sustancia. El origen del agua helada en Mercurio no es conocido a ciencia cierta, pero se especula que o bien se congeló de agua del interior del planeta o vino de cometas que impactaron contra el suelo.[14]​
Cartografía de Mercurio realizada por la Mariner 10 en el periodo 1974-1975
Mosaico de la mitad de Cuenca de Caloris. Fue fotografiado por la sonda Mariner 10
La formación geomorfológica conocida como Terreno Extraño
Imagen radar del polo norte de Mercurio
Una fractura en el terreno mercuriano, Discovery Scarp, de unos 350 km. de largo
Una vieja cuenca, de 190 km. de diámetro (43°S, 55°O)
Una foto de la parte no revelada hasta la llegada de la sonda MESSENGER
El estudio de la interacción de Mercurio con el viento solar ha puesto en evidencia la existencia de una magnetosfera en torno al planeta. El origen de este campo magnético no es conocido. En 2007, observaciones muy precisas realizadas desde la Tierra mediante radar, demostraron un bamboleo del eje de rotación compatible solo con un núcleo del planeta parcialmente fundido.[4]​[5]​ Un núcleo parcialmente fundido con materiales ferromagnéticos podría ser la causa de su campo magnético.
La intensidad del campo magnético es de 220 nT.[15]​
La órbita de Mercurio es la más excéntrica entre todos los planetas que orbitan el Sol, (antes de ser reclasificado como planeta enano, esa característica le correspondía al entonces planeta Plutón). La distancia de Mercurio al Sol varía en un rango entre 46 000 000 y 70 000 000 (entre cuarenta y seis millones, y setenta millones) de kilómetros. Tarda 88 días terrestres en dar una traslación completa. La inclinación de su plano orbital con respecto al plano de la eclíptica es de 7°.
En la imagen anexa se ilustran los efectos de la excentricidad, mostrando la órbita de Mercurio sobre una órbita circular que tiene el mismo semieje. La elevada velocidad del planeta cuando está cerca del perihelio hace que cubra esta mayor distancia en un intervalo de solo cinco días. El tamaño de las esferas, inversamente proporcional a la distancia al Sol, es usado para ilustrar la distancia variable heliocéntrica. Esta distancia variable al Sol, combinada con la rotación planetaria de Mercurio de 3:2 alrededor de su eje (rota tres veces en dos órbitas: 3 días en 2 años mercurianos), resulta en complejas variaciones de la temperatura de su superficie, pasando de los −185 °C durante las noches hasta los 430 °C durante el día.
La inclinación de su eje de rotación respecto del eje perpendicular a su plano orbital es de tan solo 0,01° (grados sexagesimales), unas 300 veces menos que la de Júpiter, que es el segundo planeta en esta estadística con 3,1° (en la Tierra la inclinación es de 23,5°). De esta forma, un observador en el ecuador de Mercurio durante el mediodía local nunca vería el Sol más que 0,01° al norte o al sur del cenit. Análogamente, en los polos el centro del Sol nunca pasa más de 0,01° por encima del horizonte.
En Mercurio existe el fenómeno de los amaneceres dobles, cuando el Sol sale aproximadamente dos tercios de su tamaño, se detiene, se esconde nuevamente casi exactamente por donde salió y luego vuelve a salir para continuar su recorrido por el cielo; esto solo ocurre en algunos puntos de la superficie, a 180º de longitud de estos lo que se observa es un doble anochecer.
Debido al mismo mecanismo, en el resto del planeta se observa que el Sol aparentemente se detiene en el cielo y realiza un movimiento de retroceso.[16]​ Esto se debe a que aproximadamente cuatro días terrestres antes del perihelio, la velocidad angular orbital de Mercurio iguala a su velocidad angular de rotación, lo que hace que el movimiento aparente del Sol cese, se invierta el movimiento durante los ocho días seguidos en los que la velocidad angular orbital es superior a la de rotación, y finalmente cuatro días después del perihelio el Sol vuelva a detenerse y recuperar su sentido de movimiento inicial.
Justo en el perihelio es cuando la velocidad angular orbital de Mercurio excede en mayor magnitud a la velocidad angular de rotación, y es entonces cuando la velocidad aparente de retroceso del Sol es la máxima.
El avance del perihelio de Mercurio fue observado por primera vez en el siglo XIX al ver la lenta precesión de la línea de los ápsides de la órbita del planeta alrededor del Sol, la cual no conseguía ser explicada completamente por las leyes de Newton ni por perturbaciones de planetas conocidos (trabajo muy notable del matemático francés Urbain Le Verrier). Se conjeturó entonces que otro planeta desconocido en una órbita más interior al Sol era el causante de estas perturbaciones (se consideraron otras teorías como un leve achatamiento de los polos solares). El éxito de la búsqueda de Neptuno a consecuencia de las perturbaciones orbitales de Urano hicieron poner mucha fe a los astrónomos para esta hipótesis. A este hipotético planeta desconocido se le denominaría planeta Vulcano. Sin embargo, a comienzos del siglo XX, la Teoría General de la Relatividad de Albert Einstein explicó completamente la precesión observada, descartando al inexistente planeta (véase órbita planetaria relativista). El efecto en el avance del perihelio mercuriano es muy pequeño: apenas de 42,98 segundos de arco por siglo, por lo que necesita más de 12 500 000 (doce millones quinientas mil) órbitas para exceder una vuelta completa.
La expresión que proporciona la Relatividad General para calcular la precesión del perihelio de un planeta, en radianes por revolución es:[17]​
G = Constante de gravitación universal
M = Masa del Sol
a = Semieje mayor de la órbita
e = Excentricidad de la órbita
c = Velocidad de la luz
Esta expresión proporciona 42,98″ de arco por siglo para Mercurio y valores mucho menores para el resto de planetas, dando 8,52 arcosegundos por siglo para Venus, 3,84 para la Tierra, 1,35 para Marte, y 10,05 para el asteroide de tipo Apolo (1566) Ícaro.[18]​[19]​
Durante muchos años se pensó que la misma cara de Mercurio miraba siempre hacia el Sol, de forma sincrónica, similar a como lo hace la Luna respecto a la Tierra. No fue sino hasta 1965 cuando observaciones por radio (véase Observación con Grandes Telescopios) descubrieron una resonancia orbital de 2:3, rotando tres veces cada dos años mercurianos; la excentricidad de la órbita de Mercurio hace esta resonancia estable en el perihelio, cuando la marea solar es más fuerte, el Sol está todavía en el cielo de Mercurio. La razón por la que los astrónomos pensaban que Mercurio giraba de manera sincrónica era que siempre que el planeta estaba en mejor posición para su observación, mostraba la misma cara. Ya que Mercurio gira en un 3:2 de resonancia orbital, un día solar (la duración entre dos tránsitos meridianos del Sol) son unos 176 días terrestres. Un día sideral es de unos 58,6 días terrestres.
Simulaciones orbitales indican que la excentricidad de la órbita de Mercurio varía caóticamente desde 0 (circular) a 0,47 a lo largo de millones de años. Esto da una idea para explicar la resonancia orbital mercuriana de 2:3, cuando lo más usual es 1:1, ya que esto es más razonable para un periodo con una excentricidad tan alta.[20]​
La magnitud aparente de Mercurio varía entre −2,0 (brillante como la estrella Sirio) y 5,5.[21]​ La observación de Mercurio es complicada por su proximidad al Sol, perdido en el resplandor de la estrella madre durante un período muy grande. Mercurio solo se puede observar por un corto período durante el crepúsculo de la mañana o de la noche. El telescopio espacial Hubble no puede observar Mercurio, ya que por procedimientos de seguridad se evita un enfoque tan cercano al Sol.
Como la Luna, Mercurio exhibe fases vistas desde la Tierra, siendo nueva en conjunción inferior y llena en conjunción superior. El planeta deja de ser invisible en ambas ocasiones por la virtud de este ascenso y ubicación acuerdo con el Sol en cada caso. La primera y última fase ocurre en máxima elongación este y oeste, respectivamente, cuando la separación de Mercurio del rango del Sol es de 18,5° en el periastro y 28,3 en el apoastro. En máxima elongación oeste, Mercurio se eleva antes que el Sol y en la este después que el Sol.
Mercurio alcanza una conjunción inferior cada 116 días de media, pero este intervalo puede cambiar de 111 a 121 días por la excentricidad de la órbita del planeta. Este periodo de movimiento retrógrado visto desde la Tierra puede variar de 8 a 15 días en cualquier lado de la conjunción inferior. Esta larga variación de tiempo es consecuencia también de la elevada excentricidad orbital.
Mercurio es más fácil de ver desde el hemisferio sur de la Tierra que desde el hemisferio norte; esto se debe a que la máxima elongación del oeste posible de Mercurio siempre ocurre cuando es otoño en el hemisferio sur, mientras que la máxima elongación del este ocurre cuando es invierno en el hemisferio norte. En ambos casos, el ángulo de Mercurio incide de manera máxima con la eclíptica, permitiendo elevarse varias horas antes que el Sol y no se pone hasta varias horas después del ocaso en los países situados en latitudes templadas del hemisferio sur, como Argentina y Nueva Zelanda. Por contraste, en las latitudes templadas del hemisferio norte, Mercurio nunca está por encima del horizonte en más o menos a medianoche. Como muchos otros planetas y estrellas brillantes, Mercurio puede ser visto durante un eclipse solar.
Además, Mercurio es más brillante visto desde la Tierra cuando se encuentra entre la fase creciente o la menguante y la llena. Aunque el planeta está más lejos en ese momento que cuando está creciente, el área iluminada visible mayor compensa esa mayor distancia. Justo al contrario que Venus, que aparece más brillante cuando está en cuarto creciente, porque está mucho más cerca de la Tierra.
El tránsito de Mercurio es el paso, observado desde la Tierra, de este planeta por delante del Sol. La alineación de estos tres astros (Sol, Mercurio y la Tierra) produce este particular efecto, solo comparable con el tránsito de Venus. El hecho de que Mercurio esté en un plano diferente en la eclíptica que nuestro planeta (7° de diferencia) hace que solo una vez cada varios años ocurra este fenómeno. Para que el tránsito se produzca, es necesario que la Tierra esté cerca de los nodos de la órbita. La Tierra atraviesa cada año la línea de los nodos de la órbita de Mercurio el 8-9 de mayo y el 10-11 de noviembre; si para esa fecha coincide una conjunción inferior habrá paso. Existe una cierta periodicidad en estos fenómenos aunque obedece a reglas complejas. Es claro que tiene que ser múltiplo del periodo sinódico. Mercurio suele transitar el disco solar un promedio de unas 13 veces al siglo en intervalos de 3, 7, 10 y 13 años.[22]​
Las primeras menciones conocidas de Mercurio, hechas por los sumerios, datan del tercer milenio a. C. Los babilonios (2000-500 a. C.) hicieron igualmente nuevas observaciones sobre el planeta, denominándolo como Nabu o Nebu, el mensajero de los dioses en su mitología.[23]​
Los observadores de la Antigua Grecia llamaron al planeta de dos maneras: Apolo cuando era visible en el cielo de la mañana y Hermes cuando lo era al anochecer. Sin embargo, los astrónomos griegos se dieron cuenta de que se referían al mismo cuerpo celeste, siendo Pitágoras el primero en proponer la idea.[24]​
Las primeras observaciones con telescopio de Mercurio datan de Galileo en el siglo XVII. Aunque él observara las fases planetarias cuando miraba a Venus, su telescopio no era lo suficientemente potente para distinguir las fases de Mercurio. En 1631, el polímata francés Pierre Gassendi realizó las primeras observaciones del tránsito de Mercurio cruzando el Sol cuando vio el tránsito de Mercurio predicho por Johannes Kepler. En 1639, Giovanni Zupi usó un telescopio para descubrir que el planeta tenía una fase orbital similar a la de Venus y la Luna. La observación demostró de manera concluyente que Mercurio orbitaba alrededor del Sol.
Un hecho extraño en la astronomía es que un planeta pase delante de otro (ocultación), visto desde la Tierra. Mercurio y Venus se ocultan cada varios siglos, y, el 28 de mayo de 1737, ocurrió el único e histórico registrado. El astrónomo que lo observó fue John Bevis en el Real Observatorio de Greenwich.[25]​ La próxima ocultación ocurrirá en el año 2133.[26]​
En 1800, el astrónomo alemán Johann Schröter pudo hacer algunas observaciones de la superficie, pero erróneamente estimó que el planeta tenía un período de rotación similar a la terrestre, de unas 24 horas. En la década de 1880, Giovanni Schiaparelli realizó un mapa de Mercurio más correcto, y sugirió que su rotación era de 88 días, igual que su período de traslación (Rotación síncrona).[27]​
La teoría por la cual la rotación de Mercurio era sincrónica se hizo extensamente establecida, y fue un giro de 180° cuando los astrónomos mediante observaciones de radio en los años 1960 cuestionaron la teoría. Si la misma cara de Mercurio estuviera dirigida siempre hacia el Sol, la parte en sombra estaría extremadamente fría, pero las mediciones de radio revelaron que estaba mucho más caliente de lo esperado. En 1965, se constató que definitivamente el periodo de rotación era de 59 días. El astrónomo italiano Giuseppe Colombo notó que este valor era sobre dos terceras partes del período orbital de Mercurio, y propuso una forma diferente de la fuerza de marea que hizo que los períodos orbitales y rotatorios del planeta se quedasen en 3:2 más bien que en 1:1 (resonancia orbital).[28]​ Más tarde, la Mariner 10 lo confirmó.[29]​
Las observaciones por grandes telescopios en tierra no arrojaron mucha luz sobre este mundo difícil de ver, y no fue hasta la llegada de sondas espaciales que visitaron Mercurio cuando se descubrieron y confirmaron grandes e importantes propiedades del planeta. No obstante, recientes avances tecnológicos han llevado a observaciones mejoradas: en el año 2000, el telescopio de alta resolución del Observatorio Monte Wilson de 1500 mm proporcionó las primeras imágenes que resolvieron algunos rasgos superficiales sobre las regiones de Mercurio que no fueron fotografiadas durante las misiones del Mariner.[30]​ Imágenes recientes apuntan al descubrimiento de una cuenca de impacto de doble anillo más largo que la Cuenca de Caloris, en el hemisferio no fotografiado por la Mariner. Es informalmente conocido como Cuenca de Shinakas.
Llegar hasta Mercurio desde la Tierra supone un significativo reto tecnológico, ya que la órbita del planeta está mucho más cerca que la terrestre del Sol. Una nave espacial con destino a Mercurio lanzada desde nuestro planeta deberá de recorrer unos 91 000 000 (noventa y un millones) de kilómetros por los puntos de potencial gravitatorio del Sol. Comenzando desde la órbita terrestre a unos 30 km/s, el cambio de velocidad que la nave debe realizar para entrar en una órbita de transferencia, conocida como órbita de transferencia de Hohmann (en la que se usan dos impulsos del motor cohete) para pasar cerca de Mercurio es muy grande comparado con otras misiones planetarias.
Además, para conseguir entrar en una órbita estable el vehículo espacial debe confiar plenamente en sus motores de propulsión, puesto que el aerofrenado está descartado por la falta de atmósfera significativa en Mercurio. Un viaje a este planeta en realidad es más costoso en lo que a combustible se refiere por este hecho que hacia cualquier otro planeta del sistema solar.[cita requerida]
La sonda Mariner 10 (1974-1975), o Mariner X, fue la primera nave en estudiar en profundidad el planeta Mercurio. Había visitado también Venus, utilizando la asistencia de trayectoria gravitacional de Venus para acelerar hacia el planeta.
Realizó tres sobrevuelos a Mercurio; el primero, a una distancia de 703 km del planeta; el segundo, a 48.069 km; y, el tercero, a 327 km. Mariner tomó en total diez mil imágenes de gran parte de la superficie del planeta. La misión finalizó el 24 de marzo de 1975, cuando se quedó sin combustible y no podía mantener control de orientación.[31]​
MErcury Surface, Space ENvironment, GEochemistry and Ranging (Superficie de Mercurio, Entorno Espacial, Geoquímica y Extensión) fue una sonda lanzada en agosto de 2004 para ponerse en órbita alrededor de Mercurio en marzo de 2011. Se esperaba que esta nave aumentara considerablemente el conocimiento científico sobre este planeta. Para ello, la nave había de orbitar Mercurio y hacer tres sobrevuelos —los días 14 de enero de 2008, 6 de octubre de 2008, y 29 de septiembre de 2009—. La misión estaba previsto que durase un año. El 18 de marzo de 2011, se produjo con éxito la inserción orbital de la sonda.[32]​ Finalmente el fin de esta exitosa misión se produjo el 30 de abril de 2015, cuando la sonda se precipitó sobre la superficie del planeta produciéndose un impacto controlado.
Es una misión conjunta de la Agencia Espacial Europea (ESA) y de la Agencia Japonesa de Exploración Espacial (JAXA), que consiste en dos módulos orbitantes u orbitadores que realizarán una completa exploración de Mercurio. El primero de los orbitadores será el encargado de fotografiar y analizar el planeta y el segundo investigará la magnetosfera. Su lanzamiento se realizó con éxito el día 20 de octubre de 2018,[33]​ su llegada al planeta está prevista el 5 de diciembre de 2025, después de un sobrevuelo de la Tierra, dos de Venus y seis del propio Mercurio.[34]​ El final de la misión está programado para un año más tarde, con una posible extensión de un año más.[35]​

En taxonomía, se denomina especie (del latín species) a la unidad básica de clasificación biológica. Una especie es un conjunto de organismos o poblaciones naturales capaces de entrecruzarse y producir descendencia fértil, aunque —en principio—  no con miembros de poblaciones pertenecientes a otras especies. En muchos casos los individuos que se separan de la población original y se aíslan del resto pueden alcanzar una diferenciación suficiente como para convertirse en una nueva especie, por lo tanto, el aislamiento reproductivo respecto de otras poblaciones es crucial. En definitiva, una especie es un grupo de organismos reproductivamente homogéneo, aunque muy cambiante a lo largo del tiempo y del espacio.
Mientras que en muchos casos esta definición es adecuada, es a menudo difícil demostrar si dos poblaciones pueden cruzarse y dar descendientes fértiles (por ejemplo, muchos organismos no pueden mantenerse en laboratorio suficiente tiempo). Además, es imposible aplicarla a organismos que no se reproducen sexualmente (como las bacterias) ni a organismos extintos conocidos solo por sus fósiles. En la actualidad suelen aplicarse técnicas moleculares, como las basadas en la semejanza del ADN.
Los nombres comunes de las plantas y los animales corresponden algunas veces con su respectiva especie biológica (por ejemplo, «león», «morsa» y «árbol del alcanfor»), pero con mucha frecuencia no es así: por ejemplo, la palabra pato se refiere a una veintena de especies de diversos géneros, como el pato doméstico. Por ello, para la denominación de las especies se utiliza la nomenclatura binomial, mediante la cual cada especie queda inequívocamente definida con dos palabras —por ejemplo Homo sapiens, la especie humana—. En esta nomenclatura, el primer término corresponde al género: el rango taxonómico superior en el que se pueden agrupar las especies.
La determinación de los límites de una especie es puramente subjetiva y, por tanto, expuesta a la interpretación personal. Algunos conceptos usuales son antiquísimos, muy anteriores al establecimiento científico de esta categoría taxonómica. Por el contrario, existen otros de límites muy vagos, en los cuales los sistemáticos están en completo desacuerdo. Si las especies fueran inmutables, se podría definir fácilmente cada una de ellas diciendo que es el conjunto de individuos (que fueron, que son y que serán, de no extinguirse) de caracteres cualitativamente idénticos. Una entidad así determinada no es realmente una especie, sino lo que usualmente se llama una línea pura o un clon.
La delimitación de especies afecta directamente a aspectos tan importantes y actuales como la Biología de la conservación, o a campos aplicados como la modelización de distribuciones, de las que se puede obtener información muy valiosa.
El número de especies presentes en algún territorio es una forma de estimar la riqueza, complejidad y aportación al patrimonio natural de sus habitantes.[1]​
Subdividiendo, se encuentran las siguientes categorías:
El término especie se refiere a tres conceptos distintos, aunque relacionados. El rango especie, que es el nivel más básico de la taxonomía de Linneo; los taxones especie, que son un grupo de organismos descritos y asignados a la categoría especie, y las especies biológicas, que son entes capaces de evolucionar. Respecto a esto, debemos decir que la idea de evolución ya se encontraba en la antigüedad clásica. Anaximandro afirmó que las primeras criaturas habían surgido del agua para pasar a tierra; mientras que Empédocles aseguró que eran partes separadas que, en un momento dado, llegaron a juntarse, lo que nos recuerda la teoría de la simbiogénesis.[2]​[3]​ Cicerón escogió el término latino species, 'aspecto característico, forma', para traducir el término griego ἰδέα (idéa) 'aspecto, apariencia, forma',  relacionado con εῖδος, (eîdos) ‘vista, visión, aspecto’, término platónico cargado de connotaciones dialécticas y lógicas.
Para el biólogo Ernst Mayr, Platón fue "el gran antihéroe del evolucionismo" por causa de su creencia en el Mundo de las ideas.[4]​ Aristóteles, en cambio, se mantuvo en una posición ambigua: aportó lo que él consideraba "pruebas" de una generación espontánea, pero habló de una "causa final" de toda especie,[5]​ su entelequia, y rechazó explícitamente la idea de Empédocles que decía que las criaturas vivientes se habrían originado por casualidad.[6]​ Zenón de Citio, según Cicerón, siguió en esta línea.
De los datos que nos aporta Lucrecio, extraemos que Epicuro se anticipó a la ley de la selección natural.[7]​ El degradacionista Agustín de Hipona dice, en cambio, que el Génesis debe interpretarse y que no hemos de suponer que Dios creara las especies que vemos ahora con sus imperfecciones. Este hecho y el proceso contra Galileo fue clave, siglos más tarde, para la rápida aceptación por la Iglesia católica de la teoría de la evolución. Durante la Edad Media, en una época de indeterminación fomentada por la inestabilidad política, se confundieron frecuentemente los términos "especie" y "género". Esto podría justificarse con base en el texto de la Vulgata:
Se dejó abierta la posibilidad de que hubiera especies y géneros no creados por Dios o no descubiertos por el hombre europeo. El nominalismo tuvo sus raíces en el siglo XIV con Guillermo de Ockham. Esta doctrina señalaba que no existía ninguna entidad entre el término y los individuos a los que este se refería, es decir, solo existían los individuos. Según esta doctrina, las especies son fruto de nuestra razón y el concepto especie se utiliza solo con el fin de agruparlos por su parecido y darles un nombre. En pocas palabras, el nominalismo no reconoce a las especies como entidades reales.
Linneo y John Ray, por su parte, afianzaron la idea del carácter discreto y de la posesión de atributos objetivos de las especies que permitían su delimitación; es decir, la realidad de las especies. A partir de la publicación de El origen de las especies de Charles Darwin en 1859, se comenzó a considerar a la especie como un agregado de poblaciones morfológicamente variables y con la capacidad de evolucionar. El concepto aristotélico-linneano fue gradualmente reemplazado por una concepción evolutiva basada en la selección natural y en el aislamiento reproductivo.
John Ray definió a la especie como un grupo de individuos semejantes, con antepasados comunes. Igualmente, expresó que "una especie nunca nace de la semilla de otra especie", es decir, los conejos no nacen de monos, ni las arvejas dan rosas.
A mediados del siglo XX se plantearon dos posturas respecto a las especies: el realismo evolutivo y el nominalismo. El último sostuvo que en la naturaleza solo existen los organismos individuales y, según los taxónomos evolutivos, las especies son entidades reales de la naturaleza y constituyen unidades de evolución. A partir de la década de 1980, se afianzó la postura realista con respecto a las especies biológicas, conjuntamente con el enfoque filogenético de la clasificación.[8]​
De acuerdo con Häuser (1987), los atributos generales del concepto especie deben ser universalidad, aplicabilidad práctica y criterio decisivo.[9]​ La mayoría de los biólogos que se ocupan de la sistemática de plantas y animales usan el CBE en conjunto con la descripción de la morfoespecie (King, 1993).
Existen multitud de definiciones de especie:
Los nombres de las especies son binominales, es decir, formados por dos palabras, que deben escribirse en un tipo de letra distinto del texto general (usualmente en cursiva; de las dos palabras citadas, la primera corresponde al nombre del género al que pertenece y se escribe siempre con la inicial en mayúscula; la segunda palabra es el epíteto específico o nombre específico y debe escribirse enteramente en minúscula y debe concordar gramaticalmente con el nombre genérico). Así, en Mantis religiosa, Mantis es el nombre genérico, religiosa el nombre específico y el binomio Mantis religiosa designa esta especie de insecto.
En el nombre científico asignado a las especies, el nombre específico nunca debe ir aislado del genérico ya que carece de identidad propia y puede coincidir en especies diferentes. Si se ha citado previamente el nombre completo y no cabe ninguna duda de a qué género se refiere, el nombre del género puede abreviarse a su inicial (M. religiosa).
En los libros y artículos académicos, a veces, no se identifican intencionalmente las especies plenamente y se recurre a utilizar la abreviatura "sp." en singular o "spp." en plural, en lugar del epíteto específico, por ejemplo: Canis sp. La abreviatura plural "spp." se utiliza generalmente para referirse a todas las especies individuales dentro de un género. Para una especie concreta cuyo epíteto específico es desconocido o carece de importancia se utiliza "sp.".
Esto ocurre comúnmente en los siguientes tipos de situaciones:
En los libros y artículos, los nombres de géneros y especies generalmente se imprimen en letra cursiva. Las abreviaciones como «sp.», «spp.», «subsp.», etc., no deben estar en cursivas.
El número real de especies es muy impreciso y varía notablemente según las fuentes. Algunas estimaciones abarcaban un total de entre 1,5 y 2 millones de especies, aunque un estudio de 2011 aumenta esa cifra hasta una horquilla entre los 7,5 y los 10 millones.[19]​ Las especies colocadas en la siguiente lista son especies conocidas y existentes actualmente. No se han incluido fósiles (pues se podría añadir gran número, principalmente de artrópodos, invertebrados menores, peces y reptiles).
Cada año se describen alrededor de 10 000 nuevas especies, de las cuales, solo una decena son vertebrados, y, estadísticamente, solo 0,4 anuales son mamíferos.
Tamaño relativo de los grandes grupos de animales.
(Fuente de datos animales: Brusca, R. C. & Brusca, G. J. 1990. Invertebrates. Sinauer Associates, Sunderland.)
Reeder T. et al. 2002. Amer. Mus. Novit.

En biología, los músculos son estructuras o tejidos existentes en el ser humano y en la mayoría de los animales que tienen la capacidad de generar movimiento al contraerse y relajarse.[1]​
El tejido que forma el músculo se llama tejido muscular y está formado por células especializadas llamadas miocitos que tienen la propiedad de aumentar o disminuir su longitud cuando son estimuladas por impulsos eléctricos procedentes del sistema nervioso.
Existen tres tipos de tejido muscular: 
En el cuerpo humano y en todos los vertebrados, los músculos estriados están unidos al esqueleto por medio de los tendones y son los responsables de la ejecución de los movimientos corporales voluntarios. El músculo cardíaco y el músculo liso se contraen de forma automática por los impulsos que reciben a través del sistema nervioso autónomo.
La unidad funcional y estructural del músculo esquelético es la fibra muscular o miocito, varias fibras musculares se agrupan para formar un fascículo, varios fascículos se reúnen y forma el músculo completo que está envuelto por una membrana de tejido conjuntivo llamada fascia. El cuerpo humano contiene aproximadamente 650 músculos estriados.
La palabra músculo proviene del diminutivo latino musculus, formado por mus (ratón) y la terminación diminutiva -culus, porque en el momento de la contracción, los romanos lo comparaban con un pequeño ratón, debido a la forma que adquiere durante este proceso.
El tejido muscular está formado por células llamadas miocitos y tiene cuatro propiedades principales que lo diferencian del resto de los tejidos:[2]​[3]​
Si se compara el tejido muscular con otros tejidos como el tejido óseo que forma los huesos, puede comprenderse fácilmente la importancia de estas cuatro propiedades. El tejido óseo no es excitable eléctricamente, tampoco tiene capacidad de contraerse o variar de forma. No es extensible, si sufre un alargamiento se rompe provocando una fractura.
Es el encargado del movimiento de los esqueletos axial y apendicular, y del mantenimiento de la postura o posición corporal. Gracias al músculo estriado podemos realizar los movimientos voluntarios, mover el tronco y las extremidades, andar, saltar, correr, levantar objetos, masticar y mover los ojos en todas direcciones. Cada músculo estriado o esquelético se fija en los huesos por medio de prolongaciones fibrosas llamadas tendones y está rodeado por una membrana que recibe el nombre de aponeurosis.[4]​
La unidad fundamental que constituye el músculo esquelético es la fibra muscular. Cada una de ellas es en realidad una célula de forma cilíndrica muy larga que posee numerosos núcleos situados en su periferia. Un grupo de fibras se agrupan para formar un fascículo,  varios fascículos se unen y originan el músculo completo.[4]​[5]​
El tejido muscular liso a diferencia del esquelético no participa en los movimientos voluntarios. Se encuentra en la pared de las estructuras internas huecas, incluyendo la pared del tubo digestivo, vesícula biliar, vasos sanguíneos, vías aéreas, bronquios, uréteres, vejiga urinaria y útero.  También existe músculo liso en la piel asociado a los folículos pilosos y en el ojo donde tiene la función de contraer y dilatar la pupila y permitir el enfoque variando la forma del cristalino.  Recibe su nombre porque si se observa al microscopio una muestra de tejido, no son visibles estriaciones, razón por la que se le llama liso. Los músculos lisos del organismo realizan funciones de gran importancia y se contraen o relajan de manera automática en respuesta a estímulos nerviosos generados por el sistema nervioso autónomo.[2]​
Las fibras musculares lisas son más cortas que las esqueléticas y poseen un único núcleo, cuentan con filamentos internos que son de dos tipos: gruesos y finos.  Estos filamentos no tienen una distribución compacta por lo que no existen estriaciones visibles. La contracción de la fibra lisa se basa en los mismos principios que en el músculo esquelético, pero tiene algunas propiedades particulares, es de inicio más lento pero de mayor duración que en el músculo esquelético y además las fibras puede estirarse o acortarse en un grado mucho mayor sin perder su capacidad contráctil.[2]​
La musculatura lisa del útero es la que hace posible el proceso del parto, durante el mismo el útero se contrae periódicamente con intensidad creciente que alcanza su máximo durante el periodo expulsivo. [6]​
Es de naturaleza estriada modificada y de control involuntario. Está presente únicamente en el corazón y genera los movimientos por los que este órgano impulsa la sangre a través del sistema circulatorio.
El 75% del volumen total del corazón es músculo. El tejido muscular cardíaco tiene algunas características especiales, las células que lo componen están ramificadas y disponen de unas estructuras llamadas discos intercalares que unen los extremos de dos miocitos colindantes, de tal forma que el órgano se contrae de forma sincronizada. [7]​
La regulación de la fuerza y velocidad de contracción es involuntaria y se realiza a través del sistema nervioso autónomo. El sistema nervioso simpático tiene una acción positiva aumentando la frecuencia de las contracciones, mientras que el estímulo del sistema nervioso parasimpático tiene la acción contraria.
La contractibilidad es la propiedad que tienen las fibras musculares para acortarse y hacerse más gruesas. Ello es posible porque cada célula contiene numerosos filamentos que están formados de dos proteínas diferentes llamadas actina y miosina, ambos tipos tienen aspecto diferente, los filamentos de actina son delgados y de color claro, mientras que los de miosina son de color oscuro y gruesos. Se alternan entre sí, imbricados como cuando se entrelazan los dedos de las manos.[4]​ 
Según el modelo del filamento deslizante, en situación de reposo la fibra muscular presenta un grado moderado de solapamiento entre los filamentos de actina y miosina, en estado de contracción el solapamiento aumenta, mientras que si se produce una elongación muscular el solapamiento disminuye y puede llegar a ser nulo.[8]​
Los músculos esqueléticos están unidos por sus extremos a los huesos mediante tendones. Por ello existe una resistencia que el músculo debe vencer para poder acortarse.  Cuando la resistencia es superior a la tensión que se establece en el músculo activado, este no se puede acortar y no se produce movimiento, mientras que cuando la resistencia es inferior a la tensión generada se produce un acortamiento que será más rápido cuanto menor sea la carga. El  término contracción se utiliza aquí para designar el desarrollo de tensión en el músculo, pero no implica necesariamente que este se acorte, pues ello depende de la resistencia externa que exista.
Con base en lo expuesto pueden existir varios tipos de contracciones, dependiendo de si generan o no movimiento:[9]​
Pueden definirse otros tipos de contracciones que en realidad no son más que la combinación de las tres básicas anteriormente reseñadas:
La fibra muscular estriada es una célula alargada de forma cilíndrica. Mide 50 micras de diámetro y puede alcanzar una longitud de varios centímetros. Es el resultado de la fusión de varias células, por lo que presenta numerosos núcleos situados en su periferia (célula multinucleada). Está envuelta por una membrana que se llama sarcolema, mientras que la región interior (citoplasma) se denomina sarcoplasma.[5]​ 
El sarcoplasma contiene numerosas estructuras longitudinales (miofibrillas) que están dispuestas de forma compacta y se forman por la alternancia de dos tipos de filamentos: Los filamentos gruesos compuestos por moléculas proteicas de miosina y los filamentos delgados que están compuestos por moléculas proteicas de actina. Ambos tipos de filamentos se alternan entre sí formando una estructura perfectamente ordenada que es la responsable de la contracción muscular.  La miofibrilla está cortada regularmente por unas estrías de color oscuro que se llaman estrías Z. La región existente entre dos estrías Z sucesivas recibe el nombre de  sarcómero. El sarcómero es la unidad básica de contracción muscular. Cada fibra muscular contiene gran cantidad de sarcómeros dispuestos en un conjunto ordenado con perfecta regularidad. [5]​

Existen dos tipos de fibras musculares esqueléticas que se diferencian por su actividad funcional y algunos aspectos de su estructura: fibras musculares tipo I, denominadas también rojas o de contracción lenta y fibras musculares tipo II, llamadas también blancas o de contracción rápida. Dentro de un músculo suelen existir fibras de ambos tipos, aunque según el tipo de movimiento habitualmente realizado predominan los de uno de ellos. Las fibras rojas predominan en los músculos posturales (músculos del tronco) cuya actividad es continua y las blancas en los músculos relacionados con el movimiento (músculos de las extremidades) que necesitan contraerse con mayor rapidez.[10]​
Se entiende por sistemas energéticos , a las vías metabólicas que el organismo (interno) emplea para obtener energía. Esta energía se utiliza para realizar un trabajo, por ejemplo la contracción muscular (Externa o interna).
Las fibras individuales del músculo estriado están cubiertas por una capa de tejido conjuntivo que se llama endomisio. Varias fibras se agrupan y forma una estructura mayor denominada fascículo muscular que está cubierto por el perimisio. Por último varios fascículos unidos forman el músculo completo, el cual se encuentra cubierto por el epimisio. La mayor parte de los músculos se insertan en  el hueso mediante tendones que están formados por tejido fibroso y sólido con cierto grado de elasticidad. Lo habitual es que existan 2 tendones uno en cada extremo del músculo.[5]​
La forma de los músculos es muy variable dependiendo de su función y localización, la mayor parte son alargados o fusiformes como el bíceps braquial, otros son planos como el músculo recto abdominal, algunos tienen forma de abanico como el pectoral mayor. Determinados músculos tienen formas especiales con una abertura en el centro para adaptarse a una cavidad, entre ellos el orbicular de los labios y el orbicular de los ojos.
Son diferentes dependiendo del tipo de músculo: estriado, cardíaco o liso.[2]​
En biología, la fuerza se define como la capacidad que tienen los músculos para contraerse venciendo una resistencia.  Es necesaria para la mayor parte de las actividades cotidianas, pues todos los movimientos están originados por una fuerza, tanto los necesarios para realizar desplazamientos como aquellos destinados a mover objetos más o menos pesados. Dentro del concepto de fuerza se incluyen tres aspectos diferentes: [11]​
La fuerza que puede alcanzar un individuo determinado se relaciona con diversos factores, uno de ellos es la sección transversal del músculo. Un músculo puede generar de 3 a 4 kg de fuerza por cm² de sección  transversal. Por lo tanto los músculos de mayor sección son los que desarrollan mayor fuerza, aunque no siempre el incremento en el tamaño del músculo se acompaña con aumento de la fuerza que puede desarrollar.
El músculo esquelético está formado por un 75% de agua y un 20 % de proteínas. El 5% restante corresponde a otras sustancias como grasas, glucógeno, sodio, potasio, calcio y fósforo.[12]​
Las enfermedades y trastornos de la musculatura son variadas y de diversas etiologías.

La sordera es la dificultad o la imposibilidad de usar el sentido del oído debido a una pérdida de la capacidad auditiva parcial (hipoacusia) o total (cofosis), y unilateral o bilateral. Así pues, una persona sorda será incapaz o tendrá problemas para oír. Esta puede ser un rasgo hereditario o puede ser consecuencia de una enfermedad, traumatismo, exposición a largo plazo al ruido, o medicamentos agresivos para el nervio auditivo.[cita requerida]
La pérdida de audición se produce cuando se disminuye la sensibilidad a los sonidos que normalmente se sienten.[1]​ Los términos de deterioro o con problemas de audición son generalmente reservados a las personas que tienen insensibilidad relativa a los sonidos en las frecuencias del habla. La gravedad de la pérdida auditiva se clasifica de acuerdo con la necesidad de ampliar el aumento del volumen por encima del nivel normal antes de que el oyente lo pueda detectar.
La sordera se define como un grado de deterioro que una persona es incapaz de comprender el habla, incluso en la presencia de amplificación.[1]​ En sordera profunda, incluso los sonidos más fuertes producidos por un audiómetro (un instrumento utilizado para medir la audición mediante la producción de tonos puros a través de una gama de frecuencias) no pueden ser detectados. En sordera total, no se percibe ningún sonido en absoluto, independientemente de la amplificación o método de producción escuchado.
En el caso humano, al someter el tímpano a 150 decibelios éste se rompe, quedando la persona sorda.[2]​
Otro aspecto de la audición implica la claridad percibida de un sonido en lugar de su amplitud. En los seres humanos, este aspecto se mide generalmente por medio de pruebas de percepción de habla. Estas pruebas miden la capacidad para entender el habla, no sólo la detección del sonido. Hay tipo de trastornos muy raros que afectan la audición y la comprensión del habla.[3]​
La sordera también se da por desgaste de los oídos; esto explica por qué los adultos no pueden oír algunas frecuencias que personas de menor edad sí pueden.
Para comprobar el grado de sordera de una persona, se le hace una prueba de audiometría, de manera que una persona con sordera puede tener problemas en la percepción correcta de la intensidad (decibelios) o de la frecuencia (hercios) de sonidos relacionados con el lenguaje oral, y es frecuente que se den resultados diferentes para cada oído. La pérdida de la capacidad auditiva generalmente se describe como leve, benigna, moderada, severa o profunda, dependiendo de dicha prueba. Generalmente, cuando una persona cuya pérdida de la capacidad auditiva supere a los 90 dB, se considera entonces persona sorda.[4]​
Existen diversos criterios a la hora de clasificar las diferentes tipologías de pérdida auditiva o sordera.[cita requerida]
Causadas por enfermedades u obstrucciones en el oído exterior o medio (las vías de conducción a través de las cuales el sonido llega al oído interior), las pérdidas auditivas conductivas normalmente afectan a todas las frecuencias del oído de manera uniforme.
Son en los casos en los que las células ciliadas del oído interno, o los nervios que lo abastecen, se encuentran dañados. Estas pérdidas auditivas pueden abarcar desde pérdidas leves a profundas. A menudo afectan a la habilidad de la persona para escuchar ciertas frecuencias más que otras, de manera que escucha de forma distorsionada el sonido, aunque utilice un audífono amplificador. No obstante, en la actualidad, las grandes prestaciones tecnológicas de los audífonos digitales son capaces de amplificar solamente las frecuencias deficientes, distorsionando inversamente la onda para que la persona sorda perciba el sonido de la forma más parecida posible como sucedería con una persona oyente.
Se refiere a aquellos casos en los que existen aspectos de pérdidas conductivas y sensoriales, de manera que existen problemas tanto en el oído externo o medio y el interno. Este tipo de pérdida también puede deberse a daños en el núcleo del sistema nervioso central, ya sea en las vías al cerebro o en el mismo cerebro. Es importante tener cuidado con todo tipo de golpes fuertes en la zona auditiva, ya que son los principales causantes de este tipo de sordera.
Autores como Valmaseda y Díaz-Estébanez (1999) hablan de esta cuarta tipología, que hace referencia solo y exclusivamente a lesiones en los centros auditivos del cerebro.[cita requerida]
Se mide en decibelios y se clasifica en pérdida leve, moderada, severa o profunda (cofosis):
Para comprobar el grado de sordera de una persona, se le hace una prueba de audiometría, de forma que una persona con sordera puede tener problemas en la percepción correcta de la intensidad (decibelios) o de la frecuencia (hercio) de sonidos relacionados con el lenguaje oral, y es frecuente que se den resultados diferentes para cada oído. La pérdida de la capacidad auditiva generalmente se describe como leve, benigna, moderada, severa o profunda, dependiendo de la mencionada prueba. Generalmente, cuando un niño con una pérdida de la capacidad auditiva supere a los 90 dB, se considera entonces que necesita un método educativo específico para personas sordas.[cita requerida]
Umbral por encima de 90 dB. Puede ser debido a malformaciones internas del canal auditivo o a la pérdida total de los restos auditivos por motivos genéticos. Entre todas las personas sordas, el porcentaje de personas que padecen cofosis es muy pequeño, casi insignificante, ya que se trata de una malformación (ausencia de cóclea, por ejemplo).
La etiología de la discapacidad auditiva puede ser por causas exógenas como la rubeola materna durante el embarazo, incompatibilidad del factor Rh... y que suelen provocar otros problemas asociados (dificultades visuales, motoras, cognitivas). O bien puede ser una sordera hereditaria, la cual, al ser recesiva, no suele conllevar trastornos asociados.
El momento en el que aparece la discapacidad auditiva es determinante para el desarrollo del lenguaje del individuo, por lo que se pueden distinguir 3 grupos:[cita requerida]
Pueden estar involucrados genes recesivos o dominantes. Los más comunes son el síndrome de Stickler y el síndrome de Waardenburg entre los de genes dominantes y el síndrome de Pendred, el síndrome de acueducto vestibular dilatado y el síndrome de Usher entre los de genes recesivos.[cita requerida]
Algunos medicamentos pueden causar daños irreversibles al oído; entre ellos se encuentra el grupo de los aminoglucósidos. Otros medicamentos tienen efectos negativos, pero son reversibles.
Se estima que la mitad de los casos de deterioro de la audición y la sordera son evitables.[1]​ Hay toda una serie de estrategias preventivas que son eficaces incluyendo la inmunización contra la rubeola para reducir las infecciones congénitas, la inmunización contra H. influenza y S. pneumoniae para reducir los casos de otitis mediana y evitar o proteger contra la exposición excesiva al ruido.[1]​ La educación sobre los peligros de la exposición al ruido peligroso aumenta el uso de protectores auditivos.[9]​
La pérdida auditiva se describe como:
Sordera verdadera o pérdida del umbral de intensidad de audición.

En Argentina las obras sociales y las empresas de medicina prepaga cubren el tratamiento de la hipoacusia, los audífonos y prótesis auditivas y la rehabilitación. También cubren el implante coclear, que es un dispositivo electrónico que se pone detrás del oído y posibilita a la persona sorda la recuperación parcial de la audición.
En Argentina las empresas de telefonía domiciliaria y móvil deben proveer equipos para personas con discapacidad auditiva y dar un servicio adecuado para permitirles la comunicación.
Las personas con discapacidad auditiva tienen prioridad en la instalación de este servicio de telefonía cuando el teléfono sea su única forma de comunicarse y no deben pagar una mayor tarifa por usar estos teléfonos, las tarifas deben ser iguales a las de los teléfonos domiciliarios convencionales.
Las empresas de telefonía móvil están obligadas a vender equipos compatibles para personas hipoacúsicas y los equipos especiales no pueden ser superiores a los de los equipos habituales.
En muchos casos puede ampliar considerablemente en la forma en que la persona sorda se relaciona con su entorno humano, al encontrarse con una nueva forma de comunicación aparte la sonora, es decir, con la lengua visual. Sin embargo, el modo en que se entienden las consecuencias de esa nueva capacidad puede variar considerablemente, de manera que la persona sorda adquiere la otra perspectiva fundamental acerca la existencia de la lengua visual y su otra identidad.
Estudios recientes, a partir de los trabajos de William C. Stokoe en 1960, fundamentalmente, proponen abordar la sordera desde un punto de vista antropológico. Un colectivo de personas sordas que se comunican entre sí por medio de una lengua de señas puede ser considerado una comunidad lingüística minoritaria, con una cultura propia. La literatura especializada hace muchas veces la distinción entre Sordera, con una mayúscula inicial, para referirse a la antropológica, y sordera, para la definida clínicamente.
Dependiendo de los casos, una persona sorda normalmente puede desarrollar una idiosincrasia con las personas que se comunican por el canal visual, es decir, con la lengua de señas (LS), considerándose como una colectividad cultural y social propia diferenciada, normalmente con la definición de Comunidad Sorda. El vínculo social entre los sordos signantes suele ser muy fuerte debido, sobre todo, al aislamiento social con respecto a los oyentes, provocado por el escaso conocimiento de su problemática común, o estilo de vida, así como la escasa relación social por motivos de entendimiento lingüístico o también por ideas preconcebidas que las personas tienen acerca de los sordos las cuales pueden ir cambiando mediante la completa interacción dentro de su cultura.
De hecho, en esta colectividad se definen a sí mismos como personas sordas signantes, y suelen clasificar su entorno social entre oyentes a las personas que no tienen sordera (entre los que puede haber algún oyente signante, si conoce una LS), y al resto de las personas sordas que, dependiendo del país, pueden formar parte también de la Comunidad Sorda. Entre las personas sordas, además, los sordos signantes se distinguen de las personas sordas oralistas, es decir, quienes no utilicen habitualmente una LS o usan una comunicación bimodal (léxico de una LS con estructura gramatical de una lengua oral). Por último, están los sordos implantados, es decir, quienes llevan un implante coclear en vez de un audífono, que pueden ser signantes u oralistas.
En cambio, las personas sordas oralistas, es decir, aquellas personas sordas que han recibido una intensa reeducación del lenguaje oral en su infancia y que no usan una lengua de señas (a menudo como consecuencia de una prohibición expresa de los educadores), suelen adoptar una actitud de invisibilidad social respecto a su condición de persona sorda, a veces incluso no reconociéndose como tales (recurriendo a otras definiciones como discapacitado auditivo, hipoacúsico, medio oyente, etc.). Asimismo, este grupo suele asociar como personas sordas únicamente aquellos que son signantes, o bien diferenciándose de ellos, definiéndolos erróneamente como personas sordomudas, especialmente a aquellos no hablan un lenguaje oral correctamente en el aspecto gramatical.
Esta última definición, la de sordomudez, es considerada peyorativa por los sordos signantes y sus aliados, pues consideran que el alto analfabetismo de la lengua oral entre las personas sordas no tiene ninguna relación con la mudez, sino que se debe a un fracaso del método oralista y método pedagógico en el sistema educativo. De hecho, llamar "sordomudo" a una persona sorda por no hablar en lengua oral correctamente, equivaldría llamar "manco" a una persona por no escribir con la grafía correcta, o "ciego y manco" por no saber leer y escribir. Por último, en el sentido estricto, la "sordomudez" solo sería aplicable a aquellos que padezcan sordera y, además, son incapaces de generar sonidos humanos por la ausencia o el daño en las cuerdas vocales, siendo aspectos independientes entre sí.
Debido a unos ciertos factores que han vivido de manera común entre todo el colectivo sordo, con el paso del tiempo se han ido formando varias federaciones y asociaciones de personas sordas donde pueden encontrar todo tipos de ayudas, facilidades, actividades de ocio y compañía. A nivel mundial, hay dos que son las más importantes: la World Federation of the Deaf (WFD) o también llamada Federación Mundial de Sordos (FMS) y la Unión Europea de Sordos (EUD).
En España encontramos la Confederación Estatal de Personas Sordas (CNSE),[13]​ que está formada por todas las federaciones y asociaciones del país. Cada comunidad autónoma tiene una federación y hay más de 115 asociaciones provinciales y locales de personas sordas.
Ha habido una gran controversia dentro de la comunidad sorda culturalmente con los implantes cocleares. En la mayor parte, hay poca objeción a los que han perdido la audición más tarde en la vida, o adultos culturalmente sordos (voluntariamente) que han elegido de estar proveídos de un implante coclear.
Muchas personas en la comunidad sorda se oponen firmemente al hecho que un niño sordo esté equipado con un implante coclear (a menudo con el asesoramiento de un médico especialista); los nuevos padres pueden no tener suficiente información sobre la crianza de niños sordos y realizan programas de exposición oral que hace hincapié en la capacidad de hablar y escuchar sobre otras formas de comunicación, como la lengua de signos o la comunicación total. Otras preocupaciones incluyen la pérdida de la cultura y las limitaciones de los sordos sobre la restauración de la audición.
La mayoría de los padres y los médicos dicen a los niños que no tienen que jugar deportes o participar en actividades que pueden causar lesiones a la cabeza, por ejemplo el fútbol, el hockey, o el baloncesto. Un niño con una pérdida auditiva puede preferir mantenerse alejado de lugares con mucho ruido, como por ejemplo conciertos de rock, partidos de fútbol, aeropuertos, etc., puesto que esto puede causar un desbordamiento de ruido, un tipo de dolor de cabeza que se produce en muchos niños y adultos cuando están cerca de los ruidos fuertes.
La lengua de señas se transmite a través de la comunicación manual y del cuerpo en vez de la transmisión acústica. Esto puede implicar combinar simultáneamente formas de la mano, la orientación y el movimiento de las manos, brazos o el cuerpo y expresiones faciales para expresar de manera fluida los pensamientos de la persona.
No hay una sola "lengua de señas". En las comunidades de personas sordas se desarrolla esta lengua. Mientras que utilizan el espacio para la gramática de una manera que las lenguas orales no lo hacen, las lenguas de signos presentan las mismas propiedades lingüísticas y utilizan la misma facultad del lenguaje como lo hacen las lenguas orales. Centenares de lenguas de signos están en uso en todo el mundo y se utilizan en los núcleos de la cultura sorda local. Algunas lenguas de signos han obtenido algún tipo de reconocimiento legal, mientras que otros no tienen ningún estatus. Las lenguas de signos para sordos no se basan en las lenguas habladas en la región, y a menudo tienen una sintaxis muy diferente en parte, pero no completamente, por su capacidad de utilizar las relaciones espaciales para expresar aspectos de significado. (Véase Relaciones de las lenguas de signos con los idiomas hablados.)
El abad Charles-Michel del Épée fue la primera persona que abrió una escuela para sordos. El Épée enseñó la lengua de signos francesa a los niños, y empezó la propagación de muchas escuelas para sordos en Europa. Thomas Gallaudet viajaba a Inglaterra para iniciar una escuela para sordos. Su inspiración fue una niña de nueve años, que vivía junto a su casa. Al ver a conquistar sus luchas realizadas Gallaudet quiso enseñar y ver a otros niños conquistar sus propias discapacidades. Gallaudet fue testigo de una demostración de las habilidades de enseñanza de sordos de Roch-Ambroise Cucurron Sicard, Jean Massieu, y Laurent Clerc, los maestros de la enseñanza de los niños sordos en la época. Después de la demostración, Gallaudet estudió con los maestros franceses y perfeccionar sus habilidades de enseñanza. Un golpe acabó el aprendizaje Gallaudet y Clerc viajaron a los Estados Unidos y abrieron la primera escuela para sordos en Hartford, Connecticut. El American Sign Language, o ASL, iniciada a evolucionar a partir principalmente de la francesa, y otras influencias externas.[14]​
Las personas sordas han visto históricamente como se ha deshabilitado el acceso en una educación pública libre y adecuada. Un niño con problemas de audición tiene que recibir un plan de educación individualizado, como la IEP, que considera que proporciona "el lenguaje y las necesidades de comunicación del niño. El IEP tiene que incluir oportunidades para la comunicación directa con compañeros y profesionales. También tiene que incluir el nivel académico del estudiante, y, finalmente, tiene que incluir toda la gama de necesidades de los estudiantes"[15]​ El gobierno también distingue entre la sordera y la pérdida de audición. El Departamento de Educación de los Estados Unidos afirma que la sordera es la pérdida severa que una persona no puede procesar cualquier tipo de información oral, incluso si esta persona tiene algún tipo de mejora audioprótesis de audición. El Departamento de Educación de los Estados Unidos afirma que una deficiencia auditiva es cuando la educación de una persona se ve afectada por la posibilidad de ser capaz de sentir. Esta definición no se incluye en el término sordera. Porque una persona pueda necesitar servicios especiales, la persona tiene que sentir más de 20 decibelios y su rendimiento educativo se tiene que ver afectado por la pérdida de audición. Esto es lo que el gobierno dice sobre las políticas gubernamentales y los servicios individualizados.
Hay opiniones variadas sobre el tema entre los que viven en comunidades de sordos, y los que tienen familiares sordos que no viven en comunidades de sordos. Las comunidades de sordos son donde típicamente sólo se utilizan lenguas de signos.[cita requerida]
Muchos padres que tienen un hijo con problemas de oído prefieren que su hijo sea al entorno menos restrictivo de su escuela. Esto puede ser porque la mayoría de niños sin oído nacen a padres que sienten. Esto también puede ser debido a la reciente campaña para la inclusión en las escuelas públicas.
Se suele entender mal que un entorno menos restrictivo significa integración o inclusión. A veces, los recursos disponibles en las escuelas públicas no se ajustan a los recursos de una escuela residencial para sordos. Muchos padres optan para educar su hijo sordo en un aula de educación convencional, puesto que se los dice que la integración en el ámbito público es el entorno menos restrictivo; cosa que no siempre es así. Aun así, hay colectivos de padres que consideran que el aula de educación convencional no es el ambiente menos restrictivo. Estos padres consideran que poner su hijo en una escuela residencial donde todos los niños son sordos puede ser más adecuado para su hijo, puesto que el personal tiende a ser más consciente de las necesidades y luchas de los niños sordos. Otra razón por la cual estos padres sienten que una escuela residencial puede ser más adecuada que un aula de educación convencional es que el alumno no podrá comunicarse con los compañeros debido a la barrera idiomática.[cita requerida]
En una escuela residencial donde todos los niños utilizan el mismo lenguaje (ya sea una escuela con ASL, Comunicación Total, u Oralismo), los estudiantes podrán interactuar con normalidad con otros estudiantes sin tener que preocuparse para ser fiscalizados. Por otro lado, un argumento que mejor apoya la idea de inclusión dice que exponer el alumno entre personas que no son como él lo prepara para la vida adulta. Mediante la interacción, los niños con discapacidad auditiva pueden conocer otras culturas que, en el futuro, pueden ser beneficiosas para encontrar trabajo y vivir por su cuenta en una sociedad donde su discapacidad los pueda situar en minoría. Estos son algunos de los motivos por los cuales una persona puede querer o no poner su hijo en una aula de inclusión.[15]​
Hay varios mitos sobre personas sordas:

Cada año se celebra el Día Internacional de las Personas Sordas la última semana del mes de septiembre, concordando con el último domingo. Aunque en España, suele coincidir con el último viernes o sábado del mismo mes.
En este día se reivindican los derechos y demandas de las personas con discapacidad auditiva, y se hacen visible ante la sociedad la riqueza cultural de las comunidades sordas de todo el mundo.
Esta fecha fue elegida por la Fundación Mundial de Personas Sordas (WFD) para conmemorar el primer Congreso Mundial de la WFD que tuvo lugar el septiembre de 1951.[cita requerida]


La tuberculosis (abreviada TBC o TB), llamada alternativa e históricamente peste blanca o tisis [1]​ (del griego φθίσις, a través del latín phthisis), es una infección bacteriana contagiosa que afecta a los pulmones, pero puede propagarse a otros órganos. La especie de bacteria más importante y representativa causante de la tuberculosis es Mycobacterium tuberculosis o bacilo de Koch, perteneciente al complejo Mycobacterium tuberculosis.[2]​
Existe una vacuna (BCG) para poder prevenir esta enfermedad.
Es, tal vez, la enfermedad infecciosa más prevalente del mundo. Considerando su forma latente, en la cual no presenta síntomas, se estima que afecta al 33 % de la población mundial.[3]​ Es la segunda causa global de muerte, y la primera entre las enfermedades infecciosas.[4]​[5]​[6]​[7]​
Otras micobacterias, como Mycobacterium bovis, Mycobacterium africanum, Mycobacterium canetti y Mycobacterium microti pueden causar tuberculosis, pero todas esas especies no suelen hacerlo en un individuo sano.
Los síntomas de tuberculosis son: tos crónica con esputo sanguinolento, fiebre, sudores nocturnos y pérdida de peso. La infección de otros órganos causa una amplia variedad de síntomas. 
El diagnóstico se basa en la radiología (radiografías torácicas), una prueba de la tuberculina cutánea y análisis de sangre, así como un examen al microscopio y un cultivo microbiológico de los fluidos corporales como las expectoraciones. El tratamiento es complicado y requiere largos periodos de exposición con antibióticos. Los familiares del enfermo también son analizados.
Durante los últimos años, la tuberculosis ha presentado una creciente resistencia a los múltiples antibióticos y para ello se ha optado, como medida de prevención, por campañas de vacunación, en general con la vacuna Bacillus Calmette-Guérin (BCG)[8]​.[cita requerida]
Se contagia por vía aérea, cuando las personas infectadas tosen, estornudan o escupen. Además, un número creciente de personas del mundo la contrae debido a que su sistema inmunitario se debilita por medicamentos inmunosupresores o el sida. La distribución de la tuberculosis no es uniforme en el mundo; el 80 % de la población de países asiáticos y africanos da positivo, porcentaje que baja a 5-10 % de la población en Estados Unidos.
Según datos de la Organización Mundial de la Salud (OMS), en 2013 nueve millones de personas enfermaron de tuberculosis y 1.5 millones murieron por esta causa, de los cuales 360 000 tenían infección por VIH. Cerca del 60 % de los casos y muertes ocurre en hombres. De las 510 000 mujeres que murieron por esta causa en ese período, más de un tercio tenían infección por  VIH. La OMS estima que el diagnóstico precoz y el tratamiento efectivo logró que se salvaran 37,2 millones de personas entre 2000 y 2013, pero considera «todavía inaceptablemente alta» la cantidad de estas muertes prevenibles.[9]​
La tuberculosis se puede manifestar por signos clínicos y síntomas pulmonares o extrapulmonares.
La tuberculosis extrapulmonar, puede aparecer en el contexto de una tuberculosis miliar, la reactivación de un foco pulmonar o en ausencia de enfermedad clínica pulmonar. Incluye:
- Diseminados (TBC miliar)
La transmisión de la tuberculosis solo puede realizarse por personas que tengan activa la enfermedad. La TBC se transmite a través de partículas expelidas por el paciente bacilífero (con TBC activa) con la tos, estornudo, hablando, escupida, etc., por lo que se recomienda no tener contacto con terceros. Las gotas infecciosas (flügge's o droplets) son de un diámetro entre 0,5 a 5 µm, pueden producirse alrededor de 400 000 con un solo estornudo.[13]​ Cada una de esas gotitas proveniente de un enfermo activo puede transmitir el microorganismo, en especial sabiendo que la dosis infectante de la tuberculosis es considerada baja, de modo que la inhalación de una sola bacteria puede infectar.[14]​ La probabilidad de una transmisión eficaz aumenta con el número de partículas contaminadas expelidas por el enfermo, en lo buena que sea la ventilación del área, la duración de la exposición y en la virulencia de la cepa del M. tuberculosis. Las personas con contactos frecuentes, prolongados, o intensos tienen un riesgo de alrededor del 25 % de ser infectados. Para un fumador las posibilidades de enfermar se multiplican por 2,5.[15]​ Un paciente con TBC activa sin tratamiento puede infectar entre 10-15 personas por año. Otros riesgos incluyen aquellas áreas donde la TBC es frecuente, en pacientes inmunodeprimidos con condiciones como malnutrición y sida, poblaciones étnicas en alto riesgo y trabajadores de la salud sirviendo en regiones de alto riesgo.[16]​ En los pacientes con sida, la TBC actúa como enfermedad oportunista (coinfección) con fuerte asociación. También puede transmitirse por vía digestiva, sobre todo al ingerir leche no higienizada procedente de vacas tuberculosas infectadas con Mycobacterium bovis.
La cadena de transmisión puede romperse si se aísla al enfermo con tuberculosis activa y comenzando de inmediato una terapia antituberculosis efectiva. Después de dos semanas con dicho tratamiento, aquellos pacientes con TBC activa y no-resistente dejan de ser contagiosos. Si una persona llegase a quedar infectada, le tomará menos de 20 a 60 días antes que pueda comenzar a transmitir la enfermedad a otros.[17]​
En el comienzo de la enfermedad, las personas con tuberculosis pueden tener síntomas comunes a otras enfermedades, como son fiebre, cansancio, falta de apetito, pérdida de peso, depresión, sudor nocturno y disnea en casos avanzados; más cuando se agregan las aflicciones de tos y expectoración purulenta por más de quince días debe estudiarse, pues se considera un síntoma respiratorio.
En un 25 por ciento de los casos activos, la infección se traslada de los pulmones, causando otras formas de tuberculosis. Ello ocurre con más frecuencia en aquellos pacientes inmunosuprimidos y en niños. Las infecciones extrapulmonares incluyen la pleura, el sistema nervioso central causando meningitis, el sistema linfático causando escrófula del cuello, el sistema genitourinario causando tuberculosis urogenital y los huesos o articulaciones en el caso de la enfermedad de Pott. Una forma muy seria de diseminada es la tuberculosis miliar. A pesar de que la extrapulmonar no es contagiosa, puede coexistir con la contagiosa tuberculosis pulmonar.[18]​
La tuberculosis es una de las enfermedades humanas más antiguas. Aunque se estima entre 15 000 a 22 000 años, se acepta más que esta especie evolucionó de otros microorganismos más primitivos dentro del género Mycobacterium. Puede pensarse que en un momento, alguna especie de microbacterias traspasara la barrera biológica, por presión selectiva, y pasara a tener un reservorio en animales. Es posible que esto haya dado lugar a un anciano progenitor del Mycobacterium bovis, aceptada por muchos como la más antigua de las especies del complejo Mycobacterium tuberculosis, que incluye M. tuberculosis, M. bovis, M. africanum y M. microti. El "escalón" siguiente sería el paso del M. bovis a la especie humana, coincide con la domesticación de los animales. Así pudo surgir como patógeno para el perro.
La tuberculosis constituye un paradigma de la interacción de un agente exógeno y la respuesta inmunitaria del huésped. La Organización Mundial de la Salud estima 2000 millones de infectados por el M. tuberculosis y ocho millones de nuevos infectados cada año, venciendo la batalla en la mayoría de las ocasiones. Sin embargo, mueren casi dos millones de personas al año por causa de esta enfermedad.
Entonces empieza la infección latente, caracterizada por la presencia de respuesta inmune específica, control de la concentración bacilar, pero con la presencia de bacilos latentes (en estado estacionario) en el tejido necrótico. A medida que los macrófagos van drenando este tejido, los bacilos latentes se confunden con esta necrosis y son drenados hacia el espacio alveolar, donde pueden reactivar su crecimiento de nuevo. De esta manera se mantiene la infección durante años.
Desde lo clínico, la infección tuberculosa latente no genera síntomas. Su diagnóstico se basa en el test cutáneo de Mantoux. Los individuos con esta infección no pueden infectar a nadie. Sin embargo, en un 10 % de los casos, el control de la concentración bacilar se pierde, se reanuda el crecimiento y se puede generar una tuberculosis activa, o enfermedad tuberculosa propia. Por ello debe tratarse, sobre todo los pacientes recién infectados. El tratamiento representa la administración de isoniazida durante 9 meses, hecho que dificulta su seguimiento.
Progresa de infección tuberculosa a enfermedad tuberculosa. Puede ocurrir de forma temprana (tuberculosis primaria, alrededor del 1-5 %) o varios años después de la infección (tuberculosis postprimaria, secundaria, reactivación tuberculosa en alrededor del 5 al 9 %). El riesgo de reactivación se ve incrementado con alteraciones en el sistema inmunitario, tales como las causadas por el VIH. En pacientes coinfectados de VIH y TBC, el riesgo de reactivación se incrementa un 10 % por año, mientras que en una persona inmunocompetente el riesgo es del 5 al 10 % durante toda la vida.
Algunos fármacos, incluyendo tratamientos usados en la artritis reumatoide que bloquean el factor de necrosis tumoral, aumentan el riesgo de activación de una TBC latente debido a la importante acción de esta citoquina en la respuesta inmune contra la TBC.
La TBC activa se diagnostica por la detección de Mycobacterium tuberculosis en cualquier muestra del tracto respiratorio (TBC pulmonar) o fuera de él (TBC extrapulmonar). Aunque algunos métodos más modernos (diagnóstico molecular) han sido desarrollados, la visión microscópica de bacilos ácido-alcohol resistentes (BAAR) y el cultivo en medio de Löwenstein-Jensen siguen siendo el gold standard del diagnóstico de la TBC, en especial en países con bajos recursos sanitarios, aunque el método MODS viene siendo validado dando resultados con una sensibilidad y especificidad superiores al cultivo. La microsocopía de BAAR es rápida y barata y un método muy eficiente para detectar pacientes contagiosos. El uso de cultivo en la TBC se realiza cuando hay poca carga bacteriana (mayor sensibilidad), para la identificación de la cepa y para el estudio de sensibilidades a los distintos tratamientos.Tanto la microscopia como el cultivo pueden usarse para monitorizar el tratamiento.[19]​
La Universidad Autónoma de Madrid publicó en el Journal of Clinical Microbiology un trabajo donde se describe por primera vez que las micobacterias son capaces de emitir fluorescencia, lo que permite verlas en un microscopio de fluorescencia sin necesidad de una tinción previa. Esta característica presenta interés para el diagnóstico de la tuberculosis, ya que antes era necesario recurrir a las tinciones específicas para poder observar la mayoría de las bacterias, ya que muy pocas presentan autofluorescencia. Sin embargo, la autofluorescencia emitida por las micobacterias de color azul celeste es tan intensa y brillante como cuando son teñidas de verde con el método antiguo. Además se ha constatado que el fenómeno es permanente, no disminuyendo la autofluorescencia con el paso del tiempo por lo que no es necesaria una conservación especial de las muestras para su mantenimiento.[20]​
La radiografía es esencial en el diagnóstico de la enfermedad. Las lesiones típicas radiológicas son apicales, en hemitórax derecho, en segmentos posteriores y en general forman cavidades.
La herramienta para el diagnóstico de caso de tuberculosis es la bacteriología (baciloscopía y cultivo) por su alta especificidad, sensibilidad y valor predictivo. En aquellas situaciones donde los estudios bacteriológicos no sean concluyentes será necesario realizar el seguimiento diagnóstico de acuerdo con la organización de la red de servicios de salud, utilizando otros criterios: clínico, epidemiológico, diagnóstico por imágenes, inmunológico, anatomopatológico.
Toda persona con diagnóstico de tuberculosis previa consejería y aceptación se deberá realizar la prueba de diagnóstico para VIH.
Consiste en una prueba seriada (tres días consecutivos), donde se toma una muestra de esputo para ver qué bacteria se encuentra. Con un costo bajo y de rápida ejecución, la baciloscopia es una técnica que permite identificar al 70-80 % de los casos pulmonares positivos.[21]​ La bacteria Mycobacterium tuberculosis posee una estructura de pared diferente de aquellas que son capaces de ser tipificables por la tinción Gram, al presentar una cantidad de lípidos muy abundante. Se le denomina ácido-alcohol resistente y esta característica es la que permite su observación por la tinción de Ziehl Neelsen.
El cultivo puede hacerse en el medio de Löwenstein-Jensen, que está constituido por:
Crece muy lento (30 a 90 días) a 37 °C en atmósfera con dióxido de carbono (en cultivo crecen mejor a pesar de ser aerobio estricto), dando colonias con aspecto de migas de pan (o huevos de araña), secas amarillentas y rugosas.
Es una prueba cutánea (intradermoreacción) para detectar infección tuberculosa. Se utiliza como reactivo el PPD (Derivado Proteico Purificado). La prueba de la tuberculina Mantoux solo implica contacto, no infección.
La susceptibilidad a drogas de Mycobacterium tuberculosis mediante observación microscópica (MODS) es un método de desarrollo reciente que posee una sensibilidad y especificidad muy elevadas, como también una gran reducción del tiempo para el diagnóstico de infección por el Mycobacterium tuberculosis, a la vez que evalúa la resistencia a antibióticos de primera línea, como la isoniacida y la rifampicina para los pacientes TB-MDR (multidrogorresistentes).[22]​
El tratamiento de la tuberculosis se realiza con combinaciones de fármacos antituberculosos, haciendo eficaces las pautas de seis meses de tratamiento, dos en la primera fase de tratamiento y cuatro meses en la segunda fase.[23]​
La tuberculosis es curable, pero es necesario un diagnóstico temprano (acudir de inmediato al médico), ya que es una enfermedad grave si no se sigue el tratamiento. Además, es indispensable no abandonar el tratamiento dado por el médico porque, al suspenderlo, la enfermedad empeora rápido y favorece la proliferación de bacilos resistentes a los medicamentos.
Se inicia a mediados del siglo XIX y primera mitad del XX, se generaliza como base del tratamiento, sobre todo en los países desarrollados, llega a ser uno de los índices que determinan el nivel sanitario de un país.
Los sanatorios se construían a gran altura, basándose en la teoría fisiológica de aumentar el flujo sanguíneo pulmonar, por la taquicardia inducida por la altura. Sin embargo, la evidencia de su eficacia resultó dudosa.
Se realizaron diversas técnicas, todas ellas basadas en la colapsoterapia, que consistía en hacer colapsar el pulmón para que permaneciera en reposo y así ayudar a una cicatrización de las lesiones.
La historia de la tuberculosis cambia después de la introducción de los agentes antibióticos. El tratamiento de la tuberculosis es fundamental para su control, dado que con él se rompe la cadena de trasmisión cuando el tratamiento es correcto y se sigue completo.
El tratamiento farmacológico comenzó en 1944 con la estreptomicina (SM) y el ácido paraaminosalicílico (PAS).[24]​ En 1950, se realizó el primer ensayo clínico comparando la eficacia de la SM y el PAS en conjunto o en monoterapia. El estudio demostró que la terapia combinada era más efectiva. En 1952, un tercer fármaco, la isoniacida (INH), se añadió a la combinación, mejorando en forma espectacular la eficacia del tratamiento, aunque todavía con una duración de 18-24 meses. El etambutol se introdujo en 1960, sustituyendo al PAS en los esquemas de tratamiento y reduce la duración a 18 meses. En la década de 1970, con la introducción de la rifampicina (RAM) en la combinación, el tratamiento se acorta a nueve meses. En 1980, la pirazinamida (PZA) se introduce en el esquema terapéutico, que puede reducirlo a seis meses.[25]​
Dos hechos biológicos explican por qué la terapia combinada es más efectiva en el tratamiento de la TBC que la monoterapia. El primero es que el tratamiento con un solo fármaco induce la selección de bacilos resistentes y en consecuencia el fallo en eliminar la enfermedad. El segundo es que las diferentes poblaciones bacilares pueden coexistir en un paciente.
Los antituberculostáticos se clasifican en dos grupos en función de su eficacia, potencia y efectos secundarios:
Un problema que se extiende en los últimos años es la aparición de M. tuberculosis resistentes a antibióticos.[27]​ Teniendo en cuenta las resistencias a antibióticos que presentan distintas cepas, es posible distinguir entre cepas multirresistentes (MDR), que son bacterias que desarrollan resistencia frente a rifampicina (RMP) e isoniacida (INH), y cepas ultrarresistentes (XDR), resistentes a drogas de primera línea y a cualquier miembro de la familia de las fluoroquinolonas y al menos frente a uno de segunda línea.[28]​
Se previene mediante una vida sana e higiénica, con identificación temprana de los enfermos y asegurando su curación para no contagiar a otras personas, por medio de la vacuna BCG.
La vacunación sistemática con la BCG en los recién nacidos se abandonó en España en 1980.[29]​[30]​[31]​
La vacuna BCG (Bacillus Calmette-Guérin), usada desde 1921, se sigue usando en muchos países como parte de los programas de control de la tuberculosis, en especial en niños. Esta vacuna fue desarrollada en el Instituto Pasteur, Francia, entre 1905 y 1921.[32]​ Sin embargo, las vacunaciones masivas no comenzaron hasta después de la Segunda Guerra Mundial.[33]​ La eficacia en la protección de la BCG en formas graves de tuberculosis (p. ej.: meningitis) en niños menores de 4 años es grande (BCG protege contra la infección por M. tuberculosis, así como contra la progresión de la infección a la enfermedad. La revisión sistemática y metaanálisis se realizó en niños menores de 16 vacunados y no vacunados con exposición reciente a infección tuberculosa),[34]​ y está alrededor del 80 %; su eficacia en adolescentes y adultos es más variable, estando entre el 0 y el 80 %.[35]​
RUTI es una vacuna terapéutica que se desarrolla en la Unidad de Tuberculosis Experimental de Badalona (España) para disminuir el tratamiento de la infección tuberculosa latente[36]​[37]​ de 9 a 1 mes de administración de isoniacida. Responsables son Archivel Farma y el Hospital Germans Trias i Pujol de Badalona, conocido como Can Ruti, lo que ha dado nombre a la vacuna.
La MTBVAC es la primera vacuna con el bacilo atenuado de tuberculosis humano. La actual BCG, es de bos taurus.[38]​ El grupo de la universidad de Zaragoza, dirigido por el investigador Carlos Martín Montañés, en colaboración con la farmacéutica gallega Biofabri, del grupo Zendal, ha demostrado que su vacuna desarrollada da mayor protección contra el patógeno que la vacuna actual.[39]​ Biofabri realizó en el años 2019 con éxito los primeros ensayos clínicos en humanos y tiene prevista su producción industrial.[40]​
La OMS estipuló que el 24 de marzo sería el Día Mundial de la Lucha contra la Tuberculosis. Se conmemora que el 24 de marzo de 1882 el doctor Robert Koch anunció el descubrimiento del bacilo de la tuberculosis.
En 1982 se realizó el primer Día Mundial de la Lucha contra la Tuberculosis, con el patrocinio de la Organización Mundial de la Salud (OMS) y la Unión Internacional Contra la Tuberculosis y las Enfermedades Respiratorias (UICTER). Este evento buscaba educar al público sobre las devastadoras consecuencias económicas y de salud causadas por la tuberculosis, su efecto en los países en desarrollo y su impacto continuo y trágico en la salud global.
Según la Organización Mundial de la Salud (OMS), cerca de 2000 millones de personas, un tercio de la población del mundo, han estado expuestas al patógeno de la tuberculosis. Sin embargo, no todas las infecciones por M. tuberculosis causa la tuberculosis y muchas infecciones son asintomáticas. Cada año, ocho millones de personas se enferman con la tuberculosis, y dos millones de personas mueren de la enfermedad a escala mundial. En 2004, alrededor de 14,6 millones de personas tenían la enfermedad activa con 9 millones de nuevos casos. La tasa de incidencia anual varía de 356 por 100 000 en África y 41 por 100 000 en América. Provoca enfermedades infecciosas en las mujeres en edad reproductiva y es la principal causa de muerte entre las personas con sida.
En 2005, el país con la mayor incidencia estimada de tuberculosis fue de Suazilandia, con 1262 casos por cada 100 000 personas. La India tiene el mayor número de infecciones, con más de 1,8 millones de casos. En los países desarrollados, la tuberculosis es menos común y es una enfermedad urbana. En el Reino Unido, la incidencia de tuberculosis va desde 40 por 100 000 en Londres, a menos de 5 por 100 000 en zonas rurales del sur oeste de Inglaterra, de la media nacional es de 13 por 100 000. Las tasas más altas de Europa occidental se sitúan en Portugal (31,1 por 100 000 en 2005) y España (20 por 100 000). Estos rangos comparan con 113 por 100 000 en China y 64 por 100 000 en Brasil. En los Estados Unidos, la tasa general de casos de tuberculosis fue de 4,9 por 100 000 personas en 2004. En España la tuberculosis sigue siendo endémica en algunas zonas rurales. La incidencia de la tuberculosis varía con la edad. En África, la tuberculosis afecta a adolescentes y adultos jóvenes. Sin embargo, en países donde la tuberculosis ha pasado de alta a baja incidencia, como los Estados Unidos es una enfermedad de personas mayores o de los inmunocomprometidos.
Las infecciones, el aumento del VIH y el descuido de control de la tuberculosis por programas han permitido su resurgimiento. La aparición de resistencia en unas cepas también ha contribuido a una nueva epidemia, de 2000 a 2004, el 20 % de los casos de tratamientos estándar eran resistentes a de medicamentos de segunda línea. El ritmo de los nuevos casos varía con amplitud, incluso en los países vecinos, debido a las filas en los sistemas de atención sanitaria.
Un problema que se está extendiendo en los últimos años es la aparición de M. tuberculosis resistentes a antibióticos. La tuberculosis multirresistente se ha encontrado en casi todos los países estudiados. En 2012, entre los casos notificados de tuberculosis pulmonar hubo unos 450 000 casos de tuberculosis multirresistente. Casi el 50 % de ellos correspondían a la India, China y la Federación Rusa. Se cree que un 9,6 % de los casos de tuberculosis multirresistente presentaban tuberculosis ultrarresistente.[9]​
Hay una serie de factores que hacen que las personas sean más susceptibles a la infección; el más importante de ellos es el VIH. La coinfección con el VIH es un problema particular en el África subsahariana, debido a la alta incidencia de VIH en estos países. Los fumadores que consumen más de 20 cigarrillos al día también aumentan el riesgo de la tuberculosis de dos a cuatro veces. La diabetes mellitus es un factor de riesgo que está creciendo en importancia en los países en desarrollo. 
Otros estados de enfermedad que aumentan el riesgo de desarrollar tuberculosis son el linfoma de Hodgkin, el final de la enfermedad renal, enfermedad pulmonar crónica, la desnutrición y el alcoholismo. 
La dieta también puede modular el riesgo. Por ejemplo, entre los inmigrantes en Londres desde el subcontinente indio, los vegetarianos hindúes tenían un 8,5 veces más riesgo de tuberculosis, en comparación con los musulmanes que comían carne y pescado todos los días. A pesar de una relación de causalidad no se prueba por estos datos este aumento del riesgo que podría ser causado por las deficiencias de micronutrientes, es posible que de hierro, vitamina B12 o vitamina D. 
Otros estudios han proporcionado más evidencias de una relación entre la deficiencia de vitamina D y un mayor riesgo de contraer tuberculosis. La malnutrición grave común en algunas partes del mundo en desarrollo provoca un gran aumento en el riesgo de desarrollar tuberculosis activa, debido a sus efectos nocivos sobre el sistema inmunitario. Junto con el hacinamiento, la mala alimentación puede contribuir al fuerte vínculo entre la tuberculosis y la pobreza.

Los mamíferos (Mammalia) son una clase de animales vertebrados amniotas homeotermos (de sangre caliente) que poseen glándulas mamarias productoras de leche con las que alimentan a las crías. La mayoría son vivíparos (con la excepción de los monotremas: ornitorrinco y equidnas). Se trata de un taxón monofilético; es decir, todos descienden de un antepasado común que se remonta probablemente a finales del Triásico, hace más de 200 millones de años. Pertenecen al clado sinápsidos, que incluye a los mal llamados reptiles mamiferoides, un grupo de sinápsidos que no eran reptiles ni tampoco mamíferos, aunque estaban más relacionados con estos últimos que con los primeros,[1]​[2]​ como los pelicosaurios y los cinodontos. Se conocen unas 5486 especies actuales,[3]​ de las cuales 5 son monotrematas,[4]​ 272 son marsupiales[5]​ y el resto, 5209, son placentarios.
La especialidad de la zoología que estudia específicamente a los mamíferos se denomina teriología, mastozoología o mamiferología.
La clase de los mamíferos es un grupo monofilético, ya que todos sus miembros comparten una serie de novedades evolutivas exclusivas (sinapomorfías) que no aparecen en ninguna otra especie animal no incluida en ella:
Pero a pesar de estas y otras similitudes que no son definitorias de la clase, su diversidad es tal que son muchas más las diferencias existentes, especialmente en cuanto a aspecto externo se refiere.
Ya se han apuntado los caracteres sinapomórficos de la clase Mammalia. Todas las especies los presentan y son exclusivos además de la clase:
Los dientes se componen de sustancias que no pertenecen al sistema óseo, sino al tegumentario, como la piel, las uñas y el pelo. La materia que forma el cuerpo del diente es el marfil o dentina, que por lo general está revestido en el exterior de otra sustancia muy dura, el esmalte, mientras que en la base del diente la envoltura externa está compuesta por una tercera sustancia llamada cemento.
En los mamíferos, los dientes se hallan siempre insertos en los huesos del cráneo que rodean la boca, que son, arriba, dos maxilares y dos premaxilares, y abajo, una mandíbula o quijada, que se articula directamente con la caja del cráneo. Este último, a su vez, enlaza con la columna vertebral por medio de dos abultamientos, o cóndilos, que hay a uno y otro lado del agujero por donde la médula espinal penetra para unirse al encéfalo. Aunque el número de vértebras de la columna vertebral varía mucho según las especies, las cervicales o vértebras del cuello son siete en todos los mamíferos a excepción de los perezosos que pueden tener hasta 10 y de los manatíes que solo poseen seis. Pero, además, existen otras características comunes a estas especies que sirven también para identificarlas como parte del taxón:
La piel, generalmente espesa, está formada por una capa externa o epidermis, una capa profunda o dermis y un estrato subcutáneo repleto de grasa que le sirve de protección contra las pérdidas de calor, ya que los mamíferos son animales homeotermos.
En ella se hallan dos de las sinapomorfias de la clase Mammalia: el pelo y las glándulas mamarias.
Está implicada directamente en la protección del animal, la capacidad de termorregulación, la excreción de productos de desecho, la comunicación animal y la producción de leche (glándulas mamarias).
Otras formaciones cutáneas de naturaleza córnea que presentan los mamíferos son las uñas, garras, cascos, pezuñas, cuernos y el pico en el caso del ornitorrinco.
El aparato locomotor es el conjunto de sistemas y tejidos que posibilitan el mantenimiento del cuerpo del animal y su movimiento.
Además existen otras formaciones óseas como los huesos del aparato hioides (sostén de la lengua), del oído medio, el hueso peneano de algunos carnívoros e incluso los huesos cardíacos de algunos bóvidos en los que osifica el cartílago cardíaco.
Además del sistema óseo, el aparato locomotor está formado por el sistema muscular y el sistema articular.
El aparato digestivo consiste en un conducto de entrada, o esófago, un tubo intestinal con salida al exterior y un estómago, más algunas glándulas anexas, las más importantes de las cuales son el hígado y el páncreas. Salvo contadas excepciones, el alimento sufre una preparación previa, la masticación, por medio de los dientes, órganos duros que guarnecen la boca y cuyo número y forma varían en gran medida según la alimentación de cada animal. En la mayoría de los casos hay, ante todo, unos dientes cortantes, llamados incisivos, a continuación, otros aptos para desgarrar, que son los colmillos, o caninos, y, por último, otros que sirven para triturar y moler, denominados muelas o molares. Por regla general, los mamíferos poseen una serie de dientes cuando son jóvenes y más tarde los cambian por otros.
El aparato digestivo de los mamíferos es un complejo visceral tubular en el que los alimentos se someten a un intenso tratamiento para obtener el máximo aprovechamiento de los nutrientes.
Durante el tránsito digestivo, desde que se ingiere hasta que se excreta, el alimento es sometido a un intenso proceso de degradación mecánica y química en el que intervienen una serie de órganos y tejidos encadenados estratégicamente.
La dieta del animal determina notablemente la fisiología y la anatomía de este aparato orgánico.
Estos dos aparatos son los encargados del intercambio de gases y su distribución por el organismo.
Los mamíferos respiran el oxígeno presente en el aire que es inspirado a través de las vías respiratorias (boca, nariz, laringe y tráquea) y se distribuye por bronquios y bronquiolos a todo el complejo sacular que constituyen los alvéolos pulmonares.
La sangre procedente de los tejidos transporta dióxido de carbono y al alcanzar los capilares alveolares, lo elimina a la vez que capta oxígeno. este será transportado nuevamente al corazón y desde allí a todos los tejidos para proporcionarles el gas necesario para la respiración celular, volviendo a transportar el dióxido de carbono residual hasta los pulmones.
El diseño y el funcionamiento de todos estos órganos y tejidos está perfectamente sincronizado para rentabilizar el proceso, especialmente en especies acuáticas o subterráneas en las que el aporte de oxígeno es limitado.
El sistema nervioso es un complejo conjunto de células, tejidos y órganos altamente especializados que tiene como misión recibir estímulos de distinta naturaleza, transformarlos en electro-químicos para transportarlos hasta el cerebro, traducirlos aquí y ordenar una respuesta que será transmitida nuevamente como señales electro-químicas hasta el órgano o tejido implicado en la ejecución de la misma.
El esquema del sistema nervioso es básicamente:
Los órganos de los sentidos, por su parte, son órganos ricos en terminaciones nerviosas capaces de traducir los estímulos externos en información para relacionar al individuo con su entorno. De manera general, los más importantes en los mamíferos son el olfato, el oído, la vista y el tacto, si bien en determinados grupos, otros sentidos como la ecolocalización, la magnetosensibilidad o el gusto adquieren mayor importancia.
Los mamíferos no se han especializado en el desarrollo de toxinas de la misma forma que otras clases como anfibios o reptiles. Debido a su gran tamaño, la fuerza física y el uso de garras y colmillos ha sido suficiente para alimentarse y defenderse. No obstante, algunos mamíferos de pequeño tamaño de los órdenes Monotremata, Chiroptera, Primates y Eulipotyphla sí han optado por el empleo de toxinas como estrategia evolutiva. Por ejemplo, el Solenodon paradoxus ha desarrollado veneno hipotensivo que emplea a la hora de cazar presas.
La gran mayoría de mamíferos venenosos pertenecen al orden Eulipotyphla. Se puede observar una convergencia evolutiva en la composición del veneno y este es producido en las glándulas salivales submaxilares a partir de KLK1.
En todos los mamíferos se presentan los sexos separados y la reproducción es de tipo vivípara, excepto en el grupo de los monotremas, que es ovípara.
El desarrollo del embrión va acompañado de la formación de una serie de anexos embrionarios, como son el corion, amnios, alantoides y el saco vitelino. Las vellosidades del corion, junto con el alantoides, se unen a la pared del útero y dan lugar a la placenta. Esta permanece unida al embrión por el cordón umbilical, y es a través de él por donde pasan las sustancias procedentes del cuerpo de la madre al del feto.
El periodo de gestación y el número de crías por camada varían mucho según los grupos. Normalmente, cuanto mayor es el tamaño del animal, más largo es el periodo de gestación y menor el número de crías. La mayor parte de los mamíferos proporcionan a sus hijos cuidados paternales.
Por último, es también característico de los mamíferos su modo de reproducirse. Si bien algunas especies son ovíparas, es decir, el óvulo fecundado sale al exterior formando un huevo, en la inmensa mayoría el embrión se desarrolla dentro del cuerpo de la madre y nace en un estado más o menos avanzado. De aquí se deriva una primera clasificación del grupo en mamíferos que ponen huevos y mamíferos vivíparos. A los segundos se les ha llamado terios, término derivado del griego clásico que significa «animales», y a los que son ovíparos, prototerios, esto es, «primeros animales», ya que el registro fósil permite suponer que los primeros mamíferos que aparecieron en el mundo pertenecían a esta categoría.
Todavía en los terios cabe distinguir entre los mamíferos cuyos hijos nacen en un estado de desarrollo muy atrasado, teniendo que pasar algún tiempo en una bolsa que la hembra posee en la piel del vientre, y aquellos otros en que no se observa semejante particularidad. Los primeros son los metaterios (también denominados marsupiales), es decir, «los animales que vienen detrás», los que siguen a los prototerios, y los últimos los euterios o mamíferos placentarios. Dentro de la clase que nos ocupa, estos constituyen la gran mayoría.
Los mamíferos constituyen un grupo de seres vivos muy diverso y, a pesar del reducido número de especies que lo forman en comparación con otros taxones del reino animal o vegetal, su estudio es con mucho el más profundo en el campo de la Zoología.
Para ilustrar con un ejemplo esta diversidad fenotípica, anatomo-fisiológica y etológica basta relacionar algunas de sus especies, como el ser humano (Homo sapiens), un canguro rojo (Macropus rufus), una chinchilla (Chinchilla lanigera), una ballena blanca (Delphinapterus leucas), una jirafa (Giraffa camelopardalis), un lémur de cola anillada (Lemur catta), un jaguar (Panthera onca) o los murciélagos («Chiroptera»).
Solo con comparar la especie animal de mayor envergadura que ha existido, la ballena azul (Balaenoptera musculus), que puede alcanzar las 160 t, con el murciélago de hocico de cerdo de Kitti (Craseonycteris thonglongyai), considerado el mamífero de menor tamaño, cuyos adultos apenas alcanzan los 2 g de peso, podemos observar que entre las especies más y menos voluminosas la diferencia en masa corporal es de 80 millones de veces.
La gran adaptabilidad de los individuos que integran la clase los ha llevado a habitar todos los ecosistemas del planeta, lo que ha dado lugar a multitud de diferencias anatómicas, fisiológicas y de comportamiento, convirtiéndolos en su conjunto en uno de los grupos dominantes sobre La Tierra. Han sido capaces de colonizar el dosel verde de la jungla y el subsuelo de los desiertos, los fríos hielos polares y las cálidas aguas tropicales, los enrarecidos ambientes de las altas cumbres y las fértiles y extensas sabanas y praderas.
Reptan, saltan, corren, nadan y vuelan. Muchos de ellos son capaces de aprovechar la más variada gama de recursos alimenticios, mientras otros están especializados en determinados alimentos. Este sinfín de circunstancias ha forzado a estos animales a evolucionar adoptando una multitud de formas, estructuras, capacidades y funciones.
Resulta curioso comprobar cómo en muchos casos, especies muy distanciadas entre sí geográfica y filogenéticamente han adoptado estructuras morfológicas, funciones fisiológicas y aptitudes de comportamiento similares. A este fenómeno se le conoce como evolución convergente. La similitud en la cabeza de un lobo gris (Canis lupus, un placentario), y un tilacino (Thylacinus cynocephalus, un marsupial), es sorprendente, siendo dos especies tan distanciadas filogenéticamente.
El erizo común europeo (Erinaceus europaeus, placentario) y el equidna común (Tachyglossus aculeatus, monotrema) pueden confundir a cualquier profano, pues no solo han adoptado la misma estructura de defensa, sino que comparten morfologías parecidas para explotar recursos alimenticios similares.
La gran diversidad de los mamíferos es fruto de una extraordinaria capacidad de adaptación que les ha permitido distribuirse por la gran mayoría de los ambientes del planeta.
Los mecanismos desarrollados por cada especie para conseguir adaptarse al medio evolucionaron de forma independiente. Así, mientras que algunas especies como el oso polar (Ursus maritimus) se protegieron del frío con una densa capa de pelo que con el reflejo de luz se ve blanco, otros como los pinnípedos o los cetáceos lo hicieron produciendo una densa capa de tejido graso bajo la piel.
En otros casos, especies muy distanciadas filogenéticamente recurren a mecanismos similares para adaptarse a circunstancias parecidas. El desarrollo de los pabellones auriculares del fénec (Vulpes zerda) y del elefante africano (Loxodonta africana) para incrementar la superficie de intercambio calórico y favorecer la homeostasis es un claro ejemplo.
La reconquista de las aguas por parte de animales que eran completamente terrestres es otra de las muestras de la capacidad de adaptación de los mamíferos. Distintos grupos de la clase han evolucionado de forma totalmente independiente para retornar al medio acuoso y explotar los nichos marinos y fluviales.
Por citar algunos ejemplos que ilustren la variabilidad de los mecanismos desarrollados para adaptarse a la vida acuática, dos órdenes cuyas especies son estrictamente acuáticas, Cetacea y Sirenia, las familias de carnívoros Odobenidae (morsa), Phocidae (focas) y Otariidae (osos y leones marinos), mustélidos como la nutria de mar (Enhydra lutris) y otras especies fluviales, roedores como el castor (Castor sp.) o la capibara (Hydrochoerus hydrochaeris), el desmán de los Pirineos (Galemys pyrenaicus), el hipopótamo (Hippopotamus amphibius), el yapok (Chironectes minimus), el ornitorrinco (Ornithorhynchus anatinus)...
Junto con las aves y los extintos pterosaurios, un grupo de mamíferos, los quirópteros han sido capaces de desplazarse mediante vuelo activo. No solo han desarrollado estructuras anatómicas imprescindibles como las alas, sino que también han desarrollado adaptaciones fisiológicas que permiten el ahorro energético compensando así el tremendo gasto que supone el vuelo.
Estos animales, además, teniendo que desenvolverse en la más estricta oscuridad de la noche y en el interior de las cavernas, han evolucionado perfeccionando el sistema de ecolocalización que les permite percibir con exactitud el mundo que los rodea.
Topos y otros zapadores, principalmente roedores, lagomorfos y algunos marsupiales habitan bajo tierra, algunos pasando enterrados la mayor parte de su vida. Han conseguido conquistar el interior de la superficie terrestre, pero la percepción del exterior, el movimiento bajo tierra, las relaciones entre individuos y los requisitos nutricionales y respiratorios han sido algunas de las cuestiones que han tenido que resolver a lo largo de su evolución, sufriendo durante ella notables transformaciones y especializaciones imprescindibles.
Tal especialización provoca que, en caso de una alteración del entorno, las especies puedan llegar hasta la extinción. De este modo, especies, familias e incluso órdenes enteros han desaparecido al verse modificado su hábitat. En los últimos años, el ser humano ha causado la destrucción de algunos entornos naturales. Por ejemplo, la desaparición de terrenos de caza vírgenes está provocando la extinción del lince ibérico (Lynx pardina) y la tala de bosques ha amenazado en gran medida al panda gigante (Ailuropoda melanoleuca). Del mismo modo, la introducción de especies foráneas como gatos, perros o zorros, ha producido la reducción del número de ejemplares de gatos marsupiales australianos.
Intentar resumir el papel ecológico que juegan las alrededor de 5000 especies de mamíferos resulta tan difícil como hacerlo con respecto a todos los seres vivos y su entorno, puesto que dada la diversidad de ecosistemas colonizados, comportamientos biológicos y sociales así como anatomía y adaptaciones morfológicas de todos ellos, da lugar a una variabilidad desconocida en cualquier otro grupo animal o vegetal sobre el planeta, a pesar de ser el grupo menos numeroso en cuanto a diversidad.
Por otra parte los altos requerimientos energéticos debidos a la necesidad de mantener constante la temperatura de su cuerpo condicionan notablemente las repercusiones que tienen las interacciones de estos animales sobre el entorno.
En general los depredadores suponen un gran impacto sobre las poblaciones de sus presas, que en alto número son otras especies mamíferas, mientras que precisamente estas pueden suponer en algunos casos la base de la alimentación de muchas otras.
Hay especies que con individuos escasos dan lugar a interacciones ecológicas de gran magnitud como ocurre con los castores y las corrientes de agua que detienen, mientras que otras, lo que supone una intensa presión es el número de ejemplares que llegan a reunirse como es el caso de las grandes manadas de herbívoros de las praderas o sabanas.
Un capítulo aparte supone la interacción ejercida por los humanos sobre todos y cada uno de los ecosistemas, habitados o no por él.
Los mamíferos son los únicos animales capaces de distribuirse por, prácticamente, la totalidad de la superficie del planeta, con excepción de las tierras heladas de la Antártida, aunque algunas especies de foca habiten en sus costas. En el extremo opuesto, el área de distribución de la foca híspida (Pusa hispida) alcanza las proximidades del Polo Norte.
Otra excepción la constituyen las islas remotas, alejadas de las costas continentales, en las que solo se dan casos de especies introducidas por el hombre, con el consabido desastre ecológico que ello supone.
En tierra, se hallan desde nivel del mar hasta los 6500 metros de altitud, poblando todos los biomas existentes. Y lo hacen no solo sobre la superficie, sino también bajo ella, e incluso por encima, tanto entre las ramas de los árboles como habiendo sufrido modificaciones anatómicas que les permiten el vuelo activo como es el caso de los murciélagos, o pasivo como es el de colugos, petauros y ardillas voladoras.
También el medio acuático ha sido conquistado por estos animales. Hay constancia de que a lo largo y ancho del planeta, los mamíferos pueblan sus ríos, lagos, humedales, zonas costeras, mares y océanos alcanzando profundidades superiores a los 1000 metros. De hecho, cetáceos y carnívoros marinos son dos de los grupos de mamíferos más ampliamente distribuidos por el planeta.
Como grupos taxonómicos, roedores y murciélagos, además de ser los más numerosos en especies, son los que han llegado a poblar las mayores superficies, pues salvo en la Antártida, pueden encontrarse en todo el planeta, incluidas islas no tan cercanas a la costa, imposibles de colonizar por otras especies terrestres.
En el extremo opuesto, los órdenes con pocas especies, son los de menor área de distribución global, con especial mención a dos de los tres órdenes de marsupiales americanos que se circunscriben a un área relativamente limitada del subcontinente meridional, especialmente el monito del monte (Dromiciops australis), único representante del orden Microbiotheria.
Los sirenios, aunque con áreas limitadas para cada una de las pocas especies con ejemplares vivos, pueden encontrarse en Asia, África, Centro y Sudamérica y Oceanía. Algunos órdenes son exclusivos de continentes determinados, habiendo evolucionado aislados del resto de los mamíferos, como ocurre con los cingulados en Sudamérica, con los tubulidentados en África o los dasyuroformes en Oceanía, por citar algunos ejemplos.
Si exceptuamos al hombre (Homo sapiens), y a los animales asociados a él tanto domésticos como salvajes, de entre las demás especies, quizá sean el lobo gris (Canis lupus) o el zorro rojo (Vulpes vulpes), las más ampliamente distribuidas, pues sus ejemplares se encuentran por la mayor parte del hemisferio norte. También el leopardo (Panthera pardus), que lo hace desde África hasta India o el puma (Puma concolor), desde Canadá hasta la Patagonia austral, son dos especies con áreas de distribución muy extensas. Otros carnívoros como el león (Panthera leo), el tigre (Panthera tigris) o el oso pardo (Ursus arctos) se han extendido por gran parte de la tierra hasta tiempos relativamente recientes, aunque sus áreas de distribución hayan ido disminuyendo paulatinamente hasta fraccionarse y acabar desapareciendo de la mayor parte de ellas en la actualidad.
En contraposición, un número mucho mayor de ellas ocupan áreas limitadas y no todas porque las hayan visto reducidas por alguna causa, sino porque a lo largo de su evolución no han podido o no han necesitado extenderlas más allá de las actuales.
Pero no solo especies determinadas han sido las que han desaparecido de regiones más o menos amplias del planeta, sino que algunos grupos enteros de mamíferos que en otros tiempos poblaron determinados continentes, no han logrado sobrevivir hasta los tiempos actuales. Los équidos por ejemplo, que poblaban en estado salvaje en casi todo el planeta, hoy solo existen en libertad en Asia y África, habiendo sido reintroducidos por el hombre en estado doméstico en el resto del planeta.
Y en otros casos la introducción fortuita o voluntaria de ciertas especies en regiones en las que no existían, ha puesto en peligro e incluso ha provocado la desaparición de las especies nativas.
En este apartado no figuran todas las especies de mamíferos de cada país.[6]​
También las altas necesidades energéticas de estos animales condicionan su comportamiento que, si bien varía sustancialmente de unas especies a otras, siempre tiene como meta el ahorro de energía para mantener la temperatura corporal.
Mientras que los mamíferos que habitan las regiones frías del planeta tienen que evitar la pérdida de calor corporal, los que habitan climas secos y calientes dirigen sus esfuerzos a evitar el sobrecalentamiento y la deshidratación. El comportamiento de todos por tanto va encaminado a mantener el equilibrio fisiológico, a pesar de las condiciones ambientales.
Los mamíferos, en general, exhiben todo tipo de formas de vida: hay especies de hábitos arborícolas y otras terrestres, existen mamíferos exclusivamente acuáticos y otros anfibios, e incluso aquellos que pasan su vida bajo el suelo excavando galerías en la arena. Los estilos de locomoción también son diversos por tanto: unos nadan, otros vuelan, corren, saltan, trepan, reptan o planean.
También el comportamiento social es muy diferente entre las especies: los hay solitarios, otros viven en pareja, en pequeños grupos familiares, en colonias medianas e incluso en grandes manadas de millares de individuos.
Por otra parte, muestran su actividad en distintos momentos del día: diurnos, nocturnos, crepusculares, vespertinos e incluso aquellos como el yapok (Chironectes minimus) que parecen no mostrar ritmo circadiano.
Los mamíferos actuales descienden de los sinápsidos primitivos, grupo de tetrápodos amniotas que comenzó a florecer a principios del Pérmico, hace unos 280 millones de años, y continuaron dominando sobre los «reptiles» terrestres hasta hace unos 245 millones de años (principios del Triásico), cuando empezaron a despuntar los primeros dinosaurios. Debido a su superioridad competitiva, estos últimos hicieron desaparecer a la mayoría de los sinápsidos. No obstante, algunos sobrevivieron y sus descendientes, los mammaliaformes, se convirtieron posteriormente en los primeros mamíferos verdaderos hacia finales del Triásico, hace unos 220 millones de años.
Los mamíferos más antiguos que se conocen son, por un lado los multituberculados y por otro los australosfénidos, grupos que datan del Jurásico Medio.[8]​
No obstante, debe tenerse en cuenta que la organización mamaliana, después de un éxito inicial durante el Pérmico y el Triásico, fue suplantada casi por completo, en el Jurásico y el Cretácico (durante unos 100 millones de años), por los reptiles diápsidos (dinosaurios, pterosaurios, cocodrilos, plesiosaurios, ictiosaurios, mosasaurios y pliosaurios), y no fue hasta el choque del meteorito que causó la extinción masiva del Cretácico-Terciario cuando los mamíferos se diversificaron y alcanzaron su papel dominante.[9]​
Aprovechar los recursos sin tener que competir con animales de mayor envergadura suponía adaptarse a regiones inhóspitas de clima normalmente frío, a los hábitos nocturnos, también con bajas temperaturas y además escasa iluminación.
A lo largo de la historia evolutiva de los mamíferos acontecen una serie de hechos que van a determinar la adquisición de los rasgos que caracteriza a la clase. La capacidad homeotérmica, es decir, de regular su temperatura corporal, es sin duda alguna la característica que permite a los mamíferos un mundo libre de competencia y rico en recursos altamente nutritivos. Fue gracias a ella que pudieron conquistar territorios fríos y sobre todo, desarrollar una actividad nocturna.
El crecimiento de pelo protegiéndoles el cuerpo de la pérdida de calor y el desarrollo de una visión apta para bajos índices de luminosidad fueron las otras dos circunstancias que colaboraron en la conquista de estos nichos ecológicos hasta el momento libres de animales superiores. Las adaptaciones del esqueleto fueron el primer paso para conseguir mayor efectividad energética basada en el incremento del aprovechamiento de los recursos y en la disminución del gasto.
El cráneo va haciéndose más efectivo, pierde masa, mantiene resistencia y simplifica estructuras a la vez que permite el desarrollo y efectividad muscular además del incremento cerebral (cerebro) y mayor inteligencia. Las modificaciones del cráneo llevan además consigo la formación de un paladar secundario, la formación de la cadena ósea del oído medio y la especialización de las piezas dentales. La mandíbula se constituye a partir de un único hueso (el dentario) y esta es la principal característica para determinar si el fósil de un animal pertenece a la clase de los mamíferos, debido a la usual pérdida de tejidos blandos durante la fosilización.
Las extremidades dejan paulatinamente de articularse a ambos lados del tronco para hacerlo por debajo. De este modo, a la vez que aumenta la movilidad del animal, disminuye el gasto energético al hacer menores los requerimientos para el desplazamiento y el mantenimiento del cuerpo erguido. Por su parte, la gestación interna de las crías y el proporcionarles a estas los alimentos para la primera edad sin tener que buscarlos (leche), permitió mayor libertad de movimiento a las madres y con ello un avance en su capacidad de supervivencia tanto individual como de la especie.
En todos estos cambios evolutivos se vieron involucradas todas y cada una de las estructuras orgánicas, así como los procesos fisiológicos. La maquinaria biológica especializándose requería mayor efectividad de los procesos respiratorios y digestivos, provocando el perfeccionamiento de los aparatos circulatorio y respiratorio con relación a la efectividad fisiológica, y el del digestivo para conseguir un mayor aprovechamiento nutritivo de los alimentos fueron otros de los logros conseguidos por estos animales durante su evolución.
El sistema nervioso central fue adquiriendo un tamaño y estructura histológica que no se conoce en otros animales, y la deficiencia de iluminación a que se enfrentaban las especies nocturnas se vio compensada con el desarrollo de los otros órganos sensoriales, en especial del oído y el olfato. Todos estos fenómenos evolutivos tardaron varios cientos de millones de años, tras los cuales los mamíferos hemos llegado a dominar la vida sobre la Tierra.
Los estudios genéticos revelan las siguientes relaciones filogenéticas para los mamíferos con respecto a otros tetrápodos vivos (incluido las secuencias proteicas obtenidas de Tyrannosaurus rex y Brachylophosaurus canadensis). Los mamíferos constituyen el grupo de amniotas basales, ya que se separaron de los reptiles y las aves a mediados del Carbonífero.[10]​[11]​[12]​[13]​
La filogenia entre órdenes existentes es la siguiente según estudios genéticos recientes (incluyendo las secuencias proteicas obtenidas de los meridiungulados Toxodon y Macrauchenia):[14]​[15]​[16]​[17]​[18]​[19]​
El siguiente cladograma muestra las relaciones filogenéticas de los mamíferos con algunos de sus ancestros:[cita requerida]
Las relaciones filogenéticas entre los principales grupos de mamíferos son, según Tree of Life Web Proyect,[20]​ las siguientes:
La taxonomía clásica se ha basado fundamentalmente en datos morfológicos para establecer similitudes y diferencias que permitan clasificar a las distintas especies, pero los nuevos descubrimientos paleontológicos y los continuos avances en genética y biología molecular ponen en entredicho bastantes de las teorías evolutivas hasta el momento aceptadas.
Como resumen cladístico de lo que se expone en el artículo principal puede servir el árbol siguiente en el que solo aparecen taxones de distinto rango entroncados directamente con la clase Mammalia o pendientes de una jerarquización más precisa:[21]​[22]​
Unas veces, y otras por temores infundados, son muchas las especies de mamíferos consideradas negativas por los humanos.
Algunas especies de mamíferos se alimentan de grano, fruta y otros productos vegetales, aprovechando los cultivos humanos para obtener el alimento.
Por su parte, los carnívoros pueden suponer en general una amenaza para la vida de los ganados e incluso del hombre.
Otros mamíferos habitan las áreas urbanas y suburbanas ocasionando algunos problemas considerables a la población: accidentes automovilísticos, rotura y deterioro de bienes materiales, plagas infecciosas y parasitarias, etcétera. Hay que apuntar que en este grupo incluimos tanto a los animales salvajes o semisalvajes como a los domésticos.
Canguros en Australia, mapaches en Norteamérica o zorros y jabalíes en la Europa mediterránea ilustran algunos ejemplos de situaciones de peligro real o potencial para las poblaciones, pero además enfermedades como la rabia, la peste bubónica, la tuberculosis, la toxoplasmosis o la leishmaniosis están estrechamente vinculadas a otras especies de mamíferos, normalmente en estrecho contacto con los humanos.
Los animales domésticos además, especialmente las especies introducidas en nuevos ecosistemas, han causado y causan auténticas tragedias ecológicas en la flora y fauna local, lo que indirectamente repercute de forma negativa no solo en los hombres, sino en el resto de las especies vivas del planeta, tanto animales como vegetales. En numerosas islas oceánicas la introducción de animales domésticos como el perro o el gato, la cabra o la oveja ha supuesto la desaparición total o parcial de numerosas especies.
Los mamíferos suponen un importante recurso económico para los seres humanos.
Muchas especies se han domesticado para obtener de ellas recursos alimenticios: la leche de vacas, búfalas, cabras y ovejas, la carne de estas especies y de otras como el cerdo, el conejo, el caballo, la capibara y otros roedores e incluso el perro en ciertas regiones del sudeste asiático.
Otras, para servirse de ellas para el transporte o para trabajos que requieren la fuerza u otra cualidad de la que el hombre no dispone: équidos como el asno, el caballo y su híbrido el mulo, camélidos como la llama o el dromedario, bóvidos como el buey o el yak, el elefante asiático o los perros tiradores de trineos son algunos de estos ejemplos.
Sin embargo, antes de alcanzar esta superioridad, es muy posible que los primitivos mamíferos tuvieran que convertirse en animales nocturnos para evitar la competencia con los dinosaurios. Y es probable que, para sobrevivir al frío de la noche, comenzasen a desarrollar la endotermia, es decir, la autorregulación interna de la temperatura corporal —la vulgarmente llamada «sangre caliente»—, gracias a la aparición del pelo y del sebo que lo impermeabiliza (la secreción de las glándulas sebáceas), y al sudor de las glándulas sudoríparas. Una vez adquirida la endotermia, los primeros mamíferos verdaderos mejoraron su capacidad competitiva frente a otros tetrápodos terrestres, porque su metabolismo continuo les permitió hacer frente a los rigores climáticos, tener un crecimiento más rápido y ser más prolíficos. Además de los caracteres esqueléticos y de otros ya mencionados —presencia de pelo y de glándulas cutáneas— que les valieron el predominio sobre la tierra a partir del Paleoceno, los mamíferos presentan otras características menos distintivas.
De otros se obtienen fibras y cueros para la fabricación de vestuario, calzado y otros utensilios: la lana de ovejas, alpacas, llamas y cabras, el cuero de reses sacrificadas para consumo, o el de animales de peletería criados en cautividad para tal fin pueden servirnos como algunos de estos casos.
Otros mamíferos se domestican para ser animales de compañía. El perro es sin duda el más cercano al hombre en la mayor parte del planeta y el más versátil (pastoreo, salvamento, seguridad, caza, espectáculo,…). Pero otros como el gato, el hámster, el cobaya, el conejo, el hurón, el colicorto, y algunos primates se cuentan entre las mascotas más extendidas por todo el mundo.
La caza es otra actividad de la que el hombre se beneficia de los mamíferos. Desde el principio de la humanidad hasta nuestros días, la caza ha supuesto y supone aún en algunas sociedades humanas un importante recurso alimenticio.
También se domestican animales para actividades lúdicas o deportivas: la práctica de la equitación supone el aprovechamiento de una de las especies de mamíferos más conocidas y apreciadas por casi todas las culturas y civilizaciones: el caballo (Equus caballus).
Los espectáculos circenses y los parques zoológicos también son dos empresas en las que el hombre se beneficia de los mamíferos y otros animales.
También algunos mamíferos salvajes suponen un beneficio directo para los humanos sin que estos intervengan para nada. Los murciélagos por ejemplo son el gran aliado contra las plagas de insectos en las cosechas o las áreas pobladas, controlando además por tanto a los vectores de ciertas enfermedades infecciosas y parasitarias que pondrían en serio riesgo la salud de las poblaciones.
En el último medio milenio, más de 80 especies distintas se han extinguido. La sobreexplotación de la tierra, la destrucción del hábitat, la fragmentación de los territorios por los que se distribuyen, la introducción de especies exóticas y otras presiones ejercidas por el hombre amenazan a los mamíferos de todo el mundo.
En la actualidad, la Unión Internacional para la Conservación de la Naturaleza y los Recursos Naturales (IUCN) considera que alrededor de 1000 especies más se encuentran bajo riesgo de desaparecer.
Algunos factores que contribuyen a la extinción de las especies son:

Un árbol es una planta, de tallo leñoso, que se ramifica a cierta altura del suelo.[1]​ El término hace referencia habitualmente a aquellas plantas cuya altura supera un determinado límite en la madurez, diferente según las fuentes: dos metros,[2]​ tres metros,[3]​[4]​ cinco metros[5]​ o los seis metros.[6]​ Además, producen ramas secundarias nuevas cada año, que parten de un único fuste o tronco, con clara dominancia apical,[7]​ dando lugar a una nueva copa separada del suelo. Algunos autores establecen un mínimo de 10 cm de diámetro en el tronco (la longitud de la circunferencia sería de unos 30 cm).[8]​ Las plantas leñosas que no reúnen estas características por tener varios troncos o por ser de pequeño tamaño son consideradas arbustos. 
Los árboles presentan una mayor longitud que otros tipos de plantas. Ciertas especies de vegetales (como las secuoyas) pueden superar los 100 m de altura, y llegar a vivir durante miles de años.[9]​ Los árboles han existido desde hace 370 millones de años. Se estima que hay poco más de tres billones de árboles maduros en el mundo.[10]​
Un estudio realizado por la Universidad de Yale y luego publicado en la revista Nature estima que en la Tierra hay alrededor de tres billones de árboles, y su cantidad se redujo un 46 % desde que comenzó la civilización humana,[11]​ dando en promedio 422 árboles por persona, pero cada año se pierden 15 000 millones de ejemplares.[12]​[10]​
Los árboles son un importante componente del paisaje natural debido a que previenen la erosión y proporcionan un ecosistema protegido de las inclemencias del tiempo en su follaje y por debajo de él. También desempeñan un papel importante a la hora de producir oxígeno y reducir el dióxido de carbono en la atmósfera, así como moderar las temperaturas en el suelo. También, son elementos en el paisajismo y la agricultura, tanto por su atractivo aspecto como por su producción de frutos en huertos de frutales. La madera de los árboles es un material de construcción, así como una fuente de energía primaria en muchos países en vías de desarrollo. Los árboles desempeñan también un importante papel en muchas mitologías del mundo.[13]​
Los árboles están formados por tres partes: la raíz, el tronco y la copa. Los dos primeros son los que diferencian, fundamentalmente, a un árbol de un arbusto. Los arbustos son más pequeños y no tienen un único tallo sino que están formados por varios. No obstante, ha de señalarse que algunas especies se pueden desarrollar como árboles pequeños o como arbustos, dependiendo de las circunstancias ambientales.
Las raíces fijan el árbol al suelo. Las raíces pueden tener una raíz principal, o bien, ser numerosas raíces en las que ninguna de ellas predomina, adoptando la forma de raíz ramificada fasciculada. Muchas raíces se combinan simbióticamente con micelios de hongos. Los hongos pueden conectar diferentes árboles y formar una red que transmite nutrientes y señales.[14]​[15]​[16]​ Las raíces aéreas son más raras dentro de los árboles, pero se dan en algunas especies que viven en entornos pantanosos, por ejemplo el mangle (Rhizophora).
El tronco es la estructura que sostiene la copa. Está formado por una capa exterior, la corteza, de espesor y color variables, que sirve para proteger el tejido vivo del árbol. El centro, más oscuro, es el duramen, formado  por células leñosas muertas de xilema. La albura es la parte más joven de la madera y más cercana a la corteza. Entre la albura y la corteza hay una sola capa de células por la que el tronco está creciendo, llamada cambium; se divide a su vez en dos partes: la interior formará el xilema (albura y duramen) y la exterior forma la corteza interna (floema).
Las características de la parte visible del tronco, la corteza, son una ayuda para identificar las especies arbóreas. Por ejemplo, el haya común la tiene gris y lisa hasta edades muy avanzadas; el pino piñonero de color pardo gris o pardo rojizo, forma surcos oscuros y grandes planchas como escamas; y el olmo común de color pardo gris, cuarteado por grietas, tanto horizontales como transversales.
Cuando se corta un tronco de forma transversal (tocón), pueden verse unos círculos concéntricos, los anillos, cuyo número muestra la edad del árbol, ya que cada año se forma un anillo de mayor o menor grosor, dependiendo de varios factores: los estrechos evidencian años de dificultades, como periodos fríos o secos. Los anillos anchos se generan durante los años en que los factores ambientales no han afectado adversamente su crecimiento. En los árboles de zonas templadas es más fácil diferenciar cada anillo, ya que en los trópicos con un clima regular a lo largo de todo el ciclo vegetativo, no se aprecia la formación de anillos anuales.[17]​
Las ramas suelen brotar a cierta altura del suelo, de manera que dejan una franja de tronco libre. Las ramas y hojas forman la copa. La copa adopta formas diversas, según las especies, distinguiéndose básicamente tres tipos: la alargada y vertical, la redondeada o la que se extiende de manera horizontal, como si fuera una sombrilla. Las ramas salen del tronco, se subdividen en ramas menores y en estas están las yemas y las hojas. De la yema nacerá una flor, una rama, u hojas. Las yemas que quedan en el extremo de las ramitas se llaman yemas terminales. Suelen estar cubiertas por escamas o catafilos como forma de protección.
A través de las hojas el árbol realiza la fotosíntesis y  por lo tanto debe alimentarse. Las raíces absorben el agua con minerales disueltos en ella. Suben por el tronco hasta las hojas. Allí reaccionan con el carbono procedente del anhídrido carbónico y forman azúcares. Luego el azúcar se transforma en celulosa, que es la materia prima de la madera. La hoja tiene una parte superior (haz) y otra inferior (envés), en el que se encuentran los estomas, pequeñas aberturas por las que penetra el anhídrido carbónico y por los que sale el agua sobrante y el oxígeno.
Las hojas son un elemento primordial a la hora de diferenciar entre las distintas especies arbóreas. Pueden señalarse cuatro tipos básicos de hojas: 
Pueden tener una sola forma (aovada, acorazonada, sagitadas, reniformes, lanceoladas, etc.) o bien ser recortada, lobulada, con entrantes más o menos marcados. El borde de la hoja (borde foliar) también es un elemento de distinción, pues puede ser entero (liso), crenado, dentado (con pequeños picos), aserrado y doble aserrado (como dientes de sierra), sinuado y lobulado; además, el borde puede ser espinoso (con espinas en el borde, como en el borde dentado punzante).
Acícula de abeto blanco, insertada individualmente.
Acículas de cedro del Líbano, varias insertadas juntas.
Conos y hojas escuamiformes del ciprés común.
Hojas del serbal de los cazadores, pinnatifolios impares alternos.
Hojas del castaño de Indias, opuestas, largamente pedunculadas, pinnatipalmeadas.
Hojas verticiladas de la catalpa.
Hojas simples, alternas, del haya común.
Los árboles, como las coníferas, son gimnospermas y se caracterizan por portar estructuras reproductivas llamadas conos, pero la mayoría de las especies son angiospermas, por lo que poseen órganos reproductivos en forma de flor. El gingko es un caso particular, ya que aunque es gimnosperma, no es una conífera. En algunas especies las flores surgen aisladas (magnolias), en otras se agrupan formando ramilletes llamados inflorescencias. No todos los árboles tienen flores completas, con órganos reproductores masculinos y femeninos en el mismo árbol (monoicos), como los abedules, nogales, robles); en algunas especies, cada ejemplar posee flores de un solo sexo (dioicos), como el gingko.
El tamaño de los árboles varía enormemente, la especie que se considera de mayor tamaño son las secuoyas. Las alturas de los árboles más altos del mundo han sido objeto de controversia y exageración. Las medidas modernas verificadas se hacen con aparatos láser, otros métodos de medida, o con medidas de cinta corrida realizada por investigadores o miembros de grupos como la U.S. Eastern Native Tree Society, han demostrado que los antiguos métodos de medición a menudo no son fiables, a veces producen exageraciones de 5 % a 15 % o más por encima de la verdadera altura. Pretensiones históricas de árboles que crecieron hasta más de 130 metros o incluso 150 ahora se consideran en gran medida poco fidedignas, y atribuidas al error humano. Mediciones históricas de árboles caídos realizadas con el tronco postrado en el suelo se consideran algo más fidedignas. Actualmente se acepta que las especies más altas son:
En cuanto a la edad, los árboles son los seres vivos que pueden vivir mayor cantidad de años. Los árboles más longevos son las secuoyas, que pueden llegar a vivir 2000-3000 años. Le siguen algunas especies pináceas propias de la alta montaña y el drago canario. Se ha calculado que el drago de Icod de los Vinos, aunque se le llama "milenario", tiene una edad 500 y los 600 años. Los árboles más antiguos se determinan por la dendrocronología o crecimiento de los anillos, que puede verse si el árbol es cortado, o en catas tomadas desde la corteza hacia el centro del tronco. La determinación exacta solo es posible para árboles que producen anillos de crecimiento, generalmente en climas con estaciones diferenciadas. Los árboles en climas tropicales, que no diferencia entre estaciones no tienen anillos distintivos.[17]​ También es solo posible en árboles que son sólidos por el centro. Muchos árboles viejos se van vaciando por dentro cuando están muertos al decaer la madera muerta. Para alguna de estas especies, la edad estimada se ha hecho sobre la base de extrapolar los ritmos de crecimiento actuales, pero los resultados son normalmente en gran medida fruto de la especulación. White (1998)[19]​ propone un método de estimar la edad de árboles grandes y antiguos en el Reino Unido, a través de la correlación entre el diámetro de la rama del árbol, carácter de crecimiento y edad.[20]​
Los dos árboles más antiguos son:
El grosor de un árbol es normalmente más fácil de medir que la altura, pues se trata solo de medir con cinta alrededor del tronco, tensarlo y así hallar la circunferencia. El árbol con el tronco más grueso del mundo es un baobab africano: 15,9 m, Glencoe Baobab (medido cerca del suelo) en Sudáfrica.[23]​ El célebre árbol del Tule en México, que es una especie de ahuehuete (Taxodium mucronatum): 11,62 m, Árbol del Tule, Santa María del Tule, Oaxaca, México.[24]​
Hay árboles por todo el mundo, siendo particularmente ricas en diversidad de especies arbóreas las franjas tropicales. Los árboles tropicales se hallan en las selvas tropicales y ecuatoriales de América Central, América del Sur, África y Asia. Pero también hay árboles en las zonas templadas y llega hasta latitudes muy altas. En este último caso, los bosques suelen presentar menos diversidad de especies y estar formados por una o pocas especies.

Los árboles son parte predominante del ecosistema de los continentes debido a que previenen la erosión, constituyendo los elementos primordiales del paisaje, la agricultura, los llamados ecosistemas forestales, los bosques y las selvas, además de encontrarse dispersos en ambientes como las sabanas o las orillas fluviales. Los árboles tienen gran importancia ecológica, puesto que fijan el suelo, impidiendo que la delgada capa fértil quede barrida por las lluvias o los vientos. Proporciona refugio y alimento a numerosas especies animales.El grado de humedad y la naturaleza del terreno suelen determinar qué tipo de bosque se dará, y no solo la temperatura o la latitud. Cuanto mayor sea la humedad, más espeso será el bosque. La aridez determina que los árboles se encuentren en ejemplares aislados o bosquecillos en torno a una fuente de agua, como un pozo o un río. Dependiendo de la altura se darán unas especies u otras. Normalmente en las partes bajas habrá bosques de frondosas como robles, hayas y castaños, y más arriba aparecerán las coníferas. Cuanta mayor sea la altura, más empezará a ralear el terreno, hasta que llegue un momento en que desaparezcan los árboles y solo queden hierbas perennes y líquenes. Esa línea máxima que pueden alcanzar los árboles es la llamada línea de árboles. Dependiendo de la exposición al sol, los vientos o la pluviosidad, puede darse la circunstancia de que en una ladera crezcan los árboles hasta una altura y en la otra, más expuesta, la línea de árboles esté a menor altura.
Varios biotopos se definen en gran medida por los árboles que los habitan, como por ejemplo el bosque templado de caducifolios. Un paisaje de árboles disperso por un amplio espacio es la sabana. Un bosque de gran edad se llama bosque primario.
Hay diversos tipos de clasificaciones dentro de las especies arbóreas. Por el tipo de hoja, se puede distinguir entre árboles caducifolios o planifolios, que pierden su follaje durante una parte del año, normalmente la estación fría en los climas templados, y la árida en los climas cálidos y áridos, y árboles perennifolios, que no es que no pierdan las hojas, sino que no las pierden todas a la vez ni tampoco con ritmo anual, sino más largo.
La principal distinción es la que se establece entre árboles de crecimiento monopódico y árboles de crecimiento simpódico. En los monopódicos el crecimiento en longitud se basa en un tallo principal vertical del que salen, con ángulos marcados, ramas laterales subordinadas, de menor grosor. El crecimiento monopódico da lugar a un porte piramidal, como el que es característico de las coníferas. En el crecimiento simpódico, las ramas derivadas se desarrollan cerca del ápice (extremo) de aquellas en que se asientan, sustituyéndolas en el crecimiento. Las copas de estos árboles suelen ser más esféricas o cilíndricas y menos piramidales.
En inglés, pero habitualmente no en castellano, se trata de árboles a las palmeras (palm trees). El biotipo palmeroide se presenta en varios grupos de plantas, destacando las cícadas (Cycadophyta) y, especialmente, las angiospermas de la familia arecáceas (Arecaceae).
Un árbol es una forma de planta que aparece en muchos órdenes y familias de plantas diferentes. Los árboles muestran una variedad de formas de crecimiento, formas de hojas, características de la corteza y órganos reproductivos.
La forma de árbol ha evolucionado separadamente en clases de plantas sin parentesco, en respuesta a unos desafíos medioambientales similares, haciendo de él un ejemplo clásico de evolución en paralelo. Con unas 100 000 especies arbóreas aproximadas, el número de especies en todo el mundo puede suponer el 25 % de todas las especies de plantas vivas.[25]​ La mayoría de las especies arbóreas crecen en regiones tropicales del mundo y muchas de estas áreas no han sido aún investigadas por los botánicos, haciendo de la diversidad de especies y áreas de distribución se entienden de manera fragmentaria.[26]​
Actualmente (abril de 2007) la datación de los primeros árboles conocidos es del rango de los 380 millones de años antes del presente, en pleno período devónico cuando los animales vertebrados apenas comenzaban a colonizar las tierras emergidas. Esos árboles, del género Wattieza, que poblaban zonas actualmente correspondientes a Sur y Norteamérica, probablemente enriquecieron la atmósfera con oxígeno producido mediante la fotosíntesis favoreciendo de este modo el desarrollo de especies superiores de animales fuera de los mares. Los árboles más antiguos eran helechos arborescentes, equisetáceas y licofitas, que crecieron en bosques en el período carbonífero; aún sobreviven helechos arborescentes, pero las únicas equisetáceas y licofitas que quedan no tienen forma de árbol. Más tarde, en el período Triásico, aparecieron las coníferas, los ginkgos, las cícadas y otras gimnospermas, y posteriormente las plantas con flor en el período Cretácico. La mayor parte de las especies actuales son plantas con flor (angiospermas) y coníferas.
Plantas con el biotipo de árbol se encuentran en todas las clases de la superdivisión Spermatophyta (las antes llamadas fanerógamas), salvo en las cícadas (Cycadophyta), que son de biotipo Palmeroide.
Se llama dendrología al estudio de los árboles en aquello que les es propio como tales, y silvicultura al estudio científico y la práctica de su cuidado o cultivo, del que se ocupan los ingenieros forestales.
El ser humano explota los árboles de diferentes maneras. Desde la antigüedad, la madera se ha usado como combustible. Se habla de especies forestales, que son aquellas que suministran madera y productos derivados. La madera de los árboles es un material común de construcción de edificios y de muebles. La pulpa se emplea para la industria papelera.
Hay árboles frutales, que se caracterizan por producir frutos comestibles y con tal finalidad se plantan por el hombre.
Un tercer tipo de uso es el adorno u ornamento de fincas particulares y espacios públicos. Se habla así de especies ornamentales. Los árboles forman parte del mobiliario urbano: en las ciudades se utilizan los árboles en calles, parques y jardines, como algo  ornamental y creando así puntos de descanso, refresco y esparcimiento para los ciudadanos.
Los árboles han jugado un importante papel en la religión, en la magia y la industria, como por ejemplo el árbol de Navidad, y tienen también un gran simbolismo en la filosofía y la cultura, por ejemplo el árbol de la sabiduría. Asimismo tienen un gran protagonismo en relación con el calentamiento global.
En diversas culturas el árbol se ha considerado sagrado. En la iconografía cristiana tiene asociada toda una iconografía. Es el eje entre los mundos inferior, terrestre y celeste. Coincide con la cruz de la Redención. La cruz está representada muchas veces como árbol de la vida. Este árbol de la vida surge por primera vez en el arte de los pueblos orientales; es el hom o árbol central colocado entre dos animales afrontados o dos seres fabulosos; es un tema mesopotámico que pasó a Extremo Oriente y Occidente por medio de los persas, árabes y bizantinos. Para las teogonías orientales el hom tiene un sentido cósmico, está situado en el centro del Universo y se mueve con la idea del dios creador. Dos árboles míticos o simbólicos mencionados por primera vez en la Biblia en el libro del Génesis. Estos árboles serían llamados "árbol del conocimiento del bien y el mal" y el "árbol de la vida". En el paraíso el árbol de la vida estaba en medio del huerto, pero protegido de los hombres.[27]​ En el claustro de la iglesia de Santa María la Real de Nieva, se encuentra la representación del hom oriental como símbolo del árbol de la vida en algunos capiteles :
Los budistas, hinduistas y jainistas consideran sagrado cierto tipo de higuera llamada por ello higuera sagrada bajo la cual, creen, Buda alcanzó el nirvana. Yggdrasil es el árbol mítico de los nórdicos, un fresno perenne al que consideraban el "árbol de la vida", o "fresno del universo". Los antiguos sajones tenían también un árbol sagrado, Irminsul, que Carlomagno ordenó destruir cuando los atacó.[28]​
En la mitología grecorromana, distintos tipos de árboles y otras plantas han sido consagrados a diferentes divinidades:
Los árboles están desapareciendo de forma masiva de la superficie de la tierra en un proceso de deforestación sin precedentes.  Se calcula que un tercio de los bosques del mundo han desaparecido. Se debe en parte a la sobreexplotación que padecen, por ejemplo las selvas tropicales, pero también a los incendios forestales, la mayor parte de los cuales son producidos por el hombre, bien de forma intencionada, bien por negligencia. Además, el hombre efectúa talas intensivas para hacer sitio a otro tipo de cultivo que da un rendimiento económico mayor a corto plazo, por ejemplo, para abrir pastos para la ganadería o para el cultivo de grandes extensiones de soja. Las consecuencias negativas son: la pérdida de hábitats para diversas especies animales y vegetales, la erosión, al dejar el terreno libre a la acción desecante del viento y la libre circulación de las aguas, lo que provoca que se pierda la capa fértil de suelo y ocasiona que el terreno se vaya desertificando.
La solución, además del abandono de determinadas prácticas, como la quema intencionada del bosque para obtener pastos, pasa por una explotación racional, que implique no solo tala sino también reforestación con ejemplares jóvenes que constituyan el bosque del futuro. El Programa de las Naciones Unidas para el Medio Ambiente ha iniciado una campaña mundial Plantemos para el Planeta con el objetivo de plantar 7000 millones de árboles, o sea 1 árbol por habitante de la tierra para finales de 2009. Además, se protegen extensiones de aquellas áreas más ricas en biodiversidad, o de las especies endémicas, muchas de ellas en peligro de extinción.
También hay riesgos naturales que amenazan los bosques, como el fuego, las plagas de insectos y enfermedades.

El estaño es un elemento químico de símbolo Sn (del latín stannum) y número atómico 50. Está situado en el grupo 14 de la tabla periódica de los elementos. Se conocen 10 isótopos estables. Su principal mena es la casiterita.
Es un elemento sólido a temperatura ambiente (20 °C). Es maleable, y se oxida de forma superficial a temperatura ambiente. Este efecto lo hace resistente a la corrosión mediante pasivación. Por tanto se utiliza para recubrir otros metales, protegiéndolos así de la corrosión. Se encuentra además en muchas aleaciones. 
Al doblar una barra de este metal se produce un sonido característico llamado grito del estaño, producido por la fricción de los cristales que la componen. Una de sus características más llamativas es que bajo determinadas condiciones térmicas sufre la peste del estaño.
El estaño puro tiene dos variantes alotrópicas: el estaño gris, polvo no metálico, semiconductor, de estructura cúbica y estable a temperaturas inferiores a 13,2 °C, que es muy frágil y tiene un peso específico más bajo que el blanco; y el estaño blanco, el normal, metálico, conductor eléctrico, de estructura tetragonal y estable a temperaturas por encima de 13,2 °C.
El estaño es un maleable blando, dúctil y un metal de color blanco plateado muy cristalino. Cuando se dobla una barra de estaño, puede oírse un sonido crepitante conocido como el "grito del estaño" debido a la gemelos de los cristales.[1]​ El estaño se funde a temperaturas bajas de aproximadamente 232 grados Celsius (449,6 °F), las más bajas del grupo 14. El punto de fusión se reduce aún más a 177,3 grados Celsius (351,1 °F) para las partículas de 11 nm.[2]​[3]​
El β-estaño (la forma metálica, o estaño blanco, estructura BCT), que es estable a la temperatura ambiente y por encima de ella, es maleable. En cambio, el α-estaño (forma no metálica, o estaño gris), que es estable por debajo de 13,2 grados Celsius (55,8 °F), es frágil. El α-estaño tiene una estructura cristalina cúbica similar a la del diamante, el silicio o el germanio. El α-estaño no tiene propiedades metálicas porque sus átomos forman una estructura covalente en la que los electrones no pueden moverse libremente. Se trata de un material pulverulento de color gris apagado que no tiene más usos comunes que unas pocas aplicaciones especializadas de semiconductores.[1]​ Estos dos alótropos, el α-estaño y el β-estaño, se conocen más comúnmente como estaño gris y estaño blanco, respectivamente. Otros dos alótropos, γ y σ, existen a temperaturas superiores a 161 grados Celsius (321,8 °F)  y presiones superiores a varios GPa.[4]​ En condiciones de frío, el estaño β tiende a transformarse espontáneamente en estaño α, un fenómeno conocido como "plaga del estaño" o "enfermedad del estaño". Algunas fuentes no verificables dicen también que, durante la campaña rusa de 1812 de Napoleón, las temperaturas se volvieron tan frías que los botones de estaño de los uniformes de los soldados se desintegraron con el tiempo, contribuyendo a la derrota de la Grande Armée,[5]​ una leyenda persistente que probablemente no tenga ningún trasfondo en hechos reales. [6]​[7]​[8]​
Aunque la temperatura de transformación α-β es nominalmente 13,2 grados Celsius (55,8 °F), las impurezas (por ejemplo, Al, Zn, etc.) reducen la temperatura de transición muy por debajo de 0 grados Celsius (32,0 °F) y, al añadir antimonio o bismuto, la transformación podría no producirse en absoluto, aumentando la durabilidad del estaño. [9]​
Los grados comerciales de estaño (99,8%) resisten la transformación debido al efecto inhibidor de las pequeñas cantidades de bismuto, antimonio, plomo y plata presentes como impurezas. Los elementos de aleación como el cobre, el antimonio, el bismuto, el cadmio y la plata aumentan su dureza. El estaño tiende a formar con bastante facilidad fases intermetálicas duras y quebradizas, que suelen ser indeseables. No forma amplios rangos de solución sólida en otros metales en general, y pocos elementos tienen una solubilidad sólida apreciable en el estaño. Sin embargo, se dan sistemas simples de eutéctico con bismuto, galio, plomo, talio y zinc.[9]​
El estaño se convierte en un superconductor por debajo de 3,72 K[10]​ y fue uno de los primeros superconductores que se estudiaron; el efecto Meissner, uno de los rasgos característicos de los superconductores, se descubrió por primera vez en los cristales de estaño superconductores.[11]​
El estaño resiste la corrosión, pero puede ser atacado por ácidos y álcalis. El estaño puede ser muy pulido y se utiliza como capa protectora para otros metales.[1]​ Una capa protectora de óxido (pasivación) impide una mayor oxidación, la misma que se forma en el estaño y otras aleaciones de estaño.[12]​ El estaño actúa como catalizador cuando el oxígeno está en solución y ayuda a acelerar la reacción química.[1]​
El estaño tiene diez isótopos estables, con masas atómicas de 112, 114 a 120, 122 y 124, el mayor número de cualquier elemento. De ellos, los más abundantes son el 120Sn (casi un tercio de todo el estaño), el 118Sn y el 116Sn, mientras que el menos abundante es el 115Sn. Los isótopos con número de masa par no tienen  espín nuclear, mientras que los impares tienen un espín de +1/2. El estaño, con sus tres isótopos comunes 116Sn, 118Sn, y 120Sn, es uno de los elementos más fáciles de detectar y analizar por espectroscopia de resonancia magnética nuclear, y sus  desplazamientos químicos están referenciados frente a SnMe4.[13]​
Se cree que este gran número de isótopos estables es un resultado directo del número atómico 50, un "número mágico" en la física nuclear. El estaño también se presenta en 31 isótopos inestables, que abarcan todas las masas atómicas restantes de 99 a 139. Aparte del 126Sn, con una vida media de 230 000 años, todos los radioisótopos tienen una vida media inferior a un año. El radiactivo 100Sn, descubierto en 1994, y el 132Sn son dos de los pocos núclidos con un núcleo " doblemente mágico": a pesar de ser inestables, al tener relaciones protón-neutrón muy asimétricas, representan puntos finales más allá de los cuales la estabilidad cae rápidamente. [14]​ Se han caracterizado otros 30  isómeros metaestables para isótopos entre 111 y 131, siendo el más estable el 121mSn con una vida media de 43,9 años.[15]​
Las diferencias relativas en las abundancias de los isótopos estables del estaño pueden explicarse por sus diferentes modos de formación en la nucleosíntesis estelar. El  116Sn hasta el 120Sn inclusive se forman en el s (captura lenta de neutrones) en la mayoría de las estrellas y, por tanto, son los isótopos más comunes, mientras que el 122Sn y el 124Sn sólo se forman en el r-proceso (captura rápida de neutrones) en supernovas y son menos comunes. (Los isótopos 117Sn hasta 120Sn también reciben contribuciones del proceso r). Por último, los isótopos más raros ricos en protones, 112Sn, 114Sn y 115Sn, no pueden producirse en cantidades significativas en los procesos s o r y se consideran entre los núcleos p, cuyos orígenes aún no se conocen bien. Algunos de los mecanismos especulados para su formación incluyen la captura de protones así como la fotodesintegración, aunque el 115Sn también podría producirse parcialmente en el proceso s, tanto directamente, como en calidad de hija de los 115In de larga vida. [16]​
El uso del estaño comenzó en el Cercano Oriente y los Balcanes alrededor del 2000 a. C., utilizándose en aleación con el cobre para producir un nuevo material, el bronce, dando así origen a la denominada Edad de Bronce. La importancia de la nueva aleación, con la que se fabricaban armas y herramientas más eficaces que las de piedra o de hueso habidas hasta entonces, originó durante toda la Antigüedad un intenso comercio a largas distancias con las zonas donde existían yacimientos de estaño.[17]​ 
Los primeros objetos de bronce tenían un contenido de estaño o arsénico inferior al 2%, por lo que se cree que son el resultado de una aleación no intencionada debida al contenido de trazas de metal en el mineral de cobre.[18]​ La adición de un segundo metal al cobre aumenta su dureza, reduce la temperatura de fusión y mejora el proceso de fundición al producir una masa fundida más fluida que se enfría hasta obtener un metal más denso y menos esponjoso. [18]​ Esta fue una innovación importante que permitió las formas mucho más complejas fundidas en moldes cerrados de la Edad de Bronce. Los objetos de bronce arsenical aparecen primero en el Cercano Oriente, donde el arsénico se encuentra comúnmente en asociación con el mineral de cobre, pero los riesgos para la salud se dieron cuenta rápidamente y la búsqueda de fuentes de los minerales de estaño, mucho menos peligrosos, comenzó a principios de la Edad del Bronce.[19]​ Esto creó la demanda del raro metal de estaño y formó una red comercial que unió las fuentes distantes de estaño con los mercados de las culturas de la Edad de Bronce.[cita requerida]
La casiterita (SnO2), la forma de óxido de estaño, fue muy probablemente la fuente original de estaño en la antigüedad. Otras formas de minerales de estaño son los sulfuros menos abundantes, como la estannita, que requieren un proceso de fundición más complicado. La casiterita suele acumularse en canales aluviales como  depósitos de placer porque es más dura, más pesada y más resistente químicamente que el granito que la acompaña.[18]​ La casiterita suele ser de color negro o generalmente oscuro, y estos depósitos pueden verse fácilmente en las riberas de los ríos. Los  depósitos aluviales pueden haber sido recogidos y separados incidentalmente por métodos similares al bateo de oro.[20]​
El estaño se obtiene del mineral casiterita donde se presenta como óxido (óxido de estaño (IV) o dióxido de estaño). Dicho mineral se muele y se enriquece en dióxido de estaño por flotación, después se tuesta y se calienta con coque en un horno de reverbero con lo cual se obtiene el metal.[21]​[22]​[23]​
Las aleaciones con base de estaño, también conocidas como metales blancos, generalmente contienen cobre, antimonio y plomo. Estas aleaciones tienen diferentes propiedades mecánicas, dependiendo de su composición.[24]​
Algunas aleaciones de estaño, cobre y antimonio son utilizadas como materiales antifricción en cojinetes, por su baja resistencia de cizalladura y su reducida adherencia.[24]​
Las aleaciones estaño y plomo se comercializan en varias composiciones y puntos de fusión, siendo la aleación eutéctica aquella que tiene un 61,9 % de estaño y un 38,1 % de plomo, con un punto de fusión de 183 °C.[25]​ El resto de aleaciones estaño-plomo funden en un rango de temperaturas en el cual hay un equilibrio entre la fase sólida y la fase líquida durante los procesos de fusión y de solidificación, dando lugar a la segregación de la fase sólida durante la solidificación y, por tanto, a estructuras cristalinas diferentes. La aleación eutéctica, que necesita menor temperatura para llegar a la fase líquida es muy utilizada en la soldadura blanda de componentes electrónicos para disminuir las probabilidades de daño por sobrecalentamiento de dichos componentes. Algunas aleaciones basadas en estaño y plomo tienen además pequeñas proporciones de antimonio (del orden del 2,5 %). El principal problema de las aleaciones con plomo es el impacto ambiental potencial de sus residuos, por lo que están en desarrollo aleaciones libres de plomo, como las aleaciones de estaño-plata-cobre o algunas aleaciones estaño-cobre.
El peltre es una aleación de estaño, plomo y antimonio utilizada para utensilios decorativos. El estaño también es utilizado en aleaciones de prótesis dentales, aleaciones de bronce y aleaciones de titanio y circonio.[24]​
Los principales productores de estaño del mundo son China, Malasia, Perú, Indonesia, Bolivia y Brasil[26]​ (especialmente en el estado de Minas Gerais)[27]​
Tanto el estaño metálico como sus compuestos orgánicos e inorgánicos, ya sean formados de manera natural o en sus usos industriales, puede producir efectos tóxicos sobre el medio ambiente y los seres vivos expuestos a ellos.
El estaño es liberado en el medio ambiente por procesos naturales y por las actividades humanas, tales como la minería, la combustión de petróleo y carbón, además de las actividades industriales asociadas a la producción y usos del estaño.
El estaño metálico cuando se encuentra en la atmósfera en forma gaseosa se adhiere a las partículas de polvo, las cuales pueden ser movilizadas por la acción del viento la lluvia o la nieve.
Cuando se libera el estaño metálico en el medio ambiente, este se puede unir con el cloro, azufre u oxígeno para formar compuestos inorgánicos de estaño, tales como el cloruro de estaño, sulfuro de estaño, u dióxido de estaño. Este tipo de compuestos no pueden ser degradados y solo pueden cambiar su forma química, de manera que son adheridos por el suelo y los sedimentos o son disueltos en el agua.
Cuando se combina con el carbono puede formar compuestos orgánicos tales como dibutilestaño, tributilo de estaño y el trifenilestaño. Este tipo de compuestos pueden ser acumulados en el suelo o en el agua, o ser degradados a compuestos inorgánicos por la acción de la luz solar o las bacterias. El tiempo de permanencia en el medio de estos compuestos es variable en función del compuesto, pudiendo ser desde días hasta meses en el agua, y años si se encuentran en el suelo. Debido a su forma química los compuestos orgánicos de estaño también pueden bioacumularse al ser asimilado por el metabolismo de los seres vivos, sufriendo un proceso de biomagnificación a lo largo de las diferentes redes tróficas.
Las principales vías de intoxicación con estaño en humanos son:
El estaño metálico en sí no es muy tóxico para el ser humano ya que en el tracto digestivo no se absorbe de manera efectiva, pero la inhalación de los vapores de estaño sí que es nociva para el aparato respiratorio.
La ingestión de grandes cantidades de compuestos inorgánicos de estaño puede producir dolores de estómago, anemia, y alteraciones en el hígado y los riñones.
La inhalación o la ingesta de compuestos orgánicos de estaño (tales como el trimetilestaño y el trietilestaño) pueden interferir con el funcionamiento del sistema nervioso y el cerebro. En casos graves, puede causar la muerte. Otros compuestos orgánicos de estaño (tales como el dibutilestaño y el tributilestaño) afectan el sistema inmunitario y a la reproducción en animales, aunque esto no se ha evaluado aún en seres humanos.
Tanto compuestos orgánicos como inorgánicos pueden producir irritación por contacto con la piel o los ojos.
Los límites legales de contenido de estaño inorgánico marcados por la Unión Europea son:

El tiempo atmosférico o meteorológico es el estado de la atmósfera en un momento y lugar determinado[1]​ definido por diversas variables meteorológicas[2]​ como la temperatura, la presión, el viento, la radiación solar, la humedad y la precipitación.[3]​ [4]​ La mayoría de los fenómenos del tiempo ocurren en la troposfera,[5]​[6]​ la capa por debajo de la Estratósfera, siendo la capa inferior de la atmósfera que está en contacto con la superficie. Es importante diferenciar tiempo de clima, ya que este último se refiere a las condiciones atmosféricas promedio que caracterizan a un lugar.[7]​ Esos promedios suelen realizarse en periodos de varias décadas. 
El tiempo es impulsado por la presión de aire, la temperatura y las diferencias de humedad entre un lugar y otro.  Estas diferencias pueden ocurrir debido al ángulo del sol en cualquier sitio particular, el cual varía por latitud desde los trópicos. [8]​ El fuerte contraste de temperatura entre el aire polar y el tropical da origen a las circulaciones atmosféricas de mayor escala: la Célula de Hadley, la célula de Ferrel, la célula polar y la corriente en chorro.[8]​ Sistemas de tiempo en las latitudes medias, como los ciclones extratropicales, son causados por inestabilidades del flujo de corriente en chorro. Debido a que el eje de la Tierra está inclinado en relación con su plano orbital, la luz solar incide en  ángulos diferentes en los distintos meses del año. Sobre la superficie de la Tierra, las temperaturas normalmente varían anualmente entre ±40 °C.[8]​  A lo largo de miles de años, cambios en la órbita terrestre pueden afectar la cantidad y distribución de la energía solar recibida por la Tierra, influenciando así el clima a largo plazo y el cambio climático global.
Las diferencias de temperatura de la superficie a su vez causan diferencias de presión.  Las altitudes más elevadas son más frías que las bajas debido a diferencias en calentamiento de compresión. El pronóstico del tiempo es la aplicación de la ciencia y tecnología para pronosticar el estado de la atmósfera para un momento futuro y una ubicación dada.  El sistema es un caótico; cambios tan pequeños a una parte del sistema puede crecer para tener efectos grandes en todo el sistema. A través de la historia, han existido intentos humanos de controlar el tiempo y existe evidencia de que las actividades humanas como la agricultura y la industria han modificado los patrones atmosféricos.
El estudio sobre cómo funciona el tiempo en otros planetas han sido útiles en comprender su funcionamiento en la Tierra.  Un lugar famoso en el sistema solar, es la Gran Mancha Roja de Júpiter, es una tormenta anticiclónica que existe desde al menos 300 años. No obstante, el tiempo no se limita a los cuerpos planetarios. La corona de una estrella se pierde constantemente en el espacio, creando lo que esencialmente es una muy delgada atmósfera a través del sistema solar. El transporte de masa expulsado del Sol se conoce como viento solar.
Casi la totalidad de la energía solar que genera todos los cambios atmosféricos procede de la radiación solar, es decir, de la insolación. Pero los rayos solares no calientan directamente al aire atmosférico por la propiedad del aire en su conjunto de la diatermancia que explica que la atmósfera se deja atravesar por los rayos solares sin prácticamente calentarse. Así el calentamiento de la atmósfera por la radiación solar es indirecto: los rayos solares calientan primero la litósfera (de manera rápida) y la hidrósfera (más lentamente que la litosfera). Cuando tanto la litósfera como la hidrósfera se han calentado, van cediendo ese calor a la atmósfera, la primera rápidamente y la segunda más lentamente, todo ello de acuerdo a lo explicado sobre el calentamiento de la litosfera y la hidrosfera en el artículo ya citado (diatermancia). La imagen del delta del río Amazonas que aquí se presenta está tomada durante la mañana. Si la comparásemos con una imagen similar durante el anochecer ese mismo día (ello se hace posible, no en una imagen del espectro visible, sino en una imagen infrarroja) veríamos que la situación se invierte, apareciendo mayor condensación sobre los ríos que sobre las tierras.
Debido a que el eje de la Tierra está inclinado con respecto a su plano orbital, la luz solar incide en diferentes ángulos en diferentes épocas del año. En junio, el hemisferio norte está inclinado hacia el sol, por lo que en cualquier latitud del hemisferio norte la luz solar cae más directamente en ese lugar que en diciembre.[10]​ Este efecto provoca estaciones. Durante miles a cientos de miles de años, los cambios en los parámetros orbitales de la Tierra afectan la cantidad y distribución de la energía solar recibida por la Tierra e influyen en el clima a largo plazo. (Ver ciclos de Milankovitch).[11]​
El calentamiento solar desigual (la formación de zonas de gradientes de temperatura y humedad, o frontogénesis) también puede deberse al clima mismo en forma de nubosidad y precipitación.[12]​ Las altitudes más altas son típicamente más frías que las altitudes más bajas, lo que es el resultado de una temperatura superficial más alta y un calentamiento por radiación, que produce la tasa de caída adiabática. [13]​[14]​ En algunas situaciones, la temperatura aumenta con la altura. Este fenómeno se conoce como inversión y puede hacer que las cimas de las montañas sean más cálidas que los valles que se encuentran debajo. Las inversiones pueden conducir a la formación de niebla y, a menudo, actúan como un límite que suprime el desarrollo de tormentas eléctricas. A escalas locales, las diferencias de temperatura pueden ocurrir porque diferentes superficies (como océanos, bosques, capas de hielo u objetos artificiales) tienen diferentes características físicas como reflectividad, rugosidad o contenido de humedad.
Además de la radiación solar existen tres fuentes menores de energía térmica que pueden calentar la atmósfera:
El tiempo cambia movido por las diferencias de energía solar percibida en cada área diferenciada de acuerdo con una escala de tiempo que va desde menos de un día (diferencias de radiación entre el día y la noche) hasta períodos estacionales a lo largo del año.  Las estaciones meteorológicas miden las distintas variables locales del tiempo como la temperatura, la presión atmosférica, la humedad, la nubosidad, el viento y el monto pluviométrico de las lluvias o precipitaciones. Conocidas estas variables directas, se pueden averiguar otras derivadas, como la presión de vapor de condensación, la temperatura de sensación o la temperatura de bochorno.
Mediante redes de estaciones meteorológicas locales, estaciones en barcos y satélites meteorológicos, la meteorología intenta averiguar las variables meteorológicas en los vértices de una malla tridimensional del menor tamaño posible. A partir de estas condiciones iniciales y aplicando las leyes de la física, se intenta predecir la evolución del tiempo. Para ello hay que usar potentes ordenadores que se encargan de realizar los cálculos usando un modelo predictivo de tipo empírico.
La realización de pronósticos meteorológicos a una escala regional y, especialmente, a escala local, constituye hoy en día una actividad sumamente importante y extendida en casi todo el mundo y en numerosas actividades. La organización de la aviación civil (horarios, previsiones, alternativas de vuelo, etc.) depende en gran manera, y cada vez más, de los pronósticos meteorológicos muy detallados. Lo mismo sucede con otros tipos de actividades (agricultura, transporte, comercio, servicios de todo tipo, etc.). Esta actividad se basa en los datos suministrados por las estaciones meteorológicas estratégicamente ubicadas e intercomunicadas entre sí y por la información obtenida en tiempo real de multitud de satélites meteorológicos, principalmente, satélites geoestacionarios, drones (vuelos no tripulados) y otros medios de obtención de datos atmosféricos. 
Un ejemplo de los datos obtenidos casi en tiempo real y con imágenes en secuencia del hemisferio occidental y sectores del mismo (que permiten ver el movimiento de las masas nubosas, desplazamiento y energía transportada por las nubes, etc.) son los que proporciona el sitio web de la NASA de satélites geoestacionarios Goes (http://www.goes.noaa.gov/). Una visión animada de la secuencia de imágenes del Caribe y Atlántico al norte del ecuador y que se actualizan cada 30 minutos, puede verse en: [1].
El clima, visto desde una perspectiva antropológica, es algo que todos los humanos en el mundo experimentan constantemente a través de sus sentidos, al menos mientras están afuera. Hay conocimientos construidos social y científicamente sobre qué es el clima, qué lo hace cambiar, el efecto que tiene en los humanos en diferentes situaciones, etc.[15]​ Por lo tanto, el clima es algo sobre lo que la gente suele comunicarse. Los países cuentan con un Servicio Meteorológico Nacional que generalmente produce un informe anual de muertes, lesiones y costos totales de daños que incluyen cultivos y propiedades. Por ejemplo en Estados Unidos a partir de 2019, los tornados han tenido el mayor impacto en los seres humanos con 42 muertes y han costado daños a cultivos y propiedades de más de 3000 millones de dólares.[16]​
El clima ha jugado un papel importante y, a veces, directo en la historia de la humanidad. Aparte de los cambios climáticos que han provocado la deriva gradual de las poblaciones (por ejemplo, la desertificación del Medio Oriente y la formación de puentes terrestres durante los períodos glaciares), los fenómenos meteorológicos extremos han provocado movimientos de población a menor escala y se han inmiscuido directamente en los acontecimientos históricos. Uno de esos eventos es la salvación de Japón de la invasión de la flota mongola de Kublai Khan por los vientos Kamikaze en 1281.[17]​ Las reclamaciones francesas sobre Florida llegaron a su fin en 1565 cuando un huracán destruyó la flota francesa, lo que permitió a España conquistar Fort Caroline.[18]​ Más recientemente, el huracán Katrina redistribuyó a más de un millón de personas de la costa central del Golfo en otras partes de los Estados Unidos, convirtiéndose en la diáspora más grande en la historia de los Estados Unidos.[19]​
La Pequeña Edad de Hielo produjo la pérdida de cosechas y hambrunas en Europa. Durante el período denominado la Fluctuación  Grindelwald (1560-1630), eventos producto de actividades volcánicas[20]​ parece que produjeron eventos meterorológicos extremos.[21]​  Estos incluyeron sequías, tormentas y ventiscas fuera de temporada, además de causar la expansión del glaciar suizo Grindelwald. La década de 1690 vio la peor hambruna en Francia desde la Edad Media. Finlandia sufrió una hambruna severa en 1696-1697, durante la cual murió aproximadamente un tercio de la población finlandesa.[22]​
La aspiración de controlar el clima es evidente a lo largo de la historia de la humanidad: desde los antiguos rituales destinados a traer lluvia para las cosechas hasta la Operación Popeye del ejércoto de los Estados Unidos, un intento de interrumpir las líneas de suministro alargando el monzón de Vietnam del Norte. Los intentos más exitosos de influir en el clima involucran la siembra de nubes; incluyen las técnicas de dispersión de niebla y nubes stratus bajas empleadas por los principales aeropuertos, técnicas utilizadas para aumentar las precipitaciones invernales sobre las montañas y técnicas para suprimir el granizo.[23]​ Un ejemplo reciente de control del clima fue la preparación de China para los Juegos Olímpicos de Verano de 2008. China disparó 1.104 cohetes de dispersión de lluvia desde 21 sitios en la ciudad de Beijing en un esfuerzo por mantener la lluvia alejada de la ceremonia inaugural de los juegos el 8 de agosto de 2008. Guo Hu, jefe de la Oficina Meteorológica Municipal de Beijing (BMB), confirmó el éxito de la operación con 100 milímetros cayendo en la ciudad de Baoding de la provincia de Hebei, al suroeste y en el distrito de Fangshan de Beijing registrando una precipitación de 25 milímetros.[24]​
Si bien no existen pruebas concluyentes de la eficacia de estas técnicas, existen numerosas pruebas de que la actividad humana, como la agricultura y la industria, produce modificaciones climáticas involuntarias: [23]​
Los efectos de la modificación climática inadvertida pueden representar graves amenazas para muchos aspectos de la civilización, incluidos los ecosistemas, los recursos naturales, la producción de alimentos y fibras, el desarrollo económico y la salud humana.[27]​
El estudio de cómo funciona el clima en otros planetas ha resultado útil para comprender cómo funciona en la Tierra.[28]​ El clima en otros planetas sigue muchos de los mismos principios físicos que el clima en Tierra, pero ocurre en diferentes escalas y en atmósferas que tienen diferente composición química. La misión  Cassini-Huygens  a Titán descubrió nubes formadas a partir de metano o etano que depositan lluvia compuesta de metano líquido y otros compuestos orgánicos.[29]​  La atmósfera de la Tierra incluye seis zonas de circulación latitudinales, tres en cada hemisferio.[30]​  En contraste el aspecto en franjas de Júpiter indica la presencia de numerosas zonas,[31]​ Titán tiene una sola corriente en chorro cerca del paralelo 50 de latitud norte,[32]​ y Venus tiene una sola corriente en chorro cerca del ecuador.[33]​
Uno de los hitos más famosos del Sistema Solar, la Gran Mancha Roja de Júpiter, es una tormenta anticiclónica que se sabe que ha existido durante al menos 300 años.[34]​  En otros gigantes gaseosos, la falta de superficie permite que el viento alcance velocidades enormes: ráfagas de hasta 2100 km/h se han medido en el planeta Neptuno.[35]​ Esto ha creado un acertijo para los científicos planetarios. En última instancia, el clima es creado por energía solar y la cantidad de energía recibida por Neptuno es solo aproximadamente 1⁄900 de la recibida por la Tierra, sin embargo, la intensidad de los fenómenos climáticos en Neptuno es mucho mayor que en la Tierra.[36]​ Los vientos planetarios más fuertes descubiertos hasta ahora están en el planeta extrasolar HD 189733 b, que se cree que tiene vientos del este que se mueven a más de 9600 km/h.[37]​

Un campo magnético es una descripción matemática de la influencia magnética de las corrientes eléctricas y de los materiales magnéticos.[1]​ El campo magnético en cualquier punto está especificado por dos valores, la dirección y la magnitud; de tal forma que es un campo vectorial. Específicamente, el campo magnético es un vector axial, como lo son los momentos mecánicos y los campos rotacionales. El campo magnético es más comúnmente definido en términos de la fuerza de Lorentz ejercida en cargas eléctricas.[2]​: ch1 [3]​ 
El término se usa para dos campos distintos pero estrechamente relacionados, indicados por los símbolos B y H, donde, en el Sistema Internacional de Unidades, H se mide en unidades de amperios por metro y B se mide en teslas o newtons entre metro por amperio. En un vacío, H y B son lo mismo aparte de las unidades; pero en un material con magnetización (denotado por el símbolo M), B es solenoidal (no tiene divergencia en su dependencia espacial) mientras que H es no rotacional (libre de ondulaciones). 
Los campos magnéticos se producen por cualquier carga eléctrica producida por los electrones en movimiento y el momento magnético intrínseco de las partículas elementales asociadas con una propiedad cuántica fundamental, su espín. En la relatividad especial, campos eléctricos y magnéticos son dos aspectos interrelacionados de un objeto, llamado el tensor electromagnético. Las fuerzas magnéticas dan información sobre la carga que lleva un material a través del efecto Hall. La interacción de los campos magnéticos en dispositivos eléctricos tales como transformadores es estudiada en la disciplina de circuitos magnéticos.
Los campos magnéticos se utilizan en toda la tecnología moderna, especialmente en ingeniería eléctrica y electromecánica. Los campos magnéticos giratorios se utilizan tanto en los motores eléctricos como en los generadores. La interacción de los campos magnéticos en dispositivos eléctricos como los transformadores se conceptualiza e investiga como circuito magnético. Las fuerzas magnéticas dan información sobre los portadores de carga en un material a través del efecto Hall. La Tierra produce su propio campo magnético, que protege la capa de ozono de la Tierra del viento solar y es importante en la navegación mediante una brújula.
Entre las definiciones de campo magnético se encuentra la dada por la fuerza de Lorentz. Esto sería el efecto generado por una corriente eléctrica o un imán, sobre una región del espacio en la que una carga eléctrica puntual de valor (q), que se desplaza a una velocidad , experimenta los efectos de una fuerza que es secante y proporcional tanto a la velocidad (v) como al campo (B). Así, dicha carga percibirá una fuerza descrita con la siguiente ecuación:

Donde F es la fuerza magnética, v es la velocidad y B el campo magnético, también llamado inducción magnética y densidad de flujo magnético.
(Nótese que tanto F como v y B son magnitudes vectoriales y el producto vectorial tiene como resultante un vector perpendicular tanto a v como a B).
El módulo de la fuerza resultante será:

La existencia de un campo magnético se pone de manifiesto gracias a la propiedad de orientar un magnetómetro (laminilla de acero imantado que puede girar libremente). La aguja de una brújula, que evidencia la existencia del campo magnético terrestre, puede ser considerada un magnetómetro. La ley de Lorentz establece que una partícula cargada q que circula a una velocidad v→ por un punto en el que existe una intensidad de campo magnético B→, sufrirá la acción de una fuerza F→ denominada fuerza de Lorentz cuyo valor es proporcional al valor de q, B→ y v→  se obtiene por medio de la siguiente expresión:
Si bien algunos materiales magnéticos han sido conocidos desde la antigüedad, como por ejemplo el poder de atracción que la magnetita ejerce sobre el hierro, no fue sino hasta el siglo XIX cuando la relación entre la electricidad y el magnetismo quedó plasmada, pasando ambos campos de ser diferenciados a formar el cuerpo de lo que se conoce como electromagnetismo.
Antes de 1820, el único magnetismo conocido era el del hierro. Esto cambió con un profesor de ciencias poco conocido de la Universidad de Copenhague, Dinamarca, Hans Christian Oersted. En 1820 Oersted preparó en su casa una demostración científica a sus amigos y estudiantes. Planeó demostrar el calentamiento de un hilo por una corriente eléctrica y también llevar a cabo demostraciones sobre el magnetismo, para lo cual dispuso de una aguja de brújula montada sobre una peana de madera.
Mientras llevaba a cabo su demostración eléctrica, Oersted notó para su sorpresa que cada vez que se conectaba la corriente eléctrica, se movía la aguja de la brújula. Se calló y finalizó las demostraciones, pero en los meses sucesivos trabajó duro intentando explicarse el nuevo fenómeno.¡Pero no pudo! La aguja no era ni atraída ni repelida por la corriente. En vez de eso tendía a quedarse en ángulo recto. Hoy sabemos que esto es una prueba fehaciente de la relación intrínseca entre el campo magnético y el campo eléctrico plasmada en las ecuaciones de Maxwell.
Como ejemplo para ver la naturaleza un poco distinta del campo magnético basta considerar el intento de separar el polo de un imán. Aunque rompamos un imán por la mitad este "reproduce" sus dos polos. Si ahora volvemos a partir otra vez en dos, nuevamente tendremos cada trozo con los polos norte y sur diferenciados. En magnetismo no se han observado los monopolos magnéticos.
La fuerza sobre una carga eléctrica depende de su ubicación, velocidad y dirección; se utilizan dos campos vectoriales para describir esta fuerza.[2]​: ch1  El primero es el campo eléctrico, que describe la fuerza que actúa sobre una carga estacionaria y da la componente de la fuerza que es independiente del movimiento. El campo magnético, en cambio, describe la componente de la fuerza que es proporcional tanto a la velocidad como a la dirección de las partículas cargadas.[2]​: ch13  El campo se define por la ley de Lorentz y es, en cada instante, perpendicular tanto al movimiento de la carga como a la fuerza que experimenta.
El nombre de campo magnético o intensidad del campo magnético se aplica a dos magnitudes:
Desde un punto de vista físico, ambos son equivalentes en el vacío, salvo en una constante de proporcionalidad (permeabilidad) que depende del sistema de unidades: 1 en el sistema de Gauss,  en el SI. Solo se diferencian en medios materiales con el fenómeno de la magnetización.
El campo H se ha considerado tradicionalmente el campo principal o intensidad de campo magnético, ya que se puede relacionar con unas cargas, masas o polos magnéticos por medio de una ley similar a la de Coulomb para la electricidad. Maxwell, por ejemplo, utilizó este enfoque, aunque aclarando que esas cargas eran ficticias. Con ello, no solo se parte de leyes similares en los campos eléctricos y magnéticos (incluyendo la posibilidad de definir un potencial escalar magnético), sino que en medios materiales, con la equiparación matemática de H con E, por un lado, y de B con D, por otro, se pueden establecer paralelismos útiles en las condiciones de contorno y las relaciones termodinámicas; las fórmulas correspondientes en el sistema electromagnético de Gauss son:

En electrotecnia no es raro que se conserve este punto de vista porque resulta práctico.
Con la llegada de las teorías del electrón de Lorentz y Poincaré, y de la relatividad de Einstein, quedó claro que estos paralelismos no se corresponden con la realidad física de los fenómenos, por lo que hoy es frecuente, sobre todo en física, que el nombre de campo magnético se aplique a B (por ejemplo, en los textos de Alonso-Finn y de Feynman).[4]​ En la formulación relativista del electromagnetismo, E no se agrupa con H para el tensor de intensidades, sino con B.
En 1944, F. Rasetti preparó un experimento para dilucidar cuál de los dos campos era el fundamental, es decir, aquel que actúa sobre una carga en movimiento, y el resultado fue que el campo magnético real era B y no H.[5]​
Para caracterizar H y B se ha recurrido a varias distinciones. Así, H describe cuan intenso es el campo magnético en la región que afecta, mientras que B es la cantidad de flujo magnético por unidad de área que aparece en esa misma región. Otra distinción que se hace en ocasiones es que H se refiere al campo en función de sus fuentes (las corrientes eléctricas) y B al campo en función de sus efectos (fuerzas sobre las cargas).
Las fórmulas derivadas para el campo magnético anteriores son correctas cuando se trata de la corriente completa. Sin embargo, un material magnético colocado dentro de un campo magnético genera su propia magnetización, que puede ser un reto para calcular. (Esta corriente ligada se debe a la suma de los bucles de corriente de tamaño atómico y al espín de las partículas subatómicas, como los electrones, que componen el material). El campo H, tal y como se ha definido anteriormente, ayuda a factorizar esta corriente ligada; pero para ver cómo, ayuda a introducir primero el concepto de magnetización.
El campo vectorial de magnetización M representa la fuerza con la que una región de material está magnetizada. Se define como el momento dipolar magnético neto por unidad de volumen de esa región. La magnetización de un imán uniforme es por tanto una constante del material, igual al momento magnético m del imán dividido por su volumen. Como la unidad SI del momento magnético es A⋅m2, la unidad SI de la magnetización M es el amperio por metro, idéntica a la del campo H.
El campo de magnetización M de una región apunta en la dirección del momento dipolar magnético medio en esa región. Las líneas del campo de magnetización, por lo tanto, comienzan cerca del polo sur magnético y terminan cerca del polo norte magnético. (La magnetización no existe fuera del imán).
En el modelo de bucles amperianos, la magnetización se debe a la combinación de muchos bucles amperianos diminutos para formar una corriente resultante llamada corriente ligada. Esta corriente ligada, entonces, es la fuente del campo magnético B debido al imán. (Ver Dipolos magnéticos más adelante y polos magnéticos vs. corrientes atómicas para más información). Dada la definición de dipolo magnético, el campo de magnetización sigue una ley similar a la de la ley de Ampere:[6]​
donde la integral es una integral de línea sobre cualquier bucle cerrado y Ib es la corriente límite encerrada por ese bucle cerrado.
En el modelo de los polos magnéticos, la magnetización comienza y termina en los polos magnéticos. Por lo tanto, si una región determinada tiene una "fuerza de polo magnético" neta positiva (correspondiente a un polo norte), entonces tiene más líneas de campo de magnetización que entran en ella que las que salen. Matemáticamente esto equivale a:
donde la integral es una integral de superficie cerrada sobre la superficie cerrada S y qM es la "carga magnética" (en unidades de flujo magnético) encerrada por S. (Una superficie cerrada rodea completamente una región sin agujeros que dejen escapar las líneas de campo). El signo negativo se produce porque el campo de magnetización se desplaza de sur a norte.
En unidades del SI, el campo H está relacionado con el campo B por
En términos del campo H, la ley de Ampere es
donde If representa la "corriente libre" encerrada por la espira, de modo que la integral de línea de H' no depende en absoluto de las corrientes ligadas.[7]​
Para el equivalente diferencial de esta ecuación véase Ecuaciones de Maxwell. La ley de Ampere conduce a la condición de contorno.
donde Kf es la densidad de corriente libre superficial y la normal unitaria  apunta en la dirección del medio 2 al medio 1.[8]​
De forma similar, una integral de superficie de H sobre cualquier superficie cerrada es independiente de las corrientes libres y recoge las "cargas magnéticas" dentro de esa superficie cerrada:
que no depende de las corrientes libres.
El campo H, por lo tanto, se puede separar en dos[note 1]​ partes independientes:
donde H0 es el campo magnético aplicado debido solo a las corrientes libres y Hd es el campo desmagnetizante debido solo a las corrientes ligadas.
El campo magnético H, por tanto, refactoriza la corriente ligada en términos de "cargas magnéticas". Las líneas de campo H hacen un bucle solo alrededor de la "corriente libre" y, a diferencia del campo magnético B, comienza y termina también cerca de los polos magnéticos.
La mayoría de los materiales responden a un campo B aplicado produciendo su propia magnetización M y, por tanto, sus propios campos B. Normalmente, la respuesta es débil y solo existe cuando se aplica el campo magnético. El término magnetismo describe cómo los materiales responden a nivel microscópico a un campo magnético aplicado y se utiliza para categorizar la fase magnética de un material. Los materiales se dividen en grupos según su comportamiento magnético:
son materiales que se caracterizan por una conductividad perfecta por debajo de una temperatura y un campo magnético críticos. También son altamente magnéticos y pueden ser diamantes perfectos por debajo de un campo magnético crítico inferior. Los superconductores suelen tener un amplio rango de temperaturas y campos magnéticos (el llamado Estado mixto) bajo el cual exhiben una dependencia histerética compleja de M en B.
En el caso del paramagnetismo y el diamagnetismo, la magnetización M suele ser proporcional al campo magnético aplicado de forma que:
donde μ es un parámetro dependiente del material llamado permeabilidad. En algunos casos, la permeabilidad puede ser un tensor de segundo rango, de modo que H puede no apuntar en la misma dirección que B. Estas relaciones entre B y H son ejemplos de ecuación constitutiva. Sin embargo, los superconductores y los ferromagnetos tienen una relación más compleja entre B y H; véase histéresis magnética.
Un campo magnético tiene dos fuentes que lo originan. Una de ellas es una corriente eléctrica de conducción, que da lugar a un campo magnético estático, si es constante. Por otro lado una corriente de desplazamiento origina un campo magnético variante en el tiempo, incluso aunque aquella sea estacionaria.
La relación entre el campo magnético y una corriente eléctrica está dada por la ley de Ampère. El caso más general, que incluye a la corriente de desplazamiento, lo da la ley de Ampère-Maxwell.
El campo magnético generado por una única carga en movimiento (no por una corriente eléctrica) se puede calcular de manera aproximada a partir de la siguiente expresión derivada de la ley de Biot-Savart:

Donde . Esta última expresión define un campo vectorial solenoidal, para distribuciones de cargas en movimiento la expresión es diferente, pero puede probarse que el campo magnético sigue siendo un campo solenoidal. Es una aproximación debido a que, al partir de una corriente continua de cargas e intentar transformar la ley para cargas puntuales, se desprecian las interacciones entre las cargas de la corriente. Esta aproximación es útil para bajas velocidades (respecto a la velocidad de la luz).

La inexistencia de cargas magnéticas lleva a que el campo magnético es un campo solenoidal, lo que lleva a que localmente puede ser derivado de un potencial vector , es decir:
A su vez este potencial vector puede ser relacionado con el vector densidad de corriente mediante la relación:

La ecuación anterior planteada sobre , con una distribución de cargas contenida en un conjunto compacto, la solución es expresable en forma de integral. Y el campo magnético de una distribución de carga viene dado por:

Cabe destacar que, a diferencia del campo eléctrico, en el campo magnético no se ha comprobado la existencia de monopolos magnéticos, solo dipolos magnéticos, lo que significa que las líneas de campo magnético son cerradas, esto es, el número neto de líneas de campo que entran en una superficie es igual al número de líneas de campo que salen de la misma superficie. Un claro ejemplo de esta propiedad viene representado por las líneas de campo de un imán, donde se puede ver que el mismo número de líneas de campo que salen del polo norte vuelve a entrar por el polo sur, desde donde vuelven por el interior del imán hasta el norte.
Como se puede ver en el dibujo, independientemente de que la carga en movimiento sea positiva o negativa, en el punto A nunca aparece campo magnético; sin embargo, en los puntos B y C el campo magnético invierte su dirección dependiendo de si la carga es positiva o negativa. La dirección del campo magnético viene dado por la regla de la mano derecha.
Para determinar la dirección, se toma un vector , en la misma dirección de la trayectoria de la carga en movimiento. La dirección de este vector depende del signo de la carga, esto es, si la carga es positiva y se mueve hacia la derecha, el vector  estará orientado hacia la derecha. No obstante, si la carga es negativa y se mueve hacia la derecha, el vector  va hacia la izquierda. A continuación, se recorre señalando con los cuatro dedos de la mano derecha, desde el primer vector  hasta el segundo vector , por el camino más corto o, lo que es lo mismo, el camino que forme el ángulo menor entre los dos vectores. El pulgar extendido indicará en ese punto la dirección del campo magnético.
La energía es necesaria para generar un campo magnético, para trabajar contra el campo eléctrico que un campo magnético crea y para cambiar la magnetización de cualquier material dentro del campo magnético. Para los materiales no-dispersivos, se libera esta misma energía tanto cuando se destruye el campo magnético para poder modelar esta energía, como siendo almacenado en el campo magnético.
Para materiales lineales y no dispersivos (tales que  donde μ es independiente de la frecuencia), la densidad de energía es:

Si no hay materiales magnéticos alrededor, entonces el μ se puede substituir por μ0. La ecuación antedicha no se puede utilizar para los materiales no lineales, se utiliza una expresión más general dada abajo. 
Generalmente la cantidad incremental de trabajo por el δW del volumen de unidad necesitado para causar un cambio pequeño del δB del campo magnético es:
δW= H*δB
Una vez que la relación entre H y B se obtenga, esta ecuación se utiliza para determinar el trabajo necesitado para alcanzar un estado magnético dado. Para los materiales como los ferromagnéticos y superconductores el trabajo necesitado también dependerá de cómo se crea el campo magnético.
El campo magnético para cargas que se mueven a velocidades pequeñas comparadas con velocidad de la luz, puede representarse por un campo vectorial. 
Sea una carga eléctrica de prueba  en un punto P de una región del espacio moviéndose a una cierta velocidad arbitraria v respecto a un cierto observador que no detecte campo eléctrico. Si el observador detecta una deflexión de la trayectoria de la partícula entonces en esa región existe un campo magnético. El valor o intensidad de dicho campo magnético puede medirse mediante el llamado vector de inducción magnética B, a veces llamado simplemente "campo magnético", que estará relacionado con la fuerza F y la velocidad v medida por dicho observador en el punto P: Si se varía la dirección de v por P, sin cambiar su magnitud, se encuentra, en general, que la magnitud de F varía, si bien se conserva perpendicular a v. A partir de la observación de una pequeña carga eléctrica de prueba puede determinarse la dirección y módulo de dicho vector del siguiente modo:

En consecuencia: Si una carga de prueba positiva  se dispara con una velocidad v por un punto P y si obra una fuerza lateral F sobre la carga que se mueve, hay una inducción magnética B en el punto P siendo B el vector que satisface la relación:

La magnitud de F, de acuerdo a las reglas del producto vectorial, está dada por la expresión:

Expresión en la que  es el ángulo entre v y B.
El hecho de que la fuerza magnética sea siempre perpendicular a la dirección del movimiento implica que el trabajo realizado por la misma sobre la carga, es cero. En efecto, para un elemento de longitud  de la trayectoria de la partícula, el trabajo  es  que vale cero por ser  y  perpendiculares. Así pues, un campo magnético estático no puede cambiar la energía cinética de una carga en movimiento. 
Si una partícula cargada se mueve a través de una región en la que coexisten un campo eléctrico y uno magnético la fuerza resultante está dada por:

Esta fórmula es conocida como Relación de Lorentz
La teoría de la relatividad especial probó que de la misma manera que espacio y tiempo no son conceptos absolutos, la parte eléctrica y magnética de un campo electromagnético dependen del observador. Eso significa que dados dos observadores  y  en movimiento relativo uno respecto a otro el campo magnético y eléctrico medido por cada uno de ellos no será el mismo. En el contexto de la relatividad especial si los dos observadores se mueven uno respecto a otro con velocidad uniforme v dirigida según el eje X, las componentes de los campos eléctricos medidas por uno y otro observador vendrán relacionadas por:

Y para los campos magnéticos se tendrá:

Nótese que en particular un observador en reposo respecto a una carga eléctrica detectará solo campo eléctrico, mientras que los observadores que se mueven respecto a las cargas detectarán una parte eléctrica y magnética.
El campo magnético creado por una carga en movimiento puede probarse por la relación general:

que es válida tanto en mecánica newtoniana como en mecánica relativista. Esto lleva a que una carga puntual moviéndose a una velocidad v proporciona un campo magnético dado por:

donde el ángulo  es el ángulo formado por los vectores  y . Si el campo magnético es creado por una partícula cargada que tiene aceleración la expresión anterior contiene términos adicionales (ver potenciales de Liénard-Wiechert).
La unidad de B en el SI es el tesla, que equivale a wéber por metro cuadrado (Wb/m²) o a voltio segundo por metro cuadrado (V s/m²); en unidades básicas es kg s−2 A−1. Su unidad en sistema de Gauss es el gauss (G); en unidades básicas es cm−1/2 g1/2 s−1.
La unidad de H en el SI es el amperio por metro, A/m (a veces llamado amperivuelta por metro, Av/m). Su unidad en el sistema de Gauss es el oérsted (Oe), que es dimensionalmente igual al Gauss.
La magnitud del campo magnético terrestre en la superficie de la Tierra es de alrededor de 0.5G. Los imanes permanentes comunes, de hierro, generan campos de unos pocos cientos de Gauss, esto es a corto alcance la influencia sobre una brújula es alrededor de mil veces más intensa que la del campo magnético terrestre; como la intensidad se reduce con el cubo de la distancia, a distancias relativamente cortas el campo terrestre vuelve a dominar. Los imanes comerciales más potentes, basados en combinaciones de metales de transición y tierras raras generan campos hasta diez veces más intensos, de hasta 3000-4000 G, esto es, 0.3-0.4 T. El límite teórico para imanes permanentes es alrededor de diez veces más alto, unos 3 Tesla. Los centros de investigación especializados obtienen de forma rutinaria campos hasta diez veces más intensos, unos 30T, mediante electroimanes; se puede doblar este límite mediante campos pulsados, que permiten enfriarse al conductor entre pulsos. En circunstancias extraordinarias, es posible obtener campos incluso de 150 T o superiores, mediante explosiones que comprimen las líneas de campo; naturalmente en estos casos el campo dura solo unos microsegundos. Por otro lado, los campos generados de forma natural en la superficie de un púlsar se estiman en el orden de los cientos de millones de Tesla.[14]​
El campo puede visualizarse mediante un conjunto de líneas de campo magnético, que siguen la dirección del campo en cada punto. Las líneas pueden construirse midiendo la intensidad y la dirección del campo magnético en un gran número de puntos (o en cada punto del espacio). A continuación, se marca cada lugar con una flecha, llamada vector, que apunta en la dirección del campo magnético local con su magnitud proporcional a la intensidad del campo magnético. La conexión de estas flechas forma un conjunto de líneas de campo magnético. La dirección del campo magnético en cualquier punto es paralela a la dirección de las líneas de campo cercanas, y la densidad local de líneas de campo puede hacerse proporcional a su fuerza. Las líneas de campo magnético son como las  líneas de corriente en el flujo de fluidos, en el sentido de que representan una distribución continua, y una resolución diferente mostraría más o menos líneas.Una ventaja de utilizar las líneas de campo magnético como representación es que muchas leyes del magnetismo (y del electromagnetismo) pueden enunciarse de forma completa y concisa utilizando conceptos simples como el "número" de líneas de campo que atraviesan una superficie. Estos conceptos pueden "traducirse" rápidamente a su forma matemática. Por ejemplo, el número de líneas de campo a través de una superficie dada es la integral de superficie del campo magnético.[15]​
Varios fenómenos "muestran" las líneas de campo magnético como si las líneas de campo fueran fenómenos físicos. Por ejemplo, las limaduras de hierro colocadas en un campo magnético forman líneas que corresponden a las "líneas de campo".[note 2]​ Las "líneas" del campo magnético también se muestran visualmente en las auroras polares o boreales, en las que las interacciones dipolares de las partículas del plasma crean rayas visibles de luz que se alinean con la dirección local del campo magnético de la Tierra.
Las líneas de campo pueden utilizarse como herramienta cualitativa para visualizar las fuerzas magnéticas. En las sustancias  ferromagnéticas como el hierro y en los plasmas, las fuerzas magnéticas pueden entenderse imaginando que las líneas de campo ejercen una tensión, (como una banda elástica) a lo largo de su longitud, y una presión perpendicular a su longitud sobre las líneas de campo vecinas. Los polos "diferentes" de los imanes se atraen porque están unidos por muchas líneas de campo; los polos "similares" se repelen porque sus líneas de campo no se encuentran, sino que corren paralelas, empujándose mutuamente. La forma rigurosa de este concepto es el tensor de energía-impulso electromagnético.
En el mundo microscópico, atendiendo a los valores del momento dipolar de iones magnéticos típicos y a la ecuación que rige la propagación del campo generado por un dipolo magnético, se verifica que a un nanómetro de distancia, el campo magnético generado por un electrón aislado es del orden de 3 G, el de una molécula imán típica, del orden de 30 G y el de un ion magnético típico puede tener un valor intermedio, de 5 a 15 G. A un angstrom, que es un valor corriente para un radio atómico y por tanto el valor mínimo para el que puede tener sentido referirse al momento magnético de un ion, los valores son mil veces más elevados, esto es, del orden de magnitud del Tesla.

La velocidad de la luz en el vacío es una constante universal con el valor de 299 792 458 m/s,[2]​[3]​ aunque suele aproximarse a 3·108 m/s. Se simboliza con la letra c, proveniente del latín celéritās (en español, celeridad o rapidez).
El valor de la velocidad de la luz en el vacío fue incluido oficialmente en el Sistema Internacional de Unidades como constante el 21 de octubre de 1983,[4]​ pasando así el metro a ser una unidad derivada de esta constante. También se emplea en la definición del año luz, unidad de longitud equivalente a 9,46·1015 m, ya que la velocidad de la luz también se puede expresar como 9,46·1015 m/año.
La rapidez a través de un medio que no sea el "vacío" depende de su permitividad eléctrica, de su permeabilidad magnética, y otras características electromagnéticas. En medios materiales, esta velocidad es inferior a c y queda codificada en el índice de refracción. En modificaciones del vacío más sutiles, como espacios curvos, efecto Casimir, poblaciones térmicas o presencia de campos externos, la velocidad de la luz depende de la densidad de energía de ese vacío.[5]​

De acuerdo con la física moderna toda radiación electromagnética (incluida la luz visible) se propaga o mueve con una rapidez constante en el vacío, conocida —aunque impropiamente[cita requerida]— como "velocidad de la luz" (magnitud vectorial), en vez de "rapidez de la luz" (magnitud escalar). Esta es una constante física denotada como c. La rapidez c es también la rapidez de la propagación de la gravedad en la teoría general de la relatividad.
Una consecuencia que se obtiene a partir de las leyes del electromagnetismo (tales como las ecuaciones de Maxwell) es que la rapidez c de la radiación electromagnética no depende de la rapidez del objeto que emite tal radiación. Así, por ejemplo, la luz emitida por una fuente de luz que se mueve muy rápidamente, viajaría con la misma rapidez que la luz proveniente de una fuente estacionaria (aunque el color, la frecuencia, la energía y el momentum de la luz cambiarán; fenómeno que se conoce como efecto Doppler).
Si se combina esta observación con el principio de relatividad, se concluye que todos los observadores medirán la rapidez de la luz en el vacío como una misma cantidad, sin importar el marco de referencia del observador o la rapidez del objeto que emite la luz. Debido a esto, se puede ver a c como una constante física fundamental. Este hecho, entonces, puede ser usado como base en la teoría de la relatividad especial. La constante es la rapidez c, en vez de la luz en sí misma, lo cual es fundamental para la relatividad especial. De este modo, si la luz es de alguna manera retardada para viajar a una rapidez menor de c, esto no afectará directamente a la teoría de la relatividad especial.
Observadores que viajan con gran rapidez encontrarán que las distancias y los tiempos se distorsionan de acuerdo con la transformación de Lorentz. Sin embargo, las transformaciones distorsionan tiempos y distancias de manera que la rapidez de la luz permanece constante. Una persona viajando con una rapidez cercana a c también encontrará que los colores de la luz al frente se tornan azules y atrás se tornan rojos.
Si la información pudiese viajar más rápido que c en un marco de referencia, la causalidad sería violada: en otros marcos de referencia, la información sería recibida antes de ser mandada; así, la causa podría ser observada después del efecto. Debido a la dilatación del tiempo de la relatividad especial, el cociente del tiempo percibido entre un observador externo y el tiempo percibido por un observador que se mueve cada vez más cerca de la rapidez de la luz se aproxima a cero. Si algo pudiera moverse más rápidamente que la luz, este cociente no sería un número real. Tal violación de la causalidad nunca se ha observado.
Un cono de luz define la ubicación que está en contacto causal y aquellas que no lo están. Para exponerlo de otro modo, la información se propaga de y hacia un punto de regiones definidas por un cono de luz. El intervalo AB en el diagrama a la derecha es de "tipo tiempo" (es decir, hay un marco de referencia en el que los acontecimientos A y B ocurren en la misma ubicación en el espacio, separados solamente por su ocurrencia en tiempos diferentes, y si A precede a B en ese marco entonces A precede a B en todos los marcos: no hay marco de referencia en el cual el evento A y el evento B ocurren simultáneamente). De este modo, es hipotéticamente posible para la materia (o la información) viajar de A hacia B, así que puede haber una relación causal (con A la causa y B el efecto).
Por otra parte, el intervalo AC es de "tipo espacio"[cita requerida] (es decir, existe un marco de referencia donde el evento A y el evento C ocurren simultáneamente). Sin embargo, también existen marcos en los que A precede a C, o en los que C precede a A. Confinando una manera de viajar más rápido que la luz, no será posible para ninguna materia (o información) viajar de A hacia C o de C hacia A. De este modo no hay conexión causal entre A y C.
En acuerdo con la definición actual, adoptada en 1983, la rapidez de la luz es exactamente 299 792 458 m/s (aproximadamente 3 × 108 metros por segundo, 300 000 km/s o 300 m por millonésima de s).
El valor de c define la permitividad eléctrica del vacío () en unidades del SIU como:
La permeabilidad magnética del vacío () no es dependiente de c y es definida en unidades del SIU como:
Estas constantes aparecen en las ecuaciones de Maxwell, que describen el electromagnetismo y están relacionadas por:
Las distancias astronómicas son normalmente medidas en años luz (que es la distancia que recorre la luz en un año, aproximadamente 9,46×1012 km (9,46 billones de km).
Históricamente, el metro había sido definido como la diezmillonésima parte de la longitud del arco de meridiano terrestre comprendido entre el polo norte y el ecuador a través de París, con referencia a la barra estándar, y con referencia a una longitud de onda de una frecuencia particular de la luz.
En 1967 la XIII Conferencia General de Pesos y Medidas definió el segundo del tiempo atómico como la duración de 9 192 631 770  períodos de radiación correspondiente a la transición entre dos niveles hiperfinos del estado fundamental del átomo cesio-133, que en la actualidad sigue siendo la definición del segundo.
En 1983 la Conferencia General de Pesos y Medidas resolvió modificar la definición del metro como unidad de longitud del Sistema Internacional, estableciendo su definición a partir de la velocidad de la luz:[6]​
En consecuencia, este reajuste efectuado en la definición del metro permite que la velocidad de la luz tenga un valor exacto de 299 792 458 m/s cuando se expresa en metros/segundo. Esta modificación aprovecha de forma práctica una de las bases de la teoría de la relatividad de Einstein, que establece que la magnitud de la velocidad de la luz en el vacío es independiente del sistema de referencia utilizado para medirla.
La motivación en el cambio de la definición del metro, así como todos los cambios en la definición de unidades, fue proveer una definición precisa de la unidad que pudiese ser fácilmente usada para calibrar homogéneamente dispositivos en todo el mundo. La barra estándar no era práctica en este sentido, ya que no podía ser sacada de su cámara o utilizada por dos científicos al mismo tiempo. También era propensa a cambios significativos en su longitud debido a variaciones de temperatura, desgaste de los extremos, oxidación, etc., incompatible con la exactitud necesaria para establecer una de las unidades básicas del Sistema Internacional de unidades.
La rapidez de la luz es de gran importancia para las telecomunicaciones. Por ejemplo, dado que el perímetro de la Tierra es de 40 075 km (en la línea ecuatorial) y c es teóricamente la velocidad más rápida en la que un fragmento de información puede viajar, el período más corto de tiempo para llegar al otro extremo del globo terráqueo sería 0.067 s.
En realidad, el tiempo de viaje es un poco más largo, en parte debido a que la velocidad de la luz es cerca de un 30% menor en una fibra óptica, y raramente existen trayectorias rectas en las comunicaciones globales; además se producen retrasos cuando la señal pasa a través de interruptores eléctricos o generadores de señales. En 2004, el retardo típico de recepción de señales desde Australia o Japón hacia los Estados Unidos era de 0.18 s. Adicionalmente, la velocidad de la luz afecta al diseño de las comunicaciones inalámbricas.
La velocidad finita de la luz se hizo aparente a todo el mundo en el control de comunicaciones entre el Control Terrestre de Houston y Neil Armstrong, cuando este se convirtió en el primer hombre que puso un pie sobre la Luna: después de cada pregunta, Houston tenía que esperar cerca de 3 s para el regreso de una respuesta aun cuando los astronautas respondían inmediatamente.
De manera similar, el control remoto instantáneo de una nave interplanetaria es imposible debido a que una nave suficientemente alejada de nuestro planeta podría tardar algunas horas desde que envía información al centro de control terrestre y recibe las instrucciones.
La velocidad de la luz también puede tener influencia en distancias cortas. En los superordenadores la velocidad de la luz impone un límite de rapidez a la que pueden ser enviados los datos entre procesadores. Si un procesador opera a 1 GHz, la señal solo puede viajar a un máximo de 300 mm en un ciclo único. Por lo tanto, los procesadores deben ser colocados cerca uno de otro para minimizar los retrasos de comunicación. Si las frecuencias de un reloj continúan incrementándose, la rapidez de la luz finalmente se convertirá en un factor límite para el diseño interno de chips individuales.
Es importante observar que la velocidad de la luz no es un límite de velocidad en el sentido convencional. Un observador que persigue un rayo de luz lo mediría al moverse paralelamente él mismo viajando a la misma velocidad como si fuese un observador estacionario. Esto se debe a que la velocidad medida por este observador depende no solo de la diferencia de distancias recorridas por él y por el rayo, sino también de su tiempo propio que se ralentiza con la velocidad del observador. La ralentización del tiempo o dilatación temporal para el observador es tal que siempre percibirá a un rayo de luz moviéndose a la misma velocidad.
La mayoría de los individuos están acostumbrados a la regla de la adición de velocidades: si dos coches se acercan desde direcciones opuestas, cada uno viajando a una velocidad de 50 km/h, se esperaría (con un alto grado de precisión) que cada coche percibiría al otro en una velocidad combinada de 50 + 50=100 km/h. Esto sería correcto en todos los casos si pudiéramos ignorar que la medida física del tiempo transcurrido es relativa según el estado de movimiento del observador.
Sin embargo, a velocidades cercanas a la de la luz, en resultados experimentales se hace claro que esta regla no se puede aplicar por la dilatación temporal. Dos naves que se aproximen una a otra, cada una viajando al 90% de la velocidad de la luz relativas a un tercer observador entre ellas, no se percibirán mutuamente a un 90% + 90%=180% de la velocidad de la luz. En su lugar, cada una percibirá a la otra aproximándose a menos de un 99.5% de la velocidad de la luz. Este resultado se da por la fórmula de adición de la velocidad de Einstein:
donde v y w son las velocidades de las naves observadas por un tercer observador, y u es la velocidad de cualquiera de las dos naves observada por la otra.
Contrariamente a la intuición natural, sin importar la velocidad a la que un observador se mueva relativamente hacia otro observador, ambos medirán la velocidad de un rayo de luz que se avecina con el mismo valor constante, la velocidad de la luz.
La ecuación anterior fue derivada por Einstein de su teoría de relatividad especial, la cual toma el principio de relatividad como premisa principal. Este principio (originalmente propuesto por Galileo Galilei) requiere que actúen leyes físicas de la misma manera en todos los marcos de referencia.
Ya que las ecuaciones de Maxwell otorgan directamente una velocidad de la luz, debería ser lo mismo para cada observador; una consecuencia que sonaba obviamente equivocada para los físicos del siglo XIX, quienes asumían que la velocidad de la luz dada por la teoría de Maxwell es válida en relación con el "éter lumínico".
Pero el experimento de Michelson y Morley, puede que el más famoso y útil experimento en la historia de la física, no pudo encontrar este éter, sugiriendo en su lugar que la velocidad de la luz es una constante en todos los marcos de referencia.
Aunque no se sabe si Einstein conocía los resultados de los experimentos de Michelson y Morley, él dio por hecho que la velocidad de la luz era constante, lo entendió como una reafirmación del principio de relatividad de Galileo, y dedujo las consecuencias, ahora conocidas como la teoría de la relatividad especial, que incluyen la anterior fórmula auto-intuitiva.
Debe tenerse presente, especialmente si se consideran sistemas de referencia no inerciales, que la observación experimental de constancia de la luz se refiere a la velocidad física de la luz. La diferencia entre ambas magnitudes ocasionó ciertos malentendidos a los teóricos de principios de siglo XX. Así Pauli llegó a escribir:

Sin embargo, ese comentario es cierto predicado de la velocidad coordenada de la luz (cuya definición no involucra los coeficientes métricos del tensor métrico), sin embargo, una definición adecuada de velocidad física de la luz involucrando las componentes del tensor métrico de sistemas de referencia no inerciales lleva a que la velocidad física sí sea constante.
El índice de refracción de un material indica cuán lenta es la velocidad de la luz en ese medio comparada con el vacío. La disminución de la velocidad de la luz en los materiales puede causar el fenómeno denominado refracción, como se puede observar en un prisma atravesado por un rayo de luz blanca formando un espectro de colores y produciendo su dispersión.
Al pasar a través de los materiales, la luz se propaga a una velocidad menor que c, expresada por el cociente denominado «índice de refracción» del material.
La rapidez de la luz en el aire es solo levemente menor que c. Medios más densos, como el agua y el vidrio, pueden disminuir más la rapidez de la luz, a fracciones como 3/4 y 2/3 de c. Esta disminución de velocidad también es responsable de doblar la luz (modificando su trayectoria según un quiebro con un ángulo dado) en una interfase entre dos materiales con índices diferentes, un fenómeno conocido como refracción. Esto se debe a que dentro de los medios transparentes, la luz en tanto que onda electromagnética interacciona con la materia, que a su vez produce campos de respuesta, y la luz a través del medio es el resultado de la onda inicial y la respuesta de la materia. Esta onda electromagnética que se propaga en el material tiene una velocidad de propagación menor que la luz en el vacío. El índice de refracción "n" de un medio viene dado por la siguiente expresión, donde "v" es la velocidad de la luz en ese medio (debido a que, como ya se ha señalado, la velocidad de la luz en un medio es menor que la velocidad de la luz en el vacío):
Ya que la velocidad de la luz en los materiales depende del índice de refracción, y el índice de refracción depende de la frecuencia de la luz, la luz a diferentes frecuencias viaja a diferentes velocidades a través del mismo material. Esto puede causar distorsión en ondas electromagnéticas compuestas por múltiples frecuencias; un fenómeno llamado dispersión.
Los ángulos de incidencia (i) y de refracción (r) entre dos medios, y los índices de refracción, están relacionados por la Ley de Snell. Los ángulos se miden con respecto al vector normal a la superficie entre los medios:

A escala microscópica, considerando la radiación electromagnética como una partícula, la refracción es causada por una absorción continua y re-emisión de los fotones que componen la luz a través de los átomos o moléculas por los que está atravesando. En cierto sentido, la luz por sí misma viaja solo a través del vacío existente entre estos átomos, y es obstaculizada por los átomos. Alternativamente, considerando la radiación electromagnética como una onda, las cargas de cada átomo (primariamente electrones) interfieren con los campos eléctricos y electromagnéticos de la radiación, retardando su progreso.
Una evidencia experimental reciente demuestra que es posible para la velocidad de grupo de la luz exceder c. Un experimento hizo que la velocidad de grupo de rayos láser viajara distancias extremadamente cortas a través de átomos de cesio a 300 veces c. Sin embargo, no es posible usar esta técnica para transferir información más rápido que c: la rapidez de la transferencia de información depende de la velocidad frontal (la rapidez en la cual el primer incremento de un pulso sobre cero la mueve adelante) y el producto de la velocidad agrupada y la velocidad frontal es igual al cuadrado de la velocidad normal de la luz en el material.
El exceder la velocidad de grupo de la luz de esta manera, es comparable a exceder la velocidad del sonido emplazando personas en una línea espaciada equidistantemente, y pidiéndoles a todos que griten una palabra uno tras otro con intervalos cortos, cada uno midiendo el tiempo al mirar su propio reloj para que no tengan que esperar a escuchar el grito de la persona previa.
La rapidez de la luz también puede parecer superada en cierto fenómeno que incluye ondas evanescentes, tales como túneles cuánticos. Los experimentos indican que la velocidad de fase de ondas evanescentes pueden exceder a c; sin embargo, parecería que ni la velocidad agrupada ni la velocidad frontal exceden c, así, de nuevo, no es posible que la información sea transmitida más rápido que c.
En algunas interpretaciones de la mecánica cuántica, los efectos cuánticos pueden ser retransmitidos a velocidades mayores que c (de hecho, la acción a distancia se ha percibido largamente como un problema con la mecánica cuántica: ver paradoja EPR). Por ejemplo, los estados cuánticos de dos partículas pueden estar enlazados, de manera que el estado de una partícula condicione el estado de otra partícula (expresándolo de otra manera, uno debe tener un espín de +½ y el otro de -½). Hasta que las partículas son observadas, estas existen en una superposición de dos estados cuánticos (+½, –½) y (–½, +½). Si las partículas son separadas y una de ellas es observada para determinar su estado cuántico, entonces el estado cuántico de la segunda partícula se determina automáticamente. Si, en algunas interpretaciones de mecánica cuántica, se presume que la información acerca del estado cuántico es local para una partícula, entonces se debe concluir que la segunda partícula toma su estado cuántico instantáneamente, tan pronto como la primera observación se lleva a cabo. Sin embargo, es imposible controlar qué estado cuántico tomará la primera partícula cuando sea observada, así que ninguna información puede ser transmitida de esta manera. Las leyes de la Física también parecen prevenir que la información sea transmitida a través de maneras más astutas, y esto ha llevado a la formulación de reglas tales como el teorema de no clonación.
El llamado movimiento superluminar también es visto en ciertos objetos astronómicos, tales como los jet de Galaxia activa, galaxias activas y cuásares. Sin embargo, estos jets no se mueven realmente a velocidades excedentes a la de la luz: el movimiento aparente superluminar es una proyección del efecto causado por objetos moviéndose cerca de la velocidad de la luz en un ángulo pequeño del horizonte de visión.
Aunque puede sonar paradójico, es posible que las ondas expansivas se hayan formado con la radiación electromagnética, ya que una partícula cargada que viaja a través de un medio insolado, interrumpe el campo electromagnético local en el medio. Los electrones en los átomos del medio son desplazados y polarizados por el campo de la partícula cargada, y los fotones que son emitidos como electrones se restauran a sí mismos para mantener el equilibrio después de que la interrupción ha pasado (en un conductor, la interrupción puede ser restaurada sin emitir un fotón).
En circunstancias normales, estos fotones interfieren destructivamente unos con otros y no se detecta radiación. Sin embargo, si la interrupción viaja más rápida que los mismos fotones, los fotones interferirán constructivamente e intensificarán la radiación observada. El resultado (análogo a una explosión sónica) es conocido como radiación Cherenkov.
La habilidad de comunicarse o viajar más rápido que la luz es un tema popular en la ciencia ficción. Se han propuesto partículas que viajan más rápido que la luz, taquiones, doblados[cita requerida] por la física de partículas, aunque nunca se han observado.
Algunos físicos (entre ellos João Magueijo y John Moffat) han propuesto que en el pasado la luz viajaba mucho más rápido que a la velocidad actual. Esta teoría se conoce como velocidad de la luz variable, y sus proponentes afirman que este fenómeno tiene la habilidad de explicar mejor muchos enigmas cosmológicos que su teoría rival, el modelo inflacionario del universo. Sin embargo, esta teoría no ha ganado suficiente aceptación.
En septiembre de 2011, en las instalaciones del CERN en Ginebra, del laboratorio subterráneo de Gran Sasso (Italia), se observaron unos neutrinos que aparentemente superaban la velocidad de la luz, llegando (60.7 ± 6.9 (stat.) ± 7.4 (sys.)) nanosegundos antes (que corresponde a unos 18 metros en una distancia total de 732 kilómetros).[8]​ Desde el primer momento, la comunidad científica se mostró escéptica ante la noticia, ya que varios años antes, el proyecto Milos de la Fermilab de Chicago había obtenido resultados parecidos que fueron descartados porque el margen de error era demasiado alto.[9]​ Y, efectivamente, en este caso también resultó ser un error de medición.[10]​[11]​ En febrero de 2012, los científicos del CERN anunciaron que las mediciones habían sido erróneas debido a una conexión defectuosa.[12]​
Fenómenos refractivos tales como el arco iris tienden a retardar la velocidad de la luz en un medio (como el agua, por ejemplo). En cierto sentido, cualquier luz que viaja a través de un medio diferente del vacío viaja a una velocidad menor que c como resultado de la refracción. Sin embargo, ciertos materiales tienen un índice de refracción excepcionalmente alto: en particular, la densidad óptica del condensado de Bose-Einstein puede ser muy alta.
En 1999, un equipo de científicos encabezados por Lene Hau pudo disminuir la velocidad de un rayo de luz a cerca de 17 m/s, y en 2001 pudieron detener momentáneamente un rayo de luz.[13]​
En 2003, Mijaíl Lukin, junto con científicos de la Universidad Harvard y el Instituto de Física Lébedev (de Moscú), tuvieron éxito en detener completamente la luz al dirigirla a una masa de gas rubidio caliente, cuyos átomos, en palabras de Lukin, se comportaron como «pequeños espejos» debido a los patrones de interferencia en dos rayos de control.[14]​
Es importante mencionar que, la velocidad de la luz se tiende a retardar en cuanto se desplaza por un medio con una densidad mayor que el vacío, esto aplica a la luz moviendose a través de un medio como el aire, agua, aceite, entre otros...
Hasta tiempos relativamente recientes, la velocidad de la luz fue un tema sujeto a grandes conjeturas. Empédocles creía que la luz era algo en movimiento, y que por lo tanto en su viaje tenía que transcurrir algún tiempo.
Por el contrario, Aristóteles creía que «la luz está sujeta a la presencia de algo, pero no es el movimiento». Además, si la luz tiene una velocidad finita, esta tenía que ser inmensa. Aristóteles afirmó: «La tensión sobre nuestro poder de creencias es demasiado grande para creer esto».[cita requerida]
Una de las teorías antiguas de la visión es que la luz es emitida por el ojo, en lugar de ser generada por una fuente y reflejada en el ojo. En esta teoría, Herón de Alejandría adelantó el argumento de que la velocidad de la luz debería ser infinita, ya que cuando uno abre los ojos objetos distantes como las estrellas aparecen inmediatamente. Durante el siglo VI, Boecio intentó documentar la velocidad de la luz.[28]​  
Los filósofos islámicos Avicena y Alhacén creían que la luz tenía una velocidad finita, aunque en este punto otros filósofos convinieron con Aristóteles.[cita requerida]
La escuela Ayran de filosofía en la antigua India también mantuvo que la velocidad de la luz era finita.[cita requerida]
Johannes Kepler creía que la velocidad de la luz era finita ya que el espacio vacío no representa un obstáculo para ella. Francis Bacon argumentó que la velocidad de la luz no es necesariamente finita, ya que algo puede viajar tan rápido como para ser percibido.
René Descartes argumentó que si la velocidad de la luz era finita, el Sol, la Tierra y la Luna estarían perceptiblemente fuera de alineación durante un eclipse lunar. Debido a que tal desalineación no se ha observado, Descartes concluyó que la velocidad de la luz es infinita. De hecho, Descartes estaba convencido de que si la velocidad de la luz era finita, todo su sistema de filosofía sería refutado. [29]​
La historia de la medición de la velocidad de la luz comienza en el siglo XII en los albores de la revolución científica. Un estudio histórico relativo a las mediciones de la velocidad de la luz señala una docena de métodos diferentes para determinar el valor de "c".[30]​ La mayor parte de los primeros experimentos para intentar medir la velocidad de la luz fracasaron debido a su alto valor, y tan solo se pudieron obtener medidas indirectas a partir de fenómenos astronómicos. En el siglo XIX se pudieron realizar los primeros experimentos directos de medición de la velocidad de la luz confirmando su naturaleza electromagnética y las ecuaciones de Maxwell.
En 1629 Isaac Beeckman, un amigo de René Descartes, propuso un experimento en el que se pudiese observar el fogonazo de un cañón reflejándose en un espejo ubicado a una milla (1,6 km) del primero. En 1638, Galileo propuso un experimento para medir la velocidad de la luz, intentando detectar un posible lapso al destapar una linterna cuando es observada a cierta distancia. René Descartes criticó esta tentativa como algo superfluo, dado que la observación de eclipses, un procedimiento con un potencial mucho mayor para detectar una rapidez finita de la luz, había dado un resultado negativo. La Accademia del Cimento de Florencia puso en práctica en 1667 el experimento que había ideado Galileo, con las linternas separadas una milla entre sí, sin observarse ningún retraso. Robert Hooke explicó los resultados negativos tal como Galileo había hecho: precisando que tales observaciones no establecerían la velocidad infinita de la luz, sino tan solo que dicha velocidad debía ser muy grande.
En 1676 Ole Rømer realizó la primera estimación cuantitativa de la velocidad de la luz estudiando el movimiento del satélite Ío de Júpiter con un telescopio. Es posible medir el tiempo de la revolución de Ío debido a sus movimientos de entrada y salida en la sombra arrojada por Júpiter en intervalos regulares. Rømer observó que Ío gira alrededor de Júpiter cada 42.5 h cuando la Tierra esta más cerca de Júpiter. También observó que, cuando la Tierra y Júpiter se mueven separándose, la salida de Ío fuera de la proyección de la sombra comenzaba progresivamente más tarde de lo predicho. Las observaciones detalladas mostraban que estas señales de salida necesitaban más tiempo en llegar a la Tierra, ya que la Tierra y Júpiter se separaban cada vez más. De este modo el tiempo extra utilizado por la luz para llegar a la Tierra podía utilizarse para deducir la rapidez de esta. Seis meses después, las entradas de Ío en la proyección de la sombra se adelantaban, ya que la Tierra y Júpiter se acercaban uno a otro. Con base a estas observaciones, Rømer estimó que la luz tardaría 22 min en cruzar el diámetro de la órbita de la Tierra (es decir, el doble de la unidad astronómica); las estimaciones modernas se acercan más a la cifra de 16 min y 40 s.
Alrededor de la misma época, la unidad astronómica (radio de la órbita de la Tierra alrededor del Sol) se estimaba en cerca de 140 millones de km. Este dato y la estimación del tiempo de Rømer fueron combinados por Christian Huygens, quien consideró que la velocidad de la luz era cercana a 1000 diámetros de la Tierra por minuto, es decir, unos 220 000 km/s, muy por debajo del valor actualmente aceptado, pero mucho más rápido que cualquier otro fenómeno físico entonces conocido.
Isaac Newton también aceptó el concepto de velocidad finita. En su libro Opticks expone el valor más preciso de 16 minutos para que la luz recorra el diámetro de la órbita terrestre,[cita requerida] valor que al parecer dedujo por sí mismo (se desconoce si fue a partir de los datos de Rømer o de alguna otra manera).
El mismo efecto fue subsecuentemente observado por Rømer en un punto en rotación con la superficie de Júpiter. Observaciones posteriores también mostraron el mismo efecto con las otras tres lunas galileanas, en las que era más difícil de observar al estar estos satélites más alejados de Júpiter y proyectar sombras menores sobre el planeta.
Aunque por medio de estas observaciones la velocidad finita de la luz no fue establecida para la satisfacción de todos (notablemente Jean-Dominique Cassini), después de las observaciones de James Bradley (1728), la hipótesis de velocidad infinita se consideró totalmente desacreditada. Bradley dedujo que la luz de las estrellas que llega sobre la Tierra parecería provenir en un ángulo leve, que podría ser calculado al comparar la velocidad de la Tierra en su órbita con la velocidad de la luz. Se observó esta llamada aberración de la luz, estimándose en 1/200 de un grado.
Bradley calculó la velocidad de la luz en alrededor de 301 000 km/s.[21]​ Esta aproximación es solamente un poco mayor que el valor actualmente aceptado. El efecto de aberración fue estudiado extensivamente en los siglos posteriores, notablemente por Friedrich Georg Wilhelm Struve y Magnus Nyren.
La segunda medida acertada de la velocidad de la luz, primera mediante un aparato terrestre, fue realizada por Hippolyte Fizeau en 1849. El experimento de Fizeau era conceptualmente similar a aquellos propuestos por Beeckman y Galileo. Un rayo de luz se dirigía a un espejo a cientos de metros de distancia. En su trayecto desde la fuente hacia el espejo, el rayo pasaba a través de un engranaje rotatorio. A cierto nivel de rotación, el rayo pasaría a través de un orificio en su camino de salida y en otro en su camino de regreso. Pero en niveles ligeramente menores, el rayo se proyectaría en uno de los dientes y no pasaría a través de la rueda. Conociendo la distancia hasta el espejo, el número de dientes del engranaje y el índice de rotación, se podría calcular la velocidad de la luz. Fizeau reportó la velocidad de la luz como 313 000 km/s. El método de Fizeau fue refinado más tarde por Marie Alfred Cornu (1872) y Joseph Perrotin (1900), pero fue el físico francés Léon Foucault quien más profundizó en la mejora del método de Fizeau al reemplazar el engranaje por un espejo rotatorio. El valor estimado por Foucault, publicado en 1862, fue de 298 000 km/s. El método de Foucault también fue usado por Simon Newcomb y Albert Michelson, quien comenzó su larga carrera replicando y mejorando este método.
En 1926, Michelson utilizó espejos rotatorios para medir el tiempo que tardaba la luz en hacer un viaje de ida y vuelta entre la montaña Wilson y la montaña San Antonio en California. De las mediciones cada vez más exactas, resultó una velocidad de 299 796 km/s.
Otra forma de obtener la velocidad de la luz es medir independientemente la frecuencia  y la longitud de onda  de una onda electromagnética en el vacío. El valor de c puede entonces ser calculado mediante el uso de la relación . Una opción es medir la frecuencia de resonancia en una cavidad de resonancia. Si se conocen con precisión sus dimensiones, estas pueden ser utilizadas para determinar la longitud de onda de un haz de luz. En 1946, Louis Essen y AC Gordon-Smith utilizaron este método (las dimensiones de la cavidad de resonancia se establecieron con una precisión de alrededor de ± 0,8 micras utilizando medidores calibrados por interferometría), obteniendo un resultado de 299 792 ±9 kilómetros/s, sustancialmente más preciso que los valores calculados usando técnicas ópticas. En 1950, las mediciones repetidas establecieron un resultado de 299 792,5 ±3,0 kilómetros/s.
La interferometría es otro método para encontrar la longitud de onda de la radiación electromagnética para determinar la velocidad de la luz. Un haz de luz coherente (por ejemplo, un láser), con una frecuencia conocida , se divide siguiendo dos recorridos distintos y luego se recombina. Mediante el ajuste de la longitud del camino recorrido mientras se observa el patrón de interferencia, midiendo cuidadosamente el cambio en la longitud de la trayectoria, se puede determinar la longitud de onda de la luz .
La velocidad de la luz se calcula como en el caso anterior, utilizando la ecuación .
Antes de la llegada de la tecnología láser, se utilizaron fuentes coherentes de radio para las mediciones de interferometría de la velocidad de la luz. Sin embargo el método interferométrico se vuelve menos preciso con longitudes de onda reducidas, y los experimentos fueron por tanto limitados a la precisión de la longitud de onda larga (~ 0,4 cm ) de las ondas de radio. La precisión puede ser mejorada mediante el uso de luz con una longitud de onda más corta, pero a continuación, se hace difícil medir directamente su frecuencia. Una forma de evitar este problema es comenzar con una señal de baja frecuencia (cuyo valor se puede medir con precisión), y a partir de esta señal sintetizar progresivamente señales de frecuencias superiores, cuya frecuencia puede entonces relacionarse con la señal original. La frecuencia de un láser se puede fijar con notable precisión, y su longitud de onda se puede determinar entonces utilizando interferometría. Esta técnica la desarrolló un grupo del National Bureau of Standards (NBS) (que más tarde se convirtió en el NIST). Se utilizó en 1972 para medir la velocidad de la luz en el vacío con una incertidumbre fraccionaria de 3,5 × 10-9.
Con base en el trabajo de James Clerk Maxwell, se sabe que la velocidad de la radiación electromagnética es una constante definida por las propiedades electromagnéticas del vacío (constante dieléctrica y permeabilidad).
En 1887, los físicos Albert Michelson y Edward Morley realizaron el influyente experimento Michelson-Morley para medir la velocidad de la luz relativa al movimiento de la Tierra. La meta era medir la velocidad de la Tierra a través del éter, el medio que se pensaba en ese entonces necesario para la transmisión de la luz. Tal como se muestra en el diagrama del interferómetro de Michelson, se utilizó un espejo con media cara plateada para dividir un rayo de luz monocromática en dos rayos que viajaban en ángulos rectos uno respecto del otro. Después de abandonar la división, cada rayo era reflejado de ida y vuelta entre los espejos en varias ocasiones (el mismo número para cada rayo para dar una longitud de trayectoria larga pero igual; el experimento Michelson-Morley actual usa más espejos) entonces una vez recombinados producen un patrón de interferencia constructiva y destructiva.
Cualquier cambio menor en la velocidad de la luz en cada brazo del interferómetro cambiaría la cantidad de tiempo utilizada en su tránsito, que sería observado como un cambio en el patrón de interferencia. Durante los ensayos realizados, el experimento dio un resultado nulo.
Ernst Mach estuvo entre los primeros físicos que sugirieron que el resultado del experimento era una refutación a la teoría del éter. El desarrollo en física teórica había comenzado a proveer una teoría alternativa, la contracción de Lorentz, que explicaba el resultado nulo del experimento.
Es incierto si Einstein conocía los resultados de los experimentos de Michelson y Morley, pero su resultado nulo contribuyó en gran medida a la aceptación de su teoría de relatividad. La teoría de Einstein no requirió un elemento etérico sino que era completamente consistente con el resultado nulo del experimento: el éter no existe y la velocidad de la luz es la misma en cada dirección. La velocidad constante de la luz es uno de los postulados fundamentales (junto con el principio de causalidad y la equivalencia de los marcos de inercia) de la relatividad especial.

Alimento es cualquier sustancia consumida para proporcionar apoyo nutricional a un ser vivo.[1]​ Los alimentos suelen ser de origen vegetal, animal o fúngico y contienen nutrientes esenciales, como carbohidratos, grasas, proteínas, vitaminas o minerales. La sustancia es ingerida por un organismo y asimilada por las células del organismo para proporcionar energía, mantener la vida o estimular el crecimiento. Las diferentes especies de animales tienen diferentes comportamientos de alimentación que satisfacen las necesidades de sus metabolismos únicos, a menudo evolucionados para llenar un nicho ecológico específico dentro de contextos geográficos específicos.  
Además de los fines nutricionales, la alimentación humana se asocia a aspectos sociales y culturales, de salud y psicológicos. Así, por ejemplo, las bebidas alcohólicas en la alimentación humana no tienen interés nutricional, pero sí tienen un interés fruitivo. Por ello, son consideradas alimento. Por el contrario, no se consideran alimentos las sustancias que no se ingieren o que, una vez ingeridas, alteran las funciones metabólicas del organismo. De esta manera, la goma de mascar, el tabaco, los medicamentos y demás drogas no se consideran alimentos. 
Los alimentos contienen nutrientes y no-nutrientes, como la fibra vegetal, que aunque no proporcione a los humanos materia y energía, favorece el funcionamiento de la digestión.  Los alimentos sanitarios son el objeto de estudio de diversas disciplinas científicas: la biología, y en especial la ciencia de la nutrición, estudia los mecanismos de digestión y metabolización de los alimentos, así como la eliminación de los desechos por parte de los organismos; la ecología estudia las cadenas alimentarias; la química de alimentos analiza la composición de los alimentos y los cambios químicos que experimentan cuando se les aplican procesos tecnológicos, y la tecnología de los alimentos que estudia la elaboración, producción y manejo de los productos alimenticios destinados al consumo humano, y de algunas otras especies dependiendo el valor nutritivo y propiedades.
Casi todos los alimentos son de origen animal o vegetal, aunque existen algunas excepciones. Los alimentos que no provienen de fuentes animales o vegetales incluyen varios hongos comestibles, incluidos los champiñones. Los hongos, las bacterias ambientales son usadas en la preparación de alimentos encurtidos y fermentados, tales como pan con levadura, vino, cerveza, queso, pepinillos y yogur.
Muchas culturas consumen algas, e incluso cianobacterias como la espirulina.[2]​ Adicionalmente, la sal es frecuentemente consumida como saborizante o conservador, y el bicarbonato de sodio es usado en la preparación de alimentos. Ambas sustancias son inorgánicas y como el agua, una parte importante de la dieta humana.
Muchas plantas o sus partes son comidas como alimento. Existen aproximadamente 2000 especies de plantas cultivadas para alimento, y muchas tienen varios tipos de cultivo distintivos.[2]​ Los alimentos de origen vegetal pueden ser clasificados como con los nutrientes necesarios del crecimiento inicial de las plantas. Como consecuencia de esto, las semillas están frecuentemente llenas con energía, y son buenas fuentes de alimento para animales, incluidos los humanos. De hecho, la mayoría de todos los alimentos consumidos por los seres humanos son semillas. Esto incluye cereales (tales como el maíz, el trigo y el arroz), leguminosas (tales como frijoles, guisantes y lentejas) y nueces. Las oleaginosas, frecuentemente se prensan para producir aceites, incluyendo el de oliva, el girasol, la canola y el sésamo.[2]​
Las frutas son las extensiones maduras de las plantas, que incluyen en su interior las semillas. Las frutas suelen tener aspecto atractivo para los animales, de manera que estos se las coman y excreten las semillas a largas distancias. Las frutas son una parte significativa de la dieta de la mayoría de las culturas. Algunas frutas, tales como la calabaza y la berenjena, se consumen como vegetales.[2]​
Las verduras son un segundo tipo de materia vegetal consumido como alimento. Esto incluye raíces vegetales (tales como papas y zanahorias), hojas vegetales (tales como espinacas y lechugas), troncos vegetales (tales como bambú y espárragos) e inflorescencias vegetales (tales como alcachofas y brócoli). Muchas hierbas y especias son vegetales altamente saborizados.[2]​
Los animales pueden ser sacrificados y utilizados como alimentos directamente, o indirectamente por los productos que ellos producen. Cabe hacer notar que en el concepto de animales sacrificables por el hombre se incluyen: todas las especies de ganado de abasto (bovinos, porcinos, ovinos y caprinos), todo tipo de aves de corral (pollo, pavo, pato, ganso, etc), así como diversidad de especies de pescado y de los llamados mariscos (crustáceos y moluscos). La carne en un ejemplo de un producto directo tomado de un animal, el cual proviene ya sea del sistema muscular o a partir de órganos. Los productos alimenticios producidos por animales incluyen la leche producida por los mamíferos, la cual en muchas culturas es bebida o procesada en productos lácteos tales como el queso o la mantequilla. Además, las aves y otros animales producen huevos, los cuales son frecuentemente consumidos. También se suele incluir en el grupo, los productos que se obtienen de las abejas:miel un endulzante popular en muchas culturas, jalea real y cera. Algunas culturas consumen sangre, algunas en la forma de salchichas, como un producto para espesar salsas, o salada para tiempos de escasez de comida y otros usan una gran diversidad de vísceras comestibles: hígado, riñones, pulmones, piernas etc.
Desde que el humano comenzó a desarrollar su habilidad para hacer herramientas, sus técnicas para obtener alimento fueron evolucionando para satisfacer la demanda de estos. Comenzaron a sustituir la recolección por la agricultura. Todas las civilizaciones que se desarrollaron en la antigüedad desarrollaron técnicas de riego, almacenamiento y cultivo de productos vegetales, así como la ganadería que le permitía obtener alimento de los animales terrestres, basada en la domesticación de animales como la vaca, la oveja, el caballo o el perro; y la pesca que le permitía obtener alimentos provenientes del mar.
Actualmente, en las naciones desarrolladas, el suministro de alimentos cada vez depende más de la agricultura intensiva, del cultivo industrial, de la piscicultura o de otras técnicas que aumentan las cantidades de alimentos producidos a la vez que disminuyen su costo. Estas técnicas a su vez dependen del desarrollo de herramientas mecanizadas, desde la máquina de trillar y la sembradora hasta el tractor y la trilladora. Estas herramientas han sido combinadas con pesticidas que aseguran el elevado rendimiento de cosechas y combaten insectos o mamíferos que perjudican la producción. Sin embargo, las técnicas modernas no están ampliamente distribuidas en países donde la agricultura es potencialmente explotable, como en varios países del África ecuatorial, Asia meridional o América latina.
También se ha recurrido a la modificación genética de las plantas comestibles (OGM), para hacerlas más resistentes a las enfermedades y a los parásitos, a la vez que más productivas. Estas técnicas son muy contestadas, pues deja en manos de las multinacionales la producción de semillas, a la vez que se supone que la polinización cruzada de plantas naturales por plantas modificadas, puede alterar la calidad de aquellas.
Más recientemente se nota una tendencia creciente hacia prácticas agrícolas más sostenibles, que recurren a sistemas naturales de producción. Estos métodos, que se están extendiendo gracias a la demanda del consumo, estimula la biodiversidad, la auto-seguridad local y el cultivo orgánico.[3]​
La preparación de alimentos de origen animal que están dentro de los cárnicos, implica el sacrificio de los animales. Las técnicas del sacrificio son relativamente nuevas, ya que hasta la revolución hidráulica del siglo XIX, las carnicerías de ciudades europeas y americanas sacrificaban animales en plena vía pública, por lo que los restos de cerdos, cabras, reses y aves terminaban sobre las calles. Posteriormente, gente dedicada a esto, recogía los huesos y vísceras de animales que, o bien, estaban en un pozo, o de la misma calle para darle otros usos: los huesos de los animales se trituraban y se hacía abono de ellos.[4]​ Actualmente el proceso se ha vuelto más complejo, que incluye diversas etapas: sacrificio, evisceración, colgado, partición o corte y distribución. En países desarrollados, se realiza en una edificación especializada (matadero) que se usan para el procesado de animales en masa; estos mataderos quedan regulados por la ley solo en algunos países; en el caso de los Estados Unidos se estableció el Acta de 1958: el Sacrificio Humano, que específica que un animal debe ser atontado o golpeado antes de ser sacrificado. Este acto, como muchos otros en diversos países, queda exento en diversas leyes religiosas, como kosher. Interpretaciones estrictas del Kashrut requieren que el animal esté completamente consciente cuando su carótida sea cortada.[2]​
Algunas culturas producen alimentos para venderlos a restaurantes, y que se encarguen de distribuirlos de una manera más específica, por la que pagan los consumidores. Estos restaurantes suelen tener chefs entrenados quienes preparan la comida, mientras los meseros atienden a los clientes. El término restaurante viene de un vocablo francés empleado en la Francia del siglo XIX. Sin embargo, antes de la acuñación del término, la idea de un establecimiento que atendiera a varias personas, mientras en la cocina se preparaban todos los alimentos, dada de otros lugares y fechas anteriores: tanto en la ciudad de Pompeya como en la China de la dinastía Song; en los diversos países hispanos, estos establecimientos se llamaban anteriormente fondas. Los cafés, más tarde llamados cafeterías, del siglo XVII eran establecimientos pequeños donde solo se servía café en diversas presentaciones y con diversos acompañamientos, y se considera una versión temprana de los restaurantes.[5]​
Cada vez es más común la adquisición de alimentos mediante las tiendas de autoservicio (abiertas las 24 horas), las misceláneas o supermercados. Para que esto sea factible, es necesario que estos sean empacados (en el caso de alimentos sólidos) o embotellados (para alimentos sólidos o líquidos). Los empaques más comunes son los de plástico (bolsas cerradas herméticamente en las que se venden el pan de caja o el pan molido, granos como el arroz o el maíz; o bien, botellas plásticas, en las que se guardan jugos, refrescos, leche o agua), los de papel (bolsas cerradas con pegamento no tóxico, como las harinas), cajas (cereales, algunos jugos o leches, galletas, etc.) o de metal (latas o bolsas de aluminio o estaño).
El Banco Mundial reportó que la Unión Europea fue el principal importador de alimentos en el 2005 seguido a buena distancia por los Estados Unidos y Japón. Actualmente, los alimentos son comercializados y mercadeados globalmente. La variedad y la disponibilidad de alimentos ya no está restringida por la diversidad alimentos que crecen localmente o por las limitaciones de la temporada de crecimiento local.[6]​ Entre 1961 y 1999 ha habido un incremento del 400 % en la exportación de alimentos a nivel mundial.[7]​ actualmente algunos países son económicamente dependientes de la exportación de alimentos, la cual en algunos casos da cuenta por más del 80 % de todas las exportaciones.[8]​
En 1994, en la Reunión de Uruguay, más de 100 países se volvieron signatarios del acuerdo General sobre tarifas y comercio en un incremento dramático en la liberación del comercio, que incluyó un acuerdo para reducir el pago de subsidio a los agricultores, apuntalado por la Organización Mundial de Comercio en la aplicación de subsidios a la agricultura, tarifas, cuotas de importación y el acuerdo de disputas comerciales que no pueden ser resueltas bilateralmente.[9]​ Donde son levantadas barreras comerciales sobre disputas en asuntos de salud pública y seguridad, el WTO refiere la disputa a la Comisión del Codex Alimentarius, la cual fue fundada en 1962 por la Organización de las Naciones Unidas para la alimentación y agricultura y la Organización Mundial de la Salud. La liberalización del comercio ha afectado en gran medida el comercio mundial de alimentos.[10]​
El mercado de alimentos une al productor con el consumidor. Es la cadena de actividades que trae los alimentos desde la “puerta de la granja hasta el plato”.[11]​ El mercadeo de un solo producto alimenticio puede ser un proceso complicado involucrando muchos productores y compañías. Por ejemplo, 57 compañías están involucradas en la manufactura de la sopa de pollo con fideos enlatada. Estos negocios incluyen no solamente el pollo y los procesadores de vegetales sino también las compañías que transportan los ingredientes y las que imprimen las etiquetas o manufacturan las latas.[12]​ El sistema de mercadeo de alimentos es el mayor empleador no gubernamental tanto en forma directa como indirecta en los Estados Unidos.
En la era premoderna, la venta del excedente de alimentos se llevaba a cabo una vez a la semana cuando los granjeros llevaban sus mercancías el día de mercado, al mercado de alimentos local. Allí los alimentos se vendían a los tenderos para que a su vez los revendieran en sus tiendas donde los compraban los consumidores locales.[13]​[14]​ Con el comienzo de la industrialización y el desarrollo de la industria procesadora de alimentos, se pudo distribuir y vender en localidades distantes una mayor variedad de alimentos. Los primeros comercios de comestibles fueron tiendas con mostrador, en las cuales los compradores pedían al dependiente lo que querían, y este lo buscaba para el comprador.[13]​
Los supermercados nacieron en el siglo XX. Los supermercados implantaron la idea del autoservicio en la compra, usando carritos de mercado y ofrecieron alimentos de calidad a un precio menor gracias a la reducción de los costos de personal y a la economías de escala. En la última parte del siglo veinte, esto ha sido más revolucionario por el desarrollo de enormes supermercados del tamaño de depósitos, ubicados en las afueras de las ciudades vendiendo una amplia variedad de comidas de todo el mundo.[15]​
A diferencia de los procesadores de alimentos, la venta de alimentos al detal es un mercado de dos niveles en el cual un pequeño número de compañías muy grandes controlan una gran parte de los supermercados. Los supermercados gigantes ejercen un gran poder de compra sobre granjeros y procesadores y una fuerte influencia sobre los consumidores. No obstante, menos del 10 % de lo que los consumidores gastan en comida va a los agricultores, con grandes porcentajes destinados a la propaganda, transporte y corporaciones intermediarias.[16]​
Se denomina alimento orgánico, alimento ecológico,[17]​ o alimento biológico al producto agrícola o agroindustrial que se produce bajo un conjunto de procedimientos denominados “ecológicos”. En general, los métodos ecológicos evitan el uso de ciertos productos sintéticos, como pesticidas, herbicidas y fertilizantes artificiales específicos.
El movimiento de agricultura ecológica surgió en la década de 1940 como respuesta a la industrialización de la producción agrícola denominada revolución verde.[18]​ Actualmente la agricultura ecológica, así como la ganadería ecológica, es una industria fuertemente regulada, que en países como Japón, Canadá o la Unión Europea requiere certificaciones especiales para poder comercializar sus productos.
Comida orgánica o Alimento orgánico es un término que define los alimentos destinados al consumo que han sido producidos sin productos químicos y procesados sin aditivos (carnes, productos agrícolas, vinos y bebidas).
En el sector se utilizan en muchas ocasiones indistintamente los términos “ecológico”, “biológico” y “orgánico”. Sin embargo es conveniente resaltar el uso que se le da a cada palabra en el mercado. En España la fórmula más habitual es “ecológico”, al igual que en Alemania (biologisch) y Francia. En el mundo anglosajón el término “orgánico” es el más utilizado .
El empleo de estos términos está regulado por la ley, aunque en ocasiones autoriza que algunos alimentos utilicen el término “natural” en su denominación. Este es el caso del yogur, el café y el agua mineral, entre otros. Sin embargo, ciertos productos emplean este término como reclamo publicitario asociado a una mayor calidad.
Hay que fijarse bien ya que muchos productos del mercado utilizan la palabra “bio” en su etiqueta pero no son de origen biológico, sino que contienen bífidus, por ejemplo, por lo que diversas asociaciones ecológicas han denunciado el hecho.
Cuando se utilice el logotipo comunitario, la indicación del lugar en que se hayan obtenido las materias primas agrarias de que se compone el producto deberá figurar también en el mismo campo visual que el logotipo y adoptará diversas formas dependiendo de si todas las materias primas agrícolas han sido obtenidas en la Unión Europea o se utilizan todas o partes de ellas obtenidas en un tercer país.
Mientras que hay alimentos que pueden consumirse tal cual se obtienen (crudos) otros requieren procesarse por razones de seguridad, o bien, simple cuestión organoléptica (mejorar el olor, el sabor o el color); este tipo de métodos pueden ser de lo más sencillos, como el lavado, el cortado, el adorno o la mezcla de alimentos. Cuando todos estos procedimientos se juntan se dice que se está preparando una comida.
Una comida es la mezcla de uno o más alimentos sometidos a un proceso físico o químico, o bien, ambos. Dentro de los procesos físicos se contemplan el cortado, el mezclado, la trituración, la licuefacción (mezcla por corte de cuchillas), etc. Entre los procesos químicos, se encuentran la cocción, la fermentación, siendo estos los más comunes.
A la técnica de medición, preparación y perfecta combinación de ingredientes para formar un platillo, así como la sazón, se conoce como arte culinario.
El término cocina encierra un gran conjunto de métodos, herramientas e ingredientes para mejorar el sabor o digestibilidad de los alimentos. La técnica del cocinado, llamado arte culinario, generalmente requiere la selección, medición y combinación de ingredientes en un proceso ordenado para lograr el resultado deseado. Todo esto está aunado a la variedad de los ingredientes, condiciones ambientales, herramientas y, por supuesto, la destreza del cocinero.[2]​ La cocina es un elemento cultural que caracteriza a una nación o región del resto del mundo; las razones de que sea reconocible una comida de otra es su entorno geográfico, que incluye la especie de plantas y animales de la región, el clima y las necesidades nutricionales de los habitantes. También importan los facteres políticos, económicos y religiosos.[13]​
La cocción implica la aplicación de calor a un alimento que, usualmente, lo transforma químicamente, alterando el sabor, la textura, la apariencia y las propiedades nutricionales.[2]​ Cocinar supone someter los alimentos a distintas técnicas culinarias, generalmente, aplicando calor de una u otra forma. Cocer, propiamente, no es lo mismo que asar, ya que requiere necesariamente la inmersíón del alimento en agua, técnica que ha sido practicada desde el décimo milenio a. C. con la introducción de la alfarería.[2]​ Existe evidencia arqueológica de que el Homo erectus asaba alimentos en sus campamentos; el asado es la aplicación directa de calor o fuego sobre un alimento, sin que el agua figure como intermediaria.[19]​
La gastronomía (del griego γαστρονομία [gastronomía], del prefijo gastro = estómago y del sufijo -nomía = norma, regla.)[20]​ es el arte que estudia la relación del ser humano con su alimentación y su medio ambiente o entorno. El gastrónomo es el individuo que se preocupa de este arte.[21]​[22]​ A menudo se cree erróneamente que el término gastronomía únicamente tiene relación con el arte culinario y la cubertería en torno a una mesa. Sin embargo, esta es una pequeña parte del campo de estudio de dicha disciplina: no siempre se puede afirmar que un cocinero es un gastrónomo. La gastronomía estudia varios componentes culturales, tomando como eje central la comida.
La dieta régimen alimentario o hábito alimentario es la composición, frecuencia y cantidad de comida y bebidas que constituye la alimentación de los seres vivos conformando hábitos o comportamientos nutricionales.[24]​
En el caso de la alimentación humana, la dieta presenta grandes variaciones históricas y geográficas de acuerdo con factores culturales, individuales, ambientales, económicos, familiares, de disponibilidad de alimentos y otros. La relación entre la dieta y la salud está ampliamente estudiada por la medicina moderna y se ha definido una cantidad grande de dietas, ya sea para mantenerse saludable física y mentalmente, para corregir problemas de salud o bien para modificar características constitucionales. Estas definiciones incluyen la caracterización de los nutrientes, su cantidad y la frecuencia de consumo, así como parámetros metabólicos y físicos que constituyen sus fines objetivos, como, por ejemplo, mantener o lograr cierto peso corporal en relación con la estatura y constitución física (índice de masa corporal).[25]​[26]​
La dieta humana se considera equilibrada si aporta los nutrientes y energía en cantidades tales que permiten mantener las funciones del organismo en un contexto de salud física y mental.[27]​
Esta dieta equilibrada es particular de cada individuo y se adapta a su sexo, edad, peso y situación de salud. No obstante, existen diversos factores (geográficos, sociales, económicos, patológicos, etc.) que influyen en el equilibrio de la dieta.
Las dietas (cantidades y variedades de alimentos a consumir), se utilizan para el tratamiento y prevención de diversas patologías (dietoterapia) y para Etimológicamente la palabra «dieta» proviene del griego dayta, que significa ‘régimen de vida’. Se acepta como sinónimo de régimen alimenticio, que alude al ‘conjunto y cantidades de los alimentos o mezclas de alimentos que se consumen habitualmente’. También puede hacer referencia al régimen que, en determinadas circunstancias, realizan personas sanas, enfermas o convalecientes en el comer, beber y dormir. la alimentación a diversas situaciones fisiológicas.
En nutrición, la dieta es la suma de las comidas que realiza una persona u otro organismo, mientras que los hábitos dietéticos conforman el patrón de alimentación que sigue a diario, esto incluye las preferencias alimentarias, la influencia familiar y cultural sobre el individuo en lo que respecta a los alimentos que ingiere. Aunque los humanos en general son omnívoros, cada cultura mantiene preferencias y mitos sobre algunos alimentos. Por otra parte, tales preferencias alimentarias individuales puede ser o no saludables desde el punto de vista nutricional. Una alimentación equilibrada requiere una ingestión variada de alimentos con el fin de obtener las cantidades adecuadas de energía y nutrientes.
Los hábitos dietéticos son las decisiones habituales que una persona o cultura realiza cuando escoge los alimentos que comerá habitualmente.[28]​ Aunque los humanos son omnívoros, muchas culturas mantienen algunas preferencias alimenticias y algunos tabús alimenticios. También algunas dietas vienen definidas por la cultura o la religión. Por ejemplo, solo los alimentos kosher son permitidos por el judaísmo y alimentos halal/haram por el Islam, en la dieta de los creyentes.[29]​ Los hábitos dietéticos en diferentes países o regiones tiene diferentes características, muy relacionadas con una cultura culinaria. 
Los hábitos de dietéticos juegan un papel significativo y la salud y la mortalidad de todos los humanos. El desequilibrio entre el combustible consumido y energía gastada resulta en hambre o reservas excesivas de tejido adiposo, conocida como grasa corporal.[30]​ El consumo pobre de varias vitaminas y minerales puede conducir enfermedades las cuales pueden tener efectos de gran alcance sobre la salud. Por ejemplo, el 30 % de la población mundial tiene, o está en riesgo de desarrollar, deficiencia de iodo. Se ha estimado que por lo menos 3 millones de niños están ciegos debido a la deficiencia de vitamina A. La deficiencia de vitamina C resulta en escorbuto. El calcio, la vitamina D y el fósforo están interrelacionados; el consumo de cada uno puede afectar la solución de los otros. El Kwashiorkor y el marasmo son desórdenes de la niñez causados por la pérdida de proteína dietética.[31]​
Muchos individuos limitan los alimentos que consumen por razones morales u otros hábitos. Por ejemplo los vegetarianos escogen no consumir alimentos de origen animal en diferentes grados. Otros escogen una dieta más saludable, evitando azúcares o grasas animales e incrementando el consumo de fibra dietaria y antioxidantes.[32]​ La obesidad, un serio problema en el mundo occidental, incrementa la posibilidad de desarrollar enfermedades cardiacas, diabetes y muchas otras enfermedades.[31]​ Más recientemente, los hábitos dietéticos han sido influenciados por la preocupación que algunas personas tienen acerca del posible impacto sobre la salud o el medio ambiente a por el uso de alimentos modificados genéticamente.[33]​ Una preocupación adicional acerca del impacto de la agricultura industrial sobre el bienestar animal, la salud humana y el medio ambiente está también teniendo un efecto sobre los hábitos dietarios humanos contemporáneos. Esto ha conducido al surgimiento de una contracultura con una preferencia por los alimentos orgánicos y locales.[34]​
Entre los extremos de la salud óptima y la muerte por hambre o malnutrición, existe una serie de estados patológicos que pueden ser causados o mejorados por cambios en la dieta. Carencias, excesos o desequilibrios en la dieta pueden producir un impacto negativo sobre la salud, que puede conducir a enfermedades tales como el escorbuto, la obesidad o la osteoporosis, así como llevar a problemas psicológicos o de comportamiento. La ciencia de la nutrición trata de entender cómo y por qué ciertos aspectos específicos de la dieta tienen influjo sobre la salud.
Los nutrientes en los alimentos están agrupados dentro de varias categorías. Macro nutrientes: lípidos (grasas), proteínas y carbohidratos. Micronutrientes: vitaminas y minerales. Adicionalmente los alimentos contienen agua y fibra dietética.
Un nutrimento o nutriente es un producto químico procedente de la célula y que esta necesita para realizar sus funciones vitales. Es tomado por la célula y transformado en constituyente celular a través de un proceso metabólico de biosíntesis llamado anabolismo, o bien, es degradado para la obtención de otras moléculas, energía y nutrientes.
Llamamos alimentos a todas las sustancias que los seres vivos necesitan para llevar a cabo sus funciones vitales. Los alimentos contienen nutrientes que son sustancias con función directa o indirecta en las actividades celulares que ninguna otra sustancia puede desempeñar. 
Los nutrientes son cualquier elemento o compuesto químico necesario para el metabolismo de un ser vivo. Es decir, los nutrientes son algunas de las sustancias contenidas en los alimentos que participan activamente en las reacciones metabólicas para mantener todas las funciones del organismo.
Desde el punto de vista de la botánica y la ecología, los nutrimentos básicos son el oxígeno, el agua y los minerales necesarios para la vida de las plantas, que a través de la fotosíntesis incorporan la materia viva, constituyendo así la base de la cadena alimentaria, una vez que estos vegetales van a servir de alimento a los animales. Pero, luego de las cosechas, la planta se lleva consigo los nutrientes que extrajo para crecer. Para que el suelo se recupere de manera óptima, es esencial el uso de fertilizantes; productos naturales o sintéticos que reponen los nutrientes que las plantas necesitan para volver a crecer. La Urea Granulada es el fertilizante que mejor repone nitrógeno al suelo.[35]​
Los seres vivos que no tienen capacidad fotosintética, como los animales, los hongos y muchos protoctistas, se alimentan de plantas y de otros animales, ya sea vivos o en descomposición. Para estos seres, los nutrimentos son los compuestos orgánicos e inorgánicos contenidos en los alimentos y que, de acuerdo con su naturaleza química, se clasifican en los siguientes tipos de sustancias:
Los carbohidratos son azúcares integrados por monosacáridos. Los carbohidratos son clasificados por el número de unidades de azúcar: monosacáridos (tales como la glucosa, la fructosa y la galactosa), disacáridos (tales como la sacarosa, lactosa y maltosa) y polisacáridos (tales como el almidón, el glucógeno y la celulosa). Los carbohidratos brindan energía por más tiempo que las grasas.
Las proteínas son compuestos orgánicos que consiste en aminoácidos unidos por enlaces peptídicos. El organismo no puede fabricar alguno de los aminoácidos (llamados aminoácidos esenciales). Las proteínas crean enzimas, queratina, energía, anticuerpos, aumenta el sistema inmune y ayudan al crecimiento y desarrollo celular. En nutrición, las proteínas son degradadas por la pepsina, hasta aminoácidos libres, durante la digestión.
Las grasas consisten en una molécula de glicerina con tres ácidos grasos unidos. Los ácidos grasos son una larga cadena hidrocarbonada lineal no ramificada, conectadas solo por enlaces sencillos (ácidos grasos saturados) o por enlaces dobles y sencillos (ácidos grasos insaturados).
Las grasas son necesarias para mantener el funcionamiento apropiado de las membranas celulares, para aislar las vísceras contra el choque, para mantener estable la temperatura corporal y para mantener saludable el cabello y la piel. El organismo no fabrica ciertos ácidos grasos (llamados ácidos grasos esenciales) y la dieta debe suplirlos.
Las grasas tienen un contenido energético de 37,7 kJ/g (9 kcal/g); proteínas y carbohidratos tienen 16,7 kJ/g (4 kcal/g). El etanol tienen contenido de energía de 29,3 kJ/g (7 kcal/g).
Los lípidos regulan la temperatura del cuerpo por el aislamiento, y provee energía al cuerpo.
Las sales minerales son todos aquellos compuestos denominados como sales neutras, en las que todos los hidrógenos sustituibles son reemplazados por iones metálicos. La sal más importante que se puede obtener en cualquier dieta es el cloruro de sodio (NaCl), o sal de mesa, y es muy común su adición por parte de la mayoría de la población. La sal de mesa se ha asociado mucho con el sabor de las comidas, por lo que muchas personas la consumen en todos sus platos, tan así, que la comida les llega a saber insípida sin sal. Algunos problemas como la hipertensión arterial o la obesidad están relacionados con la ingesta excesiva de sal, ya que en ocasiones se llega a consumir hasta 15 g de sal al día, cuando la dosis recomendada es de 6 g. Como alternativa al consumo de sal excesivo, han aparecido en el mercado compuestos como el cloruro de potasio (KCl) o el cloruro amónico (NH4Cl).
Las sales minerales de cualquier tipo, son importantes en su consumo debido a que mantienen un correcto equilibrio metabólico al estar junto a los azúcares. Además de que ayudan a retener agua en el cuerpo para evitar la deshidratación y en caso de que haya escasez de líquido o que el cuerpo presente diarrea.
Las vitaminas son compuestos químicos en general muy complejos, de distinta naturaleza, pero que tienen en común que cantidades asombrosamente pequeñas son imprescindibles para el funcionamiento del organismo. La ausencia de algunas vitaminas causa enfermedades que pueden ser graves, y la ingesta de pequeñísimas cantidades (miligramos) puede subsanar este problema. Las cáscaras de las frutas son una fuente importante de algunas vitaminas. Hay dos tipos de vitaminas:
Si bien, existe la creencia popular de que las vitaminas pueden curar todo, desde resfriados hasta cáncer, actualmente se sabe que se eliminan fácilmente y el cuerpo no las absorbe, y que algunas vitaminas liposolubles cancelan a las vitaminas hidrosolubles.
De acuerdo con la manera en la que un alimento surge en la naturaleza, la presencia de ciertos compuestos inorgánicos como los minerales o elementos químicos es inherente en ellos. Los organismos son incapaces de producir los compuestos inorgánicos (compuestos cuya estructura básica no es el carbono). Dentro de los compuestos inorgánicos tenemos a los minerales, y se clasifican también, en un grupo aparte, al agua (H2O), dióxido de carbono (CO2), el nitrógeno (N2), el fósforo (PO4−3) y el azufre (S2).
Los minerales inorgánicos son necesarios para la reconstrucción de tejidos, reacciones enzimáticas, contracción muscular, reacciones nerviosas y coagulación sanguínea. Los minerales deben adquirirse mediante la dieta, contenidos en diversos alimentos, siendo los principales proveedores de minerales las plantas. Estos se dividen en dos clases:
Los aditivos son sustancias cuya función es proveer de características organolépticas diferentes a las naturales de un alimento, como son color, olor y sabor; o bien, alargar la vida útil o de anaquel de ese producto.
Los aditivos se clasifican en grupos de acuerdo a la alteración física o química que generen, teniendo así 7 grupos de aditivos:
Se incluyen aquí todas aquellas sustancias que alteren el color, ya sean químicas o naturales. Dentro de los colorantes naturales tenemos a la clorofila, los carotenoides o las tiocianinas. Dentro de los químicos, se pueden considerar los compuestos minerales como las sales de calcio y hierro, que además de ser buenos colorantes, aportan un valor nutricional.
Los conservantes son utilizados para evitar que microorganismos patógenos proliferen o envenenen un producto, aumentando así su vida útil. Dentro de los mejores conservadores, podemos incluir: al ácido sórbico, al ácido benzoico (y sus respectivas sales, dióxidos de sulfuros, nitritos y nitratos). Los conservadores cumplen también con la función de detener la rancidez de grasas o desnaturalización de proteínas. En la comida destacan 3 tipos de conservadores: naturales, físicos y químicos.
Los naturales son aquellos como la sal, el vinagre (ya sea blanco o de manzana) y el humo (un ejemplo sería la comida ahumada), los físicos son la esterilización y la pasteurización y hay varios químicos, como el benzoato de sodio.
Las grasas son compuestos que se oxidan con mucha facilidad, a ese fenómeno se le denomina rancidez. La presencia de la oxidación puede desnaturalizar las vitaminas liposolubles. Entre los principales compuestos antioxidantes, se pueden mencionar los ésteres de ácido gálico y el butil-hidroxitolueno. Es considerable destacar que los compuestos antioxidantes usados en alimentos surten efectos también en el cuerpo humano.
Los alimentos que han pasado por un proceso fermentativo (como el yogur) requieren de una atención especial para evitar que la acidez continúe subiendo. Son usados cualquier tipo de hidróxidos, sin embargo, no todos tienen una legalización en su uso (variando en cada país). En otras ocasiones, los ácidos sirven para dar sabor a un producto, por lo cual su adición es prescindible, pero para evitar que el medio continúe bajando el pH del producto se recomienda que se adicionen neutralizantes una vez que el sabor ha sido fijado. Una función adicional de los ácidos es su increíble acción antimicrobiana, por lo que una vez controlados se deben neutralizar los ácidos empleados.
En este grupo de aditivos podemos mencionar a todas aquellas sustancias que ayuden a los aceites y grasas a disolverse con agua y formar emulsiones suaves. Al igual, se utilizan para aumentar la duración de los productos horneados. La lecitina es el estabilizador comercial más empleado, mientras que las gomas vegetales y la pectina son perfectos emulgentes.
Estos evitan que los polvos (sales, harinas y demás) se hagan piedra o compactos por la presencia de humedad. De manera empírica, el arroz funciona como antiapelmazante de la sal de cocina. Se emplean polifosfatos y silicatos.
Su función es aumentar el sabor de una sustancia: endulzantes, acidificadores, extractos y demás.
La ayuda alimentaria puede beneficiar a personas que sufren escasez de alimentos. Puede usarse para mejorar la vida de las personas a corto plazo, de tal manera que una sociedad puede incrementar su nivel de vida hasta el momento en que la ayuda alimentaria no se necesite.[38]​ por otro lado, la ayuda alimentaria mal manejada puede crear problemas al irrumpir en los mercados locales, deprimiendo el precio de las cosechas y desincentivando la producción de alimentos. A veces se puede desarrollar un ciclo de dependencia a la ayuda alimentaria.[39]​ su provisión, o la amenaza de cese de la misma, muchas veces se usa como herramienta política para influir en la política del país beneficiario, una estrategia conocida como política alimentaria. A veces, las condiciones para dar la ayuda alimentaria, incluyen que ciertos tipos de alimentos sean comprados a ciertos vendedores y la ayuda alimentaria puede ser mal empleada para mejorar los mercados importadores del país donante.[40]​ los esfuerzos internacionales para distribuir alimentos a los países más necesitados frecuentemente están coordinados por el Programa Mundial de Alimentos (World Food Programme).[38]​
La privación de alimentos conduce a la malnutrición y por último a la inanición. Esto está frecuentemente relacionado con la hambruna, que supone la ausencia de alimento en comunidades enteras. Puede tener un efecto amplio y devastador en la salud y mortalidad humana. El racionamiento es usado a veces para distribuir alimentos en tiempos de escasez, frecuentemente en tiempos de guerra.[41]​
La inanición es un problema importante a nivel internacional. Aproximadamente 815 millones de personas están desnutridos y más de 16.000 niños mueren por día por causas relacionadas con el hambre.[42]​ La privación de alimento se considera como una necesidad insatisfecha, según la Jerarquía de necesidades de Maslow y se mide usando la escala de hambruna.[43]​
El derecho a la alimentación es un derecho humano que protege el derecho de las personas a alimentarse con dignidad, lo que implica que haya suficientes alimentos disponibles, que las personas tengan los medios para acceder a ellos y que satisfagan adecuadamente las necesidades dietéticas de las personas. El derecho a la alimentación protege el derecho de todos los seres humanos a no padecer hambre, inseguridad alimentaria ni malnutrición.[47]​ El derecho a la alimentación no implica que los gobiernos tengan la obligación de entregar alimentos gratis a todos los que los deseen, ni el derecho a ser alimentados. Sin embargo, si las personas se ven privadas del acceso a los alimentos por razones que escapan a su control, por ejemplo, porque están detenidas, en tiempos de guerra o después de desastres naturales, el derecho requiere que el gobierno proporcione alimentos directamente.[48]​
El derecho se deriva del Pacto Internacional de Derechos Económicos, Sociales y Culturales[48]​ que contaba con la suscripción de 170 Estados partes en abril de 2020.[45]​ Los Estados que firman el pacto acuerdan tomar medidas hasta el máximo de sus recursos disponibles para lograr progresivamente la plena realización del derecho a una alimentación adecuada, tanto a nivel nacional como internacional.[47]​[49]​ En un total de 106 países, el derecho a la alimentación es aplicable a través de arreglos constitucionales de diversas formas o mediante la aplicabilidad directa en la ley de varios tratados internacionales en los que se protege el derecho a la alimentación.[50]​
En la Cumbre Mundial sobre la Alimentación de 1996, los gobiernos reafirmaron el derecho a la alimentación y se comprometieron a reducir a la mitad el número de personas hambrientas y desnutridas de 840 a 420 millones para 2015. Sin embargo, el número ha aumentado en los últimos años, alcanzando un récord infame en 2009 de más de mil millones de personas desnutridas en todo el mundo.[47]​ Además, el número de personas que padecen hambre oculta (carencias de micronutrientes que pueden provocar un retraso en el crecimiento físico e intelectual de los niños) asciende a más de 2000 millones de personas en todo el mundo.[51]​
Las enfermedades transmitidas por los alimentos, comúnmente conocidas como envenenamiento alimentario, son causadas por bacterias, toxinas, virus, parásitos y priones. Cerca de 7 millones de personas mueren por envenenamiento alimentario cada año, con aproximadamente 10 veces más sufriendo de un envenenamiento no fatal.[55]​ Los dos factores más comunes que conducen a casos de enfermedades transmitidas por los alimentos de origen bacteriano son la contaminación cruzada de la comida lista para comer a partir de otros alimentos crudos y el control de temperatura inadecuado. Menos comúnmente, reacciones adversas agudas pueden también ocurrir si ocurre la contaminación química de los alimentos, por ejemplo a partir de almacenaje inapropiado o el uso de jabones y desinfectantes de grado no alimento. El alimento también puede ser adulterado por un muy amplio rango de artículos (conocidos como cuerpos extraños) durante la agricultura, la manufactura, la cocción, el empaquetamiento, la distribución o la venta. Estos cuerpos extraños pueden incluir plagas o sus desechos, cabellos, colillas de cigarrillos, astillas de madera y cualquier otra clase de contaminantes. Es posible que ciertos tipos de alimentos se contaminen cuando se almacenan o venden en envases no seguros, tal como un tarro de cerámica con esmaltado con una base de plomo.[55]​
El envenenamiento alimentario, ha sido reconocido por el hombre común enfermedad desde tiempos tan tempranos como Hipócrates.[56]​ La venta de alimentos rancios contaminados o adulterados fue una práctica común hasta la introducción de la higiene, refrigeración y control de vermes en el siglo XIX. El descubrimiento de técnicas para matar bacterias usando calor y otros estudios microbiológicos realizados por científicos tales como Luis Pasteur contribuyeron a la normativa sanitaria moderna que hoy en día es omnipresente en países desarrollados. Los trabajos de Justus von Liebig, también contribuyeron al desarrollo de los métodos modernos de almacenamiento y preservación de alimentos.[52] En años más recientes, un mayor entendimiento de las causas de las enfermedades transmitidas por los alimentos, lleva al desarrollo de estudios más sistemáticos tales como el Análisis de Peligros y Puntos de Control Críticos (APPCC), antiguamente ARICPC (Análisis de Riesgos y Puntos de Control Crítico), el cual puede identificar y eliminar muchos riesgos.[6]​
Desde que un alimento se produce (agrícolas, ganaderos, pesqueros, etc.) o fabrica (cualquier alimento manufacturado: pan, queso, entre otros), tiene riesgos de ser contaminado. Cuando un alimento se contamina, es porque el alimento ha adquirido nuevas propiedades que son perjudiciales para la salud humana. Un alimento puede tener tres tipos de contaminaciones:
La perecebilidad es el tiempo que tarda un alimento en comenzar a degradarse perdiendo sus propiedades nutrimentales. Se le conoce también como caducidad. De acuerdo a ese tiempo de duración, los alimentos se clasifican en:
Es importante no confundir en el etiquetado de alimentos la fecha de caducidad con la de consumo preferente.[61]​ La primera indica cuándo no debe tomarse un alimento porque pone en peligro la salud, mientras que la segunda solo indica la pérdida probable de propiedades del producto.
En los alimentos perecederos el transporte de los mismos forma una parte esencial. Es importante conocer la legislación y normas que rigen en el transporte de alimentos perecederos.[62]​
Algunas personas tienen alergias o sensibilidad a ciertos alimentos, que no constituyen un problema para el resto la gente. Esto ocurre cuando el sistema inmune de la persona confunde alguna proteína del alimento con un agente extraño dañino y lo ataca. Aproximadamente el 2 % de los adultos y el 8 % de los niños tienen alergias alimentarias.[55]​ En un alimento, la cantidad de la sustancia alergénica, requerida para provocar una reacción en un individuo particularmente sensible, puede ser pequeña. Se ha sabido que en algunas circunstancias, trazas de esas sustancias en el alimento, demasiado pequeñas para ser percibidas a través de olfato, han provocado reacciones letales en individuos extremadamente sensibles. Los alergenos alimenticios más comunes son el gluten, maíz, moluscos, cacahuete y soja.[55]​ Los alergenos frecuentemente producen síntomas tales como diarrea, erupciones, edema, vómitos y regurgitación. Normalmente las molestias digestivas se desarrollan dentro de la media hora de ingerido el alérgeno.[55]​
Rara vez las alergias alimenticias pueden conducir a una urgencia médica, tal como el shock anafiláctico, la hipotensión (baja presión arterial) y pérdida de la conciencia. Un alergeno asociado con este tipo de reacción es el cacahuete, aunque los productos del látex pueden inducir reacciones similares.[55]​ El tratamiento inicial es con epinefrina (adrenalina).
Algunos países tienen una definición legal de alimento. Estos países consideran alimento como cualquier artículo que es procesado, parcialmente procesado o de lo procesado para el consumo. El listado de artículos, incluye como comestibles cualquier sustancia, que intente ser, o que razonablemente se espera que sea, ingerida por humanos. En adición a estos comestibles, bebidas, goma de mascar, agua u otros artículos procesados y llamados artículos alimenticios son parte de la definición legal de alimento. Los artículos no incluidos en la definición legal de alimento incluyen a alimento para animales, animales vivos a menos que estén preparados para vender en un mercado, plantas antes de la cosecha, productos medicinales, cosméticos, tabaco y productos del tabaco, sustancias narcóticas o psicotrópicas y residuos y contaminantes.[63]​
El Diccionario de la Real Academia Española tiene una definición para alimento.

Se denomina azúcar en el uso más extendido de la palabra, a la sacarosa, cuya fórmula química es C12H22O11, también llamada «azúcar común», o «azúcar de mesa».
La sacarosa es un disacárido formado por una molécula de glucosa y una de fructosa, que se obtiene principalmente de la caña de azúcar o de la remolacha. El 27 % de la producción total mundial se realiza a partir de la remolacha y el 73 % a partir de la caña de azúcar. En Tailandia también suelen sacar azúcar de coco. 
La sacarosa se encuentra en todas las plantas, y en cantidades apreciables en otras plantas distintas de la caña de azúcar o la remolacha, como el sorgo, el coco y el arce azucarero.[1]​
En ámbitos industriales se usa la palabra azúcar o azúcares para designar los diferentes monosacáridos y disacáridos, que generalmente tienen sabor dulce, aunque por extensión se refiere a todos los hidratos de carbono.
El azúcar de coco es un producto derivado de extraer la savia de la flor del coco y calentarla para que se evapore la mayor parte del agua. Este procedimiento consigue que se retengan parte de los nutrientes de la flor y se obtiene un endulzante que puede tener una textura cristalina similar a la del azúcar moreno, con un ligero color y aroma tostado.
Funde a los 160 °C y calentada a 210 °C se transforma en una masa de color pardo denominada caramelo,[1]​ utilizada en la elaboración de dulces y pasteles, así como para la saborización y coloración de líquidos.[1]​
Si se calienta por encima de 145 °C en presencia de compuestos amino (NH2), derivados por ejemplo de proteínas, tiene lugar el complejo sistema de reacciones de Maillard, que genera colores, olores y sabores generalmente apetecibles, y también pequeñas cantidades de compuestos indeseables.
El azúcar es una importante fuente de calorías en la dieta alimenticia moderna, pero es frecuentemente asociada a calorías vacías, debido a la completa ausencia de vitaminas,minerales y sales.
En alimentos industrializados el porcentaje de azúcar puede llegar al 80 %.[2]​ La Organización Mundial de la Salud recomienda que el azúcar no supere el 10 % de las calorías diarias consumidas.[3]​[4]​ Investigaciones científicas y la práctica médica han asociado el exceso de consumo de azúcar con las siguientes afectaciones a la salud humana, como: Hipertensión arterial, osteoporosis, hiperactividad, avitaminosis, obesidad, cáncer, diabetes mellitus tipo 2, alzheimer, caries, entre otros.
El azúcar se ha producido en el subcontinente indio desde la antigüedad.[5]​ No era abundante o barata en los primeros tiempos y la miel se utilizaba con más frecuencia para endulzar en casi todo el mundo. Originalmente, la gente masticaba la caña de azúcar en bruto para extraer su dulzura. La caña de azúcar era una especie nativa de los trópicos, en Asia meridional y en el sudeste asiático.[6]​ Las diferentes especies de caña parecen tener su origen en diferentes lugares, siendo Saccharum barberi originaria de la India y S. edule y S. officinarum provenientes de Nueva Guinea.[6]​[7]​ Una de las referencias históricas más tempranas a la caña de azúcar está en manuscritos chinos del siglo VIII a. C. que afirman que el uso de la caña de azúcar se originó en la India.[cita requerida]
El azúcar no tuvo apenas importancia hasta que los indios descubrieron métodos para convertir el jugo de la caña de azúcar en cristales granulados que eran más fáciles de almacenar y transportar.[8]​ Fueron descubiertos cristales de azúcar de la época de la Gupta Imperial, alrededor del siglo V d. C.[8]​ En la lengua indígena local, estos cristales se llaman khanda (खण्ड, khaṇḍa).[9]​
Los marineros indios, quienes llevaban mantequilla y azúcar como suministros, introdujeron el conocimiento del azúcar en las diversas rutas comerciales por las que viajaban. Los monjes budistas, en sus viajes, llevaron los métodos de cristalización de azúcar a China. Durante el reinado de Harsha (606 a 647) en el norte de la India, los enviados de la India en la China Tang enseñaron métodos de cultivo de la caña de azúcar después del reinado de Li Shimin (que reinó de 626-649), que manifestó su interés por el azúcar. Posteriormente, China estableció sus primeras plantaciones de caña de azúcar en el siglo VII.[10]​ Los documentos chinos confirman, al menos, dos expediciones a la India, iniciadas en el 647, para obtener la tecnología para el refinado del azúcar.[11]​ En el sur de Asia, en Oriente Medio y en China, el azúcar se convirtió en un elemento básico de la cocina y de los postres.
Las conquistas de Alejandro Magno se detuvieron a orillas del río Indo por la negativa de sus tropas para ir más al este. Allí vieron a las personas en el subcontinente indio cultivando la caña y fabricando un dulce granulado, localmente llamado sharkara (शर्करा, sarkara), pronunciado como saccharum (ζάκχαρι). En su viaje de regreso, los soldados macedonios se llevaron "cañas de miel" con ellos. La caña de azúcar se mantuvo como un cultivo poco conocido en Europa durante más de un milenio. El azúcar era un bien escaso y los comerciantes de azúcar eran ricos.[12]​
Los cruzados trajeron con ellos el azúcar a Europa después de sus campañas en Tierra Santa, donde se encontraron con caravanas que transportaban esa "sal dulce". A principios del siglo XII, Venecia adquirió algunas aldeas cerca de Tiro y estableció fincas para producir azúcar para exportar a Europa, donde se complementaba con la miel, que anteriormente había sido el único edulcorante disponible.[13]​ El cronista de las cruzadas Guillermo de Tiro, en un escrito de finales del siglo XII, describió el azúcar como un producto "muy necesario para el uso y la salud de la humanidad".[14]​ En el siglo XV, Venecia era el principal centro de refinación y distribución de azúcar de Europa.[12]​
En agosto de 1492, Cristóbal Colón se detuvo en La Gomera, en las Islas Canarias para cargar vino y agua, con la intención de permanecer solo cuatro días. No obstante, tuvo una relación sentimental con Beatriz de Bobadilla y se quedó un mes. Cuando finalmente iba a partir, ella le dio unas cañas de azúcar, que fueron las primeras en llegar a América.[15]​
Los portugueses llevaron el azúcar a Brasil. En torno a 1540 había 800 fábricas de azúcar de caña en la isla de Santa Catarina y había otras 2000 en la costa norte de Brasil, en Demarara y en Surinam. La primera zafra tuvo lugar en la isla de La Española en 1501; y se construyeron muchos ingenios azucareros (fábricas) en Cuba y Jamaica en la década de 1520.[16]​
El azúcar fue un lujo en Europa hasta el siglo XVIII, en que se hizo más asequible. Luego se popularizó y en el siglo XIX el azúcar llegó a ser considerado una necesidad. Esta evolución del gusto y de la demanda de azúcar como ingrediente de alimentos esenciales desató grandes cambios económicos.[17]​ Durante los siglos XVIII y XIX muchos europeos prosperaron con la industria azucarera en las Antillas y otros lugares de América. La demanda de mano de obra barata para realizar el duro trabajo necesario para su cultivo y procesamiento aumentó la demanda de la trata de esclavos del África subsahariana. También hubo una gran demanda de trabajadores semi-esclavos contratados en Asia. La mezcla étnica moderna de muchas regiones ha sido influenciada por la demanda de azúcar.[18]​[19]​[20]​
El azúcar también llevó a algunos industrialización de las antiguas colonias. Por ejemplo, el teniente J. Paterson, del establecimiento de Bengala, convenció al gobierno británico que la caña de azúcar podría ser cultivada en la India británica con muchas ventajas y a menor coste que en las Indias Occidentales. Como resultado, las fábricas de azúcar se establecieron en Bihar, al este de la India.[21]​
En el siglo XVIII se descubrió la manera de extraer azúcar de una variante de la remolacha. Durante las guerras napoleónicas, la producción de remolacha azucarera aumentó en la Europa continental debido a la dificultad de importar azúcar cuando el envío fue objeto de bloqueo. En 1880, la remolacha azucarera era la principal fuente de azúcar en Europa. Esta se cultivaba en Lincolnshire y otras partes de Inglaterra, aunque el Reino Unido siguió importando la parte principal de su azúcar de sus colonias.[22]​
Hasta finales del siglo XIX, el azúcar fue comprado en panes de azúcar (en inglés, sugarloafs, que significa hogazas de azúcar), que tenían que ser cortados.[23]​ En años posteriores, el azúcar se vendió habitualmente granulado y en bolsas.
Los terrones de azúcar se produjeron en el siglo XIX. El primer inventor de un proceso para disponer el azúcar en forma de cubo fue el moravo Jakub Kryštof Rad, director de una empresa azucarera en Dačice. Comenzó la producción de terrones de azúcar después de haber adquirido una patente de cinco años, el 23 de enero de 1843. Henry Tate, de Tate & Lyle, fue otro de los primeros fabricantes de terrones de azúcar en sus refinerías en Liverpool y Londres. Tate adquirió una patente para la fabricación de terrones de azúcar del alemán Eugen Langen, quien en 1872 había inventado un método diferente de procesamiento de terrones de azúcar.[24]​
El azúcar es un endulzante de origen natural, sólido, cristalizado, constituido esencialmente por cristales sueltos de sacarosa, obtenidos a partir de la caña de azúcar (saccharum officinarum L) o de la remolacha azucarera (beta vulgaris L) mediante procedimientos industriales apropiados. Un grano de azúcar es entre 30 y 70 % menor que el grano de arroz.
El azúcar blanco se somete a un proceso de purificación química —llamado sulfitación— haciendo pasar a través del jugo de caña el gas SO2 obtenido por combustión de azufre.
La película de miel que rodea el cristal de azúcar moreno o rubio contiene sustancias como minerales y vitaminas. En el argot azucarero, a estas sustancias se les llama impurezas. Cabe aclarar que, durante el proceso de refinación, a todas las sustancias que no son sacarosa se consideran impurezas, pero son inofensivas para la salud. Y son estas las que le otorgan el color y sabor particular.
Cada día es mucho más frecuente en platos y dulces preparados encontrarse otros azúcares diferentes; glucosa, fructosa —básicamente de la planta de maíz, preferida por su asimilación más lenta[cita requerida]— o combinados con edulcorantes artificiales.
La palabra azúcar viene del sánscrito sharkara, que los persas transformaron en sakar. Los griegos tomarían el término persa y lo llamarían sakjar. El árabe clásico tomó el término griego y lo llamó sukkar, y posteriormente el árabe hispano lo llamó assúkar. El sánscrito tomó la palabra sharkara de çarkara, que significa arenilla, ya que llamaban así al polvo blanquecino de la caña de azúcar.[26]​
Según la Real Academia Española, el azúcar tiene género ambiguo, pero cuando va sin especificativo es mayoritario su empleo en masculino.[27]​
A pesar de que no empieza con una letra a tónica, su artículo siempre se utiliza masculino.[28]​
El azúcar se puede clasificar por su origen (de caña de azúcar o remolacha), pero también por su grado de refinación o sus características. Normalmente, la refinación se expresa visualmente a través del color (azúcar moreno, azúcar rubio, blanco), que está dado principalmente por el porcentaje de sacarosa que contienen los cristales.
Los tipos de azúcar que se comercializan habitualmente son los siguientes:
El procesamiento del azúcar se puede dividir en las siguientes etapas:
En el mercado del azúcar se distinguen dos tipos de productos, el azúcar cruda y el azúcar refinada o blanca. Dentro de cada tipo existen diferentes categorías según sus diferentes calidades. El azúcar cruda se produce solamente de caña de azúcar, en tanto el azúcar refinada se produce tanto de caña de azúcar como de remolacha azucarera. En este sentido, se considera que la industria de la caña de azúcar tiene una mayor flexibilidad para responder a los cambios de precios relativos entre azúcar cruda y azúcar refinada.[32]​[33]​
El mercado mundial del azúcar es uno de los más distorsionados del mundo como resultado de un amplio conjunto de políticas de protección y de subsidio a la producción y exportaciones por parte de los principales países productores y consumidores del mundo. A nivel general, se pueden distinguir, básicamente, dos tipos de mercados de azúcar: el mercado protegido y el mercado libre.[33]​
El mercado protegido consiste en acuerdos preferenciales y contratos de largo plazo que incluyen el sistema de cuotas de los Estados Unidos, las cuotas de la Unión Europea, las exportaciones de Cuba a China y las exportaciones de Australia a Canadá.[34]​[33]​
En el mercado libre se transan los volúmenes no cubiertos por convenios especiales. Estas transacciones se realizan preferentemente en las diferentes bolsas azucareras, entre las cuales se encuentran las de Nueva York, Londres, París y Hong Kong. Además de transacciones spot, en el mercado libre de azúcar se utilizan instrumentos tales como forward, futuros y derivados.[35]​[33]​
Los principales productores de azúcar son:
En 2003, 16 países concentraban el 87,1 % de la producción mundial.[37]​
El consumo mundial se presenta del siguiente modo:
El alto consumo de azúcar demostró que aumenta significativamente la tensión sistólica y la presión arterial diastólica; las personas que consumen el 25% o más de calorías de azúcar tienen casi tres veces mayor riesgo de muerte por enfermedad cardiovascular.[39]​[2]​
Elimina calcio del hueso: el cuerpo extrae calcio del hueso para neutralizar la acidez provocada por el azúcar refinado y ese calcio se pierde por la orina.
Un metaanálisis reveló que el consumo de azúcar no mejora el estado de ánimo, pero que puede reducir el estado de alerta y aumentar la fatiga.[40]​ Algunos estudios reportan relación causa efecto entre un alto consumo de azúcar refinado e hiperactividad,[41]​ otros estudios sugieren que la cantidad de azúcar en la dieta no influye en el comportamiento infantil, sino que los padres que tienen prejuicios hacia los efectos de los dulces perciben erróneamente que sus hijos están más inquietos y nerviosos cuando comen golosinas.[42]​
Los alimentos ricos en azúcar suelen contener menor cantidad de vitaminas y minerales y pueden estar reemplazando a alimentos más nutritivos. Asimismo, contienen un exceso de calorías, lo cual puede ocasionar obesidad.[43]​ Esto es lo que se conoce como el argumento de la caloría vacía.
Diversos estudios de investigación indican que las células cancerosas consumen más azúcar (glucosa) que las células normales. No obstante, ningún estudio ha demostrado que consumir azúcar empeore el cáncer ni que eliminar su consumo lo haga disminuir o desaparecer[44]​[45]​ y diferentes estudios evidencian que no existe asociación entre el consumo de azúcar y el cáncer. Solo existe evidencia posible de una relación entre la ingesta de monosacáridos (fructosa y glucosa) y el riesgo de desarrollar cáncer de páncreas, y entre el índice glucémico (IG) y el cáncer colorrectal.[45]​
No obstante, algunos autores señalan que una alimentación con un alto contenido de azúcar puede ocasionar un excesivo aumento de peso, y la obesidad está asociada a un riesgo elevado de padecer diversos tipos de cáncer.[44]​ Otros autores señalan que la evidencia sobre la asociación entre la ingesta de azúcar añadido y el riesgo de desarrollar cáncer en adultos o en niños es insuficiente.[45]​
Actualmente, se conoce que las dietas ricas en azúcar pueden provocar un aumento excesivo de peso y resistencia a la insulina,[46]​ lo cual predispone a padecer diabetes mellitus tipo 2 (DMT2). Esta enfermedad ha experimentado un drástico aumento de incidencia en las últimas décadas, principalmente debido a factores del estilo de vida occidental, como la falta de ejercicio y las dietas altas en calorías.
Se ha demostrado consistentemente que la DMT2 es un factor de riesgo para la enfermedad de Alzheimer. Por lo tanto, los cambios en la dieta pueden reducir significativamente el riesgo de desarrollar DMT2 y enfermedad de Alzheimer, y con ello aumentar la calidad de vida y mejorar la longevidad.[47]​
Una dieta alta en azúcar es la principal causa de la aparición de caries dental.[48]​ Un informe de la OMS afirmó que "el azúcar es indudablemente el factor dietético más importante en el desarrollo de caries" [49]​ Una revisión de estudios en humanos mostró que la incidencia de caries es menor cuando la ingesta de azúcar es inferior al 10% de la energía total consumida.[50]​
Como alternativas al azúcar corriente en su condición de edulcorante, tratando de evitar sus malas consecuencias, con consecuencias negativas menores o a veces mayores aunque diversas, ya que la polémica continúa, se han presentado sustancias tales como el acesulfamo-K, el aspartamo, la estevia, la fructosa, el jarabe de agave, el maltitol, el manitol, la miel, la sucralosa, entre otros.
El Diccionario de la Real Academia Española tiene una definición para azúcar.

La geometría (del latín geometrĭa, y este del griego γεωμετρία de γῆ gē, ‘tierra’, y μετρία metría, ‘medida’) es una rama de las matemáticas que se ocupa del estudio de las propiedades de las figuras  en el plano o el espacio,[1]​ incluyendo: puntos, rectas, planos, politopos (como paralelas, perpendiculares, curvas, superficies, polígonos, poliedros, etc.).
Es la base teórica de la geometría descriptiva o del dibujo técnico. También da fundamento a instrumentos como el compás, el teodolito, el pantógrafo o el sistema de posicionamiento global (en especial cuando se la considera en combinación con el análisis matemático y sobre todo con las ecuaciones diferenciales).
Sus orígenes se remontan a la solución de problemas concretos relativos a medidas. Tiene su aplicación práctica en física aplicada, mecánica, arquitectura, geografía, cartografía, astronomía, náutica, topografía, balística, etc., y es útil en la preparación de diseños e incluso en la fabricación de artesanía. 
La civilización babilónica fue una de las primeras culturas en incorporar el estudio de la geometría. La invención de la rueda abrió el camino al estudio de la circunferencia y posteriormente al descubrimiento del número π (pi); También desarrollaron el sistema sexagesimal, al conocer que cada año cuenta con 365 días, además implementaron una fórmula para calcular el área del trapecio rectángulo.[2]​
En el antiguo Egipto estaba muy desarrollada, según los textos de Heródoto, Estrabón y Diodoro Sículo. Euclides, en el siglo III a. C. configuró la geometría en forma axiomática y constructiva,[3]​ tratamiento que estableció una norma a seguir durante muchos siglos: la geometría euclidiana descrita en Los Elementos.
La geometría se propone ir más allá de lo alcanzado por la intuición. Por ello, es necesario un método riguroso, sin errores; para conseguirlo se han utilizado históricamente los sistemas axiomáticos. El primer sistema axiomático lo establece Euclides, aunque era incompleto. David Hilbert propuso a principios del siglo XX otro sistema axiomático, este ya completo.
Como en todo sistema formal, las definiciones, no solo pretenden describir las propiedades de los objetos, o sus relaciones. Cuando se axiomatiza algo, los objetos se convierten en entes abstractos ideales y sus relaciones se denominan modelos.
Esto significa que las palabras «punto», «recta» y «plano» deben perder todo significado material. Cualquier conjunto de objetos que verifique las definiciones y los axiomas cumplirá también todos los teoremas de la geometría en cuestión, y sus relaciones serán virtualmente idénticas al del modelo «tradicional».
Los siguientes son algunos de los conceptos más importantes en geometría. [4]​     [5]​[6]​
En geometría euclidiana, los axiomas y postulados son proposiciones que relacionan conceptos, definidos en función del punto, la recta y el plano. Euclides planteó cinco postulados y fue el quinto (el postulado de paralelismo) el que siglos después —cuando muchos geómetras lo cuestionaron al analizarlo— originará nuevas geometrías: la elíptica (geometría de Riemann) o la hiperbólica de Nikolái Lobachevski.
En geometría analítica, los axiomas se definen en función de ecuaciones de puntos, basándose en el análisis matemático y el álgebra. Adquiere otro nuevo sentido hablar de puntos, rectas o planos. f(x) puede definir cualquier función, llámese recta, circunferencia, plano, etc.
Euclides adoptó un enfoque abstracto de la geometría en sus Elementos, [7]​ uno de los libros más influyentes jamás escritos. [8]​ Euclides introdujo ciertos axiomas o postulados que expresan propiedades primarias o evidentes de puntos, líneas y planos. [9]​ Procedió a deducir rigurosamente otras propiedades mediante el razonamiento matemático. El rasgo característico de la aproximación de Euclides a la geometría fue su rigor, y ha llegado a conocerse como geometría axiomática o sintética. [10]​ A principios del siglo XIX, el descubrimiento de geometrías no euclidianas por Nikolai Ivanovich Lobachevsky (1792-1856), János Bolyai (1802-1860), Carl Friedrich Gauss (1777-1855) y otros [11]​ llevaron a un resurgimiento del interés por esta disciplina, y en el siglo XX, David Hilbert (1862 –1943) empleó el razonamiento axiomático en un intento de proporcionar una base moderna de la geometría. [12]​
Los puntos se consideran objetos fundamentales en la geometría euclidiana. Se han definido de diversas formas, incluida la definición de Euclides como "aquello que no tiene parte" [13]​ y mediante el uso de álgebra o conjuntos anidados. [14]​ En muchas áreas de la geometría, como la geometría analítica, la geometría diferencial y la topología, se considera que todos los objetos se construyen a partir de puntos. Sin embargo, se ha realizado algún estudio de geometría sin referencia a puntos. [15]​
Euclides describió una línea como "longitud sin ancho" que "se encuentra igualmente con respecto a los puntos sobre sí misma". [13]​ En las matemáticas modernas, dada la multitud de geometrías, el concepto de línea está estrechamente relacionado con la forma en que se describe la geometría. Por ejemplo, en geometría analítica, una línea en el plano a menudo se define como el conjunto de puntos cuyas coordenadas satisfacen una ecuación lineal dada, [16]​ pero en un entorno más abstracto, como la geometría de incidencia, una línea puede ser un objeto independiente, distinto del conjunto de puntos que se encuentran en él. [17]​ En geometría diferencial, una geodésica es una generalización de la noción de línea a espacios curvos.  [18]​
Un plano es una superficie plana bidimensional que se extiende infinitamente. [13]​ Los planos se utilizan en todas las áreas de la geometría. Por ejemplo, los planos se pueden estudiar como una superficie topológica sin hacer referencia a distancias o ángulos; [19]​ se puede estudiar como un espacio afín , donde se pueden estudiar la colinealidad y las proporciones pero no las distancias; [20]​ se puede estudiar como el plano complejo utilizando técnicas de análisis complejo ; [21]​ y así sucesivamente.
Euclides define un ángulo plano como la inclinación entre sí, en un plano, de dos líneas que se encuentran y no son rectas entre sí. [13]​En términos modernos, un ángulo es la figura formada por dos rayos de luz, llamados lados del ángulo, que comparten un punto final común, llamado vértice del ángulo.[22]​
En la geometría euclidiana, los ángulos se utilizan para estudiar polígonos y triángulos, además de formar un objeto de estudio por derecho propio.[13]​ El estudio de los ángulos de un triángulo o de los ángulos en un círculo unitario forma la base de la trigonometría. [23]​
En geometría diferencial y cálculo, los ángulos entre curvas planas o curvas espaciales o superficies se pueden calcular utilizando la derivada  [24]​[25]​
Una curva es un objeto unidimensional que puede ser recto (como una línea) o no; las curvas en el espacio bidimensional se denominan curvas planas y las del espacio tridimensional se denominan curvas espaciales. [26]​
En topología, una curva se define mediante una función de un intervalo de los números reales a otro espacio. [19]​ En geometría diferencial, se usa la misma definición, pero se requiere que la función definitoria sea diferenciable.[27]​ La geometría algebraica estudia las curvas algebraicas, que se definen como variedades algebraicas de dimensión uno. [28]​
Una superficie es un objeto bidimensional, como una esfera o un paraboloide. [29]​ En geometría diferencial [27]​   y topología, [19]​ las superficies se describen mediante "parches" bidimensionales (o vecindades ) que se ensamblan mediante difeomorfismos u homeomorfismos, respectivamente. En geometría algebraica, las superficies se describen mediante ecuaciones polinómicas. [28]​
Una variedad es una generalización de los conceptos de curva y superficie. En topología, una variedad es un espacio topológico donde cada punto tiene una vecindad que es homeomorfa al espacio euclidiano. [19]​En geometría diferencial, una variedad diferenciable es un espacio donde cada vecindario es difeomórfico al espacio euclidiano. [27]​
Las variedades se utilizan ampliamente en física, incluida la relatividad general y la teoría de cuerdas. [30]​
La longitud, el área y el volumen describen el tamaño o la extensión de un objeto en una dimensión, dos dimensiones y tres dimensiones, respectivamente. [31]​
En geometría euclidiana y geometría analítica, la longitud de un segmento de línea a menudo se puede calcular mediante el teorema de Pitágoras. [32]​
El área y el volumen pueden definirse como cantidades fundamentales separadas de la longitud, o pueden describirse y calcularse en términos de longitudes en un plano o espacio tridimensional. [31]​ Los matemáticos han encontrado muchas fórmulas explícitas para el área y fórmulas para el volumen de varios objetos geométricos. En cálculo, el área y el volumen se pueden definir en términos de integrales, como la integral de Riemann [33]​ o la integral de Lebesgue. [34]​
El concepto de longitud o distancia se puede generalizar, dando lugar a la idea de métricas. [35]​ Por ejemplo, la métrica euclidiana mide la distancia entre puntos en el plano euclidiano, mientras que la métrica hiperbólica mide la distancia en el plano hiperbólico. Otros ejemplos importantes de métricas incluyen la métrica de Lorentz de la relatividad especial y la métrica semirriemanniana de la relatividad general. [36]​
En otra dirección, los conceptos de longitud, área y volumen se amplían con la teoría de la medida, que estudia métodos de asignación de un tamaño o medida a conjuntos, donde las medidas siguen reglas similares a las del área y volumen clásicos. [37]​
La congruencia y la similitud son conceptos que describen cuando dos formas tienen características similares. [38]​ En la geometría euclidiana, la similitud se usa para describir objetos que tienen la misma forma, mientras que la congruencia se usa para describir objetos que son iguales tanto en tamaño como en forma. [39]​ Hilbert, en su trabajo sobre la creación de una base más rigurosa para la geometría, trató la congruencia como un término indefinido cuyas propiedades están definidas por axiomas.
La congruencia y la similitud se generalizan en la geometría de transformación, que estudia las propiedades de los objetos geométricos que se conservan mediante diferentes tipos de transformaciones. [40]​
Los geómetras clásicos prestaron especial atención a la construcción de objetos geométricos que se habían descrito de alguna otra manera. Clásicamente, los únicos instrumentos permitidos en las construcciones geométricas son el compás y la regla. Además, cada construcción tenía que completarse en un número finito de pasos. Sin embargo, algunos problemas resultaron difíciles o imposibles de resolver solo por estos medios, y se encontraron ingeniosas construcciones utilizando parábolas y otras curvas, así como dispositivos mecánicos.
Donde la geometría tradicional permitía las dimensiones 1 (una línea), 2 (un plano ) y 3 (nuestro mundo ambiental concebido como un espacio tridimensional ), los matemáticos y físicos han utilizado dimensiones superiores durante casi dos siglos.[41]​ Un ejemplo de uso matemático para dimensiones superiores es el espacio de configuración de un sistema físico, que tiene una dimensión igual a los grados de libertad del sistema. Por ejemplo, la configuración de un tornillo se puede describir mediante cinco coordenadas. [42]​
En topología general, el concepto de dimensión se ha extendido desde los números naturales hasta la dimensión infinita (espacios de Hilbert, por ejemplo) y los números reales positivos (en geometría fractal).[43]​ En geometría algebraica, la dimensión de una variedad algebraica ha recibido varias definiciones aparentemente diferentes, que son todas equivalentes en los casos más comunes.[44]​
El tema de la simetría en geometría es casi tan antiguo como la ciencia de la geometría misma.[45]​ Las formas simétricas como el círculo, los polígonos regulares y los sólidos platónicos tenían un significado profundo para muchos filósofos antiguos [46]​ y fueron investigadas en detalle antes de la época de Euclides.[9]​ Los patrones simétricos ocurren en la naturaleza y fueron representados artísticamente en una multitud de formas, incluyendo los gráficos de Leonardo da Vinci, MC Escher y otros.[47]​ En la segunda mitad del siglo XIX, la relación entre simetría y geometría fue objeto de un intenso escrutinio. 
El programa de Erlangen de Felix Klein proclamó que, en un sentido muy preciso, la simetría, expresada a través de la noción de un grupo de transformación, determina qué es la geometría.[48]​ La simetría en la geometría euclidiana clásica está representada por congruencias y movimientos rígidos, mientras que en la geometría proyectiva juegan un papel análogo las colinaciones, transformaciones geométricas que convierten las líneas rectas en líneas rectas.[49]​ Sin embargo, fue en las nuevas geometrías de Bolyai y Lobachevsky, Riemann, Clifford y Klein, y Sophus Lieque la idea de Klein de "definir una geometría a través de su grupo de simetría" encontró su inspiración.[50]​ Tanto las simetrías discretas como las continuas juegan un papel destacado en la geometría, la primera en la topología y la teoría de grupos geométricos,[51]​[52]​ la última en la teoría de Lie y la geometría de Riemann. [53]​[54]​
Un tipo diferente de simetría es el principio de dualidad en la geometría proyectiva, entre otros campos. Este meta-fenómeno se puede describir aproximadamente de la siguiente manera: en cualquier teorema, intercambiar ”punto” con “plano”, “unirse” con “encuentro”, “se encuentra” con “contiene”, y el resultado es un teorema igualmente verdadero.[55]​  Existe una forma de dualidad similar y estrechamente relacionada entre un espacio vectorial y su espacio dual.[56]​
El campo de la topología, que tuvo un gran desarrollo en el siglo XX, es en sentido técnico un tipo de geometría transformacional, en que las transformaciones que preservan las propiedades de las figuras son los homeomorfismos (por ejemplo, esto difiere de la geometría métrica, en que las transformaciones que no alteran las propiedades de las figuras son las isometrías). Esto ha sido frecuentemente expresado en la forma del dicho: "la topología es la geometría de la página de goma".
Desde los antiguos griegos, han existido numerosas contribuciones a la geometría, particularmente a partir del siglo XVIII. Eso ha hecho que proliferen numerosas subramas de la geometría con enfoques muy diferentes. Para clasificar los diferentes desarrollos de la geometría moderna se pueden recurrir a diferentes enfoques:
Los antiguos griegos manejaban un único tipo de geometría, a saber, la geometría euclídea, hábilmente codificada en los Elementos de Euclides por una escuela alejandrina encabezada por Euclides. Este tipo de geometría se basó en un estilo formal de deducciones a partir de cinco postulados básicos. Los cuatro primeros fueron ampliamente aceptados y Euclides los usó extensivamente, sin embargo, el quinto postulado fue menos usado y con posterioridad diversos autores trataron de demostrarlo a partir de los demás, la imposibilidad de dicha deducción llevó a constatar que junto con la geometría euclídea existían otros tipos de geometrías en que el quinto postulado de Euclídes no participaba. De acuerdo a las modificaciones introducidas en ese quinto postulado se llega a familias diferentes de geometrías o espacios geométricos diferentes entre ellos:
A partir del siglo XIX se llegó a la conclusión de que podían definirse geometrías no euclídeas entre ellas:
En el siglo XIX, Klein desarrolló el denominado Programa de Erlange que establecía otra forma de enfocar los conceptos geométricos: estudiar bajo qué diferentes tipos de  transformaciones matemáticas se verificaban invarianzas. Así se identificaron grupos dotados de diversas operaciones y se plantearon subdisciplinas con base en cuales eran los tipos particulares de transformaciones bajo las cuales se registraban invarianzas. Este estudio permitió la siguiente clasificación geométrica:
Si bien Euclides básicamente se restringió a conceptos geométricos representables mediante figuras (puntos, líneas, círculos, etc.) el desarrollo de otras ramas de las matemáticas no conectadas inicialmente con la geometría propiamente dicha, llevó a poder aplicar las herramientas de otras ramas a problemas propiamente geométricos así nacieron:
. La geometría algebraica
. La geometría analítica
. La geometría descriptiva
Además de las subramas propiamente dichas modernamente han surgido numerosas aplicaciones prácticas de la geometría entre ellas:
El aprendizaje de la geometría implica el desarrollo de habilidades visuales y de argumentación.
Para que el aprendizaje de la geometría no carezca de sentido, es importante que el grupo docente se preocupe por buscar un equilibrio entre la asociación de habilidades de visualización y argumentación, pues ambas habilidades son fundamentales dentro del proceso formativo del individuo. Es decir, no se trata solo de enseñar contenidos como una “receta” o por cumplir con lo estipulado en el currículo sino que se pretende que con la enseñanza de la geometría el estudiantado aprenda a pensar lógicamente.[57]​
El ser humano, desde su infancia, crea representaciones del mundo físico que le rodea. Estas le generan una necesidad (teórica y práctica) para lograr el entendimiento de ese mundo. El hemisferio derecho del cerebro resulta ser el más beneficiado ante la presencia de estímulos visuales, a diferencia del hemisferio izquierdo, que tiene la responsabilidad de desarrollar las capacidades verbales. El estudio de la geometría contribuye significativamente al desarrollo de esas necesidades espaciales de visualización; sin embargo, hasta una época histórica reciente, que data a partir de la década de los años 50, es cuando educadores matemáticos se interesaron por el estudio de dicho campo, al vincular la capacidad matemática con la capacidad espacial.[57]​
Respecto a las dificultades que las estudiantes y los estudiantes presentan al estudiar geometría se encuentran: resolver un problema algebraicamente; calcular perímetros, áreas y volúmenes, debido a que no identifican cuál fórmula aplicar y dificultad para interpretar qué es lo que dice un problema. Al realizar el análisis por nivel, se puede observar que en el ciclo diversificado (décimo y undécimo año) la principal dificultad que presentan es interpretar lo que dice un problema. La principal dificultad de las alumnas y alumnos de séptimo, octavo y noveno año, es, respectivamente, comprender las fórmulas del perímetro, áreas y volúmenes y aprender las definiciones; resolver una situación problema algebraicamente y dificultad para extraer información de un dibujo geométrico.[57]​

El tornillo es una máquina simple que deriva directamente de una pieza metálica y siempre trabaja asociado a un orificio roscado. Básicamente puede definirse como un plano inclinado enrollado sobre un cilindro, o lo que es más realista, un surco helicoidal tallado en la superficie de un cilindro (si está tallado sobre un cilindro afilado o un cono tendremos un tirafondo). En él se distinguen tres partes básicas: cabeza, cuello y rosca.[1]​El tornillo deriva directamente de la máquina simple conocida como plano inclinado y siempre trabaja asociado a un orificio roscado.[2]​ Los tornillos permiten que las piezas sujetas con los mismos puedan ser desmontadas cuando lo requiera.
Los tornillos están fabricados en muchos materiales y aleaciones; en los tornillos realizados en plástico su resistencia está relacionada con la del material empleado. Un tornillo de aluminio será más ligero que uno de acero (aleación de hierro y carbono), pero será menos resistente ya que el acero tiene mejor capacidad metalúrgica que el aluminio; una aleación de duraluminio mejorará las capacidades de resistencia del aluminio pero disminuirá las de tenacidad, ya que al endurecer el aluminio con silicio o metales como cromo o titanio, se aumentará su dureza pero también su coeficiente de fragilidad a partirse. Los metales más duros son menos tenaces ya que son cualidades antagónicas. La mayoría de las aleaciones especiales de aceros, bronces y aceros inoxidables contienen una proporción de metales variable para adecuar su uso a una aplicación determinada.
Siempre hay que usar el tornillo adecuado para cada aplicación. Si se usa un tornillo con demasiada resistencia de tensión (dureza) que no está ajustado al valor de diseño, podría romperse, como se rompe un cristal, por ser demasiado duro. Esto es porque los tornillos de alta tensión tienen menor resistencia a la fatiga (tenacidad) que los tornillos con un valor de tensión más bajo. Un tornillo compuesto por una aleación más blanda se podría deformar, pero sin llegar a partirse, con lo cual quizá no podría desmontarse pero seguiría cumpliendo su misión de unión.
El estándar ISO se marca con dos números sobre la cabeza del tornillo, por ejemplo "8.8". El primer número indica la resistencia de tensión (la dureza del material); el segundo número significa la resistencia a punto cedente, es decir, la tenacidad del material. Si un tornillo está marcado como 8.8, tiene una dureza (resistencia de tensión) de 800  MPa (megapascales), y una tenacidad (resistencia de tensión) del 80 %. Una marca de 10.9 indica un valor de tensión de 1000 MPa con una resistencia a punto cedente de 900 MPa, 90 % de resistencia de tensión.
Los tornillos pueden soportar hasta un mayor peso o tracción, pero rebasada su capacidad se rajarán, pudiendo quebrarse. Los tornillos fabricados con aleaciones más duras pueden soportar un mayor peso o tracción, pero tienen igualmente un límite y menor tenacidad que los tornillos fabricados en aleaciones más blandas. Si usa un tornillo que ha sido sobreajustado, sea cual sea su dureza, puede quebrarse con facilidad ya que su resistencia de tensión (tenacidad) es muy baja.
Los tornillos los definen las siguientes características:
El término tornillo se utiliza generalmente en forma genérica: son muchas las variedades de materiales, tipos y tamaños que existen. Una primera clasificación puede ser la siguiente:[3]​
Los tornillos para madera reciben el nombre de tirafondo para madera. Su tamaño y calidad está regulado por la norma DIN-97 y tienen una rosca que ocupa 3/4 de la longitud de la espiga. Pueden ser de acero dulce, inoxidable, latón, cobre, bronce o aluminio, y pueden estar galvanizados, niquelados, bicromatados, etc.
Este tipo de tornillo se estrecha en la punta como una forma de ir abriendo camino a medida que se inserta para facilitar el autorroscado, porque no es necesario hacer un agujero previo, y el filete es afilado y cortante. Normalmente se atornillan con destornillador eléctrico o manual.
Sus cabezas pueden ser planas, ovales o redondeadas; cada cual cumplirá una función específica.
Cabeza plana: se usa en carpintería, en general, en donde es necesario dejar la cabeza del tornillo sumergida o a ras con la superficie.
Cabeza puntiaguda: la porción inferior de la cabeza tiene una forma que le permite hundirse en la superficie y dejar sobresaliendo sólo la parte superior redondeada. Son más fáciles para sacar y tienen mejor presentación que los de cabeza plana. Se usan para fijación de elementos metálicos, como herramientas o chapas de picaportes.
Cabeza redondeada: se usa para fijar piezas demasiado delgadas como para permitir que el tornillo se hunda en ellas; también para unir partes que requerirán arandelas. En general se emplean para funciones similares a los de cabeza oval, pero en agujeros sin avellanar. Este tipo de tornillo resulta muy fácil de remover.
Las cabezas pueden ser de diferentes clases:
Cabeza fresada (ranura recta): tienen las ranuras rectas tradicionales.
Cabeza Phillips: tienen ranuras en forma de cruz para minimizar la posibilidad de que el destornillador se deslice.
Cabeza tipo Allen: con un hueco hexagonal, para encajar una llave Allen.
Cabeza Torx: con un hueco en la cabeza en forma de estrella de diseño exclusivo Torx.
Las características que definen a los tornillos de madera son: tipo de cabeza, material constituyente, diámetro de la caña y longitud.
Hay una variedad de tornillos que son más gruesos que los clásicos de madera, que se llaman tirafondos y se utilizan mucho para atornillar los soportes de elementos pesados que vayan colgados en las paredes de los edificios, como por ejemplo, toldos, aparatos de aire acondicionado, etc. En estos casos se perfora la pared al diámetro del tornillo elegido, y se inserta un taco de plástico, a continuación se atornilla el tornillo que rosca a presión el taco de plástico y así queda sujeto firmemente el soporte. También se utiliza por ejemplo para el atornillado de la madera de grandes embalajes. Estos tornillos tienen la cabeza hexagonal y una gama de M5 a M12.
Ambos tipos de tornillos pueden abrir su propio camino. Se fabrican en una amplia variedad de formas especiales. Se selecciona el adecuado atendiendo al tipo de trabajo que realizará y el material en el cual se empleará.
Los autorroscantes tienen la mayor parte de su caña cilíndrica y el extremo en forma cónica. Pueden ser de cabeza plana, oval, redondeada o chata. La rosca es delgada, con su fondo plano, para que la plancha se aloje en él. Se usan en láminas o perfiles metálicos, porque permiten unir metal con madera, metal con metal, metal con plástico o con otros materiales. Estos tornillos son completamente tratados (desde la punta hasta la cabeza) y sus bordes son más afilados que los de los tornillos para madera.
En los autoperforantes su punta es una broca, lo que evita tener que hacer perforaciones guías para instalarlos. Se usan para metales más pesados: van cortando una rosca por delante de la pieza principal del tornillo.
Las dimensiones, tipo de cabeza y calidad están regulados por normas DIN. Estas normas son requerimientos técnicos de calidad y seguridad establecidos por el Instituto Alemán de Normalización y se aplican en la industria, el comercio, la ciencia, etc. Son especialmente consideradas en lo que a mecanismos y piezas industriales se refiere, como es el caso de los tornillos, tuercas, arandelas, etc. 
Se usan también para ser fijados en las paredes por medio de anclajes de plástico llamados ramplugs.
Para la unión de piezas metálicas se utilizan tornillos con rosca triangular que pueden ir atornillados en un agujero ciego o en una tuerca con arandela en un agujero pasante.
Este tipo de tornillos es el que se utiliza normalmente en las máquinas y lo más importante que se requiere de los mismos es que soporten bien los esfuerzos a los que están sometidos y que no se aflojen durante el funcionamiento de la máquina donde están insertados.
Lo destacable de estos tornillos es el sistema de rosca y el tipo de cabeza que tengan puesto que hay variaciones de unos sistemas a otros. Por el sistema de rosca los más usados son los siguientes
Por el tipo de cabeza que tengan, los tornillos más utilizados son los siguientes:
De acuerdo a la Iram 4520* (1999), en su ítem 2.1.2 “Representación convencional “, normalmente por convención, la representación de los filetes y de las partes roscadas en todos los tipos de dibujo técnico se simplifican como lo muestra la figura adjunta (representación gráfica de un tornillo). Tanto en los tornillos como en los agujeros roscados la cresta que representa el coronamiento de la rosca se representa con trazo continuo grueso y la raíz con trazo fino. En vistas ocultas, ambas se trazan con trazo fino discontinuo. En las secciones, el rayado se prolonga hasta la corona. En una vista de frente de un tornillo, la línea que representa la raíz (diámetro al fondo de una rosca) abarcará aproximadamente 3/4 de circunferencia para evitar errores de interpretación (preferentemente abierto en el cuadrante superior derecho). En los dibujos de partes roscadas ensambladas (conjuntos), las líneas que representan la rosca externa (tornillo, perno roscado, tubo), se mostrará siempre cubriendo la interna y no estará oculta por ella. 
Se entiende por coronamiento al diámetro mayor en roscas externas (diámetro nominal de la rosca) y al diámetro menor en roscas internas (diámetro del agujero).
Cuando hablamos de raíz, se refiere normalmente al diámetro menor en roscas externas (diámetro al fondo de la rosca) y al diámetro mayor en roscas internas (diámetro nominal de la rosca).
El diseño de las cabezas de los tornillos responde, en general, a dos necesidades: por un lado, conseguir la superficie de apoyo adecuada para la herramienta de apriete de forma tal que se pueda alcanzar la fuerza necesaria sin que la cabeza se rompa o deforme. Por otro, necesidades de seguridad implican (incluso en reglamentos oficiales de obligado cumplimiento) que ciertos dispositivos requieran herramientas especiales para la apertura, lo que exige que el tornillo (si este es el medio elegido para asegurar el cierre) no pueda desenroscarse con un destornillador convencional, dificultando así que personal no autorizado acceda al interior.
Así, se tienen cabezas de distintas formas:
A partir de determinados diámetros, lo normal es que la cabeza de los tornillos comerciales sea hexagonal, principalmente los que enroscan en piezas metálicas o en su correspondiente tuerca. Hay varios tipos de tornillos comerciales de cabeza hexagonal fabricados según normas DIN que difieren unos de otros en la longitud de la rosca que tienen sus cañas.[4]​
Al igual que con las cabezas hexagonales hay varios modelos de tornillos con cabeza Allen todos ellos normalizados según las normas DIN correspondiente. Los tornillos con cabeza hexagonal se utilizan principalmente cuando se desean superficies lisas y las fuerzas de apriete no son muy elevadas.[5]​
Normalmente, este tipo de tornillo se utiliza en trabajos de matricería ya que la cabeza queda embutida dentro de los moldes. También se utiliza para mecánica de automoción y motociclismo debido a la facilidad que da el apriete cuando se encuentran en lugares difíciles de acceder. Dependiendo de su grueso y a partir de un largo, viene con un cuello sin roscar.[6]​
Con los modernos destornilladores eléctricos y neumáticos que existen el uso de tornillos de autorroscado se utiliza mucho en los diversos tipos de carpintería tanto de madera como metálica ya que es un sistema rápido de atornillado. En el atornillado de piezas metálicas se utiliza menos porque el par de apriete que se ejerce es bajo y está expuesto a que se afloje durante el funcionamiento de la máquina.
Los tornillos son elementos presentes en casi todos los campos de construcciones metálicas, de madera o de otras actividades, por eso hay muchos tipos, tamaños, y procesos de fabricación.
Desde el punto de vista de la utilización se pueden citar los siguientes tipos de tornillos.
La producción actual de tornillería está muy automatizada tanto en lo que respecta a la estampación de la cabeza como a la laminación de la rosca. Por lo tanto es fácil encontrar en los establecimientos especializados el tornillo que se necesite, siempre que esté dentro de la gama normal de fabricación.
Los tornillos normales diferencian su calidad en función de la resistencia mecánica que tienen. La norma (EN ISO 898-1) establece el siguiente código de calidades 4.6, 5.6, 5.8, 6.8, 8.8, 10.9 y 12.9. Los fabricantes están obligados a estampar en la cabeza de los tornillos la calidad a la que pertenecen.
El primer número multiplicado por 100 nos indicará la resistencia a la rotura en Newtons/mm². Por lo tanto, un tornillo 10.9 tendrá una resistencia de 10*100=1.000 N/mm².
El segundo número indica que porcentaje del límite de rotura es el límite elástico (es la tensión máxima que puede soportar un material «elástico» sin sufrir deformaciones permanentes). Para traducirlo a algo más entendible, indica cuanto podemos apretar el tornillo sin que se deforme (y antes de partirse), por eso se indica como porcentaje. Por lo tanto, un tornillo 10.9 tendrá un límite elástico de 900 N/mm².
En cuanto a dimensiones todas están normalizadas por normas DIN, y los tamaños disponibles, en rosca métrica por ejemplo con cabeza hexagonal, oscilan entre M3 y M68; la longitud de los tornillos estándar es variable en un escalón de 5 mm, desde un mínimo a un máximo según sea su diámetro. Sin embargo, si fuese necesario disponer de forma esporádica de tornillos de mayor longitud, se fabrican unas varillas roscadas de 1 m de longitud, donde es posible cortar a la longitud que se desee obtener y con una fijación de dos tuercas por los extremos realizar la fijación que se desee.
Con el desarrollo de componentes electrónicos cada vez más pequeños ha sido necesario desarrollar y fabricar tornillería especialmente pequeña; este tipo de tornillos se caracteriza por ser autorroscante en materias blandas tales como plásticos, y su cabeza está adaptada para ser accionados por destornilladores muy pequeños y de precisión; el material de estos tornillos puede ser de acero inoxidable, acero normal o latón.
Los tornillos de alta resistencia se designan por las letras TR, seguidas del diámetro de la caña y la longitud del vástago, separados por el signo x; seguirá el tipo de acero del que están construidos
Las tuercas se designarán con las letras MR, el diámetro nominal y el tipo del acero.
Las características del acero utilizado para la fabricación de los tornillos y tuercas definidos como de alta resistencia están normalizadas.
El fabricante de este tipo de tornillos se ve obligado a entregar un certificado de garantía por lo que no son necesarios los ensayos de recepción, a no ser que el Pliego de Prescripciones Técnicas Particulares los imponga.
Los tornillos de alta resistencia llevarán en la cabeza, marcadas en relieve, las letras TR, la designación del tipo de acero, y el nombre o signo de la marca registrada del fabricante.
Sobre una de sus bases, las tuercas de alta resistencia llevarán, marcadas en relieve, las letras MR, la designación del tipo de acero, y el nombre de la marca registrada del fabricante.[7]​
Alternativamente, con la aparición de los eurocódigos en los últimos años, la nomenclatura de Tornillos de Alta Resistencia sin pretensar ha pasado a ser métrica + longitud + clase de resistencia, donde la clase se compone de dos números separados por un punto. El primero de ellos indica el valor nominal del límite de rotura por 100 (fub) en N/mm², y el segundo el valor nominal del límite elástico (fyb) en N/mm², siendo este valor el producto del límite de rotura por este segundo número dividido por 10.
Por ejemplo, M18x120 10.9 indica un tornillo de alta resistencia métrica 18, longitud nominal 120 mm, límite de rotura 1000 N/mm² y límite elástico 900 N/mm². Y M8x60 8.8 indica un tornillo de métrica 8, longitud nominal 60 mm, límite de rotura 800 N/mm² y límite elástico 640 N/mm².
Otros ejemplos de clases de resistencia normalizados son 4.6, 4.8, 5.6, 5.8, 6.8, 8.8, 10.9, 12.9.
Los tornillos de precisión se instalan cuando las presiones, esfuerzos y velocidades de los procesos exigen uniones más fuertes y tornillos más fiables que eviten fallos que puedan desencadenar una avería en la máquina o estructura donde van instalados.
Estos tornillos se caracterizan por tener una resistencia extra a los esfuerzos de tracción y fatiga. La resistencia media que pueden tener estos tornillos es de 1300 N/mm² frente a los 1220  N/mm² que tienen los de la gama ordinaria.
Esta gran resistencia posibilita el montaje de tornillos de dimensiones más pequeñas o menos tornillos, ahorrando espacio, material y tiempo.
El perfil del filete de estos tornillos es redondeado eliminando la punta V aguda que es la causa principal del fallo de muchos tornillos.[8]​
Los tornillos inviolables son un tipo de tornillería especial que una vez atornillados en el lugar correspondiente ya es imposible quitarlos, a menos que se fuercen y rompan. Esto es gracias al diseño que tiene la cabeza, que es inclinada en su interior, de forma tal que si se intenta aflojar sale la llave sin conseguirlo. Son tornillos llamados antivandálicos y son muy utilizados en trabajos de cerrajería realizados en sitios con acceso a las calles o lugares donde pudiesen actuar personas malintencionadas. Al igual que se fabrican tornillos inviolables también se fabrican tuercas inviolables. Las normas de estos tornillos de rosca métrica corresponden a la ISO-7380 y ISO-7991 y se fabrican con cabeza Allen y con cabeza Torx.[9]​
También se utilizan algunos a los que se les acopla un sello a la cabeza, impidiendo introducir una llave para aflojarlo. Estos tornillos se venden con su tapa correspondiente, y suelen ser para llave Allen. Como solución temporal o improvisada, se pueden introducir a golpe de martillo unos plomitos redondos de pesca en el mismo lugar.
Con las tecnologías modernas actuales es posible fabricar aquellos tornillos que por sus dimensiones se salgan de la producción estándar. Para estos casos siempre se debe actuar de acuerdo a las especificaciones técnicas que tenga el tornillo que se desea fabricar, tamaño, material, calidad, etc.
Uno de los elementos imprescindibles para muchas de las aplicaciones quirúrgicas del titanio es poder disponer de toda la gama de tornillos que puedan ser necesarios de acuerdo con la aplicación requerida.
Desde que se empezó a utilizar el titanio en el tratamiento de las fracturas y en ortopedia no se ha reportado ningún caso de incompatibilidad.
La aleación de titanio más empleada en este campo contiene aluminio y vanadio según la composición: Ti6Al4V. El aluminio incrementa la temperatura de la transformación entre las fases alfa y beta. El vanadio disminuye esa temperatura. La aleación puede ser bien soldada. Tiene alta tenacidad.[10]​
En la práctica, la mayoría de tornillos que se fabrican son de acero o aluminio. Los tornillos fabricados en aluminio son frecuentes en uniones de materiales blandos como la madera o el plástico, para aplicaciones caseras o donde se aprecia su ligereza. Entre los tornillos de aleaciones de acero hay que destacar los aceros inoxidables para aplicaciones específicas por su durabilidad, en la industria alimentaria o en condiciones corrosivas con atmósferas adversas. En los aceros, un contenido bajo de carbono permite mantener la ductilidad a pesar de la dureza del carbono; con el contenido de manganeso y silicio se consigue un tratamiento térmico a bajo coste y con el niobio se mantiene el control de tamaño del grano a alta temperatura. En los aceros inoxidables además, el cromo, junto al níquel y sobre todo el molibdeno determinan la calidad de la aleación.
El proceso industrial de fabricación de tornillos mediante estampación y laminación requiere el uso de acero de gran ductilidad, es decir con poco contenido de carbono. Esta particularidad hace que los tornillos de menor resistencia, 4. 6, 5. 6, 5. 8 y 6. 8, no reciban tratamiento térmico de endurecimiento.
Para fabricar tornillos más resistentes de calidades 8. 8 y 10. 9, la empresa productora de acero Sidenor,[12]​ por ejemplo, produce un acero creado ex profeso para tornillería denominado DÚCTIL  80 y DÚCTIL  100 que se caracteriza por ser pretratado antes del proceso de fabricación de los tornillos, gracias que su composición química permite que siga siendo dúctil aunque ya tenga más resistencia mecánica, posibilitando la fabricación de tornillos en frío.
La composición química del denominado DÚCTIL  80 es la siguiente:
C: (0, 06/0, 08), Mn: (1, 30/1, 80), Si: (0, 20/0, 40), Cr: (0, 20/0, 50), Ti: (0, 20/0, 40), Nb: (0, 03/0, 05)
Este contenido tan bajo en C permite mantener la ductilidad a pesar de su dureza, con el contenido de Mn y Si se consigue templabilidad a bajo coste y con el Nb se mantiene el control de tamaño del grano a alta temperatura.
Composición parecida tiene el acero denominado DÚCTIL  100, aunque en este acero el contenido de C pasa a ser de (0, 05/0, 20) para elevar su resistencia mecánica.
Para la fabricación de tornillos de gran resistencia se suelen utilizar aceros normales (y por tanto más baratos que los aceros especiales) que permiten un temple mayor después de un tratamiento por cementación o nitruración. Un inconveniente de alguno de estos tratamientos es que el tornillo recibe una cianuración que en el tornillo es inocua, pero convierte los desechos en altamente contaminantes por el cianuro venenoso que contienen.
El acero es el metal más empleado en la fabricación de tornillos. Satisface la mayor parte de las demandas de las principales industrias en términos de calidad técnica y económica para determinados usos. Sin embargo, existen una serie de limitaciones. Por ejemplo, los aceros comunes no son muy resistentes a la corrosión.
Generalmente, la función de los tornillos forma parte del soporte de la carga, por lo que una exposición prolongada puede dar lugar a daños en la integridad de la estructura con el consiguiente coste de reparación y/o sustitución. Además muchos tornillos trabajan a la intemperie. Por esta razón se utiliza la galvanización en caliente como uno de los métodos que se utilizan para mejorar la resistencia a la corrosión de los tornillos mediante un pequeño recubrimiento sobre la superficie. El galvanizado permite el recubrimiento de los tornillos mediante su inmersión en un baño de cinc fundido.
La técnica de cincado electrolítico o mecánico es la que más se utiliza para el recubrimiento anticorrosivo de los tornillos. Esta técnica consiste en depositar sobre la pieza una capa de cinc mediante corriente continua a partir de una disolución salina que contiene cinc. El proceso se utiliza para proteger piezas más pequeñas, cuando requieren un acabado más uniforme que el proporcionado por el galvanizado en caliente. No obstante, los espesores de la capa de cinc son pequeños y, por tanto, su durabilidad es más reducida.
Otro proceso de protección anticorrosiva lo constituye el tratamiento llamado pavonado.
El pavonado es un acabado negro o azulado, brillante o mate, para piezas de acero, de gran duración, efecto decorativo y resistencia a la corrosión.
El pavonado atrae y retiene los aceites lubricantes. El revestimiento no aumenta ni disminuye las dimensiones de los metales tratados, por lo que las tolerancias para el ajuste de piezas no se ven afectadas. Además, las superficies tratadas pueden ser soldadas, enceradas, barnizadas o pintadas. Se obtiene un revestimiento mate cuando se aplica sobre una superficie tratada con chorro de arena o con un mordiente químico, y un revestimiento brillante sobre una superficie pulida o lisa. Los colores que se pueden obtener varían del negro al azulado, según la clase de aleación tratada.
Para situaciones de mayor protección anticorrosiva se utiliza tornillería fabricada con acero inoxidable que lógicamente es más cara, e incluso para casos más específicos se fabrican tornillos de titanio cuya resistencia anticorrosiva es casi total
Existen dos medios diferentes para medir o verificar la rosca de los tornillos los que son de medición directa y aquellos que son de medición indirecta.
Para la medición directa se utilizan generalmente micrómetros cuyas puntas están adaptadas para introducirse en el flanco de las roscas. Otro método de medida directa es hacerlo con el micrómetro y un juego de varillas que se introducen en los flancos de las roscas y permite medir de forma directa los diámetros medios en los flancos de acuerdo con el diámetro que tengan las varillas.
Para la medición indirecta de las roscas se utilizan varios métodos, el más común es el de las galgas. Con estas galgas compuesta de dos partes en las que una de ellas se llama PASA y la otra NO PASA.
También hay una galga muy común que es un juego de plantillas de los diferentes pasos de rosca de cada sistema, donde de forma sencilla permite identificar cual es el paso que tiene un tornillo o una tuerca. En laboratorios de metrología también se usan los proyectores de perfiles ideales para la verificación de roscas de precisión.[13]​
El apriete regulado se establece normalmente como la precarga que se debe aplicar al atornillar un tornillo mediante la herramienta adecuada.
Los pares de apriete recomendables varían en función del límite elástico, el límite de rotura y las dimensiones y calidades que tenga el tornillo. También se ha de tener en cuenta los materiales de las piezas a unir, puesto que un apriete fuerte podría deformar las piezas y/o llevarlos a un estado de plasticidad en el que las piezas serán incapaces de ejercer la fuerza de reacción para mantener el tornillo tenso. En ocasiones los tornillos se aprietan con una tensión superior a su límite elástico para deformarlos y así impedir que se aflojen, calculando que en ningún caso se vaya a superar el límite de rotura. También dependerá si empleamos arandelas planas, tensoras grower, de levas tipo Nordlock, etc. Pero a pesar de las variaciones, como dato general existen tablas que regulan los pares de apriete recomendado para cada caso.[14]​
Resulta crucial que se preste atención a los pares de apriete y a las instrucciones de instalación en los casos que lo determinen las especificaciones de montaje. Los motores de los vehículos son especialmente sensibles a un par de apriete inadecuado. Los motores modernos reaccionan de un modo particularmente sensible a los errores de montaje.
La herramienta que se utiliza para apretar un tornillo con el par regulado se llama llave dinamométrica.
La tornillería en general es parte importante de la rigidez y buen funcionamiento que cabe esperar y desear de los elementos ensamblados. Por eso los fallos o defectos que pueda tener un tornillo puede ocasionar un fallo o una avería indeseada.
El primer defecto que puede presentar un tornillo es un defecto de diseño o de cálculo porque sus dimensiones o calidades no sean las adecuadas. En este caso el fallo que se puede provocar es una rotura prematura del tornillo por no poder soportar las tensiones y esfuerzos a los que está sometido.
El segundo defecto en importancia que puede tener un tornillo es un defecto de fabricación donde la calidad del material constituyente no sea la prevista en el diseño, o un defecto dimensional en lo que respecta principalmente a las tolerancias que debe tener su roscado. En este caso se puede producir una rotura del tornillo o un deterioro de la rosca.
El tercer defecto puede ser un montaje deficiente por no aplicar el par de apriete adecuado, de acuerdo con su calidad y dimensiones. En este caso si es un exceso de apriete se puede producir la rotura del tornillo o el deterioro de la rosca, y si es un defecto de apriete el ensamblaje queda flojo y si es un objeto en movimiento aparecen vibraciones indeseadas que ocasionan una avería en el mecanismo ensamblado.
El cuarto defecto se produce por deterioro del tornillo si resulta atacado por la oxidación y corrosión si no ha sido protegido debidamente. En este caso y durante las operaciones rutinarias de mantenimiento preventivo del mecanismo se deben sustituir todos los tornillos deteriorados por unos nuevos y protegerlos adecuadamente de la corrosión y oxidación.
El último defecto grave que puede tener un tornillo es cuando se procede al desmontaje de un ensamblaje y si por causa de la oxidación y corrosión el tornillo se descabeza en el momento de intentar aflojarlo. Para estos casos de tornillos deteriorados se deben utilizar productos lubricantes que permitan el aflojamiento sin que se rompa el tornillo.
Los primeros antecedentes de la utilización de roscas se remontan al tornillo de Arquímedes, desarrollado por el sabio griego alrededor del 300  a.  C., empleándose ya en aquella época profusamente en el valle del Nilo para la elevación de agua.
Durante el Renacimiento las roscas comienzan a emplearse como elementos de fijación en relojes, máquinas de guerra y en otras construcciones mecánicas. Leonardo da Vinci desarrolló por entonces métodos para el tallado de roscas; sin embargo, estas seguirán fabricándose a mano y sin ninguna clase de normalización hasta bien entrada la Revolución industrial.
En 1841, el ingeniero británico Joseph Whitworth definió la rosca que lleva su nombre.
En 1864, William Sellers hizo lo mismo en Estados Unidos.
Esta situación se prolongó hasta 1946, cuando la Organización Internacional de Normalización (ISO) definió el sistema de rosca métrica, adoptado actualmente en prácticamente todos los países. En los Estados Unidos, en cambio, se sigue empleando la norma SAE (Society of Automotive Engineers: Sociedad de Ingenieros de Automoción).
La rosca métrica tiene una sección triangular que forma un ángulo de 60° y la cabeza un poco truncada para facilitar el engrase.


El transistor es un dispositivo electrónico semiconductor. Permite el paso de una señal en respuesta a otra. Se puede configurar o "comportar" como amplificador, oscilador, conmutador o rectificador. El término «transistor», del acrónimo transfer resistor (resistor de transferencia). Se encuentra prácticamente en todos los aparatos electrónicos como radios, televisores y computadoras. Habitualmente dentro de los llamados circuitos integrados.
El físico austro-húngaro  Julius Edgar Lilienfeld solicitó en Canadá en el año 1925[1]​ una patente para lo que él denominó «un método y un aparato para controlar corrientes eléctricas» y que se considera el antecesor de los actuales transistores de efecto campo, ya que estaba destinado a ser un reemplazo de estado sólido del triodo. Lilienfeld también solicitó patentes en los Estados Unidos en los años 1926[2]​ y 1928.[3]​[4]​ Sin embargo, Lilienfeld no publicó ningún artículo de investigación sobre sus dispositivos, ni sus patentes citan algún ejemplo específico de un prototipo de trabajo. Debido a que la producción de materiales semiconductores de alta calidad no estaba disponible por entonces, las ideas de Lilienfeld sobre amplificadores de estado sólido no encontraron un uso práctico en los años 1920 y 1930, aunque acabara de construir un dispositivo de este tipo.[5]​
En 1934, el inventor alemán Oskar Heil patentó en Alemania y Gran Bretaña[6]​ un dispositivo similar. Cuatro años después, los también alemanes Robert Pohl y Rudolf Hilsch efectuaron experimentos en la Universidad de Göttingen, con cristales de bromuro de potasio, usando tres electrodos, con los cuales lograron la amplificación de señales de 1 Hz, pero sus investigaciones no condujeron a usos prácticos.[7]​ Mientras tanto, la experimentación en los Laboratorios Bell con rectificadores a base de óxido de cobre y las explicaciones sobre rectificadores a base de semiconductores por parte del alemán Walter Schottky y del inglés Nevill Mott, llevaron a pensar en 1938 a William Shockley que era posible lograr la construcción de amplificadores a base de semiconductores, en lugar de tubos de vacío.[7]​
Desde el 17 de noviembre de 1947 hasta el 23 de diciembre de 1947, los físicos estadounidenses John Bardeen y Walter Houser Brattain de los Laboratorios Bell[8]​ llevaron a cabo diversos experimentos y observaron que cuando dos contactos puntuales de oro eran aplicados a un cristal de germanio, se produjo una señal con una potencia de salida mayor que la de entrada.[9]​ El líder del Grupo de Física del Estado Sólido William Shockley vio el potencial de este hecho y, en los siguientes meses, trabajó para ampliar en gran medida el conocimiento de los semiconductores. El término «transistor» fue sugerido por el ingeniero estadounidense John R. Pierce, basándose en dispositivos semiconductores ya conocidos entonces, como el termistor y el varistor y basándose en la propiedad de transrresistencia que mostraba el dispositivo.[10]​ Según una biografía de John Bardeen, Shockley había propuesto que la primera patente para un transistor de los Laboratorios Bell debía estar basado en el efecto de campo y que él fuera nombrado como el inventor. Habiendo redescubierto las patentes de Lilienfeld que quedaron en el olvido años atrás, los abogados de los Laboratorios Bell desaconsejaron la propuesta de Shockley porque la idea de un transistor de efecto de campo no era nueva. En su lugar, lo que Bardeen, Brattain y Shockley inventaron en 1947 fue el primer transistor de contacto de punto, cuya primera patente solicitaron los dos primeros nombrados, el 17 de junio de 1948,[11]​ a la cual siguieron otras patentes acerca de aplicaciones de este dispositivo.[12]​[13]​[14]​ En reconocimiento a este logro, Shockley, Bardeen y Brattain fueron galardonados conjuntamente con el Premio Nobel de Física de 1956 «por sus investigaciones sobre semiconductores y su descubrimiento del efecto transistor».[15]​
En 1948, el transistor de contacto fue inventado independientemente por los físicos alemanes Herbert Mataré y Heinrich Welker, mientras trabajaban en la Compagnie des Freins et Signaux, una subsidiaria francesa de la estadounidense Westinghouse. Mataré tenía experiencia previa en el desarrollo de rectificadores de cristal de silicio y de germanio mientras trabajaba con Welker en el desarrollo de un radar alemán durante la Segunda Guerra Mundial. Usando este conocimiento, él comenzó a investigar el fenómeno de la «interferencia» que había observado en los rectificadores de germanio durante la guerra. En junio de 1948, Mataré produjo resultados consistentes y reproducibles utilizando muestras de germanio producidas por Welker, similares a lo que Bardeen y Brattain habían logrado anteriormente en diciembre de 1947. Al darse cuenta de que los científicos de Laboratorios Bell ya habían inventado el transistor antes que ellos, la empresa se apresuró a poner en producción su dispositivo llamado «transistron» para su uso en la red telefónica de Francia.[16]​ El 26 de junio de 1948, Wiliam Shockley solicitó la patente del transistor bipolar de unión[17]​ y el 24 de agosto de 1951 solicitó la primera patente de un transistor de efecto de campo,[18]​ tal como se declaró en ese documento, en el que se mencionó la estructura que ahora posee. Al año siguiente, George Clement Dacey e Ian Ross, de los Laboratorios Bell, tuvieron éxito al fabricar este dispositivo,[19]​ cuya nueva patente fue solicitada el 31 de octubre de 1952.[20]​ Meses antes, el 9 de mayo de ese año, el ingeniero Sidney Darlington solicitó la patente del arreglo de dos transistores conocido actualmente como transistor Darlington.[21]​
El primer transistor de alta frecuencia fue el transistor de barrera de superficie de germanio desarrollado por los estadounidenses John Tiley y Richard Williams de Philco Corporation en 1953,[22]​ capaz de operar con señales de hasta 60 MHz.[23]​ Para fabricarlo, se usó un procedimiento creado por los ya mencionados inventores mediante el cual eran grabadas depresiones en una base de germanio tipo N de ambos lados con chorros de sulfato de indio hasta que tuviera unas diez milésimas de pulgada de espesor. El Indio electroplateado en las depresiones formó el colector y el emisor.[24]​ El primer receptor de radio para automóviles que fue producido en 1955 por Chrysler y Philco; usó estos transistores en sus circuitos y también fueron los primeros adecuados para las computadoras de alta velocidad de esa época.[25]​[26]​
El primer transistor de silicio operativo fue desarrollado en los Laboratorios Bell en enero de 1954 por el químico Morris Tanenbaum.[27]​ El 20 de junio de 1955, Tanenbaum y Calvin Fuller, solicitaron una patente para un procedimiento inventado por ambos para producir dispositivos semiconductores.[28]​ El primer transistor de silicio comercial fue producido por Texas Instruments en 1954 gracias al trabajo del experto Gordon Teal quien había trabajado previamente en los Laboratorios Bell en el crecimiento de cristales de alta pureza.[29]​ El primer transistor MOSFET fue construido por el coreano-estadounidense Dawon Kahng y el egipcio Martin Atalla, ambos ingenieros de los Laboratorios Bell, en 1960.[30]​[31]​
El transistor consta de tres partes dopadas artificialmente (contaminadas con materiales específicos en cantidades específicas) que forman dos uniones bipolares: el emisor que emite portadores, el colector que los recibe o recolecta y la tercera, que está intercalada entre las dos primeras, modula el paso de dichos portadores (base). A diferencia de las válvulas, el transistor es un dispositivo controlado por corriente y del que se obtiene corriente amplificada. En el diseño de circuitos a los transistores se les considera un elemento activo,[32]​ a diferencia de los resistores, condensadores e inductores que son elementos pasivos.[33]​
De manera simplificada, la corriente que circula por el colector es función amplificada de la que se inyecta en el emisor, pero el transistor solo gradúa la corriente que circula a través de sí mismo, si desde una fuente de corriente continua se alimenta la base para que circule la carga por el colector, según el tipo de circuito que se utilice. El factor de amplificación o ganancia logrado entre corriente de colector y corriente de base, se denomina Beta del transistor. Otros parámetros a tener en cuenta y que son particulares de cada tipo de transistor son: Tensiones de ruptura de Colector Emisor, de Base Emisor, de Colector Base, Potencia Máxima, disipación de calor, frecuencia de trabajo, y varias tablas donde se grafican los distintos parámetros tales como corriente de base, tensión Colector Emisor, tensión Base Emisor, corriente de Emisor, etc. Los tres tipos de esquemas(configuraciones) básicos para utilización analógica de los transistores son emisor común, colector común y base común.
Modelos posteriores al transistor descrito, el transistor bipolar (transistores FET, MOSFET, JFET, CMOS, VMOS, etc.) no utiliza la corriente que se inyecta en el terminal de base para modular la corriente de emisor o colector, sino la tensión presente en el terminal de puerta y gradúa la conductancia del canal entre los terminales de Fuente y Drenaje. Cuando la conductancia es nula y el canal se encuentra estrangulado, por efecto de la tensión aplicada entre Compuerta y Fuente, es el campo eléctrico presente en el canal el responsable de impulsar los electrones desde la fuente al drenaje. De este modo, la corriente de salida en la carga conectada al Drenaje (D) será función amplificada de la tensión presente entre la compuerta y la fuente, de manera análoga al funcionamiento del triodo.
Los transistores de efecto de campo son los que han permitido la integración a gran escala disponible hoy en día; para tener una idea aproximada pueden fabricarse varios cientos de miles de transistores interconectados, por centímetro cuadrado y en varias capas superpuestas.
Llamado también «transistor de punta de contacto», fue el primer transistor capaz de obtener ganancia, inventado en 1947 por John Bardeen y Walter Brattain. Consta de una base de germanio, semiconductor para entonces mejor conocido que la combinación cobre-óxido de cobre, sobre la que se apoyan, muy juntas, dos puntas metálicas que constituyen el emisor y el colector. La corriente de base es capaz de modular la resistencia que se «ve» en el colector, de ahí el nombre de transfer resistor. Se basa en efectos de superficie, poco conocidos en su día. Es difícil de fabricar (las puntas se ajustaban a mano), frágil (un golpe podía desplazar las puntas) y ruidoso. Sin embargo convivió con el transistor de unión debido a su mayor ancho de banda. En la actualidad ha desaparecido.
El transistor de unión bipolar (o BJT, por sus siglas del inglés bipolar junction transistor) se fabrica sobre un monocristal de material semiconductor como el germanio, el silicio o el arseniuro de galio, cuyas cualidades son intermedias entre las de un conductor eléctrico y las de un aislante. Sobre el sustrato de cristal se contaminan en forma muy controlada tres zonas sucesivas, N-P-N o P-N-P, dando lugar a dos uniones PN.
Las zonas N (en las que abundan portadores de carga Negativa) se obtienen contaminando el sustrato con átomos de elementos donantes de electrones, como el arsénico o el fósforo; mientras que las zonas P (donde se generan portadores de carga Positiva o «huecos») se logran contaminando con átomos aceptadores de electrones, como el indio, el aluminio o el galio.
La tres zonas contaminadas dan como resultado transistores PNP o NPN, donde la letra intermedia siempre corresponde a la región de la base, y las otras dos al emisor y al colector que, si bien son del mismo tipo y de signo contrario a la base, tienen diferente contaminación entre ellas (por lo general, el emisor está mucho más contaminado que el colector).
El mecanismo que representa el comportamiento semiconductor dependerá de dichas contaminaciones, de la geometría asociada y del tipo de tecnología de contaminación (difusión gaseosa, epitaxial, etc.) y del comportamiento cuántico de la unión.
El transistor de efecto de campo de unión (JFET), fue el primer transistor de efecto de campo en la práctica. Lo forma una barra de material semiconductor de silicio de tipo N o P. En los terminales de la barra se establece un contacto óhmico, tenemos así un transistor de efecto de campo tipo N de la forma más básica. Si se difunden dos regiones P en una barra de material N y se conectan externamente entre sí, se producirá una puerta. A uno de estos contactos le llamaremos surtidor y al otro drenador. Aplicando tensión positiva entre el drenador y el surtidor y conectando la puerta al surtidor, estableceremos una corriente, a la que llamaremos corriente de drenador con polarización cero. Con un potencial negativo de puerta al que llamamos tensión de estrangulamiento, cesa la conducción en el canal.
El transistor de efecto de campo, o FET por sus siglas en inglés, que controla la corriente en función de una tensión; tienen alta impedancia de entrada.
Los fototransistores son sensibles a la radiación electromagnética en frecuencias cercanas a la de la luz visible; debido a esto su flujo de corriente puede ser regulado por medio de la luz incidente. Un fototransistor es, en esencia, lo mismo que un transistor normal, solo que puede trabajar de 2 maneras diferentes:
Con el desarrollo tecnológico y evolución de la electrónica, la capacidad de los dispositivos semiconductores para soportar cada vez mayores niveles de tensión y corriente ha permitido su uso en aplicaciones de potencia. Es así como actualmente los transistores son empleados en conversores estáticos de potencia, controles para motores y llaves de alta potencia (principalmente inversores), aunque su principal uso está basado en la amplificación de corriente dentro de un circuito cerrado.
Los primeros transistores bipolares de unión se fabricaron con germanio (Ge). Los transistores de Silicio (Si) actualmente predominan, pero ciertas versiones avanzadas de microondas y de alto rendimiento ahora emplean el compuesto semiconductor de arseniuro de galio (GaAs) y la aleación semiconductora de silicio-germanio (SiGe). El material semiconductor a base de un elemento (Ge y Si) se describe como elemental.
Los parámetros en bruto de los materiales semiconductores más comunes utilizados para fabricar transistores se dan en la tabla adjunta; estos parámetros variarán con el aumento de la temperatura, el campo eléctrico, nivel de impurezas, la tensión, y otros factores diversos.
La tensión directa de unión es la tensión aplicada a la unión emisor-base de un transistor bipolar de unión con el fin de hacer que la base conduzca a una corriente específica. La corriente aumenta de manera exponencial a medida que aumenta la tensión en directa de la unión. Los valores indicados en la tabla son las típicos para una corriente de 1 mA (los mismos valores se aplican a los diodos semiconductores). Cuanto más bajo es la tensión de la unión en directa, mejor, ya que esto significa que se requiere menos energía para colocar en conducción al transistor. La tensión de unión en directa para una corriente dada disminuye con el aumento de la temperatura. Para una unión de silicio típica, el cambio es de –2.1 mV/°C.[34]​ En algunos circuitos deben usarse elementos compensadores especiales (sensistores) para compensar tales cambios.
La densidad de los portadores móviles en el canal de un MOSFET es una función del campo eléctrico que forma el canal y de varios otros fenómenos tales como el nivel de impurezas en el canal. Algunas impurezas, llamadas dopantes, se introducen deliberadamente en la fabricación de un MOSFET, para controlar su comportamiento eléctrico.
Las columnas de movilidad de electrones y movilidad de huecos de la tabla muestran la velocidad media con que los electrones y los huecos se difunden a través del material semiconductor con un campo eléctrico de 1 voltio por metro, aplicado a través del material. En general, mientras más alta sea la movilidad electrónica, el transistor puede funcionar más rápido. La tabla indica que el germanio es un material mejor que el silicio a este respecto. Sin embargo, el germanio tiene cuatro grandes deficiencias en comparación con el silicio y arseniuro de galio:
Debido a que la movilidad de los electrones es más alta que la movilidad de los huecos para todos los materiales semiconductores, un transistor bipolar n-p-n dado tiende a ser más rápido que un transistor equivalente p-n-p. El arseniuro de galio tiene el valor más alto de movilidad de electrones de los tres semiconductores. Es por esta razón que se utiliza en aplicaciones de alta frecuencia. Un transistor FET de desarrollo relativamente reciente, el transistor de alta movilidad de electrones (HEMT), tiene una heteroestructura (unión entre diferentes materiales semiconductores) de arseniuro de galio-aluminio (AlGaAs)-arseniuro de galio (GaAs), que tiene el doble de la movilidad de los electrones que una unión de barrera GaAs-metal. Debido a su alta velocidad y bajo nivel de ruido, los HEMT se utilizan en los receptores de satélite que trabajan a frecuencias en torno a los 12 GHz. Los HEMT basados en nitruro de galio y nitruro de galio aluminio (AlGaN/GaN HEMT) proporcionan una movilidad de los electrones aún mayor y se están desarrollando para diversas aplicaciones.
Los valores de la columna de Máximo valor de temperatura de la unión han sido tomados a partir de las hojas de datos de varios fabricantes. Esta temperatura no debe ser excedida o el transistor puede dañarse.
Los datos de la fila Al-Si de la tabla se refieren a los diodos de barrera de metal-semiconductor de alta velocidad (de aluminio-silicio), conocidos comúnmente como diodos Schottky. Esto está incluido en la tabla, ya que algunos transistor IGFET de potencia de silicio tienen un diodo Schottky inverso «parásito» formado entre la fuente y el drenaje como parte del proceso de fabricación. Este diodo puede ser una molestia, pero a veces se utiliza en el circuito del cual forma parte.
El comportamiento del transistor se puede ver como dos diodos (Modelo de Ebers-Moll), uno entre base y emisor, polarizado en directo y otro diodo entre base y colector, polarizado en inverso. Esto quiere decir que entre base y emisor tendremos una tensión igual a la tensión directa de un diodo, es decir 0,6 a 0,8 V para un transistor de silicio y unos 0,4 para el germanio.
Lo interesante del dispositivo es que en el colector tendremos una corriente proporcional a la corriente de base: IC = β IB, es decir, ganancia de corriente cuando β>1. Para transistores normales de señal, β varía entre 100 y 300. Existen tres configuraciones para el amplificador transistorizado: emisor común, base común y colector común.
La señal se aplica a la base del transistor y se extrae por el colector. El emisor se conecta al punto de tierra (masa) que será común, tanto de la señal de entrada como para la de salida. En esta configuración, existe ganancia tanto de tensión como de corriente. Para lograr la estabilización de la etapa ante las variaciones de la señal, se dispone de una resistencia de emisor, (RE) y para frecuencias bajas, la impedancia de salida se aproxima a RC. La ganancia de tensión se expresa:

El signo negativo, indica que la señal de salida está invertida con respecto a la señal de entrada.
Si el emisor está conectado directamente a masa, la ganancia queda expresada de la siguiente forma:

Como la base está conectada al emisor por un diodo polarizado en directo, entre ellos se puede suponer que existe una tensión constante, denominada  y que el valor de la ganancia (β) es constante. Del gráfico adjunto, se deduce que la tensión de emisor es:

Y la corriente de emisor:
.
La corriente de emisor es igual a la de colector más la de base:

Despejando la corriente de colector:

La tensión de salida, que es la de colector se calcula así:

Como β >> 1, se puede aproximar:

y, entonces es posible calcular la tensión de colector como:

La parte entre paréntesis es constante (no depende de la señal de entrada), y la restante expresa la señal de salida. El signo negativo indica que la señal de salida está desfasada 180° respecto a la de entrada.
Finalmente, la ganancia es expresada como:

La corriente de entrada, ,  si  puede expresarse como sigue:

Suponiendo que , podemos escribir:

Al dividir la tensión y corriente en la base, la impedancia o resistencia de entrada queda como:

Para tener en cuenta la influencia de frecuencia se deben utilizar modelos de transistor más elaborados. Es muy frecuente usar el modelo en pi.
Recta de carga
Esta recta se traza sobre las curvas características de un transistor que proporciona el fabricante.
Los puntos para el trazado de la misma son:  
y la tensión de la fuente de alimentación 
En los extremos de la misma, se observan las zonas de corte y de saturación, que tienen utilidad cuando el transistor actúa como interruptor. Conmutará entre ambos estados de acuerdo a la polarización de la base.
La elección del punto Q es fundamental para una correcta polarización. Un criterio extendido es el de adoptar  , si el circuito no posee . De contar con  como es el caso del circuito a considerar, el valor de  se medirá desde el colector a masa.
El punto Q, se mantiene estático mientras la base del transistor no reciba una señal.
Ejercicio
Procederemos a determinar los valores de 
Datos: 


Esta aproximación se admite porque 


Para que el circuito opere en una zona de eficacia, la corriente a través del divisor de voltaje 
 y , debe ser mucho mayor que la corriente de base; como mínimo en una relación 10:1

   utilizando el valor de  obtenido anteriormente
     

La resistencia dinámica del diodo en la juntura del emisor , se calcula tomando el valor del voltaje térmico en la misma, y está dado por: 
Con este valor, se procede a calcular la ganancia de voltaje de la etapa; 
No se toma en cuenta  ya que el emisor se encuentra a nivel de masa para la señal por medio de , que en el esquema se muestra como ; entonces, la impedancia de salida , toma el valor de  si el transistor no tiene carga. Si se considera la carga ,  se determina por  considerando que  tiene el valor , 
Al considerar la, la ganancia de tensión se ve modificada: 
La impedancia de entrada en la base del transistor para el ejemplo, está dada por 
Mientras que la impedancia de entrada a la etapa, se determina: 
La reactancia de los capacitores no se ha tenido en cuenta en los cálculos, porque se han elegido de una capacidad tal, que su reactancia  en las frecuencias de señales empleadas.
La señal se aplica al emisor del transistor y se extrae por el colector. La base se conecta a las masas tanto de la señal de entrada como a la de salida. En esta configuración se tiene ganancia solo de tensión. La impedancia de entrada es baja y la ganancia de corriente algo menor que uno, debido a que parte de la corriente de emisor sale por la base. Si añadimos una resistencia de emisor, que puede ser la propia impedancia de salida de la fuente de señal, un análisis similar al realizado en el caso de emisor común, da como resultado que la ganancia aproximada es:
La base común se suele utilizar para adaptar fuentes de señal de baja impedancia de salida como, por ejemplo, micrófonos dinámicos.
La señal se aplica a la base del transistor y se extrae por el emisor. El colector se conecta a las masas tanto de la señal de entrada como a la de salida. En esta configuración se tiene ganancia de corriente, pero no de tensión que es ligeramente inferior a la unidad. La impedancia de entrada es alta, aproximadamente β+1 veces la impedancia de carga. Además, la impedancia de salida es baja, aproximadamente β veces menor que la de la fuente de señal.
Antes de la aparición del transistor, eran usadas las válvulas termoiónicas. Las válvulas tienen características eléctricas similares a la de los transistores de efecto campo (FET): la corriente que los atraviesa depende de la tensión en el terminal llamado rejilla. Las razones por las que el transistor reemplazó a la válvula termoiónica son varias:
Como ejemplo de todos estos inconvenientes se puede citar a la primera computadora digital, llamada ENIAC, la cual pesaba más de treinta toneladas y consumía 200 kilovatios, suficientes para alimentar una pequeña ciudad, a causa de sus aproximadamente 18 000 válvulas, de las cuales algunas se quemaban cada día, necesitando una logística y una organización importantes para mantener este equipo en funcionamiento.
El transistor bipolar reemplazó progresivamente a la válvula termoiónica durante la década de 1950, pero no del todo. En efecto, durante los años 1960, algunos fabricantes siguieron utilizando válvulas termoiónicas en equipos de radio de gama alta, como Collins y Drake; luego el transistor desplazó a la válvula de los transmisores pero no del todo en los amplificadores de radiofrecuencia. Otros fabricantes de instrumentos eléctricos musicales como Fender, siguieron utilizando válvulas en sus amplificadores de audio para guitarras eléctricas. Las razones de la supervivencia de las válvulas termoiónicas son varias:

Un motor de combustión interna o motor de explosión es un tipo de máquina que obtiene energía mecánica directamente de la energía química de un combustible que arde dentro de la cámara de combustión. El nombre se debe a que dicha combustión se produce dentro de la propia máquina, a diferencia de, por ejemplo, la máquina de vapor.
Existen los diésel y gasolina, tanto en 2T como en 4T.
La invención se puede remontar a dos italianos: el padre Eugenio Barsanti, un sacerdote escolapio, y Felice Matteucci, ingeniero hidráulico y mecánico, que ya en 1853 detallaron documentos de operación, construcción y patentes pendientes en varios países europeos como Gran Bretaña, Francia, Italia y Alemania.[1]​
Los primeros prototipos carecían de la fase de compresión; es decir, la fase de succión terminaba prematuramente con el cierre de la válvula de admisión antes de que el pistón llegase a la mitad, lo que provocaba que la chispa que generaba la combustión que empuja la carrera del pistón fuese débil. Como consecuencia, el funcionamiento de estos primeros motores era deficiente. Fue la fase de compresión la que dio una eficiencia significativa al motor de combustión interna, que lograría el reemplazo definitivo de los motores a vapor e impulsaría el desarrollo de los automóviles, ya que lograba desarrollar una potencia igual o mayor en dimensiones considerablemente mucho más reducidas.
Las primeras aplicaciones prácticas de los motores de combustión interna fueron los motores fuera de borda. Esto fue debido a que el principal impedimento para la aplicación práctica del motor de combustión interna en vehículos terrestres era el hecho de que, a diferencia de la máquina de vapor, no podía comenzar desde parado. Los motores marinos no sufren este problema, ya que las hélices están libres de un momento de inercia significativo.
El motor, tal como lo conocemos hoy, fue desarrollado por el alemán Nikolaus Otto, quien en 1886 patentó el diseño de un motor de combustión interna a cuatro tiempos, basado en los estudios del inventor francés Alphonse Beau de Rochas de 1862, que a su vez se basó en el modelo de combustión interna de Barsanti y Matteucci.[cita requerida]
Las diferentes variantes de los dos ciclos, tanto en diésel como en gasolina, tienen cada uno su ámbito de aplicación.
Los motores Otto y los diésel tienen los mismos elementos principales: (bloque, cigüeñal, biela, pistón, culata, válvulas) y otros específicos de cada uno, como la bomba inyectora de alta presión en los diésel, o antiguamente el carburador en los Otto.
En los 4T es muy frecuente designarlos mediante su tipo de distribución: SV, OHV, SOHC, DOHC. Es una referencia a la disposición del (o los) árbol de levas.
La cámara de combustión es un cilindro, por lo general fijo, cerrado en un extremo superior por la culata o tapa de block y dentro del cual se desliza un pistón el mismo que tiene acoplado aros o anillos, los cuales mejoran el sellado de la cámara de  combustión y lubrican el área de rozamiento, estos tienen como función principal mejorar el ajuste del pistón al cilindro, para que no exista fuga de combustión y tampoco suba el aceite a la cámara de combustión en el ciclo de admisión. La posición hacia dentro y hacia fuera del pistón modifica el volumen que existe entre la cara exterior del pistón y las paredes internas de la cámara. La cara interior del pistón está unida por una biela al cigüeñal, el mismo que convierte en movimiento lineal del pistón en rotatorio circular.
En los motores de varios cilindros, el cigüeñal tiene una posición de partida, llamada espiga de cigüeñal y conectada a cada eje, con lo que la energía producida por cada cilindro se aplica al cigüeñal en un punto determinado de la rotación. Los cigüeñales cuentan con pesados volantes y contrapesos cuya inercia reduce la irregularidad del movimiento del eje. Un motor alternativo puede tener de 1 a 28 cilindros.
El sistema de alimentación de combustible de un motor Otto consta de un depósito, una bomba de combustible y un dispositivo dosificador de combustible que vaporiza o atomiza el combustible desde el estado líquido, en las proporciones correctas para poder ser quemado.
Se llama carburador al dispositivo que hasta ahora venía siendo utilizado con este fin en los motores Otto. Ahora los sistemas de inyección de combustible lo han sustituido por completo por motivos medioambientales. Su mayor precisión en la dosificación de combustible inyectado reduce las emisiones de CO2, y asegura una mezcla más estable. En los motores diésel se dosifica el combustible gasoil de manera no proporcional al aire que entra, sino en función del mando de aceleración y el régimen motor (mecanismo de regulación) mediante una bomba inyectora de combustible.
En los motores de varios cilindros el combustible vaporizado se lleva a los cilindros a través de un tubo ramificado llamado colector de admisión. La mayor parte de los motores cuentan con un colector de escape o de expulsión, que transporta fuera del vehículo y amortigua el ruido de los gases producidos en la combustión.
Cada cilindro toma el combustible y expulsa los gases a través de válvulas de cabezal o válvulas deslizantes. Un muelle mantiene cerradas las válvulas hasta que se abren en el momento adecuado, al actuar las levas de un árbol de levas rotatorio movido por el cigüeñal, estando el conjunto coordinado mediante la cadena o la correa de distribución. Ha habido otros diversos sistemas de distribución, entre ellos la distribución por camisa corredera (sleeve-valve).
Los motores necesitan una forma de iniciar la combustión dentro del cilindro. En los motores Otto, el sistema de encendido consiste en un componente llamado bobina de encendido, que es un auto-transformador de alto voltaje al que está conectado un conmutador que interrumpe la corriente del primario para que se induzca un impulso eléctrico de alto voltaje en el secundario.
Dicho impulso está sincronizado con el tiempo de compresión de cada uno de los cilindros; el impulso se lleva al cilindro correspondiente (aquel que está en compresión en ese momento) utilizando un distribuidor rotativo y unos cables que llevan la descarga de alto voltaje a la bujía. El dispositivo que produce el encendido de la mezcla combustible/aire es la bujía, que, instalada en cada cilindro, dispone de electrodos separados unas décimas de milímetro, el impulso eléctrico produce una chispa en el espacio entre un electrodo y otro, que inflama el combustible; hay bujías con varios electrodos, bujías que usan el proceso de 'descarga de superficie' para producir la chispa, y 'bujías incandescentes' (Glow-plug).
Si la bobina está en mal estado se recalienta; eso produce pérdidas de energía, reduce la chispa de las bujías y causa fallos en el sistema de encendido del automóvil. De los sistemas de generación de electricidad en los motores, las magnetos dan un bajo voltaje a pocas rpm, aumentando el voltaje de la chispa al aumentar las rpm, mientras los sistemas con batería dan una buena chispa a bajas rpm, pero la intensidad de la chispa baja al aumentar las rpm.
Dado que la combustión produce calor, todos los motores deben disponer de algún tipo de sistema de refrigeración. Algunos motores estacionarios de automóviles y de aviones, y los motores fueraborda, se refrigeran con aire. Los cilindros de los motores que utilizan este sistema cuentan en el exterior con un conjunto de láminas de metal que emiten el calor producido dentro del cilindro. En otros motores se utiliza refrigeración por agua, lo que implica que los cilindros se encuentran dentro de una carcasa llena de agua que en los automóviles se hace circular mediante una bomba. El agua se refrigera al pasar por las láminas de un radiador. Es importante que el líquido que se usa para enfriar el motor no sea agua común y corriente porque los motores de combustión trabajan regularmente a temperaturas más altas que la temperatura de ebullición del agua. Esto provoca una alta presión en el sistema de enfriamiento, dando lugar a fallas en los empaques y sellos de agua, así como en el radiador; se usa un refrigerante, pues no hierve a la misma temperatura que el agua, sino a más alta temperatura, y que tampoco se congela a temperaturas muy bajas.
Otra razón por la cual se debe usar un refrigerante es que este no produce costras ni sedimentos que se adhieran a las paredes del motor y del radiador, formando una capa aislante que disminuiría la capacidad de enfriamiento del sistema. En los motores navales se utiliza agua del mar para la refrigeración.
Al contrario que los motores y las turbinas de vapor, los motores de combustión interna no producen un par de fuerzas cuando arrancan (véase Momento de fuerza), lo que implica que debe provocarse el movimiento del cigüeñal para que se pueda iniciar el ciclo. Los motores de automoción utilizan un motor eléctrico (el motor de arranque) conectado al cigüeñal por un embrague automático que se desacopla en cuanto arranca el motor. Por otro lado, algunos motores pequeños se arrancan a mano girando el cigüeñal con una cadena o tirando de una cuerda que se enrolla alrededor del volante del cigüeñal.
Otros sistemas de encendido de motores son los iniciadores de inercia, que aceleran el volante manualmente o con un motor eléctrico hasta que tiene la velocidad suficiente como para mover el cigüeñal. Ciertos motores grandes utilizan iniciadores explosivos que, mediante la explosión de un cartucho mueven una turbina acoplada al motor y proporcionan el oxígeno necesario para alimentar las cámaras de combustión en los primeros movimientos. Los iniciadores de inercia y los explosivos se utilizan sobre todo para arrancar motores de aviones.

El motor convencional del tipo Otto es un motor de tipo alternativo de cuatro tiempos (4T), aunque en fuera borda y vehículos de dos ruedas hasta una cierta cilindrada se utilizó mucho el motor de dos tiempos (2T). El rendimiento térmico de los motores Otto modernos se ve limitado por varios factores, entre otros la pérdida de energía por la fricción, la refrigeración y falta de constancia en las condiciones de funcionamiento.
La termodinámica nos dice que el rendimiento de un motor alternativo depende en primera aproximación del grado de compresión. Esta relación suele ser de 8 a 1 o 10 a 1 en la mayoría de los motores Otto modernos. Se pueden utilizar proporciones mayores, como de 12 a 1, aumentando así la eficiencia del motor, pero este diseño requiere la utilización de combustibles de alto índice de octano para evitar el fenómeno de la detonación, que puede producir graves daños en el motor. La eficiencia o rendimiento medio de un buen motor Otto es de un 20 a un 25 %: solo la cuarta parte de la energía calorífica se transforma en energía mecánica.
Casi todos los motores de este tipo se fabrican para el transporte y deben trabajar suministrando diferentes potencias en cada momento.
Debido a esto el rendimiento de los mismos cae bruscamente al trabajar con carga parcial, ya que, cuando esto sucede, la cámara de compresión mantiene su volumen, dando una compresión final baja y transformando gran parte de la energía en calor.
Funcionamiento (Figura 1)
1. Tiempo de admisión - El aire y el combustible mezclados entran por la válvula de admisión.
2. Tiempo de compresión - La mezcla aire/combustible es comprimida y encendida mediante la bujía.
3. Tiempo de combustión - El combustible se inflama y el pistón es empujado hacia abajo.
4. Tiempo de escape - Los gases de escape se conducen hacia fuera a través de la válvula de escape.
También existe una variación del ciclo Otto que mejora la eficiencia del motor al aumentar el tiempo de expansión con respecto al tiempo de compresión conocido como Ciclo Miller.
En teoría, el ciclo diésel difiere del ciclo Otto en que la combustión tiene lugar en este último a volumen constante en lugar de producirse a una presión constante. La mayoría de los motores diésel son asimismo del ciclo de cuatro tiempos, salvo los de tamaño muy grande, ferroviarios o marinos, que son de dos tiempos. Las fases son diferentes de las de los motores de gasolina.
En la primera carrera, la de admisión, el pistón sale, y se absorbe aire hacia la cámara de combustión. En la segunda carrera, la fase de compresión, en que el pistón se acerca. el aire se comprime a una parte de su volumen original, lo cual hace que suba su temperatura hasta unos 850 °C. Al final de la fase de compresión se inyecta el combustible a gran presión mediante la inyección de combustible con lo que se atomiza dentro de la cámara de combustión, produciéndose la inflamación a causa de la alta temperatura del aire. En la tercera fase, la fase de trabajo, los gases producto de la combustión empujan el pistón hacia fuera, trasmitiendo la fuerza longitudinal al cigüeñal a través de la biela, transformándose en fuerza de giro par motor. La cuarta fase es, al igual que en los motores Otto, la fase de escape, cuando vuelve el pistón hacia dentro.
Algunos motores diésel utilizan un sistema auxiliar de ignición para encender el combustible al arrancar el motor y mientras alcanza la temperatura adecuada.
La eficiencia o rendimiento (proporción de la energía del combustible que se transforma en trabajo y no se pierde como calor) de los motores diésel dependen, de los mismos factores que los motores Otto, es decir de las presiones (y por tanto de las temperaturas) inicial y final de la fase de compresión. Por lo tanto es mayor que en los motores de gasolina, llegando a superar el 40 %. en los grandes motores de dos tiempos de propulsión naval. Este valor se logra con un grado de compresión de 20 a 1 aproximadamente, contra 9 a 1 en los Otto. Por ello es necesaria una mayor robustez, y los motores diésel son, por lo general, más pesados que los motores Otto. Esta desventaja se compensa con el mayor rendimiento y el hecho de utilizar combustibles más baratos.
Los motores diésel grandes de 2T suelen ser motores lentos con velocidades de cigüeñal de 100 a 750 revoluciones por minuto (rpm o r/min) (grandes barcos), mientras que los motores de 4T trabajan hasta 2500 rpm (camiones y autobuses) y 5000 rpm. (automóviles)
Con un diseño adecuado puede conseguirse que un motor Otto o diésel funcione a dos tiempos, con un tiempo de potencia cada dos fases en lugar de cada cuatro fases. La eficiencia de este tipo de motores es menor que la de los motores de cuatro tiempos, pero al necesitar solo dos tiempos para realizar un ciclo completo, producen más potencia que un motor cuatro tiempos del mismo tamaño.
El principio general del motor de dos tiempos es la reducción de la duración de los periodos de absorción de combustible y de expulsión de gases a una parte mínima de uno de los tiempos, en lugar de que cada operación requiera un tiempo completo. El diseño más simple de motor de dos tiempos utiliza, en lugar de válvulas en la culata, lumbreras, orificios (que quedan expuestos al ir subiendo y bajando el pistón). En los motores de dos tiempos, casi siempre lubricados añadiendo aceite a la gasolina, la mezcla de combustible y aire entra en el cilindro a través de la lumbrera de admisión cuando el pistón está en la posición más alejada de la culata. El primer tiempo es la compresión-encendido, en la que se inicia la combustión de la carga de mezcla aire/combustible/aceite cuando el pistón avanza hasta el final del ese tiempo (PMS). Después, el pistón se retira en la fase de explosión, abriendo el orificio de expulsión y permitiendo que los gases salgan de la cámara. De los dos procedimientos para el 'barrido' dentro de los cilindros de los motores de dos tiempos, proceso por el cual entra la nueva carga y se expulsan al escape los gases procedentes de la combustión de la mezcla de trabajo, se ha demostrado (SAE) que el llamado: 'barrido en lazo' ('Loop scavenging' en inglés) da siempre mejores resultados que el sistema llamado: 'Unidireccional' ('Uniflow scavenging' en inglés).
Hacia 1879 Nicolaus August Otto diseñó y construyó un motor con doble expansión, concepto propuesto por los ingleses Jonathan Hornblower y Artur Woolf en 1781, antes de que Watt llevase a la práctica la máquina de vapor. La primera expansión se hacía en el cilindro donde se realizó la combustión, y una segunda en otro pistón, este a baja presión, con el objetivo de lograr el aprovechamiento de la energía de los gases de escape; incluso se han construido motores con triple expansión, como el Troy, y el principio se usó en muchos motores marinos. En 1906 la empresa EHV radicada en Connecticut, EE. UU., fabricó un motor de combustión interna de tres cilindros y doble expansión que montaron en un automóvil. Al igual que el motor construido por Otto, cuyo comprador lo devolvió, el motor de EHV no demostró en la práctica las ventajas de menor consumo de combustible esperadas. En España hay dos patentes concedidas de motores con un principio similar, una de 1942 a Francisco Jimeno Cataneo (N.º OEPM 0156621) y otra de 1975 a Carlos Ubierna Laciana (N.º OEPM 0433850), en el INTA se construyó un prototipo de motor de aviación con cilindros en estrella y un principio parecido, ideado por el ingeniero J Ortuño García, patentes 0230551 y 0249247 y al que se atribuyó un consumo muy bajo de combustible, está expuesto en el Museo del Aire en Cuatro Vientos, Madrid. El año 2009, la empresa británica ILMOR presentó en una exposición internacional de motores en Stuttgart, un prototipo de motor de 5 tiempos, según una patente concedida en EE. UU. a Gerhard Schmitz. Para este motor anunciaron un consumo específico de 215 g/kWh, una relación de compresión efectiva de 14'5/1 y un peso inferior en 20 % a los motores convencionales equivalentes.[2]​[3]​[4]​
En la década de 1950, el ingeniero alemán Félix Wankel completó el desarrollo de un motor de combustión interna con un diseño revolucionario, actualmente conocido como Motor Wankel. Utiliza un rotor triangular-lobular dentro de una cámara ovalada, en lugar de un pistón y un cilindro.
La mezcla de combustible y aire es absorbida a través de un orificio de aspiración y queda atrapada entre una de las caras del rotor y la pared de la cámara. La rotación del rotor comprime la mezcla, que se enciende con una bujía. Los gases se expulsan a través de un orificio de expulsión con el movimiento del rotor. El ciclo tiene lugar una vez en cada una de las caras del rotor, produciendo tres fases de potencia en cada giro.
El motor de Wankel es compacto y ligero en comparación con los motores de pistones, por lo que ganó importancia durante la crisis del petróleo en las décadas de 1970 y 1980. Además, funciona casi sin vibraciones y su sencillez mecánica permite una fabricación barata. No requiere mucha refrigeración, y su centro de gravedad bajo aumenta la seguridad en la conducción. No obstante salvo algunos ejemplos prácticos como algunos vehículos Mazda, ha tenido problemas de durabilidad.
Una variante del motor de encendido con bujías es el motor de carga estratificada, diseñado para reducir las emisiones sin necesidad de un sistema de re-circulación de los gases resultantes de la combustión y sin utilizar un catalizador. La clave de este diseño es una cámara de combustión doble dentro de cada cilindro, con una antecámara que contiene una mezcla rica de combustible y aire mientras la cámara principal contiene una mezcla pobre. La bujía enciende la mezcla rica, que a su vez enciende la de la cámara principal. La temperatura máxima que se alcanza es suficientemente baja como para impedir la formación de óxidos de nitrógeno, mientras que la temperatura media es la suficiente para limitar las emisiones de monóxido de carbono e hidrocarburos.

La pólvora es una mezcla deflagrante utilizada principalmente para propulsar proyectiles en las armas de fuego, y con fines acústicos y visuales en los espectáculos pirotécnicos. La palabra pólvora comenzó designando primero a la pólvora negra empleada hasta fines del siglo XIX y principios del XX, cuando fue sustituida por la más potente pólvora sin humo, también llamada pólvora blanca, que además presentaba otras ventajas. 
La pólvora negra está compuesta de determinadas proporciones de carbón vegetal, azufre y nitrato de potasio,[1]​ pero con la aparición de los propelentes nitrocelulósicos modernos, la denominación de pólvora se extendió a una mezcla de productos químicamente distintos bajo el nombre de pólvora sin humo.
La pólvora negra más popular tiene 75 % de nitrato de potasio, 15 % de carbono y 10 % de azufre (porcentajes en masa). Actualmente se utiliza en pirotecnia y como propelente de proyectiles en armas antiguas. Las modernas pólvoras sin humo están basadas en materiales energéticos, principalmente nitrocelulosa (monobásicas) y nitrocelulosa más nitroglicerina (bibásicas). Las ventajas de las pólvoras modernas son su bajo nivel de humo, bajo nivel de residuo de productos de combustión en el arma y su homogeneidad y consistencia, lo que garantiza más precisión en los disparos.
El consenso entre las diferentes corrientes de estudio es que la pólvora se inventó en China, se distribuyó en los Orientes Medio y Próximo y a través de este último se introdujo en Europa;[2]​ sin embargo no hay consenso en cómo esta invención militar china influyó en los avances tecnológicos acerca de la pólvora en el Oriente Medio y Europa.[3]​[4]​ La distribución de la pólvora a lo largo de Asia desde China se atribuye en gran parte a los mongoles. Uno de los primeros ejemplos de europeos enfrentándose contra ejércitos con armas de fuego fue la batalla de Mohi, en 1241. En esta batalla los mongoles usaron pólvora tanto en armas de fuego como también en granadas.
La pólvora fue descubierta en China cuando los taoístas intentaban crear una poción para la inmortalidad. Las fuerzas militares chinas usaban armas basadas en pólvora (cohetes, mosquetes, cañones) y explosivos (granadas y diferentes tipos de bombas) contra los mongoles cuando estos intentaban entrar en sus tierras en la frontera norte. Después de que los mongoles conquistaran China y fundaran la dinastía Yuan usaron la tecnología militar china para su intento de invasión de Japón, donde también utilizaron la pólvora para propulsar sus cohetes.
Primeros cohetes chinos.
Armas de fuego con llave de mecha originarias de Europa usadas durante la Dinastía Ming.
Una bomba mongola arrojada contra un Samurái en carga durante las Invasiones mongolas a Japón después de fundar la Dinastía Yuan, 1281.
Cañón de mano de bronce de la Dinastía Yuan 1332; describe una mezcla de seis partes de azufre y seis partes de salitre y una parte de Aristolochia (de donde se obtenía el carbón).[5]​
El salitre -mezcla de nitrato de potasio (KNO3) y nitrato de sodio (NaNO3)- era conocido para los chinos desde antes del siglo I a. C. y hay evidencia clara del uso del salitre y sulfuro en muchas combinaciones médicas.[6]​ Un texto alquimista chino, fechado en el 492 d. C., menciona salitre quemándose con una llama púrpura, mostrándolo como una forma práctica y fiable de distinguirlo de otras sales inorgánicas, permitiendo de esta manera a los alquimistas evaluar y comparar técnicas de purificación. Los registros más antiguos de purificación de salitre se fechan antes del 1200 d. C.[7]​
La primera referencia a las propiedades incendiarias de dichas mezclas es el pasaje de Zhenyuan miaodao yaolüe, un texto taoísta datado a mediados del siglo IX d. C.[7]​

La palabra china para "pólvora" es en chino, 火药/火藥; pinyin, huŏ yào /xuou yɑʊ/, la cual significa literalmente "medicina de fuego".[9]​ Sin embargo, este nombre solo se empezó a usar algunos siglos después del descubrimiento de la mezcla.[10]​ Durante el siglo IX d. C. monjes taoístas o alquimistas chinos buscando el elixir de la inmortalidad encontraron accidentalmente la pólvora.[2]​[11]​
Los chinos tardaron poco tiempo en utilizar la pólvora para usos bélicos y en los siguientes siglos produjeron una gran variedad de armas de fuego, incluyendo lanzallamas, cohetes, bombas y minas terrestres antes de inventar el arma de fuego moderna, la cual utiliza un proyectil metálico.[12]​ Evidencia arqueológica de un cañón de mano datado a finales del siglo XIII fue encontrada en Manchuria[13]​ y bombas explosivas han sido descubiertas en un naufragio en las costas de Japón, datadas en el 1281, durante las invasiones mongolas de Japón[14]​
La obra china "Wu Ching Tsung Yao" (Fundamentos Esenciales de la Milicia Clásica), escrita por Tseng Kung-Liang entre 1040-1044 d. C., provee referencias enciclopédicas de una variedad de mezclas que incluía petroquímicos —así como miel y ajo. Una mecha de acción lenta para mecanismos lanzallamas usando el efecto sifón para cohetes y fuegos artificiales aparece también mencionada. Sin embargo, las recetas para las mezclas de este libro no contienen suficiente salitre para crear un explosivo, siendo limitadas a un máximo de 50 % de salitre solamente producen un efecto incendiario.[15]​ Esta obra fue escrita por un burócrata de la corte de la dinastía Song, y se cree que hay poca evidencia del impacto inmediato de esta obra en los conflictos armados. No hay mención del uso de pólvora en las crónicas de las guerras contras los tangut en el siglo XI d. C., además de que China en esta época se encontraba en una situación de paz. La primera crónica del uso de "lanzas de fuego" es en el sitio de De'an en 1132.[16]​
Fórmula para pólvora en 1044 Wujing zongyao parte I vol 12.
Instrucciones para una bomba de fuego en Wujing zongyao.
Bomba de fuego.
Granada de fuego.
Proto-cañón del texto de la dinastía ming Huolongjing.
Mina terrestre del texto de la dinastía ming Huolongjing.
Lanza-cohetes "Flecha de fuego" del Wujing zongyao.
Los musulmanes adquirieron el conocimiento de la pólvora entre 1240-1280, cuando el sirio Hasan al-Rammah había escrito en arábico, recetas para la pólvora, instrucciones para la purificación de salitre y descripciones de armas incendiarias. La pólvora llegó al oriente Próximo posiblemente a través de la India y esta de China. Esto se deduce de la forma de llamar a la pólvora que al-Rammah usaba, donde al salitre lo llamaba "nieve china" (en árabe: ثلج الصين‎ thalj al-ṣīn), a los fuegos artificiales como "flores chinas" y a los cohetes como "flechas chinas".[17]​ Los persas llamaban al salitre "sal china"[18]​[19]​[20]​[21]​[22]​ o "sal de los pantanos de sal chinos" (en persa: نمک شوره چيني‎ namak shūra chīnī).[23]​[24]​
Al-Hassan afirmó en la batalla de Ain Jalut en 1260, que los mamelucos usaron en "el primer cañón de la historia" contra los mongoles una mezcla de pólvora con una composición casi idéntica a la pólvora explosiva.[25]​ La evidencia documental sobreviviente más antigua que se conoce acerca del uso del cañón de mano, considerada el tipo más antiguo de arma de fuego, proviene de varios manuscritos árabes del siglo XIV.[26]​ Al-Hassan argumenta que estos están basados en originales más antiguos y que ellos reportan que los mamelucos usaron cañones de mano en la batalla de Ain Jalut en 1260.[25]​Hasan al-Rammah incluía 107 recetas para pólvora en su obra al-Furusiyyah wa al-Manasib al-Harbiyya (El libro de la caballería militar y dispositivos ingeniosos de guerra) y 22 recetas para cohetes, donde estas recetas tenían una composición casi idéntica a la composición moderna de la pólvora.[25]​
La pólvora fue inventada en China, aproximadamente en el siglo IX de nuestra era. Los bizantinos y los árabes la introdujeron en Europa alrededor del 1200. Es probable que la pólvora se introdujera en Europa procedente del Oriente Próximo. La primera referencia a su fabricación en Europa se encuentra en un documento de Roger Bacon, la Epistola de secretis operibus Artis et Naturae, et de nullitate Magiae (ca. 1250). En este texto leemos:
Berthold Schwarz, un monje alemán, a comienzos del siglo XIV, puede haber sido el primero en emplear pólvora para impulsar un proyectil, aunque parece ser que por esa misma época los árabes ya la habían usado con ese mismo fin en la Península ibérica, según se desprende de las crónicas del rey Alfonso XI de Castilla. El siguiente párrafo, transcrito y adaptado al castellano moderno, corresponde a la crónica del rey Alfonso XI sobre el sitio de Algeciras (1343), y es la primera referencia escrita del empleo de la pólvora con fines militares, si bien hay quien sostiene que esa misma sustancia ya había sido utilizada, también por los árabes, en la defensa de la ciudad de Niebla (Huelva) cuando fue sitiada por Alfonso X el Sabio, casi un siglo antes.[27]​

Sean cuales fueren los datos precisos y las identidades de sus descubridores y primeros usuarios, lo cierto es que la pólvora se fabricaba en Italia en 1326,[28]​ en Inglaterra en 1334 y que en 1340, en territorios hoy pertenecientes a Alemania se contaba con instalaciones para producirla. El primer intento de emplear la pólvora para minar los muros de las fortificaciones se lleva a cabo durante el sitio de Pisa (Italia) en 1403. En la segunda mitad del siglo XVI, la fabricación de pólvora era un monopolio del Estado en la mayoría de los países. Fue el único explosivo conocido hasta el descubrimiento (1585) del denominado aurum fulminans / oro fulminante, un poderoso explosivo muy inestable, sin fórmula definida, aunque mezclaba tricloruro de oro y amoniaco, utilizado por primera vez en 1628 durante las contiendas bélicas que se desarrollaron en el continente europeo.[29]​ El tomo V del Diccionario de Autoridades (1737) lo define así:
La pólvora y las armas de fuego fueron traídas a la India a través de las invasiones mongolas a la India.[30]​ El almirante Otomano Seydi Ali Reis introdujo las primeras versiones de armas de fuego con llave de mecha, la cual los otomanos utilizaron contra los portugueses en el Sitio de Diu (1531). Después de eso se presentaron muchas variedades de armas de fuego en Tanjore, Daca, Bijaour y Murshidabad.[31]​
El Imperio mogol produjo masivamente armas de fuego de llave de mecha para su ejército. El Imperio mogol fue el primero en desarrollar cohetes de bambú, principalmente para señalizaciones y para el uso de los zapadores. El emperador mogol se enfrentó a los británicos y otros europeos en la provincia de Guyarat, de donde los europeos extraían salitre para la fabricación de su pólvora durante el siglo XVII.[32]​
En el año 1780 los británicos empezaron a anexarse los territorios del sultanato de Mysore, durante la Segunda guerra anglo-mysore. El batallón británico fue derrotado durante la batalla de Guntur, por las fuerzas de Hyder Ali, quien usó de manera efectiva los cohetes Mysore y artillería de cohetes contra las tropas británicas cuyas filas estaban muy apretadas. Esta tecnología fue copiada y utilizada en las guerras napoleónicas en Europa.[33]​
Evidencia documental y arqueológica indica que comerciantes árabes o indios introdujeron la pólvora, mosquetes y cañones en Indonesia alrededor del siglo XIV.[34]​ Los invasores portugueses y españoles se enfrentaban con estas armas de fuego y generalmente eran superados.[35]​ El imperio Singhasari tenía armas de fuego y cañones.[36]​ Los pobladores de Java tenían cañones de bronce, usados ampliamente por la armada de los Majapahit así como por piratas.
La pólvora fue introducida en América por los conquistadores españoles y portugueses los cuales la utilizaron en contra de los aztecas, mayas, incas, etc. En varias regiones de México se podían encontrar fácilmente yacimientos de salitre y azufre, por lo que las fuerzas de los conquistadores pudieron reponer la pólvora que utilizaban sus armas de fuego.
Popularmente se cree que las armas de fuego fueron un factor determinante en la derrota de las civilizaciones locales, sin embargo la evidencia arqueológica y documental muestra que las armas de fuego que portaban los europeos no eran aún tan efectivas y tenían poca ventaja táctica, tampoco causaban pánico en los habitantes locales como popularmente se cree, ya que las fuerzas locales se acostumbraron rápidamente a su uso. Inclusive aprendieron cómo funcionaban los mosquetes y cañones, evitando ser alcanzados por ellos.
Las armas de fuego basadas en pólvora se empezaron a usar ya sea por los locales o las expediciones europeas, enfrentándose desde el siglo XV hasta principios del siglo XX, ya que la pólvora y las armas de fuego fueron comercializadas a los nativos americanos,[40]​ principalmente por los franceses y portugueses, intentando debilitar la influencia de sus rivales europeos (ingleses y españoles). A finales del siglo XIX en enfrentamientos entre fuerzas nativas americanas contra fuerzas de los EE. UU. las armas de fuego no traían un gran beneficio estratégico, permitiendo a los locales ganar batallas como la de Little Big Horn, donde los lakotas, los cheyennes y los arapahoes derrotaron al 7.º Regimiento de Caballería. Su derrota se atribuye en parte a la negativa de usar ametralladoras Gatling.[41]​
Las armas de fuego basadas en pólvora empezaron a tener una ventaja militar considerable hasta la introducción de las armas de fuego de repetición, desarrolladas a finales del siglo XIX, las cuales fueron un factor determinante en la culminación de las largas guerras contra los nativos americanos. Esta arma se utilizó contras estas poblaciones principalmente en EE. UU., México[42]​ y Argentina entre otros.
Cada país desarrollo su propia pólvora variando las proporciones de la mezcla, la siguiente tabla indica algunas las proporciones adoptadas por diferentes naciones. Tabla 1
Tabla 1 Tabla de proporciones de componentes de la pólvora por nacionalidad que la producía.[43]​
Azufre
Salitre
Carbón vegetal
Químicamente, el carbón y el azufre arden gracias al nitrato potásico, que es el comburente, pues suministra el oxígeno para la combustión. Se puede emplear nitrato de sodio, pero es higroscópico (condensa sobre sí la humedad del ambiente). También hay otra pólvora, comúnmente usada en el pasado, que, en vez de nitrato potásico, lleva clorato de potasio (KClO3), cuyo uso era normal en pirotecnia; pero fue abandonado gradualmente a causa de su peligrosa alta sensibilidad a la temperatura, la fricción y los golpes en favor del más estable oxidante perclorato de potasio. Usando nitrato de sodio (Nitrato de Chile), que es más higroscópico que el nitrato de potasio, es necesario disminuir la cantidad, mientras que se debe aumentar la dosis de azufre.
Una buena dosis experimental, en porcentaje de masa, es:
- nitrato de potasio al 70%;
- carbón vegetal 17%;
- azufre 13%.
Entonces obtendrá la siguiente reacción:
10 KNO3 + 5 S + 2 C7H3O  =>  5 Na2O + 3 H2O + 5 SO2 + 14 CO + 5 N2
Para una combustión rápida, se utiliza carbón vegetal producido por pirólisis de madera a 500 °C. Los mejores resultados se obtienen con carbón de Frangula alnus o arraclán, Solanum mauritianum, Prunus doméstica o ciruelo, Salix caprea o sauce cabruno, y Fraxinus americana o fresno blanco. El azufre y el carbono deben reducirse a un polvo con partículas de menos de 80 micrómetros antes de mezclarlo con un molino de bolas. Posteriormente se agrega una mezcla de nitrato de sodio y alcohol y luego todo se mezcla en una licuadora, o en una mezcladora, para obtener una mezcla muy homogénea. Finalmente, la mezcla se seca a baja temperatura y se reduce suavemente a polvo usando una mano de mortero. Así se obtiene un polvo negro que arde con un destello.
El clorato de potasio no es higroscópico y funciona mejor que el nitrato de potasio, pero la combustión junto al carbón y al azufre se hace mucho más rápidamente, siendo casi explosiva; por ello se usa en pirotecnia. Las cantidades de cada componente son: 50 % KClO3, 35 % carbón y 15 % azufre. El azufre ayuda en la combustión, porque cuando se quema, se produce dióxido y trióxido de azufre, SO2 y SO3, y al juntarse con moléculas de agua procedentes, no de la combustión, sino de la humedad, se producen ácido sulfúrico (H2SO4) y sulfuroso (H2SO3), que reaccionan violentamente con el clorato de potasio, haciendo que se descomponga muy rápidamente.
Aunque aún se pueda encontrar este tipo de pólvora para los fines descritos anteriormente, ésta fue desplazada por la pólvora nitrocelulósica o sin humo en la última década del siglo XIX, substituyéndola totalmente por las notables ventajas que tenía sobre la otra.
Aun cuando este fenómeno parece efectuarse instantáneamente, es un hecho comprobado que se verifica de una manera progresiva y empleándose un tiempo más o menos largo, el necesario, no tan solo para que la inflamación se propague a toda la masa de pólvora que constituye una carga, sino también para la combustión total de cada grano. Para calcular la cantidad de pólvora quemada en un tiempo dado, deduciendo de ella la de los gases producidos se hace preciso tener en cuenta la mayor o menor rapidez, o sea la velocidad de cada uno de los dos fenómenos que constituyen el de la deflagración (un tipo de oxidación menos rápida que la explosión), que son:  la inflamación y la combustión, entendiéndose por inflamación la propagación del calor a toda la carga, por efecto de la fuerza expansiva de los gases a la alta temperatura con que se producen desde el primer instante, y por combustión, las combinaciones que tienen lugar entre los elementos químicos de cada grano o de todos los que forman la carga.[43]​
La inflamación puede producirse por la chispa o centella que produce el choque de hierro sobre hierro, hierro sobre latón y latón sobre latón; es menos fácil si chocan hierro sobre cobre y cobre sobre cobre; se produce, además, por el de bronce sobre cobre, hierro sobre mármol, cuarzo sobre cuarzo, plomo sobre plomo, plomo sobre madera, muy raramente por el de cobre sobre madera y nunca por el de madera sobre madera, observándose que la interposición de una hoja de papel entre los cuerpos que chocan hace que la inflamación se favorezca. Se produce también la inflamación de la pólvora por la mera elevación de temperatura. Según las experiencias de Guillaume Piobert comprobadas por Horsley es preciso para esto que la temperatura llegue a ser de 300° a 315° y según Leygue y Champion basta sea 283° para la pólvora de caza y 293º para la de guerra. Cuando la temperatura se eleva de una manera gradual, se funde el azufre antes de llegar a los 300° unen los granos formando una pasta: si continúa elevándose la temperatura, puede vaporizarse el azufre y arrastrar en parte al carbón, llegando a descomponerse la pólvora sin deflagrar, siempre que no se llegue a la temperatura de ebullición del primero. Para la determinación de la temperatura de inflamación de la pólvora, emplearon Leygue y Champion una barra de cobre que calentaban por sus extremidades, aislando el foco de calor por medio de una pantalla para impedir la radiación: observaban la temperatura en diferentes puntos de la barra que distaban entre sí una magnitud fija, para lo que se valían de termómetros colocados en unas cavidades abiertas en ella y llenas de aceite. Cuando permanecía constante la temperatura marcada en los termómetros, colocaban en la extremidad más fría de la barra la pólvora sometida a la experiencia, aproximándola al otro extremo hasta tanto que se verificaba la inflamación o descomposición.[43]​
Las cifras expresadas no deben ser tomadas como precisas; sin embargo, si deben ser consideradas como indicador invariablemente de la temperatura de inflamación de la pólvora; sirven tan solo para marcar un punto alrededor del cual oscilan más o menos las diversas clases de pólvora. La divergencia que se nota en ellas debe principalmente atribuirse al estado de trituración de los ingredientes, siendo tanto mayor la temperatura necesaria para la inflamación cuanto más perfecta sea aquella.[43]​
El contacto con cuerpos inflamados es también causa de que la pólvora deflagre, siendo en este caso necesario que la temperatura sea muy elevada, habiéndose observado que una llama puede estar algunos segundos tocando a la pólvora sin que tal fenómeno se produzca.[44]​
El mejor medio y más seguro para producir la inflamación de la pólvora es por el contacto de cuerpos en ignición.[44]​
Diversas son las causas que pueden favorecer o retardar la inflamación. Estando húmeda la pólvora, se retarda por la pérdida de calórico para evaporar el agua, pudiendo suceder que, si la humedad es grande, no deflagre y solo se produzca una combustión lenta: las pólvoras angulosas son más fácilmente inflamables que las redondas y más también las no pavonadas que las que lo están.[44]​
Un ejemplo más claro de la deflagración de la pólvora es en el disparo de una bala mediante un arma de fuego.
Existen varios métodos para la determinación de restos de pólvora en las manos del tirador, que lo que hacen es utilizar reactivos más eficaces que la difenilamina, pero que no pueden solucionar los problemas de los falsos negativos, ya que siempre dependen que el disparo deje o no restos de pólvora en las manos del tirador. Lo que si se hace es utilizar otros componentes de las pólvoras para obtener el positivo, que no sean esos elementos, tan generalizados, en el medio ambiente, lo cual puede determinar con absoluta certeza que efectivamente se haya producido el disparo, ya que ello constituiría un falso positivo. No se conoce aún el método que puede determinar la existencia de la pólvora en general y no de algunos de sus componentes, como sucede en cualquier método químico o físico.[45]​
El método de Maloney se utiliza para la determinación de componentes de la pólvora la brusina, o sea la dimetoxiestricnina, en medio sulfúrico, que es más eficaz que la difenilamina.[45]​
Igualmente se emplean métodos que sirven al mismo tiempo para determinar la existencia de restos de pólvoras en otros elementos, como ropa, piel en orificios de entradas (tatuajes y/o ahumamientos), u otros elementos en donde exista un impacto de proyectil de arma de fuego a cortas distancias.[45]​
El realizado con microscopio de barrido electrónico es un método muy moderno, que analiza los elementos químicos que contiene el soporte que se requiere analizar, dando cantidad y calidad al mismo tiempo, es decir cuales elementos químicos contiene y cuantos. Es un método aconsejable, con el que se pueden analizar los residuos extraídos con los elementos señalados, o el taco de piel en la zona que se quiere analizar. Debemos aclarar que si bien es mucho más exacto que los métodos anteriores, también analiza algunos componentes de las pólvoras, pero no la pólvora en su conjunto. Lo interesante del tema, es que si conocemos cuales son los componentes de la pólvora que se desean analizar, se puede saber con más exactitud si es pólvora y de que marca.[45]​
El simple contacto con un arma de fuego deja residuos en sus manos, que pueden ser detectados con los métodos de análisis de laboratorio. Los resultados positivos, no indican incondicionalmente que una persona haya disparado un arma de fuego.[45]​
Los controles de contaminación de residuos metálicos de los materiales usados en los laboratorios fueron procesados por varias pruebas, y comprobando que existe la posibilidad de que en ciertos materiales, como el agua, ácido nítrico, recipiente de plástico, vidrios, etc., puedan contribuir para dar falso positivo, así mismo se debe mantener un constante control de fuentes que promueven una contaminación que no sea de laboratorios solamente).[45]​
La identificación de rastros de pólvora puede llevarse a cabo mediante tres tipos de investigaciones:
Química, Electrónica y Microscópica

El Coliseo o Anfiteatro Flavio (en latín Colosseum, en italiano Colosseo)[1]​ es un anfiteatro de la época del Imperio romano, construido en el siglo I. Está ubicado en el este del Foro Romano, y fue el más grande de los que se construyeron en el Imperio romano. Conocido originalmente como Anfiteatro Flavio (Amphitheatrum Flavium) pasa a ser llamado Coliseo (Colosseum) porque a su lado había una gran estatua, el Coloso de Nerón,[2]​ un monumento dedicado al emperador Nerón que posteriormente sufrió transformaciones y llegó a desaparecer.[3]​ 
Los materiales utilizados en la construcción de este son bloques de travertino, hormigón, madera, ladrillo, piedra (toba), mármol y estuco.
En la antigüedad poseía un aforo para unos 65 000 espectadores, con ochenta filas de gradas.[4]​[5]​[6]​ Los que estaban cerca de la arena eran el Emperador, su familia y los senadores, y a medida que se ascendía se situaban los estratos inferiores de la sociedad. En el Coliseo tenían lugar luchas de gladiadores y espectáculos públicos. Se construyó justo al este del Foro Romano, y las obras empezaron entre 70 d. C. y 72 d. C., bajo el mandato del emperador Vespasiano. El anfiteatro, que era el más grande jamás construido en el Imperio romano, se completó en 80 d. C. por el emperador Tito, y fue modificado durante el reinado de Domiciano.[2]​ Su inauguración duró 100 días, participando en ella todo el pueblo romano y muriendo en su celebración decenas de gladiadores y fieras que fueron sacrificados por el placer y el espectáculo del pueblo.[2]​ 
El Coliseo se usó durante casi cinco siglos, celebrándose en los últimos juegos de la historia en el siglo VI, bastante más tarde de la tradicional fecha de la caída del Imperio romano de Occidente en 476 d. C. Los bizantinos también lo utilizaron durante el siglo VI. Además de las peleas de gladiadores, muchos otros espectáculos públicos tenían lugar aquí, como naumaquias, caza de animales, ejecuciones, recreaciones de famosas batallas y obras de teatro basadas en la mitología clásica. El edificio dejó de emplearse para estos propósitos en la Alta Edad Media. Más tarde, sirvió como refugio, fábrica, sede de una orden religiosa, fortaleza y cantera. De sus ruinas se extrajo abundante material para la construcción de otros edificios, hasta que fue convertido en santuario cristiano, en honor a los cautivos martirizados durante los primeros años del cristianismo. Esta medida contribuyó a detener su expolio y a que se conservara.
Aunque la estructura está seriamente dañada debido a los terremotos y los picapedreros, el Coliseo siempre ha sido visto como un icono de la Roma Imperial y es uno de los ejemplos mejor conservados de la arquitectura romana. Es una de las atracciones turísticas más populares de la moderna Roma y aún está muy ligado a la Iglesia católica, por lo que el papa encabeza el viacrucis hasta el anfiteatro cada Viernes Santo.[7]​[8]​
El Coliseo, junto con todo el centro histórico de Roma, fue admitido en la lista del Patrimonio de la Humanidad por la UNESCO en 1980. El 7 de julio de 2007 fue reconocido como una de las siete nuevas maravillas del mundo moderno.
El nombre original del Coliseo era Amphitheatrum Flavium (Anfiteatro Flavio), y se le dio por haber sido construido en el reinado de los emperadores de la dinastía Flavia, después del reinado del emperador Nerón. Curiosamente, este nombre no era exclusivo del Coliseo, ya que Vespasiano[9]​ y Tito, constructores del Coliseo, también edificaron un anfiteatro que lleva el mismo nombre en el municipio de Puteoli (nombre moderno Pozzuoli).[10]​ La denominación Anfiteatro Flavio todavía se utiliza hoy en día, pero la estructura es más conocida popularmente como Coliseo. En la antigüedad también es posible que los romanos se refirieran al Coliseo por el nombre no oficial Amphitheatrum Caesareum, aunque este nombre podría haber sido estrictamente poético.[11]​[12]​
El nombre con el que es conocido actualmente, Coliseo, se empezó a divulgar a partir del siglo VIII y se piensa que se debe a una gran estatua del emperador Nerón que había al lado del edificio, llamada popularmente el Coloso (Colossus).[2]​ La estatua posiblemente fue derribada para reciclar el bronce; solo la base sobrevive, y está situada entre el anfiteatro y el templo de Venus y Roma.[3]​
El originario nombre latino de Colosseum fue derivando, a la Edad Media, hacia Coliseum, palabra que ha dado el castellano Coliseo, que pasó a tener más aceptación que el de Anfiteatro Flavio. Como referente posterior para las grandes construcciones destinadas al espectáculo, se llaman también coliseos los grandes teatros y, en general, cualquier otro edificio notable.
En 29 a. C. el cónsul romano Estatilio Tauro construyó un anfiteatro en el Campo de Marte, el primero de gran tamaño de la ciudad, con todas las instalaciones necesarias. Este edificio quedó destruido en el gran incendio de Roma del año 64, surgiendo la necesidad de un nuevo anfiteatro para la urbe romana.
El Coliseo fue un regalo del emperador Vespasiano al pueblo. Tenía el carácter público y de edificio civil donde se ofrecían espectáculos gratuitos de lucha entre gladiadores y fieras salvajes, escenificaban batallas mitológicas y se hacían simulacros de batallas navales. Eran espectáculos diurnos al aire libre. Cada espectador ocupaba el espacio según su categoría dentro de la sociedad romana: los asientos inferiores eran por el emperador y los senadores. Los peores lugares, arriba del todo, estaban reservados para los esclavos, los extranjeros y las mujeres.[13]​
El Coliseo simboliza y glorifica el emperador Vespasiano. Fue creado para dar una imagen benefactora y con una clara intención propagandística. Las obras del Coliseo empezaron bajo el mandato del emperador Vespasiano, entre 70 y 72 d. C,[4]​ y finalizó el 80dc, durante el reinado del emperador Tito.[14]​ El emplazamiento elegido era un llano entre las colinas de Celio, Esquilino y Palatino, a través del cual fluía una corriente canalizada. El emplazamiento donde se construyó el anfiteatro había sido devastado años atrás por el Gran incendio de Roma en 64 d. C., y aprovechando esta circunstancia, Nerón se apropió de gran parte del terreno para edificar su residencia: la grandiosa Domus Aurea. En ella ordenó construir una laguna artificial, la Stagnum Neronis, rodeada de jardines y pórticos. El ya existente acueducto de Aqua Claudia se amplió para que llegara hasta esa zona, y la gigantesca estatua de bronce conocida como el Coloso de Nerón se colocó al lado de la entrada de la Domus Aurea. De esta estatua recibe el anfiteatro el nombre de Coliseo.[15]​
El área se transformó durante el reinado de Vespasiano y sus sucesores. Aunque se conservó el Coloso, se derribó buena parte de la Domus Aurea. El lago se rellenó y la tierra sirvió como emplazamiento para el nuevo Anfiteatro Flavio. Se construyeron escuelas de gladiadores y otros edificios relacionados en los alrededores, donde anteriormente se encontraba la Domus Aurea. Según una inscripción reconstruida que se encontró en el lugar, «el emperador Vespasiano ordenó que este nuevo anfiteatro se erigiera usando su parte del botín como general». Esto puede referirse al gran tesoro que robaron los romanos tras su victoria en la primera guerra judeo-romana de 70 d. C. El Coliseo puede así ser interpretado como un gran monumento triunfal, siguiendo la tradición de celebrar las grandes victorias.[15]​ La decisión de Vespasiano de construir el Coliseo en el emplazamiento del lago de Nerón puede verse como un gesto popular para devolver a la gente una parte de la ciudad de la que Nerón se había apropiado para uso exclusivo. Al contrario que muchos otros anfiteatros, que se hallaban a las afueras de la ciudad, como el Anfiteatro Castrense, el Coliseo se levantaba justo en el centro de la urbe, situándolo literal y simbólicamente en el corazón de Roma.[16]​
Entre los siglos V y VI se prohibieron las luchas de gladiadores y de animales salvajes, y en el siglo XIII, el Coliseo se convierte en fortaleza. El último espectáculo que albergó el Coliseo del que se tiene noticia es en el año 523 por orden del rey godo Teodorico. Posteriormente el anfiteatro fue abandonado, e incluso parte de sus piedras, como la de tantos otros edificios históricos de los Foros Imperiales, se utilizaron como canteras para otros edificios más modernos. Fue solo a finales del siglo XIX cuando se excavó la estructura bajo la arena, y volvió a ser símbolo de la gloria de Roma.
El Coliseo albergó espectáculos como las venationes (peleas de animales) o los noxii (ejecuciones de prisioneros por animales), así como las munera: peleas de gladiadores. Se calcula que en estos juegos murieron unas 200 000 personas.[17]​ Asimismo, se celebraban naumachiae, espectaculares batallas navales que requerían inundar la arena de agua. Es probable que fueran en los primeros años, antes de construirse los sótanos bajo la arena. El Coliseo poseía un avanzado sistema de canalización de agua que permitía llenar y vaciar rápidamente el piso inferior.
Se desconoce la identidad del arquitecto del edificio, como ocurría en general con la mayoría de las obras romanas: las edificaciones públicas se erigían para mayor gloria de los emperadores. A lo largo de los años se han barajado los nombres de Rabirio, Severo, Gaudencio o incluso Apolodoro de Damasco, aunque se sabe que este último llegó a Roma en el año 105.
Cuando Vespasiano murió en 79, el Coliseo ya estaba completo hasta el tercer piso. Su hijo Tito terminó el nivel superior e inauguró el edificio en 80.[2]​ Dión Casio dice que se mató a más de 9000 animales salvajes durante los juegos inaugurales del anfiteatro. Más adelante se remodeló el edificio bajo el mandato del hijo pequeño de Vespasiano, el recientemente nombrado emperador Domiciano, quien construyó el hipogeo, túneles subterráneos que se usaban para alojar animales y esclavos. También añadió una galería en la parte superior del Coliseo para aumentar su aforo.[2]​[16]​[18]​
En 217, el Coliseo fue gravemente dañado por un gran incendio (causado por una tormenta eléctrica, según Dión Casio)[19]​ que destruyó el suelo de madera en el interior del anfiteatro. No se reparó del todo hasta 240 y se siguió remodelando en 250 o 252, y de nuevo en el año 320. Una inscripción recoge que varias partes del Coliseo fueron restauradas por Teodosio II y Valentiniano III (que reinaron de 425 a 450), posiblemente para reparar los daños que causó un terremoto en 443; las obras prosiguieron en 484 y 508. La arena se seguía usando para competiciones hasta bien entrado el siglo VI, registrándose la última pelea de gladiadores de la historia cerca del 435. La caza de animales continuó por lo menos hasta el año 523.[15]​
El Coliseo experimentó grandes cambios en su uso durante el periodo medieval. A finales del siglo VI se construyó una pequeña iglesia dentro de la estructura del anfiteatro, aunque aparentemente no le dio un significado religioso al edificio entero. La arena se transformó en un cementerio. Los numerosos espacios entre las arcadas y bajo los asientos se convirtieron en fábricas y refugios, y según las fuentes se alquilaron hasta el siglo XII.
Durante el papado de Gregorio Magno, muchos de los monumentos antiguos pasaron a manos de la Iglesia, que era la única autoridad efectiva. Sin embargo, carecía de recursos para mantenerlos, por lo que cayeron en el abandono y el expolio. En la Edad Media, la decadencia de la ciudad afectó a todos los monumentos imperiales. Los terremotos de 801 y 847 provocaron grandes destrozos en un edificio prácticamente abandonado en las afueras de la ciudad medieval.[18]​
Cuando en 1084 el papa Gregorio VII fue expulsado de la ciudad, muchos monumentos pasaron a manos de familias nobles romanas, que los usaron como fortalezas. Alrededor de 1200 la familia Frangipani se apropió del Coliseo y lo fortificó, usándolo de forma parecida a un castillo y convirtiéndolo en su área de influencia. El Coliseo fue cambiando de manos hasta 1312, en que volvió a la Iglesia.[20]​
El gran terremoto de 1349 dañó severamente la estructura del Coliseo, haciendo que el lado externo sur se derrumbase. Muchas de esas piedras desprendidas fueron recuperadas para construir palacios, iglesias (incluidos edificios de la Ciudad del Vaticano), hospitales y otros edificios en toda Roma. Una orden religiosa se asentó en el tercio norte del Coliseo y siguió habitándolo hasta principios del siglo XIX. La piedra del interior del anfiteatro fue picada en exceso, para reutilizarla en otra parte o (en caso de la fachada de mármol) quemarla para obtener cal viva.[15]​ Las abrazaderas de bronce que sostenían la mampostería fueron arrancadas de las paredes, dejando numerosas marcas. Aún hoy pueden observarse dichas cicatrices en el edificio.[20]​
A lo largo de los siglos XV y XVI, el travertino que lo recubría fue arrancado para emplearlo en otras construcciones, por ejemplo, el Palacio Barberini y el Puerto de Ripetta. Un conocido dicho latino reza Quod non fecerunt Barbari, fecerunt Barberini (lo que no hicieron los bárbaros, lo hicieron los Barberini). También se utilizó para quemarlo y obtener cal. El expolio de piedras continuó hasta 1749, en que Benedicto XIV consagró el monumento como lugar santo en memoria de los mártires allí ejecutados (si bien se cree que la mayoría de estos fueron martirizados en el Circo Máximo).[13]​
Durante los siglos XVI y XVII, funcionarios de la Iglesia buscaron un papel productivo para el casco en ruinas del gran Coliseo. El papa Sixto V (1585-1590) previó convertir el edificio en una fábrica de lana para emplear a las prostitutas de Roma, aunque esta propuesta fracasó debido a su muerte prematura. El siglo XIX, en cambio, comenzaron una serie de obras para estabilizar muchos monumentos antiguos. El 1820 se terminaron varios contrafuertes, restauración llevada a cabo por orden de Pío VII, que son claramente distinguibles hoy en día, y sin los cuales el edificio probablemente se habría derrumbado. Durante todo el siglo se sucedieron obras de consolidación y mejora, en un proceso que aún continúa.[20]​
Junto al Coliseo se encontraba la Meta Sudans, construida por el emperador Domiciano entre el 89 y el 96 ; era una fuente monumental de forma cónica con una altura de 17 metros y rodeada de agua. Su función era señalar la intersección de los cuatro distritos de la ciudad. En el año 1936 Mussolini la mandó demoler por «molestar» por la creación de la Via dei Fori Imperiali.[21]​ Una de las últimas barbaridades que sufrió el Coliseo fue ser objeto de simbolizar el borrador de la historia de Italia por parte de los militares; la parte del edificio que falta en la primera foto fue una bomba caída en el mismo durante la Segunda Guerra Mundial.
El Coliseo es sin duda uno de los grandes atractivos turísticos de Roma. Ha sido llevado al cine en múltiples ocasiones, destacando sobre todo la reconstrucción digital mostrada en la película Gladiator.
En 1980, la Unesco declaró el centro histórico de Roma, incluido el Coliseo, Patrimonio de la Humanidad.[22]​ Desde 2000, las autoridades mantienen el edificio iluminado durante 48 horas cada vez que en algún lugar del mundo se le conmuta o aplaza una sentencia de muerte a un condenado. 
Este monumento de la Roma Clásica ha sido designado una de Las Nuevas Siete Maravillas del Mundo Moderno, según la designación honorífica realizada en Lisboa el 7 de julio de 2007,[23]​ en el marco del concurso New 7 Wonders, organizado por el suizo Bernard Weber, del cual la Unesco se ha desmarcado completamente.[24]​
El Coliseo fue desalojado el 7 de agosto de 2011 por una alarma de bomba, que resultó inexistente. Una llamada telefónica había informado que había una lata con cables colgando. Según el alcalde de Roma, «Tenía algo de trementina, una batería y dos cables, pero ningún material explosivo».[25]​[26]​
En el verano de 2016 culminó una trabajosa tarea de limpieza del exterior del edificio, la primera de tipo integral que se le ha realizado en toda su historia; un proceso iniciado casi tres años antes. Financiada por la firma de calzados Tod's, la restauración consistió en el lavado de superficies mediante agua pulverizada (respetando la pátina de piedra y mármoles) y en la sustitución de estucados no idóneos. A este trabajo, con un presupuesto de 10 millones de euros, seguirá la restauración del interior del Coliseo y una modernización de los servicios al visitante, gracias a otra partida de 15 millones.[27]​
El Anfiteatro Flavio es un enorme edificio ovalado de 189 metros de largo, 156 de ancho y 48 de altura, con un perímetro elíptico de 524 metros. La organización de la obra se ejecutó con varios turnos de trabajadores durante el día y se llevó a cabo con elementos prefabricados que permitieron una construcción modular. Por este motivo se dice que este edificio ha sido un modelo para los recintos deportivos modernos, ya que tiene un diseño ingenioso y aporta soluciones a problemas actuales.[3]​
El edificio se basa en una construcción arquitrabada y rodeada a los tres primeros pisos que lo conforman. Se puede apreciar la utilización de la arquitectura típica de los romanos, con arcos de medio punto sobre unos pilares gruesos que también sostienen columnas adosadas de tipo decorativo. Los pilares aguantan los arquitrabes, los frisos y cornisas que fijan el límite de cada uno de los tres primeros pisos.[13]​
En el interior del edificio está el gran tesoro del Coliseo: la grada (o cávea) no aprovecha los desniveles del terreno, sino que se levanta sobre muchas vueltas superpuestas, apoyadas sobre pilastras de travertino. El corazón del edificio, gracias a ello, está lleno de kilómetros de pasillos abovedados y de escaleras: es una obra pensada para garantizar el movimiento fluido de mucha gente. En su configuración se usó la bóveda de cañón y la bóveda de arista, más compleja.
La fachada se erige sobre un estilóbato sobre el que se levantan cuatro pisos. El primer piso, de 80 arcadas, es de orden dórico toscano y refleja robustez y virilidad; el segundo piso es de orden jónico y se compone de base, fuste más esbelto que el dórico y un capitel de volutas; el tercer piso es de orden compuesto, con el capitel con hojas de acanto. El cuarto piso, el último, presenta un ático macizo, decorado con lesenas de estilo corintio. Este nivel hacía varias funciones: aumentaba la cabida, proporcionaba más obra en el interior y mejoraba el efecto visual. En este piso había 240 palos de madera que servían para aguantar las astas a las que se fijaba un toldo inmenso.[3]​
En el interior, los intercolumnios del segundo y tercer piso, había estatuas; los pasillos de la grada, relieves de mármol y de estuco.
La grada rodea la arena y bajo esta hay un subsuelo para agilizar el espectáculo. Las fieras eran conducidas a la arena con ascensor y también había pasillos y escaleras y una estudiada distribución de habitaciones y cuartos; además, se instaló un sistema de conducción de agua para transformar la arena en una gran piscina para las naumaquias.[20]​
El Coliseo romano fue quizás la obra más grandiosa de la arquitectura romana, y en él se utilizaron las más variadas técnicas de construcción. Las pilastras y los arcos son de travertino colocado sin argamasa. En las partes inferiores y en los sótanos se empleó la toba del mismo modo. Muchos de estos sillares iban sujetos con grapas metálicas. Las bóvedas que sostienen la cávea se hicieron vertiendo argamasa de cemento directamente sobre cimbras de madera, una innovación que aligeraba la fábrica.[28]​
El hecho de que el edificio se ubicase sobre una laguna obligó a excavar hasta 14 metros de limos inservibles y realizar una cimentación de casi 13 metros de opus cementicium (hiladas de argamasa de cal y piedras alternadas).
El terreno de juego propiamente dicho era un óvalo de 75 por 44 metros, y en realidad era una plataforma construida en madera y cubierta de arena. Todo el subsuelo era un complejo de túneles y mazmorras (el hipogeo) en el que se alojaba a los gladiadores, a los condenados y a los animales. El suelo disponía de varias trampillas y montacargas que comunicaban con el sótano y que podían ser usadas durante el espectáculo.[20]​
El plano de la arena tenía un completo sistema de drenaje, conectado a cuatro imponentes cloacas. Se ha sugerido que obedecen a la necesidad de evacuar el agua tras los espectáculos navales. Sin embargo, parece ser que ya Domiciano, abandonando la idea de la naumaquia, pavimentó las cloacas y colocó en la arena los montacargas para los combates de gladiadores. La cubierta de madera ya no se conserva, con lo que todo el laberinto subterráneo permanece hoy al aire libre.
El amplio graderío interior estaba diferenciado en gradus, pisos reservados para las diferentes clases sociales:[29]​
Además, algunos órdenes sociales, como los tribunos, sacerdotes o la milicia, tenían sectores reservados.[30]​ 
El acceso desde los pasillos hasta las gradas se producía a través de los vomitorios, llamados así porque permitían salir una enorme cantidad de gente en poco tiempo. Estaban tan bien diseñados que los 50 000 espectadores podían ser evacuados en poco más de treinta minutos.[13]​[30]​
El exterior se articula en cuatro órdenes, cuyas alturas no se corresponden con los pisos interiores. Los tres órdenes inferiores los forman 80 arcos sobre pilastras y con semicolumnas adosadas que soportan un entablamento puramente decorativo. El cuarto lo forma una pared ciega, con pilastras adosadas y ventanas en uno de cada dos vanos.
Los órdenes de cada piso son sucesivamente toscano, jónico y corintio. El último piso tiene un estilo indefinido que fue catalogado en el siglo XVI como compuesto. Era corriente superponer estilos diferentes en pisos sucesivos, pero no era habitual hacer edificios con cuatro órdenes superpuestos. Las comunicaciones entre cada piso se realizaban a través de escaleras y galerías concéntricas. El emperador tenía una entrada principal en la parte norte para él y su familia, y las otras tres entradas axiales eran para los cónsules.[13]​
El cubrimiento de los muros se realizó con estuco, aunque actualmente se encuentran a la vista los bloques de travertino colocados en hiladas y unidos con juntas de mortero y sujetos con grapas de plomo y bronce; en la zona superior del último piso se aprecian materiales más ligeros como el ladrillo.[30]​
El Coliseo contaba con una cubierta de tela desplegable accionada mediante poleas. Esta cubierta, hecha primero con tela de vela y luego sustituida por lino (más ligero), se apoyaba en un entramado de cuerdas del que poco se sabe. Cada sector de tela podía moverse por separado de los de alrededor y era accionado por un destacamento de marineros de la flota romana.[30]​
En la parte superior de la fachada se han identificado los huecos en los que se colocaban los 250 mástiles de madera que soportaban los cables. Al parecer las cuerdas se anclaban en el suelo, pues de otro modo los mástiles soportarían demasiado peso. A tal efecto había un anillo concéntrico de piedras o cipos situados a 18 metros de la fachada en la explanada exterior, y que también permitían controlar el público para evitar aglomeraciones. La franja entre la fachada y los cipos estaba pavimentada con travertino.[13]​
El Coliseo se usaba para peleas de gladiadores, así como una gran variedad de eventos. Los espectáculos, llamados munera, siempre eran patrocinados por ciudadanos en vez de por el Estado. Tenían un fuerte elemento religioso, pero también eran una demostración de poder e influencia familiar, y resultaron ser increíblemente populares en la plebe. Otro espectáculo popular era la caza de animales, o venatio. En ella se usaban una gran variedad de bestias salvajes, la mayoría importadas de África, como rinocerontes, hipopótamos, elefantes, jirafas, leones, panteras, leopardos, cocodrilos y avestruces.[20]​ Las batallas y la caza se representaban en escenarios con árboles y edificios móviles. Estos festejos se celebraban a veces a gran escala; se dice que Trajano celebró sus victorias en Dacia en 107 con juegos en los que participaron 11 000 animales y 10 000 gladiadores, desarrollándose durante 123 días. El público o el emperador podía salvar el gladiador que quedaba malherido mediante la posición en que pusiera su pulgar, arriba o abajo, representando la vida o la muerte del luchador vencido.[20]​
Durante los primeros días del Coliseo, los escritores clásicos decían que el edificio servía para naumachiae (más conocidas como navalia proelia) o simulaciones de batallas navales. Las fuentes que nos cuentan los juegos inaugurales que hizo Tito en el año 80 describen que el piso inferior se llenaba de agua para mostrar a caballos y toros previamente entrenados nadando. También nos relatan una recreación de una famosa batalla naval entre los griegos de Corfú y los corintios. Esto ha sido objeto de debate por los historiadores, ya que, aunque llenar el edificio de agua no hubiera presentado problemas, no está claro cómo podían haber hecho que la arena fuese impermeable, ni si hubiera habido espacio suficiente para que los barcos de guerra se moviesen. Se ha sugerido que las fuentes hablaban de otro lugar, o que el Coliseo tenía en sus orígenes un ancho canal inundable que iba hasta su eje central, y que posteriormente habría sido sustituido por el hipogeo.[15]​
El poeta Marcial también se hizo eco de dichos juegos inaugurales, y más concretamente, nos describe una lucha de gladiadores que pasaría a la historia, la de Vero y Prisco. Ambos lucharon hasta la extenuación ante el emperador Tito sin que ninguno de los dos llegara a imponerse sobre el otro. Tal empeño y capacidad de resistencia fue recompensado con el clamor popular, que llevó al César a perdonarles. Tan excepcional fue este hecho que Marcial lo recogió en su obra Liber spectaculorum.[31]​
Asimismo, se hacían sylvae o recreaciones de paisajes naturales en la arena. Pintores, técnicos y arquitectos construían una simulación de un bosque con árboles y arbustos reales que se plantaban en el suelo de la arena. Ponían animales para poblar el paisaje y asombrar a la multitud. Esos escenarios podrían haberse usado simplemente para mostrar un entorno natural a la población urbana, o como telón de fondo para la caza u obras que narraban episodios mitológicos. Ocasionalmente se usaban para ejecuciones en los que el héroe de la historia —interpretado por el condenado a muerte— era asesinado de manera espantosa pero mitológicamente auténtica, siendo devorado por bestias o quemado hasta la muerte.
Durante mucho tiempo se ha considerado al Coliseo como la escena de numerosos martirios de los primeros cristianos. De todas formas, esta creencia parece haber surgido durante el siglo XVI. Las fuentes romanas y de la Alta Edad Media se refieren a martirios cristianos en lugares de Roma vagamente descritos (en el anfiteatro, en la arena, etc.) pero sin especificar cuál; había, de hecho, numerosos estadios, anfiteatros y circos en Roma. A menudo se dice que san Telémaco, por ejemplo, murió en el Coliseo, pero Teodoreto, en sus escritos acerca de esta muerte, dice que falleció en el estadio (eis to stadio). El martirio de san Ignacio de Antioquía ocurrió en "la arena", según las fuentes, pero sin concretar qué arena.[32]​
En la Edad Media, el Coliseo no era visto como un lugar sagrado. Su uso primero como fortaleza y después como cantera demuestra la poca importancia espiritual que se le atribuía, en un tiempo en el que los lugares asociados con mártires eran muy venerados. No estaba incluido en los itinerarios reunidos para uso de los peregrinos ni en obras tales como la Mirabilia Urbis Romae ("Maravillas de la ciudad de Roma"), del siglo XII, que dice que el Circo Flaminio —y no el Coliseo— fue el lugar donde ocurrieron estos martirios. Parte de la estructura estaba habitada por una orden cristiana, pero aparentemente no tenían motivos religiosos ni espirituales para vivir allí.[20]​
Parece que durante los siglos XVI y XVII empezó a considerarse lugar santo al Coliseo. Se dice que el papa Pío V recomendó que los peregrinos reunieran arena del Coliseo como si fuera una reliquia, ya que estaba impregnada de la sangre de los mártires. Esta seguramente fue una visión minoritaria hasta que se hizo popular casi un siglo más tarde por Fioravante Martinelli, que puso al Coliseo a la cabeza de una lista de lugares sagrados a causa de los martirios que en ellos se celebraron, en su libro Roma ex ethnica sacra, de 1653.
Evidentemente, el libro de Martellini tuvo un claro efecto en la opinión pública. Como respuesta a la propuesta que algunos años después hizo el Cardenal Altieri de convertir el Coliseo en una plaza de toros, Carlo Tomassi publicó un panfleto como protesta a lo que consideraba una profanación.[20]​ La controversia que siguió persuadió al papa Clemente X para que cerrara las arcadas externas del Coliseo y lo declarara santuario cristiano, aunque el debate sobre cuán sacro era el edificio continuaría por algún tiempo más.
A petición de san Leonardo de Porto Maurizio, el papa Benedicto XIV prohibió que se usara el Coliseo como cantera y erigió un Viacrucis alrededor de la arena, que permaneció allí hasta febrero de 1874.[20]​ San Benito José Labre pasó los últimos años de su vida entre los muros del Coliseo, viviendo de la caridad de los fieles, hasta su muerte en 1783.[33]​ Varios papas del siglo XIX mandaron realizar trabajos de reparación y restauración en el Coliseo, por lo que el edificio aún conserva una conexión con la cristiandad. Se pusieron cruces en varios puntos alrededor de la arena y cada Viernes Santo el papa encabeza una procesión al anfiteatro en memoria de los mártires cristianos.[34]​
Siendo un icono de la cultura occidental, el Coliseo ha aparecido en numerosas películas y obras de arte de la cultura popular:
La fama de Coliseo como lugar de entretenimiento ha hecho que su nombre fuera aplicado a otros edificios públicos modernos, particularmente en los Estados Unidos, donde teatros, salas de conciertos y estadios se llaman comúnmente coliseos.[35]​
Numerosos artistas, especialmente del siglo XVIII y XIX, la han hecho objeto de sus obras de dibujo, grabado o pintura, entre ellos:
Capricci Romano por Bernardo Bellotto Il Canaletto (1742 - 1747), pintura al óleo.
Interno del Colosseo cono edicole por la Vía Crucis Giovanni Battista Piranesi (c.1750) grabado.
El Coliseo por Giovanni Battista Piranesi (1757), grabado.
Colosseum Roma por Heinrich August Pierer (1891), grabado.
Kolosseum und Forum Romanum de Rudolf Wiegmann ( 1835 ), pintura al óleo.
Coliseo por Franz Ludwig Catel (c.1830), pintura.
Interior of the Colosseum por Thomas Cole (1832), pintura.

El término ópera (del italiano òpera, "obras musicales") designa un género de música teatral en el que una acción escénica se armoniza, se canta y tiene acompañamiento instrumental. Las representaciones suelen ofrecerse en teatros de ópera, acompañadas por una orquesta o una agrupación musical menor. Forma parte de la tradición de la música clásica europea y occidental.
A diferencia del oratorio, la ópera es una obra destinada a ser representada. Algunos géneros de teatro musical están estrechamente relacionados con la ópera, como son la zarzuela española, el singspiel alemán, la opereta vienesa, la opéra-comique francesa y el musical inglés y estadounidense. Cada una de estas variantes del teatro musical tiene sus características propias, sin que tales les sean privativas y, en no pocas ocasiones, dando lugar a que las fronteras entre tales géneros no sean claras. En la ópera, como en varios otros géneros del teatro musical, se une:
La ópera se suele diferenciar de los otros géneros de teatro musical aceptándose que, la ópera es una representación completamente acompañada por música. La historia del género demuestra que tal afirmación no es correcta. Si bien la ópera se diferencia del teatro recitado por la extraordinaria participación de la música en su constitución, ya desde el barroco se conocían formas limítrofes como la mascarada, la ópera de baladas, la zarzuela y el singspiel que se confunden en no pocos casos con la ópera. Así, los singspiele de Wolfgang Amadeus Mozart se consideran óperas al igual que las zarzuelas de José de Nebra, mientras que Die Dreigroschenoper (La ópera de los tres centavos) de Kurt Weill está, en realidad, mucho más próxima al teatro recitado que a la ópera. Finalmente, hay otros géneros próximos a la ópera como lo son la ópera-ballet del barroco francés y algunas obras neoclasicistas del siglo XX como, por ejemplo, algunas obras de Igor Stravinsky. No obstante, en estas obras la parte principal expresiva recae en la danza mientras que el canto es relegado a un papel secundario. Al respecto de la diferencia entre la ópera y la zarzuela, la opereta, el singspiel y el musical inglés y estadounidense, la delimitación nace de una diferencia formal.
La palabra «opera» significa 'obra' en italiano (de la voz latina «opus», 'obra' o 'labor') sugiriendo que combina las artes del canto coral y solista, declamación, actuación y danza en un espectáculo escénico.
Algunos autores señalan como precursores formales de la ópera a la tragedia griega, a los cantos carnavalescos italianos del siglo XIV (la mascherata italiana) y a los intermedios del siglo XV (pequeñas piezas musicales que se insertaban durante las representaciones teatrales).[1]​
Dafne de Jacopo Peri fue la primera composición considerada ópera, tal como la entendemos hoy. Fue escrita durante 1597, bajo la gran inspiración de un círculo elitista de literatos humanistas florentinos, conocidos como la "Camerata de' Bardi" o "Camerata Florentina". Significativamente, Dafne fue un intento de revivir la tragedia griega clásica, parte del más amplio revivir de las características de la antigüedad, propio del Renacimiento. Los miembros de la Camerata consideraban que las partes corales de las tragedias griegas fueron originalmente cantadas, y posiblemente el texto entero de todos los roles; la ópera entonces fue concebida como una manera de "restaurar" esta situación.
Dafne se representó en privado por primera vez el 26 de diciembre de 1598 en el Palacio Tornabuoni de la ciudad de Florencia (Italia), y en público el 21 de enero de 1599 en el Palacio Pitti, de Florencia.[2]​
Dafne se halla perdida. Una obra posterior de Peri, Euridice, de 1600, es la primera ópera que ha sobrevivido. El honor de ser la primera ópera que aún se presenta regularmente le corresponde a L'Orfeo de Claudio Monteverdi, compuesta para la corte de Mantua en 1607.
La ópera no iba a permanecer confinada a las audiencias cortesanas por mucho tiempo; sin embargo en 1637 la idea de una «temporada» (Carnaval) de óperas de «interés público», sostenida por la venta de boletos, surgió en Venecia. Monteverdi se había radicado allí, y compuso sus últimas óperas, Il ritorno d'Ulisse in patria y L'incoronazione di Poppea, para el teatro veneciano en la década de 1640. Su principal seguidor, Francesco Cavalli, colaboró en la propagación de la ópera en Italia. En estas primeras óperas barrocas, la amplia comedia fue combinada con elementos trágicos en una mezcla que sacudió algunas sensibilidades educadas, apareciendo el primero de varios movimientos reformistas de la ópera. Tal movimiento contó con el patrocinio de la Academia Arcadiana de Venecia, que estaba asociada con el poeta Metastasio. Los libretos de este autor ayudaron a cristalizar el género de la ópera seria, la cual se convirtió en la forma dominante de ópera italiana hasta fines del siglo XVIII. Una vez que el ideal metastasiano fue establecido firmemente por toda familia de la comedia en la ópera de la era barroca fue reservada para la que sería conocida como opera buffa.
Antes de que estos elementos fueran expulsados de la ópera seria, muchos libretos ofrecían por separado una trama cómica desdoblada, como una especie de «ópera dentro de una ópera». Un motivo de peso para que esto ocurriera fue el intento de atraer a los teatros de ópera públicos a los miembros de la creciente clase mercantil, nuevamente vigorosa, pero aún menos culta que las clases nobles. Estos argumentos separados fueron resucitados casi inmediatamente en el desarrollo por separado de una nueva tradición, que derivó en parte de la commedia dell' arte (como de hecho, tales tramas siempre habían estado), una largamente floreciente etapa de la tradición italiana. Apenas estos «intermedios» fueron presentados una vez en medio de los actos de una representación escénica, las óperas en el nuevo género del «intermezzi» se desarrollaron ampliamente en Nápoles durante las décadas de 1710 y 1720, siendo inicialmente presentadas durante los intermedios de la ópera seria. Llegaron a ser tan populares, sin embargo, que pronto eran ofrecidas como producciones separadas.
La ópera seria fue elevada en tono y altamente estilizada en forma, consistiendo generalmente en recitativo «secco» intercalado con largas arias «da capo». Esto produjo grandes oportunidades para el virtuosismo vocal, y durante la era dorada de la ópera seria el cantante se convirtió en estrella. Los roles de héroe fueron usualmente escritos para la voz de castrato, tales como Farinelli y Senesino, así como las heroínas sopranos como Faustina Bordoni, fueron altamente demandados a lo largo de toda Europa, mientras la ópera seria rigió los escenarios de cada país, excepto Francia. De hecho, Farinelli fue el cantante más famoso del siglo XVIII. La ópera italiana fijó el estándar barroco. Los libretos italianos fueron la norma, incluso cuando un compositor alemán como Handel se encontró escribiendo para las audiencias londinenses. Los libretos italianos continuaron dominando en el período clásico, por ejemplo, con óperas de Mozart, quien escribió en Viena casi un siglo después. Los principales compositores nativos italianos de ópera seria fueron Alessandro Scarlatti, Antonio Vivaldi y Nicola Porpora.
Véase también Reforma operística
La «ópera seria» tuvo sus debilidades y críticas, siendo atacados principalmente su gusto por el adorno vocal de cantantes magníficamente entrenados y el uso del espectáculo como reemplazo de la pureza y la unidad dramáticas. Ensayo sobre la ópera (1755), de Francesco Algarotti, demostró ser una inspiración para las reformas de Christoph Willibald Gluck. Sostuvo que la «ópera seria» tenía que volver a sus bases, el ideal metastasiano, y que todos los diversos elementos —música (instrumental y vocal), ballet, y puesta en escena— deben subordinarse al drama. Sus ideas no resultaron aceptadas por todos los compositores, dando inicio a la querella de gluckistas y piccinnistas. De igual modo, varios compositores del período, incluyendo a Niccolò Jommelli y Tommaso Traetta, intentaron poner en práctica sus ideales. El primero en tener consentimiento y dejar una impronta permanente en la historia de la ópera, sin embargo, fue Gluck. Gluck trató de gestar una «bella simplicidad». Esto ha sido ilustrado en la primera de sus óperas «reformadas», Orfeo ed Euridice, donde las líneas vocales carentes de virtuosismo son apoyadas por armonías simples y una presencia orquestal notablemente más rica de lo usual.
Las reformas de Gluck han tenido resonancia a través de la historia operística. Weber, Mozart y Wagner, en particular, fueron influenciados por sus ideas. Mozart, en muchos sentidos, el sucesor de Gluck, combinó un magnífico sentido del drama, armonía, melodía y contrapunto para componer una serie de óperas cómicas, especialmente Le nozze di Figaro, Don Giovanni y Così fan tutte (en colaboración con Lorenzo da Ponte), que permanecieron entre las más populares, amadas y conocidas del repertorio. Pero la contribución de Mozart a la «ópera seria» fue menos clara, a pesar de sus excelentes obras Idomeneo y La clemenza di Tito. Su estado de salud y su precoz muerte no le permitieron hacer renacer el género.
El movimiento operístico de bel canto floreció a principios del siglo XIX, siendo ejemplificado por las óperas de Rossini, Bellini, Donizetti, Pacini, Mercadante y muchos otros. Bel canto, en italiano, significa «canto bello», y la ópera deriva de la escuela estilística italiana de canto del mismo nombre. Las líneas belcantistas son típicamente floridas e intrincadas, requiriendo suprema agilidad y control del tono.
Continuando con la era del bel canto, un estilo más directo y vigoroso fue rápidamente popularizado por Giuseppe Verdi, comenzando con su ópera bíblica Nabucco. Las óperas de Verdi resonaban con el crecimiento del espíritu del nacionalismo italiano en la era posnapoleónica, y rápidamente se convirtió en un ícono del movimiento patriótico (aun cuando sus propias políticas no fueron quizás tan radicales). A principios de la década de 1850, Verdi produjo sus tres óperas más populares: Rigoletto, Il trovatore y La traviata. Pero continuó desarrollando su estilo, componiendo tal vez la mayor Grand Opéra francesa, Don Carlo, y culminando su carrera con dos trabajos inspirados en obras de Shakespeare, Otello y Falstaff, las cuales revelan el gran crecimiento en sofisticación de la ópera italiana desde principios del siglo XIX.
Luego de Verdi, el melodrama sentimental «realista» del verismo apareció en Italia. Este fue un estilo introducido por Pietro Mascagni con su Cavalleria Rusticana y Ruggero Leoncavallo con Pagliacci, estilo que llegó virtualmente a dominar los escenarios de ópera mundiales con obras tan populares como La bohème, Tosca, y Madama Butterfly de Giacomo Puccini. Compositores italianos posteriores, tales como Luciano Berio y Luigi Nono, experimentaron con el modernismo. En España, un ejemplo reciente de este verismo es la Ópera La Casa de Bernarda Alba interpretada en 2018 en el Teatro de la Zarzuela con música de Miquel Ortega.
La primera ópera alemana fue Dafne, compuesta por Heinrich Schütz en 1627, cuya música se ha perdido. La ópera italiana continuó teniendo gran presencia e influencia sobre los países de habla alemana hasta finales del siglo XVIII. De todas maneras, se desarrollaron formas nativas. En 1644, Sigmund Staden produjo el primer Singspiel, Seelewig, una forma popular de ópera en idioma alemán, en el cual se alternan canto con diálogo hablado. A fines del siglo XVII y principios del siglo XVIII, el Theater am Gänsemarkt en Hamburgo presentó óperas alemanas de Keiser, Telemann y Handel. Aún muchos compositores alemanes importantes de la época, entre ellos el mismo Handel, Graun, Hasse y más tarde Gluck, elegían escribir la mayoría de sus óperas en idiomas extranjeros, en especial, italiano.
Los «Singspiele» de Mozart, Die Entführung aus dem Serail (1782) y Die Zauberflöte (1791) fueron un importante salto para la consecución del reconocimiento internacional de la ópera alemana. La tradición fue desarrollada en el siglo XIX por Beethoven con su Fidelio (1805), inspirada en el clima de la Revolución francesa. Carl Maria von Weber estableció la ópera dentro del Romanticismo alemán oponiéndose a la dominación del «Bel canto» italiano. Su Der Freischütz (1821) muestra su genialidad para la creación de atmósferas sobrenaturales. Otros compositores de la época fueron Marschner, Schubert, Schumann y Lortzing, pero la figura más significativa indudablemente fue Richard Wagner.
Wagner fue uno de los compositores más revolucionarios y controvertidos de la historia de la música. Comenzó bajo la influencia de Weber y Meyerbeer, gradualmente desarrolló un nuevo concepto de ópera como una “Gesamtkunstwerk” ("obra de arte completa"), una fusión entre música, poesía y pintura. En sus dramas musicales maduros, Tristan und Isolde, Die Meistersinger von Nürnberg, Der Ring des Nibelungen y Parsifal, abolió la distinción entre aria y recitativo en favor de un flujo continuo de "melodía sin fin". Incrementó en gran medida el protagonismo y el poder de la orquesta, creando partituras con una compleja red de leitmotiv, temas recurrentes a menudo asociados con los personajes y conceptos del drama; y estuvo preparado para saltarse convenciones musicales aceptadas, tales como la tonalidad, en este caso, para mayor expresividad. Wagner también trajo una nueva dimensión filosófica para la ópera en sus obras, las cuales usualmente se basan en historias de la mitología germana o leyendas arturianas. Wagner construyó su propio teatro en Bayreuth, dedicado exclusivamente a la representación de sus obras en el estilo que él deseaba.
La ópera no volvió a ser la misma después de Wagner y para muchos compositores su legado constituyó una pesada carga. Por otra parte, Richard Strauss aceptó las ideas wagnerianas pero la tomó en una dirección muy diferente. Su primer éxito lo consiguió con la escandalosa Salomé y la tragedia negra Elektra, en la cual la tonalidad fue llevada hasta sus límites. Luego Strauss cambió de rumbo en lo que sería su mayor éxito, Der Rosenkavalier, donde Mozart y los valses vieneses se convirtieron en gran influencia, como Wagner. Strauss continuó produciendo un altamente variado cuerpo de obras operísticas, a menudo con libretos del poeta Hugo von Hofmannsthal, hasta Capriccio en 1942. Otros compositores que hicieron sus aportes individuales a la ópera alemana en el siglo XX fueron Alexander von Zemlinsky, Paul Hindemith, Kurt Weill y el italiano Ferruccio Busoni. Las innovaciones operísticas de Arnold Schoenberg y sus sucesores serán discutidas en el apartado de Modernismo.
Rivalizando con las producciones importadas de la ópera italiana, una tradición francesa aparte fue fundada por el italiano Jean-Baptiste Lully en la corte del Rey Luis XIV. Desafiando su origen extranjero, Lully estableció una Académie Royale de Musique (Academia Nacional de Música) y monopolizó la ópera francesa desde 1672. Comenzando con Cadmus et Hermione, Lully y su libretista Quinault crearon la tragédie en musique, una forma en la cual las música para danza y para coro fueron particularmente prominentes. Las óperas de Lully también muestran preocupación por el recitativo expresivo, el cual ajustó a los contornos de la lengua francesa.
En el siglo XVIII, el sucesor más importante de Lully fue Jean-Philippe Rameau, quien compuso cinco tragédies en musique, como también numerosos trabajos en otros géneros tales como la opéra-ballet, todos notables por su rica orquestación y sus audaces armonías. Luego de la muerte de Rameau, el alemán Gluck fue convencido de producir seis óperas para los escenarios parisinos en la década de 1770. Estas mostraron la influencia de Rameau, pero simplificadas y muy enfocadas en el drama.
Al mismo tiempo, a mediados de siglo, otro género fue adquiriendo popularidad en Francia: la opéra-comique. Esta fue el equivalente del singspiel alemán, donde las arias eran alternadas con diálogo hablado. Destacados ejemplos en este estilo fueron producidos por Monsigny, Philidor y, sobre todo, por Grétry. Durante el período de la Revolución francesa y de las guerras napoleónicas, compositores tales como Méhul, Cherubini y Spontini, que fueron seguidores de Gluck, trajeron una nueva seriedad al género, el cual nunca había sido cómico, en tal caso.
En la década de 1820, la influencia gluckista en Francia llevó al gusto por el bel canto italiano, especialmente tras la llegada a París de Rossini. Su ópera Guillaume Tell colaboró en fundar un nuevo género, la Grand Opera, una forma en la cual el más famoso exponente fue también un extranjero, Giacomo Meyerbeer. Las obras de Meyerbeer, tales como Les Huguenots enfatizaron el canto virtuoso y extraordinarios efectos escénicos. La ligera opéra-comique también gozó de éxitos tremendos de las manos de Boïeldieu, Auber, Hérold y Adam. En este clima, las óperas del compositor nativo Hector Berlioz lucharon para ganar audiencia. La obra maestra épica de Berlioz Les Troyens, culminación de la tradición Gluckista, no tuvo representación en casi cien años.
En la segunda mitad del siglo XIX, Jacques Offenbach creó la opereta con obras ingeniosas y cínicas como Orphée aux enfers, así como también la ópera Les Contes d'Hoffmann; Charles Gounod registró éxitos masivos con Faust; y Georges Bizet compuso Carmen, la cual, ni bien la audiencia aprendió a aceptar su combinación de Romanticismo y Realismo, se convirtió en la más popular de todas las opéra-comiques. Realizó otra menos aclamada pero de gran calidad en 1863, Les pecheurs de perles. Massenet compuso Werther y Delibes con Lakme con el famoso dúo de las esclavas, así como Saint-Saëns compusieron obras que forman parte del repertorio estándar. Al mismo tiempo, la influencia de Richard Wagner fue sentida como un desafío a la tradición francesa. Muchos críticos franceses rechazaron airadamente los dramas musicales de Wagner, mientras que muchos compositores franceses los imitaron de cerca con éxito variable. Probablemente la respuesta más interesante llegó de Claude Debussy. Como en una obra de Wagner, la orquesta ocupa un rol principal en la única ópera de Debussy, Pelléas et Mélisande (1902) y no hay verdaderas arias, sólo recitativos. Pero el drama es incomprensible, enigmático y completamente no wagneriano.
Otros destacados nombres del siglo XX son Ravel, Dukas, Roussel y Milhaud. Francis Poulenc, uno de los poquísimos compositores posguerra de cualquier nacionalidad, cuyas óperas (incluyendo Dialogues des carmélites) han ganado un equilibrio en el repertorio internacional. Saint François d'Assise (1983), el extenso drama sagrado de Olivier Messiaen, ha atraído también amplia atención.[3]​
La ópera fue traída a Rusia en la década de 1730 por las compañías operísticas italianas y pronto se convirtieron en parte importante en el entretenimiento de la Corte Imperial Rusa y la aristocracia. Algunos compositores extranjeros como Baldassare Galuppi, Giovanni Paisiello, Giuseppe Sarti, y Domenico Cimarosa, entre otros, fueron invitados a Rusia a componer nuevas óperas, la mayoría, en idioma italiano. Simultáneamente algunos músicos nacionales como Maksym Berezovsky y Dmitri Bortniansky fueron enviados al extranjero a aprender a escribir óperas. La primera ópera escrita en idioma ruso fue Tsefal i Prokris del compositor italiano Francesco Araja (1755). El desarrollo de la ópera en lengua rusa fue apoyado por los compositores nativos Vasili Pashkévich, Yevstignéi Fomín y Alekséi Verstovski.
De todas maneras, el nacimiento real de la ópera rusa llegó con Mijaíl Glinka y sus dos grandes óperas Una vida por el Zar (1836) y Ruslán y Liudmila (1842). Posteriormente, en el siglo XIX en Rusia se escribieron obras maestras del género operístico, como Rusalka y El convidado de piedra de Aleksandr Dargomizhski, Boris Godunov y Jovánschina de Modest Músorgski, El príncipe Ígor de Aleksandr Borodín, Eugenio Onegin y La dama de picas de Piotr Ilich Chaikovski, y La doncella de nieve y Sadkó de Nikolái Rimski-Kórsakov. Estos desarrollos reflejaron el crecimiento del nacionalismo ruso a lo largo del espectro artístico, como parte del más general movimiento eslavofílico.
En el siglo XX las tradiciones de ópera rusa fueron desarrolladas por varios compositores, entre ellos, Serguéi Rajmáninov con sus obras El caballero avaro y Francesca da Rimini, Ígor Stravinski con El ruiseñor, Mavra, Oedipus rex, y The Rake's Progress, Serguéi Prokófiev con El jugador, El amor de las tres naranjas, El ángel de fuego, Compromiso en un monasterio y Guerra y paz; como también Dmitri Shostakóvich con La nariz y Lady Macbeth de Mtsensk, Edison Denisov con L'écume des jours, y Alfred Schnittke con La vida con un idiota, e Historia von D. Johann Fausten .
Desde la primera mitad del siglo XVII se estrenan en España cientos de obras en las que existen rasgos estilísticos propios y a veces muy singulares. En España no ocurre como en el resto de los países europeos, que imitan Italia a la hora de producir obras teatrales y líricas, sino que estos géneros escénicos y cantados surgen casi simultáneamente en ambos países, España e Italia, por tener el imperio español territorios en Italia, lo que había llevado a ambos países a tener numerosos intercambios culturales. La ópera bufa italiana, la opéra-comique francesa, el Singspiel alemán, la comedia musical, la opereta francesa etc. son géneros que dan lugar, todos ellos, a formas teatrales alternativamente cantadas y habladas, pero surgen en el siglo XVIII (en el XIX en el caso de la opereta) cuando desde los años 1620 Italia y España ya tenían óperas propiamente dichas y cuando desde los años 1640 España ya producía zarzuelas. Es crucial la aparición de esta última, la zarzuela, género lírico que alterna partes cantadas con partes habladas en castellano, antes incluso que el Singspiel alemán y que el opéra-comique francés, géneros líricos que también comparten esa característica.
Debido a su éxito, este legado lírico español, compuesto de óperas y zarzuelas, permite calificar a España como uno de los países más interesados en cualquier tipo de teatro cantado, y esto desde el nacimiento mismo del arte lírico. Así, en pleno Siglo de Oro, en 1622, fue estrenada en el Palacio de Aranjuez una obra considerada como precursora de todas las óperas españolas: La gloria de Niquea (con música de Mateo Romero, Juan de Palomares, Juan Blas de Castro y Álvaro de los Ríos).[4]​ Este estreno de 1622 sigue por poco tiempo a las primeras óperas representadas en Roma, pero precede a las primeras en ser representadas en Venecia. Francia y Alemania tendrán todavía que esperar para asistir al estreno de sus respectivas primeras obras de arte lírico. A La gloria de Niquea sucedió, en 1627, La selva sin amor,[5]​ otra obra teatral enteramente cantada con libreto de Félix Lope de Vega. La obra fue representada en el Alcázar de Madrid.
Poco después fue el Palacio del Buen Retiro el que se convirtió en el receptáculo habitual de las obras líricas de la corte española. Este palacio madrileño, hoy en día desaparecido (fue destruido en su mayor parte, en 1808, durante la guerra de Independencia), incluía entre sus dependencias un teatro cubierto, realizado siguiendo la moda de los nuevos teatros que empezaban entonces a ser construidos en Italia. Muchas partituras desaparecieron en el incendio que destruyó el Buen Retiro en el siglo XIX, partituras de obras en gran parte estrenadas en el teatro del palacio, aunque algunas se han conservado por otras fuentes. Entre los compositores de obras líricas del siglo XVII, conservadas hasta la fecha, pueden ser citados, entre otros, Cristóbal Galán, Juan de Navas, Juan de Serqueira y, sobre todo, Juan Hidalgo de Polanco (1614-1685). De este último se conserva Celos aun del aire matan,[6]​ ópera creada en el teatro del Palacio del Buen Retiro en 1660, con libreto de Pedro Calderón de la Barca y es considerada la ópera cantada más antigua que se conserva en España.
Durante los siglos XVII y XVIII, otros fenómenos líricos enriquecieron los escenarios con una cantidad de producción difícilmente cuantificable: destacan tres mil tonadillas (canciones que interpretan uno o dos intérpretes con estructura tripartita imitando lenguaje del pueblo) y doce mil zarzuelas. Además de loas, bailes, autos jácaras y mojigangas. Estos datos nos representan a un pueblo que vive la lírica. Sin dejar atrás que las estadísticas de consumo, de teatros musicales en activo, cantantes y edición de partituras de teatro musical son igualmente impresionantes. Destacan especialmente las figuras de Lope de Vega (1562-1635) y Calderón de la Barca (1600-1681)
En el siglo XVIII se mezclan influencias entre la zarzuela española y la ópera italiana. Por su imperio y por su permeabilidad cultural con Italia, España se enriqueció con compositores italianos que compusieron óperas con libreto en español, como Francesco Corradini (1700-1769) o Luigi Boccherini (1743-1805), aunque en la España peninsular, la metrópolis del imperio, también se escribieron o representaron libretos en italiano. En 1799 se acabó por prohibir la representación de las obras que no hubiesen sido traducidas al castellano, lo que hizo que muchos de los dramaturgos que escribían sainetes, tradujeran obras del italiano al castellano, nacionalizándolas. La creación operística española que tuvo lugar a lo largo de la primera mitad del siglo XVIII vio la aparición de músicos tan relevantes como Sebastián Durón —autor de la primera obra repertoriada en llevar el calificativo español de «ópera», La guerra de los gigantes, estrenada en 1700—, Antonio Literes, José de Nebra, Antonio Rodríguez de Hita o Domingo Terradellas, desembocando en las últimas décadas del siglo XVIII en dos creadores de la talla de Vicente Martín y Soler y Manuel del Pópulo Vicente García.
El siglo XIX trajo consigo una necesidad de liberación de la dependencia italiana, la necesidad de una creación lírica propia. Se construyeron teatros y se produjo una gran actividad crítica con numerosas polémicas y escritos. Por todo ello la ópera y la zarzuela (los dos pilares del arte lírico español) fueron el principal género de la música española del siglo XIX. La primera mitad del siglo (que culmina en 1850 con la inauguración del Teatro Real) supuso un período de gran vitalidad en torno a la ópera nacional española. El dualismo de la ópera y de la zarzuela provocó a partir de entonces una clara división de fuerzas. Por unos años, los músicos de mayor prestigio escogieron el camino de la zarzuela porque era el único que les garantizaba realmente el éxito y la subsistencia. Los grandes maestros especialistas en ópera, Ramón Carnicer i Batlle y Baltasar Saldoni, trataron a partir de 1838 de convertirse en los primeros compositores de ópera nacionalista española alentados por los intelectuales de la época, seguidos más tarde por Miguel Hilarión Eslava, Emilio Arrieta (autor de Marina), Antonio Reparaz o Valentín Zubiaurre. La época de la Restauración borbónica (iniciada en 1874) permitió a la clase acomodada frecuentar de nuevo los espectáculos, y al Teatro Real convertirse con renovada fuerza en aquello para lo que realmente servía: ser un lugar de representación escénica y lírica. En los cincuenta años siguientes se potenció en España el cultivo de la música como arte y ese ha sido, hasta ahora, el período más prolífico de la ópera española.
La primera concepción operística nacional de España fue plasmada por Tomás Bretón con su ópera Los amantes de Teruel. Estrenada en 1889 en el Teatro Real, se representa con gran éxito tanto en Madrid como en Barcelona. Seguidamente, la década de los años 1890 es en la historia de la ópera española rica y compleja; una de las épocas que más aportaciones ha dejado en la historia de la música española. Los músicos españoles hicieron muestra de tener una idea individual de lo que debía de ser la ópera española, y por ello estaban preparados para, desde un dominio del género, usar las fórmulas italianas, wagnerianas y populistas, pero casi siempre con un fuerte estrato nacionalista. En consecuencia la ópera española produjo obras diversas, variadas y originales, firmadas por ejemplo por Ruperto Chapí (autor de Margarita la Tornera) o Emilio Serrano. En los años 1890 se produjo también el advenimiento, en el mundo de la ópera española, de dos de los más influyentes protagonistas musicales de finales del siglo XIX, Enrique Granados e Isaac Albéniz.
Durante el siglo XX, aunque los compositores alternan con otros formatos musicales, en ópera destacan autores ya mencionados como Isaac Albéniz, con Pepita Jiménez (1896) o Merlin (1902), y Enrique Granados con Goyescas (1916), aparte de otros autores también destacados como Vicente Lleó Balbastre, con Inés de Castro (1903), Manuel Penella Moreno con El gato montés (1916), Joaquín Turina Pérez con Jardín de Oriente (1922), Conrado del Campo con La tragedia del beso (1911), Amadeo Vives con Artús (1897) y Maruxa (1914), Felipe Pedrell, Manuel de Falla con La vida breve (1913) y El retablo de Maese Pedro (1923), Jesús Guridi con Mirentxu (1913), Federico Moreno Torroba con La virgen de mayo (1925) y El poeta (1980) o Pablo Sorozábal con Adiós a la bohemia (1933) y Juan José (1968).
La ópera china es una forma de drama en China. Se practica desde la dinastía Tang con el emperador Xuanzong (712-755), quien fundó el "Jardín de peras", la primera compañía de ópera que se conozca en China. La compañía estaba casi exclusivamente al servicio de los emperadores. En la actualidad, las profesiones de la ópera son todavía llamadas Disciplinas del jardín de las peras. Durante la dinastía Yuan (1279-1368), se introdujeron en la ópera las variedades como el Zaju, con actuaciones basadas en esquemas de rima e innovaciones como la introducción de papeles especializados como "Dan" (femenino), "Sheng" (masculino) y "Chou" (payaso). Actualmente existen más de 300 variedades de ópera china, la más conocida en la Ópera de Pekín, que tomó su forma actual a mediados del siglo XIX y fue extremadamente popular durante la dinastía Qing (1644-1911). En la Ópera de Pekín, las cuerdas tradicionales de China y los instrumentos de percusión proveen un acompañamiento rítmico a la actuación. La actuación se basa en alusiones, gestos y otros movimientos de coreografía que expresan acciones como montar un caballo, remar en un bote o abrir una puerta. El diálogo hablado puede ser un texto recitado, empleado por los personajes serios de la trama o un texto coloquial empleado por mujeres y payasos. Los papeles están estrictamente definidos. Los maquillajes elaborados permiten distinguir el personaje que se está representando. El repertorio tradicional de la Ópera de Pekín incluye más de 1000 piezas, la mayoría proveniente de relatos históricos sobre enfrentamientos políticos y militares.
El 19 de octubre de 1701 se estrenó en el Virreinato del Perú la ópera La púrpura de la rosa, ópera en un acto compuesta por Tomás de Torrejón y Velasco sobre un libreto de Pedro Calderón de la Barca. Es la primera ópera compuesta y ejecutada en América y la única ópera sobreviviente de Torrejón y Velasco. La obra cuenta el mito de los amores de Venus y Adonis, que provoca los celos de Marte y su deseo de venganza. En 1711 se estrenó en la ciudad de México la ópera La Parténope con música de Manuel de Sumaya, maestro de la capilla catedralicia y el más grande compositor barroco mexicano. La especial importancia de esta ópera es que es la primera compuesta en América del Norte y la primera ópera compuesta en el continente por un americano. Esta ópera da inicio a la fecunda y aún poco estudiada historia de la creación operística latinoamericana no interrumpida desde entonces durante trescientos años. La primera ópera compuesta y estrenada en el Brasil fue I Due Gemelli, de José Maurício Nunes García, cuyo texto se perdió posteriormente. Se puede consdierar como la primera ópera genuinamente brasileña con texto en portugués A Noite de São João, de Elías Álvares Lobo.
El compositor de óperas brasileñas más famoso fue Carlos Gomes. Una parte importante de sus óperas fueron estrenadas en Italia con texto en italiano. No obstante, Carlos Gomes frecuentemente usó temáticas típicamente del Brasil. Tal es el caso de sus óperas Il Guarany y Lo Schiavo.
La ópera Guatemotzín del mexicano decimonónico Aniceto Ortega es el primer intento consciente por incorporar elementos nativos a las características formales de la ópera. Dentro de la producción operística mexicana del siglo XIX sobresalen la ópera Agorante, rey de la Nubia de Miguel Meneses, estrenada durante las festividades conmemorativas por el cumpleaños del emperador Maximiliano I de México, las óperas Pirro de Aragón de Leonardo Canales, Keofar de Felipe Villanueva.
La obra operística de Melesio Morales es la más importante de México del siglo XIX Sus obras Romeo y Julieta, Ildegonda, Gino Corsini, Cleopatra tuvieron gran éxito entre el público de la Ciudad de México y se estrenaron en Europa.
En Colombia el compositor José María Ponce de León había presentado desde su niñez una opereta (Un alcalde a la antigua y dos primos a la moderna), ya en 1874 presenta su primera ópera Ester y posteriormente en 1876 la zarzuela El castillo misterioso y en 1880 su ópera mayor Florinda. En Venezuela se mencionan compositores como Reynaldo Hahn, José Ángel Montero, Pedro Elías Gutiérrez, Federico Ruiz. La primera ópera de Venezuela es El maestro Rufo Zapatero, ópera bufa compuesta en 1847 por José María Osorio, aunque generalmente se piensa que fue Virginia, de Montero, estrenada en 1877. Anteriormente Osorio había compuesto varias zarzuelas, aunque se cree que la primera que se estrenó fue Los alemanes en Italia, de José Ángel Montero, en la década de 1860.
En Brasil destacan de la primera mitad del siglo XX compositores de ópera como Heitor Villa-Lobos, autor de óperas como Izath, Yerma y Aglaia, y Mozart Camargo Guarnieri, autor de Um Homem Só y Pedro Malazarte.
La ópera contemporánea del Brasil sigue las tendencias vanguardistas, como es el caso de obras como Olga, de Jorge Antunes, A Tempestade, de Ronaldo Miranda, e O Cientista, de Silvio Barbato.
En la primera parte de siglo XX, se produjeron en Ecuador óperas nacionalistas. Entre ellas tres inspiradas en la novela Cumandá o un drama entre salvajes del ecuatoriano Juan León Mera debidas a Luis Humberto Salgado, Pedro Pablo Traversari Salazar y Sixto María Durán Cárdenas (1875-1947).
Luis H. Salgado compuso además las óperas El tribuno, El Centurión y Eunice; la opereta Ensueño de amor y la ópera-ballet Día de Corpus.
En la primera mitad del siglo XX sobresalen en la creación operística mexicana Julián Carrillo y los compositores cercanos a él como Antonio Gomezanda, Juan León Mariscal, Julia Alonso, Sofía Cancino de Cuevas, José F. Vásquez, Arnulfo Miramontes, Rafael J. Tello, Francisco Camacho Vega y Efraín Pérez Cámara.
Todos ellos han sido relegados por la historiografía musical oficial que tan solo reconoció la obra de los compositores nacionalistas.[cita requerida] Desde finales del siglo XX en México (y toda Latinoamérica) hay un creciente interés de los compositores por escribir ópera. Entre los compositores mexicanos de inicios del siglo XXI que sobresalen con sus óperas debe mencionarse a Federico Ibarra, Daniel Catán, Leandro Espinosa, Marcela Rodríguez, Víctor Rasgado, Javier Álvarez, Roberto Bañuelas, Luis Jaime Cortez, Julio Estrada, Gabriela Ortiz, Enrique González Medina, Manuel Henríquez Romero, Leopoldo Novoa, Hilda Paredes, Mario Stern, René Torres, Juan Trigos, Samuel Zyman, Mathias Hinke, Ricardo Zohn-Muldoon, Isaac Bañuelos, Gabriel de Dios Figueroa, Enrique González-Medina, José Carlos Ibáñez Olvera, Víctor Mendoza y Emmanuel Vázquez.
Entre finales del siglo XX y principios del siglo XXI se han compuesto en Ecuador las óperas: Los Enemigos (1997) de Mesías Maiguashca, basada en el cuento El milagro secreto de Jorge Luis Borges, Manuela y Simón (2006) de Diego Luzuriaga y la "ópera para la voz de los instrumentos" El árbol de los pájaros (2002-2003) de Arturo Rodas.
Una de las principales tradiciones operísticas es la constituida por la ópera inglesa. Si bien Gran Bretaña fue centro activo de vida operística, contó con pocos compositores reconocidos a nivel mundial. Henry Purcell fue el más notable compositor barroco, entre sus aportes destacan su obra maestra, Dido and Aeneas (1689) y su obra shakesperiana La reina de las hadas (1692). Ya en el siglo XX, la ópera en inglés logra su esplendor principalmente gracias al aporte de Benjamin Britten, con óperas entre las cuales se cuentan Peter Grimes y La violación de Lucrecia; y se suman las composiciones de importantes creadores estadounidenses tales como George Gershwin, con su famosa Porgy and Bess, Treemonisha (1972) de Scott Joplin, Candide (1956) de Leonard Bernstein, Orpheé (1993) y La belle et la bête (1994) de Philip Glass, o Gian Carlo Menotti con Vanessa y La médium.
Los compositores checos también desarrollaron un próspero movimiento nacional operístico en el siglo XIX, comenzando con Bedřich Smetana que escribió ocho óperas, incluyendo la internacionalmente popular La novia vendida. Antonín Dvořák, más famoso por Rusalka, escribió 13 óperas; y Leoš Janáček obtuvo reconocimiento internacional en el siglo XX por sus obras innovadoras, entre ellas Jenůfa, La zorra astuta, y Katia Kabanová.
La figura clave de la ópera nacional húngara en el siglo XIX fue Ferenc Erkel, cuyos trabajos se centraron principalmente en temas históricos. Entre sus obras más representadas se encuentran Hunyadi László y Bánk bán. La ópera húngara moderna más famosa es El castillo de Barba Azul de Béla Bartók.
El más reconocido compositor de la ópera polaca fue Stanisław Moniuszko, aclamado por la ópera Straszny Dwór. En el siglo XX, otras óperas creadas por compositores polacos son El rey Roger de Karol Szymanowski y Ubu Rex de Krzysztof Penderecki.
Entre los compositores neerlandeses, se destacan Willem Pijper, con su ópera basada en la leyenda folclórica neerlandesa de Halewijn, y su discípulo Henk Badings, que compuso varias óperas para radio.
La relación entre estas dos formas artísticas es más que aparente, ambas son artes integradoras, es decir incluyen en su composición a otras formas artísticas. La ópera se constituye a partir de una pieza dramática con una estructura narrativa que la asemeja a una pieza teatral pero con otro componente, la música, que el mismo Claudio Monteverdi planteó en el siglo XVII, la música es lo primordial en la ópera. María Callas mencionaba que todas las intenciones dramáticas de los personajes y la historia en una Ópera están en la partitura, en la relación entre los sonidos.
La cinematografía es una forma audiovisual donde la secuencia narrativa está conducida por la peculiar manera en que se da la sucesión y construcción de las imágenes.
Existen varios acercamientos entre ambas formas, pero en muchos casos la cinematografía se usa solo como registro visual de la representación escénica.
Sin embargo se han dado varias cintas donde se aprovechan las cualidades de la sintaxis cinematográfica, lográndose una interrelación entre las imágenes, el texto, las acciones y el drama musical. Un ejemplo de ello es la cinta de 1963 a partir de la ópera de Leonard Berstein West Side Story. Recientemente se ha producido una versión de La Bohème de Puccini, con los cantantes Anna Netrebko y Rolando Villazón, donde se da esta integración de los tres conductores narrativos junto con el texto.
Una proposición significativa fue la realizada por Philip Glass cuando tomó la película La belle et la bête de Jean Cocteau, cronometró los diálogos y, en el mismo espacio de tiempo, los puso en metro músico. El estreno de la nueva ópera se realizó presentando la película en el escenario mientras músicos y cantantes ejecutaban la ópera desde el foso de la orquesta. La obra ha abierto nuevas posibilidades de integración de las artes. Si bien, en este caso, la ópera cinematográfica utiliza un material existente, en el caso de la ópera Bola Negra la concepción avanza un paso allende lo propuesto por Philip Glass. La ópera Bola Negra fue concebida desde su inicio como un experimento cinematográfico por el autor del libreto Mario Bellatín y la compositora Marcela Rodríguez. Es decir, no se utilizó material existente, como en el caso propuesto por Philip Glass, sino la ópera se concibió, en todas sus partes, como una proposición cinematográfica.
Asimismo óperas inspiradas o sobre historias que inspiraron películas son Muerte en Venecia, Dead Man Walking y Brokeback Mountain.
La voz para el canto operístico requiere ciertas características y cualidades. Se desarrolla con largos años de estudio y entrenamiento porque implica un desarrollo muscular de todo el aparato fonador o emisor de la voz que incluye además de las cuerdas vocales, el uso del diafragma y la resonancia de la cavidad bucal y los senos paranasales. De todo esto resulta el tipo de voz peculiar que se denomina «impostada», es decir que es producto de la colocación y proyección específica y controlada del aire. Se divide en tres partes, voz de pecho, voz de garganta y voz de cabeza.
Cada cantante posee una extensión y un registro particular. Dentro de esa extensión (número de notas que abarca, que va de 2 a 3 octavas), se encuentran los cambios de cada una de las tres colocaciones de la voz.
Por lo anterior, la voz operística demanda un entrenamiento constante y continuo conocido como vocalización, pues el abandono de dicho entrenamiento conlleva la disminución de la capacidad muscular y respiratoria.
El cantante lírico, sea en ópera o recital, no utiliza micrófono sino el caudal natural de su voz llevado a su máxima posibilidad.
Los cantantes de ópera y los roles que interpretan se clasifican en tipo de voz, basado en características tales como tesitura, agilidad, potencia y timbre de sus voces.
Los varones pueden ser clasificados por su rango vocal como bajo, bajo-barítono, barítono, tenor y contratenor, y las mujeres en contralto, mezzosoprano y soprano.
Los varones algunas veces pueden cantar en el registro vocal femenino, en cuyo caso se denominan sopranista o contratenor. De estos, sólo el contratenor es comúnmente encontrado en la ópera, algunas veces cantando partes escritas para castrato (varones castrados en una temprana edad para darles un rango vocal más alto del normal).
Las clasificaciones pueden ser más específicas, por ejemplo, una soprano puede ser descrita como soprano ligera, soprano de coloratura, soprano soubrette, soprano lírica, soprano lírico spinto, soprano dramática o soprano lírica-ligera. Estos términos, aunque no describan totalmente la voz, la asocian con los roles más convenientes para las características vocales del cantante.
La voz de cada cantante en particular puede cambiar drásticamente a lo largo de su vida; raramente se alcanza la madurez vocal antes de la tercera década, y algunas veces, hasta la mediana edad.
El espectro vocal podría resumirse en registros vocales así dispuestos desde la voz más aguda a la más grave:
La voz de soprano se ha utilizado típicamente a través de historia operística como la voz elegida para la protagonista femenina de la ópera en cuestión. El énfasis actual en un amplio rango vocal fue sobre todo una invención del Período Clásico. Antes de eso, el virtuosismo vocal, no la amplitud, fue la prioridad, con partes para soprano que raramente se extendían hasta un fa agudo (Händel, por ejemplo, sólo escribió un rol que se extiende hasta un do agudo), aunque el castrato Farinelli fue catalogado de poseer un re agudo.
La mezzosoprano, denominación de origen comparativamente reciente, posee también un extenso repertorio, partiendo desde el rol principal femenino de Dido and Aeneas de Purcell hasta los roles "pesados" como Brangäne en Tristan und Isolde de Wagner (ambos roles son a menudo interpretados por sopranos).
Para las verdaderas contraltos, el número de partes es más limitado, lo que ha dado lugar a la broma de que las contralto sólo cantan roles de "brujas, malvadas, y varones". En años recientes muchos de los "roles en pantalones" de la era barroca, escritos originalmente para mujeres, e interpretados originalmente por "castrati", han sido reasignados a los contratenores.
La voz de tenor, desde la era clásica en adelante, ha sido tradicionalmente asignada al protagonista masculino. Muchos de los roles de tenor más desafiantes en el repertorio han sido escritos durante la era del bel canto, tal como la secuencia de nueve do sobre el do central en la secuencia de La fille du régiment de Donizetti. Con Wagner llega el énfasis en la influencia vocal para sus protagonistas, con la categoría vocal denominada heldentenor; esta voz heroica tuvo su contrapartida italiana en roles como Calaf en Turandot de Puccini.
La voz de bajo tiene una larga historia en la ópera, habiendo sido empleada en papeles secundarios en la "ópera seria", y algunas veces para relieve cómico (bajo buffo) proveyendo un contraste con la preponderancia de voces altas en este género. El repertorio de bajo es amplio y variado, incluyendo desde la comedia con Leporello en Don Giovanni a la nobleza de Wotan en El anillo del nibelungo de Wagner.
En medio del tenor y el bajo se encuentra el barítono, el cual varía en "peso", desde Guglielmo en Così fan tutte de Mozart hasta Posa en Don Carlos de Verdi; la actual designación de "barítono" no ha sido utilizada sino hasta mediados del siglo XIX.
Las primeras representaciones operísticas fueron demasiado infrecuentes como para que los cantantes pudieran vivir exclusivamente del estilo, pero con el nacimiento de la ópera comercial a mediados del siglo XVII, comenzaron a surgir intérpretes profesionales.
El rol principal de héroe masculino era usualmente confiado a un castrato, y ya en el siglo XVIII, cuando la ópera italiana se presentaba en toda Europa, los castratos principales, que poseían un extraordinario virtuosismo vocal, tales como Senesino y Farinelli, se convirtieron en estrellas internacionales.
La carrera de la primera gran estrella femenina (o prima donna), Anna Renzi, data de mediados del siglo XVII. En el siglo XVIII, un número de sopranos italianas ganaron renombre mundial y frecuentemente se enredaban en feroces rivalidades, tal el caso de Faustina Bordoni y Francesca Cuzzoni, quienes comenzaron una pelea a golpes durante una representación de una ópera de Handel.
Los franceses no gustaban de los “castrati”, preferían que sus héroes fueran interpretados por un haute-contre (tenor alto), de los cuales Joseph Legros fue un ejemplo destacado.[9]​
En el siglo XIX hubo una figura femenina de gran éxito comercial para los parámetros de la época, María Malibrán, para quien algunos compositores reconocidos como Félix Mendelssohn realizaron piezas. Contaba con una amplio registro de casi 3 octavas. La Malibrán era hija del maestro de canto Manuel García y hermana de la asimismo célebre Pauline Viardot. Otras famosas divas de la época fueron Giuditta Pasta, Jenny Lind y Giulia Grisi.
El siglo XX se inicia de la mano del que muchos consideran el mayor tenor y del cual se cuenta con registros fonográficos, el italiano Enrico Caruso, quien triunfó en los principales teatros de ópera en Europa, así como en Nueva York y Buenos Aires y fue el primer artista en vender 1 millón de copias de una grabación en disco. En los años previos a la Segunda Guerra Mundial, la ópera contó con un período de apogeo representado por cantantes como Kirsten Flagstad, Miguel Fleta, Feodor Chaliapin, Frida Leider, Lotte Lehmann, Giuseppe Martinelli, María Jeritza, Melico Salazar y Gonzalo Castellón.
En la inmediata posguerra fue figura fundamental la soprano Maria Callas, cuya fama trascendió el ámbito musical al mundo de la política y la farándula así como su rivalidad con la italiana Renata Tebaldi evocó las confrontaciones de divas del siglo XVIII. Más voces femeninas importantes de la segunda mitad del siglo XX son la cantante sueca Birgit Nilsson, las estadounidenses Beverly Sills, Renée Fleming, Jessye Norman, Leontyne Price y Kathleen Battle, las catalanas Montserrat Caballé y Victoria de los Ángeles, las italianas Mirella Freni, Cecilia Bartoli y Renata Scotto, las búlgaras Ljuba Welitsch y Ghena Dimitrova, Angela Gheorghiu, Diana Damrau, las francesas Régine Crespin y Natalie Dessay y Anna Netrebko. Algunas cantantes han sido distinguidas por sus servicios por la corona británica con el título de Dame DBE, entre otras Dame Clara Butt, Eva Turner, Joan Sutherland, Kiri Te Kanawa (quién cantó en la boda del príncipe Carlos con la princesa Diana y se calcula que dicha presentación fue vista por 600 millones de espectadores por televisión), Gwyneth Jones, Janet Baker, Elisabeth Schwarzkopf, Margaret Price y Sarah Connolly. En el área germánica se otorga el título Kammersänger (o Kammersängerin para las mujeres cantantes) traducido como cantante de la corte.
En el campo masculino hay una extensa lista también, mencionando a un barítono contemporáneo de María Callas, Tito Gobbi quién compartió en varias óperas con Callas incluso su última presentación en vivo en 1964 interpretando Tosca. En los años 30 incluiremos al mexicano Juan Arvizu.[10]​[11]​ En los años 1940 incluiremos al chileno Ramón Vinay, el danés Lauritz Melchior y el mexicano Néstor Mesta Cháyres.[12]​ En los años 1960 incluiremos al español Alfredo Kraus, el alemán Fritz Wunderlich y el italiano Giuseppe Di Stefano. Los años 1970 fueron el despegue y la gran marquesina de algunos de los cantantes más famosos reconocidos tanto por el gran público aficionado y soportado por el público conocedor y especialistas. Estos son el mexicano Francisco Araiza, reconocido mundialmente por su participación en las óperas de Mozart y Rossini, José Carreras, Luciano Pavarotti y Plácido Domingo. A principios de la década de los 90 se unen Pavarotti, Carreras y Domingo para formar el primer evento masivo-operístico conocido como Los Tres Tenores en 1990. En el ambiente musical de México varios cantantes fueron reconocidos en EE. UU. y Europa al grado que uno de ellos, el tenor Ramón Vargas participó en los conciertos conmemorativos del centenario luctuoso de Giuseppe Verdi en 2001. Están también Fernando de la Mora y David Lomelí.
La tradición continúa en el siglo XXI en tenores como el alemán Jonas Kaufmann, el peruano Juan Diego Flórez, los mexicanos Rolando Villazón y Javier Camarena,  los costarricenses Gonzalo Castellón e Íride Martínez, el estadounidense Michael Fabiano y el polaco Piotr Beczala. Vale destacar la irrupción de los contratenores gracias al auge de la música temprana y barroca en las figuras de Andreas Scholl, David Daniels, Philippe Jaroussky y el argentino Franco Fagioli entre otros.
Aunque el patrocinio de la ópera ha disminuido en el siglo pasado a favor de otras artes y medios, tales como musical, cine, radio, televisión y grabaciones, los medios de comunicación también han apoyado la popularidad de cantantes de ópera famosos gracias a las transmisiones por televisión y en cines desde las grandes casas de ópera así como en estadios multitudinarios.

El karate o kárate[1]​ (del japonés 空手, karate; literalmente, 'mano vacía') es un arte marcial tradicional basada en algunos estilos de las artes marciales chinas (wushu), y en menor medida en otras disciplinas provenientes del sureste asiático proveniente de Okinawa (Japón). El nombre japonés se compone de las palabras 空 (kara, 'vacío') y 手 (te, 'mano'). A la persona que lo practica se la llama karateca.[2]​
El kárate tiene su origen durante el siglo XVI en las técnicas marciales nativas de las islas Ryukyu, (hoy día Okinawa) caracterizadas por el uso de los puños llamadas (Te / to-de / tuidi), además de técnicas provenientes de la lucha nativa o (tegumi); siendo influenciado por algunos estilos de las artes marciales chinas (kung-fu) y en menor medida por otras disciplinas provenientes de otros países del sureste asiático como Tailandia, Filipinas e Indonesia. Ya, en el siglo XX este estilo marcial fue influenciado en un principio por varios conceptos técnicos, tácticos y filosóficos procedentes de algunas de las artes marciales japonesas modernas, como: el kendo, el judo, y finalmente el aikido. En un principio, El "Te" siendo el arte antecesor al karate moderno surgió de la necesidad de los guerreros nobles de la isla (los pechin) de proteger al último rey de Okinawa, Sho Tai, y a sí mismos de los varios abusos perpetrados por los guerreros con armadura (los samuráis), quienes hacían parte de los invasores japoneses pertenecientes al clan Satsuma[3]​, en el siglo XVII. Poco a poco, el "Te" fue desarrollado en el reino de Ryukyu, y posteriormente se expandió: se enseñó sistemáticamente en Japón después de la era Taisho en el siglo XX, donde fue renombrado como karate-Do, como consecuencia de los intercambios culturales entre los japoneses y los habitantes de las islas Ryukyu. Incorporándose así a la cultura de las artes marciales tradicionales del Japón o Budo.
El karate-Do de hoy en día se caracteriza fundamentalmente por el empleo de golpes de puño, bloqueos, patadas y golpes de mano abierta, donde las diferentes técnicas  reciben varios nombres, según la zona del cuerpo a defender o atacar. Sin embargo el karate, no restringe su repertorio solo a estos, ya que además incluye: varios barridos, algunos lanzamientos y derribos, unas pocas luxaciones articulares; además de golpes a puntos vulnerables, y a puntos nerviosos, en su currículo. En los golpes del karate-Do se unifican la fuerza, la rapidez, la respiración, el equilibrio, la tensión y la relajación al aplicar un correcto giro de cadera y una conexión o sinergia muy precisa de músculos y articulaciones, trasladando una gran parte del peso corporal y del centro de gravedad al impacto. Generalmente, y a diferencia de otras disciplinas, se busca derrotar al adversario mediante un impacto contundente (o unos pocos), preciso y definitivo, buscando ser lo más eficaz posible. A ese concepto se le llama "Ikken hissatsu" o "un golpe, una muerte", de forma semejante a la estocada o al corte de una katana o sable japonés. El karate-do parte de la idea de forjar el cuerpo como un arma, de tal forma que se pueda llegar a defenderse y sin sufrir mayor daño, de ahí que en las escuelas tradicionales se haga tanto ahínco en el endurecimiento físico, con combates a contacto pleno y sin ninguna protección, de esa forma se logra endurecer y potenciar la mayoría de partes del cuerpo a la vez que otorga al practicante un control de sus golpes y un conocimiento de sus habilidades y límites.
Las sucesivas prohibiciones al porte de armas en la historia de la isla de Okinawa y la importancia dada a las artes marciales sin armas se debe a que la isla, mucho antes de ser anexada al Shogunato de Japón, ya era un puerto libre y reino independiente donde atracaban numerosas embarcaciones provenientes de varias partes de Asia (China, Corea, Tailandia, Indonesia, Filipinas). La isla de Okinawa fue asimismo el primer lugar donde llegó la nave del comodoro Perry de los EE. UU. en el siglo XIX antes de llegar a la ciudad puerto Yokohama, en el Japón, para obligar a los japoneses a abrir sus rutas comerciales; pues desde 1639 hasta 1853 tanto japoneses como okinawenses habían vivido aislados del mundo exterior por decreto del shōgun (líder militar) Tokugawa Iemitsu, hasta la época moderna (siglo XX), en que el último de los Tokugawa, Tokugawa Yoshinobu, cedió el poder total y definitivamente al emperador Meiji entre 1868 y 1902.
En la isla de Okinawa se vivía una situación naval y comercial de gran intercambio entre varios reinos, similar a la de las islas Filipinas, aunque con varias prohibiciones al porte de armas que se iniciaron en 1409 por el entonces rey Sho Shin, que favorecieron la unificación de los pequeños feudos en que se encontraba dividida la isla, evitando así futuras divisiones y conflictos entre los visitantes y los nativos. Estas medidas fueron luego enfatizadas de nuevo ya en 1609 por los guerreros samurái japoneses invasores pertenecientes al clan Satsuma, quienes confiscaron las armas restantes. Durante este periodo la vida fue aún más austera y restrictiva, obligando tanto a los nobles (Pechin) como al pueblo a desarrollar aún más los métodos de combate tanto con implementos agrícolas (kobudō), como a mano vacía (karate) respectivamente.
Durante los siglos XIX y XX se encontraban establecidos ciertos estilos de acuerdo a la estricta división regional por clases sociales, y según el énfasis, entre los movimientos circulares o lineales, así como la preferencia por el combate a distancia media y larga. De esta forma, las principales variantes del Te, o mano practicadas en Okinawa eran: Shuri-Te, Naha-Te, y Tomari-Te. Cada una de ellas contaba con características particulares tanto en las técnicas como en los métodos de práctica. En este período tres maestros se encargaron de sistematizar y revivir la práctica del karate: Anko Itosu (Shuri-Te), Kanryo Higaonna (Naha-Te),  y Kosaku Matsumura (Tomari-Te). Con el fin de integrarse al sistema educativo escolar militar japonés. En 1872 el emperador Meiji otorgó la isla de Okinawa al clan samurái Satsuma, y nombra a sus miembros como sus únicos representantes en el territorio. En 1879 el gobierno Meiji dicta: la abolición de la familia real 'Sho' de las islas Ryukyu, el exilio del rey Sho Tai, y crea la Prefectura de Okinawa.
Los términos empleados en esa época para denominar de manera general a estos estilos nativos fueron Te o Ti (手, 'Te o Ti'?   literalmente, "la mano"), Okinawa-Te (沖縄手, 'Okinawa-Te'?   literalmente, "la mano de Okinawa") y . Términos que a principios del siglo XX, fueron asimilados dentro del término genérico de Kárate.
De 1901 a 1905 las escuelas de la prefectura de Okinawa comienzan a adoptar el tuidi como parte del programa de educación física.[4]​ En esta época, el maestro Anko Itosu (糸洲安恒, Itosu Ankō?) cambió la pronunciación de 唐手 desde tode o tuidi a karate. Entre 1904 y 1905, Chomo Hanashiro (estilo Shorin Ryu) y posteriormente otros maestros empiezan a emplear por primera vez los kanji 空手 en lugar de los ideogramas 唐手. En 1933, en el capítulo dedidado a Okinawa reconocido por la asociación nacional de las artes marciales del Japón o Dai Nihon Butokukai se reconoce al kárate (空手, karate?) como arte marcial japonés.[5]​
Al karate se le conoce hoy en día como "el camino de la mano vacía". Siendo esta la interpretación de los ideogramas para el término Karate-Do, popularizada en occidente por el maestro Masatoshi Nakayama, representante de la Asociación Japonesa de Karate (o JKA), que promovió el estilo Shotokan o  (JKA) después de la segunda guerra mundial (1939-1945), cuando se buscaba mostrar al Japón como un país pacífico ante la ocupación de los Estados Unidos, quienes prohibieron la práctica de las artes marciales por considerarlas un remanente del espíritu imperial del Japón. Esta traducción fue aceptada como alusión a la no inclusión de armas, o de ánimo bélico en el karate moderno. Sin embargo, hay que notar que todos los máximos exponentes y maestros del kárate; hasta muy recientemente, tenían conocimientos del arte del kobudo, o arte marcial del manejo de las armas tradicionales de Okinawa, como el bastón largo o bo, las macanas o tonfa, los tridentes o dagas sai, los molinos de arroz/ bridas del caballo, o nunchaku, las hoces de segar o kama, los nudillos de hierro o tekko, etc. Incluidas y preservadas hoy en día como un arte marcial por separado, aunque preservado dentro de algunos estilos de karate. Así mismo, han sido varios los maestros de kárate que practicaron de manera paralela el arte del sable japonés moderno o kendo, y/o el arte moderno de la lucha o Judo. También, otra traducción de la palabra Karate es "la mano que emerge/contiene al vacío, al todo" o "la mano del absoluto", "el camino de la voluntad". Podría hablarse inclusive del "camino del absoluto" debido a la profundidad filosófica, física y técnico-táctica del arte no solo en lo físico, sino en su posible aplicación mental y a la vida diaria, llegando a definir la vida de algunos practicantes. Otra posible traducción es "el camino de la mano y de la vida" pues el vacío o "kara" filosóficamente lo contiene todo; como esencia sin ataduras, sin juicios, sin límites, sin forma.
Si bien se reconocen como los precursores del karate clásico a los maestros Kanga Sakukawa ( también conocido como To-de Sakukawa) y Sokon Matsumura, así como a sus discípulos: Chutoku Kyan, Asato Ankō, Anko Itosu, entre muchos otros. Fue uno de sus alumnos de tercera generación, el maestro Gichin Funakoshi, el fundador del karate estilo Shotokan, a quien se le conoce como el "padre del karate moderno", siendo el mayor responsable de haber introducido y popularizado el karate en las islas principales de Japón. Sin embargo hay que notar, que durante el inicio del siglo XX varios otros maestros de Okinawa estuvieron dedicados a la enseñanza,  por lo que fueron también responsables del desarrollo del karate en las islas principales del Japón. Gichin Funakoshi fue estudiante de Asato Ankō y Anko Itosu (quienes habían trabajado para introducir el karate en el sistema escolar de la prefectura de Okinawa desde 1902). Durante esta época, los maestros destacados que también influyeron en la difusión del Karate en Japón incluyen a Kenwa Mabuni (estilo Shito Ryu), Chojun Miyagi (estilo Goju Ryu), Motobu Chōki (estilo Motobu Ryu), Kanken Toyama (estilo Shudokan, quien incluso instruyó a los coreanos de los que surgiría el taekwondo ) y Kanbun Uechi (estilo Uechi Ryu). Este fue un período turbulento en la historia de la región, que incluyó eventos como la anexión del archipiélago de Okinawa al Japón en 1872, la Primera Guerra Sino-Japonesa (1894-1895), la guerra ruso-japonesa (1904-1905), la invasión y anexión de Corea (1910-1945), y el ascenso del militarismo japonés (1905-1945), además de la segunda guerra mundial (1939-1945).
La primera demostración pública del Karate en el Japón fue en 1917 en el Butoku-den de Kioto, por Gichin Funakoshi. Esta y posteriores demostraciones dejaron bastante impresionados a muchos japoneses, entre ellos al príncipe heredero Hirohito, que quedó entusiasmado con el arte marcial de Okinawa. En 1922, el Dr. Jigoro Kano, fundador del arte japonés del Judo, invitó al maestro Funakoshi al Dojo Kodokan para hacer una demostración y permanecer en Japón para enseñar karate. Este patrocinio fue clave para el establecimiento y posterior desarrollo del karate en Japón. Sin el respaldo del maestro Kano, el arte marcial okinawense, considerado hasta entonces como un "arte campesino, provincial, atrasado", habría sido despreciado aún más por los japoneses. Por otro lado, en el año 1929 el Maestro Kenwa Mabuni se instaló en la ciudad de Osaka para enseñar su estilo de karate, el karate Shito Ryu.
En 1949 se fundó la Asociación Japonesa de Karate (o JKA según sus siglas en inglés, o Japan Karate Association). Liderada por un alumno del Funakoshi. El maestro Masatoshi Nakayama. La JKA realizó los primeros campeonatos de kárate del Japón en 1957. La asociación pretendió inicialmente agrupar a los diferentes estilos del arte, a medida que popularizaba su práctica en occidente; pero finalmente se convirtió en la representante a nivel mundial del karate estilo Shotokan variante JKA o Kyokai, como es conocido en Japón.
El uniforme de práctica (gi) empleado en el karate es el keikogi o karategi, (gi = traje) compuesto por una chaqueta, pantalones y un cinturón. El karategi se deriva del judogi, dada la influencia de Jigorō Kanō fundador del arte marcial y deporte olímpico del Judo; a principios del siglo XX en las artes marciales japonesas modernas o Gendai Budō.
Actualmente existen dos tipos de karate-gi para competición: el de kumite o combate, que es más ligero, y el de katas o formas, que es más grueso y pesado.
La existencia de cinturones varía de unos estilos a otros, pero por lo general suelen ser reconocidos: los llamados grados Kyu o cinturones de nivel inferior, e intermedio y los Dan o cinturones negros superiores. 
Tomado el  modelo del Judo, se establecieron los grados kyus o cinturones de nivel inferior. Los kyus comienzan con el blanco para los principiantes. Con el aprendizaje progresivo de las técnicas se va subiendo de nivel y va cambiando el color del cinturón. Al blanco (Rukkyu) le siguen, el celeste (Raitoburū), el amarillo (Yonkyu), el naranja (Sankyu), el verde (Nikyu), el azul, el marrón (Ikkyu), el negro (Kuroi) (Maestro), el rojo-blanco (Sensei) y por último el rojo (Sensei), aunque con puntos intermedios entre una mezcla del anterior con el posterior (celeste-amarillo, amarillo-naranja, naranja-verde, verde-azul, azul-marrón y marrón-negro). No obstante, los cinturones de colores también pueden variar según las escuelas, ya que en algunas escuelas alteran el orden de los colores, o quitan alguno de los colores antes nombrados.
Estos cinturones intermedios se dan a los alumnos que tienen poca edad (aproximadamente hasta los 13-14 años) porque aprenden más lentamente y el hecho de poder examinarse de estos cinturones evita su aburrimiento al tener que esperar para pasar de un cinturón a otro. La Federación Mundial de Karate (WKF) establece como requisito el tener una edad mínima de dieciséis años para estar en posesión de primer dan. Los cinturones intermedios son los siguientes: celeste-amarillo, amarillo-naranja, naranja-verde, verde-azul y azul-marrón. Después de este último se pasa al marrón y después directamente al negro. 
Una vez se es cinturón negro, se sigue aumentando progresivamente en grados, llamados danes. La numeración es ascendente, de primer a décimo dan: 
1.er dan (Shodan), 2.º dan (Nidan), 3.er dan (Sandan), 4.º dan (Yondan) y 5.º dan (Godan). De ahí en adelante, en la normativa actual de algunos estilos el color del cinturón cambia, pasando a ser: 6.º dan (Rokudan), 7.º dan (Sichidan) y 8.º dan (Hachidan) de color rojo-blanco, mientras que el 9.º dan (Kudan) y el 10.º dan (Judan) pueden ser de color rojo, que es el último nivel existente. Jigoro Kano fue quien quiso establecer las rayas alternativas de color blanco y rojo a partir del 6.º y 7.º Dan y el cinturón rojo para el 9.º y 10.º Dan ya que un cinturón negro se desgasta después de muchos años y vuelve a convertirse en blanco simbolizando el fin filosófico del color. Por ello, se añaden las franjas rojas en el color blanco cuando se llega a tan alto nivel.
Al margen de la Federación, las distintas escuelas y estilos suelen seguir sus propios sistemas sin atender en muchos casos lo establecido por este organismo. Esta numeración varía según la escuela, siendo lo habitual en la actualidad siete grados, pero manteniéndose en algunas escuelas tradicionales un sistema de cinco danés. Tradicionalmente era solamente hasta el 5.º dan; esto por varias razones: una es la que se asocia a la progresión con los cinco círculos del legendario guerrero samurái Miyamoto Musashi; algunas escuelas aun mantienen hasta el 5.º dan, generalmente las más tradicionales o que tienen una relación directa con el maestro fundador del karate Shotokan Gichin Funakoshi, tales como por ejemplo: Shotokai y Shotokan of America (o SKA), ya que según la escala del maestro Funakoshi el grado más alto era el 5.º dan; de hecho, en la época en la que Funakoshi aprendió el arte del tuidi, to-de o te, aun no existían los grados dan, sin embargo muchas escuelas alejándose de la parte tradicional han adquirido hasta el décimo dan.
Sensei (先生?) es el término japonés con el que se designa a un maestro, a un sabio o a una persona docta. Fuera del Japón se emplea sobre todo en el mundo de las artes marciales tradicionales o gendai budō (entre estas, el Aikido, el karate-Do, el Judo, el Kendo, etc.). Literalmente, el término sensei significa 'el que ha nacido antes', a partir de los caracteres kanji sen (先 antes?) sei (生 nacer, vida?). O bien, desde la filosofía, como 'el que ha recorrido el camino', un guía.
Artículo Principal: ShihanShihan (師範?) es un término del idioma japonés, generalmente usado en las artes marciales procedentes de Japón como un título honorífico para referirse a los maestros de maestros en un estilo específico.
Las diferentes organizaciones de artes marciales tienen diversos requerimientos para el uso del término en carácter de título honorífico, generalmente involucra ciertos derechos como el de usar símbolos especiales que identifican el rango y otorgar grados en el arte en el cual se ostenta.
Por ejemplo en la organización del arte del ninjutsu, la Bujinkan se genera el derecho cuando los otros Shihan se refieren a la persona con ese título (debe tener como mínimo 10 dan ya que esta organización son 15 el máximo), en Judo, y en aikidō se llega al título de Shihan al alcanzar al menos el grado de sexto dan.
En Karate se llega a este título en cuarto dan con el nombramiento de Shihan-Dai, posteriormente pasado el séptimo dan se le nombra Shihan únicamente, en noveno y décimo  se nombra Soke .
Los cinturones de estos shihanes varían según la organización a la que pertenezcan siendo muy habitual la utilización de una franja roja en medio del cinturón el cual va creciendo al mismo tiempo que el Shihan avanza de grado hasta que en el décimo dan o soke la franja cubre toda la cara frontal del cinturón, pero en los sistemas los cuales llegan hasta séptimo dan  este es de color rojo con blanco.
Antes de comenzar la práctica y al terminarla, y también antes de comenzar un ejercicio específico, se realizan sencillos Rei (saludos) como ritual, con el fin de que los practicantes interioricen los valores de cortesía y respeto por los demás. Estos saludos consisten en inclinaciones del tronco sentados o de pie, hechos en grupo o por parejas según el momento, y al entrar y salir de la clase o el tatami (estera). A veces se acompaña el saludo con la expresión "¡Oss!". Algunos de esos saludos son:
Además en las enseñanzas de karate hay que respetar ciertas normas como esperar a la orden del instructor para dar por finalizado un ejercicio o realizar ciertas acciones, no conversar en la clase, prestar atención siempre que el instructor explique algo, tener siempre una actitud constructiva cuando se hable, etc.
En el kárate hay un gran uso del ki o intención emocional, además de una alineación corporal precisa. Los katas o formas/coreografías y las formas de defensa son esquemas rítmicos, pero rígidos. Las técnicas utilizan diferentes partes del cuerpo para golpear, como las manos (canto, palma, dedos, nudillos), los pies (talón, borde externo, borde interno, planta, base o punta de los dedos), los codos, los antebrazos, las rodillas o la cabeza, o el hueso tibial, en las técnicas de patadas bajas a los muslos o "low kicks"/ gedan geri.
Actualmente el karate, Deriva su metodología de enseñanza del método "Kaisen" o búsqueda del mejoramiento continuo por medio de la repetición, la observación, el análisis, la retroalimentación, la ejecución y la repetición consciente (incluyendo sus aspectos físico, técnico, táctico y de condicionamiento psicológico ritual). Este método de enseñanza es común a las artes marciales tradicionales contemporáneas del Japón (Gendai Budō). El kárate asimismo toma su sistema de grados por cinturones (kyu-dan), varios de sus barridos a los pies y uniforme del Judo.
En las primeras etapas, es decir en los grados kyu o de cinturones de colores, hasta el grado de cinturón negro 1 Dan; se busca que el practicante del arte desarrolle: la correcta alineación corporal, el ajuste (o relación tensión - relajación ), los bloqueos/chequeos, las esquivas, los golpes a puntos vulnerables, los desplazamientos, los barridos, los contraataques, y el acondicionamiento físico específico caracterizado por el "endurecimiento" y/o desensibilización de varios segmentos corporales. Posteriormente, ya en los grados superiores se da un mayor énfasis a: los lanzamientos y derribos, los golpes a puntos vitales, algunas luxaciones articulares, incluyendo unas pocas estrangulaciones, y varias técnicas de combate desde el suelo; como barridos, atrapes y luxaciones. Y finalmente tras muchos años de práctica se llega al tratamiento de lesiones, los métodos de reanimación y el estudio de los circuitos metabólicos y nerviosos de estimulación o depresión energética por "puntos vitales", o "kyusho" como ocurre en las  disciplinas predecesoras del arte, siendo estas las artes marciales chinas.
Las técnicas, tácticas, estrategia y métodos propios de preparación física del kárate están descritos de la siguiente manera:
Nótese que en su parte metodológica, la enseñanza y el entrenamiento del karate, se divide por lo general según los siguientes objetivos de enseñanza:
- En Karate-Do moderno y karate deportivo: Kihon, Kata, ippon kumite, y Shiai kumite. 
- En karate tradicional y de autodefensa: Kihon, Hojo Undo, kata, Bunkai, kakie, ippon kumite, ju kumite.
Series de técnicas básicas ejecutadas por separado o en combinación con otras, en varias direcciones, ejecutadas al aire de forma fluida (para estudiar los detalles) o con fuerza (para practicar la velocidad, intensidad, etc) o contra implementos, como el makiwara, el saco, los guantes de foco, etc. (para practicar mejorando el impacto). Se busca mejorar la alineación corporal, el tomar conciencia del alcance de las diferentes técnicas, desarrollar coordinación lineal y cruzada, tomar conciencia de la sinergia muscular (conexión) necesaria de los grupos musculares específicos a ser usados en cada técnica, desarrollar los reflejos y la velocidad de reacción, desarrollar la flexibilidad gestual, reforzar el condicionamiento neural motriz, trabajar de diferentes maneras la respiración, desarrollar la intención emocional, además de potenciar la autoconfianza.
Después del Kihon se pasa a aprender los Kata, que son secuencias preestablecidas de esas técnicas, elaboradas por maestros para practicar un subestilo de autodefensa dentro del karate o aspectos concretos del mismo.
Se considera parte del Kihon el combate preestablecido con técnicas tradicionales de ataque y defensa en secuencia, o combinaciones de estas, realizadas por parejas a 5, 3, y 1 paso, o kihon kumite, esta práctica es similar al bunkai pero mucho más simplificada.
Kata significa "forma". A nivel básico, se toma como una sucesión de técnicas de defensa y ataque enlazadas y coordinadas contra uno o varios enemigos imaginarios. Siempre siguiendo el embusen: la línea imaginaria para realizar la kata.
Todo el volumen de técnicas, tácticas y algunos apartados de acondicionamiento físico para la práctica de esta arte marcial se encuentran resumidos en los Katas. El Kata es la base, el fundamento del entrenamiento clásico del Karate como arte marcial y método de defensa personal civil. Deben ser decodificados, interpretados, practicados y aplicados mediante la práctica del bunkai.
Casi la totalidad de los Katas son de origen chino, modificados por los maestros de Okinawa y readaptados por los japoneses. Cada estilo trabaja y estudia ciertos katas, variando los tipos y número de katas en cada estilo y habiendo diferencias (en ocasiones notables) de un estilo a otro en un mismo kata (ritmo, trayectoria, uso de las distancias, aplicación de la potencia, énfasis en técnicas a mano abierta o cerrada, de corto o largo alcance, etc). 
Respecto de los orígenes históricos de las katas cabe destacar la investigación llevada a cabo por el antropólogo español Pablo Pereda sobre el antiguo To-De Okinawense, que las enlaza con algunas técnicas espirituales usadas en el taoísmo. Su trabajo está publicado por la Universidad de León, España.[6]​
La inclusión de la modalidad de katas para competición ha sido sujeto de controversias durante décadas. Para los tradicionalistas desvirtúa el karate tradicional, al dejar a un lado varias acciones motrices puntuales que son modificadas para la estética de la competición, y al perder parte de la adaptación motriz necesaria para la defensa personal. Para los seguidores del karate deportivo fomenta el trabajo uniforme de la técnica básica y el entrenamiento memorístico de la misma. En el kata de competición se prefieren movimientos casi gimnásticos, ejecutando gestos más vistosos, (como las patadas altas), o amplios y/o cortos pero muy rápidos, distintos de la aplicación a la defensa personal original, a media y corta distancia. Los rangos de movimiento tienden a ampliarse, así como varios de los movimientos se han hecho más explosivos con el fin de hacerlos más vistosos. Con esos cambios, es de notarse que gran parte de este tipo de movimientos deportivos no son prácticos y eficaces en una situación de defensa personal real.
Durante la competición, en la modalidad de katas o formas tipo WKF (World Karate Federation) se enfrentan dos contrincantes. Cada uno llevará un cinturón de color rojo y azul. El color del cinturón se sortea antes de la competición, siendo totalmente independiente del grado de los participantes. El participante Aka (rojo) será el que ejecute su kata primero, y Ao (azul) ejecutará su kata en segundo lugar. El jurado valorará y comparará ambas ejecuciones y el ganador se decidirá mediante la puntuación establecida por estos, como dice la nueva normativa de la WKF.[7]​ Si son tres árbitros, el que reciba dos banderas a favor será el vencedor; si son cinco, deberá conseguir un mínimo de tres banderas a favor. Si uno de los dos participantes se equivoca durante su ejecución será directamente eliminado, ganando así el oponente.
En el caso de competición por equipos, serán tres personas por equipo, se observan, entre otros: la sincronía de los participantes, la explosividad y la secuencia técnica. Solo el karateka en el centro del grupo indicará el nombre del kata y dará la orden de comenzar,
Kumite significa "entrelazar/ cruzar / unir las manos" o "combate". Es la aplicación práctica de las técnicas a un enfrentamiento contra un oponente real.
En el karate actual, existen varios tipos de combate , o kumite, según sea la finalidad de la práctica, sea tradicional o deportiva, así: 
El desarrollo de los diferentes tipos de combate o kumite , se inició, por el kihon kumite, o combate de aprendizaje por medio de movimientos formales preestablecidos, este modelo fue desarrollado en Japón con base en el arte del sable o kendo. Este tipo de  combate consiste en la aplicación por parejas de técnicas en ataque, defensa y contraataque recogidas en el kihon y en los katas, realizándolo en varios pasos hasta llegar a un solo paso. Pudiéndolo ejecutar a varios niveles (alto) jodan, (medio) chudan, (bajo) gedan, (ushiro) desde atrás, (yoko) desde uno o ambos lados; alternando niveles, velocidades, uso de los pies y manos de forma alterna, por separado o de forma simultánea, incluyendo finalmente dentro del estudio del karate como Budo (arte marcial) a modo de semi-contacto, técnicas poco comunes como: agarres, lanzamientos, algunas luxaciones y unas pocas estrangulaciones, permitiendo el estudio y madurez técnica y emocional. A su vez, desde la metodología el kihon kumite, o  combate formativo Se divide en: 
Con el fin de hacerlo más dinámico, en esta forma de combate le fueron dadas normas de puntuación, las cuales se reformaron en el 2011, posteriormente las técnicas se les dieron puntos así:
La siguiente etapa histórica fue el combate deportivo, o Shiai kumite (o combate deportivo libre reglado entre dos oponentes), donde se incluyen las pocas técnicas de golpeo válidas en el combate deportivo, y finalmente desarrollar con los grados medios (o desde el 5 kyu) y los grados altos es decir ( desde el 2 kyu y después del cinturón negro primer (1) Dan), quienes ya deben tener la suficiente madurez emocional, precisión, rapidez y control, el Ju-Shiai kumite (o combate libre deportivo entre dos oponentes), para más adelante en el transcurso de la evolución marcial del practicante, realizar el kata bunkai kumite, donde se busca explorar, realizar y aplicar un repertorio seleccionado y variado de las técnicas y tácticas contenidas en las formas del karate, como Budo, es decir orientadas a la defensa personal. 
Actualmente, permanecen dos modalidades de Kumite deportivo o Shiai kumite , o combate de competición: el Jyu Kumite, tiene una mayor difusión realizándose al punto o con contacto ligero, conocido como kumite tipo JKA, o de la Japanese karate Association/ Asociación Japonesa de Karate, y el kumite tipo WKF, en alusión a la Federación Mundial de Karate o 'World Karate Federation'. Estos reciben el nombre de Shiai - Kumite. Se trata de combates entre dos deportistas con reglas, en el que cada contrincante debe anotar el mayor número de puntos, en un tiempo límite, intentando marcar algunas técnicas no letales, sobre el rival en zonas y con superficies de contacto permitidas. Con los requisitos de: buena forma, actitud vigorosa, deportividad, distancia correcta y tiempo adecuado. Generalmente los deportistas van protegidos por una serie de protecciones reglamentarias; como guantines, protectores bucal e inguinal, espinilleras y zapatones de espuma. El casco se usa para algunas categorías y modalidades.
Así, dos oponentes con cinturones de diferentes colores (rojo o azul) se sitúan en los extremos del tatami (o superficie de competencia), y cuando se les da la orden, entran y saludan a los jueces y al rival esperando a que se dé la señal de comenzar el combate. Para ganar, los competidores deberán marcar el máximo de puntos posibles (golpes de puño, de pie, lanzamientos, y barridos reglamentarios) en un tiempo límite. Al finalizar el tiempo, y una vez declarada la victoria de uno de los competidores, estos saludarán al árbitro y luego se saludarán entre ellos mismos, dándose también la mano y despidiéndose al salir de la estera o Tatami.
El término bunkai se refiere a la aplicación en combate o autodefensa de las técnicas incluidas en las formas o kata. En las escuelas clásicas del karate de Okinawa (como por ejemplo: Shorin Ryu, Goju Ryu, y Uechi Ryu) el bunkai, o porqué de la técnica (gesto) y la táctica (desplazamiento) son de vital importancia y ocupan una gran parte del tiempo de entrenamiento, además del trabajo de acondicionamiento físico. Sin estos aspectos, la comprensión de los kata quedaría incompleta. Sin embargo, para las escuelas tradicionales y modernas de karate-do provenientes del Japón, (como por ejemplo: Shotokan, Shito ryu, Wado Ryu, etc.) quienes poseen un mayor énfasis deportivo; el bunkai es un complemento de las formas o kata transmitido de forma repetitiva, sin profundizar, siendo menos importante y detallado, orientado hacia el cómo, en búsqueda de una ejecución técnica perfecta. Más en las escuelas provenientes de Okinawa se busca el desarrollo de los diferentes medios físicos de autodefensa para una situación de peligro real, es decir sin reglas. Actualmente el estudio del bunkai como medio de legítima defensa está recibiendo cada vez mayor atención en el mundo del karate occidental. Algunos de los pioneros más conocidos en esta faceta de la disciplina son: Patrick McCarthy, Harry Cook, Iain Abernethy, Ryan Parker, entre otros. Ya que en las últimas décadas la faceta de la defensa personal ha sido abandonada en muchas escuelas de karate-Do actuales, favoreciendo la formación del individuo y la orientación deportiva limitándose a la metodología del perfeccionamiento estético de los kata, el kihon, y al desarrollo del combate deportivo o kumite, sea a los puntos; o bien con contacto a similitud del kickboxing, como ocurre en el estilo Kyokushinkai.
El Bunkai como aplicación de las técnicas de básicas o "kihon" y de los movimientos pertenecientes a las formas o "kata" sirve para ajustar y adoptar el uso técnicas y tácticas reales (incluyendo por ejemplo: las patadas bajas, la alternancia entre posiciones altas y bajas, el uso de la mano en "hikite" en todos los golpes, haciendo un mayor énfasis en las combinaciones de golpes, luxaciones, estrangulaciones, golpes de mano abierta, y lanzamientos).  Haciendo frente a un oponente palpable y no imaginario, haciéndose patente el estudio del propio rango o distancia, la potencia a aplicar, el manejo de la media distancia, el desarrollo de los reflejos, la precisión en los golpes, los desplazamientos, el uso de las posiciones en movimiento, etc. Elementos que en la práctica individual no pueden ser desarrollados con eficacia. Existe aun el estereotipo pedagógico según el cual, el kata es un combate imaginario contra varios atacantes, mientras que en la visión tradicional, generalmente, tal como decía el maestro Kenwa Mabuni, fundador del estilo Shito Ryu o tal como se practica en Okinawa; el kata se considera como una serie de movimientos seguidos hechos contra un mismo atacante que no se rinde o que no es fácil de reducir, que da pelea, que se resiste de forma hábil, que está vivo. Dado el carácter amplio del estudio de las katas y su bunkai, la posibilidad de varios atacantes también puede ser tenida en cuenta, pero originalmente no era el sentido principal de los kata. El Bunkai no deportivo se estudia (no son modelos coreograficos, sino un rompecabezas cinético) principalmente por parejas: por turnos uno hace de atacante y el otro aplica las técnicas del kata como defensa, primero poco a poco y progresivamente más rápido y fuerte; y con variantes progresivas en los contraataques y ángulos de ejecución. El estudio del bunkai no se limita a una o dos aplicaciones sino que se investigan las distintas posibilidades de cada movimiento, posición, y desplazamiento con el fin de aprender los principios que hacen del karate un arte de autodefensa eficaz y versátil. Algunos kata ofrecen movimientos simplificados pero su aplicación es diferente a lo que se aparenta. Otros ofrecen múltiples aplicaciones interpretando los movimientos a partir de los movimientos de los gestos básicos o Kihon. Gran parte de estas aplicaciones queda a la interpretación, comparación, exploración e investigación del karateka experimentado, con base en: la anatomía, la biomecánica, la fisiología y la psicología. Contemplando las diferentes técnicas de golpeo, luxación, lanzamiento, derribo, sujeción, uso de los puntos vulnerables, y el conocimiento y representación de las fases de escalación del conflicto. Así el practicante puede darse cuenta de que la defensa personal se trata de proteger a sus seres queridos, y sobre todo de evitar el conflicto o de sobrevivir a este, por encima del propio ego.
El Hojo Undo se refiere a una serie de instrumentos y prácticas corporales (ej: kote- kitae) destinadas a preparar al cuerpo para el combate. Los instrumentos son generalmente objetos pesados con los que se realizan diversos ejercicios. Su uso se distingue del uso de pesas en otras disciplinas (por ejemplo el fitness) en que están diseñados para desarrollar la fuerza y el control de los movimientos de karate de una forma específica. También incluye prácticas y objetos de acondicionamiento de las partes del cuerpo destinadas a ser utilizadas como defensa o ataque (nudillos, antebrazos, pies, tibias, dedos, codos, etc.), mediante golpeo repetitivo para desarrollar dureza y/o insensibilidad por impacto tanto en huesos y tendones como en la piel, al mismo tiempo que se mejora la estructura corporal mediante una práctica progresiva (por lo descrito en las leyes de Wolff y Depeche sobre regeneración adaptativa de los tejidos sometidos a presión, antes de la etapa adulta). Aunque suele decirse que es con el fin de insensibilizar esas zonas y desarrollar callos, solo algunas escuelas aun lo hacen con ese fin. 
Algunos de los instrumentos utilizados en la práctica del Hojo Undo son: la tabla de golpeo, o Makiwara, Chishi o candado de piedra , Nigiri game o jarrón/es de boca ancha, Kakete iki, Kongoken o anillo de hierro, Ishisashi, Tan-Tou o vara de hierro, Jari-bako, Tetsu-geta o sandalias de plomo, Sashi-ishi, Makiage-kigu, Tetsuarei, Temochi-shiki makiwara, makiwara cilíndrico, entre otros.
Actualmente los objetos de Hojo Undo tradicional se complementan con elementos modernos para el acondicionamiento físico, como: los sacos de boxeo, los escudos de golpeo, las mancuernas, los discos, las barras, las máquinas de polea o resorte, etc.
Con 50 millones de practicantes en el mundo, el karate es el segundo arte marcial y deporte de combate más practicado en el mundo, después del llamado «karate coreano» o taekwondo con 60 millones, y mucho más que el Judo que tiene 16 millones. Al contrario que estas, el karate no llegó a ser una disciplina olímpica hasta Tokio 2020, pese a que el taekwondo en sus dos modalidades (WTF e ITF) es de por sí una variación de los estilos de karate (shudokan, shotokan y shito ryu respectivamente, junto a las técnicas de pateo derivadas del arte y danza marcial coreana del taekyon). Esto teniendo en cuenta que se realizan numerosas competiciones de karate a nivel local, regional, nacional, continental, internacional y mundial. Incluso el karate está incluido en numerosos eventos como: los juegos mundiales, los juegos de Asia, los juegos Mediterráneos y los juegos Panamericanos.
El «karate coreano» o taekwondo se convirtió en disciplina olímpica desde los juegos del año 2000 en Sídney, Australia, por el impulso del entonces presidente del Comité Olímpico Internacional (COI), Juan Antonio Samaranch. En 2005, durante la sesión 117.ª del COI en Singapur, en la que se decidió que el baseball y el softball no seguirían en el programa olímpico de los juegos a partir de 2012, dejando así dos lugares disponibles para la inclusión de nuevas disciplinas. Cinco deportes fueron examinados por las comisión del programa olímpico: el patinaje, el squash, el golf, el karate y el rugby siete. Dos fueron tenidos en cuenta para el programa de los juegos de Londres 2012: el squash y el karate, que tenían el 60% de los votos necesarios a su favor, pero se requería una mayoría de ⅔ partes para ser elegidos. Tras una nueva sesión hecha en octubre de 2009 en Copenhague, se determinó qué ciudad llevaría a cabo los juegos de Río en 2016, y cuáles serían los deportes incluidos. El karate se presentó por novena vez entre cinco deportes no olímpicos a elegir y no alcanzó la mayoría de votos necesaria. Los contactos entre las federaciones de los deportes paralímpicos y el comité paralímpico internacional buscan que el handikarate o parakarate (que se realiza en silla de ruedas), sea un deporte de demostración en los juegos de Londres de 2012.
En el 2009, En la votación hecha durante el comité Internacional olímpico número 121; el kárate no recibió la mayoría de los votos (2/3 partes de los votos) para convertirse en deporte olímpico. A pesar de que se le estaba considerando para los olímpicos de 2020, pero tras la reunión del comité ejecutivo del COI. que se realizó en Rusia el 29 de mayo de 2013, se decidió que el kárate junto con el wushu y otras disciplinas no relacionadas con las artes marciales, no sería tenido en cuenta para ser incluido en los juegos de 2020. Asimismo esta decisión fue ratificada en la sesión número 125 del COI llevada a cabo en Buenos Aires, Argentina, en septiembre de 2013.
Los maestros del arte marcial tradicional tanto en Okinawa como en el Japón temen que, una vez que el karate sea ratificado como deporte olímpico, y debido a la especialización competitiva, se pierda aún más su faceta como método de autodefensa, tal como ocurre actualmente con el Judo y en el Taekwondo, que dejaron de practicar varias de las técnicas y tácticas propias de la defensa personal, como varias técnicas de golpeo, desarmes, luxaciones y lanzamientos, además de los métodos de golpeo a puntos vulnerables y vitales, perdiendo efectividad ante el surgimiento de nuevos deportes de contacto como las artes marciales mixtas (Mma / Amm ).
El 3 de agosto de 2016, el Comité Olímpico Internacional (COI) aprobó incluir cinco nuevos deportes para los Juegos Olímpicos de Tokio 2020: béisbol/sóftbol, surf, escalada deportiva, karate[8]​ y skateboarding.[9]​
Se disputaron una prueba femenina y otra masculina de kata (series de movimientos) y tres pesos por sexo en kumite (combate).[10]​
Tras los juegos olímpicos de Tokio el karate deja de ser deporte olímpico[11]​ y es sustituido en favor de otras disciplinas como el break dance.
Como en otras artes marciales modernas, o (Gendai Budo) en el karate se establece una diferencia entre la práctica meramente técnica y la de crecimiento interior del practicante, utilizando para ello la palabra "Dō" (camino, búsqueda espiritual) que en las tradiciones chinas y japonesas se utiliza para señalar a aquellas actividades que se practican con esa intención de crecimiento personal, en contraste con una práctica meramente técnica (jutsu).
El maestro Gichin Funakoshi, fundador del estilo Shotokan plasmó en su obra autobiográfica: "Karate-Dō: Mi camino"[12]​ la filosofía de lo que para él era realmente el Karate. Lo entendió como "el purgar de uno mismo los pensamientos egoístas y malos. Porque solo con la mente despejada y consciente puede uno entenderse, así como el conocimiento que recibe". También afirmó: Karate ni senté nashi, que significa que en el karate no existe un primer ataque, entendiéndose que un practicante de Karate nunca debe albergar, mostrar una actitud arrogante y violenta, sino que al Karate-Dō se le debe considerar como un medio para la evolución personal continua a través de un tipo específico de acondicionamiento físico y la adquisición de habilidades.
Funakoshi, quien era un asiduo practicante de la filosofía del Confucionismo, creía que uno debe ser "interior y exteriormente, humilde". Solo al comportarse con humildad se puede estar abierto a muchas opiniones respecto al karate. Esto permite escuchar y ser receptivo ante la crítica. A su juicio, la cortesía era de primordial importancia. Dijo que los practicantes de karate "nunca ser fácilmente arrastrados a una lucha". Se entiende que un golpe de un verdadero experto podría significar la muerte. Está claro que los que abusan de lo que han aprendido se deshonran a sí mismos. Asimismo, el maestro Funakoshi promovió la convicción personal y el pensamiento de que en "tiempos de graves crisis pública, hay que tener el coraje para hacer frente a … un millón de rivales". Enseñando además que la indecisión es una debilidad.
Existe una historia escrita por Funakoshi, que refleja el sentido del karate. Es una parábola acerca del Dō (camino) y un hombre insignificante:
Un karateka pregunta a su Sensei (o maestro que ha recorrido el camino): ¿Cuál es la diferencia entre un hombre del Dō y un hombre insignificante?
El Sensei respondió: "Cuando el hombre insignificante recibe el cinturón negro primer Dan, corre rápidamente a su casa gritando a todos el hecho. Después de recibir su segundo Dan, escala el techo de su casa, y lo grita a todos. Al obtener el tercer Dan, recorrerá la ciudad contándoselo a cuantas personas encuentre."
El Sensei continuó: "Un hombre del "Do" que recibe su primer Dan, inclinará su cabeza en señal de gratitud; después de recibir su segundo Dan, inclinará su cabeza y sus hombros; y al llegar al tercer Dan, se inclinará hasta la cintura, y en la calle, caminará junto a la pared, para pasar desapercibido. Cuanto más grande sea la experiencia, habilidad y potencia, mayor será también su prudencia y humildad".
La práctica del Karate-Dō no se refiere tan solo al desarrollo técnico y táctico, al acondicionamiento físico, al estudio de los katas y al combate real o deportivo. También debe ir de la mano del desarrollo vivencial de la parte humana y la parte espiritual, el crecimiento como personas y ciudadanos ejemplares que unidos por el bien común beneficien a la sociedad. Para lograr esto, el Karate-Do posee principios y objetivos comunes para el crecimiento de sus alumnos: respeto, justicia, armonía y esfuerzo son los primordiales.
En el caso del Karate-Dō, la ética deriva de las filosofías del confucianismo y del budismo zen, aplicados al Karate-Dō. Estos principios fundamentales están basados en el código de los guerreros medievales japoneses o samurái, llamado bushidō. En resumen, estos se podrían sintetizar como los siguientes:
Los valores éticos del Karate se recuerdan en cada Dojo, mediante el Dojo Kun o código de normas de conducta el cual es recitado en cada clase, a manera de recordatorio de la filosofía buscando aplicar los principios filosóficos del karate-Do a la vida diaria, para beneficio del individuo, y la sociedad.
Hoy en día ya son muchos los estilos de Karate-Do tanto de origen okinawense como japonés reconocidos por la Federación Mundial de Kárate o World Karate Federation (WKF), siendo los más difundidos los siguientes: Shorin-Ryu, Goju-Ryu, Uechi-Ryu, Isshin-Ryu, Shorinji-Ryu, Ryuei-Ryu, Shito-ryu, Shotokan, Kushin Ryu, Wado-ryu, y Kyokushinkai. Algunos de estos se describen a continuación: 
Registrado oficialmente en 1908 por el maestro Chosin Chibana. Es un sistema de karate derivado del Shuri-te o estilo del palacio de Okinawa. Tiene sus bases en las técnicas y tácticas para las distancias media y larga heredadas de los pioneros del arte como: Kanga Sakukawa (To-de Sakukawa) y Sokon Matsumura, modificadas por su discípulo Yasutsune Itosu o Anko Itosu. El estilo fue posteriormente nombrado Shorin Ryu por uno de sus alumnos, el maestro Chosin Chibana. Este estilo de karate con el tiempo derivaría en variantes como Matsumura seito, Kobayashi, Shobayashi y Matsubayashi Shorin-ryu (combinación de karate Shorin-ryu con el Tomari-te del maestro Chotoku Kyan, uno de los tres estilos de To-de de Okinawa) y sería la base técnica de las variantes de karate japonés, provenientes de la región de Honshu en el Japón como: el Shotokan, el Shito-ryu y el Wado-ryu. El karate Shorin-Ryu  propuesto por Chosin Chibana sería conocido más tarde como Kobayashi Shorin-ryu, siendo una de sus principales características la enseñanza de dos de los estilos más importantes de Kobudo (arte de manejo de armas ancestrales de okinawa): el kobudo estilo Ryukyu y el Kobudo estilo Matayoshi-ryu fundado por Shinpo Matayoshi. También cabe mencionar que este estilo Shorin-ryu o Kobayashi Shorin-ryu es el único estilo de karate que preserva y enseña el programa completo de kobudo. Actualmente, hay varios maestros de otros estilos de kárate, que aprendieron kobudo con maestros de Shorin-ryu. Sin embargo, en el ámbito del kárate japonés, al kobudo se le considera como un arte marcial aparte del karate. Como estilo, el Shorin Ryu hace fundamental hincapié en el combate en las distancias media y larga, así como en la rapidez, la movilidad, la creación de potencia de golpeo (generada momentáneamente por el movimiento de anteversión de la pelvis, la transferencia del peso corporal, y la contracción temporal fija de los músculos del torso y el abdomen además de la rotación de la cadera), los desplazamientos naturales, los bloqueos o chequeos en ángulo, la penetración en los golpes y la eliminación de cualquier movimiento que no posea un objetivo específico. 
Desarrollado a partir del Naha-te o estilo de la ciudad puerto de Naha. Su popularidad se debió principalmente al maestro Kanryo Higaonna (1853-1915), quien abrió un dojo en la ciudad de Naha, basándose en ocho formas traídas de China. Su mejor alumno, Chojun Miyagi (1888-1953), más tarde fundó formalmente el Gōjū Ryū, o "estilo suave y duro" en 1930. En el karate Goju-ryu se pone mucho énfasis en el combate a media y corta distancia combinando técnicas de bloqueo circular suave con contraataques fuertes y rápidos ejecutados en una rápida sucesión, así como agarres y técnicas de inmovilización. Actualmente, el  Goju-ryu tiene variantes tanto de la región de Honshu (islas principales del Japón) así como en Okinawa. De las variantes de la región de Honshu están la Goju-kai, la Seigokan Goju-ryu fundada por Seigo Tada, Nihon Goju-ryu fundado por Gogen Yamaguchi y entre las variantes practicadas en Okinawa están el Gohakukai-ryu que es una combinación de Goju-ryu con el Tomari-te fundado por Iken Tokashiki, y el Yuishinkan Goju-ryu fundado por Tomoharu Kisaki" que es una combinación de Goju-ryu con el Shuri-te y técnicas de Judo.
Fundado en 1915 por Kanbun Uechi y denominado con su nombre actual en 1939, su origen está compuesto por un antiguo estilo de kung fu (paigonoon, o estilo del tigre y la grulla) que aprendió en China y el Tode proveniente de la ciudad okinawense de Naha o Naha-te. Este estilo de karate es uno de los tres estilos principales de la prefectura de Okinawa junto con el karate Goju-ryu y el karate Shorin-ryu, además de ser el origen de estilos japoneses en la región de Honshu como el Koshukai Uechi-ryu y el Seishin-ryu. El estilo Uechi Ryū hace principal énfasis en movimientos circulares y fuertes, el uso de la distancia y corta, un gran acondicionamiento físico, así como técnicas de agarres, golpes de mano abierta y derribos.
Fundado en 1991 por la organización OKIKUKAI-Okinawa Uechi-Ryu Karate-Do Association, debido a que esta organización se separó de la organización Kokusai Shubukai Uechi-ryu Kyokai por problemas con el tercer soke del estilo Uechi-ryu, Kanmei Uechi (décimo Dan). Los maestros de esta organización juntaron a varios maestros del estilo Uechi-ryu de Okinawa y Japón para hacer crecer más la organización, sin embargo hubo algunos maestros que no aceptaron la invitación entre los que destacan sensei Gushi, sensei Seiko Toyama, sensei Kiyohide Shinjo de Kenyukai, entre otros que decidieron apoyar a la familia del sensei Kanbun Uechi. En el año de 1991, en una reunión en la ciudad de Naha en la prefectura de Okinawa, Japón, los altos maestros de la OKIKUKAI deciden fundar el estilo Shohei-ryu que significa estilo del sol naciente, usando los kanjis de la era showa y el hei, lo anterior debido a que no podían usar el nombre de Uechi-Ryu por problemas de derecho de autor, y que ya no contaban con la autorización del tercer Soke del estilo Kanmei Uechi.
Para diferenciar el estilo Shohei ryu, se crearon nuevas katas para el estilo entre las que destacan las katas Ryuko (en honor al hanshi Ryuko Tomoyose), Kata Giken creada por el Hanshi Yoshitsune Senaga (fundador de la organización Kenseikai Uechi-ryu, afiliada a la OKIKUKAI), Kata Kokahuho creada por el hanshi Tsuneo Shimabukuro (fundador de la organización Uechi-ryu Konan-ryu, afiliada a la OKIKUKAI), kata Shinshu creada por el Hanshi Satoru Shinki (antes de que la OKIKUKAI se separara de la organización Shubukai, esta kata solo se enseñaba a los maestros de alto rango o hanshis, pero en Shohei-ryu se incluyó dentro del programa de grados danes). 
En el año 2010 el Hanshi Shintoku Takara junto con un grupo pequeño de maestros deciden separarse de la OKIKUKAI, por problemas con otros maestros y fundar la Okinawa Uechi-ryu Karate-do Association, aunque aun usando el logo de la OKIKUKAI, por lo que se tenían 2 OKIKUKAIS, una Uechi-ryu y otra Shohei-ryu, un año más tarde las organizaciones afiliadas y que apoyaban la OKIKUKAI entre las que se encuentran Kenseikan de Yoshitsune Senaga y Konan-ryu de Tsuneo Shimabukuro también se separan de las 2 OKIKUKAIS.
En el año 2014 muere el Hanshi Shigeru Takamiyagi de OKIKUKAI Shohei-ryu, perdiendo a uno de sus más grandes exponentes, y en el año 2016 al hacerse el Karate-do deporte olímpico para los juegos de Tokio 2020, deciden apoyar dicha causa, cosa que el resto de maestros del estilo en Okinawa y Japón no vieron de buena manera. En el año 2017, el uso del nombre Uechi-ryu pasa a ser de dominio público, según las leyes japonesas, por lo que la OKIKUKAI abandona el nombre de Shohei-ryu y retoma el nombre de Uechi-ryu conservando las katas nuevas creadas hasta el momento y estando en proceso de crear una nueva kata. Cabe mencionar que el tercer soke Kanmei Uechi murió en 2010, por lo que el cuarto soke Kanji Uechi tomó el mando del estilo Uechi-ryu, y dio nuevamente la autorización a OKIKUKAI de usar el nombre otra vez. En el año 2019 fallecen los maestros Toshio Higa y Ryuko Tomoyose por lo que la OKIKUKAI pierde a otros dos de sus grandes maestros en un año.
En agosto del año 2019, el kyoshi Kazuma Shibahara Cinta Negra octavo Dan de la OKIKUKAI, junto con su amigo el kyoshi Yohei Nakajima séptimo Dan de Ryuei-ryu deciden salir de Japón, el sensei Kazuma rompe vínculos con las dos OKIKUKAIS (la de Shintoku Takara y la comandada por Tsutomu Nakahodo que era la del Shohei-ryu), ambos emigran a Latinoamérica, el sensei Kazuma emigra a la ciudad de Querétaro en México, y el kyoshi Yohei a Guatemala. La razón de esto es que ambos deciden crear un nuevo estilo de Karate-do usando el nombre de Shohei-ryu, por lo que crean la organización Kokusai Seidokan Nihon Shohei-ryu Karate-do Kyokai. El nuevo o resurgido Shohei-ryu conserva las katas del estilo uechi-ryu, las katas que se crearon en Okikukai, y agregan las katas del estilo Ryuei-ryu y dos katas de kyokushin que son Garyu y Yantsu. Con lo anterior el estilo Shohei-ryu pierde su característica de estilo okinawense pasando a ser parte de los estilos japoneses, pero aún más fortalecido que antes.
Fundado en 1928 por Kenwa Mabuni (1889-1952), influenciado directamente tanto por el Naha-te como por el Shuri-te, aunque enfatizando más el Shorin-Ryu que el Goju-Ryu, básicamente es un estilo mixto de karate-do. El nombre Shito se deriva de la combinación de los caracteres japoneses de los nombres de los maestros de Mabuni: Anko Itosu y Kanryo Higaonna. Las escuelas de Shitō-ryū poseen un gran número de katas, tomadas de varios estilos como el Shorin-Ryu, Shorinji-Ryu, Isshin-Ryu y Goju-Ryu. El Shito Ryu se caracteriza por un especial énfasis en la velocidad de ejecución de las técnicas y en la precisión de sus movimientos.
Registrado por Gichin Funakoshi (1868-1957) en Japón en 1938, aunque fue dado a conocer desde 1922. "shoto" era el seudónimo usado en sus poemas por el maestro Funakoshi y "kan" (escuela o dojo), se considera el primer estilo propiamente japonés de karate-Do. Asimismo, al maestro Funakoshi se considera el fundador del karate moderno, ya que fue el primero en dar a conocer el arte en la región de Honshu en el Japón. Funakoshi nació en la ciudad de Shuri en Okinawa. En su infancia comenzó a estudiar karate (conocido en ese entonces como: te, tuidi, o to-te) con el maestro Yasutsune Azato, uno de los mayores expertos de Okinawa en este arte, y posteriormente con el reconocido maestro Anko Itosu. En 1921 Funakoshi introdujo de manera pública por primera vez el Karate en Tokio. En 1936, a los 70 años, abrió su propia sala de entrenamiento. El dojo fue nombrado por sus primeros alumnos, y se llamó Shotokan. El Shotokan actual tiene características muy específicas que no se deben al tipo de karate de Okinawa enseñado inicialmente por el maestro Gichin Funakoshi; sino a las varias innovaciones incluidas por sus alumnos incluyendo a su hijo Yoshitaka, y a su alumno el maestro Masatoshi Nakayama. Innovaciones hacia el aspecto deportivo, las cuales aunque Gichin Funakoshi no siempre las compartió; las permitió en pos de hacer del karate más que un método de defensa personal, buscando un medio de formación del individuo. Este estilo se caracteriza por el uso asiduo de las posiciones bajas, amplias y fuertes y el trabajo de rotación de la cadera, los bloqueos en ángulo, la preferencia por la distancia larga, la rotación de la cadera tanto en el ataque como en la defensa, y el uso de la sinergia muscular para generar la potencia, tanto en las técnicas de ataque con puño y mano abierta, como en las técnicas de defensa; asimismo posee: algunos lanzamientos y varios barridos similares al Judo, basados en varias técnicas de lucha provenientes de la lucha típica de Okinawa, o "Tegumi". Al igual que varios conceptos y métodos de entrenamiento tradicional derivados del kendo (esgrima japonesa), debido a que estas artes marciales fueron practicadas por varios de sus maestros iniciadores, y pioneros en su difusión en occidente.
Fundado en 1939 por Hironori Otsuka, es un sistema de karate desarrollado a partir del kobayashi Shorin Ryu, enseñado por Gichin Funakoshi antes de desarrollar el estilo Shotokan, junto con varios elementos tácticos (desplazamientos) y técnicos (luxaciones, lanzamientos y estrangulaciones) derivados del jiu-jitsu japonés (concretamente del estilo Shindō Yōshin-ryū, 新道楊心流), estilo del cual que el maestro Otsuka era ya considerado un gran maestro. El Wadō-ryū pone un fuerte énfasis en la suavidad, la absorción, sin olvidar la aplicación precisa de la fuerza. También incluye la disciplina espiritual, llevando al practicante a armonizarse con su entorno, siendo esto lo que significa "Wado Ryu": "camino de la armonía".
Fundado por Hanshi Kosuke Heianna Zukeran 10.º Dan, su estilo combina Técnicas y Katas de Uechi-Ryu, Ryuei-Ryu y Nihon Goju-ryu. Sensei Heianna fue alumno de Kanei Uechi con el que aprendió Uechi-Ryu, fue alumno después de Kenko Nakaima y compañero de Tsuguo Sakumoto con los que aprendió Ryuei-Ryu y más tarde entreno Nihon Yuishinkan Goju-Ryu con Tomoharu Kisaki. En 1979 oficialmente funda su propio estilo conocido como SeitoKaiKan o Seitokaikan-ryu, y abre su hombu dojo en la ciudad de México. Al año siguiente crea la organización Heianna Group que rige su estilo en México y Chile. Las Katas del estilo Seito-Kai-Kan son Sanchin, Suparimpei, Seienchin, Seisan, Saifa, Niseishi, Anan, Heiku, Paiku, Paiho, Pachu, Ohan, Kanshiwa, Kanshu, Kanchin, Seichi, Seiryu, Heianna Kata Ichi, Heianna Kata Ni, Kata Hiho, Kata Hiho Ni y Panchudo. Seito-Kai-Kan es un estilo de Karate-Do que combina Uechi-Ryu, Ryuei-Ryu y Nihon Goju-Ryu. 
Además de estos estilos base, existen infinidad de variantes y combinaciones de ellos. Algunos son originarios de Okinawa, como el Isshin-ryū del maestro Tatsuo Shimabuku. Otros surgieron de la fusión de otros estilos o por divisiones internas de los anteriores, incluyendo elementos de otras artes marciales, como el Ken-Shin-Kan fundado por Seiichi Akamine, el Shindo Jinen Ryu fundado por Yasuhiro Konishi, el Kyokushin o Kyokushinkai fundado por Masutatsu Ōyama, el Gensei Ryu del Renbu Kai de Geka Yung, el Shōtōkai de Shigeru Egami, el Kushin Ryu, Shokundo-Ryu de Taito Kumagawa,Shudokan del maestro Kanken Toyama, entre otros.
El Taekwondo o "kárate coreano" es un deporte olímpico de combate que se basa fundamentalmente en artes marciales mucho más antiguas como el taekkyon coreano en la forma y realización de los golpes con el pie y desplazamientos; y en los estilos Shūdōkan, shito ryu y shotokan (en el taekwondo WTF/hoy día WT) y shotokan (en el taekwondo ITF) del karate-Do japonés de donde obtiene los golpes con el puño, los golpes a mano abierta, la planimetría (o división del cuerpo por zonas alta/media/baja), los bloqueos, las posiciones, el sistema de grados por cinturones, su primer uniforme, y sus primeras formas. Las formas del Taekwondo fueron desarrolladas entre 1960 y 1990 siendo conocidas como "Poomsae"/pumse en la WT (World Taekwondo) y "Hyong" en la ITF (International Taekwon-Do Federation), estas se establecieron con el fin de promover aún más su propia identidad. 
Asimismo el Tangsudo, o tang soo do, otro estilo similar al taekwondo, pero que busca ser más tradicional, al no ser muy competitivo, fue muy influenciado por el Kárate-Do japonés, su fundador el coreano kuk Wan Lee, adoptó la gran mayoría de los katas del estilo de kárate Shotokan (exceptuando las posiciones bajas), y preservó  incluso el manejo del bastón largo japonés o bō dentro de su estilo. y renombrándo las técnicas, posiciones y comandos en idioma coreano. A estas técnicas adicionó las técnicas patadas del arte y danza marcial clásica coreana del Taekkyon. Posteriormente en los años 50, varias corrientes y asociaciones del Hapkido coreano adoptaron varias de las técnicas del Tangsudo de bloqueo, golpes a mano abierta y de puño, como complemento y conexión para la ejecución de varias técnicas de luxación y lanzamiento, que también fueron previamente adaptadas de las disciplinas japonesas del judo y del Daito Ryu aiki jiu-jitsu , (siendo este el antecesor directo del Aikido).   
El full contact o karate a "contacto pleno" es un deporte de combate que nació en los Estados Unidos en los años 60 como respuesta a las expectativas de eficacia de muchos de los practicantes occidentales de artes marciales tradicionales en EE. UU., como el Karate-Do, el Taekwondo, y el kung-fu/wu shu, quienes, contando con una muy buena preparación física, observaron que los campeonatos tradicionales "al punto" no eran los suficientemente cercanos y justos a la realidad del combate. Sus técnicas incluyen el uso de todos los golpes del boxeo, las técnicas de patadas altas y los barridos, exceptuando las técnicas de patadas a los muslos tipo "low kick", buscando la puesta fuera de combate del adversario o "knock out" (nocaut). El primer Campeonato Profesional fue llevado a cabo por la United States Karate Association en 1964. Entre la lista de Campeones Mundiales de Karate Profesional se encuentran Joe Lewis, Chuck Norris y Bill Wallace. El full contact es actualmente una de las modalidades de competición del kick boxing y se le considera uno de los antecesores históricos de las modernas Artes Marciales Mixtas (AMM / MMA).
El Daido-Juku Kudo (大道塾空道 Daidō Juku Kūdō?) es también conocido simplemente como Kudo (空道 Kūdō?) es un arte marcial moderno de origen japonés, fundado en 1981 por Takashi Azuma, quien fuera campeón de karate kyokushin. Esta disciplina es principalmente una combinación de kárate y Judo, aunque incluye elementos del boxeo y del kickboxing. Los competidores de Daido-Juku visten un "kudogi" oficial, basado en el tradicional judogi. Este diseño es ideal para las técnicas de proyección y agarre. En competición, los kudokas han de utilizar protecciones genitales y bucales, junto con coderas, guantes de artes marciales mixtas y casco ("SuperSafe") homologados.
A lo largo de su historia, el karate se ha adaptado no solo a la defensa personal y al uso militar sino que desde su llegada al Japón en el siglo xx, la práctica del karate se ha enfocado en la educación emocional y moral de sus practicantes; y como cualquier arte marcial tradicional, enseñado de forma progresiva, debidamente planificado, y valorado conlleva un sinfín de beneficios. Tanto para niños, para los adolescentes y para los adultos. Los principales beneficios de la práctica constante del karate y su filosofía de vida incluyen tanto la salud del cuerpo, como la de la mente junto a la obtención de una actitud consciente, perseverante, reflexiva, respetuosa y decidida. De manera general la práctica:
En lo que respecta a los beneficios del entrenamiento en las artes marciales formativas gendai budo en los niños, son:
Los niños a los que les guste este arte marcial pueden  comenzar a practicarlo desde los 4 años de edad, o especialmente durante la adolescencia, cuando buscan su identidad; la práctica constante les ayudará al desarrollo de su personalidad y a la construcción de su carácter, a concentrarse mejor a la hora de estudiar, mejorando la toma de sus futuras propias decisiones, y a tener una buena disciplina en el colegio, respetando a sus compañeros, y superiores.
Al adulto, la práctica ha de proporcionarle una mejor calidad de vida, preparándole para la vejez, siendo un espacio donde reflexionar, aplicar y desarrollar aún más, su inteligencia emocional.

Las locuciones antigua Grecia y Grecia antigua (griego clásico: Ἀρχαία Ἑλλάς; neogriego: Αρχαία Ελλάδα; latín: Graecia antiqua) se refieren al período de la historia griega que abarca desde la Edad Oscura de Grecia, comenzando en el año 1200 a. C. y la invasión dórica, hasta el año 146 a. C. y la conquista romana de Grecia tras la batalla de Corinto. Se considera generalmente como la cultura seminal que sirvió de base a la civilización occidental. La cultura griega tuvo una influencia notable sobre el Imperio romano, que la difundió a través de sus territorios en Europa, norte de África y Oriente Próximo. La civilización de los antiguos griegos ha sido enormemente influyente para la lengua, la política, los sistemas educativos, la filosofía, la ciencia y las artes, dando origen a la corriente renacentista de los siglos xv y xvi en el continente europeo y resurgiendo también durante los movimientos neoclásicos de los siglos xviii y xix en Europa y América.
La civilización griega era básicamente marítima, comercial y expansiva. Una realidad histórica en la que el componente geográfico jugó un papel crucial en la medida en que las características físicas del sur de la península de los Balcanes, de accidentado relieve, complicaban la actividad agrícola y las comunicaciones internas, mientras que su dilatada longitud costera favorecía su expansión hacia ultramar. Un fenómeno sobre el que incidirían también de forma substancial la presión demográfica originada por las sucesivas oleadas de pueblos (entre ellos los aqueos, los jonios y los dorios) que invadieron y ocuparon la Hélade a lo largo de los milenios iii y ii  a. C.
Algunos historiadores consideran que los primeros Juegos Olímpicos antiguos en el 776 a. C. señalan el comienzo del período conocido como la Antigua Grecia. Entre el fin del período micénico y los primeros olímpicos transcurre una época llamada la Edad Oscura de Grecia, de la cual no existe ningún escrito y quedan pocas reliquias arqueológicas. Hoy en día, este período se incluye en las locuciones «Grecia Antigua» y «Antigua Grecia».
Tradicionalmente se consideraba que la época de la Antigua Grecia finalizaba con la muerte de Alejandro Magno en el 323 a. C., dando comienzo al período helenístico.[1]​ No obstante, se extiende el período de la Grecia Antigua muchas ocasiones para incluir el tiempo hasta la conquista romana de 146 a. C. Algunos autores tratan la cronología de esta era como un continuo hasta la llegada del cristianismo en el siglo IV; pero esta opinión es poco convencional.
La historia de la Grecia Antigua suele subdividirse en varios períodos según la alfarería y los sucesos políticos, sociales y culturales:[2]​
Los primeros griegos se organizaban en clanes familiares. Con el tiempo, los clanes se aliaron y formaron comunidades, aunque estaban separadas entre sí debido al relieve montañoso de la región. Esto favoreció que se convirtieran en territorios independientes con gobierno y ejército propios. En griego antiguo esas poblaciones eran llamadas polis. Pese a compartir esencialmente el mismo espacio geográfico, lengua y cultura, la organización política de las polis era muy diversa, incluyendo un amplio abanico de sistemas de gobierno, que abarcaba desde la tiranía hasta la democracia.
Podemos ver estas diferencias al comparar Esparta y Atenas, dos de las más importantes ciudades de la Antigua Grecia. Esparta era gobernada por reyes; a sus habitantes se les educaba para la guerra, por lo que debían ser fuertes y hábiles en el manejo de las armas; a las mujeres se les enseñaba a luchar igual que a los hombres, tenían derechos y libertad para elegir a sus esposos. Por su parte, en Atenas los gobernantes eran elegidos por el voto de los ciudadanos; los hombres no eran educados para la guerra; las mujeres no iban a la escuela, sólo podían salir acompañadas de sus familiares y no tenían derechos políticos. Aunque las ciudades-estado eran independientes y continuamente se enfrentaban, también se unían cuando eran atacadas por enemigos comunes, como el Imperio persa.
Tras las civilizaciones minoica y micénica, en los siglos oscuros (entre el XIII y el XII a. C.) la fragmentación existente en la Hélade constituirá el marco en el que se desarrollarán pequeños núcleos políticos organizados en ciudades, las polis.
A lo largo del período arcaico (siglos VIII al V a. C.) y del clásico (siglo V a. C.), las polis fueron la verdadera unidad política, con sus instituciones, costumbres y leyes, y se constituyeron como el elemento identificador de esa época. En el período arcaico ya se perfiló el protagonismo de dos ciudades, Esparta y Atenas, con modelos de organización política extremos entre el régimen aristocrático y la democracia. La actividad de las polis hacia ultramar fue un elemento importante de su propia existencia y dio lugar a luchas hegemónicas entre ellas y al desarrollo de un proceso de expansión colonial por la cuenca mediterránea. La decadencia de las polis favoreció su absorción por el reino de Macedonia a mediados del siglo IV a. C. y el inicio de un período con unas connotaciones nuevas, el helenístico, por el que la unificación de Grecia daría paso con Alejandro Magno a la construcción de un imperio, sometiendo al Imperio aqueménida y al egipcio. En opinión de algunos especialistas, en esta fase la historia de Grecia volvía a formar parte de la historia de Oriente y se consumaría la síntesis entre el helenismo y el orientalismo. La civilización griega se desarrolló en el extremo nororiental del mar Mediterráneo, en los territorios que hoy ocupan Grecia, Asia Menor (Turquía), y en varias islas como Creta, Chipre, Rodas y Sicilia (Italia).
La mayoría de los historiadores y escritores políticos cuyas obras han sobrevivido ―principalmente Heródoto, Tucídides, Jenofonte, Demóstenes, Platón y Aristóteles― eran o atenienses o proatenienses. Por eso sabemos mucho más sobre la historia y la política de Atenas que de cualquier otra ciudad griega. Además, estos escritores se centran en la historia política, militar y diplomática, prestándole relativamente poca importancia a la historia económica o social.[3]​
El concepto de «Antigua Grecia» comprende, desde una perspectiva geográfica, un conjunto de territorios diversos unidos por un mismo proceso histórico con base en los fuertes vínculos que sus pueblos mantuvieron y en los aspectos comunes que compartieron. Sus habitantes se referían a este conjunto como Hélade,[4]​ y fueron los romanos quienes posteriormente le asignaron el nombre de Grecia.[5]​
La Hélade tuvo su base en tres grandes regiones, dos de ellas continentales y, una tercera, insular. Las regiones continentales comprendían la península balcánica y las tierras costeras del Asia Menor (actual Turquía); la insular, por su parte, incluía el conjunto de islas del mar Egeo (Creta, el archipiélago del Dodecaneso, el archipiélago de las Cícladas y las tierras cercanas a la costa asiática).[4]​
La zona septentrional de la península de los Balcanes, caracterizada por la alternancia de relieve tabular y llanura esteparia, fue la zona de mayor contacto con el resto de Europa oriental. Allí se establecieron las antiguas Tesalia, Macedonia, Etolia, Acarnania y Epiro. La zona central, comunicada dificultosamente con la anterior a través de angostos desfiladeros y de un terreno igualmente irregular de macizos y llanuras, vio prosperar a las antiguas regiones de Dórida, Fócida, Beocia y Ática. La zona meridional, abundante en macizos y fosas y comunicada con la anterior a través del istmo de Corinto, comprendía a su vez las regiones de Acaya, Arcadia, Argólida, Laconia y Mesenia.[4]​
Las características geomorfológicas de estas zonas favorecieron la diversificación de los recursos naturales y el fraccionamiento político, determinando la extensión reducida y las fronteras difusas de las polis, así como las peculiaridades de sus economías y vínculos recíprocos.[6]​ La región del Ática contaba con minas de plata; Laconia, Beocia y Eubea poseían hierro; esta última, de igual manera, disponía de cobre, pero le era necesario obtener el estaño —para la aleación del bronce— del extranjero. La región de Macedonia, por su parte, era rica en oro.[6]​ La abundancia de la arcilla en la península favoreció el desarrollo de centros ceramistas y de la manufactura de objetos de gran valor humano agregado, claves en su comercio exterior; de igual manera sucedió con la piedra y el mármol, abundantes en dichas zonas rocosas.[6]​
La colonización del Asia Menor respondió a los impulsos migratorios de los griegos europeos durante la época arcaica y se limitó a la ocupación de la faja litoral, distinguiéndose en ella tres grandes regiones: Eólida, Jonia y Dórida. La Eólida limitaba al norte con las costas del mar Negro y al sur con la desembocadura del río Hermo. Más al sur se estableció Jonia. Dórida se ubicaba en la zona más meridional del litoral y ocupaba la franja costera de un macizo de difícil acceso, con una costa abrupta y un suelo pobre.[4]​
Exceptuando la Dórida, cuyas características geográficas actuaban negativamente en el desarrollo de la agricultura y, en contraposición, su cercanía y acceso a otras tierras de Oriente favorecían el desarrollo comercial de sus pueblos, las colonias griegas de la Grecia asiática poseían características orográficas, fluviales y climáticas favorables al trabajo de la tierra. Estas zonas, ricas en ríos y valles, fueron prósperas productoras de cereales, olivo, vid y hortalizas. Por otra parte, algunas de las regiones poseían gran cantidad de pinos y cedro, fundamentales para la construcción naval.[4]​
Las islas del Egeo representaron los puntos de contacto del mundo griego y actuaron como facilitadoras de las comunicaciones y el comercio. Eubea, separada por un estrecho canal de las costas orientales de Beocia y Ática (el estrecho de Euripo), se constituye de colinas onduladas con un suelo fértil, apto para los cultivos agrícolas, las actividades ganaderas y la extracción y el trabajo del cobre. Entre las islas Cícladas, por su parte, hay algunas de características volcánicas y otras de suelo fértil aptas para el cultivo de cítricos y vid. Algunas de ellas, como Paros, basaron su economía en la producción de mármol; Sifnos, por su parte, fue un importante centro de producción de plata en la época arcaica. Del archipiélago del Dodecaneso, conjunto de islas del sudoeste de Anatolia, destacan Samos e Icaria que, alternando terrenos montañosos con llanuras, propició la producción de cereales en estas últimas, así como de olivo y vid en las laderas. La isla de Rodas, del mismo archipiélago, fue escala obligada en las rutas comerciales con Egipto y Oriente Medio, convirtiéndose en un importante centro comercial. En todo caso, los terrenos del Dodecaneso son aún más fértiles que los de las Cícladas, en buena parte debido al clima húmedo y a la abundancia de lluvias. Al norte, entre las islas de Egeo oriental, destacan por su fertilidad agrícola Quíos y Lesbos.[4]​
La isla de Creta, límite sur del mar Egeo, se caracteriza por su orografía accidentada, en ciertas zonas similares a la de la Grecia meridional europea. Al norte, las extensas llanuras hicieron posible el desarrollo de la agricultura mediterránea; sus bahías, constituyeron lugares idóneos para la navegación y el desarrollo comercial, así como para la pesca, otra de las actividades primarias de su subsistencia. El terreno montañoso existente más al sur, y los bosques que crecían en él, propició la producción de madera, mientras que sus montes bajos fueron el terreno ideal para el ejercicio de la ganadería extensiva y la caza (esta última fue decisiva en la economía de las civilizaciones neolíticas). En conjunto, la extensión de la isla, su variedad de recursos (mineros, agrícolas, ganaderos y pesqueros) y su localización comercial estratégica, hicieron posible que se desarrollase en ella una civilización propia, la minoica.[4]​
Los primeros hallazgos de vida humana en territorio griego constatan la existencia de poblaciones autóctonas en el Paleolítico, hacia el 7000 a. C.[7]​ Alrededor del 6000 a. C. ―en los albores del Neolítico―, los pueblos nativos desarrollaron la agricultura y con ello se volvieron progresivamente sedentarios, extendieron la práctica de la alfarería y crearon instituciones políticas básicas.[7]​ Posteriormente comenzaron a utilizar el bronce, pero el refinamiento de su uso se produjo tras el contacto con poblaciones inmigrantes.
Se cree que las tribus que se convertirían en los griegos emigraron hacia el sur a los Balcanes en varias oleadas comenzando a mediados de la Edad del Bronce (alrededor de 2000 a. C.).[8]​ Otras fuentes indican un proceso migratorio ya en el quinto milenio a. C., proveniente de Mesopotamia y Siria. Según estas, los primeros inmigrantes encontraron habitantes nativos que dejaron a los recién llegados una gran cantidad de tradiciones; mientras que estos llevaron a la zona la cultura de la alfarería, agricultura y una primera deidad de la fertilidad (que más tarde sería Deméter).[9]​
Esta última versión, de ser exacta, negaría la existencia de un período neolítico en los pueblos autóctonos con anterioridad a la inmigración extranjera, situándolos en un período cultural más cercano al mesolítico.
El idioma protogriego se fecharía hacia el período inmediatamente anterior a estas migraciones, ya sea a finales del III milenio a. C. o, a más tardar, al siglo XVII a. C. La civilización de los protogriegos de la Edad del Bronce es generalmente conocida como heládica y precedió a lo que es conocido como Grecia Antigua o Antigua Grecia.
El período heládico, según algunos historiadores,[10]​ puede ser dividido analíticamente en cuatro estadios bien delimitados:
Durante el período en que la Grecia peninsular todavía resplandecía bajo la impresionante cultura micénica, en la isla de Creta se producía el florecimiento de la civilización minoica cretense con capital en Cnosos (1600-1250 a. C.).[11]​
Esta civilización debe su nombre al semilegendario rey Minos. Los cretenses comerciaban por todo el Mediterráneo y exportaban cerámica, tejidos, objetos de bronce y orfebrería.
Es probable, por su parte, que la cultura micénica se viera influida por la minoica, particularmente en el período de mayor esplendor de esta última.[7]​ La sensación de poderío de los reyes de Creta era tal que las ciudades, palacios y templos cretenses ni siquiera estaban rodeados por murallas.[11]​ Las excavaciones han encontrado maravillosas evidencias del auge y avance tecnológico del que gozaban los minoicos en ese entonces: lujosos lavabos, instalaciones de ventilación, pozos higiénicos, filtros, elaboradas pinturas y escudos de armas.
En esa época era frecuente que los hijos de príncipes extranjeros fueran enviados a luchar contra un toro en forma de sacrificio, y en tal sentido son interpretadas las representaciones pictóricas de jóvenes de ambos sexos bailando alrededor de un toro o luchando con él.[11]​ Por su parte, esta práctica tiene su claro punto de contacto mitológico con la leyenda del Minotauro, «toro de Minos», que recibía periódicamente el tributo de varios jóvenes atenienses para sacrificio.
La civilización minoica pereció poco antes que la micénica; algunas versiones señalan que fueron invadidos por estos últimos, mientras que otras se inclinan a afirmar que la desaparición del reino de Creta se debió a una catástrofe natural.[11]​
Desde 1200 hasta el siglo VIII a. C. se conoce como la Edad oscura —siguiendo al colapso de la Edad del Bronce—. De esta etapa no ha sobrevivido ningún texto primario, y solamente queda escasa evidencia arqueológica. Algunos textos secundarios y terciarios contienen breves cronologías y listas de los reyes de este período, incluyendo Historia de Heródoto, Descripción de Grecia de Pausanias, Biblioteca histórica de Diodoro Sículo y Chronicon de Jerónimo.
La carencia de documentos primarios se explica por la virtual desaparición del sistema de escritura micénico (Lineal B). En la cultura micénica, dicho sistema estaba restringido a pequeños círculos, particularmente a los escribas de los palacios, que tenían a su cargo el grabado de recuentos de movimiento y distribución de bienes; hundida la economía micénica, ya no fueron necesarias personas que realizaran dicha tarea.[12]​ Las tradiciones y leyendas sobrevivieron, desde la Edad del Bronce hasta la Época Arcaica, gracias exclusivamente a la transmisión oral.[12]​
En la época se produjo un abrupto descenso demográfico y una serie masiva de migraciones que determinaron el establecimiento de poblaciones espontáneas y poco organizadas en diferentes puntos de la Grecia continental, las islas Cícladas y el poniente de Asia menor. Estas migraciones tuvieron un carácter étnico; así, por ejemplo, los dorios ocuparon la mayor parte del Peloponeso, Grecia Central y Creta, mientras que los jonios colonizaron la mayor parte de las Cícladas.[13]​ Lo anterior se reflejó en el idioma, que derivó, asimismo, en multitud de dialectos.[13]​
La economía, floreciente en el período micénico, se vio reducida a la agricultura, sustentada por esclavos, jornaleros (thêtes) y aparceros (hektemoroi).[10]​ Se generalizó la pobreza y la escasez del ganado, que fue adquirido por unos pocos terratenientes.[10]​ No hay registro de Estados organizados políticamente en esta época y mucho menos de las estructuradas normas de tipo micénico, que regulaban la economía y aseguraban una relativa distribución de la riqueza, permitiendo que la vida diaria de los agricultores, pastores y ceramistas resultase tolerable.[14]​
En este contexto, los trabajadores de la tierra se dedicaron a la agricultura de subsistencia, organizados en pequeñas comunidades que raramente excedían las veinte personas.[14]​ La necesidad de nuevas pasturas para los animales produjo a su vez un incremento del nomadismo.[14]​ En el ámbito religioso, continuaron los cultos micénicos.[10]​
En el terreno del arte y la cerámica, se produjo un empobrecimiento de las formas micénicas; generándose posteriormente dos períodos arqueológicos: el protogeométrico (1050-950 a. C.) y el geométrico (950-700 a. C.), que harían evolucionar lentamente la calidad y técnica artesanales hasta concluir, ya en los albores de la Época Arcaica, en un mundo ornamental nuevo y plenamente desarrollado.[10]​ La evolución mencionada durante estos períodos se limita casi exclusivamente a la cerámica; no existe evidencia de que se hayan erigido monumentos durante la Edad oscura ―práctica común durante la época micénica― y las representaciones antropomórficas fueron usualmente grabadas en ánforas.[14]​ En el ámbito de la arquitectura, se abandonó la construcción en piedra.[7]​
Atenas fue la excepción a la regla del derrumbe de la civilización. Su acrópolis, centro civilizado en los últimos tiempos de la Edad del Bronce, no sufrió daños, y transitó la «Edad Oscura» en el marco de una prosperidad relativa.[15]​ Sin embargo, sus instituciones sociales y políticas no lograron salir airosas de este período y, en los albores de la «época arcaica», Atenas había perdido el acervo cultural sociopolítico acumulado en el período micénico, viéndose obligada a reconstruir sus instituciones sin mucho más que la monogamia como sustento institucional heredado.[15]​
En el siglo VIII a. C., Grecia empezó a salir de la Edad Oscura que siguió a la caída de la civilización micénica. Al pueblo le faltaba alfabetización y se había olvidado el sistema de escritura micénico, Lineal B. Pero los griegos adoptaron el alfabeto fenicio y lo modificaron para crear el alfabeto griego.[16]​[17]​ A partir del siglo IX a. C.[18]​ ―según algunos autores, específicamente en el VIII a. C.―[19]​ empezaron a aparecer escritos.
Grecia se dividió en muchas comunidades autónomas pequeñas. Esta pauta fue impuesta en gran parte por la geografía griega, donde cada isla, valle y llanura está aislada de las demás por el mar o las sierras.[20]​ Como producto directo de las migraciones previas, dichas comunidades mostraban un carácter étnico: durante el siglo VII a. C. surgió Argos, habitada por dorios, como una de las ciudades principales del Peloponeso.[21]​ Dicha ciudad fue cediendo gradualmente influencia a su rival Esparta, también dórica.[21]​ Por su parte, Atenas se convirtió en la residencia principal de los jonios en los Balcanes.[22]​
La primera mitad del siglo VII a. C. vio la guerra Lelantina (hacia el 710-650 a. C.), un conflicto prolongado que se distingue como la guerra documentada más temprana del período de la Grecia Antigua. En ella se enfrentaron las ciudades-estado entonces importantes Calcis y Eretria sobre la fértil llanura lelantina de Eubea. Ambas ciudades parecen haber sufrido declives como resultado de esta larga guerra, aunque Calcis fue la vencedora nominal.
En la primera mitad del siglo VII surgió una clase mercantil y, en el correr del siglo VI a. C., se comenzaron a utilizar monedas (probablemente por imitación a los lidios), aunque serían necesarios siglos para el desarrollo de una economía monetaria plena.[17]​ Parece haberse gestado tensión en muchas ciudades-estado. Los regímenes aristocráticos que por lo general gobernaban las llamadas polis se sentían amenazados por la nueva riqueza de los comerciantes, que a su vez deseaban poder político.
A partir de 650 a. C., las aristocracias tenían que luchar para evitar ser derrocadas y reemplazadas por tiranos populistas. La palabra deriva de la palabra griega no peyorativa τύραννος tyrannos, que significa 'soberano ilegítimo', que se podía aplicar tanto a buenos como a malos líderes.[23]​[24]​
Una población cada vez mayor y la falta de tierras provocaron conflictos internos entre los pobres y los ricos en muchas ciudades-estado. En Esparta, las guerras mesenias resultaron en la conquista de Mesenia y la esclavitud de los mesenios, a partir de la segunda mitad del siglo VIII a. C., constituyendo un acto sin precedentes en la Grecia Antigua. Esta práctica produjo una revolución social.[25]​
La población subyugada, desde entonces conocida como ilotas, labraban y trabajaban para Esparta, mientras todos los ciudadanos varones se convertían en soldados de un estado permanentemente militarizado. Incluso las élites eran obligadas a vivir y a entrenarse como soldados; esta igualdad entre los pobres y los ricos servía para distender los conflictos sociales. Las reformas precedentes, atribuidas al enigmático Licurgo de Esparta fueron probablemente completadas antes de 650 a. C.

Atenas, por su parte, sufrió falta de tierras y una crisis agraria a finales del siglo VII, lo que también resultó en conflictos civiles. El arconte (magistrado) Dracón promulgó reformas severas en 621 a. C. (de ahí la palabra moderna «draconiano»), pero estas no pudieron acallar el conflicto. Al final las reformas moderadas de Solón (594 a. C.) le dieron a Atenas cierta estabilidad, mejorando la vida de los pobres aun cuando afianzaron a la aristocracia en el poder.Para el siglo VI a. C. varias ciudades se habían vuelto dominantes en la civilización griega: Atenas, Esparta, Corinto y Tebas. Cada una había puesto las áreas rurales y los pueblos menores a su alrededor bajo su control. Además, Atenas y Corinto se habían convertido en grandes potencias marítimas y mercantiles.
Los rápidos aumentos de población en los siglos VIII y VII desencadenaron un fenómeno emigratorio que afectó a muchos griegos, estableciendo estos colonias en Magna Grecia (Mezzogiorno), Asia Menor y más lejos (ver abajo). La emigración cesó finalmente en el siglo VI. Para entonces el mundo griego había difundido su cultura y su lengua en una extensión que superaba ampliamente los límites de la actual Grecia. Las colonias griegas no eran controladas políticamente por las ciudades que las habían fundado, aunque muchas veces mantenían vínculos religiosos y comerciales entre ellas.
Durante este período ocurrieron grandes desarrollos económicos en Grecia y también en sus colonias de ultramar, que experimentaron crecimiento en el comercio y la manufactura. El nivel de vida de la población también mejoró de manera considerable. Un estudio de Ian Morris estima que la casa griega típica aumentó de coste entre cinco y diez veces entre el 800 y 300 a. C.[26]​
En la segunda mitad del siglo VI, Atenas cayó bajo la tiranía de Pisístrato, y luego de sus herederos Hipias e Hiparco. Sin embargo, en 510 a. C., por pedido del aristócrata Clístenes de Atenas, el rey espartano Cleómenes I ayudó a los atenienses a derrocar la tiranía. Poco después, empero, Esparta y Atenas iniciaron relaciones hostiles, y Cleómenes I instauró a Iságoras como arconte pro-espartano.
Con el objetivo de evitar que Atenas se convirtiera en un «gobierno de paja» bajo el reinado espartano, Clístenes propuso a sus conciudadanos atenienses que Atenas sufriera una revolución política, que todos los ciudadanos compartieran el poder independientemente de su estatus, que Atenas se volviera una «democracia». Los atenienses abrazaron esta idea con tantas ganas que después de derrocar a Iságoras e implantar las reformas de Clístenes, pudieron repeler fácilmente una invasión a tres frentes que los espartanos condujeron para reinstaurar a Iságoras.[27]​ La llegada de la democracia resolvió muchos de los problemas de Atenas, dando inicio a una «edad de oro» para los atenienses.
En el siglo V a. C. Atenas y Esparta, rivales tradicionales, tendrían que aliarse ante la mayor amenaza a la que la Grecia Antigua se enfrentaría hasta la conquista romana; después de aplastar la revuelta jónica (una rebelión de las ciudades griegas de Jonia), Darío I de Persia, rey de los reyes de la dinastía aqueménida, decidió subyugar a Grecia. Su invasión en el 490 a. C. fue sofocada por la heroica victoria ateniense en la batalla de Maratón bajo Milcíades el Joven. Jerjes I de Persia, heredero de Darío I, intentó su propia invasión diez años después.
Pero a pesar del número abrumador de soldados en su ejército, Jerjes I fue derrotado después de la famosa batalla de retaguardia de las Termópilas y las victorias de los aliados griegos en las batallas de Salamina, Mícala y Platea. Las guerras médicas continuaron hasta 449 a. C., conducidas por los atenienses y su Confederación de Delos, durante las que Macedonia, Tracia, las Islas del Egeo y Jonia fueron liberadas de la influencia de Persia.
La posición entonces dominante del imperio ateniense marítimo amenazó a Esparta y a la Liga del Peloponeso, compuesta de ciudades de Grecia continental. Inevitablemente, encendió la guerra del Peloponeso (431 a. C.-404 a. C.). Aunque la inmensa mayoría de la guerra fue un punto muerto, Atenas sufrió varios reveses durante el conflicto. Una gran peste en el 430 a. C., seguida por una campaña militar desastrosa llamada la expedición a Sicilia, debilitó severamente a Atenas.[28]​ Esparta provocó una rebelión entre los aliados de Atenas, debilitando aún más la capacidad ateniense de hacer la guerra. El momento decisivo llegó en el 405 a. C. cuando Esparta cortó las provisiones de grano del Helesponto a Atenas. Obligada a atacar, la armada ateniense paralizada fue decisivamente vencida por los espartanos bajo el mando de Lisandro en Egospótamos. En 404 a. C. Atenas demandó la paz y Esparta dictó un acuerdo previsiblemente severo: Atenas perdió sus murallas (incluyendo los Muros Largos), su armada y todas sus posesiones en ultramar.
Entonces Grecia empezó el siglo IV a. C. bajo hegemonía espartana, pero estaba claro desde el principio que era débil. Una crisis demográfica privó a Esparta de parte de su población, y para 395 a. C. Atenas, Argos, Tebas y Corinto sentían que podían desafiar el dominio espartano, resultando en la guerra de Corinto (395-387 a. C.). Otra guerra llena de puntos muertos que terminó restableciendo el statu quo.
La hegemonía espartana duró 16 años más hasta que, al tratar de imponer su voluntad sobre los tebanos, los espartanos sufrieron una derrota decisiva en Leuctra (371 a. C.). A continuación el brillante general tebano Epaminondas condujo a las tropas tebanas hacia el Peloponeso, donde otras ciudades-estado desertaron de la causa espartana. Por lo tanto los tebanos pudieron marchar a Mesenia y liberar a la población. Privada de sus tierras y sus siervos, Esparta declinó y se convirtió en una potencia de segunda clase. La nueva hegemonía tebana duró poco tiempo; en la batalla de Mantinea en el 362 a. C., Tebas perdió a su líder clave, Epaminondas, y muchísimas tropas, aunque salió victoriosa en la batalla. De hecho, todas las ciudades-estado perdieron bastantes hombres, de manera que ninguna pudo restablecer su dominio.
La situación de debilidad de la Grecia central coincidió con el surgimiento de Macedonia, encabezada por Filipo II. En veinte años Filipo había unificado su reino, mientras lo ampliaba hacia el norte y el oeste a costa de tribus ilirias y conquistaba Tesalia y Tracia. Sus éxitos en parte se debían a sus muchas innovaciones militares. Filipo solía intervenir en los asuntos de las ciudades-estado del sur, culminando en su invasión de 338 a. C. Al derrotar decisivamente al ejército aliado de Tebas y Atenas en la batalla de Queronea, se convirtió en el hegemón de facto de toda Grecia. Obligó a la mayoría de las ciudades-estado a unirse a la Liga de Corinto, aliándose con ellas y previniendo que lucharan entre sí. Luego Filipo entró en guerra contra la dinastía aqueménida (persas), pero fue asesinado por Pausanias de Orestis al comienzo del conflicto.
Alejandro Magno, heredero de Filipo, prosiguió la guerra. Alejandro derrotó a Darío III de Persia y desmanteló completamente la dinastía aqueménida, anexionándola a Macedonia y ganándose el epíteto de «Magno». A la muerte de Alejandro en el 323 a. C., el poder y la influencia de Grecia estaban en su apogeo. Sin embargo hubo un cambio fundamental, fuera de la fuerte independencia y la cultura clásica de las polis, y hacia la cultura helenística en vías de desarrollo.
El período helenístico duró desde 323 a. C., cuando terminaron las guerras de Alejandro Magno, hasta la conquista de Grecia por la república romana en el 146 a. C. Aunque el establecimiento del dominio romano no rompió la prolongada continuidad en la sociedad y la cultura helenísticas —que se mantendrían en la misma forma básica hasta la llegada del cristianismo— sí señaló el final de la independencia política griega.
Durante el período helenístico la importancia de «la propia Grecia» (es decir, el territorio de la actual Grecia) se redujo bruscamente en el mundo grecoparlante. Los grandes centros de la cultura helenística eran Alejandría y Antioquía, las capitales de Egipto ptolemaico y Siria seléucida respectivamente.[29]​
Las conquistas de Alejandro tuvieron varias consecuencias para las ciudades-estado griegas. Ampliaron enormemente las fronteras de los griegos y acabaron en una emigración continua, especialmente de los jóvenes y los ambiciosos, hacia los nuevos imperios griegos al este.[30]​ Muchos griegos emigraron a Alejandría, Antioquía y a las muchas otras ciudades helenísticas nuevas que se fundaron en la estela de Alejandro, tan lejos como los actuales Afganistán y Pakistán, donde sobrevivieron los reinos grecobactriano e indogriego hasta finales del siglo I a. C., culminando así una era histórica donde las culturas de Europa, África y Asia se fusionaron, las rutas comerciales y el intercambio cultural tuvieron una extensión sin precedentes.
Después de la muerte de Alejandro y tras varios conflictos, su imperio se dividió entre sus generales, resultando en el Reino Ptolemaico (basado en Egipto), el Imperio seléucida (basado en el Levante), Mesopotamia y Persia, y la Dinastía Antigónida (basada en Macedonia). En el período intermedio, las polis de Grecia pudieron recobrar un poco de su libertad, aunque tenían que rendirle cuentas nominalmente al Reino Macedonio. Las ciudades-estado se quedaron en dos ligas: la Liga Aquea (incluyendo Tebas, Corinto y Argos) y la Liga Etolia (incluyendo Esparta y Atenas). En la mayor parte del período hasta la conquista romana, estas ligas solían estar en guerra entre sí, mientras se aliaban a partidos distintos en los conflictos entre los diádocos (antiguos generales de Alejandro, herederos de su reino).
El reino antigónida de Macedonia se implicó en una guerra con la república romana a finales del siglo III a. C. Aunque la primera guerra macedónica quedó inconclusa, los romanos siguieron haciendo la guerra con Macedonia en las denominadas guerras macedónicas. Coincidentemente con el desarrollo de la segunda guerra púnica entre Roma y Cartago, durante la primera guerra macedónica el reino antigónida, bajo Filipo V, se alió con Cartago. Dicha alianza no tuvo mayores consecuencias e, inclusive, en esta lucha entre grandes potencias como Macedonia, Roma y Cartago, algunos sectores griegos tomaron partido por Roma.[31]​
Hacia el año 168 a. C., finalizada la tercera guerra macedónica y derrotado Perseo ―heredero de Filipo V―, Macedonia fue anexada por Roma y dividida en cuatro repúblicas independientes que no tenían permitido ni el comercio ni el matrimonio entre sus habitantes.[32]​ En 150 a. C., Andrisco diciéndose hijo de Perseo de Macedonia, realizó varias ofensivas contra Roma, hasta su derrota y la conversión definitiva de Macedonia en provincia romana.[32]​
La Liga Etolia se había vuelto recelosa de la participación romana en Grecia, y se había puesto de parte de los seléucidas en la Guerra romano-siria. Cuando los romanos terminaron victoriosos, esta liga también se anexionó a la república. Aunque la Liga Aquea duró más que la Liga Etolia y Macedonia, también fue derrotada e incorporada por los romanos en el 146 a. C. ―y la rica ciudad de Corinto destruida tras un intento inútil de resistencia―, terminando Roma con la independencia de toda Grecia. La república romana había desarrollado con éxito su estrategia de dividir y enfrentar entre sí a sus adversarios, lo que posteriormente se conocería como divide et impera, locución que pasaría a la Historia en diferentes contextos.[33]​
Con la locución Grecia Romana se denomina al período de la Historia de Grecia que siguió a la victoria romana sobre los corintios tras la batalla de Corinto, en el año 146 a. C., hasta el restablecimiento de la ciudad de Bizancio y su nombramiento, por el emperador Constantino I, como capital del Imperio romano (la Nueva Roma) renombrada Constantinopla en el año 330.
La colonización política de Grecia por parte de Roma tuvo su contrapartida en una especie de colonización cultural inversa. La cultura romana fue, de hecho, una cultura greco-romana. El griego, como idioma, se convirtió en lengua franca en el Este y en Italia. En las casas de los nobles romanos, por su parte, dicho idioma se convirtió en el usual y los niños nobles solían ser educados por preceptores griegos.[34]​
La vida interna de Grecia durante el dominio romano no se vio culturalmente afectada. Sí hubo, en cambio, modificaciones importantes en la organización de la estructura social. A la desaparición de la llamada «clase media» siguió el desvanecimiento de la diferencia clásica entre patricios y plebeyos, formándose, en cambio, una nueva capa compuesta por patricios y plebeyos ricos: la nobleza oficial, cerrada al movimiento social y aspirante a la ocupación de los mejores puestos públicos,[35]​ así como un nuevo sector financiero protocapitalista, beneficiado con la caída de las ricas ciudades comerciales de Cartago y Corinto.[35]​
Durante los siglos II y III, Grecia fue dividida en provincias, que incluían a Achaea, Macedonia, Epiro, Tracia, y Moesia.
Aunque Grecia siguió siendo parte de la relativamente unificada «mitad levantina» del Imperio romano, durante el reino de Constantino el centro del Oriente se desplazó a Constantinopla y Anatolia. Atenas, Esparta y las otras ciudades griegas perdieron su importancia y muchas de sus estatuas y otras manifestaciones artísticas fueron llevadas a Constantinopla.
Durante la época arcaica, la población de Grecia creció fuera de la capacidad de su limitada tierra arable (según un cálculo, la población se multiplicó más de diez veces entre el 800 y el 400 a. C., desde 800 000 hasta una población total estimada entre 10 y 13 millones).[36]​ Hacia 750 a. C. los griegos empezaron 250 años de expansión, colonizando en todas las direcciones. Al este, colonizaron primero la costa egea de Asia Menor; luego Chipre y las costas de Tracia, el mar de Mármara y la costa meridional del mar Negro. Al final la colonización griega llegó tan lejos que alcanzó, al noreste, zonas de Ucrania y Rusia (Taganrog). Al oeste colonizaron las costas de Iliria, Sicilia e Italia del sur; luego Francia del sur, Córcega y aun España del noreste. También se establecieron colonias griegas en Egipto y Libia. Las actuales Siracusa, Nápoles, Marsella y Estambul empezaron como las colonias griegas Syracusae (Συρακούσαι), Neápolis (Νεάπολις), Massalia (Μασσαλία) y Byzantion (Βυζάντιον). Estas colonias desempeñaron un papel en la difusión de la influencia griega a través de Europa, y también ayudaron a establecer redes de comercio de larga distancia entre las ciudades-estado griegas, estimulando la economía en la Antigua Grecia.
La Grecia Antigua se componía de varios centenares de ciudades-estado polis que eran casi independientes. Esta situación era diferente a la de la mayoría de las otras sociedades, que eran o pueblos de pequeñas cantidades de personas o reinos soberanos de territorios extensos. Sin duda, la geografía de Grecia —que está dividida y subdividida por colinas, montañas y ríos— contribuía a la naturaleza fragmentada de la Antigua Grecia. Es probable que existiera una estructura política similar en las grandes ciudades-estado marítimas de Fenicia. Sin embargo, hasta cierto punto, la situación era única en la Antigua Grecia. Por un lado, los griegos antiguos no dudaban que eran «un pueblo singular»; compartían la misma religión, la misma cultura básica y la misma lengua, además de ser muy conscientes de sus orígenes tribales; Heródoto pudo clasificar las ciudades-estado por tribu. Por otro lado, aunque existían estas relaciones del más alto nivel, parece que rara vez jugaban un papel en la política griega. La independencia de las polis se defendía con fiereza; los antiguos griegos rara vez contemplaron la unificación de Grecia. Aun cuando un grupo de ciudades-estado se aliaron para defender Grecia durante la segunda invasión persa, la inmensa mayoría de los estados permanecieron neutrales y tras derrotar a los persas los «aliados» volvieron a sus luchas internas.[37]​
Las mayores peculiaridades del sistema político en la Grecia Antigua eran:
Las rarezas del sistema griego son más evidentes en las colonias que los griegos establecieron alrededor del Mar Mediterráneo. Aunque cada una podía considerar a cierta polis griega como su «madre» (y mantenerse amable o parcial con ella), era enteramente independiente de la ciudad que la fundó.
Inevitablemente, las polis menores podían ser dominadas por sus vecinos mayores, pero las conquistas y los reinados directos fueron bastante raros. Al contrario, las polis se organizaban en ligas, cuyos afiliados estaban en un estado constante de cambio. Después, en el período clásico, el número de ligas decreció y las ligas se hicieron mayores. Cada una era dominada por una única ciudad (por ejemplo Atenas, Esparta o Tebas), y muchas veces una polis era obligada a afiliarse a una liga bajo la amenaza de la guerra (o bajo las condiciones de un tratado de paz). Aún después de que Filipo II de Macedonia «conquistara» los centros de la Antigua Grecia, no trató de anexionar el territorio ni lo unificó en una provincia nueva; simplemente obligó a la mayoría de las polis a unirse a su propia Liga de Corinto.

Parece que al principio muchas ciudades-estado griegas eran reinos menores; muchas veces había un funcionario municipal que cumplía funciones residuales y ceremoniales del rey (basileo), e. g. el arconte basileo en Atenas.[38]​ Sin embargo, para la Época arcaica y la primera conciencia histórica, la mayoría de estas ciudades-estado ya se habían convertido en oligarquías aristocráticas. No se sabe precisamente cómo ocurrió este cambio. Por ejemplo, para 1050 a. C. en Atenas el puesto del rey se había reducido al de magistrado principal (arconte), hereditario y de por vida. En 753 a. C. se había convertido en un arcontado elegido decenalmente; y finalmente, en el 683 a. C. era un cargo elegido anualmente. En cada etapa ganaba más poder la aristocracia en su totalidad y se reducía el del individuo común.
Con el tiempo, el dominio político y la riqueza de grupos pequeños de familias propendía a provocar descontento social en muchas polis. En muchas ciudades un tirano[39]​ en cierto punto tomaba el control y gobernaba según su propia voluntad; una agenda populista solía ayudarlo a mantenerse en el poder.
Atenas cayó bajo una tiranía en la segunda mitad del siglo VI a. C. Cuando esta tiranía terminó, se propuso una reforma radical para que la aristocracia no recobrara el poder: los atenienses fundaron la primera democracia del mundo. Una asamblea de ciudadanos para la discusión de la política municipal (la Ekklesía) había existido desde las reformas de Dracón en el 621 a. C., y a todos los ciudadanos se les permitía que asistieran según las reformas de Solón (principios del siglo VI a. C.); pero los ciudadanos más pobres no podían hablar ante la asamblea o postularse como candidatos, excepto en el caso de ciertos cargos públicos cuya elección era aleatoria.[40]​ Al establecer la democracia, la asamblea se convirtió en el mecanismo de iure del gobierno; todos los ciudadanos entonces tuvieron igualdad de derechos (isopoliteia) en la asamblea. Sin embargo, los que no eran ciudadanos ―los metecos (extranjeros que vivían en Atenas) y los esclavos― no gozaban de ningún derecho político en absoluto. En Esparta existía la diarquía (gobierno de dos reyes), uno se ocupaba de los asuntos administrativos, económicos y sociales, el otro se ocupaba de la defensa.
Después del surgimiento de la democracia en Atenas, otras ciudades-estado fundaron democracias. No obstante, muchas retuvieron formas de gobierno más tradicionales. Según su costumbre en otros asuntos, Esparta era una excepción notable al resto de Grecia, y a través de la época fue gobernada no por uno, sino por dos monarcas hereditarios bajo una forma de diarquía. La monarquía espartana pertenecía a los Agíadas y los Euripóntidas, descendientes de Eurístenes y Procles, respectivamente. Se cree que los dos fundadores de sus dinastías eran hijos gemelos de Aristodemo, un soberano heráclida. Sin embargo, el poder de estos reyes era limitado tanto por un consejo de ancianos (la Gerusía) como por magistrados (los éforos) específicamente designados para vigilar a los reyes.
Guerra en la Antigua Grecia es la locución usada para describir la guerra de las polis griegas (las ciudades-estado de la Grecia Antigua), entre la revolución hoplítica del siglo VIII a. C. y el inicio del imperio macedonio en el siglo IV a. C.
Pocas civilizaciones fueron tan belicosas como las polis griegas, a pesar de que fueron sociedades poco militarizadas hasta el siglo IV a. C. Los templos tienen representaciones en sus frontones y sus frisos con dioses con indumentaria de hoplita. Los vasos cerámicos glorifican las filas de la falange.
Las estelas funerarias representan las muertes de los soldados de infantería. Platón utiliza a menudo el modelo de la guerra para ilustrar sus teorías de la virtud y del conocimiento y extrae frecuentemente sus ejemplos de la experiencia militar personal de Sócrates. Para Heródoto, Tucídides o Jenofonte, era aparentemente inconcebible relatar otras cosas. Para Sócrates, matar hombres guerreando por Atenas no se oponía a la práctica de la dialéctica o de la reflexión abstracta.[41]​
Las guerras más significativas fueron:
Solamente los hombres nativos y libres que eran dueños de tierras podían ser ciudadanos, y gozar de la protección entera de la Ley en una ciudad-Estado (si bien más tarde Pericles introdujo excepciones a la restricción sobre los nativos). En la mayoría de las ciudades-Estado, la gente que tenía importancia social no gozaba de ningún derecho especial. Por ejemplo, nacer en una cierta familia no solía ofrecer privilegios especiales. A veces, ciertas familias controlaban algunas funciones religiosas públicas, pero no solían lograr ningún poder de más en el gobierno. En Atenas, la población se dividía en cuatro clases sociales según su riqueza. La gente podía cambiar de clase por ganar más dinero. En Esparta, todos los ciudadanos varones se nombraban iguales si terminaban su educación. Sin embargo, los reyes espartanos, que servían de jefes militares y religiosos en la ciudad-Estado, venían de dos familias.
Los esclavos en la Antigua Grecia eran mayormente cautivos de guerra, o ciudadanos que infringían las leyes, lo que hoy serían los presos, solo que, a diferencia del encierro castigador actual, tampoco realizaban tareas sacrificadas como otras culturas, sino que según cada ciudad-estado, los esclavos vivían en libertad. En Esparta les eran asignadas tierras, las que trabajaban y percibían ingresos, en otras polis tenían diversos empleos, percibían dádivas, y podían criar una familia. Incluso podían obtener la libertad si un mercader los compraba y liberaba. Un esclavo célebre fue el filósofo Diógenes, apresado por falsificar monedas. Estando en la plaza mercantil, le ordena a un veedor: «véndeme a ese, necesita un esclavo que lo oriente».
Los esclavos tenían el derecho de criar una familia y ser dueños de propiedades, pero no tenían derechos políticos. Para 600 a. C. la esclavitud-mercantil se había difundido en Grecia. Para el siglo III a. C. los esclavos componían un tercio de la población entera en algunas ciudades-Estado.[47]​ Los esclavos fuera de Esparta casi nunca se sublevaron porque conformaban demasiadas nacionalidades y estaban demasiado dispersos para organizarse. O quizá no era conveniente perder sus cómodas posiciones administrando fincas por una sublevación.
La mayoría de las familias tenían esclavos como sirvientes domésticos y peones, y aun algunas familias pobres podían tener unos pocos esclavos. No se permitía que los dueños pegaran o mataran a sus esclavos. Los dueños muchas veces prometían a sus esclavos liberarlos. Los libertos (esclavos liberados) no se convertían en ciudadanos. En su lugar, se mezclaban con la población de los metecos, que incluía a la gente de países extranjeros o de otras ciudades-estado a los que oficialmente se les dejaba vivir en el estado.
Las ciudades-Estado legalmente tenían esclavos. Los esclavos públicos gozaban de una mayor independencia que los esclavos que pertenecían a las familias, viviendo solos y realizando tareas especiales. En Atenas, los esclavos públicos se entrenaban para detectar monedas falsas, mientras que los esclavos del templo actuaban como sirvientes de la deidad del templo.
Esparta tenía un tipo especial de esclavo llamado ilota. Los ilotas eran cautivos de guerra griegos que pertenecían al estado y eran asignados a familias en cuyo hogar eran obligados a permanecer. Los ilotas cultivaban alimentos y hacían tareas domésticas para que las mujeres pudieran centrarse en criar hijos fuertes mientras los hombres se dedicaban a entrenarse para ser hoplitas.
Los griegos daban vital importancia a la vida atlética, desarrollaron una notable variedad de disciplinas, muchas vigentes hoy en día, como la jabalina, y el lanzamiento de disco. Contaban con espacios destinados a tal fin y con el tiempo fueron celebrándose festividades que nucleaban tanto a hombres como mujeres. Para estos juegos las polis aportaban atletas que representaban a su ciudad. Entre tantos otros, en la vida cotidiana tenían especial adhesión los juegos con pelota.
Las fuentes señalan el más popular al episkyros, que se lo refiere como el antecesor más claro del fútbol.[48]​ Se jugaba con una pelota de cuero llamada folis, pintada con colores brillantes,[48]​ donde se enfrentaban dos equipos de entre 12 y 14 jugadores cada uno, y según las reglas se permitía también el uso de las manos.[49]​ A pesar de que era un juego de pelota, era violento, al menos en Esparta.[50]​
Sobre los juegos en equipos con pelota, existen relatos de Homero, relatando tipos de fouls (o tacles), Antífanes sobre la habilidad del engaño y amagues en los pases, Galeno señalaba sobre la importancia de la estrategia y la comparación con la guerra, o bien Plutarco sobre Alejandro Magno, quien recibió quejas de un compañero de equipo porque nunca le pasaba la pelota. Los relatos dirigen directamente a un lenguaje que se remonta al fútbol, o bien al rugby, y en tales casos, revelan que la pasión por los deportes con pelota no es una cosa actual, sino de antaño.
Había otros deportes que practicaban los griegos que se jugaban con pelota. Existía un juego similar llamado φαινίνδα - faininda,[51]​ llamado «juego engañoso», que provenía del verbo φενακίζω - fenakizo, «(I) engañar, mentir».[52]​) Otros fueron: ἀπόῤῥαξις (aporrhaxis) («juego de pelota de rebotes»), οὐρανία (ourania), «lanzar pelotas altas»[53]​ y quizás el σφαιρομαχία (sphairomachia), literalmente «batalla del balón»,[54]​ de σφαῖρα (sphaira) «pelota, esfera»[55]​ y μάχη (mache), «batalla»,[56]​ aunque se ha argumentado que el σφαιρομαχία, es de hecho una competición de boxeo —las esferas eran en realidad un tipo de guantes.—[57]​
Los inicios de los certámenes atléticos panhelénicos tienen como posible origen la conmemoración de las batallas en honor a sus caídos, y tienen cita en los juegos fúnebres, celebrados por Aquiles en la Guerra de Troya, relatada por Homero. En las mismas, tenían lugar simulacros de combates, competencias de carros, lucha, carreras de a pie y otros. A continuación, una breve descripción de los distintos certámenes deportivos (y artísticos) que se celebraban en la Grecia Antigua.
Los Juegos Olímpicos (en griego: Ολυμπιακοί αγώνες ; Olympiakoi Agones) fueron una serie de competiciones atléticas disputadas por representantes de diversas ciudades estado de la Grecia Antigua. Los registros indican que comenzaron en el 776 a. C. en Olimpia (Grecia), y se celebraron hasta el 393 d. C. Los Juegos se disputaban normalmente cada 4 años o una Olimpiada, que era una unidad de tiempo. Durante la celebración de los Juegos se promulgaba una tregua o paz olímpica, para permitir a los atletas viajar en condiciones de seguridad desde sus polis o ciudades-estado hasta Olimpia. Los antiguos Juegos Olímpicos fueron bastante diferentes de los modernos; había menos eventos y sólo los hombres libres que hablaban griego podían competir, además de que se celebraban siempre en Olimpia, en vez de moverse a diferentes lugares cada vez.
Cuando los niños varones griegos libres cumplían doce años ingresaban en la palestra, donde se les enseñaba a desarrollar los músculos y a disciplinar los nervios. A los dieciséis años entraban al gimnasio, donde los griegos realizaban ejercicios físicos y atletismo. Los gimnasios contaban con una pista y lugares de ejercicio al aire libre entre los bosques. A los veinte años los griegos concluían su formación deportiva donde se les entregaban las armas y estaban capacitados para participar en los Juegos Olímpicos.
Los últimos Juegos Olímpicos de la Antigüedad se celebraron en el 393 d. C., casi doce siglos después de sus comienzos.[58]​ Tras la adopción obligatoria del cristianismo como religión oficial en todo el imperio romano (Edicto de Tesalónica, año 380 d. C.), el emperador hispanorromano Teodosio I finalmente prohibió toda celebración y culto pagano, incluyendo los Juegos. Se restablecieron tras 1500 años, ya en la era moderna y con carácter internacional.
Las Panateneas (en griego antiguo Παναθήναια/Panatếnaia) eran unas fiestas religiosas que se llevaban a cabo todos los años en Atenas dedicadas a Atenea, diosa Poliada (protectora de la ciudad), y que tenían lugar durante algunos días del mes de hecatombeón (primer mes en el calendario ático) equivalente a finales del mes de julio actual o principios de agosto. Eran las celebraciones religiosas más antiguas e importantes de Atenas. Había desfiles militares desde el Cerámico hasta la Acrópolis pasando por el Ágora. Las Grandes Panateneas fueron remodeladas o creadas en 566 a. C., bajo el arconte Hipoclides o, según otras fuentes, por Pisístrato.[59]​[60]​ Esta reorganización, inspirada en los Juegos Píticos, incluía competiciones deportivas además de certámenes de poesía y música. Las competiciones eran exclusivas para los atenienses y otras abiertas para todos los griegos. Entre estas últimas, se incluían carreras (estadio, diaulo, dólico y carreras de hombres armados), pugilato, lucha, pancracio, pentatlón, lanzamiento de jabalina a caballo y carreras de cuadrigas de muy diversas modalidades. Los vencedores de los certámenes artísticos (recitado y música) eran premiados con una corona de oro y con dinero.[61]​[62]​
La crónica de Paros indica que los Juegos Nemeos fueron fundados en el 1251 a. C.[63]​ Los Juegos Nemeos eran una de las competiciones deportivas panhelénicas que se disputaban en la Antigua Grecia, en una sede ubicaba en la Argólide denominada Nemea. En su origen se trataba de unos juegos fúnebres y los jueces iban vestidos de luto (color negro).[64]​ Cerca del lugar donde se celebraban estos juegos se hallaba el templo de Zeus Nemeo. Desde su nacimiento tuvieron lugar cada dos años en el mes de julio, en el segundo y cuarto año de la olimpiada. El programa tenía competiciones atléticas (carreras, pentatlón, pancracio, pugilato, lucha),[65]​ hípicas y musicales.[66]​ El estadio podía albergar hasta 40 000 espectadores. Había tres categorías de competidores: niños, jóvenes y adultos. Según Eusebio de Cesarea, las competiciones se iniciaron en el 573 a. C., que es la fecha en la que se considera que adquirieron un carácter panhelénico. A las competiciones gímnicas e hípicas se añadieron las musicales en el periodo helenístico.
Los Juegos Píticos fueron uno de los cuatro Juegos Panhelénicos con los de Olimpia, los Juegos Nemeos y los Juegos Ístmicos. Fueron realizados en el santuario de Delfos, y eran consagrados a Apolo; se daba como premio una corona de laurel. Según la Crónica de Paros, en 590 a. C.[67]​ ya tuvo lugar un agon gymnikos khrematites, es decir, una competición gimnástica con premios de gran valor procedentes de botines de guerra, pero desde 582 a. C. fue un agon stephanites, una competición con una corona de laurel como único premio para el vencedor.[68]​ Esta fecha marca el comienzo oficial de la era de los Juegos Píticos. Después de las competiciones musicales se celebraban las competiciones deportivas: estadio, carrera larga (dólico, 24 estadios de 178 metros), carrera doble (diaulo, de dos estadios), pancracio, lucha, pugilato, carrera con armas, pentatlón (cada atleta se presentaba a las pruebas de carrera, salto de longitud, lucha, lanzamiento de disco y de jabalina). Se introdujo una nueva categoría, los «imberbes» (ageneioi), cuya edad se situaba entre la categoría juvenil y la adulta. Se registran victorias de mujeres pero se desconoce si la participación de las mujeres en estos juegos tenía carácter habitual o era esporádica. Comenta Pausanias, que añade que Hesíodo no pudo tomar parte en la prueba porque no se sabía acompañar con la cítara, y que Homero no participaba porque ya había quedado ciego.[69]​
Los Juegos Ístmicos fueron unos Juegos Panhelénicos de la Antigua Grecia, llamados así porque se celebraban en el istmo de Corinto, en honor de Poseidón.[70]​[71]​ El santuario panhelénico de este dios en Istmia fue acondicionado para darles acogida. La excepcional situación geográfica de Corinto «enclavada entre dos mares», en el estrecho istmo que une las dos partes de la Grecia continental, contribuyó al éxito y a la importancia política de los Juegos Ístmicos, con Poseidón y Melicertes como figuras claves. Los Juegos Ístmicos tenían lugar cada dos años en primavera y duraban varios días. El programa abarcaba certámenes gímnicos (carrera, pugilato, pancracio, pentatlón) e hípicos.[72]​ Además, cuando en el siglo IV a. C. se construyó el teatro se añadieron competiciones musicales y poéticas y es posible que incluso hubiera un concurso de pintura. Se registran victorias de mujeres tanto en competiciones atléticas como poéticas y musicales pero se desconoce si la participación de las mujeres en estos juegos tenía carácter habitual o era esporádica. Durante los juegos se celebraban rituales religiosos que incluían libaciones, sacrificios y una procesión en honor de Poseidón, Anfítrite, Leucótea y Palemón.[73]​
Los juegos hereos (en griego τὰ Ἡραῖα "ta Hêraia") de la Grecia Antigua eran concursos deportivos organizados en Argos y en Olimpia en honor de la diosa Hera y reservado a las mujeres. Pueden considerarse el antepasado del deporte femenino de competición y la versión femenina de los juegos olímpicos antiguos. Son mencionados por Pausanias.[74]​ Pausanias relata la tradición de que en Olimpia los juegos hereos habían sido instituidos por Hipodamía como acción de gracias a Hera por su boda con Pélope, aunque también señala un origen histórico de estos juegos en torno al 580 a. C: el tirano de Pisa (Grecia), Damofonte, había causado muchos males a los habitantes de Élide por lo que, a su muerte, para tratar de reparar los males causados se formó un grupo de 16 mujeres (una por cada ciudad que había entonces en Élide). Estas mujeres establecieron los Juegos Hereos y cada cinco años tejían un peplo para Hera. Los juegos estaban organizados por las denominadas «dieciséis mujeres» y consistían en carreras de muchachas donde había tres categorías de edades. Se corría con el pelo suelto y las participantes vestían una túnica que llegaba hasta un poco más arriba de la rodilla y dejaba al descubierto la zona del hombro derecho hasta el pecho.[75]​
En la mayor parte de la historia griega, la educación fue privada, salvo en Esparta. Durante el período helenístico, algunas ciudades-estado establecieron escuelas públicas. Solamente las familias adineradas podían contratar un maestro. Los niños varones aprendían a leer, escribir y citar la literatura. También aprendían a cantar y tocar un instrumento musical, y a entrenarse como soldados para el servicio militar. Estudiaban no para trabajar, sino para convertirse en buenos ciudadanos. Las niñas también aprendían a leer, escribir y hacer la aritmética elemental para dirigir el hogar. Casi nunca recibían ninguna educación después de la niñez.
Los niños entraban en la escuela al cumplir siete años, o iban a los barracones si vivían en Esparta. Los tres tipos de enseñanzas eran: grammatistes para la aritmética, kitharistes para la música, y paedotribae para los deportes.
Un niño de una familia adinerada que asistía una escuela privada era cuidado por un paidagogos, un esclavo doméstico designado para esta tarea que acompañaba el chico todo el día. Las clases se impartían en las casas privadas de los maestros e incluían aritmética, lectura, escritura, canto y ejecución de instrumentos musicales como la lira y la flauta. Al cumplir el joven doce años de edad, a los estudios se agregaban deportes como la lucha, carrera, lanzamiento de disco y de jabalina. En Atenas, algunos jóvenes mayores asistían a una academia para las disciplinas más finas como la cultura, las ciencias, la música y las artes. Un muchacho terminaba sus estudios al cumplir 18 años, luego empezaba su entrenamiento militar en el ejército por uno o dos años.[76]​
Una minoría de niños continuaban su educación después de la niñez, como en la agogé espartana. Una parte crucial de la educación de un muchacho rico era un aprendizaje con un mayor, que podían incluir el amor pederástico. El muchacho aprendía por mirar a su mentor mientras hablaba de la política en el ágora, ayudándolo a rendir sus deberes públicos, haciendo ejercicios con él en el gimnasio y asistiendo a simposios con él. Los estudiantes más ricos proseguían su educación estudiando con maestros famosos. Algunas de las mayores escuelas eran el Liceo (la llamada escuela peripatética fundada por Aristóteles de Estagira) y la Academia platónica (fundada por Platón de Atenas). El sistema educacional de los antiguos griegos ricos también se llama paideia.
A su apogeo económico en los siglos V y IV a. C., la Antigua Grecia tenía la economía más avanzada del mundo. Ya desde mucho antes del siglo V a. C. se utilizaba la moneda para el intercambio de bienes y servicios.
Esto es demostrado por el salario diario promedio de un trabajador griego que era, relacionando con los bienes (ejemplo, en términos de trigo), cercano a 12 kg de trigo diarios: más de tres veces que el salario diario promedio de un trabajador egipcio, cercano a los 3,75 kg de trigo diarios.[77]​ Platón, en La República, hace mención del inconformismo que tenían los maestros respecto a sus sueldos.
La palabra filosofía fue utilizada por primera vez por Pitágoras en el siglo VI a. C., quien ya mucho antes que los posteriores filósofos, tenía un recinto dedicado a la investigación y al saber, conocido hoy como la hermandad pitagórica. Allí se establecieron los fundamentos matemáticos que se utilizan hoy en día, se realizaban estudios sobre la astronomía, y todo cuanta ciencia requiriera fundamento o apertura. La filosofía griega se centraba en el papel de la razón y la investigación. Se destacan también Sócrates, Platón, y Aristóteles. En Tracia se destacaba Demócrito, considerado «el padre de la física».
La filosofía griega tiene una influencia fundamental en la filosofía y las ciencia modernas. Líneas de influencia claras y continuas se conducen desde la Antigua Grecia y los filósofos helenísticos, por los filósofos y científicos musulmanes medievales, por el Renacimiento y la Ilustración en Europa, hasta las ciencias seculares de nuestros días.
Se han destacado, entre otros, Homero, quien escribió la Ilíada y la Odisea, el poeta y fabulista Esopo, Los dramaturgos Sófocles y Aristófanes, cuyas obras se representaban en los teatros. Heródoto geógrafo e historiador.
La ciencia en la Antigua Grecia sentó las bases de la ciencia moderna.
La matemática, que es la base de todo conocimiento científico, fue cultivada por la escuela filosófica que acaudillaba Pitágoras. Destacándose tanto en geometría (recuérdese el famoso teorema de Pitágoras que permite resolver los triángulos rectángulos) como en aritmética, los números y las líneas ocuparon un lugar muy importante en sus especulaciones.
Antes del surgimiento de la medicina como ciencia, los griegos consideraban las enfermedades como un castigo de los dioses. El dios griego de la medicina era Asclepio y Apolo, y en sus templos la gente enferma les ofrecía sacrificios, pasando allí la noche con la esperanza de que al amanecer ya se hubiesen curado.
Muchas de las sustancias que usaban los antiguos egipcios en su farmacopea, fueron exportadas a Grecia y su influencia aumentó tras el establecimiento de una escuela de medicina griega en Alejandría, ciudad fundada por Alejandro Magno en Egipto tras liberarlos de Persia.
Hipócrates, el «padre de la Medicina», estableció su propia escuela de medicina en Cos y creó la Medicina Hipocrática. Una de las características de la medicina hipocrática es la teoría de los cuatro humores, que está relacionada con la teoría de los cuatro elementos (propuesta por Empédocles). También, Hipócrates y algunos contemporáneos acordaron que las enfermedades se encontraban en la sangre, por lo que empezó la práctica de extraer un poco de sangre de los brazos de los pacientes, pero en la mayoría de los casos se les recetaban diferentes hierbas. En todos los casos Hipócrates hablaba de los beneficios del agua (hidroterapia) y de las plantas.
La astronomía fue estudiada por los griegos desde tiempos antiguos. Ésta se suele dividir en dos períodos: Grecia Clásica y Helenística. Recibió importantes influencias de otras civilizaciones de la Antigüedad, y las que ejercieron mayor influencia fueron las provenientes de India y Babilonia. Durante la época helenística y el Imperio romano, muchos astrónomos trabajaron en el estudio de las tradiciones astronómicas clásicas, en la Biblioteca de Alejandría y en el Museion. Los calendarios de los antiguos griegos estaban basados en los ciclos lunares y solares. El calendario helénico incorporó esos ciclos, Un calendario lunisolar basado en ambos ciclos es difícil de aplicar, por lo que muchos astrónomos se dedicaron a la elaboración de un calendario basado en los eclipses.

El período de mayor esplendor del arte griego fue el denominado Siglo de Pericles. Los griegos consideraban a las artes un motor importante en sus vidas, especialmente la música, la poesía, el teatro, la danza y la artesanía. Así como proliferaban templos y filósofos, también proliferaban teatros, poetas y músicos. Rendían culto a un dios importante, Apolo, patrón de las Bellas Artes, a quien le dedicaron una gran cantidad de templos, siendo Delfos uno de los más importantes. Por primera vez en la historia, la música estaba conformada y teorizada. Fueron los que introdujeron los conceptos de la polifonía, estableciendo estudios de las escalas, con coros de hombres y mujeres (mixtos), utilizando instrumentos de cuerdas (cítaras, arpas, liras, y bandurrias llamada panduris), vientos (el aulos, una flauta doble), de percusión (tambores y platos de bronce). Existían asociaciones de músicos, el músico hábil gozaba de muy buena reputación en todo el territorio. Las obras artísticas que se hacían más frecuentemente eran las esculturas. Entre los escultores clásicos más destacados se encuentran Alcámenes, Mirón y Fidias.Mosaico del hydraulis, un órgano hidráulico, el primer instrumento de teclado en la historia, precursor de los instrumentos de pulsación. Inventado en el siglo III a. C. por Ctesibio de Alejandría.
Mujer ejecutando el aulos (instrumento aerófono), flauta doble.
Mujer tocando la pandura. siglo II a. C.
La pandura, antecesor del laúd y la guitarra. La pandura es un instrumento cordófono con la innovación de mástil y trastes. De su nombre proviene la bandurria. Edad del bronce, siglo XIV a. C.
Himno délfico.

Los griegos contemplaban en su mitología y teología como la base para el desarrollo en casi todas las áreas, desde la guerra (Atenea y Ares), hasta la música y el deporte (consagrados a Apolo). La mitología griega se compone de relatos y escritos en formato de versos, sobre los orígenes, la naturaleza del mundo, y la importancia de la tradición religiosa. Cabe destacar que en la Antigua Grecia no existía una obligatoriedad de profesar ningún culto religioso. Los dioses griegos más importantes eran los doce olímpicos. Se hallan abundantes escritos en la Ilíada, la Odisea, y Teogonía
Otras deidades importantes eran:
Los padres de Zeus eran Crono y Rea que también eran los padres de Poseidón, Hades, Deméter, Hestia y Hera. Este culto permanece activo en la actualidad como tradición religiosa, resurgida en Grecia y otras partes del mundo hace pocos años, el helenismo.
Como no existían leyes que prohibiesen las distintas elecciones sexuales (ya que en su mitología tampoco había nada que lo prohíba), los ciudadanos de cada polis (ciudad) tenía derecho a elegir su camino sexual a su antojo, en las polis existían leyes de edades mínimas, donde era importante que no ocurra en relaciones de parentesco, o de manera abusiva.
En Esparta, las mujeres casadas podían tener amantes varones, siempre y cuando fuese más apuesto, más fuerte y más joven que su esposo. En Grecia existían las hétere (ἑταῖραι), era el nombre que recibían en la Grecia Antigua las cortesanas (una combinación de dama de compañía y prostituta refinada). Eran mujeres independientes y de gran prestigio social. El colectivo estaba formado principalmente por antiguas esclavas y extranjeras, y eran célebres por su preparación para la danza y la música, así como por su aspecto físico. Existen evidencias de que pagaban impuestos, recibían educación, y podían participar en los simposios (συμπόσιον), siendo sus opiniones y creencias muy respetadas por los hombres.
Demóstenes, en su Contra Neera, escribió «Tenemos a las heteras para darnos placer, a las criadas para que se hagan cargo de nuestras necesidades corporales diarias y a las esposas para que nos den hijos legítimos y sean fieles centinelas de nuestras casas».
Algunas heteras famosas fueron:
Otra cortesana, Arqueanasa, a quien Platón le escribió los siguientes versos: «Poseo a Arqueanasa Colofonia, sobre cuya rugosa y senil frente, acerbo amor se esconde ¡Míseros de vosotros que gozásteis su juventud primera! ¡Oh cuán activo ardor sufrir debisteis!».[85]​
La prostitución fue, desde la Época Arcaica, una actividad común en la vida cotidiana de las ciudades griegas más importantes. Particularmente en las zonas portuarias, daba trabajo, de forma legal, a un número significativo de personas, constituyendo una actividad económica de primer nivel. Ejercida mayormente por mujeres de todas las edades, la clientela era generalmente masculina. Se atribuye a Solón la creación en Atenas de burdeles estatales a precios moderados.
En cuanto a los templos sagrados, un aspecto universal del culto de Afrodita y sus predecesoras que muchos mitógrafos de los siglos XIX y XX han omitido[86]​ es la práctica de la prostitución religiosa en algunos santuarios y templos determinados. El eufemismo griego para estas prostitutas es hieródula, ‘sierva sagrada’. Esta costumbre fue una práctica inherente a los rituales dedicados a las antecesoras de Oriente Medio de Afrodita, la sumeria Inanna y la acadia Istar, cuyas meretrices de los templos eran ‘mujeres de Ishtar’, ishtarium.[87]​ Esta práctica ha sido documentada en Babilonia, Siria, en ciudades fenicias y en la colonia tiria de Cartago, y para la Afrodita helénica en Chipre, el centro de su culto, Citera, Corinto y Sicilia.[87]​ Afrodita es en todas partes la patrona de las heteras y cortesanas.
La pederastia griega (del griego παιδεραστία), idealizada por los griegos desde la época arcaica, era una relación entre un o una joven adolescente (εραστες, erastes, «amada», y ἐρώμενος, erōmenos, 'amado') y un hombre adulto que no pertenecía a su familia próxima (ἐραστής, erastēs, 'amante'). Surgió como una tradición aristocrática educativa y de formación moral. Los griegos la consideraban por ello un elemento esencial de su cultura ya desde los tiempos de Homero.[88]​ Es importante señalar que la diferencia de edad entre erōmenos y erastēs es paralela a la que se daba entre los contrayentes del matrimonio en la antigua Grecia: un hombre en la treintena y una jovencita de entre quince y dieciocho años.[89]​ También cabe remarcar que el erómeno era un adolescente ya entrado en la pubertad y no un niño, como se entiende en el concepto actual de pederastia.
El término deriva de la combinación de dos vocablos griegos: παιδ- (raíz de παῖς, παιδός, 'niño/niña') y ἐραστής (erastēs, 'amante'; cf. erotismo). En un sentido más amplio, la palabra se refiere al amor erótico entre adolescentes y hombres adultos. Los griegos consideraban normal que un hombre se sintiese atraído por la belleza de un o una joven, tal como refiriese Aristóteles, el Humano siente amor espontáneo ante aquello que percibe como visualmente bello".[90]​[91]​ Sólo había controversia sobre la forma en que debía expresarse este deseo.
La pederastia estaba muy relacionada con la tradición atlética y artística de la desnudez en la gimnasia, hombres y mujeres realizaban el culto atlético de manera mixta, como así también realizaban las prácticas artísticas, como el teatro, la danza y la música.
La relación homosexual se daba entre hombres adultos y chicos adolescentes, conocida como homo pederastia. Las relaciones entre hombres de edad equivalente eran más raras. Las relaciones entre mujeres en la sociedad podían reflejarse en pasajes de la mitología griega, dando por sabido que Afrodita, cuando no disponía de su amado Ares, tenía ocasionalmente relaciones con otras diosas, existen ejemplos tan antiguos como el de Safo de Lesbos.[92]​ Sobre la homosexualidad en los ejércitos de la antigua Grecia se menciona a la tropa sagrada tebana. Homero no describe una relación sexual entre dos hombres en ninguna de sus obras ni tampoco nada sobre Aquiles y Patroclo como pareja.

El apartheid (lit. 'separación' en afrikáans)[1]​ fue el sistema de segregación racial en Sudáfrica y Namibia[nota 1]​  en vigor hasta 1992. Este sistema de segregación racial consistía en la creación de lugares separados, tanto habitacionales como académicos o recreativos, para los diferentes grupos raciales, en el poder exclusivo de las personas de piel blanca para ejercer el voto y en la prohibición de matrimonios o incluso relaciones sexuales entre blancos y negros.
Su propósito era conservar el poder para la minoría blanca (21 % de la población), que en otras condiciones habría perdido su posición de privilegio. Antes de la victoria del Partido Nacional en 1948 las personas negras podían votar, pero con muchas restricciones.
En teoría, el sistema consistía básicamente en la división de los diferentes grupos raciales para promover el «desarrollo». Todo este movimiento estaba dirigido por la raza blanca, que instauró todo tipo de leyes que cubrían, en general, aspectos sociales. Se hacía una clasificación racial de acuerdo a la apariencia, la aceptación social o la ascendencia. Este nuevo sistema produjo revoluciones y resistencias por parte de los ciudadanos no blancos del país.
A finales de los años 1980, en el marco de la guerra de la frontera de Sudáfrica (Namibia y Angola), la Unión Soviética retiró su apoyo económico y bélico a Angola y Cuba, haciendo inviable para ambos países proseguir la lucha; y del mismo modo, los Estados Unidos cesaron su apoyo financiero a Sudáfrica, lo cual trajo graves consecuencias al gobierno de Pretoria, entre ellas el principio del fin del régimen del apartheid en el sur de África, que estuvo en vigor hasta los años 1990, siendo en 1992 la última vez en que solo votaron plenamente las personas blancas.
El sistema fue efectivamente practicado en Sudáfrica durante siglos por los colonos blancos de origen neerlandés (los afrikáner) contra la población negra de este territorio, pero desde la instauración de la Colonia del Cabo en 1814 por el Reino Unido de Gran Bretaña e Irlanda, carecía de respaldo jurídico en las normas impuestas por los británicos.
Asimismo, el abierto racismo de la mayoría de la población blanca de origen afrikáner no era compartido por la minoría de blancos de origen británico, ni por las autoridades coloniales designadas desde el Reino Unido.
A pesar de que el racismo contra la población negra no tenía respaldo oficial de las autoridades británicas, durante varias décadas, y especialmente después de las dos Guerras de los Bóeres, la población afrikáner había luchado para evitar el "peligro inglés", rechazando las medidas liberales de las autoridades coloniales británicas, insistiendo en que el racismo contra los negros era parte de la lucha por la "preservación de la identidad nacional afrikáner", siendo ésta la meta esencial de su actividad política.
Después de que en 1910 la Unión Sudafricana lograra la autonomía interna dentro de la Mancomunidad británica, los políticos afrikáneres insistieron en mantener una política de segregación racial de facto, aprovechando el debilitamiento del control británico y emitiendo normas internas para frustrar el desarrollo político y económico de la población negra. Así, la presión afrikáner impidió otorgar el sufragio a los negros y vetó que estos asumieran cargos en la administración pública. Todas estas normas segregacionistas y racistas, impuestas de modo semioficial por la presión afrikáner, fueron llamadas Pequeño Apartheid.
La discriminación racial de los afrikáneres hacia la población negra no fue formalizada sino hasta 1948, fecha en la cual empezó a tomar forma jurídica al ser respaldada por leyes promulgadas a tal efecto. En las elecciones generales de 1948, el radical Partido Nacional ganó las elecciones en una coalición con el Partido Afrikáans, dirigido por el pastor protestante Daniel François Malan, oponiéndose a los candidatos más liberales que contaban con apoyo británico, todo por una perversión de la ley electoral[cita requerida] que le dio mayoría a pesar de obtener menos votos que su rival, el Partido Unido; y lo mismo ocurrió en 1953, cuando se repitió el triunfo del Partido Nacional. Malan, en su primer discurso de 1948, consideró que su triunfo electoral significaba que «Hoy día Sudáfrica vuelve a ser nuestra, Dios permita que sea nuestra siempre», entendiendo que el término «nuestra» abarcaba solamente a los blancos de origen afrikáner. Para esa fecha, la población blanca formaba el 21 % de los habitantes de Sudáfrica, siendo el 68 % de raza negra y el 11 % restante mestizos e indostanos.
Poco después del triunfo nacionalista, el gobierno de Malan emitió leyes para segregar oficialmente a cada individuo de acuerdo a su raza, estableciendo un registro racial obligatorio bajo control del gobierno. En 1949 otra ley prohibía los matrimonios interraciales y castigaba como un delito a las relaciones sexuales entre individuos de razas diferentes.
Una ley promulgada en 1951 reservaba ciertos distritos en las ciudades donde solo podían habitar los blancos, forzando a los no blancos a emigrar a otros lugares, lo cual estaba previsto para mantener la mayor cantidad posible de negros en las zonas rurales y evitar su transformación en una clase media urbana. También se prohibió para todos los efectos que la población negra pudiera comprar inmuebles dentro de los centros urbanos. Para el nuevo régimen afrikáner el "peligro étnico" ya no estaba encarnado en los británicos, sino en la población negra, a la cual se marginaba de todas las maneras posibles para conservar el país bajo dominio de los blancos.
En 1953 se establecieron zonas segregadas en las ciudades de Sudáfrica, abarcando sitios tales como playas, autobuses, hospitales, escuelas y hasta bancos en los parques públicos, siendo separados en "lugares sólo para blancos" o "sólo para negros", siendo estos últimos casi siempre de muy inferior calidad (autobuses escasos y anticuados, hospitales sobrepoblados y con escaso personal profesional, escuelas hacinadas donde la enseñanza se reducía a actividades manuales). Inclusive las ambulancias estaban segregadas, por lo cual en caso de accidente era indispensable avisar de la raza de la víctima en tanto una ambulancia "para blancos" tenía derecho de negarse a llevar un negro mientras que las ambulancias "para negros" rara vez contaban con equipo médico para emergencias. Las paradas de autobuses estaban segregadas y estaba prohibido emplear la parada de "otra raza", fuera cual fuese el motivo.
Los negros debían, por otra parte, portar documentos de identidad en todo momento y les estaba prohibido quedarse en algunas ciudades o incluso entrar en ellas sin el debido permiso de las autoridades blancas. Ese mismo año de 1953 se estableció también la segregación completa en la educación para todos sus niveles. El retiro de Malan en 1954 llevó al poder a Johannes Strijdom, quien continuó con la aplicación del apartheid.
Johannes Gerhardus Strijdom, que sucedió a Malan como primer ministro en 1954, instauró además las siguientes leyes:
Las principales consecuencias de esta situación fueron:
Ante las condenas internacionales ocurridas desde la década de 1950, los defensores del apartheid decían que la discriminación racial contra los negros estaba basada legalmente en que estos no eran ciudadanos de Sudáfrica, sino ciudadanos de otros estados independientes (llamados bantustanes), por lo cual carecían de ciudadanía sudafricana y no tenían derechos que reclamar al gobierno de Pretoria.
En efecto, desde 1960 el gobierno de Sudáfrica procedió a crear diez estados autónomos para otorgar la ciudadanía de estos a los negros que constituían el 70 % de la población sudafricana. Así, a una gran parte de la población negra se le eliminó la ciudadanía sudafricana para otorgarles la nacionalidad de algún bantustán. Gracias a este argumento, a dicha población negra se le consideraba como "transeúntes" o "población temporal" que debía circular por el territorio de Sudáfrica solamente si estaba provista de pasaportes en lugar de pases. Durante las décadas de 1960 hasta 1980, el gobierno forzó a un gran porcentaje de la población negra a reubicarse en dichos estados que habían sido designados para ellos. Un total de 3 millones y medio de individuos se vieron obligados a desplazarse hacia estas zonas para vivir allí, o en caso de que ello no fuera posible se les otorgó la nacionalidad de un "Estado" donde jamás habían vivido.
La creación de los bantustánes fue mal recibida por la ONU y en el extranjero se acusó al gobierno de Sudáfrica de "inventar" Estados solamente para privar de derechos a la población negra. Cabe destacar que todos los bantustanes fueron fundados mediante leyes del gobierno sudafricano (ninguno por decisión popular de sus habitantes) y sus fronteras habían sido diseñadas por las autoridades de raza blanca para no interferir en "zonas de interés" de los blancos. La propia realidad mostraba que hasta dos tercios de los "ciudadanos" de los bantustánes en realidad no vivían en ellos sino que residían en la "Sudáfrica blanca" como "trabajadores extranjeros" sin derechos políticos.
Hubo casos de vecindarios de población negra ubicados en las afueras de las grandes ciudades, cuyos residentes fueron expulsados de sus hogares para ejecutar proyectos urbanos en favor de la población blanca. El caso más publicitado fue el de Johannesburgo, donde en 1954 unos 60.000 habitantes negros fueron reubicados en una zona llamada Soweto. Otro caso fue el de Sophiatown, un lugar "multirracial" donde a los negros les permitían poseer tierras. Sin embargo, la expansión de la población y de la zona industrial en Johannesburgo convertía esta zona en un lugar estratégico para dicha expansión. En febrero de 1955, los cincuenta mil habitantes negros en la zona fueron evacuados a la fuerza por la policía, localizándolos en una zona denominada Meadowlands, actualmente anexa a Soweto. Sophiatown fue totalmente destruida por topadoras y se construyó una nueva urbanización llamada Triomf para la población blanca.
La población de Sudáfrica estaba clasificada en cuatro grupos. Los «de color» (en afrikáans kleurling) lo componían los mulatos provenientes de la mezcla de bantúes y khoisan con personas de ascendencia europea. La determinación de quién era catalogado como mulato a veces era un tanto difícil, llegando al extremo de examinar las encías de los individuos para distinguirlos entre negros y mulatos.
Los mulatos también fueron objeto de discriminación abierta desde 1948 y obligados a reubicarse en zonas asignadas a ellos, a veces abandonando casas y tierras que les habían pertenecido por muchas generaciones. Si bien los de color o kleurling recibían mejor trato que la población de raza puramente negra, jugaron un papel preponderante en la lucha contra el apartheid. El derecho al sufragio les era negado a los "de color" en la misma forma que a los negros.
En 1983 una reforma a la Constitución permitió a los de color e indios (estos últimos inmigrantes originarios de la India y Pakistán y establecidos desde los años de la dominación británica) participar en unas elecciones separadas para formar un parlamento de color que actuara como subordinado al parlamento de los blancos. La teoría del apartheid era que los individuos de color debían ser considerados como ciudadanos de Sudáfrica pero con derechos bastante reducidos, mientras que los negros solamente podían ser "ciudadanos" de algún  bantustán, nombre dado a unos diez estados presuntamente "autónomos" creados específicamente para albergar población negra.
Ocasionalmente, se daban casos en los que hermanos descendientes de padres de diferentes razas eran "racialmente separados" por la variación del color de su piel, lo cual obligaba a que individuos de una misma familia debieran residir y trabajar en áreas diferentes del país, imposibilitados muchas veces de visitarse, y con derechos personales muy diferentes entre sí.
La intensificación de la discriminación movió al Congreso Nacional Africano (ANC), formado por sudafricanos "negros" a desarrollar un plan de resistencia que incluía desobediencia pública y marchas de protesta. En 1955 en un congreso llevado a cabo en Kliptown, cerca de Johannesburgo, un número de organizaciones incluyendo el ANC y el Congreso Indio formaron una coalición adoptando una Carta de Libertad, que contemplaba la creación de un Estado donde se eliminara totalmente la discriminación racial.
En 1959 y 1960 un grupo del ANC decidió salirse de las filas del partido para formar otro más radical al que denominaron Partido del Congreso Africano (ACP). El principal objetivo del nuevo partido era organizar una protesta a nivel nacional en repudio a las leyes discriminatorias. El 21 de marzo de 1960 un grupo se congregó en Sharpeville, un pueblo cerca de Vereeniging para protestar contra la exigencia que los negros portaran pases. Si bien no se sabe con exactitud el número de manifestantes, lo cierto es que la policía abrió fuego contra la multitud matando a 69 personas e hiriendo a 186. Todas las víctimas eran negros y la mayoría habían sido disparados por la espalda. Seguidamente el ANC y el ACP fueron ilegalizados.
Este evento tuvo un gran significado, ya que la protesta pacífica se tornó en protesta con violencia, si bien, militarmente, los proscritos partidos políticos no eran una gran amenaza para el gobierno por carecer de una estructura armada, como sucedía en Mozambique o Angola contra el gobierno colonial portugués.
Las protestas siguieron hasta tal punto que en 1963 el primer ministro Hendrik Frensch Verwoerd declaró un estado de emergencia, permitiendo la detención de personas sin orden judicial. Más de 18.000 manifestantes fueron arrestados, incluyendo la mayoría de los dirigentes del ANC y del ACP. Las protestas tomaron en adelante la forma de sabotaje a través de la sección armada de dichos partidos. En julio de 1963 varios dirigentes políticos fueron arrestados, entre ellos Nelson Mandela. Mandela fue condenado a cadena perpetua acusado de alta traición junto con el resto de los miembros del ANC. En el Proceso de Rivonia en junio de 1964, Mandela y otros siete disidentes políticos fueron condenados por traición y sentenciados a cadena perpetua.
La declaración de Mandela en dicho juicio se hizo memorable: "He luchado contra la dominación de los blancos y contra la dominación de los negros. He deseado una democracia ideal y una sociedad libre en que todas las personas vivan en armonía y con iguales oportunidades. Es un ideal con el cual quiero vivir y lograr. Pero si fuese necesario, también sería un ideal por el cual estoy dispuesto a morir".
El juicio fue condenado en las Naciones Unidas y fue un elemento muy importante para implantar sanciones contra el régimen racista de Sudáfrica. Con los partidos de los hombres perseguidos y sus dirigentes en prisión, Sudáfrica entró en la etapa más turbia con la comunidad internacional de su historia. La aplicación del apartheid se intensificó. El primer ministro Verwoerd fue asesinado, pero sus sucesores B.J. Vorster y P.W. Botha mantuvieron sus políticas.
Durante la década de 1970 la resistencia al apartheid se intensificó. Al principio fue a través de huelgas y más adelante a través de los estudiantes dirigidos por Steve Biko. Biko, un estudiante de medicina, fue la fuerza principal detrás del Movimiento de Conciencia Negro que abogaba por la liberación de los negros, el orgullo de la raza y la oposición no violenta.
En 1974 el gobierno emitió una ley que obligaba el uso del idioma afrikáans en todas las escuelas, incluyendo las de los negros. Esta medida fue muy impopular, pues se consideraba como el idioma de la opresión blanca. El 30 de abril de 1976 las escuelas de Soweto se declararon en rebeldía. El 16 de junio de 1976 los estudiantes organizaron una marcha que terminó en violencia, donde 566 niños murieron a consecuencia de los disparos de la Policía, los cuales habían respondido con balas a las piedras que lanzaban los manifestantes. Este incidente inició una ola de violencia que se extendió por toda Sudáfrica.
En septiembre de 1977 Steve Biko fue arrestado. Las torturas a las que fue sometido fueron tan brutales que falleció tres días después de su arresto. Un juez dictaminó que no había culpables, si bien la Sociedad Médica de Sudáfrica afirmó que murió a causa de los vejámenes recibidos y la falta de atención médica. Después de estos incidentes Sudáfrica cambió radicalmente. Una nueva generación de jóvenes negros estaban dispuestos a luchar con el lema "liberación antes que educación".
Si bien la mayoría de los blancos de origen afrikáner en Sudáfrica estaban de acuerdo con mantener el apartheid, había una minoría opuesta a esto, principalmente entre los blancos de origen británico. El grupo político blanco opuesto al apartheid se centraba en torno al Partido Progresista, liderado por Helen Suzman, y luego se adhirió al Partido Progresista Federal desde 1977. Otros grupos blancos opuestos al apartheid eran el colectivo civil Black Sash y el United Democratic Front, un partido político multirracial.
En 1960 después de la Masacre de Sharpeville, Verwoerd llevó a cabo un referéndum pidiendo al pueblo blanco que se pronunciara a favor o en contra de la unión con el Reino Unido. El 52 % votaron en contra. Sudáfrica se independizó del Reino Unido, pero permaneció en la Mancomunidad de Naciones. Su permanencia en esta organización se hizo cada vez más difícil, pues los estados africanos y asiáticos intensificaron su presión para expulsar a Sudáfrica, que finalmente se retiró de la Mancomunidad el 31 de mayo de 1961, fecha en que se proclamó como una república independiente.
Al año siguiente dio comienzo la Guerra de la frontera de Sudáfrica, entre la policía primero y después las Fuerzas de Defensa de Sudáfrica, contra la SWAPO, la guerrilla independentista de Namibia. La SWAPO actuaba desde Zambia y, a partir de 1975, desde Angola. El Ejército sudafricano era el más poderoso del área y podía imponerse a cualquier país del continente por lo que decidió invadir en reiteradas ocasiones las dos naciones que daban apoyo a la SWAPO. Sin embargo, el masivo apoyo enviado por la URSS, Cuba (y en menor medida Etiopía) frenaron el avance sudafricano y comenzó una de las guerras más largas del continente, muy unida a la Guerra civil de Angola.
Al mismo tiempo el gobierno de Sudáfrica financió al grupo insurgente RENAMO para tratar de derrocar al gobierno socialista de Mozambique instaurado en 1975, alimentando así una guerra civil en dicha nación, considerando que su gobierno era frontalmente opuesto al apartheid.
Pese a las condenas contra el régimen del apartheid, muchos países de África gobernados por líderes de raza negra aceptaron ayuda financiera y tecnológica de Sudáfrica[cita requerida] e inclusive admitieron realizar proyectos económicos de infraestructura basados en capital sudafricano: tal fue el caso de Malaui y Botsuana, que pese a ello no se retractaron de sus condenas al apartheid. Otros países africanos que paulatinamente aceptaron mantener lazos comerciales y financieros con Sudáfrica fueron Ghana, Gabón, Liberia, Madagascar, Zaire, y Costa de Marfil,[cita requerida] aunque ninguno de ellos abandonó sus críticas oficiales al apartheid.
El regímen capitalista no veía con buenos ojos al gobierno comunista mozambiqueño, y menos a sus acciones. La Guerra Fría y el anticomunismo demostrado por Pretoria convertían a Sudáfrica en un buen aliado de Estados Unidos para detener la Teoría del Dominó. Los gobiernos occidentales, especialmente Estados Unidos, apoyaron así al gobierno sudafricano con armas y dinero en su guerra contra el comunismo en el sur de África, prefiriendo omitir las denuncias contra el apartheid. De esta forma las protestas de Estados Unidos y la OTAN no fueron significativas cuando el régimen de Sudáfrica comenzó su programa nuclear en 1977 (muy opuestas a cuando Libia o Irak lo intentaron), ni tampoco cuando hipotéticamente detonó su primera bomba atómica en 1979.[2]​El régimen sudafricano fue uno de los primeros en reconocer la instauración  de Israel en mayo de 1948, fue reconocido rápidamente por el gobierno sudafricano y las relaciones fueron muy fluidas siendo Israel un máximo aliado y socio estratégico. Entre principios de la década de 1970 y por veinte años, Israel fue el principal proveedor militar y asesor en temas de seguridad en Sudafrica, con un intercambio bilateral estimado en 10 000 millones de dólares.[cita requerida]
En razón de su fuerte aislamiento, Sudáfrica se vio obligada a buscar alianzas con países en situación similar de aislamiento internacional, así fue como en la década de 1970 y década de 1980 sus nuevos aliados fueron Brasil, Chile e Israel quienes debido a sus políticas internas (los dos primeros países estaban gobernados por dictaduras militares) y externas en el caso de Israel (no reconocido por sus vecinos árabes) eran rechazados en el concierto internacional, se llegó a hablar del "triángulo" (por la ubicación geográfica) Santiago-Brasilia-Jerusalén-Pretoria. No obstante, la comparativa debilidad económica de Chile y Brasil, así como la siempre difícil situación de Israel en Oriente Medio causó que esta alianza no fuera del todo útil para Sudáfrica, al no poder con ella sustituir el decisivo apoyo financiero y comercial estadounidense y británico, no obstante los votos de Chile y Brasil le eran vitales en la ONU especialmente considerando que la diplomacia chilena y brasileña eran hábiles sorteando las condenas en la Asamblea General de la ONU, razón por la cual la dictadura militar brasileña duró 21 años sin mayores problemas y la chilena 17 años en condiciones similares a la brasileña, chilenos y brasileños eran condenados de manera unilateral por gobiernos pero estos no tenían el peso para condenarles en organismos internacionales donde chilenos y brasileños, como se señaló previamente, tenían una buena diplomacia que permitía anular las pretensiones de sus adversarios.[cita requerida]
La política de apartheid promovió el aislamiento de Sudáfrica en el plano internacional que fue incrementándose con el tiempo, el cual afectó severamente la economía y la estabilidad del país. La guerra en Namibia no parecía terminar ni ganarse, y consumía gran parte de las finanzas del país, de hecho Sudáfrica invirtió grandes recursos humanos y materiales en ella y en 1987 llegó a librar en suelo namibio la Batalla de Cuito Cuanavale la mayor batalla de la historia del África Subsahariana contra tropas angoleñas y cubanas. Muchas naciones de Europa, así como Canadá y Australia, paulatinamente prohibieron a sus compañías hacer negocios con el país y hasta a los equipos deportivos de Sudáfrica les era impedido participar en campeonatos internacionales en tanto el gobierno sudafricano prohibía los elencos deportivos formados por personas de distintos grupos raciales: se mantenían "equipos para blancos", "equipos de color" o "equipos para negros" para cada selección nacional de cualquier deporte.
El apartheid motivó a que el Comité Olímpico Internacional impidiera la participación de los "equipos segregados" de Sudáfrica en los Juegos Olímpicos desde 1964 y así Sudáfrica se apartó en la práctica del movimiento olímpico. Idéntica posición adoptó la FIFA, causando la expulsión de Sudáfrica de su seno en 1963; en 1970 Sudáfrica fue excluida de la Copa Davis de tenis también por su insistencia en presentar equipos "sólo para blancos". La International Rugby Board hoy World Rugby mantuvo a Sudáfrica como afiliada durante todo el periodo del apartheid debido al excelente nivel (y gran popularidad) del rugby sudafricano, pero las giras internacionales de equipos sudafricanos (o las visitas de equipos extranjeros a Sudáfrica) siempre se vieron rodeadas de protestas políticas y diplomáticas. Como consecuencia de este ambiente negativo la Selección de rugby de Sudáfrica no disputó partidos internacionales desde 1981 (en una visita a los All Blacks neozelandeses) hasta 1994. El Gran Premio de Sudáfrica de Fórmula 1 que se disputaba oficialmente desde 1962, se corrió por última vez en 1985 cuando los equipos franceses (Ligier y Renault) boicotearon la carrera por presiones del gobierno francés. La FIA anunció más tarde que ningún campeonato sancionado por la federación y por la FISA volvería a competir en Sudáfrica por el Apartheid. La máxima categoría del automovilismo volvería en 1992 y 1993.
A inicios de la década de 1980 Sudáfrica confiaba en su riqueza en materia prima de gran valor (como oro y platino) así como en su anticomunismo, como elementos que permitirían mantener apoyo político y económico de Estados Unidos y el Reino Unido, que podían tolerar el apartheid a cambio de que el régimen sudafricano combatiera a los aliados africanos de la Unión Soviética durante la Guerra Fría, pero esta política se hizo cada vez más difícil de sostener conforme pasaban los años y el gobierno sudafricano no daba señales de modificar su política racista.
La designación del arzobispo 
anglicano Desmond Tutu como Premio Nobel de la Paz en diciembre de 1984 causó mayores condenas a Sudáfrica entre la opinión pública británica y estadounidense, cuyos gobiernos veían cada vez más difícil justificar la tolerancia a Sudáfrica solo por su calidad de "bastión anticomunista". Ello terminó forzando a que los gobiernos de EE. UU. y el Reino Unido presionaran a su vez a Sudáfrica para iniciar cambios políticos relevantes contra el apartheid, pero sin lograr mayor éxito. La Comunidad Económica Europea tampoco ocultaba su condena al apartheid, al igual que países ligados tradicionalmente a la Mancomunidad como Canadá, Nueva Zelanda, o Australia.
Debido a su política del apartheid, el COI decidió que Sudáfrica fuera excluida de manera permanente de los Juegos Olímpicos hasta que no eliminara dicha ley que discriminaba a la población de raza negra que vivía en el país (entre ellos, los deportistas). Según las normas olímpicas, el COI prohíbe que "sólo un sector" (racismo y exclusión de las mujeres) de un país participe en los Juegos Olímpicos. Y como en Sudáfrica existía el racismo hacia los negros, el gobierno sudafricano trató inútilmente de convencer al COI, de participar en los Juegos Olímpicos, pero sin eliminar el apartheid; el COI solo permitiría su participación cuando Sudáfrica hubiese eliminado dicha ley.
Sudáfrica participó por primera vez en los Juegos Olímpicos de Verano en Londres 1908, y siguió participando consecutivamente hasta Roma 1960. Sin embargo, en Tokio 1964, se le prohibió participar (debido a su política del apartheid) hasta que no cambiara su política de racismo.
Recién, en 1990, cuando Sudáfrica elimina el apartheid, se le permitió volver a participar nuevamente en los Juegos Olímpicos de Verano, en Barcelona 1992, pero no pudo utilizar su bandera, tuvo que usar una bandera especial para dicha edición. Luego, en Atlanta 1996, pudo volver a utilizar nuevamente su bandera.
Sudáfrica participó por primera vez en los Juegos Olímpicos de Invierno de Squaw Valley 1960. Pero debido a su política racista, se le negó participar en las siguientes ediciones.
Cuando en 1990, fue abolida la ley del apartheid, Sudáfrica volvería a participar nuevamente en los Juegos Olímpicos de Invierno en la edición de Lillehammer 1994, debido a que no se clasificó a Albertville 1992.
Hasta 1993, Sudáfrica era el único país del África negra gobernado por una minoría blanca. Pero desde muchos sectores de la población afrikáner las reformas se veían necesarias, aunque acarrearan la pérdida de privilegios. La economía sudafricana tenía como principales bases su producción de oro, platino y diamantes, pero el comercio internacional estaba casi paralizado para otro tipo de exportación. El crecimiento económico se había detenido en tanto el precio de la materia prima del país (como el oro) bajaba en los mercados mundiales, causando una recesión económica a mediados de la década de 1980.
El apartheid prohibía asimismo que millones de sudafricanos negros pudieran realmente integrarse a la economía nacional más que como mano de obra barata, privando a las empresas sudafricanas de un gran mercado interno potencial. Inclusive la expansión de la industria se veía frenada por la escasez de trabajadores calificados, en tanto el acceso a la educación especializada solo era permitido a los sudafricanos blancos. La urgencia de mantener un aparato militar-policial represivo consumía la mano de obra de los blancos, quienes a causa del apartheid ocupaban todos los cargos en la administración pública y las fuerzas armadas.
Así la aerolínea de bandera sudafricana produjo la campaña mostrando aeropuertos y terminales vacíos con el eslogan:
La demografía de Sudáfrica también mostraba la insostenibilidad del apartheid a largo plazo, pues la mejora en las atenciones médicas, pese a la marginación y la discriminación, permitieron un crecimiento demográfico sostenido de la población negra, el cual resultaba superior al de los blancos. De hecho, hacia 1985 los blancos ya constituían menos del 15 % de la población de Sudáfrica, mientras que en 1948 eran el 21 % de los habitantes del país, siendo cada vez más difícil en la práctica que una minoría tan exigua impusiera por la fuerza su dominio político y económico a las mayorías, peor todavía hacerlo sobre la base de criterios abiertamente racistas. Para entonces el apoyo incondicional de Estados Unidos y el Reino Unido al "bastión anticomunista" era ya escaso debido a la presión de la opinión pública mundial, contraria al único Estado del mundo que se proclamaba abiertamente racista; inclusive se habían impuesto sanciones económicas por parte de la ONU y algunas naciones también requerían la desinversión total en Sudáfrica. La moneda sudafricana, el rand, llegó a un nivel tan bajo que el gobierno se vio obligado a declarar un estado de emergencia en 1985 que se mantuvo durante cinco años.
En 1984 el presidente Pieter Willem Botha inició políticas destinadas a evitar que creciera el descontento entre la población negra, permitiendo que los "no blancos" como indostanos, mulatos, y asiáticos, instalen sus domicilios en algunas "áreas de blancos", aboliendo la prohibición de matrimonios interraciales, y tolerando las agrupaciones políticas multirraciales, pero negándose a otorgar mayores libertades a los negros. La política del apartheid creaba cada vez más controversias y oposición de la comunidad internacional, mientras que dentro de Sudáfrica diversos líderes blancos asumían seriamente que, ante una inminente crisis social y económica, y con la reprobación de casi todo el planeta, el apartheid no podría existir muchos años más.
El inicio de la perestroika y el glasnost en la Unión Soviética bajo el liderazgo de Mijaíl Gorbachov desde 1985 causó que paulatinamente el gobierno soviético se concentrase en los problemas internos de la URSS y retirase financiamiento a gobiernos aliados en todo el mundo. África no fue la excepción pues la Unión Soviética negoció públicamente con Estados Unidos poner fin a la guerra de la frontera de Sudáfrica (librada por Sudáfrica contra y Angola en el territorio de Namibia), para lo cual la URSS retiró paulatinamente su apoyo económico y bélico a Angola y Cuba, haciendo cada vez inviable para ambos países proseguir la lucha; del mismo modo los EE. UU. cesaron su apoyo financiero a Sudáfrica, lo cual trajo graves consecuencias al gobierno de Pretoria, y acelerando el fin del régimen del apartheid en el sur de África.
También la URSS retiró la mayor parte de su ayuda financiera y militar al régimen aliado de Mozambique como había hecho con Angola, alegando la urgencia de ahorrar recursos internos y eliminando así la última posible justificación para el apoyo estadounidense a Sudáfrica. Al reducirse dramáticamente la influencia soviética en el África Subsahariana desde 1987, desaparecían los pretextos para que el gobierno de Washington evitara aún actuar decididamente contra el apartheid, por lo cual altas autoridades de EE. UU. empezaron a expresar juicios abiertamente condenatorios hacia el gobierno de Sudáfrica. Hasta entonces el régimen estadounidense de Ronald Reagan rechazaba toda censura abierta al apartheid e insistía en la necesidad de apoyar a Sudáfrica en el campo económico y diplomático para combatir la "amenaza comunista" en el África subsahariana, pero la retirada política de la URSS del escenario africano eliminó toda posible excusa de EE. UU. para seguir auxiliando al régimen del apartheid; de hecho la pérdida de apoyo político, diplomático, y financiero de Estados Unidos significó para Sudáfrica un duro golpe en su ya perjudicada economía.
A lo largo de 1988 se realizaron las negociaciones de paz para el fin de la guerra de la frontera de Sudáfrica, donde ambos bandos pactaron el fin de la contienda tras presiones de la URSS y EE. UU. en tal sentido, celebrando los acuerdos finales entre Sudáfrica, Angola, y Cuba en diciembre de 1988, poniendo fin a la guerra y fijando las bases para la independencia de Namibia que se haría efectiva a inicios de 1990 bajo supervisión de la ONU. 
En medio de este contexto, en febrero de 1989, el presidente P.W. Botha sufrió un ataque de apoplejía y fue reemplazado por su ministro conservador Frederik de Klerk, a lo largo de este año en el resto del mundo la caída del Muro de Berlín y las Revoluciones de 1989 en Europa Oriental precipitaban la disolución del Pacto de Varsovia y aceleraban la retirada político-económica de la URSS del África subsahariana, lo cual no dejaba de tener efectos en la política sudafricana.
Tras una serie de choques con Botha a lo largo de 1989, De Klerk logró asumir el cargo de presidente de Sudáfrica el 20 de septiembre de 1989. Comprendiendo que el transformado escenario de la política mundial presionaba a realizar grandes cambios políticos en Sudáfrica, y que el régimen segregacionista ya no podría contar con el apoyo incondicional de EE. UU., De Klerk abandonó rápido sus posiciones conservadoras en nombre de un "realismo político" ante la situación internacional. A fines de 1989 De Klerk inició negociaciones con otros políticos blancos para poner fin al apartheid al hacerse cada vez más difícil su sostenimiento político y militar, trabajando primero para suprimir la legislación racista que había estado en vigor durante los últimos 40 años.
En su discurso de apertura del parlamento el 2 de febrero de 1990, Frederik de Klerk anunció que empezaría un proceso de "eliminación de leyes discriminatorias", y que levantaría la prohibición contra los partidos políticos proscritos incluyendo el principal y más relevante partido de oposición negro, el Congreso Nacional Africano (más conocido como ANC, del inglés ‘‘African National Congress’’), que había sido declarado ilegal 30 años antes, anunciando también el fin del estado de emergencia declarado por el expresidente Botha, una moratoria de la pena de muerte y la liberación del encarcelado líder negro Nelson Mandela, el cual fue efectivamente liberado de la cárcel nueve días después, el 11 de febrero, junto con otros 120 integrantes del ANC.
La legalización del ANC y la liberación de Mandela precipitaron la participación política de la población negra hasta entonces excluida, y entre 1990 y 1991 fue desmantelado de modo organizado el sistema legal sobre el que se basaba el apartheid, derogando paulatinamente las leyes que habían dispuesto la segregación racial desde 1948 y que habían privado de derechos políticos a la población negra, no obstante la oposición conservadora blanca reprochaba a De Klerk que no contaba con un "mandato expreso de los electores" (entendiendo que sólo los blancos podían ser electores) para tomar tales decisiones, y menos para un diálogo político con el ANC u otros grupos políticos de la población negra. En respuesta, el gobierno convocó una consulta popular para marzo de 1992, la última ocasión en que solo los blancos votaron, para determinar el apoyo o rechazo a las negociaciones para el término del apartheid; así se realizó el referéndum de 1992 donde tuvieron derecho a voto apenas 3.3 millones de blancos. El resultado indicó un apoyo al gobierno del 68% de los votantes contra un rechazo del 31%, por lo cual el gobierno de F.W. De Klerk continuó avanzando en negociaciones para una nueva constitución con el ANC y otros grupos políticos surgidos entre la población negra. 
Las mismas se prolongaron durante largos meses al surgir graves tensiones entre los sectores conservadores y liberales de la etnia afrikáner mientras aparecían también serias y violentas rivalidades entre el ANC y el partido Inkatha, de mayoría étnica zulú, además de las pugnas entre De Klerk y Mandela sobre el futuro régimen político del país, en tanto Mandela rechazaba acuerdos que mantuvieran algún favorecimiento político hacia los blancos (como escaños fijos en el Parlamento o cargos en el Ejecutivo) mientras De Klerk dudaba que un gobierno de mayoría negra surgido de la democracia representativa pudiera operar eficazmente.
Finalmente al terminar 1993 las partes llegaron a un acuerdo sobre un borrador de constitución que otorgaba derechos fundamentales a toda la población, sin distinciones raciales o étnicas, y así suprimía los últimos vestigios del apartheid para todos los efectos; también fijaron una fecha tentativa para las nuevas elecciones para el presidente y el parlamento: éstas se producirían entre el 27 y el 29 de abril de 1994, fecha en las cuales la población negra mayor de edad ejerció su derecho al sufragio por primera vez en la historia del país, gozando de plena igualdad de condiciones con el principio "un votante, un voto" y donde resultó amplio vencedor el ANC, eliminando en la práctica el apartheid siendo que el líder del ANC, Nelson Mandela, resultó elegido presidente del país.
Por extensión se denomina apartheid a cualquier tipo de diferenciación social dentro del contexto de una nación, mediante la cual un sector de la población tiene plenos derechos y otro sector se relega a un estatus de marginalidad.
Mientras la mayoría de los ciudadanos del país tienen restringidas sus libertades y derechos, una minoría privilegiada y los extranjeros pueden acceder sin límites a las opciones comúnmente generales en cualquier Estado de Derecho.
El concepto de apartheid se manifiesta en este caso en la prohibición al individuo común del derecho a disfrutar de centros turísticos y de salud exclusivos para extranjeros y personas vinculadas a la alta dirigencia de la nación, salida y entrada libre del territorio nacional, posesión de medios de comunicación como la telefonía móvil, acceso a Internet y libertad de expresión del pensamiento, así como restricciones para el acceso a empleos teniendo la capacidad física e intelectual exigida para los mismos, se antepone la obligatoriedad de pertenecer a instituciones político-ideológicas.
En 1965, después de formar parte de la Federación de Rodesia y Nyasalandia, debido al miedo de la minoría blanca gobernante, la colonia de Rodesia del Sur declaró unilateralmente su independencia el 11 de noviembre de 1965 de la mano del primer ministro Ian Smith, sin que el gobierno británico ni la Mancomunidad lo consintieran, instaurando un régimen de segregación racial muy parecido al instaurado en Sudáfrica (razón por la que se lo conoce como el mini-apartheid), aun cuando no llegaron a los mismos extremos. 
Debido a esto, el país sufrió de aislamiento internacional, prácticamente sus únicos aliados en la región eran los sudafricanos y los portugueses, quienes les proveían de dinero y armas. Desde 1964, en el país había estallado una auténtica guerra civil entre el gobierno de Rodesia y grupos guerrilleros como el ZANLA de Robert Mugabe o el ZAPU de Joshua Nkomo. Desde 1979, tras un referéndum, se empezó a instaurar de a poco un régimen multirracial, presidido por Abel Muzorewa, quien empezó a negociar con la guerrilla y con los británicos.
Finalmente, tras unas elecciones, Robert Mugabe se hizo con el poder del país, declarando la independencia de Zimbabue con reconocimiento internacional el 18 de abril de 1980, dando por finiquitado el régimen de apartheid vigente hasta entonces en el país.
Algunas películas que muestran la vida cotidiana de Sudáfrica o su historia pueden ser:


El polo norte,[1]​ también conocido como polo norte geográfico, es uno de los dos puntos de la superficie de un planeta coincidente con el eje de rotación; es opuesto al polo sur.[cita requerida] Todos los cuerpos celestes poseen un polo norte y otro sur, cuyo eje de rotación no suele ser perpendicular al eje de traslación. Así, los de la Tierra forman un ángulo de 23,5°, y los de Urano, 97°.[cita requerida]
El polo norte geográfico terrestre está situado en el océano Ártico, donde el mar está cubierto por un casquete de hielo o banquisa.[cita requerida]
Aparte del polo norte geográfico existen otros polos relacionados: el polo norte celeste, el polo de inaccesibilidad, el magnético y el geomagnético.[cita requerida]
La profundidad del mar al polo Norte ha sido medida en 4261 metros.[4]​[5]​[6]​ Se considera que la tierra más cercana es la isla Kaffeklubben, a unos 700 km de Polo (83°40′N 29°50′O﻿ / ﻿83.667, -29.833), al norte de las costas de Groenlandia.
El eje de rotación de la Tierra - y por lo tanto la posición del polo Norte - se creía que era fija (en relación con la superficie de la Tierra) hasta que, en el XVIII, el matemático Leonhard Euler predijo que el eje podría tener un ligero "balanceo". A comienzos del XX, los astrónomos observaron una pequeña "variación de la latitud", determinada fijando un punto de la Tierra mediante la observación de las estrellas. Parte de esta variación se puede atribuir a un desplazamiento de Pol a través de la superficie de la Tierra, en un rango de unos pocos metros. El desplazamiento tiene varios componentes periódicos y un componente irregular. El componente con un periodo de unos 435 días se identifica con los 8 meses de desplazamiento predichos por Euler que actualmente se denomina Bamboleo de Chandler, su descubridor. El punto exacto de intersección del eje de la Tierra y la superficie de la Tierra, en un momento dado, se denomina el "polo instantáneo", pero debido al "balanceo" esto no puede ser usado como una definición de un polo Norte fijo (o el polo Sur).
Es mejor vincular el sistema de coordenadas de la Tierra (latitud, longitud, y las elevaciones o orografía) a las formaciones terrestres fijas. Por supuesto, teniendo en cuenta la tectónica de placas y la isostasia, no hay ningún sistema en el cual se puedan fijar todos los elementos geográficos. Sin embargo, el Servicio Internacional de Rotación de la Tierra y la Unión Astronómica Internacional, han definido un marco llamado lo Sistema Internacional de Referencia Terrestre. El polo Norte de este sistema define el Norte geográfico para trabajos de precisión, sin que coincida con el eje de rotación.
Ya en el XVI, muchas personas eminentes creyeron (correctamente) que el polo Norte estaba al mar, que en el XIX se denominó Polinia o Mar polar abierto.[7]​ Por lo tanto, se esperaba que se podría encontrar un camino a través del hielo en épocas del año. Varias expediciones se propusieron encontrar el camino, generalmente en barcos balleneros, ya de uso común en las frías latitudes norteñas.
Una de las primeras expediciones creadas con la intención explícita de lograr alcanzar el polo Norte era la de un funcionario naval británico, William Edward Parry, que en 1827 llegó a la latitud 82° 45′ N. En 1871 la Expedición Polaris, un intento conducido por el estadounidense Charles Francis Hall, acabó en catástrofe. Entre 1879 y 1881 una expedición oficial de la Marina de los Estados Unidos comandada por George Washington DeLong también acabó trágicamente cuando su barco, el USS Jeanette, fue destrozado por el hielo. Más de la mitad de la tripulación, incluidos los DeLong, desaparecieron.
En abril de 1895 los exploradores noruegos Fridtjof Nansen y Fredrik Hjalmar Johansen hicieron una expedición en esquíes saliendo desde un barco. La pareja llegó a la latitud 86° 14′ N antes de verse obligados a volver, se acercaron hasta 3 grados y 46 minutos del polo norte geográfico.
En 1897 el ingeniero sueco Salomon August Andrée y dos compañeros trataron de llegar al polo Norte en el globo de hidrógeno Örnen ("Águila"), pero cayeron a 300 km al norte de Kvitøya, el extremo más septentrional del archipiélago de Svalbard, y murieron. En 1930 los restos de esta expedición fueron encontrados por la expedición noruega Bratvaag.
El explorador italiano Luigi Amedeo, duque de los Abruzzi y el capitán Umberto Cagni de la Marina Real Italiana (Marina Regía) navegaron con el ballenero Stella Polare desde Noruega en 1899. El 11 de marzo de 1900 Cagni condujo una expedición por sobre el hielo y llegó a la latitud 86° 34' el 25 de abril, estableciendo un nuevo récord por encima del resultado de Nansen de 1895 por 35 o 40 kilómetros. Cagni a penas consiguieron volver al campamento, permaneciendo allí hasta el 23 de junio. El 16 de agosto el Stella Polare zarpó en dirección sur y volvió a Noruega.
El explorador americano Frederick Albert Cook, dijo que había llegado al polo Norte el 21 de abril de 1908 con dos hombres inuit, Ahwelah y Etukishook, pero fue incapaz de probarlo y su reclamación no es ampliamente aceptada. Se descubrió que su afirmación era un engaño.[9]​
La conquista del polo Norte fue durante muchos años atribuida al ingeniero de la Marina Americana Robert Peary, que afirmó haber llegado al Polo el 6 de abril de 1909, acompañado del también americano Matthew Henson y de cuatro hombres inuit llamados Ootah, Seeglo, Egingwah, y Ooqueah. Aun así, la reclamación de Peary sigue siendo controvertida. La expedición que acompañó Peary en la etapa final del viaje no incluía nadie formado en navegación que pudiera confirmar con precisión el hito, que algunos afirman haber sido particularmente descuidado a medida que se acercaban al Polo.
Las distancias y velocidades que Peary afirma haber logrado cuando la última expedición de apoyo dio marcha atrás parecen increíbles para muchas personas, puesto que son casi tres veces más grandes que las que habían hecho hasta entonces. La narración de Peary del viaje al Polo y regreso, viajando a lo largo de una línea directa - la única estrategia coherente con los datos de tiempos que aportó - se contradice con los datos de Henson que menciona desviaciones tortuosas para evitar aperturas al hielo y otros elementos peligrosos. La velocidad que informó en su última semana (135 millas náuticas en 6 días) nunca ha sido igualada en el hielo del océano Ártico, incluso en moto de nieve.
El explorador británico Wally Herbert, inicialmente partidario de Peary, investigó sus registros en 1989 y llegó a la conclusión que tenían que haber sido falsificados y que Peary no había llegado al Polo.[10]​ El apoyo a Peary llegó de nuevo en 2005, cuando el explorador británico Tom Avery y cuatro compañeros recrearon el marco de la expedición de Peary en su viaje con réplicas de madera, trineos de perros esquimales del Canadá y otros equipos, llegando al polo Norte en 36 días, 22 horas - casi cinco horas más rápido que Peary. Avery escribe en su lugar web que "la admiración y el respeto que tengo por Robert Peary, Matthew Henson y los cuatro inuits que se aventuraron hacia el norte en 1909, ha crecido enormemente desde que salimos del cabo Colúmbia. Habiendo visto por mí mismo como viajó a través de la capa de hielo, estoy más convencido de que nunca que Peary llegó al polo Norte.”[11]​
La primera alegación de un vuelo sobre el polo fue hecha el 9 de mayo de 1926, por un oficial de la marina de los Estados Unidos Richard E. Byrd y Floyd Bennett pilotando un avión Fokker trimotor. A pesar de ser verificada en aquel momento por la Marina de los Estados Unidos y un comité de la National Geographic Society, esta afirmación ha sido discutida.[12]​
De acuerdo con Standish, "Cualquier que esté familiarizado con los hechos y tenga un mínimo de razonamiento lógico no puede evitar llegar a la conclusión que ni Cook ni Peary, ni Byrd llegaron al Polo Norte; y todos lo sabían."[13]​
Según algunos, la primera vez que se puede considerar que se llegó al Polo -basándose en hechos consistentes, verificados y científicos- fue el 12 de mayo de 1926. Y fue el explorador noruego Roald Amundsen, con su patrocinador estadounidense Lincoln Ellsworth, desde el dirigible "Norge".[14]​ El "'Norge", a pesar de ser de propiedad noruega, fue diseñado y pilotado por el italiano Umberto Nobile. El vuelo se inició en Svalbard y cruzó el Océano Ártico hasta Alaska. Nobile, junto con varios científicos y la tripulación del "Norge", sobrevolaron el Polo por segunda vez el 24 de mayo de 1928 con el dirigible "Italia". El "Italia" se estrelló en su regreso del Polo, con la pérdida de la mitad de la tripulación.
La primera conquista  confirmada del polo norte sobre la superficie del hielo fue la de Ralph Plaisted, Walt Pederson, Gerry Pitzl y Bombardier, Jean-Luc, que viajó por el hielo en moto de nieve y llegó el 19 de abril de 1968.[cita requerida]
El 2 de agosto de 2007 dos batiscafos rusos Mir realizaron una inmersión en el océano Glacial Ártico, en el polo norte, e instalaron en el fondo una bandera rusa, así como una cápsula con mensaje para generaciones venideras.[cita requerida]
En 2007 el programa de televisión Top Gear de la BBC hizo un episodio especial, en el que los presentadores realizaron un viaje en coche especialmente preparado al polo norte desde Canadá, aunque no se trató del polo norte geográfico, sino la posición del año 1996 del polo norte magnético (78°35′7″N 104°11′9″O﻿ / ﻿78.58528, -104.18583).[15]​
Usando vehículos terrestres, el polo norte geográfico fue alcanzado por primera vez el 26 de abril de 2009 por la expedición rusa MLAE-2009 (МЛАЭ-2009), que consistía en dos automóviles anfibios Yemelia-1 (Емеля-1) y Yemelia-2 con remolques. La expedición fue comandada por el constructor de los vehículos, Vasili Yelagin.[16]​[17]​
El 10 de abril de 2013, el argentino Juan Benegas llegó a pie al polo norte geográfico a partir de la base rusa Barneo, distante unos 170 km del objetivo. Fue acompañado por cinco rusos, dos de los cuales fueron evacuados a los dos días de iniciada la marcha al no poder soportar las bajísimas temperaturas, además de un italiano y un belga.[18]​[19]​

La presencia de osos polares se considera rara más allá de los 82º norte, debido a la escasez de alimentos, aunque a veces se los puede seguir la pista hasta las proximidades del polo Norte, y en 2006 una expedición informó del avistamiento de un oso polar a sólo una milla (1,6 km) del Polo.[20]​[21]​ La foca anillada también ha sido vista en el Polo, así como el zorro ártico, ejemplares del cual se han observado a menos de 60 km del pol a 89° 40′ N.[22]​[23]​
En cuanto a los pájaros, cerca de Polo se han visto ejemplares de Plectrophenax nivalis, de Fulmarus glacialis y de Rissa tridactyla, aunque algunas observaciones pueden ser distorsionadas por el hecho que las aves tienden a seguir los barcos y expediciones.[24]​
También se han visto peces en las aguas del polo Norte, a pesar de que estos son probablemente pocos en número.[24]​ Un miembro del equipo ruso que bajó hasta los fondos marinos del polo Norte en agosto de 2007 informó que no había visto ninguna criatura viviente.[25]​
En el polo Norte, el Sol está permanentemente por encima del horizonte durante los meses de verano y por debajo del horizonte durante los meses de invierno. El ortus es justo antes del equinoccio vernal (alrededor del 19 de marzo), el Sol se toma tres meses para llegar a su punto más alto del cercando de los 23½° de elevación en solsticio estival (alrededor del 21 de junio), posteriormente empieza a bajar, llegando al ocaso inmediatamente después del equinoccio de otoño (alrededor del 24 de septiembre). Cuando el solo es visible en el cielo polar, parece que se mueve en un círculo hacia la derecha sobre el horizonte. Este círculo se eleva gradualmente desde cerca del horizonte justo después del equinoccio de primavera en su máxima elevación (en grados) sobre el horizonte en el solsticio de verano y después va bajando antes de desaparecer por debajo el horizonte en el equinoccio de otoño.
El crepúsculo civil es un periodo de dos semanas que se produce antes del amanecer y después del atardecer; el crepúsculo náutico es un periodo de cinco semanas que se produce antes del amanecer y después de la puesta del Sol y el crepúsculo astronómico que dura siete semanas y se produce antes del amanecer y después del atardecer.
Estos efectos son causados por una combinación de la inclinación axial de la Tierra y su revolución alrededor del sol. La dirección de la inclinación del eje de la Tierra, así como su ángulo respecto al plan de la órbita terrestre alrededor del sol continúa siendo casi constando a lo largo de un año (los dos cambian muy lentamente en largos periodos). A mediados del verano septentrional, el polo Norte está orientado hacia el sol en su máxima extensión. En el transcurso del año cuando la Tierra va cercando el sol, el polo Norte se aleja gradualmente del solo hasta que a mediados del invierno llega en su punto más alejado del sol. Una secuencia parecida se observa al polo Sur, con una diferencia de seis meses.
En la mayoría de los lugares de la Tierra, la hora local es determinada por la longitud, de manera que la hora del día es más o menos sincronizada con la posición del sol en el cielo (por ejemplo, al mediodía el sol está más o menos en su mayor altura). Esta línea de razonamiento no tiene sentido en el polo norte, donde el sol sale y se pone solo una vez por año, y todas las líneas de longitud, y por lo tanto todas las zonas horarias, convergen. No hay presencia humana permanente en el polo norte y no hay zona horaria en particular asignada. Las expediciones polares pueden utilizar cualquier zona horaria que les sea conveniente, como la hora del meridiano de Greenwich, o la zona horaria del país del que partieron.[cita requerida]
El polo norte es sustancialmente más caliente que el polo sur, ya que se encuentra a nivel del mar en medio de un océano (que actúa como un reservorio de calor), en lugar de en la altura de una masa de tierra continental.[26]​
En invierno (enero) las temperaturas en el polo norte pueden variar de aproximadamente de -43 °C a -26 °C, tal vez con un promedio de alrededor de -34 °C. Las temperaturas de verano (junio, julio y agosto) en promedio son alrededor del punto de congelación (0 °C). La temperatura más alta registrada hasta ahora es de 7,2 °C, mucho más caliente que el récord de solo -12.3 °C del polo sur.[27]​ 
El hielo marino en el polo norte es típicamente de alrededor de 2 a 3 m de espesor,[28]​ aunque el espesor del hielo, su extensión espacial y la fracción de aguas abiertas dentro de la bolsa de hielo pueden variar rápida y profundamente en respuesta al tiempo y el clima.[29]​ Los estudios han demostrado que el espesor medio del hielo ha disminuido en los últimos años.[30]​ Es probable que el calentamiento global haya contribuido a la expansión de Groenlandia por  ello, pero no es posible atribuir del todo la reciente abrupta disminución en el grosor al calentamiento observado en el Ártico.[31]​ Los informes también han pronosticado que en próximas décadas el océano Ártico estará completamente libre de hielo en el verano.[32]​ Esto podría tener implicaciones comerciales significativas, más adelante.
La retirada del hielo marino del Ártico acelerará el calentamiento global, ya que con menos cubierta de hielo refleja menos radiación solar, y esto puede tener repercusiones importantes sobre el clima mediante la contribución a la generación de ciclones árticos.[33]​
Bajo el derecho internacional, ningún país posee el polo norte o la región del océano Ártico que lo rodea. Los cinco países árticos circundantes, Rusia, Noruega, Dinamarca (Groenlandia), Canadá, y Estados Unidos (Alaska), están limitados a unas 200 millas náuticas (370 km; 230 millas) de zona económica exclusiva alrededor de sus costas, y el área más allá es administrada por la Autoridad Internacional de los Fondos Marinos.
En algunas culturas occidentales, el Polo Norte geográfico es descrito como el lugar donde vive el Papá Noel, aunque la no coincidencia del Polo Norte geográfico y el magnético. En Canadá, el Servicio postal tiene asignado un código por el Polo Norte, el H0H 0H0, en referencia a la expresión tradicional del Papá Noel "Ho ho ho"[34]​
También hay una referencia a la antigua y mitológica Hiperbórea que sitúa el Polo Norte, por encima del eje del mundo, como el tabernáculo donde hay Dios y los seres sobrehumanos (véase Joscelyn Godwin, Arktos: The Polar Myth). La figura popular de Santa Claus habitante en el Polo funciona como un arquetipo de la pureza espiritual y trascendencia.[35]​ Tal como ha documentado Henry Corbin, el Polo Norte tiene un papel fundamental en la cosmovisión cultural del Sufismo y el misticismo iraní. "El Oriente buscado por los místicos, el Oriente que no se puede localizar en los mapas, es en la dirección norteña, más allá del norte".[36]​
Debido a su lejanía, Pol se identifica a veces con una misteriosa montaña de la antigua tradición Islámica llamada Montaña Qaf (Jabal Qaf), el "punto más lejano de la tierra".[37]​[38]​ Según algunos autores, el. Jabal Qaf de la cosmología del Islam medieval es una versión de Rupes Nigra, una montaña donde la ascensión, como la ascensión al Purgatorio descrita por Dante Alighieri, representa un progreso del peregrino a través de los estados espirituales[39]​ En la teosofía iraní, el polo celeste, el punto focal de la ascensión espiritual, actúa como un imán que atrae los seres a sus "palacios quemando en materia inmaterial."[40]​
En 2007 el programa de televisión Top Gear de la BBC emitió un episodio especial, en el que los presentadores realizaron un viaje en un coche especialmente preparado al polo Norte desde el Canadá; no obstante no se trataba del polo Norte geográfico sino de la posición del polo Norte magnético en 1996.[41]​



Canadá (en inglés: Canada, AFI: /ˈkænədə/; en francés: Canada, AFI: /kanadɔ/ o /kanada/) es un país soberano ubicado en América del Norte, cuya forma de gobierno es la monarquía parlamentaria federal. Su territorio está organizado en diez provincias y tres territorios. Su capital es la ciudad de Ottawa y su ciudad más poblada es Toronto, seguida por Montreal y Vancouver. Es comúnmente considerado uno de los paises más desarrollados y con mejor calidad de vida del mundo contando con la octava economia más grande del mundo, a pesar de solo contar con un aproximado de 40 millones de personas.
Ubicado en el extremo norte del subcontinente norteamericano, se extiende desde el océano Atlántico al este, el océano Pacífico al oeste, y hacia el norte hasta el océano Ártico. Comparte frontera con los Estados Unidos al sur, al noroeste con su estado federado Alaska y al noreste con Groenlandia, territorio que forma parte del Reino de Dinamarca. Es el segundo país más extenso del mundo después de Rusia, y también el más septentrional. Ocupa cerca de la mitad del territorio de Norteamérica. A causa de su clima, es uno de los 15 países con menor densidad poblacional del mundo, con aproximadamente 4 hab/km².
El territorio ocupado por Canadá fue habitado por los diversos grupos de población aborigen durante milenios. Desde finales del siglo XV, numerosas expediciones británicas y francesas exploraron a lo largo de la costa atlántica, donde más tarde se establecieron. Francia cedió casi todas sus colonias norteamericanas en 1763 después de la Guerra Franco-india.
En 1867, con la unión de tres colonias británicas norteamericanas mediante la Confederación, Canadá se formó como un dominio federal de cuatro provincias.[5]​[6]​ Esto hizo que comenzara una acumulación de provincias y territorios, y un proceso de autonomía frente al Reino Unido. Esta autonomía cada vez mayor se puso de relieve en el Estatuto de Westminster de 1931 y culminó en el Acta de Constitución de Canadá de 1982, que rompió los vestigios de la dependencia jurídica en el parlamento británico.[7]​ Está gobernada como una democracia parlamentaria y monarquía constitucional con Carlos III como jefe de Estado. Es un país bilingüe con el inglés y el francés como lenguas oficiales en el ámbito federal.
Canadá es un país industrial y tecnológicamente pionero y avanzado, ampliamente autosuficiente en energía gracias a sus relativamente extensos depósitos de combustibles fósiles y a la amplia generación de energía nuclear y energía hidroeléctrica. Siendo uno de los países más desarrollados, tiene una economía diversificada, que la hace independiente por sus grandes yacimientos y abundantes recursos naturales así como del comercio, particularmente con los Estados Unidos y México. En la actualidad es miembro de la OEA, el G-7, el G-20, la OTAN, la OCDE, la OMC, la UKUSA, la APEC, la Mancomunidad de Naciones, la Francofonía y de la Organización de las Naciones Unidas. Es considerado uno de los países con mejor calidad de vida.[4]​
El nombre "Canadá" proviene de la raíz iroquesa kanāta que significa ‘poblado’, ‘asentamiento’ o, refiriéndose inicialmente a Stadaconé, un asentamiento en el sitio de la actual ciudad de Quebec.[8]​[9]​ El explorador francés Jacques Cartier utilizó la palabra Canada para referirse no solo a esa aldea en particular, sino también a toda el área bajo el mandato de Stadaconé; para 1545, los mapas y libros europeos habían comenzado a referirse a toda la región como Canadá.[10]​
Desde el siglo XVII en adelante, la región de Nueva Francia que se encontraba cerca del río San Lorenzo y al margen de la costa norte de los Grandes Lagos fue conocida como Canadá. Más tarde, la zona fue dividida en dos colonias británicas: Canadá Superior y Canadá Inferior, aunque en 1841 se unieron nuevamente como la Provincia de Canadá.[11]​ Tras la Confederación de 1867, el nombre de Canadá fue adoptado como el nombre legal para el nuevo país y el Dominio (un término del Salmo 72:8)[12]​ fue el título conferido al país. Combinados, el término Dominio de Canadá solía usarse hasta la década de 1950.[13]​ Como el Dominio afirmó su autonomía política del Reino Unido, el gobierno federal utilizó cada vez más el término "Canada" en documentos de Estado y tratados, un hecho que se refleja en el cambio de nombre de la fiesta nacional en 1982, cuando pasó de ser el Día del Dominio al Día de Canadá.[13]​
Los primeros habitantes del territorio que actualmente comprende Canadá fueron las «Primeras Naciones»,[14]​ los esquimales[15]​ y los métis.[16]​ Los términos «indios» y «eskimos» han caído en desuso.[17]​ Estudios arqueológicos y genéticos comprueban la presencia humana en el norte del Yukón hace 26 500 años, y en el sur de Ontario hace 9500 años.[18]​[19]​[20]​ Old Crow Flats y Bluefish Caves son los sitios arqueológicos más antiguos dejados por los primeros habitantes canadienses.[21]​[22]​[23]​ Entre las tradiciones de las «Primeras Naciones», se encuentran las ocho historias únicas que describen la creación del mundo y de sus tribus.[24]​ Estos pueblos aborígenes se caracterizan por sus asentamientos urbanos que han perdurado hasta el siglo XXI, por su arquitectura civil y monumental y por una jerarquización social compleja.[25]​ Algunas de estas civilizaciones desaparecieron mucho tiempo antes de la llegada de los europeos (siglos XV y XVI), y han sido descubiertas recientemente por las excavaciones arqueológicas.
La cultura mestiza de los métis se originó a mediados del siglo XVII, cuando algunos europeos se juntaron con aborígenes de las «Primeras Naciones».[26]​ Por su parte, durante las primeras décadas, los esquimales tuvieron un contacto más limitado con los colonizadores europeos.[27]​
Se estima que a finales del siglo XV la población aborigen estaba entre los 200 000[28]​ y los dos millones de habitantes.[29]​ Los múltiples brotes de enfermedades infecciosas traídas por los europeos como la influenza, el sarampión y la viruela (a las cuales ellos no tenían inmunidad biológica), combinados con otros efectos del contacto con los europeos, resultaron en una disminución de la población aborigen de entre un 85 % y un 95 %.[30]​ También se investiga, desde el año 2016, sobre la desaparición física de entre 1200 y 4000 mujeres indígenas y la presunción de responsabilidad por omisión del gobierno de Canadá.[31]​[32]​[33]​[34]​[35]​
Los europeos llegaron por primera vez al continente americano cuando los vikingos se asentaron brevemente en L'Anse aux Meadows en la isla de Terranova alrededor del año 1000; tras el fracaso de esa colonia, el próximo intento para la exploración del territorio canadiense se realizó en 1497, cuando el navegante italiano Giovanni Caboto (Juan Caboto) exploró la costa atlántica de América del Norte al servicio de Inglaterra. En 1534, Jacques Cartier hizo lo mismo en nombre de Francia.[36]​ El explorador francés Samuel de Champlain llegó en 1603 y estableció los primeros asentamientos europeos permanentes de la región: Port Royal en 1605 y la ciudad de Quebec en 1608.[37]​ Entre los colonos franceses de Nueva Francia, los canadiens se asentaron en el Valle de río San Lorenzo, mientras que los acadiens en las provincias marítimas actuales. Los comerciantes de pieles franceses y los misioneros católicos exploraron la zona de los Grandes Lagos, la bahía de Hudson y la cuenca del Misisipi hasta Luisiana. Las Guerras de los Castores estallaron por el control del comercio de pieles.[38]​
Los ingleses establecieron puestos de pesca avanzada en Terranova alrededor del año 1610 y establecieron las Trece Colonias al sur.[39]​ Una serie de cuatro guerras intercoloniales se desataron entre 1689 y 1763.[40]​ En 1713, la parte continental de Nueva Escocia quedó bajo dominio británico con el Tratado de Utrecht. Más tarde, al finalizar la Guerra Franco-india en 1763, con la firma del Tratado de París Francia cedió Canadá y la mayor parte de Nueva Francia a Gran Bretaña.[41]​
Artículo principal: Norteamérica británica
La Proclamación Real de 1763 separó a la provincia de Quebec de Nueva Francia y anexó la isla del Cabo Bretón a Nueva Escocia.[13]​ En 1769, la isla de San Juan (ahora la isla del Príncipe Eduardo) se convirtió en una colonia separada.[42]​ Para evitar conflictos en Quebec, los británicos aprobaron el Acta de Quebec de 1774, la cual amplió el territorio de Quebec hasta la zona de los Grandes Lagos y el Valle del Ohio. En estos lugares se restableció el idioma francés, la fe católica y el derecho civil francés. Esto enfureció a muchos residentes de las Trece Colonias e influyó en el inicio de la Revolución estadounidense.[13]​

En el Tratado de París (1783) se reconoció la independencia de los Estados Unidos, cediendo los territorios al sur de los Grandes Lagos. Alrededor de 50 000 partidarios de la ocupación inglesa huyeron de los Estados Unidos a Canadá.[43]​ Con este cambio, Nuevo Brunswick se separó de Nueva Escocia para reorganizar los nuevos asentamientos de los partidarios ingleses en las provincias marítimas. Para acomodar a los inmigrantes de habla inglesa en Quebec, la Ley constitucional de 1791 dividió la provincia en Canadá Inferior de habla francófona (más tarde la provincia de Quebec) y Canadá Superior de habla anglosajona (más tarde Ontario), concediendo a cada una el derecho de elegir su propia Asamblea legislativa.[44]​Canadá (Superior e Inferior) fue el frente principal durante la guerra de 1812, librada entre los Estados Unidos y el Imperio Británico. Tras la guerra, la inmigración a gran escala desde Gran Bretaña e Irlanda comenzó en 1815,[45]​ de tal modo que, desde 1825 a 1846, 626 628 inmigrantes europeos desembarcaron en puertos canadienses.[46]​ Antes de 1891, entre un cuarto y un tercio de todos los europeos que emigraron al país, habían fallecido a causa de las enfermedades infecciosas.[30]​ Durante el siglo XIX, la industria de la madera superó el comercio de pieles en importancia económica.
El deseo de tener un gobierno responsable resultó en las rebeliones abortadas de 1837. Posteriormente, el Informe de Durham recomendó la implantación de un gobierno responsable y la asimilación de los canadienses franceses en la cultura británica.[13]​ El Acta de Unión de 1840 fusionó las Canadás en la Provincia Unida de Canadá. En 1849, el gobierno responsable fue establecido para todas las provincias británicas en Norteamérica.[47]​ La firma del Tratado de Oregón por el Reino Unido y los Estados Unidos en 1846 puso fin a la disputa por los límites de Oregón, ampliando la frontera noroeste a lo largo del paralelo 49° N. Esto abrió el camino para la fundación de dos colonias británicas en la isla de Vancouver (1849) y en la Columbia Británica (1858).[48]​ Además, el gobierno local comenzó una serie de expediciones exploratorias para reclamar la Tierra de Rupert y la región del Ártico.
Después de varias conferencias constitucionales, el Acta Constitucional de 1867 proclamó oficialmente la Confederación Canadiense el 1 de julio de 1867, inicialmente con cuatro provincias: Ontario, Quebec, Nueva Escocia y Nuevo Brunswick.[49]​[50]​ Canadá tomó control de la Tierra de Rupert y el Territorio Noroeste para formar los Territorio del Noroeste. Los Métis sublevados llevaron a cabo la rebelión del río Rojo en 1869, creando un gobierno autónomo, aunque sofocada por la recientemente creada policía montada.[51]​ Esto llevó a la creación de la provincia de Manitoba en julio de 1870. Columbia Británica e Isla de Vancouver (quienes se unieron en 1866) se unieron en 1871, mientras que la Isla del Príncipe Eduardo se unió en 1873.[52]​
El primer ministro John A. Macdonald organiza una política descrita como «etnocida» hacia los amerindios de las llanuras centrales del país para apropiarse de sus tierras, causando intencionadamente hambrunas, ejecuciones arbitrarias y asimilación forzada de niños.[53]​
El Parlamento Canadiense aprobó un proyecto de ley presentado por el partido Conservador que estableció una política nacional de aranceles para proteger la naciente industria manufacturera.[50]​ Para poblar el oeste, se aprobó la construcción de tres tranvías transcontinentales (incluyendo el Canadian Pacific Railway), abriendo las planicies a la colonización con el Acta de Tierras, además de crear la Policía Montada para asegurar la soberanía sobre el territorio.[54]​[55]​ En 1898, durante la fiebre del oro de Klondike, se creó el Territorio del Yukón. El gobierno liberal de Wilfrid Laurier atrajo pobladores europeos para colonizar las planicies, además de crear las provincias de Alberta y Saskatchewan en 1905.[52]​
La participación militar en la Primera Guerra Mundial ayudó a fomentar un sentido de nacionalidad. El Ministerio de Guerra en 1922 reportó aproximadamente 67 000 muertos y 173 000 heridos durante la guerra. El apoyo al Reino Unido durante la Primera Guerra Mundial provocó una importante crisis política sobre el servicio militar obligatorio, con los francoparlantes, principalmente de Quebec. Durante la crisis, un gran número de enemigos extranjeros (especialmente los ucranianos y alemanes) fueron puestos bajo el control del gobierno. El Partido Liberal estaba profundamente dividido, con la mayoría de sus líderes angloparlantes uniéndose al gobierno unionista encabezado por el primer ministro Robert Borden, líder del partido conservador. Los liberales recuperaron su influencia después de la guerra bajo el liderazgo de William Lyon Mackenzie King, quien ejerció como primer ministro con tres términos separados entre 1921 y 1948.
La Ley de Votantes Militares de 1917 dio el voto a las mujeres que eran viudas de guerra, o tenían hijos o esposos sirviendo en el extranjero. Sindicalistas como el primer ministro Borden se comprometieron durante la campaña de 1917 a la igualdad de sufragio para las mujeres. Después de su aplastante victoria, introdujo un proyecto de ley en 1918 para extender el sufragio a las mujeres. Esta ley no tuvo contrarios, pero no se aplicaba a las elecciones provinciales y municipales de Quebec. Las mujeres de Quebec consiguieron el sufragio universal en 1940. La primera mujer elegida para el Parlamento era Agnes Macphail de Ontario en 1921.
Convencido de que Canadá había destacado en los campos de batalla de Europa, el primer ministro Robert Borden exigió que tuviera un asiento separado en la Conferencia de Paz de París en 1919. A esto se opuso inicialmente no solo Reino Unido, sino también los Estados Unidos. Borden respondió señalando que como Canadá había perdido casi 60 000 hombres, el derecho a la igualdad de condición como nación había sido consagrado en el campo de batalla. El primer ministro británico David Lloyd George, finalmente cedió, y convenció a los norteamericanos que se resistían a aceptar la presencia de delegaciones de Canadá, India, Australia, Terranova, Nueva Zelanda y Sudáfrica. Estos también recibieron sus propios asientos en la Liga de las Naciones. Jugó un papel modesto en París, pero solo tener un asiento era una cuestión de orgullo.
En 1931 el Parlamento británico aprobó el Estatuto de Westminster, que le dio a cada dominio de la oportunidad de independencia legislativa casi completa de Londres. Si bien Terranova nunca adoptó el estatuto, para Canadá, el Estatuto de Westminster se convirtió en su declaración de independencia.
El país fue duramente golpeado por la Gran Depresión que comenzó en 1929. Entre 1929 y 1933, el producto nacional bruto se redujo un 40 % (frente al 37 % en los Estados Unidos). El desempleo alcanzó el 27 % en 1933. En 1930, en la primera etapa de la larga crisis, el primer ministro liberal Mackenzie King creía que la crisis era temporal y que la economía se recuperaría pronto y sin la intervención del gobierno. Se negó a proporcionar alivio del desempleo o ayuda federal a las provincias, diciendo que si los gobiernos provinciales exigieran dinero federal, él no les daría «ni cinco centavos». En las elecciones de 1930 el tema principal fue el rápido deterioro de la economía. El ganador de las elecciones de 1930 fue el conservador Richard Bedford Bennett. Bennett había prometido aranceles elevados, pero a medida que aumento el déficit, tuvo que recortar severamente el gasto federal. Con la caída de apoyo y la depresión volviéndose cada vez peor, Bennett intentó introducir políticas basadas en el New Deal de los Estados Unidos, sin resultado. El gobierno de Bennett se convirtió en un foco de descontento popular. El fracaso conservador para restaurar la prosperidad dio lugar al regreso de los liberales de Mackenzie King en las elecciones de 1935.
Prometiendo un tratado comercial tan deseado con los Estados Unidos, el gobierno de Mackenzie King aprobó el Acuerdo Comercial Recíproco de 1935. Esto marcó el punto de inflexión en las relaciones económicas canadienses con Estados Unidos generando la reducción de aranceles, y produciendo un aumento dramático en el comercio.
La participación de Canadá en la Segunda Guerra Mundial comenzó cuando Canadá declaró la guerra a la Alemania nazi el 10 de septiembre de 1939, una semana después del Reino Unido, para demostrar simbólicamente su independencia. La guerra restauró la economía de Canadá y su confianza en sí mismo, ya que jugó un papel importante en el Atlántico y en Europa.
El gobierno movilizó con éxito la economía de guerra, con resultados impresionantes en la producción industrial y agrícola. La depresión terminó, la prosperidad regresó, y la economía de Canadá se expandió de manera significativa. En el aspecto político, Mackenzie King rechazó cualquier idea de un gobierno de unidad nacional. La elección federal canadiense de 1940 se llevó a cabo según lo programado, produciendo otra victoria de los liberales.
Después del inicio de la guerra con Japón en diciembre de 1941, el Gobierno, en cooperación con los Estados Unidos, comenzó el internamiento japonés-canadiense, que envió 22 000 habitantes de ascendencia japonesa a campos de reasentamiento lejos de la costa. La razón fueron los temores de espionaje o sabotaje. El gobierno ignoró los informes de los militares y de la policía montada de que la mayoría de los japoneses eran respetuosos de la ley y no una amenaza.
La batalla del Atlántico comenzó inmediatamente, y fue dirigida por Leonard W. Murray, de Nueva Escocia. Submarinos alemanes operaron en aguas de Canadá durante toda la guerra, hundiendo muchos buques de guerra y mercantes. El ejército canadiense participó en la defensa fallida de Hong Kong, la infructuosa incursión de Dieppe en agosto de 1942, la invasión aliada de Italia, y la invasión de gran éxito de Francia y los Países Bajos.
Del aproximadamente 1,1 millones de canadienses que sirvieron en las fuerzas armadas en la Segunda Guerra Mundial, más de 45 000 murieron y otros 55 000 resultaron heridos.
El Dominio de Terranova (ahora Terranova y Labrador), en ese tiempo un dominio con un gobierno similar al de Australia, se unió a Canadá en 1949.
Entre finales del siglo XIX y 1996, más de 150 000 niños aborígenes fueron separados de sus familias y colocados en escuelas residenciales religiosas. Muchos niños murieron allí debido a la falta de asistencia sanitaria. Además, entre los años sesenta y ochenta, 20 000 niños aborígenes fueron secuestrados de sus familias y colocados con familias no aborígenes.[53]​
El crecimiento del país, combinado con las políticas de los sucesivos gobiernos liberales, dieron lugar a la aparición de una nueva identidad canadiense, que se caracterizó por la aprobación de la actual bandera de la hoja de arce en 1965,[56]​ la aplicación del bilingüismo oficial (inglés y francés) en 1969[57]​ y el multiculturalismo oficial en 1971.[58]​ También se crearon programas impulsados por los socialdemócratas, tales como la asistencia sanitaria universal, el plan de pensiones y los préstamos a estudiantes; algunos gobiernos provinciales, especialmente los de Quebec y Alberta, se opusieron a muchas de ellas, tales como las incursiones federales en sus jurisdicciones.[59]​ Por último, en 1982 otra serie de conferencias constitucionales dio como resultado la «patriación» de la Constitución de Canadá desde el Reino Unido, simultáneo con la creación de la Carta de los Derechos y Libertades.[60]​ En 1999, Nunavut se convirtió en el tercer territorio de la federación tras una serie de negociaciones con el gobierno federal.[61]​
Al mismo tiempo, Quebec fue experimentando profundos cambios sociales y económicos a través de la Revolución Tranquila, dando lugar al nacimiento de un movimiento nacionalista en la provincia y a un frente más radical llamado el Frente de Liberación de Quebec (FLQ), cuyas acciones provocaron la Crisis de octubre en 1970.[62]​ Una década más tarde, en 1980, se celebró un fallido referéndum sobre la soberanía/asociación de la provincia.[62]​ En 1990, hubo intentos fallidos de realizar una enmienda constitucional, seguidos por un segundo referéndum en 1995, en el que la soberanía fue rechazada por un 50,6 % en contra y un 49,4 % a favor.[63]​ En 1997, la Corte Suprema dictaminó que la secesión unilateral por una provincia sería inconstitucional, por lo que el parlamento aprobó la Ley de Claridad, que describe los términos de negociación para que una provincia se retire de la Confederación.[63]​
Además de las cuestiones de soberanía de Quebec, una serie de crisis sacudió la sociedad canadiense en la década de 1980 y principios de la década de 1990. Estos incluyen la explosión del Vuelo 182 de Air India en 1985, la masacre de la Escuela Politécnica de Montreal en 1989 y la Crisis de Oka en 1990, la primera de una serie de violentos enfrentamientos entre el gobierno y los grupos aborígenes.[64]​ Ese mismo año, Canadá se unió a la Guerra del Golfo como parte de una fuerza de coalición dirigida por Estados Unidos y participó en varias misiones de «mantenimiento de la paz» durante el resto de la década. Aunque envió tropas hacia Afganistán en 2001, se negó a enviar tropas a Irak cuando fue invadido por Estados Unidos en 2003.[65]​
Canadá tiene un gobierno parlamentario con fuertes tradiciones democráticas. El parlamento está compuesto por una Cámara de los Comunes electa por el pueblo y un Senado designado. Cada miembro del parlamento en la Cámara de los Comunes es elegido por mayoría simple en un distrito electoral o su equivalente. El primer ministro es quien convoca a elecciones generales, las cuales deben ser máximo cinco años después de la elección anterior, o pueden ser desencadenadas por el gobierno al pedir la moción de censura en el parlamento.[66]​
Los miembros del Senado, cuyos escaños son repartidos según cada región, son elegidos por el primer ministro y formalmente nombrados por el Gobernador General y pueden estar en funciones hasta la edad de 75 años.[67]​ En las elecciones de 2015, cinco partidos contaban con representantes elegidos para el parlamento federal: el Partido Liberal de Canadá (partido gobernante), el Partido Conservador de Canadá (la oposición oficial), el Nuevo Partido Democrático (NDP), el Bloc Québécois y el Partido Verde de Canadá.
La estructura federal divide las responsabilidades del gobierno entre el gobierno federal y el de las diez provincias. Las legislaturas provinciales son unicamerales y operan de manera similar a la Cámara de los Comunes.[68]​ Los tres territorios también tienen sus propias legislaturas, pero con menos responsabilidades constitucionales que las provincias y con algunas diferencias estructurales (por ejemplo, las asambleas legislativas de los territorios del noroeste y Nunavut no obedecen a partidos políticos y operan bajo un consenso).[69]​[70]​[71]​
Canadá también es una monarquía constitucional, en la que el soberano británico, Carlos III es jefe de estado; no obstante el gobernador general, que actúa en representación del jefe de estado, y los vicegobernadores, realizan la mayoría de funciones ceremoniales del monarca.[72]​ El poder ejecutivo, en materia de política, consiste en el primer ministro (jefe de gobierno) y el gabinete, quienes llevan las decisiones diarias del gobierno. El gabinete está compuesto por ministros que generalmente son seleccionados de la Cámara de los Comunes, encabezados por el primer ministro, que normalmente es el líder del partido que mantiene la mayoría en dicha cámara.[73]​ A diferencia de otros países de la Mancomunidad de Naciones, como el caso de Australia, no se ha planteado la celebración de un referéndum que cuestione la posibilidad de convertir al país en una república.
La Oficina del primer ministro (OPM) es una de las instituciones más poderosas en el gobierno, cuya función es la de iniciar el proceso legislativo para su aprobación en el parlamento y la de seleccionar, además de los miembros del gabinete, a senadores, magistrados de la corte federal, jefes de empresas de la Corona y de organismos gubernamentales y al gobernador general. Formalmente, la Corona aprueba la legislación parlamentaria y las órdenes del primer ministro.[74]​ El líder del partido político que cuenta con la segunda mayor cantidad de asientos, se convierte en el líder de la oposición y es parte de un sistema parlamentario acusatorio, destinado a mantener el gobierno en una constante verificación. Mary Simon es la gobernadora general desde el 26 de julio de 2021;[75]​ Justin Trudeau, líder del Partido Liberal de Canadá, es el primer ministro desde el 4 de noviembre de 2015;[76]​ y Andrew Scheer, líder del Partido Conservador de Canadá, es el líder de la oposición desde el 27 de mayo de 2017.
El poder judicial desempeña un papel importante en la interpretación de las leyes y tiene el poder de revocar aquellas que violen la Constitución. La Corte Suprema de Canadá es el tribunal más alto y constituye la última instancia para los delitos; desde el año 2000 es presidida por la jueza Beverley McLachlin.[77]​ Sus nueve miembros son nombrados por el gobernador general con el asesoramiento del primer ministro y del ministro de justicia. Todos los jueces a nivel superior y de apelación son nombrados previa consulta con los órganos judiciales no gubernamentales. El gobierno federal también nombra a los jueces a los tribunales superiores en los niveles provinciales y territoriales. Los judiciales de los niveles inferiores en provincias y territorios son responsabilidad de sus respectivos gobiernos.[78]​
El derecho anglosajón (Common law) prevalece en todo el país, excepto en Quebec, donde predomina el sistema romano francés.[79]​ El derecho penal es responsabilidad federal y es uniforme en todo el territorio nacional.[79]​ La aplicación de la ley y los tribunales penales son responsabilidad del gobierno provincial, aunque en las zonas rurales de todas las provincias, excepto en Ontario y Quebec, corre por cuenta de la Policía Montada del Canadá.[80]​
Canadá y los Estados Unidos comparten la frontera desarmada más larga del mundo, cooperan en las campañas y ejercicios militares, además de ser cada uno el mayor socio comercial del otro.[81]​ Sin embargo, el primero tiene una política exterior independiente, notable por mantener plenas relaciones con Cuba y por negarse a participar en la Guerra de Irak. El país también mantiene lazos históricos con el Reino Unido, Francia y otras antiguas colonias británicas y francesas, gracias a su adhesión en la Mancomunidad de Naciones y en la Francofonía.[82]​ Además, otro aspecto notable de sus relaciones exteriores es la relación fuerte y positiva con los Países Bajos, y tradicionalmente el gobierno neerlandés regala tulipanes (símbolo nacional neerlandés) a Canadá cada año, en recuerdo de la contribución de este último país a su liberación de la ocupación alemana.[83]​
Actualmente emplea a una fuerza militar profesional, cuyo personal asciende a cerca de 67 000 efectivos y unos 26 000 en la reserva.[84]​ Las Fuerzas Canadienses (FC) unifican en un solo cuerpo al ejército, la armada y la fuerza aérea. Los grandes equipos militares de las FC incluyen 1400 vehículos blindados, 33 buques de guerra y 861 aviones caza.[85]​
El fuerte apego al Imperio Británico y a la Mancomunidad, llevó al país a participar en la mayoría de las incursiones militares británicas en la Segunda Guerra de los Bóeres, la Primera Guerra Mundial y la Segunda Guerra Mundial. Desde entonces, Canadá ha sido un defensor del multilateralismo, realizando esfuerzos para resolver los problemas mundiales en colaboración con otras naciones.[86]​[87]​ Además, fue uno de los países fundadores de la Organización de las Naciones Unidas en 1945 y de la Organización del Tratado del Atlántico Norte en 1949. Durante la Guerra Fría, fue un importante contribuyente a las fuerzas de las Naciones Unidas en la Guerra de Corea y junto con Estados Unidos fundó el Mando Norteamericano de Defensa Aeroespacial (NORAD), para defenderse de posibles ataques aéreos de la Unión Soviética.[88]​
Durante la Crisis de Suez de 1956, el futuro primer ministro Lester Bowles Pearson alivió las tensiones al proponer la creación de las Fuerzas de paz de las Naciones Unidas, con lo que fue galardonado con el Premio Nobel de la Paz de 1957.[89]​ Como se trataba de la primera misión de mantenimiento de paz de la ONU, a menudo Pearson es acreditado como el inventor de este concepto. Canadá ha servido desde entonces en cincuenta misiones de mantenimiento de paz, incluyendo cada campaña realizada por la ONU hasta 1989,[90]​ y mantiene varios regimientos en misiones internacionales en Ruanda, la antigua Yugoslavia y otros lugares; el país ha enfrentado controversia sobre su participación en el extranjero, en particular en el Asunto de Somalia de 1993.[91]​ El número de militares canadienses participando en misiones de mantenimiento de paz disminuyó considerablemente en los dos últimos decenios. El 30 de junio de 2006, 133 canadienses se encontraban en misiones de la ONU en todo el mundo, incluyendo a 55 miembros de las FC, en comparación con los 1044 militares que estaban bajo el mando de la ONU el 31 de diciembre de 1996.[92]​[93]​
Canadá se unió a la Organización de los Estados Americanos (OEA) en 1990, además de que la Asamblea General de la OEA se celebró en Windsor, Ontario, en junio de 2000 y la tercera Cumbre de las Américas en la ciudad de Quebec en abril de 2001.[94]​ También pretende ampliar sus lazos con las economías de la cuenca del Pacífico a través de su membresía en el Foro de Cooperación Económica Asia-Pacífico (APEC).[95]​
Desde 2001, las FC han tenido tropas desplegadas en Afganistán como parte de la Fuerza de Estabilización de Estados Unidos y de la Fuerza Internacional de Asistencia para la Seguridad. Sin embargo, se comprometieron a retirarse de la provincia de Kandahar para el año 2011,[96]​ al tiempo que se anunció que el gasto total estimado de la guerra en Afganistán es de más 11,3 millones de dólares.[97]​ Canadá y los Estados Unidos siguen integrando agencias estatales y provinciales para reforzar la seguridad a lo largo de su frontera, a través de la Iniciativa de Viaje del Hemisferio Occidental.[98]​
En febrero de 2007, Canadá, Italia, Reino Unido, Noruega y Rusia anunciaron sus planes de financiación de un proyecto de 1,5 millones de dólares para ayudar a desarrollar vacunas que podrían salvar millones de vidas en las naciones pobres e invitaron a otros países a unirse a ellos.[99]​ En agosto de 2007, la soberanía canadiense en aguas del Ártico fue desafiada después de una expedición submarina rusa al Polo Norte; los canadienses han considerado esa área como territorio soberano desde 1925.[100]​
El departamento del gobierno de Canadá responsable de los programas sociales y del mercado laboral a nivel federal (Employment and Social Development Canada) es miembro de la Conferencia interamericana de seguridad social (CISS).
En materia de derechos humanos, respecto a la pertenencia a los siete organismos de la Carta Internacional de Derechos Humanos, que incluyen al Comité de Derechos Humanos (HRC), Canadá ha firmado o ratificado:
Canadá es una federación compuesta de diez provincias y tres territorios. A su vez, pueden ser agrupados en regiones: Oeste de Canadá, Canadá Central, Canadá Atlántica y Norte de Canadá (el último está formado por los tres territorios). El Este de Canadá se refiere a Canadá Central y Canadá Atlántica juntos. Las provincias tienen más autonomía que los territorios, además de que son responsables de la mayoría de los programas sociales del país (tales como la atención de la salud, educación y bienestar) y juntas reciben más ingresos que el gobierno federal, una estructura casi única entre las federaciones en el mundo. Sin embargo, es el gobierno federal quien puede iniciar las políticas nacionales en áreas provinciales, como la Ley de Salud de Canadá; las provincias pueden optar por modelos distintos a estos, pero rara vez lo hacen en la práctica. Los pagos de compensación son hechos por el gobierno federal para asegurar que se mantengan normas razonablemente uniformes de servicios e impuestos entre las provincias más ricas y más pobres.[111]​
Notas:
Canadá ocupa gran parte del norte de Norteamérica, compartiendo sus fronteras terrestres con los Estados Unidos contiguos al sur y con el estado y territorio de Alaska hacia el noroeste, extendiéndose desde el océano Atlántico en el este hasta el océano Pacífico en el oeste; al norte limita con el océano Ártico. Por su área total (incluyendo sus aguas), es el segundo país más grande del mundo, después de Rusia.[113]​ Por su área de tierra, ocupa el cuarto lugar (el área de tierra es el área total menos el área de los lagos y ríos).[114]​
Desde 1925, reclama la región del Ártico entre los meridianos 60° O y 141° O,[115]​ pero esta afirmación no es reconocida universalmente. El asentamiento más septentrional en el país (y en el mundo) es la estación de Alert de las FC, ubicada en el extremo norte de la isla Ellesmere a 82,5° N, a solamente 817 kilómetros del Polo Norte.[116]​ Gran parte del Ártico canadiense está cubierto por hielo y permafrost. También es el país con el litoral más extenso del mundo: 202 080 kilómetros.[113]​
La densidad de población, de 3,3 hab/km², está entre las más bajas en el mundo. La parte más densamente poblada del país es la zona de la ciudad de Quebec y el corredor Windsor, (situado en el sur de Quebec y Ontario) a la orilla de los Grandes Lagos y el río San Lorenzo.[117]​
Canadá tiene un litoral extenso, hacia el este, norte y oeste, y desde el último período glaciar cuenta con ocho regiones de bosques distintos, incluyendo una amplia zona de taiga sobre el Escudo Canadiense.[119]​ La inmensidad y la variedad de la geografía, la ecología, la vegetación y el relieve de Canadá, han dado lugar a una amplia variedad de climas en todo el país.[120]​ Debido también a su gran tamaño, Canadá tiene más lagos que cualquier otro país, por lo que también contiene gran parte del agua dulce del mundo.[121]​ También hay glaciares que contienen agua dulce en las Montañas Rocosas y en las Montañas Costeras.
La temperatura promedio en invierno y verano varía según la ubicación. Los inviernos pueden ser duros en muchas regiones del país, especialmente en las provincias del interior y en las praderas, donde se experimenta un clima continental, con temperaturas promedio diarias de −15 °C, pero pueden llegar por debajo de los −40 °C.[122]​ En las regiones sin costas, la nieve puede cubrir el suelo durante casi seis meses (más en el norte). La costa de Columbia Británica goza de un clima templado, con un invierno más cálido, pero lluvioso. En las costas este y oeste, generalmente las temperaturas promedio no pasan de los 20 °C, mientras que entre las costas, la temperatura promedio máxima en verano oscila entre 25 y 30 °C, con ondas de calor ocasionales en el interior que superan los 40 °C.[123]​
Canadá también es geológicamente activo, ya que en el país se presentan terremotos y volcanes potencialmente activos, como el Monte Meager, el Monte Garibaldi, el Monte Cayley y el complejo volcánico Monte Edziza.[124]​ En 1775, la erupción volcánica del cono Tseax provocó una catástrofe, matando a 2000 personas de la etnia nisga'a y causando la destrucción de su aldea en el valle del río Nass, al norte de la Columbia Británica; la erupción produjo un flujo de lava de 22,5 kilómetros, y de acuerdo con la leyenda de los nisga'a, bloqueó el curso del río Nass.[125]​
En junio de 2019, el gobierno anunció la reanudación de las obras de ampliación del oleoducto Trans Mountain a la costa oeste de Canadá. La decisión es bien recibida por la industria petrolera, pero criticada por los ambientalistas. Una vez finalizado, el proyecto podría dar lugar a un aumento adicional de 15 millones de toneladas en las emisiones de gases de efecto invernadero de Canadá.[126]​
Dada su enorme extensión, el país posee una variada topografía e importantes diferencias climáticas, lo que proporciona una flora y fauna muy diversa. Una buena parte del territorio canadiense está cubierto por bosques de árboles maderables, donde se destacan el pino, el cedro y el arce; también existen territorios dominados por las amplias praderas. La fauna es muy similar a la de Europa del Norte y Asia, por lo que pueden encontrarse osos, lobos, coyotes, pumas, entre otros animales carnívoros. En las regiones árticas la fauna es propia de la tundra y en ella habitan gran variedad de focas, ballenas y osos polares, mientras que en la flora predominan los musgos y líquenes. En algunas regiones pueden verse castores, puercoespines, topos y numerosos roedores. Algunas zonas de Canadá también son el hábitat de antílopes, renos y alces.[127]​
Canadá es una de las naciones más ricas del mundo, con una renta per cápita alta, y es miembro de la Organización para la Cooperación y el Desarrollo Económico (OCDE) y el G8.[128]​ El país cuenta con una economía de libre mercado, clasificada por encima de los Estados Unidos en el índice de libertad económica de la Heritage Foundation, donde también superó a la mayoría de las naciones europeas occidentales.[129]​ Los mayores importadores de bienes canadienses son los Estados Unidos, el Reino Unido y Japón.[130]​ En 2008, las mercancías importadas en Canadá ascendieron a más de 442,9 mil millones de dólares, de los cuales 280,8 mil millones de dólares provenían de los Estados Unidos, 11,7 mil millones de dólares de Japón y 11,3 mil millones de dólares procedían del Reino Unido.[130]​
En octubre de 2009, la tasa de desempleo de Canadá era del 8,6 %. Las tasas provinciales de desempleo varían desde el 5,8 % en Manitoba hasta un máximo del 17 % en Terranova y Labrador.[131]​ En 2008, la deuda pública del país era la más baja entre los miembros del G8.[132]​ Entre 2008 y 2009, esta deuda aumentó 6100 millones de dólares hasta un total de 463 700 millones de dólares.[133]​ En el siglo pasado, el crecimiento de la fabricación, la minería y el sector de servicios transformó a la nación de una economía prácticamente rural a una más industrial y urbana. Como otras naciones del primer mundo, la economía canadiense está dominada por el sector terciario, que emplea a alrededor de las tres cuartas partes de los canadienses.[134]​ Entre los países desarrollados, el país brinda una inusual importancia a su sector primario, en la que las industrias del petróleo y de la madera son dos de las más sobresalientes.[135]​
La idea de que Canadá es un producto de la explotación de su riqueza natural conforma una visión historiográfica quizás influenciada por la escuela de Annales. En este respecto la obra de H. A. Innis citada por Fohlen[136]​ menciona cómo el comercio de la metrópoli fundó las líneas de comunicación a partir de las materias primas, y cómo estas «forjaron la unidad política» de este país. La obra de K. Buckley «ha sostenido que la staple theory explicaba perfectamente el desarrollo de Canadá hasta 1920 aproximadamente, pero que era necesaria una explicación distinta para el periodo reciente».[136]​ La dependencia económica canadiense de estos productos se puso de manifiesto con la llegada de la Gran Depresión de 1929; «la mayor parte de la renta nacional provenía de las exportaciones, y (…) los dos tercios de estas exportaciones consistían en materias primas».[136]​
Canadá es uno de los pocos países desarrollados que son exportadores netos de energía.[137]​ Canadá Atlántica tiene grandes depósitos de gas natural en sus costas, y Alberta tiene reservas importantes de petróleo y gas. Las arenas de alquitrán de Athabasca le dan al país las terceras reservas de petróleo más grandes del mundo, detrás de las de Venezuela y Arabia Saudita.[138]​
También es uno de los proveedores mundiales de productos agrícolas más importantes: las praderas canadienses son unos de los principales productores de trigo, colza y otros cereales.[139]​ También es el mayor productor de zinc y uranio, y es una fuente primordial de muchos otros recursos minerales, tales como el oro, el níquel, el aluminio y el plomo.[137]​ Muchas ciudades en el norte, donde la agricultura es difícil, se sustentan gracias a la cercanía de minas y aserraderos. Canadá también tiene un sector manufacturero considerable, concentrado en el sur de Ontario y Quebec, siendo las industrias automovilísticas y aeronáuticas las más importantes.[140]​
La industria minera recibe un fuerte apoyo del Gobierno. El país alberga la sede y las principales filiales de la mayoría de las empresas mineras del mundo. La legislación federal y provincial fomenta el desarrollo del mercado bursátil al permitir formas más flexibles de divulgación de los yacimientos que en otros lugares, y la fiscalidad es favorable a las empresas. Además, la red diplomática de Canadá proporciona apoyo político en el extranjero a cualquier empresa minera registrada en el país. El Primer Ministro Stephen Harper (2006-2015) ha declarado que quiere convertir a Canadá en uno de los mayores exportadores de recursos naturales del mundo. Se le ha acusado de debilitar deliberadamente las protecciones medioambientales existentes para favorecer a la industria minera.[141]​
La integración económica con los Estados Unidos ha aumentado significativamente desde la Segunda Guerra Mundial. Esto preocupa a varios grupos nacionalistas canadienses, que se interesan por la autonomía e independencia cultural y económica en una era de globalización, ya que la mayoría de los bienes y medios de comunicación estadounidenses se han vuelto omnipresentes en todo el país.[142]​ El Tratado de Comercio de Productos Automotrices de 1965 abrió las fronteras al comercio dentro de la industria automovilística. En la década de 1970, las especulaciones sobre la autosuficiencia energética y a la propiedad extranjera en los sectores de fabricación llevaron a que el gobierno liberal del primer ministro Pierre Trudeau, creara el Programa Nacional de Energía (NEP) y la Agencia de Revisión de Inversiones Exteriores (FIRA).[143]​
En la década de 1980, el primer ministro Brian Mulroney abolió el NEP y cambió el nombre de la FIRA a «Inversiones Canadá», a fin de fomentar la inversión extranjera.[144]​ El Tratado de Libre Comercio Estados Unidos - Canadá de 1988 eliminó las barreras arancelarias entre los dos países, mientras que en la década de 1990, el Tratado de Libre Comercio de América del Norte (TLCAN) amplió la zona de libre comercio para incluir a México.[139]​ A mediados de esa década, el gobierno liberal de Jean Chrétien comenzó a publicar los excedentes presupuestarios anuales y comenzó el pago de la deuda nacional.[145]​ La crisis económica de 2008 causó la caída de la economía dentro de una recesión, que podría aumentar la tasa de desempleo hasta un 10 %.[146]​
Según el Instituto de Investigación de Economía Contemporánea (IRÉC), los activos canadienses en los siete principales paraísos fiscales se multiplicaron por 37,6 entre 1987 y 2014. Entre 1999 y 2013, se presentaron media docena de proyectos de ley para limitar o detener la evasión fiscal, pero todos fueron rechazados.[147]​
En 2016, los 100 empleadores canadienses mejor pagados ganaron un promedio de 10,4 millones C$, más de 200 veces el ingreso promedio de un trabajador canadiense en 2016. De 2015 a 2016, la remuneración media de los directores generales aumentó un 8 %. El declive del sindicalismo en Canadá habría conducido a un aumento del 15 % en la desigualdad de los ingresos laborales.[148]​
Cada una de las provincias y territorios son responsables de la educación; cada uno de estos sistemas tienen características similares, al mismo tiempo que reflejan la historia, la cultura y la geografía propias de cada región.[149]​ La edad en la que los niños comienzan su educación oscila entre los 5 y 7 años,[149]​ lo que contribuye a una tasa de alfabetización de adultos del 99 %.[113]​ La educación superior también es administrada por los gobiernos provinciales y territoriales, que proporcionan la mayor parte de su financiación; el gobierno federal otorga becas, préstamos estudiantiles y becas de investigación adicional. En 2002, el 43 % de los canadienses de 25 a 64 años de edad poseía una educación postsecundaria; para los de 25 a 34 años de edad, la tasa de educación postsecundaria alcanzó un 51 %.[150]​
Canadá es una país industrial, con un sector de ciencia y tecnología altamente desarrollado. Casi el 1,88 % del PIB nacional se asigna a la investigación y desarrollo (i+d).[151]​ 18 canadienses han ganado un premio Nobel en física, química y medicina.[152]​ Es considerado el 12° país con más usuarios de Internet en el mundo, ya que cuenta con 33 millones de usuarios, es decir, el 90,1 % de la población total.[153]​ Según el Índice global de innovación, a cargo de la Organización Mundial de la Propiedad Intelectual, en su versión 2022, Canadá se ubicó en lugar 15 en innovación entre 132 países del mundo.[154]​
La Agencia Espacial Canadiense tiene como principal función la exploración espacial y planetaria, la investigación de la aviación, así como el desarrollo de cohetes y satélites. En 1984, Marc Garneau se convirtió en el primer astronauta canadiense, sirviendo como especialista de carga del STS-41-G. Canadá es un participante en la
Estación Espacial Internacional y un pionero en el campo de la robótica espacial, gracias al desarrollo del Canadarm, el Canadarm 2 y el Dextre. Fue clasificado tercero de veinte países en el campo de ciencias del espacio.[155]​ Desde la década de 1960, Aerospace Industries de Canadá ha diseñado y construido diez satélites, incluyendo RADARSAT-1, RADARSAT-2 y MOST.[156]​ El país también produjo uno de los más exitosos cohetes sonda, el Black Brant; más de 1000 de estos cohetes se han puesto en marcha desde que fueron producidos inicialmente en 1961.[157]​ Las universidades canadienses están trabajando en el primer proyecto nacional de aterrizaje de una nave espacial: la Northern Light, diseñada para buscar vida en Marte e investigar el entorno del planeta en cuestiones de radiación electromagnética y propiedades atmosféricas. Si Northern Light tiene éxito, Canadá será el tercer país en aterrizar una nave en otro planeta.[158]​
Una estimación para diciembre de 2020 calculó la población total de Canadá en 38.005.238 habitantes.[159]​ El crecimiento de la población se debe principalmente a la inmigración y, en menor medida, al crecimiento natural. Alrededor del 80 % de la población canadiense vive a menos de 150 kilómetros de la frontera con Estados Unidos.[160]​ Un porcentaje similar vive en las zonas urbanas, concentrado principalmente en las ciudades de Quebec, el corredor Windsor (Toronto, Montreal y Ottawa), el Lower Mainland de la Columbia Británica (formado por la región que la rodea a Vancouver) y el corredor Calgary–Edmonton en Alberta.[161]​
De acuerdo con el censo de 2006, el grupo étnico más grande son los anglocanadienses (21 %), seguidos por los francocanadienses (15,8 %), los escoceses (15,2 %), los irlandeses (13,9 %), los alemanes (10,2 %), los italianos (5 %), los chinos (3,9 %), los ucranianos (3,6 %) y los descendientes de las Primeras Naciones (3,5 %). Aproximadamente un tercio de los encuestados identificó a su origen étnico como «canadiense».[162]​ Hay 600 tribus o gobiernos reconocidos de las Primeras Naciones, los cuales agrupan a 1 172 790 personas.[163]​
La población aborigen de Canadá está creciendo a casi el doble de la tasa nacional, y en el censo de 2006 el 3,8 % de los canadienses encuestados afirmó que eran de origen indígena. Otro 16,2 % de la población pertenecían a minorías visibles de origen extranjero.[164]​ Las minorías más importantes del país son los sudasiáticos (4 %), los chinos (3,9 %) y los afroamericanos (2,5 %).[165]​ En 1961, menos del 2 % de la población canadiense (unas 300 000 personas) podría clasificarse como una minoría visible y menos del 1 % como aborígenes.[166]​ En 2006, el 51 % de la población de Vancouver y el 46,9 % de la población de Toronto eran miembros de estas minorías visibles.[167]​[168]​ Entre 2001 y 2006, la población de las minorías visibles aumentó en un 27,2 %.[165]​ De acuerdo con un pronóstico de 2005 hecho por Estadísticas de Canadá, la proporción de las «minorías» podría llegar hasta un 23 % para el 2017. En 2007, casi uno de cada cinco canadienses (19,8 %) había nacido en el extranjero.[169]​ Cerca del 60 % de los nuevos inmigrantes provienen de Asia (incluyendo el Medio Oriente).[169]​ Para el 2031, uno de cada tres canadienses pertenecerá a una «minoría».[170]​
Canadá tiene la tasa de inmigración per cápita más alta en el mundo, impulsado por la política económica, la existencia de una amplia oferta de empleo por el bajo índice de paro del país, y la reintegración familiar, y se cree que en 2010 llegarán entre 240 000 y 265 000 nuevos residentes permanentes.[171]​ Canadá también acepta a gran número de refugiados. La mayoría de los nuevos inmigrantes se asientan en las principales zonas urbanas como Toronto y Vancouver,[172]​ mientras que la inmigración procedente de países de habla francesa se concentra en las ciudades francófonas del país, mayormente en Montreal y Quebec.
Como la mayoría de los países desarrollados, el país está experimentando un cambio demográfico hacia una población de mayor edad, con más jubilados y menos personas en edad de trabajar. En 2006, la edad promedio de los canadienses era de 39,5 años.[173]​ Los resultados del censo también indican que, a pesar de un aumento en la inmigración desde 2001 (que dio a Canadá una mayor tasa de crecimiento de la población que en el período intercensal anterior), el envejecimiento de la población canadiense no fue más lento durante el mismo período.
Aunque la Constitución del país no establece ninguna religión de estado oficial, el pluralismo religioso es una parte importante de la cultura política de Canadá. Según el censo de 2011, el 67,3 % de los canadienses se identifican como cristianos (representaban el 77,1 % en 2001);[175]​ de estos, los católicos constituyen el grupo más grande (el 38,7 % de la población).[176]​
Antes de la colonización europea, las religiones aborígenes eran en gran parte animistas o chamánicas, incluida una intensa reverencia tribal por los espíritus y la naturaleza. La colonización francesa que comenzó en el siglo XVI estableció una población francófona católica en Nueva Francia, especialmente Acadia (más tarde en el Bajo Canadá, ahora Nueva Escocia y Quebec). La colonización británica trajo oleadas de anglicanos y otros protestantes al Alto Canadá (actual Ontario). El Imperio ruso extendió la ortodoxia oriental en pequeña medida a las tribus en las lejanas costas del norte y oeste, particularmente entre nómadas hiperbóreos como los inuit. La ortodoxia llegaría al continente con inmigrantes de la Unión Soviética, el Bloque Oriental, Grecia y otros lugares durante el siglo XX.
La denominación protestante más grande es la Iglesia Unida de Canadá, de rito presbiteriano (conformada por el 6,1 % de los canadienses), seguida por los anglicanos (5 %) y los bautistas (1,9 %). El porcentaje restante pertenecía a otras corrientes cristianas.[176]​ Cerca del 23,9 % de los canadienses declararon no tener ninguna afiliación religiosa (representaban el 16,5 % en 2001) y el 8,8 % restante están afiliados con las religiones no cristianas (cifra que era de un 6,3 % en 2001), de las cuales las más importantes son el islam (3,2 %) y el hinduismo (1,5 %).[176]​
Los dos idiomas oficiales de Canadá son el inglés y el francés. El bilingüismo oficial se define en la Carta Canadiense de Derechos y Libertades, la Ley sobre las Lenguas Oficiales y el Reglamento Oficial del Lenguaje, los cuales se aplican por el Comisionado de las Lenguas Oficiales. El inglés y el francés tienen igualdad de condición en los tribunales federales, en el parlamento y en todas las instituciones federales. Los ciudadanos canadienses tienen el derecho, donde hay suficiente demanda, para recibir los servicios del gobierno federal en inglés o francés, y los idiomas oficiales de las minorías tienen garantizadas sus propias escuelas en todas las provincias y territorios.[177]​
El inglés y el francés son las lenguas maternas del 59,7 % y del 23,2 % de la población, respectivamente,[178]​ y los idiomas más hablados en casa por el 68,3 % y el 22,3 % de la población, respectivamente.[179]​ El 98,5 % de los canadienses hablan inglés o francés (el 67,5 % solo hablan inglés, el 13,3 % solo hablan francés y el 17,7 % ambos).[180]​
La Carta de la lengua francesa establece al francés como el idioma oficial en Quebec.[181]​ A pesar de que más del 85 % de los canadienses francófonos viven en Quebec, hay importantes concentraciones en Ontario, Alberta y al sur de Manitoba, siendo Ontario la provincia que tiene la mayor población de habla francesa fuera de Quebec.[182]​ Nuevo Brunswick, la única provincia oficialmente bilingüe, tiene una minoría acadiana de habla francesa, que constituye el 33 % de la población. También hay grupos de acadianos en el suroeste de Nueva Escocia, en la Isla de Cabo Bretón y en la parte central y occidental de la Isla Príncipe Eduardo.[183]​
Otras provincias no tienen lenguas oficiales como tal, pero además del inglés, se utiliza el francés como un idioma de instrucción en los tribunales y para otros servicios del gobierno. Manitoba, Ontario y Quebec permiten que el inglés y el francés sean utilizados en las legislaturas provinciales y las leyes se promulgan en ambos idiomas. En Ontario, el francés tiene algo de estatus legal, pero no es totalmente cooficial.[184]​
Hay once grupos de lenguas aborígenes, compuestos por más de 65 dialectos distintos.[185]​ De estos, solamente el cree, el inuktitut y el ojibwa tienen una población de hablantes con fluidez lo suficientemente grande para considerarse capaces de sobrevivir a largo plazo.[186]​ Varios idiomas aborígenes tienen estatus oficial en los Territorios del Noroeste.[187]​ El esquimal es el idioma de la mayoría de los habitantes de Nunavut y uno de los tres idiomas oficiales en ese territorio.[188]​
Más de seis millones de personas en Canadá tienen un idioma no oficial como su lengua materna. Entre los idiomas no oficiales más hablados en el país están: el chino (principalmente cantonés; 1 012 065 hablantes como lengua materna), el italiano (455 040), el alemán (450 570), el panyabí (367 505) y el español (345 345).[178]​
Históricamente, la cultura canadiense ha sido influenciada por las tradiciones y costumbres de las culturas británica, francesa e indígena. En todo el territorio nacional, se pueden encontrar varias formas de expresiones culturales, lingüísticas, artísticas y musicales distintivas de cada región.[190]​[191]​ Muchas palabras, inventos y juegos de los indígenas de Norteamérica se han convertido en parte de la vida cotidiana de los canadienses. La canoa, las raquetas de nieve, la pista de trineo, el lacrosse, el tira y afloja, el jarabe de arce y el tabaco son ejemplos de esos productos, invenciones y juegos.[192]​ Algunas palabras de origen indígena son barbacoa, caribú, tamias, woodchuck, hamaca, mofeta, caoba, huracán y alce.[193]​ Numerosas zonas, ciudades y ríos tienen nombres de origen indígena. El nombre de la provincia de Saskatchewan se deriva del nombre en idioma cree del río Saskatchewan, Sipi Kisiskatchewani.[194]​ Otro ejemplo es el nombre de la capital de Canadá, Ottawa, que proviene del algonquín adawe, que significa «al comercio».[194]​ El Día Nacional de los Aborígenes reconoce las culturas y contribuciones de los pueblos aborígenes canadienses.[195]​
Aparte de los indígenas, la cultura canadiense ha sido enormemente influenciada por los inmigrantes procedentes de todo el mundo, por eso la mayoría de las personas perciben a Canadá como una nación multicultural.[60]​ Sin embargo, la cultura del país también posee muchas características de la cultura estadounidense, debido en gran parte a su proximidad y a la alta tasa de migración entre los dos países. Entre 1755 y 1815, la gran mayoría de los inmigrantes de habla inglesa provenían de Estados Unidos; durante y después de la guerra de independencia de las Trece Colonias, 46 000 estadounidenses leales a la Corona británica se establecieron en el país.[196]​ Entre 1785 y 1812, se dio una nueva oleada de inmigrantes estadounidenses que llegaron al país en respuesta a las promesas de tierras.[197]​
De esta forma, los medios y celebridades estadounidenses se volvieron muy populares, si no dominantes, en Canadá anglosajona; por el contrario, muchos de los productos culturales y artistas canadienses son exitosos en Estados Unidos y otras partes del mundo.[198]​ Muchos de estos se comercializan hacia un mercado norteamericano unificado o global. La creación y la preservación de la cultura canadiense claramente son compatibles con los programas del gobierno federal, las leyes e instituciones, tales como la Canadian Broadcasting Corporation (CBC), la National Film Board of Canada y la Comisión de Telecomunicaciones y de la Radio y Televisión Canadiense.[199]​
Las artes visuales canadienses han sido dominadas por Tom Thomson (uno de los pintores canadienses más famosos) y por el Grupo de los Siete. La breve carrera de Thomson pintando paisajes canadienses, abarcó solo una década hasta su muerte ocurrida en 1917, cuando tenía 39 años.[200]​ El Grupo de los Siete fueron pintores con un enfoque nacionalista e idealista, que exhibieron sus obras distintivas por primera vez en mayo de 1920. Aunque su nombre indica que existían siete miembros, cinco artistas (Lawren Harris, A. Y. Jackson, Arthur Lismer, J. E. H. MacDonald y Federico Varley) fueron los responsables de la articulación de las ideas del grupo. Se les unieron brevemente Frank Johnston y el artista comercial Franklin Carmichael. A. J. Casson se convirtió en parte del grupo en 1926.[201]​ Otra artista canadiense destacada que tuvo conexiones con el grupo fue Emily Carr, conocida por sus paisajes y retratos de los pueblos indígenas de la costa noroeste.[202]​
Canadá ha desarrollado una infraestructura e industria musical, con la radiodifusión regulada por la Comisión de Telecomunicaciones y de la Radio y Televisión Canadiense.[203]​[204]​ La industria musical canadiense ha producido compositores, músicos y grupos de renombre internacional, al tiempo que la Academia Canadiense de Ciencias y Artes administra los premios de la industria musical de Canadá, los Premios Juno, que comenzaron en 1970.[205]​ El himno nacional O Canada, adoptado en 1980, originalmente fue encargado por el teniente gobernador de Quebec, Théodore Robitaille, para la ceremonia del Día de San Juan Bautista de 1880.[206]​ Calixa Lavallée compuso la música, que fue un acompañamiento para un poema patriótico, escrito por el poeta y juez Adolphe-Basile Routhier. La letra fue escrita originalmente en francés, y en 1906 se tradujo al inglés.[207]​
Los símbolos nacionales hacen referencia a elementos naturales, históricos e indígenas con los que se identifican al país. El uso de la hoja de arce como un símbolo canadiense se remonta a principios del siglo XVIII. La hoja de arce es representada en las banderas nacionales anteriores y actuales, en las monedas y en el escudo de armas.[208]​ Otros símbolos importantes son el castor, la barnacla canadiense, el tótem, el inukshuk, el colimbo grande, la Corona y la Policía Montada.[208]​[209]​
Oficialmente, los deportes nacionales son el hockey sobre hielo en invierno y el lacrosse en verano.[210]​ El hockey es un pasatiempo nacional y el deporte de espectadores más popular en el país. También es el deporte más practicado por los canadienses, ya que en 2004 existían más de 1,65 millones de jugadores de hockey.[211]​ Siete de las áreas metropolitanas más grandes en Canadá (Toronto, Montreal, Vancouver, Ottawa, Calgary, Edmonton y Winnipeg) tienen franquicias en la National Hockey League (NHL), mientras que Quebec tuvo una franquicia hasta que en 1995 esta se trasladó a Denver, y la mayoría (54 %) de los jugadores en esta liga son de origen canadiense. Otros deportes de espectador populares son el curling y el fútbol canadiense; este último cuenta con una liga profesional, la Canadian Football League (CFL).[211]​
Canadá cuenta con franquicias en la numerosas ligas profesionales estadounidenses: Major League Baseball (Toronto Blue Jays), National Basketball Association (Toronto Raptors) y Major League Soccer (Toronto FC, Vancouver Whitecaps y Montreal Impact). El golf, el béisbol, el esquí, el fútbol, el voleibol y el baloncesto empiezan a cobrar fuerza entre los jóvenes y a nivel amateur, pero las ligas profesionales y las franquicias no están muy extendidas.[211]​
En los Juegos Olímpicos, Canadá tiene éxito particularmente en los deportes de invierno (hockey, patinaje sobre pista corta y larga, patinaje artístico, esquí acrobático, curling). Este fenómeno se observa tanto en categoría masculina como en la categoría femenina. Canadá ha albergado los Juegos Olímpicos de Montreal 1976, los Juegos Olímpicos de Calgary 1988 y los Juegos Olímpicos de Vancouver 2010, además de otros varios eventos deportivos internacionales tales como los Juegos Panamericanos de 1967, 1999 y 2015, el Gran Premio de Canadá de Fórmula 1, el Gran Premio de Toronto de la IndyCar Series, el Gran Premio de Montreal y el Gran Premio de Quebec de ciclismo del UCI World Tour, el Abierto de Canadá de Golf del PGA Tour, la Copa Mundial de Fútbol Sub-20 de 2007,[212]​ la Copa Mundial Femenina de Fútbol Sub-20 de 2014, la Copa Mundial Femenina de Fútbol de 2015[213]​ y albergará la Copa Mundial de Fútbol de 2026.


Rusia (en ruso: Росси́я, romanización Rossíya),[n. 1]​ conocida formalmente[n. 2]​ como Federación de Rusia[n. 1]​ (en ruso: Росси́йская Федера́ция, romanización Rossíyskaya Federátsiya) y también citada en ocasiones como Federación Rusa, es un país que se extiende sobre Europa del Este y Asia del Norte.[n. 3]​ Es el país más extenso del mundo, con una superficie de 17 125 191 km²,[2]​ equivalente a algo más de la novena parte de la tierra firme del planeta, y posee una gran variedad de relieve y de ecosistemas. Su capital es la ciudad federal de Moscú.
La forma de gobierno es la república semiparlamentaria formada por ochenta y cinco sujetos federales, y es el noveno país con mayor población en el mundo al tener 145 478 097 habitantes.[11]​ En Rusia existen once zonas horarias, desde UTC+2 hasta UTC+12.[n. 4]​ Rusia tiene las mayores reservas de recursos energéticos y minerales del mundo aún sin explotar, y es considerada la mayor superpotencia energética. Posee las mayores reservas de recursos forestales y la cuarta parte del agua dulce sin congelar del mundo.
Rusia es el país que limita con mayor número de países, un total de dieciséis,[n. 5]​ y el que tiene las fronteras más extensas. Limita con los siguientes países (empezando por el noroeste y siguiendo el sentido antihorario): Noruega, Finlandia, Estonia, Letonia, Bielorrusia, Lituania,[n. 6]​ Polonia,[n. 6]​ Ucrania,[n. 7]​ Georgia,[n. 8]​ Azerbaiyán, Kazajistán, República Popular China, Mongolia y Corea del Norte. Tiene límites de aguas territoriales con varios de los anteriores, con Japón y con Estados Unidos (en concreto, con el estado de Alaska). Limita también con los Estados de reconocimiento limitado Abjasia y Osetia del Sur y con las repúblicas de Donetsk y Lugansk, reclamadas y parcialmente controladas por la propia Rusia. Sus costas están bañadas por el océano Glacial Ártico, el norte del océano Pacífico y mares interiores como el Báltico, el Negro y el Caspio.
La historia de Rusia comienza con los pueblos eslavos orientales. Los eslavos emergieron como un grupo reconocible en Europa entre los siglos III y VIII d. C.[12]​ El primer Estado de los eslavos orientales, la Rus de Kiev, surgió en el siglo IX y en el año 988 adoptó el cristianismo ortodoxo, producto de la cristianización de la Rus de Kiev llevada a cabo por Cirilo y Metodio, enviados desde el Imperio bizantino. Comenzó entonces una síntesis de las culturas bizantina y eslava que definiría la rusa durante el siguiente milenio.[13]​ Más tarde, la Rus de Kiev se desintegró en muchos pequeños Estados feudales, de los cuales el más poderoso fue el Principado de Vladímir-Súzdal, que posteriormente se transformó en Principado de Moscú, el cual se convirtió en la fuerza principal en el proceso de la reunificación rusa y la lucha por la independencia contra la Horda de Oro. Moscú reunificó gradualmente los principados rusos circundantes y comenzó a dominar en el legado cultural y político de la Rus de Kiev. En el siglo XVIII d. C., el país se expandió mediante la conquista, la anexión y la exploración hasta convertirse en el tercer imperio más grande de la historia, el ruso, al extenderse desde Polonia, en poniente, hasta el océano Pacífico y Alaska, en el este.
Rusia ha tenido poder y mucha influencia en el mundo: primero, en la época del Imperio ruso; después, como el país dominante de la Unión Soviética (URSS), el primero y más grande de los Estados socialistas constitucionalmente establecidos y una superpotencia reconocida como tal; y, actualmente, como la Federación de Rusia. Tiene una larga tradición de calidad en todos los aspectos de las artes y de las ciencias.[12]​ La Federación de Rusia se fundó en 1991, al disolverse la Unión Soviética, y es reconocida como la heredera de la personalidad legal de esta.[14]​ Es el octavo país por PIB nominal y el sexto por PIB (PPA). Es uno de los cinco países con armas nucleares reconocidos, posee el mayor arsenal de armas de destrucción masiva del mundo,[15]​ tiene el segundo ejército más poderoso del mundo[16]​ y el cuarto gasto militar más alto.[17]​ Rusia es miembro permanente del Consejo de Seguridad de Naciones Unidas, miembro del G20, del APEC y de la OCS, y tiene mucha influencia en los países que fueron repúblicas soviéticas, y aún más en los países miembros Comunidad de Estados Independientes (CEI). Rusia también alberga el noveno mayor número de sitios nombrados por la Unesco como Patrimonio de la Humanidad.
El nombre Rusia se deriva de Rus, un estado medieval poblado principalmente por los eslavos orientales.[18]​ Sin embargo, el nombre propio se hizo más prominente en la historia posterior, y el país fue llamado típicamente por sus habitantes Rússkaya zemliá (Русская земля), que puede traducirse como «tierra rusa».[19]​ Para distinguir este estado de otros estados derivados de él, la historiografía moderna lo denota como Rus de Kiev. El nombre de Rus en sí proviene de la temprana Edad Media, usado para un grupo de comerciantes y guerreros nórdicos que se trasladaron desde el otro lado del Mar Báltico, que fundaron un estado centrado en Nóvgorod, que más tarde se convirtió en la Rus de Kiev.[20]​
Una versión latina medieval del nombre Rus fue Rutenia, que se usó como una de varias designaciones para las regiones eslavas orientales y ortodoxas, y comúnmente como designación para las tierras de Rus'.[21]​ El nombre actual del país, Rusia (en ruso: Росси́я; pronunciado Rossiya), proviene de la designación griega bizantina de la Rus', Ρωσσία Rossía —deletreado Ρωσία (Rosía se pronuncia [roˈsia]) en griego moderno.[22]​ La forma típica de referirse a los ciudadanos de Rusia es la de «rusos» en español;[23]​ más hay dos palabras en ruso que se traducen comúnmente al español como «rusos»: una es «ру́сские» (rússkiye), que generalmente se refiere a los rusos étnicos, y la otra es «россия́не» (rossiyáne), que se refiere a los ciudadanos de la Federación de Rusia, independientemente de su origen étnico.[24]​
El primer asentamiento humano en Rusia se remonta al período olduvayense en el Paleolítico Inferior temprano. Hace unos 2 millones de años, ejemplares del Homo erectus emigraron a la península de Tamán en el sur de Rusia.[25]​ Se han descubierto herramientas de pedernal, de unos 1.5 millones de años, en el Cáucaso Norte.[26]​ Los especímenes datados por radiocarbono de las cuevas de Denísova en el macizo de Altái estiman que el espécimen más antiguo del hombre de Denísova vivió hace 195 000 a 122 700 años.[27]​ Dentro de esta última cueva también se encontraron fósiles de «Denny», un híbrido humano arcaico que era mitad neandertal y mitad denisovano, y que vivió hace unos 90 000 años.[28]​ Rusia fue el hogar de algunos de los últimos neandertales supervivientes, de hace unos 45 000 años, encontrados en la cueva de Mezmáiskaya.[29]​
El primer rastro de un humano moderno primitivo en Rusia data de hace 45 000 años, en Siberia occidental.[30]​ El descubrimiento de una alta concentración de restos culturales humanos anatómicamente modernos, de hace al menos 40 000 años, se encontró en Kostionki-Borshchiovo,[31]​ y en Sungir, que datan de hace 34 600 años —ambos, respectivamente, en Rusia occidental.[32]​ Los humanos llegaron al Ártico ruso hace al menos 40 000 años, en Mámontovaya Kurya.[33]​
El pastoreo nómada se desarrolló en la estepa póntica a partir del Calcolítico.[35]​ Se descubrieron restos de estas civilizaciones esteparias en lugares como Ipátovo,[35]​ Sintashta,[36]​ Arkaim y Pazyryk,[37]​[38]​ que contienen los primeros rastros conocidos de caballos en la guerra.[36]​ En la antigüedad clásica, la estepa póntica se conocía como Escitia.[39]​ A finales del siglo VIII a. C., los comerciantes griegos antiguos llevaron la civilización clásica a los emporios comerciales de Tanais y Fanagoria.[40]​
En los siglos III y IV d. C., existía el reino godo de Oium en el sur de Rusia, que más tarde fue invadido por los hunos.[41]​ Entre los siglos III y VI d. C., el Reino del Bósforo, que era un estado helenístico que sucedió a las colonias griegas,[42]​ también se vio abrumado por las invasiones nómadas dirigidas por tribus guerreras como los hunos y los ávaros euroasiáticos.[43]​ Los jázaros, de origen túrquico, gobernaron las estepas de la cuenca baja del Volga entre los mares Caspio y Negro hasta el siglo X.[44]​
Los antepasados de los rusos se encuentran entre las tribus eslavas que se separaron de los protoindoeuropeos, que aparecieron en la parte nororiental de Europa hace aprox. 1500 años.[45]​ Los eslavos orientales se asentaron gradualmente en el oeste de Rusia en dos oleadas: una que se movió desde Kiev hacia las actuales Súzdal y Múrom y otra desde Pólotsk hacia Nóvgorod y Rostov. Desde el siglo VII en adelante, los eslavos orientales constituyeron la mayor parte de la población en el oeste de Rusia y asimilaron lenta pero pacíficamente a los pueblos fínicos nativos.[46]​[41]​
El establecimiento de los primeros estados eslavos orientales en el siglo IX coincidió con la llegada de los varegos, los vikingos que se aventuraron a lo largo de las vías fluviales que se extendían desde el Báltico oriental hasta los mares Negro y Caspio.[47]​ Según la Crónica Primaria, un varego del pueblo Rus, llamado Rúrik, fue elegido gobernante de Nóvgorod en 862. En 882, su sucesor Oleg se aventuró al sur y conquistó Kiev, que anteriormente había estado pagando tributo a los jázaros.[41]​ Posteriormente, el hijo de Rúrik, Ígor, y el hijo de Ígor, Sviatoslav, sometieron a todas las tribus eslavas orientales locales al dominio de Kiev, destruyeron el jaganato jázaro y lanzaron varias expediciones militares a Bizancio y Persia.[48]​[49]​[50]​
En los siglos X y XI, la Rus de Kiev se convirtió en uno de los estados más grandes y prósperos de Europa. Los reinados de Vladímir el Grande (980–1015) y su hijo Yaroslav el Sabio (1019–1054) constituyen la Edad de Oro de Kiev, que vio la aceptación del cristianismo ortodoxo de Bizancio y la creación del primer código legal escrito en eslavo oriental, la Rússkaya Pravda.[41]​ Había llegado la era del feudalismo y la descentralización, marcada por constantes luchas internas entre los miembros de la dinastía ruríkida que gobernaba colectivamente la Rus de Kiev. El dominio de Kiev se desvaneció, en beneficio de Vladímir-Súzdal en el noreste, la República de Nóvgorod en el noroeste y Galitzia-Volinia en el suroeste.[41]​
La Rus de Kiev finalmente se desintegró, y el golpe final fue la invasión mongola entre 1237 y 1240, que dio como resultado el saqueo de Kiev y la muerte de una parte importante de la población de la Rus.[41]​ Los invasores, más tarde conocidos como tártaros, formaron el estado de la Horda de Oro, que saqueó los principados rusos y gobernó el sur y el centro de Rusia durante más de dos siglos.[51]​
Galitzia-Volinia finalmente fue asimilada por el Reino de Polonia, mientras que la República de Nóvgorod y Vladímir-Súzdal, dos regiones en la periferia de Kiev, establecieron las bases para la nación rusa moderna.[41]​ Liderados por el príncipe Alejandro Nevski, los habitantes de Nóvgorod repelieron a los invasores suecos en la batalla del Nevá en 1240,[52]​ así como a los cruzados germánicos en la batalla del Hielo en 1242.[53]​
El estado más poderoso que finalmente surgió después de la destrucción de la Rus de Kiev fue el Principado de Moscú, inicialmente era parte del Principado de Vladímir-Súzdal.[54]​ Mientras aún se encontraba bajo el yugo de los mongol-tártaros y con su connivencia, Moscú comenzó a afirmar su influencia en la región a principios del siglo XIV, convirtiéndose gradualmente en la fuerza líder en el proceso de reunificación de las tierras de la Rus y la expansión de Rusia.[55]​ El último rival de Moscú, la República de Nóvgorod, prosperó como el principal centro de comercio de pieles y el puerto más oriental de la Liga Hanseática.[56]​
El ejército unido de los principados rusos, liderado por el príncipe Dmitri Donskói de Moscú y ayudado por la Iglesia Ortodoxa Rusa, infligió una derrota histórica a los tártaros mongoles en la Batalla de Kulikovo en 1380.[41]​ Moscú absorbió gradualmente a su matriz Vladímir-Súzdal, y luego a los principados circundantes, incluidos rivales anteriormente fuertes como Tver y Nóvgorod.[54]​
Iván III «el Grande» finalmente se deshizo del control de la Horda de Oro y consolidó todo el norte de la Rus bajo el dominio de Moscú, y fue el primer gobernante ruso en tomar el título de «Gran príncipe de toda Rusia». Después de la caída de Constantinopla en 1453, Moscú reclamó la sucesión del legado del Imperio Romano de Oriente. Iván III se casó con Sofía Paleólogo, la sobrina del último emperador bizantino, Constantino XI, e hizo suyo el símbolo de la águila bicéfala bizantina, que finalmente aparecería en el escudo de armas de Rusia.[54]​
El gran duque Iván IV «El Terrible» fue coronado oficialmente como el primer zar de Rusia en 1547, durante el desarrollo de las ideas de la Tercera Roma. El zar promulgó un nuevo código de leyes (Sudébnik de 1550), estableció el primer organismo representativo feudal ruso (Zemski Sobor), renovó las fuerzas armadas, frenó la influencia del clero y reorganizó el gobierno local.[54]​ Durante su largo reinado, Iván casi duplicó el ya extenso territorio ruso al anexar los tres kanatos tártaros: Kazán y Astracán a lo largo del Volga,[57]​ y el kanato de Sibir en el suroeste de Siberia. Finalmente, a fines del siglo XVI, Rusia se expandió al este de los montes Urales.[58]​ Sin embargo, el zarato se vio debilitado por la larga y fallida guerra de Livonia contra la coalición del Reino de Polonia y el Gran Ducado de Lituania (de su posterior unión nació la Mancomunidad de Polonia-Lituania), el Reino de Suecia y Dinamarca-Noruega por el acceso a la costa báltica y el comercio marítimo.[59]​ En 1572, un ejército invasor de tártaros de Crimea fue completamente derrotado en la crucial batalla de Molodi.[60]​
La muerte de los hijos de Iván marcó el final de la antigua dinastía ruríkida en 1598 y, en combinación con la desastrosa hambruna de 1601-1603, condujo a una guerra civil, el gobierno de los pretendientes y la intervención extranjera durante la Época de la Inestabilidad a principios del siglo XVII.[61]​ La Mancomunidad de Polonia-Lituania, aprovechándose, ocupó partes de Rusia y se extendió hasta la capital, Moscú.[62]​ En 1612, los polacos se vieron obligados a retirarse por el cuerpo de voluntarios rusos, dirigido por el comerciante Kuzmá Minin y el príncipe Dmitri Pozharski.[63]​ La dinastía Románov accedió al trono en 1613 por decisión del Zemski Sobor, y el país inició su recuperación gradual de la crisis.[64]​
Rusia continuó su crecimiento territorial durante el siglo XVII, considerada la era de los cosacos.[65]​ En 1654, el líder cosaco, Bogdán Jmelnitski, ofreció colocar a Ucrania bajo la protección del zar ruso Alejo; cuya aceptación de esta oferta condujo a otra guerra ruso-polaca. En última instancia, Ucrania se dividió a lo largo del Dniéper, dejando la parte oriental (Ucrania del Margen Izquierdo y Kiev) bajo el mandato ruso.[66]​ En el este, continuó la rápida exploración y colonización rusa de la vasta Siberia, en busca de valiosas pieles y marfil. Los exploradores rusos avanzaron hacia el este principalmente a lo largo de las vías fluviales de Siberia y, a mediados del siglo XVII, había asentamientos rusos en el este de Siberia, en la península de Chukchi, a lo largo del río Amur y en la costa del Océano Pacífico.[65]​ En 1648, Semión Dezhniov se convirtió en el primer europeo en navegar por el estrecho de Bering.[67]​
Durante el reinado de Pedro el Grande, Rusia fue proclamada imperio en 1721 y se convirtió en una de las grandes potencias europeas. Pedro, que gobernó de 1682 a 1725, derrotó a Suecia en la gran guerra del Norte (1700-1721), asegurando el acceso de Rusia al mar y al comercio marítimo. En 1703, en el Mar Báltico, Pedro fundó San Petersburgo como la nueva capital de Rusia. A lo largo de su gobierno, se realizaron reformas radicales, que trajeron importantes influencias culturales de Europa occidental a Rusia.[68]​ El reinado de Isabel (1741-1762), hija de Pedro I, vio la participación de Rusia en la guerra de los Siete Años (1756-1763). Durante el conflicto, las tropas rusas invadieron Prusia Oriental e incluso llegaron a las puertas de Berlín.[69]​ Sin embargo, tras la muerte de Isabel, todas estas conquistas fueron devueltas al Reino de Prusia por el proprusiano Pedro III de Rusia.[70]​
Catalina II («la Grande»), que gobernó entre 1762 y 1796, presidió el Siglo de las Luces ruso. Extendió el control político ruso sobre la Mancomunidad de Polonia-Lituania e incorporó la mayoría de sus territorios a Rusia, convirtiéndolo en el país más poblado de Europa.[71]​ En el sur, después de las exitosas guerras ruso-turcas contra el Imperio otomano, Catalina extendió las fronteras de Rusia hasta el Mar Negro, disolvió el kanato de Crimea y anexó Crimea.[72]​ Como resultado de las victorias sobre el Irán kayarí a través de las guerras ruso-persas, en la primera mitad del siglo XIX, Rusia también obtuvo ganancias territoriales significativas en el Cáucaso.[73]​ Pablo I, hijo y sucesor de Catalina, era inestable y se concentraba predominantemente en asuntos domésticos.[74]​ Después de su breve reinado, la estrategia de Catalina continuó con Alejandro I (1801-1825) arrebatando Finlandia a la debilitada Suecia en 1809,[75]​ y Besarabia a los otomanos en 1812.[76]​ En América del Norte, los rusos se convirtieron en los primeros europeos en alcanzar y colonizar Alaska.[77]​ Entre 1803 y 1806 se realizó la primera circunnavegación rusa.[78]​ En 1820, una expedición rusa descubrió el continente de la Antártida.[79]​
Durante las guerras napoleónicas, Rusia se alió con varias potencias europeas y luchó contra Francia. La invasión francesa de Rusia en pleno apogeo del poder de Napoleón llegó a Moscú, en 1812, pero finalmente fracasó estrepitosamente ya que la obstinada resistencia en combinación con el gélido invierno ruso condujo a una derrota desastrosa de los invasores, en la que la Grande Armée paneuropea se enfrentó por completo. destrucción. El Ejército Imperial Ruso, dirigido por Mijaíl Kutúzov y Barclay de Tolly, derrocó a Napoleón y atravesó toda Europa en la guerra de la Sexta Coalición, finalmente entró en París.[80]​ Alejandro I controló la delegación de Rusia en el Congreso de Viena, que definió el mapa de la Europa posnapoleónica.[81]​
Los oficiales que persiguieron a Napoleón en Europa Occidental llevaron a Rusia las ideas del liberalismo e intentaron limitar los poderes del zar durante la frustrada revuelta decembrista de 1825.[82]​ El final del reinado conservador de Nicolás I (1825-1855), un periodo cenital del poder y la influencia de Rusia en Europa, se vio interrumpido por la derrota en la guerra de Crimea.[83]​ El sucesor de Nicolás, Alejandro II (1855-1881), promulgó importantes cambios en todo el país, incluida la reforma emancipadora de 1861.[84]​ Estas reformas impulsaron la industrialización y modernizaron el ejército imperial ruso, que liberó gran parte de los Balcanes del dominio otomano tras la guerra ruso-turca (1877-1878).[85]​ Durante la mayor parte del siglo XIX y principios del XX, Rusia y Gran Bretaña se confabularon en torno a Afganistán y sus territorios vecinos en Asia central y meridional; la rivalidad entre los dos grandes imperios europeos llegó a conocerse como el Gran Juego.[86]​
A fines del siglo XIX, se produjo el surgimiento de varios movimientos socialistas en Rusia. Alejandro II fue asesinado en 1881 por terroristas revolucionarios.[87]​ El reinado de su hijo Alejandro III (1881-1894) fue menos liberal pero más pacífico.[88]​ El último emperador ruso, Nicolás II (1894-1917), no pudo evitar los acontecimientos de la revolución rusa de 1905, desencadenada por la humillante guerra ruso-japonesa y el incidente de la manifestación conocido como Domingo Sangriento.[89]​[90]​ El levantamiento fue sofocado, pero el gobierno se vio obligado a conceder importantes reformas (Constitución rusa de 1906), incluida la concesión de las libertades de expresión y reunión, la legalización de los partidos políticos y la creación de un órgano legislativo electo, la Duma Imperial.[91]​
En 1914, Rusia entró en la Primera Guerra Mundial en respuesta a la declaración de guerra del Imperio austrohúngaro a Serbia,[92]​ aliado de Rusia, y luchó en múltiples frentes mientras estaba aislada de sus aliados de la Triple Entente.[93]​ En 1916, la ofensiva Brusílov del Ejército Imperial Ruso destruyó casi por completo al Ejército austrohúngaro.[94]​ Sin embargo, la desconfianza pública ya existente hacia el régimen se profundizó por los crecientes costos de la guerra, el alto número de bajas y los rumores de corrupción y traición. Todo esto formó el clima de la revolución rusa de 1917, llevada a cabo en dos grandes actos.[95]​ A principios de 1917, Nicolás II se vio obligado a abdicar; él y su familia fueron encarcelados y luego ejecutados en Ekaterimburgo durante la guerra civil rusa.[96]​
A raíz de la Revolución de Febrero, la monarquía fue reemplazada por una inestable coalición de partidos políticos que se autoproclamó Gobierno Provisional.[97]​ El Gobierno Provisional proclamó la República Rusa en septiembre. El 6 de enerojul./ 19 de enero de 1918greg., la Asamblea Constituyente Rusa declaró a Rusia una república federal democrática (ratificando así la decisión del Gobierno Provisional). Al día siguiente, la Asamblea Constituyente, en la que dominaban los socialistas revolucionarios, fue disuelta por el Comité Ejecutivo Central Panruso[95]​ dominado por los bolcheviques.
Coexistía un establecimiento socialista alternativo, el Sóviet de Petrogrado, que ejercía el poder a través de los consejos de trabajadores y campesinos elegidos democráticamente, llamados sóviets. El gobierno de las nuevas autoridades solo agravó la crisis del país en lugar de resolverla y, finalmente, la Revolución de Octubre, encabezada por el líder bolchevique Vladímir Lenin, derrocó al Gobierno Provisional y otorgó pleno poder al gobierno bolchevique, lo que condujo a la creación del primer estado socialista del mundo.[95]​ Estalló la Guerra Civil Rusa entre el movimiento blanco monárquico, socialdemócratas moderados, como los socialistas revolucionarios, y el nuevo régimen soviético con su Ejército Rojo.[98]​ A raíz de la firma del Tratado de Brest-Litovsk que puso fin a las hostilidades con las potencias centrales de la Primera Guerra Mundial; la Rusia bolchevique entregó la mayor parte de sus territorios occidentales, que albergaban al 34 % de su población, el 54 % de sus industrias, el 32 % de sus tierras agrícolas y aproximadamente el 90 % de sus minas de carbón.[99]​
Las potencias aliadas lanzaron una infructuosa intervención militar en apoyo de las fuerzas anticomunistas.[100]​ Al mismo tiempo, tanto los bolcheviques como el movimiento blanco llevaron a cabo campañas de deportaciones y ejecuciones de forma mutua, conocidas respectivamente como el Terror Rojo y el Terror Blanco.[101]​ Al final de la violenta guerra civil, la economía y la infraestructura de Rusia sufrieron graves daños y hasta 10 millones perecieron durante la guerra, en su mayoría eran civiles.[102]​ Millones se convirtieron en emigrados blancos y la hambruna rusa de 1921-1922 se cobró hasta cinco millones de víctimas.[103]​[104]​
Lenin y sus colaboradores, el 30 de diciembre de 1922, fundaron la Unión Soviética, uniendo la RSFS de Rusia en un solo estado con las repúblicas de Bielorrusia, Transcaucasia y Ucrania.[105]​ Con el tiempo, los cambios en las fronteras internas y las anexiones durante la Segunda Guerra Mundial crearon una unión de 15 repúblicas; la más grande en tamaño y población era la RSFS de Rusia, que lideró la unión durante toda su historia; política, cultural y económicamente.[106]​ Tras la muerte de Lenin en 1924, se designó una troika para hacerse cargo. Finalmente, Iósif Stalin, el Secretario General del Partido Comunista, logró suprimir todas las facciones de oposición y consolidar el poder en sus manos para convertirse en el único gobernante del país en la década de 1930.[107]​ León Trotski, el principal defensor de la revolución mundial, fue exiliado de la Unión Soviética en 1929,[108]​ y la idea de Stalin del socialismo en un solo país se convirtió en la política oficial.[109]​ La continua lucha interna dentro del partido bolchevique culminó con la Gran Purga.[110]​
Bajo el liderazgo de Stalin, el gobierno puso en marcha una economía planificada, la industrialización del país mayoritariamente rural y la colectivización de su agricultura. Durante este período de rápidos cambios económicos y sociales, millones de personas fueron enviadas a campos de trabajos forzados, incluidos muchos convictos políticos por su supuesta o real oposición al gobierno de Stalin;[111]​ y millones fueron deportados y exiliados a áreas remotas de la Unión Soviética.[112]​ La desorganización de la transición de la agricultura del país, combinada con las duras políticas estatales y la sequía, condujo a la hambruna soviética de 1932-1933; que mató hasta 8,7 millones de personas.[113]​ La Unión Soviética, en última instancia, hizo la costosa transformación de una economía mayoritariamente agraria a una gran potencia industrial en un corto período de tiempo.[114]​
La Unión Soviética entró en la Segunda Guerra Mundial el 17 de septiembre de 1939 con su invasión de Polonia,[115]​ de acuerdo con un protocolo dentro del Pacto Ribbentrop-Mólotov con la Alemania nazi.[116]​ Posteriormente la Unión Soviética invadió Finlandia,[117]​ ocupó y anexó las repúblicas bálticas,[118]​ así como partes de Rumania.[119]​: 91–95  El 22 de junio de 1941, Alemania rompió el tratado de no agresión e invadió la Unión Soviética;[120]​ lo que abrió el Frente Oriental, el escenario más grande de la Segunda Guerra Mundial.[121]​: 7 
Finalmente, unos 5 millones de soldados del Ejército Rojo fueron capturados por los nazis;[122]​: 272  éstos mataron deliberadamente de hambre o de otra forma a 3,3 millones de prisioneros de guerra soviéticos y a una gran cantidad de civiles, ya que el «Plan Hambre» buscaba cumplir con el Plan General del Este.[123]​: 175–186  Aunque la Wehrmacht tuvo un éxito inicial considerable, su ataque se detuvo en la batalla de Moscú.[124]​ Posteriormente, los alemanes sufrieron grandes derrotas, primero en la batalla de Stalingrado en el invierno entre 1942 y 1943;[125]​ y luego en la batalla de Kursk en el verano de 1943.[126]​ Otro fracaso alemán fue el sitio de Leningrado, en el que la ciudad fue totalmente bloqueada por tierra entre 1941 y 1944 por fuerzas alemanas y finlandesas, sufrió hambre y más de un millón de muertos, pero nunca se rindió.[127]​ Las fuerzas soviéticas atravesaron toda Europa central y oriental entre 1944 y 1945; capturaron Berlín en mayo de 1945.[128]​ En agosto de 1945, el ejército soviético invadió Manchuria y expulsó a los japoneses del noreste de Asia, lo que contribuyó a la victoria aliada sobre Japón.[129]​
El período de 1941 a 1945 de la Segunda Guerra Mundial se conoce en Rusia como la Gran Guerra Patria.[130]​ La Unión Soviética, junto con los Estados Unidos, el Reino Unido y China fueron considerados los Cuatro Grandes de las potencias aliadas en la Segunda Guerra Mundial, y más tarde se convirtieron en las Cuatro Potencias, que fue la base del Consejo de Seguridad de las Naciones Unidas.[131]​: 27  Durante la guerra, las muertes de civiles y militares soviéticos fueron entre 26 y 27 millones,[132]​ lo que representa aproximadamente la mitad de todas las perdidas civiles y militares de la Segunda Guerra Mundial.[133]​: 295  La economía y la infraestructura soviéticas sufrieron una devastación masiva, lo que provocó la hambruna soviética de 1946-1947.[134]​ Sin embargo, a costa de un gran sacrificio, la Unión Soviética emergió como una superpotencia mundial.[135]​
Después de la Segunda Guerra Mundial, partes de Europa Central y del Este, incluida Alemania Oriental y partes del este de Austria, fueron ocupadas por el Ejército Rojo según la Conferencia de Potsdam.[136]​ Se instalaron gobiernos comunistas dependientes en los estados satélites del Bloque del Este.[137]​ La Unión Soviética, después de convertirse en la segunda potencia nuclear del mundo,[138]​ estableció la alianza del Pacto de Varsovia y entró en una lucha por el dominio mundial,[139]​ conocida como la Guerra Fría, con sus rivales Estados Unidos y la OTAN.[140]​ Después de la muerte de Stalin en 1953 y un breve período de gobierno colectivo, el nuevo líder Nikita Jrushchov denunció a Stalin y lanzó la política de desestalinización, liberando a muchos presos políticos de los campos de trabajo del Gulag.[141]​ El relajamiento general de las políticas represivas se denominó más tarde como el deshielo de Jrushchov.[142]​ Al mismo tiempo, las tensiones de la Guerra Fría alcanzaron su punto máximo cuando los dos rivales se enfrentaron por el despliegue de los misiles Jupiter estadounidenses en Turquía y los misiles soviéticos en Cuba.[143]​
En 1957, la Unión Soviética lanzó el primer satélite artificial del mundo, el Sputnik 1, dando así comienzo a la era espacial.[144]​ El cosmonauta ruso Yuri Gagarin se convirtió en el primer ser humano en orbitar la Tierra, a bordo de la nave espacial tripulada Vostok 1, el 12 de abril de 1961.[145]​ Tras la destitución de Jrushchov en 1964, se produjo otro período de gobierno colectivo, hasta que Leonid Brézhnev se convirtió en el líder. La época de la década de 1970 y principios de la de 1980 fue designada más tarde como el estancamiento brezhneviano. La reforma de Kosygin de 1965 tenía como objetivo la descentralización parcial de la economía soviética.[146]​ En 1979, después de una revolución liderada por los comunistas en Afganistán, las fuerzas soviéticas invadieron el país y finalmente comenzaron la guerra afgano-soviética.[147]​ En mayo de 1988, los soviéticos comenzaron a retirarse de Afganistán debido a la oposición internacional, la persistente guerra de guerrillas antisoviética y la falta de apoyo de los ciudadanos soviéticos.[148]​
A partir de 1985, el último líder soviético, Mijaíl Gorbachov, que buscaba promulgar reformas liberales en el sistema soviético, introdujo las políticas de glasnost (apertura) y perestroika (reestructuración) en un intento por poner fin al período de estancamiento económico y democratizar el gobierno.[149]​ Sin embargo, esto condujo al surgimiento de fuertes movimientos nacionalistas y separatistas en todo el país.[150]​ Antes de 1991, la economía soviética era la segunda más grande del mundo, pero durante sus últimos años entró en crisis.[151]​
En 1991, la agitación económica y política comenzó a desbordarse cuando los países bálticos optaron por separarse de la Unión Soviética.[152]​ El 17 de marzo se llevó a cabo un referéndum en el que la gran mayoría de los ciudadanos participantes votaron a favor de convertir la Unión Soviética en una federación renovada.[153]​ En junio de 1991, Boris Yeltsin se convirtió en el primer presidente elegido directamente en la historia de Rusia cuando fue elegido presidente de la RSFS de Rusia.[154]​ En agosto de 1991, un intento de golpe de Estado por parte de miembros del gobierno de Gorbachov, dirigido contra Gorbachov y con el objetivo de preservar la Unión Soviética, condujo al final del Partido Comunista de la Unión Soviética.[155]​ El 25 de diciembre de 1991, tras la disolución de la Unión Soviética, junto con la Rusia contemporánea, surgieron otros catorce estados postsoviéticos.[156]​
El colapso económico y político de la Unión Soviética llevó a Rusia a una profunda y prolongada depresión. Durante y después de la desintegración de la Unión Soviética, se emprendieron reformas de amplio alcance, incluidas la Privatización y la liberalización del mercado y el comercio, incluidos cambios radicales en el sentido de la «terapia de choque».[157]​ La privatización cambió en gran medida el control de las empresas de las agencias estatales a personas con conexiones internas en el gobierno, lo que condujo al surgimiento de los infames oligarcas rusos.[158]​ Muchos de los nuevos ricos movieron miles de millones en efectivo y activos fuera del país en una enorme fuga de capitales.[159]​ La depresión de la economía condujo al colapso de los servicios sociales — la tasa de natalidad se desplomó mientras que la tasa de mortalidad se disparó y millones se hundieron en la pobreza;[160]​[161]​[162]​ mientras que la corrupción extrema,[163]​ así como las bandas criminales y el crimen organizado aumentaron significativamente.[164]​
A fines de 1993, las tensiones entre Yeltsin y el parlamento ruso culminaron en una crisis constitucional que terminó violentamente por medio de la fuerza militar. Durante la crisis, Yeltsin fue respaldada por los gobiernos occidentales y más de 100 personas murieron.[165]​ En diciembre, se llevó a cabo y aprobó un referéndum que introdujo una nueva constitución, lo que otorgó al presidente amplios poderes.[166]​ La década de 1990 estuvo plagada de conflictos armados en el Cáucaso Norte, tanto escaramuzas étnicas locales como insurrecciones islamistas separatistas.[167]​ Desde el momento en que los separatistas chechenos declararon su independencia a principios de la década de 1990, se libró una intermitente guerra de guerrillas entre los grupos rebeldes y las fuerzas rusas.[168]​ Los separatistas chechenos llevaron a cabo ataques terroristas contra civiles, que cobraron la vida de miles de civiles rusos.[169]​
Tras la disolución de la Unión Soviética, Rusia asumió la responsabilidad de saldar las deudas externas de esta última.[170]​ En 1992, se eliminaron la mayoría de los controles de precios al consumidor, lo que provocó una inflación extrema y una devaluación significativa del rublo.[171]​ Los altos déficits presupuestarios, junto con el aumento de la fuga de capitales y la incapacidad para pagar las deudas, provocaron la crisis financiera rusa de 1998, que resultó en una mayor caída del PIB.[172]​
En 1999, el presidente Yeltsin renunció inesperadamente y entregó el cargo al primer ministro recientemente designado y su sucesor elegido, Vladímir Putin.[173]​ Putin luego ganó las elecciones presidenciales de 2000 y derrotó a los separatistas chechenos en la segunda guerra chechena.[174]​[175]​ Él obtuvo un segundo mandato presidencial en 2004.[176]​ Los altos precios del petróleo y el aumento de la inversión extranjera hicieron que la economía rusa se expandiera significativamente durante nueve años consecutivos.[177]​ El gobierno de Putin aumentó la estabilidad, lo que mejoró la calidad de vida y aumentó la influencia de Rusia en el escenario mundial.[178]​ En 2008, Dmitri Medvédev fue elegido presidente de Rusia, mientras que Putin asumió el cargo de primer ministro tras haber alcanzado el límite legal de mandatos.[179]​
Una crisis diplomática con la vecina Georgia; desembocó en la guerra ruso-georgiana, tuvo lugar del 1 al 12 de agosto de 2008, lo que provocó que Rusia impusiera dos estados no reconocidos en el territorio de Georgia. Fue la primera guerra europea del siglo XXI.[180]​
En 2014, tras el Euromaidán en Ucrania, Rusia anexó la península de Crimea del país vecino,[181]​ lo que contribuyó al estallido de la guerra en el este de Ucrania.[182]​ El 21 de febrero de 2022, el Gobierno ruso reconoció la independencia de las provincias ucranianas de Donetsk y Lugansk autoproclamadas en 2014 con el apoyo de Moscú.[183]​ La guerra ruso-ucraniana escaló abruptamente el 24 de febrero de 2022, cuando Rusia lanzó una invasión a gran escala de Ucrania,[184]​ convirtiéndose en la guerra convencional más grande en Europa desde la Segunda Guerra Mundial.[185]​ Estas acciones fueron respondidas con una condena generalizada en los países occidentales[186]​ y una ampliación de las sanciones contra Rusia.[187]​[188]​[189]​ A su vez, Rusia respondió con su retirada del Consejo de Europa en marzo, tras la cual fue suspendida del Consejo de Derechos Humanos de las Naciones Unidas en abril.[190]​[191]​ A junio de 2022, las fuerzas rusas ocupaban parte de seis de los veinticuatro óblasts ucranianos, aproximadamente una quinta parte del territorio del país.[192]​
Según su constitución, la Federación de Rusia es una república federal asimétrica,[193]​ con un sistema semipresidencialista, en el que el presidente es el jefe de estado y el primer ministro es el jefe de gobierno.[194]​ Está estructurado como una democracia representativa multipartidista, con el gobierno federal compuesto por tres poderes:[195]​
El presidente de Rusia es el jefe de estado, protector de la Constitución, los derechos y libertades de los ciudadanos y debe tomar cualquier medida para proteger la integridad de la soberanía rusa.[198]​ Es él quien representa a Rusia en las reuniones diplomáticas. El presidente es elegido por voto popular por un período de seis años y no puede ser elegido más de dos veces.[199]​ Los ministerios del gobierno están compuestos por el primer ministro y sus diputados, ministros y otras personas seleccionadas; todos son nombrados por el presidente por recomendación del primer ministro (mientras que el nombramiento de este último requiere el consentimiento de la Duma del Estado). Rusia Unida es el partido político dominante en Rusia y que a su vez es partido de gobierno, ha sido descrito como «atrapalotodo».[200]​[201]​ El actual presidente de Rusia es Vladímir Putin, en el cargo desde marzo de 2012, que sucede a Dmitri Medvédev.[202]​
En materia de derechos humanos, respecto a la pertenencia a los siete organismos de la Carta Internacional de Derechos Humanos, que incluyen al Comité de Derechos Humanos (HRC), Rusia ha firmado o ratificado:
La Federación de Rusia es reconocida en el Derecho internacional como continuidad de la personalidad jurídica de la antigua Unión Soviética.[213]​ En 2019, Rusia tenía la quinta red diplomática más grande del mundo. Mantiene relaciones diplomáticas con 190 estados miembros de las Naciones Unidas, cuatro estados parcialmente reconocidos y tres estados observadores de las Naciones Unidas; junto con 144 embajadas.[214]​ Rusia es uno de los cinco miembros permanentes del Consejo de Seguridad de las Naciones Unidas. Es considerada una gran potencia; es miembro del G20, la OSCE y la APEC.[215]​ Rusia también asume un rol de liderazgo en organizaciones como la CEI,[216]​ UEEA,[217]​ OTSC,[218]​ OCS y BRICS.[219]​[220]​
Rusia mantiene estrechas relaciones con la vecina Bielorrusia, que forman parte del Estado de la Unión, una confederación supranacional de esta última con Rusia.[221]​ Serbia ha sido un aliado cercano históricamente de Rusia, ya que ambos países comparten una fuerte afinidad cultural, étnica y religiosa mutua.[222]​ India es el mayor cliente de equipo militar ruso y los dos países comparten una fuerte relación estratégica y diplomática desde la era soviética.[223]​ Rusia ejerce una enorme influencia en Transcaucasia y Asia Central, geopolíticamente importantes; por lo que estas dos regiones se consideran bajo la esfera de influencia de Rusia.[224]​[225]​
En el siglo XXI, las relaciones entre Rusia y China se han fortalecido significativamente a nivel bilateral y económico; debido a intereses políticos compartidos.[226]​ Turquía y Rusia comparten una compleja relación estratégica, energética y de defensa.[227]​ Rusia mantiene relaciones cordiales con Irán, ya que es un aliado estratégico y económico.[228]​ Rusia también ha comenzado a expandir cada vez más su influencia en el Ártico,[229]​ Asia-Pacífico,[230]​ África,[231]​ Medio Oriente y América Latina.[232]​[233]​ Por el contrario, las relaciones de Rusia con el mundo occidental; especialmente Estados Unidos, la Unión Europea y la OTAN; han empeorado.[234]​
Las Fuerzas Armadas de Rusia se dividen en las Fuerzas Terrestres, la Armada y las Fuerzas Aeroespaciales — también hay dos ramas de servicio independientes: las Tropas de Misiles Estratégicos y las Tropas Aerotransportadas.[4]​ A partir de 2021, las fuerzas armadas tienen alrededor de un millón de personal en servicio activo, que es el quinto más grande del mundo, además de entre 2 y 20 millones de personal de reserva.[236]​[237]​ Es obligatorio que todos los ciudadanos varones de 18 a 27 años sean reclutados por un año de servicio en las Fuerzas Armadas.[4]​
Rusia cuenta con el segundo ejército más poderoso del mundo.[16]​ Es uno de los cinco países con armas nucleares ratificados, tiene el arsenal de armas nucleares más grande del mundo; más de la mitad de las armas nucleares del mundo son propiedad de Rusia.[238]​ Rusia posee la segunda flota más grande de submarinos de misiles balísticos y es uno de los únicos tres países que operan bombarderos estratégicos.[239]​[240]​ Posee la primera fuerza terrestre, así como de la segunda fuerza aérea y flota naval más poderosas del mundo.[241]​[242]​[243]​ Rusia mantiene el cuarto gasto militar más alto del mundo, con un gasto de 61 700 millones de dólares en 2020.[17]​ Es el segundo exportador de armas más grande del mundo, tiene una industria de defensa grande y completamente local, que produce la mayor parte de su propio equipo militar.[244]​
Según la constitución, la Federación Rusa está compuesta por 89 sujetos federales. En 1993, cuando se adoptó la nueva constitución, había 89 sujetos federales enumerados, pero algunos se fusionaron posteriormente. Los sujetos federales tienen igual representación —dos delegados cada uno— en el Consejo de la Federación, la cámara alta de la Asamblea Federal.[245]​ Sin embargo, difieren en el grado de autonomía que disfrutan.[246]​ Los distritos federales de Rusia fueron establecidos por Putin en el año 2000 para facilitar el control del gobierno central sobre los sujetos federales.[247]​ Originalmente siete, actualmente hay ocho distritos federales, cada uno encabezado por un enviado designado por el presidente.[248]​
La vasta masa terrestre de Rusia se extiende por la parte más oriental de Europa y la parte más septentrional de Asia.[255]​ Se extiende por el borde más septentrional de Eurasia; y tiene la cuarta línea de costa más larga del mundo, con más de 37 653 km.[256]​ Rusia se encuentra entre las latitudes 41° y 82° N, y las longitudes 19° E y 169° O, extendiéndose unos 9000 km de este a oeste y de 2500 a 4000 km de norte a sur.[257]​ Rusia, según su masa terrestre, es más grande que tres continentes y tiene la misma superficie que Plutón.[258]​
Rusia tiene nueve cadenas montañosas principales, y se encuentran a lo largo de las regiones más meridionales, que comparten una parte importante de la cordillera del Cáucaso (que contiene el monte Elbrús, que con 5642 m es el pico más alto de Rusia y Europa);[4]​ el macizo de Altái y los montes Sayanes en Siberia; y en las montañas de Siberia oriental y la península de Kamchatka en el Extremo Oriente ruso (se encuentra Kliuchevskoi, que a 4750 m es el volcán activo más alto de Eurasia).[259]​[260]​ Los montes Urales, que se extienden de norte a sur por el oeste del país, son ricos en recursos minerales y forman el límite tradicional entre Europa y Asia.[261]​ El punto más bajo de Rusia y Europa, está situado en la cabecera del Mar Caspio, donde la depresión del Caspio alcanza unos 29 metros bajo el nivel del mar.[262]​
Rusia es uno de los únicos tres países del mundo que bordean tres océanos,[255]​ por lo que tiene vínculos con una gran cantidad de mares.[263]​ Sus principales islas y archipiélagos incluyen Nueva Zembla, la Tierra de Francisco José, la Tierra del Norte, las islas de Nueva Siberia, la isla de Wrangel, las islas Kuriles y Sajalín.[264]​[265]​ Las islas Diómedes, administradas por Rusia y los Estados Unidos, están a solo 3,8 km de distancia;[266]​ y Kunashir, parte de las islas Kuriles, está a solo 20 km de Hokkaido, Japón.[267]​
Hogar de más de 100 000 ríos,[255]​ Rusia tiene uno de los recursos de agua superficial más grandes del mundo, con lagos que contienen aproximadamente una cuarta parte del agua dulce en estado líquido del mundo.[260]​ El lago Baikal, el más grande y prominente entre los cuerpos de agua dulce de Rusia, es el lago de agua dulce más profundo, puro, antiguo y de mayor capacidad del mundo, y contiene más de una quinta parte del agua dulce superficial del mundo.[268]​ Los lagos Ládoga y Onega en el noroeste de Rusia son dos de los lagos más grandes de Europa.[255]​ Rusia ocupa el segundo lugar después de Brasil por el total de recursos hídricos renovables.[269]​ El río Volga en el oeste de Rusia, ampliamente considerado como el río nacional de Rusia, es el río más largo de Europa; y forma el delta del Volga, el delta fluvial más grande del continente.[270]​ Los ríos siberianos Ob, Yeniséi, Lena y Amur se encuentran entre los ríos más largos del mundo.[271]​
El tamaño de Rusia y la lejanía de muchas de sus zonas del mar dan como resultado el predominio del clima húmedo continental en la mayor parte del país, excepto en la tundra y el extremo suroeste. Las cadenas montañosas en el sur y el este obstruyen el flujo de masas de aire cálido de los océanos Índico y Pacífico, mientras que la gran llanura europea que se extiende por el oeste y el norte la abre a la influencia de los océanos Atlántico y Ártico.[272]​ La mayor parte del noroeste de Rusia y Siberia tienen un clima subártico, con inviernos extremadamente severos en las regiones interiores del noreste de Siberia (principalmente Sajá, donde se encuentra el polo norte del frío con una temperatura mínima récord de -71,2 °C) e inviernos más moderados en otros lugares.[264]​ La vasta costa de Rusia a lo largo del océano Ártico y las islas del Ártico ruso tienen un clima polar.[272]​
La parte costera del krai de Krasnodar en el mar Negro, sobre todo Sochi, y algunas franjas costeras e interiores del Cáucaso septentrional poseen un clima húmedo subtropical con inviernos templados y húmedos.[272]​ En muchas regiones del este de Siberia y el Lejano Oriente ruso, el invierno es seco en comparación con el verano; mientras que otras partes del país experimentan precipitaciones más uniformes en todas las estaciones. La precipitación invernal en la mayor parte del país suele caer en forma de nieve. Las partes más occidentales del óblast de Kaliningrado y algunas partes del sur del krai de Krasnodar y el Cáucaso septentrional tienen un clima oceánico.[272]​ La región a lo largo de la costa del Bajo Volga y del mar Caspio, así como algunas franjas más al sur de Siberia, poseen un clima semiárido.[273]​
En gran parte del territorio, solo hay dos estaciones bien diferenciadas, invierno y verano; ya que la primavera y el otoño suelen ser períodos breves de cambio entre temperaturas extremadamente bajas y extremadamente altas. El mes más frío es enero (febrero en la costa); el más cálido suele ser julio. Son típicos altos rangos de temperatura. En invierno, las temperaturas son más frías tanto de sur a norte como de oeste a este. Los veranos pueden ser bastante calurosos, incluso en Siberia.[274]​ El cambio climático en Rusia provoca incendios forestales más frecuentes, que descongelan la gran extensión de permafrost del país.[275]​[276]​
Rusia, gracias a su gigantesco tamaño, presenta diversos ecosistemas, que incluyen los desiertos polares, la tundra, la tundra forestal, la taiga, los bosques templados de frondosas y mixtos, la estepa forestal, la estepa, el semidesierto y los subtrópicos.[277]​ Aproximadamente la mitad del territorio de Rusia está cubierto de bosques y tiene las reservas forestales más grandes del mundo,[4]​[278]​ que absorben algunas de las cantidades más altas de dióxido de carbono del mundo.[279]​[280]​
La biodiversidad de Rusia incluye 12 500 especies de plantas vasculares, 2200 especies de briófitas, alrededor de 3000 especies de líquenes, 7000 a 9000 especies de algas y de 20 000 a 25 000 especies de hongos. La fauna de Rusia está compuesta por 320 especies de mamíferos, más de 732 especies de aves, 75 especies de reptiles, unas 30 especies de anfibios, 343 especies de peces de agua dulce (alto endemismo), aproximadamente 1500 especies de peces de agua salada, 9 especies de ciclostomas y aproximadamente de 100 000 a 150 000 invertebrados (alto endemismo).[277]​[281]​ Aproximadamente 1100 especies de plantas y animales raros, y en peligro de extinción están incluidas en el Libro Rojo de Rusia.[277]​
Los ecosistemas totalmente naturales de Rusia se conservan en casi 15 000 territorios naturales especialmente protegidos de varios estatutos, que ocupan más del 10% del área total del país.[277]​ Incluyen 45 reservas de la biósfera,[282]​ 64 parques nacionales y 101 reservas naturales.[283]​ Rusia aún conserva muchos ecosistemas que aún no han sido tocados por el hombre; principalmente en las áreas del norte de la taiga y la tundra subártica de Siberia. Rusia tuvo una puntuación media en el Índice de integridad del paisaje forestal de 9.02 en 2019, ocupando el décimo lugar entre 172 países; y la primera nación importante clasificada a nivel mundial.[284]​
Rusia presenta una economía mixta,[287]​ cuenta con enormes recursos naturales, en particular petróleo y gas natural.[288]​ Tiene la undécima economía más grande del mundo por PIB nominal y la sexta más grande por PPA. En 2017, el gran sector de servicios contribuyó con el 62 % del PIB total, el sector industrial con el 32 % y el sector agrícola con aproximadamente el 5 %. Rusia tiene una baja tasa de desempleo del 4.5 % y más del 70 % de su población se clasifica como clase media.[289]​[290]​ Las reservas de divisas de Rusia tienen un valor de USD 638 000 millones y son las cuartas más grandes del mundo.[291]​ Tiene una fuerza laboral de aproximadamente 70 millones, que es la sexta más grande del mundo.[292]​ La gran industria automotriz de Rusia se ubica como la décima más grande del mundo por producción.[293]​
Rusia es el decimotercer mayor exportador y el vigésimo primer importador del mundo.[294]​[295]​ El sector del petróleo y el gas representó el 45 % de los ingresos del presupuesto federal de Rusia en enero de 2022, y hasta el 60 % de sus exportaciones en 2019.[296]​[297]​ En 2019, el Ministerio de Recursos Naturales y Medio Ambiente estimó que el valor de los recursos naturales era el 60 % del PIB del país.[298]​ Rusia tiene uno de los niveles más bajos de deuda externa entre las principales economías,[299]​ aunque la desigualdad de ingresos y riqueza de los hogares es una de las más altas entre los países desarrollados.[300]​
El sector agrícola de Rusia aporta alrededor del 5 % del PIB total del país, aunque el sector emplea alrededor de una octava parte de la fuerza laboral total.[301]​ Posee la tercera área cultivada más grande del mundo, con 1 265 267 km². Sin embargo, debido a la dureza de su entorno, alrededor del 13.1 % de su tierra es agrícola y solo el 7.4 % de su tierra es cultivable.[4]​[302]​ Las tierras agrícolas del país se consideran parte del «granero» de Europa.[303]​ Más de un tercio de la superficie sembrada se dedica a cultivos forrajeros y el resto de las tierras de cultivo se dedica a cultivos industriales, hortalizas y frutas.[301]​ El principal producto de la agricultura rusa siempre ha sido el grano, que ocupa considerablemente más de la mitad de las tierras de cultivo.[301]​ Rusia es el mayor exportador mundial de trigo,[304]​[305]​ el mayor productor de cebada y trigo sarraceno, uno de los mayores exportadores de maíz y aceite de girasol, y el principal productor de fertilizantes.[306]​
Varios analistas de la adaptación al cambio climático prevén grandes oportunidades para la agricultura rusa durante el resto del siglo XXI, a medida que aumenta la capacidad de cultivo en Siberia, lo que conduciría a la migración interna y externa a la región.[307]​ Debido a su gran costa a lo largo de tres océanos y doce mares marginales, Rusia mantiene la sexta industria pesquera más grande del mundo; capturó casi 5 millones de toneladas de pescado en 2018.[308]​ Es el hogar del mejor caviar del mundo, el esturión beluga; produce alrededor de un tercio de todo el pescado enlatado y alrededor de una cuarta parte del pescado fresco y congelado total del mundo.[301]​
Según la Organización Mundial del Turismo, Rusia fue el decimosexto país más visitado del mundo y el décimo país más visitado de Europa en 2018, con más de 24.6 millones de visitantes.[309]​ Rusia ocupó el puesto 39 en el Informe de Competitividad de Viajes y Turismo de 2019.[310]​ Según la Agencia Federal para el Turismo, la cantidad de viajes entrantes de ciudadanos extranjeros a Rusia ascendió a 24.4 millones en 2019.[311]​ Los ingresos por turismo internacional de Rusia en 2018 ascendieron a USD 11 600 millones.[309]​ En 2019, los viajes y el turismo representaron alrededor del 4.8 % del PIB total del país.[312]​
Las principales rutas turísticas de Rusia incluyen un viaje por el Anillo de Oro de Rusia, una ruta temática de antiguas ciudades rusas, cruceros por grandes ríos como el Volga, caminatas por cadenas montañosas como la cordillera del Cáucaso y viajes en el famoso Ferrocarril Transiberiano.[313]​[314]​ Los lugares de interés más visitados y populares de Rusia incluyen la Plaza Roja, el Palacio Peterhof, el Kremlin de Kazán, el Laura de la Trinidad y San Sergio y el lago Baikal.[315]​
Moscú, la capital cosmopolita y el centro histórico de la nación, es una rebosante megaciudad. Conserva su arquitectura clásica y de la era soviética; mientras cuenta con arte superior, ballet de clase mundial y modernos rascacielos.[316]​ San Petersburgo, la capital imperial, es famosa por su arquitectura clásica, catedrales, museos y teatros, noches blancas, ríos entrecruzados y numerosos canales.[317]​ Rusia es famosa en todo el mundo por sus ricos museos, como el Museo Estatal Ruso, el Museo del Hermitage y la Galería Tretiakov; y por teatros como el Bolshói y el Mariinski. El Kremlin de Moscú y la Catedral de San Basilio se encuentran entre los hitos culturales de Rusia.[318]​
El transporte ferroviario en Rusia está principalmente bajo el control de la empresa estatal Ferrocarriles Rusos. La longitud total de las vías férreas de uso común es la tercera más larga del mundo y supera los 87 000 km.[321]​ A partir de 2016, Rusia tiene la quinta red de carreteras más grande del mundo, con 1 452 000 km de carreteras,[322]​ mientras que la densidad de carreteras se encuentra entre las más bajas del mundo.[323]​ Las vías navegables interiores de Rusia son las más largas del mundo y suman 102 000 km.[324]​ Sus oleoductos suman unos 251 800 km y son los terceros más largos del mundo.[325]​ Entre los 1218 aeropuertos de Rusia,[326]​ el más transitado es el Aeropuerto Internacional de Moscú-Sheremétievo. El puerto más grande de Rusia es el Puerto de Novorosíisk en el krai de Krasnodar al lado del Mar Negro.[327]​
Rusia es ampliamente descrita como una superpotencia energética.[328]​ Tiene las primeras reservas probadas de gas,[329]​ las segundas reservas de carbón y las octavas reservas de petróleo más grandes del mundo;[330]​[331]​ y las reservas de esquistos bituminosos más grandes de Europa.[332]​ Rusia también es el principal exportador de gas natural,[333]​ el segundo mayor productor de gas natural,[334]​ y el segundo mayor productor y exportador de petróleo del mundo.[335]​[336]​ La producción de petróleo y gas de Rusia ha dado lugar a relaciones económicas profundas con la Unión Europea, China y los estados de la antigua Unión Soviética y del Bloque del Este.[337]​[338]​ Por ejemplo, durante la última década, la participación de Rusia en el suministro de la demanda total de gas de la Unión Europea (incluido el Reino Unido) aumentó del 25 % en 2009 al 32 % en febrero de 2022.[338]​ Rusia depende en gran medida de ingresos por impuestos y tarifas de exportación relacionados con el petróleo y el gas, que representaron el 45 % de su presupuesto federal en enero de 2022.[297]​
Rusia está comprometida con el Acuerdo de París, luego de unirse formalmente al pacto en 2019.[339]​ Las emisiones de gases de efecto invernadero de Rusia son las cuartas más grandes del mundo.[340]​ Rusia es el cuarto mayor productor de electricidad y el noveno mayor productor de energía renovable del mundo en 2019.[341]​[342]​ También fue el primer país del mundo en desarrollar energía nuclear para fines civiles y en construir la primera planta de energía nuclear del mundo.[343]​ Rusia también fue el cuarto mayor productor de energía nuclear en 2019 y fue el quinto mayor productor hidroeléctrico del mundo en 2021.[344]​[345]​
Hay 400 agencias de noticias en Rusia, entre las cuales las más importantes que operan internacionalmente son TASS, RIA Novosti, Sputnik e Interfax.[347]​ La televisión es el medio más popular en Rusia.[348]​ Entre las 3000 estaciones de radio con licencia en todo el país, las más notables incluyen Radio Rossii, Vesti FM, Eco de Moscú, Radio Mayak y Rússkoye Radio. De los 16 000 periódicos registrados Argumenty i Fakty, Komsomólskaya Pravda, Rossíiskaya Gazeta, Izvestia y Moskovski Komsomolets son los más populares. Piervy Kanal y Rossiya 1 son los principales canales de noticias, mientras que RT es la cadena insignia de las operaciones de medios internacionales de Rusia.[348]​ Rusia tiene el mercado de videojuegos más grande de Europa, con más de 65 millones de jugadores en todo el país.[349]​
Rusia gastó alrededor del 1 % de su PIB en investigación y desarrollo en 2019, con el décimo presupuesto más alto del mundo.[350]​ También ocupó el décimo lugar a nivel mundial en el número de publicaciones científicas en 2020, con aproximadamente 1.3 millones de artículos.[351]​ Desde 1904, se han otorgado los premios Nobel a 26 soviéticos y rusos en física, química, medicina, economía, literatura y paz.[352]​ Rusia ocupó el puesto 45 en el Índice global de innovación de la OMPI en 2021,[353]​y el lugar 47 en 2022.[354]​
Mijaíl Lomonósov propuso la conservación de la masa en las reacciones químicas, descubrió la atmósfera de Venus y fundó la geología moderna.[355]​ Desde los tiempos de Nikolái Lobachevski, pionero de la geometría no euclidiana, y Pafnuti Chebyshov, un destacado tutor; los matemáticos rusos se convirtieron en los más influyentes del mundo.[356]​ Dmitri Mendeléyev inventó la tabla periódica, el marco principal de la química moderna.[357]​ Sofia Kovalévskaya fue pionera entre las mujeres en matemáticas en el siglo XIX.[358]​ Nueve matemáticos soviéticos y rusos han sido galardonados con la Medalla Fields. Grigori Perelmán recibió el primer premio de los Problemas del milenio, por su demostración final de la conjetura de Poincaré en 2002, así como la Medalla Fields en 2006.[359]​
Aleksandr Popov fue uno de los inventores de la radio,[360]​ mientras que Nikolái Basov y Aleksandr Prójorov fueron coinventores del láser y el máser.[361]​ Zhores Alferov contribuyó significativamente a la creación de la física y la electrónica de heteroestructuras modernas.[362]​ Oleg Lósev realizó contribuciones cruciales en el campo de las uniones de semiconductores y descubrió los diodos emisores de luz (LED).[363]​ Vladímir Vernadski es considerado uno de los fundadores de la geoquímica, la biogeoquímica y la radiogeología.[364]​ Iliá Méchnikov es conocido por su innovadora investigación en inmunología.[365]​ Iván Pávlov es conocido principalmente por su trabajo en el condicionamiento clásico.[366]​ Lev Landáu hizo contribuciones fundamentales a muchas áreas de la física teórica.[367]​
Nikolái Vavílov fue más conocido por haber identificado los centros de origen de las plantas cultivadas.[368]​ Trofim Lysenko fue conocido principalmente por el lysenkoísmo.[369]​ Muchos científicos e inventores rusos famosos eran emigrantes blancos. Ígor Sikorski fue un pionero de la aviación.[370]​ Vladímir Zvorikin fue el inventor de sistemas de televisión como el iconoscopio y el kinescopio.[371]​ Theodosius Dobzhansky fue la figura central en el campo de la biología evolutiva por su trabajo en la configuración de la síntesis evolutiva moderna.[372]​ Gueorgui Gámov fue uno de los principales defensores de la teoría del Big Bang.[373]​ Muchos científicos extranjeros vivieron y trabajaron en Rusia durante un largo período, como Leonhard Euler y Alfred Nobel.[374]​[375]​
Roscosmos es la agencia espacial nacional de Rusia. Los logros del país en el campo de la tecnología espacial y la exploración espacial se remontan a Konstantín Tsiolkovski, el padre de la astronáutica teórica, cuyas obras inspiraron a los principales ingenieros de cohetes soviéticos, como Serguéi Koroliov, Valentín Glushkó y muchos otros que contribuyeron al éxito del programa espacial soviético en las primeras etapas de la carrera espacial.[377]​: 6–7, 333 
En 1957 se lanzó el primer satélite artificial en órbita terrestre, el Sputnik 1. En 1961, el primer viaje humano al espacio fue realizado con éxito por Yuri Gagarin. Siguieron muchos otros registros de exploración espacial soviéticos y rusos. En 1963, Valentina Tereshkova se convirtió en la primera y la más joven mujer en el espacio, después de volar en una misión en solitario en el Vostok 6.[378]​ En 1965, Alekséi Leónov se convirtió en el primer ser humano en realizar una caminata espacial, saliendo de la cápsula espacial a bordo del Vosjod 2.[379]​
En 1957; Laika, una perra del programa espacial soviético, se convirtió en el primer animal en orbitar la Tierra a bordo del Sputnik 2.[380]​ En 1966, Luna 9 se convirtió en la primera nave espacial en lograr un aterrizaje exitoso sobre un cuerpo celeste, la Luna.[381]​ En 1968, Zond 5 llevó a los primeros terrícolas (dos tortugas y otras formas de vida) para circunnavegar la Luna.[382]​ En 1970, Venera 7 se convirtió en la primera nave espacial en aterrizar en otro planeta, Venus.[383]​ En 1971, Mars 3 se convirtió en la primera nave espacial en aterrizar en Marte.[384]​: 34–60  Durante el mismo período, Lunojod 1 se convirtió en el primer rover de exploración espacial,[385]​ mientras que Saliut 1 se convirtió en la primera estación espacial del mundo.[386]​ En abril de 2022, Rusia tenía 172 satélites activos en el espacio, la tercera flota de satélites más grande del mundo.[387]​
Rusia es uno de los países menos densamente poblados y más urbanizados del mundo,[4]​ con la gran mayoría de su población concentrada en su parte occidental.[388]​ Tenía una población de 142.8 millones según el censo de 2010,[389]​ que aumentó a aproximadamente 145.5 millones en 2022.[11]​ Rusia es el país más poblado de Europa y el noveno país más poblado del mundo, con una densidad de población de 9 habitantes por kilómetro cuadrado.[390]​

Desde la década de 1990, la tasa de mortalidad de Rusia ha superado su tasa de natalidad, lo que los analistas han calificado como una crisis demográfica.[392]​ En 2019, la tasa total de fertilidad en Rusia se estimó en 1.5 hijos nacidos por mujer,[393]​ que está por debajo de la tasa de reemplazo de 2.1, y es una de las tasas de fertilidad más bajas del mundo.[394]​ Asimismo, la nación tiene una de las poblaciones más envejecidas del mundo, con una edad media de 40.3 años.[4]​ En 2009 registró un crecimiento demográfico anual por primera vez en quince años; y desde la década de 2010, Rusia ha experimentado un mayor crecimiento de la población debido a la disminución de las tasas de mortalidad, el aumento de las tasas de natalidad y el aumento de la inmigración.[395]​ Sin embargo, desde 2020, debido al exceso de muertes por la pandemia de COVID-19, la población de Rusia ha sufrido la mayor disminución de su historia en tiempos de paz.[396]​
Rusia es un estado multinacional con muchas entidades subnacionales asociadas con distintas minorías.[397]​ Hay más de 193 grupos étnicos en todo el país. En el censo de 2010, aproximadamente el 81 % de la población era de etnia rusa y el 19 % restante de la población eran minorías étnicas;[398]​ más de las cuatro quintas partes de la población de Rusia eran descendientes de europeos, de los cuales la gran mayoría eran eslavos,[399]​ con una minoría sustancial de pueblos fínicos y germánicos.[400]​[401]​ Según la Organización de Naciones Unidas, la población inmigrante de Rusia es la tercera más grande del mundo, con más de 11.6 millones;[402]​ la mayoría de los cuales son de estados postsoviéticos, principalmente ucranianos.[403]​

El ruso es el idioma oficial y predominantemente hablado en Rusia.[405]​ Es el idioma nativo más hablado en Europa, el idioma más extendido geográficamente de Eurasia, así como la lengua eslava más hablada del mundo.[406]​ El ruso es uno de los dos idiomas oficiales a bordo de la Estación Espacial Internacional,[407]​ así como uno de los seis idiomas oficiales de la Organización de las Naciones Unidas.[406]​
Rusia es una nación multilingüe; Se hablan aproximadamente entre 100 y 150 idiomas minoritarios en todo el país.[408]​[409]​ Según el censo ruso de 2010, 137.5 millones en todo el país hablaban ruso, 4.3 millones hablaban tártaro y 1.1 millones hablaban ucraniano.[410]​ La constitución otorga a las repúblicas individuales del país el derecho a establecer sus propios idiomas estatales además del ruso, así como también garantiza a sus ciudadanos el derecho a preservar su idioma nativo y crear condiciones para su estudio y desarrollo.[411]​ Sin embargo, varios expertos han afirmado que la diversidad lingüística de Rusia está disminuyendo rápidamente debido a que muchos idiomas están en peligro de extinción.[412]​[413]​
Rusia es un estado secular,[414]​ la libertad de culto está garantizada por la Constitución.[415]​ La religión más extendida es el cristianismo ortodoxo oriental, representada principalmente por la Iglesia ortodoxa rusa.[416]​ El cristianismo ortodoxo, junto con el islam, el budismo y el paganismo (ya sea preservado o revivido), son reconocidos por el derecho ruso como religiones tradicionales del país como parte de su patrimonio histórico.[417]​[418]​ Las enmiendas a la constitución de 2020 introdujeron, en el artículo 67, la continuidad del estado ruso en la historia basada en la preservación de la «memoria de los antepasados», los «ideales y la creencia en Dios» en general que transmitieron los antepasados.[419]​
Después del colapso de la Unión Soviética, hubo un resurgimiento de las religiones en Rusia, con la reaparición de las creencias tradicionales y el surgimiento de nuevas formas dentro de las creencias tradicionales, así como muchos nuevos movimientos religiosos.[421]​[422]​ El islam es la segunda religión más grande de Rusia, y es la religión tradicional entre la mayoría de los pueblos del norte del Cáucaso y entre algunos pueblos túrquicos dispersos a lo largo de la región del Volga-Ural.[416]​ Grandes poblaciones de budistas se encuentran en Kalmukia, Buriatia, el krai de Zabaikalie, y representan gran parte de la población en Tuvá.[416]​ Muchos rusos practican otras religiones, incluido el rodnovery (neopaganismo eslavo),[423]​ el asianismo (neopaganismo escita),[424]​ otros paganismos étnicos y movimientos interpaganos como el anastasianismo,[425]​ varios movimientos del hinduismo,[426]​ el chamanismo siberiano y el tengrismo,[427]​ varios movimientos neoteosóficos como el roeriquismo entre otras creencias.[428]​[429]​
En 2012, la organización de investigación Sreda, en cooperación con el Ministerio de Justicia, publicó Arena Atlas, un complemento del censo de 2010, que enumera en detalle las poblaciones y nacionalidades religiosas de Rusia, con base en una encuesta nacional de gran muestra. Los resultados mostraron que el 47.3 % de los rusos se declararon cristianos —que incluye el 41 % ortodoxos rusos, el 1.5 % simplemente ortodoxos o miembros de iglesias ortodoxas no rusas, el 4.1 % cristianos no afiliados y menos del 1 % de viejos creyentes, católicos o protestantes— 25 % eran creyentes sin afiliación a ninguna religión específica, el 13 % eran ateos, el 6.5 % eran musulmanes, el 1.2 % eran seguidores de «religiones tradicionales que honran a dioses y antepasados» (Rodnovery, otros paganismos, chamanismo siberiano y tengrismo), el 0.5 % eran budistas, el 0.1 % eran judíos religiosos y el 0.1 % eran hindúes.[416]​
De acuerdo a su constitución, Rusia otorga educación gratuita a sus ciudadanos.[431]​ Casi toda la población adulta está alfabetizada.[432]​ El Ministerio de Educación de Rusia es responsable de la educación primaria y secundaria, así como de la educación vocacional; mientras que el Ministerio de Educación y Ciencia de Rusia es responsable de la ciencia y la educación superior.[433]​ Las autoridades regionales regulan la educación dentro de sus jurisdicciones dentro del marco vigente de las leyes federales. Rusia se encuentra entre los países más educados del mundo y tiene la tercera proporción más alta de graduados de educación superior en términos de porcentaje de la población, con un 62 %.[434]​ Gastó aproximadamente el 4,7 % de su PIB en educación en 2018.[435]​
El sistema de educación preescolar de Rusia está muy desarrollado y es opcional;[436]​ unas cuatro quintas partes de los niños de 3 a 6 años asisten a guarderías o jardines de infancia. La educación básica es obligatoria durante once años, a partir de entre los 6 y 7 años, se egresa con el certificado de educación general básica.[433]​ Se requieren dos o tres años adicionales de escolaridad para obtener el certificado de nivel secundario, alrededor de siete octavos de los rusos continúan su educación más allá de este nivel.[437]​
La admisión a un instituto de educación superior es selectiva y altamente competitiva:[431]​ los cursos de primer grado suelen durar cinco años.[437]​ Las universidades más grandes y antiguas de Rusia son la Universidad Estatal de Moscú y la Universidad Estatal de San Petersburgo.[438]​ Hay diez universidades federales de gran prestigio en todo el país. Rusia fue el quinto destino mundial para estudiantes internacionales en 2019, alberga aproximadamente 300 000 de ellos.[439]​
Según la constitución, Rusia garantiza atención médica universal y gratuita para todos los ciudadanos rusos, a través de un programa de seguro médico estatal obligatorio.[441]​ El Ministerio de Salud de la Federación de Rusia supervisa el sistema de salud pública del país, el sector emplea a más de dos millones de personas. Las regiones federales también tienen sus propios departamentos de salud que supervisan la administración local. Se necesita un plan de seguro médico privado separado para acceder a la atención médica privada en Rusia.[442]​
Rusia gastó el 5.65 % de su PIB en atención médica en 2019.[443]​ Su gasto en atención médica es notablemente más bajo que el de otras naciones desarrolladas.[444]​ Rusia tiene una población femenina mayor que la masculina, representa una de las tasas de desproporción por sexo más altas del mundo, con 0.859 hombres por cada mujer,[4]​ debido a su alta tasa de mortalidad masculina.[445]​ En 2019, la esperanza de vida general en Rusia al nacer era de 73,2 años (68,2 años para los hombres y 78,0 años para las mujeres) y tenía una tasa de mortalidad infantil muy baja (5 por cada 1000 nacidos vivos).[446]​[447]​
La principal causa de muerte en Rusia son las enfermedades cardiovasculares.[448]​ La obesidad es un problema de salud frecuente en Rusia; 61,1 % de los adultos rusos tenían sobrepeso u obesidad en 2016.[449]​ Sin embargo, la históricamente alta tasa de consumo de alcohol de Rusia es el mayor problema de salud en el país,[450]​ ya que sigue siendo una de las más altas del mundo, a pesar de una marcada disminución en la última década.[451]​ El tabaquismo es otro problema de salud en el país.[452]​ La alta tasa de suicidios del país, aunque en declive,[453]​ sigue siendo un problema social importante.[454]​
La cultura de Rusia se ha formado por la historia de la nación, su ubicación geográfica y su vasta extensión, las tradiciones religiosas y sociales y la influencia occidental.[455]​ Los escritores y filósofos rusos han jugado un papel importante en el desarrollo del pensamiento europeo.[456]​[457]​ Los rusos también han influido mucho en la música clásica,[458]​ el ballet,[459]​ el deporte,[460]​ la pintura y el cine.[461]​[462]​ La nación también ha hecho contribuciones pioneras a la ciencia y la tecnología y la exploración espacial.[463]​[464]​
Rusia alberga 30 sitios declarados Patrimonio de la Humanidad por la Unesco, 19 de los cuales son culturales; mientras que 27 sitios más se encuentran en la lista tentativa.[465]​ La gran diáspora rusa alrededor del globo también ha jugado un papel importante en la difusión de la cultura rusa en todo el mundo. El símbolo nacional de Rusia, el águila bicéfala, se remonta a la época del zarismo y aparece en su escudo de armas y heráldica.[54]​ El Oso ruso y la Madre Rusia se utilizan a menudo como personificaciones nacionales del país.[466]​[467]​ Las matrioshkas son consideradas un ícono cultural de Rusia.[468]​
Rusia tiene ocho días festivos oficiales —públicos, patrióticos y religiosos—.[469]​ El año comienza el 1 de enero con el Día de Año Nuevo, seguido pronto por la Navidad ortodoxa rusa el 7 de enero; ambos son los días festivos más populares del país.[470]​ El Día de los defensores de la Patria, dedicado a los hombres, se celebra el 23 de febrero.[471]​ El Día Internacional de la Mujer, el 8 de marzo, cobró impulso en Rusia durante la era soviética. La celebración anual de la mujer se ha vuelto tan popular, especialmente entre los hombres rusos, que los vendedores de flores de Moscú a menudo obtienen ganancias «15 veces» mayores que en otras festividades.[472]​ El Día de la Primavera y del Trabajo, originalmente una fiesta de la era soviética dedicada a los trabajadores, se celebra el 1 de mayo.[473]​
El Día de la Victoria, que honra la victoria soviética sobre la Alemania nazi y el fin de la Segunda Guerra Mundial en Europa, se celebra con un gran desfile anual en la Plaza Roja de Moscú;[474]​ y marca el famoso evento civil del Regimiento inmortal.[475]​ Otros días festivos patrióticos incluyen el Día de Rusia el 12 de junio, celebrado para conmemorar la declaración de soberanía de Rusia tras el colapso de la Unión Soviética;[476]​ y el Día de la Unidad Popular el 4 de noviembre, en conmemoración del levantamiento de 1612 que marcó el fin de la ocupación polaca de Moscú.[477]​
Hay muchos días festivos populares no públicos. El Año nuevo viejo se celebra el 14 de enero.[478]​ Maslenitsa es una antigua y popular fiesta popular eslava oriental.[479]​ El Día de la Cosmonáutica el 12 de abril, en homenaje al primer viaje humano al espacio.[480]​ Dos importantes fiestas cristianas son la Pascua y el Domingo de Trinidad.[481]​
La pintura rusa temprana está representada en íconos y frescos vibrantes. A principios del siglo XV, el maestro pintor de íconos Andréi Rublev creó algunas de las obras de arte religiosas más preciadas de Rusia.[482]​ La Academia de Artes de Rusia, que se estableció en 1757 para capacitar a artistas rusos, trajo técnicas occidentales de pintura secular a Rusia.[68]​ En el siglo XVIII, los académicos Iván Argunov, Dmitri Levitski y Vladímir Borovikovski se volvieron influyentes.[483]​ A principios del siglo XIX, se vieron muchas pinturas destacadas de Karl Briulov y Aleksandr Ivánov, ambos conocidos por sus históricos lienzos románticos.[484]​[485]​
En la década de 1860, un grupo de realistas críticos (Peredvízhniki), dirigido por Iván Kramskói, Iliá Repin y Vasili Perov; rompió con la academia y retrató los múltiples aspectos de la vida social en pinturas.[486]​ La llegada del siglo XX vio el surgimiento del simbolismo; representada por Mijaíl Vrúbel y Nikolái Roerich.[487]​[488]​ La vanguardia rusa floreció desde aproximadamente 1890 hasta 1930; y los artistas influyentes a nivel mundial de esta época fueron El Lisitski,[489]​ Kazimir Malévich, Natalia Goncharova, Vasili Kandinski y Marc Chagall.[490]​
La historia de la arquitectura rusa comienza con los primeros edificios de artesanía en madera de los antiguos eslavos y la arquitectura de las iglesias de la Rus de Kiev.[491]​ Tras la cristianización de la Rus de Kiev, durante varios siglos estuvo influenciada predominantemente por la arquitectura bizantina.[492]​ Aristóteles Fioravanti y otros arquitectos italianos trajeron las tendencias renacentistas a Rusia.[493]​ El siglo XVI vio el desarrollo de iglesias únicas en forma de cubierta en pabellón; y el diseño de cúpula bulbosa, que es una característica distintiva de la arquitectura rusa.[494]​ En el siglo XVII, el «estilo ardiente» de ornamentación floreció en Moscú y Yaroslavl, allanando gradualmente el camino para el barroco Naryshkin de la década de 1680.[495]​
Después de las reformas de Pedro el Grande, la arquitectura de Rusia se vio influenciada por los estilos de Europa occidental. El gusto del siglo XVIII por la arquitectura rococó dio lugar a las espléndidas obras de Bartolomeo Rastrelli y sus seguidores. Los arquitectos rusos más influyentes del siglo XVIII; Vasili Bazhénov, Matvéi Kazakov e Iván Stárov crearon monumentos duraderos en Moscú y San Petersburgo, y establecieron una base para las futuras formas de la arquitectura rusa.[482]​ Durante el reinado de Catalina la Grande, San Petersburgo se transformó en un museo al aire libre de arquitectura neoclásica.[496]​ Bajo Alejandro I, el estilo Imperio se convirtió en el estilo arquitectónico de facto.[497]​ La segunda mitad del siglo XIX estuvo dominada por el estilo neobizantino y el estilo neorruso.[498]​ A principios del siglo XX, el renacimiento neoclásico ruso se convirtió en una tendencia.[499]​ Los estilos predominantes de finales del siglo XX fueron el modernismo,[500]​ el constructivismo y el clasicismo socialista.[501]​[502]​
Hasta el siglo XVIII, la música en Rusia consistía principalmente en música de iglesia y canciones y bailes folclóricos. En el siglo XIX, se definió por la tensión entre el compositor clásico Mijaíl Glinka y sus seguidores, el Grupo de los Cinco, que luego fueron sucedidos por el círculo Belyayev, y la Sociedad musical rusa dirigida por los compositores Anton y Nikolái Rubinstein. La tradición posterior de Piotr Ilich Chaikovski, uno de los más grandes compositores de la época romántica, fue continuada en el siglo XX por Sergei Rajmáninov, uno de los últimos grandes representantes del romanticismo en la música clásica rusa y europea. Los compositores de renombre mundial del siglo XX incluyen a Aleksandr Scriabin, Aleksandr Glazunov, Igor Stravinski, Sergei Prokófiev y Dmitri Shostakovich, y más tarde a Edison Denisov, Sofia Gubaidulina, Georgy Sviridov y Alfred Schnittke.
Los conservatorios soviéticos y rusos han producido generaciones de solistas de renombre mundial. Entre los más conocidos se encuentran los violinistas David Oistrakh y Gidon Kremer; el violonchelista Mstislav Rostropovich; los pianistas Vladimir Horowitz, Sviatoslav Richter y Emil Gilels; y la vocalista Galina Vishnevskaya.
Durante la era soviética, la música popular también produjo una serie de figuras de renombre, como los dos baladistas —Vladimir Vysotsky y Bulat Okudzhava, y artistas como Alla Pugacheva. El jazz, incluso con las sanciones de las autoridades soviéticas, floreció y evolucionó hasta convertirse en una de las formas musicales más populares del país. En la década de 1980, la música rock se hizo popular en Rusia y produjo bandas como Aria, Aquarium, DDT y Kino; el líder de este último, Viktor Tsoi, fue en particular una figura gigantesca. La música pop ha seguido floreciendo en Rusia desde la década de 1960, con actos mundialmente famosos como t.A.T.u.
La literatura rusa se considera una de las más influyentes y desarrolladas del mundo, contribuyendo con muchas de las más conocidas obras literarias. La historia literaria rusa data del siglo X, y de principios del XIX emergió una tradición nativa, desarrollando a los más grandes escritores de todos los tiempos. Este periodo y la Edad de oro de la poesía rusa comenzó con Aleksandr Pushkin, considerado el fundador de la literatura rusa moderna y frecuentemente descrito como el Shakespeare ruso. Entre los más renombrados poetas y escritores rusos del siglo XIX están Yevgueni Baratynski, Mijaíl Lérmontov, León Tolstói, Nikolái Gógol, Iván Turguénev y Fiódor Dostoyevski. Iván Goncharov, Mijaíl Saltykov-Shchedrín, Antón Chéjov, Alekséi Písemski y Nikolái Leskov hicieron aportaciones duraderas a la prosa rusa. Tolstói y Dostoyevski en particular fueron unas figuras titánicas hasta el punto de que muchos críticos literarios caracterizaron a uno o al otro como el mejor novelista que jamás haya existido.
En los años 1880 la literatura rusa empezó a cambiar. La era de los grandes novelistas había acabado y los relatos cortos y poesía empezaron a ser los géneros dominantes para las siguientes décadas conocidas como la Edad de plata de la poesía rusa. Dominada anteriormente por el realismo, la literatura rusa entre 1893 y 1914 estaba dominada por el simbolismo. Los escritores destacados de este período incluyen a Valeri Briúsov, Andréi Bely, Viacheslav Ivánov,[503]​ Aleksandr Blok, Nikolái Gumiliov, Dmitri Merezhkovski, Fiódor Sologub, Anna Ajmátova, Ósip Mandelshtam, Marina Tsvetáyeva, Leonid Andréyev, Iván Bunin y Máximo Gorki.
Después de la revolución rusa de 1917 y la guerra civil, la vida cultural estaba en caos. Algunos arraigados escritores salieron de Rusia, mientras que estaba emergiendo una nueva generación de escritores con talento quienes simpatizaban con la revolución. Los más entusiastas se unieron en organizaciones con el objetivo de crear una nueva y distintiva cultura proletaria para un nuevo estado. En los años 1920 los escritores disfrutaron de una amplia tolerancia. En los años 1930 la censura se endureció en línea con la política de Stalin del realismo socialista. Después de su muerte hubo un deshielo en las restricciones, que fueron disminuidas. En los años 1970 y años 1980, los escritores ignoraban cada vez más la guía del realismo socialista. Los principales escritores de la era soviética son Vladímir Mayakovski, Yevgueni Zamiatin, Isaak Bábel, Ilf y Petrov, Yuri Olesha, Vladímir Nabókov, Mijaíl Bulgákov, Borís Pasternak, Serguéi Yesenin, Mijaíl Shólojov, Aleksandr Solzhenitsyn, Yevgueni Yevtushenko y Andréi Voznesenski.
Mientras que en los países industrializados del Occidente, las imágenes en movimiento se consideraron al principio como una forma barata de recreación y ocio para la clase trabajadora, la producción del cine ruso destacó a partir de la revolución de 1917 al explorar la edición como la forma primaria de expresión cinematográfica. El cine ruso y posteriormente soviético era el núcleo de la invención en el período inmediatamente posterior a la revolución de 1917, resultando en películas mundialmente renombradas como El acorazado Potemkin. Los directores de cine de la era soviética, particularmente Serguéi Eisenstein y Andréi Tarkovski, se convertirían en los cinematográficos más innovadores e influyentes del mundo.
Lev Kuleshov, profesor de Eisenstein, cinematográfico y teórico, formuló el innovador proceso llamado montaje en la primera escuela del cine del mundo, la Universidad Panrusa Gerásimov de Cinematografía en Moscú. Dziga Vértov, cuya teoría Cine-Ojo sobre que la cámara, como el ojo humano, es mejor para explorar la vida real, tuvo un gran impacto en el desarrollo de la realización de documentales y el realismo del cine. En 1932, Stalin hizo del realismo socialista la política estatal, lo que reprimió la creatividad, a pesar de lo cual muchos filmes soviéticos eran artísticamente exitosos, por ejemplo Chapáyev (sobre Vasili Chapáyev), Cuando pasan las cigüeñas de Mijaíl Kalatózov y Balada sobre un soldado de Grigori Chujrái. Las comedias de Leonid Gaidái de los años 1960 y años 1970 fueron inmensamente populares, cuyos latiguillos siguen en uso en la actualidad. 1969 fue el año del lanzamiento de la película Sol blanco del desierto de Vladímir Motyl,[504]​ con la que comenzó el género de los osterns.[505]​ Una de las tradiciones de los cosmonautas es ver este filme antes de cada viaje al espacio.[506]​
Las décadas de 1980 y 1990 fueron años de crisis para el cine ruso. A pesar de la recientemente adquirida libertad de expresión, los subsidios estatales se redujeron drásticamente, disminuyendo el número de filmes producidos. En los primeros años del siglo XXI aumentó la audiencia con la subsecuente prosperidad de la industria gracias al rápido desarrollo económico. Los niveles de producción alcanzaron los del Reino Unido y Alemania.[507]​ Si en 1996 los ingresos de las taquillas eran de unos 6 millones de dólares, en 2007 fueron de 565 millones (un 37% más que en 2006).[508]​ El cine ruso sigue obteniendo reconocimiento internacional. El arca rusa (2002) de Aleksandr Sokúrov fue el primer largometraje consistente en una sola toma sin editar.
Los deportes de invierno tienen la mayor popularidad en Rusia. El patinaje sobre hielo y el hockey sobre hielo son muy populares como deportes de ocio y como deportes para espectadores. La selección rusa de hockey sobre hielo ganó el campeonato mundial en el año 2008, en tanto que los Liga Continental de Hockey se ha expandido a varios países de Europa Oriental. En el patinaje artístico Rusia cuenta con deportistas tan destacados como Yevgueni Pliúshchenko. El esquí a campo traviesa tenía gran popularidad como deporte de ocio durante la época soviética, aunque su popularidad se ha disminuido en los últimos años.
Entre otros deportes, el atletismo tiene mucha popularidad, en casi todas sus facetas. En tenis se han destacado María Sharápova, Marat Safin u Yevgueni Káfelnikov (en tenis), que han logrado el primer puesto en la clasificación mundial. En gimnasia sobresalen Alekséi Nemov, Aliyá Mustáfina y Svetlana Jórkina, que han logrado medallas de oro, plata y bronce en diferentes competencias de gimnasia, incluyendo en los Juegos Olímpicos.
Después de la desaparición de la Unión Soviética, el fútbol ha llegado a los primeros planos, pasando a ser una de las disciplinas dominantes. Existen clubes conocidos a nivel internacional, como lo son el Spartak Moscú (campeón de la liga rusa en diez ocasiones); el CSKA Moscú (campeón de la Copa de la UEFA en 2005), el Zenit San Petersburgo (ganador del mismo torneo en 2008) y campeón de la supercopa europea de ese mismo año y Rubin Kazan (campeón de la liga premier de Rusia en 2008).
La selección rusa es considerada como la sucesora oficial de la selección de la Unión Soviética, por lo que todos los logros que consiguió dicha nación son oficialmente heredados para Rusia. A lo largo de su historia ha participado en once mundiales de este deporte, en ocho ocasiones como Unión Soviética (Suecia 58, Chile 62, Inglaterra 66, México 70 y 86, España 82 e Italia 90) y cuatro como la actual Rusia (USA 94, Corea-Japón 2002, Brasil 2014 y como sede en 2018). Su mejor actuación como Unión Soviética fue haber conseguido el cuarto lugar en la edición de  1966; y con el nombre de Rusia llegó hasta los cuartos de final de la edición  2018. Ha asistido a la Eurocopa de naciones en 11 ocasiones, siendo campeón de la primera edición en 1960 y subcampeón en 1964, 1972 y 1988 como Unión Soviética. Su mejor participación como Rusia fue en 2008, en donde logró conseguir el tercer lugar.
Rusia ha dado dos grandes futbolistas que forman parte de la historia del balompié mundial, como lo son Lev Yashin, que es considerado el mejor portero de todos los tiempos; y Oleg Salenko, quien posee el histórico récord de 5 goles en un mismo partido, logrado el 28 de junio de 1994 ante Camerún en el Mundial de Estados Unidos.
La selección de baloncesto ganó el Campeonato Europeo en 2007. También, como Unión Soviética ha ganado 14 Eurobasket y tres campeonatos mundiales de baloncesto, en 1967, 1974 y 1982.
El rugby también es uno de los deportes que poco a poco se ha estado ganando la popularidad de los jóvenes. Durante este año logró ser invitada al Circuito Mundial de Rugby 7 en Hong Kong, y su selección de XV conocida como los osos disputó la Copa Mundial de Rugby 2011, pero terminó siendo derrotada en sus 4 partidos del grupo C frente a selecciones experimentadas como Irlanda, Australia, Estados Unidos e Italia.
El voleibol ruso es uno de los más importantes a nivel mundial tanto en su rama masculina como en la femenina.
El ajedrez es otro deporte que se practica, podría ser considerado el deporte nacional. Rusia tiene el honor de tener el mayor número de campeones mundiales (muchos de los cuales figuran con bandera de la Unión Soviética, pero nacieron en territorio ruso actual): Mijaíl Botvínnik, Vasili Smyslov, Borís Spaski, Mijaíl Tal, Tigran Petrosian, Alexander Alekhine. En esta disciplina, Garry Kaspárov y Anatoli Kárpov son los ajedrecistas más conocidos a nivel mundial en la época contemporánea, ya que ambos han ganado el Campeonato del mundo de ajedrez. Tras el Cisma del ajedrez ha habido más campeones rusos como Vladímir Krámnik. La Escuela Soviética de ajedrez es una de las más famosas y profesionales a nivel mundial, la cual fundada por Mijaíl Botvínnik. El ajedrez ruso también hizo una aparición importante en la reciente serie de Netflix Gambito de Dama, en la cual el Campeón mundial de Ajedrez es precisamente de proveniencia rusa. Hoy en día Rusia cuenta con varios deportistas de élite mundial en el mundo del ajedrez, tales como Yan Nepómniashchi, Daniel Dubov, Alexander Grischuk, entre otros.
En ciclismo Rusia tiene a cinco de los mejores ciclistas de los últimos años en las figuras de Vladímir Karpets, Vladímir Yefimkin, Pável Tonkov, Yevgueni Berzin y Denís Menshov. También destacan en el ciclismo en pista, modalidad en la que han conseguido varios campeonatos del mundo.
En fútbol playa Rusia es el actual campeón de la Eurocopa de fútbol playa y bicampeón mundial, pues obtuvo la Copa mundial de fútbol playa en 2011 y 2013.
Rusia acogió los Juegos Olímpicos de invierno del año 2014 en la ciudad de Sochi. En verano del año 1980, la ciudad de Moscú (por entonces capital de la Unión Soviética) fue la sede de los XXII Juegos Olímpicos.
El 2 de diciembre de 2010, la FIFA dio a conocer que Rusia ganó la candidatura para organizar la Copa Mundial de Fútbol de 2018, en la cual preparó 12 sedes que dieron vida a la mayor cita de balompié del planeta. Las ciudades sedes fueron: Moscú (dos estadios), Kaliningrado, Samara, San Petersburgo, Ekaterimburgo, Rostov del Don, Sochi, Krasnodar, Nizhni Nóvgorod, Kazán, Saransk y Volgogrado; inició el 14 de junio y finalizó el 15 de julio. En balonmano tuvo su época dorada en los 90 con varios títulos europeos y mundiales.

Bangkok u oficialmente Krung Thep Mahanakhon ([bāːŋ kɔ̀ːk] en tailandés: กรุงเทพมหานคร o กรุงเทพ  [krūŋ tʰêːp mahǎː nákʰɔ̄ːn] (?·i), RTGS: Krung Thep Mahanakhon traducido como «La ciudad de los ángeles»)[1]​ es la capital y ciudad principal de Tailandia.[2]​ Aunque el nombre oficial de la capital no es Bangkok, es comúnmente empleado internacionalmente para referirse a la ciudad.[3]​
Bangkok significa «aldea de la ciruela silvestre» y es el nombre de una parte del lado del río Thon Buri. Así pues, Bangkok es conocida como Krung Thep Mahanakhon. Fue un pequeño puesto de comercio en la desembocadura del río Chao Phraya durante el reino de Ayutthaya. Llegó al primer plano de Siam, cuando recibió el estatus de ciudad capital en 1768 después de la quema de Ayutthaya.[4]​ Sin embargo el actual reino Rattanakosin no comenzó hasta 1782 cuando Rama I trasladó la capital a la isla de Rattanakosin, después de la muerte del rey Taksin. La capital de Rattanakosin es ahora formalmente llamada «Phra Nakhon» (en tailandés: พระนคร), perteneciente a los antiguos límites en el núcleo de la metrópolis y el nombre de Bangkok incorpora la acumulación urbana desde el siglo XVIII, con su propia administración pública y gobernador.
Durante los últimos doscientos años Bangkok ha crecido hasta llegar a ser el centro político, social y económico de Tailandia ampliando su pujanza hacia Indochina y el Sudeste asiático. Su influencia en el arte, la política, moda, educación y entretenimiento, así como en los negocios, le ha proporcionado a Bangkok el estatus de ciudad global. En 2016, según un informe elaborado por MasterCard, fue reconocida como la ciudad más visitada por turistas extranjeros con 21,47 millones superando a Londres.[5]​
La ciudad tenía una población de alrededor de 8.5 millones de habitantes, mientras que el área de gran Bangkok posee 11.971.000 habitantes (a enero de 2008).[6]​ Esto, a su vez, ha cambiado el país ya que ha pasado de ser una población tailandesa homogénea a una heterogénea donde se incluye ciudadanos de procedencia occidental, con grupos de India o China, otorgando a la ciudad un estatus cosmopolita.[7]​
El nombre oficial de la ciudad es Krung Thep (กรุงเทพฯ) de la palabra thai, proveniente del jmer, Krung que significa «capital», y thep del pali, forma del antiguo deva, que significa «divinidad» y que suele traducirse como «ciudad de ángeles», pero equivale más bien a «ciudad de los dioses».[8]​ 
Bangkok (en thai: บางกอก) es un nombre coloquial de significado impreciso adoptado por los extranjeros a partir de una pequeña aldea local y que se usa en el contexto internacional. Bang es la palabra thai para denominar a una aldea situada a orillas de un arroyo,[9]​ y ko es la palabra para isla; por lo cual el nombre original, Bang Ko (บางเกาะ), significa «la aldea de la isla».[10]​ Otra etimología sostiene que es una contracción de Bang Makok (บางมะกอก), makok es el nombre de un arbusto local, el Elaeocarpus hygrophilus, del género Elaeocarpus, el cual da un fruto de tamaño y forma similar a la aceituna. El nombre signifcaría, en ese caso, «aldea del makok».[11]​ De hecho el cercano templo de Wat Arun, suele ser llamado Wat Makok.[12]​ 
La forma larga y en desuso del nombre de la ciudad es: Krungteb Mahanakon Amon Ratta Nakosin Mahin Tharayud Tayama Mahadihlok Pharad Chataniburom Rich Rad Chaniwet Maha Satan Amon Phiman Awatansathit Sakthatthiyavid Sanukamprasit (กรุงเทพมหานคร อมรรัตนโกสินทร์ มหินทรายุธยามหาดิลก ภพนพรัตน์ ราชธานีบุรีรมย์ อุดมราชนิเวศน์ มหาสถาน อมรพิมาน อวตารสถิต สักกะทัตติยะ วิษณุกรรมประสิทธิ์) es decir: «Ciudad de los devas-Gran Ciudad de los Inmortales-Honrada con nueve copas, residencia real- ciudad con muchos palacios, hechizo divino, erigida por Visvakarman a instancias de Indra». Este nombre ceremonial emplea palabras en pali y sánscrito, precedidas por el vocablo thai: Krung: capital,[13]​ nombre que ha sido considerado como el más largo topónimo del mundo ya que cuenta con 163 caracteres.[14]​[15]​[16]​ 
El Libro Guiness no la reconoce como la ciudad con el nombre más largo del mundo ya que es un topónimo en desuso.[1]​[fuente cuestionable]
Bangkok inicialmente fue un pequeño centro comercial y comunidad portuaria llamada Bang Makok (lugar de ciruelos oliváceos), sirviendo a la ciudad de Ayutthaya, que era la capital de Siam hasta que cayera en manos de Birmania en 1767. La capital fue establecida en Thon Buri (ahora parte de Bangkok) sobre el lado oeste del río. 
En 1782 el rey Rama I construyó un palacio sobre la orilla este e hizo de Bangkok su capital, renombrándola Krung Thep, que significa «ciudad de ángeles». El pueblo de Bangkok dejó de existir pero su nombre sigue siendo usado por los extranjeros.
El área administrativa especial de Bangkok cubre 8.461.568,7 km², convirtiéndose en la 68.ª provincia más grande de Tailandia. La mayoría del área la ocupa la ciudad de Bangkok haciendo de ella una de las ciudades más grandes del mundo.[17]​ El río Chao Phraya, que abarca 372 km, es el principal accidente geográfico de Bangkok. La cuenca del río Chao Phraya, los alrededores de Bangkok y las provincias cercanas incluyen una serie de llanuras y deltas de los ríos que desembocan en la bahía de Bangkok, a unos 30 km al sur del centro de la ciudad. Esto dio lugar a la denominación de Bangkok como la «Venecia de Oriente», debido al número de canales y vías que dividen el área en parcelas separadas de la tierra. Antiguamente la ciudad utilizaba estos canales, muy abundantes en Bangkok, como divisiones de los distritos de la ciudad. Sin embargo, tras el rápido crecimiento de la ciudad en la segunda mitad del siglo XX, el plan fue abandonado y se adoptó un sistema diferente de división.
Bangkok se encuentra a unos dos metros sobre el nivel del mar, lo que provoca problemas para la protección de la ciudad contra inundaciones durante la temporada de los monzones. A menudo, después de un aguacero, el agua en los canales y el río se desborda de las riberas, dando lugar a graves anegamientos, como la acaecida en 2011. La Administración Metropolitana de Bangkok (BMA) ha instalado más diques al lado de algunos canales para evitar que los niveles de agua lleguen al nivel urbano. Sin embargo, hay algunas desventajas para las amplias rutas de canal en Bangkok, ya que se rumorea que la ciudad se hunde un promedio de dos centímetros al año ya que se encuentra completamente en un pantano.[18]​
Bangkok tiene un Clima tropical de sabana (clasificación climática de Köppen: Aw). La temperatura más alta registrada en la ciudad fue de 40 °C en mayo de 1983, mientras que la más baja registrada tuvo lugar en enero de 1955, cuando se registró 9,9 °C. Las temperaturas más bajas fueron archivadas en los meses de enero de 1924, 1955 y 1974, así como en diciembre de 1999. El año más caluroso en Bangkok fue 1997 (con una media de 30 °C registrados en el aeropuerto Don Mueang) y el más frío fue 1975 (con 26,3 °C). La temperatura diurna más fría fue de 19,9 °C en diciembre de 1992. Las granizadas son algo prácticamente desconocidas en la ciudad, con un solo registro en los últimos cincuenta años.[19]​
Los distritos de Bangkok no suelen representar con exactitud las divisiones funcionales de sus barrios ni el uso del suelo. Aunque las políticas de planificación urbana se remontan a la comisión del «Plan Litchfield» en 1960, que establecía estrategias para el uso del suelo, el transporte y la mejora de las infraestructuras generales, las normas de zonificación no se aplicaron plenamente hasta 1992. En consecuencia, la ciudad creció de forma orgánica durante el periodo de su rápida expansión, tanto en sentido horizontal, al extenderse las urbanizaciones a lo largo de las carreteras recién construidas, como en sentido vertical, con la construcción de un número creciente de rascacielos en las zonas comerciales.[22]​
La ciudad ha crecido desde su centro original a lo largo del río hasta convertirse en una metrópolis en expansión rodeada de franjas de desarrollo residencial suburbano que se extienden hacia el norte y el sur de las provincias vecinas. Las ciudades de Nonthaburi, Pak Kret, Rangsit y Samut Prakan, muy pobladas y en crecimiento, son ahora suburbios de Bangkok. No obstante, siguen existiendo grandes zonas agrícolas dentro de la ciudad propiamente dicha, en sus márgenes oriental y occidental, y un pequeño número de zonas forestales se encuentra dentro de los límites de la ciudad: 3.887 rai (6,2 km²; 2,4 millas cuadradas), que representan el 0,4% de la superficie de la ciudad.[23]​ El uso del suelo en la ciudad se compone de un 23% de uso residencial, un 24% de uso agrícola y un 30% de uso comercial, industrial y gubernamental. El Departamento de Planificación de la Ciudad (CPD) de la BMA es el responsable de planificar y dar forma al desarrollo futuro. Publicó actualizaciones del plan maestro en 1999 y 2006, y una tercera revisión está siendo sometida a audiencias públicas en 2012.[24]​
Bangkok está situada en el delta del río Chao Phraya, en la llanura central de Tailandia. El río serpentea por la ciudad en dirección sur y desemboca en el golfo de Tailandia a unos 25 km al sur del centro. La zona es llana y baja, con una altitud media de 1,5 metros sobre el nivel del mar.[25]​ La mayor parte de la zona era originalmente pantanos, que se fueron drenando y regando para la agricultura con la construcción de canales (khlong) entre los siglos XVI y XIX. El curso del río a su paso por Bangkok ha sido modificado por la construcción de varios canales de atajo.
La red fluvial de la ciudad fue el principal medio de transporte hasta finales del siglo XIX, cuando empezaron a construirse carreteras modernas. Hasta entonces, la mayoría de la gente vivía cerca o en el agua, lo que llevó a la ciudad a ser conocida en el siglo XIX como la "Venecia del Este".[26]​ Muchos de estos canales se han rellenado o pavimentado, pero otros siguen cruzando la ciudad, sirviendo como importantes canales de drenaje y rutas de transporte. La mayoría de ellos están muy contaminados, aunque la BMA se ha comprometido a tratar y limpiar varios.[27]​
La geología de la zona de Bangkok se caracteriza por una capa superior de arcilla marina blanda, conocida como "arcilla de Bangkok", de 15 metros de espesor de media, que recubre un sistema acuífero formado por ocho unidades conocidas. Esta característica ha contribuido a los efectos del hundimiento causado por el extenso bombeo de aguas subterráneas. Reconocido por primera vez en la década de 1970, el hundimiento pronto se convirtió en un problema crítico, alcanzando una tasa de 120 milímetros (4,7 pulgadas) por año en 1981. Desde entonces, la gestión de las aguas subterráneas y las medidas de mitigación han disminuido la gravedad de la situación, y el índice de hundimiento se redujo a entre 10 y 30 milímetros (0,39 y 1,18 pulgadas) al año a principios de la década de 2000, aunque algunas partes de la ciudad se encuentran ahora 1 metro (3 pies 3 pulgadas) por debajo del nivel del mar.[28]​
El hundimiento ha aumentado el riesgo de inundaciones, pues Bangkok ya es propensa a sufrirlas debido a su escasa altitud y a una infraestructura de drenaje inadecuada, a lo que a menudo se suma la obstrucción provocada por la contaminación de la basura (sobre todo, residuos plásticos).[29]​ En la actualidad, la ciudad utiliza barreras contra inundaciones y aumenta el drenaje de los canales mediante bombeo y la construcción de túneles de desagüe, pero algunas zonas de Bangkok y sus suburbios siguen inundándose con regularidad. Los principales factores desencadenantes son los fuertes aguaceros que provocan que la escorrentía urbana desborde los sistemas de drenaje y la descarga de escorrentía de las zonas situadas aguas arriba.[30]​ En 1995 y 2011 se produjeron graves inundaciones que afectaron a gran parte de la ciudad. En 2011, la mayor parte de los distritos del norte, este y oeste de Bangkok se inundaron, en algunos lugares durante más de dos meses.
Luego, en octubre de 2021 durante plena pandemia las inundaciones en la ciudad de Bangkok y otras zonas al norte, que llegaron a afectar a casi 1 millón de personas. [31]​
La situación costera de Bangkok la hace especialmente vulnerable a la subida del nivel del mar debida al calentamiento global y al cambio climático. Un estudio de la OCDE ha estimado que 5,138 millones de personas en Bangkok pueden estar expuestas a inundaciones costeras en 2070, la séptima cifra más alta entre las ciudades portuarias del mundo.[32]​ Se teme que la ciudad pueda quedar sumergida en 2030.[33]​ Un estudio publicado en octubre de 2019 en Nature Communications corrigió modelos anteriores de elevaciones costeras[34]​ y concluyó que hasta 12 millones de tailandeses -la mayoría en el área metropolitana de Bangkok- se enfrentan a la perspectiva de inundaciones anuales.[35]​ A esto se suma la erosión costera, que es un problema en la zona costera del golfo, una pequeña parte de la cual se encuentra en el distrito de Bang Khun Thian de Bangkok. En la costa existían ecosistemas de llanura mareal, pero muchos han sido recuperados para la agricultura, la acuicultura y las salinas.[36]​
En Bangkok no hay montañas. La cadena montañosa más cercana es el macizo de Khao Khiao, a unos 40 km al sureste de la ciudad. Phu Khao Thong, la única colina del área metropolitana, tiene su origen en un chedi muy grande que el rey Rama III (1787-1851) construyó en Wat Saket. El chedi se derrumbó durante la construcción porque el suelo blando no podía soportar su peso. En las décadas siguientes, la estructura abandonada de barro y ladrillo adquirió la forma de una colina natural y se llenó de maleza. Los lugareños la llamaban phu khao (ภูเขา), como si fuera un elemento natural. En la década de 1940 se añadieron muros de hormigón para impedir que la colina se erosionara.
Administrativamente, Bangkok es una de las dos áreas de administración especial de Tailandia, (la otra es Pattaya), en que los ciudadanos votan para elegir a su gobernador, a diferencia de en las otras 75 provincias (changwat) de Tailandia. En la última elección gubernativa, en 2009, fue elegido gobernador Mom Rajawongse Sukhumbhand Paripatra.
La dispersión urbana de la Gran Metrópolis de Bangkok se extiende más allá de las fronteras de la provincia de Bangkok, se desparrama en las provincias vecinas de Nonthaburi, Samut Prakan y Pathum Thani. La provincia, como es hoy, fue creada en 1972, cuando la anterior provincia de Bangkok (changwat Phra Nakhon) fue fusionada con la provincia Thon Buri.
El escudo de la ciudad muestra al dios Indra que cabalga en las nubes sobre Erawan (Airavata), una criatura mitológica con forma de elefante (a veces representado con tres cabezas). En su mano Indra sostiene un rayo relampagueante, que es su arma para ahuyentar la sequía. El escudo se basa en una pintura hecha por Somdej Chaofa Kromphraya Narisra-nuwattiwong. El símbolo de árbol de Bangkok es el ficus benjamina.
Bangkok cuenta con cincuenta distritos (Khet, también llamados Amphoe) como subdivisiones administrativas bajo la autoridad legal de BMA. De los cuales treinta y cinco pertenecen legalmente al este del río Chao Phraya y quince al oeste del área conocida por los lugareños como antigua provincia de Thonburi (en la actualidad distrito de Thon Buri). Siendo su código oficial, el siguiente:

La ciudad de Bangkok tiene una población de 8 280 925 habitantes según el porcentaje del censo de 2010.[38]​ Sin embargo hay sólo 5 701 394 personas censadas, pertenecientes a 2 400 540 hogares.[39]​ Un gran número de la población durante el día se desplaza de provincias circundantes en la región metropolitana de Bangkok, la población total es 14 565 547. Bangkok es una ciudad cosmopolita, el censo mostró que es el hogar de 81 570 japoneses y 55 893 ciudadanos chinos, así como 117 071 expatriados procedentes de otros países asiáticos, 48 341 de Europa, 23 418 de América, 5289 y 3022 de Australia y África. Los inmigrantes de los países vecinos incluyen 303 595 birmanos, 63 438 camboyanos y 18 126 de Laos.[40]​
A pesar de que ha sido el mayor núcleo de población de Tailandia desde su establecimiento como capital en 1782, la ciudad creció sólo ligeramente a lo largo de los siglos XVIII y XIX. El diplomático británico John Crawfurd, durante su visita a la ciudad en 1822, estimó su población en no más de 50 000 habitantes.[41]​ Como resultado de la medicina occidental traída por los misioneros, así como la creciente inmigración de ambos dentro de Siam y en el extranjero, la población de Bangkok aumentó gradualmente como la ciudad modernizada en el siglo XIX. Este crecimiento se hizo aún más pronunciado en la década de 1930, tras el descubrimiento de los antibióticos. Aunque la planificación familiar y el control de natalidad se introdujo en la década de 1960, la tasa de natalidad baja fue más que compensada por el aumento de la migración de las provincias, que la expansión económica aceleró. Tailandia hacía tiempo que había llegado a ser altamente centralizada en torno a la capital. En 1980 Bangkok tenía cincuenta y un veces mayor población que la de Hat Yai y Songkhla, el segundo centro urbano más grande.[42]​
La mayoría de la población de Bangkok son de origen thai, aunque los detalles sobre étnica de la ciudad no están disponibles en el censo nacional. El pluralismo cultural de la ciudad se remonta a los primeros tiempos de su fundación. Varias comunidades étnicas fueron formadas por inmigrantes y colonos, incluyendo los Jemer, del norte de Tailandia, Laos, Vietnam y Malasia.[43]​
Los más destacados fueron los chinos, que desempeñaron un papel importante en el comercio de la ciudad y se convirtieron en el mayor grupo étnico de la ciudad, estimados poblacionales incluyen hasta tres cuartas partes de la población en 1828 eran chinos y casi la mitad en la década de 1950.[44]​ Sin embargo la inmigración china fue restringida desde la década de 1930 y cesó efectivamente después de la revolución china en 1949. Bangkok es sin embargo todavía el hogar de una gran comunidad china, con la mayor concentración en Yaowarat, el barrio chino de Bangkok.
La mayoría (91 por ciento) de la población de la ciudad es budista. Otras religiones incluyen el islam (4,7 %), el cristianismo (2,0 %), el hinduismo (0,5 %), el sijismo (0,1 %) y el confucianismo (0,1 %).[45]​ Aparte de Yaowarat, la ciudad también tiene otros barrios étnicos distintos. La comunidad india se centra en Phahurat, donde está ubicado el Gurdwara Siri Guru Singh Sabha, fundado en 1933. Khrua Ban en Saep Saen Canal es el hogar de los descendientes del pueblo cham, que se establecieron a finales del siglo XVIII.
Aunque los portugueses que se establecieron durante el período de Thonburi han dejado de existir como una comunidad distinta, su pasado se refleja en la iglesia de la Santa Cruz en la orilla oeste del río. Del mismo modo la catedral de la Asunción en Charoen Krung Road es uno de muchos edificios de estilo europeo en el antiguo Farang Quarter, donde los diplomáticos europeos y comerciantes vivieron durante finales del siglo XIX y a principios del siglo XX. Muy cerca está la mezquita Haroon, centro de la comunidad musulmana. Recientes comunidades de expatriados se han establecido a lo largo de Sukhumvit Road, incluida la comunidad japonesa cerca de Soi Phrom Phong, y el barrio árabe y de África del Norte a lo largo de Soi Nana.
Bangkok es el centro económico de Tailandia. El río Chao Phraya permite que Bangkok funcione como puerto. La Bolsa de Comercio de Tailandia está ubicada en Bangkok. El turismo es una de las fuentes principales de ingresos.La ciudad contiene muchos templos budistas (conocidos en tailandés como Wats), entre los más conocidos están Wat Pho y Wat Arun. La carretera o calle Khaosan, cerca del complejo de 200.000 metros cuadrados del emblemático Gran Palacio,[46]​ es un destino popular para jóvenes mochileros. Las instalaciones educativas y culturales de Bangkok incluyen varias universidades (como la Universidad de Bangkok, el Asian institute of technology, el Bangkok Thonburi College, el SAE Institute Bangkok, etc., una academia de bellas artes, un teatro nacional y un museo nacional.
El alimento procesado, la madera, y los tejidos encabezan las exportaciones. Las plantas industriales incluyen molinos de arroz, fábricas de cemento, aserraderos, refinerías de petróleo, y astilleros. La ciudad es un centro de joyería famoso, comprando y vendiendo plata y piezas de bronce.
Una compleja red de canales (khlong) dio a la ciudad el apodo la Venecia del Este, cuando todo el transporte se hacía por barco. Hoy están cegados y convertidos en calles. Sin embargo, muchos existen realmente todavía, con gente que vive a lo largo de ellos, y mercados ubicados allí también.
Se han construido varias carreteras elevadas, y hay una autopista de rodeo del Gran Bangkok parcialmente terminada, para terminar con los embotellamientos.
En 1999 se terminó el metro aéreo de Bangkok, una línea férrea elevada doble oficialmente llamado BTS. En julio de 2004 se abrió al público la primera línea del metro subterráneo de Bangkok. Los restos de un fracasado proyecto de ferrocarril elevado (el proyecto Hopewell) aún se pueden ver desde la estación principal del ferrocarril hasta el aeropuerto de Don Mueang. Debido a la crisis financiera asiática la construcción se detuvo y los pilares de hormigón fueron abandonados.
En julio de 2004, fue inaugurado un nuevo sistema de metro, el MRT, que une la estación de ferrocarril del norte Bang Sue con la estación de ferrocarril Hua Lamphong, cerca del centro, pasando por la parte este de la ciudad. Conecta con las estaciones BTS Mo Chit, Asok y Sala Daeng.
Para viajes por el tren, la mayor parte de los pasajeros comienzan sus viajes en Hua Lamphong. Allí, los trenes unen Bangkok con el sur de Tailandia Chiang Mai y más al norte, y Khon Kaen y más al nordeste.
En autobús se puede ir prácticamente a todas las ciudades y provincias desde Bangkok. Para destinos del sudoeste y del oeste, los autobuses salen de la Terminal Sur de Autobuses, al oeste de la ciudad. Para destinos del sudeste, como Pattaya y Ko Samet, los autobuses salen de la Terminal Este de Autobuses, en Ekkamai. Y para todos los destinos del norte y nordeste, la Terminal Norte de Autobuses en Mo Chit, a la que se llega tanto por Skytrain como por metro.
El aeropuerto Internacional Suvarnabhumi, el más transitado del Sudeste Asiático, está localizado al sureste de la ciudad, ahora ya rodeado por áreas urbanas. La construcción del nuevo Aeropuerto Suvarnabhumi (se pronuncia Suwannapum), en el distrito Bang Phli, de la provincia Samut Prakan, al sudeste de la ciudad comenzó en 2002. Desde su inauguración en septiembre de 2006 se convirtió en el aeropuerto internacional oficial de Bangkok. Este nuevo aeropuerto cuenta con una línea de tren con varias paradas hasta el centro de Bangkok desde 2010 y otra línea de tren exprés directa al centro de Bangkok desde 2011.
Bangkok fue la ciudad sede de los Juegos Asiáticos de 1966,[47]​ 1970,[48]​ 1978,[49]​ y 1998.[50]​
Bangkok es la ciudad en la que transcurren, entre otras, las películas The Hangover Part II, Ong-Bak y Bangkok Dangerous.
Bangkok tiene relaciones de hermandad o acuerdos de colaboración con 23 ciudades en 16 países diferentes.[51]​ Las siguientes ciudades forman parte de esa red:


La Meca (en árabe: مكة المكرمة‎ [Makkah al-Mukarrama] o simplemente مكة‎‎ [Makkah]) es la principal ciudad de la región del Hiyaz,[5]​ en la actual Arabia Saudita, y una de las más importantes de la península de Arabia. Está situada al oeste de la península y cuenta con 1 675 368 habitantes (censo de 2010), localizada en un estrecho valle, a 277 m sobre el nivel del mar; se ubica a 80 km del mar Rojo.
El nombre completo que lleva desde tiempos musulmanes es Makka al-Mukarrama, que significa "Makkah, con honor". Ptolomeo, en el siglo II, se refirió a ella con el nombre de Makoraba, helenización del árabe Makkah Harb, o "Meca de Harb" (nombre de una tribu).
Ciudad natal de Mahoma, es la más importante de todas las ciudades santas del islam, visitada cada año por millones de peregrinos. Antes de que Mahoma predicara el islam, esta era ya para los paganos una ciudad santa, con varios lugares de importancia religiosa, entre ellos el más importante de todos: la Kaaba.
Para los musulmanes, el peregrinaje a La Meca forma parte de uno de los aspectos fundamentales de su fe, los denominados pilares del islam. Cada año, cerca de tres millones de peregrinos se dirigen a la ciudad santa para realizar el peregrinaje mayor o Haŷŷ durante el mes musulmán de du l-hiyya.
Muchos más hacen la peregrinación menor o Umrah, que puede realizarse durante todo el año. Un cómputo aproximado de los musulmanes que visitan anualmente La Meca, alcanzaría los 13 millones de visitantes, sumando el peregrinaje mayor y menor. Muy pocos no musulmanes han podido ver los ritos y rituales del Haŷŷ, ya que está totalmente prohibida la entrada de no creyentes en La Meca y en Medina.
«La Meca» es la forma española tradicional evolucionada del nombre árabe de la ciudad, aunque la transliteración oficial del gobierno saudí es Makkah, que se acerca más a la pronunciación árabe.[6]​[7]​ En español, la palabra «Meca», que proviene del árabe hispánico Makka y este del árabe clásico Mákkah, ha terminado por ser usada para hacer referencia a un lugar donde acuden muchas personas por ser el centro principal o mejor sitio donde se realiza algo. Este uso, que se extiende también a otras lenguas, resulta ofensivo para muchos musulmanes.[6]​ El gobierno saudí adoptó la forma Makkah como escritura oficial en los años 1980, pero esta no es usada de forma generalizada en todo el mundo.[6]​ El nombre oficial completo de la ciudad es Macarama (المكرمة), pronunciado [makka lmukarrama] o [makkah almukarrama]), que significa «Meca la Honrada», pero que a veces se traduce como «La sagrada ciudad de la Meca».[6]​
El antiguo o primitivo nombre para el asentamiento de La Meca es Baca o Bakkah (también transliterado como Baka, Bakah, Bakka, Becca, Bekka, etc.).[8]​[9]​[10]​ La etimología de esta palabra árabe, al igual que ocurre con el origen de la palabra Meca, es desconocida.[11]​ Está bastante extendida la creencia de que se trata de un sinónimo de Meca, aunque se dice que más específicamente se trata del nombre primitivo del valle que allí se encuentra, mientras que los investigadores árabes lo usan generalmente para referirse a la zona sagrada de la ciudad que se encuentra en las inmediaciones de la Kaaba.[12]​[13]​
La forma Baca se usa en lugar de La Meca en el Corán en 3:96, mientras que la forma Meca se usa en 48:24.[11]​[14]​ En el idioma surarábigo, el idioma hablado en la parte sur de la península arábiga en los tiempos de Mahoma, la «b» y la «m» podían intercambiarse.[14]​ Otras referencias a La Meca en el Corán (6:92, 42:5) la llaman Umm al-Qura, que significa «madre de todos los asentamientos».[14]​ Otro nombre para La Meca es Tihama.[15]​
Otro nombre para La Meca, o para las tierras salvajes y montañas que rodean la ciudad, según la tradición árabe e islámica, es Farán o Parán, y se refiere al desierto de Farán o Parán.[16]​ La tradición árabe e islámica sostiene que la zona salvaje de Farán mencionada en la Biblia es, en términos generales, Tihama, y La Meca es el lugar donde se asentó Ismael.[16]​ Abu Abdallah Yaqut ibn-Abdallah al-Rumi al-Hamawi, geógrafo sirio del siglo XII, escribió que Fārān era «una palabra hebrea arabizada. Uno de los nombres de La Meca mencionados en la Torá».[17]​
La ciudad de La Meca está gobernada por el municipio homónimo, cuyo gobierno está formado por 14 miembros electos localmente y presididos por un alcalde (llamado Al-Amin) nombrado por el gobierno del país. El actual alcalde de la ciudad es Usama al-Bar.[18]​
La Meca es la capital de la provincia homónima, que incluye la ciudad de Yeda, la mayor de la provincia. El gobernador provincial fue, entre 2000 y 2007, año de su fallecimiento, el príncipe Abdul Majeed bin Abdulaziz Al Saud.[19]​ El 16 de mayo de 2007, el príncipe Khalid bin Faisal Al Saud fue nombrado nuevo gobernador.[20]​
Según la tradición islámica es Adán quien lleva a cabo la primera construcción en la Meca a petición de Alá. Con el tiempo la construcción va desapareciendo hasta los días de Ismael y Abraham. Sarai, esposa de Abraham al ser estéril, ofreció a su esclava Agar a Abraham. Agar concibe pronto. Sarai, celosa, trata a Agar duramente, forzándola a huir con su hijo al desierto. Al estar ambos al borde de morir de sed, aparece ante la madre de Ismael un ángel y le pide que golpee el suelo. Al golpearlo comienza a brotar del suelo agua y se forma el pozo de Zamzam. Alrededor de este pozo se instalan junto con más gente del desierto y surge Meca (Beka).
Tras varias visitas de Abraham a Ismael, Abraham se propone, por petición de Alá, construir la habitación de la Kaaba para que la gente peregrine a este lugar. Con el tiempo se fue olvidando el adorar a un solo dios, y las distintas tribus comenzaron a llegar a La Meca llevando sus dioses de piedra. La Meca se convirtió pronto en un lugar de peregrinaje para distintas formas de idolatría. Esta situación duró hasta que llegó Mahoma, quien "recordó" a los pueblos, entre otras cosas, el adorar solo a Alá sin asociarle nada.
Las duras condiciones de la península arábiga significaban, por lo general, un estado de conflicto constante entre las distintas tribus de Arabia, pero una vez al año se declaraba una tregua y convergían en La Meca en un peregrinaje anual. Este viaje se emprendía por razones religiosas, para rendir homenaje al santuario y para beber las aguas del Pozo de Zamzam. Pero también servía cada año como momento y lugar donde arbitrar las controversias, para resolver deudas y desarrollar el comercio en las ferias de La Meca. Estos eventos anuales de las tribus dieron un sentido de identidad común e hicieron de La Meca una ciudad muy importante en toda la península.
En el siglo V, la tribu Quraysh se hizo con el control de La Meca y sus miembros se convirtieron en expertos mercaderes y comerciantes. En el siglo VI se sumaron al lucrativo comercio de especias, ya que las luchas en otras partes del mundo fueron motivo para desviar las rutas comerciales de las peligrosas rutas marítimas a las relativamente más seguras rutas terrestres.
Era miembro de una pequeña facción, la hachemita, de la tribu gobernante Quraysh. Después de que comenzara a recibir revelaciones y empezara la predicación en contra del paganismo de la ciudad, emigró (véase Hégira) en el año 622 con algunos seguidores a la ciudad de Medina, y lanzó una serie de redadas contra el comercio de la Meca, atacando caravanas. En la batalla de Badr diezmó el liderazgo que ostentaba La Meca y ganó para sí un considerable prestigio entre las tribus beduinas. El conflicto siguió, como en la batalla de Uhud y la batalla de la Trinchera.
En 628, Mahoma adoptó una postura más pacífica: él y algunos seguidores trataron de entrar en La Meca en peregrinación para mostrar que los rituales tradicionales podrían ser adoptados por el islam. Con el tratado de Hudaybiyyah se acordó una tregua que permitiría a los musulmanes entrar en la ciudad. Dos años después, la tregua se rompió, pero en lugar de combatir, la ciudad de La Meca simplemente se entregó a Mahoma, quien declaró la amnistía para los habitantes y dio generosos regalos a los principales Quraysh.
Realizó así los principales cambios, ordenó retirar y destruir todas las imágenes de culto del interior de la Kaaba, que se convertía en ese momento en el lugar más sagrado para el islam y centro de la peregrinación musulmana. Regresó a Medina después de nombrar a Attab Bin Usaid gobernador con un sueldo de 1 dirham al día. Muchas de las tribus arábicas decidieron aceptar el islam como su propia fe. Mahoma logró entonces algo que parecía imposible: unir a las tribus guerreras de la península arábiga en una sola umma. Su predicación y coránicas visiones se basaron en una síntesis de múltiples sistemas de creencias, combinando elementos de la Arabia preislámica con ideas religiosas judías y cristianas.
Mahoma murió en 632, pero con el sentido de unidad que él había transmitido a los árabes, el islam comenzó una rápida expansión, y en los próximos cien años se extendió hasta África del Norte y Asia. A medida que el Imperio islámico crecía, La Meca continuó atrayendo peregrinos, no solo de Arabia, sino en adelante de todo el Imperio.
Otro cambio importante fue que los musulmanes se habían postrado en dirección a Jerusalén en sus oraciones diarias, pero, tras su enfrentamiento con la comunidad judía de Medina, Mahoma aseguró que había descendido una aleya del Corán (Qur'an), revelándole el cambio de esta práctica y exigiendo a todos mirar hacia la Kaaba en su lugar.
A lo largo de la historia islámica, La Meca nunca ha sido capital de ningún califato. La emigración de Mahoma a Medina estableció la ciudad como la primera capital de la Umma. Ali Ibn Abi Tálib, yerno de Mahoma y el cuarto de los llamados califas bien guiados, trasladó la capital de la Ummah a Kufa, en Irak. La ciudad volvió a entrar brevemente en la historia política islámica cuando un grupo liderado por Abd Allah ibn al-Zubayr, se opuso a los nuevos califas omeyas. El califa Yazid I sitió La Meca en el 683. Cuando la Dinastía de los Omeyas tomó el poder, trasladó la capital a Damasco, en Siria, y luego el Califato Abbasí transfirió la capital a Bagdad, en Irak. El centro de la Úmmah musulmana permaneció en Bagdad durante casi 500 años, y floreció como centro de investigación y comercio. Sin embargo, La Meca no permaneció al margen de la lucha entre facciones musulmanas. En el año 930 los cármatas conquistaron y saquearon la ciudad, apoderándose de la piedra negra de la Kaaba, que no devolverían hasta varias décadas después.
En el siglo XIII, los mongoles invadieron Bagdad y saquearon la ciudad. Este evento fue una de las acciones más detestadas en la historia islámica. Poco después de la batalla de Bagdad (1258), los mongoles reaparecen en el oeste y conquistan Siria. El Cairo, que en árabe significa "La triunfante" o "La radiante", emerge rápidamente como el nuevo centro de poder en la Umma. Cuando el Imperio otomano se volvió poderoso, la capital se trasladó a Estambul (Constantinopla). Cuando los peregrinos llegaban para el Hach, a menudo financiaban el viaje con los bienes que se podrían vender en los mercados de La Meca, y adquirían bienes que podrían vender cuando regresaran a casa.
Posteriormente, la ciudad jugó poco papel en la política, ya que más bien era una ciudad destinada a la devoción. Durante siglos fue regida por los hachemitas, jerifes de La Meca, descendientes de Mahoma por su nieto Hasan ibn Ali. Los jerifes gobernaban en nombre del califa o cualquier gobernante musulmán, y se habían destacado como los guardianes de las Dos Mezquitas Sagradas.
En el siglo XVI, los turcos se apoderaron de la ciudad, que quedó bajo su dominio aunque dejaron que siguieran administrándola los jerifes hachemíes, descendientes de Mahoma y de su yerno Ali. En 1916 el jerife Husayn ibn Ali se convierte en rey del Hiyaz tras un levantamiento contra el poder otomano, que se conocería como Rebelión Árabe. La Meca fue capital de este efímero reino, que en 1924 fue conquistado por los saudíes y anexionado a su reino de Arabia central (en 1932 ambos territorios pasarían a llamarse Arabia Saudita).
La Meca cuenta con un clima extremadamente árido. A diferencia de otras ciudades de Arabia Saudita, La Meca mantiene su temperatura cálida durante el invierno, que puede variar de entre 18 °C durante la noche, a 30 °C por las tardes. Las temperaturas en verano son consideradas muy calurosas, a menudo superan los 40 °C por las tardes, cayendo a 30 °C por las noches. Las lluvias suelen caer en pequeñas cantidades entre los meses de noviembre y enero.
Debido a esta escasez, las precipitaciones suponen una amenaza de inundación y han sido un peligro desde tiempos remotos. En el último siglo, la más severa ocurrió en 1942. Desde entonces, se han construido varias presas para solucionar el problema.
La principal industria en La Meca en la época moderna sigue siendo acoger a la peregrinación anual del Hach, así como a los peregrinos que visitan la ciudad en todas las demás épocas del año. Las principales paradas de su visita son la mezquita Masyid al-Haram y el pozo de Zamzam.
La Masyid al-Haram (en árabe المسجد الحرام, "la mezquita sagrada") es la mezquita más importante de la ciudad de La Meca y el primer lugar santo del islam. Es considerada la mezquita más grande del mundo.
La actual mezquita data de 1570. Tiene la forma de un cuadrado central rodeado por muros de piedra. Alrededor del santuario interior existe un pavimento de mármol, el mataf. En el centro del patio central se encuentra la Kaaba, el templo más sagrado del islam.
La primera mezquita se construyó en el año 638, cuando el aumento de los musulmanes llevó al califa Úmar ibn al-Jattab a ampliar el lugar. Los otomanos la agrandaron otra vez y la reformaron en su estilo en 1570. Desde los años 1990 el gobierno saudí inició un gran programa de ampliación del patio exterior y la creación de accesos, destruyendo antiguas mezquitas y cementerios que había alrededor (destrucción de patrimonio islámico en Arabia Saudí).
La Kaaba (en árabe: الكعبة‎, al-ka‘ba, lit. 'el dado' o 'el cubo'), la «casa de Dios», es un fragmento de meteorito recubierto de la kiswa —una tela de seda negra con caligrafías bordadas en oro de la profesión de fe musulmana y versos coránicos— hacia el cual todos los musulmanes oran cinco veces al día. La dirección de la oración se conoce con el nombre de alquibla, que está representada por uno de los muros de la mezquita, fácilmente reconocible porque en él se abre el nicho llamado mihrab.
Todos los peregrinos están obligados a caminar a la izquierda alrededor de la Kaaba siete veces y tienen que intentar tocar la esquina de la Piedra Negra, en un ritual llamado Tawaf.
Los musulmanes creen que el Pozo de Zamzam fue revelado a Agar, madre de Ismael, ya que ella fue a buscar agua desesperadamente para su hijo pequeño. No encontró agua hasta que descubrió el Zamzam, que calmó la sed del pequeño. La Meca se halla en un valle seco caliente con pocas fuentes de agua, y según la tradición, el agua del Zamzam está bendecida por Dios.
Se cree que el agua de Zamzam tiene propiedades especiales, que calma el hambre y cura enfermedades. Son pocos los peregrinos que regresan de su peregrinación sin una botella con agua de Zamzam. Es el agua que se sirve al público a través de refrigeradores en todo el Masyid al-Haram en La Meca y la Masyid al-Nabawi en Medina. Todos los peregrinos hacen todo lo posible para beber de esta agua durante su peregrinación y algunos mojan sus hábitos del ihram en ella, de modo que la tela se podrá utilizar para su propio entierro, como mortaja cuando mueran.
Durante el peregrinaje mayor, los peregrinos viajan hacia Mina, un pequeño pueblo cercano, en el que el diablo está representado por un muro de 26 metros. Los peregrinos lo apedrean simbólicamente, en lo que se denomina la Lapidación del diablo. Hasta el año 2004 el diablo estaba representado por tres columnas, pero entonces el gobierno saudí las sustituyó por el citado muro. Después de Mina, los peregrinos se dirigen al monte Arafat, de 70 metros de altura, lugar destinado a la oración. Se cree que fue ahí donde el profeta Mahoma pronunció su discurso final.
Un número importante de peregrinos viajan anualmente para participar en el Haŷŷ en La Meca. Desde mediados del siglo XX, el constante incremento de peregrinos debido a la generalización de los transportes modernos y el gran aumento de la natalidad en los países musulmanes producen que con tanta gente reunida en un mismo sitio en un mismo momento, cualquier error en el control de la muchedumbre pueda ocasionar un auténtico desastre. Algunas de las tragedias más recientes han sido:[23]​
La economía de La Meca depende casi exclusivamente del dinero gastado por las personas que asisten al Hach. Durante esta peregrinación, ingresan a la ciudad más de 100 millones de dólares, y el gobierno saudí gasta cerca de 50 millones de dólares en servicios para los peregrinos.
Hay algunas industrias y fábricas en la ciudad, pero La Meca ya no desempeña un papel importante en la economía de Arabia Saudita, que se basa principalmente en las exportaciones de petróleo. Las pocas industrias que operan en La Meca son de textiles y muebles. El agua es escasa y los alimentos deben ser importados. [cita requerida]
La asistencia sanitaria es proporcionada por el gobierno saudí de forma gratuita a todos los peregrinos. Hay siete hospitales principales en La Meca:
Hay también muchas clínicas disponibles tanto para residentes como para peregrinos.

El océano Glacial Ártico[2]​ (o simplemente océano Ártico) es la parte del océano mundial más pequeña y más septentrional del planeta. Se encuentra principalmente al norte del círculo polar ártico, ocupando el área entre Europa, Asia y América del Norte.[1]​ Abarca unos 14 056 000 km² de extensión[1]​ y sus profundidades oscilan entre los 2000 m y 4000 m en la región central, y los 100 m en la plataforma continental. Su profundidad media es de 1205 m bajo el nivel del mar.[1]​
Este océano limita con la parte norte del Atlántico, recibiendo grandes masas de agua a través del estrecho de Fram y el mar de Barents. Está limitado por el estrecho de Bering, entre Chukotka (Rusia) y Alaska (EE.UU.), que lo separa del Pacífico; por la costa septentrional de Alaska y Canadá. También limita con una porción del litoral boreal de Europa y con el de Asia.
Grandes masas de hielo protegen durante todo el año a este océano de las influencias atmosféricas. En su parte central pueden encontrarse casquetes de hielo de hasta cuatro metros de espesor. Las grandes capas de hielo suelen formarse por el deslizamiento de grandes paquetes de hielo uno sobre otro.
Las temperaturas en invierno suelen rondar los −50 °C debido a los fuertes vientos provenientes de Siberia (Rusia); mientras que en el verano apenas pueden superar el 0 °C; en tanto que en la plataforma continental pueden darse temperaturas de hasta 30 °C.
El océano Glacial Ártico ocupa una cuenca aproximadamente circular y se extiende por una superficie de alrededor de 14 056 000 kilómetros cuadrados,[1]​ casi el tamaño de Rusia.[3]​ La costa tiene 45 389 kilómetros de largo.[1]​ Está rodeado por las masas terrestres de Eurasia, América del Norte, Groenlandia y por varias islas. Generalmente se considera que incluye la bahía de Baffin, el mar de Barents, el mar de Beaufort, el mar de Chukotka, el mar de Siberia Oriental, el mar de Groenlandia, la bahía de Hudson, el estrecho de Hudson, el mar de Kara, el mar de Laptev, el mar Blanco y otros conjuntos hídricos. Se conecta con el océano Pacífico a través del estrecho de Bering y con el océano Atlántico a través del mar de Groenlandia y el mar de Labrador.[4]​
Una dorsal oceánica, la dorsal de Lomonósov, separa la honda cuenca polar septentrional marino en dos cuencas oceánicas: la Euroasiática que tiene una profundidad de entre 4000 y 4500 metros, y la Asiático-americana (a veces llamada de Norteamérica o cuenca hiperbórea), de alrededor de 4000 metros de profundidad. La batimetría del fondo oceánico está marcado por dorsales de fallas, llanuras de la zona abisal, profundidades del océanos y cuencas. La profundidad media del océano Glacial Ártico es de 1038 metros.[5]​ El punto más profundo está en la cuenca euroasiática, con 5450 metros.
Las dos grandes cuencas están subdivididas a su vez por dorsales en la cuenca canadiense (entre Alaska/Canadá y la dorsal Alpha), la cuenca de Makarov (entre las crestas Alpha y de Lomonósov), la cuenca del Fram (entre la dorsal de Lomonosov y la de Gakkel) y la cuenca de Nansen (cuenca de Amundsen) (entre la dorsal de Gakkel y la plataforma continental que incluye la Tierra de Francisco José).
Según estudios realizados por especialistas de la Universidad de Oxford (Reino Unido) y del Instituto Real de los Países Bajos para la Investigación Marina, el océano Ártico gozaba, hace unos setenta millones de años, de temperaturas similares a las que hoy en día se encuentran en el mar Mediterráneo, con mediciones de unos 15 °C; y temperaturas de unos 20 °C hace unos veinte millones de años.
Llegaron a esta conclusión los investigadores después de estudiar materiales orgánicos encontrados en el lodo de islotes de hielo del océano Glacial Ártico. No se sabe aún por qué se daban estas temperaturas en aquellos tiempos, pero se cree en que el responsable puede haber sido el efecto invernadero derivado de una fuerte concentración de dióxido de carbono en la atmósfera (el problema de esta hipótesis es el extraordinariamente mínimo efecto invernadero del gas carbónico).
La ocupación humana en la región polar de América del Norte se remonta al menos a entre 17 000 y 50 000 años atrás, durante la glaciación de Wisconsin. En ese momento, la caída del nivel del mar permitió a las personas cruzar el puente terrestre de Bering que unía Siberia con Alaska, lo que condujo al poblamiento de América.[6]​
Los primeros grupos paleoesquimales incluyeron los Pre-Dorset (c. 3200-850 a. C.), la cultura Saqqaq de Groenlandia (2500-800  a. C.), las culturas Independencia I e Independencia II del noreste de Canadá y Groenlandia (c. 2400-1800 a. C. y c. 800-1 a. C.) y los Groswater de Labrador y Nunavik. La cultura Dorset se extendió por el Ártico de América del Norte entre el 500 a. C. y el 1500 d. C. Los Dorset fueron la última gran cultura paleoesquimal en el Ártico antes de la migración hacia el este desde la actual Alaska de los Thule, los antepasados de los inuit modernos.[7]​
La tradición Thule duró desde aproximadamente el 200 a. C. hasta el 1600 d. C., surgió alrededor del estrecho de Bering y más tarde abarcó casi toda la región ártica de Norteamérica. El pueblo Thule fue el antepasado de los inuit, que actualmente viven en Alaska, Territorios del Noroeste, Nunavut, el norte de Quebec, Labrador y Groenlandia.[8]​
Durante gran parte de la historia europea, las regiones del polo norte permanecieron en gran parte inexploradas y su geografía era conjetural. Piteas de Massilia registró un relato de un viaje hacia el norte en el 325 a. C., a una tierra que llamó "Eschate Thule", donde el sol solo se ponía durante tres horas al día y el agua era reemplazada por una sustancia congelada "en la que no se puede caminar ni navegar". Probablemente estaba describiendo icebergs, mientras que "Thule" era probablemente Noruega, aunque también se han sugerido las islas Feroe o Shetland.[9]​
El Glacial Ártico está bajo un clima polar caracterizado por un frío persistente y rangos de temperatura anuales relativamente estrechos. Los inviernos se caracterizan por la noche polar, el frío extremo, las frecuentes inversiones de temperatura en niveles bajos y las condiciones climáticas estables.[10]​ Los ciclones solo son comunes en el lado atlántico.[11]​ Los veranos se caracterizan por la luz diurna continua (sol de medianoche) y la temperatura del aire puede elevarse ligeramente por encima de los 0 °C. Los ciclones son más frecuentes en verano y pueden traer lluvia o nieve.[11]​ Presenta nubes durante todo el año, con una nubosidad media que oscila entre el 60 % en invierno y más del 80 % en verano.[12]​
La densidad del agua de mar, a diferencia del agua dulce, aumenta a medida que se acerca al punto de congelación y, por lo tanto, tiende a hundirse. En general, es necesario que los 100-150 m superiores de agua del océano se enfríen hasta el punto de congelación para que se forme hielo marino.[13]​
El clima de la región ártica ha variado significativamente durante la historia de la Tierra. Durante el máximo térmico del Paleoceno-Eoceno hace 55 millones de años, cuando el clima global experimentó un calentamiento de aproximadamente 5 a 8 °C, la región alcanzó una temperatura anual promedio de 10 a 20 °C.[14]​[15]​[16]​ Las aguas superficiales del Océano Ártico más septentrional se calentaron, al menos estacionalmente, lo suficiente para sustentar formas de vida tropicales (los dinoflagelados Apectodinium augustum) que requieren de temperaturas de más de 22 °C.[17]​ Actualmente, la región ártica se está calentando dos veces más rápido que el resto del planeta.[18]​[19]​
Debido a su relativo aislamiento de otros océanos, el Glacial Ártico tiene un sistema de flujo de agua singularmente complejo. Se asemeja a algunas características hidrológicas del mar Mediterráneo, refiriéndose a sus aguas profundas que tienen una comunicación limitada a través del estrecho de Fram con la cuenca atlántica, "donde la circulación está dominada por el forzamiento termohalino".[20]​ El océano Ártico tiene un volumen total de 18,07 × 106 km³, equivalente a aproximadamente el 1,3 % del océano mundial. La circulación superficial media es predominantemente ciclónica en el lado euroasiático y anticiclónica en la cuenca canadiense.[21]​
El agua ingresa desde los océanos Pacífico y Atlántico y se puede dividir en tres masas de agua únicas. La masa de agua más profunda se llama Agua del Fondo Ártico y comienza alrededor de los 900 m de profundidad.[20]​ Está compuesto por el agua más densa del océano mundial y tiene dos fuentes principales: el agua de la plataforma ártica y las aguas profundas del mar de Groenlandia. El agua en la región de la plataforma que comienza cuando la afluencia del Pacífico pasa a través del estrecho estrecho de Bering a una tasa promedio de 0,8 Sverdrups y llega al mar de Chukotka.[22]​ Durante el invierno, los vientos fríos de Alaska soplan sobre el mar de Chukotka, congelando el agua de la superficie y empujando este hielo recién formado hacia el Pacífico. La velocidad del desplazamiento del hielo es de aproximadamente 1 a 4 cm/s.[21]​ Este proceso deja aguas densas y saladas en el mar que se hunden sobre la plataforma continental en el océano Ártico occidental y crean una haloclina.[23]​
Esta agua se encuentra con el agua profunda del mar de Groenlandia, que se forma durante el paso de las tormentas invernales. A medida que las temperaturas se enfrían drásticamente en el invierno, se forma hielo y la convección vertical intensa permite que el agua se vuelva lo suficientemente densa como para hundirse debajo del agua salina cálida que se encuentra debajo.[20]​ El agua del fondo del Ártico es de importancia crítica debido a su flujo de salida, que contribuye a la formación de las aguas profundas del Atlántico. El vuelco de esta agua juega un papel clave en la circulación global y la moderación del clima.
En el rango de profundidad de 150 a 900 m hay una masa de agua denominada agua del Atlántico. La afluencia de la corriente del Atlántico norte entra a través del estrecho de Fram, enfriándose y hundiéndose para formar la capa más profunda de la haloclina, donde rodea la cuenca ártica en sentido contrario a las agujas del reloj. Esta es la afluencia volumétrica más alta al Océano Ártico, que equivale aproximadamente a 10 veces la afluencia del Pacífico, y crea la Corriente Límite del Océano Ártico.[22]​ Fluye lentamente, a unos 0,02 m/s.[20]​ El agua del Atlántico tiene la misma salinidad que el agua del fondo del Ártico, pero es mucho más cálida (hasta 31 °C). De hecho, esta masa de agua es en realidad más cálida que el agua superficial y permanece sumergida solo debido al papel de la salinidad en la densidad.[20]​ Cuando el agua llega a la cuenca, los fuertes vientos la empujan hacia una gran corriente circular llamada Giro oceánico de Beaufort. El agua en Beaufort es mucho menos salina que la del mar de Chukotka debido a la afluencia de grandes ríos canadienses y siberianos.[23]​
La masa de agua final definida en el Océano Ártico se llama agua de superficie ártica y se encuentra en el rango de profundidad de 150 a 200 m. La característica más importante de esta masa de agua es una sección denominada capa subterránea. Es un producto del agua del Atlántico que ingresa a través de cañones y se somete a una intensa mezcla en la plataforma siberiana.[20]​ A medida que se arrastra, se enfría y actúa como escudo térmico para la capa superficial. Este aislamiento evita que el agua cálida del Atlántico derrita el hielo de la superficie. Además, esta agua forma las corrientes más rápidas del Ártico, con una velocidad de alrededor de 0,3 a 0,6 m/s.[20]​ Complementando el agua de los cañones, también contribuye a esta masa de agua algo de agua del Pacífico que no se hunde hasta la región de la plataforma después de pasar por el estrecho de Bering.
Las aguas que se originan en el Pacífico y el Atlántico salen por el estrecho de Fram entre Groenlandia y la isla de Svalbard, que tiene aproximadamente 2700 m de profundidad y 350 km de ancho. Esta salida es de aproximadamente 9 Sv.[22]​ El ancho del estrecho de Fram es lo que permite tanto la entrada como la salida en el lado atlántico del Océano Ártico. Debido a esto, está influenciado por la fuerza de Coriolis, que concentra el flujo de salida a la Corriente de Groenlandia Oriental en el lado occidental y el flujo de entrada a la Corriente de Noruega en el lado oriental.[20]​ El agua del Pacífico también sale a lo largo de la costa occidental de Groenlandia y el estrecho de Hudson (a 1-2 Sv), proporcionando nutrientes al archipiélago canadiense.[22]​
Como se señaló, el proceso de formación y movimiento del hielo es un factor clave en la circulación del océano Ártico y la formación de masas de agua; con esta dependencia, dicho océano experimenta variaciones debido a los cambios estacionales en la capa de hielo marino. El movimiento del hielo marino es el resultado de la fuerza del viento, que está relacionado con una serie de condiciones meteorológicas que experimenta el Ártico durante todo el año. Por ejemplo, el anticiclón de Beaufort, una extensión del sistema del anticiclón de Siberia, es un sistema de presión que impulsa el movimiento anticiclónico del del Giro de Beaufort.[21]​ Durante el verano, esta zona de alta presión se acerca más a sus lados siberiano y canadiense. Además, hay una cresta de presión a nivel del mar sobre Groenlandia que impulsa fuertes vientos del norte a través del estrecho de Fram, lo que facilita la exportación de hielo. En el verano, el contraste de la presión es menor, produciendo vientos más débiles. Un último ejemplo de movimiento del sistema de presión estacional es el sistema de baja presión que existe sobre los mares nórdico y de Barents. Es una extensión de la depresión de Islandia, que crea una circulación oceánica ciclónica en esta área. La depresión se desplaza hacia el centro sobre el Polo Norte en el verano. Todas estas variaciones en el Ártico contribuyen a que la deriva del hielo alcance su punto más débil durante los meses de verano. También hay evidencia de que la deriva está asociada con la fase de Oscilación Ártica y Oscilación Multidecadal Atlántica.[21]​
Existen unas cuatrocientas especies animales en esta zona. De ellas, la más conocida es el oso polar, el mayor carnívoro del lugar. Llega a tener un peso de 800 kg y se alimenta de focas y peces, aunque si no logra atraparlos puede reemplazarlos momentáneamente por musgos y líquenes.
Seis especies de focas habitan este lugar, aunque su número ha ido decreciendo desde el siglo XIX debido a su depredador natural, el oso polar, y a la caza indiscriminada a que fue sometida por el hombre debido a lo preciado de su piel y su grasa. Otro poblador típico de la zona es la ballena, igualmente amenazada y que, actualmente, se halla protegida de la captura indiscriminada.
Debido a la pronunciada estacionalidad de 2 a 6 meses de sol de medianoche y noche polar en el Glacial Ártico,[24]​ la producción primaria de organismos fotosintetizadores como las algas heladas y el fitoplancton se limita a los meses de primavera y verano.[25]​ Los consumidores importantes de los autótrofos en el Océano Ártico central y los mares de la plataforma adyacente incluyen el zooplancton, especialmente los copépodos (Calanus finmarchicus, Calanus glacialis y Calanus hyperboreus)[26]​ y los eufáusidos,[27]​ así como la fauna asociada al hielo (como los anfípodos).[26]​ Estos forman un vínculo importante entre los productores primarios y los niveles tróficos superiores. La composición de los niveles tróficos más altos en el Océano Ártico varía según la región (lado del Atlántico frente al lado del Pacífico) y con la capa de hielo marino. Los consumidores secundarios del mar de Barents, un mar de la plataforma ártica de influencia atlántica, son principalmente especies subárticas, como el arenque, el bacalao y el capelán.[27]​ En las regiones cubiertas de hielo del Océano Ártico central, el bacalao polar es un depredador de los consumidores primarios.
La banquisa polar está adelgazando, y en muchos años habrá un agujero estacional en la capa de ozono.[28]​
La reducción de la superficie de hielo en el océano Ártico reduce el albedo medio del planeta, lo que posiblemente dé como resultado el calentamiento global en un mecanismo de retroalimentación positiva.[29]​ La investigación muestra que el Ártico puede quedar libre de hielo por primera vez en la historia de la Humanidad entre el año 2013 y 2040.[30]​ Muchos científicos están actualmente preocupados por el calentamiento de las temperaturas en el Ártico, porque podrían causar que grandes cantidades de agua fresca derretida entrase en el Atlántico norte, posiblemente perturbando los patrones de corrientes oceánicas globales. Potencialmente pueden ocurrir después drásticos cambios en el clima de la Tierra.[29]​
Los investigadores predicen que, en no más de cincuenta años, el océano Ártico será perfectamente navegable durante el verano.[31]​ Es que el hielo que cubre esta masa oceánica se está haciendo cada vez más delgado, debido a que el tiempo de duración de altas temperaturas es cada vez mayor. Durante los pasados años se ha observado la fusión de la capa de hielos y, en agosto de 2004, científicos estadounidenses que navegaban en un buque y ruso, denunciaron la existencia de una laguna en el Polo Norte, que no pudo ser confirmada por imágenes satelitales, pero que en modo alguno sorprendió a la comunidad científica, quienes vienen alertando sobre el peligro del calentamiento global.
Se sabe, pues, que el espesor de la capa de hielos del océano Ártico ha disminuido un 40 % durante los pasados cincuenta años y los resultados indican que si esto continúa, la fusión de los hielos será más rápida cada vez, culminando con la desaparición de estos durante el verano, con serias consecuencias para el equilibrio ecológico de la zona y para el hábitat de ciertas especies, como el oso polar que necesita de esas capas de hielo para sobrevivir y cazar sus alimentos.
Otras preocupaciones medioambientales se refieren a la contaminación radiactiva del océano Ártico por, por ejemplo, los residuos radiactivos rusos en el mar de Kara[32]​ y pruebas nucleares realizadas durante la época de la Guerra fría en lugares como Nueva Zembla.[33]​
El deshielo del Ártico abre nuevas posibilidades para explotar sus recursos naturales. En el lecho marino del Ártico se encuentra el 25 por ciento de las reservas mundiales de petróleo y gas natural. También el estaño, manganeso, oro, níquel, plomo y platino están presentes en cantidades importantes. Por ello y sumado a la importancia geoestratégica, el 2 de agosto de 2007 dos batiscafos rusos «Mir» realizaron una inmersión en el océano Glacial Ártico, en el Polo Norte, e instalaron en el fondo una bandera rusa, así como una cápsula con mensaje para generaciones venideras. Los Mir recogieron pruebas para demostrar que las cordilleras subacuáticas Lomonósov y Mendeléiev son la extensión natural de la plataforma continental de Rusia, hipótesis que, de ser confirmada, permitiría a Rusia reivindicar en el futuro derechos exclusivos sobre la explotación de los recursos minerales en esta zona.[34]​[35]​
Ocasionalmente se disgregan islas de hielo de la parte norte de la isla Ellesmere, y se forman icebergs a partir de los glaciares de la costa occidental de Groenlandia y el extremo noreste de Canadá. El permafrost se encuentra en la mayor parte de las islas. El océano está virtualmente cerrado por el hielo desde octubre hasta junio, y los barcos que lo naveguen están amenazados con quedar cubiertos de hielo desde octubre hasta mayo.[36]​ Antes de que llegaran los modernos rompehielos, los barcos que zarpaban al océano Glacial Ártico se arriesgaban a quedar atrapados o aplastados por los témpanos de hielo (aunque el SS Baychimo vagó por el océano Ártico desatendido durante décadas a pesar de estos riesgos).

El mar del Norte es un mar marginal del océano Atlántico, situado entre las costas de Noruega y Dinamarca al este, las de las islas británicas al oeste y las de Alemania, los Países Bajos, Bélgica y Francia al sur. El Skagerrak constituye una especie de bahía al este del mar, la cual lo conecta con el Báltico a través del Kattegat; también está conectado con el Báltico mediante el canal de Kiel. El canal de la Mancha lo conecta al resto del Atlántico por el sur, mientras que por el norte conecta a través del mar de Noruega, que es el nombre que adopta el mar al norte de las islas Shetland.
Las mareas son bastante irregulares ya que confluyen en él una corriente proveniente del norte y otra del sur. Hay mucha lluvia y niebla durante todo el año, y del noroeste vienen violentas tormentas que hacen la navegación peligrosa.
Tiene una superficie de unos 750 000 km²,[1]​ una longitud aproximada de 960 km y una anchura máxima de 480 km. Es un mar muy poco profundo, con una profundidad media de 95 metros: el hecho que en el banco Dogger, en medio del mar y a una profundidad de unos 25 metros, se hayan encontrado restos de mamuts prueba que durante la última glaciación o bien estaba cubierto de hielo o bien estaba emergido. Con el deshielo, el banco se convirtió en una especie de último reducto en forma de isla.
Durante la Edad Antigua este mar se conocía como Oceanum o Mare Germanicum. El nombre actual se cree que surgió desde el punto de vista de las islas Frisias, desde donde quedaba totalmente al norte, y por oposición al mar del Sur (el mar de Frisia y el Zuiderzee, en los Países Bajos). A la larga, el nombre actual se acabó imponiendo, de manera que ya era predominante durante la Edad Moderna. En la citada Edad Moderna fue común llamar Mar del Norte o Mar del Nord a todo el océano Atlántico, siendo por contrapartida llamado «Mar del Sur» o «Mar del Sud» todo el océano Pacífico.
Según las lenguas oficiales de los estados que lo rodean, se denomina Mer du Nord, en francés; Noordzee, en neerlandés; Nordsee, en alemán; Nordsjön, en sueco; Nordsøen, en danés; Nordsjøen, en noruego; y North Sea en inglés. En frisón se dice Noardsee y en gaélico escocés A' Mhuir en Tuath.
Tiene importantes yacimientos de petróleo y gas natural, los cuales comenzaron a explotarse en los años 1970.
La máxima autoridad internacional en materia de delimitación de mares, la Organización Hidrográfica Internacional («International Hydrographic Organization, IHO), considera el mar del Norte como un mar. En su publicación de referencia mundial, «Limits of oceans and seas» (Límites de océanos y mares, 3.ª edición de 1953), le asigna el número de identificación 4 y lo define de la forma siguiente:
Durante la Edad Antigua este mar se conocía como Oceanum o Mare Germanicum. El nombre actual se cree que surgió desde el punto de vista de las Islas Frisias, desde donde quedaba totalmente al norte, y por oposición al mar del Sur (actualmente mar de Wadden, en los Países Bajos). A la larga, el nombre actual se acabó imponiendo, de forma que ya era predominante durante la Edad Moderna.
Según las lenguas oficiales de los estados que lo rodean, es denominado Mer du Nord en francés, Noordzee en neerlandés, Nordsee en alemán, Nordsøen en danés, Nordsjøen en noruego y North Sea en inglés. En frisón occidental se dice Noardsee, en frisón septentrional Weestsiie (el mar Occidental) y en gaélico escocés A' Mhuir a Tuath.
Uno de los primeros nombres que se recogen es el de "Septentrionalis Oceanus", o "Océano norteño", que fue citado por Plinio. Aun así, los celtas que vivían a lo largo de su costa se referían cómo el “Morimaru”, o el "mar muerto", que también fue adoptado por los pueblos germánicos, resultando en “Morimarusa”. Este nombre se refiere a la "agua muerta" o franjas resultantes de una capa de agua dulce que reponía a la parte superior de una capa de agua salada hecho que resultaba una agua de aspecto muy tranquilo. La referencia a este mismo fenómeno se prolongó en la Edad Media, por ejemplo, “giliberōt” del alto alemán antiguo o en neerlandés medio “lebermer” o “libersee”.
Otros nombres comunes en el uso durante largos periodos fueron los términos latinos Mare Frisicum y Mare Germanicum (u Oceanus Germanicus), así como sus equivalentes en otros idiomas.
El mar del Norte está delimitado, en la parte occidental, por las islas Orcadas y la costa este de Inglaterra y Escocia,[3]​ en la parte oriental por el extremo sur-occidental de la península de Escandinavia y el oeste de Jutlandia (litoral noruego y danés, respectivamente) y al sur por la costa occidental de Alemania, el litoral de los Países Bajos y Bélgica y del extremo norte de Francia hasta el canal de la Mancha.[4]​ Al suroeste, más allá del estrecho de Dover o de Calais, el mar del Norte se convierte en el canal de la Mancha, conexión con el océano Atlántico.[3]​[4]​ Al este, se conecta con el mar Báltico a través del Skagerrak y Kattegat, dos estrechos que separan Dinamarca de Noruega y Suecia, respectivamente.[3]​ Al norte está bordeado por las Shetland y se conecta con el mar de Noruega, que se encuentra al extremo  nororiental del Atlántico.[3]​[5]​
Se extiende a lo largo de 970 kilómetros y tiene una anchura de 580 kilómetros, ocupa una superficie de 750 000 kilómetros cuadrados y un volumen de 94 000 kilómetros cúbicos.[6]​ Todo alrededor del litoral del mar del Norte hay islas y archipiélagos importantes, como por ejemplo las islas Shetland, las Orcadas y las islas Frisias.[4]​ El mar del Norte recibe agua dulce proveniente de una serie de cuencas continentales europeas, así como de las cuencas hidrográficas de las Islas Británicas. También va a parar el agua del mar Báltico, donde desembocan también una gran cantidad de ríos, por lo cual se trata de una mar con poca salinidad. Los ríos principales que desaguan al mar del Norte son el Elba, el Rin y el Mosa. La cuenca del Elba drena una superficie de 149 000 kilómetros cuadrados, que incluye 18 ciudades y sus afluentes. El delta del Rin-Mosa recibe las descargas de agua de una superficie de 199 000 kilómetros cuadrados, donde se encuentran 68 ciudades.[7]​[8]​ Alrededor de 184 millones de personas viven en la cuenca de los ríos que fluyen hacia el mar del Norte. Esta área contiene densas concentraciones industriales.[9]​
Hacia el norte a partir de latitud norte 53 ° 24', de una manera general, el fondo del mar del Norte 	
desciende en forma inadecuada. Al sur, se inclina hacia el paso de Calais.
En su mayor parte, el mar del Norte se encuentra a la plataforma continental europea, con una profundidad mediana de 90 metros;[3]​[10]​ tiene pocas zonas más profundas de 100 metros. La única excepción es la zanja de Noruega, que se extiende paralela a la costa de Noruega desde Oslo hasta una zona al norte de Bergen.[3]​ Tiene entre 20 y 30 kilómetros de amplitud,[3]​ con unos 300 metros de profundidad ante Bergen, y una profundidad máxima de 725 metros en Skagerrak.[11]​[10]​ Al este de la Gran Bretaña, el banco Dogger, una meseta que proviene de una grande morrena del periodo glacial, producto de la acumulación de escombros de glaciares no consolidados, se eleva entre 15 y 30 metros por debajo de la superficie del mar.[12]​[13]​ Esta característica ha producido una zona muy rica para la pesca.[3]​
Long Forties y Broad Fourteens son áreas que reciben los nombres por la profundidad que hay medida en brazas; cuarenta brazas y catorce brazas o 73 y 26 metros de profundidad, respectivamente. Estos grandes bancos y otros similares hacen del mar del Norte unas zonas especialmente peligrosas para navegar,[14]​ que ha sido aliviada por la aplicación de los nuevos sistemas de navegación por satélite.[15]​
También hay grandes profundidades en la parte occidental del mar del Norte, como por ejemplo el Agujero del Diablo[16]​ a lo largo de Edimburgo, hasta 460 metros, y algunos fuera de la bahía de The Wash.[17]​ Estos corredores podrían haber sido formados por los ríos durante la última glaciación. De hecho, en este momento de la glaciación, el nivel del mar del Norte era más bajo que el nivel actual (regresión marina). Los ríos entonces habrían erosionado ciertas partes entonces a cuerpo descubierto que el mar cubre la actualidad (transgresión marina). Lo más probable es que sean restos de la valle del túnel, manteniéndose abiertas para las corrientes de marea.
La temperatura mediana en verano es de 17 °C y 6 °C en invierno.[6]​ El cambio climático se ha atribuido a un aumento en la temperatura media del mar del Norte.[18]​ Las temperaturas del aire en enero, de media se mueven en el rango de 0 a 4 °C y en julio, entre 13 y 18 °C. Durante los meses invernales son frecuentes los temporales y las tormentas.[3]​
Las medias de salinidad son entre 34 y 35 gramos de sal por litro de agua.[6]​ La salinidad tiene una mayor variabilidad donde hay entradas de agua dulce, como en los estuarios del Rin y el Elba, la conexión con el mar Báltico, y a lo largo de la costa de Noruega.[19]​
La pauta principal para el flujo de agua al mar del Norte es un antigiro en sentido hacia la derecha a lo largo de las aceras.[20]​ El mar del Norte es un brazo del océano Atlántico y recibe la mayoría de los corrientes oceánicas desde la apertura norteña-oeste y, en menor grado, una parte que proviene de la corriente cálida de la pequeña apertura del canal de la Mancha. Estas corrientes provienen de la costa de Noruega.[21]​ Tanto las corrientes de aguas profundas como los de superficie se mueven en diferentes direcciones; las aguas costeras superficiales de baja salinidad se desplazan hacia el exterior, y las más profundas y densas con una alta salinidad se mueven hacia la costa.[22]​
El mar del Norte se encuentra dentro de la plataforma continental y tiene unos tipos de olas más diferentes que las de las aguas oceánicas de más profundidad. Las velocidades de las olas se ven disminuidas y las amplitudes de las olas aumentan. Al mar del Norte hay dos sistemas anfidrómicos y un tercer sistema anfidrómico incompleto.[23]​[24]​ En el mar del Norte, la diferencia en la amplitud mediana de la marea es de entre 0 a 8 metros.[6]​
La marea Kelvin del océano Atlántico es una ola semidiurna que viaja hacia el norte. Parte de la energía de esta ola se propaga a través del canal de la Manguera hacia el mar del Norte. Posteriormente, la ola todavía viaja hacia el norte del océano Atlántico, y una vez pasadas las Islas Británicas, la ola Kelvin se desplaza hacia el este y al sur, y una vez más entra en el mar del Norte.[25]​
Los principales ríos que desembocan son:
Las costas este y oeste del mar del Norte son irregulares, formadas por los glaciares durante la época glacial. Las costas a lo largo de la parte sur están cubiertas con los restos de los sedimentos glaciales depositado.[3]​ La llegada de las montañas de Noruega hasta a la orilla del mar, ha provocado la creación de profundos fiordos y archipiélagos. Al sur de Stavanger, la costa se suaviza y cada vez hay menos islas.[3]​ La costa este de Escocia es bastante similar, aunque menos marcada que la de Noruega. Al nordeste de Inglaterra, los acantilados son de menos altitud y se componen de morrenas menos resistentes, lo que ha traído como consecuencia que la superficie se erosione más fácilmente, de forma que las costas tienen unos contornos más redondeados.[26]​[27]​ En los Países Bajos, Bélgica y al este de Inglaterra (Ánglia del Este), el litoral es bajo y pantanoso.[3]​ La costa este y el sudeste del mar del Norte, en el mar de Wadden, es principalmente arenosa y muy recta, particularmente en Bélgica y Dinamarca.[28]​
Las zonas costeras del sur fueron originalmente planas de inundación y tierras pantanosas. En las zonas especialmente vulnerables a las mareas tempestuosas, la gente se asentó detrás de los diques de elevación y en espacios naturales con tierras altas, como por ejemplo cordones litorales y los geests.[29]​ Ya en 500 aC, la gente realizaba la construcción de vivienda en cerros más altos artificiales para que la inundación no afectase.[29]​[30]​ No fue hasta principios de la Edad Media, el 1200, que los habitantes empezaron a conectar los diques de una sola sortija en una línea de diques a lo largo de toda la costa, convirtiendo así a las regiones anfibias de tierra y mar en tierra firme permanente.[29]​
La forma moderna que ha complementado el desbordamiento de diques y canales de desvío lateral, empezaron a aparecer en los siglos XVII y XVIII, construido en los Países Bajos.[31]​ Las inundaciones del mar del Norte del 1953 y 1962 fueron un impulso para la ulterior elevación de los diques, así como la reducción de la línea de la costa para presentar como la superficie lo menos posible el castigo de la mar y las tormentas.[32]​ Actualmente, el 27% de los Países Bajos está por debajo del nivel del mar protegida por diques, dunas y apartamentos playeros.[33]​
La gestión costera se compone hoy de varios niveles.[34]​ El dique pendiente reduce la energía de la mar de entrada, de forma que el dique no se le asigna el impacto. Diques que se encuentran directamente sobre el mar son especialmente reforzados.[34]​ De los diques en los últimos años, se ha reforzado en repetidas ocasiones, a veces de hasta 9 metros y se han reducido para reducir más la erosión de las oleadas,[35]​[36]​ donde las dunas son suficientes para proteger de la tierra detrás de ellos desde el mar, las dunas están plantadas con pasto de la playa para protegerlos de la erosión por el viento, el agua, y el tráfico de pie.[37]​
Las mareas tempestuosas tradicionalmente han amenazado, en particular, las costas de los Países Bajos, Bélgica, Alemania, y Dinamarca y zonas bajas del este de Inglaterra, en particular alrededor de The Wash y The Fens (los pantanos).[28]​ Las mareas de tormenta son causados por cambios en la presión atmosférica en combinación con los fuertes vientos creados por la acción de las olas.[38]​
El primer registro que existe de una inundación por una marea tempestuosa fue la Julianenflut, que se produjo el 17 de febrero de 1164. Entre sus acciones, empezó a formarse la Jadebusen, una bahía de la costa de Alemania. El año 1228, se registró una gran marea tempestuosa que provocó la muerte de más de 100 000 personas.[39]​ En 1362, la segunda Grote Manndränke golpeó la costa sur del mar del Norte. Las crónicas de la época hablan de más de 100 000 muertos, y una gran parte de la costa se perdió de forma permanente inundada por aguas del mar, incluida la legendaria ciudad perdida de Rungholt.[40]​ Ya en el XX, las inundaciones del mar del Norte de 1953, devastaron varias costas de los países circundantes, con la muerte de más de 2 000 personas.[41]​ Más adelante, en 1962, 315 ciudadanos de Hamburgo murieron de las inundaciones del mar del Norte de 1962.[42]​ La «inundación del siglo", producida en 1976 y la inundación norteña de Frísia de 1981 trajeron los niveles más altos de agua que se han medido hasta la fecha en las costas del mar del Norte, pero debido a las defensas costeras construidas, como por ejemplo sistemas de alerta y mejora de los diques y otras modificaciones posteriores a la inundación de 1962, las inundaciones sólo provocaron daños a la propiedad.[42]​[43]​
Los desprendimientos de Storegga (Storegga Slide) fueron una serie de desprendimientos submarinos de tierra, en que un trozo de la plataforma continental de Noruega deslizó hacia el mar de Noruega. La inmensos desprendimientos que sucedieron entre los años 8150 a. C. y 6000 a. C., provocó un tsunami de hasta 20 metros de altura  que se extendió por el mar del Norte, y que tuvo un impacto más grande en Escocia y las Islas Feroe.[44]​[45]​
El terremoto del estrecho de Dover (o de Calais) de 1580 es uno de los terremotos de los cuales se tiene conocimiento en el mar del Norte, y midió entre 5,3 y 5,9 en la escala de Richter. Este acontecimiento causó grandes daños en Calais, tanto a través de sus temblores y dos tsunamis.[46]​
El terremoto más grande registrado en el Reino Unido fue el terremoto de 1931 en Dogger Bank, que midió 6,1 en la escala de Richter y provocó un tsunami que inundó partes de la costa británica.[46]​[47]​
Mares epicontinentales poco profundos, como el actual mar del Norte han existido desde hace tiempo en la plataforma continental. La dislocación que formaban la parte septentrional del océano Atlántico durante los periodos Jurásico y Cretáceo, hace unos 150 millones, causó un movimiento tectónico que creó las Islas Británicas. Desde entonces, ha existido de forma permanente entre los cerros de Escandinavia y las Islas Británicas. Este precursor del actual mar del Norte ha crecido y se ha reducido con la subida y la caída del nivel del mar durante los diferentes periodos geológicos. A veces, estaba conectado con otros mares, como el Mar Paratetis, hoy desaparecido.
Durante el Cretáceo superior, hace alrededor de 85 millones de años, gran parte de la Europa continental moderna a excepción de Escandinavia era una dispersión de islas. En el oligoceno, hace entre 34 y 28 millones de años, la emersión de la Europa occidental y central había dejado casi completamente separados el mar del Norte y el Mar Tetis, que se redujo gradualmente a convertirse en el Mar Mediterráneo en el sur de Europa, y en tierra seca al sur de Asia Occidental. El mar del Norte estaba separado del Canal de la Mancha por un estrecho puente terrestre hasta que este fue inundado por al menos dos inundaciones catastróficas entre 450 000 y 180 000 años atrás. Desde el comienzo del periodo Cuaternario alrededor de 2,6 millones de años atrás, el nivel del mar eustático se ha reducido durante cada periodo glacial y, a continuación, ha subido de nuevo. Cada vez que la capa de hielo logró su mayor magnitud, el mar del Norte se secó casi completamente. La actual costa del mar del Norte se formó cuando, después del último pico de glaciación, durante la última época glacial 20 000 años atrás, cuando el mar empezó a inundar la plataforma continental europea. La costa de mar del Norte sufre todavía cambios a raíz de variaciones en el nivel del mar en todo el mundo, de movimientos tectónicos, de las mareas, de la erosión, la subida y la caída de los niveles del mar, la deriva de guijarros...
El mar del Norte es muy importante para el tráfico marítimo. Algunos de los puertos más grandes del mundo se sitúan en sus costas o bien en las orillas de los ríos pocos kilómetros río arriba de su desembocadura (es el caso, por ejemplo, de Róterdam —tercer puerto del mundo—, Amberes, Hamburgo y Londres), o bien tienen un fácil acceso, como el de Ámsterdam, cosa que hace que disponga de rutas marítimas muy solicitadas. Es vital para el comercio de Europa Occidental.

