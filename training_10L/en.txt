

Sarah Bernhardt (French: [saʁa bɛʁnɑʁt];[note 1] born Henriette-Rosine Bernard; 22 October 1844 – 26 March 1923) was a French stage actress who starred in some of the more popular French plays of the late 19th and early 20th centuries, including La Dame Aux Camelias by Alexandre Dumas fils; Ruy Blas by Victor Hugo, Fédora and La Tosca by Victorien Sardou, and L'Aiglon by Edmond Rostand. She also played male roles, including Shakespeare's Hamlet. Rostand called her "the queen of the pose and the princess of the gesture", and Hugo praised her "golden voice". She made several theatrical tours around the world, and she was one of the early prominent actresses to make sound recordings and to act in motion pictures.
She is also linked with the success of artist Alphonse Mucha, whose work she helped to publicize. Mucha became one of the more sought-after artists of this period for his Art Nouveau style.
Henriette-Rosine Bernard[1] was born at 5 rue de L'École-de-Médicine in the Latin Quarter of Paris on 22 October 1844.[note 2][2] She was the daughter of Judith Bernard (also known as Julie and in France as Youle), a Dutch Jewish courtesan with a wealthy or upper-class clientele.[3][4][5][6] The name of her father was not recorded for a long time. We know now that he was an attorney in Le Havre.[7] Bernhardt later wrote that her father's family paid for her education, insisted she be baptised as a Catholic, and left a large sum to be paid when she came of age.[8] Her mother travelled frequently, and saw little of her daughter. She placed Bernhardt with a nurse in Brittany, then in a cottage in the Paris suburb of Neuilly-sur-Seine.[9]
When Bernhardt was seven, her mother sent her to a boarding school for young ladies in the Paris suburb of Auteuil, paid with funds from her father's family. There, she acted in her first theatrical performance in the play Clothilde, where she held the role of the Queen of the Fairies, and performed her first of many dramatic death scenes.[9] While she was in the boarding school, her mother rose to the top ranks of Parisian courtesans, consorting with politicians, bankers, generals, and writers. Her patrons and friends included Charles de Morny, Duke of Morny, the half-brother of Emperor Napoleon III and President of the French legislature.[10] At the age of 10, with the sponsorship of Morny, Bernhardt was admitted to Grandchamp, an exclusive Augustine convent school near Versailles.[11] At the convent, she performed the part of the Archangel Raphael in the story of Tobias and the Angel.[12] She declared her intention to become a nun, but did not always follow convent rules; she was accused of sacrilege when she arranged a Christian burial, with a procession and ceremony, for her pet lizard.[13] She received her first communion as a Roman Catholic in 1856, and thereafter she was fervently religious. However, she never forgot her Jewish heritage. When asked years later by a reporter if she were a Christian, she replied: "No, I'm a Roman Catholic, and a member of the great Jewish race. I'm waiting until Christians become better."[14] That contrasted her answer, "No, never. I'm an atheist" to an earlier question by composer and compatriot Charles Gounod if she ever prayed.[15] Regardless, she accepted the last rites shortly before her death.[16]
In 1857, Bernhardt learned that her father had died overseas.[17] Her mother summoned a family council, including Morny, to decide what to do with her. Morny proposed that Bernhardt should become an actress, an idea that horrified Bernhardt, as she had never been inside a theatre. [18] Morny arranged for her to attend her first theatre performance at the Comédie Française in a party which included her mother, Morny, and his friend Alexandre Dumas père. The play they attended was Britannicus, by Jean Racine, followed by the classical comedy Amphitryon by Plautus. Bernhardt was so moved by the emotion of the play, she began to sob loudly, disturbing the rest of the audience.[18] Morny and others in their party were angry at her and left, but Dumas comforted her, and later told Morny that he believed that she was destined for the stage. After the performance, Dumas called her "my little star".[19]
Morny used his influence with the composer Daniel Auber, the head of the Paris Conservatory, to arrange for Bernhardt to audition. She began preparing, as she described it in her memoirs, "with that vivid exaggeration with which I embrace any new enterprise."[20] Dumas coached her. The jury was composed of Auber and five leading actors and actresses from the Comédie Française. She was supposed to recite verses from Racine, but no one had told her that she needed someone to give her cues as she recited. Bernhardt told the jury she would instead recite the fable of the Two Pigeons by La Fontaine. The jurors were skeptical, but the fervor and pathos of her recitation won them over, and she was invited to become a student.[21]
Debut of Bernhardt in Les Femmes Savantes at the Comédie Française, 1862
Sarah Bernhardt in 1864; age 20, by photographer Félix Nadar
Bernhardt photographed by Nadar, 1865
Portrait of Sarah Bernhardt as Tosca by Nadar, 1887
Bernhardt studied acting at the Conservatory from January 1860 until 1862 under two prominent actors of the Comédie Française, Joseph-Isidore Samson and Jean-Baptiste Provost. She wrote in her memoirs that Provost taught her diction and grand gestures, and Samson taught her the power of simplicity.[22] For the stage, she changed her name from "Bernard" to "Bernhardt". While studying, she also received her first marriage proposal, from a wealthy businessman who offered her 500,000 francs. He wept when she refused. Bernhardt wrote that she was "confused, sorry, and delighted—because he loved me the way people love in plays at the theater."[23]
Before the first examination for her tragedy class, she tried to straighten her abundance of frizzy hair, which made it even more uncontrollable, and came down with a bad cold, which made her voice so nasal that she hardly recognised it. Furthermore, the parts assigned for her performance were classical and required carefully stylised emotions, but she preferred romanticism and fully and naturally expressing her emotions. The teachers ranked her 14th in tragedy and second in comedy.[24] Once again, Morny came to her rescue. He put in a good word for her with the National Minister of the Arts, Camille Doucet. Doucet recommended her to Edouard Thierry, the chief administrator of the Théâtre Français,[24] who offered Bernhardt a place as a pensionnaire at the theater, at a minimum salary.[25]
Bernhardt made her debut with the company on 31 August 1862 in the title role of Racine's Iphigénie.[26][note 3] Her premiere was not a success. She experienced stage fright and rushed her lines. Some audience members made fun of her thin figure. When the performance ended, Provost was waiting in the wings, and she asked his forgiveness. He told her, "I can forgive you, and you'll eventually forgive yourself, but Racine in his grave never will."[27] Francisque Sarcey, the influential theater critic of L'Opinion Nationale and Le Temps, wrote: "she carries herself well and pronounces with perfect precision. That is all that can be said about her at the moment."[27]
Bernhardt did not remain long with the Comédie-Française. She played Henrietta in Molière's Les Femmes Savantes and Hippolyte in L'Étourdi, and the title role in Scribe's Valérie, but did not impress the critics, or the other members of the company, who had resented her rapid rise. The weeks passed, but she was given no further roles.[28] Her hot temper also got her into trouble; when a theater doorkeeper addressed her as "Little Bernhardt", she broke her umbrella over his head. She apologised profusely, and when the doorkeeper retired 20 years later, she bought a cottage for him in Normandy.[29] At a ceremony honoring the birthday of Molière on 15 January 1863, Bernhardt invited her younger sister, Regina, to accompany her. Regina accidentally stood on the train of the gown of a leading actress of the company, Zaïre-Nathalie Martel (1816–1885), known as Madame Nathalie.[30] Madame Nathalie pushed Regina off the gown, causing her to strike a stone column and gash her forehead. Regina and Madame Nathalie began shouting at one another, and Bernhardt stepped forward and slapped Madame Nathalie on the cheek. The older actress fell onto another actor. Thierry asked that Bernhardt apologise to Madame Nathalie. Bernhardt refused to do so until Madame Nathalie apologised to Regina. Bernhardt had already been scheduled for a new role with the theater, and had begun rehearsals. Madame Nathalie demanded that Bernhardt be dropped from the role unless she apologised. Because neither yielded, and Madame Nathalie was a senior member of the company, Thierry was forced to ask Bernhardt to leave.[31]
Her family could not understand her departure from the theater; it was inconceivable to them that anyone would walk away from the most prestigious theatre in Paris at age 18.[32] Instead, she went to a popular theatre, the Gymnase, where she became an understudy to two of the leading actresses. She almost immediately caused another offstage scandal, when she was invited to recite poetry at a reception at the Tuileries Palace hosted by Napoleon III and the Empress Eugenie, along with other actors of the Gymnase. She chose to recite two romantic poems by Victor Hugo, unaware that Hugo was a bitter critic of the emperor. Following the first poem, the emperor and empress rose and walked out, followed by the court and the other guests.[33] Her next role at the Gymnase, as a foolish Russian princess, was entirely unsuited for her; her mother told her that her performance was "ridiculous".[32] She decided abruptly to quit the theater to travel, and like her mother, to take on lovers. She went briefly to Spain, then, at the suggestion of Alexandre Dumas, to Belgium.[34]
She carried to Brussels letters of introduction from Dumas, and was admitted to the highest levels of society. According to some later accounts, she attended a masked ball in Brussels where she met the Belgian aristocrat Henri, Hereditary Prince de Ligne, and had an affair with him.[35] Other accounts say that they met in Paris, where the Prince came often to attend the theater.[36] The affair was cut short when she learned that her mother had had a heart attack. She returned to Paris, where she found that her mother was better, but that she was pregnant from her affair with the Prince. She did not notify the Prince. Her mother did not want the fatherless child born under her roof, so she moved to a small apartment on rue Duphot, and on 22 December 1864, the 20-year-old actress gave birth to her only child, Maurice Bernhardt.[37]
Some accounts say that Prince Henri had not forgotten her. According to these versions, he learned her address from the theatre, arrived in Paris, and moved into the apartment with Bernhardt. After a month, he returned to Brussels and told his family that he wanted to marry the actress. The family of the Prince sent his uncle, General de Ligne, to break up the romance, threatening to disinherit him if he married Bernhardt.[38] According to other accounts, the Prince denied any responsibility for the child.[36] She later called the affair "her abiding wound", but she never discussed Maurice's parentage with anyone. When asked who his father was, she sometimes answered, "I could never make up my mind whether his father was Gambetta, Victor Hugo, or General Boulanger."[39] Many years later, in January 1885, when Bernhardt was famous, the Prince allegedly came to Paris and offered to formally recognise Maurice as his son, but Maurice politely declined, explaining he was entirely satisfied to be the son of Sarah Bernhardt.[40] (While the story is in character for Maurice, note that the Prince died in 1871 when Maurice was 6.)
To support herself after the birth of Maurice, Bernhardt played minor roles and understudies at the Porte Saint-Martin theatre, a popular melodrama theatre. In early 1866, she obtained a reading with Felix Duquesnel, director of the Théâtre de L'Odéon (Odéon) on the Left Bank. Duquesnel described the reading years later, saying, "I had before me a creature who was marvelous gifted, intelligent to the point of genius, with enormous energy under an appearance frail and delicate, and a savage will." The co-director of the theatre for finance, Charles de Chilly, wanted to reject her as unreliable and too thin, but Duquesnel was enchanted; he hired her for the theater at a modest salary of 150 francs a month, which he paid out of his own pocket.[41] The Odéon was second in prestige only to the Comédie Française, and unlike that very traditional theatre, specialised in more modern productions. The Odéon was popular with the students of the Left Bank. Her first performances with the theatre were not successful. She was cast in highly stylised and frivolous 18th-century comedies, whereas her strong point on stage was her complete sincerity.[42] Her thin figure also made her look ridiculous in the ornate costumes. Dumas, her strongest supporter, commented after one performance, "she has the head of a virgin and the body of a broomstick."[43] Soon, however, with different plays and more experience, her performances improved; she was praised for her performance of Cordelia in King Lear.[citation needed] In June 1867, she played two roles in Athalie by Jean Racine; the part of a young woman and a young boy, Zacharie, the first of many male parts she played in her career. The influential critic Sarcey wrote "she charmed her audience like a little Orpheus."[43]
Her breakthrough performance was in the 1868 revival of Kean by Alexandre Dumas, in which she played the female lead part of Anna Danby. The play was interrupted in the beginning by disturbances in the audience by young spectators who called out, "Down with Dumas! Give us Hugo!". Bernhardt addressed the audience directly: "Friends, you wish to defend the cause of justice. Are you doing it by making Monsieur Dumas responsible for the banishment of Monsieur Hugo?".[44] With this the audience laughed and applauded and fell silent. At the final curtain, she received an enormous ovation, and Dumas hurried backstage to congratulate her. When she exited the theatre, a crowd had gathered at the stage door and tossed flowers at her. Her salary was immediately raised to 250 francs a month.[45]
Her next success was her performance in François Coppée's Le Passant, which premiered at the Odéon on 14 January 1868,[46] playing the part of the boy troubadour, Zanetto, in a romantic renaissance tale.[47] Critic Théophile Gautier described the "delicate and tender charm" of her performance. It played for 150 performances, plus a command performance at the Tuileries Palace for Napoleon III and his court. Afterwards, the emperor sent her a brooch with his initials written in diamonds.[48]
In her memoirs, she wrote of her time at the Odéon: "It was the theatre that I loved the most, and that I only left with regret. We all loved each other. Everyone was gay. The theatre was a like a continuation of school. All the young came there...I remember my few months at the Comédie Française. That little world was stiff, gossipy, jealous. I remember my few months at the Gymnase. There they talked only about dresses and hats, and chattered about a hundred things that had nothing to do with art. At the Odéon, I was happy. We thought only of putting on plays. We rehearsed mornings, afternoons, all the time. I adored that." Bernhardt lived with her longtime friend and assistant Madame Guérard and her son in a small cottage in the suburb of Auteuil, and drove herself to the theatre in a small carriage. She developed a close friendship with the writer George Sand, and performed in two plays that she authored.[49] She received celebrities in her dressing room, including Gustave Flaubert and Leon Gambetta. In 1869, as she became more prosperous, she moved to a larger seven-room apartment at 16 rue Auber in the center of Paris. Her mother began to visit her for the first time in years, and her grandmother, a strict Orthodox Jew, moved into the apartment to take care of Maurice. Bernhardt added a maid and a cook to her household, as well as the beginning of a collection of animals; she had one or two dogs with her at all times, and two turtles moved freely around the apartment.[50]
In 1868, a fire completely destroyed her apartment, along with all of her belongings. She had neglected to purchase insurance. The brooch presented to her by the emperor and her pearls melted, as did the tiara presented by one of her lovers, Khalid Bey. She found the diamonds in the ashes, and the managers of the Odéon organised a benefit performance. The most famous soprano of the time, Adelina Patti, performed for free. In addition, the grandmother of her father donated 120,000 francs. Bernhardt was able to buy an even larger residence, with two salons and a large dining room, at 4 rue de Rome.[51]
The outbreak of the Franco-Prussian War abruptly interrupted her theatrical career. The news of the defeat of the French Army, the surrender of Napoleon III at Sedan, and the proclamation of the Third French Republic on 4 September 1870 was followed by a siege of the city by the Prussian Army. Paris was cut off from news and from its food supply, and the theatres were closed. Bernhardt took charge of converting the Odéon into a hospital for soldiers wounded in the battles outside the city.[52] She organised the placement of 32 beds in the lobby and the foyers, brought in her personal chef to prepare soup for the patients, and persuaded her wealthy friends and admirers to donate supplies for the hospital. Besides organising the hospital, she worked as a nurse, assisting the chief surgeon with amputations and operations.[53] When the coal supply of the city ran out, Bernhardt used old scenery, benches, and stage props for fuel to heat the theater.[54] In early January 1871, after 16 weeks of the siege, the Germans began to bombard the city with long-range cannons. The patients had to be moved to the cellar, and before long, the hospital was forced to close. Bernhardt arranged for serious cases to be transferred to another military hospital, and she rented an apartment on rue de Provence to house the remaining 20 patients. By the end of the siege, Bernhardt's hospital had cared for more than 150 wounded soldiers, including a young undergraduate from the École Polytechnique, Ferdinand Foch, who later commanded the Allied armies in the First World War.[55]
The French government signed an armistice on 19 January 1871, and Bernhardt learned that her son and family had been moved to Hamburg. She went to the new chief executive of the French Republic, Adolphe Thiers, and obtained a pass to go to Germany to return them. When she returned to Paris several weeks later, the city was under the rule of the Paris Commune. She moved again, taking her family to Saint-Germain-en-Laye. She later returned to her apartment on the rue de Rome in May, after the Commune was defeated by the French Army.
Bernhardt as the Queen of Spain in Ruy Blas (1872)
Phèdre by Racine at the Comédie française, (1873)
Bernhardt in her famous coffin, in which she sometimes slept or studied her roles (c. 1873)
Portrait by Georges Clairin (1876)
Bernhardt as Doña Sol in Hernani (1878)
The Tuileries Palace, city hall of Paris, and many other public buildings had been burned by the Commune or damaged in the fighting, but the Odéon was still intact. Charles-Marie Chilly, the co-director of the Odéon, came to her apartment, where Bernhardt received him reclining on a sofa. He announced that the theaters would reopen in October 1871, and he asked her to play the lead in a new play, Jean-Marie by André Theuriet. Bernhardt replied that she was finished with the theatre and was going to move to Brittany and start a farm. Chilly, who knew Bernhardt's moods well, told her that he understood and accepted her decision, and would give the role to Jane Essler, a rival actress. According to Chilly, Bernhardt immediately jumped up from the sofa and asked when the rehearsals would begin.[55]
Jean-Marie, about a young Breton woman forced by her father to marry an old man she did not love, was another critical and popular success for Bernhardt. The critic Sarcey wrote "She has the sovereign grace, the penetrating charm, the I don't know what. She is a natural artist, an incomparable artist."[56] The directors of the Odéon next decided to stage Ruy Blas, a play written by Victor Hugo in 1838, with Bernhardt playing the role of the Queen of Spain. Hugo attended all the rehearsals. At first, Bernhardt pretended to be indifferent to him, but he gradually won her over and she became a fervent admirer. The play premiered on 16 January 1872. The opening night was attended by the Prince of Wales and by Hugo; after the performance, Hugo approached Bernhardt, dropped to one knee, and kissed her hand.[57]
Ruy Blas played to packed houses. A few months after it opened, Bernhardt received an invitation from Emile Perrin, Director of the Comédie Française, asking if she would return, and offering her 12,000 francs a year, compared with less than 10,000 at the Odéon.[58] Bernhardt asked Chilly if he would match the offer, but he refused. Always pressed by her growing expenses and growing household to earn more money, she announced her departure from the Odéon when she finished the run of Ruy Blas. Chilly responded with a lawsuit, and she was forced to pay 6,000 francs of damages. After the 100th performance of Ruy Blas, Hugo gave a dinner for Bernhardt and her friends, toasting "His adorable Queen and her Golden Voice."[57]
She formally returned to the Comédie Francaise on 1 October 1872, and quickly took on some of the more famous and demanding roles in French theatre. She played Junie in Britannicus by Jean Racine, the male role of Cherubin in The Marriage of Figaro by Pierre Beaumarchais, and the lead in Voltaire's five-act tragedy Zaïre.[59] In 1873, with just 74 hours to learn the lines and practice the part, she played the lead in Racine's Phèdre, playing opposite the celebrated tragedian, Jean Mounet-Sully, who soon became her lover. The leading French critic Sarcey wrote "This is nature itself served by marvelous intelligence, by a soul of fire, by the most melodious voice that ever enchanted human ears. This woman plays with her heart, with her entrails."[60] Phèdre became her most famous classical role, performed over the years around the world, often for audiences who knew little or no French; she made them understand by her voice and gestures.[61]
In 1877, she had another success as Doña Sol in Hernani, a tragedy written 47 years earlier by Victor Hugo. Her lover in the play was her lover off-stage, as well, Mounet-Sully. Hugo was in the audience. The next day, he sent her a note: "Madame, you were great and charming; you moved me, me the old warrior, and, at a certain moment when the public, touched and enchanted by you, applauded, I wept. The tear which you caused me to shed is yours. I place it at your feet." The note was accompanied by a tear-shaped pearl on a gold bracelet.[62]
She maintained a highly theatrical lifestyle in her house on the rue de Rome. She kept a satin-lined coffin in her bedroom, and occasionally slept in it or lay in it to study her roles, though, contrary to the popular stories, she never took it with her on her travels. She cared for her younger sister who was ill with tuberculosis, and allowed her to sleep in her own bed while she slept in the coffin. She posed in it for photographs, adding to the legends she created about herself.[63]
Bernhardt repaired her old relationships with the other members of the Comédie Française; she participated in a benefit for Madame Nathalie, the actress she had once slapped. However, she was frequently in conflict with Perrin, the director of the theatre. In 1878, during the Paris Universal Exposition, she took a flight over Paris with balloonist Pierre Giffard[citation needed] and painter Georges Clairin, in a balloon decorated with the name of her current character, Doña Sol. An unexpected storm carried the balloon far outside of Paris to a small town. When she returned by train to the city, Perrin was furious; he fined Bernhardt a thousand francs, citing a theatre rule which required actors to request permission before they left Paris. Bernhardt refused to pay, and threatened to resign from the Comédie. Perrin recognised that he could not afford to let her go. Perrin and the Minister of Fine Arts arranged a compromise; she withdrew her resignation, and in return was raised to a societaire, the highest rank of the theater.[64]
Bernhardt was earning a substantial amount at the theatre, but her expenses were even greater. By this time she had eight servants, and she built her first house, an imposing mansion on rue Fortuny, not far from the Parc Monceau. She looked for additional ways to earn money. In June 1879, while the theatre of the Comédie Française in Paris was being remodeled, Perrin took the company on tour to London. Shortly before the tour began, a British theatre impresario named Edward Jarrett traveled to Paris and proposed that she give private performances in the homes of wealthy Londoners; the fee she would receive for each performance was greater than her monthly salary with the Comédie.[65] When Perrin read in the press about the private performances, he was furious. Furthermore, the Gaiety Theatre in London demanded that Bernhardt star in the opening performance, contrary to the traditions of Comédie Française, where roles were assigned by seniority, and the idea of stardom was scorned. When Perrin protested, saying that Bernhardt was only 10th or 11th in seniority, the Gaiety manager threatened to cancel the performance; Perrin had to give in. He scheduled Bernhardt to perform one act of Phèdre on the opening night, between two traditional French comedies, Le Misanthrope and Les Précieuses.[66]
On 4 June 1879, just before the opening curtain of her premiere in Phèdre, she suffered an attack of stage fright. She wrote later that she also pitched her voice too high, and was unable to lower it.[67] Nonetheless, the performance was a triumph. Though a majority of the audience could not understand Racine's classical French, she captivated them with her voice and gestures; one member of the audience, Sir George Arthur, wrote that "she set every nerve and fibre in their bodies throbbing and held them spellbound."[68] In addition to her performances of Zaïre, Phèdre, Hernani, and other plays with her troupe, she gave the private recitals in the homes of British aristocrats arranged by Jarrett, who also arranged an exhibition of her sculptures and paintings in Piccadilly, which was attended by both the Prince of Wales and Prime Minister Gladstone. While in London, she added to her personal menagerie of animals. In London, she purchased three dogs, a parrot, and a monkey, and made a side trip to Liverpool, where she purchased a cheetah, a parrot, and a wolfhound and received a gift of six chameleons, which she kept in her rented house on Chester Square, and then took back to Paris.[69]
Back in Paris, she was increasingly discontented with Perrin and the management of the Comédie Française. He insisted that she perform the lead in the play L'Aventurière by Emile Augier, a play which she thought was mediocre. When she rehearsed the play without enthusiasm, and frequently forgot her lines, she was criticised by the playwright. She responded "I know I'm bad, but not as bad as your lines." The play went ahead, but was a failure. She wrote immediately to Perrin "You forced me to play when I was not ready...what I foresaw came to pass...this is my first failure at the Comédie and my last." She sent a resignation letter to Perrin, made copies, and sent them to the major newspapers. Perrin sued her for breach of contract; the court ordered her to pay 100,000 francs plus interest, and she lost her accrued pension of 43,000 francs.[70] She did not settle the debt until 1900. Later, however, when the Comédie Française theatre was nearly destroyed by fire, she allowed her old troupe to use her own theatre.[71]
In April 1880, as soon as he learned Bernhardt had resigned from the Comédie Française, the impresario Edward Jarrett hurried to Paris and proposed that she make a theatrical tour of England and then the United States. She could select her repertoire and the cast. She would receive 5,000 francs per performance, 15% of any earnings over 15,000 francs, plus all of her expenses, and an account in her name for 100,000 francs, the amount she owed to the Comédie Française. She accepted immediately.[72]
Now on her own, Bernhardt first assembled and tried her new troupe at the Théâtre de la Gaîté-Lyrique in Paris. She performed for the first time La Dame aux Camélias, by Alexandre Dumas fils. She did not create the role; the play had first been performed by Eugénie Dochein in 1852, but it quickly became her most performed and most famous role. She played the role more than a thousand times, and acted regularly and successfully in it until the end of her life. Audiences were often in tears during her famous death scene at the end.[73]
She could not perform La Dame aux Camélias on a London stage because of British censorship laws; instead, she put on four of her proven successes, including Hernani and Phèdre, plus four new roles, including Adrienne Lecouvreur by Eugène Scribe and the drawing-room comedy Frou-frou by Meilhac-Halévy, both of which were highly successful on the London stage.[74] In six of the eight plays in her repertoire, she died dramatically in the final act. When she returned to Paris from London, the Comédie Française asked her to come back, but she refused their offer, explaining that she was making far more money on her own. Instead, she took her new company and new plays on tour to Brussels and Copenhagen, and then on a tour of French provincial cities.[75]
She and her troupe departed from Le Havre for America on 15 October 1880, arriving in New York on 27 October. On 8 November in New York City, she performed Scribe's Adrienne Lecouvreur at Booth's Theatre before an audience which had paid a top price of $40 for a ticket, an enormous sum at the time. Few in the audience understood French, but it was not necessary; her gestures and voice captivated the audience, and she received a thunderous ovation. She thanked the audience with her distinctive curtain call; she did not bow, but stood perfectly still, with her hands clasped under her chin, or with her palms on her cheeks, and then suddenly stretched them out to the audience. After her first performance in New York, she made 27 curtain calls. Although she was welcomed by theatre-goers, she was entirely ignored by New York high society, who considered her personal life scandalous.[76]
Bernhardt's first American tour carried her to 157 performances in 51 cities.[77] She travelled on a special train with her own luxurious palace car, which carried her two maids, two cooks, a waiter, her maître d'hôtel, and her personal assistant, Madame Guérard. It also carried an actor named Édouard Angelo, whom she had selected to serve as her leading man, and, according to most accounts, her lover during the tour.[78][79] From New York, she made a side trip to Menlo Park, where she met Thomas Edison, who made a brief recording of her reciting a verse from Phèdre, which has not survived.[80] She crisscrossed the United States and Canada from Montreal and Toronto to Saint Louis and New Orleans, usually performing each evening, and departing immediately after the performance. She gave countless press interviews, and in Boston posed for photos on the back of a dead whale. She was condemned as immoral by the Bishop of Montreal and by the Methodist press, which only increased ticket sales.[80] She performed Phèdre six times and La Dame Aux Camélias 65 times (which Jarrett had renamed "Camille" to make it easier for Americans to pronounce, despite the fact that no character in the play has that name). On 3 May 1881, she gave her final performance of Camélias in New York. Throughout her life, she always insisted on being paid in cash. When Bernhardt returned to France, she brought with her a chest filled with $194,000 in gold coins.[81] She described the result of her trip to her friends: "I crossed the oceans, carrying my dream of art in myself, and the genius of my nation triumphed. I planted the French verb in the heart of a foreign literature, and it is that of which I am most proud."[82]
No crowd greeted Bernhardt when she returned to Paris on 5 May 1881, and theatre managers offered no new roles; the Paris press ignored her tour, and much of the Paris theatre world resented her leaving the most prestigious national theatre to earn a fortune abroad.[83] When no new plays or offers appeared, she went to London for a successful three-week run at the Gaiety Theater. This London tour included the first British performance of La Dame aux Camelias at the Shaftesbury Theatre; her friend, the prince of Wales, persuaded Queen Victoria to authorise the performance.[84] Many years later, she gave a private performance of the play for the queen while she was on holiday in Nice.[85] When she returned to Paris, Bernhardt contrived to make a surprise performance at the annual 14 July patriotic spectacle at the Paris Opera, which was attended by the President of France, and a houseful of dignitaries and celebrities. She recited the Marseillaise, dressed in a white robe with a tricolor banner, and at the end dramatically waved the French flag. The audience gave her a standing ovation, showered her with flowers, and demanded that she recite the song two more times.[86]
With her place in the French theatre world restored, Bernhardt negotiated a contract to perform at the Vaudeville Theatre in Paris for 1500 francs per performance as well as 25 percent of the net profit. She also announced that she would not be available to begin until 1882. She departed on a tour of theatres in the French provinces and then to Italy, Greece, Hungary, Switzerland, Belgium, Holland, Spain, Austria, and Russia. In Kyiv and Odessa, she encountered anti-Semitic crowds who threw stones at her; pogroms were being conducted, forcing the Jewish population to leave.[87] However, in Moscow and St. Petersburg, she performed before Czar Alexander III, who broke court protocol and bowed to her. During her tour, she gave performances for King Alfonso XII of Spain, and the Emperor Franz Joseph I of Austria. The only European country where she refused to play was Germany, due to the German annexation of French territory after the 1870–71 Franco-Prussian War.[88] Just before the tour began, she met Jacques Damala, who went with her as leading man and then, for eight months, became her first and only husband. (see Personal life)
When she returned to Paris, she was offered a new role in Fédora, a melodrama written for her by Victorien Sardou. It opened on 12 December 1882, with her husband Damala as the male lead, and received good reviews. Critic Maurice Baring wrote "a secret atmosphere emanated from her, an aroma, an attraction, which was at once exotic and cerebral...She literally hypnotised her audience."[89] Another journalist wrote "She is incomparable...The extreme love, the extreme agony, the extreme suffering."[90] However, the abrupt end of her marriage shortly after the premiere put her back into financial distress. She had leased and refurbished a theatre, the Ambigu, specifically to give her husband leading roles, and made her 18-year-old son Maurice, who had no business experience, the manager. Fédora ran for just 50 performances and lost 400,000 francs. She was forced to give up the Ambigu, and then, in February 1883, to sell her jewellery, her carriages, and her horses at an auction.[91]
When Damala left, she took on a new leading man and lover, the poet and playwright Jean Richepin, who accompanied her on a quick tour of European cities to help pay off her debts.[92] She renewed her relationship with the Prince of Wales, the future King Edward VII.[93] When they returned to Paris, Bernhardt leased the theatre of Porte Saint-Martin and starred in Nana-Sahib, a new play by Richepin, a costume drama about love in British India in 1857. The play and Richepin's acting were poor, and it quickly closed.[94] Richepin then wrote an adaptation of Macbeth in French, with Bernhardt as Lady Macbeth, but it was also a failure. The only person who praised the play was Oscar Wilde, who was then living in Paris. He began writing a play, Salomé, in French, especially for Bernhardt, though it was quickly banned by British censors and she never performed it.[95]
Bernhardt then performed a new play by Sardou, Theodora (1884), a melodrama set in sixth-century Byzantine Empire. Sardou wrote a nonhistoric but dramatic new death scene for Bernhardt; in his version, the empress Theodora was publicly strangled, whereas the historical empress died of cancer. Bernhardt travelled to Ravenna, Italy, to study and sketch the costumes seen in Byzantine mosaic murals, and had them reproduced for her own costumes. The play opened on 26 December 1884 and ran for 300 performances in Paris and 100 in London, and it was a financial success. She was able to pay off most of her debts, and bought a lion cub, which she named Justinian, for her home menagerie.[96] She also renewed her love affair with her former lead actor, Philippe Garnier.[97]
Theodora was followed by two failures. In 1885, in homage to Victor Hugo, who had died a few months earlier, she staged one of his older plays, Marion de Lorme, written in 1831, but the play was outdated and her role did not give her a chance to show her talents.[98] She next put on Hamlet, with her lover Philippe Garnier in the leading role and Bernhardt in the relatively minor role of Ophelia. The critics and audiences were not impressed, and the play was unsuccessful.[98] Bernhardt had built up large expenses, which included a 10,000 francs a month allowance paid to her son Maurice, a passionate gambler. Bernhardt was forced to sell her chalet in Sainte-Adresse and her mansion on rue Fortuny, and part of her collection of animals. Her impresario, Edouard Jarrett, immediately proposed she make another world tour, this time to Brazil, Argentina, Uruguay, Chile, Peru, Panama, Cuba, and Mexico, then on to Texas, New York, England, Ireland, and Scotland. She was on tour for 15 months, from early 1886 until late 1887. On the eve of departure, she told a French reporter: "I passionately love this life of adventures. I detest knowing in advance what they are going to serve at my dinner, and I detest a hundred thousand times more knowing what will happen to me, for better or worse. I adore the unexpected."[96]
In every city she visited, she was feted and cheered by audiences. The actors Edouard Angelo and Philippe Garnier were her leading men. Emperor Pedro II of Brazil attended all of her performances in Rio de Janeiro and presented her with a gold bracelet with diamonds, which was almost immediately stolen from her hotel. The two leading actors both fell ill with yellow fever, and her long-time manager, Edward Jarrett, died of a heart attack. Bernhardt was undaunted, however, and went crocodile hunting at Guayaquil, and bought more animals for her menagerie. Her performances in every city were sold out, and by the end of the tour, she had earned more than 1 million francs. The tour allowed her to purchase her final home, which she filled with her paintings, plants, souvenirs, and animals.[99]
From then on, whenever she ran short of money (which generally happened every three or four years), she went on tour, performing both her classics and new plays. In 1888, she toured Italy, Egypt, Turkey, Sweden, Norway, and Russia. She returned to Paris in early 1889 with an enormous owl given to her by the Grand Duke Alexei Alexandrovich, the brother of the Czar.[100] Her 1891–92 tour was her most extensive, including much of Europe, Russia, North and South America, Australia, New Zealand, Hawaii, and Samoa. Her personal luggage consisted of 45 costume crates for her 15 different productions, and 75 crates for her off-stage clothing, including her 250 pairs of shoes. She carried a trunk for her perfumes, cosmetics and makeup, and another for her sheets and tablecloths and her five pillows. After the tour, she brought back a trunk filled with 3,500,000 francs, but she also suffered a painful injury to her knee when she leaped off the parapet of the Castello Sant' Angelo in La Tosca. The mattress on which she was supposed to land was misplaced, and she landed on the boards.[101]
Bernhardt in La Tosca by Victorien Sardou (1887), photo by Nadar
Playing Joan of Arc in Jeanne d'Arc by Jules Barbier (1890)
Bernhardt in Cleopatra (1891)
Bernhardt in Cleopatra by Henri de Toulouse-Lautrec (1896)
When Bernhardt returned from her 1886–87 tour, she received a new invitation to return to the Comédie Française. The theatre management was willing to forget the conflict of her two previous periods there, and offered a payment of 150,000 francs a year. The money appealed to her, and she began negotiations. However, the senior members of the company protested the high salary offered, and conservative defenders of the more traditional theatre also complained; one anti-Bernhardt critic, Albert Delpit of Le Gaulois, wrote "Madame Sarah Bernhardt is forty-three; she can no longer be useful to the Comédie. Moreover, what roles could she have? I can only imagine that she could play mothers..." Bernhardt was deeply offended and immediately broke off negotiations.[102] She turned once again to Sardou, who had written a new play for her, La Tosca, which featured a prolonged and extremely dramatic death scene at the end. The play was staged at the Porte Saint-Martin Theatre, opening on 24 November 1887. It was extremely popular, and critically acclaimed. Bernhardt played the role for 29 consecutive sold-out performances. The success of the play allowed Bernhardt to buy a new pet lion for her household menagerie. She named him Scarpia, after the villain of La Tosca.[102] The play inspired Giacomo Puccini to write one of his more famous operas, Tosca (1900).[103]
Following this success, she acted in several revivals and classics, and many French writers offered her new plays. In 1887, she acted in a stage version of the controversial drama Thérèse Raquin by Emile Zola. Zola had previously been attacked due to the book's confronting content. Asked why she chose this play, she declared to reporters, "My true country is the free air, and my vocation is art without constraints."[100] The play was unsuccessful; it ran for just 38 performances.[104] She then performed another traditional melodrama, Francillon by Alexandre Dumas fils in 1888. A short drama she wrote herself, L'Aveu, disappointed both critics and the audience and lasted only 12 performances. She had considerably more success with Jeanne d'Arc by the poet Jules Barbier, in which the 45-year-old actress played Joan of Arc, a 19-year-old martyr.[105] Barbier had previously written the librettos for some of the more famous French operas of the period, including Faust by Charles Gounod and The Tales of Hoffmann by Jacques Offenbach. Her next success was another melodrama by Sardou and Moreau, Cleopatra, which allowed her to wear elaborate costumes and finished with a memorable death scene. For this scene, she kept two live garter snakes, which played the role of the poisonous asp which bites Cleopatra. For realism, she painted the palms of her hands red, though they could hardly be seen from the audience. "I shall see them," she explained. "If I catch sight of my hand, it will be the hand of Cleopatra."[106]
Bernhardt's violent portrayal of Cleopatra led to the theatrical story of a matron in the audience exclaiming to her companion "How unlike, how very unlike, the home life of our own dear Queen!"[107]
Bernhardt in Gismonda by Victorien Sardou (1894)
Poster for Gismonda by Alphonse Mucha (1894)
As Melissande in La Princesse Lointaine by Edmond Rostand (1897)
Bernhardt in Cleopatra by Sardou (1899)
Bernhardt made a two-year world tour (1891–1893) to replenish her finances. Upon returning to Paris, she paid 700,000 francs for the Théâtre de la Renaissance, and from 1893 until 1899, was its artistic director and lead actress. She managed every aspect of the theatre, from the finances to the lighting, sets, and costumes, as well as appearing in eight performances a week.[108] She imposed a rule that women in the audience, no matter how wealthy or famous, had to take off their hats during performances, so the rest of the audience could see, and eliminated the prompter's box from the stage, declaring that actors should know their lines. She abolished in her theatre the common practice of hiring claqueurs in the audience to applaud stars.[109] She used the new technology of lithography to produce vivid color posters, and in 1894, she hired Czech artist Alphonse Mucha to design the first of a series of posters for her play Gismonda. He continued to make posters of her for six years.[110]
In five years, Bernhardt produced nine plays, three of which were financially successful. The first was a revival of her performance as Phédre, which she took on tour around the world. In 1898, she had another success, in the play Lorenzaccio, playing the male lead role in a Renaissance revenge drama written in 1834 by Alfred de Musset, but never before actually staged. As her biographer Cornelia Otis Skinner wrote, she did not try to be overly masculine when she performed male roles: "Her male impersonations had the sexless grace of the voices of choirboys, or the not quite real pathos of Pierrot."[111] Anatole France wrote of her performance in Lorenzaccio: "She formed out of her own self a young man melancholic, full of poetry and of truth."[112] This was followed by another successful melodrama by Sardou, Gismonda, one of Bernhardt's few plays not finishing with a dramatic death scene. Her co-star was Lucien Guitry, who also acted as her leading man until the end of her career. Besides Guitry, she shared the stage with Édouard de Max, her leading man in 20 productions, and Constant Coquelin, who frequently toured with her.[113]
In April 1895, she played the lead role in a romantic and poetic fantasy, Princess Lointaine, by little-known 27-year-old poet Edmond Rostand. It was not a monetary success and lost 200,000 francs, but it began a long theatrical relationship between Bernhardt and Rostand. Rostand went on to write Cyrano de Bergerac and became one of the most popular French playwrights of the period.[114]
In 1898, she performed the female lead in the controversial play La Ville Morte by the Italian poet and playwright Gabriele D'Annunzio; the play was fiercely attacked by critics because of its theme of incest between brother and sister. Along with Émile Zola and Victorien Sardou, Bernhardt became an outspoken defender of Alfred Dreyfus, a Jewish army officer falsely accused of betraying France. The issue divided Parisian society; a conservative newspaper ran the headline, "Sarah Bernhardt has joined the Jews against the Army", and Bernhardt's own son Maurice condemned Dreyfus; he refused to speak to her for a year.[115]
At the Théâtre de la Renaissance, Bernhardt staged and performed in several modern plays, but she was not a follower of the more natural school of acting that was coming into fashion at the end of the 19th century, preferring a more dramatic expression of emotions. "In the theatre," she declared, "the natural is good, but the sublime is even better."[116]
The Théâtre Sarah Bernhardt (now the Théâtre de la Ville)(c. 1905)
Bernhardt in Hamlet (1899)
Poster by Mucha for Hamlet (1899)
Bernhardt in L'Aiglon (1900)
Despite her successes, her debts continued to mount, reaching two million gold francs by the end of 1898. Bernhardt was forced to give up the Renaissance, and was preparing to go on another world tour when she learned that a much larger Paris theater, the Théâtre des Nations on Place du Châtelet, was for lease. The theatre had 1,700 seats, twice the size of the Renaissance, enabling her to pay off the cost of performances more quickly; it had an enormous stage and backstage, allowing her to present several different plays a week; and because it was designed as a concert hall, it had excellent acoustics. On 1 January 1899, she signed a 25-year lease with the City of Paris, though she was already 55 years old.[117]
She renamed it the Théâtre Sarah Bernhardt, and began to renovate it to suit her needs. The façade was lit by 5,700 electric bulbs, 17 arc lights, and 11 projectors.[118] She completely redecorated the interior, replacing the red plush and gilt with yellow velvet, brocade, and white woodwork. The lobby was decorated with life-sized portraits of her in her more famous roles, painted by Mucha, Louise Abbéma, and Georges Clairin. Her dressing room was a five-room suite, which, after the success of her Napoleonic play L'Aiglon, was decorated in Empire Style, featuring a marble fireplace with a fire Bernhardt kept burning year round, a huge bathtub that was filled with the flowers she received after each performance, and a dining room fitting 12 people, where she entertained guests after the final curtain.[119]
Bernhardt opened the theatre on 21 January 1899 with a revival of Sardou's La Tosca, which she had first performed in 1887. This was followed by revivals of her other major successes, including Phédre, Theodora, Gismonda, and La Dame aux Camélias, plus Octave Feuillet's Dalila, Gaston de Wailly's Patron Bénic, and Rostand's La Samaritaine, a poetic retelling of the story of the Samaritan woman at the well from the Gospel of John. On 20 May, she premiered one of her more famous roles, playing the titular character of Hamlet in a prose adaptation which she had commissioned from Eugène Morand and Marcel Schwob.[120] She played Hamlet in a manner which was direct, natural, and very feminine.[121] Her performance received largely positive reviews in Paris, but mixed reviews in London. The British critic Max Beerbohm wrote "the only compliment one can conscientiously pay her is that her Hamlet was, from first to last, a truly grand dame."[122]
In 1900, Bernhardt presented L'Aiglon, a new play by Rostand. She played the Duc de Reichstadt, the son of Napoleon Bonaparte, imprisoned by his unloving mother and family until his melancholy death in the Schönbrunn Palace in Vienna. L'Aiglon was a verse drama, six acts long. The 56-year-old actress studied the walk and posture of young cavalry officers and had her hair cut short to impersonate the young Duke. The Duke's stage mother, Marie-Louise of Austria, was played by Maria Legault, an actress 14 years younger than Bernhardt. The play ended with a memorable death scene; according to one critic, she died "as dying angels would die if they were allowed to."[123] The play was extremely successful; it was especially popular with visitors to the 1900 Paris International Exposition, and ran for nearly a year, with standing-room places selling for as much as 600 gold francs. The play inspired the creation of Bernhardt souvenirs, including statuettes, medallions, fans, perfumes, postcards of her in the role, uniforms and cardboard swords for children, and pastries and cakes; the famed chef Escoffier added Peach Aiglon with Chantilly cream to his repertoire of desserts.[124]
Bernhardt continued to employ Mucha to design her posters and expanded his work to include theatrical sets, programs, costumes, and jewellery props. His posters became icons of the Art Nouveau style. To earn more money, Bernhardt set aside a certain number of printed posters of each play to sell to collectors.[110][125]
Bernhardt as Zoraya in La Sorcière by Sardou (1903)
Playing Pelléas in Pelléas and Mélisande (1905)
Bernhardt in the role of Phedre at the Hearst Greek Theatre at the University of California, Berkeley (1906)
Portrait of Sarah Bernhardt in 1910 by Henry Walter Barnett
After her season in Paris, Bernhardt performed L'Aiglon in London and then made her sixth tour to the United States. On this tour, she travelled with Constant Coquelin, then the most popular leading man in France. Bernhardt played the secondary role of Roxanne to his Cyrano de Bergerac, a role which he had premiered, and he co-starred with her as Flambeau in L'Aiglon and as the first grave-digger in Hamlet.[126]
She also changed, for the first time, her resolution not to perform in Germany or the "occupied territories" of Alsace and Lorraine. In 1902, at the invitation of the French ministry of culture, she took part in the first cultural exchange between Germany and France since the 1870 war. She performed L'Aiglon 14 times in Germany; Kaiser William II of Germany attended two performances and hosted a dinner in her honour in Potsdam.[127]
During her German tour, she began to suffer agonising pain in her right knee, probably connected with a fall she had suffered on stage during her tour in South America. She was forced to reduce her movements in L'Aiglon. A German doctor recommended that she halt the tour immediately and have surgery, followed by six months of complete immobilisation of her leg. Bernhardt promised to see a doctor when she returned to Paris, but continued the tour.[128]
In 1903, she had another unsuccessful role playing another masculine character in the opera Werther, a gloomy adaptation of the story by German writer Johann Wolfgang von Goethe. However, she quickly came back with another hit, La Sorcière by Sardou. She played a Moorish sorceress in love with a Christian Spaniard, leading to her persecution by the church. This story of tolerance, coming soon after the Dreyfus affair, was financially successful, with Bernhardt often giving both a matinee and evening performance.[128]
From 1904 to 1906, she appeared in a wide range of parts, including in Francesca di Rimini by Francis Marion Crawford, the role of Fanny in Sapho by Alphonse Daudet, the magician Circe in a play by Charles Richet, the part of Marie Antoinette in the historic drama Varennes by Lavedan and Lenôtre, the part of the prince-poet Landry in a version of Sleeping Beauty by Richepin and Henri Cain, and a new version of the play Pelléas and Mélisande by symbolist poet Maurice Maeterlinck, in which she played the male role of Pelléas with the British actress Mrs Patrick Campbell as Melissande.[129] She also starred in a new version of Adrienne Lecouvreur, which she wrote, different from the earlier version which had been written for her by Scribe. During this time, she wrote a drama, Un Coeur d'Homme, in which she had no part, which was performed at the Théâtre des Arts, but lasted only three performances.[130] She also taught acting briefly at the Conservatory, but found the system there too rigid and traditional. Instead, she took aspiring actresses and actors into her company, trained them, and used them as unpaid extras and bit players.[131]
Bernhardt made her first American Farewell Tour in 1905–1906, the first of four farewell tours she made to the US, Canada, and Latin America, with her new managers, the Shubert brothers. She attracted controversy and press attention when, during her 1905 visit to Montreal, the Roman Catholic bishop encouraged his followers to throw eggs at Bernhardt, because she portrayed prostitutes as sympathetic characters. The US portion of the tour was complicated due to the Shuberts' competition with the powerful syndicate of theatre owners which controlled nearly all the major theatres and opera houses in the United States. The syndicate did not allow outside producers to use their stages. As a result, in Texas and Kansas City, Bernhardt and her company performed under an enormous circus tent, seating 4,500 spectators, and in skating rinks in Atlanta, Savannah, Tampa, and other cities. Her private train took her to Knoxville, Dallas, Denver, Tampa, Chattanooga, and Salt Lake City, then on to the West Coast of the United States. She could not play in San Francisco because of the recent 1906 San Francisco earthquake, but she performed across the bay in the Hearst Greek Theatre at the University of California at Berkeley, and gave a recital, titled A Christmas Night during the Terror, for inmates at San Quentin penitentiary.[132]
Her tour continued into South America, where it was marred by a more serious event: at the conclusion of La Tosca in Rio de Janeiro, she leaped, as always, from the wall of the fortress to plunge to her death in the Tiber. This time, however, the mattress on which she was supposed to land had been positioned incorrectly. She landed on her right knee, which had already been damaged in earlier tours. She fainted and was taken from the theatre on a stretcher, but refused to be treated in a local hospital. She later sailed by ship from Rio to New York. When she arrived, her leg had swollen, and she was immobilised in her hotel for 15 days before returning to France.[133]
In 1906–1907, the French government finally awarded Bernhardt the Legion d'Honneur, but only in her role as a theatre director, not as an actress. However, the award at that time required a review of the recipients' moral standards, and Bernhardt's behavior was still considered scandalous. Bernhardt ignored the snub and continued to play both inoffensive and controversial characters. In November 1906, she starred in La Vierge d'Avila, ou La Courtisane de Dieu, by Catulle Mendes, playing Saint Theresa, followed on 27 January 1907 by Les Bouffons, by Miguel Zamocois, in which she played a young and amorous medieval lord.[134] In 1909, she again played the 19-year-old Joan of Arc in Le Procès de Jeanne d'Arc by Émile Moreau. French newspapers encouraged schoolchildren to view her personification of French patriotism.[135]
Despite the injury to her leg, she continued to go on tour every summer, when her own theatre in Paris was closed. In June 1908, she made a 20-day tour of Britain and Ireland, performing in 16 different cities.[136] In 1908–1909, she toured Russia and Poland. Her second American farewell tour (her eighth tour in America) began in late 1910. She took along a new leading man, the Dutch-born Lou Tellegen, a very handsome actor who had served as a model for the sculpture Eternal Springtime by Auguste Rodin, and who became her co-star for the next two years, as well as her escort to all events, functions, and parties. He was not a particularly good actor, and had a strong Dutch accent, but he was successful in roles such as Hippolyte in Phedre, where he could take off his shirt and show off his physique. In New York, she created yet another scandal when she appeared in the role of Judas Iscariot in Judas by the American playwright John Wesley De Kay. It was performed in New York's Globe Theater for only one night in December 1910 before it was banned by local authorities. It was also banned in Boston and Philadelphia.[137] The tour took her from Boston to Jacksonville, through Mississippi, Arkansas, Tennessee, Kentucky, West Virginia, and Pennsylvania, to Canada and Minnesota, usually one new city and one performance every day.[138]
In April 1912, Bernhardt presented a new production in her theatre, Les Amours de la reine Élisabeth, a romantic costume drama by Émile Moreau about Queen Elizabeth's romances with Robert Dudley and Robert Devereux. It was lavish and expensive, but was a monetary failure, lasting only 12 performances. Fortunately for Bernhardt, she was able to pay off her debt with the money she received from the American producer Adolph Zukor for a film version of the play.[139] (see Motion pictures)
She departed on her third farewell tour of the United States in 1913–1914, when she was 69. Her leg had not yet fully healed, and she was unable to perform an entire play, only selected acts. She also separated from her co-star and lover of the time, Lou Tellegen. When the tour ended, he remained in the United States, where he briefly became a silent movie star, and she returned to France in May 1913.[140]
In December 1913, Bernhardt performed another success with the drama Jeanne Doré. On 16 March, she was made a Chevalier of the Légion d'honneur. Despite her successes, she was still short of money. She had made her son Maurice the director of her new theatre, and permitted him to use the receipts of the theatre to pay his gambling debts, eventually forcing her to pawn some of her jewels to pay her bills.[141]
In 1914, she went as usual to her holiday home on Belle-Île with her family and close friends. There, she received the news of the assassination of Archduke Franz Ferdinand, and the beginning of the First World War. She hurried back to Paris, which was threatened by an approaching German army. In September, Bernhardt was asked by the Minister of War to move to a safer place. She departed for a villa on the Bay of Arcachon, where her physician discovered that gangrene had developed on her leg, still injured from her 1906 performance in Rio de Janeiro. She was transported to Bordeaux, where on 22 February 1915, a surgeon amputated her leg almost to the hip. She refused the idea of an artificial leg, crutches, or a wheelchair, and instead was usually carried in a palanquin she designed, supported by two long shafts and carried by two men. She had the chair decorated in the Louis XV style, with white sides and gilded trim.[142]
She returned to Paris on 15 October, and, despite the loss of her leg, continued to go on stage at her theatre; scenes were arranged so she could be seated, or supported by a prop with her leg hidden. She took part in a patriotic "scenic poem" by Eugène Morand, Les Cathédrales, playing the part of Strasbourg Cathedral; first, while seated, she recited a poem; then she hoisted herself on her one leg, leaned against the arm of the chair, and declared "Weep, weep, Germany! The German eagle has fallen into the Rhine!"[143]
Bernhardt joined a troupe of famous French actors and traveled to the Battle of Verdun and the Battle of the Argonne, where she performed for soldiers who were just returned or about to go into battle. Propped on pillows in an armchair, she recited her patriotic speech at Strasbourg Cathedral. Another actress present at the event, Beatrix Dussanne, described her performance: "The miracle again took place; Sarah, old, mutilated, once more illuminated a crowd by the rays of her genius. This fragile creature, ill, wounded and an immobile, could still, through the magic of the spoken word, re-instill heroism in those soldiers weary from battle."[144]
She returned to Paris in 1916 and made two short films on patriotic themes, one based on the story of Joan of Arc, the other called Mothers of France. Then she embarked on her final American farewell tour. Despite the threat of German submarines, she crossed the Atlantic and toured the United States, performing in major cities including New York and San Francisco. Bernhardt was diagnosed with uremia, and had to have an emergency kidney operation. She recuperated in Long Beach, California, for several months, writing short stories and novellas for publication in French magazines. In 1918, she returned to New York and boarded a ship to France, landing in Bordeaux on 11 November 1918, the day that the armistice was signed ending the First World War.[145]
In 1920, she resumed acting in her theatre, usually performing single acts of classics such as Racine's Athelee, which did not require much movement. For her curtain calls, she stood, balancing on one leg and gesturing with one arm. She also starred in a new play, Daniel, written by her grandson-in-law, playwright Louis Verneuil. She played the male lead role, but appeared in just two acts. She took the play and other famous scenes from her repertory on a European tour and then for her last tour of England, where she gave a special command performance for Queen Mary, followed by a tour of the British provinces.[146]
In 1921, Bernhardt made her last tour of the French provinces, lecturing about theatre and reciting the poetry of Rostand. Later that year, she produced a new play by Rostand, La Gloire, and another play by Verneuil, Régine Arnaud in 1922. She continued to entertain guests at her home. One such guest, French author Colette, described being served coffee by Bernhardt: "The delicate and withered hand offering the brimming cup, the flowery azure of the eyes, so young still in their network of fine lines, the questioning and mocking coquetry of the tilted head, and that indescribable desire to charm, to charm still, to charm right up to the gates of death itself."[147]
In 1922, she began rehearsing a new play by Sacha Guitry, called Un Sujet de Roman. On the night of the dress rehearsal, she collapsed, going into a coma for an hour, then awakened with the words, "when do I go on?" She recuperated for several months, with her condition improving; she began preparing for a new role as Cleopatra in Rodogune by Corneille, and agreed to make a new film by Sasha Guitry called La Voyante, for a payment of 10,000 francs a day. She was too weak to travel, so a room in her house on Boulevard Pereire was set up as a film studio, with scenery, lights, and cameras. However, on 21 March 1923, she collapsed again, and never recovered. She died from uremia on the evening of 26 March 1923. Newspaper reports stated she died "peacefully, without suffering, in the arms of her son".[148] At her request, her Funeral Mass was celebrated at the church of Saint-François-de-Sales, which she attended when she was in Paris.[149] The following day, 30,000 people attended her funeral to pay their respects, and an enormous crowd followed her casket from the Church of Saint-Francoise-de-Sales to Pere Lachaise Cemetery, pausing for a moment of silence outside her theatre.[150] The inscription on her tombstone is the name "Bernhardt".[151]
Bernhardt in the film Camille (La Dame aux camélias) with André Calmettes (1911)
As Queen Elizabeth in the film Les Amours de la reine Élisabeth (The Loves of Queen Elizabeth) with Lou Tellegen (1912)
Bernhardt was one of the early actresses to star in moving pictures. The first projected film was shown by the Lumiere brothers at the Grand Café in Paris on 28 December 1895. In 1900, the cameraman who had shot the first films for the Lumiere brothers, Clément Maurice, approached Bernhardt and asked her to make a film out of a scene from her stage production of Hamlet. The scene was Prince Hamlet's duel with Laertes, with Bernhardt in the role of Hamlet. Maurice made a phonograph recording at the same time, so the film could be accompanied by sound. The sound of the clashing wooden prop swords was not loud and realistic enough, so Maurice had a stage hand bang pieces of metal together in sync with the sword fight. Maurice's finished two-minute film, Le Duel d'Hamlet, was presented to the public at the 1900 Paris Universal Exposition from 14 April to 12 November 1900 in Paul Decauville's program, Phono-Cinéma-Théâtre. This program contained short films of many other famous French theatre stars of the day.[152] The sound quality on the wax cylinders and the synchronization were very poor, so the system never became a commercial success. Nonetheless, her film is cited as one of the early examples of a sound film.[153]
Eight years later, in 1908, Bernhardt made a second motion picture, La Tosca. This was produced by Le Film d'Art and directed by André Calmettes from the play by Victorien Sardou. The film has been lost. Her next film, with her co-star and lover Lou Tellegen, was La Dame aux Camelias, called "Camille". When she performed on this film, Bernhardt changed both the fashion in which she performed, significantly accelerating the speed of her gestural action.[154] The film was a success in the United States, and in France, the young French artist and later screenwriter Jean Cocteau wrote "What actress can play a lover better than she does in this film? No one!"[155] Bernhardt received $30,000 for her performance.
Shortly afterwards, she made another film of a scene from her play Adrienne Lecouvreur with Tellegen, in the role of Maurice de Saxe. Then, in 1912, the pioneer American producer Adolph Zukor came to London and filmed her performing scenes from her stage play Queen Elizabeth with her lover Tellegen, with Bernhardt in the role of Lord Essex.[156] To make the film more appealing, Zukor had the film print hand-tinted, making it one of the early color films. The Loves of Queen Elizabeth premiered at the Lyceum Theater in New York City on 12 July 1912, and was a financial success; Zukor invested $18,000 in the film and earned $80,000, enabling him to found the Famous Players Film Company, which later became Paramount Pictures.[157] The use of the visual arts–specifically famous c.19 painting–to frame scenes and elaborate narrative action is significant in the work.[154]
Bernhardt was also the subject and star of two documentaries, including Sarah Bernhardt à Belle-Isle (1915), a film about her daily life at home. This was one of the early films by a celebrity inviting us into the home, and it is again significant for the use it makes of contemporary art references in the mis-en-scene of the film.[154] She also made Jeanne Doré in 1916. This was produced by Eclipse and directed by Louis Mercanton and René Hervil from the play by Tristan Bernard. In 1917 she made a film called Mothers of France (Mères Françaises). Produced by Eclipse it was directed by Louis Mercanton and René Hervil with a screenplay by Jean Richepin. As Victoria Duckett explains in her book Seeing Sarah Bernhardt: Performance and Silent Film, this film was a propaganda film shot on the front line with the intent to urge America to join the War.[154]
In the weeks before her death in 1923, she was preparing to make La Voyante, another motion picture from her own home, directed by Sacha Guitry. She told journalists "They're paying me ten thousand francs a day, and plan to film for seven days. Make the calculation. These are American rates, and I don't have to cross the Atlantic! At those rates, I'm ready to appear in any films they make."[158] However, she died just before the filming began.[159]
Bernhardt began painting while she was at the Comédie-Française; because she rarely performed more than twice a week, she wanted a new activity to fill her time. Her paintings were mostly landscapes and seascapes, with many painted at Belle-Île. Her painting teachers were close and lifelong friends Georges Clairin and Louise Abbéma. She exhibited a 2-m-tall canvas, The Young Woman and Death, at the 1878 Paris Salon.[160]
Her passion for sculpture was more serious. Her sculpture teacher was Mathieu-Meusnier, an academic sculptor who specialised in public monuments and sentimental storytelling pieces.[161] She quickly picked up the techniques; she exhibited and sold a high-relief plaque of the death of Ophelia and, for the architect Charles Garnier, she created the allegorical figure of Song for the group Music on the façade of the Opera House of Monte Carlo.[162] She also exhibited a group of figures, called Après la Tempête (After the Storm), at the 1876 Paris Salon, receiving an honourable mention. Bernhardt sold the original work, the molds, and signed plaster miniatures, earning more than 10,000 francs.[162] The original is now displayed the National Museum of Women in the Arts in Washington, DC. Fifty works by Bernhardt have been documented, of which 25 are known to still exist.[163] Several of her works were also shown in the 1893 Columbia Exposition in Chicago and at the 1900 Exposition Universelle.[164] While on tour in New York, she hosted a private viewing of her paintings and sculptures for 500 guests.[165] In 1880, she made an Art Nouveau decorative bronze inkwell, a self-portrait with bat wings and a fish tail,[166] possibly inspired by her 1874 performance in Le Sphinx.[167] She set up a studio at 11 boulevard de Clichy in Montmartre, where she frequently entertained her guests dressed in her sculptor's outfit, including white satin blouse and white silk trousers. Rodin dismissed her sculptures as "old-fashioned tripe", and she was attacked in the press for pursuing an activity inappropriate for an actress. She was defended by Émile Zola, who wrote "How droll! Not content with finding her thin, or declaring her mad, they want to regulate her daily activities...Let a law be passed immediately to prevent the accumulation of talent!"[168]
In her final years, Bernhardt wrote a textbook on the art of acting. She wrote whenever she had time, usually between productions, and when she was on vacation at Belle-Île. After her death, the writer Marcel Berger, her close friend, found the unfinished manuscript among her belongings in her house on boulevard Pereire. He edited the book, and it was published as L'Art du Théâtre in 1923. An English translation was published in 1925.[169]
She paid particular attention to the use of the voice, "the instrument the most necessary to the dramatic artist." It was the element, she wrote, which connected the artist with the audience. "The voice must have all the harmonies...serious, plaintive, vibrant and metallic." For a voice to be fully complete, she wrote "It is necessary that it be very slightly nasal. An artist who has a dry voice can never touch the public." She also stressed the importance for artists to train their breathing for long passages. She suggested that an actress should be able to recite the following passage from Phédre in a single breath:
She noted that "the art of our art is not to have it noticed by the public...We must create an atmosphere by our sincerity, so that public, gasping, distracted, should not regain its equilibrium and free will until the fall of the curtain. That which is called the work, in our art, should only be the search for the truth."[171]
She also insisted that artists should express their emotions clearly without words, using "their eye, their hand, the position of the chest, the tilting of the head...The exterior form of the art is often the entire art; at least, it is that which strikes the audience the most effectively." She encouraged actors to "Work, overexcite your emotional expression, become accustomed to varying your psychological states and translating them...The diction, the way of standing, the look, the gesture are predominant in the development of the career of an artist."[172]
She explained why she liked to perform male roles: "The roles of men are in general more intellectual than the roles of women...Only the role of Phédre gives me the charm of digging into a heart that is truly anguished...Always, in the theatre, the parts played by the men are the best parts. And yet theatre is the sole art where women can sometimes be superior to men."[173]
Bernhardt had a remarkable ability to memorise a role quickly. She recounted in L'Art du Théâtre that "I only have to read a role two or three times and I know it completely; but the day that I stop playing the piece the role escapes me entirely...My memory can't contain several parts at the same time, and it's impossible for me to recite off-hand a tirade from Phèdre or Hamlet. And yet I can remember the smallest events from my childhood."[174] She also suffered, particularly early in her career, bouts of memory loss and stage fright. Once, she was seriously ill before a performance of L'Etrangère at the Gaiety Theatre in London, and the doctor gave her a dose of painkiller, either opium or morphine. During the performance, she went on stage, but could not remember what she was supposed to say. She turned to another actress, and announced, "If I made you come here, Madame, it is because I wanted to instruct you in what I want done...I have thought about it, and I do not want to tell you today", then walked offstage. The other actors, astonished, quickly improvised an ending to the scene. After a brief rest, her memory came back, and Bernhardt went back on stage, and completed the play.[174]
During another performance on her world tour, a backstage door was opened during a performance of Phèdre, and a cold wind blew across the stage as Bernhardt was reciting. Without interrupting her speech, she added "If someone doesn't close that door I will catch pneumonia." The door was closed, and no one in the audience seemed to notice the addition.[174]
French drama critics praised Bernhardt's performances; Francisque Sarcey, an influential Paris critic, wrote of her 1871 performance in Marie, "She has a sovereign grace, a penetrating charm, and I don't know what. She is a natural and an incomparable artist."[56] Reviewing her performance of Ruy Blas in 1872, the critic Théodore de Banville wrote that Bernhardt "declaimed like a bluebird sings, like the wind sighs, like the water murmurs."[175] Of the same performance, Sarcey wrote: "She added the music of her voice to the music of the verse. She sang, yes, sang with her melodious voice..."[175]
Victor Hugo was a fervent admirer of Bernhardt, praising her "golden voice". Describing her performance in his play, Ruy Blas in 1872, he wrote in his Carnets, "It is the first time this play has really been played! She is better than an actress, she is a woman. She is adorable; she is better than beautiful, she has the harmonious movements and looks of irresistible seduction."[175]
Her 1882 performance of Fédora was described by the French critic Maurice Baring: "A secret atmosphere emanated from her, an aroma, an attraction which was at once exotic and cerebral...She literally hypnotized the audience", and played "with such tigerish passion and feline seduction which, whether it be good or bad art, nobody has been able to match since."[89]
In 1884, Sigmund Freud saw Bernhardt perform Theodora, writing:
"I cannot say much for the play, but this Sarah, how she played! From the moment I heard her first lines, pronounced in her vibrant and adorable voice, I had the feeling I had known her for years. None of the lines that she spoke could surprise me; I believed immediately everything that she said. The smallest centimeter of this character was alive and enchanted you. And then, there was the manner she had to flatter, to implore, to embrace. Her incredible positions, the manner in which she keeps silent, but each of her limbs and each of her movements play the role for her! Strange creature! It is easy for me to imagine that she has no need to be any different on the street than she is on the stage!"[176]She also had her critics, particularly in her later years among the new generation of playwrights who advocated a more naturalistic style of acting. George Bernard Shaw wrote of the "childishly egotistical character of her acting, which is not the art of making you think more highly or feel more deeply but the art of making you admire her, pity her, champion her, weep with her, laugh at her jokes, follow her fortunes breathlessly and applaud her wildly when the curtain falls...It is the art of fooling you."[177] Ivan Turgenev wrote: "All she has is a marvelous voice. The rest is cold, false, and affected; the worst kind of repulsive chic Parisienne!"[178][179][180] Russian dramatist Anton Chekhov, then a young medical student, was paying for his studies by writing reviews for a Moscow newspaper. He stated that "We are far from admiring the talent of Sarah Bernhardt. She is a woman who is very intelligent and knows how to produce an effect, who has immense taste, who understands the human heart, but she wanted too much to astonish and overwhelm her audience."[178] He wrote that in her roles, "enchantment is smothered in artifice."[179]
Sarah Bernhardt's performances were seen and appraised by many of the leading literary and cultural figures of the late 19th century. Mark Twain wrote "There are five kinds of actresses. Bad actresses, fair actresses, good actresses, great actresses, and then there is Sarah Bernhardt." Oscar Wilde called her "the Incomparable One", scattered lilies in her path, and wrote a play in French, Salomé, especially for her; it was banned by British censors before it could be performed.[181] Shortly before he died, Wilde wrote: "The three women I have most admired in my life are Sarah Bernhardt, Lily Langtry, and Queen Victoria. I would have married any one of them with pleasure."[182]
After seeing a performance by Bernhardt in 1903, the British actress Ellen Terry wrote "How marvelous Sarah Bernhardt was! She had the transparence of an azalea with even more delicacy, the lightness of a cloud with less thickness. Smoke from a burning paper describes her more nearly."[183]
British author D.H. Lawrence saw Bernhardt perform La Dame aux Camelias in 1908. Afterward, he wrote to a friend:
"Sarah was wonderful and terrible. Oh, to see her, and to hear her, a wild creature, a gazelle with a beautiful panther's fascination and fury, laughing in musical French, screaming with true panther cry, sobbing and sighing like a deer sobs, wounded to the death...She is not pretty, her voice is not sweet, but there is the incarnation of wild emotion that we share with all living things..."[184]The identity of Bernhardt's father is not known for certain. Her original birth certificate was destroyed when the Paris Commune burned the Hotel de Ville and city archives in May 1871. In her autobiography, Ma Double Vie,[185] she describes meeting her father several times, and writes that his family provided funding for her education, and left a sum of 100,000 francs for her when she came of age.[186] She said he frequently travelled overseas, and that when she was still a child, he died in Pisa "in unexplained circumstances which remain mysterious."[187] In February 1914, she presented a reconstituted birth certificate, which stated that her legitimate father was one Édouard Bernhardt.[188] On 21 May 1856, when she was baptised, she was registered as the daughter of "Edouard Bernhardt residing in Le Havre and Judith Van Hard, residing in Paris."[189]
A more recent biography by Hélène Tierchant (2009) suggests her father may have been a young man named De Morel, whose family members were notable shipowners and merchants in Le Havre. According to Bernhardt's autobiography, her grandmother and uncle in Le Havre provided financial support for her education when she was young, took part in family councils about her future, and later gave her money when her apartment in Paris was destroyed by fire.[186]
Her date of birth is also uncertain due to the destruction of her birth certificate. She usually gave her birthday as 23 October 1844, and celebrated it on that day. However, the reconstituted birth certificate she presented in 1914 gave the date as 25 October.[190][191] Other sources give the date 22 October,[192] or either 22 or 23 October.[2]
Bernhardt's mother Judith, or Julie, was born in the early 1820s. She was one of six children, five daughters and one son, of a Dutch-Jewish itinerant eyeglass merchant, Moritz Baruch Bernardt, and a German laundress,[193] Sara Hirsch (later known as Janetta Hartog or Jeanne Hard). Judith's mother died in 1829, and five weeks later, her father remarried.[194] His new wife did not get along with the children from his earlier marriage. Judith and two of her sisters, Henriette and Rosine, left home, moved to London briefly, and then settled in Le Havre, on the French coast.[193] Henriette married a local in Le Havre, but Julie and Rosine became courtesans, and Julie took the new, more French name of Youle and the more aristocratic-sounding last name of Van Hard. In April 1843, she gave birth to twin girls to a "father unknown." Both girls died in the hospice in Le Havre a month later. The following year, Youle was pregnant again, this time with Sarah. She moved to Paris, to 5 rue de l'École-de-Médecine, where in October 1844, Sarah was born.[195]
Early in Bernhardt's career, she had an affair with a Belgian nobleman, Charles-Joseph Eugène Henri Georges Lamoral de Ligne (1837–1914), son of Eugène, 8th Prince of Ligne, with whom she bore her only child, Maurice Bernhardt (1864–1928). Maurice did not become an actor, but worked for most of his life as a manager and agent for various theatres and performers, frequently managing his mother's career in her later years, but rarely with great success. Maurice and his family were usually financially dependent, in full or in part, on his mother until her death. Maurice married a Polish princess, Maria Jablonowska, of the House of Jablonowski, with whom he had two daughters: Simone, who married Edgar Gross, son of a wealthy Philadelphia soap manufacturer; and Lysiana, who married the playwright Louis Verneuil.
From 1864 to 1866, after Bernhardt left the Comédie-Française, and after Maurice was born, she frequently had trouble finding roles. She often worked as a courtesan, taking wealthy and influential lovers. The French police of the Second Empire kept files on high-level courtesans, including Bernhardt; her file recorded the wide variety of names and titles of her patrons; they included Alexandre Aguado, the son of Spanish banker and Marquis Alejandro María Aguado; the industrialist Robert de Brimont; the banker Jacques Stern; and the wealthy Louis-Roger de Cahuzac.[196] The list also included Khalil Bey, the Ambassador of the Ottoman Empire to the Second Empire, best known today as the man who commissioned Gustave Courbet to paint L'Origine du monde, a detailed painting of a woman's anatomy that was banned until 1995, but now on display at the Musée d'Orsay. Bernhardt received from him a diadem of pearls and diamonds. She also had affairs with many of her leading men, and with other men more directly useful to her career, including Arsène Houssaye, director of the Théâtre-Lyrique, and the editors of several major newspapers. Many of her early lovers continued to be her friends after the affairs ended.[197]
During her time at the Odéon, she continued to see her old lovers, as well as new ones including French marshals François-Certain Canrobert and Achille Bazaine, the latter a commander of the French army in the Crimean War and in Mexico; and Prince Napoleon, son of Joseph Bonaparte and cousin of French Emperor Louis-Napoleon. She also had a two-year-long affair with Charles Haas, son of a banker and one of the more celebrated Paris dandies in the Empire, the model for the character of Swann in the novels by Marcel Proust.[198] Indeed, Swann even references her by name in Remembrance of Things Past. Sarah Bernhardt is probably one of the actresses after whom Proust modelled Berma, a character present in several volumes of Remembrance of Things Past.[citation needed]
Bernhardt took as lovers many of the male leads of her plays, including Mounet-Sully and Lou Tellegen. She possibly had an affair with the Prince of Wales, the future Edward VII, who frequently attended her London and Paris performances and once, as a prank, played the part of a cadaver in one of her plays.[199] When he was King, he travelled on the royal yacht to visit her at her summer home on Belle-Île.[200]
Her last serious love affair was with the Dutch-born actor Lou Tellegen, 37 years her junior, who became her co-star during her second American farewell tour (and eighth American tour) in 1910. He was a very handsome actor who had served as a model for sculpture Eternal Springtime by Auguste Rodin. He had little acting experience, but Bernhardt signed him as a leading man just before she departed on the tour, assigned him a compartment in her private railway car, and took him as her escort to all events, functions, and parties. He was not a particularly good actor, and had a strong Dutch accent, but he was successful in roles, such as Hippolyte in Phedre, where he could take off his shirt. At the end of the American tour, they had a dispute, he remained in the United States, and she returned to France. At first, he had a successful career in the United States and married film actress Geraldine Farrar, but when they split up his career plummeted. He committed suicide in 1934.[201]
Bernhardt's broad circle of friends included the writers Victor Hugo, Alexandre Dumas, his son Alexandre Dumas fils, Émile Zola, and the artist Gustave Doré. Her close friends included the painters Georges Clairin and Louise Abbéma (1853–1927), a French impressionist painter, some nine years her junior. This relationship was so close, the two women were rumoured to be lovers. In 1990, a painting by Abbéma, depicting the two on a boat ride on the lake in the bois de Boulogne, was donated to the Comédie-Française. The accompanying letter stated that the painting was "Peint par Louise Abbéma, le jour anniversaire de leur liaison amoureuse"[202] (loosely translated: "Painted by Louise Abbéma on the anniversary of their love affair") Clairin and Abbéma spent their holidays with Bernhardt and her family at her summer residence at Belle-Île, and remained close with Bernhardt until her death.[203]
In 1882, in Paris, Bernhardt met a Greek diplomat, Aristide Damala (known in France by his stage name Jacques Damala), who was 11 years her junior, and notorious for his romantic affairs. Bernhardt's biographer described him as "handsome as Adonis, insolent, vain, and altogether despicable."[204] His affairs with married women had already led to one suicide and two divorces, and the French government had asked him to leave Paris, transferring him to the Greek Embassy in St. Petersburg. She already had a lover at the time, Philippe Garnier, her leading man, but when she met Damala, she fell in love with him, and insisted that her tour be modified to include a stop in St. Petersburg. Garnier politely stepped aside and let her go to St. Petersburg without him. Arriving in St. Petersburg, Bernhardt invited Damala to give up his diplomatic post to become an actor in her company, as well as her lover, and before long, they decided to marry. During a break in the tour, they were married on 4 April 1882 in London. She told her friends that she married because marriage was the only thing she had never experienced.[205] Upon returning to Paris, she found a minor role for Damala in La Dame aux Camélias and a leading role in another play without her, Les Mères Ennemies by Catulle Mendès. Critics dismissed him as handsome, but without noticeable talent. Damala began taking large quantities of morphine, and following Bernhardt's great success in Fedora, Damala took every opportunity to criticise and humiliate her. She later discovered that he was using the money she gave him to buy presents for other women. In early December 1882, when she confronted him, he declared that he was going to North Africa to join the Foreign Legion, and disappeared.[206]
In early 1889, Damala reappeared at Bernhardt's door haggard, ill, and penniless. Bernhardt instantly forgave him, and offered him the role of Armand Duval in a new production of La Dame aux Camélias at the Variétés. They performed together from 18 May until 30 June. He looked exhausted and old, confused his diction, and forgot his lines. The critic for Le Rappel wrote: "Where is, alas, the handsome Armand Duval who was presented to us for the first time a few years ago at the Gaîté?" The critic Francisque Sarcey wrote simply, "he makes us feel sick." When his contract ended, he was able to get another contract as an actor at a different theater, and continued to harass Bernhardt. He attended one of her performances sitting in the first row, and made faces at her. Her current lover, Philippe Garnier, saw him and beat him. Later, he entered her house and ravaged the furniture. Bernhardt was a Roman Catholic, and did not want to divorce him.[207] He continued to act, sometimes with success, particularly in a play by Georges Ohnet, Le Maître des Forges, in 1883. However, his morphine addiction continued to worsen. In August 1889, Bernhardt learned that he had taken an overdose of morphine in Marseille. She hurried to his bedside and nursed him until he died on 18 August 1889, at the age of 34. He was buried in Athens. Bernhardt sent a bust she had made of him to be placed on his tomb, and when she toured in the Balkans, always made a detour to visit his grave. Until the end of her life, she continued to sign official documents as "Sarah Bernhardt, widow of Damala".[94]
After her 1886–87 tour, Bernhardt recuperated on Belle-Île, a small island off the coast of Brittany, 10 miles south of the Quiberon peninsula. She purchased a ruined 17th-century fortress, located at the end of the island and approached by a drawbridge, and turned it into her holiday retreat. From 1886 to 1922, she spent nearly every summer, the season when her theatre was closed, on Belle-Île. She built bungalows for her son Maurice and her grandchildren, and bungalows with studios for her close friends, the painters Georges Clairin and Louise Abbéma. She also brought her large collection of animals, including several dogs, two horses, a donkey, a hawk given to her by the Russian Grand Duke Alexis, an Andean wildcat, and a boa constrictor she had brought back from her tour of South America. She entertained many visitors at Belle-Île, including King Edward VII, who stopped by the island on a cruise aboard the royal yacht. Always wrapped in white scarves, she played tennis (under house rules that required that she be the winner) and cards, read plays, and created sculptures and ornaments in her studio. When the fishermen of the island suffered a bad season, she organised a benefit performance with leading actors to raise funds for them. She gradually enlarged the estate, purchasing a neighboring hotel and all the land with a view of the property, but in 1922, as her health declined, she abruptly sold it and never returned.[208] During the Second World War, the Germans occupied the island, and in October 1944, before leaving the island, they dynamited most of the compound. All that remains is the original old fort, and a seat cut into the rock where Bernhardt awaited the boat that took her to the mainland.[209]
Bernhardt was described as a strict vegetarian (what would later be termed vegan), as she avoided dairy, eggs and meat.[210][211] Her diet consisted of cereal, fruit, nuts and vegetables.[211] In 1913, The Literary Digest reported that she became vegetarian to lose weight and regain her figure.[210][211] However, a 1923 biography of Bernhardt noted that she consumed fish and in her older years favoured Gruyère or Pont-l'Évêque cheese.[212]
Mexican actress Virginia Fábregas (1871–1950) was nicknamed "The Mexican Sarah Bernhardt."[213]
After Bernhardt's death, her theatre was managed by her son Maurice until his death in 1928. It kept its name until the occupation of Paris by the Germans in World War II,[214] when, because of Bernhardt's Jewish ancestry, the name was changed to Théâtre de la Cité. The name was changed back to the Théâtre Sarah-Bernhardt in 1947, then in 1957 became the Théâtre des Nations. In 1968, it was renamed the Théâtre de la Ville, which is the name it has today.[215]
In 1876, Bernhardt constructed a large townhouse at 35 rue Fortuny in the 17th arrondissement, not far from Parc Monceau, for her family, servants, and animals. In 1885, when her debts mounted, she sold the house. Once her fortune was replenished by her tours abroad, she bought an even larger house at 56 avenue Pereire in the 17th arrondissement, where she died in 1923. The house was demolished in the 1960s and replaced by a modern apartment building. A plaque on the façade commemorates Bernhardt's earlier residence.[216]
In 1960, Bernhardt was inducted into the Hollywood Walk of Fame with a motion pictures star located at 1751 Vine Street. To date, she is the earliest born person on the Walk (born in 1844), followed by Thomas Edison and Siegmund Lubin.[217][218]
In 2018, Roundabout Theatre Company produced Theresa Rebeck's play Bernhardt/Hamlet. In the play, Rebeck explores the controversy surrounding Bernhardt's decision to play Hamlet. The play opened on Broadway in September at the American Airlines Theater for a limited run. It starred Janet McTeer as Bernhardt and it was directed by Moritz von Stuelpnagel.[219] McTeer received a Tony Award nomination for portraying Bernhardt.[220]
The new women's movement that took place in late nineteenth century and early twentieth century Brazil, was a movement built around a woman's ability to gain access to public spaces in Brazil. Among middle-class women, new opportunities and possibilities opened up for women allowing them professional positions in the workforce. Some women also found the acting profession to afford them freedom and independence. The theatre offered women an environment relatively free of social constraints. The profession of an actress held a controversial opinion within society. On one hand, high society embraced women who appeared in plays or opera representing a high culture. While on the other hand, female performers could suffer public scrutiny and gossip for leading unconventional lives.[221]
"The Eternal Feminine" was published 16 January 1886 by Revista Illstrada in Brazil six months before the first visiting of Sarah Bernhardt. "The Eternal Feminine" discussed advances of middle class and elite women in Brazil, citing expanding educational opportunities, acknowledging that women were capable of entering many new professions and industries that had previously been restricted to primarily men. "The Eternal Feminine" stated that "The bello sexo", as journalists so often called women, may move into new occupations, but their beauty, elegance, and eternal femininity needed to remain in place."[221]
Bernhardt's performances in Brazil had lasting effects in the sense that they encouraged new notions of possibilities for women in a patriarchal and traditional society and in theatre. Bernhardt made use of an array of tropes assigned to women to create a public personality that afforded her freedom, independence, and immense popularity at home and abroad." Even her famous cross-dressing roles such as Hamlet intervened in the tension between the traditional woman and the New Woman.[221] Bernhardt's ability to own her own theatre also speaks to the ways in which she embodies a new form of women.[citation needed]
The article Sarah Bernhardt's Knee read:
"In an era of debate about gender norms, Bernhardt's star image presented a similar fantasy scenario that fulfilled a need on the part of her public for unity, resolution, and reassurance. To her more socially conservative fans, Bernhardt appeased fears concerning the threat of the New Woman and the demise of female seduction as an everyday pleasure. She transcended the perceived conflict between the independent New Woman and the séductrice...[S]he was a living example of Marguerite Durand's contention that a woman need not lose her femininity to compete in a man's world."[221]


Frank Lloyd Wright (June 8, 1867 – April 9, 1959) was an American architect, designer, writer, and educator.
He designed more than 1,000 structures over a creative period of 70 years. Wright played a key role in the architectural movements of the twentieth century, influencing architects worldwide through his works and hundreds of apprentices in his Taliesin Fellowship.[1][2] Wright believed in designing in harmony with humanity and the environment, a philosophy he called organic architecture. This philosophy was exemplified in Fallingwater (1935), which has been called "the best all-time work of American architecture".[3]
Wright was the pioneer of what came to be called the Prairie School movement of architecture and also developed the concept of the Usonian home in Broadacre City, his vision for urban planning in the United States. He also designed original and innovative offices, churches, schools, skyscrapers, hotels, museums, and other commercial projects. Wright-designed interior elements (including leaded glass windows, floors, furniture and even tableware) were integrated into these structures. He wrote several books and numerous articles and was a popular lecturer in the United States and in Europe. Wright was recognized in 1991 by the American Institute of Architects as "the greatest American architect of all time".[3] In 2019, a selection of his work became a listed World Heritage Site as The 20th-Century Architecture of Frank Lloyd Wright.
Raised in rural Wisconsin, Wright studied civil engineering at the University of Wisconsin and then apprenticed in Chicago, briefly with Joseph Lyman Silsbee, and then with Louis Sullivan at Adler & Sullivan.  Wright opened his own successful Chicago practice in 1893 and established a studio in his Oak Park, Illinois home in 1898. His fame increased and his personal life sometimes made headlines: leaving his first wife Catherine Tobin for Mamah Cheney  in 1909; the murder of Mamah and her children and others at his Taliesin estate by a staff member in 1914; his tempestuous marriage with second wife Miriam Noel (m. 1923–1927); and his courtship and marriage with Olgivanna Lazović (m. 1928–1959).
Wright was born on June 8, 1867, in the town of Richland Center, Wisconsin, but maintained throughout his life that he was born in 1869.[4][5] In 1987 a biographer of Wright suggested that he may have been christened as "Frank Lincoln Wright" or "Franklin Lincoln Wright" but these assertions were not supported by any evidence.[6]
Wright's father, William Cary Wright (1825–1904), was a "gifted musician, orator, and sometime preacher who had been admitted to the bar in 1857."[7] He was also a published composer.[8] Originally from Massachusetts, William Wright had been a Baptist minister, but he later joined his wife's family in the Unitarian faith.
Wright's mother, Anna Lloyd Jones (1838/39–1923) was a teacher and a member of the Lloyd Jones clan; her parents had emigrated from Wales to Wisconsin.[9] One of Anna's brothers was Jenkin Lloyd Jones, an important figure in the spread of the Unitarian faith in the Midwest.
According to Wright's autobiography, his mother declared when she was expecting that her first child would grow up to build beautiful buildings. She decorated his nursery with engravings of English cathedrals torn from a periodical to encourage the infant's ambition.[10] 
Wright grew up in an "unstable household, [...] constant lack of resources, [...] unrelieved poverty and anxiety" and had a "deeply disturbed and obviously unhappy childhood".[11]  His father held pastorates in McGregor, Iowa (1869), Pawtucket, Rhode Island (1871), and Weymouth, Massachusetts (1874). Because the Wright family struggled financially also in Weymouth, they returned to Spring Green, where the supportive Lloyd Jones family could help William find employment. In 1877, they settled in Madison, where William gave music lessons and served as the secretary to the newly formed Unitarian society. Although William was a distant parent, he shared his love of music with his children.[11]
In 1876, Anna saw an exhibit of educational blocks called the Froebel Gifts, the foundation of an innovative kindergarten curriculum. Anna, a trained teacher, was excited by the program and bought a set with which the 9-year old Wright spent much time playing. The blocks in the set were geometrically shaped and could be assembled in various combinations to form two- and three-dimensional compositions. In his autobiography, Wright described the influence of these exercises on his approach to design: "For several years, I sat at the little kindergarten table-top... and played... with the cube, the sphere and the triangle – these smooth wooden maple blocks... All are in my fingers to this day... "[12]
In 1881, soon after Wright turned 14, his parents separated. In 1884, his father sued for a divorce from Anna on the grounds of "... emotional cruelty and physical violence and spousal abandonment".[13] Wright attended Madison High School, but there is no evidence that he graduated.[14] His father left Wisconsin after the divorce was granted in 1885. Wright said that he never saw his father again.[15]
In 1886, at age 19, Wright was admitted to the University of Wisconsin–Madison as a special student and worked under Allan D. Conover, a professor of civil engineering, before leaving the school without taking a degree.[16] Wright was granted an honorary doctorate of fine arts from the university in 1955.[17] In 1886 Wright collaborated with  the Chicago architectural firm of Joseph Lyman Silsbee – accredited as draftsman and construction supervisor – on the 1886 Unity Chapel for Wright's family in Spring Green, Wisconsin.[18]
In 1887, Wright arrived in Chicago in search of employment. As a result of the devastating Great Chicago Fire of 1871 and a population boom, new development was plentiful. Wright later recorded in his autobiography that his first impression of Chicago was as an ugly and chaotic city.[19] Within days of his arrival, and after interviews with several prominent firms, he was hired as a draftsman with Joseph Lyman Silsbee.[20] While with the firm, he also worked on two other family projects: All Souls Church in Chicago for his uncle, Jenkin Lloyd Jones, and the Hillside Home School I in Spring Green for two of his aunts.[21] Other draftsmen who worked for Silsbee in 1887 included future architects Cecil Corwin, George W. Maher, and George G. Elmslie. Wright soon befriended Corwin, with whom he lived until he found a permanent home.[22]
Feeling that he was underpaid for the quality of his work for Silsbee at $8 a week, the young draftsman quit and found work as an architectural designer at the firm of Beers, Clay, and Dutton. However, Wright soon realized that he was not ready to handle building design by himself; he left his new job to return to Joseph Silsbee – this time with a raise in salary.[23] Although Silsbee adhered mainly to Victorian and Revivalist architecture, Wright found his work to be more "gracefully picturesque" than the other "brutalities" of the period.[24]
Wright learned that the Chicago firm of Adler & Sullivan was "... looking for someone to make the finished drawings for the interior of the Auditorium Building".[25] Wright demonstrated that he was a competent impressionist of Louis Sullivan's ornamental designs and two short interviews later, was an official apprentice in the firm.[26] Wright did not get along well with Sullivan's other draftsmen; he wrote that several violent altercations occurred between them during the first years of his apprenticeship. For that matter, Sullivan showed very little respect for his own employees as well.[27] In spite of this, "Sullivan took [Wright] under his wing and gave him great design responsibility."[28] As an act of respect, Wright would later refer to Sullivan as Lieber Meister (German for "Dear Master").[28] He also formed a bond with office foreman Paul Mueller. Wright later engaged Mueller in the construction of several of his public and commercial buildings between 1903 and 1923.[29]
By 1890, Wright had an office next to Sullivan's that he shared with friend and draftsman George Elmslie, who had been hired by Sullivan at Wright's request.[29][30] Wright had risen to head draftsman and handled all residential design work in the office. As a general rule, the firm of Adler & Sullivan did not design or build houses, but would oblige when asked by the clients of their important commercial projects.[citation needed] Wright was occupied by the firm's major commissions during office hours, so house designs were relegated to evening and weekend overtime hours at his home studio. He later claimed total responsibility for the design of these houses, but a careful inspection of their architectural style (and accounts from historian Robert Twombly) suggests that Sullivan dictated the overall form and motifs of the residential works; Wright's design duties were often reduced to detailing the projects from Sullivan's sketches.[30] During this time, Wright worked on Sullivan's bungalow (1890) and the James A. Charnley bungalow (1890) in Ocean Springs, Mississippi, the Berry-MacHarg House, James A. Charnley House (both 1891), and the Louis Sullivan House (1892), all in Chicago.[31][32]
Despite Sullivan's loan and overtime salary, Wright was constantly short on funds. Wright admitted that his poor finances were likely due to his expensive tastes in wardrobe and vehicles, and the extra luxuries he designed into his house.[citation needed] To supplement his income and repay his debts, Wright accepted independent commissions for at least nine houses. These "bootlegged" houses, as he later called them, were conservatively designed in variations of the fashionable Queen Anne and Colonial Revival styles. Nevertheless, unlike the prevailing architecture of the period, each house emphasized simple geometric massing and contained features such as bands of horizontal windows, occasional cantilevers, and open floor plans, which would become hallmarks of his later work. Eight of these early houses remain today, including the Thomas Gale, Robert Parker, George Blossom, and Walter Gale houses.[33]
As with the residential projects for Adler & Sullivan, he designed his bootleg houses on his own time. Sullivan knew nothing of the independent works until 1893, when he recognized that one of the houses was unmistakably a Frank Lloyd Wright design.[citation needed] This particular house, built for Allison Harlan, was only blocks away from Sullivan's townhouse in the Chicago community of Kenwood.[citation needed] Aside from the location, the geometric purity of the composition and balcony tracery in the same style as the Charnley House likely gave away Wright's involvement.[citation needed] Since Wright's five-year contract forbade any outside work, the incident led to his departure from Sullivan's firm.[32] Several stories recount the break in the relationship between Sullivan and Wright; even Wright later told two different versions of the occurrence. In An Autobiography, Wright claimed that he was unaware that his side ventures were a breach of his contract. When Sullivan learned of them, he was angered and offended; he prohibited any further outside commissions and refused to issue Wright the deed to his Oak Park house until after he completed his five years. Wright could not bear the new hostility from his master and thought that the situation was unjust. He "... threw down [his] pencil and walked out of the Adler & Sullivan office never to return". Dankmar Adler, who was more sympathetic to Wright's actions, later sent him the deed.[34] However, Wright told his Taliesin apprentices (as recorded by Edgar Tafel) that Sullivan fired him on the spot upon learning of the Harlan House. Tafel also recounted that Wright had Cecil Corwin sign several of the bootleg jobs, indicating that Wright was aware of their forbidden nature. Regardless of the correct series of events, Wright and Sullivan did not meet or speak for 12 years.[32][35]
After leaving Adler & Sullivan, Wright established his own practice on the top floor of the Sullivan-designed Schiller Building on Randolph Street in Chicago. Wright chose to locate his office in the building because the tower location reminded him of the office of Adler & Sullivan. Cecil Corwin followed Wright and set up his architecture practice in the same office, but the two worked independently and did not consider themselves partners.[36]
In 1896, Wright moved from the Schiller Building to the nearby and newly completed Steinway Hall building. The loft space was shared with Robert C. Spencer, Jr., Myron Hunt, and Dwight H. Perkins.[37] These young architects, inspired by the Arts and Crafts Movement and the philosophies of Louis Sullivan, formed what became known as the Prairie School.[38] They were joined by Perkins' apprentice Marion Mahony, who in 1895 transferred to Wright's team of drafters and took over production of his presentation drawings and watercolor renderings. Mahony, the third woman to be licensed as an architect in Illinois and one of the first licensed female architects in the U.S., also designed furniture, leaded glass windows, and light fixtures, among other features, for Wright's houses. Between 1894 and the early 1910s, several other leading Prairie School architects and many of Wright's future employees launched their careers in the offices of Steinway Hall.[39][40]
Wright's projects during this period followed two basic models. His first independent commission, the Winslow House, combined Sullivanesque ornamentation with the emphasis on simple geometry and horizontal lines. The Francis Apartments (1895, demolished 1971), Heller House (1896), Rollin Furbeck House (1897) and Husser House (1899, demolished 1926) were designed in the same mode. For his more conservative clients, Wright designed more traditional dwellings. These included the Dutch Colonial Revival style Bagley House (1894), Tudor Revival style Moore House I (1895), and Queen Anne style Charles E. Roberts House (1896).[41] While Wright could not afford to turn down clients over disagreements in taste, even his most conservative designs retained simplified massing and occasional Sullivan-inspired details.[42]
Soon after the completion of the Winslow House in 1894, Edward Waller, a friend and former client, invited Wright to meet Chicago architect and planner Daniel Burnham. Burnham had been impressed by the Winslow House and other examples of Wright's work; he offered to finance a four-year education at the École des Beaux-Arts and two years in Rome. To top it off, Wright would have a position in Burnham's firm upon his return. In spite of guaranteed success and support of his family, Wright declined the offer. Burnham, who had directed the classical design of the World's Columbian Exposition and was a major proponent of the Beaux Arts movement, thought that Wright was making a foolish mistake.[citation needed] Yet for Wright, the classical education of the École lacked creativity and was altogether at odds with his vision of modern American architecture.[43][44]
Wright relocated his practice to his home in 1898 to bring his work and family lives closer. This move made further sense as the majority of the architect's projects at that time were in Oak Park or neighboring River Forest. The birth of three more children prompted Wright to sacrifice his original home studio space for additional bedrooms and necessitated his design and construction of an expansive studio addition to the north of the main house. The space, which included a hanging balcony within the two-story drafting room, was one of Wright's first experiments with innovative structure. The studio embodied Wright's developing aesthetics and would become the laboratory from which his next 10 years of architectural creations would emerge.[45]
By 1901, Wright had completed about 50 projects, including many houses in Oak Park. As his son John Lloyd Wright wrote:[46]
William Eugene Drummond, Francis Barry Byrne, Walter Burley Griffin, Albert Chase McArthur, Marion Mahony, Isabel Roberts, and George Willis were the draftsmen. Five men, two women. They wore flowing ties, and smocks suitable to the realm. The men wore their hair like Papa, all except Albert, he didn't have enough hair. They worshiped Papa! Papa liked them! I know that each one of them was then making valuable contributions to the pioneering of the modern American architecture for which my father gets the full glory, headaches, and recognition today!
Between 1900 and 1901, Frank Lloyd Wright completed four houses, which have since been identified as the onset of the "Prairie Style". Two, the Hickox and Bradley Houses, were the last transitional step between Wright's early designs and the Prairie creations.[47] Meanwhile, the Thomas House and Willits House received recognition as the first mature examples of the new style.[48][49] At the same time, Wright gave his new ideas for the American house widespread awareness through two publications in the Ladies' Home Journal. The articles were in response to an invitation from the president of Curtis Publishing Company, Edward Bok, as part of a project to improve modern house design.[citation needed] "A Home in a Prairie Town" and "A Small House with Lots of Room in it" appeared respectively in the February and July 1901 issues of the journal. Although neither of the affordable house plans was ever constructed, Wright received increased requests for similar designs in following years.[47] Wright came to Buffalo and designed homes for three of the company's executives: the Darwin D. Martin House (1904), the William R. Heath House 1905), and the Walter V. Davidson House (1908). Other Wright houses considered to be masterpieces of the Prairie Style are the Frederick Robie House in Chicago and the Avery and Queene Coonley House in Riverside, Illinois. The Robie House, with its extended cantilevered roof lines supported by a 110-foot-long (34 m) channel of steel, is the most dramatic. Its living and dining areas form virtually one uninterrupted space. With this and other buildings, included in the publication of the Wasmuth Portfolio (1910), Wright's work became known to European architects and had a profound influence on them after World War I.
Wright's residential designs of this era were known as "prairie houses" because the designs complemented the land around Chicago.[citation needed]  Prairie Style houses often have a combination of these features: one or two stories with one-story projections, an open floor plan, low-pitched roofs with broad, overhanging eaves, strong horizontal lines, ribbons of windows (often casements), a prominent central chimney, built-in stylized cabinetry, and a wide use of natural materials – especially stone and wood.[50]
By 1909, Wright had begun to reject the upper-middle-class Prairie Style single-family house model, shifting his focus to a more democratic architecture.[51] Wright went to Europe in 1909 with a portfolio of his work and presented it to Berlin publisher Ernst Wasmuth.[52] Studies and Executed Buildings of Frank Lloyd Wright, published in 1911, was the first major exposure of Wright's work in Europe. The work contained more than 100 lithographs of Wright's designs and is commonly known as the Wasmuth Portfolio.[53]
Wright designed the house of Cornell's chapter of Alpha Delta Phi literary society (1900), the Hillside Home School II (built for his aunts) in Spring Green, Wisconsin (1901) and the Unity Temple (1905) in Oak Park, Illinois.[54][55] As a lifelong Unitarian and member of Unity Temple, Wright offered his services to the congregation after their church burned down, working on the building from 1905 to 1909. Wright later said that Unity Temple was the edifice in which he ceased to be an architect of structure, and became an architect of space.[56]
Some other early notable public buildings and projects in this era: the Larkin Administration Building (1905); the Geneva Inn (Lake Geneva, Wisconsin, 1911); the Midway Gardens (Chicago, Illinois, 1913); the Banff National Park Pavilion (Alberta, Canada, 1914).
While working in Japan, Wright left an impressive architectural heritage. The Imperial Hotel, completed in 1923, is the most important.[57] Thanks to its solid foundations and steel construction, the hotel survived the Great Kanto Earthquake almost unscathed.[58] The hotel was damaged during the bombing of Tokyo and by the subsequent US military occupation of it after World War II.[59] As land in the center of Tokyo increased in value the hotel was deemed obsolete and was demolished in 1968 but the lobby was saved and later re-constructed at the Meiji Mura architecture museum in Nagoya in 1976.[60]
Jiyu Gakuen was founded as a girls' school in 1921. The construction of the main building began in 1921 under Wright's direction and, after his departure, was continued by Endo.[61]  The school building, like the Imperial Hotel, is covered with Ōya stones.[citation needed]
The Yodoko Guesthouse (designed in 1918 and completed in 1924) was built as the summer villa for Tadzaemon Yamamura.
Frank Lloyd Wright's architecture had a strong influence on young Japanese architects. The Japanese architects Wright commissioned to carry out his designs were Arata Endo, Takehiko Okami, Taue Sasaki and Kameshiro Tsuchiura. Endo supervised the completion of the Imperial Hotel after Wright's departure in 1922 and also supervised the construction of the Jiyu Gakuen Girls' School and the Yodokō Guest House. Tsuchiura went on to create so-called "light" buildings, which had similarities to Wright's later work.[62]
In the early 1920s, Wright designed a "textile" concrete block system. The system of precast blocks, reinforced by an internal system of bars, enabled "fabrication as infinite in color, texture, and variety as in that rug."[63] Wright first used his textile block system on the Millard House in Pasadena, California, in 1923. Typically Wrightian is the joining of the structure to its site by a series of terraces that reach out into and reorder the landscape, making it an integral part of the architect's vision.[64] With the Ennis House and the Samuel Freeman House (both 1923), Wright had further opportunities to test the limits of the textile block system, including limited use in the Arizona Biltmore Hotel in 1927.[65] The Ennis house is often used in films, television, and print media to represent the future.[64] Wright's son, Lloyd Wright, supervised construction for the Storer, Freeman, and Ennis Houses. Architectural historian Thomas Hines has suggested that Lloyd's contribution to these projects is often overlooked.[66]
After World War II, Wright updated the concrete block system, calling it the Usonian Automatic system, resulting in the construction of several notable homes. As he explained in The Natural House (1954), "The original blocks are made on the site by ramming concrete into wood or metal wrap-around forms, with one outside face (which may be pattered), and one rear or inside face, generally coffered, for lightness."[63]
In 1903, while Wright was designing a house for Edwin Cheney (a neighbor in Oak Park), he became enamored of Cheney's wife, Mamah. Mamah Borthwick Cheney was a modern woman with interests outside the home. She was an early feminist, and Wright viewed her as his intellectual equal. Their relationship became the talk of the town; they often could be seen taking rides in Wright's automobile through Oak Park.[citation needed] In 1909, Wright and Mamah Cheney met up in Europe, leaving their spouses and children behind. Wright remained in Europe for almost a year, first in Florence, Italy (where he lived with his eldest son Lloyd) and, later, in Fiesole, Italy, where he lived with Mamah. During this time, Edwin Cheney granted Mamah a divorce, though Kitty still refused to grant one to her husband.[citation needed] After Wright returned to the United States in October 1910, he persuaded his mother to buy land for him in Spring Green, Wisconsin. The land, bought on April 10, 1911, was adjacent to land held by his mother's family, the Lloyd-Joneses. Wright began to build himself a new home, which he called Taliesin, by May 1911. The recurring theme of Taliesin also came from his mother's side: Taliesin in Welsh mythology was a poet, magician, and priest. The family motto, "Y Gwir yn Erbyn y Byd" ("The Truth Against the World"), was taken from the Welsh poet Iolo Morganwg, who also had a son named Taliesin. The motto is still used today as the cry of the druids and chief bard of the Eisteddfod in Wales.[67]
On August 15, 1914, while Wright was working in Chicago, a servant (Julian Carlton) set fire to the living quarters of Taliesin and then murdered seven people with an axe as the fire burned.[68][69][70] The dead included Mamah; her two children, John and Martha Cheney; a gardener (David Lindblom); a draftsman (Emil Brodelle); a workman (Thomas Brunker); and another workman's son (Ernest Weston). Two people survived the mayhem, one of whom, William Weston, helped to put out the fire that almost completely consumed the residential wing of the house. Carlton swallowed hydrochloric acid immediately following the attack in an attempt to kill himself.[69] He was nearly lynched on the spot, but was taken to the Dodgeville jail.[69] Carlton died from starvation seven weeks after the attack, despite medical attention.[69]
In 1922, Kitty Wright finally granted Wright a divorce. Under the terms of the divorce, Wright was required to wait one year before he could marry his then-mistress, Maude "Miriam" Noel. In 1923, Wright's mother, Anna (Lloyd Jones) Wright, died. Wright wed Miriam Noel in November 1923, but her addiction to morphine led to the failure of the marriage in less than one year.[71] In 1924, after the separation, but while still married, Wright met Olga (Olgivanna) Lazovich Hinzenburg. They moved in together at Taliesin in 1925, and soon after Olgivanna became pregnant. Their daughter, Iovanna, was born on December 3, 1925.[72][73]
On April 20, 1925, another fire destroyed the bungalow at Taliesin. Crossed wires from a newly installed telephone system were deemed to be responsible for the blaze, which destroyed a collection of Japanese prints that Wright estimated to be worth $250,000 to $500,000 ($4,172,000 to $8,343,000 in 2022).[74] Wright rebuilt the living quarters, naming the home "Taliesin III".[75]
In 1926, Olga's ex-husband, Vlademar Hinzenburg, sought custody of his daughter, Svetlana. In October 1926, Wright and Olgivanna were accused of violating the Mann Act and arrested in Tonka Bay, Minnesota.[76] The charges were later dropped.[77]
Wright and Miriam Noel's divorce was finalized in 1927. Wright was again required to wait for one year before remarrying. Wright and Olgivanna married in 1928.[78][79]
In 1932, Wright and his wife Olgivanna put out a call for students to come to Taliesin to study and work under Wright while they learned architecture and spiritual development. Olgivanna Wright had been a student of G. I. Gurdjieff who had previously established a similar school. Twenty-three came to live and work that year, including John (Jack) H. Howe, who would become Wright's chief draftsman.[80] A total of 625 people joined The Fellowship in Wright's lifetime.[81] The Fellowship was a source of workers for Wright's later projects, including: Fallingwater; The Johnson Wax Headquarters; and The Guggenheim Museum in New York City.[82]
Considerable controversy exists over the living conditions and education of the fellows.[83][84]  Wright was reputedly a difficult person to work with. One apprentice wrote: "He is devoid of consideration and has a blind spot regarding others' qualities. Yet I believe, that a year in his studio would be worth any sacrifice."[85]  The Fellowship evolved into The School of Architecture at Taliesin which was an accredited school until it closed under acrimonious circumstances in 2020.[86][87]Taking on the name "The School of Architecture" in June 2020, the school moved to the Cosanti Foundation, which it had worked with in the past.[88]
Wright is responsible for a series of concepts of suburban development united under the term Broadacre City. He proposed the idea in his book The Disappearing City in 1932 and unveiled a 12-square-foot (1.1 m2) model of this community of the future, showing it in several venues in the following years.[citation needed] Concurrent with the development of Broadacre City, also referred to as Usonia, Wright conceived a new type of dwelling that came to be known as the Usonian House. Although an early version of the form can be seen in the Malcolm Willey House (1934) in Minneapolis, the Usonian ideal emerged most completely in the Herbert and Katherine Jacobs First House (1937) in Madison, Wisconsin.[citation needed] Designed on a gridded concrete slab that integrated the house's radiant heating system, the house featured new approaches to construction, including walls composed of a "sandwich" of wood siding, plywood cores and building paper – a significant change from typically framed walls.[citation needed] Usonian houses commonly featured flat roofs and were usually constructed without basements or attics, all features that Wright had been promoting since the early 20th century.[89]
Usonian houses were Wright's response to the transformation of domestic life that occurred in the early 20th century when servants had become less prominent or completely absent from most American households. By developing homes with progressively more open plans, Wright allotted the woman of the house a "workspace", as he often called the kitchen, where she could keep track of and be available for the children and/or guests in the dining room.[90] As in the Prairie Houses, Usonian living areas had a fireplace as a point of focus. Bedrooms, typically isolated and relatively small, encouraged the family to gather in the main living areas. The conception of spaces instead of rooms was a development of the Prairie ideal.[citation needed] The built-in furnishings related to the Arts and Crafts movement's principles that influenced Wright's early work.[citation needed] Spatially and in terms of their construction, the Usonian houses represented a new model for independent living and allowed dozens of clients to live in a Wright-designed house at relatively low cost.[citation needed] His Usonian homes set a new style for suburban design that influenced countless postwar developers. Many features of modern American homes date back to Wright: open plans, slab-on-grade foundations, and simplified construction techniques that allowed more mechanization and efficiency in building.[91]
Fallingwater, one of Wright's most famous private residences (completed 1937), was built for Mr. and Mrs. Edgar J. Kaufmann, Sr., at Mill Run, Pennsylvania. Constructed over a 30-foot waterfall, it was designed according to Wright's desire to place the occupants close to the natural surroundings. The house was intended to be more of a family getaway, rather than a live-in home.[92] The construction is a series of cantilevered balconies and terraces, using limestone for all verticals and concrete for the horizontals. The house cost $155,000 (equivalent to $3,155,000 in 2022), including the architect's fee of $8,000 (equivalent to $163,000 in 2022). It was one of Wright's most expensive pieces.[92] Kaufmann's own engineers argued that the design was not sound. They were overruled by Wright, but the contractor secretly added extra steel to the horizontal concrete elements. In 1994, Robert Silman and Associates examined the building and developed a plan to restore the structure. In the late 1990s, steel supports were added under the lowest cantilever until a detailed structural analysis could be done. In March 2002, post-tensioning of the lowest terrace was completed.[citation needed]
Taliesin West, Wright's winter home and studio complex in Scottsdale, Arizona, was a laboratory for Wright from 1937 to his death in 1959. It is now the home of the Frank Lloyd Wright Foundation.[93]
The design and construction of the Solomon R. Guggenheim Museum in New York City occupied Wright from 1943 until 1959[94] and is probably his most recognized masterpiece. The building's unique central geometry was meant to allow visitors to easily experience Guggenheim's collection of nonobjective geometric paintings by taking an elevator to the top level and then viewing artworks by walking down the slowly descending, central spiral ramp.[citation needed]
The only realized skyscraper designed by Wright is the Price Tower, a 19-story tower in Bartlesville, Oklahoma. It is also one of the two existing vertically oriented Wright structures (the other is the S.C. Johnson Wax Research Tower in Racine, Wisconsin). The Price Tower was commissioned by Harold C. Price of the H. C. Price Company, a local oil pipeline and chemical firm. On March 29, 2007, Price Tower was designated a National Historic Landmark by the United States Department of the Interior, one of only 20 such properties in Oklahoma.[95]
Monona Terrace, originally designed in 1937 as municipal offices for Madison, Wisconsin, was completed in 1997 on the original site, using a variation of Wright's final design for the exterior, with the interior design altered by its new purpose as a convention center. The "as-built" design was carried out by Wright's apprentice Tony Puttnam. Monona Terrace was accompanied by controversy throughout the 60 years between the original design and the completion of the structure.[96]
Florida Southern College, located in Lakeland, Florida, constructed 12 (out of 18 planned) Frank Lloyd Wright buildings between 1941 and 1958 as part of the Child of the Sun project. It is the world's largest single-site collection of Frank Lloyd Wright architecture.[97]
His Prairie houses use themed, coordinated design elements (often based on plant forms) that are repeated in windows, carpets, and other fittings.[citation needed] He made innovative use of new building materials such as precast concrete blocks, glass bricks, and zinc cames (instead of the traditional lead) for his leadlight windows, and he famously used Pyrex glass tubing as a major element in the Johnson Wax Headquarters.[citation needed] Wright was also one of the first architects to design and install custom-made electric light fittings, including some of the first electric floor lamps, and his very early use of the then-novel spherical glass lampshade (a design previously not possible due to the physical restrictions of gas lighting).[citation needed] In 1897, Wright received a patent for "Prism Glass Tiles" that were used in storefronts to direct light toward the interior.[98] Wright fully embraced glass in his designs and found that it fit well into his philosophy of organic architecture. According to Wright's organic theory, all components of the building should appear unified, as though they belong together. Nothing should be attached to it without considering the effect on the whole. To unify the house to its site, Wright often used large expanses of glass to blur the boundary between the indoors and outdoors.[99] Glass allowed for interaction and viewing of the outdoors while still protecting from the elements. In 1928, Wright wrote an essay on glass in which he compared it to the mirrors of nature: lakes, rivers and ponds.[100] One of Wright's earliest uses of glass in his works was to string panes of glass along whole walls in an attempt to create light screens to join solid walls. By using this large amount of glass, Wright sought to achieve a balance between the lightness and airiness of the glass and the solid, hard walls. Arguably, Wright's best-known art glass is that of the Prairie style. The simple geometric shapes that yield to very ornate and intricate windows represent some of the most integral ornamentation of his career.[101] Wright also designed some of his own clothing.[102]
Wright strongly believed in individualism and did not affiliate with the American Institute of Architects during his career, going so far as to call the organization "a harbor of refuge for the incompetent," and "a form of refined gangsterism".[103] When an associate referred to him as "an old amateur" Wright confirmed, "I am the oldest."[104] Wright rarely credited any influences on his designs, but most architects, historians and scholars agree he had five major influences:[citation needed]

Wright was given a set of Froebel gifts at about age nine, and in his autobiography he cited them indirectly in explaining that he learned the geometry of architecture in kindergarten play: For several years I sat at the little kindergarten table-top ruled by lines about four inches apart each way making four-inch squares; and, among other things, played upon these 'unit-lines' with the square (cube), the circle (sphere) and the triangle (tetrahedron or tripod)—these were smooth maple-wood blocks. All are in my fingers to this day.[106]: 359  Wright later wrote, "The virtue of all this lay in the awakening of the child-mind to rhythmic structures in Nature… I soon became susceptible to constructive pattern evolving in everything I saw."[107]: 25 [108]: 205 
He routinely claimed the work of architects and architectural designers who were his employees as his own designs, and believed that the rest of the Prairie School architects were merely his followers, imitators, and subordinates.[109] As with any architect, though, Wright worked in a collaborative process and drew his ideas from the work of others. In his earlier days, Wright worked with some of the top architects of the Chicago School, including Sullivan. In his Prairie School days, Wright's office was populated by many talented architects, including William Eugene Drummond, John Van Bergen, Isabel Roberts, Francis Barry Byrne, Albert McArthur, Marion Mahony Griffin, and Walter Burley Griffin. The Czech-born architect Antonin Raymond worked for Wright at Taliesin and led the construction of the Imperial Hotel in Tokyo. He subsequently stayed in Japan and opened his own practice. Rudolf Schindler also worked for Wright on the Imperial Hotel and his own work is often credited as influencing Wright's Usonian houses. Schindler's friend Richard Neutra also worked briefly for Wright and became an internationally successful architect. In the Taliesin days, Wright employed many architects and artists who later become notable, such as Aaron Green, John Lautner, E. Fay Jones, Henry Klumb, William Bernoudy, John Underhill Ottenheimer, and Paolo Soleri.
Wright was a passionate Japanophile — he once proclaimed Japan to be "the most romantic, artistic, nature-inspired country on earth"[110] — and throughout his entire career, Japanese art and architecture held a strong sway over him. He was particularly interested in ukiyo-e woodblock prints, to which he claimed he was "enslaved."[111] Wright spent much of his free time selling, collecting, and appreciating these prints — he held parties and other events centered around them, proclaiming their pedagogical value to his guests and students,[111] and before arriving in Japan, his impressions of the nation were based almost entirely on them.[110][112]
Wright found particular inspiration in the formal aspects of Japanese art. He described ukiyo-e prints as "organic," because of their understatedness, harmony, and ability to be appreciated on a purely aesthetic level.[112] Additionally, he cherished their freeform compositions, where elements of the scene would frequently breach in front of one another, and their lack of extraneous detail, which he called a "gospel of elimination."[110][113] His interpretation of chashitsu (tea ceremony venues), mediated by the ideas of Okakura Kakuzō, was that of an architecture which emphasized openness, the "vacant space between the roof and walls."[114][a] Wright applied these principles on a large scale, and they became trademarks of his practice.
Wright's floor plans exhibit strong similarities to their presumed Japanese forebears. The open living spaces of his early homes were likely appropriated from the World's Columbian Exposition's Ho-O-Den Pavilion, whose sliding-screen dividers were removed in preparation for the event.[115] Likewise, Unity Temple follows a gongen-zukuri layout, characteristic of Shinto shrines and likely inspired by his 1905 visit to the Rinnō-ji temple complex,[116] and the shape of many of his cantilevered towers, including the Johnson Research Tower, may have been inspired by Japanese pagodas.[117] Wright's ornamental flourishes, as seen in his leaded glass windows and lively architectural drawings, demonstrate a technical indebtedness to ukiyo-e.[113] One modern commentator, discussing the Robie House, suggests that such elements combined allow Wright's architecture to exhibit iki, a particularly Japanese aesthetic value marked by a subdued stylishness.[118]
His ideas about the art of Japan appear to have drawn greatly from the activities of Ernest Fenollosa, whose work he likely first encountered between 1890 and 1893.[119] Many of Fenollosa's ideas are quite similar to those of Wright: these include his view of architecture as a "mother art," his condemnation of the West's "separation of construction and decoration," and his identification of an "organic wholeness" within ukiyo-e prints.[113][119] Also like Wright, Fenollosa perceived a "degeneracy" in Western architecture, with particular emphasis on Renaissance architecture; Wright himself admitted that Japanese prints helped to "vulgarize" the Renaissance for him.[119] Wright's art criticism treatise, The Japanese Print: An Interpretation, may be read as a straightforward expansion upon Fenollosa's ideas.[113][119]
Though Wright always acknowledged his indebtedness to Japanese art and architecture, he took offense to claims that he copied or adapted it. In his view, Japanese art simply validated his personal principles especially well, and as such it was not a source of special inspiration.[112] Responding to a claim by Charles Robert Ashbee that he was "trying to adapt Japanese forms to the United States," Wright said that such borrowing was "against [his] very religion."[120] Nonetheless, his insistence did not stop others from observing the same throughout his life.
Frank Lloyd Wright was interested in site and community planning throughout his career. His commissions and theories on urban design began as early as 1900 and continued until his death. He had 41 commissions on the scale of community planning or urban design.[121]
His thoughts on suburban design started in 1900 with a proposed subdivision layout for Charles E. Roberts entitled the "Quadruple Block Plan". This design strayed from traditional suburban lot layouts and set houses on small square blocks of four equal-sized lots surrounded on all sides by roads instead of straight rows of houses on parallel streets. The houses, which used the same design as published in "A Home in a Prairie Town" from the Ladies' Home Journal, were set toward the center of the block to maximize the yard space and included private space in the center. This also allowed for far more interesting views from each house. Although this plan was never realized, Wright published the design in the Wasmuth Portfolio in 1910.[122]
The more ambitious designs of entire communities were exemplified by his entry into the City Club of Chicago Land Development Competition in 1913. The contest was for the development of a suburban quarter section. This design expanded on the Quadruple Block Plan and included several social levels. The design shows the placement of the upscale homes in the most desirable areas and the blue collar homes and apartments separated by parks and common spaces. The design also included all the amenities of a small city: schools, museums, markets, etc.[123] This view of decentralization was later reinforced by theoretical Broadacre City design. The philosophy behind his community planning was decentralization. The new development must be away from the cities. In this decentralized America, all services and facilities could coexist "factories side by side with farm and home".[124]
Notable community planning designs:
Wright's fashion sense was unique and he usually wore expensive suits, flowing neckties, and capes.[citation needed] He also had a fascination with automobiles, purchasing his first car in 1909, a Stoddard-Dayton roadster, and owned many exotic vehicles over the years. During the cash-strapped Depression, Wright drove cheaper vehicles. Some of his last cars in the 1950s included four Volkswagens and a Chevrolet Nomad station wagon along with flashier articles such as a Jaguar Mark VII. He owned some 50 cars between 1909 and his death, of which 10 are known to survive.[126]
Frank Lloyd Wright was married three times, fathering four sons and three daughters. He also adopted Svetlana Milanoff, the daughter of his third wife, Olgivanna Lloyd Wright.[127]
His wives were:
His children with Catherine were:
His children with Olgivanna were:
Though most famous as an architect, Wright was also an active dealer in Japanese art, primarily ukiyo-e. He frequently served as both architect and art dealer to the same clients: he designed a home, then provided the art to fill it.[133] For a time, Wright made more from selling art than from his work as an architect. He also kept a personal collection, which he used as a teaching aid with his apprentices in what were called "print parties,"[111][134] and to better suit his taste, he sometimes modified these personal prints using colored pencils and crayons.[112] Wright owned prints from masters such as Okumura Masanobu, Torii Kiyomasu I, Katsukawa Shunshō, Utagawa Toyoharu, Utagawa Kunisada, Katsushika Hokusai, and Utagawa Hiroshige;[112] he was especially fond of Hiroshige, whom he considered "the greatest artist in the world."[111]
Wright first traveled to Japan in 1905, where he bought hundreds of prints. The following year, he helped organize the world's first retrospective exhibition on Hiroshige, held at the Art Institute of Chicago,[133] a job which strengthened his reputation as an expert in Japanese art.[112] Wright did not cease buying prints in his return trips to Japan,[112] and for many years, he was a major presence in the art world, selling a great number of works both to prominent private collectors[133] and to museums such as the Metropolitan Museum of Art.[135] In sum, Wright spent over five hundred thousand dollars on prints between 1905 and 1923.[136] He penned a book on Japanese art, The Japanese Print: An Interpretation, in 1912.[110][135]
In 1920, many of the prints Wright sold had been found to exhibit signs of retouching, including pinholes and unoriginal pigments.[112][136] These retouched prints were likely made in retribution by some of his Japanese dealers, who were disgruntled by the architect's under-the-table sales.[112] In an attempt to clear his name, Wright took one of his dealers, Kyūgo Hayashi, to court over the issue; Hayashi was subsequently sentenced to one year in prison, and barred from selling prints for an extended period of time.[112]
Though Wright protested his innocence, and provided his clients with genuine prints as replacements for those he was accused of retouching, the incident marked the end of the high point of his career as an art dealer.[135] He was forced to sell off much of his art collection to pay off outstanding debts: in 1928, the Bank of Wisconsin claimed Taliesin and sold thousands of his prints — for only one dollar a piece — to collector Edward Burr Van Vleck.[133] Nonetheless, Wright continued to collect and deal in prints until his death in 1959, using them as bartering chips and collateral for loans; he often relied upon his art business to remain financially solvent.[135] He once claimed that Taliesin I and II were "practically built" by his prints.[136]
The extent of his dealings in Japanese art went largely unknown, or underestimated, among art historians for decades. In 1980, Julia Meech, then associate curator of Japanese art at the Metropolitan Museum, began researching the history of the museum's collection of Japanese prints. She discovered "a three-inch-deep 'clump of 400 cards' from 1918, each listing a print bought from the same seller — 'F. L. Wright'" — and a number of letters exchanged between Wright and the museum's first curator of Far Eastern Art, Sigisbert C. Bosch Reitz. These discoveries and subsequent research led to a renewed understanding of Wright's career as an art dealer.[135]
On April 4, 1959, Wright was hospitalized for abdominal pains and was operated on April 6. He seemed to be recovering, but he died quietly on April 9 at the age of 91 years. The New York Times then reported he was 89.[137][138]
After his death, Wright's legacy was plagued with turmoil for years. His third wife Olgivanna's dying wish had been that she and Wright, and her daughter by her first marriage, would all be cremated and interred together in a memorial garden being built at Taliesin West. According to his own wishes, Wright's body had lain in the Lloyd-Jones cemetery, next to the Unity Chapel, within view of Taliesin in Wisconsin. Although Olgivanna had taken no legal steps to move Wright's remains (and against the wishes of other family members and the Wisconsin legislature), his remains were removed from his grave in 1985 by members of the Taliesin Fellowship. They were cremated and sent to Scottsdale where they were later interred as per Olgivanna's instructions. The original grave site in Wisconsin is now empty but is still marked with Wright's name.[139]
After Wright's death, most of his archives were stored at the Frank Lloyd Wright Foundation in Taliesin (in Wisconsin), and Taliesin West (in Arizona). These collections included more than 23,000 architectural drawings, some 44,000 photographs, 600 manuscripts, and more than 300,000 pieces of office and personal correspondence. It also contained about 40 large-scale architectural models, most of which were constructed for MoMA's retrospective of Wright in 1940.[140] In 2012, to guarantee a high level of conservation and access, as well as to transfer the considerable financial burden of maintaining the archive,[141] the Frank Lloyd Wright Foundation partnered with the Museum of Modern Art and the Avery Architectural and Fine Arts Library to move the archive's content to New York. Wright's furniture and art collection remains with the foundation, which will also have a role in monitoring the archive. These three parties established an advisory group to oversee exhibitions, symposiums, events, and publications.[140]
Photographs and other archival materials are held by the Ryerson and Burnham Libraries at the Art Institute of Chicago. The architect's personal archives are located at Taliesin West in Scottsdale, Arizona. The Frank Lloyd Wright archives include photographs of his drawings, indexed correspondence beginning in the 1880s and continuing through Wright's life, and other ephemera. The Getty Research Center, Los Angeles, also has copies of Wright's correspondence and photographs of his drawings in their Frank Lloyd Wright Special Collection. Wright's correspondence is indexed in An Index to the Taliesin Correspondence, ed. by Professor Anthony Alofsin, which is available at larger libraries.
Wright designed over 400 built structures[142] of which about 300 survived as of 2023[update].[citation needed] At least five have been lost to forces of nature: the waterfront house for W. L. Fuller in Pass Christian, Mississippi, destroyed by Hurricane Camille in August 1969; the Louis Sullivan Bungalow of Ocean Springs, Mississippi, destroyed by Hurricane Katrina in 2005; and the Arinobu Fukuhara House (1918) in Hakone, Japan, destroyed in the 1923 Great Kantō earthquake. In January 2006, the Wilbur Wynant House in Gary, Indiana was destroyed by fire.[143] In 2018 the Arch Oboler complex in Malibu, California was gutted in the Woolsey Fire.[144]
Many other notable Wright buildings were intentionally demolished: Midway Gardens (built 1913, demolished 1929), the Larkin Administration Building (built 1903, demolished 1950), the Francis Apartments and Francisco Terrace Apartments (Chicago, built 1895, demolished 1971 and 1974, respectively), the Geneva Inn (Lake Geneva, Wisconsin, built 1911, demolished 1970), and the Banff National Park Pavilion (built 1914, demolished 1934). The Imperial Hotel (built 1923) survived the 1923 Great Kantō earthquake, but was demolished in 1968 due to urban developmental pressures.[145] The Hoffman Auto Showroom in New York City (built 1954) was demolished in 2013.[146]
Several of Wright's projects were either built after his death, or remain unbuilt. These include:
Later in his life (and after his death in 1959), Wright was accorded significant honorary recognition for his lifetime achievements. He received a Gold Medal award from The Royal Institute of British Architects  in 1941. The American Institute of Architects awarded him the AIA Gold Medal in 1949. That medal was a symbolic "burying the hatchet" between Wright and the AIA. In a radio interview, he commented, "Well, the AIA I never joined, and they know why. When they gave me the gold medal in Houston, I told them frankly why. Feeling that the architecture profession is all that's the matter with architecture, why should I join them?"[104] He was awarded the Franklin Institute's Frank P. Brown Medal in 1953. He received honorary degrees from several universities (including his alma mater, the University of Wisconsin), and several nations named him as an honorary board member to their national academies of art and/or architecture. In 2000, Fallingwater was named "The Building of the 20th century" in an unscientific "Top-Ten" poll taken by members attending the AIA annual convention in Philadelphia.[citation needed] On that list, Wright was listed along with many of the USA's other greatest architects including Eero Saarinen, I.M. Pei, Louis Kahn, Philip Johnson, and Ludwig Mies van der Rohe; he was the only architect who had more than one building on the list. The other three buildings were the Guggenheim Museum, the Frederick C. Robie House, and the Johnson Wax Building.
In 1992, the Madison Opera in Madison, Wisconsin, commissioned and premiered the opera Shining Brow, by composer Daron Hagen and librettist Paul Muldoon based on events early in Wright's life. The work has since received numerous revivals, including a June 2013 revival at Fallingwater, in Bull Run, Pennsylvania, by Opera Theater of Pittsburgh. In 2000, Work Song: Three Views of Frank Lloyd Wright, a play based on the relationship between the personal and working aspects of Wright's life, debuted at the Milwaukee Repertory Theater.
In 1966, the United States Postal Service honored Wright with a Prominent Americans series 2¢ postage stamp.[147]
"So Long, Frank Lloyd Wright" is a song written by Paul Simon. Art Garfunkel has stated that the origin of the song came from his request that Simon write a song about the famous architect Frank Lloyd Wright. Simon himself stated that he knew nothing about Wright, but proceeded to write the song anyway.[148]
In 1957, Arizona made plans to construct a new capitol building. Believing that the submitted plans for the new capitol were tombs to the past, Frank Lloyd Wright offered Oasis as an alternative to the people of Arizona.[149]
In 2004, one of the spires included in his design was erected in Scottsdale.[150]
The city of Scottsdale, Arizona renamed a portion of Bell Road, a major east–west thoroughfare in the Phoenix metropolitan area, in honor of Frank Lloyd Wright.
Eight of Wright's buildings – Fallingwater, the Guggenheim Museum, the Hollyhock House, the Jacobs House, the Robie House, Taliesin, Taliesin West, and the Unity Temple – were inscribed on the list of UNESCO World Heritage Sites under the title The 20th-century Architecture of Frank Lloyd Wright in July 2019. UNESCO stated that these buildings were "innovative solutions to the needs for housing, worship, work or leisure" and "had a strong impact on the development of modern architecture in Europe".[151][152]

Jean-Baptiste Poquelin (French pronunciation: ​[ʒɑ̃ batist pɔklɛ̃], [pɔkəlɛ̃]; 15 January 1622 (baptised) – 17 February 1673), known by his stage name Molière (UK: /ˈmɒliɛər, ˈmoʊl-/, US: /moʊlˈjɛər, ˌmoʊliˈɛər/,[1][2][3] French: [mɔljɛʁ]), was a French playwright, actor, and poet, widely regarded as one of the greatest writers in the French language and world literature. His extant works include comedies, farces, tragicomedies, comédie-ballets, and more. His plays have been translated into every major living language and are performed at the Comédie-Française more often than those of any other playwright today.[4] His influence is such that the French language is often referred to as the "language of Molière".[5]
Born into a prosperous family and having studied at the Collège de Clermont (now Lycée Louis-le-Grand), Molière was well suited to begin a life in the theatre. Thirteen years as an itinerant actor helped him polish his comedic abilities while he began writing, combining Commedia dell'arte elements with the more refined French comedy.[6]
Through the patronage of aristocrats including Philippe I, Duke of Orléans—the brother of Louis XIV—Molière procured a command performance before the King at the Louvre. Performing a classic play by Pierre Corneille and a farce of his own, The Doctor in Love, Molière was granted the use of salle du Petit-Bourbon near the Louvre, a spacious room appointed for theatrical performances. Later, he was granted the use of the theatre in the Palais-Royal. In both locations, Molière found success among Parisians with plays such as The Affected Ladies, The School for Husbands, and The School for Wives. This royal favour brought a royal pension to his troupe and the title Troupe du Roi ("The King's Troupe"). Molière continued as the official author of court entertainments.[7]
Despite the adulation of the court and Parisians, Molière's satires attracted criticism from other circles. For Tartuffe's impiety, the Catholic Church in France denounced this study of religious hypocrisy, which was followed by a ban by the Parlement, while Don Juan was withdrawn and never restaged by Molière.[8] His hard work in so many theatrical capacities took its toll on his health and, by 1667, he was forced to take a break from the stage. In 1673, during a production of his final play, The Imaginary Invalid, Molière, who suffered from pulmonary tuberculosis, was seized by a coughing fit and a haemorrhage while playing the hypochondriac Argan; he finished the performance but collapsed again and died a few hours later.[7]
Molière was born in Paris shortly before his christening as Jean Poquelin on 15 January 1622. Known as Jean-Baptiste, he was the first son of Jean Poquelin and Marie Cressé, who had married on 27 April 1621.[9] His mother was the daughter of a prosperous bourgeois family.[10] Upon seeing him for the first time, a maid exclaimed, "Le nez!", a reference to the infant's large nose. Molière was called "Le Nez" by his family from that time.[11] He lost his mother when he was 10,[12] and he does not seem to have been particularly close to his father. After his mother's death, he lived with his father above the Pavillon des Singes on the rue Saint-Honoré, an affluent area of Paris. It is likely that his education commenced with studies at a Parisian elementary school,[13] followed by his enrolment in the prestigious Jesuit Collège de Clermont, where he completed his studies in a strict academic environment and got a first taste of life on the stage.[14]
In 1631, his father Jean Poquelin purchased from the court of Louis XIII the posts of "valet de chambre ordinaire et tapissier du Roi" ("valet of the King's chamber and keeper of carpets and upholstery"). His son assumed the same posts in 1641.[15] The title required only three months' work and an initial cost of 1,200 livres; the title paid 300 livres a year and provided a number of lucrative contracts. Molière also studied as a provincial lawyer some time around 1642, probably in Orléans, but it is not documented that he ever qualified. So far he had followed his father's plans, which had served him well; he had mingled with nobility at the Collège de Clermont and seemed destined for a career in office.
In June 1643, when Molière was 21, he decided to abandon his social class and pursue a career on the stage. Taking leave of his father, he joined the actress Madeleine Béjart, with whom he had crossed paths before, and founded the Illustre Théâtre with 630 livres. They were later joined by Madeleine's brother and sister.
The theatre troupe went bankrupt in 1645. Molière had become head of the troupe, due in part, perhaps, to his acting prowess and his legal training. However, the troupe had acquired large debts, mostly for the rent of the theatre (a court for jeu de paume), for which they owed 2000 livres. Historians differ as to whether his father or the lover of a member of his troupe paid his debts; either way, after a 24-hour stint in prison he returned to the acting circuit. It was at this time that he began to use the pseudonym Molière, possibly inspired by a small village of the same name in the Midi near Le Vigan. It was likely that he changed his name to spare his father the shame of having an actor in the family (actors, although no longer vilified by the state under Louis XIV, were still not allowed to be buried in sacred ground).
After his imprisonment, he and Madeleine began a theatrical circuit of the provinces with a new theatre troupe; this life was to last about twelve years, during which he initially played in the company of Charles Dufresne, and subsequently created a company of his own, which had sufficient success and obtained the patronage of Philippe I, Duke of Orléans. Few plays survive from this period. The most noteworthy are L'Étourdi ou les Contretemps (The Bungler) and Le Docteur Amoureux (The Doctor in Love); with these two plays, Molière moved away from the heavy influence of the Italian improvisational Commedia dell'arte, and displayed his talent for mockery. In the course of his travels he met Armand, Prince of Conti, the governor of Languedoc, who became his patron, and named his company after him. This friendship later ended when Armand, having contracted syphilis from a courtesan, turned toward religion and joined Molière's enemies in the Parti des Dévots and the Compagnie de Saint Sacrement.
In Lyon, Mademoiselle Du Parc, known as Marquise, joined the company. Marquise was courted, in vain, by Pierre Corneille and later became the lover of Jean Racine. Racine offered Molière his tragedy Théagène et Chariclée (one of the early works he wrote after he had abandoned his theology studies), but Molière would not perform it, though he encouraged Racine to pursue his artistic career. It is said that soon thereafter Molière became angry with Racine when he was told that he had secretly presented his tragedy to the company of the Hôtel de Bourgogne as well.
Molière was forced to reach Paris in stages, staying outside for a few weeks in order to promote himself with society gentlemen and allow his reputation to feed in to Paris. Molière reached Paris in 1658 and performed in front of the King at the Louvre (then for rent as a theatre) in Corneille's tragedy Nicomède and in the farce Le Docteur Amoureux with some success. He was awarded the title of Troupe de Monsieur (Monsieur being the honorific for the king's brother Philippe I, Duke of Orléans). With the help of Monsieur, his company was allowed to share the theatre in the large hall of the Petit-Bourbon with the famous Italian Commedia dell'arte company of Tiberio Fiorillo, famous for his character of Scaramouche. (The two companies performed in the theatre on different nights.) The premiere of Molière's Les Précieuses Ridicules (The Affected Young Ladies) took place at the Petit-Bourbon on 18 November 1659.
Les Précieuses Ridicules was the first of Molière's many attempts to satirize certain societal mannerisms and affectations then common in France. It is widely accepted that the plot was based on Samuel Chappuzeau's Le Cercle des Femmes of 1656. He primarily mocks the Académie Française, a group created by Richelieu under a royal patent to establish the rules of the fledgling French theatre. The Académie preached unity of time, action, and styles of verse. Molière is often associated with the claim that comedy castigat ridendo mores or "criticises customs through humour" (a phrase in fact coined by his contemporary Jean de Santeuil and sometimes mistaken for a classical Latin proverb).[16]
Despite his own preference for tragedy, which he had tried to further with the Illustre Théâtre, Molière became famous for his farces, which were generally in one act and performed after the tragedy. Some of these farces were only partly written, and were played in the style of Commedia dell'arte with improvisation over a canovaccio (a vague plot outline). He began to write full, five-act comedies in verse (L'Étourdi (Lyon, 1654) and Le dépit amoureux (Béziers, 1656)), which although immersed in the gags of contemporary Italian troupes, were successful as part of Madeleine Béjart and Molière's plans to win aristocratic patronage and, ultimately, move the troupe to a position in a Paris theater-venue.[17] Later Molière concentrated on writing musical comedies, in which the drama is interrupted by songs and/or dances, but for years the fundamentals of numerous comedy-traditions would remain strong, especially Italian (e.g. the semi-improvisatory style that in the 1750s writers started calling commedia dell'arte), Spanish, and French plays, all also drawing on classical models (e.g. Plautus and Terence), especially the trope of the clever slave/servant.[18]
Les précieuses ridicules won Molière the attention and the criticism of many, but it was not a popular success. He then asked Fiorillo to teach him the techniques of Commedia dell'arte. His 1660 play Sganarelle, ou Le Cocu imaginaire (The Imaginary Cuckold) seems to be a tribute both to Commedia dell'arte and to his teacher. Its theme of marital relationships dramatizes Molière's pessimistic views on the falsity inherent in human relationships. This view is also evident in his later works and was a source of inspiration for many later authors, including (in a different field and with different effect) Luigi Pirandello. It describes a kind of round dance where two couples believe that each of their partners has been betrayed by the other's and is the first in Molière's "Jealousy series", which includes Dom Garcie de Navarre, L'École des maris and L'École des femmes.
In 1660 the Petit-Bourbon was demolished to make way for the eastern expansion of the Louvre, but Molière's company was allowed to move into the abandoned theatre in the east wing of the Palais-Royal. After a period of refurbishment they opened there on 20 January 1661. In order to please his patron, Monsieur, who was so enthralled with entertainment and art that he was soon excluded from state affairs, Molière wrote and played Dom Garcie de Navarre ou Le Prince jaloux (The Jealous Prince, 4 February 1661), a heroic comedy derived from a work of Cicognini's. Two other comedies of the same year were the successful L'École des maris (The School for Husbands) and Les Fâcheux (The Bores), subtitled Comédie faite pour les divertissements du Roi (a comedy for the King's amusements) because it was performed during a series of parties that Nicolas Fouquet gave in honor of the sovereign. These entertainments led Jean-Baptiste Colbert to demand the arrest of Fouquet for wasting public money, and he was condemned to life imprisonment.[19]
On 20 February 1662, Molière married Armande Béjart, whom he believed to be the sister of Madeleine. (She may have been her illegitimate daughter with the Duke of Modena.) The same year, he premiered L'École des femmes (The School for Wives), subsequently regarded as a masterpiece. It poked fun at the limited education that was given to daughters of rich families and reflected Molière's own marriage. Both this work and his marriage attracted much criticism. The play sparked the protest called the "Quarrel of L'École des femmes". On the artistic side he responded with two lesser-known works: La Critique de "L'École des femmes", in which he imagined the spectators of his previous work attending it. The piece mocks the people who had criticised L'École des femmes by showing them at dinner after watching the play; it addresses all the criticism raised about the piece by presenting the critics' arguments and then dismissing them. This was the so-called Guerre comique (War of Comedy), in which the opposite side was taken by writers like Donneau de Visé, Edmé Boursault, and Montfleury.
However, more serious opposition was brewing, focusing on Molière's politics and his personal life. A so-called parti des Dévots arose in French high society, who protested against Molière's excessive "realism" and irreverence, which were causing some embarrassment. These people accused Molière of having married his daughter. The Prince of Conti, once Molière's friend, joined them. Molière had other enemies, too, among them the Jansenists and some traditional authors. However, the king expressed support for the author, granting him a pension and agreeing to be the godfather of Molière's first son. Boileau also supported him through statements that he included in his Art poétique.
Molière's friendship with Jean-Baptiste Lully influenced him towards writing his Le Mariage forcé and La Princesse d'Élide (subtitled as Comédie galante mêlée de musique et d'entrées de ballet), written for royal "divertissements" at the Palace of Versailles.
Tartuffe, ou L'Imposteur was also performed at Versailles, in 1664, and created the greatest scandal of Molière's artistic career. Its depiction of the hypocrisy of the dominant classes was taken as an outrage and violently contested. It also aroused the wrath of the Jansenists and the play was banned.
Molière was always careful not to attack the institution of monarchy. He earned a position as one of the king's favourites and enjoyed his protection from the attacks of the court. The king allegedly suggested that Molière suspend performances of Tartuffe, and the author rapidly wrote Dom Juan ou le Festin de Pierre to replace it. It was a strange work, derived from a work by Tirso de Molina and rendered in a prose that still seems modern today. It describes the story of an atheist who becomes a religious hypocrite and, for this, is punished by God. This work too was quickly suspended. The king, demonstrating his protection once again, became the new official sponsor of Molière's troupe.
With music by Lully, Molière presented L'Amour médecin (Love Doctor or Medical Love). Subtitles on this occasion reported that the work was given "par ordre du Roi" (by order of the king) and this work was received much more warmly than its predecessors.
In 1666, Le Misanthrope was produced. It is now widely regarded as Molière's most refined masterpiece, the one with the highest moral content, but it was little appreciated at its time. It caused the "conversion" of Donneau de Visé, who became fond of his theatre. But it was a commercial flop, forcing Molière to immediately write Le médecin malgré lui (The Doctor Despite Himself), a satire against the official sciences. This was a success despite a moral treatise by the Prince of Conti, criticizing the theatre in general and Molière in particular. In several of his plays, Molière depicted the physicians of his day as pompous individuals who speak (poor) Latin to impress others with false erudition, and know only clysters and bleedings as (ineffective) remedies.
After the Mélicerte and the Pastorale comique, he tried again to perform a revised Tartuffe in 1667, this time with the name of Panulphe or L'Imposteur. As soon as the King left Paris for a tour, Lamoignon and the archbishop banned the play. The King finally imposed respect for Tartuffe a few years later, after he had gained more power over the clergy.
Molière, now ill, wrote less. Le Sicilien ou L'Amour peintre was written for festivities at the castle of Saint-Germain-en-Laye, and was followed in 1668 by Amphitryon, inspired both by Plautus' work of the same name and Jean Rotrou's successful reconfiguration of the drama. With some conjecture, Molière's play can be seen to allude to the love affairs of Louis XIV, then king of France. George Dandin, ou Le mari confondu (The Confounded Husband) was little appreciated, but success returned with L'Avare (The Miser), now very well known.
With Lully he again used music for Monsieur de Pourceaugnac, for Les Amants magnifiques, and finally for Le Bourgeois gentilhomme (The Middle Class Gentleman), another of his masterpieces. It is claimed to be particularly directed against Colbert, the minister who had condemned his old patron Fouquet. The collaboration with Lully ended with a tragédie et ballet, Psyché, written in collaboration with Pierre Corneille and Philippe Quinault.
In 1672, Madeleine Béjart died, and Molière suffered from this loss and from the worsening of his own illness. Nevertheless, he wrote a successful Les Fourberies de Scapin ("Scapin's Deceits"), a farce and a comedy in five acts. His following play, La Comtesse d'Escarbagnas, is considered one of his lesser works.
Les Femmes savantes (The Learned Ladies) of 1672 is considered another of Molière's masterpieces. It was born from the termination of the legal use of music in theatre, since Lully had patented the opera in France (and taken most of the best available singers for his own performances), so Molière had to go back to his traditional genre. It was a great success, and it led to his last work (see below), which is still held in high esteem.
In his 14 years in Paris, Molière singlehandedly wrote 31 of the 85 plays performed on his stage.
In 1661, Molière introduced the comédies-ballets in conjunction with Les Fâcheux. These ballets were a transitional form of dance performance between the court ballets of Louis XIV and the art of professional theatre which was developing in the advent of the use of the proscenium stage.[20] The comédies-ballets developed accidentally when Molière was enlisted to mount both a play and a ballet in the honor of Louis XIV and found that he did not have a big enough cast to meet these demands. Molière therefore decided to combine the ballet and the play so that his goal could be met while the performers catch their breath and change costume.[20] The risky move paid off and Molière was asked to produce twelve more comédies-ballets before his death.[20] During the comédies-ballets, Molière collaborated with Pierre Beauchamp.[20] Beauchamp codified the five balletic positions of the feet and arms and was partly responsible for the creation of the Beauchamp-Feuillet dance notation.[21] Molière also collaborated with Jean-Baptiste Lully.[20] Lully was a dancer, choreographer, and composer, whose dominant reign at the Paris Opéra lasted 15 years. Under his command, ballet and opera rightly became professional arts unto themselves.[22] The comédies-ballets closely integrated dance with music and the action of the play and the style of continuity distinctly separated these performances from the court ballets of the time;[23] additionally, the comédies-ballets demanded that both the dancers and the actors play an important role in advancing the story. Similar to the court ballets, both professionally trained dancers and courtiers socialized together at the comédies-ballets - Louis XIV even played the part of an Egyptian in Molière's Le Mariage forcé (1664) and also appeared as Neptune and Apollo in his retirement performance of Les Amants magnifiques (1670).[23]
Molière suffered from pulmonary tuberculosis, possibly contracted when he was imprisoned for debt as a young man. The circumstances of Molière's death, on 17 February 1673,[24] became legend. He collapsed on stage in a fit of coughing and haemorrhaging while performing in the last play he had written, which had lavish ballets performed to the music of Marc-Antoine Charpentier and which ironically was titled Le Malade imaginaire (The Imaginary Invalid). Molière insisted on completing his performance. Afterwards he collapsed again with another, larger haemorrhage before being taken home, where he died a few hours later, without receiving the last rites because two priests refused to visit him while a third arrived too late. The superstition that green brings bad luck to actors is said to originate from the colour of the clothing he was wearing at the time of his death.
Under French law at the time, actors were not allowed to be buried in the sacred ground of a cemetery. However, Molière's widow, Armande, asked the King if her spouse could be granted a normal funeral at night. The King agreed and Molière's body was buried in the part of the cemetery reserved for unbaptised infants.
In 1792, his remains were brought to the museum of French monuments, and in 1817, transferred to Père Lachaise Cemetery in Paris, close to those of La Fontaine.
Though conventional thinkers, religious leaders and medical professionals in Molière's time criticised his work, their ideas did not really diminish his widespread success with the public. Other playwrights and companies began to emulate his dramatic style in England and in France. Molière's works continued to garner positive feedback in 18th-century England, but they were not so warmly welcomed in France at this time. However, during the French Restoration of the 19th century, Molière's comedies became popular with both the French public and the critics. Romanticists admired his plays for the unconventional individualism they portrayed. 20th-century scholars have carried on this interest in Molière and his plays and have continued to study a wide array of issues relating to this playwright. Many critics now are shifting their attention from the philosophical, religious and moral implications in his comedies to the study of his comic technique.[25]
Molière's works were translated into English prose by John Ozell in 1714,[26] but the first complete version in English, by Baker and Miller in 1739, remained "influential" and was long reprinted.[27] The first to offer full translations of Molière's verse plays such as Tartuffe into English verse was Curtis Hidden Page, who produced blank verse versions of three of the plays in his 1908 translation.[28] Since then, notable translations have been made by Richard Wilbur, Donald M. Frame, and many others.
In his memoir A Terrible Liar, actor Hume Cronyn writes that, in 1962, celebrated actor Laurence Olivier criticized Molière. According to Cronyn, he mentioned to Olivier that he (Cronyn) was about to play the title role in The Miser, and that Olivier then responded "Molière? Funny as a baby's open grave." Cronyn comments on the incident: "You may imagine how that made me feel. Fortunately, he was dead wrong."[29]
Author Martha Bellinger points out that:
[Molière] has been accused of not having a consistent, organic style, of using faulty grammar, of mixing his metaphors, and of using unnecessary words for the purpose of filling out his lines. All these things are occasionally true, but they are trifles in comparison to the wealth of character he portrayed, to his brilliancy of wit, and to the resourcefulness of his technique. He was wary of sensibility or pathos; but in place of pathos he had "melancholy — a puissant and searching melancholy, which strangely sustains his inexhaustible mirth and his triumphant gaiety".[30]Molière is considered the creator of modern French comedy. Many words or phrases introduced in Molière's plays are still used in current French:
Molière plays a small part in Alexandre Dumas's novel The Vicomte of Bragelonne, in which he is seen taking inspiration from the muskeeter Porthos for his central character in Le Bourgeois gentilhomme.
Russian writer Mikhail Bulgakov wrote a semi-fictitious biography-tribute to Molière, titled Life of Mr. de Molière. It was written in 1932–1933 and first published 1962.
The French 1978 film simply titled Molière directed by Ariane Mnouchkine and starring Philippe Caubère presents his complete biography. It was in competition for the Palme d'Or at Cannes in 1978.
He is portrayed among other writers in The Blasphemers' Banquet (1989).
The 2000 film Le Roi Danse (The King Dances), in which Molière is played by Tchéky Karyo, shows his collaborations with Jean-Baptiste Lully, as well as his illness and on-stage death.
The 2007 French film Molière was more loosely based on the life of Molière, starring Romain Duris, Fabrice Luchini and Ludivine Sagnier.
David Hirson's play La Bête, written in the style of Molière, includes the character Elomire as an anagrammatic parody of him.




Wolfgang Amadeus Mozart[a][b] (27 January 1756 – 5 December 1791) was a prolific and influential composer of the Classical period. Despite his short life, his rapid pace of composition resulted in more than 800 works of virtually every genre of his time. Many of these compositions are acknowledged as pinnacles of the symphonic, concertante, chamber, operatic, and choral repertoire. Mozart is widely regarded as among the greatest composers in the history of Western music,[1] with his music admired for its "melodic beauty, its formal elegance and its richness of harmony and texture".[2]
Born in Salzburg, then in the Holy Roman Empire and currently in Austria, Mozart showed prodigious ability from his earliest childhood. Already competent on keyboard and violin, he composed from the age of five and performed before European royalty. His father took him on a grand tour of Europe and then three trips to Italy. At 17, he was a musician at the Salzburg court but grew restless and travelled in search of a better position.
While visiting Vienna in 1781, Mozart was dismissed from his Salzburg position. He stayed in Vienna, where he achieved fame but little financial security. During his final years there, he composed many of his best-known symphonies, concertos, and operas. His Requiem was largely unfinished by the time of his death at the age of 35, the circumstances of which are uncertain and much mythologized.
Wolfgang Amadeus Mozart was born on 27 January 1756 to Leopold Mozart (1719–1787) and Anna Maria, née Pertl (1720–1778), at Getreidegasse 9 in Salzburg.[3] Salzburg was the capital of the Archbishopric of Salzburg, an ecclesiastic principality in the Holy Roman Empire (today in Austria).[c] He was the youngest of seven children, five of whom died in infancy. His elder sister was Maria Anna Mozart (1751–1829), nicknamed "Nannerl". Mozart was baptised the day after his birth, at St. Rupert's Cathedral in Salzburg. The baptismal record gives his name in Latinized form, as Joannes Chrysostomus Wolfgangus Theophilus Mozart. He generally called himself "Wolfgang Amadè Mozart"[4] as an adult, but his name had many variants.
Leopold Mozart, a native of Augsburg,[5] then an Imperial Free City in the Holy Roman Empire, was a minor composer and an experienced teacher. In 1743, he was appointed as the fourth violinist in the musical establishment of Count Leopold Anton von Firmian, the ruling Prince-Archbishop of Salzburg.[2] Four years later, he married Anna Maria in Salzburg. Leopold became the orchestra's deputy Kapellmeister in 1763. During the year of his son's birth, Leopold published a violin textbook, Versuch einer gründlichen Violinschule, which achieved success.[6]
When Nannerl was 7, she began keyboard lessons with her father, while her three-year-old brother looked on. Years later, after her brother's death, she reminisced:
He often spent much time at the clavier, picking out thirds, which he was ever striking, and his pleasure showed that it sounded good. ... In the fourth year of his age his father, for a game as it were, began to teach him a few minuets and pieces at the clavier. ... He could play it faultlessly and with the greatest delicacy, and keeping exactly in time. ... At the age of five, he was already composing little pieces, which he played to his father who wrote them down.[7]These early pieces, K. 1–5, were recorded in the Nannerl Notenbuch. There is some scholarly debate about whether Mozart was four or five years old when he created his first musical compositions, though there is little doubt that Mozart composed his first three pieces of music within a few weeks of each other: K. 1a, 1b, and 1c.[9]
In his early years, Wolfgang's father was his only teacher. Along with music, he taught his children languages and academic subjects.[10] Solomon notes that, while Leopold was a devoted teacher to his children, there is evidence that Mozart was keen to progress beyond what he was taught.[10] His first ink-spattered composition and his precocious efforts with the violin were of his initiative and came as a surprise to Leopold,[11] who eventually gave up composing when his son's musical talents became evident.[12]
While Wolfgang was young, his family made several European journeys in which he and Nannerl performed as child prodigies. These began with an exhibition in 1762 at the court of Prince-elector Maximilian III of Bavaria in Munich, and at the Imperial Courts in Vienna and Prague. A long concert tour followed, spanning three and a half years, taking the family to the courts of Munich, Mannheim, Paris, London,[13] Dover, The Hague, Amsterdam, Utrecht, Mechelen and again to Paris, and back home via Zurich, Donaueschingen, and Munich.[14] During this trip, Wolfgang met many musicians and acquainted himself with the works of other composers. A particularly significant influence was Johann Christian Bach, whom he visited in London in 1764 and 1765. When he was eight years old, Mozart wrote his first symphony, most of which was probably transcribed by his father.[15]
The family trips were often challenging, and travel conditions were primitive.[16] They had to wait for invitations and reimbursement from the nobility, and they endured long, near-fatal illnesses far from home: first Leopold (London, summer 1764),[17] then both children (The Hague, autumn 1765).[18] The family again went to Vienna in late 1767 and remained there until December 1768.
After one year in Salzburg, Leopold and Wolfgang set off for Italy, leaving Anna Maria and Nannerl at home. This tour lasted from December 1769 to March 1771. As with earlier journeys, Leopold wanted to display his son's abilities as a performer and a rapidly maturing composer. Wolfgang met Josef Mysliveček and Giovanni Battista Martini in Bologna and was accepted as a member of the famous Accademia Filarmonica. There exists a myth, according to which, while in Rome, he heard Gregorio Allegri's Miserere twice in performance in the Sistine Chapel. Allegedly, he subsequently wrote it out from memory, thus producing the "first unauthorized copy of this closely guarded property of the Vatican". However, both origin and plausibility of this account are disputed.[19][20][d][21]
In Milan, Mozart wrote the opera Mitridate, re di Ponto (1770), which was performed with success. This led to further opera commissions. He returned with his father twice to Milan (August–December 1771; October 1772 – March 1773) for the composition and premieres of Ascanio in Alba (1771) and Lucio Silla (1772). Leopold hoped these visits would result in a professional appointment for his son, and indeed ruling Archduke Ferdinand contemplated hiring Mozart, but owing to his mother Empress Maria Theresa's reluctance to employ "useless people", the matter was dropped[e] and Leopold's hopes were never realized.[22] Toward the end of the journey, Mozart wrote the solo motet Exsultate, jubilate, K.165.
After finally returning with his father from Italy on 13 March 1773, Mozart was employed as a court musician by the ruler of Salzburg, Prince-Archbishop Hieronymus Colloredo. The composer had many friends and admirers in Salzburg[23] and had the opportunity to work in many genres, including symphonies, sonatas, string quartets, masses, serenades, and a few minor operas. Between April and December 1775, Mozart developed an enthusiasm for violin concertos, producing a series of five (the only ones he ever wrote), which steadily increased in their musical sophistication. The last three—K. 216, K. 218, K. 219—are now staples of the repertoire. In 1776, he turned his efforts to piano concertos, culminating in the E♭ concerto K. 271 of early 1777, considered by critics to be a breakthrough work.[24]
Despite these artistic successes, Mozart grew increasingly discontented with Salzburg and redoubled his efforts to find a position elsewhere. One reason was his low salary, 150 florins a year;[25] Mozart longed to compose operas, and Salzburg provided only rare occasions for these. The situation worsened in 1775 when the court theatre was closed, especially since the other theatre in Salzburg was primarily reserved for visiting troupes.[26]
Two long expeditions in search of work interrupted this long Salzburg stay. Mozart and his father visited Vienna from 14 July to 26 September 1773, and Munich from 6 December 1774 to March 1775. Neither visit was successful, though the Munich journey resulted in a popular success with the premiere of Mozart's opera La finta giardiniera.[27]
In August 1777, Mozart resigned his position at Salzburg[29][f] and on 23 September ventured out once more in search of employment, with visits to Augsburg, Mannheim, Paris, and Munich.[30]
Mozart became acquainted with members of the famous orchestra in Mannheim, the best in Europe at the time. He also fell in love with Aloysia Weber, one of four daughters of a musical family. There were prospects of employment in Mannheim, but they came to nothing,[31] and Mozart left for Paris on 14 March 1778[32] to continue his search. One of his letters from Paris hints at a possible post as an organist at Versailles, but Mozart was not interested in such an appointment.[33] He fell into debt and took to pawning valuables.[34] The nadir of the visit occurred when Mozart's mother was taken ill and died on 3 July 1778.[35] There had been delays in calling a doctor—probably, according to Halliwell, because of a lack of funds.[36] Mozart stayed with Melchior Grimm at Marquise d'Épinay's residence, 5 rue de la Chaussée-d'Antin.[37]
While Mozart was in Paris, his father was pursuing opportunities of employment for him in Salzburg.[38] With the support of the local nobility, Mozart was offered a post as court organist and concertmaster. The annual salary was 450 florins,[39] but he was reluctant to accept.[40] By that time, relations between Grimm and Mozart had cooled, and Mozart moved out. After leaving Paris in September 1778 for Strasbourg, he lingered in Mannheim and Munich, still hoping to obtain an appointment outside Salzburg. In Munich, he again encountered Aloysia, now a very successful singer, but she was no longer interested in him.[41] Mozart finally returned to Salzburg on 15 January 1779 and took up his new appointment, but his discontent with Salzburg remained undiminished.[42]
Among the better-known works which Mozart wrote on the Paris journey are the A minor piano sonata, K. 310/300d, the "Paris" Symphony (No. 31), which were performed in Paris on 12 and 18 June 1778;[43] and the Concerto for Flute and Harp in C major, K. 299/297c.[44]
In January 1781, Mozart's opera Idomeneo premiered with "considerable success" in Munich.[45] The following March, Mozart was summoned to Vienna, where his employer, Archbishop Colloredo, was attending the celebrations for the accession of Joseph II to the Austrian throne. For Colloredo, this was simply a matter of wanting his musical servant to be at hand (Mozart indeed was required to dine in Colloredo's establishment with the valets and cooks).[g] He planned a bigger career as he continued in the archbishop's service;[47] for example, he wrote to his father:
My main goal right now is to meet the emperor in some agreeable fashion, I am absolutely determined he should get to know me. I would be so happy if I could whip through my opera for him and then play a fugue or two, for that's what he likes.[48]Mozart did indeed soon meet the Emperor, who eventually was to support his career substantially with commissions and a part-time position.
In the same letter to his father just quoted, Mozart outlined his plans to participate as a soloist in the concerts of the Tonkünstler-Societät, a prominent benefit concert series;[48] this plan as well came to pass after the local nobility prevailed on Colloredo to drop his opposition.[49]
Colloredo's wish to prevent Mozart from performing outside his establishment was in other cases carried through, raising the composer's anger; one example was a chance to perform before the Emperor at Countess Thun's for a fee equal to half of his yearly Salzburg salary.
The quarrel with the archbishop came to a head in May: Mozart attempted to resign and was refused. The following month, permission was granted, but in a grossly insulting way: the composer was dismissed literally "with a kick in the arse", administered by the archbishop's steward, Count Arco. Mozart decided to settle in Vienna as a freelance performer and composer.[50]
The quarrel with Colloredo was more difficult for Mozart because his father sided against him. Hoping fervently that he would obediently follow Colloredo back to Salzburg, Mozart's father exchanged intense letters with his son, urging him to be reconciled with their employer. Mozart passionately defended his intention to pursue an independent career in Vienna. The debate ended when Mozart was dismissed by the archbishop, freeing himself both of his employer and of his father's demands to return. Solomon characterizes Mozart's resignation as a "revolutionary step" that significantly altered the course of his life.[51]
Mozart's new career in Vienna began well. He often performed as a pianist, notably in a competition before the Emperor with Muzio Clementi on 24 December 1781,[50] and he soon "had established himself as the finest keyboard player in Vienna".[50] He also prospered as a composer, and in 1782 completed the opera Die Entführung aus dem Serail ("The Abduction from the Seraglio"), which premiered on 16 July 1782 and achieved considerable success. The work was soon being performed "throughout German-speaking Europe",[50] and thoroughly established Mozart's reputation as a composer.
Near the height of his quarrels with Colloredo, Mozart moved in with the Weber family, who had moved to Vienna from Mannheim. The family's father, Fridolin, had died, and the Webers were now taking in lodgers to make ends meet.[52]
After failing to win the hand of Aloysia Weber, who was now married to the actor and artist Joseph Lange, Mozart's interest shifted to the third daughter of the family, Constanze.
The courtship did not go entirely smoothly; surviving correspondence indicates that Mozart and Constanze briefly separated in April 1782.[53] Mozart faced a challenging task in getting his father's permission for the marriage.[54] The couple were finally married on 4 August 1782 in St. Stephen's Cathedral, the day before his father's consenting letter arrived in the mail.[54]
The couple had six children, of whom only two survived infancy:[55]
In 1782 and 1783, Mozart became intimately acquainted with the work of Johann Sebastian Bach and George Frideric Handel as a result of the influence of Gottfried van Swieten, who owned many manuscripts of the Baroque masters. Mozart's study of these scores inspired compositions in Baroque style and later influenced his musical language, for example in fugal passages in Die Zauberflöte ("The Magic Flute") and the finale of Symphony No. 41.[2]
In 1783, Mozart and his wife visited his family in Salzburg. His father and sister were cordially polite to Constanze, but the visit prompted the composition of one of Mozart's great liturgical pieces, the Mass in C minor. Though not completed, it was premiered in Salzburg, with Constanze singing a solo part.[56]
Mozart met Joseph Haydn in Vienna around 1784, and the two composers became friends. When Haydn visited Vienna, they sometimes played together in an impromptu string quartet. Mozart's six quartets dedicated to Haydn (K. 387, K. 421, K. 428, K. 458, K. 464, and K. 465) date from the period 1782 to 1785, and are judged to be a response to Haydn's Opus 33 set from 1781.[57] Haydn wrote, "posterity will not see such a talent again in 100 years"[58] and in 1785 told Mozart's father: "I tell you before God, and as an honest man, your son is the greatest composer known to me by person and repute, he has taste and what is more the greatest skill in composition."[59]
From 1782 to 1785 Mozart mounted concerts with himself as a soloist, presenting three or four new piano concertos in each season. Since space in the theatres was scarce, he booked unconventional venues: a large room in the Trattnerhof apartment building, and the ballroom of the Mehlgrube restaurant.[60] The concerts were very popular, and his concertos premiered there are still firm fixtures in his repertoire. Solomon writes that during this period, Mozart created "a harmonious connection between an eager composer-performer and a delighted audience, which was given the opportunity of witnessing the transformation and perfection of a major musical genre".[60]
With substantial returns from his concerts and elsewhere, Mozart and his wife adopted a more luxurious lifestyle. They moved to an expensive apartment, with a yearly rent of 460 florins.[61] Mozart bought a fine fortepiano from Anton Walter for about 900 florins, and a billiard table for about 300.[61] The Mozarts sent their son Karl Thomas to an expensive boarding school[62][63] and kept servants. During this period Mozart saved little of his income.[64][65]
On 14 December 1784, Mozart became a Freemason, admitted to the lodge Zur Wohltätigkeit ("Beneficence").[66] Freemasonry played an essential role in the remainder of Mozart's life: he attended meetings, a number of his friends were Masons, and on various occasions, he composed Masonic music, e.g. the Maurerische Trauermusik.[67]
Despite the great success of Die Entführung aus dem Serail, Mozart did little operatic writing for the next four years, producing only two unfinished works and the one-act Der Schauspieldirektor. He focused instead on his career as a piano soloist and writer of concertos. Around the end of 1785, Mozart moved away from keyboard writing[69][page needed] and began his famous operatic collaboration with the librettist Lorenzo Da Ponte. The year 1786 saw the successful premiere of The Marriage of Figaro in Vienna. Its reception in Prague later in the year was even warmer, and this led to a second collaboration with Da Ponte: the opera Don Giovanni, which premiered in October 1787 to acclaim in Prague, but less success in Vienna during 1788.[70] The two are among Mozart's most famous works and are mainstays of operatic repertoire today, though at their premieres their musical complexity caused difficulty both for listeners and for performers. These developments were not witnessed by Mozart's father, who had died on 28 May 1787.[71]
In December 1787, Mozart finally obtained a steady post under aristocratic patronage. Emperor Joseph II appointed him as his "chamber composer", a post that had fallen vacant the previous month on the death of Gluck. It was a part-time appointment, paying just 800 florins per year, and required Mozart only to compose dances for the annual balls in the Redoutensaal (see Mozart and dance). This modest income became important to Mozart when hard times arrived. Court records show that Joseph aimed to keep the esteemed composer from leaving Vienna in pursuit of better prospects.[72][1]
In 1787, the young Ludwig van Beethoven spent several weeks in Vienna, hoping to study with Mozart.[73] No reliable records survive to indicate whether the two composers ever met.
Toward the end of the decade, Mozart's circumstances worsened. Around 1786 he had ceased to appear frequently in public concerts, and his income shrank.[74] This was a difficult time for musicians in Vienna because of the Austro-Turkish War: both the general level of prosperity and the ability of the aristocracy to support music had declined. In 1788, Mozart saw a 66% decline in his income compared to his best years in 1781.[75]
By mid-1788, Mozart and his family had moved from central Vienna to the suburb of Alsergrund.[74] Although it has been suggested that Mozart aimed to reduce his rental expenses by moving to a suburb, as he wrote in his letter to Michael von Puchberg, Mozart had not reduced his expenses but merely increased the housing space at his disposal.[76] Mozart began to borrow money, most often from his friend and fellow mason Puchberg; "a pitiful sequence of letters pleading for loans" survives.[77] Maynard Solomon and others have suggested that Mozart was suffering from depression, and it seems his musical output slowed.[78] Major works of the period include the last three symphonies (Nos. 39, 40, and 41, all from 1788), and the last of the three Da Ponte operas, Così fan tutte, premiered in 1790.
Around this time, Mozart made some long journeys hoping to improve his fortunes, visiting Leipzig, Dresden, and Berlin in the spring of 1789, and Frankfurt, Mannheim, and other German cities in 1790.
Mozart's last year was, until his final illness struck, a time of high productivity—and by some accounts, one of personal recovery.[79][h] He composed a great deal, including some of his most admired works: the opera The Magic Flute; the final piano concerto (K. 595 in B♭); the Clarinet Concerto K. 622; the last in his series of string quintets (K. 614 in E♭); the motet Ave verum corpus K. 618; and the unfinished Requiem K. 626.
Mozart's financial situation, a source of anxiety in 1790, finally began to improve. Although the evidence is inconclusive,[80] it appears that wealthy patrons in Hungary and Amsterdam pledged annuities to Mozart in return for the occasional composition. He is thought to have benefited from the sale of dance music written in his role as Imperial chamber composer.[80] Mozart no longer borrowed large sums from Puchberg and began to pay off his debts.[80]
He experienced great satisfaction in the public success of some of his works, notably The Magic Flute (which was performed several times in the short period between its premiere and Mozart's death)[81] and the Little Masonic Cantata K. 623, premiered on 17 November 1791.[82]
Mozart fell ill while in Prague for the premiere, on 6 September 1791, of his opera La clemenza di Tito, which was written in that same year on commission for Emperor Leopold II's coronation festivities.[83] He continued his professional functions for some time and conducted the premiere of The Magic Flute on 30 September. His health deteriorated on 20 November, at which point he became bedridden, suffering from swelling, pain, and vomiting.[84]
Mozart was nursed in his final days by his wife and her youngest sister, and was attended by the family doctor, Thomas Franz Closset. He was mentally occupied with the task of finishing his Requiem, but the evidence that he dictated passages to his student Franz Xaver Süssmayr is minimal.[85]
Mozart died in his home on 5 December 1791(1791-12-05) (aged 35) at 12:55 am.[86] The New Grove describes his funeral:
Mozart was interred in a common grave, in accordance with contemporary Viennese custom, at the St. Marx Cemetery outside the city on 7 December. If, as later reports say, no mourners attended, that too is consistent with Viennese burial customs at the time; later Otto Jahn (1856) wrote that Salieri, Süssmayr, van Swieten and two other musicians were present. The tale of a storm and snow is false; the day was calm and mild.[87]The expression "common grave" refers to neither a communal grave nor a pauper's grave, but an individual grave for a member of the common people (i.e., not the aristocracy). Common graves were subject to excavation after ten years; the graves of aristocrats were not.[88]
The cause of Mozart's death is not known with certainty. The official record of hitziges Frieselfieber ("severe miliary fever", referring to a rash that looks like millet seeds) is more a symptomatic description than a diagnosis. Researchers have suggested more than a hundred causes of death, including acute rheumatic fever,[89][90] streptococcal infection,[91][92] trichinosis,[93][94] influenza, mercury poisoning, and a rare kidney ailment.[89]
Mozart's modest funeral did not reflect his standing with the public as a composer; memorial services and concerts in Vienna and Prague were well-attended. Indeed, in the period immediately after his death, his reputation rose substantially. Solomon describes an "unprecedented wave of enthusiasm"[95] for his work; biographies were written first by Schlichtegroll, Niemetschek, and Nissen, and publishers vied to produce complete editions of his works.[95]
Mozart's physical appearance was described by tenor Michael Kelly in his Reminiscences: "a remarkably small man, very thin and pale, with a profusion of fine, fair hair of which he was rather vain". His early biographer Niemetschek wrote, "there was nothing special about [his] physique. ... He was small and his countenance, except for his large intense eyes, gave no signs of his genius." His facial complexion was pitted, a reminder of his childhood case of smallpox.[96] Of his voice, his wife later wrote that it "was a tenor, rather soft in speaking and delicate in singing, but when anything excited him, or it became necessary to exert it, it was both powerful and energetic."[97]
He loved elegant clothing. Kelly remembered him at a rehearsal: "[He] was on the stage with his crimson pelisse and gold-laced cocked hat, giving the time of the music to the orchestra." Based on pictures that researchers were able to find of Mozart, he seemed to wear a white wig for most of his formal occasions—researchers of the Salzburg Mozarteum declared that only one of his fourteen portraits they had found showed him without his wig.[96]
Mozart usually worked long and hard, finishing compositions at a tremendous pace as deadlines approached. He often made sketches and drafts; unlike Beethoven's, these are mostly not preserved, as his wife sought to destroy them after his death.[98]
Mozart lived at the center of the Viennese musical world, and knew a significant number and variety of people: fellow musicians, theatrical performers, fellow Salzburgers, and aristocrats, including some acquaintance with Emperor Joseph II. Solomon considers his three closest friends to have been Gottfried von Jacquin, Count August Hatzfeld, and Sigmund Barisani; others included his elder colleague Joseph Haydn, singers Franz Xaver Gerl and Benedikt Schack, and the horn player Joseph Leutgeb. Leutgeb and Mozart carried on a curious kind of friendly mockery, often with Leutgeb as the butt of Mozart's practical jokes.[99]
He enjoyed billiards, dancing, and kept pets, including a canary, a starling, a dog, and a horse for recreational riding.[100] He had a startling fondness for scatological humour, which is preserved in his surviving letters, notably those written to his cousin Maria Anna Thekla Mozart around 1777–1778, and in his correspondence with his sister and parents.[101] Mozart also wrote scatological music, a series of canons that he sang with his friends.[102] Mozart was raised a Catholic and remained a devout member of the Church throughout his life.[103][104]
Mozart's music, like Haydn's, stands as an archetype of the Classical style. At the time he began composing, European music was dominated by the style galant, a reaction against the highly evolved intricacy of the Baroque. Progressively, and in large part at the hands of Mozart himself, the contrapuntal complexities of the late Baroque emerged once more, moderated and disciplined by new forms, and adapted to a new aesthetic and social milieu. Mozart was a versatile composer, and wrote in every major genre, including symphony, opera, the solo concerto, chamber music including string quartet and string quintet, and the piano sonata. These forms were not new, but Mozart advanced their technical sophistication and emotional reach. He almost single-handedly developed and popularized the Classical piano concerto. He wrote a great deal of religious music, including large-scale masses, as well as dances, divertimenti, serenades, and other forms of light entertainment.[105]
The central traits of the Classical style are all present in Mozart's music. Clarity, balance, and transparency are the hallmarks of his work, but simplistic notions of its delicacy mask the exceptional power of his finest masterpieces, such as the Piano Concerto No. 24 in C minor, K. 491; the Symphony No. 40 in G minor, K. 550; and the opera Don Giovanni. Charles Rosen makes the point forcefully:
It is only through recognizing the violence and sensuality at the center of Mozart's work that we can make a start towards a comprehension of his structures and an insight into his magnificence. In a paradoxical way, Schumann's superficial characterization of the G minor Symphony can help us to see Mozart's daemon more steadily. In all of Mozart's supreme expressions of suffering and terror, there is something shockingly voluptuous.[106]During his last decade, Mozart frequently exploited chromatic harmony. A notable instance is his String Quartet in C major, K. 465 (1785), whose introduction abounds in chromatic suspensions, giving rise to the work's nickname, the "Dissonance" quartet.
Mozart had a gift for absorbing and adapting the valuable features of others' music. His travels helped in the forging of a unique compositional language.[107] In London as a child, he met J. C. Bach and heard his music. In Paris, Mannheim, and Vienna he met with other compositional influences, as well as the avant-garde capabilities of the Mannheim orchestra. In Italy, he encountered the Italian overture and opera buffa, both of which deeply affected the evolution of his practice. In London and Italy, the galant style was in the ascendent: simple, light music with a mania for cadencing; an emphasis on tonic, dominant, and subdominant to the exclusion of other harmonies; symmetrical phrases; and clearly articulated partitions in the overall form of movements.[108] Some of Mozart's early symphonies are Italian overtures, with three movements running into each other; many are homotonal (all three movements having the same key signature, with the slow middle movement being in the relative minor). Others mimic the works of J. C. Bach, and others show the simple rounded binary forms turned out by Viennese composers.
As Mozart matured, he progressively incorporated more features adapted from the Baroque. For example, the Symphony No. 29 in A major K. 201 has a contrapuntal main theme in its first movement, and experimentation with irregular phrase lengths. Some of his quartets from 1773 have fugal finales, probably influenced by Haydn, who had included three such finales in his recently published Opus 20 set. The influence of the Sturm und Drang ("Storm and Stress") period in music, with its brief foreshadowing of the Romantic era, is evident in the music of both composers at that time. Mozart's Symphony No. 25 in G minor K. 183 is another excellent example.
Mozart would sometimes switch his focus between operas and instrumental music. He produced operas in each of the prevailing styles: opera buffa, such as The Marriage of Figaro, Don Giovanni, and Così fan tutte; opera seria, such as Idomeneo; and Singspiel, of which Die Zauberflöte is the most famous example by any composer. In his later operas, he employed subtle changes in instrumentation, orchestral texture, and tone colour, for emotional depth and to mark dramatic shifts. Here his advances in opera and instrumental composing interacted: his increasingly sophisticated use of the orchestra in the symphonies and concertos influenced his operatic orchestration, and his developing subtlety in using the orchestra to psychological effect in his operas was in turn reflected in his later non-operatic compositions.[109]
For unambiguous identification of works by Mozart, a Köchel catalogue number is used. This is a unique number assigned, in regular chronological order, to every one of his known works. A work is referenced by the abbreviation "K." or "KV" followed by this number. The first edition of the catalogue was completed in 1862 by Ludwig von Köchel. It has since been repeatedly updated, as scholarly research improves knowledge of the dates and authenticity of individual works.[110]
Although some of Mozart's early pieces were written for harpsichord, he also became acquainted in his early years with pianos made by Regensburg builder Franz Jakob Späth [de]. Later when Mozart was visiting Augsburg, he was impressed by Stein pianos and shared this in a letter to his father.[111] On 22 October 1777, Mozart had premiered his triple-piano concerto, K. 242, on instruments provided by Stein. The Augsburg Cathedral organist Demmler was playing the first, Mozart the second and Stein the third part.[112] In 1783 when living in Vienna he purchased an instrument by Walter.[113] Leopold Mozart confirmed the attachment which Mozart had with his Walter fortepiano: "It is impossible to describe the hustle and bustle. Your brother's pianoforte has been moved at least twelve times from his house to the theatre or to someone else's house."[114]
His most famous pupil, whom the Mozarts took into their Vienna home for two years as a child, was probably Johann Nepomuk Hummel, a transitional figure between the Classical and Romantic eras.[115] More important is the influence Mozart had on composers of later generations. Ever since the surge in his reputation after his death, studying his scores has been a standard part of classical musicians' training.[116]
Ludwig van Beethoven, Mozart's junior by fifteen years, was deeply influenced by his work, with which he was acquainted as a teenager.[117] He is thought to have performed Mozart's operas while playing in the court orchestra at Bonn[118] and travelled to Vienna in 1787 hoping to study with the older composer. Some of Beethoven's works have direct models in comparable works by Mozart, and he wrote cadenzas (WoO 58) to Mozart's D minor piano concerto K. 466.[119][i]
Composers have paid homage to Mozart by writing sets of variations on his themes. Beethoven wrote four such sets (Op. 66, WoO 28, WoO 40, WoO 46).[120] Others include Fernando Sor's Introduction and Variations on a Theme by Mozart (1821), Mikhail Glinka's Variations on a Theme from Mozart's Opera The Magic Flute (1822), Frédéric Chopin's Variations on "Là ci darem la mano" from Don Giovanni (1827), and Max Reger's Variations and Fugue on a Theme by Mozart (1914), based on the variation theme in the piano sonata K. 331.[121] Pyotr Ilyich Tchaikovsky, who revered Mozart, wrote his Orchestral Suite No. 4 in G, Mozartiana (1887), as a tribute to him.[122]
See Buch 2017 for an extensive bibliography





Marco Polo (/ˈmɑːrkoʊ ˈpoʊloʊ/ (listen), Venetian: [ˈmaɾko ˈpolo], Italian: [ˈmarko ˈpɔːlo] (listen); c. 1254 –  8 January 1324)[1] was an Italian merchant, explorer and writer from the Republic of Venice[2][3] who travelled through Asia along the Silk Road between 1271 and 1295. His travels are recorded in The Travels of Marco Polo (also known as Book of the Marvels of the World  and Il Milione, c. 1300), a book that described to Europeans the then-mysterious culture and inner workings of the Eastern world, including the wealth and great size of the Mongol Empire and China in the Yuan Dynasty, giving their first comprehensive look into China, Persia, India, Japan and other Asian cities and countries.[4]
Born in Venice, Marco learned the mercantile trade from his father and his uncle, Niccolò and Maffeo, who travelled through Asia and met Kublai Khan. In 1269, they returned to Venice to meet Marco for the first time. The three of them embarked on an epic journey to Asia, exploring many places along the Silk Road until they reached Cathay (China). They were received by the royal court of Kublai Khan, who was impressed by Marco's intelligence and humility. Marco was appointed to serve as Khan's foreign emissary, and he was sent on many diplomatic missions throughout the empire and Southeast Asia, such as in present-day Burma, India, Indonesia, Sri Lanka and Vietnam.[5][6] As part of this appointment, Marco also travelled extensively inside China, living in the emperor's lands for 17 years and seeing many things that had previously been unknown to Europeans.[7] Around 1291, the Polos also offered to accompany the Mongol princess Kököchin to Persia; they arrived around 1293. After leaving the princess, they travelled overland to Constantinople and then to Venice, returning home after 24 years.[7] At this time, Venice was at war with Genoa; Marco was captured and imprisoned by the Genoans after joining the war effort and dictated his stories to Rustichello da Pisa, a cellmate. He was released in 1299, became a wealthy merchant, married, and had three children. He died in 1324 and was buried in the church of San Lorenzo in Venice.
Though he was not the first European to reach China, Marco Polo was the first to leave a detailed chronicle of his experience. This account of the Orient provided the Europeans with a clear picture of the East's geography and ethnic customs, and was the first Western record of porcelain,  gunpowder, paper money, and some Asian plants and exotic animals.[8] His travel book inspired Christopher Columbus[9] and many other travellers. There is substantial literature based on Polo's writings; he also influenced European cartography, leading to the introduction of the Catalan Atlas and the Fra Mauro map.[10]
Marco Polo was born in 1254 in Venice.[11][12]  His first known ancestor was a great uncle, Marco Polo (the older) from Venice, who lent some money and commanded a ship in Constantinople. Andrea, Marco's grandfather, lived in Venice in "contrada San Felice", he had three sons: Marco "the older", Maffeo and Niccolò (Marco's father).[13][14] Some Croatian sources claim Polo's ancestors to be of far Dalmatian origin,[15][16][17][18] but most historians consider it unfounded, as the Polo family lived in Venice since the year 971.[19]
Marco Polo is most often mentioned in the archives of the Republic of Venice as Marco Paulo de confinio Sancti Iohannis Grisostomi,[20] which means Marco Polo of the contrada of St John Chrysostom Church.
However, he was also nicknamed Milione during his lifetime (which in Italian literally means 'Million'). In fact, the Italian title of his book was Il libro di Marco Polo detto il Milione, which means "The Book of Marco Polo, nicknamed 'Milione'". According to the 15th-century humanist Giovanni Battista Ramusio, his fellow citizens awarded him this nickname when he came back to Venice because he kept on saying that Kublai Khan's wealth was counted in millions. More precisely, he was nicknamed Messer Marco Milioni (Mr Marco Millions).[21]
However, since also his father Niccolò was nicknamed Milione,[22] 19th-century philologist Luigi Foscolo Benedetto was persuaded that Milione was a shortened version of Emilione, and that this nickname was used to distinguish Niccolò's and Marco's branch from other Polo families.[23][24]
In 1168, his great-uncle, Marco Polo, borrowed money and commanded a ship in Constantinople.[25][26] His grandfather, Andrea Polo of the parish of San Felice, had three sons, Maffeo, yet another Marco, and the traveller's father Niccolò.[25] This genealogy, described by Ramusio, is not universally accepted as there is no additional evidence to support it.[27][28]
His father, Niccolò Polo, a merchant, traded with the Near East, becoming wealthy and achieving great prestige.[29][30] Niccolò and his brother Maffeo set off on a trading voyage before Marco's birth.[31][30] In 1260, Niccolò and Maffeo, while residing in Constantinople, then the capital of the Latin Empire, foresaw a political change; they liquidated their assets into jewels and moved away.[29] According to The Travels of Marco Polo, they passed through much of Asia, and met with Kublai Khan, a Mongol ruler and founder of the Yuan dynasty.[32] Their decision to leave Constantinople proved timely. In 1261 Michael VIII Palaiologos, the ruler of the Empire of Nicaea, took Constantinople, promptly burned the Venetian quarter and re-established the Byzantine Empire. Captured Venetian citizens were blinded,[33] while many of those who managed to escape perished aboard overloaded refugee ships fleeing to other Venetian colonies in the Aegean Sea.
Almost nothing is known about the childhood of Marco Polo until he was fifteen years old, except that he probably spent part of his childhood in Venice.[34][35][26] Meanwhile, Marco Polo's mother died, and an aunt and uncle raised him.[30] He received a good education, learning mercantile subjects including foreign currency, appraising, and the handling of cargo ships;[30] he learned little or no Latin.[29] His father later married Floradise Polo (née Trevisan).[28]
In 1269, Niccolò and Maffeo returned to their families in Venice, meeting young Marco for the first time.[34] In 1271, during the rule of Doge Lorenzo Tiepolo, Marco Polo (at seventeen years of age), his father, and his uncle set off for Asia on the series of adventures that Marco later documented in his book.[36]
They sailed to Acre and later rode on their camels to the Persian port Hormuz. During the first stages of the journey, they stayed for a few months in Acre and were able to speak with Archdeacon Tedaldo Visconti of Piacenza. The Polo family, on that occasion, had expressed their regret at the long lack of a pope, because on their previous trip to China they had received a letter from Kublai Khan to the Pope, and had thus had to leave for China disappointed. During the trip, however, they received news that after 33 months of vacation, finally, the Conclave had elected the new Pope and that he was exactly the archdeacon of Acre. The three of them hurried to return to the Holy Land, where the new Pope entrusted them with letters for the "Great Khan", inviting him to send his emissaries to Rome. To give more weight to this mission he sent with the Polos, as his legates, two Dominican fathers, Guglielmo of Tripoli and Nicola of Piacenza.[37]
They continued overland until they arrived at Kublai Khan's place in Shangdu, China (then known as Cathay). By this time, Marco was 21 years old.[38] Impressed by Marco's intelligence and humility, Khan appointed him to serve as his foreign emissary to India and Burma. He was sent on many diplomatic missions throughout his empire and in Southeast Asia, (such as in present-day Indonesia, Sri Lanka and Vietnam),[5][6] but also entertained the Khan with stories and observations about the lands he saw. As part of this appointment, Marco travelled extensively inside China, living in the emperor's lands for 17 years.[7]
Kublai initially refused several times to let the Polos return to Europe, as he appreciated their company and they became useful to him.[39] However, around 1291, he finally granted permission, entrusting the Polos with his last duty: accompany the Mongol princess Kököchin, who was to become the consort of Arghun Khan, in Persia (see Narrative section).[38][40] After leaving the princess, the Polos travelled overland to Constantinople. They later decided to return to their home.[38]
They returned to Venice in 1295, after 24 years, with many riches and treasures. They had travelled almost 15,000 miles (24,000 km).[30]
Marco Polo returned to Venice in 1295 with his fortune converted into gemstones. At this time, Venice was at war with the Republic of Genoa.[41] Polo armed a galley equipped with a trebuchet[42] to join the war. He was probably caught by Genoans in a skirmish in 1296, off the Anatolian coast between Adana and the Gulf of Alexandretta[43] (and not during the battle of Curzola (September 1298), off the Dalmatian coast,[44] a claim which is due to a later tradition (16th century) recorded by Giovanni Battista Ramusio[45][46]).
He spent several months of his imprisonment dictating a detailed account of his travels to a fellow inmate, Rustichello da Pisa,[30] who incorporated tales of his own as well as other collected anecdotes and current affairs from China. The book soon spread throughout Europe in manuscript form, and became known as The Travels of Marco Polo (Italian title: Il Milione, lit. "The Million", deriving from Polo's nickname "Milione". Original title in Franco-Italian : Livres des Merveilles du Monde). It depicts the Polos' journeys throughout Asia, giving Europeans their first comprehensive look into the inner workings of the Far East, including China, India, and Japan.[47]
Polo was finally released from captivity in August 1299,[30] and returned home to Venice, where his father and uncle in the meantime had purchased a large palazzo in the zone named contrada San Giovanni Crisostomo (Corte del Milion).[48] For such a venture, the Polo family probably invested profits from trading, and even many gemstones they brought from the East.[48] The company continued its activities and Marco soon became a wealthy merchant. Marco and his uncle Maffeo financed other expeditions, but likely never left Venetian provinces, nor returned to the Silk Road and Asia.[49] Sometime before 1300, his father Niccolò died.[49] In 1300, he married Donata Badoèr, the daughter of Vitale Badoèr, a merchant.[50] They had three daughters, Fantina (married Marco Bragadin), Bellela (married Bertuccio Querini), and Moreta.[51][52]
Pietro d'Abano philosopher, doctor and astrologer based in Padua, reports having spoken with Marco Polo about what he had observed in the vault of the sky during his travels. Marco told him that during his return trip to the South China Sea, he had spotted what he describes in a drawing as a star "shaped like a sack" (in Latin: ut sacco) with a big tail (magna habens caudam), most likely a comet. Astronomers agree that there were no comets sighted in Europe at the end of the thirteenth century, but there are records about a comet sighted in China and Indonesia in 1293.[53] Interestingly, this circumstance does not appear in Polo's book of Travels. Peter D'Abano kept the drawing in his volume "Conciliator Differentiarum, quæ inter Philosophos et Medicos Versantur". Marco Polo gave Pietro other astronomical observations he made in the Southern Hemisphere, and also a description of the Sumatran rhinoceros, which are collected in the Conciliator.[53]
In 1305 he is mentioned in a Venetian document among local sea captains regarding the payment of taxes.[28] His relation with a certain Marco Polo, who in 1300 was mentioned with riots against the aristocratic government, and escaped the death penalty, as well as riots from 1310 led by Bajamonte Tiepolo and Marco Querini, among whose rebels were Jacobello and Francesco Polo from another family branch, is unclear.[28] Polo is clearly mentioned again after 1305 in Maffeo's testament from 1309 to 1310, in a 1319 document according to which he became owner of some estates of his deceased father, and in 1321, when he bought part of the family property of his wife Donata.[28]
In 1323, Polo was confined to bed, due to illness.[54] On 8 January 1324, despite physicians' efforts to treat him, Polo was on his deathbed.[55] To write and certify the will, his family requested Giovanni Giustiniani, a priest of San Procolo. His wife, Donata, and his three daughters were appointed by him as co-executrices.[55] The church was entitled by law to a portion of his estate; he approved of this and ordered that a further sum be paid to the convent of San Lorenzo, the place where he wished to be buried.[55] He also set free Peter, a Tartar servant, who may have accompanied him from Asia,[56] and to whom Polo bequeathed 100 lire of Venetian denari.[57]
He divided up the rest of his assets, including several properties, among individuals, religious institutions, and every guild and fraternity to which he belonged.[55] He also wrote off multiple debts including 300 lire that his sister-in-law owed him, and others for the convent of San Giovanni, San Paolo of the Order of Preachers, and a cleric named Friar Benvenuto.[55] He ordered 220 soldi be paid to Giovanni Giustiniani for his work as a notary and his prayers.[58]
The will was not signed by Polo, but was validated by the then-relevant "signum manus" rule, by which the testator only had to touch the document to make it legally valid.[57][59] Due to the Venetian law stating that the day ends at sunset, the exact date of Marco Polo's death cannot be determined, but according to some scholars it was between the sunsets of 8 and 9 January 1324.[60] Biblioteca Marciana, which holds the original copy of his testament, dates the testament on 9 January 1323, and gives the date of his death at some time in June 1324.[59]
An authoritative version of Marco Polo's book does not and cannot exist, for the early manuscripts differ significantly, and the reconstruction of the original text is a matter of textual criticism. A total of about 150 copies in various languages are known to exist. Before the availability of printing press, errors were frequently made during copying and translating, so there are many differences between the various copies.[61][62]
Polo related his memoirs orally to Rustichello da Pisa while both were prisoners of the Genova Republic. Rustichello wrote Devisement du Monde in Franco-Venetian.[63] The idea probably was to create a handbook for merchants, essentially a text on weights, measures and distances.[64]
The oldest surviving manuscript is in Old French heavily flavoured with Italian;[65] According to the Italian scholar Luigi Foscolo Benedetto, this "F" text is the basic original text, which he corrected by comparing it with the somewhat more detailed Italian of Giovanni Battista Ramusio, together with a Latin manuscript in the Biblioteca Ambrosiana.  Other early important sources are R (Ramusio's Italian translation first printed in 1559), and Z (a fifteenth-century Latin manuscript kept at Toledo, Spain). Another Old French Polo manuscript, dating to around 1350, is held by the National Library of Sweden.[66]
One of the early manuscripts Iter Marci Pauli Veneti was a translation into Latin made by the Dominican brother Francesco Pipino [it] in 1302, just a few years after Marco's return to Venice. Since Latin was then the most widespread and authoritative language of culture, it is suggested that Rustichello's text was translated into Latin for a precise will of the Dominican Order, and this helped to promote the book on a European scale.[20]
The first English translation is the Elizabethan version by John Frampton published in 1579, The most noble and famous travels of Marco Polo, based on Santaella's Castilian translation of 1503 (the first version in that language).[67]
The published editions of Polo's book rely on single manuscripts, blend multiple versions together, or add notes to clarify, for example in the English translation by Henry Yule. The 1938 English translation by A. C. Moule and Paul Pelliot is based on a Latin manuscript found in the library of the Cathedral of Toledo in 1932, and is 50% longer than other versions.[68] The popular translation published by Penguin Books in 1958 by  R. E. Latham works several texts together to make a readable whole.[69]
The book opens with a preface describing his father and uncle travelling to Bolghar where Prince Berke Khan lived. A year later, they went to Ukek[70] and continued to Bukhara. There, an envoy from the Levant invited them to meet Kublai Khan, who had never met Europeans.[71] In 1266, they reached the seat of Kublai Khan at Dadu, present-day Beijing, China. Kublai received the brothers with hospitality and asked them many questions regarding the European legal and political system.[72] He also inquired about the Pope and Church in Rome.[73] After the brothers answered the questions he tasked them with delivering a letter to the Pope, requesting 100 Christians acquainted with the Seven Arts (grammar, rhetoric, logic, geometry, arithmetic, music and astronomy). Kublai Khan requested also that an envoy bring him back oil of the lamp in Jerusalem.[74] The long sede vacante between the death of Pope Clement IV in 1268 and the election of his successor delayed the Polos in fulfilling Kublai's request. They followed the suggestion of Theobald Visconti, then papal legate for the realm of Egypt, and returned to Venice in 1269 or 1270 to await the nomination of the new Pope, which allowed Marco to see his father for the first time, at the age of fifteen or sixteen.[75]
In 1271, Niccolò, Maffeo and Marco Polo embarked on their voyage to fulfil Kublai's request. They sailed to Acre, and then rode on camels to the Persian port of Hormuz. The Polos wanted to sail straight into China, but the ships there were not seaworthy, so they continued overland through the Silk Road, until reaching Kublai's summer palace in Shangdu, near present-day Zhangjiakou. In one instance during their trip, the Polos joined a caravan of travelling merchants whom they crossed paths with. Unfortunately, the party was soon attacked by bandits, who used the cover of a sandstorm to ambush them. The Polos managed to fight and escape through a nearby town, but many members of the caravan were killed or enslaved.[76] Three and a half years after leaving Venice, when Marco was about 21 years old, the Polos were welcomed by Kublai into his palace.[30] The exact date of their arrival is unknown, but scholars estimate it to be between 1271 and 1275.[nb 1] On reaching the Yuan court, the Polos presented the sacred oil from Jerusalem and the papal letters to their patron.[29]
Marco knew four languages, and the family had accumulated a great deal of knowledge and experience that was useful to Kublai. It is possible that he became a government official;[30] he wrote about many imperial visits to China's southern and eastern provinces, the far south and Burma.[77] They were highly respected and sought after in the Mongolian court, and so Kublai Khan decided to decline the Polos' requests to leave China. They became worried about returning home safely, believing that if Kublai died, his enemies might turn against them because of their close involvement with the ruler. In 1292, Kublai's great-nephew, then ruler of Persia, sent representatives to China in search of a potential wife, and they asked the Polos to accompany them, so they were permitted to return to Persia with the wedding party—which left that same year from Zaitun in southern China on a fleet of 14 junks. The party sailed to the port of Singapore,[78] travelled north to Sumatra,[79] and around the southern tip of India,[80] eventually crossing the Arabian Sea to Hormuz. The two-year voyage was a perilous one—of the six hundred people (not including the crew) in the convoy only eighteen had survived (including all three Polos).[81] The Polos left the wedding party after reaching Hormuz and travelled overland to the port of Trebizond on the Black Sea, the present-day Trabzon.[30]
The British scholar Ronald Latham has pointed out that The Book of Marvels was, in fact, a collaboration written in 1298–1299 between Polo and a professional writer of romances, Rustichello of Pisa.[82] It is believed that Polo related his memoirs orally to Rustichello da Pisa while both were prisoners of the Genova Republic. Rustichello wrote Devisement du Monde in Franco-Venetian language, which was a literary-only language widespread in northern Italy between the subalpine belt and the lower Po between the 13th and 15th centuries.[63][83]
Latham also argued that Rustichello may have glamorised Polo's accounts, and added fantastic and romantic elements that made the book a bestseller.[82] The Italian scholar Luigi Foscolo Benedetto had previously demonstrated that the book was written in the same "leisurely, conversational style" that characterised Rustichello's other works, and that some passages in the book were taken verbatim or with minimal modifications from other writings by Rustichello. For example, the opening introduction in The Book of Marvels to "emperors and kings, dukes and marquises" was lifted straight out of an Arthurian romance Rustichello had written several years earlier, and the account of the second meeting between Polo and Kublai Khan at the latter's court is almost the same as that of the arrival of Tristan at the court of King Arthur at Camelot in that same book.[84] Latham believed that many elements of the book, such as legends of the Middle East and mentions of exotic marvels, may have been the work of Rustichello who was giving what medieval European readers expected to find in a travel book.[85]
Apparently, from the very beginning, Marco's story aroused contrasting reactions, as it was received by some with a certain disbelief. The Dominican father Francesco Pipino was the author of a translation into Latin,  Iter Marci Pauli Veneti in 1302, just a few years after Marco's return to Venice. Francesco Pipino solemnly affirmed the truthfulness of the book and defined Marco as a "prudent, honoured and faithful man".[86]
In his writings, the Dominican brother  Jacopo d'Acqui explains why his contemporaries were sceptical about the content of the book. He also relates that before dying, Marco Polo insisted that "he had told only a half of the things he had seen".[86]
According to some recent research of the Italian scholar Antonio Montefusco, the very close relationship that Marco Polo cultivated with members of the Dominican Order in Venice suggests that local fathers collaborated with him for a Latin version of the book, which means that Rustichello's text was translated into Latin for a precise will of the Order.[20]
Since Dominican fathers had among their missions that of evangelizing foreign peoples (cf. the role of Dominican missionaries in China[87] and in the Indies[88]), it is reasonable to think that they considered Marco's book as a trustworthy piece of information for missions in the East. The diplomatic communications between Pope Innocent IV and Pope Gregory X with the Mongols[89] were probably another reason for this endorsement. At the time, there was open discussion of a possible Christian-Mongol alliance with an anti-Islamic function.[90] In fact, a Mongol delegate was solemny baptised at the Second Council of Lyon. At the council, Pope Gregory X promulgated a new Crusade to start in 1278 in liaison with the Mongols.[91]
Since its publication, some have viewed the book with skepticism.[92] Some in the Middle Ages regarded the book simply as a romance or fable, due largely to the sharp difference of its descriptions of a sophisticated civilisation in China to other early accounts by Giovanni da Pian del Carpine and William of Rubruck, who portrayed the Mongols as 'barbarians' who appeared to belong to 'some other world'.[92] Doubts have also been raised in later centuries about Marco Polo's narrative of his travels in China, for example for his failure to mention the Great Wall of China, and in particular the difficulties in identifying many of the place names he used[93] (the great majority, however, have since been identified).[94] Many have questioned whether he had visited the places he mentioned in his itinerary, whether he had appropriated the accounts of his father and uncle or other travellers, and some doubted whether he even reached China, or that if he did, perhaps never went beyond Khanbaliq (Beijing).[93][95]
It has, however, been pointed out that Polo's accounts of China are more accurate and detailed than other travellers' accounts of the period. Polo had at times refuted the 'marvellous' fables and legends given in other European accounts, and despite some exaggerations and errors, Polo's accounts have relatively few of the descriptions of irrational marvels. In many cases of descriptions of events where he was not present (mostly given in the first part before he reached China, such as mentions of Christian miracles), he made a clear distinction that they are what he had heard rather than what he had seen.  It is also largely free of the gross errors found in other accounts such as those given by the Moroccan traveller Ibn Battuta who had confused the Yellow River with the Grand Canal and other waterways, and believed that porcelain was made from coal.[96]
Modern studies have further shown that details given in Marco Polo's book, such as the currencies used, salt productions and revenues, are accurate and unique. Such detailed descriptions are not found in other non-Chinese sources, and their accuracy is supported by archaeological evidence as well as Chinese records compiled after Polo had left China. His accounts are therefore unlikely to have been obtained second hand.[97] Other accounts have also been verified; for example, when visiting Zhenjiang in Jiangsu, China, Marco Polo noted that a large number of Christian churches had been built there. His claim is confirmed by a Chinese text of the 14th century explaining how a Sogdian named Mar-Sargis from Samarkand founded six Nestorian Christian churches there in addition to one in Hangzhou during the second half of the 13th century.[98] His story of the princess Kököchin sent from China to Persia to marry the Īl-khān is also confirmed by independent sources in both Persia and China.[99]
Sceptics have long wondered whether Marco Polo wrote his book based on hearsay, with some pointing to omissions about noteworthy practices and structures of China as well as the lack of details on some places in his book. While Polo describes paper money and the burning of coal, he fails to mention the Great Wall of China, tea, Chinese characters, chopsticks, or footbinding.[100] His failure to note the presence of the Great Wall of China was first raised in the middle of the seventeenth century, and in the middle of the eighteenth century, it was suggested that he might have never reached China.[93]  Later scholars such as John W. Haeger argued that Marco Polo might not have visited Southern China due to the lack of details in his description of southern Chinese cities compared to northern ones, while Herbert Franke also raised the possibility that Marco Polo might not have been to China at all, and wondered if he might have based his accounts on Persian sources due to his use of Persian expressions.[95][101] This is taken further by Frances Wood who claimed in her 1995 book Did Marco Polo Go to China? that at best Polo never went farther east than Persia (modern Iran), and that there is nothing in The Book of Marvels about China that could not be obtained via reading Persian books.[102] Wood maintains that it is more probable that Polo only went to Constantinople (modern Istanbul, Turkey) and some of the Italian merchant colonies around the Black Sea, picking hearsay from those travellers who had been farther east.[102]
Supporters of Polo's basic accuracy countered on the points raised by sceptics such as footbinding and the Great Wall of China. Historian Stephen G. Haw argued that the Great Walls were built to keep out northern invaders, whereas the ruling dynasty during Marco Polo's visit were those very northern invaders. They note that the Great Wall familiar to us today is a Ming structure built some two centuries after Marco Polo's travels; and that the Mongol rulers whom Polo served controlled territories both north and south of today's wall, and would have no reasons to maintain any fortifications that may have remained there from the earlier dynasties.[103] Other Europeans who travelled to Khanbaliq during the Yuan dynasty, such as Giovanni de' Marignolli and Odoric of Pordenone, said nothing about the wall either. The Muslim traveller Ibn Battuta, who asked about the wall when he visited China during the Yuan dynasty, could find no one who had either seen it or knew of anyone who had seen it, suggesting that while ruins of the wall constructed in the earlier periods might have existed, they were not significant or noteworthy at that time.[103]
Haw also argued that footbinding was not common even among Chinese during Polo's time and almost unknown among the Mongols. While the Italian missionary Odoric of Pordenone who visited Yuan China mentioned footbinding (it is however unclear whether he was merely relaying something he had heard as his description is inaccurate),[104] no other foreign visitors to Yuan China mentioned the practice, perhaps an indication that the footbinding was not widespread or was not practised in an extreme form at that time.[105] Marco Polo himself noted (in the Toledo manuscript) the dainty walk of Chinese women who took very short steps.[103] It has also been noted by other scholars that many of the things not mentioned by Marco Polo such as tea and chopsticks were not mentioned by other travellers as well.[40]  Haw also pointed out that despite the few omissions, Marco Polo's account is more extensive, more accurate and more detailed than those of other foreign travellers to China in this period.[106] Marco Polo even observed Chinese nautical inventions such as the watertight compartments of bulkhead partitions in Chinese ships, knowledge of which he was keen to share with his fellow Venetians.[107]
In addition to Haw, a number of other scholars have argued in favour of the established view that Polo was in China in response to Wood's book.[40] The book has been criticized by figures including Igor de Rachewiltz (translator and annotator of The Secret History of the Mongols) and Morris Rossabi (author of Kublai Khan: his life and times).[108] The historian David Morgan points out basic errors made in Wood's book such as confusing the Liao dynasty with the Jin dynasty, and he found no compelling evidence in the book that would convince him that Marco Polo did not go to China.[109] Haw also argues in his book Marco Polo's China that Marco's account is much more correct and accurate than has often been supposed and that it is extremely unlikely that he could have obtained all the information in his book from second-hand sources.[110] Haw also criticizes Wood's approach to finding mention of Marco Polo in Chinese texts by contending that contemporaneous Europeans had little regard for using surnames and that a direct Chinese transliteration of the name "Marco" ignores the possibility of him taking on a Chinese or even Mongol name with no similarity to his Latin name.[111]
Also in reply to Wood, Jørgen Jensen recalled the meeting of Marco Polo and Pietro d'Abano in the late 13th century. During this meeting, Marco gave to Pietro details of the astronomical observations he had made on his journey. These observations are only compatible with Marco's stay in China, Sumatra and the South China Sea[112] and are recorded in Pietro's book Conciliator Differentiarum, but not in Marco's Book of Travels.
Reviewing Haw's book, Peter Jackson (author of The Mongols and the West) has said that Haw "must surely now have settled the controversy surrounding the historicity of Polo's visit to China".[113] Igor de Rachewiltz's review, which refutes Wood's points, concludes with a strongly-worded condemnation: "I regret to say that F. W.'s book falls short of the standard of scholarship that one would expect in a work of this kind. Her book can only be described as deceptive, both in relation to the author and to the public at large. Questions are posed that, in the majority of cases, have already been answered satisfactorily ... her attempt is unprofessional; she is poorly equipped in the basic tools of the trade, i.e., adequate linguistic competence and research methodology ... and her major arguments cannot withstand close scrutiny. Her conclusion fails to consider all the evidence supporting Marco Polo's credibility."[114]
Some scholars believe that Marco Polo exaggerated his importance in China. The British historian David Morgan thought that Polo had likely exaggerated and lied about his status in China,[115] while Ronald Latham believed that such exaggerations were embellishments by his ghostwriter Rustichello da Pisa.[85]
Et meser Marc Pol meisme, celui de cui trate ceste livre, seingneurie ceste cité por trois anz.
And the same Marco Polo, of whom this book relates, ruled this city for three years.
This sentence in The Book of Marvels was interpreted as Marco Polo was "the governor" of the city of "Yangiu" Yangzhou for three years, and later of Hangzhou. This claim has raised some controversy. According to David Morgan  no Chinese source mentions him as either a friend of the Emperor or as the governor of Yangzhou – indeed no Chinese source mentions Marco Polo at all.[115] In fact, in the 1960s the German historian Herbert Franke noted that all occurrences of Po-lo or Bolod in Yuan texts were names of people of Mongol or Turkic extraction.[101]
However, in the 2010s the Chinese scholar Peng Hai identified Marco Polo with a certain "Boluo", a courtier of the emperor, who is mentioned in the Yuanshi ("History of Yuan") since he was arrested in 1274 by an imperial dignitary named Saman. The accusation was that Boluo had walked on the same side of the road as a female courtesan, in contravention of the order for men and women to walk on opposite sides of the road inside the city.[116][need quotation to verify] According to the "Yuanshi" records, Boluo was released at the request of the emperor himself, and was then transferred to the region of Ningxia, in the northeast of present-day China, in the spring of 1275. The date could correspond to the first mission of which Marco Polo speaks.[117]
If this identification is correct, there is a record about Marco Polo in Chinese sources. These conjectures seem to be supported by the fact that in addition to the imperial dignitary Saman (the one who had arrested the official named "Boluo"), the documents mention his brother, Xiangwei. According to sources, Saman died shortly after the incident, while Xiangwei was transferred to Yangzhou in 1282–1283. Marco Polo reports that he was moved to Hangzhou the following year, in 1284. It has been supposed that these displacements are due to the intention to avoid further conflicts between the two.[118]
The sinologist Paul Pelliot thought that Polo might have served as an officer of the government salt monopoly in Yangzhou, which was a position of some significance that could explain the exaggeration.[115]
It may seem unlikely that a European could hold a position of power in the Mongolian empire. However, some records prove he was not the first nor the only one. In his book, Marco mentions an official named "Mar Sarchis" who probably was a Nestorian Christian bishop, and he says he founded two Christian churches in the region of "Caigiu". This official is actually mentioned in the local gazette Zhishun Zhenjian zhi under the name "Ma Xuelijisi" and the qualification of "General of Third Class". Always in the gazette, it is said Ma Xuelijsi was an assistant supervisor in the province of Zhenjiang for three years, and that during this time he founded two Christian churches.[119][120][118] In fact, it is a well-documented fact that Kublai Khan trusted foreigners more than Chinese subjects in internal affairs.[121][118]
Stephen G. Haw challenges this idea that Polo exaggerated his own importance, writing that, "contrary to what has often been said ... Marco does not claim any very exalted position for himself in the Yuan empire."[122] He points out that Polo never claimed to hold high rank, such as a darughachi, who led a tumen – a unit that was normally 10,000 strong. In fact, Polo does not even imply that he had led 1,000 personnel. Haw points out that Polo himself appears to state only that he had been an emissary of the khan, in a position with some esteem. According to Haw, this is a reasonable claim if Polo was, for example, a keshig – a member of the imperial guard by the same name, which included as many as 14,000 individuals at the time.[122]
Haw explains how the earliest manuscripts of Polo's accounts provide contradicting information about his role in Yangzhou, with some stating he was just a simple resident, others stating he was a governor, and Ramusio's manuscript claiming he was simply holding that office as a temporary substitute for someone else, yet all the manuscripts concur that he worked as an esteemed emissary for the khan.[123] Haw also objected to the approach to finding mention of Marco Polo in Chinese texts, contending that contemporaneous Europeans had little regard for using surnames, and a direct Chinese transcription of the name "Marco" ignores the possibility of him taking on a Chinese or even Mongol name that had no bearing or similarity with his Latin name.[122]
Another controversial claim is at chapter 145 when the Book of Marvels states that the three Polos provided the Mongols with technical advice on building mangonels during the Siege of Xiangyang,
Adonc distrent les .II. freres et lor filz meser Marc. "Grant Sire, nos avon avech nos en nostre mesnie homes qe firont tielz mangan qe giteront si grant pieres qe celes de la cité ne poront sofrir mes se renderont maintenant."
Then the two brothers and their son Marc said: "Great Lord, in our entourage we have men who will build such mangonels which launch such great stones, that the inhabitants of the city will not endure it and will immediately surrender."
Since the siege was over in 1273, before Marco Polo had arrived in China for the first time, the claim cannot be true.[115][124]  The Mongol army that besieged Xiangyang did have foreign military engineers, but they were mentioned in Chinese sources as being from Baghdad and had Arabic names.[101] In this respect, Igor de Rachewiltz recalls that the claim that the three Polo were present at the siege of Xiang-yang is not present in all manuscripts, but Niccolò and Matteo could have made this suggestion. Therefore, this claim seems a subsequent addition to give more credibility to the story.[125][40]
A number of errors in Marco Polo's account have been noted: for example, he described the bridge later known as Marco Polo Bridge as having twenty-four arches instead of eleven or thirteen.[40]  He also said that city wall of Khanbaliq had twelve gates when it had only eleven.[126] Archaeologists have also pointed out that Polo may have mixed up the details from the two attempted invasions of Japan by Kublai Khan in 1274 and 1281. Polo wrote of five-masted ships, when archaeological excavations found that the ships, in fact, had only three masts.[127]
Wood accused Marco Polo of taking other people's accounts in his book, retelling other stories as his own, or basing his accounts on Persian guidebooks or other lost sources. For example, Sinologist Francis Woodman Cleaves noted that Polo's account of the voyage of the princess Kököchin from China to Persia to marry the Īl-khān in 1293 has been confirmed by a passage in the 15th-century Chinese work Yongle Encyclopedia and by the Persian historian Rashid-al-Din Hamadani in his work Jami' al-tawarikh.  However, neither of these accounts mentions Polo or indeed any European as part of the bridal party,[99] and Wood used the lack of mention of Polo in these works as an example of Polo's "retelling of a well-known tale".  Morgan, in Polo's defence, noted that even the princess herself was not mentioned in the Chinese source and that it would have been surprising if Polo had been mentioned by Rashid-al-Din.[109] Historian Igor de Rachewiltz strongly criticised Wood's arguments in his review of her book.[128] Rachewiltz argued that Marco Polo's account, in fact, allows the Persian and Chinese sources to be reconciled – by relaying the information that two of the three envoys sent (mentioned in the Chinese source and whose names accord with those given by Polo) had died during the voyage, it explains why only the third who survived, Coja/Khoja, was mentioned by Rashìd al-Dìn. Polo had therefore completed the story by providing information not found in either source. He also noted that the only Persian source that mentions the princess was not completed until 1310–11, therefore Marco Polo could not have learned the information from any Persian book.  According to de Rachewiltz, the concordance of Polo's detailed account of the princess with other independent sources that gave only incomplete information is proof of the veracity of Polo's story and his presence in China.[128]
Morgan writes that since much of what The Book of Marvels has to say about China is "demonstrably correct",  any claim that Polo did not go to China "creates far more problems than it solves", therefore the "balance of probabilities" strongly suggests that Polo really did go to China, even if he exaggerated somewhat his importance in China.[129] Haw dismisses the various anachronistic criticisms of Polo's accounts that started in the 17th century, and highlights Polo's accuracy in great part of his accounts, for example on features of the landscape such as the Grand Canal of China.[130] "If Marco was a liar," Haw writes, "then he must have been an implausibly meticulous one."[131]
In 2012, the University of Tübingen Sinologist and historian Hans Ulrich Vogel released a detailed analysis of Polo's description of currencies, salt production and revenues, and argued that the evidence supports his presence in China because he included details which he could not have otherwise known.[97][132] Vogel noted that no other Western, Arab, or Persian sources have given such accurate and unique details about the currencies of China, for example, the shape and size of the paper, the use of seals, the various denominations of paper money as well as variations in currency usage in different regions of China, such as the use of cowry shells in Yunnan, details supported by archaeological evidence and Chinese sources compiled long after the Polos had left China.[133]  His accounts of salt production and revenues from the salt monopoly are also accurate, and accord with Chinese documents of the Yuan era.[134] Economic historian Mark Elvin, in his preface to Vogel's 2013 monograph, concludes that Vogel "demonstrates by specific example after specific example the ultimately overwhelming probability of the broad authenticity" of Polo's account. Many problems were caused by the oral transmission of the original text and the proliferation of significantly different hand-copied manuscripts. For instance, did Polo exert "political authority" (seignora) in Yangzhou or merely "sojourn" (sejourna) there? Elvin concludes that "those who doubted, although mistaken, were not always being casual or foolish", but "the case as a whole had now been closed": the book is, "in essence, authentic, and, when used with care, in broad terms to be trusted as a serious though obviously not always final, witness."[135]
Other lesser-known European explorers had already travelled to China, such as Giovanni da Pian del Carpine, but Polo's book meant that his journey was the first to be widely known. Christopher Columbus was inspired enough by Polo's description of the Far East to want to visit those lands for himself; a copy of the book was among his belongings, with handwritten annotations.[9] Bento de Góis, inspired by Polo's writings of a Christian kingdom in the east, travelled 4,000 miles (6,400 km) in three years across Central Asia. He never found the kingdom but ended his travels at the Great Wall of China in 1605, proving that Cathay was what Matteo Ricci (1552–1610) called "China".[136]

Marco Polo's travels may have had some influence on the development of European cartography, ultimately leading to the European voyages of exploration a century later.[137] The 1453 Fra Mauro map was said by Giovanni Battista Ramusio (disputed by historian/cartographer Piero Falchetta, in whose work the quote appears) to have been partially based on the one brought from Cathay by Marco Polo: That fine illuminated world map on parchment, which can still be seen in a large cabinet alongside the choir of their monastery [the Camaldolese monastery of San Michele di Murano] was by one of the brothers of the monastery, who took great delight in the study of cosmography, diligently drawn and copied from a most beautiful and very old nautical map and a world map that had been brought from Cathay by the most honourable Messer Marco Polo and his father.Though Marco Polo never produced a map that illustrated his journey, his family drew several maps to the Far East based on the traveller's accounts. These collections of maps were signed by Polo's three daughters, Fantina, Bellela and Moreta.[138] Not only did it contain maps of his journey, but also sea routes to Japan, Siberia's Kamchatka Peninsula, the Bering Strait and even to the coastlines of Alaska, centuries before the rediscovery of the Americas by Europeans.
There is a legend about Marco Polo importing pasta from China; however, it is actually a popular misconception,[139] originating with the Macaroni Journal, published by a food industry association with the goal of promoting the use of pasta in the United States.[140] Marco Polo describes in his book a food similar to "lasagna", but he uses a term with which he was already familiar. In fact, pasta had already been invented in Italy a long time before Marco Polo's travels to Asia.[141] According to the newsletter of the National Macaroni Manufacturers Association[141] and food writer Jeffrey Steingarten,[142] the durum wheat was introduced by Arabs from Libya, during their rule over Sicily in the late 9th century, thus predating Marco Polo's travels by about four centuries.[142] Steingarten also mentioned that Jane Grigson believed the Marco Polo story to have originated in the 1920s or 30s in an advertisement for a Canadian spaghetti company.[142]
The Marco Polo sheep, a subspecies of Ovis ammon, is named after the explorer,[143] who described it during his crossing of Pamir (ancient Mount Imeon) in 1271.[nb 2]
In 1851, a three-masted clipper built in Saint John, New Brunswick also took his name; the Marco Polo was the first ship to sail around the world in under six months.[144]
The airport in Venice is named Venice Marco Polo Airport.[145]
The frequent flyer programme of Hong Kong flag carrier Cathay Pacific is known as the "Marco Polo Club".[146]
Croatian state-owned shipping company's (Jadrolinija) ship connecting Split with Ancona in Italy is named after Marco Polo.[147]
The travels of Marco Polo are fictionalised in a number works, such as:



Michael Faraday FRS (/ˈfærədeɪ, -di/ FARR-ə-day, -⁠dee; 22 September 1791 – 25 August 1867) was an English scientist who contributed to the study of electromagnetism and electrochemistry. His main discoveries include the principles underlying electromagnetic induction, diamagnetism and electrolysis. Although Faraday received little formal education, he was one of the most influential scientists in history.[1] It was by his research on the magnetic field around a conductor carrying a direct current that Faraday established the concept of the electromagnetic field in physics. Faraday also established that magnetism could affect rays of light and that there was an underlying relationship between the two phenomena.[2][3] He similarly discovered the principles of electromagnetic induction, diamagnetism, and the laws of electrolysis. His inventions of electromagnetic rotary devices formed the foundation of electric motor technology, and it was largely due to his efforts that electricity became practical for use in technology.[4]
As a chemist, Faraday discovered benzene, investigated the clathrate hydrate of chlorine, invented an early form of the Bunsen burner and the system of oxidation numbers, and popularised terminology such as "anode", "cathode", "electrode" and "ion". Faraday ultimately became the first and foremost Fullerian Professor of Chemistry at the Royal Institution, a lifetime position. Faraday was an excellent experimentalist who conveyed his ideas in clear and simple language; his mathematical abilities, however, did not extend as far as trigonometry and were limited to the simplest algebra. James Clerk Maxwell took the work of Faraday and others and summarized it in a set of equations which is accepted as the basis of all modern theories of electromagnetic phenomena. On Faraday's uses of lines of force, Maxwell wrote that they show Faraday "to have been in reality a mathematician of a very high order – one from whom the mathematicians of the future may derive valuable and fertile methods."[5] The SI unit of capacitance is named in his honour: the farad.
Albert Einstein kept a picture of Faraday on his study wall, alongside pictures of Arthur Schopenhauer and James Clerk Maxwell.[6] Physicist Ernest Rutherford stated, "When we consider the magnitude and extent of his discoveries and their influence on the progress of science and of industry, there is no honour too great to pay to the memory of Faraday, one of the greatest scientific discoverers of all time."[1]
Michael Faraday was born on 22 September 1791 in Newington Butts,[7] Surrey (which is now part of the London Borough of Southwark).[8]  His family was not well off. His father, James, was a member of the Glasite sect of Christianity. James Faraday moved his wife, Margaret (née Hastwell),[9] and two children to London during the winter of 1790 from Outhgill in Westmorland, where he had been an apprentice to the village blacksmith.[10]  Michael was born in the autumn of that year. The young Michael Faraday, who was the third of four children, having only the most basic school education, had to educate himself.[11]
At the age of 14 he became an apprentice to George Riebau, a local bookbinder and bookseller in Blandford Street.[12] During his seven-year apprenticeship Faraday read many books, including Isaac Watts's The Improvement of the Mind, and he enthusiastically implemented the principles and suggestions contained therein.[13] During this period, Faraday held discussions with his peers in the City Philosophical Society where he attended lectures about various scientific topics.[14] He also developed an interest in science, especially in electricity. Faraday was particularly inspired by the book Conversations on Chemistry by Jane Marcet.[15][16]
In 1812, at the age of 20 and at the end of his apprenticeship, Faraday attended lectures by the eminent English chemist Humphry Davy of the Royal Institution and the Royal Society, and John Tatum, founder of the City Philosophical Society. Many of the tickets for these lectures were given to Faraday by William Dance, who was one of the founders of the Royal Philharmonic Society. Faraday subsequently sent Davy a 300-page book based on notes that he had taken during these lectures. Davy's reply was immediate, kind, and favourable. In 1813, when Davy damaged his eyesight in an accident with nitrogen trichloride, he decided to employ Faraday as an assistant. Coincidentally one of the Royal Institution's assistants, John Payne, was sacked and Sir Humphry Davy had been asked to find a replacement; thus he appointed Faraday as Chemical Assistant at the Royal Institution on 1 March 1813.[2] Very soon Davy entrusted Faraday with the preparation of nitrogen trichloride samples, and they both were injured in an explosion of this very sensitive substance.[17]
Faraday married Sarah Barnard (1800–1879) on 12 June 1821.[18] They met through their families at the Sandemanian church, and he confessed his faith to the Sandemanian congregation the month after they were married. They had no children.[7]
Faraday was a devout Christian; his Sandemanian denomination was an offshoot of the Church of Scotland. Well after his marriage, he served as deacon and for two terms as an elder in the meeting house of his youth. His church was located at Paul's Alley in the Barbican. This meeting house relocated in 1862 to Barnsbury Grove, Islington; this North London location was where Faraday served the final two years of his second term as elder prior to his resignation from that post.[19][20]  Biographers have noted that "a strong sense of the unity of God and nature pervaded Faraday's life and work."[21]
In June 1832, the University of Oxford granted Faraday an honorary Doctor of Civil Law degree. During his lifetime, he was offered a knighthood in recognition for his services to science, which he turned down on religious grounds, believing that it was against the word of the Bible to accumulate riches and pursue worldly reward, and stating that he preferred to remain "plain Mr Faraday to the end".[22] Elected a Fellow of the Royal Society in 1824, he twice refused to become President.[23] He became the first Fullerian Professor of Chemistry at the Royal Institution in 1833.[24]
In 1832, Faraday was elected a Foreign Honorary Member of the American Academy of Arts and Sciences.[25] He was elected a foreign member of the Royal Swedish Academy of Sciences in 1838. In 1840, he was elected to the American Philosophical Society.[26] He was one of eight foreign members elected to the French Academy of Sciences in 1844.[27] In 1849 he was elected as associated member to the Royal Institute of the Netherlands, which two years later became the Royal Netherlands Academy of Arts and Sciences and he was subsequently made foreign member.[28]
Faraday suffered a nervous breakdown in 1839 but eventually returned to his investigations into electromagnetism.[29] In 1848, as a result of representations by the Prince Consort, Faraday was awarded a grace and favour house in Hampton Court in Middlesex, free of all expenses and upkeep. This was the Master Mason's House, later called Faraday House, and now No. 37 Hampton Court Road. In 1858 Faraday retired to live there.[30]
Having provided a number of various service projects for the British government, when asked by the government to advise on the production of chemical weapons for use in the Crimean War (1853–1856), Faraday refused to participate, citing ethical reasons.[31]
Faraday died at his house at Hampton Court on 25 August 1867, aged 75.[32] He had some years before turned down an offer of burial in Westminster Abbey upon his death, but he has a memorial plaque there, near Isaac Newton's tomb.[33] Faraday was interred in the dissenters' (non-Anglican) section of Highgate Cemetery.[34]
Faraday's earliest chemical work was as an assistant to Humphry Davy. Faraday was involved in the study of chlorine; he discovered two new compounds of chlorine and carbon. He also conducted the first rough experiments on the diffusion of gases, a phenomenon that was first pointed out by John Dalton. The physical importance of this phenomenon was more fully revealed by Thomas Graham and Joseph Loschmidt. Faraday succeeded in liquefying several gases, investigated the alloys of steel, and produced several new kinds of glass intended for optical purposes. A specimen of one of these heavy glasses subsequently became historically important; when the glass was placed in a magnetic field Faraday determined the rotation of the plane of polarisation of light. This specimen was also the first substance found to be repelled by the poles of a magnet.[citation needed]
Faraday invented an early form of what was to become the Bunsen burner, which is still in practical use in science laboratories around the world as a convenient source of heat.[35][36]
Faraday worked extensively in the field of chemistry, discovering chemical substances such as benzene (which he called bicarburet of hydrogen) and liquefying gases such as chlorine. The liquefying of gases helped to establish that gases are the vapours of liquids possessing a very low boiling point and gave a more solid basis to the concept of molecular aggregation. In 1820 Faraday reported the first synthesis of compounds made from carbon and chlorine, C2Cl6 and C2Cl4, and published his results the following year.[37][38][39] Faraday also determined the composition of the chlorine clathrate hydrate, which had been discovered by Humphry Davy in 1810.[40][41] Faraday is also responsible for discovering the laws of electrolysis, and for popularizing terminology such as anode, cathode, electrode, and ion, terms proposed in large part by William Whewell.[42]
Faraday was the first to report what later came to be called metallic nanoparticles. In 1847 he discovered that the optical properties of gold colloids differed from those of the corresponding bulk metal. This was probably the first reported observation of the effects of quantum size, and might be considered to be the birth of nanoscience.[43]
Faraday is best known for his work on electricity and magnetism. His first recorded experiment was the construction of a voltaic pile with seven British halfpenny coins, stacked together with seven discs of sheet zinc, and six pieces of paper moistened with salt water. With this pile he decomposed sulfate of magnesia (first letter to Abbott, 12 July 1812).
In 1821, soon after the Danish physicist and chemist Hans Christian Ørsted discovered the phenomenon of electromagnetism, Davy and William Hyde Wollaston tried, but failed, to design an electric motor.[3] Faraday, having discussed the problem with the two men, went on to build two devices to produce what he called "electromagnetic rotation". One of these, now known as the homopolar motor, caused a continuous circular motion that was engendered by the circular magnetic force around a wire that extended into a pool of mercury wherein was placed a magnet; the wire would then rotate around the magnet if supplied with current from a chemical battery. These experiments and inventions formed the foundation of modern electromagnetic technology. In his excitement, Faraday published results without acknowledging his work with either Wollaston or Davy. The resulting controversy within the Royal Society strained his mentor relationship with Davy and may well have contributed to Faraday's assignment to other activities, which consequently prevented his involvement in electromagnetic research for several years.[45][46]
From his initial discovery in 1821, Faraday continued his laboratory work, exploring electromagnetic properties of materials and developing requisite experience. In 1824, Faraday briefly set up a circuit to study whether a magnetic field could regulate the flow of a current in an adjacent wire, but he found no such relationship.[47] This experiment followed similar work conducted with light and magnets three years earlier that yielded identical results.[48][49] During the next seven years, Faraday spent much of his time perfecting his recipe for optical quality (heavy) glass, borosilicate of lead,[50] which he used in his future studies connecting light with magnetism.[51]  In his spare time, Faraday continued publishing his experimental work on optics and electromagnetism; he conducted correspondence with scientists whom he had met on his journeys across Europe with Davy, and who were also working on electromagnetism.[52] Two years after the death of Davy, in 1831, he began his great series of experiments in which he discovered electromagnetic induction, recording in his laboratory diary on 28 October 1831 he was; "making many experiments with the great magnet of the Royal Society".[53]
Faraday's breakthrough came when he wrapped two insulated coils of wire around an iron ring, and found that, upon passing a current through one coil, a momentary current was induced in the other coil.[3] This phenomenon is now known as mutual induction.[54] The iron ring-coil apparatus is still on display at the Royal Institution. In subsequent experiments, he found that if he moved a magnet through a loop of wire an electric current flowed in that wire. The current also flowed if the loop was moved over a stationary magnet. His demonstrations established that a changing magnetic field produces an electric field; this relation was modelled mathematically by James Clerk Maxwell as Faraday's law, which subsequently became one of the four Maxwell equations, and which have in turn evolved into the generalization known today as field theory.[55] Faraday would later use the principles he had discovered to construct the electric dynamo, the ancestor of modern power generators and the electric motor.[56]
In 1832, he completed a series of experiments aimed at investigating the fundamental nature of electricity; Faraday used "static", batteries, and "animal electricity" to produce the phenomena of electrostatic attraction, electrolysis, magnetism, etc. He concluded that, contrary to the scientific opinion of the time, the divisions between the various "kinds" of electricity were illusory. Faraday instead proposed that only a single "electricity" exists, and the changing values of quantity and intensity (current and voltage) would produce different groups of phenomena.[3]
Near the end of his career, Faraday proposed that electromagnetic forces extended into the empty space around the conductor.[55] This idea was rejected by his fellow scientists, and Faraday did not live to see the eventual acceptance of his proposition by the scientific community. Faraday's concept of lines of flux emanating from charged bodies and magnets provided a way to visualize electric and magnetic fields; that conceptual model was crucial for the successful development of the electromechanical devices that dominated engineering and industry for the remainder of the 19th century.[citation needed]
In 1845, Faraday discovered that many materials exhibit a weak repulsion from a magnetic field: a phenomenon he termed diamagnetism.[58]
Faraday also discovered that the plane of polarization of linearly polarized light can be rotated by the application of an external magnetic field aligned with the direction in which the light is moving. This is now termed the Faraday effect.[55] In Sept 1845 he wrote in his notebook, "I have at last succeeded in illuminating a magnetic curve or line of force and in magnetising a ray of light".[59]
Later on in his life, in 1862, Faraday used a spectroscope to search for a different alteration of light, the change of spectral lines by an applied magnetic field. The equipment available to him was, however, insufficient for a definite determination of spectral change. Pieter Zeeman later used an improved apparatus to study the same phenomenon, publishing his results in 1897 and receiving the 1902 Nobel Prize in Physics for his success. In both his 1897 paper[60] and his Nobel acceptance speech,[61] Zeeman made reference to Faraday's work.
In his work on static electricity, Faraday's ice pail experiment demonstrated that the charge resided only on the exterior of a charged conductor, and exterior charge had no influence on anything enclosed within a conductor. This is because the exterior charges redistribute such that the interior fields emanating from them cancel one another. This shielding effect is used in what is now known as a Faraday cage.[55] In January 1836, Faraday had put a wooden frame, 12ft square, on four glass supports and added paper walls and wire mesh. He then stepped inside and electrified it. When he stepped out of his electrified cage, Faraday had shown that electricity was a force, not an imponderable fluid as was believed at the time.[4]
Faraday had a long association with the Royal Institution of Great Britain.  He was appointed Assistant Superintendent of the House of the Royal Institution in 1821.[62]  He was elected a Fellow of the Royal Society in 1824.[7] In 1825, he became Director of the Laboratory of the Royal Institution.[62]  Six years later, in 1833, Faraday became the first Fullerian Professor of Chemistry at the Royal Institution of Great Britain, a position to which he was appointed for life without the obligation to deliver lectures. His sponsor and mentor was John 'Mad Jack' Fuller, who created the position at the Royal Institution for Faraday.[63]
Beyond his scientific research into areas such as chemistry, electricity, and magnetism at the Royal Institution, Faraday undertook numerous, and often time-consuming, service projects for private enterprise and the British government. This work included investigations of explosions in coal mines, being an expert witness in court, and along with two engineers from Chance Brothers c.1853, the preparation of high-quality optical glass, which was required by Chance for its lighthouses. In 1846, together with Charles Lyell, he produced a lengthy and detailed report on a serious explosion in the colliery at Haswell, County Durham, which killed 95 miners.[64] Their report was a meticulous forensic investigation and indicated that coal dust contributed to the severity of the explosion.[64] The first-time explosions had been linked to dust, Faraday gave a demonstration during a lecture on how ventilation could prevent it. The report should have warned coal owners of the hazard of coal dust explosions, but the risk was ignored for over 60 years until the 1913 Senghenydd Colliery Disaster.[64]
As a respected scientist in a nation with strong maritime interests, Faraday spent extensive amounts of time on projects such as the construction and operation of lighthouses and protecting the bottoms of ships from corrosion. His workshop still stands at Trinity Buoy Wharf above the Chain and Buoy Store, next to London's only lighthouse where he carried out the first experiments in electric lighting for lighthouses.[65]
Faraday was also active in what would now be called environmental science, or engineering. He investigated industrial pollution at Swansea and was consulted on air pollution at the Royal Mint. In July 1855, Faraday wrote a letter to The Times on the subject of the foul condition of the River Thames, which resulted in an often-reprinted cartoon in Punch. (See also The Great Stink).[66]
Faraday assisted with the planning and judging of exhibits for the Great Exhibition of 1851 in London.[67] He also advised the National Gallery on the cleaning and protection of its art collection, and served on the National Gallery Site Commission in 1857.[68][69] Education was another of Faraday's areas of service; he lectured on the topic in 1854 at the Royal Institution,[70] and, in 1862, he appeared before a Public Schools Commission to give his views on education in Great Britain. Faraday also weighed in negatively on the public's fascination with table-turning,[71][72] mesmerism, and seances, and in so doing chastised both the public and the nation's educational system.[73]
Before his famous Christmas lectures, Faraday delivered chemistry lectures for the City Philosophical Society from 1816 to 1818 in order to refine the quality of his lectures.[74] 
Between 1827 and 1860 at the Royal Institution in London, Faraday gave a series of nineteen Christmas lectures for young people, a series which continues today. The objective of the lectures was to present science to the general public in the hopes of inspiring them and generating revenue for the Royal Institution. They were notable events on the social calendar among London's gentry. Over the course of several letters to his close friend Benjamin Abbott, Faraday outlined his recommendations on the art of lecturing, writing "a flame should be lighted at the commencement and kept alive with unremitting splendour to the end".[75] His lectures were joyful and juvenile, he delighted in filling soap bubbles with various gasses (in order to determine whether or not they are magnetic), but the lectures were also deeply philosophical. In his lectures he urged his audiences to consider the mechanics of his experiments: "you know very well that ice floats upon water ... Why does the ice float? Think of that, and philosophise".[76] The subjects in his lectures consisted of Chemistry and Electricity, and included: 1841: The Rudiments of Chemistry, 1843: First Principles of Electricity, 1848: The Chemical History of a Candle, 1851: Attractive Forces, 1853: Voltaic Electricity, 1854: The Chemistry of Combustion, 1855: The Distinctive Properties of the Common Metals, 1857: Static Electricity, 1858: The Metallic Properties, 1859: The Various Forces of Matter and their Relations to Each Other.[77]
A statue of Faraday stands in Savoy Place, London, outside the Institution of Engineering and Technology. The Michael Faraday Memorial, designed by brutalist architect Rodney Gordon and completed in 1961, is at the Elephant & Castle gyratory system, near Faraday's birthplace at Newington Butts, London. Faraday School is located on Trinity Buoy Wharf where his workshop still stands above the Chain and Buoy Store, next to London's only lighthouse.[78] Faraday Gardens is a small park in Walworth, London, not far from his birthplace at Newington Butts. It lies within the local council ward of Faraday in the London Borough of Southwark. Michael Faraday Primary school is situated on the Aylesbury Estate in Walworth.[79]
A building at London South Bank University, which houses the institute's electrical engineering departments is named the Faraday Wing, due to its proximity to Faraday's birthplace in Newington Butts. A hall at Loughborough University was named after Faraday in 1960. Near the entrance to its dining hall is a bronze casting, which depicts the symbol of an electrical transformer, and inside there hangs a portrait, both in Faraday's honour. An eight-story building at the University of Edinburgh's science & engineering campus is named for Faraday, as is a recently built hall of accommodation at Brunel University, the main engineering building at Swansea University, and the instructional and experimental physics building at Northern Illinois University. The former UK Faraday Station in Antarctica was named after him.[80]
Without such freedom there would have been no Shakespeare, no Goethe, no Newton, no Faraday, no Pasteur and no Lister.
—Albert Einstein's speech on intellectual freedom at the Royal Albert Hall, London having fled Nazi Germany, 3 October 1933[81]
Streets named for Faraday can be found in many British cities (e.g., London, Fife, Swindon, Basingstoke, Nottingham, Whitby, Kirkby, Crawley, Newbury, Swansea, Aylesbury and Stevenage) as well as in France (Paris), Germany (Berlin-Dahlem, Hermsdorf), Canada (Quebec City, Quebec; Deep River, Ontario; Ottawa, Ontario), the United States (Reston, Virginia), and New Zealand (Hawke's Bay).[82]
A Royal Society of Arts blue plaque, unveiled in 1876, commemorates Faraday at 48 Blandford Street in London's Marylebone district.[83] From 1991 until 2001, Faraday's picture featured on the reverse of Series E £20 banknotes issued by the Bank of England. He was portrayed conducting a lecture at the Royal Institution with the magneto-electric spark apparatus.[84] In 2002, Faraday was ranked number 22 in the BBC's list of the 100 Greatest Britons following a UK-wide vote.[85]
Faraday has been commemorated on postage stamps issued by the Royal Mail. In 1991, as a pioneer of electricity he featured in their Scientific Achievements issue along with pioneers in three other fields (Charles Babbage (computing), Frank Whittle (jet engine) and Robert Watson-Watt (radar)).[86] In 1999, under the title "Faraday's Electricity", he featured in their World Changers issue along with Charles Darwin, Edward Jenner and Alan Turing.[87]
The Faraday Institute for Science and Religion derives its name from the scientist, who saw his faith as integral to his scientific research. The logo of the institute is also based on Faraday's discoveries. It was created in 2006 by a $2,000,000 grant from the John Templeton Foundation to carry out academic research, to foster understanding of the interaction between science and religion, and to engage public understanding in both these subject areas.[88][89]
The Faraday Institution, an independent energy storage research institute established in 2017, also derives its name from Michael Faraday.[90] The organisation serves as the UK's primary research programme to advance battery science and technology, education, public engagement and market research.[90]
Faraday's life and contributions to electromagnetics was the principal topic of the tenth episode, titled "The Electric Boy", of the 2014 American science documentary series, Cosmos: A Spacetime Odyssey, which was broadcast on Fox and the National Geographic Channel.[91]
Aldous Huxley wrote about Faraday in an essay entitled, A Night in Pietramala: "He is always the natural philosopher. To discover truth is his sole aim and interest ... even if I could be Shakespeare, I think I should still choose to be Faraday."[92] Calling Faraday her "hero", in a speech to the Royal Society, Margaret Thatcher declared: "The value of his work must be higher than the capitalisation of all the shares on the Stock Exchange!" She borrowed his bust from the Royal Institution and had it placed in the hall of 10 Downing Street.[4]
In honor and remembrance of his great scientific contributions, several institutions have created prizes and awards in his name. This include:
Michael Faraday in his laboratory, c. 1850s
Michael Faraday's study at the Royal Institution
Michael Faraday's flat at the Royal Institution
Artist Harriet Jane Moore who documented Faraday's life in watercolours
Faraday's books, with the exception of Chemical Manipulation, were collections of scientific papers or transcriptions of lectures.[97]  Since his death, Faraday's diary has been published, as have several large volumes of his letters and Faraday's journal from his travels with Davy in 1813–1815.
Volumes 1-3 of Michael Faraday's "Experimental researches in electricity," from 1839, 1844, and 1855, respectively
Title page of Volume 1 of Michael Faraday's "Experimental researches in electricity," 1839
First page of Volume 1 of Michael Faraday's "Experimental researches in electricity," 1839





Nikola Tesla (/ˈtɛslə/; Serbian Cyrillic: Никола Тесла,[2] pronounced [nǐkola têsla];[a] 10 July [O.S. 28 June] 1856 – 7 January 1943) was a Serbian-American[5][6] inventor, electrical engineer, mechanical engineer, and futurist best known for his contributions to the design of the modern alternating current (AC) electricity supply system.[7]
Born and raised in the Austrian Empire, Tesla studied engineering and physics in the 1870s without receiving a degree, gaining practical experience in the early 1880s working in telephony and at Continental Edison in the new electric power industry. In 1884 he emigrated to the United States, where he became a naturalized citizen. He worked for a short time at the Edison Machine Works in New York City before he struck out on his own. With the help of partners to finance and market his ideas, Tesla set up laboratories and companies in New York to develop a range of electrical and mechanical devices. His alternating current (AC) induction motor and related polyphase AC patents, licensed by Westinghouse Electric in 1888, earned him a considerable amount of money and became the cornerstone of the polyphase system which that company eventually marketed.
Attempting to develop inventions he could patent and market, Tesla conducted a range of experiments with mechanical oscillators/generators, electrical discharge tubes, and early X-ray imaging. He also built a wirelessly controlled boat, one of the first ever exhibited. Tesla became well known as an inventor and demonstrated his achievements to celebrities and wealthy patrons at his lab, and was noted for his showmanship at public lectures. Throughout the 1890s, Tesla pursued his ideas for wireless lighting and worldwide wireless electric power distribution in his high-voltage, high-frequency power experiments in New York and Colorado Springs. In 1893, he made pronouncements on the possibility of wireless communication with his devices. Tesla tried to put these ideas to practical use in his unfinished Wardenclyffe Tower project, an intercontinental wireless communication and power transmitter, but ran out of funding before he could complete it.
After Wardenclyffe, Tesla experimented with a series of inventions in the 1910s and 1920s with varying degrees of success. Having spent most of his money, Tesla lived in a series of New York hotels, leaving behind unpaid bills. He died in New York City in January 1943.[8] Tesla's work fell into relative obscurity following his death, until 1960, when the General Conference on Weights and Measures named the SI unit of magnetic flux density the tesla in his honor. There has been a resurgence in popular interest in Tesla since the 1990s.[9]
Nikola Tesla was born an ethnic Serb in the village of Smiljan, within the Military Frontier, in the Austrian Empire (present day Croatia), on 10 July [O.S. 28 June] 1856.[11][12] His father, Milutin Tesla (1819–1879),[13] was a priest of the Eastern Orthodox Church.[14][15][16][17]
Tesla's mother, Đuka Mandić (1822–1892), whose father was also an Eastern Orthodox Church priest,[18] had a talent for making home craft tools and mechanical appliances and the ability to memorize Serbian epic poems. Đuka had never received a formal education. Tesla credited his eidetic memory and creative abilities to his mother's genetics and influence.[19][20] Tesla's ancestors were from western Serbia, near Montenegro.[21]
Tesla was the fourth of five children. He had three sisters, Milka, Angelina, and Marica, and an older brother named Dane, who was killed in a horse riding accident when Tesla was aged five.[22] In 1861, Tesla attended primary school in Smiljan where he studied German, arithmetic, and religion. In 1862, the Tesla family moved to the nearby Gospić, where Tesla's father worked as parish priest. Nikola completed primary school, followed by middle school. In 1870, Tesla moved to Karlovac[23][better source needed] to attend high school at the Higher Real Gymnasium where the classes were held in German, as it was usual throughout schools within the Austro-Hungarian Military Frontier.[24][25]
Tesla later wrote that he became interested in demonstrations of electricity by his physics professor.[26] Tesla noted that these demonstrations of this "mysterious phenomena" made him want "to know more of this wonderful force".[27] Tesla was able to perform integral calculus in his head, which prompted his teachers to believe that he was cheating.[28] He finished a four-year term in three years, graduating in 1873.[29]
After graduating Tesla returned to Smiljan but soon contracted cholera, was bedridden for nine months and was near death multiple times. In a moment of despair, Tesla's father (who had originally wanted him to enter the priesthood),[30] promised to send him to the best engineering school if he recovered from the illness.[23][better source needed]
The next year Tesla evaded conscription into the Austro-Hungarian Army in Smiljan[31] by running away southeast of Lika to Tomingaj, near Gračac. There he explored the mountains wearing hunter's garb. Tesla said that this contact with nature made him stronger, both physically and mentally. He read many books while in Tomingaj and later said that Mark Twain's works had helped him to miraculously recover from his earlier illness.[23][better source needed]
He enrolled at the Imperial-Royal Technical College in Graz in 1875 on a Military Frontier scholarship. In his autobiography Tesla said he worked hard and earned the highest grades possible, passed nine exams[23][better source needed] (nearly twice as many as required[32]) and received a letter of commendation from the dean of the technical faculty to his father, which stated, "Your son is a star of first rank."[32] At Graz, Tesla noted his fascination with the detailed lectures on electricity presented by Professor Jakob Pöschl and described how he made suggestions on improving the design of an electric motor the professor was demonstrating.[23][better source needed][33] But by his third year he was failing in school and never graduated, leaving Graz in December 1878. One biographer suggests Tesla wasn't studying and may have been expelled for gambling and womanizing.[31]
Tesla's family did not hear from him after he left school.[31] There was a rumor amongst his classmates that he had drowned in the nearby Mur River[34] but in January one of them ran into Tesla in the town of Maribor across the border in Slovenia and reported that encounter to Tesla's family.[35] It turned out Tesla had been working there as a draftsman for 60 florins per month.[31][36] In March 1879, Milutin finally located his son and tried to convince him to return home and take up his education in Prague.[35] Tesla returned to Gospić later that month when he was deported for not having a residence permit.[35] Tesla's father died the next month, on 17 April 1879, at the age of 60 after an unspecified illness.[35] During the rest of the year Tesla taught a large class of students in his old school in Gospić.
In January 1880, two of Tesla's uncles put together enough money to help him leave Gospić for Prague, where he was to study. He arrived too late to enroll at Charles-Ferdinand University; he had never studied Greek, a required subject; and he was illiterate in Czech, another required subject. Tesla did, however, attend lectures in philosophy at the university as an auditor but he did not receive grades for the courses.[37][38]
Tesla moved to Budapest, Hungary, in 1881 to work under Tivadar Puskás at a telegraph company, the Budapest Telephone Exchange. Upon arrival, Tesla realized that the company, then under construction, was not functional, so he worked as a draftsman in the Central Telegraph Office instead. Within a few months, the Budapest Telephone Exchange became functional, and Tesla was allocated the chief electrician position. During his employment, Tesla made many improvements to the Central Station equipment and claimed to have perfected a telephone repeater or amplifier, which was never patented nor publicly described.[23][better source needed]
In 1882, Tivadar Puskás got Tesla another job in Paris with the Continental Edison Company.[39] Tesla began working in what was then a brand new industry, installing indoor incandescent lighting citywide in large scale electric power utility. The company had several subdivisions and Tesla worked at the Société Electrique Edison, the division in the Ivry-sur-Seine suburb of Paris in charge of installing the lighting system. There he gained a great deal of practical experience in electrical engineering. Management took notice of his advanced knowledge in engineering and physics and soon had him designing and building improved versions of generating dynamos and motors.[40] They also sent him on to troubleshoot engineering problems at other Edison utilities being built around France and in Germany.
In 1884, Edison manager Charles Batchelor, who had been overseeing the Paris installation, was brought back to the United States to manage the Edison Machine Works, a manufacturing division situated in New York City, and asked that Tesla be brought to the United States as well.[42] In June 1884, Tesla emigrated[43] and began working almost immediately at the Machine Works on Manhattan's Lower East Side, an overcrowded shop with a workforce of several hundred machinists, laborers, managing staff, and 20 "field engineers" struggling with the task of building the large electric utility in that city.[44] As in Paris, Tesla was working on troubleshooting installations and improving generators.[45] Historian W. Bernard Carlson notes Tesla may have met company founder Thomas Edison only a couple of times.[44] One of those times was noted in Tesla's autobiography where, after staying up all night repairing the damaged dynamos on the ocean liner SS Oregon, he ran into Batchelor and Edison, who made a quip about their "Parisian" being out all night. After Tesla told them he had been up all night fixing the Oregon, Edison commented to Batchelor that "this is a damned good man".[41] One of the projects given to Tesla was to develop an arc lamp-based street lighting system.[46][47] Arc lighting was the most popular type of street lighting but it required high voltages and was incompatible with the Edison low-voltage incandescent system, causing the company to lose contracts in some cities. Tesla's designs were never put into production, possibly because of technical improvements in incandescent street lighting or because of an installation deal that Edison made with an arc lighting company.[48]
Tesla had been working at the Machine Works for a total of six months when he quit.[44] What event precipitated his leaving is unclear. It may have been over a bonus he did not receive, either for redesigning generators or for the arc lighting system that was shelved.[46] Tesla had previous run-ins with the Edison company over unpaid bonuses he believed he had earned.[49][50] In his autobiography, Tesla stated the manager of the Edison Machine Works offered a $50,000 bonus to design "twenty-four different types of standard machines" "but it turned out to be a practical joke".[51] Later versions of this story have Thomas Edison himself offering and then reneging on the deal, quipping "Tesla, you don't understand our American humor".[52][53] The size of the bonus in either story has been noted as odd since Machine Works manager Batchelor was stingy with pay[54] and the company did not have that amount of cash (equal to $1,628,519 today) on hand.[55][56] Tesla's diary contains just one comment on what happened at the end of his employment, a note he scrawled across the two pages covering 7 December 1884, to 4 January 1885, saying "Good by to the Edison Machine Works".[47][57]
Soon after leaving the Edison company, Tesla was working on patenting an arc lighting system,[58] possibly the same one he had developed at Edison.[44] In March 1885, he met with patent attorney Lemuel W. Serrell, the same attorney used by Edison, to obtain help with submitting the patents.[58] Serrell introduced Tesla to two businessmen, Robert Lane and Benjamin Vail, who agreed to finance an arc lighting manufacturing and utility company in Tesla's name, the Tesla Electric Light and Manufacturing Company.[59] Tesla worked for the rest of the year obtaining the patents that included an improved DC generator, the first patents issued to Tesla in the US, and building and installing the system in Rahway, New Jersey.[60] Tesla's new system gained notice in the technical press, which commented on its advanced features.
The investors showed little interest in Tesla's ideas for new types of alternating current motors and electrical transmission equipment. After the utility was up and running in 1886, they decided that the manufacturing side of the business was too competitive and opted to simply run an electric utility.[61] They formed a new utility company, abandoning Tesla's company and leaving the inventor penniless.[61] Tesla even lost control of the patents he had generated, since he had assigned them to the company in exchange for stock.[61] He had to work at various electrical repair jobs and as a ditch digger for $2 per day. Later in life Tesla recounted that part of 1886 as a time of hardship, writing "My high education in various branches of science, mechanics and literature seemed to me like a mockery".[61][62]
In late 1886, Tesla met Alfred S. Brown, a Western Union superintendent, and New York attorney Charles Fletcher Peck.[63] The two men were experienced in setting up companies and promoting inventions and patents for financial gain.[64] Based on Tesla's new ideas for electrical equipment, including a thermo-magnetic motor idea,[65] they agreed to back the inventor financially and handle his patents. Together they formed the Tesla Electric Company in April 1887, with an agreement that profits from generated patents would go 1⁄3 to Tesla, 1⁄3 to Peck and Brown, and 1⁄3 to fund development.[64] They set up a laboratory for Tesla at 89 Liberty Street in Manhattan, where he worked on improving and developing new types of electric motors, generators, and other devices.
In 1887, Tesla developed an induction motor that ran on alternating current (AC), a power system format that was rapidly expanding in Europe and the United States because of its advantages in long-distance, high-voltage transmission. The motor used polyphase current, which generated a rotating magnetic field to turn the motor (a principle that Tesla claimed to have conceived in 1882).[66][67][68] This innovative electric motor, patented in May 1888, was a simple self-starting design that did not need a commutator, thus avoiding sparking and the high maintenance of constantly servicing and replacing mechanical brushes.[69][70]
Along with getting the motor patented, Peck and Brown arranged to get the motor publicized, starting with independent testing to verify it was a functional improvement, followed by press releases sent to technical publications for articles to run concurrently with the issue of the patent.[71] Physicist William Arnold Anthony (who tested the motor) and Electrical World magazine editor Thomas Commerford Martin arranged for Tesla to demonstrate his AC motor on 16 May 1888 at the American Institute of Electrical Engineers.[71][72] Engineers working for the Westinghouse Electric & Manufacturing Company reported to George Westinghouse that Tesla had a viable AC motor and related power system—something Westinghouse needed for the alternating current system he was already marketing. Westinghouse looked into getting a patent on a similar commutator-less, rotating magnetic field-based induction motor developed in 1885 and presented in a paper in March 1888 by Italian physicist Galileo Ferraris, but decided that Tesla's patent would probably control the market.[73][74]
In July 1888, Brown and Peck negotiated a licensing deal with George Westinghouse for Tesla's polyphase induction motor and transformer designs for $60,000 in cash and stock and a royalty of $2.50 per AC horsepower produced by each motor. Westinghouse also hired Tesla for one year for the large fee of $2,000 ($65,100 in today's dollars[75]) per month to be a consultant at the Westinghouse Electric & Manufacturing Company's Pittsburgh labs.[76]
During that year, Tesla worked in Pittsburgh, helping to create an alternating current system to power the city's streetcars. He found it a frustrating period because of conflicts with the other Westinghouse engineers over how best to implement AC power. Between them, they settled on a 60-cycle AC system that Tesla proposed (to match the working frequency of Tesla's motor), but they soon found that it would not work for streetcars, since Tesla's induction motor could run only at a constant speed. They ended up using a DC traction motor instead.[77][78]
Tesla's demonstration of his induction motor and Westinghouse's subsequent licensing of the patent, both in 1888, came at the time of extreme competition between electric companies.[79][80] The three big firms, Westinghouse, Edison, and Thomson-Houston Electric Company, were trying to grow in a capital-intensive business while financially undercutting each other. There was even a "war of currents" propaganda campaign going on, with Edison Electric claiming their direct current system was better and safer than the Westinghouse alternating current system and Thomson-Houston sometimes siding with Edison.[81][82] Competing in this market meant Westinghouse would not have the cash or engineering resources to develop Tesla's motor and the related polyphase system right away.[83]
Two years after signing the Tesla contract, Westinghouse Electric was in trouble. The near collapse of Barings Bank in London triggered the financial panic of 1890, causing investors to call in their loans to Westinghouse Electric.[84] The sudden cash shortage forced the company to refinance its debts. The new lenders demanded that Westinghouse cut back on what looked like excessive spending on acquisition of other companies, research, and patents, including the per motor royalty in the Tesla contract.[85][86] At that point, the Tesla induction motor had been unsuccessful and was stuck in development.[83][84] Westinghouse was paying a $15,000-a-year guaranteed royalty[87] even though operating examples of the motor were rare and polyphase power systems needed to run it was even rarer.[69][84] In early 1891, George Westinghouse explained his financial difficulties to Tesla in stark terms, saying that, if he did not meet the demands of his lenders, he would no longer be in control of Westinghouse Electric and Tesla would have to "deal with the bankers" to try to collect future royalties.[88] The advantages of having Westinghouse continue to champion the motor probably seemed obvious to Tesla and he agreed to release the company from the royalty payment clause in the contract.[88][89] Six years later Westinghouse purchased Tesla's patent for a lump sum payment of $216,000 as part of a patent-sharing agreement signed with General Electric (a company created from the 1892 merger of Edison and Thomson-Houston).[90][91][92]
The money Tesla made from licensing his AC patents made him independently wealthy and gave him the time and funds to pursue his own interests.[93] In 1889, Tesla moved out of the Liberty Street shop Peck and Brown had rented and for the next dozen years worked out of a series of workshop/laboratory spaces in Manhattan. These included a lab at 175 Grand Street (1889–1892), the fourth floor of 33–35 South Fifth Avenue (1892–1895), and sixth and seventh floors of 46 & 48 East Houston Street (1895–1902).[94][95] Tesla and his hired staff conducted some of his most significant work in these workshops.
In the summer of 1889, Tesla traveled to the 1889 Exposition Universelle in Paris and learned of Heinrich Hertz's 1886–1888 experiments that proved the existence of electromagnetic radiation, including radio waves.[96] Tesla found this new discovery "refreshing" and decided to explore it more fully. In repeating and then expanding on these experiments Tesla tried powering a Ruhmkorff coil with a high speed alternator he had been developing as part of an improved arc lighting system but found that the high-frequency current overheated the iron core and melted the insulation between the primary and secondary windings in the coil. To fix this problem Tesla came up with his "oscillating transformer", with an air gap instead of insulating material between the primary and secondary windings and an iron core that could be moved to different positions in or out of the coil.[97] Later called the Tesla coil, it would be used to produce high-voltage, low-current, high frequency alternating-current electricity.[98] He would use this resonant transformer circuit in his later wireless power work.[99][100]
On 30 July 1891, aged 35, Tesla became a naturalized citizen of the United States.[101][102] In the same year, he patented his Tesla coil.[103]
After 1890, Tesla experimented with transmitting power by inductive and capacitive coupling using high AC voltages generated with his Tesla coil.[104] He attempted to develop a wireless lighting system based on near-field inductive and capacitive coupling and conducted a series of public demonstrations where he lit Geissler tubes and even incandescent light bulbs from across a stage.[105] He spent most of the decade working on variations of this new form of lighting with the help of various investors but none of the ventures succeeded in making a commercial product out of his findings.[106]
In 1893 at St. Louis, Missouri, the Franklin Institute in Philadelphia, Pennsylvania and the National Electric Light Association, Tesla told onlookers that he was sure a system like his could eventually conduct "intelligible signals or perhaps even power to any distance without the use of wires" by conducting it through the Earth.[107][108]
Tesla served as a vice-president of the American Institute of Electrical Engineers from 1892 to 1894, the forerunner of the modern-day IEEE (along with the Institute of Radio Engineers).[109]
By the beginning of 1893, Westinghouse engineer Charles F. Scott and then Benjamin G. Lamme had made progress on an efficient version of Tesla's induction motor. Lamme found a way to make the polyphase system it would need compatible with older single-phase AC and DC systems by developing a rotary converter.[110] Westinghouse Electric now had a way to provide electricity to all potential customers and started branding their polyphase AC system as the "Tesla Polyphase System". They believed that Tesla's patents gave them patent priority over other polyphase AC systems.[111]
Westinghouse Electric asked Tesla to participate in the 1893 World's Columbian Exposition in Chicago where the company had a large space in the "Electricity Building" devoted to electrical exhibits. Westinghouse Electric won the bid to light the Exposition with alternating current and it was a key event in the history of AC power, as the company demonstrated to the American public the safety, reliability, and efficiency of an alternating current system that was polyphase and could also supply the other AC and DC exhibits at the fair.[112][113][114]
A special exhibit space was set up to display various forms and models of Tesla's induction motor. The rotating magnetic field that drove them was explained through a series of demonstrations including an Egg of Columbus that used the two-phase coil found in an induction motor to spin a copper egg making it stand on end.[115]
Tesla visited the fair for a week during its six-month run to attend the International Electrical Congress and put on a series of demonstrations at the Westinghouse exhibit.[116][117] A specially darkened room had been set up where Tesla showed his wireless lighting system, using a demonstration he had previously performed throughout America and Europe;[118] these included using high-voltage, high-frequency alternating current to light wireless gas-discharge lamps.[119]
An observer noted:
Within the room were suspended two hard-rubber plates covered with tin foil. These were about fifteen feet apart and served as terminals of the wires leading from the transformers. When the current was turned on, the lamps or tubes, which had no wires connected to them, but lay on a table between the suspended plates, or which might be held in the hand in almost any part of the room, were made luminous. These were the same experiments and the same apparatus shown by Tesla in London about two years previous, "where they produced so much wonder and astonishment".[120]During his presentation at the International Electrical Congress in the Columbian Exposition Agriculture Hall, Tesla introduced his steam powered reciprocating electricity generator that he patented that year, something he thought was a better way to generate alternating current.[121] Steam was forced into the oscillator and rushed out through a series of ports, pushing a piston up and down that was attached to an armature. The magnetic armature vibrated up and down at high speed, producing an alternating magnetic field. This induced alternating electric current in the wire coils located adjacent. It did away with the complicated parts of a steam engine/generator, but never caught on as a feasible engineering solution to generate electricity.[122][123]
In 1893, Edward Dean Adams, who headed the Niagara Falls Cataract Construction Company, sought Tesla's opinion on what system would be best to transmit power generated at the falls. Over several years, there had been a series of proposals and open competitions on how best to do it. Among the systems proposed by several US and European companies were two-phase and three-phase AC, high-voltage DC, and compressed air. Adams asked Tesla for information about the current state of all the competing systems. Tesla advised Adams that a two-phased system would be the most reliable and that there was a Westinghouse system to light incandescent bulbs using two-phase alternating current. The company awarded a contract to Westinghouse Electric for building a two-phase AC generating system at the Niagara Falls, based on Tesla's advice and Westinghouse's demonstration at the Columbian Exposition. At the same time, a further contract was awarded to General Electric to build the AC distribution system.[124]
In 1895, Edward Dean Adams, impressed with what he saw when he toured Tesla's lab, agreed to help found the Nikola Tesla Company, set up to fund, develop, and market a variety of previous Tesla patents and inventions as well as new ones. Alfred Brown signed on, bringing along patents developed under Peck and Brown. The board was filled out with William Birch Rankine and Charles F. Coaney.[125] It found few investors since the mid-1890s were a tough time financially, and the wireless lighting and oscillators patents it was set up to market never panned out. The company handled Tesla's patents for decades to come.
In the early morning hours of 13 March 1895, the South Fifth Avenue building that housed Tesla's lab caught fire. It started in the basement of the building and was so intense Tesla's 4th-floor lab burned and collapsed into the second floor. The fire not only set back Tesla's ongoing projects, but it also destroyed a collection of early notes and research material, models, and demonstration pieces, including many that had been exhibited at the 1893 Worlds Colombian Exposition. Tesla told The New York Times "I am in too much grief to talk. What can I say?".[126]  After the fire Tesla moved to 46 & 48 East Houston Street and rebuilt his lab on the 6th and 7th floors.
Starting in 1894, Tesla began investigating what he referred to as radiant energy of "invisible" kinds after he had noticed damaged film in his laboratory in previous experiments[127] (later identified as "Roentgen rays" or "X-rays"). His early experiments were with Crookes tubes, a cold cathode electrical discharge tube. Tesla may have inadvertently captured an X-ray image—predating, by a few weeks, Wilhelm Röntgen's December 1895 announcement of the discovery of X-rays—when he tried to photograph Mark Twain illuminated by a Geissler tube, an earlier type of gas discharge tube. The only thing captured in the image was the metal locking screw on the camera lens.[128]
In March 1896, after hearing of Röntgen's discovery of X-ray and X-ray imaging (radiography),[129] Tesla proceeded to do his own experiments in X-ray imaging, developing a high-energy single-terminal vacuum tube of his own design that had no target electrode and that worked from the output of the Tesla coil (the modern term for the phenomenon produced by this device is bremsstrahlung or braking radiation). In his research, Tesla devised several experimental setups to produce X-rays. Tesla held that, with his circuits, the "instrument will ... enable one to generate Roentgen rays of much greater power than obtainable with ordinary apparatus".[130]
Tesla noted the hazards of working with his circuit and single-node X-ray-producing devices. In his many notes on the early investigation of this phenomenon, he attributed the skin damage to various causes. He believed early on that damage to the skin was not caused by the Roentgen rays, but by the ozone generated in contact with the skin, and to a lesser extent, by nitrous acid. Tesla incorrectly believed that X-rays were longitudinal waves, such as those produced in waves in plasmas. These plasma waves can occur in force-free magnetic fields.[131][132]
On 11 July 1934, the New York Herald Tribune published an article on Tesla, in which he recalled an event that occasionally took place while experimenting with his single-electrode vacuum tubes. A minute particle would break off the cathode, pass out of the tube, and physically strike him:[133]
Tesla said he could feel a sharp stinging pain where it entered his body, and again at the place where it passed out. In comparing these particles with the bits of metal projected by his "electric gun", Tesla said, "The particles in the beam of force ... will travel much faster than such particles ... and they will travel in concentrations".
In 1898, Tesla demonstrated a boat that used a coherer-based radio control—which he dubbed "telautomaton"—to the public during an electrical exhibition at Madison Square Garden.[135] Tesla tried to sell his idea to the US military as a type of radio-controlled torpedo, but they showed little interest.[136] Remote radio control remained a novelty until World War I and afterward, when a number of countries used it in military programs.[137] Tesla took the opportunity to further demonstrate "Teleautomatics" in an address to a meeting of the Commercial Club in Chicago, while he was travelling to Colorado Springs, on 13 May 1899.
From the 1890s through 1906, Tesla spent a great deal of his time and fortune on a series of projects trying to develop the transmission of electrical power without wires. It was an expansion of his idea of using coils to transmit power that he had been demonstrating in wireless lighting. He saw this as not only a way to transmit large amounts of power around the world but also, as he had pointed out in his earlier lectures, a way to transmit worldwide communications.
At the time Tesla was formulating his ideas, there was no feasible way to wirelessly transmit communication signals over long distances, let alone large amounts of power. Tesla had studied radio waves early on, and came to the conclusion that part of the existing study on them, by Hertz, was incorrect.[138][139][140] Also, this new form of radiation was widely considered at the time to be a short-distance phenomenon that seemed to die out in less than a mile.[141] Tesla noted that, even if theories on radio waves were true, they were totally worthless for his intended purposes since this form of "invisible light" would diminish over a distance just like any other radiation and would travel in straight lines right out into space, becoming "hopelessly lost".[142]
By the mid-1890s, Tesla was working on the idea that he might be able to conduct electricity long distance through the Earth or the atmosphere, and began working on experiments to test this idea including setting up a large resonance transformer magnifying transmitter in his East Houston Street lab.[143][144][145] Seeming to borrow from a common idea at the time that the Earth's atmosphere was conductive,[146][147] he proposed a system composed of balloons suspending, transmitting, and receiving, electrodes in the air above 30,000 feet (9,100 m) in altitude, where he thought the lower pressure would allow him to send high voltages (millions of volts) long distances.
To further study the conductive nature of low-pressure air, Tesla set up an experimental station at high altitude in Colorado Springs during 1899.[148][149][150][151]  There he could safely operate much larger coils than in the cramped confines of his New York lab, and an associate had made an arrangement for the El Paso Power Company to supply alternating current free of charge.[151] To fund his experiments, he convinced John Jacob Astor IV to invest $100,000 ($3,517,600 in today's dollars[75]) to become a majority shareholder in the Nikola Tesla Company. Astor thought he was primarily investing in the new wireless lighting system. Instead, Tesla used the money to fund his Colorado Springs experiments.[152] Upon his arrival, he told reporters that he planned to conduct wireless telegraphy experiments, transmitting signals from Pikes Peak to Paris.[153]
There, he conducted experiments with a large coil operating in the megavolts range, producing artificial lightning (and thunder) consisting of millions of volts and discharges of up to 135 feet (41 m) in length,[155] and, at one point, inadvertently burned out the generator in El Paso, causing a power outage.[156] The observations he made of the electronic noise of lightning strikes led him to (incorrectly) conclude[157][158] that he could use the entire globe of the Earth to conduct electrical energy.
During his time at his laboratory, Tesla observed unusual signals from his receiver which he speculated to be communications from another planet. He mentioned them in a letter to a reporter in December 1899[159] and to the Red Cross Society in December 1900.[160][161] Reporters treated it as a sensational story and jumped to the conclusion Tesla was hearing signals from Mars.[160] He expanded on the signals he heard in a 9 February 1901 Collier's Weekly article entitled "Talking With Planets", where he said it had not been immediately apparent to him that he was hearing "intelligently controlled signals" and that the signals could have come from Mars, Venus, or other planets.[161] It has been hypothesized that he may have intercepted Guglielmo Marconi's European experiments in July 1899—Marconi may have transmitted the letter S (dot/dot/dot) in a naval demonstration, the same three impulses that Tesla hinted at hearing in Colorado[161]—or signals from another experimenter in wireless transmission.[162]
Tesla had an agreement with the editor of The Century Magazine to produce an article on his findings. The magazine sent a photographer to Colorado to photograph the work being done there. The article, titled "The Problem of Increasing Human Energy", appeared in the June 1900 edition of the magazine. He explained the superiority of the wireless system he envisioned but the article was more of a lengthy philosophical treatise than an understandable scientific description of his work,[163] illustrated with what were to become iconic images of Tesla and his Colorado Springs experiments.
Tesla made the rounds in New York trying to find investors for what he thought would be a viable system of wireless transmission, wining and dining them at the Waldorf-Astoria's Palm Garden (the hotel where he was living at the time), The Players Club, and Delmonico's.[164] In March 1901, he obtained $150,000 ($5,276,400 in today's dollars[75]) from J. P. Morgan in return for a 51% share of any generated wireless patents, and began planning the Wardenclyffe Tower facility to be built in Shoreham, New York, 100 miles (161 km) east of the city on the North Shore of Long Island.[165]
By July 1901, Tesla had expanded his plans to build a more powerful transmitter to leap ahead of Marconi's radio-based system, which Tesla thought was a copy of his own.[160] He approached Morgan to ask for more money to build the larger system, but Morgan refused to supply any further funds.[166] In December 1901, Marconi successfully transmitted the letter S from England to Newfoundland, defeating Tesla in the race to be first to complete such a transmission. A month after Marconi's success, Tesla tried to get Morgan to back an even larger plan to transmit messages and power by controlling "vibrations throughout the globe".[160] Over the next five years, Tesla wrote more than 50 letters to Morgan, pleading for and demanding additional funding to complete the construction of Wardenclyffe. Tesla continued the project for another nine months into 1902. The tower was erected to its full height of 187 feet (57 m).[162] In June 1902, Tesla moved his lab operations from Houston Street to Wardenclyffe.[165]
Investors on Wall Street were putting their money into Marconi's system, and some in the press began turning against Tesla's project, claiming it was a hoax.[167] The project came to a halt in 1905, and in 1906, the financial problems and other events may have led to what Tesla biographer Marc J. Seifer suspects was a nervous breakdown on Tesla's part.[168] Tesla mortgaged the Wardenclyffe property to cover his debts at the Waldorf-Astoria, which eventually amounted to $20,000 ($584,300 in today's dollars[75]).[169] He lost the property in foreclosure in 1915, and in 1917 the Tower was demolished by the new owner to make the land a more viable real estate asset.
After Wardenclyffe closed, Tesla continued to write to Morgan; after "the great man" died, Tesla wrote to Morgan's son Jack, trying to get further funding for the project. In 1906, Tesla opened offices at 165 Broadway in Manhattan, trying to raise further funds by developing and marketing his patents. He went on to have offices at the Metropolitan Life Tower from 1910 to 1914; rented for a few months at the Woolworth Building, moving out because he could not afford the rent; and then to office space at 8 West 40th Street from 1915 to 1925. After moving to 8 West 40th Street, he was effectively bankrupt. Most of his patents had run out and he was having trouble with the new inventions he was trying to develop.[170]
On his 50th birthday, in 1906, Tesla demonstrated a 200 horsepower (150 kilowatts) 16,000 rpm bladeless turbine. During 1910–1911, at the Waterside Power Station in New York, several of his bladeless turbine engines were tested at 100–5,000 hp.[171] Tesla worked with several companies including from 1919 to 1922 in Milwaukee, for Allis-Chalmers.[172][173] He spent most of his time trying to perfect the Tesla turbine with Hans Dahlstrand, the head engineer at the company, but engineering difficulties meant it was never made into a practical device.[174] Tesla did license the idea to a precision instrument company and it found use in the form of luxury car speedometers and other instruments.[175]
When World War I broke out, the British cut the transatlantic telegraph cable linking the US to Germany in order to control the flow of information between the two countries. They also tried to shut off German wireless communication to and from the US by having the US Marconi Company sue the German radio company Telefunken for patent infringement.[176] Telefunken brought in the physicists Jonathan Zenneck and Karl Ferdinand Braun for their defense, and hired Tesla as a witness for two years for $1,000 a month. The case stalled and then went moot when the US entered the war against Germany in 1917.[176][177]
In 1915, Tesla attempted to sue the Marconi Company for infringement of his wireless tuning patents. Marconi's initial radio patent had been awarded in the US in 1897, but his 1900 patent submission covering improvements to radio transmission had been rejected several times, before it was finally approved in 1904, on the grounds that it infringed on other existing patents including two 1897 Tesla wireless power tuning patents.[139][178][179] Tesla's 1915 case went nowhere,[180] but in a related case, where the Marconi Company tried to sue the US government over WWI patent infringements, a Supreme Court of the United States 1943 decision restored the prior patents of Oliver Lodge, John Stone, and Tesla.[181] The court declared that their decision had no bearing on Marconi's claim as the first to achieve radio transmission, just that since Marconi's claim to certain patented improvements were questionable, the company could not claim infringement on those same patents.[139][182]
On 6 November 1915, a Reuters news agency report from London had the 1915 Nobel Prize in Physics awarded to Thomas Edison and Nikola Tesla; however, on 15 November, a Reuters story from Stockholm stated the prize that year was being awarded to William Henry Bragg and Lawrence Bragg "for their services in the analysis of crystal structure by means of X-rays".[183][184][185] There were unsubstantiated rumors at the time that either Tesla or Edison had refused the prize.[183] The Nobel Foundation said, "Any rumor that a person has not been given a Nobel Prize because he has made known his intention to refuse the reward is ridiculous"; a recipient could decline a Nobel Prize only after he is announced a winner.[183]
There have been subsequent claims by Tesla biographers that Edison and Tesla were the original recipients and that neither was given the award because of their animosity toward each other; that each sought to minimize the other's achievements and right to win the award; that both refused ever to accept the award if the other received it first; that both rejected any possibility of sharing it; and even that a wealthy Edison refused it to keep Tesla from getting the $20,000 prize money.[20][183]
In the years after these rumors, neither Tesla nor Edison won a Nobel prize (although Edison received one of 38 possible bids in 1915 and Tesla received one of 38 possible bids in 1937).[186]
Tesla won numerous medals and awards over this time. They include:
Tesla attempted to market several devices based on the production of ozone. These included his 1900 Tesla Ozone Company selling an 1896 patented device based on his Tesla coil, used to bubble ozone through different types of oils to make a therapeutic gel.[192] He also tried to develop a variation of this a few years later as a room sanitizer for hospitals.[193]
Tesla theorized that the application of electricity to the brain enhanced intelligence. In 1912, he crafted "a plan to make dull students bright by saturating them unconsciously with electricity," wiring the walls of a schoolroom and, "saturating [the schoolroom] with infinitesimal electric waves vibrating at high frequency. The whole room will thus, Mr. Tesla claims, be converted into a health-giving and stimulating electromagnetic field or 'bath.'"[194] The plan was, at least provisionally, approved by then superintendent of New York City schools, William H. Maxwell.[194]
Before World War I, Tesla sought overseas investors. After the war started, Tesla lost the funding he was receiving from his patents in European countries.
In the August 1917 edition of the magazine Electrical Experimenter, Tesla postulated that electricity could be used to locate submarines via using the reflection of an "electric ray" of "tremendous frequency," with the signal being viewed on a fluorescent screen (a system that has been noted to have a superficial resemblance to modern radar).[195] Tesla was incorrect in his assumption that high-frequency radio waves would penetrate water.[196] Émile Girardeau, who helped develop France's first radar system in the 1930s, noted in 1953 that Tesla's general speculation that a very strong high-frequency signal would be needed was correct. Girardeau said, "(Tesla) was prophesying or dreaming, since he had at his disposal no means of carrying them out, but one must add that if he was dreaming, at least he was dreaming correctly".[197]
In 1928, Tesla received patent, U.S. Patent 1,655,114, for a biplane design capable of vertical take-off and landing (VTOL), which "gradually tilted through manipulation of the elevator devices" in flight until it was flying like a conventional plane.[198] This impractical design was something Tesla thought would sell for less than $1,000.[199][200] 
Tesla had a further office at 350 Madison Ave[201] but by 1928 he no longer had a laboratory or funding.[202]
Tesla lived at the Waldorf Astoria in New York City from 1900 and ran up a large bill.[203] He moved to the St. Regis Hotel in 1922 and followed a pattern from then on of moving to a different hotel every few years and leaving unpaid bills behind.[204][205]
Tesla walked to the park every day to feed the pigeons. He began feeding them at the window of his hotel room and nursed injured birds back to health.[205][206][207] He said that he had been visited by a certain injured white pigeon daily. He spent over $2,000 (equivalent to $34,970 in 2022) to care for the bird, including a device he built to support her comfortably while her broken wing and leg healed.[31] Tesla stated:
I have been feeding pigeons, thousands of them for years. But there was one, a beautiful bird, pure white with light grey tips on its wings; that one was different. It was a female. I had only to wish and call her and she would come flying to me. I loved that pigeon as a man loves a woman, and she loved me. As long as I had her, there was a purpose to my life.[208]Tesla's unpaid bills, as well as complaints about the mess made by pigeons, led to his eviction from St. Regis in 1923. He was also forced to leave the Hotel Pennsylvania in 1930 and the Hotel Governor Clinton in 1934.[205] At one point he also took rooms at the Hotel Marguery.[citation needed]
Tesla moved to the Hotel New Yorker in 1934. At this time Westinghouse Electric & Manufacturing Company began paying him $125 (equivalent to $2,730 in 2022) per month in addition to paying his rent. Accounts of how this came about vary. Several sources claim that Westinghouse was concerned, or possibly warned, about potential bad publicity arising from the impoverished conditions in which their former star inventor was living.[209][210][211][212] The payment has been described as being couched as a "consulting fee" to get around Tesla's aversion to accepting charity. Tesla biographer Marc Seifer described the Westinghouse payments as a type of "unspecified settlement".[211] In any case, Westinghouse provided the funds for Tesla for the rest of his life.[citation needed]
In 1931, a young journalist whom Tesla befriended, Kenneth M. Swezey, organized a celebration for the inventor's 75th birthday.[213] Tesla received congratulations from figures in science and engineering such as Albert Einstein,[214] and he was also featured on the cover of Time magazine.[215] The cover caption "All the world's his power house" noted his contribution to electrical power generation.
The party went so well that Tesla made it an annual event, an occasion where he would put out a large spread of food and drink—featuring dishes of his own creation. He invited the press in order to see his inventions and hear stories about his past exploits, views on current events, and sometimes baffling claims.[216][217]
At the 1932 party, Tesla claimed he had invented a motor that would run on cosmic rays.[217]
In 1933 at age 77, Tesla told reporters at the event that, after 35 years of work, he was on the verge of producing proof of a new form of energy. He claimed it was a theory of energy that was "violently opposed" to Einsteinian physics and could be tapped with an apparatus that would be cheap to run and last 500 years. He also told reporters he was working on a way to transmit individualized private radio wavelengths, working on breakthroughs in metallurgy, and developing a way to photograph the retina to record thought.[218]
At the 1934 occasion, Tesla told reporters he had designed a superweapon he claimed would end all war.[219][220] He called it "teleforce", but was usually referred to as his death ray.[221] Tesla described it as a defensive weapon that would be put up along the border of a country and be used against attacking ground-based infantry or aircraft. Tesla never revealed detailed plans of how the weapon worked during his lifetime but, in 1984, they surfaced at the Nikola Tesla Museum archive in Belgrade.[222] The treatise, The New Art of Projecting Concentrated Non-dispersive Energy through the Natural Media, described an open-ended vacuum tube with a gas jet seal that allows particles to exit, a method of charging slugs of tungsten or mercury to millions of volts, and directing them in streams (through electrostatic repulsion).[217][223] Tesla tried to attract interest of the US War Department,[224] United Kingdom, Soviet Union, and Yugoslavia in the device.[225]
In 1935 at his 79th birthday party, Tesla covered many topics. He claimed to have discovered the cosmic ray in 1896 and invented a way to produce direct current by induction, and made many claims about his mechanical oscillator.[226] Describing the device (which he expected would earn him $100 million within two years) he told reporters that a version of his oscillator had caused an earthquake in his 46 East Houston Street lab and neighboring streets in Lower Manhattan in 1898.[226] He went on to tell reporters his oscillator could destroy the Empire State Building with 5 lbs of air pressure.[227] He also explained a new technique he developed using his oscillators he called "Telegeodynamics", using it to transmit vibrations into the ground that he claimed would work over any distance to be used for communication or locating underground mineral deposits.[133]
In his 1937 Grand Ballroom of Hotel New Yorker event, Tesla received the Order of the White Lion from the Czechoslovak ambassador and a medal from the Yugoslav ambassador.[217] On questions concerning the death ray, Tesla stated, "But it is not an experiment ... I have built, demonstrated and used it. Only a little time will pass before I can give it to the world."
In the fall of 1937 at the age of 81, after midnight one night, Tesla left the Hotel New Yorker to make his regular commute to the cathedral and library to feed the pigeons. While crossing a street a couple of blocks from the hotel, Tesla was unable to dodge a moving taxicab and was thrown to the ground. His back was severely wrenched and three of his ribs were broken in the accident. The full extent of his injuries was never known; Tesla refused to consult a doctor, an almost lifelong custom, and never fully recovered.[32][228]
On 7 January 1943, at the age of 86, Tesla died alone in Room 3327 of the Hotel New Yorker. His body was found by maid Alice Monaghan when she entered Tesla's room, ignoring the "do not disturb" sign that Tesla had placed on his door two days earlier. Assistant medical examiner H.W. Wembley examined the body and ruled that the cause of death had been coronary thrombosis (a type of heart attack).
Two days later the Federal Bureau of Investigation ordered the Alien Property Custodian to seize Tesla's belongings. John G. Trump, a professor at M.I.T. and a well-known electrical engineer serving as a technical aide to the National Defense Research Committee, was called in to analyze the Tesla items. After a three-day investigation, Trump's report concluded that there was nothing which would constitute a hazard in unfriendly hands, stating:
His [Tesla's] thoughts and efforts during at least the past 15 years were primarily of a speculative, philosophical, and somewhat promotional character often concerned with the production and wireless transmission of power; but did not include new, sound, workable principles or methods for realizing such results.[229]In a box purported to contain a part of Tesla's "death ray", Trump found a 45-year-old multidecade resistance box.[230]
On 10 January 1943, New York City mayor Fiorello La Guardia read a eulogy written by Slovene-American author Louis Adamic live over the WNYC radio while violin pieces "Ave Maria" and "Tamo daleko" were played in the background. On 12 January, two thousand people attended a state funeral for Tesla at the Cathedral of St. John the Divine in Manhattan. After the funeral, Tesla's body was taken to the Ferncliff Cemetery in Ardsley, New York, where it was later cremated. The following day, a second service was conducted by prominent priests in the Trinity Chapel (today's Serbian Orthodox Cathedral of Saint Sava) in New York City.
In 1952, following pressure from Tesla's nephew, Sava Kosanović, Tesla's entire estate was shipped to Belgrade in 80 trunks marked N.T. In 1957, Kosanović's secretary Charlotte Muzar transported Tesla's ashes from the United States to Belgrade. The ashes are displayed in a gold-plated sphere on a marble pedestal in the Nikola Tesla Museum.[231]
Tesla obtained around 300 patents worldwide for his inventions.[232] Some of Tesla's patents are not accounted for, and various sources have discovered some that have lain hidden in patent archives. There are a minimum of 278 known patents[232] issued to Tesla in 26 countries. Many of Tesla's patents were in the United States, Britain, and Canada, but many other patents were approved in countries around the globe.[233] Many inventions developed by Tesla were not put into patent protection.
Tesla was 6 feet 2 inches (1.88 m) tall and weighed 142 pounds (64 kg), with almost no weight variance from 1888 to about 1926. His appearance was described by newspaper editor Arthur Brisbane as "almost the tallest, almost the thinnest and certainly the most serious man who goes to Delmonico's regularly".[234][235] He was an elegant, stylish figure in New York City, meticulous in his grooming, clothing, and regimented in his daily activities, an appearance he maintained so as to further his business relationships.[236] He was also described as having light eyes, "very big hands", and "remarkably big" thumbs.[234]
Tesla read many works, memorizing complete books, and supposedly possessed a photographic memory.[237] He was a polyglot, speaking eight languages: Serbo-Croatian, Czech, English, French, German, Hungarian, Italian, and Latin.[238] Tesla related in his autobiography that he experienced detailed moments of inspiration. During his early life, Tesla was repeatedly stricken with illness. Blinding flashes of light would appear before his eyes, often accompanied by visions.[237] Often, the visions were linked to a word or idea he might have come across; at other times they provided the solution to a particular problem he had encountered. Just by hearing the name of an item, he could envision it in realistic detail.[237] Tesla visualized an invention in his mind with extreme precision, including all dimensions, before moving to the construction stage, a technique sometimes known as picture thinking. He typically did not make drawings by hand but worked from memory. Beginning in his childhood, Tesla had frequent flashbacks to events that had happened previously in his life.[237] He noted in his autobiography that this affliction had developed his powers of observation and enabled him to discover a "truth of great importance", namely that every thought he conceived was suggested by an external impression.[51] Tesla further wrote that "deficient observation was merely a form of ignorance and responsible for the many morbid notions and foolish ideas prevailing." 
Tesla was a lifelong bachelor, who had once explained that his chastity was very helpful to his scientific abilities.[237] He once said in earlier years that he felt he could never be worthy enough for a woman, considering women superior in every way. His opinion had started to sway in later years when he felt that women were trying to outdo men and make themselves more dominant. This "new woman" was met with much indignation from Tesla, who felt that women were losing their femininity by trying to be in power. In an interview with the Galveston Daily News on 10 August 1924 he stated, "In place of the soft-voiced, a gentlewoman of my reverent worship, has come the woman who thinks that her chief success in life lies in making herself as much as possible like man—in dress, voice and actions, in sports and achievements of every kind ... The tendency of women to push aside man, supplanting the old spirit of cooperation with him in all the affairs of life, is very disappointing to me."[citation needed] Although he told a reporter in later years that he sometimes felt that by not marrying, he had made too great a sacrifice to his work,[31] Tesla chose to never pursue or engage in any known relationships, instead finding all the stimulation he needed in his work.
Tesla was asocial and prone to seclude himself with his work.[135][239][240] However, when he did engage in social life, many people spoke very positively and admiringly of Tesla. Robert Underwood Johnson described him as attaining a "distinguished sweetness, sincerity, modesty, refinement, generosity, and force".[31] His secretary, Dorothy Skerrit, wrote: "his genial smile and nobility of bearing always denoted the gentlemanly characteristics that were so ingrained in his soul".[236] Tesla's friend, Julian Hawthorne, wrote, "seldom did one meet a scientist or engineer who was also a poet, a philosopher, an appreciator of fine music, a linguist, and a connoisseur of food and drink".[241]
Tesla was a good friend of Francis Marion Crawford, Robert Underwood Johnson,[242] Stanford White,[243] Fritz Lowenstein, George Scherff, and Kenneth Swezey.[244][245][246] In middle age, Tesla became a close friend of Mark Twain; they spent a lot of time together in his lab and elsewhere.[242] Twain notably described Tesla's induction motor invention as "the most valuable patent since the telephone".[247] At a party thrown by actress Sarah Bernhardt in 1896, Tesla met Indian Hindu monk Swami Vivekananda. Vivekananda later wrote that Tesla said he could demonstrate mathematically the relationship between matter and energy, something Vivekananda hoped would give a scientific foundation to Vedantic cosmology.[248][249] The meeting with Swami Vivekananda stimulated Tesla's interest in Eastern Science, which led to Tesla studying Hindu and Vedic philosophy for a number of years.[250] Tesla later wrote an article titled "Man's Greatest Achievenment" using Sanskrit terms akasha and prana to describe the relationship between matter and energy.[251][252] In the late 1920s, Tesla befriended George Sylvester Viereck, a poet, writer, mystic, and later, a Nazi propagandist. Tesla occasionally attended dinner parties held by Viereck and his wife.[253][254]
Tesla could be harsh at times and openly expressed disgust for overweight people, such as when he fired a secretary because of her weight.[255] He was quick to criticize clothing; on several occasions, Tesla directed a subordinate to go home and change her dress.[237] When Thomas Edison died, in 1931, Tesla contributed the only negative opinion to The New York Times, buried in an extensive coverage of Edison's life:
He had no hobby, cared for no sort of amusement of any kind and lived in utter disregard of the most elementary rules of hygiene ... His method was inefficient in the extreme, for an immense ground had to be covered to get anything at all unless blind chance intervened and, at first, I was almost a sorry witness of his doings, knowing that just a little theory and calculation would have saved him 90 percent of the labor. But he had a veritable contempt for book learning and mathematical knowledge, trusting himself entirely to his inventor's instinct and practical American sense.[256]Tesla claimed never to sleep more than two hours per night.[257] However, he did admit to "dozing" from time to time "to recharge his batteries".[258] During his second year of study at Graz, Tesla developed a passionate proficiency for billiards, chess, and card-playing, sometimes spending more than 48 hours in a stretch at a gaming table.[259] On one occasion at his laboratory, Tesla worked for a period of 84 hours without rest.[260] Kenneth Swezey, a journalist whom Tesla had befriended, confirmed that Tesla rarely slept. Swezey recalled one morning when Tesla called him at 3 a.m.: "I was sleeping in my room like one dead ... Suddenly, the telephone ring awakened me ... [Tesla] spoke animatedly, with pauses, [as he] ... work[ed] out a problem, comparing one theory to another, commenting; and when he felt he had arrived at the solution, he suddenly closed the telephone."[258]
Tesla worked every day from 9:00 a.m. until 6:00 p.m. or later, with dinner at exactly 8:10 p.m., at Delmonico's restaurant and later the Waldorf-Astoria Hotel. Tesla then telephoned his dinner order to the headwaiter, who also could be the only one to serve him. "The meal was required to be ready at eight o'clock ... He dined alone, except on the rare occasions when he would give a dinner to a group to meet his social obligations. Tesla then resumed his work, often until 3:00 a.m."[261]
For exercise, Tesla walked between 8 and 10 miles (13 and 16 km) per day. He curled his toes one hundred times for each foot every night, saying that it stimulated his brain cells.[258]
In an interview with newspaper editor Arthur Brisbane, Tesla said that he did not believe in telepathy, stating, "Suppose I made up my mind to murder you," he said, "In a second you would know it. Now, isn't that wonderful? By what process does the mind get at all this?" In the same interview, Tesla said that he believed that all fundamental laws could be reduced to one.[234]
Tesla became a vegetarian in his later years, living on only milk, bread, honey, and vegetable juices.[220][262]
Tesla disagreed with the theory of atoms being composed of smaller subatomic particles, stating there was no such thing as an electron creating an electric charge. He believed that if electrons existed at all, they were some fourth state of matter or "sub-atom" that could exist only in an experimental vacuum and that they had nothing to do with electricity.[263][264] Tesla believed that atoms are immutable—they could not change state or be split in any way. He was a believer in the 19th-century concept of an all-pervasive ether that transmitted electrical energy.[265]
Tesla was generally antagonistic towards theories about the conversion of matter into energy.[266] He was also critical of Einstein's theory of relativity, saying:
I hold that space cannot be curved, for the simple reason that it can have no properties. It might as well be said that God has properties. He has not, but only attributes and these are of our own making. Of properties we can only speak when dealing with matter filling the space. To say that in the presence of large bodies space becomes curved is equivalent to stating that something can act upon nothing. I, for one, refuse to subscribe to such a view.[267]In 1935 he described relativity as "a beggar wrapped in purple whom ignorant people take for a king" and said his own experiments had measured the speed of cosmic rays from Arcturus as fifty times the speed of light.[268]
Tesla claimed to have developed his own physical principle regarding matter and energy that he started working on in 1892,[266] and in 1937, at age 81, claimed in a letter to have completed a "dynamic theory of gravity" that "[would] put an end to idle speculations and false conceptions, as that of curved space". He stated that the theory was "worked out in all details" and that he hoped to soon give it to the world.[269] Further elucidation of his theory was never found in his writings.[270]
Tesla is widely considered by his biographers to have been a humanist in philosophical outlook.[271][272] This did not preclude Tesla, like many of his era, from becoming a proponent of an imposed selective breeding version of eugenics.
Tesla expressed the belief that human "pity" had come to interfere with the natural "ruthless workings of nature". Though his argumentation did not depend on a concept of a "master race" or the inherent superiority of one person over another, he advocated for eugenics. In a 1937 interview he stated:
... man's new sense of pity began to interfere with the ruthless workings of nature. The only method compatible with our notions of civilization and the race is to prevent the breeding of the unfit by sterilization and the deliberate guidance of the mating instinct ... The trend of opinion among eugenists is that we must make marriage more difficult. Certainly no one who is not a desirable parent should be permitted to produce progeny. A century from now it will no more occur to a normal person to mate with a person eugenically unfit than to marry a habitual criminal.[273]In 1926, Tesla commented on the ills of the social subservience of women and the struggle of women toward gender equality, and indicated that humanity's future would be run by "Queen Bees". He believed that women would become the dominant sex in the future.[274]
Tesla made predictions about the relevant issues of a post-World War I environment in a printed article entitled "Science and Discovery are the great Forces which will lead to the Consummation of the War" (20 December 1914).[275] Tesla believed that the League of Nations was not a remedy for the times and issues.[23][better source needed]
Tesla was raised an Orthodox Christian. Later in life he did not consider himself to be a "believer in the orthodox sense", said he opposed religious fanaticism, and said "Buddhism and Christianity are the greatest religions both in number of disciples and in importance."[276] He also said "To me, the universe is simply a great machine which never came into being and never will end" and "what we call 'soul' or 'spirit,' is nothing more than the sum of the functionings of the body. When this functioning ceases, the 'soul' or the 'spirit' ceases likewise."[276]
Tesla wrote a number of books and articles for magazines and journals.[277] Among his books are My Inventions: The Autobiography of Nikola Tesla, compiled and edited by Ben Johnston in 1983 from a series of 1919 magazine articles by Tesla which were republished in 1977; The Fantastic Inventions of Nikola Tesla (1993), compiled and edited by David Hatcher Childress; and The Tesla Papers.
Many of Tesla's writings are freely available online,[278] including the article "The Problem of Increasing Human Energy", published in The Century Magazine in 1900,[279] and the article "Experiments with Alternate Currents of High Potential and High Frequency", published in his book Inventions, Researches and Writings of Nikola Tesla.[280][281]
Tesla's legacy has endured in books, films, radio, TV, music, live theater, comics, and video games. The impact of the technologies invented or envisioned by Tesla is a recurring theme in several types of science fiction.
Footnotes
Citations
Books
 Prodigal Genius: The Life of Nikola Tesla  
ISBN 1-59605-713-0
Publications
Journals
Video


Jean-Paul Charles Aymard Sartre (/ˈsɑːrtrə/, US also /ˈsɑːrt/;[7] French: [saʁtʁ]; 21 June 1905 – 15 April 1980) was a French philosopher, playwright, novelist, screenwriter, political activist, biographer, and literary critic, considered a leading figure in 20th-century French philosophy and Marxism. Sartre was one of the key figures in the philosophy of existentialism (and phenomenology). His work has influenced sociology, critical theory, post-colonial theory, and literary studies, and continues to do so. He was awarded the 1964 Nobel Prize in Literature despite attempting to refuse it, saying that he always declined official honors and that "a writer should not allow himself to be turned into an institution."[8]
Sartre held an open relationship with prominent feminist and fellow existentialist philosopher Simone de Beauvoir. Together, Sartre and de Beauvoir challenged the cultural and social assumptions and expectations of their upbringings, which they considered bourgeois, in both lifestyles and thought. The conflict between oppressive, spiritually destructive conformity (mauvaise foi, literally, 'bad faith') and an "authentic" way of "being" became the dominant theme of Sartre's early work, a theme embodied in his principal philosophical work Being and Nothingness (L'Être et le Néant, 1943).[9] Sartre's introduction to his philosophy is his work Existentialism Is a Humanism (L'existentialisme est un humanisme, 1946), originally presented as a lecture.
Jean-Paul Sartre was born on 21 June 1905 in Paris as the only child of Jean-Baptiste Sartre, an officer of the French Navy, and Anne-Marie (Schweitzer).[10] 
When Sartre was two years old, his father died of an illness, which he most likely contracted in Indochina. Anne-Marie moved back to her parents' house in Meudon, where she raised Sartre with help from her father Charles Schweitzer, a teacher of German who taught Sartre mathematics and introduced him to classical literature at a very early age.[11] When he was twelve, Sartre's mother remarried, and the family moved to La Rochelle, where he was frequently bullied, in part due to the wandering of his blind right eye (sensory exotropia).[12]
As a teenager in the 1920s, Sartre became attracted to philosophy upon reading Henri Bergson's essay Time and Free Will: An Essay on the Immediate Data of Consciousness.[13] He attended the Cours Hattemer, a private school in Paris.[14] He studied and earned certificates in psychology, history of philosophy, logic, general philosophy, ethics and sociology, and physics, as well as his diplôme d'études supérieures [fr] (roughly equivalent to an MA thesis) in Paris at the École Normale Supérieure (ENS), an institution of higher education that was the alma mater for several prominent French thinkers and intellectuals.[15] (His 1928 MA thesis under the title "L'Image dans la vie psychologique: rôle et nature" ["Image in Psychological Life: Role and Nature"] was supervised by Henri Delacroix.)[15] It was at ENS that Sartre began his lifelong, sometimes fractious, friendship with Raymond Aron.[16] Perhaps the most decisive influence on Sartre's philosophical development was his weekly attendance at Alexandre Kojève's seminars, which continued for a number of years.[17]
From his first years in the École normale, Sartre was one of its fiercest pranksters.[18][19] In 1927, his antimilitarist satirical cartoon in the revue of the school, coauthored with Georges Canguilhem, particularly upset the director Gustave Lanson.[20] In the same year, with his comrades Nizan, Larroutis, Baillou and Herland,[21] he organized a media prank following Charles Lindbergh's successful New York City–Paris flight; Sartre & Co. called newspapers and informed them that Lindbergh was going to be awarded an honorary École degree. Many newspapers, including Le Petit Parisien, announced the event on 25 May. Thousands, including journalists and curious spectators, showed up, unaware that what they were witnessing was a stunt involving a Lindbergh look-alike.[20][22][23] The scandal led Lanson to resign.[20]
In 1929 at the École normale, he met Simone de Beauvoir, who studied at the Sorbonne and later went on to become a noted philosopher, writer, and feminist. The two became inseparable and lifelong companions, initiating a romantic relationship,[24] though they were not monogamous.[25] The first time Sartre took the agrégation, he failed. He took it a second time and virtually tied for first place with Beauvoir, although Sartre was eventually awarded first place, with Beauvoir second.[26][27]
From 1931 until 1945, Sartre taught at various lycées of Le Havre (at the Lycée de Le Havre, the present-day Lycée François-Ier (Le Havre) [fr], 1931–1936), Laon (at the Lycée de Laon, 1936–37), and, finally, Paris (at the Lycée Pasteur, 1937–1939, and at the Lycée Condorcet, 1941–1944;[28] see below).
In 1932, Sartre read Voyage au bout de la nuit by Louis-Ferdinand Céline, a book that had a remarkable influence on him.[29]
In 1933–34, he succeeded Raymond Aron at the Institut français d'Allemagne in Berlin where he studied Edmund Husserl's phenomenological philosophy. Aron had already advised him in 1930 to read Emmanuel Levinas's Théorie de l'intuition dans la phénoménologie de Husserl (The Theory of Intuition in Husserl's Phenomenology).[30]
The neo-Hegelian revival led by Alexandre Kojève and Jean Hyppolite in the 1930s inspired a whole generation of French thinkers, including Sartre, to discover Hegel's Phenomenology of Spirit.[31]
In 1939 Sartre was drafted into the French army, where he served as a meteorologist.[32][33] He was captured by German troops in 1940 in Padoux,[34] and he spent nine months as a prisoner of war—in Nancy and finally in Stalag XII-D [fr], Trier, where he wrote his first theatrical piece, Barionà, fils du tonnerre, a drama concerning Christmas. It was during this period of confinement that Sartre read Martin Heidegger's Sein und Zeit, later to become a major influence on his own essay on phenomenological ontology. Because of poor health (he claimed that his poor eyesight and exotropia affected his balance) Sartre was released in April 1941. According to other sources, he escaped after a medical visit to the ophthalmologist.[35] Given civilian status, he recovered his teaching position at Lycée Pasteur near Paris and settled at the Hotel Mistral. In October 1941 he was given a position, previously held by a Jewish teacher who had been forbidden to teach by Vichy law, at Lycée Condorcet in Paris.
After coming back to Paris in May 1941, he participated in the founding of the underground group Socialisme et Liberté ("Socialism and Liberty") with other writers Simone de Beauvoir, Maurice Merleau-Ponty, Jean-Toussaint Desanti, Dominique Desanti, Jean Kanapa, and École Normale students. In spring of 1941, Sartre suggested with "cheerful ferocity" at a meeting that the Socialisme et Liberté assassinate prominent war collaborators like Marcel Déat, but de Beauvoir noted his idea was rejected as "none of us felt qualified to make bombs or hurl grenades".[36] The British historian Ian Ousby observed that the French always had far more hatred for collaborators than they did for the Germans, noting it was French people like Déat that Sartre wanted to assassinate rather than the military governor of France, General Otto von Stülpnagel, and the popular slogan always was "Death to Laval!" rather than "Death to Hitler!".[37] In August Sartre and de Beauvoir went to the French Riviera seeking the support of André Gide and André Malraux. However, both Gide and Malraux were undecided, and this may have been the cause of Sartre's disappointment and discouragement. Socialisme et liberté soon dissolved and Sartre decided to write instead of being involved in active resistance. He then wrote Being and Nothingness, The Flies, and No Exit, none of which were censored by the Germans, and also contributed to both legal and illegal literary magazines.
In his essay "Paris under the Occupation", Sartre wrote that the "correct" behaviour of the Germans had entrapped too many Parisians into complicity with the occupation, accepting what was unnatural as natural:
The Germans did not stride, revolver in hand, through the streets. They did not force civilians to make way for them on the pavement. They would offer seats to old ladies on the Metro. They showed great fondness for children and would pat them on the cheek. They had been told to behave correctly and being well-disciplined, they tried shyly and conscientiously to do so. Some of them even displayed a naive kindness which could find no practical expression.[38]Sartre noted when Wehrmacht soldiers asked Parisians politely in their German-accented French for directions, people usually felt embarrassed and ashamed as they tried their best to help out the Wehrmacht which led Sartre to remark "We could not be natural".[39] French was a language widely taught in German schools and most Germans could speak at least some French. Sartre himself always found it difficult when a Wehrmacht soldier asked him for directions, usually saying he did not know where it was that the soldier wanted to go, but still felt uncomfortable as the very act of speaking to the Wehrmacht meant he had been complicit in the Occupation.[40] Ousby wrote: "But, in however humble a fashion, everyone still had to decide how they were going to cope with life in a fragmenting society ... So Sartre's worries ... about how to react when a German soldier stopped him in the street and asked politely for directions were not as fussily inconsequential as they might sound at first. They were emblematic of how the dilemmas of the Occupation presented themselves in daily life".[40] Sartre wrote the very "correctness" of the Germans caused moral corruption in many people who used the "correct" behavior of the Germans as an excuse for passivity, and the very act of simply trying to live one's day-to-day existence without challenging the occupation aided the "New Order in Europe", which depended upon the passivity of ordinary people to accomplish its goals.[38]
Throughout the occupation, it was German policy to plunder France, and food shortages were always a major problem as the majority of food from the French countryside went to Germany.[41] Sartre wrote about the "languid existence" of the Parisians as people waited obsessively for the one weekly arrival of trucks bringing food from the countryside that the Germans allowed, writing: "Paris would grow peaked and yawn with hunger under the empty sky. Cut off from the rest of the world, fed only through the pity or some ulterior motive, the town led a purely abstract and symbolic life".[41] Sartre himself lived on a diet of rabbits sent to him by a friend of de Beauvoir living in Anjou.[42] The rabbits were usually in an advanced state of decay full of maggots, and despite being hungry, Sartre once threw out one rabbit as uneatable, saying it had more maggots in it than meat.[42] Sartre also remarked that conversations at the Café de Flore between intellectuals had changed, as the fear that one of them might be a mouche (informer) or a writer of the corbeau (anonymous denunciatory letters) meant that no one really said what they meant anymore, imposing self-censorship.[43] Sartre and his friends at the Café de Flore had reasons for their fear; by September 1940, the Abwehr alone had already recruited 32,000 French people to work as mouches while by 1942 the Paris Kommandantur was receiving an average of 1,500 letters/per day sent by the corbeaux.[44]
Sartre wrote under the occupation Paris had become a "sham", resembling the empty wine bottles displayed in shop windows as all of the wine had been exported to Germany, looking like the old Paris, but hollowed out, as what had made Paris special was gone.[45] Paris had almost no cars on the streets during the occupation as the oil went to Germany while the Germans imposed a nightly curfew, which led Sartre to remark that Paris "was peopled by the absent".[46] Sartre also noted that people began to disappear under the occupation, writing:
One day you might phone a friend and the phone would ring for a long time in an empty flat. You would go round and ring the doorbell, but no-one would answer it. If the concierge forced the door, you would find two chairs standing close together in the hall with the fag-ends of German cigarettes on the floor between their legs. If the wife or mother of the man who had vanished had been present at his arrest, she would tell you that he had been taken away by very polite Germans, like those who asked the way in the street. And when she went to ask what had happened to them at the offices in the Avenue Foch or the Rue des Saussaies she would be politely received and sent away with comforting words" [No. 11 Rue des Saussaies was the headquarters of the Gestapo in Paris].[47]Sartre wrote the feldgrau ("field grey") uniforms of the Wehrmacht and the green uniforms of the Order Police which had seemed so alien in 1940 had become accepted, as people were numbed into accepting what Sartre called "a pale, dull green, unobtrusive strain, which the eye almost expected to find among the dark clothes of the civilians".[48] Under the occupation, the French often called the Germans les autres ("the others"), which inspired Sartre's aphorism in his play Huis clos ("No Exit") of "l'enfer, c'est les Autres" ("Hell is other people").[49] Sartre intended the line "l'enfer, c'est les Autres" at least in part to be a dig at the German occupiers.[49]
Sartre was a very active contributor to Combat, a newspaper created during the clandestine period by Albert Camus, a philosopher and author who held similar beliefs. Sartre and de Beauvoir remained friends with Camus until 1951, with the publication of Camus's The Rebel. Sartre wrote extensively post-war about neglected minority groups, namely French Jews and black people. In 1946, he published Anti-Semite and Jew, after having published the first part of the essay, "Portrait de l'antisémite," the year before in Les Temps modernes, No. 3. In the essay, in the course of explaining the etiology of "hate" as the hater's projective fantasies when reflecting on the Jewish question, he attacks antisemitism in France[50] during a time when the Jews who came back from concentration camps were quickly abandoned.[51] In 1947, Sartre published several articles concerning the condition of African Americans in the United States—specifically the racism and discrimination against them in the country—in his second Situations collection. Then, in 1948, for the introduction of Léopold Sédar Senghor's l'Anthologie de la nouvelle poésie nègre et malgache (Anthology of New Negro and Malagasy Poetry), he wrote "Black Orpheus" (re-published in Situations III), a critique of colonialism and racism in light of the philosophy Sartre developed in Being and Nothingness. Later, while Sartre was labeled by some authors as a resistant, the French philosopher and resistant Vladimir Jankelevitch criticized Sartre's lack of political commitment during the German occupation, and interpreted his further struggles for liberty as an attempt to redeem himself. According to Camus, Sartre was a writer who resisted; not a resister who wrote.
In 1945, after the war ended, Sartre moved to an apartment on the rue Bonaparte, where he was to produce most of his subsequent work and where he lived until 1962. It was from there that he helped establish a quarterly literary and political review, Les Temps modernes (Modern Times), in part to popularize his thought.[52] He ceased teaching and devoted his time to writing and political activism. He would draw on his war experiences for his great trilogy of novels, Les Chemins de la Liberté (The Roads to Freedom) (1945–1949).
The first period of Sartre's career, defined in large part by Being and Nothingness (1943), gave way to a second period—when the world was perceived as split into communist and capitalist blocs—of highly publicized political involvement. Sartre tended to glorify the Resistance after the war as the uncompromising expression of morality in action, and recalled that the résistants were a "band of brothers" who had enjoyed "real freedom" in a way that did not exist before nor after the war.[53] Sartre was "merciless" in attacking anyone who had collaborated or remained passive during the German occupation; for instance, criticizing Camus for signing an appeal to spare the collaborationist writer Robert Brasillach from being executed.[53] His 1948 play Les mains sales (Dirty Hands) in particular explored the problem of being a politically "engaged" intellectual. He embraced Marxism but did not join the Communist Party. For a time in the late 1940s, Sartre described French nationalism as "provincial" and in a 1949 essay called for a "United States of Europe".[54] In an essay published in the June 1949 edition of the journal Politique étrangère, Sartre wrote:
If we want French civilization to survive, it must be fitted into the framework of a great European civilization. Why? I have said that civilization is the reflection on a shared situation. In Italy, in France, in Benelux, in Sweden, in Norway, in Germany, in Greece, in Austria, everywhere we find the same problems and the same dangers ... But this cultural polity has prospects only as elements of a policy which defends Europe's cultural autonomy vis-à-vis America and the Soviet Union, but also its political and economic autonomy, with the aim of making Europe a single force between the blocs, not a third bloc, but an autonomous force which will refuse to allow itself to be torn into shreds between American optimism and Russian scientificism.[55]About the Korean War, Sartre wrote: "I have no doubt that the South Korean feudalists and the American imperialists have promoted this war. But I do not doubt either that it was begun by the North Koreans".[56] In July 1950, Sartre wrote in Les Temps Modernes about his and de Beauvoir's attitude to the Soviet Union:
As we were neither members of the [Communist] party nor its avowed sympathizers, it was not our duty to write about Soviet labor camps; we were free to remain aloof from the quarrel over the nature of this system, provided that no events of sociological significance had occurred.[57]Sartre held that the Soviet Union was a "revolutionary" state working for the betterment of humanity and could be criticized only for failing to live up to its own ideals, but that critics had to take in mind that the Soviet state needed to defend itself against a hostile world; by contrast Sartre held that the failures of "bourgeois" states were due to their innate shortcomings.[53] The Swiss journalist François Bondy wrote that, based on a reading of Sartre's numerous essays, speeches and interviews "a simple basic pattern never fails to emerge: social change must be comprehensive and revolutionary" and the parties that promote the revolutionary charges "may be criticized, but only by those who completely identify themselves with its purpose, its struggle and its road to power", deeming Sartre's position to be "existentialist".[53]
Sartre believed at this time in the moral superiority of the Eastern Bloc, arguing that this belief was necessary "to keep hope alive"[58] and opposed any criticism of Soviet Union[59] to the extent that Maurice Merleau-Ponty called him an "ultra-Bolshevik".[60] Sartre's expression "workers of Billancourt must not be deprived of their hopes"[60] (Fr. "il ne faut pas désespérer Billancourt"), became a catchphrase meaning communist activists should not tell the whole truth to the workers in order to avoid decline in their revolutionary enthusiasm.[61]
In 1954, just after Stalin's death, Sartre visited the Soviet Union, which he stated he found a "complete freedom of criticism" while condemning the United States for sinking into "prefascism".[62] Sartre wrote about those Soviet writers expelled from the Soviet Writers' Union "still had the opportunity of rehabilitating themselves by writing better books".[63] Sartre's comments on Hungarian revolution of 1956 are quite representative to his frequently contradictory and changing views. On one hand, Sartre saw in Hungary a true reunification between intellectuals and workers[64] only to criticize it for "losing socialist base".[65]
In 1964 Sartre attacked Khrushchev's "Secret Speech" which condemned the Stalinist repressions and purges. Sartre argued that "the masses were not ready to receive the truth".[66]
In 1973 he argued that "revolutionary authority always needs to get rid of some people that threaten it, and their death is the only way".[67] A number of people, starting from Frank Gibney in 1961, classified Sartre as a "useful idiot" due to his uncritical position.[68]
Sartre came to admire the Polish leader Władysław Gomułka, a man who favored a "Polish road to socialism" and wanted more independence for Poland, but was loyal to the Soviet Union because of the Oder-Neisse line issue.[69] Sartre's newspaper Les Temps Modernes devoted a number of special issues in 1957 and 1958 to Poland under Gomułka, praising him for his reforms.[69] Bondy wrote of the notable contradiction between Sartre's "ultra Bolshevism" as he expressed admiration for the Chinese leader Mao Zedong as the man who led the oppressed masses of the Third World into revolution while also praising more moderate Communist leaders like Gomułka.[69]
As an anti-colonialist, Sartre took a prominent role in the struggle against French rule in Algeria, and the use of torture and concentration camps by the French in Algeria. He became an eminent supporter of the FLN in the Algerian War and was one of the signatories of the Manifeste des 121. Consequently, Sartre became a domestic target of the paramilitary Organisation armée secrète (OAS), escaping two bomb attacks in the early '60s.[70] He later argued in 1959 that each French person was responsible for the collective crimes during the Algerian War of Independence.[71] (He had an Algerian mistress, Arlette Elkaïm, who became his adopted daughter in 1965.) He opposed U.S. involvement in the Vietnam War and, along with Bertrand Russell and others, organized a tribunal intended to expose U.S. war crimes, which became known as the Russell Tribunal in 1967.
His work after Stalin's death, the Critique de la raison dialectique (Critique of Dialectical Reason), appeared in 1960 (a second volume appearing posthumously). In the Critique Sartre set out to give Marxism a more vigorous intellectual defense than it had received until then; he ended by concluding that Marx's notion of "class" as an objective entity was fallacious. Sartre's emphasis on the humanist values in the early works of Marx led to a dispute with a leading leftist intellectual in France in the 1960s, Louis Althusser, who claimed that the ideas of the young Marx were decisively superseded by the "scientific" system of the later Marx. In the late 1950s, Sartre began to argue that the European working classes were too apolitical to carry out the revolution predicated by Marx, and influenced by Frantz Fanon started to argue it was the impoverished masses of the Third World, the "real damned of the earth", who would carry out the revolution.[72] A major theme of Sartre's political essays in the 1960s was of his disgust with the "Americanization" of the French working class who would much rather watch American TV shows dubbed into French than agitate for a revolution.[53]
Sartre went to Cuba in the 1960s to meet Fidel Castro and spoke with Ernesto "Che" Guevara. After Guevara's death, Sartre would declare him to be "not only an intellectual but also the most complete human being of our age"[73] and the "era's most perfect man".[74] Sartre would also compliment Guevara by professing that "he lived his words, spoke his own actions and his story and the story of the world ran parallel".[75] However he stood against the persecution of gays by Castro's government, which he compared to Nazi persecution of the Jews, and said: "In Cuba there are no Jews, but there are homosexuals".[76]
During a collective hunger strike in 1974, Sartre visited Red Army Faction member Andreas Baader in Stammheim Prison and criticized the harsh conditions of imprisonment.[77]
Towards the end of his life, Sartre began to describe himself as a "special kind" of anarchist.[78]
In 1964 Sartre renounced literature in a witty and sardonic account of the first ten years of his life, Les Mots (The Words). The book is an ironic counterblast to Marcel Proust, whose reputation had unexpectedly eclipsed that of André Gide (who had provided the model of littérature engagée for Sartre's generation). Literature, Sartre concluded, functioned ultimately as a bourgeois substitute for real commitment in the world. In October 1964, Sartre was awarded the Nobel Prize in Literature but he declined it. He was the first Nobel laureate to voluntarily decline the prize,[79] and remains one of only two laureates to do so.[80] According to Lars Gyllensten, in the book Minnen, bara minnen ("Memories, Only Memories") published in 2000, Sartre himself or someone close to him got in touch with the Swedish Academy in 1975 with a request for the prize money, but was refused.[81] In 1945, he had refused the Légion d'honneur.[82] The Nobel prize was announced on 22 October 1964; on 14 October, Sartre had written a letter to the Nobel Institute, asking to be removed from the list of nominees, and warning that he would not accept the prize if awarded, but the letter went unread;[83] on 23 October, Le Figaro published a statement by Sartre explaining his refusal. He said he did not wish to be "transformed" by such an award, and did not want to take sides in an East vs. West cultural struggle by accepting an award from a prominent Western cultural institution.[83] Nevertheless, he was that year's prizewinner.[84] 
Though his name was then a household word (as was "existentialism" during the tumultuous 1960s), Sartre remained a simple man with few possessions, actively committed to causes until the end of his life, such as the May 1968 strikes in Paris during the summer of 1968 during which he was arrested for civil disobedience. President Charles de Gaulle intervened and pardoned him, commenting that "you don't arrest Voltaire".[85]
In 1975, when asked how he would like to be remembered, Sartre replied:
I would like [people] to remember Nausea, [my plays] No Exit and The Devil and the Good Lord, and then my two philosophical works, more particularly the second one, Critique of Dialectical Reason. Then my essay on Genet, Saint Genet. ... If these are remembered, that would be quite an achievement, and I don't ask for more. As a man, if a certain Jean-Paul Sartre is remembered, I would like people to remember the milieu or historical situation in which I lived, ... how I lived in it, in terms of all the aspirations which I tried to gather up within myself.[86]Sartre's physical condition deteriorated, partially because of the merciless pace of work (and the use of amphetamine)[87] he put himself through during the writing of the Critique and a massive analytical biography of Gustave Flaubert (The Family Idiot), both of which remained unfinished. He had hypertension,[88] and became almost completely blind in 1973. Sartre was a notorious chain smoker, which could also have contributed to the deterioration of his health.[89]
According to Pierre Victor (a.k.a. Benny Levy), who spent much of his time with the dying Sartre and interviewed him on several of his views, Sartre had a drastic change of mind about the existence of God and started gravitating toward Messianic Judaism. This is Sartre's before-death profession, according to Pierre Victor: "I do not feel that I am the product of chance, a speck of dust in the universe, but someone who was expected, prepared, prefigured. In short, a being whom only a Creator could put here; and this idea of a creating hand refers to god."[90] Simone de Beauvoir later revealed her anger at his change of mind by stating, "How should one explain this senile act of a turncoat? All my friends, all the Sartreans, and the editorial team of Les Temps Modernes supported me in my consternation."
Sartre died on 15 April 1980 in Paris from pulmonary edema. He had not wanted to be buried at Père-Lachaise Cemetery between his mother and stepfather, so it was arranged that he be buried at Montparnasse Cemetery. At his funeral on Saturday, 19 April, 50,000 Parisians descended onto boulevard du Montparnasse to accompany Sartre's cortege.[91][92] The funeral started at "the hospital at 2:00 p.m., then filed through the fourteenth arrondissement, past all Sartre's haunts, and entered the cemetery through the gate on the Boulevard Edgar Quinet". Sartre was initially buried in a temporary grave to the left of the cemetery gate.[93] Four days later the body was disinterred for cremation at Père-Lachaise Cemetery, and his ashes were reburied at the permanent site in Montparnasse Cemetery, to the right of the cemetery gate.[94]
Sartre's primary idea is that people, as humans, are "condemned to be free".[95] "This may seem paradoxical because condemnation is normally an external judgment which constitutes the conclusion of a judgment. Here, it is not the human who has chosen to be like this. There is a contingency of human existence. It is a condemnation of their being. Their being is not determined, so it is up to everyone to create their own existence, for which they are then responsible. They cannot not be free, there is a form of necessity for freedom, which can never be given up."[96]
This theory relies upon his position that there is no creator, and is illustrated using the example of the paper cutter. Sartre says that if one considered a paper cutter, one would assume that the creator would have had a plan for it: an essence. Sartre said that human beings have no essence before their existence because there is no Creator. Thus: "existence precedes essence".[95] This forms the basis for his assertion that because one cannot explain one's own actions and behavior by referring to any specific human nature, they are necessarily fully responsible for those actions. "We are left alone, without excuse." "We can act without being determined by our past which is always separated from us."[97]
Sartre maintained that the concepts of authenticity and individuality have to be earned but not learned. We need to experience "death consciousness" so as to wake up ourselves as to what is really important; the authentic in our lives which is life experience, not knowledge.[98] Death draws the final point when we as beings cease to live for ourselves and permanently become objects that exist only for the outside world.[99] In this way death emphasizes the burden of our free, individual existence. "We can oppose authenticity to an inauthentic way of being. Authenticity consists in experiencing the indeterminate character of existence in anguish. It is also to know how to face it by giving meaning to our actions and by recognizing ourselves as the author of this meaning. On the other hand, an inauthentic way of being consists in running away, in lying to oneself in order to escape this anguish and the responsibility for one's own existence."[96]
While Sartre had been influenced by Heidegger, the publication of Being and Nothingness did mark a split in their perspectives, with Heidegger remarking in Letter on Humanism,
Existentialism says existence precedes essence. In this statement he is taking existentia and essentia according to their metaphysical meaning, which, from Plato's time on, has said that essentia precedes existentia. Sartre reverses this statement. But the reversal of a metaphysical statement remains a metaphysical statement. With it, he stays with metaphysics, in oblivion of the truth of Being.[100]Herbert Marcuse also had issues with Sartre's opposition to metaphysics in Being and Nothingness and suggested the work projected anxiety and meaninglessness onto the nature of existence itself: "Insofar as Existentialism is a philosophical doctrine, it remains an idealistic doctrine: it hypostatizes specific historical conditions of human existence into ontological and metaphysical characteristics. Existentialism thus becomes part of the very ideology which it attacks, and its radicalism is illusory."[101] Sartre also took inspiration from phenomenological epistemology, explained by Franz Adler in this way: "Man chooses and makes himself by acting. Any action implies the judgment that he is right under the circumstances not only for the actor, but also for everybody else in similar circumstances."[102] Also important is Sartre's analysis of psychological concepts, including his suggestion that consciousness exists as something other than itself, and that the conscious awareness of things is not limited to their knowledge: for Sartre intentionality applies to the emotions as well as to cognitions, to desires as well as to perceptions.[103] "When an external object is perceived, consciousness is also conscious of itself, even if consciousness is not its own object: it is a non-positional consciousness of itself."[104] However his critique of psychoanalysis, particularly of Freud has faced some counter-critique. Richard Wollheim and Thomas Baldwin argued that Sartre's attempt to show that Sigmund Freud's theory of the unconscious is mistaken was based on a misinterpretation of Freud.[105][106]
While the broad focus of Sartre's life revolved around the notion of human freedom, he began a sustained intellectual participation in more public matters towards the end of the Second World War, around 1944–1945.[107] Before World War II, he was content with the role of an apolitical liberal intellectual: "Now teaching at a lycée in Laon ... Sartre made his headquarters the Dome café at the crossing of Montparnasse and Raspail boulevards. He attended plays, read novels, and dined [with] women. He wrote. And he was published."[108] Sartre and his lifelong companion, de Beauvoir, existed, in her words, where "the world about us was a mere backdrop against which our private lives were played out".[109]
The war opened Sartre's eyes to a political reality he had not yet understood until forced into continual engagement with it: "the world itself destroyed Sartre's illusions about isolated self-determining individuals and made clear his own personal stake in the events of the time."[110] Returning to Paris in 1941 he formed the "Socialisme et Liberté" resistance group. In 1943, after the group disbanded, Sartre joined a writers' Resistance group,[111] in which he remained an active participant until the end of the war. He continued to write ferociously, and it was due to this "crucial experience of war and captivity that Sartre began to try to build up a positive moral system and to express it through literature".[112]
The symbolic initiation of this new phase in Sartre's work is packaged in the introduction he wrote for a new journal, Les Temps modernes, in October 1945. Here he aligned the journal, and thus himself, with the Left and called for writers to express their political commitment.[113] Yet, this alignment was indefinite, directed more to the concept of the Left than a specific party of the Left.
Sartre's philosophy lent itself to his being a public intellectual. He envisaged culture as a very fluid concept; neither pre-determined, nor definitely finished; instead, in true existential fashion, "culture was always conceived as a process of continual invention and re-invention." This marks Sartre, the intellectual, as a pragmatist, willing to move and shift stance along with events. He did not dogmatically follow a cause other than the belief in human freedom, preferring to retain a pacifist's objectivity. It is this overarching theme of freedom that means his work "subverts the bases for distinctions among the disciplines".[114] Therefore, he was able to hold knowledge across a vast array of subjects: "the international world order, the political and economic organisation of contemporary society, especially France, the institutional and legal frameworks that regulate the lives of ordinary citizens, the educational system, the media networks that control and disseminate information. Sartre systematically refused to keep quiet about what he saw as inequalities and injustices in the world."[115]
Sartre always sympathized with the Left, and supported the French Communist Party (PCF) until the 1956 Soviet invasion of Hungary. Following the Liberation the PCF were infuriated by Sartre's philosophy, which appeared to lure young French men and women away from the ideology of communism and into Sartre's own existentialism.[116] From 1956 onwards Sartre rejected the claims of the PCF to represent the French working classes, objecting to its "authoritarian tendencies". In the late 1960s Sartre supported the Maoists, a movement that rejected the authority of established communist parties.[1] However, despite aligning with the Maoists, Sartre said after the May events: "If one rereads all my books, one will realize that I have not changed profoundly, and that I have always remained an anarchist."[117] He would later explicitly allow himself to be called an anarchist.[118][119]
In the aftermath of a war that had for the first time properly engaged Sartre in political matters, he set forth a body of work which "reflected on virtually every important theme of his early thought and began to explore alternative solutions to the problems posed there".[120] The greatest difficulties that he and all public intellectuals of the time faced were the increasing technological aspects of the world that were outdating the printed word as a form of expression. In Sartre's opinion, the "traditional bourgeois literary forms remain innately superior", but there is "a recognition that the new technological 'mass media' forms must be embraced" if Sartre's ethical and political goals as an authentic, committed intellectual are to be achieved: the demystification of bourgeois political practices and the raising of the consciousness, both political and cultural, of the working class.[121]
The struggle for Sartre was against the monopolising moguls who were beginning to take over the media and destroy the role of the intellectual. His attempts to reach a public were mediated by these powers, and it was often these powers he had to campaign against. He was skilled enough, however, to circumvent some of these issues by his interactive approach to the various forms of media, advertising his radio interviews in a newspaper column for example, and vice versa.[122]
Sartre's role as a public intellectual occasionally put him in physical danger, such as in June 1961, when a plastic bomb exploded in the entrance of his apartment building. His public support of Algerian self-determination at the time had led Sartre to become a target of the campaign of terror that mounted as the colonists' position deteriorated. A similar occurrence took place the next year and he had begun to receive threatening letters from Oran, Algeria.[123]
Sartre's role in this conflict included his comments in his preface to Frantz Fanon's The Wretched of the Earth that, "To shoot down a European is to kill two birds with one stone, to destroy an oppressor and the man he oppresses at the same time: there remains a dead man and a free man". This comment led to some criticisms from the right, such as by Brian C. Anderson and Michael Walzer. Writing for the Hoover Institution, Walzer suggested that Sartre, a European, was a hypocrite for not volunteering to be killed.[124][125]
However Sartre's stances regarding post-colonial conflict have not been entirely without controversy on the left; Sartre's preface is omitted from some editions of The Wretched of the Earth printed after 1967. The reason for this is for his public support for Israel in the Six-Day War. Fanon's widow, Josie considered Sartre's pro-Israel stance as inconsistent with the anti-colonialist position of the book so she omitted the preface.[126] When interviewed at Howard University in 1978, she explained "when Israel declared war on the Arab countries [during the Six-Day War], there was a great pro-Zionist movement in favor of Israel among western (French) intellectuals. Sartre took part in this movement. He signed petitions favoring Israel. I felt that his pro-Zionist attitudes were incompatible with Fanon's work".[126] Recent reprints of Fanon's book have generally included Sartre's preface.
Sartre wrote successfully in a number of literary modes and made major contributions to literary criticism and literary biography. His plays are richly symbolic and serve as a means of conveying his philosophy. The best-known, Huis-clos (No Exit), contains the famous line "L'enfer, c'est les autres", usually translated as "Hell is other people."[127] Aside from the impact of Nausea, Sartre's major work of fiction was The Roads to Freedom trilogy which charts the progression of how World War II affected Sartre's ideas. In this way, Roads to Freedom presents a less theoretical and more practical approach to existentialism.
John Huston got Sartre to script his film Freud: The Secret Passion.[128] However it was too long and Sartre withdrew his name from the film's credits.[129] Nevertheless, many key elements from Sartre's script survive in the finished film.[128]
Despite their similarities as polemicists, novelists, adapters, and playwrights, Sartre's literary work has been counterposed, often pejoratively, to that of Camus in the popular imagination. In 1948 the Roman Catholic Church placed Sartre's œuvre on the Index Librorum Prohibitorum (List of Prohibited Books).


Charles André Joseph Marie de Gaulle (/də ˈɡoʊl, -ˈɡɔːl/; French pronunciation: [ʃaʁl də ɡol] (listen);[1] 22 November 1890 – 9 November 1970), commonly known in France simply as "le général" ("the general"), was a French army officer and statesman who led Free France against Nazi Germany in World War II and chaired the Provisional Government of the French Republic from 1944 to 1946 in order to restore democracy in France. In 1958, he came out of retirement when appointed President of the Council of Ministers (Prime Minister) by President René Coty. He rewrote the Constitution of France and founded the Fifth Republic after approval by referendum. He was elected President of France later that year, a position to which he was reelected in 1965 and held until his resignation in 1969.
Born in Lille, he graduated from Saint-Cyr in 1912. He was a decorated officer of the First World War, wounded several times and later taken prisoner by the Germans at Verdun. During the interwar period, he advocated mobile armoured divisions. During the German invasion of May 1940, he led an armoured division which counterattacked the invaders; he was then appointed Undersecretary for War. Refusing to accept his government's armistice with Germany, de Gaulle fled to England and exhorted the French to resist occupation and to continue the fight in his Appeal of 18 June. He led the Free French Forces and later headed the French National Liberation Committee against the Axis. Despite frosty relations with the United States, he generally had Winston Churchill's support, and emerged as the undisputed leader of Free France. He became head of the Provisional Government of the French Republic in June 1944, the interim government of France following its liberation. As early as 1944, de Gaulle introduced a dirigiste economic policy, which included substantial state-directed control over a capitalist economy, which was followed by 30 years of unprecedented growth, known as the Trente Glorieuses. Frustrated by the return of petty partisanship in the new Fourth Republic, he resigned in early 1946, but continued to be politically active as founder of the Rassemblement du Peuple Français (RPF; "Rally of the French People"). He retired in the early 1950s and wrote his War Memoirs, which quickly became a staple of modern French literature.
When the Algerian War was ripping apart the unstable Fourth Republic, the National Assembly brought him back to power during the May 1958 crisis. He founded the Fifth Republic with a strong presidency, and he was elected to continue in that role. He managed to keep France together while taking steps to end the war, much to the anger of the Pieds-Noirs (ethnic French born in Algeria) and the armed forces; both previously had supported his return to power to maintain colonial rule. He granted independence to Algeria and acted progressively towards other French colonies. In the context of the Cold War, de Gaulle initiated his "politics of grandeur", asserting that France as a major power should not rely on other countries, such as the United States, for its national security and prosperity. To this end, he pursued a policy of "national independence" which led him to withdraw from NATO's integrated military command and to launch an independent nuclear strike force that made France the world's fourth nuclear power. He restored cordial Franco-German relations to create a European counterweight between the Anglo-American and Soviet spheres of influence through the signing of the Élysée Treaty on 22 January 1963.
De Gaulle opposed any development of a supranational Europe, favouring Europe as a continent of sovereign nations. De Gaulle openly criticised the United States intervention in Vietnam and the "exorbitant privilege" of the United States dollar. In his later years, his support for the slogan "Vive le Québec libre" and his two vetoes of Britain's entry into the European Economic Community generated considerable controversy in both North America and Europe. Although reelected to the presidency in 1965, he faced widespread protests by students and workers in May 1968, but had the Army's support and won an election with an increased majority in the National Assembly. De Gaulle resigned in 1969 after losing a referendum in which he proposed more decentralisation. He died a year later at the age of 79, leaving his presidential memoirs unfinished.
Many French political parties and leaders claim a Gaullist legacy, that is, having a political ideaology similar to Charles de Gaulle. Many streets and monuments in France were dedicated to his memory after his death.
Charles André Joseph Marie de Gaulle was born on 22 November 1890 in Lille in the Nord department, the third of five children.[2] He was raised in a devoutly Catholic and traditional family. His father, Henri de Gaulle, was a professor of history and literature at a Jesuit college and eventually founded his own school.[3]: 42–47 
Henri de Gaulle came from a long line of parliamentary gentry from Normandy and Burgundy.[4]: 13–16 [5] The name is thought to be Dutch in origin, and may well have derived from van der Walle, de Walle ("from the rampart, defensive wall") or de Waal ("the wall")[6][3]: 42  De Gaulle's mother, Jeanne (born Maillot), descended from a family of wealthy entrepreneurs from Lille. She had French, Irish, Scottish, and German ancestry.[4]: 13–16 [5]
De Gaulle's father encouraged historical and philosophical debate between his children at mealtimes, and through his encouragement, de Gaulle grew familiar with French history from an early age. Struck by his mother's tales of how she cried as a child when she heard of the French capitulation to the Germans at Sedan in 1870, he developed a keen interest in military strategy. He was also influenced by his uncle, also named Charles de Gaulle, who was a historian and passionate Celticist who wrote books and pamphlets advocating the union of the Welsh, Scots, Irish, and Bretons into one people. His grandfather Julien-Philippe was also a historian, and his grandmother Josephine-Marie wrote poems which impassioned his Christian faith.[7][3]: 42–47 
By the time he was ten he was reading medieval history. De Gaulle began writing in his early teens, especially poetry, and later his family paid for a composition, a one-act play in verse about a traveller, to be privately published.[8] A voracious reader, he favored philosophical tomes by such writers as Bergson, Péguy, and Barrès. In addition to the German philosophers Nietzsche, Kant, and Goethe, he read the works of the ancient Greeks (especially Plato) and the prose of the romanticist poet Chateaubriand.[8]
De Gaulle was educated in Paris at the Collège Stanislas and studied briefly in Belgium where he continued to display his interest in reading and studying history and shared the great pride many of his countrymen felt in their nation's achievements.[3]: 51–53  At the age of fifteen he wrote an essay imagining "General de Gaulle" leading the French Army to victory over Germany in 1930; he later wrote that in his youth he had looked forward with somewhat naive anticipation to the inevitable future war with Germany to avenge the French defeat of 1870.[9]
France during de Gaulle's teenage years was a divided society, with many developments which were unwelcome to the de Gaulle family: the growth of socialism and syndicalism, the legal separation of Church and state in 1905, and the reduction in the term of military service to two years in the same year. Equally unwelcome were the Entente Cordiale with Britain, the First Moroccan Crisis, and above all the Dreyfus Affair. Henri de Gaulle came to be a supporter of Dreyfus, but was less concerned with his innocence per se than with the disgrace which the army had brought onto itself. The same period also saw a resurgence in evangelical Catholicism, the dedication of the Sacré-Cœur, Paris, and the rise of the cult of Joan of Arc.[3]: 50–51 [9]
De Gaulle was not an outstanding pupil until his mid-teens, but from July 1906 he worked harder at school as he focused on winning a place to train as an army officer at the military academy, Saint-Cyr.[10] Lacouture suggests that de Gaulle joined the army, despite being by inclination more suited to a career as a writer and historian, partly to please his father and partly because it was one of the few unifying forces which represented the whole of French society.[11] He later wrote that "when I entered the Army, it was one of the greatest things in the world",[3]: 51  a claim which Lacouture points out needs to be treated with caution: the army's reputation was at a low ebb in the early 1900s after the Dreyfus Affair. It was used extensively for strike-breaking and there were fewer than 700 applicants for St Cyr in 1908, down from 2,000 at the turn of the century.[11]
De Gaulle won a place at St Cyr in 1909. His class ranking was mediocre (119th out of 221 entrants), but he was relatively young and this was his first attempt at the exam.[10] Under a law of 21 March 1905, aspiring army officers were required to serve a year in the ranks, including time both as a private and as an NCO, before attending the academy. Accordingly, in October 1909, de Gaulle enlisted (for four years, as required, rather than the normal two-year term for conscripts) in the 33rd Infantry Regiment [fr] of the French Army, based at Arras.[12] This was a historic regiment with Austerlitz, Wagram, and Borodino amongst its battle honours.[13] In April 1910 he was promoted to corporal. His company commander declined to promote him to sergeant, the usual rank for a potential officer, commenting that the young man clearly felt that nothing less than Constable of France would be good enough for him.[14][12] He was eventually promoted to sergeant in September 1910.[15]
De Gaulle took up his place at St Cyr in October 1910. By the end of his first year he had risen to 45th place.[16] At St Cyr, de Gaulle acquired the nickname of "the great asparagus" because of his height (196 cm, 6'5"), high forehead, and nose.[3]: 301  He did well at the academy and received praise for his conduct, manners, intelligence, character, military spirit, and resistance to fatigue. In 1912, he graduated 13th in his class[17] and his passing-out report noted that he was a gifted cadet who would undoubtedly make an excellent officer. The future Marshal Alphonse Juin passed out first in the class, although the two do not appear to have been close friends at the time.[18]
Preferring to serve in France rather than the distant overseas colonies, in October 1912 he rejoined the 33rd Infantry Regiment as a sous-lieutenant (second lieutenant). The regiment was now commanded by Colonel (and future Marshal) Philippe Pétain, whom de Gaulle would follow for the next 15 years. He later wrote in his memoirs: "My first colonel, Pétain, taught me the art of command".[19][18]
It has been claimed that in the build-up to World War I, de Gaulle agreed with Pétain about the obsolescence of cavalry and of traditional tactics in the age of machine guns and barbed wire, and often debated great battles and the likely outcome of any coming war with his superior.[7] Lacouture is sceptical, pointing out that although Pétain wrote glowing appraisals of de Gaulle in the first two-quarters of 1913, it is unlikely that he stood out among the 19 captains and 32 lieutenants under his command. De Gaulle would have been present at the 1913 Arras manoeuvres, at which Pétain criticised General Gallet [fr] to his face, but there is no evidence in his notebooks that he accepted Pétain's unfashionable ideas about the importance of firepower against the dominant doctrine emphasizing "offensive spirit". De Gaulle stressed how Maurice de Saxe had banned volley fire, how French armies of the Napoleonic period had relied on infantry column attack, and how French military power had declined in the nineteenth century because of – supposedly – excessive concentration on firepower (e.g. the Chassepot rifle) rather than élan. He also appears to have accepted the then fashionable lesson drawn from the recent Russo-Japanese War, of how bayonet charges by Japanese infantry with high morale had succeeded in the face of enemy firepower.[20]
De Gaulle was promoted to first lieutenant in October 1913.[21]
When war finally broke out in France in early August 1914, the 33rd Regiment, considered one of the best fighting units in France, was immediately thrown into checking the German advance at Dinant. However, the French Fifth Army commander, General Charles Lanrezac, remained wedded to 19th-century battle tactics, throwing his units into pointless bayonet charges with bugles and full colours flying against German artillery, incurring heavy losses.[7]
As a platoon commander, de Gaulle was involved in fierce fighting from the outset. He received his baptism of fire on 15 August and was among the first to be wounded, receiving a bullet in the knee at the Battle of Dinant.[15][3]: 58  It is sometimes claimed that in hospital, he grew bitter at the tactics used, and spoke with other injured officers against the outdated methods of the French army. However, there is no contemporary evidence that he understood the importance of artillery in modern warfare. Instead, in his writing at the time, he criticised the "overrapid" offensive, the inadequacy of French generals, and the "slowness of the English troops".[22]
He rejoined his regiment in October, as commander of the 7th company. Many of his former comrades were already dead. In December he became regimental adjutant.[15]
De Gaulle's unit gained recognition for repeatedly crawling out into no man's land to listen to the conversations of the enemy in their trenches, and the information brought back was so valuable that on 18 January 1915 he received the Croix de Guerre. On 10 February he was promoted to captain, initially on probation.[15] On 10 March 1915, de Gaulle was shot in the left hand, a wound which initially seemed trivial but became infected.[23] The wound incapacitated him for four months and later forced him to wear his wedding ring on the right hand.[3]: 61 [15][24] In August he commanded the 10th company before returning to duty as regimental adjutant. On 3 September 1915 his rank of captain became permanent. In late October, returning from leave, he returned to command of 10th company again.[15]
As a company commander at Douaumont (during the Battle of Verdun) on 2 March 1916, while leading a charge to try to break out of a position which had become surrounded by the enemy, he received a bayonet wound to the left thigh after being stunned by a shell and was captured after passing out from the effects of poison gas. He was one of the few survivors of his battalion.[25][15][3]: 63  He was pulled out of an empty shell crater by German soldiers and taken prisoner. The circumstances of his capture would later become a subject of debate as anti-Gaullists spread rumour that he had actually surrendered, a claim de Gaulle nonchalantly dismissed.[26]
De Gaulle spent 32 months in six different prisoner camps, but he spent most time in the Ingolstadt Fortress [de],[27]: 40  where his treatment was satisfactory.[25]
In captivity, de Gaulle read German newspapers (he had learned German at school and spent a summer vacation in Germany) and gave talks on his view of the course of the conflict to fellow prisoners. His patriotic fervour and confidence in victory earned him yet another nickname, Le Connétable ("The Constable"), the title of the medieval commander-in-chief of the French army.[28] In Ingolstadt were also journalist Remy Roure, who would eventually become a political ally of de Gaulle,[29][30] and Mikhail Tukhachevsky, a future commander of the Red Army. During his time as a POW, de Gaulle became well acquainted with Tukhachevsky, whose theories about a fast-moving, mechanized army closely resembled his. While a prisoner of war, de Gaulle wrote his first book, Discorde chez l'ennemi (The Enemy's House Divided), analysing the issues and divisions within the German forces. The book was published in 1924.[3]: 83 
Originally interned at Rosenberg Fortress, he was quickly moved to progressively higher-security facilities like Ingolstadt. In total, De Gaulle made five unsuccessful escape attempts,[15] and was routinely punished with long periods of solitary confinement and the withdrawal of privileges such as newspapers and tobacco. He attempted escape by hiding in a laundry basket, digging a tunnel, digging a hole through a wall, and even posing as a nurse to fool his guards.[31][19] In his letters to his parents, he constantly spoke of his frustration that the war was continuing without him, calling the situation "a shameful misfortune" and compared it to being cuckolded. As the war neared its end, he grew depressed that he was playing no part in the victory, but despite his efforts, he remained in captivity until the armistice. On 1 December 1918, three weeks later, he returned to his father's house in the Dordogne to be reunited with his three brothers, who had all served in the army and survived the war.
After the armistice, de Gaulle served with the staff of the French Military Mission to Poland as an instructor of Poland's infantry during its war with communist Russia (1919–1921). He distinguished himself in operations near the River Zbrucz, with the rank of major in the Polish army, and won Poland's highest military decoration, the Virtuti Militari.[3]: 71–74 
De Gaulle returned to France, where he became a lecturer in military history at St Cyr. He was already a powerful speaker, after practice as a prisoner of war.[32] He then studied at the École de Guerre (staff college) from November 1922 to October 1924. Here he clashed with his instructor Colonel Moyrand by arguing for tactics based on circumstances rather than doctrine, and after an exercise in which he had played the role of commander, he refused to answer a question about supplies, replying "de minimis non curat praetor" (roughly: "a leader does not concern himself with trivia") before ordering the responsible officer to answer Moyrand. He obtained respectable, but not outstanding grades – 15 or so out of 20 – on many of his assessments. Moyrand wrote in his final report that he was "an intelligent, cultured and serious-minded officer; has brilliance and talent" but criticised him for not deriving as much benefit from the course as he should have done, and for his arrogance: his "excessive self-confidence", his harsh dismissal of the views of others "and his attitude of a King in exile". Having entered 33rd out of 129, he graduated in 52nd place, with a grade of assez bien ("good enough"). He was posted to Mainz to help supervise supplies of food and equipment for the French Army of Occupation.[33][3]: 82 
De Gaulle's book La Discorde chez l'ennemi had appeared in March 1924. In March 1925 he published an essay on the use of tactics according to circumstances, a deliberate gesture in defiance of Moyrand.[34]
De Gaulle's career was saved by Pétain, who arranged for his staff college grade to be amended to bien ("good"—but not the "excellent" which would have been needed for a general staff posting).[3]: 82–83  From 1 July 1925 he worked for Pétain (as part of the Maison Pétain), largely as a "pen officer" (ghostwriter).[35] De Gaulle disapproved of Pétain's decision to take command in Morocco in 1925 (he was later known to remark that "Marshal Pétain was a great man. He died in 1925, but he did not know it") and of what he saw as the lust for public adulation of Pétain and his wife. In 1925 de Gaulle began to cultivate Joseph Paul-Boncour, his first political patron.[36] On 1 December 1925 he published an essay on the "Historical Role of French Fortresses". This was a popular topic because of the Maginot Line which was then being planned, but his argument was quite nuanced: he argued that the aim of fortresses should be to weaken the enemy, not to economise on defence.[35]
Friction arose between de Gaulle and Pétain over Le Soldat, a history of the French soldier which he had ghost-written and for which he wanted greater writing credit. He had written mainly historical material, but Pétain wanted to add a final chapter of his own thoughts. There was at least one stormy meeting late in 1926 after which de Gaulle was seen to emerge, white with anger, from Pétain's office.[37] In October 1926 he returned to his duties with the Headquarters of the Army of the Rhine.[38]
De Gaulle had sworn that he would never return to the École de Guerre except as commandant, but at Pétain's invitation, and introduced to the stage by his patron, he delivered three lectures there in April 1927: "Leadership in Wartime", "Character", and "Prestige". These later formed the basis for his book The Edge of the Sword (1932). Many of the officers in the audience were his seniors, who had taught and examined him only a few years earlier.[39]
After spending twelve years as a captain, a normal period, de Gaulle was promoted to commandant (major) on 25 September 1927.[39] In November 1927 he began a two-year posting as commanding officer of the 19th chasseurs à pied (a battalion of élite light infantry) with the occupation forces at Trier (Treves).[40][3]: 94 
De Gaulle trained his men hard (a river crossing exercise of the freezing Moselle River at night was vetoed by his commanding general). He imprisoned a soldier for appealing to his deputy (Member of Parliament) for a transfer to a cushier unit, and when investigated initially tried to invoke his status as a member of the Maison Pétain, eventually appealing to Pétain to protect himself from a reprimand for interfering with the soldier's political rights. An observer wrote of de Gaulle at this time that although he encouraged young officers, "his ego...glowed from far off". In the winter of 1928–1929, thirty soldiers ("not counting Annamese") died from so-called "German flu", seven of them from de Gaulle's battalion. After an investigation, he was singled out for praise in the ensuing parliamentary debate as an exceptionally capable commanding officer, and mention of how he had worn a mourning band for a private soldier who was an orphan earned an exclamation of praise from the Prime Minister Raymond Poincaré.[41]
The breach between de Gaulle and Pétain over the ghost-writing of Le Soldat had deepened in 1928. Pétain brought in a new ghostwriter, Colonel Audet, who was unwilling to take on the job and wrote to de Gaulle in some embarrassment to take over the project. Pétain was quite friendly about the matter but did not publish the book.[42] In 1929 Pétain did not use de Gaulle's draft text for his eulogy for the late Ferdinand Foch, whose seat at the Academie Française he was assuming.[37]
The Allied occupation of the Rhineland was coming to an end, and de Gaulle's battalion was due to be disbanded, although the decision was later rescinded after he had moved to his next posting. De Gaulle wanted a teaching post at the École de Guerre in 1929.[43] There was apparently a threat of mass resignation of the faculty were he appointed to a position there. There was talk of a posting to Corsica or North Africa, but on Pétain's advice he accepted a two-year posting to Lebanon and Syria.[3]: 93–94  In Beirut he was chief of the 3rd Bureau (military operations) of General Louis-Paul-Gaston de Bigault du Granrut, who wrote him a glowing reference recommending him for high command in the future.[44]
In the spring of 1931, as his posting in Beirut drew to a close, de Gaulle once again asked Pétain for a posting to the École de Guerre. Pétain tried to obtain an appointment for him as Professor of History there, but once again the faculty would not have him. Instead de Gaulle, drawing on plans he had drawn up in 1928 for reform of that institution, asked Pétain to create a special post for him which would enable him to lecture on "the Conduct of War" both to the École de Guerre and to the Centre des Hautes Études Militaires (CHEM – a senior staff college for generals, known as the "school for marshals"), and also to civilians at the École Normale Supérieure, and to civil servants.[45]
Pétain instead advised him to apply for a posting to the Secrétariat Général du Conseil Supérieur de la Défense Nationale (SGDN – General Secretariat of the Supreme War Council, reporting to the Under-Secretary to the Prime Minister, although later moved to the Ministry of War in 1936) in Paris. Pétain promised to lobby for the appointment, which he thought would be good experience for him. De Gaulle was posted to SGDN in November 1931, initially as a "drafting officer".[45][3]: 94 
He was promoted to lieutenant-colonel in December 1932 and appointed Head of the Third Section (operations). His service at SGDN gave him six years' experience of the interface between army planning and government, enabling him to take on ministerial responsibilities in 1940.[3]: 97 [46]
After studying arrangements in the US, Italy, and Belgium, de Gaulle drafted a bill for the organisation of the country in time of war. He made a presentation about his bill to the CHEM. The bill passed the Chamber of Deputies but failed in the Senate.[47]
Unlike Pétain, de Gaulle believed in the use of tanks and rapid maneuvers rather than trench warfare.[3]: 108  De Gaulle became a disciple of Émile Mayer (1851–1938), a retired lieutenant-colonel (his career had been damaged by the Dreyfus Affair) and military thinker. Mayer thought that although wars were still bound to happen, it was "obsolete" for civilised countries to threaten or wage war on one another as they had in previous centuries. He had a low opinion of the quality of French generals, and was a critic of the Maginot Line and a proponent of mechanised warfare. Lacouture suggests that Mayer focused de Gaulle's thoughts away from his obsession with the mystique of the strong leader (Le Fil d'Epée: 1932) and back to loyalty to Republican institutions and military reform.[48]
In 1934 de Gaulle wrote Vers l'Armée de Métier (Towards a Professional Army). He proposed mechanization of the infantry, with stress on an élite force of 100,000 men and 3,000 tanks. The book imagined tanks driving around the country like cavalry. De Gaulle's mentor Emile Mayer was somewhat more prophetic than he was about the future importance of air power on the battlefield. Such an army would both compensate for France's population shortage, and be an efficient tool to enforce international law, particularly the Treaty of Versailles, which forbade Germany from rearming. He also thought it would be a precursor to a deeper national reorganisation, and wrote that "a master has to make his appearance [...] whose orders cannot be challenged – a man upheld by public opinion".[49]
Only 700 copies were sold in France; the claim that thousands of copies were sold in Germany[19] is thought to be an exaggeration. De Gaulle used the book to widen his contacts among journalists, notably with André Pironneau, editor of L'Écho de Paris. The book attracted praise across the political spectrum, apart from the hard left who were committed to the Republican ideal of a citizen army.[50] De Gaulle's views attracted the attention of the maverick politician Paul Reynaud, to whom he wrote frequently, sometimes in obsequious terms. Reynaud first invited him to meet him on 5 December 1934.[51]
The de Gaulle family were very private.[52] De Gaulle was deeply focused on his career at this time. There is no evidence that he was tempted by fascism, and there is little evidence of his views either on domestic upheavals in 1934 and 1936 or the many foreign policy crises of the decade.[53] He approved of the rearmament drive which the Popular Front government began in 1936, although French military doctrine remained that tanks should be used in penny packets for infantry support (ironically, in 1940 it would be German panzer units that would be used in a manner similar to what de Gaulle had advocated).[54] A rare insight into de Gaulle's political views is a letter to his mother warning her that war with Germany was sooner or later inevitable and reassuring her that Pierre Laval's pact with the USSR in 1935 was for the best, likening it to Francis I's alliance with the Turks against the Emperor Charles V.[55]
From April 1936, whilst still in his staff position at SGDN, de Gaulle was also a lecturer to generals at CHEM.[47] De Gaulle's superiors disapproved of his views about tanks, and he was passed over for promotion to full colonel in 1936, supposedly because his service record was not good enough. He interceded with his political patron Reynaud, who showed his record to the Minister of War Édouard Daladier. Daladier, who was an enthusiast for rearmament with modern weapons, ensured that his name was entered onto the promotion list for the following year.[3]: 109 [56]
In 1937 General Bineau, who had taught him at St Cyr, wrote on his report on his lectureship at CHEM that he was highly able and suitable for high command in the future, but that he hid his attributes under "a cold and lofty attitude".[47] He was put in command of the 507th Tank Regiment (consisting of a battalion of medium Char D2s and a battalion of R35 light tanks) at Metz on 13 July 1937, and his promotion to full colonel took effect on 24 December that year. De Gaulle attracted public attention by leading a parade of 80 tanks into the Place d'Armes at Metz, in his command tank "Austerlitz".[57]
By now de Gaulle was beginning to be a well-known figure, known as "Colonel Motor(s)".[3]: 117  At the invitation of the publisher Plon, he produced another book, La France et son Armée (France and Her Army) in 1938. De Gaulle incorporated much of the text he had written for Pétain a decade earlier for the uncompleted book Le Soldat, to Pétain's displeasure. In the end, de Gaulle agreed to include a dedication to Pétain (although he wrote his own rather than using the draft Pétain sent him), which was later dropped from postwar editions. Until 1938 Pétain had treated de Gaulle, as Lacouture puts it, "with unbounded good will", but by October 1938 he privately thought his former protégé "an ambitious man, and very ill-bred".[58]
At the outbreak of World War II, de Gaulle was put in command of the French Fifth Army's tanks (five scattered battalions, largely equipped with R35 light tanks) in Alsace. On 12 September 1939 he attacked at Bitche, simultaneously with the Saar Offensive.[59][3]: 118 
At the start of October 1939, Reynaud asked for a staff posting under de Gaulle, but in the event remained at his post as Minister of Finance. De Gaulle's tanks were inspected by President Lebrun, who was impressed, but regretted that it was too late to implement his ideas.[60] He wrote a paper L'Avènement de la force mécanique (The coming of the Armoured Force) which he sent to General Georges (commander-in-chief on the northeast front – who was not especially impressed) and the politician Leon Blum. Daladier, Prime Minister at the time, was too busy to read it.[61]
In late-February 1940, Reynaud told de Gaulle that he had been earmarked for command of an armoured division as soon as one became available.[62] Early in 1940 (the exact date is uncertain), de Gaulle proposed to Reynaud that he be appointed Secretary-General of the War Council, which would in effect have made him the government's military adviser. When Reynaud became prime minister in March he was reliant on Daladier's backing, so the job went instead to the politician Paul Baudouin.[63]
In late-March, de Gaulle was told by Reynaud that he would be given command of the 4th Armoured Division, due to form by 15 May.[64] The government appeared likely to be restructured, as Daladier and Maurice Gamelin (commander-in-chief) were under attack in the aftermath of the Allied defeat in Norway, and had this happened de Gaulle, who on 3 May, was still lobbying Reynaud for a restructuring of the control of the war, might well have joined the government.[65] By 7 May he was assembling the staff of his new division.[66]
The Germans attacked the West on 10 May.[65] De Gaulle activated his new division on 12 May.[66] The Germans broke through at Sedan on 15 May 1940.[67] That day, with three tank battalions assembled, less than a third of his paper strength, he was summoned to headquarters and told to attack to gain time for General Robert Touchon's Sixth Army to redeploy from the Maginot Line to the Aisne. General Georges told him it was his chance to implement his ideas.[68][19]
De Gaulle commandeered some retreating cavalry and artillery units and also received an extra half-brigade, one of whose battalions included some heavy B1 bis tanks. The attack at Montcornet, a key road junction near Laon, began around 04:30 on 17 May. Outnumbered and without air support, he lost 23 of his 90 vehicles to mines, anti-tank weapons, and Stukas. On 18 May he was reinforced by two fresh regiments of armoured cavalry, bringing his strength up to 150 vehicles. He attacked again on 19 May and his forces were once again devastated by Stukas and artillery. He ignored orders from General Georges to withdraw, and in the early afternoon demanded two more divisions from Touchon, who refused his request.[69] Although de Gaulle's tanks forced German infantry to retreat to Caumont, the action brought only temporary relief and did little to slow the spearhead of the German advance. Nevertheless, it was one of the few successes the French enjoyed while suffering defeats elsewhere across the country.[70][71]
He delayed his retreat until 20 May. On 21 May, at the request of propaganda officers, he gave a talk on French radio about his recent attack.[72] In recognition for his efforts de Gaulle was promoted to the rank of temporary (acting, in Anglophone parlance) brigadier-general on 23 May 1940. Despite being compulsorily retired as a colonel on 22 June (see below) he would wear the uniform of a brigadier-general for the rest of his life.[73][71]
On 28–29 May, de Gaulle attacked the German bridgehead south of the Somme at Abbeville, taking around 400 German prisoners in the last attempt to cut an escape route for the Allied forces falling back on Dunkirk.[74][3]: 127 
The future General Paul Huard, who served under de Gaulle at this time, recorded how he would often stand on a piece of high ground, keeping other officers literally at six yards' distance, subjecting his subordinates to harsh criticism and making all decisions autocratically himself, behaviour consistent with his later conduct as a political leader. Lacouture points out that for all his undoubted energy and physical courage there is no evidence in his brief period of command that he possessed the "hunter's eye" of the great battlefield commander, and that not a single one of his officers joined him in London, although some joined the Resistance in France.[75]
De Gaulle's rank of brigadier-general became effective on 1 June 1940.[3]: 127  That day he was in Paris. After a visit to his tailor to be fitted for his general's uniform, he visited Reynaud, who appears to have offered him a government job for the first time, and later afterwards the commander-in-chief Maxime Weygand, who congratulated him on saving France's honour and asked him for his advice.[76] On 2 June he sent a memo to Weygand vainly urging that the French armoured divisions be consolidated from four weak divisions into three stronger ones and concentrated into an armoured corps under his command. He made the same suggestion to Reynaud.[76]
On 5 June, the day the Germans began the second phase of their offensive (Fall Rot), Prime Minister Paul Reynaud appointed de Gaulle a government minister, as Under-Secretary of State for National Defence and War,[77] with particular responsibility for coordination with the British.[78] Weygand objected to the appointment, thinking him "a mere child".[79] Pétain (Deputy Prime Minister) was also displeased at his appointment and told Reynaud the story of the ghost-writing of Le Soldat.[79] His appointment received a good deal of press attention, both in France and in the UK. He asked for an English-speaking aide and Geoffroy Chodron de Courcel was given the job.[80]
On 8 June, de Gaulle visited Weygand, who believed it was "the end" and that after France was defeated Britain would also soon sue for peace. He hoped that after an armistice the Germans would allow him to retain enough of a French Army to "maintain order" in France. He gave a "despairing laugh" when de Gaulle suggested fighting on.[81]
On 9 June, de Gaulle flew to London and met British Prime Minister Winston Churchill for the first time. It was thought that half a million men could be evacuated to French North Africa, provided the British and French navies and air forces coordinated their efforts. Either at this meeting or on 16 June, he urged Churchill in vain to throw more Royal Air Force (RAF) aircraft into the Battle of France, but conceded there and then that Churchill was right to refuse.[82]
In his memoirs, de Gaulle mentioned his support for the proposal to continue the war from French North Africa, but at the time he was more in favour of the plan to form a "redoubt" in Brittany than he later admitted.[83]
Italy entered the war on 10 June. That day de Gaulle was present at two meetings with Weygand (he only mentions one in his memoirs), one at the defence committee and a second where Weygand barged into Reynaud's office and demanded an armistice. When Weygand asked de Gaulle, who wanted to carry on fighting, if he had "anything to suggest", de Gaulle replied that it was the government's job to give orders, not to make suggestions. De Gaulle wanted Paris to be stubbornly defended by de Lattre, but instead it was declared an open city. At around 23:00 Reynaud and de Gaulle left Paris for Tours; the rest of the government left Paris on 11 June.[84]
On 11 June, de Gaulle drove to Arcis-sur-Aube and offered General Charles Huntziger (Commander of the Central Army Group) Weygand's job as Commander-in-Chief. Huntziger accepted in principle (although according to Henri Massis he was merely amused at the prospect of forming a Breton redoubt – Huntziger would sign the armistice on behalf of Pétain a few weeks later) but de Gaulle was unable to persuade Reynaud to sack Weygand.[84]
Later on 11 June, de Gaulle attended the meeting of the Anglo-French Supreme War Council at the Chateau du Muguet at Briare. The British were represented by Churchill, Anthony Eden, General John Dill (Chief of the Imperial General Staff), General Hastings Ismay and Edward Spears, and the French by Reynaud, Pétain, Weygand, and Georges. Churchill demanded that the French take to guerrilla warfare, and reminded Pétain of how he had come to the aid of the British with forty French divisions in March 1918, receiving a dusty answer in each case. De Gaulle's fighting spirit made a strong impression on the British. At the meeting de Gaulle met Pétain for the first time in two years. Pétain noted his recent promotion to general, adding that he did not congratulate him, as ranks were of no use in defeat. When de Gaulle protested that Pétain himself had been promoted to brigadier-general and division commander at the Battle of the Marne in 1914, he replied that there was "no comparison" with the present situation. De Gaulle later conceded that Pétain was right about that much at least.[85] De Gaulle missed the second day of the conference as he was in Rennes for a meeting (not mentioned in his memoirs) to discuss the plans for the Breton redoubt with General René Altmayer. He then returned to attend a cabinet meeting, at which it was clear that there was a growing movement for an armistice, and which decided that the government should move to Bordeaux rather than de Gaulle's preference for Quimper in Brittany.[86]
On 13 June, de Gaulle attended another Anglo-French conference at Tours with Churchill, Lord Halifax, Lord Beaverbrook, Spears, Ismay, and Alexander Cadogan. This time few other major French figures were present apart from Reynaud and Baudoin. He was an hour late, and his account is not reliable. Reynaud demanded that France be released from the agreement which he had made with Prime Minister Neville Chamberlain in March 1940, so that France could seek an armistice. De Gaulle wrote that Churchill was sympathetic to France seeking an armistice, provided that an agreement was reached about what was to happen to the French fleet. This claim was later made by apologists for the Vichy Regime, e.g., General Georges, who claimed that Churchill had supported the armistice as a means of keeping the Germans out of French North Africa. However, is not supported by other eyewitnesses (Churchill himself, Roland de Margerie, Spears) who agree that Churchill said that he "understood" the French action but that he did not agree with it. He murmured at de Gaulle that he was "l'homme du destin (the man of destiny)", although it is unclear whether de Gaulle actually heard him.[87] At the cabinet meeting that evening Pétain strongly supported Weygand's demand for an armistice, and said that he himself would remain in France to share the suffering of the French people and to begin the national rebirth. De Gaulle was dissuaded from resigning by the Interior Minister Georges Mandel, who argued that the war was only just beginning, and that de Gaulle needed to keep his reputation unsullied.[88]
De Gaulle arrived at Bordeaux on 14 June, and was given a new mission to go to London to discuss the potential evacuation to North Africa. He had a brief meeting with Admiral Darlan about the potential role of the French Navy. That evening, by coincidence, he dined in the same restaurant as Pétain: he went over to shake his hand in silence, the last time they ever met. Next morning no aircraft could be found so he had to drive to Brittany, where he visited his wife and daughters, and his aged mother (whom he never saw again, as she died in July), before taking a boat to Plymouth (he asked the skipper if he would be willing to carry on the war under the British flag), where he arrived on 16 June. He ordered the boat Pasteur, with a cargo of munitions, to be diverted to a British port, which caused some members of the French Government to call for him to be put on trial.[88]
On the afternoon of Sunday 16 June, de Gaulle was at 10 Downing Street for talks about Jean Monnet's mooted Anglo-French political union. He telephoned Reynaud – they were cut off during the conversation and had to resume later – with the news that the British had agreed.[89] He took off from London on a British aircraft at 18:30 on 16 June (it is unclear whether, as was later claimed, he and Churchill agreed that he would be returning soon), landing at Bordeaux at around 22:00 to be told that he was no longer a minister, as Reynaud had resigned as prime minister after the Franco-British Union had been rejected by his cabinet. Pétain had become prime minister with a remit of seeking an armistice with Nazi Germany. De Gaulle was now in imminent danger of arrest.[90]
De Gaulle visited Reynaud, who still hoped to escape to French North Africa and declined to come to London. Reynaud still had control of secret government funds until the handover of power the next day. It has been suggested that he ordered de Gaulle to go to London, but no written evidence has ever been found to confirm this. Georges Mandel also refused to come.[91]
At around 09:00 on the morning of 17 June, de Gaulle flew to London on a British aircraft with Edward Spears. The escape was hair-raising. Spears claimed that de Gaulle had been reluctant to come, and that he had pulled him into the aircraft at the last minute, although de Gaulle's biographer does not accept this. Jean Laurent brought 100,000 gold francs in secret funds provided to him by Reynaud. De Gaulle later told André Malraux of the mental anguish which his flight to London – a break with the French Army and with the recognised government, which would inevitably be seen as treason by many – had caused him.[92]
De Gaulle landed at Heston Airport soon after 12:30 on 17 June 1940. He saw Churchill at around 15:00 and Churchill offered him broadcast time on BBC. They both knew about Pétain's broadcast earlier that day that stated that "the fighting must end" and that he had approached the Germans for terms. That evening de Gaulle dined with Jean Monnet and denounced Pétain's "treason".[27]: 125–128  The next day the British Cabinet (Churchill was not present, as it was the day of his "Finest Hour" speech) were reluctant to agree to de Gaulle giving a radio address, as Britain was still in communication with the Pétain government about the fate of the French fleet. Duff Cooper (Minister of Information) had an advance copy of the text of the address, to which there were no objections. The cabinet eventually agreed after individual lobbying, as indicated by a handwritten amendment to the cabinet minutes.[93][67]
De Gaulle's Appeal of 18 June exhorted the French people not to be demoralized and to continue to resist the occupation of France. He also – apparently on his own initiative – declared that he would broadcast again the next day.[94] Few listened to the 18 June speech;[27]: 4–6  the speech was published in some newspapers in metropolitan (mainland) France. It was largely aimed at French soldiers who were then in Britain after being evacuated from Norway and Dunkirk; most showed no interest in fighting for de Gaulle's Free French Forces and were repatriated back to France to become German prisoners of war.[95]
In his next speech, intended for 19 June, de Gaulle denied the legitimacy of the government at Bordeaux.[94] He called on the North African troops to live up to the tradition of Bertrand Clausel, Thomas Robert Bugeaud, and Hubert Lyautey by defying orders from Bordeaux. The British Foreign Office protested to Churchill,[96] and it was not actually broadcast although appearing in de Gaulle's collection of his speeches. Britain still hoped that the new French government would cooperate, and did not want to publicly support a possible alternative whom Alexander Cadogan described as a "crank".[27]: 128 
The 18 June speech invited French soldiers and civilians to join de Gaulle. Although the French embassy did not release his address, some found de Gaulle at his borrowed apartment. The general told one visitor "We are starting from zero" as he had no men, money, or premises. The visitor could help, de Gaulle said, by staying while he went to lunch so that someone would be there to answer the phone or door.[27]: 129 
De Gaulle also tried, largely in vain, to attract the support of French forces in the French Empire. He telegraphed to General Charles Noguès (Resident-General in Morocco and Commander-in-Chief of French forces in North Africa), offering to serve under him or to cooperate in any way. Noguès, who was dismayed by the armistice but agreed to go along with it, refused to cooperate and forbade the press in French North Africa to publish de Gaulle's appeal. Noguès told the British liaison officer that de Gaulle's attitude was "unseemly".[97] De Gaulle also sent a telegram to Weygand offering to serve under his orders, receiving a dismissive reply.[98]
In the US, the France Forever organization was founded.[99][100][101][102]
After the armistice was signed on 21 June 1940, de Gaulle spoke at 20:00 on 22 June to denounce it.[103] The Bordeaux government reacted immediately, annulling his temporary promotion to brigadier-general with effect from the same day, and forcibly retiring him from the French Army (with the rank of colonel) on 23 June "as a disciplinary measure" (par mesure de discipline).[104][105] On 23 June the British Government denounced the armistice as a breach of the Anglo-French treaty signed in March, and stated that they no longer regarded the Bordeaux Government as a fully independent state. They also "took note" of the plan to establish a French National Committee (FNC) in exile, but did not mention de Gaulle by name. Jean Monnet, Chairman of the Anglo-French Coordinating Committee, believed de Gaulle could not yet claim that he alone represented fighting France, and that French opinion would not rally to a man operating from British soil. He said this in a letter to de Gaulle on June 23, and noted he had made his concerns known to British Foreign Office officials Cadogan and Robert Vansittart, as well as Edward Spears.[106] Monnet soon resigned as Chairman of the Anglo-French Coordinating Committee, and departed for the US to continue his work securing supplies from North America (now with the British Purchasing Commission.)[107]
The armistice took effect from 00:35 on 25 June.[103] Alexander Cadogan of the Foreign Office sent Gladwyn Jebb, then a fairly junior official, to ask de Gaulle to tone down his next broadcast on 26 June; de Gaulle backed down under protest when Jebb told him that he would otherwise be banned from broadcasting. He claimed erroneously that the French fleet was to be handed over to the Germans.[108] On 26 June de Gaulle wrote to Churchill demanding recognition of his French Committee.[94] On 28 June, after Churchill's envoys had failed to establish contact with the French leaders in North Africa, the British Government recognised de Gaulle as leader of the Free French, despite the reservations of Halifax and Cadogan at the Foreign Office.[109] Cadogan later wrote that de Gaulle was "that c*** of a fellow", but other Foreign Office figures Robert Vansittart and Oliver Harvey were quite sympathetic, as was The Times which gave de Gaulle plenty of coverage.[110]
De Gaulle had little success in attracting the support of major figures. Ambassador Charles Corbin, who had strongly supported the mooted Anglo-French Union on 16 June, resigned from the French Foreign Office but retired to South America. Alexis Leger, Secretary-General at the Quai d'Orsay (who hated Reynaud for sacking him) came to London but went on to the US. Roland de Margerie stayed in France despite his opposition to the armistice. De Gaulle received support from Captain Tissier and André Dewavrin (both of whom had been fighting in Norway prior to joining the Free French), Gaston Palewski, Maurice Schumann, and the jurist René Cassin.[111]
Pétain's government was recognised by the US, the USSR, and the Papacy, and controlled the French fleet and the forces in almost all her colonies. At this time de Gaulle's followers consisted of a secretary of limited competence, three colonels, a dozen captains, a famous law professor (Cassin), and three battalions of legionnaires who had agreed to stay in Britain and fight for him. For a time the New Hebrides were the only French colony to back de Gaulle.[112] On 30 June 1940 Admiral Muselier joined the Free French.[113]
De Gaulle initially reacted angrily to news of the Royal Navy's attack on the French fleet (3 July); Pétain and others wrongly blamed him for provoking it by his 26 June speech (in fact it had been planned at least as early as 16 June). He considered withdrawing to Canada to live as a private citizen and waited five days before broadcasting. Spears called on de Gaulle on 5 July and found him "astonishingly objective" and acknowledging that it was the right thing from the British point of view. Spears reported to Churchill that de Gaulle had shown "a splendid dignity". In his broadcast of 8 July he spoke of the "pain and anger" caused by the attack and that it was a "hateful tragedy not a glorious battle", but that one day the enemy would have used the ships against England or the French Empire, and that the defeat of England would mean "bondage forever" for France. "Our two ancient nations...remain bound to one another. They will either go down both together or both together they will win".[114]
On Bastille Day (14 July) 1940 de Gaulle led a group of between 200 and 300 sailors to lay a wreath at the statue of Ferdinand Foch at Grosvenor Gardens.[115] A mass of anonymous flowers were left on his mother's grave on 16 July 1940, suggesting he was not without admirers in France.[116]
From 22 July 1940 de Gaulle used 4 Carlton Gardens in central London as his London headquarters. His family had left Brittany (the other ship which left at the same time was sunk) and lived for a time at Petts Wood. As his daughter Anne was terrified by the Blitz they moved to Ellesmere in Shropshire, a four-hour journey from London and where de Gaulle was only able to visit them once a month. His wife and daughter also lived for a time in the country at Rodinghead House, Little Gaddesden, in Hertfordshire, 45 kilometres (28 miles) from central London. De Gaulle lived at the Connaught Hotel in London, then from 1942 to 1944 he lived in Hampstead, North London.[117]
The small audience of the 18 June appeal grew for later speeches,[27]: 5–6  and the press by early August described Free French military as fighting under de Gaulle's command,[118] although few in France knew anything about him. (When he returned to France after liberation, people sometimes greeted another, more senior officer with him as de Gaulle, believing that he must be a five-star general.) Many thought that "Degaule", "Dugaul", or "Gaul" was a nom de guerre, disbelieving that the mysterious general describing himself as the nation's liberator was called the ancient name of France. Agnès Humbert, who had heard the 18 June speech, wrote in her diary of distributing pamphlets supporting de Gaulle's cause, despite his being[27]: 5–6 
a leader of whom we know absolutely nothing, of whom none of us has ever seen a photograph. In the whole course of human history, has there ever been anything quite like it? Thousands upon thousands of people, fired by blind faith, following an unknown figure. Perhaps this strange anonymity is even an asset: the mystery of the unknown!The Vichy regime had already sentenced de Gaulle to four years' imprisonment; on 2 August 1940 he was condemned to death by court martial in absentia,[119] although Pétain commented that he would ensure that the sentence was never carried out.[105] De Gaulle said of the sentence, "I consider the act of the Vichy men as void; I shall have an explanation with them after the victory".[118] He and Churchill reached agreement on 7 August 1940, that Britain would fund the Free French, with the bill to be settled after the war (the financial agreement was finalised in March 1941). A separate letter guaranteed the territorial integrity of the French Empire.[120]
General Georges Catroux, Governor of French Indo-China (which was increasingly coming under Japan's thumb), disapproved of the armistice and congratulated de Gaulle, whom he had known for many years. He was sacked by Vichy and arrived in London on 31 August; de Gaulle had gone to Dakar, but they met in Chad four weeks later. He was the most senior military figure to defect to the Free French.[113]
De Gaulle's support grew out of a base in the colonial French Equatorial Africa. In the fall of 1940, the colonial empire largely supported the Vichy regime. Félix Éboué, governor of Chad, switched his support to General de Gaulle in September. Encouraged, de Gaulle traveled to Brazzaville in October, where he announced the formation of an Empire Defense Council[121] in his "Brazzaville Manifesto",[122] and invited all colonies still supporting Vichy to join him and the Free French forces in the fight against Germany, which most of them did by 1943.[121][123]
In October 1940, after talks between the Foreign Office and Louis Rougier, de Gaulle was asked to tone down his attacks on Pétain. On average he spoke on BBC radio three times a month.[124]
De Gaulle established the Order of Liberation in Brazzaville in November 1940.[125]
Prime Minister Pétain moved the government to Vichy (2 July) and had the National Assembly (10 July) vote to dissolve itself and give him dictatorial powers, making the beginning of his Révolution nationale (National Revolution) intended to "reorient" French society. This was the dawn of the Vichy regime.[105]
De Gaulle's subsequent speeches reached many parts of the territories under the Vichy regime, helping to rally the French resistance movement and earning him much popularity amongst the French people and soldiers. The British historian Christopher Flood noted that there were major differences between the speeches of de Gaulle and Pétain, which reflected their views on themselves and of France. Pétain always used the personal pronoun je, portrayed himself as both a Christ-like figure sacrificing himself for France while also assuming a God-like tone of a semi-omniscient narrator who knew truths about the world that the rest of the French did not.[126] De Gaulle began by making frequent use of "I" and "me" in his war-time speeches, but over time, their use declined. Unlike Pétain, de Gaulle never invoked quasi-religious imagery to enhance his prestige.[126] De Gaulle always mentioned Pétain by name whereas Pétain never mentioned de Gaulle directly, referring to him as the "faux ami" ("false friend").[126]
Pétain exonerated the French military of responsibility for the defeat of 1940 which he blamed on the moral decline of French society (thus making his Révolution nationale necessary) while de Gaulle blamed the military chiefs while exonerating French society for the defeat (thus suggesting that French society was nowhere near as rotten as Pétain claimed, making the Révolution nationale unnecessary).[126] Pétain claimed that France had "stupidly" declared war on Germany in 1939 at British prompting while de Gaulle spoke of the entire era since 1914 as "la guerre de trente ans" ("the thirty years' war"), arguing the two world wars were really one with a long truce in between.[126] The only historical figure Pétain invoked was Joan of Arc as a model of self-sacrificing French patriotism in the "eternal struggle" against England whereas de Gaulle invoked virtually every major French historical figure from the ancient Gauls to World War I.[126] De Gaulle's willingness to invoke historical figures from before and after 1789 was meant to suggest that his France was an inclusive France where there was room for both left and right, in contrast to Pétain's demand for national unity under his leadership.[126] Most significantly, Pétain's speeches always stressed the need for France to withdraw from a hostile and threatening world to find unity.[126] By contrast, de Gaulle's speeches, while praising the greatness of France, lacked Pétain's implicit xenophobia; the fight for a free, democratic and inclusive France was always portrayed as part of a wider worldwide struggle for world freedom, where France would be an anchor for a new democratic order.[126]
De Gaulle spoke more of "the Republic" than of "democracy"; before his death René Cassin claimed that he had "succeeded in turning de Gaulle towards democracy". However, claims that de Gaulle was surrounded by Cagoulards, Royalists and other right-wing extremists are untrue. Some of André Dewavrin's closest colleagues were Cagoulards, although Dewavrin always denied that he himself was. Many leading figures of the Free French and the Resistance, e.g., Jean Moulin and Pierre Brossolette, were on the political left.[127] By the end of 1940 de Gaulle was beginning to be recognised as the leader of the Resistance, a position cemented after Jean Moulin's visit to London in autumn 1941.[116] In the summer of 1941 the BBC set aside five minutes per day (later increased to ten) for the Free French, with Maurice Schumann as the main spokesman, and eventually there was a programme "Les Francais parlent aux Francais". A newspaper France was also soon set up.[124]
De Gaulle organised the Free French Forces and the Allies gave increasing support and recognition to de Gaulle's efforts. In London in September 1941 de Gaulle formed the French National Committee, with himself as president. It was an all-encompassing coalition of resistance forces, ranging from conservative Catholics like himself to communists. By early 1942, the "Fighting French" movement, as it was now called, gained rapidly in power and influence; it overcame Vichy in Syria and Lebanon, adding to its base. Dealing with the French communists was a delicate issue, for they were under Moscow's control and the USSR was friendly with Germany in 1940–41 as a result of the Molotov–Ribbentrop Pact. They came into the Free French movement only when Germany invaded Russia in June 1941. De Gaulle's policy then became one of friendship directly with Moscow, but Stalin showed little interest.[128] In 1942, de Gaulle created the Normandie-Niemen squadron, a Free French Air Force regiment, in order to fight on the Eastern Front. It is the only Western allied formation to have fought until the end of the war in the East.[129]
In his dealings with the British and Americans (both referred to as the "Anglo-Saxons", in de Gaulle's parlance), he always insisted on retaining full freedom of action on behalf of France and was constantly on the verge of losing the Allies' support. Some writers have sought to deny that there was deep and mutual antipathy between de Gaulle and British and American political leaders.[130][131]

De Gaulle personally had ambivalent feelings about Britain, possibly in part because of childhood memories of the Fashoda Incident. As an adult he spoke German much better than he spoke English.[132] He had a multilingual translator and driver, Olivia Jordan, from 1940 to 1943.[133] He had thought little of the British Army's contribution to the First World War, and even less of that of 1939–40, and in the 1930s he had been a reader of the journal Action Française which blamed Britain for German foreign policy gains at France's expense.[132] De Gaulle explained his position:Never did the Anglo-Saxons really treat us as real allies. They never consulted us, government to government, on any of their provisions. For political purpose or by convenience, they sought to use the French forces for their own goals, as if these forces belonged to them, alleging that they had provided weapons to them [...] I considered that I had to play the French game, since the others were playing theirs ... I deliberately adopted a stiffened and hardened attitude ....[134]
In addition, de Gaulle harboured a suspicion of the British in particular, believing that they were seeking to seize France's colonial possessions in the Levant. Winston Churchill was often frustrated at what he perceived as de Gaulle's patriotic arrogance, but also wrote of his "immense admiration" for him during the early days of his British exile. Although their relationship later became strained, Churchill tried to explain the reasons for de Gaulle's behaviour in the second volume of his history of World War II:He felt it was essential to his position before the French people that he should maintain a proud and haughty demeanour towards "perfidious Albion", although in exile, dependent upon our protection and dwelling in our midst. He had to be rude to the British to prove to French eyes that he was not a British puppet. He certainly carried out this policy with perseverance.De Gaulle described his adversarial relationship with Churchill in these words: "When I am right, I get angry. Churchill gets angry when he is wrong. We are angry at each other much of the time."[135] On one occasion in 1941 Churchill spoke to him on the telephone. De Gaulle said that the French people thought he was a reincarnation of Joan of Arc, to which Churchill replied that the English had had to burn the last one.[136] Clementine Churchill, who admired de Gaulle, once cautioned him, "General, you must not hate your friends more than you hate your enemies." De Gaulle himself stated famously, "No Nation has friends, only interests."[137]
After his initial support, Churchill, emboldened by American antipathy to the French general, urged his War Cabinet to remove de Gaulle as leader of the Free France. But the War Cabinet warned Churchill that a precipitate break with de Gaulle would have a disastrous effect on the whole resistance movement. By autumn 1943, Churchill had to acknowledge that de Gaulle had won the struggle for leadership of Free France.[138]
De Gaulle's relations with Washington were even more strained. President Roosevelt for a long time refused to recognize de Gaulle as the representative of France, insisting on negotiations with the Vichy government. Roosevelt in particular hoped that it would be possible to wean Pétain away from Germany.[140] Roosevelt maintained recognition of the Vichy regime until late 1942, and saw de Gaulle as an impudent representative of a minority interest.[141]
After 1942, Roosevelt championed General Henri Giraud, more compliant with US interests than de Gaulle, as the leader of the Free France. At the Casablanca Conference (1943), Roosevelt forced de Gaulle to cooperate with Giraud, but de Gaulle was considered as the undisputed leader of the Resistance by the French people and Giraud was progressively deprived of his political and military roles.[142] The British and Soviet governments urged Roosevelt to recognise de Gaulle's provisional government, but Roosevelt delayed doing so as long as possible and even recognised the Italian provisional government before the French one. British and Soviet allies were outraged that the US president unilaterally recognised the new government of a former enemy before de Gaulle's one and both recognised the French government in retaliation, forcing Roosevelt to recognise de Gaulle in late 1944,[143] but Roosevelt managed to exclude de Gaulle from the Yalta Conference.[144] Roosevelt eventually abandoned his plans to rule France as an occupied territory and to transfer French Indochina to the United Nations.[145][146]
On 21 April 1943, de Gaulle was scheduled to fly in a Wellington bomber to Scotland to inspect the Free French Navy. On take-off, the bomber's tail dropped, and the plane nearly crashed into the airfield's embankment. Only the skill of the pilot, who became aware of sabotage on takeoff, saved them. On inspection, it was found that aeroplane's separator rod had been sabotaged, using acid.[147][148] Britain's MI6 investigated the incident, but no one was ever apprehended. Publicly, blame for the incident was cast on German intelligence;[citation needed] however, behind closed doors de Gaulle blamed the Western Allies, and later told colleagues that he no longer had confidence in them.[148]
Working with the French Resistance and other supporters in France's colonial African possessions after Operation Torch in November 1942, de Gaulle moved his headquarters to Algiers in May 1943, leaving Britain to be on French territory. He became first joint head (with the less resolutely independent General Henri Giraud, the candidate preferred by the US who wrongly suspected de Gaulle of being a British puppet) and then—after squeezing out Giraud by force of personality—sole chairman of the French Committee of National Liberation.[67]
De Gaulle was held in high regard by Allied commander General Dwight Eisenhower.[149] In Algiers in 1943, Eisenhower gave de Gaulle the assurance in person that a French force would liberate Paris and arranged that the army division of French General Philippe Leclerc de Hauteclocque would be transferred from North Africa to the UK to carry out that liberation.[149] Eisenhower was impressed by the combativeness of units of the Free French Forces and "grateful for the part they had played in mopping up the remnants of German resistance"; he also detected how strongly devoted many were to de Gaulle and how ready they were to accept him as the national leader.[149]
As preparations for the liberation of Europe gathered pace, the US in particular found de Gaulle's tendency to view everything from the French perspective to be extremely tiresome. Roosevelt, who refused to recognize any provisional authority in France until elections had been held, referred to de Gaulle as "an apprentice dictator", a view backed by a number of leading Frenchmen in Washington, including Jean Monnet, who later became an instrumental figure in the setting up of the European Coal and Steel Community that led to the modern European Union. Roosevelt directed Churchill to not provide de Gaulle with strategic details of the imminent invasion because he did not trust him to keep the information to himself. French codes were considered weak, posing a risk since the Free French refused to use British or American codes.[150] De Gaulle refused to share coded information with the British, who were then obliged secretly to break the codes to read French messages.[151]
Nevertheless, a few days before D-Day, Churchill, whose relationship with the General had deteriorated since he arrived in Britain, decided he needed to keep him informed of developments, and on 2 June he sent two passenger aircraft and his representative, Duff Cooper, to Algiers to bring de Gaulle back to Britain. De Gaulle refused because of Roosevelt's intention to install a provisional Allied military government in the former occupied territories pending elections, but he eventually relented and flew to Britain the next day.
Upon his arrival at RAF Northolt on 4 June 1944 he received an official welcome, and a letter reading "My dear general! Welcome to these shores, very great military events are about to take place!"[150] Later, on his personal train, Churchill informed him that he wanted him to make a radio address, but when informed that the Americans continued to refuse to recognise his right to power in France, and after Churchill suggested he request a meeting with Roosevelt to improve his relationship with the president, de Gaulle became angry, demanding to know why he should "lodge my candidacy for power in France with Roosevelt; the French government exists".[3]
De Gaulle became worried that the German withdrawal from France might lead to a breakdown of law and order in the country and even a possible communist takeover.[150] During the general conversation which followed with those present, de Gaulle was involved in an angry exchange with the Labour minister, Ernest Bevin, and, raising his concerns about the validity of the new currency to be circulated by the Allies after the liberation, de Gaulle commented scornfully, "go and wage war with your false money". De Gaulle was very concerned that an American takeover of the French administration would just provoke a communist uprising.[citation needed]
Churchill then lost his temper, saying that Britain would always be an ally to the United States, and that under the circumstances, if they had to choose between France and the US, Britain would always choose the latter. De Gaulle replied that he realised this would always be the case. The next day, de Gaulle refused to address the French nation as the script again made no mention of his being the legitimate interim ruler of France. It instructed the French people to obey Allied military authorities until elections could be held, and so the row continued, with de Gaulle calling Churchill a "gangster". Churchill accused de Gaulle of treason in the height of battle, and demanded that he be flown back to Algiers "in chains if necessary".[150]
De Gaulle and Churchill had a complex relationship during the wartime period. De Gaulle did show respect and admiration for Churchill, and even some light humorous interactions between the two have been noted by observers such as Duff Cooper, the British Ambassador to the French Committee of Liberation.[152] Churchill explained his support for de Gaulle during the darkest hours, calling him "L'homme du destin".[152][153]
In Casablanca in 1943, Churchill supported de Gaulle as the embodiment of a French Army that was otherwise defeated, stating that "De Gaulle is the spirit of that Army. Perhaps the last survivor of a warrior race."[152] Churchill supported de Gaulle as he had been one of the first major French leaders to reject Nazi German rule outright, stating in August 1944 that "I have never forgotten, and can never forget, that he [de Gaulle] stood forth as the first eminent Frenchman to face the common foe in what seemed to be the hour of ruin of his country and possibly, of ours."[152]
In the years to come, the sometimes hostile, sometimes friendly dependent wartime relationship of de Gaulle and his future political peers reenacted the historical national and colonial rivalry and lasting enmity between the French and the British,[154] and foreshadowed the deep distrust of France for post-war Anglo-American partnerships.
De Gaulle ignored les Anglo-Saxons, and proclaimed the authority of Free France over the metropolitan territory the next day.[when?] Under the leadership of General de Lattre de Tassigny, France fielded an entire army – a joint force of Free French together with French colonial troops from North Africa – on the Western Front. Initially landing as part of Operation Dragoon, in the south of France, the French First Army helped to liberate almost one third of the country and participated in the invasion and occupation of Germany. As the invasion slowly progressed and the Germans were pushed back, de Gaulle made preparations to return to France.
On 14 June 1944, he left Britain for France for what was supposed to be a one-day trip. Despite an agreement that he would take only two staff, he was accompanied by a large entourage with extensive luggage, and although many rural Normans remained mistrustful of him, he was warmly greeted by the inhabitants of the towns he visited, such as the badly damaged Isigny. Finally he arrived at the city of Bayeux, which he now proclaimed as the capital of Free France. Appointing his Aide-de-Camp Francois Coulet as head of the civil administration, de Gaulle returned to the UK that same night on a French destroyer, and although the official position of the supreme military command remained unchanged, local Allied officers found it more practical to deal with the fledgling administration in Bayeux in everyday matters.[150] De Gaulle flew to Algiers on 16 June and then went on to Rome to meet the Pope and the new Italian government. At the beginning of July he at last visited Roosevelt in Washington, where he received the 17-gun salute of a senior military leader rather than the 21 guns of a visiting head of state. The visit was 'devoid of trust on both sides' according to the French representative,[3] however, Roosevelt did make some concessions towards recognising the legitimacy of the Bayeux administration.
Meanwhile, with the Germans retreating in the face of the Allied onslaught, harried all the way by the resistance, there were widespread instances of revenge attacks on those accused of collaboration. A number of prominent officials and members of the feared Milice were murdered, often by exceptionally brutal means, provoking the Germans into appalling reprisals, such as in the destruction of the village of Oradour-sur-Glane and the killing of its 642 inhabitants.[155]
Liberation of the French capital was not high on the Allies' list of priorities as it had comparatively little strategic value, but both de Gaulle and the commander of the French 2nd Armored Division, General Philippe Leclerc were still extremely concerned about a communist takeover. De Gaulle successfully lobbied for Paris to be made a priority for liberation on humanitarian grounds and obtained from Allied Supreme Commander General Dwight D. Eisenhower an agreement that French troops would be allowed to enter the capital first. A few days later, General Leclerc's division entered the outskirts of the city, and after six days of fighting in which the resistance played a major part, the German garrison of 5000 men surrendered on 25 August, although some sporadic outbreaks of fighting continued for several days. General Dietrich von Choltitz, the commander of the garrison, was instructed by Adolf Hitler to raze the city to the ground, however, he simply ignored the order and surrendered his forces.[156]
It was fortunate for de Gaulle that the Germans had forcibly removed members of the Vichy government and taken them to Germany a few days earlier on 20 August; it allowed him to enter Paris as a liberator in the midst of the general euphoria,[157] but there were serious concerns that communist elements of the resistance, which had done so much to clear the way for the military, would try to seize the opportunity to proclaim their own 'Peoples' Government' in the capital. De Gaulle made contact with Leclerc and demanded the presence of the 2nd Armoured Division to accompany him on a massed parade down the Champs-Élysées, "as much for prestige as for security".[157] This was in spite of the fact that Leclerc's unit was fighting as part of the American 1st Army and were under strict orders to continue their next objective without obeying orders from anyone else. In the event, the American General Omar Bradley decided that Leclerc's division would be indispensable for the maintenance of order and the liquidation of the last pockets of resistance in the French capital. Earlier, on 21 August, de Gaulle had appointed his military advisor General Marie-Pierre Koenig as Governor of Paris.
As his procession came along the Place de la Concorde on Saturday 26 August, it came under machine gun fire by Vichy militia and fifth columnists. Later, on entering the Notre Dame Cathedral to be received as head of the provisional government by the Committee of Liberation, loud shots broke out again, and Leclerc and Koenig tried to hustle him through the door, but de Gaulle shook off their hands and never faltered. While the battle began outside, he walked slowly down the aisle. Before he had gone far a machine pistol fired down from above, at least two more joined in, and from below the FFI and police fired back.[158] A BBC correspondent who was present reported;
... the General is being presented to the people. He is being received...they have opened fire! ... firing started all over the place ... that was one of the most dramatic scenes I have ever seen. ... General de Gaulle walked straight ahead into what appeared to me to be a hail of fire ... but he went straight ahead without hesitation, his shoulders flung back, and walked right down the centre aisle, even while the bullets were pouring about him. It was the most extraordinary example of courage I have ever seen ... there were bangs, flashes all about him, yet he seemed to have an absolutely charmed life.[159]De Gaulle himself though wrote, "There were no bullets whistling around my ears." (Aucune balle ne siffle à mes oreilles.) He thought the shots were probably over-excited troops firing at shadows. No culprits, if there were any, were ever identified.[160]
Later, in the great hall of the Hôtel de Ville, de Gaulle was greeted by a jubilant crowd and, proclaiming the continuity of the Third Republic, delivered a famous proclamation;
Paris! Paris outraged, Paris broken, Paris martyred, but Paris liberated! Liberated by itself, liberated by its people with the assistance of the armies of France, with the support and assistance of the whole of France! ... The enemy is faltering but he is not yet beaten. He is still on our soil. It will not suffice that we, with the assistance of our dear and admirable allies, will have chased him from our home in order to be satisfied after what has happened. We want to enter his territory, as is fitting, as conquerors. ... It is for this revenge, this vengeance and this justice, that we will continue to fight until the last day, until the day of the total and complete victory.[161]That evening, the Wehrmacht launched a massive aerial and artillery barrage of Paris in revenge, leaving several thousand dead or injured.[157] The situation in Paris remained tense, and a few days later de Gaulle, still unsure of the trend of events asked General Eisenhower to send some American troops into Paris as a show of strength. This he did 'not without some satisfaction',[157] and so, on 29 August, the US 28th Infantry Division was rerouted from its journey to the front line and paraded down the Champs Elysees.[162]
The same day, Washington and London agreed to accept the position of the Free French. The following day General Eisenhower gave his de facto blessing with a visit to the General in Paris.[163]
Roosevelt insisted that an Allied Military Government for Occupied Territories (AMGOT) should be implemented in France, but this was opposed by both the Secretary of War and the Under-Secretary for War, as well as by Eisenhower, who had been strongly opposed to the imposition of AMGOT in North Africa. Eisenhower, unlike Roosevelt, wanted to cooperate with de Gaulle, and he secured a last-minute promise from the President on the eve of D-Day that the Allied officers would not act as military governors and would instead cooperate with the local authorities as the Allied forces liberated French Territory. De Gaulle would later claim in his memoirs that he blocked AMGOT.[164]
With the prewar parties and most of their leaders discredited, there was little opposition to de Gaulle and his associates forming an interim administration. In order not to be seen as presuming on his position in such austere times, de Gaulle did not use one of the grand official residences such as Hotel de Matignon or the presidential palace on the Elysee, but resided briefly in his old office at the War Ministry. When he was joined by his wife and daughters a short while later, they moved into a small state-owned villa on edge of Bois de Boulogne which had once been set aside for Hermann Göring.[165]
Living conditions immediately after the liberation were even worse than under German rule. About 25% of the city was in ruins and public services and fuel were almost nonexistent. Large-scale public demonstrations erupted all over France, protesting the apparent lack of action at improving the supply of food, while in Normandy, bakeries were pillaged. The problem was not French agriculture, which had largely continued operating without problems, but the near-total breakdown of the country's infrastructure. Large areas of track had been destroyed by bombing, most modern equipment, rolling stock, lorries and farm animals had been taken to Germany and all the bridges over the Seine, the Loire and the Rhone between Paris and the sea had been destroyed. The black market pushed real prices to four times the level of 1939, causing the government to print money to try to improve the money supply, which only added to inflation.[165]
On 10 November 1944, Churchill flew to Paris to a reception by de Gaulle and the two together were greeted by thousands of cheering Parisians on the next day.[152] Harold Nicolson stated that Anthony Eden told him that "not for one moment did Winston stop crying, and that he could have filled buckets by the time he received the Freedom of Paris."[152] He said "they yelled for Churchill in a way that he has never heard any crowd yell before." At an official luncheon, de Gaulle said, "It is true that we would not have seen [the liberation] if our old and gallant ally England, and all the British dominions under precisely the impulsion and inspiration of those we are honouring today, had not deployed the extraordinary determination to win, and that magnificent courage which saved the freedom of the world. There is no French man or woman who is not touched to the depths of their hearts and souls by this."[152]
After the celebrations had died down, de Gaulle began conferring with leading Resistance figures who, with the Germans gone, intended to continue as a political and military force, and asked to be given a government building to serve as their headquarters. The Resistance, in which the Communists were competing with other trends for leadership, had developed its own manifesto for social and political change known as the National Council of the Resistance (CNR) Charter, and wanted special status to enter the army under their own flags, ranks and honours. Despite their decisive support in backing him against Giraud, de Gaulle disappointed some of the Resistance leaders by telling them that although their efforts and sacrifices had been recognised, they had no further role to play and, that unless they joined the regular army, they should lay down their arms and return to civilian life.[165]
Believing them to be a dangerous revolutionary force, de Gaulle moved to break up the liberation committees and other militias. The communists were not only extremely active, but they received a level of popular support that disturbed de Gaulle. As early as May 1943, the US Secretary of State Cordell Hull had written to Roosevelt urging him to take action to attempt to curb the rise of communism in France.[7]
On 10 September 1944 the Provisional Government of the French Republic, or Government of National Unanimity, formed. It included many of de Gaulle's Free French associates such as Gaston Palewski, Claude Guy, Claude Mauriac and Jacques Soustelle, together with members of the main parties, which included the Socialists and a new Christian Democratic Party, the MRP under the leadership of Georges Bidault, who served as Foreign Minister. The president of the prewar Senate Jules Jeanneney was brought back as second-ranking member, but because of their links with Russia, de Gaulle allowed the Communists only two minor positions in his government. While they were now a major political force with over a million members, of the full cabinet of 22 men, only Augustin Laurent and Charles Tillon—who as head of Francs-Tireurs et Partisans had been one of the most active members of the resistance—were given ministries. However, de Gaulle did pardon the Communists' leader Maurice Thorez, who had been sentenced to death in absentia by the French government for desertion. On his return home from Russia, Thorez delivered a speech supporting de Gaulle in which he said that for the present, the war against Germany was the only task that mattered.
There were also a number of new faces in the government, including a literary academic, Georges Pompidou, who had written to one of de Gaulle's recruiting agents offering his services, and Jean Monnet, who in spite of his past opposition to the General now recognized the need for unity and served as Commissioner for Economic Planning. Of equal rank to ministers and answerable only to the prime minister, a number of Commissioners of the Republic (Commissaires de la République) were appointed to re-establish the democratic institutions of France and to extend the legitimacy of the provisional government. A number of former Free French associates served as commissioners, including Henri Fréville, Raymond Aubrac and Michel Debré, who was charged with reforming the civil service. Controversially, de Gaulle also appointed Maurice Papon as Commissioner for Aquitaine in spite of his involvement in the deportation of Jews while serving as a senior police official in the Vichy regime during the occupation. (Over the years, Papon remained in high official positions but continued to be implicated in controversial events such as the Paris massacre of 1961, eventually being convicted of crimes against humanity in 1998.)
In social policy, legislation was introduced[by whom?] in February 1945 that provided for the establishment of works committees in all private industrial establishments employing more than 50 (originally more than 100) people.[166]
De Gaulle's policy was to postpone elections as long as 2.6 million French were in Germany as prisoners of war and forced laborers. In mid-September, he embarked upon a tour of major provincial cities to increase his public profile and to help cement his position. Although he received a largely positive reception from the crowds who came out to see him, he reflected that only a few months previously the very same people had come out to cheer Marshal Pétain when he was serving the Vichy regime. Raymond Aubrac said that the General showed himself to be ill-at-ease at social functions; in Marseille and Lyon he became irate when he had to sit next to former Resistance leaders and also voiced his distaste for the rowdy, libidinous behavior of French youths during the Maquisard parades which preceded his speech.[165] When he reached Toulouse, de Gaulle also had to confront the leaders of a group which had proclaimed themselves to be the provincial government of the city.[3]
During the tour, de Gaulle showed his customary lack of concern for his own safety by mixing with the crowds and thus making himself an easy target for an assassin. Although he was naturally shy, the good use of amplification and patriotic music enabled him to deliver his message that though all of France was fragmented and suffering, together they would rise again. During every speech he would stop halfway through to invite the crowd to join him in singing La Marseillaise, before continuing and finishing by raising his hands in the air and crying "Vive la France!"[165]
As the war entered the final stages, the nation was forced to confront the reality of how many of its people had behaved under German rule. In France, collaborators were more severely punished than in most other occupied countries.[167] Immediately after the liberation, countless women accused of aiding, abetting, and taking German soldiers as lovers were subjected to public humiliations such as being shaved bald and paraded through the streets in their underwear. Many others were simply attacked by lynch mobs. With so many of their former members having been hunted down and killed by the Nazis and paramilitary Milice, the Partisans had already summarily executed an estimated 4,500 people,[167] and the Communists in particular continued to press for severe action against collaborators. In Paris alone, over 150,000 people were at some time detained on suspicion of collaboration, although most were later released. Famous figures accused included the industrialist Louis Renault, the actress Arletty, who had lived openly with a German officer in the Ritz, the opera star Tino Rossi, the chanteuse Édith Piaf, the stage actor Sacha Guitry and Coco Chanel, who was briefly detained but fled to Switzerland.[3]
Keenly aware of the need to seize the initiative and to get the process under firm judicial control, de Gaulle appointed Justice Minister François de Menthon to lead the Legal Purge (Épuration légale) to punish traitors and to clear away the traces of the Vichy regime. Knowing that he would need to reprieve many of the 'economic collaborators'—such as police and civil servants who held minor roles under Vichy in order to keep the country running as normally as possible—he assumed, as head of state, the right to commute death sentences.[3] Of the near 2,000 people who received the death sentence from the courts, fewer than 800 were executed. De Gaulle commuted 998 of the 1,554 capital sentences submitted before him, including all those involving women. Many others were given jail terms or had their voting rights and other legal privileges taken away. It is generally agreed that the purges were conducted arbitrarily, with often absurdly severe or overly lenient punishments being handed down.[165] It was also notable that the less well-off people who were unable to pay for lawyers were more harshly treated. As time went by and feelings grew less intense, a number of people who had held fairly senior positions under the Vichy government—such as Maurice Papon and René Bousquet—escaped consequences by claiming to have worked secretly for the resistance or to have played a double game, working for the good of France by serving the established order.[165]
Later, there was the question of what to do with the former Vichy leaders when they were finally returned to France. Marshal Pétain and Maxime Weygand were war heroes from World War I and were now extremely old; convicted of treason, Pétain received a death sentence which his old protégé de Gaulle commuted to life imprisonment, while Weygand was eventually acquitted. Three Vichy leaders were executed. Joseph Darnand, who became an SS officer and led the Milice paramilitaries who hunted down members of the Resistance, was executed in October 1945. Fernand de Brinon, the third-ranking Vichy official, was found guilty of war crimes and executed in April 1947. The two trials of the most infamous collaborator of all, Pierre Laval, who was heavily implicated in the murder of Jews, were widely criticised as being unfair for depriving him of the opportunity to properly defend himself, although Laval antagonized the court throughout with his bizarre behavior. He was found guilty of treason in May 1945 and de Gaulle was adamant that there would be no commuting the death sentence, saying that Laval's execution was "an indispensable symbolic gesture required for reasons of state". There was a widespread belief, particularly in the years that followed, that de Gaulle was trying to appease both the Third Republic politicians and the former Vichy leaders who had made Laval their scapegoat.[165]
The winter of 1944–45 was especially difficult for most of the population. Inflation showed no sign of slowing down and food shortages were severe. The prime minister and the other Gaullists were forced to try to balance the desires of ordinary people and public servants for a return to normal life with pressure from Bidault's MRP and the Communists for the large scale nationalisation programme and other social changes that formed the main tenets of the CNR Charter. At the end of 1944 the coal industry and other energy companies were nationalised, followed shortly afterwards by major banks and finance houses, the merchant navy, the main aircraft manufacturers, airlines and a number of major private enterprises such as the Renault car company at Boulogne-Billancourt, whose owner had been implicated as a collaborator and accused of having made huge profits working for the Nazis.[3] In some cases, unions, feeling that things were not progressing quickly enough, took matters into their own hands, occupying premises and setting up workers' committees to run the companies.[165] Women were also allowed the vote for the first time, a new social security system was introduced to cover most medical costs, unions were expanded and price controls introduced to try to curb inflation. At de Gaulle's request, the newspaper Le Monde was founded in December 1944 to provide France with a quality daily journal similar to those in other countries. Le Monde took over the premises and facilities of the older Le Temps, whose independence and reputation had been badly compromised during the Vichy years.[3]
During this period there were a number of minor disagreements between the French and the other Allies. The British ambassador to France Duff Cooper said that de Gaulle seemed to seek out real or imagined insults to take offence at whatever possible.[3] De Gaulle believed Britain and the US were intending to keep their armies in France after the war and were secretly working to take over its overseas possessions and to prevent it from regaining its political and economic strength. In late October he complained that the Allies were failing to adequately arm and equip the new French army and instructed Bidault to use the French veto at the European Council.[3]
On Armistice Day in 1945, Winston Churchill made his first visit to France since the liberation and received a good reception in Paris where he laid a wreath to Georges Clemenceau. The occasion also marked the first official appearance of de Gaulle's wife Yvonne, but the visit was less friendly than it appeared. De Gaulle had instructed that there be no excessive displays of public affection towards Churchill and no official awards without his prior agreement. When crowds cheered Churchill during a parade down the Elysee, de Gaulle was heard to remark, "Fools and cretins! Look at the rabble cheering the old bandit".[165]
With the Russian forces making more rapid advances into German-held territory than the West, there was a sudden public realisation that the Soviet Union was about to dominate large parts of eastern Europe. In fact, in October 1944, Churchill had reluctantly agreed to allow Bulgaria and Romania, already occupied by the Red Army, and Hungary to fall under the Soviet sphere of influence after the war, with shared influence in Yugoslavia.[168] The UK was to retain hegemony over Greece, although there had been no agreement over Poland, whose eastern territories were already in Soviet hands under the Molotov–Ribbentrop Pact with Germany, and which retained a government in exile in London.[168] De Gaulle had not been invited to any of the 'Big Three' Conferences, although the decisions made by Stalin, Churchill and Roosevelt in dividing up Europe were of huge importance to France.[citation needed]
De Gaulle and his Foreign Minister Bidault stated that they were not in favour of a 'Western Bloc' that would be separate from the rest of Europe, and hoped that a resurgent France might be able to act as a 'third force' in Europe to temper the ambitions of the two emerging superpowers, America and Soviet Union.[7] He began seeking an audience with Stalin to press his 'facing both ways' policy, and finally received an invitation in late 1944. In his memoirs, de Gaulle devoted 24 pages to his visit to the Soviet Union, but a number of writers make the point that his version of events differs significantly from that of the Soviets, of foreign news correspondents, and with their own eyewitness accounts.[7][165]
De Gaulle wanted access to German coal in the Ruhr as reparations after the war, the left bank of the Rhine to be incorporated into French territory, and for the Oder-Neisse line in Poland to become Germany's official eastern border. De Gaulle began by requesting that France enter into a treaty with the Soviet Union on this basis, but Stalin, who remained in constant contact with Churchill throughout the visit, said that it would be impossible to make such an agreement without the consent of Britain and America. He suggested that it might be possible to add France's name to the existing Anglo-Soviet Agreement if they agreed to recognise the Soviet-backed provisional Polish government known as the Lublin Committee as rightful rulers of Poland, but de Gaulle refused on the grounds that this would be 'un-French', as it would mean it being a junior partner in an alliance.[7] During the visit, de Gaulle accompanied the deputy Soviet leader Vyacheslav Molotov on a tour of the former battleground at Stalingrad, where he was deeply moved at the scene of carnage he witnessed and surprised Molotov by referring to "our joint sacrifice".[7]
Though the treaty which was eventually signed by Bidault and Molotov carried symbolic importance in that it enabled de Gaulle to demonstrate that he was recognised as the official head of state and show that France's voice was being heard abroad, it was of little relevance to Stalin due to France's lack of real political and military power; it did not affect the outcome of the post-war settlement. Stalin later commented that like Churchill and Roosevelt, he found de Gaulle to be awkward and stubborn and believed that he was 'not a complicated person' (by which he meant that he was an old-style nationalist).[7] Stalin also felt that he lacked realism in claiming the same rights as the major powers and did not object to Roosevelt's refusal to allow de Gaulle to attend the 'Big Three' conferences that were to come at Yalta and Potsdam.
At the end of 1944 French forces continued to advance as part of the American armies, but during the Ardennes Offensive there was a dispute over Eisenhower's order to French troops to evacuate Strasbourg, which had just been liberated so as to straighten the defensive line against the German counterattack.[7] Strasbourg was an important political and psychological symbol of French sovereignty in Alsace and Lorraine, and de Gaulle, saying that its loss would bring down the government, refused to allow a retreat, predicting that "Strasbourg will be our Stalingrad".[3]
By early 1945 it was clear that the price controls which had been introduced to control inflation had only served to boost the black market and prices continued to move ever upwards. By this time the army had swelled to over 1.2 million men and almost half of state expenditure was going to military spending.[165] De Gaulle was faced with his first major ministerial dispute when the very able but tough-minded economics minister Pierre Mendès France demanded a programme of severe monetary reform which was opposed by the Finance Ministry headed by Aime Lepercq, who favoured a programme of heavy borrowing to stimulate the economy.[165] When de Gaulle, knowing there would be little appetite for further austerity measures sided with Lepercq, Mendès France tendered his resignation, which was rejected because de Gaulle knew he needed him. Lepercq was killed in a road accident a short time afterwards and was succeeded by Pleven, but when in March, Mendès France asked unsuccessfully for taxes on capital earnings and for the blocking of certain bank accounts, he again offered his resignation and it was accepted.[165]
De Gaulle was never invited to the summit conferences of Allied leaders such as Yalta and Potsdam. He never forgave the Big Three leaders (Churchill, Roosevelt and Stalin) for their neglect and continued to rage against it as having been a negative factor in European politics for the rest of his life.[7]
After the Rhine crossings, the French First Army captured a large section of territory in southern Germany, but although this later allowed France to play a part in the signing of the German surrender, Roosevelt in particular refused to allow any discussion about de Gaulle participating in the Big Three conferences that would shape Europe in the post-war world. Churchill pressed hard for France to be included 'at the inter-allied table', but on 6 December 1944 the American president wired both Stalin and Churchill to say that de Gaulle's presence would "merely introduce a complicating and undesirable factor".[169]
At the Yalta Conference in February 1945, despite Stalin's opposition, Churchill and Roosevelt insisted that France be allowed a post-war occupation zone in Germany, and also made sure that it was included among the five nations that invited others to the conference to establish the United Nations.[168] This was important because it guaranteed France a permanent seat on the UN Security Council, a prestigious position that, despite pressure from emerging nations, it still holds today.
On his way back from Yalta, Roosevelt asked de Gaulle to meet him in Algiers for talks. The General refused, believing that there was nothing more to be said, and for this he received a rebuke from Georges Bidault and from the French press, and a severely angered Roosevelt criticised de Gaulle to Congress. Soon after, on 12 April 1945, Roosevelt died, and despite their uneasy relationship de Gaulle declared a week of mourning in France and forwarded an emotional and conciliatory letter to the new American president, Harry S. Truman, in which he said of Roosevelt, "all of France loved him".[3]
De Gaulle's relationship with Truman was to prove just as difficult as it had been with Roosevelt. With Allied forces advancing deep into Germany, another serious situation developed between American and French forces in Stuttgart and Karlsruhe, when French soldiers were ordered to transfer the occupation zones to US troops. Wishing to retain as much German territory in French hands as possible, de Gaulle ordered his troops, who were using American weapons and ammunition, to resist, and an armed confrontation seemed imminent.[165] Truman threatened to cut off supplies to the French army and to take the zones by force, leaving de Gaulle with little choice but to back down. De Gaulle never forgave Truman and hinted he would work closely with Stalin, leading Truman to tell his staff, "I don't like the son of a bitch."[170]
The first visit by de Gaulle to Truman in the U.S. was not a success. Truman told his visitor that it was time that the French got rid of the Communist influence from its government, to which de Gaulle replied that this was France's own business.[7] But Truman, who admitted that his feelings towards the French were becoming "less and less friendly", went on to say that under the circumstances, the French could not expect much economic aid and refused to accept de Gaulle's request for control of the west bank of the Rhine. During the argument which followed, de Gaulle reminded Truman that the US was using the French port of Nouméa in New Caledonia as a base against the Japanese.[7]
In May 1945 the German armies surrendered to the Americans and British at Rheims, and a separate armistice was signed with France in Berlin.[167] De Gaulle refused to allow any British participation in the victory parade in Paris. However, among the vehicles that took part was an ambulance from the Hadfield-Spears Ambulance Unit, staffed by French doctors and British nurses. One of the nurses was Mary Spears, who had set up the unit and had worked almost continuously since the Battle of France with Free French forces in the Middle East, North Africa and Italy. Mary's husband was General Edward Spears, the British liaison to the Free French who had personally spirited de Gaulle to safety in Britain in 1940. When de Gaulle saw the Union Flags and Tricolours side by side on the ambulance, and heard French soldiers cheering, "Voilà Spears! Vive Spears!", he ordered that the unit be closed down immediately and its British staff sent home. A number of French troops returned their medals in protest and Mary wrote, "it is a pitiful business when a great man suddenly becomes small."[171]
Another confrontation with the Americans broke out soon after the armistice when the French sent troops to occupy the French-speaking Italian border region of Val d'Aoste. The French commander threatened to open fire on American troops if they tried to stop them, and an irate Truman ordered the immediate end to all arms shipments to France. Truman sent de Gaulle an angry letter saying that he found it unbelievable that the French could threaten to attack American troops after they had done so much to liberate France.[7]
However, de Gaulle was generally well received in the United States immediately after World War II and supported the United States in public comments. He visited New York City on 27 August 1945 to great welcome by thousands of people of the city and its mayor Fiorello La Guardia.[172][173] On that day, de Gaulle wished "Long live the United States of America". He visited New York City Hall and Idlewild Airport (now John F. Kennedy International Airport), and presented LaGuardia with the Grand Croix of the Legion of Honour award.[172][173]
On VE Day, there were also serious riots in French Tunisia. A dispute with Britain over control of Syria and Lebanon quickly developed into an unpleasant diplomatic incident that demonstrated France's weaknesses. In April, de Gaulle sent General Beynet to establish an air base in Syria and a naval base in Lebanon, provoking an outbreak of nationalism in which some French nationals were attacked and killed. On 20 May, French artillery and warplanes fired on demonstrators in Damascus. After several days, upwards of 800 Syrians lay dead.[174]
Churchill's relationship with de Gaulle was now at rock bottom. In January he told a colleague that he believed that de Gaulle was "a great danger to peace and for Great Britain. After five years of experience, I am convinced that he is the worst enemy of France in her troubles ... he is one of the greatest dangers to European peace.... I am sure that in the long run no understanding will be reached with General de Gaulle".[3]: 287 
On 31 May, Churchill told de Gaulle "immediately to order French troops to cease fire and withdraw to their barracks". British forces moved in and forced the French to withdraw from the city; they were then escorted and confined to barracks.[175] With this political pressure added, the French ordered a ceasefire; De Gaulle raged but France was isolated and suffering a diplomatic humiliation. The secretary of the Arab League Edward Atiyah said, "France put all her cards and two rusty pistols on the table".[176] De Gaulle saw it as a heinous Anglo-Saxon conspiracy: he told the British ambassador Duff Cooper, "I recognise that we are not in a position to wage war against you, but you have betrayed France and betrayed the West. That cannot be forgotten".[3]: 42–47 
At the Potsdam Conference in July, to which de Gaulle was not invited, a decision was made to divide Vietnam, which had been a French colony for over a hundred years, into British and Chinese spheres of influence.[168] Soon after the surrender of Japan in August 1945, de Gaulle sent the French Far East Expeditionary Corps to re-establish French sovereignty in French Indochina. However, Ho Chi Minh, leader of the Viet Minh independence movement, declared independence of Vietnam in September, and the First Indochina War broke out that lasted until France was defeated in 1954.[177]
Since the liberation, the only parliament in France had been an enlarged version of the Algiers Provisional Consultative Assembly, and at last, in October 1945, elections were held for a new Constituent Assembly whose main task was to provide a new constitution for the Fourth Republic. De Gaulle favoured a strong executive for the nation,[19] but all three of the main parties wished to severely restrict the powers of the president. The Communists wanted an assembly with full constitutional powers and no time limit, whereas de Gaulle, the Socialists and the Popular Republican Movement (MRP) advocated one with a term limited to only seven months, after which the draft constitution would be submitted for another referendum.[178]
In the election, the second option was approved by 13 million of the 21 million voters. The big three parties won 75% of the vote, with the Communists winning 158 seats, the MRP 152 seats, the Socialists 142 seats and the remaining seats going to the various far right parties.
On 13 November 1945, the new assembly unanimously elected Charles de Gaulle head of the government, but problems immediately arose when it came to selecting the cabinet, due to his unwillingness once more to allow the Communists any important ministries. The Communists, now the largest party and with their charismatic leader Maurice Thorez back at the helm, were not prepared to accept this for a second time, and a furious row ensued, during which de Gaulle sent a letter of resignation to the speaker of the Assembly and declared that he was unwilling to trust a party that he considered to be an agent of a foreign power (Russia) with authority over the police and armed forces of France.[7]
Eventually, the new cabinet was finalised on 21 November, with the Communists receiving five out of the twenty-two ministries, and although they still did not get any of the key portfolios, de Gaulle believed that the draft constitution placed too much power in the hands of parliament with its shifting party alliances. One of his ministers said he was "a man equally incapable of monopolizing power as of sharing it".[179]
De Gaulle outlined a programme of further nationalisations and a new economic plan which were passed, but a further row came when the Communists demanded a 20 percent reduction in the military budget. Refusing to "rule by compromise", de Gaulle once more threatened to resign. There was a general feeling that he was trying to blackmail the assembly into complete subservience by threatening to withdraw his personal prestige which he insisted was what alone kept the ruling coalition together.[165] Although the MRP managed to broker a compromise which saw the budget approved with amendments, it was little more than a stop-gap measure.[7]
Barely two months after forming the new government, de Gaulle abruptly resigned on 20 January 1946. The move was called "a bold and ultimately foolish political ploy", with de Gaulle hoping that as a war hero, he would be soon brought back as a more powerful executive by the French people.[180] However, that did not turn out to be the case. With the war finally over, the initial period of crisis had passed. Although there were still shortages, particularly of bread, France was now on the road to recovery, and de Gaulle suddenly did not seem so indispensable. The Communist publication Combat wrote, "There was no cataclysm, and the empty plate didn't crack".[165]
After monopolizing French politics for six years, Charles de Gaulle suddenly dropped out of sight, and returned to his home in Colombey to write his war memoirs. De Gaulle had told Pierre Bertaux in 1944 that he planned to retire because "France may still one day need an image that is pure ... If Joan of Arc had married, she would no longer have been Joan of Arc".[181] The famous opening paragraph of Mémoires de guerre begins by declaring, "All my life, I have had a certain idea of France (une certaine idée de la France)",[182]: 2  comparing his country to an old painting of a Madonna, and ends by declaring that, given the divisive nature of French politics, France cannot truly live up to this ideal without a policy of "grandeur". During this period of formal retirement, however, de Gaulle maintained regular contact with past political lieutenants from wartime and RPF days, including sympathizers involved in political developments in French Algeria, becoming "perhaps the best-informed man in France".[19]
In April 1947, de Gaulle made a renewed attempt to transform the political scene by creating a Rassemblement du Peuple Français (Rally of the French People, or RPF), which he hoped would be able to move above the familiar party squabbles of the parliamentary system. Despite the new party's taking 40 percent of the vote in local elections and 121 seats in 1951, lacking its own press and access to television, its support ebbed away. In May 1953, he withdrew again from active politics,[19] though the RPF lingered until September 1955.[183]
As with all colonial powers France began to lose its overseas possessions amid the surge of nationalism. French Indochina (now Vietnam, Laos, and Cambodia), colonised by France during the mid-19th century, had been lost to the Japanese after the defeat of 1940. De Gaulle had intended to hold on to France's Indochina colony, ordering the parachuting of French agents and arms into Indochina in late 1944 and early 1945 with orders to attack the Japanese as American troops hit the beaches.[184] Although de Gaulle had moved quickly to consolidate French control of the territory during his brief first tenure as president in the 1940s, the communist Vietminh under Ho Chi Minh began a determined campaign for independence from 1946 onwards. The French fought a bitter seven-year war (the First Indochina War) to hold on to Indochina. It was largely funded by the United States and grew increasingly unpopular, especially after the stunning defeat at the Battle of Dien Bien Phu. France pulled out that summer under Prime Minister Pierre Mendès France.
The independence of Morocco and Tunisia was arranged by Mendès France and proclaimed in March 1956. Meanwhile, in Algeria some 350,000 French troops were fighting 150,000 combatants of the Algerian Liberation Movement (FLN). Within a few years, the Algerian war of independence reached a summit in terms of savagery and bloodshed and threatened to spill into metropolitan France itself.
Between 1946 and 1958 the Fourth Republic had 24 separate ministries. Frustrated by the endless divisiveness, de Gaulle famously asked "How can you govern a country which has 246 varieties of cheese?"[185]
The Fourth Republic was wracked by political instability, failures in Indochina, and inability to resolve the Algerian question.[186][187]
On 13 May 1958, the Pied-Noir settlers seized the government buildings in Algiers, attacking what they saw as French government weakness in the face of demands among the Berber and Arab majority for Algerian independence. A "Committee of Civil and Army Public Security" was created under the presidency of General Jacques Massu, a Gaullist sympathiser. General Raoul Salan, Commander-in-Chief in Algeria, announced on radio that he was assuming provisional power, and appealed for confidence in himself.[188]
At a 19 May press conference, de Gaulle asserted again that he was at the disposal of the country. As a journalist expressed the concerns of some who feared that he would violate civil liberties, de Gaulle retorted vehemently: "Have I ever done that? On the contrary, I have re-established them when they had disappeared. Who honestly believes that, at age 67, I would start a career as a dictator?"[189] A constitutionalist by conviction, he maintained throughout the crisis that he would accept power only from the lawfully constituted authorities. De Gaulle did not wish to repeat the difficulty the Free French movement experienced in establishing legitimacy as the rightful government. He told an aide that the rebel generals "will not find de Gaulle in their baggage".[19]
The crisis deepened as French paratroops from Algeria seized Corsica and a landing near Paris was discussed (Operation Resurrection).[190]
Political leaders on many sides agreed to support the General's return to power, except François Mitterrand, Pierre Mendès France, Alain Savary, the Communist Party, and certain other leftists.
On 29 May the French President, René Coty, told parliament that the nation was on the brink of civil war, so he was "turning towards the most illustrious of Frenchmen, towards the man who, in the darkest years of our history, was our chief for the reconquest of freedom and who refused dictatorship in order to re-establish the Republic. I ask General de Gaulle to confer with the head of state and to examine with him what, in the framework of Republican legality, is necessary for the immediate formation of a government of national safety and what can be done, in a fairly short time, for a deep reform of our institutions."[191] De Gaulle accepted Coty's proposal under the precondition that a new constitution would be introduced creating a powerful presidency in which a sole executive, the first of which was to be himself, ruled for seven-year periods. Another condition was that he be granted extraordinary powers for a period of six months.[192]
De Gaulle remained intent on replacing the weak constitution of the Fourth Republic. He is sometimes described as the author of the new constitution, as he commissioned it and was responsible for its overall framework. The actual drafter of the text was Michel Debré who wrote up de Gaulle's political ideas and guided the text through the enactment process. On 1 June 1958, de Gaulle became Prime Minister and was given emergency powers for six months by the National Assembly,[193] fulfilling his desire for parliamentary legitimacy.[19]
De Gaulle's cabinet received strong support from right parties, split support from left of center parties, and strong opposition from the Communist Party. In the vote on 1 June 1958, 329 votes were cast in favor and 224 against, out of 593 deputies.[194] The composition of the vote was as follows:
Votes in favor: 42 out of 95 Socialists led by Guy Mollet, 24 out of 42 Radicals led by Felix Gaillard, 10 out of 20 UDSR deputies led by Rene Pleven, all 14 RGR deputies led by Edgar Faure, 70 out of 74 MRP deputies led by Georges Bidault, around 22 Social Republicans led by Jacques Chelmas-Delban, around 95 Independent-Peasants led by Antoine Pinay, and around 52 Poujadists.[194]
Votes against: 141 Communists, 49 Socialists led by Gaston Deferre, 18 Radicals led by Pierre Mendes-France, and 4 UDSR deputies led by François Mitterrand.[194]
On 28 September 1958, a referendum took place and 82.6 percent of those who voted supported the new constitution and the creation of the Fifth Republic. The colonies (Algeria was officially a part of France, not a colony) were given the choice between immediate independence and the new constitution. All African colonies voted for the new constitution and the replacement of the French Union by the French Community, except Guinea, which thus became the first French African colony to gain independence and immediately lost all French assistance.[195]
In the November 1958 election, Charles de Gaulle and his supporters (initially organised in the Union pour la Nouvelle République-Union Démocratique du Travail, then the Union des Démocrates pour la Vème République, later still the Union des Démocrates pour la République, UDR) won a comfortable majority. On 21 December, he was elected President of France by the electoral college with 78% of the vote; he was inaugurated in January 1959. As head of state, he also became ex officio the Co-Prince of Andorra.[196]
De Gaulle oversaw tough economic measures to revitalise the country, including the issuing of a new franc (worth 100 old francs).[197] Less than a year after taking office, he was confronted with national tragedy, after the Malpasset Dam in Var collapsed in early December, killing over 400 in floods. Internationally, he rebuffed both the United States and the Soviet Union, pushing for an independent France with its own nuclear weapons and strongly encouraged a "Free Europe", believing that a confederation of all European nations would restore the past glories of the great European empires.[4]: 411, 428 
He set about building Franco-German cooperation as the cornerstone of the European Economic Community (EEC), paying the first state visit to Germany by a French head of state since Napoleon.[198] In January 1963, Germany and France signed a treaty of friendship, the Élysée Treaty.[4]: 422  France also reduced its dollar reserves, trading them for gold from the Federal government of the United States, thereby reducing American economic influence abroad.[4]: 439 
On 23 November 1959, in a speech in Strasbourg, he announced his vision for Europe:
Oui, c'est l'Europe, depuis l'Atlantique jusqu'à l'Oural, c'est toute l'Europe, qui décidera du destin du monde.
("Yes, it is Europe, from the Atlantic to the Urals, it is the whole of Europe, that will decide the destiny of the world.")His expression, "Europe, from the Atlantic to the Urals", has often been cited throughout the history of European integration. It became, for the next ten years, a favourite political rallying cry of de Gaulle's. His vision stood in contrast to the Atlanticism of the United States and Britain, preferring instead a Europe that would act as a third pole between the United States and the Soviet Union. By including in his ideal of Europe all the territory up to the Urals, de Gaulle was implicitly offering détente to the Soviets.
As the last chief of government of the Fourth Republic, de Gaulle made sure that the Treaty of Rome creating the European Economic Community was fully implemented, and that the British project of Free Trade Area was rejected, to the extent that he was sometimes considered as a "Father of Europe".[199]
Upon becoming president, de Gaulle was faced with the urgent task of finding a way to bring to an end the bloody and divisive war in Algeria.[200] His intentions were obscure. He had immediately visited Algeria and declared, Je vous ai compris—'I have understood you', and each competing interest had wished to believe it was them that he had understood. The settlers assumed he supported them, and would be stunned when he did not. In Paris, the left wanted independence for Algeria. Although the military's near-coup had contributed to his return to power, de Gaulle soon ordered all officers to quit the rebellious Committees of Public Safety. Such actions greatly angered the pieds-noirs and their military supporters.[201]
He faced uprisings in Algeria by the pied-noirs and the French armed forces. On assuming the prime minister role in June 1958 he immediately went to Algeria, and neutralised the army there, with its 600,000 soldiers. The Algiers Committee of Public Safety was loud in its demands on behalf of the settlers, but de Gaulle made more visits and sidestepped them. For the long term he devised a plan to modernize Algeria's traditional economy, deescalated the war, and offered Algeria self-determination in 1959. A pied-noir revolt in 1960 failed, and another attempted coup failed in April 1961. French voters approved his course in a 1961 referendum on Algerian self-determination. De Gaulle arranged a cease-fire in Algeria with the March 1962 Evian Accords, legitimated by another referendum a month later. It gave victory to the FLN, which came to power and declared independence. The long crisis was over.[202]
Although the Algerian issue was settled, Prime Minister Michel Debré resigned over the final settlement and was replaced with Georges Pompidou on 14 April 1962. France recognised Algerian independence on 3 July 1962, and a blanket amnesty law was belatedly voted in 1968, covering all crimes committed by the French army during the war. In just a few months in 1962, 900,000 Pied-Noirs left the country. After 5 July, the exodus accelerated in the wake of the French deaths during the Oran massacre of 1962.
De Gaulle was targeted for death by the Organisation armée secrète (OAS), in retaliation for his Algerian initiatives. Several assassination attempts were made on him; the most famous occurred on 22 August 1962, when he and his wife narrowly escaped from an organized machine gun ambush on their Citroën DS limousine. De Gaulle commented "Ils tirent comme des cochons" ("They shoot like pigs").[203] The attack was arranged by Colonel Jean-Marie Bastien-Thiry at Petit-Clamart.[4]: 381 Bastien-Thiry was later executed by firing squad on March 11 1963, the last execution done by this method in France.[204] The other perpetrators had their sentence commuted from death to life imprisonment. Frederick Forsyth used this incident as a basis for his novel The Day of the Jackal.
It is claimed that there were at least 30 assassination attempts against de Gaulle throughout his lifetime.[205][206][207]
In September 1962, de Gaulle sought a constitutional amendment to allow the president to be directly elected by the people and issued another referendum to this end. After a motion of censure voted by the parliament on 4 October 1962, de Gaulle dissolved the National Assembly and held new elections. Although the left progressed, the Gaullists won an increased majority—this despite opposition from the Christian democratic Popular Republican Movement (MRP) and the National Centre of Independents and Peasants (CNIP) who criticised de Gaulle's euroscepticism and presidentialism.[208][209]
De Gaulle's proposal to change the election procedure for the French presidency was approved at the referendum on 28 October 1962 by more than three-fifths of voters despite a broad "coalition of no" formed by most of the parties, opposed to a presidential regime. Thereafter the president was to be elected by direct universal suffrage for the first time since Louis Napoleon in 1848.[210]
With the Algerian conflict behind him, de Gaulle was able to achieve his two main objectives, the reform and development of the French economy, and the promotion of an independent foreign policy and a strong presence on the international stage. This was named by foreign observers the "politics of grandeur" (politique de grandeur).[211] See Gaullism.
In the immediate post-war years France was in poor shape;[155] wages remained at around half prewar levels, the winter of 1946–1947 did extensive damage to crops, leading to a reduction in the bread ration, hunger and disease remained rife and the black market continued to flourish. Germany was in an even worse position, but after 1948 things began to improve dramatically with the introduction of Marshall Aid—large scale American financial assistance given to help rebuild European economies and infrastructure. This laid the foundations of a meticulously planned program of investments in energy, transport and heavy industry, overseen by the government of Prime Minister Georges Pompidou.
In the context of a population boom unseen in France since the 18th century, the government intervened heavily in the economy, using dirigisme—a unique combination of free-market and state-directed economy—with indicative five-year plans as its main tool. This was followed by a rapid transformation and expansion of the French economy.
High-profile projects, mostly but not always financially successful, were launched: the extension of Marseille's harbour (soon ranking third in Europe and first in the Mediterranean); the promotion of the Caravelle passenger jetliner (a predecessor of Airbus); the decision to start building the supersonic Franco-British Concorde airliner in Toulouse; the expansion of the French auto industry with state-owned Renault at its centre; and the building of the first motorways between Paris and the provinces.
Aided by these projects, the French economy recorded growth rates unrivalled since the 19th century. In 1964, for the first time in nearly 100 years[212] France's GDP overtook that of the United Kingdom for a time. This period is still remembered in France with some nostalgia as the peak of the Trente Glorieuses ("Thirty Glorious Years" of economic growth between 1945 and 1974).[213]
In 1967, de Gaulle decreed a law that obliged all firms over certain sizes to distribute a small portion of their profits to their employees. By 1974, as a result of this measure, French employees received an average of 700 francs per head, equivalent to 3.2% of their salary.[214]
During his first tenure as president, de Gaulle became enthusiastic about the possibilities of nuclear power. France had carried out important work in the early development of atomic energy and in October 1945, he established the French Atomic Energy Commission Commissariat à l'énergie atomique, (CEA) responsible for all scientific, commercial, and military uses of nuclear energy. However, partly due to communist influences in government opposed to proliferation, research stalled and France was excluded from American, British and Canadian nuclear efforts.[citation needed]
By October 1952, the United Kingdom had become the third country—after the United States and the Soviet Union—to independently test and develop nuclear weapons. This gave Britain the capability to launch a nuclear strike via its Vulcan bomber force and they began developing a ballistic missile program known as Blue Streak.[citation needed]
As early as April 1954 while out of power, de Gaulle argued that France must have its own nuclear arsenal as nuclear weapons were seen as a national status symbol and a way of maintaining international prestige with a place at the 'top table' of the United Nations. Full-scale research began again in late 1954 when Prime Minister Pierre Mendès France authorized a plan to develop the atomic bomb; large deposits of uranium had been discovered near Limoges in central France, providing the researchers with an unrestricted supply of nuclear fuel. France's independent Force de Frappe (strike force) came into being soon after de Gaulle's election with his authorization for the first nuclear test.
With the cancellation of Blue Streak, the US agreed to supply Britain with its Skybolt and later Polaris weapons systems, and in 1958, the two nations signed the Mutual Defence Agreement forging close links which have seen the US and UK cooperate on nuclear security matters ever since. Although at the time it was still a full member of NATO, France proceeded to develop its own independent nuclear technologies—this would enable it to become a partner in any reprisals and would give it a voice in matters of atomic control.[215]
After six years of effort, on 13 February 1960, France became the world's fourth nuclear power when a high-powered nuclear device was exploded in the Sahara some 700 miles south-south-west of Algiers.[216] In August 1963, France decided against signing the Partial Test Ban Treaty designed to slow the arms race because it would have prohibited it from testing nuclear weapons above ground. France continued to carry out tests at the Algerian site until 1966, under an agreement with the newly independent Algeria. France's testing program then moved to the Mururoa and Fangataufa Atolls in the South Pacific.
In November 1967, an article by the French Chief of the General Staff (but inspired by de Gaulle) in the Revue de la Défense Nationale caused international consternation. It was stated that the French nuclear force should be capable of firing "in all directions"—thus including even America as a potential target. This surprising statement was intended as a declaration of French national independence, and was in retaliation to a warning issued long ago by Dean Rusk that US missiles would be aimed at France if it attempted to employ atomic weapons outside an agreed plan. However, criticism of de Gaulle was growing over his tendency to act alone with little regard for the views of others.[217] In August, concern over de Gaulle's policies had been voiced by Valéry Giscard d'Estaing when he queried 'the solitary exercise of power'.[218]
With the onset of the Cold War and the perceived threat of invasion from the Soviet Union and the countries of the eastern bloc, the United States, Canada and a number of western European countries set up the North Atlantic Treaty Organization (NATO) to co-ordinate a military response to any possible attack. France played a key role during the early days of the organisation, providing a large military contingent and agreeing—after much soul-searching—to the participation of West German forces. But after his election in 1958 Charles de Gaulle took the view that the organisation was too dominated by the US and UK, and that America would not fulfill its promise to defend Europe in the event of a Soviet invasion.
De Gaulle demanded political parity with Britain and America in NATO, and for its geographic coverage to be extended to include French territories abroad, including Algeria[citation needed], then experiencing civil war. This was not forthcoming, and so in March 1959 France, citing the need for it to maintain its own independent military strategy, withdrew its Mediterranean Fleet (ALESCMED) from NATO, and a few months later de Gaulle demanded the removal of all US nuclear weapons from French territory.
De Gaulle hosted a superpower summit on 17 May 1960 for arms limitation talks and détente efforts in the wake of the 1960 U-2 incident between United States President Dwight Eisenhower, Soviet Premier Nikita Khrushchev, and United Kingdom Prime Minister Harold Macmillan.[219] De Gaulle's warm relations with Eisenhower were noticed by United States military observers at that time. De Gaulle told Eisenhower: "Obviously you cannot apologize but you must decide how you wish to handle this. I will do everything I can to be helpful without being openly partisan." When Khrushchev condemned the United States U-2 flights, de Gaulle expressed to Khrushchev his disapproval of 18 near-simultaneous secret Soviet satellite overflights of French territory; Khrushchev denied knowledge of the satellite overflights. Lieutenant General Vernon A. Walters wrote that after Khrushchev left, "De Gaulle came over to Eisenhower and took him by the arm. He took me also by the elbow and, taking us a little apart, he said to Eisenhower, 'I do not know what Khrushchev is going to do, nor what is going to happen, but whatever he does, I want you to know that I am with you to the end.' I was astounded at this statement, and Eisenhower was clearly moved by his unexpected expression of unconditional support". General Walters was struck by de Gaulle's "unconditional support" of the United States during that "crucial time".[220] De Gaulle then tried to revive the talks by inviting all the delegates to another conference at the Élysée Palace to discuss the situation, but the summit ultimately dissolved in the wake of the U-2 incident.[219]
In 1964, de Gaulle visited the Soviet Union, where he hoped to establish France as an alternative influence in the Cold War. De Gaulle always viewed Communism as a passing phenomenon, and never used the term 'Soviet Union', always calling it Russia. In his view, Russian national interests rather than Communist ideology determined the decision-making in the Kremlin. Later, he proclaimed a new alliance between the nations, but although Soviet premier Alexei Kosygin later visited Paris, the Soviets clearly did not consider France a superpower and knew that they would remain dependent on the NATO alliance in the event of a war. In 1965, de Gaulle pulled France out of SEATO, the southeast Asian equivalent of NATO, and refused to participate in any future NATO maneuvers.
In February 1966, France withdrew from the NATO Military Command Structure, but remained within the organisation. De Gaulle, haunted by the memories of 1940, wanted France to remain the master of the decisions affecting it, unlike in the 1930s when it had to follow in step with its British ally. He also ordered all foreign military personnel to leave France within a year.[4]: 431  This latter action was particularly badly received in the US, prompting Dean Rusk, the US Secretary of State, to ask de Gaulle whether the removal of American military personnel was to include exhumation of the 50,000 American war dead buried in French cemeteries.[221]
France, experiencing the disintegration of its colonial empire and severe problems in Algeria, turned towards Europe after the Suez Crisis, and to West Germany in particular.[221] In the years after, the economies of both nations integrated and they led the drive towards European unity.[citation needed]
One of the conditions of Marshall Aid was that the nations' leaders must co-ordinate economic efforts and pool the supply of raw materials. By far the most critical commodities in driving growth were coal and steel. France assumed it would receive large amounts of high-quality German coal from the Ruhr as reparations for the war, but the US refused to allow this, fearing a repetition of the bitterness after the Treaty of Versailles which partly caused World War II.[155]
Under the inspiration of the French statesmen Jean Monnet and Robert Schuman, together with the German leader Konrad Adenauer, the rift between the two nations had begun to heal and in 1951, along with Italy and the Benelux countries, they formed the European Coal and Steel Community. Following the Treaty of Rome of 1957, this became the European Economic Community.
De Gaulle had not been instrumental in setting up the new organization and, from the start, he opposed efforts by fellow EEC member countries to move toward some form of political integration that, in de Gaulle's thinking, would impinge on the sovereignty of France, both internally and externally. To counter those supranational tendencies that he disparaged,[222] he put forward in 1961 the so-called Fouchet Plan that maintained all decision-making powers in the hands of governments, reducing the projected European parliamentary assembly to a mere consultative assembly. As expected, the plan was rejected by France's partners. In July 1965, de Gaulle provoked a major six-month crisis when he ordered the boycott of EEC institutions (see Empty chair crisis below) until his demands – the withdrawal of a European Commission proposal to reinforce the community institutions to the detriment of national sovereignty, and the acceptance of France's proposal regarding the financing of the newly established Common Agricultural Policy (CAP) – were met with the Luxembourg compromise.[citation needed]
De Gaulle, who in spite of recent history admired Germany and spoke excellent German,[223] as well as English,[224] established a good relationship with the aging West German Chancellor Konrad Adenauer—culminating in the Elysee Treaty in 1963—and in the first few years of the Common Market, France's industrial exports to the other five members tripled and its farm export almost quadrupled. The franc became a solid, stable currency for the first time in half a century, and the economy mostly boomed. Adenauer however, all too aware of the importance of American support in Europe, gently distanced himself from the general's more extreme ideas, wanting no suggestion that any new European community would in any sense challenge or set itself at odds with the US. In Adenauer's eyes, the support of the US was more important than any question of European prestige.[225] Adenauer was also anxious to reassure Britain that nothing was being done behind its back and was quick to inform British Prime Minister Harold Macmillan of any new developments.
Great Britain initially declined to join the EEC, preferring to remain with another organisation known as the European Free Trade Area, mostly consisting of the northern European countries and Portugal. By the late 1950s, German and French living standards began to exceed those in Britain, and the government of Harold Macmillan, realising that the EEC was a stronger trade bloc than EFTA, began negotiations to join.
De Gaulle vetoed the British application to join the European Economic Community (EEC) in 1963, famously uttering the single word 'non' into the television cameras at the critical moment, a statement used to sum up French opposition towards Britain for many years afterwards.[226] Macmillan said afterwards that he always believed that de Gaulle would prevent Britain joining, but thought he would do it quietly, behind the scenes. He later complained privately that "all our plans are in tatters".[221]
American President John F. Kennedy urged de Gaulle to accept the United Kingdom in the EEC, stating that a Europe without Great Britain would create a situation in which the United States were bearing the enormous costs of Europe's protection without any voice. Kennedy applied pressure to de Gaulle by threatening to withdraw American troops from European soil, but de Gaulle believed that the United States would lose the Cold War if they were to leave Europe.[227] It encouraged de Gaulle to see Great Britain as America's "Trojan Horse".[228]
British Prime Minister Churchill once said to him that if he had the choice between France and the United States, he would always choose the United States. Churchill's successor, Macmillan, prioritised the rebuilding of the Anglo-American "Special Relationship". With the American agreement to supply Britain with the Skybolt nuclear missile, de Gaulle thought that the United Kingdom would not go along with his vision for a West European strategically independent from the United States.[229][230] He maintained there were incompatibilities between continental European and British economic interests. In addition, he demanded that the United Kingdom accept all the conditions laid down by the six existing members of the EEC (Belgium, France, West Germany, Italy, Luxembourg, Netherlands) and revoke its commitments to countries within its own free trade area (which France had not done with its own). He supported a deepening and an acceleration of Common Market integration rather than an expansion.[231]
However, in this latter respect, a detailed study of the formative years of the EEC argues that the defence of French economic interests, especially in agriculture, in fact played a more dominant role in determining de Gaulle's stance towards British entry than the various political and foreign policy considerations that have often been cited.[232]
Dean Acheson believed that Britain made a grave error in not signing up to European integration right from the start, and that they continued to suffer the political consequences for at least two decades afterwards. However he also stated his belief that de Gaulle used the 'Common Market' (as it was then termed) as an "exclusionary device to direct European trade towards the interest of France and against that of the United States, Britain and other countries."[233]
Claiming continental European solidarity, de Gaulle again rejected British entry when they next applied to join the community in December 1967 under the Labour leadership of Harold Wilson. During negotiations, de Gaulle chided Britain for relying too much on the Americans, saying that sooner or later they would always do what was in their best interests. Wilson said he then gently raised the spectre of the threat of a newly powerful Germany as a result of the EEC, which de Gaulle agreed was a risk.[234] After de Gaulle left office the United Kingdom applied again and finally became a member of the EEC in January 1973.[235]
In January 1964, France was, after the UK, among the first of the major Western powers to open diplomatic relations with the People's Republic of China (PRC), which was established in 1949 and which was isolated on the international scene.[236] By recognizing Mao Zedong's government, de Gaulle signaled to both Washington and Moscow that France intended to deploy an independent foreign policy.[236] The move was criticized in the United States as it seemed to seriously damage US policy of containment in Asia.[236] De Gaulle justified this action by "the weight of evidence and reason", considering that China's demographic weight and geographic extent put it in a position to have a global leading role.[236] De Gaulle also used this opportunity to arouse rivalry between the USSR and China, a policy that was followed several years later by Henry Kissinger's "triangular diplomacy" which also aimed to create a Sino-Soviet split.[236]
France established diplomatic relations with the People's Republic of China – the first step towards formal recognition without first severing links with the Republic of China (Taiwan), led by Chiang Kai-shek. Hitherto the PRC had insisted that all nations abide by a "one China" condition, and at first it was unclear how the matter would be settled.[237] However, the agreement to exchange ambassadors was subject to a delay of three months, and in February, Chiang Kai-shek resolved the problem by cutting off diplomatic relations with France.[238] Eight years later, US President Richard Nixon visited the PRC and began normalising relations—a policy which was confirmed in the Shanghai Communiqué of 28 February 1972.[239]
As part of a European tour, Nixon visited France in 1969.[240] He and de Gaulle both shared the same non-Wilsonian approach to world affairs, believing in nations and their relative strengths, rather than in ideologies, international organisations, or multilateral agreements. De Gaulle is famously known for calling the UN the pejorative "le Machin [fr]"[241] ("the thingamajig").
During the autumn of 1964, de Gaulle embarked on a grueling 20,000-mile trek across Latin America despite being a month away from his 75th birthday, a recent operation for prostate cancer, and concerns over security. He had visited Mexico the previous year and spoke, in Spanish, to the Mexican people on the eve of their celebrations of their independence at the Palacio Nacional in Mexico City. During his new 26-day visit, he was again keen to gain both cultural and economic influence.[242] He spoke constantly of his resentment of US influence in Latin America—"that some states should establish a power of political or economic direction outside their own borders". Yet France could provide no investment or aid to match that from Washington.[4]: 427 
In the Bretton Woods system put in place in 1944, US dollars were convertible to gold. In France, it was called "America's exorbitant privilege"[243] as it resulted in an "asymmetric financial system" where foreigners "see themselves supporting American living standards and subsidizing American multinationals". As American economist Barry Eichengreen summarized: "It costs only a few cents for the Bureau of Engraving and Printing to produce a $100 bill, but other countries had to pony up $100 of actual goods in order to obtain one".[243] In February 1965, President Charles de Gaulle announced his intention to exchange its US dollar reserves for gold at the official exchange rate. He sent the French Navy across the Atlantic to pick up the French reserve of gold, which had been moved there during World War II, and was followed by several countries. As it resulted in considerably reducing US gold stock and US economic influence, it led US President Richard Nixon to unilaterally end the convertibility of the dollar to gold on 15 August 1971 (the "Nixon Shock"). This was meant to be a temporary measure but the dollar became permanently a floating fiat money and in October 1976, the US government officially changed the definition of the dollar as references to gold were removed from statutes.[244][245]
In December 1965, de Gaulle returned as president for a second seven-year term. In the first round he did not win the expected majority, receiving 45% of the vote. Both of his main rivals did better than expected; the leftist François Mitterrand received 32% and Jean Lecanuet, who advocated for what Life described as "Gaullism without de Gaulle", received 16%.[246] De Gaulle won a majority in the second round, with Mitterrand receiving 44.8%.[247]
In September 1966, in a famous speech in Phnom Penh in Cambodia, he expressed France's disapproval of the US involvement in the Vietnam War, calling for a US withdrawal from Vietnam as the only way to ensure peace.[248] De Gaulle considered the war to be the "greatest absurdity of the twentieth century".[249] De Gaulle conversed frequently with George Ball, United States President Lyndon Johnson's Under Secretary of State, and told Ball that he feared that the United States risked repeating France's tragic experience in Vietnam, which de Gaulle called "ce pays pourri" ("the rotten country"). Ball later sent a 76-page memorandum to Johnson critiquing Johnson's current Vietnam policy in October 1964.[250]
De Gaulle later visited Guadeloupe for two days, in the aftermath of Hurricane Inez, bringing aid which totaled billions of francs.[251]
During the establishment of the European Community, de Gaulle helped precipitate the Empty Chair Crisis, one of the greatest crises in the history of the EEC. It involved the financing of the Common Agricultural Policy, but almost more importantly the use of qualified majority voting in the EC (as opposed to unanimity). In June 1965, after France and the other five members could not agree, de Gaulle withdrew France's representatives from the EC. Their absence left the organisation essentially unable to run its affairs until the Luxembourg compromise was reached in January 1966.[252] De Gaulle succeeded in influencing the decision-making mechanism written into the Treaty of Rome by insisting on solidarity founded on mutual understanding.[253] He vetoed Britain's entry into the EEC a second time, in June 1967.[254]
With tension rising in the Middle East in 1967, de Gaulle on 2 June declared an arms embargo against Israel, just three days before the outbreak of the Six-Day War. This, however, did not affect spare parts for the French military hardware with which the Israeli armed forces were equipped.[255]
This was an abrupt change in French policy. In 1956, France, Britain and Israel had cooperated in an elaborate effort to retake the Suez Canal from Egypt. Israel's air force operated French Mirage and Mystère jets in the Six-Day War, and its navy was building its new missile boats in Cherbourg. Though paid for, their transfer to Israel was now blocked by de Gaulle's government. But they were smuggled out in an operation that drew further denunciations from the French government. The last boats took to the sea in December 1969, directly after a major deal between France and now-independent Algeria exchanging French armaments for Algerian oil.[256]
Under de Gaulle, following the independence of Algeria, France embarked on foreign policy more favorable to the Arab side. President de Gaulle's position in 1967 at the time of the Six-Day War played a part in France's new-found popularity in the Arab world.[257] Israel turned towards the United States for arms, and toward its own industry. In a televised news conference on 27 November 1967, de Gaulle described the Jewish people as "this elite people, sure of themselves and domineering".[258]
In his letter to David Ben-Gurion dated 9 January 1968, he explained that he was convinced that Israel had ignored his warnings and overstepped the bounds of moderation by taking possession of Jerusalem, and Jordanian, Egyptian, and Syrian territory by force of arms. He felt Israel had exercised repression and expulsions during the occupation and that it amounted to annexation. He said that provided Israel withdrew its forces, it appeared that it might be possible to reach a solution through the UN framework which could include assurances of a dignified and fair future for refugees and minorities in the Middle East, recognition from Israel's neighbours, and freedom of navigation through the Gulf of Aqaba and the Suez Canal.[259]
The Eastern Region of Nigeria declared itself independent under the name of the Independent Republic of Biafra on 30 May 1967. On 6 July, the first shots in the Nigerian Civil War were fired, marking the start of a conflict that lasted until January 1970.[260] Britain provided military aid to the Federal Republic of Nigeria—yet more was made available by the Soviet Union. Under de Gaulle's leadership, France embarked on a period of interference outside the traditional French zone of influence. A policy geared toward the break-up of Nigeria put Britain and France into opposing camps. Relations between France and Nigeria had been under strain since the third French nuclear explosion in the Sahara in December 1960. From August 1968, when its embargo was lifted, France provided limited and covert support to the Biafra rebels. Although French arms helped to keep Biafra in action for the final 15 months of the civil war, its involvement was seen as insufficient and counterproductive. The Biafran chief of staff stated that the French "did more harm than good by raising false hopes and by providing the British with an excuse to reinforce Nigeria."[261]
In July 1967, de Gaulle visited Canada, which was celebrating its centenary with a world fair in Montreal, Expo 67. On 24 July, speaking to a large crowd from a balcony at Montreal's city hall, de Gaulle shouted "Vive le Québec libre! Vive le Canada français! Et vive la France!" (Long live free Quebec! Long live French Canada, and long live France!).[262] The Canadian media harshly criticized the statement, and the Prime Minister of Canada, Lester B. Pearson, stated that "Canadians do not need to be liberated".[263] De Gaulle left Canada abruptly two days later, without proceeding to Ottawa as scheduled.[264] He never returned to Canada. The speech offended many English-speaking Canadians and was heavily criticized in France as well,[265] and led to a significant diplomatic rift between the two countries.[266]
The event however was seen as a watershed moment by the Quebec sovereignty movement,[267] and is still a significant milestone of Quebec's history to the eyes of most Quebecers.[268]
In the following year, de Gaulle visited Brittany, where he declaimed a poem written by his uncle (also called Charles de Gaulle) in the Breton language. The speech followed a series of crackdowns on Breton nationalism. De Gaulle was accused of hypocrisy, on the one hand supporting a "free" Quebec because of linguistic and ethnic differences from other Canadians, while on the other hand suppressing a regional and ethnic nationalist movement in Brittany.[269]
General de Gaulle paid an official visit to Poland on 6 September 1967 and spent an entire week there.[270] De Gaulle described it as his "pilgrimage to Poland" and visited Warsaw, Gdańsk, Kraków and Nazi death camp Auschwitz-Birkenau. He met with crowds of people on the streets and shouted (in Polish) "Long live Poland! Our dear, noble and brave Poland!". Without discussion, de Gaulle announced that France officially recognized the new Polish western border that was established in 1945.[citation needed]
De Gaulle's government was criticized within France, particularly for its heavy-handed style. While the written press and elections were free, and private stations such as Europe 1 were able to broadcast in French from abroad, the state's ORTF had a monopoly on television and radio. This monopoly meant that the government was in a position to directly influence broadcast news. In many respects, Gaullist France was conservative, Catholic, and there were few women in high-level political posts (in May 1968, the government's ministers were 100% male).[271] Many factors contributed to a general weariness of sections of the public, particularly the student youth, which led to the events of May 1968.
The mass demonstrations and strikes in France in May 1968 severely challenged De Gaulle's legitimacy. He and other government leaders feared that the country was on the brink of revolution or civil war. On 29 May, De Gaulle disappeared without notifying Prime Minister Pompidou or anyone else in the government, stunning the country. He fled to Baden-Baden in Germany to meet with General Massu, head of the French military there, to discuss possible army intervention against the protesters. De Gaulle returned to France after being assured of the military's support, in return for which De Gaulle agreed to amnesty for the 1961 coup plotters and OAS members.[272][273]
In a private meeting discussing the students' and workers' demands for direct participation in business and government he coined the phrase "La réforme oui, la chienlit non", which can be politely translated as 'reform yes, masquerade/chaos no.' It was a vernacular scatological pun meaning 'chie-en-lit, no' (shit-in-bed, no). The term is now common parlance in French political commentary, used both critically and ironically referring back to de Gaulle.[274]
But de Gaulle offered to accept some of the reforms the demonstrators sought. He again considered a referendum to support his moves, but on 30 May, Pompidou persuaded him to dissolve parliament (in which the government had all but lost its majority in the March 1967 elections) and hold new elections instead. The June 1968 elections were a major success for the Gaullists and their allies; when shown the spectre of revolution or civil war, the majority of the country rallied to him. His party won 352 of 487 seats,[275] but de Gaulle remained personally unpopular; a survey conducted immediately after the crisis showed that a majority of the country saw him as too old, too self-centered, too authoritarian, too conservative, and too anti-American.[272]
De Gaulle resigned the presidency at noon, 28 April 1969,[276] following the rejection of his proposed reform of the Senate and local governments in a nationwide referendum. In an eight-minute televised speech two days before the referendum, De Gaulle warned that if he was "disavowed" by a majority of the voters, he would resign his office immediately. This ultimatum, coupled with increased De Gaulle fatigue among the French, convinced many that this was an opportunity to be rid of the 78-year-old general and the reform package was rejected. Two months later Georges Pompidou was elected as his successor.[277]
De Gaulle retired once again to his beloved nine-acre country estate, La Boisserie (the woodland glade), in Colombey-les-Deux-Églises, 120 miles southeast of Paris. There the General, who often described old age as a "shipwreck",[278] continued his memoirs, dictated to his secretary from notes. To visitors, de Gaulle said, "I will finish three books, if God grants me life." The Renewal, the first of three planned volumes to be called Memoirs of Hope, was quickly finished and immediately became the fastest seller in French publishing history.
On 9 November 1970, less than two weeks before his 80th birthday, Charles de Gaulle died suddenly, despite enjoying very robust health his entire life (except for a prostate operation a few years earlier). He had been watching the evening news on television and playing Solitaire around 7:40 p.m. when he suddenly pointed to his head and said, "I feel a pain right here", and then collapsed. His wife called the doctor and the local priest, but by the time they arrived he had died from an aneurysm.[citation needed] His wife asked that she be allowed to inform her family before the news was released. She was able to contact her daughter in Paris quickly, but their son, who was in the navy, was difficult to track down. President Georges Pompidou was not informed until 4 AM the next day, and announced the general's death on television some 18 hours after the event. He simply said, "Le général de Gaulle est mort; la France est veuve." ("General de Gaulle is dead. France is a widow.")
De Gaulle had made arrangements that insisted his funeral be held at Colombey, and that no presidents or ministers attend his funeral—only his Compagnons de la Libération.[279] Despite his wishes, such were the number of foreign dignitaries who wanted to honor de Gaulle that Pompidou was forced to arrange a separate memorial service at the Notre-Dame Cathedral, to be held at the same time as his actual funeral.
The funeral on 12 November 1970 was the biggest such event in French history, with hundreds of thousands of French people — many carrying blankets and picnic baskets — and thousands of cars parked in the roads and fields along the routes to the two venues. On the day of the funeral, there was national mourning, many entertainment and cultural events were canceled, and schools and offices were closed.[280][281] Thousands of guests attended the event, included De Gaulle's successor Georges Pompidou, U.S. president Richard Nixon, British prime minister Edward Heath, UN secretary-general U Thant, Soviet statesman Nikolai Podgorny, Italian president Giuseppe Saragat, West German chancellor Willy Brandt and Queen Juliana of the Netherlands. Special trains were laid on to bring extra mourners to the region and the crowd was packed so tightly that those who fainted had to be passed overhead toward first-aid stations at the rear.[278] The General was conveyed to the church on an armoured reconnaissance vehicle and carried to his grave, next to his daughter Anne, by eight young men of Colombey. As he was lowered into the ground, the bells of all the churches in France tolled, starting from Notre Dame and spreading out from there.[282]
De Gaulle specified that his tombstone bear the simple inscription of his name and his years of birth and death. Therefore, it simply states, "Charles de Gaulle, 1890–1970".[283] At the service, President Pompidou said, "de Gaulle gave France her governing institutions, her independence and her place in the world."[citation needed] André Malraux, the writer and intellectual who served as his Minister of Culture, called him "a man of the day before yesterday and the day after tomorrow."[citation needed] De Gaulle's family turned the La Boisserie residence into a foundation. It currently houses the Charles de Gaulle Museum.[citation needed]
De Gaulle married Yvonne Vendroux on 7 April 1921 in Église Notre-Dame de Calais. They had three children: Philippe (born 1921), Élisabeth (1924–2013), who married General Alain de Boissieu, and Anne (1928–1948). Anne had Down syndrome and died of pneumonia at the age of 20. He always had a particular love for Anne; one Colombey resident recalled how he used to walk with her hand-in-hand around the property, caressing her and talking quietly about the things she understood.[278]
De Gaulle had an older brother Xavier (1887–1955) and sister Marie-Agnes (1889–1983), and two younger brothers, Jacques (1893–1946) and Pierre (1897–1959). He was particularly close to the youngest, Pierre, who so resembled him that presidential bodyguards often saluted him by mistake when he visited his famous brother or accompanied him on official visits.[citation needed]
One of De Gaulle's grandsons, also named Charles de Gaulle, was a member of the European Parliament from 1994 to 2004, his last tenure being for the National Front.[284] The younger Charles de Gaulle's move to the anti-Gaullist Front National was widely condemned by other family members, in open letters and newspaper interviews. "It was like hearing the pope had converted to Islam", one said.[285] Another grandson, Jean de Gaulle, was a member of the French parliament until his retirement in 2007.[286]
De Gaulle made 31 regional tours during his presidency, visiting every French department; for many small towns, the visit was an important moment in history. He enjoyed entering the welcoming crowds; an aide noted how often people said "he saw me" or "he touched me", and another recalled how a mother begged de Gaulle for the king's touch on her baby. They, supporters, and opponents surmised that de Gaulle was a monarch-like figure for the French.[27]: 616–618 
Historians have accorded Napoleon and de Gaulle the top-ranking status of French leaders in the 19th and 20th centuries.[287] According to a 2005 survey, carried out in the context of the tenth anniversary of the death of Socialist President François Mitterrand, 35 percent of respondents said Mitterrand was the best French president ever, followed by Charles de Gaulle (30 percent) and then Jacques Chirac (12 percent).[288] Another poll by BVA four years later showed that 87% of French people regarded his presidency positively.[289]
Statues honouring de Gaulle have been erected in London, Warsaw, in Moscow, Bucharest and Quebec. The first Algerian president, Ahmed Ben Bella, said that de Gaulle was the "military leader who brought us the hardest blows" prior to Algerian independence, but "saw further" than other politicians, and had a "universal dimension that is too often lacking in current leaders."[290] Likewise, Léopold Sédar Senghor, the first president of Senegal, said that few Western leaders could boast of having risked their lives to grant a colony independence.
In 1990, President Mitterrand, de Gaulle's old political rival, presided over the celebrations to mark the 100th anniversary of his birth. Mitterrand, who once wrote a vitriolic critique of him called the "Permanent Coup d'État", quoted a recent opinion poll, saying, "As General de Gaulle, he has entered the pantheon of great national heroes, where he ranks ahead of Napoleon and behind only Charlemagne."[291] Under the influence of Jean-Pierre Chevènement, the leader of CERES, the left-wing and souverainist faction of the Socialist Party, Mitterrand had, except on certain economic and social policies, rallied to much of Gaullism. Between the mid-1970s and mid-1990s there developed a left-right consensus, dubbed "Gaullo-Mitterrandism", behind the "French status" in NATO: i.e. outside the integrated military command.
Although he initially enjoyed good relations with US President John F. Kennedy, who admired his stance against the Soviet Union—particularly when the Berlin Wall was being built—and who called him "a great captain of the Western world", their relationship later cooled.[3] He was Kennedy's most loyal ally during the Cuban Missile Crisis and supported the right that the US claimed to defend its interests in the western hemisphere, in contrast to German Chancellor Konrad Adenauer who doubted Kennedy's commitment to Europe and thought the crisis could have been avoided.[292] De Gaulle accepted that it might be necessary for the United States to take preemptive military action against Cuba, unlike many other European leaders of his time.[220] De Gaulle was a prominent figure at the state funerals of two American presidents: Kennedy and Dwight Eisenhower (Eisenhower's funeral was his only visit to the U.S. since the funeral of JFK).[293][294]
De Gaulle was admired by the later President Nixon. After a meeting at the Palace of Versailles just before the general left office, Nixon declared that "He did not try to put on airs but an aura of majesty seemed to envelop him ... his performance—and I do not use that word disparagingly—was breathtaking."[3] On arriving for his funeral several months later, Nixon said of him, "greatness knows no national boundaries".[278]
Lt. General Vernon A. Walters, a military attaché of Dwight Eisenhower and later military attaché in France from 1967 to 1973, noted the strong relationship between de Gaulle and Eisenhower, de Gaulle's unconditional support of Eisenhower during the U-2 incident, and de Gaulle's strong support of John F. Kennedy during the Cuban Missile Crisis. Thus Walters was intensely curious as to the great contrast between de Gaulle's close relations with two United States presidents during notable Cold War crises and de Gaulle's later decision to withdraw France from NATO's military command, and Walters spoke with many close military and political aides of de Gaulle.[220]
Walters' conclusion, based upon de Gaulle's comments to many of his aides (and to Eisenhower during a meeting at Ramboullet Castle in 1959), is that de Gaulle feared that later United States presidents after Eisenhower would not have Eisenhower's special ties to Europe and would not risk nuclear war over Europe.[220] Also, de Gaulle interpreted the peaceful resolution of the Cuban Missile Crisis without fighting to take back Cuba from communism a mere 90 miles from the United States as an indication that the United States might not fight for Europe's defense 3,500 miles away following Soviet aggression in Europe, but would only go to war following a nuclear strike against the United States itself.[220] De Gaulle told Eisenhower that France did not seek to compete with the Strategic Air Command or army of the United States, but believed that France needed a way to strike the Soviet Union.[220]
A number of commentators have been critical of de Gaulle for his failure to prevent the massacres after Algerian independence[167] while others take the view that the struggle had been so long and savage that it was perhaps inevitable.[3] The Australian historian Brian Crozier wrote, "that he was able to part with Algeria without civil war was a great though negative achievement which in all probability would have been beyond the capacity of any other leader France possessed."[295] In April 1961, when four rebel generals seized power in Algeria, he "did not flinch in the face of this daunting challenge", but appeared on television in his general's uniform to forbid Frenchmen to obey the rebels' orders in an "inflexible display of personal authority".[citation needed]
De Gaulle was an excellent manipulator of the media, as seen in his shrewd use of television to persuade around 80% of Metropolitan France to approve the new constitution for the Fifth Republic. In so doing, he refused to yield to the reasoning of his opponents who said that, if he succeeded in Algeria, he would no longer be necessary. He afterwards enjoyed massive approval ratings, and once said that "every Frenchman is, has been or will be Gaullist".[223]
That de Gaulle did not necessarily reflect mainstream French public opinion with his veto was suggested by the decisive majority of French people who voted in favour of British membership when the much more conciliatory Pompidou called a referendum on the matter in 1972. His early influence in setting the parameters of the EEC can still be seen today, most notably with the controversial Common Agricultural Policy.
Some writers take the view that Pompidou was a more progressive and influential leader than de Gaulle because, though also a Gaullist, he was less autocratic and more interested in social reforms.[167][296] Although he followed the main tenets of de Gaulle's foreign policy, he was keen to work towards warmer relations with the United States. A banker by profession, Pompidou is also widely credited, as de Gaulle's prime minister from 1962 to 1968, with putting in place the reforms which provided the impetus for the economic growth which followed.[citation needed]
In 1968, shortly before leaving office, de Gaulle refused to devalue the Franc on grounds of national prestige, but upon taking over Pompidou reversed the decision almost straight away. It was ironic, that during the financial crisis of 1968, France had to rely on American (and West German) financial aid to help shore up the economy.[167]
Perry has written that the "events of 1968 illustrated the brittleness of de Gaulle's rule. That he was taken by surprise is an indictment of his rule; he was too remote from real life and had no interest in the conditions under which ordinary French people lived. Problems like inadequate housing and social services had been ignored. The French greeted the news of his departure with some relief as the feeling had grown that he had outlived his usefulness. Perhaps he clung onto power too long, perhaps he should have retired in 1965 when he was still popular."[167]
Brian Crozier said "the fame of de Gaulle outstrips his achievements, he chose to make repeated gestures of petulance and defiance that weakened the west without compensating advantages to France"[295]
Régis Debray called de Gaulle "super-lucide"[223] and pointed out that virtually all of his predictions, such as the fall of communism, the reunification of Germany and the resurrection of 'old' Russia, came true after his death.[297] Debray compared him with Napoleon ('the great political myth of the 19th century'), calling de Gaulle his 20th century equivalent. "The sublime, it seems, appears in France only once a century ... Napoleon left two generations dead on battlefield. De Gaulle was more sparing with other people's blood; even so, he left us, as it were, stranded, alive but dazed... A delusion, perhaps, but one that turns the world upside down: causes events and movements; divides people into supporters and adversaries; leaves traces in the form of civil and penal codes and railways, factories and institutions (the Fifth Republic has already lasted three times as long as the Empire). A statesman who gets something going, who has followers, escapes the reality of the reports and statistics and become part of imagination. Napoleon and de Gaulle modified the state of things because they modified souls".[223]
However, Debray pointed out that there is a difference between Napoleon and de Gaulle: "How can the exterminator be compared with the liberator? ... The former ran the whole enterprise into the ground, while the latter managed to save it. So that to measure the rebel against the despot, the challenger against the leader, is just glaringly idiotic. You simply do not put an adventurer who worked for himself or his family on the same level as a commander-in-chief serving his country. ... Regrettably, Gaullism and Bonapartism have a number of features in common, but Napoleon and de Gaulle do not have the same moral value. ... the first wanted a Holy French Empire without the faith, a Europe under French occupation. The second wanted to rescue the nation from the emperors and establish a free France in a free Europe".[223]
While de Gaulle had many admirers, he was at the same time one of the most hated and reviled men in modern French history.[298]
A number of monuments have been built to commemorate the life of Charles de Gaulle.
France's largest airport, located in Roissy, outside Paris, is named Charles de Gaulle Airport in his honour. France's nuclear-powered aircraft carrier is also named after him.



Suleiman I (Ottoman Turkish: سليمان اول, romanized: Süleyman-ı Evvel; Turkish: I. Süleyman; 6 November 1494 – 6 September 1566), commonly known as Suleiman the Magnificent in the West and Suleiman the Lawgiver (Ottoman Turkish: قانونى سلطان سليمان, romanized: Ḳānūnī Sulṭān Süleymān) in his realm, was the tenth and longest-reigning Sultan of the Ottoman Empire from 1520 until his death in 1566.[2]: 541–45  Under his administration, the Ottoman Empire ruled over at least 25 million people.
Suleiman succeeded his father, Selim I, as sultan on 30 September 1520 and began his reign with campaigns against the Christian powers in central Europe and the Mediterranean. Belgrade fell to him in 1521 and the island of Rhodes in 1522–23. At Mohács, in August 1526, Suleiman broke the military strength of Hungary.
Suleiman became a prominent monarch of 16th-century Europe, presiding over the apex of the Ottoman Empire's economic, military and political power. Suleiman personally led Ottoman armies in conquering the Christian strongholds of Belgrade and Rhodes as well as most of Hungary before his conquests were checked at the siege of Vienna in 1529. He annexed much of the Middle East in his conflict with the Safavids and large areas of North Africa as far west as Algeria. Under his rule, the Ottoman fleet dominated the seas from the Mediterranean to the Red Sea and through the Persian Gulf.[4]: 61 
At the helm of an expanding empire, Suleiman personally instituted major judicial changes relating to society, education, taxation and criminal law. His reforms, carried out in conjunction with the empire's chief judicial official Ebussuud Efendi, harmonized the relationship between the two forms of Ottoman law: sultanic (Kanun) and religious (Sharia).[5] He was a distinguished poet and goldsmith; he also became a great patron of culture, overseeing the "Golden" age of the Ottoman Empire in its artistic, literary and architectural development.[6]
Breaking with Ottoman tradition, Suleiman married Hürrem Sultan, a woman from his harem, an Orthodox Christian of Ruthenian origin who converted to Islam, and who became famous in the West by the name Roxelana, due to her red hair. Their son, Selim II, succeeded Suleiman following his death in 1566 after 46 years of rule. Suleiman's other potential heirs, Mehmed and Mustafa, had died; Mehmed had died in 1543 from smallpox, and Mustafa had been strangled to death in 1553 at the sultan's order. His other son Bayezid was executed in 1561 on Suleiman's orders, along with Bayezid's four sons, after a rebellion. Although scholars typically regarded the period after his death to be one of crisis and adaptation rather than simple decline,[7][8][9] the end of Suleiman's reign was a watershed in Ottoman history. In the decades after Suleiman, the empire began to experience significant political, institutional, and economic changes, a phenomenon often referred to as the Transformation of the Ottoman Empire.[10]: 11 [11]
Suleiman the Magnificent (محتشم سليمان Muḥteşem Süleymān), as he was known in the West, was also called Suleiman the First (سلطان سليمان أول Sulṭān Süleymān-ı Evvel), and Suleiman the Lawgiver (قانونی سلطان سليمان Ḳānūnī Sulṭān Süleymān) for his reform of the Ottoman legal system.[12]
It is unclear when exactly the term Kanunî (the Lawgiver) first came to be used as an epithet for Suleiman. It is entirely absent from sixteenth and seventeenth-century Ottoman sources and may date from the early 18th century.[13]
There is a tradition of western origin, according to which Suleiman the Magnificent was "Suleiman II", but that tradition has been based on an erroneous assumption that Süleyman Çelebi was to be recognised as a legitimate sultan.[14]
Suleiman was born in Trabzon on the southern coast of the Black Sea to Şehzade Selim (later Selim I), probably on 6 November 1494, although this date is not known with absolute certainty or evidence.[15] His mother was Hafsa Sultan, a convert to Islam of unknown origins, who died in 1534.[16]: 9  At the age of seven, Suleiman began studies of science, history, literature, theology and military tactics in the schools of the imperial Topkapı Palace in Constantinople. As a young man, he befriended Pargalı Ibrahim, a Greek slave who later became one of his most trusted advisers (but who was later executed on Suleiman's orders).[17] At age seventeen, he was appointed as the governor of first Kaffa (Theodosia), then Manisa, with a brief tenure at Edirne.
Upon the death of his father, Selim I (r. 1512–1520), Suleiman entered Constantinople and ascended to the throne as the tenth Ottoman Sultan. An early description of Suleiman, a few weeks following his accession, was provided by the Venetian envoy Bartolomeo Contarini: 
The sultan is only twenty-five years [actually 26] old, tall and slender but tough, with a thin and bony face. Facial hair is evident, but only barely. The sultan appears friendly and in good humor. Rumor has it that Suleiman is aptly named[clarification needed], enjoys reading, is knowledgeable and shows good judgment."[16]: 2 Upon succeeding his father, Suleiman began a series of military conquests, eventually leading to a revolt led by the Ottoman-appointed governor of Damascus in 1521. Suleiman soon made preparations for the conquest of Belgrade from the Kingdom of Hungary—something his great-grandfather Mehmed II had failed to achieve because of John Hunyadi's strong defense in the region. Its capture was vital in removing the Hungarians and Croats who, following the defeats of the Albanians, Bosniaks, Bulgarians, Byzantines and the Serbs, remained the only formidable force who could block further Ottoman gains in Europe. Suleiman encircled Belgrade and began a series of heavy bombardments from an island in the Danube. Belgrade, with a garrison of only 700 men, and receiving no aid from Hungary, fell in August 1521.[18]: 49 
The road to Hungary and Austria lay open, but Suleiman turned his attention instead to the Eastern Mediterranean island of Rhodes, the home base of the Knights Hospitaller. Suleiman built a large fortification, Marmaris Castle, that served as a base for the Ottoman Navy. Following the five-month Siege of Rhodes (1522), Rhodes capitulated and Suleiman allowed the Knights of Rhodes to depart.[19] The conquest of the island cost the Ottomans 50,000[20][21] to 60,000[21] dead from battle and sickness (Christian claims went as high as 64,000 Ottoman battle deaths and 50,000 disease deaths).[21]
As relations between Hungary and the Ottoman Empire deteriorated, Suleiman resumed his campaign in Central Europe, and on 29 August 1526 he defeated Louis II of Hungary (1506–1526) at the Battle of Mohács. Upon encountering the lifeless body of King Louis, Suleiman is said to have lamented: "I came indeed in arms against him; but it was not my wish that he should be thus cut off before he scarcely tasted the sweets of life and royalty."[22] While Suleiman was campaigning in Hungary, Turkmen tribes in central Anatolia (in Cilicia) revolted under the leadership of Kalender Çelebi.[23]
Some Hungarian nobles proposed that Ferdinand, who was the ruler of neighboring Austria and tied to Louis II's family by marriage, be King of Hungary, citing previous agreements that the Habsburgs would take the Hungarian throne if Louis died without heirs.[18]: 52  However, other nobles turned to the nobleman John Zápolya, who was being supported by Suleiman. Under Charles V and his brother Ferdinand I, the Habsburgs reoccupied Buda and took possession of Hungary. Reacting in 1529, Suleiman marched through the valley of the Danube and regained control of Buda; in the following autumn, his forces laid siege to Vienna. This was to be the Ottoman Empire's most ambitious expedition and the apogee of its drive to the West. With a reinforced garrison of 16,000 men,[24] the Austrians inflicted the first defeat on Suleiman, sowing the seeds of a bitter Ottoman–Habsburg rivalry that lasted until the 20th century. His second attempt to conquer Vienna failed in 1532, as Ottoman forces were delayed by the siege of Güns and failed to reach Vienna. In both cases, the Ottoman army was plagued by bad weather, forcing them to leave behind essential siege equipment, and was hobbled by overstretched supply lines.[25]: 444  In 1533 the Treaty of Constantinople was signed by Ferdinand I, in which he acknowledged Ottoman suzerainty and recognised Suleiman as his “father and suzerain”, he also agreed to pay an annual tribute and accepted the Ottoman grand vizier as his brother and equal in rank.[26][27][28][29][30]
By the 1540s, a renewal of the conflict in Hungary presented Suleiman with the opportunity to avenge the defeat suffered at Vienna. In 1541, the Habsburgs attempted to lay siege to Buda but were repulsed, and more Habsburg fortresses were captured by the Ottomans in two consecutive campaigns in 1541 and 1544 as a result,[18]: 53  Ferdinand and Charles were forced to conclude a humiliating five-year treaty with Suleiman. Ferdinand renounced his claim to the Kingdom of Hungary and was forced to pay a fixed yearly sum to the Sultan for the Hungarian lands he continued to control. Of more symbolic importance, the treaty referred to Charles V not as 'Emperor' but as the 'King of Spain', leading Suleiman to identify as the true 'Caesar'.[18]: 54 
In 1552, Suleiman's forces laid siege of Eger, located in the northern part of the Kingdom of Hungary, but the defenders led by István Dobó repelled the attacks and defended the Eger Castle.[31]
Suleiman's father had made war with Persia a high priority.  At first, Suleiman shifted attention to Europe and was content to contain Persia, which was preoccupied by its own enemies to its east. After Suleiman stabilized his European frontiers, he now turned his attention to Persia, the base for the rival Islamic faction of Shi'a.  The Safavid dynasty became the main enemy after two episodes. First, Shah Tahmasp killed  the Baghdad governor loyal to Suleiman, and put his own man in. Second, the governor of Bitlis had defected and sworn allegiance to the Safavids.[18]: 51  As a result, in 1533, Suleiman ordered his Pargalı Ibrahim Pasha to lead an army into eastern Asia Minor where he retook Bitlis and occupied Tabriz without resistance. Suleiman joined Ibrahim in 1534. They made a push towards Persia, only to find the Shah sacrificing territory instead of facing a pitched battle, resorting to harassment of the Ottoman army as it proceeded along the harsh interior.[32] In 1535 Suleiman made a grand entrance into Baghdad. He enhanced his local support by restoring the tomb of Abu Hanifa, the founder of the Hanafi school of Islamic law to which the Ottomans adhered.[33]
Attempting to defeat the Shah once and for all, Suleiman embarked upon a second campaign in 1548–1549. As in the previous attempt, Tahmasp avoided confrontation with the Ottoman army and instead chose to retreat, using scorched earth tactics in the process and exposing the Ottoman army to the harsh winter of the Caucasus.[32] Suleiman abandoned the campaign with temporary Ottoman gains in Tabriz and the Urmia region, a lasting presence in the province of Van, control of the western half of Azerbaijan and some forts in Georgia.[34]
In 1553 Suleiman began his third and final campaign against the Shah. Having initially lost territories in Erzurum to the Shah's son, Suleiman retaliated by recapturing Erzurum, crossing the Upper Euphrates and laying waste to parts of Persia. The Shah's army continued its strategy of avoiding the Ottomans, leading to a stalemate from which neither army made any significant gain. In 1555, a settlement known as the Peace of Amasya was signed, which defined the borders of the two empires. By this treaty, Armenia and Georgia were divided equally between the two, with Western Armenia, western Kurdistan, and western Georgia (incl. western Samtskhe) falling in Ottoman hands while Eastern Armenia, eastern Kurdistan, and eastern Georgia (incl. eastern Samtskhe) stayed in Safavid hands.[35] The Ottoman Empire obtained most of Iraq, including Baghdad, which gave them access to the Persian Gulf, while the Persians retained their former capital Tabriz and all their other northwestern territories in the Caucasus and as they were prior to the wars, such as Dagestan and all of what is now Azerbaijan.[36][37]
Ottoman ships had been sailing in the Indian Ocean since the year 1518. Ottoman admirals such as Hadim Suleiman Pasha, Seydi Ali Reis[38] and Kurtoğlu Hızır Reis are known to have voyaged to the Mughal imperial ports of Thatta, Surat and Janjira. The Mughal Emperor Akbar the Great himself is known to have exchanged six documents with Suleiman the Magnificent.[38][39][40]
Suleiman led several naval campaigns against the Portuguese in an attempt to remove them and reestablish trade with the Mughal Empire. Aden in Yemen was captured by the Ottomans in 1538, in order to provide an Ottoman base for raids against Portuguese possessions on the western coast of the Mughal Empire.[41] Sailing on, the Ottomans failed against the Portuguese at the siege of Diu in September 1538, but then returned to Aden, where they fortified the city with 100 pieces of artillery.[41][42] From this base, Sulayman Pasha managed to take control of the whole country of Yemen, also taking Sana'a.[41]
With its strong control of the Red Sea, Suleiman successfully managed to dispute control of the trade routes to the Portuguese and maintained a significant level of trade with the Mughal Empire throughout the 16th century.[43]
From 1526 until 1543, Suleiman stationed over 900 Turkish soldiers to fight alongside the Somali Adal Sultanate led by Ahmad ibn Ibrahim al-Ghazi during the Conquest of Abyssinia.[44] After the first Ajuran-Portuguese war, the Ottoman Empire would in 1559 absorb the weakened Adal Sultanate into its domain. This expansion furthered Ottoman rule in Somalia and the Horn of Africa. This also increased its influence in the Indian Ocean to compete with the Portuguese Empire with its close ally, the Ajuran Empire.[45]
In 1564, Suleiman received an embassy from Aceh (a sultanate on Sumatra, in modern Indonesia), requesting Ottoman support against the Portuguese. As a result, an Ottoman expedition to Aceh was launched, which was able to provide extensive military support to the Acehnese.[46]
The discovery of new maritime trade routes by Western European states allowed them to avoid the Ottoman trade monopoly. The Portuguese discovery of the Cape of Good Hope in 1488 initiated a series of Ottoman-Portuguese naval wars in the Ocean throughout the 16th century. The Ajuran Sultanate allied with the Ottomans defied the Portuguese economic monopoly in the Indian Ocean by employing a new coinage which followed the Ottoman pattern, thus proclaiming an attitude of economic independence in regard to the Portuguese.[47]
Having consolidated his conquests on land, Suleiman was greeted with the news that the fortress of Koroni in Morea (the modern Peloponnese, peninsular Greece) had been lost to Charles V's admiral, Andrea Doria. The presence of the Spanish in the Eastern Mediterranean concerned Suleiman, who saw it as an early indication of Charles V's intention to rival Ottoman dominance in the region. Recognizing the need to reassert naval preeminence in the Mediterranean, Suleiman appointed an exceptional naval commander in the form of Khair ad Din, known to Europeans as Barbarossa. Once appointed admiral-in-chief, Barbarossa was charged with rebuilding the Ottoman fleet.
In 1535, Charles V led a Holy League of 26,700 soldiers (10,000 Spaniards, 8,000 Italians, 8,000 Germans, and 700 Knights of St. John)[21] to victory against the Ottomans at Tunis, which together with the war against Venice the following year, led Suleiman to accept proposals from Francis I of France to form an alliance against Charles.[18]: 51  Huge Muslim territories in North Africa were annexed. The piracy carried on thereafter by the Barbary pirates of North Africa can be seen in the context of the wars against Spain.
In 1541, the Spaniards led an unsuccessful expedition to Algiers. In 1542, facing a common Habsburg enemy during the Italian Wars, Francis I sought to renew the Franco-Ottoman alliance. In early 1542, Polin successfully negotiated the details of the alliance, with the Ottoman Empire promising to send 60,000 troops against the territories of the German king Ferdinand, as well as 150 galleys against Charles, while France promised to attack Flanders, harass the coasts of Spain with a naval force, and send 40 galleys to assist the Turks for operations in the Levant.[48]
In August 1551, Ottoman naval commander Turgut Reis attacked and captured Tripoli which had been a possession of the Knights of Malta since 1530. In 1553, Turgut Reis was nominated commander of Tripoli by Suleiman, making the city an important center for piratical raids in the Mediterranean and the capital of the Ottoman province of Tripolitania.[49] In 1560, a powerful naval force was sent to recapture Tripoli, but that force was defeated in the Battle of Djerba.[50]
Elsewhere in the Mediterranean, when the Knights Hospitallers were re-established as the Knights of Malta in 1530, their actions against Muslim navies quickly drew the ire of the Ottomans, who assembled another massive army in order to dislodge the Knights from Malta. The Ottomans invaded Malta in 1565, undertaking the Great Siege of Malta, which began on 18 May and lasted until 8 September, and is portrayed vividly in the frescoes of Matteo Perez d'Aleccio in the Hall of St. Michael and St. George. At first, it seemed that this would be a repeat of the battle on Rhodes, with most of Malta's cities destroyed and half the Knights killed in battle; but a relief force from Spain entered the battle, resulting in the loss of 10,000 Ottoman troops and the victory of the local Maltese citizenry.[51]
While Sultan Suleiman was known as "the Magnificent" in the West, he was always Kanuni Suleiman or "The Lawgiver" (قانونی) to his Ottoman subjects. The overriding law of the empire was the Shari'ah, or Sacred Law, which as the divine law of Islam was outside of the Sultan's powers to change. Yet an area of distinct law known as the Kanuns (قانون, canonical legislation) was dependent on Suleiman's will alone, covering areas such as criminal law, land tenure and taxation.[18]: 244  He collected all the judgments that had been issued by the nine Ottoman Sultans who preceded him. After eliminating duplications and choosing between contradictory statements, he issued a single legal code, all the while being careful not to violate the basic laws of Islam.[52]: 20  It was within this framework that Suleiman, supported by his Grand Mufti Ebussuud, sought to reform the legislation to adapt to a rapidly changing empire. When the Kanun laws attained their final form, the code of laws became known as the kanun‐i Osmani (قانون عثمانی), or the "Ottoman laws". Suleiman's legal code was to last more than three hundred years.[52]: 21 
The Sultan also played a role in protecting the Jewish subjects of his empire for centuries to come. In late 1553 or 1554, on the suggestion of his favorite doctor and dentist, the Spanish Jew Moses Hamon, the Sultan issued a firman (فرمان) formally denouncing blood libels against the Jews.[4]: 124  Furthermore, Suleiman enacted new criminal and police legislation, prescribing a set of fines for specific offenses, as well as reducing the instances requiring death or mutilation. In the area of taxation, taxes were levied on various goods and produce, including animals, mines, profits of trade, and import-export duties.
Higher medreses provided education of university status, whose graduates became imams (امام) or teachers. Educational centers were often one of many buildings surrounding the courtyards of mosques, others included libraries, baths, soup kitchens, residences and hospitals for the benefit of the public.[53]
Under Suleiman's patronage, the Ottoman Empire entered the golden age of its cultural development. Hundreds of imperial artistic societies (called the اهل حرف Ehl-i Hiref, "Community of the Craftsmen") were administered at the Imperial seat, the Topkapı Palace. After an apprenticeship, artists and craftsmen could advance in rank within their field and were paid commensurate wages in quarterly annual installments. Payroll registers that survive testify to the breadth of Suleiman's patronage of the arts, the earliest of the documents dating from 1526 list 40 societies with over 600 members. The Ehl-i Hiref attracted the empire's most talented artisans to the Sultan's court, both from the Islamic world and from the recently conquered territories in Europe, resulting in a blend of Arabic, Turkish and European cultures.[6] Artisans in service of the court included painters, book binders, furriers, jewellers and goldsmiths. Whereas previous rulers had been influenced by Persian culture (Suleiman's father, Selim I, wrote poetry in Persian), Suleiman's patronage of the arts saw the Ottoman Empire assert its own artistic legacy.[4]: 70 
Suleiman himself was an accomplished poet, writing in Persian and Turkish under the takhallus (nom de plume) Muhibbi (محبی, "Lover"). Some of Suleiman's verses have become Turkish proverbs, such as the well-known Everyone aims at the same meaning, but many are the versions of the story. When his young son Mehmed died in 1543, he composed a moving chronogram to commemorate the year: Peerless among princes, my Sultan Mehmed.[54] In Turkish the chronogram reads شهزاده‌لر گزیده‌سی سلطان محمدم (Şehzadeler güzidesi Sultan Muhammed'üm), in which the Arabic Abjad numerals total 955, the equivalent in the Islamic calendar of 1543 AD. In addition to Suleiman's own work, many great talents enlivened the literary world during Suleiman's rule, including Fuzûlî and Bâkî. The literary historian Elias John Wilkinson Gibb observed that "at no time, even in Turkey, was greater encouragement given to poetry than during the reign of this Sultan".[55] Suleiman's most famous verse is:
The people think of wealth and power as the greatest fate,
But in this world a spell of health is the best state.
What men call sovereignty is a worldly strife and constant war;
Worship of God is the highest throne, the happiest of all estates.[4]: 84 
Suleiman also became renowned for sponsoring a series of monumental architectural developments within his empire. The Sultan sought to turn Constantinople into the center of Islamic civilization by a series of projects, including bridges, mosques, palaces and various charitable and social establishments. The greatest of these were built by the Sultan's chief architect, Mimar Sinan, under whom Ottoman architecture reached its zenith. Sinan became responsible for over three hundred monuments throughout the empire, including his two masterpieces, the Süleymaniye and Selimiye mosques—the latter built in Adrianople (now Edirne) in the reign of Suleiman's son Selim II. Suleiman also restored the Dome of the Rock in Jerusalem and the Walls of Jerusalem (which are the current walls of the Old City of Jerusalem), renovated the Kaaba in Mecca, and constructed a complex in Damascus.[56]
Suleiman loved gardens and his shaykh grew a white tulip in one of the gardens. Some of the nobles in the court had seen the tulip and they also began growing their own.[57] Soon images of the tulip were woven into rugs and fired into ceramics.[58] Suleiman is credited with large-scale cultivation of the tulip and it is thought that the tulips spread throughout Europe because of Suleiman. It is thought that diplomats who visited him were gifted the flowers while visiting his court.[59]
Suleiman had two known consorts, though in total there were 17 women in his harem when he was a Şehzade. The mothers of Mahmud, Murad and Raziye are unknown.[60]
Suleiman I had eight sons:
Suleiman was infatuated with Hurrem Sultan, a harem girl from Ruthenia, then part of Poland. Western diplomats, taking notice of the palace gossip about her, called her "Russelazie" or "Roxelana", referring to her Ruthenian origins.[69] The daughter of an Orthodox priest, she was captured by Tatars from Crimea, sold as a slave in Constantinople, and eventually rose through the ranks of the Harem to become Suleiman's favorite. Hurrem, a former concubine, became the legal wife of the Sultan, much to the astonishment of the observers in the palace and the city.[4]: 86  He also allowed Hurrem Sultan to remain with him at court for the rest of her life, breaking another tradition—that when imperial heirs came of age, they would be sent along with the imperial concubine who bore them to govern remote provinces of the Empire, never to return unless their progeny succeeded to the throne.[18]: 90 
Under his pen name, Muhibbi, Sultan Suleiman composed this poem for Hurrem Sultan:
Throne of my lonely niche, my wealth, my love, my moonlight.
My most sincere friend, my confidant, my very existence, my Sultan, my one and only love.
The most beautiful among the beautiful ...
My springtime, my merry faced love, my daytime, my sweetheart, laughing leaf ...
My plants, my sweet, my rose, the one only who does not distress me in this room ...
My Istanbul, my karaman, the earth of my Anatolia
My Badakhshan, my Baghdad and Khorasan
My woman of the beautiful hair, my love of the slanted brow, my love of eyes full of misery ...
I'll sing your praises always
I, lover of the tormented heart, Muhibbi of the eyes full of tears, I am happy.[70]
Before his downfall, Pargalı Ibrahim Pasha was an inseparable friend and lover of Suleiman. In fact, he is referred to by his chroniclers as 'the favourite' (Maḳbūl) along with 'the executed' (Maḳtūl).[71][72] Historians state that Suleiman I is remembered for 'his passion for two of his slaves: for his beloved Ibrahim when the sultan was a hot-blooded youth, and for his beloved Hurrem when he was mature.'[72]
Ibrahim was originally a Christian from Parga (in Epirus), who was captured in a raid during the 1499–1503 Ottoman–Venetian War, and was given as a slave to Suleiman most likely in 1514.[73] Ibrahim converted to Islam and Suleiman made him the royal falconer, then promoted him to first officer of the Royal Bedchamber.[4]: 87  It was reported that they slept together in the same bed.[72][74] The sultan also built Ibrahim a lavish palace on the ancient Hippodrome, Istanbul's main forum outside the Hagia Sophia and Topkapı Palace. Despite his following marriage and his new sumptuous residence, Ibrahim sometimes spent the night with Suleiman I at Topkapı Palace. In turn, the sultan occasionally slept at Ibrahim's lodgings.[72] Ibrahim Pasha rose to Grand Vizier in 1523 and commander-in-chief of all the armies. Suleiman also conferred upon Ibrahim Pasha the honor of beylerbey of Rumelia (first-ranking military governor-general), granting Ibrahim authority over all Ottoman territories in Europe, as well as command of troops residing within them in times of war. At the time, Ibrahim was only about thirty years old and lacked any actual military expertise; it is said that 'tongues wagged' at this unprecedented promotion straight from palace service to the two highest offices of the empire.[72]
During his thirteen years as Grand Vizier, his rapid rise to power and vast accumulation of wealth had made Ibrahim many enemies at the Sultan's court. Suleiman's suspicion of Ibrahim was worsened by a quarrel between the latter and the finance secretary (defterdar) İskender Çelebi. The dispute ended in the disgrace of Çelebi on charges of intrigue, with Ibrahim convincing Suleiman to sentence the defterdar to death. Ibrahim also supported Şehzade Mustafa as the successor of Suleiman. This caused disputes between him and Hürrem Sultan, who wanted her sons to succeed to the throne. Ibrahim eventually fell from grace with the Sultan and his wife. Suleiman consulted his Qadi, who suggested that Ibrahim be put to death. The Sultan recruited assassins and ordered them to strangle Ibrahim in his sleep.[75]
Sultan Suleiman's two known consorts (Hürrem and Mahidevran) had borne him six sons, four of whom survived past the 1550s. They were Mustafa, Selim, Bayezid, and Cihangir. Of these, the eldest was not Hürrem's son, but rather Mahidevran's. Hürrem is usually held at least partly responsible for the intrigues in nominating a successor, though there is no evidence to support this.[65] Although she was Suleiman's wife, she exercised no official public role. This did not, however, prevent Hürrem from wielding powerful political influence. Since the Empire lacked, until the reign of Ahmed I, any formal means of nominating a successor, successions usually involved the death of competing princes in order to avert civil unrest and rebellions.
By 1552, when the campaign against Persia had begun with Rüstem appointed commander-in-chief of the expedition, intrigues against Mustafa began. Rüstem sent one of Suleiman's most trusted men to report that since Suleiman was not at the head of the army, the soldiers thought the time had come to put a younger prince on the throne; at the same time, he spread rumours that Mustafa had proved receptive to the idea. Angered by what he came to believe were Mustafa's plans to claim the throne, the following summer upon return from his campaign in Persia, Suleiman summoned him to his tent in the Ereğli valley.[76] When Mustafa entered his father's tent to meet with him, Suleiman's eunuchs attacked Mustafa, and after a long struggle the mutes killed him using a bow-string.
Cihangir is said to have died of grief a few months after the news of his half-brother's murder.[4]: 89  The two surviving brothers, Selim and Bayezid, were given command in different parts of the empire. Within a few years, however, civil war broke out between the brothers, each supported by his loyal forces. With the aid of his father's army, Selim defeated Bayezid in Konya in 1559, leading the latter to seek refuge with the Safavids along with his four sons. Following diplomatic exchanges, the Sultan demanded from the Safavid Shah that Bayezid be either extradited or executed. In return for large amounts of gold, the Shah allowed a Turkish executioner to strangle Bayezid and his four sons in 1561,[4]: 89  clearing the path for Selim's succession to the throne five years later.
On 6 September 1566, Suleiman, who had set out from Constantinople to command an expedition to Hungary, died before an Ottoman victory at the Siege of Szigetvár in Hungary at the age of 71[2]: 545  and his Grand Vizier Sokollu Mehmed Pasha kept his death secret during the retreat for the enthronement of Selim II.  The sultan's body was taken back to Istanbul to be buried, while his heart, liver, and some other organs were buried in Turbék, outside Szigetvár. A mausoleum constructed above the burial site came to be regarded as a holy place and pilgrimage site. Within a decade a mosque and Sufi hospice were built near it, and the site was protected by a salaried garrison of several dozen men.[77]
The formation of Suleiman's legacy began even before his death. Throughout his reign literary works were commissioned praising Suleiman and constructing an image of him as an ideal ruler, most significantly by Celalzade Mustafa, chancellor of the empire from 1534 to 1557.[10]: 4–5, 250  Later Ottoman writers applied this idealised image of Suleiman to the Near Eastern literary genre of advice literature named naṣīḥatnāme, urging sultans to conform to his model of rulership and to maintain the empire's institutions in their sixteenth-century form. Such writers were pushing back against the political and institutional transformation of the empire after the middle of the sixteenth century, and portrayed deviation from the norm as it had existed under Suleiman as evidence of the decline of the empire.[78]: 54–55, 64  Western historians, failing to recognise that these 'decline writers' were working within an established literary genre and often had deeply personal reasons for criticizing the empire, long took their claims at face value and consequently adopted the idea that the empire entered a period of decline after the death of Suleiman.[78]: 73–77  Since the 1980s this view has been thoroughly reexamined, and modern scholars have come to overwhelmingly reject the idea of decline, labelling it an "untrue myth".[7]
Suleiman's conquests had brought under the control of the Empire major Muslim cities (such as Baghdad), many Balkan provinces (reaching present day Croatia and Hungary), and most of North Africa. His expansion into Europe had given the Ottoman Turks a powerful presence in the European balance of power. Indeed, such was the perceived threat of the Ottoman Empire under the reign of Suleiman that Austria's ambassador Busbecq warned of Europe's imminent conquest: "On [the Turks'] side are the resources of a mighty empire, strength unimpaired, habituation to victory, endurance of toil, unity, discipline, frugality and watchfulness ... Can we doubt what the result will be? ... When the Turks have settled with Persia, they will fly at our throats supported by the might of the whole East; how unprepared we are I dare not say."[79] Suleiman's legacy was not, however, merely in the military field. The French traveler Jean de Thévenot bears witness a century later to the "strong agricultural base of the country, the well being of the peasantry, the abundance of staple foods and the pre-eminence of organization in Suleiman's government".[80]
Even thirty years after his death, "Sultan Solyman" was quoted by the English playwright William Shakespeare as a military prodigy in The Merchant of Venice, where the Prince of Morocco boasts about his prowess by saying that he defeated Suleiman in three battles (Act 2, Scene 1).[81][82]
Through the distribution of court patronage, Suleiman also presided over a Golden Age in Ottoman arts, witnessing immense achievement in the realms of architecture, literature, art, theology and philosophy.[6][83] Today the skyline of the Bosphorus and of many cities in modern Turkey and the former Ottoman provinces, are still adorned with the architectural works of Mimar Sinan. One of these, the Süleymaniye Mosque, is the final resting place of Suleiman: he is buried in a domed mausoleum attached to the mosque.
Nevertheless, assessments of Suleiman's reign have frequently fallen into the trap of the Great Man theory of history. The administrative, cultural, and military achievements of the age were a product not of Suleiman alone, but also of the many talented figures who served him, such as grand viziers Ibrahim Pasha and Rüstem Pasha, the Grand Mufti Ebussuud Efendi, who played a major role in legal reform, and chancellor and chronicler Celalzade Mustafa, who played a major role in bureaucratic expansion and in constructing Suleiman's legacy.[2]: 542 
In an inscription dating from 1537 on the citadel of Bender, Moldova, Suleiman the Magnificent gave expression to his power:[84]
I am God's slave and sultan of this world. By the grace of God I am head of Muhammad's community. God's might and Muhammad's miracles are my companions. I am Süleymân, in whose name the hutbe is read in Mecca and Medina. In Baghdad I am the shah, in Byzantine realms the caesar, and in Egypt the sultan; who sends his fleets to the seas of Europe, the Maghrib and India. I am the sultan who took the crown and throne of Hungary and granted them to a humble slave. The voivoda Petru raised his head in revolt, but my horse's hoofs ground him into the dust, and I conquered the land of Moldovia.Suleiman, as sculpted by Joseph Kiselewski,[85] is present on one of the 23 relief portraits over the gallery doors of the House Chamber of the United States Capitol that depicts historical figures noted for their work in establishing the principles that underlie American law.[86]

Free will is the notional capacity or ability to choose between different possible courses of action unimpeded.[1]
Free will is closely linked to the concepts of moral responsibility, praise, culpability, sin, and other judgements which apply only to actions that are freely chosen. It is also connected with the concepts of advice, persuasion, deliberation, and prohibition. Traditionally, only actions that are freely willed are seen as deserving credit or blame. Whether free will exists, what it is and the implications of whether it exists or not are some of the longest running debates of philosophy and religion. Some conceive of free will as the ability to act beyond the limits of external influences or wishes.
Some conceive free will to be the capacity to make choices undetermined by past events. Determinism suggests that only one course of events is possible, which is inconsistent with a libertarian model of free will.[2] Ancient Greek philosophy identified this issue,[3] which remains a major focus of philosophical debate. The view that conceives free will as incompatible with determinism is called incompatibilism and encompasses both metaphysical libertarianism (the claim that determinism is false and thus free will is at least possible) and hard determinism (the claim that determinism is true and thus free will is not possible). Incompatibilism also encompasses hard incompatibilism, which holds not only determinism but also indeterminism to be incompatible with free will and thus free will to be impossible whatever the case may be regarding determinism.
In contrast, compatibilists hold that free will is compatible with determinism. Some compatibilists even hold that determinism is necessary for free will, arguing that choice involves preference for one course of action over another, requiring a sense of how choices will turn out.[4][5] Compatibilists thus consider the debate between libertarians and hard determinists over free will vs. determinism a false dilemma.[6] Different compatibilists offer very different definitions of what "free will" means and consequently find different types of constraints to be relevant to the issue. Classical compatibilists considered free will nothing more than freedom of action, considering one free of will simply if, had one counterfactually wanted to do otherwise, one could have done otherwise without physical impediment. Contemporary compatibilists instead identify free will as a psychological capacity, such as to direct one's behavior in a way responsive to reason, and there are still further different conceptions of free will, each with their own concerns, sharing only the common feature of not finding the possibility of determinism a threat to the possibility of free will.[7]
The problem of free will has been identified in ancient Greek philosophical literature. The notion of compatibilist free will has been attributed to both Aristotle (fourth century BCE) and Epictetus (1st century CE); "it was the fact that nothing hindered us from doing or choosing something that made us have control over them".[3][8] According to Susanne Bobzien, the notion of incompatibilist free will is perhaps first identified in the works of Alexander of Aphrodisias (third century CE); "what makes us have control over things is the fact that we are causally undetermined in our decision and thus can freely decide between doing/choosing or not doing/choosing them".
The term "free will" (liberum arbitrium) was introduced by Christian philosophy (4th century CE). It has traditionally meant (until the Enlightenment proposed its own meanings) lack of necessity in human will,[9] so that "the will is free" meant "the will does not have to be such as it is". This requirement was universally embraced by both incompatibilists and compatibilists.[10]
The underlying questions are whether we have control over our actions, and if so, what sort of control, and to what extent. These questions predate the early Greek stoics (for example, Chrysippus), and some modern philosophers lament the lack of progress over all these centuries.[11][12]
On one hand, humans have a strong sense of freedom, which leads them to believe that they have free will.[13][14] On the other hand, an intuitive feeling of free will could be mistaken.[15][16]
It is difficult to reconcile the intuitive evidence that conscious decisions are causally effective with the view that the physical world can be explained entirely by physical law.[17] The conflict between intuitively felt freedom and natural law arises when either causal closure or physical determinism (nomological determinism) is asserted. With causal closure, no physical event has a cause outside the physical domain, and with physical determinism, the future is determined entirely by preceding events (cause and effect).
The puzzle of reconciling 'free will' with a deterministic universe is known as the problem of free will or sometimes referred to as the dilemma of determinism.[18] This dilemma leads to a moral dilemma as well: the question of how to assign responsibility for actions if they are caused entirely by past events.[19][20]
Compatibilists maintain that mental reality is not of itself causally effective.[21][22] Classical compatibilists have addressed the dilemma of free will by arguing that free will holds as long as humans are not externally constrained or coerced.[23] Modern compatibilists make a distinction between freedom of will and freedom of action, that is, separating freedom of choice from the freedom to enact it.[24] Given that humans all experience a sense of free will, some modern compatibilists think it is necessary to accommodate this intuition.[25][26] Compatibilists often associate freedom of will with the ability to make rational decisions.
A different approach to the dilemma is that of incompatibilists, namely, that if the world is deterministic, then our feeling that we are free to choose an action is simply an illusion. Metaphysical libertarianism is the form of incompatibilism which posits that determinism is false and free will is possible (at least some people have free will).[27] This view is associated with non-materialist constructions,[15] including both traditional dualism, as well as models supporting more minimal criteria; such as the ability to consciously veto an action or competing desire.[28][29] Yet even with physical indeterminism, arguments have been made against libertarianism in that it is difficult to assign Origination (responsibility for "free" indeterministic choices).
Free will here is predominantly treated with respect to physical determinism in the strict sense of nomological determinism, although other forms of determinism are also relevant to free will.[30] For example, logical and theological determinism challenge metaphysical libertarianism with ideas of destiny and fate, and biological, cultural and psychological determinism feed the development of compatibilist models. Separate classes of compatibilism and incompatibilism may even be formed to represent these.[31]
Below are the classic arguments bearing upon the dilemma and its underpinnings.
Incompatibilism is the position that free will and determinism are logically incompatible, and that the major question regarding whether or not people have free will is thus whether or not their actions are determined. "Hard determinists", such as d'Holbach, are those incompatibilists who accept determinism and reject free will. In contrast, "metaphysical libertarians", such as Thomas Reid, Peter van Inwagen, and Robert Kane, are those incompatibilists who accept free will and deny determinism, holding the view that some form of indeterminism is true.[32] Another view is that of hard incompatibilists, which state that free will is incompatible with both determinism and indeterminism.[33]
Traditional arguments for incompatibilism are based on an "intuition pump": if a person is like other mechanical things that are determined in their behavior such as a wind-up toy, a billiard ball, a puppet, or a robot, then people must not have free will.[32][34] This argument has been rejected by compatibilists such as Daniel Dennett on the grounds that, even if humans have something in common with these things, it remains possible and plausible that we are different from such objects in important ways.[35]
Another argument for incompatibilism is that of the "causal chain". Incompatibilism is key to the idealist theory of free will. Most incompatibilists reject the idea that freedom of action consists simply in "voluntary" behavior. They insist, rather, that free will means that someone must be the "ultimate" or "originating" cause of his actions. They must be causa sui, in the traditional phrase. Being responsible for one's choices is the first cause of those choices, where first cause means that there is no antecedent cause of that cause. The argument, then, is that if a person has free will, then they are the ultimate cause of their actions. If determinism is true, then all of a person's choices are caused by events and facts outside their control. So, if everything someone does is caused by events and facts outside their control, then they cannot be the ultimate cause of their actions. Therefore, they cannot have free will.[36][37][38] This argument has also been challenged by various compatibilist philosophers.[39][40]
A third argument for incompatibilism was formulated by Carl Ginet in the 1960s and has received much attention in the modern literature. The simplified argument runs along these lines: if determinism is true, then we have no control over the events of the past that determined our present state and no control over the laws of nature. Since we can have no control over these matters, we also can have no control over the consequences of them. Since our present choices and acts, under determinism, are the necessary consequences of the past and the laws of nature, then we have no control over them and, hence, no free will. This is called the consequence argument.[41][42] Peter van Inwagen remarks that C.D. Broad had a version of the consequence argument as early as the 1930s.[43]
The difficulty of this argument for some compatibilists lies in the fact that it entails the impossibility that one could have chosen other than one has. For example, if Jane is a compatibilist and she has just sat down on the sofa, then she is committed to the claim that she could have remained standing, if she had so desired. But it follows from the consequence argument that, if Jane had remained standing, she would have either generated a contradiction, violated the laws of nature or changed the past. Hence, compatibilists are committed to the existence of "incredible abilities", according to Ginet and van Inwagen. One response to this argument is that it equivocates on the notions of abilities and necessities, or that the free will evoked to make any given choice is really an illusion and the choice had been made all along, oblivious to its "decider".[42] David Lewis suggests that compatibilists are only committed to the ability to do something otherwise if different circumstances had actually obtained in the past.[44]
Using T, F for "true" and "false" and ? for undecided, there are exactly nine positions regarding determinism/free will that consist of any two of these three possibilities:[45]
Incompatibilism may occupy any of the nine positions except (5), (8) or (3), which last corresponds to soft determinism. Position (1) is hard determinism, and position (2) is libertarianism. The position (1) of hard determinism adds to the table the contention that D implies FW is untrue, and the position (2) of libertarianism adds the contention that FW implies D is untrue. Position (9) may be called hard incompatibilism if one interprets ? as meaning both concepts are of dubious value. Compatibilism itself may occupy any of the nine positions, that is, there is no logical contradiction between determinism and free will, and either or both may be true or false in principle. However, the most common meaning attached to compatibilism is that some form of determinism is true and yet we have some form of free will, position (3).[46]
Alex Rosenberg makes an extrapolation of physical determinism as inferred on the macroscopic scale by the behaviour of a set of dominoes to neural activity in the brain where; "If the brain is nothing but a complex physical object whose states are as much governed by physical laws as any other physical object, then what goes on in our heads is as fixed and determined by prior events as what goes on when one domino topples another in a long row of them."[47] Physical determinism is currently disputed by prominent interpretations of quantum mechanics, and while not necessarily representative of intrinsic indeterminism in nature, fundamental limits of precision in measurement are inherent in the uncertainty principle.[48] The relevance of such prospective indeterminate activity to free will is, however, contested,[49] even when chaos theory is introduced to magnify the effects of such microscopic events.[29][50]
Below these positions are examined in more detail.[45]
Determinism can be divided into causal, logical and theological determinism.[51] Corresponding to each of these different meanings, there arises a different problem for free will.[52] Hard determinism is the claim that determinism is true, and that it is incompatible with free will, so free will does not exist. Although hard determinism generally refers to nomological determinism (see causal determinism below), it can include all forms of determinism that necessitate the future in its entirety.[53] Relevant forms of determinism include:
Other forms of determinism are more relevant to compatibilism, such as biological determinism, the idea that all behaviors, beliefs, and desires are fixed by our genetic endowment and our biochemical makeup, the latter of which is affected by both genes and environment, cultural determinism and psychological determinism.[52] Combinations and syntheses of determinist theses, such as bio-environmental determinism, are even more common.
Suggestions have been made that hard determinism need not maintain strict determinism, where something near to, like that informally known as adequate determinism, is perhaps more relevant.[30] Despite this, hard determinism has grown less popular in present times, given scientific suggestions that determinism is false – yet the intention of their position is sustained by hard incompatibilism.[27]
Metaphysical libertarianism is one philosophical view point under that of incompatibilism. Libertarianism holds onto a concept of free will that requires that the agent be able to take more than one possible course of action under a given set of circumstances.[62]
Accounts of libertarianism subdivide into non-physical theories and physical or naturalistic theories. Non-physical theories hold that the events in the brain that lead to the performance of actions do not have an entirely physical explanation, which requires that the world is not closed under physics. This includes interactionist dualism, which claims that some non-physical mind, will, or soul overrides physical causality. Physical determinism implies there is only one possible future and is therefore not compatible with libertarian free will. As consequent of incompatibilism, metaphysical libertarian explanations that do not involve dispensing with physicalism require physical indeterminism, such as probabilistic subatomic particle behavior – theory unknown to many of the early writers on free will. Incompatibilist theories can be categorised based on the type of indeterminism they require; uncaused events, non-deterministically caused events, and agent/substance-caused events.[59]
Non-causal accounts of incompatibilist free will do not require a free action to be caused by either an agent or a physical event. They either rely upon a world that is not causally closed, or physical indeterminism. Non-causal accounts often claim that each intentional action requires a choice or volition – a willing, trying, or endeavoring on behalf of the agent (such as the cognitive component of lifting one's arm).[63][64] Such intentional actions are interpreted as free actions. It has been suggested, however, that such acting cannot be said to exercise control over anything in particular. According to non-causal accounts, the causation by the agent cannot be analysed in terms of causation by mental states or events, including desire, belief, intention of something in particular, but rather is considered a matter of spontaneity and creativity. The exercise of intent in such intentional actions is not that which determines their freedom – intentional actions are rather self-generating. The "actish feel" of some intentional actions do not "constitute that event's activeness, or the agent's exercise of active control", rather they "might be brought about by direct stimulation of someone's brain, in the absence of any relevant desire or intention on the part of that person".[59] Another question raised by such non-causal theory, is how an agent acts upon reason, if the said intentional actions are spontaneous.
Some non-causal explanations involve invoking panpsychism, the theory that a quality of mind is associated with all particles, and pervades the entire universe, in both animate and inanimate entities.
Event-causal accounts of incompatibilist free will typically rely upon physicalist models of mind (like those of the compatibilist), yet they presuppose physical indeterminism, in which certain indeterministic events are said to be caused by the agent. A number of event-causal accounts of free will have been created, referenced here as deliberative indeterminism, centred accounts, and efforts of will theory.[59] The first two accounts do not require free will to be a fundamental constituent of the universe. Ordinary randomness is appealed to as supplying the "elbow room" that libertarians believe necessary. A first common objection to event-causal accounts is that the indeterminism could be destructive and could therefore diminish control by the agent rather than provide it (related to the problem of origination). A second common objection to these models is that it is questionable whether such indeterminism could add any value to deliberation over that which is already present in a deterministic world.
Deliberative indeterminism asserts that the indeterminism is confined to an earlier stage in the decision process.[65][66] This is intended to provide an indeterminate set of possibilities to choose from, while not risking the introduction of luck (random decision making). The selection process is deterministic, although it may be based on earlier preferences established by the same process. Deliberative indeterminism has been referenced by Daniel Dennett[67] and John Martin Fischer.[68] An obvious objection to such a view is that an agent cannot be assigned ownership over their decisions (or preferences used to make those decisions) to any greater degree than that of a compatibilist model.
Centred accounts propose that for any given decision between two possibilities, the strength of reason will be considered for each option, yet there is still a probability the weaker candidate will be chosen.[60][69][70][71][72][73][74] An obvious objection to such a view is that decisions are explicitly left up to chance, and origination or responsibility cannot be assigned for any given decision.
Efforts of will theory is related to the role of will power in decision making. It suggests that the indeterminacy of agent volition processes could map to the indeterminacy of certain physical events – and the outcomes of these events could therefore be considered caused by the agent. Models of volition have been constructed in which it is seen as a particular kind of complex, high-level process with an element of physical indeterminism. An example of this approach is that of Robert Kane, where he hypothesizes that "in each case, the indeterminism is functioning as a hindrance or obstacle to her realizing one of her purposes – a hindrance or obstacle in the form of resistance within her will which must be overcome by effort."[29] According to Robert Kane such "ultimate responsibility" is a required condition for free will.[75] An important factor in such a theory is that the agent cannot be reduced to physical neuronal events, but rather mental processes are said to provide an equally valid account of the determination of outcome as their physical processes (see non-reductive physicalism).
Although at the time quantum mechanics (and physical indeterminism) was only in the initial stages of acceptance, in his book Miracles: A preliminary study C.S. Lewis stated the logical possibility that if the physical world were proved indeterministic this would provide an entry point to describe an action of a non-physical entity on physical reality.[76] Indeterministic physical models (particularly those involving quantum indeterminacy) introduce random occurrences at an atomic or subatomic level. These events might affect brain activity, and could seemingly allow incompatibilist free will if the apparent indeterminacy of some mental processes (for instance, subjective perceptions of control in conscious volition) map to the underlying indeterminacy of the physical construct. This relationship, however, requires a causative role over probabilities that is questionable,[77] and it is far from established that brain activity responsible for human action can be affected by such events. Secondarily, these incompatibilist models are dependent upon the relationship between action and conscious volition, as studied in the neuroscience of free will. It is evident that observation may disturb the outcome of the observation itself, rendering limited our ability to identify causality.[48] Niels Bohr, one of the main architects of quantum theory, suggested, however, that no connection could be made between indeterminism of nature and freedom of will.[49]
Agent/substance-causal accounts of incompatibilist free will rely upon substance dualism in their description of mind. The agent is assumed power to intervene in the physical world.[78][79][80][81][82][83][84][85]
Agent (substance)-causal accounts have been suggested by both George Berkeley[86] and Thomas Reid.[87] It is required that what the agent causes is not causally determined by prior events. It is also required that the agent's causing of that event is not causally determined by prior events. A number of problems have been identified with this view. Firstly, it is difficult to establish the reason for any given choice by the agent, which suggests they may be random or determined by luck (without an underlying basis for the free will decision). Secondly, it has been questioned whether physical events can be caused by an external substance or mind – a common problem associated with interactionalist dualism.
Hard incompatibilism is the idea that free will cannot exist, whether the world is deterministic or not. Derk Pereboom has defended hard incompatibilism, identifying a variety of positions where free will is irrelevant to indeterminism/determinism, among them the following:
Pereboom calls positions 3 and 4 soft determinism, position 1 a form of hard determinism, position 6 a form of classical libertarianism, and any position that includes having F as compatibilism.
John Locke denied that the phrase "free will" made any sense (compare with theological noncognitivism, a similar stance on the existence of God). He also took the view that the truth of determinism was irrelevant. He believed that the defining feature of voluntary behavior was that individuals have the ability to postpone a decision long enough to reflect or deliberate upon the consequences of a choice: "...the will in truth, signifies nothing but a power, or ability, to prefer or choose".[88]
The contemporary philosopher Galen Strawson agrees with Locke that the truth or falsity of determinism is irrelevant to the
problem.[89] He argues that the notion of free will leads to an infinite regress and is therefore senseless.
According to Strawson, if one is responsible for what one does in a given situation, then one must be responsible for the way one is in certain mental respects. But it is impossible for one to be responsible for the way one is in any respect. This is because to be responsible in some situation S, one must have been responsible for the way one was at S−1. To be responsible for the way one was at S−1, one must have been responsible for the way one was at S−2, and so on. At some point in the chain, there must have been an act of origination of a new causal chain. But this is impossible. Man cannot create himself or his mental states ex nihilo. This argument entails that free will itself is absurd, but not that it is incompatible with determinism. Strawson calls his own view "pessimism" but it can be classified as hard incompatibilism.[89]
Causal determinism is the concept that events within a given paradigm are bound by causality in such a way that any state (of an object or event) is completely determined by prior states. Causal determinism proposes that there is an unbroken chain of prior occurrences stretching back to the origin of the universe. Causal determinists believe that there is nothing uncaused or self-caused. The most common form of causal determinism is nomological determinism (or scientific determinism), the notion that the past and the present dictate the future entirely and necessarily by rigid natural laws, that every occurrence results inevitably from prior events. Quantum mechanics poses a serious challenge to this view.
Fundamental debate continues over whether the physical universe is likely to be deterministic. Although the scientific method cannot be used to rule out indeterminism with respect to violations of causal closure, it can be used to identify indeterminism in natural law. Interpretations of quantum mechanics at present are both deterministic and indeterministic, and are being constrained by ongoing experimentation.[90]
Destiny or fate is a predetermined course of events. It may be conceived as a predetermined future, whether in general or of an individual. It is a concept based on the belief that there is a fixed natural order to the cosmos.
Although often used interchangeably, the words "fate" and "destiny" have distinct connotations.
Fate generally implies there is a set course that cannot be deviated from, and over which one has no control. Fate is related to determinism, but makes no specific claim of physical determinism. Even with physical indeterminism an event could still be fated externally (see for instance theological determinism). Destiny likewise is related to determinism, but makes no specific claim of physical determinism. Even with physical indeterminism an event could still be destined to occur.
Destiny implies there is a set course that cannot be deviated from, but does not of itself make any claim with respect to the setting of that course (i.e., it does not necessarily conflict with incompatibilist free will). Free will if existent could be the mechanism by which that destined outcome is chosen (determined to represent destiny).[91]
Discussion regarding destiny does not necessitate the existence of supernatural powers. Logical determinism or determinateness is the notion that all propositions, whether about the past, present, or future, are either true or false. This creates a unique problem for free will given that propositions about the future already have a truth value in the present (that is it is already determined as either true or false), and is referred to as the problem of future contingents.
Omniscience is the capacity to know everything that there is to know (included in which are all future events), and is a property often attributed to a creator deity. Omniscience implies the existence of destiny. Some authors have claimed that free will cannot coexist with omniscience. One argument asserts that an omniscient creator not only implies destiny but a form of high level predeterminism such as hard theological determinism or predestination – that they have independently fixed all events and outcomes in the universe in advance. In such a case, even if an individual could have influence over their lower level physical system, their choices in regard to this cannot be their own, as is the case with libertarian free will. Omniscience features as an incompatible-properties argument for the existence of God, known as the argument from free will, and is closely related to other such arguments, for example the incompatibility of omnipotence with a good creator deity (i.e. if a deity knew what they were going to choose, then they are responsible for letting them choose it).
Predeterminism is the idea that all events are determined in advance.[92][93] Predeterminism is the philosophy that all events of history, past, present and future, have been decided or are known (by God, fate, or some other force), including human actions. Predeterminism is frequently taken to mean that human actions cannot interfere with (or have no bearing on) the outcomes of a pre-determined course of events, and that one's destiny was established externally (for example, exclusively by a creator deity). The concept of predeterminism is often argued by invoking causal determinism, implying that there is an unbroken chain of prior occurrences stretching back to the origin of the universe. In the case of predeterminism, this chain of events has been pre-established, and human actions cannot interfere with the outcomes of this pre-established chain. Predeterminism can be used to mean such pre-established causal determinism, in which case it is categorised as a specific type of determinism.[92][94] It can also be used interchangeably with causal determinism – in the context of its capacity to determine future events.[92][95] Despite this, predeterminism is often considered as independent of causal determinism.[96][97] The term predeterminism is also frequently used in the context of biology and heredity, in which case it represents a form of biological determinism.[98]
The term predeterminism suggests not just a determining of all events, but the prior and deliberately conscious determining of all events (therefore done, presumably, by a conscious being). While determinism usually refers to a naturalistically explainable causality of events, predeterminism seems by definition to suggest a person or a "someone" who is controlling or planning the causality of events before they occur and who then perhaps resides beyond the natural, causal universe. Predestination asserts that a supremely powerful being has indeed fixed all events and outcomes in the universe in advance, and is a famous doctrine of the Calvinists in Christian theology. Predestination is often considered a form of hard theological determinism.
Predeterminism has therefore been compared to fatalism.[99] Fatalism is the idea that everything is fated to happen, so that humans have no control over their future.
Theological determinism is a form of determinism stating that all events that happen are pre-ordained, or predestined to happen, by a monotheistic deity, or that they are destined to occur given its omniscience. Two forms of theological determinism exist, here referenced as strong and weak theological determinism.[100]
There exist slight variations on the above categorisation. Some claim that theological determinism requires predestination of all events and outcomes by the divinity (that is, they do not classify the weaker version as 'theological determinism' unless libertarian free will is assumed to be denied as a consequence), or that the weaker version does not constitute 'theological determinism' at all.[53] Theological determinism can also be seen as a form of causal determinism, in which the antecedent conditions are the nature and will of God.[54] With respect to free will and the classification of theological compatibilism/incompatibilism below, "theological determinism is the thesis that God exists and has infallible knowledge of all true propositions including propositions about our future actions," more minimal criteria designed to encapsulate all forms of theological determinism.[30]
There are various implications for metaphysical libertarian free will as consequent of theological determinism and its philosophical interpretation.
The basic argument for theological fatalism in the case of weak theological determinism is as follows:
This argument is very often accepted as a basis for theological incompatibilism: denying either libertarian free will or divine foreknowledge (omniscience) and therefore theological determinism. On the other hand, theological compatibilism must attempt to find problems with it. The formal version of the argument rests on a number of premises, many of which have received some degree of contention. Theological compatibilist responses have included:
In the definition of compatibilism and incompatibilism, the literature often fails to distinguish between physical determinism and higher level forms of determinism (predeterminism, theological determinism, etc.) As such, hard determinism with respect to theological determinism (or "Hard Theological Determinism" above) might be classified as hard incompatibilism with respect to physical determinism (if no claim was made regarding the internal causality or determinism of the universe), or even compatibilism (if freedom from the constraint of determinism was not considered necessary for free will), if not hard determinism itself. By the same principle, metaphysical libertarianism (a form of incompatibilism with respect to physical determinism) might be classified as compatibilism with respect to theological determinism (if it was assumed such free will events were pre-ordained and therefore were destined to occur, but of which whose outcomes were not "predestined" or determined by God). If hard theological determinism is accepted (if it was assumed instead that such outcomes were predestined by God), then metaphysical libertarianism is not, however, possible, and would require reclassification (as hard incompatibilism for example, given that the universe is still assumed to be indeterministic – although the classification of hard determinism is technically valid also).[53]
The idea of free will is one aspect of the mind–body problem, that is, consideration of the relation between mind (for example, consciousness, memory, and judgment) and body (for example, the human brain and nervous system). Philosophical models of mind are divided into physical and non-physical expositions.
Cartesian dualism holds that the mind is a nonphysical substance, the seat of consciousness and intelligence, and is not identical with physical states of the brain or body. It is suggested that although the two worlds do interact, each retains some measure of autonomy. Under cartesian dualism external mind is responsible for bodily action, although unconscious brain activity is often caused by external events (for example, the instantaneous reaction to being burned).[107] Cartesian dualism implies that the physical world is not deterministic – and in which external mind controls (at least some) physical events, providing an interpretation of incompatibilist free will. Stemming from Cartesian dualism, a formulation sometimes called interactionalist dualism suggests a two-way interaction, that some physical events cause some mental acts and some mental acts cause some physical events. One modern vision of the possible separation of mind and body is the "three-world" formulation of Popper.[108] Cartesian dualism and Popper's three worlds are two forms of what is called epistemological pluralism, that is the notion that different epistemological methodologies are necessary to attain a full description of the world. Other forms of epistemological pluralist dualism include psychophysical parallelism and epiphenomenalism. Epistemological pluralism is one view in which the mind-body problem is not reducible to the concepts of the natural sciences.
A contrasting approach is called physicalism. Physicalism is a philosophical theory holding that everything that exists is no more extensive than its physical properties; that is, that there are no non-physical substances (for example physically independent minds). Physicalism can be reductive or non-reductive. Reductive physicalism is grounded in the idea that everything in the world can actually be reduced analytically to its fundamental physical, or material, basis. Alternatively, non-reductive physicalism asserts that mental properties form a separate ontological class to physical properties: that mental states (such as qualia) are not ontologically reducible to physical states. Although one might suppose that mental states and neurological states are different in kind, that does not rule out the possibility that mental states are correlated with neurological states. In one such construction, anomalous monism, mental events supervene on physical events, describing the emergence of mental properties correlated with physical properties – implying causal reducibility. Non-reductive physicalism is therefore often categorised as property dualism rather than monism, yet other types of property dualism do not adhere to the causal reducibility of mental states (see epiphenomenalism).
Incompatibilism requires a distinction between the mental and the physical, being a commentary on the incompatibility of (determined) physical reality and one's presumably distinct experience of will. Secondarily, metaphysical libertarian free will must assert influence on physical reality, and where mind is responsible for such influence (as opposed to ordinary system randomness), it must be distinct from body to accomplish this. Both substance and property dualism offer such a distinction, and those particular models thereof that are not causally inert with respect to the physical world provide a basis for illustrating incompatibilist free will (i.e. interactionalist dualism and non-reductive physicalism).
It has been noted that the laws of physics have yet to resolve the hard problem of consciousness:[109] "Solving the hard problem of consciousness involves determining how physiological processes such as ions flowing across the nerve membrane cause us to have experiences."[110] According to some, "Intricately related to the hard problem of consciousness, the hard problem of free will represents the core problem of conscious free will: Does conscious volition impact the material world?"[15] Others however argue that "consciousness plays a far smaller role in human life than Western culture has tended to believe."[111]
Compatibilists maintain that determinism is compatible with free will. They believe freedom can be present or absent in a situation for reasons that have nothing to do with metaphysics. For instance, courts of law make judgments about whether individuals are acting under their own free will under certain circumstances without bringing in metaphysics. Similarly, political liberty is a non-metaphysical concept.[citation needed] Likewise, some compatibilists define free will as freedom to act according to one's determined motives without hindrance from other individuals. So for example Aristotle in his Nicomachean Ethics,[112] and the Stoic Chrysippus.[113]
In contrast, the incompatibilist positions are concerned with a sort of "metaphysically free will", which compatibilists claim has never been coherently defined. Compatibilists argue that determinism does not matter; though they disagree among themselves about what, in turn, does matter. To be a compatibilist, one need not endorse any particular conception of free will, but only deny that determinism is at odds with free will.[114]
Although there are various impediments to exercising one's choices, free will does not imply freedom of action. Freedom of choice (freedom to select one's will) is logically separate from freedom to implement that choice (freedom to enact one's will), although not all writers observe this distinction.[24] Nonetheless, some philosophers have defined free will as the absence of various impediments. Some "modern compatibilists", such as Harry Frankfurt and Daniel Dennett, argue free will is simply freely choosing to do what constraints allow one to do. In other words, a coerced agent's choices can still be free if such coercion coincides with the agent's personal intentions and desires.[35][115]
Most "classical compatibilists", such as Thomas Hobbes, claim that a person is acting on the person's own will only when it is the desire of that person to do the act, and also possible for the person to be able to do otherwise, if the person had decided to. Hobbes sometimes attributes such compatibilist freedom to each individual and not to some abstract notion of will, asserting, for example, that "no liberty can be inferred to the will, desire, or inclination, but the liberty of the man; which consisteth in this, that he finds no stop, in doing what he has the will, desire, or inclination to doe [sic]."[116] In articulating this crucial proviso, David Hume writes, "this hypothetical liberty is universally allowed to belong to every one who is not a prisoner and in chains."[117] Similarly, Voltaire, in his Dictionnaire philosophique, claimed that "Liberty then is only and can be only the power to do what one will." He asked, "would you have everything at the pleasure of a million blind caprices?" For him, free will or liberty is "only the power of acting, what is this power? It is the effect of the constitution and present state of our organs."
Compatibilism often regards the agent free as virtue of their reason. Some explanations of free will focus on the internal causality of the mind with respect to higher-order brain processing – the interaction between conscious and unconscious brain activity.[118] Likewise, some modern compatibilists in psychology have tried to revive traditionally accepted struggles of free will with the formation of character.[119] Compatibilist free will has also been attributed to our natural sense of agency, where one must believe they are an agent in order to function and develop a theory of mind.[120][121]
The notion of levels of decision is presented in a different manner by Frankfurt.[115] Frankfurt argues for a version of compatibilism called the "hierarchical mesh". The idea is that an individual can have conflicting desires at a first-order level and also have a desire about the various first-order desires (a second-order desire) to the effect that one of the desires prevails over the others. A person's will is identified with their effective first-order desire, that is, the one they act on, and this will is free if it was the desire the person wanted to act upon, that is, the person's second-order desire was effective. So, for example, there are "wanton addicts", "unwilling addicts" and "willing addicts". All three groups may have the conflicting first-order desires to want to take the drug they are addicted to and to not want to take it.
The first group, wanton addicts, have no second-order desire not to take the drug. The second group, "unwilling addicts", have a second-order desire not to take the drug, while the third group, "willing addicts", have a second-order desire to take it. According to Frankfurt, the members of the first group are devoid of will and therefore are no longer persons. The members of the second group freely desire not to take the drug, but their will is overcome by the addiction. Finally, the members of the third group willingly take the drug they are addicted to. Frankfurt's theory can ramify to any number of levels. Critics of the theory point out that there is no certainty that conflicts will not arise even at the higher-order levels of desire and preference.[122] Others argue that Frankfurt offers no adequate explanation of how the various levels in the hierarchy mesh together.[123]
In Elbow Room, Dennett presents an argument for a compatibilist theory of free will, which he further elaborated in the book Freedom Evolves.[124] The basic reasoning is that, if one excludes God, an infinitely powerful demon, and other such possibilities, then because of chaos and epistemic limits on the precision of our knowledge of the current state of the world, the future is ill-defined for all finite beings. The only well-defined things are "expectations". The ability to do "otherwise" only makes sense when dealing with these expectations, and not with some unknown and unknowable future.
According to Dennett, because individuals have the ability to act differently from what anyone expects, free will can exist.[124] Incompatibilists claim the problem with this idea is that we may be mere "automata responding in predictable ways to stimuli in our environment". Therefore, all of our actions are controlled by forces outside ourselves, or by random chance.[125] More sophisticated analyses of compatibilist free will have been offered, as have other critiques.[114]
In the philosophy of decision theory, a fundamental question is: From the standpoint of statistical outcomes, to what extent do the choices of a conscious being have the ability to influence the future? Newcomb's paradox and other philosophical problems pose questions about free will and predictable outcomes of choices.
Compatibilist models of free will often consider deterministic relationships as discoverable in the physical world (including the brain). Cognitive naturalism[126] is a physicalist approach to studying human cognition and consciousness in which the mind is simply part of nature, perhaps merely a feature of many very complex self-programming feedback systems (for example, neural networks and cognitive robots), and so must be studied by the methods of empirical science, such as the behavioral and cognitive sciences (i.e. neuroscience and cognitive psychology).[107][127] Cognitive naturalism stresses the role of neurological sciences. Overall brain health, substance dependence, depression, and various personality disorders clearly influence mental activity, and their impact upon volition is also important.[118] For example, an addict may experience a conscious desire to escape addiction, but be unable to do so. The "will" is disconnected from the freedom to act. This situation is related to an abnormal production and distribution of dopamine in the brain.[128] The neuroscience of free will places restrictions on both compatibilist and incompatibilist free will conceptions.
Compatibilist models adhere to models of mind in which mental activity (such as deliberation) can be reduced to physical activity without any change in physical outcome. Although compatibilism is generally aligned to (or is at least compatible with) physicalism, some compatibilist models describe the natural occurrences of deterministic deliberation in the brain in terms of the first person perspective of the conscious agent performing the deliberation.[15] Such an approach has been considered a form of identity dualism. A description of "how conscious experience might affect brains" has been provided in which "the experience of conscious free will is the first-person perspective of the neural correlates of choosing."[15]
Recently,[when?] Claudio Costa developed a neocompatibilist theory based on the causal theory of action that is complementary to classical compatibilism. According to him, physical, psychological and rational restrictions can interfere at different levels of the causal chain that would naturally lead to action. Correspondingly, there can be physical restrictions to the body, psychological restrictions to the decision, and rational restrictions to the formation of reasons (desires plus beliefs) that should lead to what we would call a reasonable action. The last two are usually called "restrictions of free will". The restriction at the level of reasons is particularly important since it can be motivated by external reasons that are insufficiently conscious to the agent. One example was the collective suicide led by Jim Jones. The suicidal agents were not conscious that their free will have been manipulated by external, even if ungrounded, reasons.[129]
Alternatives to strictly naturalist physics, such as mind–body dualism positing a mind or soul existing apart from one's body while perceiving, thinking, choosing freely, and as a result acting independently on the body, include both traditional religious metaphysics and less common newer compatibilist concepts.[130] Also consistent with both autonomy and Darwinism,[131] they allow for free personal agency based on practical reasons within the laws of physics.[132] While less popular among 21st-century philosophers, non-naturalist compatibilism is present in most if not almost all religions.[133]
Some philosophers' views are difficult to categorize as either compatibilist or incompatibilist, hard determinist or libertarian. For example, Ted Honderich holds the view that "determinism is true, compatibilism and incompatibilism are both false" and the real problem lies elsewhere. Honderich maintains that determinism is true because quantum phenomena are not events or things that can be located in space and time, but are abstract entities. Further, even if they were micro-level events, they do not seem to have any relevance to how the world is at the macroscopic level. He maintains that incompatibilism is false because, even if indeterminism is true, incompatibilists have not provided, and cannot provide, an adequate account of origination. He rejects compatibilism because it, like incompatibilism, assumes a single, fundamental notion of freedom. There are really two notions of freedom: voluntary action and origination. Both notions are required to explain freedom of will and responsibility. Both determinism and indeterminism are threats to such freedom. To abandon these notions of freedom would be to abandon moral responsibility. On the one side, we have our intuitions; on the other, the scientific facts. The "new" problem is how to resolve this conflict.[134]
David Hume discussed the possibility that the entire debate about free will is nothing more than a merely "verbal" issue. He suggested that it might be accounted for by "a false sensation or seeming experience" (a velleity), which is associated with many of our actions when we perform them. On reflection, we realize that they were necessary and determined all along.[136]
According to Arthur Schopenhauer, the actions of humans, as phenomena, are subject to the principle of sufficient reason and thus liable to necessity. Thus, he argues, humans do not possess free will as conventionally understood. However, the will [urging, craving, striving, wanting, and desiring], as the noumenon underlying the phenomenal world, is in itself groundless: that is, not subject to time, space, and causality (the forms that governs the world of appearance). Thus, the will, in itself and outside of appearance, is free. Schopenhauer discussed the puzzle of free will and moral responsibility in The World as Will and Representation, Book 2, Sec. 23:
But the fact is overlooked that the individual, the person, is not will as thing-in-itself, but is phenomenon of the will, is as such determined, and has entered the form of the phenomenon, the principle of sufficient reason. Hence we get the strange fact that everyone considers himself to be a priori quite free, even in his individual actions, and imagines he can at any moment enter upon a different way of life... But a posteriori through experience, he finds to his astonishment that he is not free, but liable to necessity; that notwithstanding all his resolutions and reflections he does not change his conduct, and that from the beginning to the end of his life he must bear the same character that he himself condemns, and, as it were, must play to the end the part he has taken upon himself.[137]Schopenhauer elaborated on the topic in Book IV of the same work and in even greater depth in his later essay On the Freedom of the Will. In this work, he stated, "You can do what you will, but in any given moment of your life you can will only one definite thing and absolutely nothing other than that one thing."[138]
In his book Free Will, philosopher and neuroscientist Sam Harris argues that free will is an illusion, stating that "thoughts and intentions emerge from background causes of which we are unaware and over which we exert no conscious control."[139]
Rudolf Steiner, who collaborated in a complete edition of Arthur Schopenhauer's work,[140] wrote The Philosophy of Freedom, which focuses on the problem of free will. Steiner (1861–1925) initially divides this into the two aspects of freedom: freedom of thought and freedom of action. The controllable and uncontrollable aspects of decision making thereby are made logically separable, as pointed out in the introduction. This separation of will from action has a very long history, going back at least as far as Stoicism and the teachings of Chrysippus (279–206 BCE), who separated external antecedent causes from the internal disposition receiving this cause.[141]
Steiner then argues that inner freedom is achieved when we integrate our sensory impressions, which reflect the outer appearance of the world, with our thoughts, which lend coherence to these impressions and thereby disclose to us an understandable world. Acknowledging the many influences on our choices, he nevertheless points out that they do not preclude freedom unless we fail to recognise them. Steiner argues that outer freedom is attained by permeating our deeds with moral imagination. "Moral" in this case refers to action that is willed, while "imagination" refers to the mental capacity to envision conditions that do not already hold. Both of these functions are necessarily conditions for freedom. Steiner aims to show that these two aspects of inner and outer freedom are integral to one another, and that true freedom is only achieved when they are united.[142]
William James' views were ambivalent. While he believed in free will on "ethical grounds", he did not believe that there was evidence for it on scientific grounds, nor did his own introspections support it.[143] Ultimately he believed that the problem of free will was a metaphysical issue and, therefore, could not be settled by science. Moreover, he did not accept incompatibilism as formulated below; he did not believe that the indeterminism of human actions was a prerequisite of moral responsibility. In his work Pragmatism, he wrote that "instinct and utility between them can safely be trusted to carry on the social business of punishment and praise" regardless of metaphysical theories.[144] He did believe that indeterminism is important as a "doctrine of relief" – it allows for the view that, although the world may be in many respects a bad place, it may, through individuals' actions, become a better one. Determinism, he argued, undermines meliorism – the idea that progress is a real concept leading to improvement in the world.[144]
In 1739, David Hume in his A Treatise of Human Nature approached free will via the notion of causality. It was his position that causality was a mental construct used to explain the repeated association of events, and that one must examine more closely the relation between things regularly succeeding one another (descriptions of regularity in nature) and things that result in other things (things that cause or necessitate other things).[145] According to Hume, 'causation' is on weak grounds: "Once we realise that 'A must bring about B' is tantamount merely to 'Due to their constant conjunction, we are psychologically certain that B will follow A,' then we are left with a very weak notion of necessity."[146]
This empiricist view was often denied by trying to prove the so-called apriority of causal law (i.e. that it precedes all experience and is rooted in the construction of the perceivable world):
In the 1780s Immanuel Kant suggested at a minimum our decision processes with moral implications lie outside the reach of everyday causality, and lie outside the rules governing material objects.[149] "There is a sharp difference between moral judgments and judgments of fact... Moral judgments... must be a priori judgments."[150]
Freeman introduces what he calls "circular causality" to "allow for the contribution of self-organizing dynamics", the "formation of macroscopic population dynamics that shapes the patterns of activity of the contributing individuals", applicable to "interactions between neurons and neural masses... and between the behaving animal and its environment".[151] In this view, mind and neurological functions are tightly coupled in a situation where feedback between collective actions (mind) and individual subsystems (for example, neurons and their synapses) jointly decide upon the behaviour of both.
Thirteenth century philosopher Thomas Aquinas viewed humans as pre-programmed (by virtue of being human) to seek certain goals, but able to choose between routes to achieve these goals (our Aristotelian telos). His view has been associated with both compatibilism and libertarianism.[152][153]
In facing choices, he argued that humans are governed by intellect, will, and passions. The will is "the primary mover of all the powers of the soul... and it is also the efficient cause of motion in the body."[154] Choice falls into five stages: (i) intellectual consideration of whether an objective is desirable, (ii) intellectual consideration of means of attaining the objective, (iii) will arrives at an intent to pursue the objective, (iv) will and intellect jointly decide upon choice of means (v) will elects execution.[155] Free will enters as follows: Free will is an "appetitive power", that is, not a cognitive power of intellect (the term "appetite" from Aquinas's definition "includes all forms of internal inclination").[156] He states that judgment "concludes and terminates counsel. Now counsel is terminated, first, by the judgment of reason; secondly, by the acceptation of the appetite [that is, the free-will]."[157]
A compatibilist interpretation of Aquinas's view is defended thus: "Free-will is the cause of its own movement, because by his free-will man moves himself to act. But it does not of necessity belong to liberty that what is free should be the first cause of itself, as neither for one thing to be cause of another need it be the first cause. God, therefore, is the first cause, Who moves causes both natural and voluntary. And just as by moving natural causes He does not prevent their acts being natural, so by moving voluntary causes He does not deprive their actions of being voluntary: but rather is He the cause of this very thing in them; for He operates in each thing according to its own nature."[158][159]
Historically, most of the philosophical effort invested in resolving the dilemma has taken the form of close examination of definitions and ambiguities in the concepts designated by "free", "freedom", "will", "choice" and so forth. Defining 'free will' often revolves around the meaning of phrases like "ability to do otherwise" or "alternative possibilities". This emphasis upon words has led some philosophers to claim the problem is merely verbal and thus a pseudo-problem.[160] In response, others point out the complexity of decision making and the importance of nuances in the terminology.[citation needed]
Buddhism accepts both freedom and determinism (or something similar to it), but in spite of its focus towards the human agency, rejects the western concept of a total agent from external sources.[161] According to the Buddha, "There is free action, there is retribution, but I see no agent that passes out from one set of momentary elements into another one, except the [connection] of those elements."[161] Buddhists believe in neither absolute free will, nor determinism. It preaches a middle doctrine, named pratītyasamutpāda in Sanskrit, often translated as "dependent origination", "dependent arising" or "conditioned genesis". It teaches that every volition is a conditioned action as a result of ignorance. In part, it states that free will is inherently conditioned and not "free" to begin with. It is also part of the theory of karma in Buddhism. The concept of karma in Buddhism is different from the notion of karma in Hinduism. In Buddhism, the idea of karma is much less deterministic. The Buddhist notion of karma is primarily focused on the cause and effect of moral actions in this life, while in Hinduism the concept of karma is more often connected with determining one's destiny in future lives.
In Buddhism it is taught that the idea of absolute freedom of choice (that is that any human being could be completely free to make any choice) is unwise, because it denies the reality of one's physical needs and circumstances. Equally incorrect is the idea that humans have no choice in life or that their lives are pre-determined. To deny freedom would be to deny the efforts of Buddhists to make moral progress (through our capacity to freely choose compassionate action). Pubbekatahetuvada, the belief that all happiness and suffering arise from previous actions, is considered a wrong view according to Buddhist doctrines. Because Buddhists also reject agenthood, the traditional compatibilist strategies are closed to them as well. Instead, the Buddhist philosophical strategy is to examine the metaphysics of causality. Ancient India had many heated arguments about the nature of causality with Jains, Nyayists, Samkhyists, Cārvākans, and Buddhists all taking slightly different lines. In many ways, the Buddhist position is closer to a theory of "conditionality" (idappaccayatā) than a theory of "causality", especially as it is expounded by Nagarjuna in the Mūlamadhyamakakārikā.[161]
The six orthodox (astika) schools of thought in Hindu philosophy do not agree with each other entirely on the question of free will. For the Samkhya, for instance, matter is without any freedom, and soul lacks any ability to control the unfolding of matter. The only real freedom (kaivalya) consists in realizing the ultimate separateness of matter and self.[162] For the Yoga school, only Ishvara is truly free, and its freedom is also distinct from all feelings, thoughts, actions, or wills, and is thus not at all a freedom of will. The metaphysics of the Nyaya and Vaisheshika schools strongly suggest a belief in determinism, but do not seem to make explicit claims about determinism or free will.[163]
A quotation from Swami Vivekananda, a Vedantist, offers a good example of the worry about free will in the Hindu tradition.
Therefore we see at once that there cannot be any such thing as free-will; the very words are a contradiction, because will is what we know, and everything that we know is within our universe, and everything within our universe is moulded by conditions of time, space and causality. ... To acquire freedom we have to get beyond the limitations of this universe; it cannot be found here.[164]However, the preceding quote has often been misinterpreted as Vivekananda implying that everything is predetermined. What Vivekananda actually meant by lack of free will was that the will was not "free" because it was heavily influenced by the law of cause and effect – "The will is not free, it is a phenomenon bound by cause and effect, but there is something behind the will which is free."[164] Vivekananda never said things were absolutely determined and placed emphasis on the power of conscious choice to alter one's past karma: "It is the coward and the fool who says this is his fate. But it is the strong man who stands up and says I will make my own fate."[164]
Science has contributed to the free will problem in at least three ways. First, physics has addressed the question of whether nature is deterministic, which is viewed as crucial by incompatibilists (compatibilists, however, view it as irrelevant). Second, although free will can be defined in various ways, all of them involve aspects of the way people make decisions and initiate actions, which have been studied extensively by neuroscientists. Some of the experimental observations are widely viewed as implying that free will does not exist or is an illusion (but many philosophers see this as a misunderstanding). Third, psychologists have studied the beliefs that the majority of ordinary people hold about free will and its role in assigning moral responsibility.
From an anthropological perspective, free will can be regarded as an explanation for human behavior that justifies a socially sanctioned system of rewards and punishments.  Under this definition, free will may be described as a political ideology.  In a society where people are taught to believe that humans have free will, free will may be described as a political doctrine.
Early scientific thought often portrayed the universe as deterministic – for example in the thought of Democritus or the Cārvākans – and some thinkers claimed that the simple process of gathering sufficient information would allow them to predict future events with perfect accuracy. Modern science, on the other hand, is a mixture of deterministic and stochastic theories.[165] Quantum mechanics predicts events only in terms of probabilities, casting doubt on whether the universe is deterministic at all, although evolution of the universal state vector is completely deterministic. Current physical theories cannot resolve the question of whether determinism is true of the world, being very far from a potential theory of everything, and open to many different interpretations.[166][167]
Assuming that an indeterministic interpretation of quantum mechanics is correct, one may still object that such indeterminism is for all practical purposes confined to microscopic phenomena.[168] This is not always the case: many macroscopic phenomena are based on quantum effects. For instance, some hardware random number generators work by amplifying quantum effects into practically usable signals. A more significant question is whether the indeterminism of quantum mechanics allows for the traditional idea of free will (based on a perception of free will). If a person's action is, however, only a result of complete quantum randomness, mental processes as experienced have no influence on the probabilistic outcomes (such as volition).[29] According to many interpretations, non-determinism enables free will to exist,[169] while others assert the opposite (because the action was not controllable by the physical being who claims to possess the free will).[170]
Like physicists, biologists have frequently addressed questions related to free will. One of the most heated debates in biology is that of "nature versus nurture", concerning the relative importance of genetics and biology as compared to culture and environment in human behavior.[171] The view of many researchers is that many human behaviors can be explained in terms of humans' brains, genes, and evolutionary histories.[172][173][174] This point of view raises the fear that such attribution makes it impossible to hold others responsible for their actions. Steven Pinker's view is that fear of determinism in the context of "genetics" and "evolution" is a mistake, that it is "a confusion of explanation with exculpation". Responsibility does not require that behavior be uncaused, as long as behavior responds to praise and blame.[175] Moreover, it is not certain that environmental determination is any less threatening to free will than genetic determination.[176]
It has become possible to study the living brain, and researchers can now watch the brain's decision-making process at work. A seminal experiment in this field was conducted by Benjamin Libet in the 1980s, in which he asked each subject to choose a random moment to flick their wrist while he measured the associated activity in their brain; in particular, the build-up of electrical signal called the readiness potential (after German Bereitschaftspotential, which was discovered by Kornhuber & Deecke in 1965.[177]). Although it was well known that the readiness potential reliably preceded the physical action, Libet asked whether it could be recorded before the conscious intention to move. To determine when subjects felt the intention to move, he asked them to watch the second hand of a clock. After making a movement, the volunteer reported the time on the clock when they first felt the conscious intention to move; this became known as Libet's W time.[178]
Libet found that the unconscious brain activity of the readiness potential leading up to subjects' movements began approximately half a second before the subject was aware of a conscious intention to move.[178][179]
These studies of the timing between actions and the conscious decision bear upon the role of the brain in understanding free will. A subject's declaration of intention to move a finger appears after the brain has begun to implement the action, suggesting to some that unconsciously the brain has made the decision before the conscious mental act to do so. Some believe the implication is that free will was not involved in the decision and is an illusion. The first of these experiments reported the brain registered activity related to the move about 0.2 s before movement onset.[180] However, these authors also found that awareness of action was anticipatory to activity in the muscle underlying the movement; the entire process resulting in action involves more steps than just the onset of brain activity. The bearing of these results upon notions of free will appears complex.[181][182]
Some argue that placing the question of free will in the context of motor control is too narrow. The objection is that the time scales involved in motor control are very short, and motor control involves a great deal of unconscious action, with much physical movement entirely unconscious. On that basis "...free will cannot be squeezed into time frames of 150–350 ms; free will is a longer term phenomenon" and free will is a higher level activity that "cannot be captured in a description of neural activity or of muscle activation..."[183] The bearing of timing experiments upon free will is still under discussion.
More studies have since been conducted, including some that try to:
Benjamin Libet's results are quoted[184] in favor of epiphenomenalism, but he believes subjects still have a "conscious veto", since the readiness potential does not invariably lead to an action. In Freedom Evolves, Daniel Dennett argues that a no-free-will conclusion is based on dubious assumptions about the location of consciousness, as well as questioning the accuracy and interpretation of Libet's results. Kornhuber and Deecke underlined that absence of conscious will during the early Bereitschaftspotential (termed BP1) is not a proof of the non-existence of free will, as also unconscious agendas may be free and non-deterministic. According to their suggestion, man has relative freedom, i.e. freedom in degrees, that can be increased or decreased through deliberate choices that involve both conscious and unconscious (panencephalic) processes.[185]
Others have argued that data such as the Bereitschaftspotential undermine epiphenomenalism for the same reason, that such experiments rely on a subject reporting the point in time at which a conscious experience occurs, thus relying on the subject to be able to consciously perform an action. That ability would seem to be at odds with early epiphenomenalism, which according to Huxley is the broad claim that consciousness is "completely without any power... as the steam-whistle which accompanies the work of a locomotive engine is without influence upon its machinery".[186]
Adrian G. Guggisberg and Annaïs Mottaz have also challenged those findings.[187]
A study by Aaron Schurger and colleagues published in the Proceedings of the National Academy of Sciences[188] challenged assumptions about the causal nature of the readiness potential itself (and the "pre-movement buildup" of neural activity in general), casting doubt on conclusions drawn from studies such as Libet's[178] and Fried's.[189]
A study that compared deliberate and arbitrary decisions, found that the early signs of decision are absent for the deliberate ones.[190]
It has been shown that in several brain-related conditions, individuals cannot entirely control their own actions, though the existence of such conditions does not directly refute the existence of free will. Neuroscientific studies are valuable tools in developing models of how humans experience free will.
For example, people with Tourette syndrome and related tic disorders make involuntary movements and utterances (called tics) despite the fact that they would prefer not to do so when it is socially inappropriate. Tics are described as semi-voluntary or unvoluntary,[191] because they are not strictly involuntary: they may be experienced as a voluntary response to an unwanted, premonitory urge. Tics are experienced as irresistible and must eventually be expressed.[191] People with Tourette syndrome are sometimes able to suppress their tics for limited periods, but doing so often results in an explosion of tics afterward. The control exerted (from seconds to hours at a time) may merely postpone and exacerbate the ultimate expression of the tic.[192]
In alien hand syndrome, the affected individual's limb will produce unintentional movements without the will of the person. The affected limb effectively demonstrates 'a will of its own.' The sense of agency does not emerge in conjunction with the overt appearance of the purposeful act even though the sense of ownership in relationship to the body part is maintained. This phenomenon corresponds with an impairment in the premotor mechanism manifested temporally by the appearance of the readiness potential recordable on the scalp several hundred milliseconds before the overt appearance of a spontaneous willed movement. Using functional magnetic resonance imaging with specialized multivariate analyses to study the temporal dimension in the activation of the cortical network associated with voluntary movement in human subjects, an anterior-to-posterior sequential activation process beginning in the supplementary motor area on the medial surface of the frontal lobe and progressing to the primary motor cortex and then to parietal cortex has been observed.[193] The sense of agency thus appears to normally emerge in conjunction with this orderly sequential network activation incorporating premotor association cortices together with primary motor cortex. In particular, the supplementary motor complex on the medial surface of the frontal lobe appears to activate prior to primary motor cortex presumably in associated with a preparatory pre-movement process. In a recent study using functional magnetic resonance imaging, alien movements were characterized by a relatively isolated activation of the primary motor cortex contralateral to the alien hand, while voluntary movements of the same body part included the natural activation of motor association cortex associated with the premotor process.[194] The clinical definition requires "feeling that one limb is foreign or has a will of its own, together with observable involuntary motor activity" (emphasis in original).[195] This syndrome is often a result of damage to the corpus callosum, either when it is severed to treat intractable epilepsy or due to a stroke. The standard neurological explanation is that the felt will reported by the speaking left hemisphere does not correspond with the actions performed by the non-speaking right hemisphere, thus suggesting that the two hemispheres may have independent senses of will.[196][197]
In addition, one of the most important ("first rank") diagnostic symptoms of schizophrenia is the patient's delusion of being controlled by an external force.[198] People with schizophrenia will sometimes report that, although they are acting in the world, they do not recall initiating the particular actions they performed. This is sometimes likened to being a robot controlled by someone else. Although the neural mechanisms of schizophrenia are not yet clear, one influential hypothesis is that there is a breakdown in brain systems that compare motor commands with the feedback received from the body (known as proprioception), leading to attendant hallucinations and delusions of control.[199]
Experimental psychology's contributions to the free will debate have come primarily through social psychologist Daniel Wegner's work on conscious will. In his book, The Illusion of Conscious Will,[200] Wegner summarizes what he believes is empirical evidence supporting the view that human perception of conscious control is an illusion. Wegner summarizes some empirical evidence that may suggest that the perception of conscious control is open to modification (or even manipulation). Wegner observes that one event is inferred to have caused a second event when two requirements are met:
For example, if a person hears an explosion and sees a tree fall down that person is likely to infer that the explosion caused the tree to fall over. However, if the explosion occurs after the tree falls down (that is, the first requirement is not met), or rather than an explosion, the person hears the ring of a telephone (that is, the second requirement is not met), then that person is not likely to infer that either noise caused the tree to fall down.
Wegner has applied this principle to the inferences people make about their own conscious will. People typically experience a thought that is consistent with a behavior, and then they observe themselves performing this behavior. As a result, people infer that their thoughts must have caused the observed behavior. However, Wegner has been able to manipulate people's thoughts and behaviors so as to conform to or violate the two requirements for causal inference.[200][201] Through such work, Wegner has been able to show that people often experience conscious will over behaviors that they have not, in fact, caused – and conversely, that people can be led to experience a lack of will over behaviors they did cause. For instance, priming subjects with information about an effect increases the probability that a person falsely believes is the cause.[202] The implication for such work is that the perception of conscious will (which he says might be more accurately labelled as 'the emotion of authorship') is not tethered to the execution of actual behaviors, but is inferred from various cues through an intricate mental process, authorship processing. Although many interpret this work as a blow against the argument for free will, both psychologists[203][204] and philosophers[205][206] have criticized Wegner's theories.
Emily Pronin has argued that the subjective experience of free will is supported by the introspection illusion. This is the tendency for people to trust the reliability of their own introspections while distrusting the introspections of other people. The theory implies that people will more readily attribute free will to themselves rather than others. This prediction has been confirmed by three of Pronin and Kugler's experiments. When college students were asked about personal decisions in their own and their roommate's lives, they regarded their own choices as less predictable. Staff at a restaurant described their co-workers' lives as more determined (having fewer future possibilities) than their own lives. When weighing up the influence of different factors on behavior, students gave desires and intentions the strongest weight for their own behavior, but rated personality traits as most predictive of other people.[207]
Caveats have, however, been identified in studying a subject's awareness of mental events, in that the process of introspection itself may alter the experience.[208]
Regardless of the validity of belief in free will, it may be beneficial to understand where the idea comes from. One contribution is randomness.[209] While it is established that randomness is not the only factor in the perception of the free will, it has been shown that randomness can be mistaken as free will due to its indeterminacy. This misconception applies both when considering oneself and others. Another contribution is choice.[210] It has been demonstrated that people's belief in free will increases if presented with a simple level of choice. The specificity of the amount of choice is important, as too little or too great a degree of choice may negatively influence belief. It is also likely that the associative relationship between level of choice and perception of free will is influentially bidirectional. It is also possible that one's desire for control, or other basic motivational patterns, act as a third variable.
Other experiments have also been proposed to test free will. Ender Tosun argues for the reality of free will, based on combined experiments consisting of empirical and thought experiments. In the empirical part of these experiments, experimenter 2 is expected to predict which object experimenter 1 will touch. Experimenter 1 is always able to negate the prediction of experimenter 2. In the thought experiment part, Laplace's demon makes the predictions and experimenter 1 is never able to negate his predictions. Based on the non-correspondence of the predictions of experimenter 2 in the empirical experiment with the predictions of Laplace's demon, and contradictions in the possible layers of causality, Tosun concludes that free will is real. He also extends these experiments to indeterministic processes and real-time brain observations while willing, assuming that an agent has every technological means to probe and rewire his brain. In this thought experiment, experimenter 1 notices the "circuit" of his brain which disables him from willing one of the alternatives, then he probes other circuits to see if he can have the will to rewire that circuit. Experimenter 1 notices that all circuits of his brain being so as to prevent him from rewiring or bypassing the circuits which prevent him from willing to touch one of the objects is impossible.[citation needed]
Since at least 1959,[211] free will belief in individuals has been analysed with respect to traits in social behaviour. In general, the concept of free will researched to date in this context has been that of the incompatibilist, or more specifically, the libertarian, that is freedom from determinism.
Whether people naturally adhere to an incompatibilist model of free will has been questioned in the research. Eddy Nahmias has found that incompatibilism is not intuitive – it was not adhered to, in that determinism does not negate belief in moral responsibility (based on an empirical study of people's responses to moral dilemmas under a deterministic model of reality).[212] Edward Cokely has found that incompatibilism is intuitive – it was naturally adhered to, in that determinism does indeed negate belief in moral responsibility in general.[213] Joshua Knobe and Shaun Nichols have proposed that incompatibilism may or may not be intuitive, and that it is dependent to some large degree upon the circumstances; whether or not the crime incites an emotional response – for example if it involves harming another human being.[214] They found that belief in free will is a cultural universal, and that the majority of participants said that (a) our universe is indeterministic and (b) moral responsibility is not compatible with determinism.[215]
Studies indicate that peoples' belief in free will is inconsistent. Emily Pronin and Matthew Kugler found that people believe they have more free will than others.[216]
Studies also reveal a correlation between the likelihood of accepting a deterministic model of mind and personality type. For example, Adam Feltz and Edward Cokely found that people of an extrovert personality type are more likely to dissociate belief in determinism from belief in moral responsibility.[217]
Roy Baumeister and colleagues reviewed literature on the psychological effects of a belief (or disbelief) in free will and found that most people tend to believe in a sort of "naive compatibilistic free will".[218][219]
The researchers also found that people consider acts more "free" when they involve a person opposing external forces, planning, or making random actions.[220] Notably, the last behaviour, "random" actions, may not be possible; when participants attempt to perform tasks in a random manner (such as generating random numbers), their behaviour betrays many patterns.[221][222]
A recent 2020 survey has shown that compatibilism is quite a popular stance among those who specialize in philosophy (59.2%). Belief in libertarianism amounted to 18.8%, while a lack of belief in free will equaled 11.2%.[223]
79 percent of evolutionary biologists said that they believe in free will according to a survey conducted in 2007, only 14 percent chose no free will, and 7 percent did not answer the question.[224]
Baumeister and colleagues found that provoking disbelief in free will seems to cause various negative effects. The authors concluded, in their paper, that it is belief in determinism that causes those negative effects.[218] Kathleen Vohs has found that those whose belief in free will had been eroded were more likely to cheat.[225] In a study conducted by Roy Baumeister, after participants read an article arguing against free will, they were more likely to lie about their performance on a test where they would be rewarded with cash.[226] Provoking a rejection of free will has also been associated with increased aggression and less helpful behaviour.[226] However, although these initial studies suggested that believing in free will is associated with more morally praiseworthy behavior, more recent studies (including direct, multi-site replications) with substantially larger sample sizes have reported contradictory findings (typically, no association between belief in free will and moral behavior), casting doubt over the original findings.[227][228][229][230][231]
An alternative explanation builds on the idea that subjects tend to confuse determinism with fatalism... What happens then when agents' self-efficacy is undermined? It is not that their basic desires and drives are defeated. It is rather, I suggest, that they become skeptical that they can control those desires; and in the face of that skepticism, they fail to apply the effort that is needed even to try. If they were tempted to behave badly, then coming to believe in fatalism makes them less likely to resist that temptation.
—Richard Holton[232]
Moreover, whether or not these experimental findings are a result of actual manipulations in belief in free will is a matter of debate.[232] First of all, free will can at least refer to either libertarian (indeterministic) free will or compatibilistic (deterministic) free will. Having participants read articles that simply "disprove free will" is unlikely to increase their understanding of determinism, or the compatibilistic free will that it still permits.[232] In other words, experimental manipulations purporting to "provoke disbelief in free will" may instead cause a belief in fatalism, which may provide an alternative explanation for previous experimental findings.[232][233] To test the effects of belief in determinism, it has been argued that future studies would need to provide articles that do not simply "attack free will", but instead focus on explaining determinism and compatibilism.[232][234]
Baumeister and colleagues also note that volunteers disbelieving in free will are less capable of counterfactual thinking.[218] This is worrying because counterfactual thinking ("If I had done something different...") is an important part of learning from one's choices, including those that harmed others.[235] Again, this cannot be taken to mean that belief in determinism is to blame; these are the results we would expect from increasing people's belief in fatalism.[232]
Along similar lines, Tyler Stillman has found that belief in free will predicts better job performance.[236]
The notions of free will and predestination are heavily debated among Christians. Free will in the Christian sense is the ability to choose between good or evil. Among Catholics, there are those holding to Thomism, adopted from what Thomas Aquinas put forth in the Summa Theologica. There are also some holding to Molinism which was put forth by Jesuit priest Luis de Molina. Among Protestants there is Arminianism, held primarily by the Methodist Churches, and formulated by Dutch theologian Jacobus Arminius; and there is also Calvinism held by most in the Reformed tradition which was formulated by the French Reformed theologian, John Calvin. John Calvin was heavily influenced by Augustine of Hippo views on predestination put forth in his work On the Predestination of the Saints. Martin Luther seems to hold views on predestination similar to Calvinism in his On the Bondage of the Will, thus rejecting free will. In condemnation of Calvin and Luther views, the Roman Catholic Council of Trent declared that "the free will of man, moved and excited by God, can by its consent co-operate with God, Who excites and invites its action; and that it can thereby dispose and prepare itself to obtain the grace of justification. The will can resist grace if it chooses. It is not like a lifeless thing, which remains purely passive. Weakened and diminished by Adam's fall, free will is yet not destroyed in the race (Sess. VI, cap. i and v)." John Wesley, the father of the Methodist tradition, taught that humans, enabled by prevenient grace, have free will through which they can choose God and to do good works, with the goal of Christian perfection.[237] Upholding synergism (the belief that God and man cooperate in salvation), Methodism teaches that "Our Lord Jesus Christ did so die for all men as to make salvation attainable by every man that cometh into the world. If men are not saved that fault is entirely their own, lying solely in their own unwillingness to obtain the salvation offered to them. (John 1:9; I Thess. 5:9; Titus 2:11-12)."[238]
Paul the Apostle discusses Predestination in some of his Epistles.
"For whom He foreknew, He also predestined to become conformed to the image of His Son, that He might be the first-born among many brethren; and whom He predestined, these He also called; and whom He called, these He also justified; and whom He justified, these He also glorified." —Romans 8:29–30
"He predestined us to adoption as sons through Jesus Christ to Himself, according to the kind intention of His will." —Ephesians 1:5
There are also mentions of moral freedom in what are now termed as 'Deuterocanonical' works which the Orthodox and Catholic Churches use. In Sirach 15 the text states:
"Do not say: "It was God's doing that I fell away," for what he hates he does not do. Do not say: "He himself has led me astray," for he has no need of the wicked. Abominable wickedness the Lord hates and he does not let it happen to those who fear him. God in the beginning created human beings and made them subject to their own free choice. If you choose, you can keep the commandments; loyalty is doing the will of God. Set before you are fire and water; to whatever you choose, stretch out your hand. Before everyone are life and death, whichever they choose will be given them. Immense is the wisdom of the Lord; mighty in power, he sees all things. The eyes of God behold his works, and he understands every human deed. He never commands anyone to sin, nor shows leniency toward deceivers."
- Ben Sira 15:11-20 NABRE
The exact meaning of these verses has been debated by Christian theologians throughout history.
In Jewish thought the concept of "Free will" (Hebrew: בחירה חפשית, romanized: bechirah chofshit; בחירה, bechirah) is foundational. 
The most succinct statement is by Maimonides, in a two part treatment, where human free will is specified as part of the universe's Godly design:
In Islam the theological issue is not usually how to reconcile free will with God's foreknowledge, but with God's jabr, or divine commanding power. al-Ash'ari developed an "acquisition" or "dual-agency" form of compatibilism, in which human free will and divine jabr were both asserted, and which became a cornerstone of the dominant Ash'ari position.[241][242] In Shia Islam, Ash'aris understanding of a higher balance toward predestination is challenged by most theologians.[243] Free will, according to Islamic doctrine is the main factor for man's accountability in his/her actions throughout life. Actions taken by people exercising free will are counted on the Day of Judgement because they are their own; however, the free will happens with the permission of God.[244]
The philosopher Søren Kierkegaard claimed that divine omnipotence cannot be separated from divine goodness.[245] As a truly omnipotent and good being, God could create beings with true freedom over God. Furthermore, God would voluntarily do so because "the greatest good... which can be done for a being, greater than anything else that one can do for it, is to be truly free."[246] Alvin Plantinga's free-will defense is a contemporary expansion of this theme, adding how God, free will, and evil are consistent.[247]
Some philosophers follow William of Ockham in holding that necessity and possibility are defined with respect to a given point in time and a given matrix of empirical circumstances, and so something that is merely possible from the perspective of one observer may be necessary from the perspective of an omniscient.[248] Some philosophers follow Philo of Alexandria, a philosopher known for his homocentrism, in holding that free will is a feature of a human's soul, and thus that non-human animals lack free will.[249]
This article incorporates material from the Citizendium article "Free will", which is licensed under the Creative Commons Attribution-ShareAlike 3.0 Unported License but not under the GFDL.




Buddhism (/ˈbʊdɪzəm/ BUUD-ih-zəm, US also /ˈbuːd-/ BOOD-),[1][2][3] also known as Buddha Dharma, and Dharmavinaya (transl. "doctrines and disciplines"), is an Indian religion or philosophical tradition based on teachings attributed to the Buddha.[4] It originated in present-day North India as a śramaṇa–movement in the 5th century BCE, and gradually spread throughout much of Asia via the Silk Road. It is the world's fifth-largest religion,[5][6] with over 520 million followers (Buddhists) who comprise seven percent of the global population.[7][8][9] 
The Buddha's central teachings emphasize the aim of attaining liberation from attachment or clinging to existence, which is said to be marked by impermanence (anitya), dissatisfaction/suffering (duḥkha), and the absence of lasting essence (anātman).[10] He endorsed the Middle Way, a path of spiritual development that avoids both extreme asceticism and hedonism. A summary of this path is expressed in the Noble Eightfold Path, a cultivation of the mind through observance of meditation and Buddhist ethics. Other widely observed practices include: monasticism; "taking refuge" in the Buddha, the dharma, and the saṅgha; and the cultivation of perfections (pāramitā).[11]
Buddhist schools vary in their interpretation of the paths to liberation (mārga) as well as the relative importance and 'canonicity' assigned to various Buddhist texts, and their specific teachings and practices.[12][13] Two major extant branches of Buddhism are generally recognized by scholars: Theravāda (lit. 'School of the Elders') and Mahāyāna (lit. 'Great Vehicle'). The Theravada tradition emphasizes the attainment of nirvāṇa (lit. 'extinguishing') as a means of transcending the individual self and ending the cycle of death and rebirth (saṃsāra),[14][15][16] while the Mahayana tradition emphasizes the Bodhisattva-ideal, in which one works for the liberation of all beings. The Buddhist canon is vast, with many different textual collections in different languages (such as Sanskrit, Pali, Tibetan, and Chinese).[17]
The Theravāda branch has a widespread following in Sri Lanka as well as in Southeast Asia, namely Myanmar, Thailand, Laos, and Cambodia. The Mahāyāna branch—which includes the traditions of Zen, Pure Land, Nichiren, Tiantai, Tendai, and Shingon—is predominantly practised in Nepal, Bhutan, China, Malaysia, Vietnam, Taiwan, Korea, and Japan. Additionally, Vajrayāna (lit. 'Indestructible Vehicle'), a body of teachings attributed to Indian adepts, may be viewed as a separate branch or tradition within Mahāyāna.[18] Tibetan Buddhism, which preserves the Vajrayāna teachings of eighth-century India, is practised in the Himalayan states as well as in Mongolia[19] and Russian Kalmykia.[20] Historically, until the early 2nd millennium, Buddhism was widely practised in the Indian subcontinent;[21][22][23] it also had a foothold to some extent elsewhere in Asia, namely Afghanistan, Uzbekistan, and the Philippines.
Buddhism is an Indian religion[24] or philosophy. The Buddha ("the Awakened One"), a Śramaṇa; who lived in South Asia c. 6th or 5th century BCE.[25][26] 
Followers of Buddhism, called Buddhists in English, referred to themselves as Sakyan-s or Sakyabhiksu in ancient India.[27][28] Buddhist scholar Donald S. Lopez asserts they also used the term Bauddha,[29] although scholar Richard Cohen asserts that that term was used only by outsiders to describe Buddhists.[30]
Details of the Buddha's life are mentioned in many Early Buddhist Texts but are inconsistent. His social background and life details are difficult to prove, and the precise dates are uncertain, although the 5th century BCE seems to be the best estimate.[31][note 1]
Early texts have the Buddha's family name as "Gautama" (Pali: Gotama), while some texts give Siddhartha as his surname. He was born in Lumbini, present-day Nepal and grew up in Kapilavastu,[note 2] a town in the Ganges Plain, near the modern Nepal–India border, and he spent his life in what is now modern Bihar[note 3] and Uttar Pradesh.[39][31] Some hagiographic legends state that his father was a king named Suddhodana, his mother was Queen Maya.[40] Scholars such as Richard Gombrich consider this a dubious claim because a combination of evidence suggests he was born in the Shakya community, which was governed by a small oligarchy or republic-like council where there were no ranks but where seniority mattered instead.[41][note 4] Some of the stories about the Buddha, his life, his teachings, and claims about the society he grew up in may have been invented and interpolated at a later time into the Buddhist texts.[44][45]
According to early texts such as the Pali Ariyapariyesanā-sutta ("The discourse on the noble quest", MN 26) and its Chinese parallel at MĀ 204, Gautama was moved by the suffering (dukkha) of life and death, and its endless repetition due to rebirth.[46] He thus set out on a quest to find liberation from suffering (also known as "nirvana").[47] Early texts and biographies state that Gautama first studied under two teachers of meditation, namely Āḷāra Kālāma (Sanskrit: Arada Kalama) and Uddaka Ramaputta (Sanskrit: Udraka Ramaputra), learning meditation and philosophy, particularly the meditative attainment of "the sphere of nothingness" from the former, and "the sphere of neither perception nor non-perception" from the latter.[48][49][note 5]
Finding these teachings to be insufficient to attain his goal, he turned to the practice of severe asceticism, which included a strict fasting regime and various forms of breath control.[52] This too fell short of attaining his goal, and then he turned to the meditative practice of dhyana. He famously sat in meditation under a Ficus religiosa tree — now called the Bodhi Tree — in the town of Bodh Gaya and attained "Awakening" (Bodhi).[53]
According to various early texts like the Mahāsaccaka-sutta, and the Samaññaphala Sutta, on awakening, the Buddha gained insight into the workings of karma and his former lives, as well as achieving the ending of the mental defilements (asavas), the ending of suffering, and the end of rebirth in saṃsāra.[52] This event also brought certainty about the Middle Way as the right path of spiritual practice to end suffering.[54][55] As a fully enlightened Buddha, he attracted followers and founded a Sangha (monastic order).[56] He spent the rest of his life teaching the Dharma he had discovered, and then died, achieving "final nirvana", at the age of 80 in Kushinagar, India.[57][34]
The Buddha's teachings were propagated by his followers, which in the last centuries of the 1st millennium BCE became various Buddhist schools of thought, each with its own basket of texts containing different interpretations and authentic teachings of the Buddha;[58][59][60] these over time evolved into many traditions of which the more well known and widespread in the modern era are Theravada, Mahayana and Vajrayana Buddhism.[61][62][note 6]
The term "Buddhism" is an occidental neologism, commonly (and "rather roughly" according to Donald S. Lopez Jr.) used as a translation for the Dharma of the Buddha, fójiào in Chinese, bukkyō in Japanese, nang pa sangs rgyas pa'i chos in Tibetan, buddhadharma in Sanskrit, buddhaśāsana in Pali.[65]
The Four Truths express the basic orientation of Buddhism: we crave and cling to impermanent states and things, which is dukkha, "incapable of satisfying" and painful.[66][67] This keeps us caught in saṃsāra, the endless cycle of repeated rebirth, dukkha and dying again.[note 7]
But there is a way to liberation from this endless cycle[73] to the state of nirvana, namely following the Noble Eightfold Path.[note 8]
The truth of dukkha is the basic insight that life in this mundane world, with its clinging and craving to impermanent states and things[66] is dukkha, and unsatisfactory.[68][79][web 1] Dukkha can be translated as "incapable of satisfying",[web 5] "the unsatisfactory nature and the general insecurity of all conditioned phenomena"; or "painful".[66][67] Dukkha is most commonly translated as "suffering", but this is inaccurate, since it refers not to episodic suffering, but to the intrinsically unsatisfactory nature of temporary states and things, including pleasant but temporary experiences.[note 9] We expect happiness from states and things which are impermanent, and therefore cannot attain real happiness.
In Buddhism, dukkha is one of the three marks of existence, along with impermanence and anattā (non-self).[85] Buddhism, like other major Indian religions, asserts that everything is impermanent (anicca), but, unlike them, also asserts that there is no permanent self or soul in living beings (anattā).[86][87][88] The ignorance or misperception (avijjā) that anything is permanent or that there is self in any being is considered a wrong understanding, and the primary source of clinging and dukkha.[89][90][91]
Saṃsāra means "wandering" or "world", with the connotation of cyclic, circuitous change.[92][93] It refers to the theory of rebirth and "cyclicality of all life, matter, existence", a fundamental assumption of Buddhism, as with all major Indian religions.[93][94] Samsara in Buddhism is considered to be dukkha, unsatisfactory and painful,[95] perpetuated by desire and avidya (ignorance), and the resulting karma.[93][96][97] Liberation from this cycle of existence, nirvana, has been the foundation and the most important historical justification of Buddhism.[98][99]
Buddhist texts assert that rebirth can occur in six realms of existence, namely three good realms (heavenly, demi-god, human) and three evil realms (animal, hungry ghosts, hellish).[note 10] Samsara ends if a person attains nirvana, the "blowing out" of the afflictions through insight into impermanence and "non-self".[101][102][103]
Rebirth refers to a process whereby beings go through a succession of lifetimes as one of many possible forms of sentient life, each running from conception to death.[104] In Buddhist thought, this rebirth does not involve a soul or any fixed substance. This is because the Buddhist doctrine of anattā (Sanskrit: anātman, no-self doctrine) rejects the concepts of a permanent self or an unchanging, eternal soul found in other religions.[105][106]
The Buddhist traditions have traditionally disagreed on what it is in a person that is reborn, as well as how quickly the rebirth occurs after death.[107][108] Some Buddhist traditions assert that "no self" doctrine means that there is no enduring self, but there is avacya (inexpressible) personality (pudgala) which migrates from one life to another.[107] The majority of Buddhist traditions, in contrast, assert that vijñāna (a person's consciousness) though evolving, exists as a continuum and is the mechanistic basis of what undergoes the rebirth process.[68][107] The quality of one's rebirth depends on the merit or demerit gained by one's karma (i.e. actions), as well as that accrued on one's behalf by a family member.[note 11] Buddhism also developed a complex cosmology to explain the various realms or planes of rebirth.[95]
In Buddhism, karma (from Sanskrit: "action, work") drives saṃsāra – the endless cycle of suffering and rebirth for each being. Good, skilful deeds (Pāli: kusala) and bad, unskilful deeds (Pāli: akusala) produce "seeds" in the unconscious receptacle (ālaya) that mature later either in this life or in a subsequent rebirth.[110][111] The existence of karma is a core belief in Buddhism, as with all major Indian religions, and it implies neither fatalism nor that everything that happens to a person is caused by karma.[112][note 12]
A central aspect of Buddhist theory of karma is that intent (cetanā) matters and is essential to bring about a consequence or phala "fruit" or vipāka "result".[113][note 13] However, good or bad karma accumulates even if there is no physical action, and just having ill or good thoughts creates karmic seeds; thus, actions of body, speech or mind all lead to karmic seeds.[112] In the Buddhist traditions, life aspects affected by the law of karma in past and current births of a being include the form of rebirth, realm of rebirth, social class, character and major circumstances of a lifetime.[112][117][118] It operates like the laws of physics, without external intervention, on every being in all six realms of existence including human beings and gods.[112][119]
A notable aspect of the karma theory in Buddhism is merit transfer.[120][121] A person accumulates merit not only through intentions and ethical living, but also is able to gain merit from others by exchanging goods and services, such as through dāna (charity to monks or nuns).[122] Further, a person can transfer one's own good karma to living family members and ancestors.[121][note 14]
The cessation of the kleshas and the attainment of nirvana (nibbāna), with which the cycle of rebirth ends, has been the primary and the soteriological goal of the Buddhist path for monastic life since the time of the Buddha.[75][125][126] The term "path" is usually taken to mean the Noble Eightfold Path, but other versions of "the path" can also be found in the Nikayas.[note 15] In some passages in the Pali Canon, a distinction is being made between right knowledge or insight (sammā-ñāṇa), and right liberation or release (sammā-vimutti), as the means to attain cessation and liberation.[128][129]
Nirvana literally means "blowing out, quenching, becoming extinguished".[130][131] In early Buddhist texts, it is the state of restraint and self-control that leads to the "blowing out" and the ending of the cycles of sufferings associated with rebirths and redeaths.[132][133][134] Many later Buddhist texts describe nirvana as identical with anatta with complete "emptiness, nothingness".[135][136][137][note 16] In some texts, the state is described with greater detail, such as passing through the gate of emptiness (sunyata) – realising that there is no soul or self in any living being, then passing through the gate of signlessness (animitta) – realising that nirvana cannot be perceived, and finally passing through the gate of wishlessness (apranihita) – realising that nirvana is the state of not even wishing for nirvana.[125][139][note 17]
The nirvana state has been described in Buddhist texts partly in a manner similar to other Indian religions, as the state of complete liberation, enlightenment, highest happiness, bliss, fearlessness, freedom, permanence, non-dependent origination, unfathomable, and indescribable.[141][142] It has also been described in part differently, as a state of spiritual release marked by "emptiness" and realisation of non-self.[143][144][145][note 18]
While Buddhism considers the liberation from saṃsāra as the ultimate spiritual goal, in traditional practice, the primary focus of a vast majority of lay Buddhists has been to seek and accumulate merit through good deeds, donations to monks and various Buddhist rituals in order to gain better rebirths rather than nirvana.[148][149][note 19]
Pratityasamutpada, also called "dependent arising, or dependent origination", is the Buddhist theory to explain the nature and relations of being, becoming, existence and ultimate reality. Buddhism asserts that there is nothing independent, except the state of nirvana.[152] All physical and mental states depend on and arise from other pre-existing states, and in turn from them arise other dependent states while they cease.[153]
The 'dependent arisings' have a causal conditioning, and thus Pratityasamutpada is the Buddhist belief that causality is the basis of ontology, not a creator God nor the ontological Vedic concept called universal Self (Brahman) nor any other 'transcendent creative principle'.[154][155] However, Buddhist thought does not understand causality in terms of Newtonian mechanics; rather it understands it as conditioned arising.[156][157] In Buddhism, dependent arising refers to conditions created by a plurality of causes that necessarily co-originate a phenomenon within and across lifetimes, such as karma in one life creating conditions that lead to rebirth in one of the realms of existence for another lifetime.[158][159][160]
Buddhism applies the theory of dependent arising to explain origination of endless cycles of dukkha and rebirth, through Twelve Nidānas or "twelve links". It states that because Avidyā (ignorance) exists, Saṃskāras (karmic formations) exist; because Saṃskāras exist therefore Vijñāna (consciousness) exists; and in a similar manner it links Nāmarūpa (the sentient body), Ṣaḍāyatana (our six senses), Sparśa (sensory stimulation), Vedanā (feeling), Taṇhā (craving), Upādāna (grasping), Bhava (becoming), Jāti (birth), and Jarāmaraṇa (old age, death, sorrow, and pain).[161][162] By breaking the circuitous links of the Twelve Nidanas, Buddhism asserts that liberation from these endless cycles of rebirth and dukkha can be attained.[163]
A related doctrine in Buddhism is that of anattā (Pali) or anātman (Sanskrit). It is the view that there is no unchanging, permanent self, soul or essence in phenomena.[164] The Buddha and Buddhist philosophers who follow him such as Vasubandhu and Buddhaghosa, generally argue for this view by analyzing the person through the schema of the five aggregates, and then attempting to show that none of these five components of personality can be permanent or absolute.[165] This can be seen in Buddhist discourses such as the Anattalakkhana Sutta.
"Emptiness" or "voidness" (Skt: Śūnyatā, Pali: Suññatā), is a related concept with many different interpretations throughout the various Buddhisms. In early Buddhism, it was commonly stated that all five aggregates are void (rittaka), hollow (tucchaka), coreless (asāraka), for example as in the Pheṇapiṇḍūpama Sutta (SN 22:95).[166] Similarly, in Theravada Buddhism, it often means that the five aggregates are empty of a Self.[167]
Emptiness is a central concept in Mahāyāna Buddhism, especially in Nagarjuna's Madhyamaka school, and in the Prajñāpāramitā sutras. In Madhyamaka philosophy, emptiness is the view which holds that all phenomena (dharmas) are without any svabhava (literally "own-nature" or "self-nature"), and are thus without any underlying essence, and so are "empty" of being independent. This doctrine sought to refute the heterodox theories of svabhava circulating at the time.[168]
All forms of Buddhism revere and take spiritual refuge in the "three jewels" (triratna): Buddha, Dharma and Sangha.[169]
While all varieties of Buddhism revere "Buddha" and "buddhahood", they have different views on what these are. Regardless of their interpretation, the concept of Buddha is central to all forms of Buddhism.
In Theravada Buddhism, a Buddha is someone who has become awake through their own efforts and insight. They have put an end to their cycle of rebirths and have ended all unwholesome mental states which lead to bad action and thus are morally perfected.[170] While subject to the limitations of the human body in certain ways (for example, in the early texts, the Buddha suffers from backaches), a Buddha is said to be "deep, immeasurable, hard-to-fathom as is the great ocean," and also has immense psychic powers (abhijñā).[171] Theravada generally sees Gautama Buddha (the historical Buddha Sakyamuni) as the only Buddha of the current era.
Mahāyāna Buddhism meanwhile, has a vastly expanded cosmology, with various Buddhas and other holy beings (aryas) residing in different realms. Mahāyāna texts not only revere numerous Buddhas besides Shakyamuni, such as Amitabha and Vairocana, but also see them as transcendental or supramundane (lokuttara) beings.[172] Mahāyāna Buddhism holds that these other Buddhas in other realms can be contacted and are able to benefit beings in this world.[173] In Mahāyāna, a Buddha is a kind of "spiritual king", a "protector of all creatures" with a lifetime that is countless of eons long, rather than just a human teacher who has transcended the world after death.[174] Shakyamuni's life and death on earth is then usually understood as a "mere appearance" or "a manifestation skilfully projected into earthly life by a long-enlightened transcendent being, who is still available to teach the faithful through visionary experiences."[174][175]
The second of the three jewels is "Dharma" (Pali: Dhamma), which in Buddhism refers to the Buddha's teaching, which includes all of the main ideas outlined above. While this teaching reflects the true nature of reality, it is not a belief to be clung to, but a pragmatic teaching to be put into practice. It is likened to a raft which is "for crossing over" (to nirvana) not for holding on to.[176] It also refers to the universal law and cosmic order which that teaching both reveals and relies upon.[177] It is an everlasting principle which applies to all beings and worlds. In that sense it is also the ultimate truth and reality about the universe, it is thus "the way that things really are."
The third "jewel" which Buddhists take refuge in is the "Sangha", which refers to the monastic community of monks and nuns who follow Gautama Buddha's monastic discipline which was "designed to shape the Sangha as an ideal community, with the optimum conditions for spiritual growth."[178] The Sangha consists of those who have chosen to follow the Buddha's ideal way of life, which is one of celibate monastic renunciation with minimal material possessions (such as an alms bowl and robes).[179]
The Sangha is seen as important because they preserve and pass down Buddha Dharma. As Gethin states "the Sangha lives the teaching, preserves the teaching as Scriptures and teaches the wider community. Without the Sangha there is no Buddhism."[180] The Sangha also acts as a "field of merit" for laypersons, allowing them to make spiritual merit or goodness by donating to the Sangha and supporting them. In return, they keep their duty to preserve and spread the Dharma everywhere for the good of the world.[181]
There is also a separate definition of Sangha, referring to those who have attained any stage of awakening, whether or not they are monastics. This sangha is called the āryasaṅgha "noble Sangha".[182] All forms of Buddhism generally reveres these āryas (Pali: ariya, "noble ones" or "holy ones") who are spiritually attained beings. Aryas have attained the fruits of the Buddhist path.[183] Becoming an arya is a goal in most forms of Buddhism. The āryasaṅgha includes holy beings such as bodhisattvas, arhats and stream-enterers.
Mahāyāna Buddhism also differs from Theravada and the other schools of early Buddhism in promoting several unique doctrines which are contained in Mahāyāna sutras and philosophical treatises.
One of these is the unique interpretation of emptiness and dependent origination found in the Madhyamaka school. Another very influential doctrine for Mahāyāna is the main philosophical view of the Yogācāra school variously, termed Vijñaptimātratā-vāda ("the doctrine that there are only ideas" or "mental impressions") or Vijñānavāda ("the doctrine of consciousness"). According to Mark Siderits, what classical Yogācāra thinkers like Vasubandhu had in mind is that we are only ever aware of mental images or impressions, which may appear as external objects, but "there is actually no such thing outside the mind."[184] There are several interpretations of this main theory, many scholars see it as a type of Idealism, others as a kind of phenomenology.[185]
Another very influential concept unique to Mahāyāna is that of "Buddha-nature" (buddhadhātu) or "Tathagata-womb" (tathāgatagarbha). Buddha-nature is a concept found in some 1st-millennium CE Buddhist texts, such as the Tathāgatagarbha sūtras. According to Paul Williams these Sutras suggest that 'all sentient beings contain a Tathagata' as their 'essence, core inner nature, Self'.[186][note 20] According to Karl Brunnholzl "the earliest mahayana sutras that are based on and discuss the notion of tathāgatagarbha as the buddha potential that is innate in all sentient beings began to appear in written form in the late second and early third century."[188] For some, the doctrine seems to conflict with the Buddhist anatta doctrine (non-Self), leading scholars to posit that the Tathāgatagarbha Sutras were written to promote Buddhism to non-Buddhists.[189][190] This can be seen in texts like the Laṅkāvatāra Sūtra, which state that Buddha-nature is taught to help those who have fear when they listen to the teaching of anatta.[191] Buddhist texts like the Ratnagotravibhāga clarify that the "Self" implied in Tathagatagarbha doctrine is actually "not-self".[192][193] Various interpretations of the concept have been advanced by Buddhist thinkers throughout the history of Buddhist thought and most attempt to avoid anything like the Hindu Atman doctrine.
These Indian Buddhist ideas, in various synthetic ways, form the basis of subsequent Mahāyāna philosophy in Tibetan Buddhism and East Asian Buddhism.
The Bodhipakkhiyādhammā are seven lists of qualities or factors that contribute to awakening (bodhi). Each list is a short summary of the Buddhist path, and the seven lists substantially overlap. The best-known list in the West is the Noble Eightfold Path, but a wide variety of paths and models of progress have been used and described in the different Buddhist traditions. However, they generally share basic practices such as sila (ethics), samadhi (meditation, dhyana) and prajña (wisdom), which are known as the three trainings. An important additional practice is a kind and compassionate attitude toward every living being and the world. Devotion is also important in some Buddhist traditions, and in the Tibetan traditions visualisations of deities and mandalas are important. The value of textual study is regarded differently in the various Buddhist traditions. It is central to Theravada and highly important to Tibetan Buddhism, while the Zen tradition takes an ambiguous stance.
An important guiding principle of Buddhist practice is the Middle Way (madhyamapratipad). It was a part of Buddha's first sermon, where he presented the Noble Eightfold Path that was a 'middle way' between the extremes of asceticism and hedonistic sense pleasures.[194][195] In Buddhism, states Harvey, the doctrine of "dependent arising" (conditioned arising, pratītyasamutpāda) to explain rebirth is viewed as the 'middle way' between the doctrines that a being has a "permanent soul" involved in rebirth (eternalism) and "death is final and there is no rebirth" (annihilationism).[196][197]
A common presentation style of the path (mārga) to liberation in the Early Buddhist Texts is the "graduated talk", in which the Buddha lays out a step by step training.[198]
In the early texts, numerous different sequences of the gradual path can be found.[199] One of the most important and widely used presentations among the various Buddhist schools is The Noble Eightfold Path, or "Eightfold Path of the Noble Ones" (Skt. 'āryāṣṭāṅgamārga'). This can be found in various discourses, most famously in the Dhammacakkappavattana Sutta (The discourse on the turning of the Dharma wheel).
Other suttas such as the Tevijja Sutta, and the Cula-Hatthipadopama-sutta give a different outline of the path, though with many similar elements such as ethics and meditation.[199]
According to Rupert Gethin, the path to awakening is also frequently summarized by another a short formula: "abandoning the hindrances, practice of the four establishings of mindfulness, and development of the awakening factors."[200]
The Eightfold Path consists of a set of eight interconnected factors or conditions, that when developed together, lead to the cessation of dukkha.[201] These eight factors are: Right View (or Right Understanding), Right Intention (or Right Thought), Right Speech, Right Action, Right Livelihood, Right Effort, Right Mindfulness, and Right Concentration.
This Eightfold Path is the fourth of the Four Noble Truths, and asserts the path to the cessation of dukkha (suffering, pain, unsatisfactoriness).[202][203] The path teaches that the way of the enlightened ones stopped their craving, clinging and karmic accumulations, and thus ended their endless cycles of rebirth and suffering.[204][205][206]
The Noble Eightfold Path is grouped into three basic divisions, as follows:[207][208][209]
In various suttas which present the graduated path taught by the Buddha, such as the Samaññaphala Sutta and the Cula-Hatthipadopama Sutta, the first step on the path is hearing the Buddha teach the Dharma. This then said to lead to the acquiring of confidence or faith in the Buddha's teachings.[199]
Mahayana Buddhist teachers such as Yin Shun also state that hearing the Dharma and study of the Buddhist discourses is necessary "if one wants to learn and practice the Buddha Dharma."[219] Likewise, in Indo-Tibetan Buddhism, the "Stages of the Path" (Lamrim) texts generally place the activity of listening to the Buddhist teachings as an important early practice.[220]
Traditionally, the first step in most Buddhist schools requires taking of the "Three Refuges", also called the Three Jewels (Sanskrit: triratna, Pali: tiratana) as the foundation of one's religious practice.[221] This practice may have been influenced by the Brahmanical motif of the triple refuge, found in the Rigveda 9.97.47, Rigveda 6.46.9 and Chandogya Upanishad 2.22.3–4.[222] Tibetan Buddhism sometimes adds a fourth refuge, in the lama. The three refuges are believed by Buddhists to be protective and a form of reverence.[221]
The ancient formula which is repeated for taking refuge affirms that "I go to the Buddha as refuge, I go to the Dhamma as refuge, I go to the Sangha as refuge."[223] Reciting the three refuges, according to Harvey, is considered not as a place to hide, rather a thought that "purifies, uplifts and strengthens the heart".[169]
Śīla (Sanskrit) or sīla (Pāli) is the concept of "moral virtues", that is the second group and an integral part of the Noble Eightfold Path.[210] It generally consists of right speech, right action and right livelihood.[210]
One of the most basic forms of ethics in Buddhism is the taking of "precepts". This includes the Five Precepts for laypeople, Eight or Ten Precepts for monastic life, as well as rules of Dhamma (Vinaya or Patimokkha) adopted by a monastery.[224][225]
Other important elements of Buddhist ethics include giving or charity (dāna), Mettā (Good-Will), Heedfulness (Appamada), 'self-respect' (Hri) and 'regard for consequences' (Apatrapya).
Buddhist scriptures explain the five precepts (Pali: pañcasīla; Sanskrit: pañcaśīla) as the minimal standard of Buddhist morality.[211] It is the most important system of morality in Buddhism, together with the monastic rules.[226]
The five precepts are seen as a basic training applicable to all Buddhists. They are:[224][227][228]
Undertaking and upholding the five precepts is based on the principle of non-harming (Pāli and Sanskrit: ahiṃsa).[235] The Pali Canon recommends one to compare oneself with others, and on the basis of that, not to hurt others.[236] Compassion and a belief in karmic retribution form the foundation of the precepts.[237][238] Undertaking the five precepts is part of regular lay devotional practice, both at home and at the local temple.[239][240] However, the extent to which people keep them differs per region and time.[241][240] They are sometimes referred to as the śrāvakayāna precepts in the Mahāyāna tradition, contrasting them with the bodhisattva precepts.[242]
Vinaya is the specific code of conduct for a sangha of monks or nuns. It includes the Patimokkha, a set of 227 offences including 75 rules of decorum for monks, along with penalties for transgression, in the Theravadin tradition.[243] The precise content of the Vinaya Pitaka (scriptures on the Vinaya) differs in different schools and tradition, and different monasteries set their own standards on its implementation. The list of pattimokkha is recited every fortnight in a ritual gathering of all monks.[243] Buddhist text with vinaya rules for monasteries have been traced in all Buddhist traditions, with the oldest surviving being the ancient Chinese translations.[244]
Monastic communities in the Buddhist tradition cut normal social ties to family and community, and live as "islands unto themselves".[245] Within a monastic fraternity, a sangha has its own rules.[245] A monk abides by these institutionalised rules, and living life as the vinaya prescribes it is not merely a means, but very nearly the end in itself.[245] Transgressions by a monk on Sangha vinaya rules invites enforcement, which can include temporary or permanent expulsion.[246]
Another important practice taught by the Buddha is the restraint of the senses (indriyasamvara). In the various graduated paths, this is usually presented as a practice which is taught prior to formal sitting meditation, and which supports meditation by weakening sense desires that are a hindrance to meditation.[247] According to Anālayo, sense restraint is when one "guards the sense doors in order to prevent sense impressions from leading to desires and discontent."[247] This is not an avoidance of sense impression, but a kind of mindful attention towards the sense impressions which does not dwell on their main features or signs (nimitta). This is said to prevent harmful influences from entering the mind.[248] This practice is said to give rise to an inner peace and happiness which forms a basis for concentration and insight.[248]
A related Buddhist virtue and practice is renunciation, or the intent for desirelessness (nekkhamma).[249] Generally, renunciation is the giving up of actions and desires that are seen as unwholesome on the path, such as lust for sensuality and worldly things.[250] Renunciation can be cultivated in different ways. The practice of giving for example, is one form of cultivating renunciation. Another one is the giving up of lay life and becoming a monastic (bhiksu o bhiksuni).[251] Practicing celibacy (whether for life as a monk, or temporarily) is also a form of renunciation.[252] Many Jataka stories such as the focus on how the Buddha practiced renunciation in past lives.[253]
One way of cultivating renunciation taught by the Buddha is the contemplation (anupassana) of the "dangers" (or "negative consequences") of sensual pleasure (kāmānaṃ ādīnava). As part of the graduated discourse, this contemplation is taught after the practice of giving and morality.[254]
Another related practice to renunciation and sense restraint taught by the Buddha is "restraint in eating" or moderation with food, which for monks generally means not eating after noon. Devout laypersons also follow this rule during special days of religious observance (uposatha).[255] Observing the Uposatha also includes other practices dealing with renunciation, mainly the eight precepts.
For Buddhist monastics, renunciation can also be trained through several optional ascetic practices called dhutaṅga.
In different Buddhist traditions, other related practices which focus on fasting are followed.
The training of the faculty called "mindfulness" (Pali: sati, Sanskrit: smṛti, literally meaning "recollection, remembering") is central in Buddhism. According to Analayo, mindfulness is a full awareness of the present moment which enhances and strengthens memory.[256] The Indian Buddhist philosopher Asanga defined mindfulness thus: "It is non-forgetting by the mind with regard to the object experienced. Its function is non-distraction."[257] According to Rupert Gethin, sati is also "an awareness of things in relation to things, and hence an awareness of their relative value."[258]
There are different practices and exercises for training mindfulness in the early discourses, such as the four Satipaṭṭhānas (Sanskrit: smṛtyupasthāna, "establishments of mindfulness") and Ānāpānasati (Sanskrit: ānāpānasmṛti, "mindfulness of breathing").
A closely related mental faculty, which is often mentioned side by side with mindfulness, is sampajañña ("clear comprehension"). This faculty is the ability to comprehend what one is doing and is happening in the mind, and whether it is being influenced by unwholesome states or wholesome ones.[259]
A wide range of meditation practices has developed in the Buddhist traditions, but "meditation" primarily refers to the attainment of samādhi and the practice of dhyāna (Pali: jhāna). Samādhi is a calm, undistracted, unified and concentrated state of awareness. It is defined by Asanga as "one-pointedness of mind on the object to be investigated. Its function consists of giving a basis to knowledge (jñāna)."[257]
Dhyāna is "state of perfect equanimity and awareness (upekkhā-sati-parisuddhi)," reached through focused mental training.[260]
The practice of dhyāna aids in maintaining a calm mind, and avoiding disturbance of this calm mind by mindfulness of disturbing thoughts and feelings.[261][note 21]
The earliest evidence of yogis and their meditative tradition, states Karel Werner, is found in the Keśin hymn 10.136 of the Rigveda.[262] While evidence suggests meditation was practised in the centuries preceding the Buddha,[263] the meditative methodologies described in the Buddhist texts are some of the earliest among texts that have survived into the modern era.[264][265] These methodologies likely incorporate what existed before the Buddha as well as those first developed within Buddhism.[266][note 22]
There is no scholarly agreement on the origin and source of the practice of dhyāna. Some scholars, like Bronkhorst, see the four dhyānas as a Buddhist invention.[270] Alexander Wynne argues that the Buddha learned dhyāna from Brahmanical teachers.[271]
Whatever the case, the Buddha taught meditation with a new focus and interpretation, particularly through the four dhyānas methodology,[272] in which mindfulness is maintained.[273][274] Further, the focus of meditation and the underlying theory of liberation guiding the meditation has been different in Buddhism.[263][275][276] For example, states Bronkhorst, the verse 4.4.23 of the Brihadaranyaka Upanishad with its "become calm, subdued, quiet, patiently enduring, concentrated, one sees soul in oneself" is most probably a meditative state.[277] The Buddhist discussion of meditation is without the concept of soul and the discussion criticises both the ascetic meditation of Jainism and the "real self, soul" meditation of Hinduism.[278]
Often grouped into the jhāna-scheme are four other meditative states, referred to in the early texts as arupa samāpattis (formless attainments). These are also referred to in commentarial literature as immaterial/formless jhānas (arūpajhānas). The first formless attainment is a place or realm of infinite space (ākāsānañcāyatana) without form or colour or shape. The second is termed the realm of infinite consciousness (viññāṇañcāyatana); the third is the realm of nothingness (ākiñcaññāyatana), while the fourth is the realm of "neither perception nor non-perception".[279] The four rupa-jhānas in Buddhist practice lead to rebirth in successfully better rupa Brahma heavenly realms, while arupa-jhānas lead into arupa heavens.[280][281]
In the Pali canon, the Buddha outlines two meditative qualities which are mutually supportive: samatha (Pāli; Sanskrit: śamatha; "calm") and vipassanā (Sanskrit: vipaśyanā, insight).[282] The Buddha compares these mental qualities to a "swift pair of messengers" who together help deliver the message of nibbana (SN 35.245).[283]
The various Buddhist traditions generally see Buddhist meditation as being divided into those two main types.[284][285] Samatha is also called "calming meditation", and focuses on stilling and concentrating the mind i.e. developing samadhi and the four dhyānas. According to Damien Keown, vipassanā meanwhile, focuses on "the generation of penetrating and critical insight (paññā)".[286]
There are numerous doctrinal positions and disagreements within the different Buddhist traditions regarding these qualities or forms of meditation. For example, in the Pali Four Ways to Arahantship Sutta (AN 4.170), it is said that one can develop calm and then insight, or insight and then calm, or both at the same time.[287] Meanwhile, in Vasubandhu's Abhidharmakośakārikā, vipaśyanā is said to be practiced once one has reached samadhi by cultivating the four foundations of mindfulness (smṛtyupasthānas).[288]
Beginning with comments by La Vallee Poussin, a series of scholars have argued that these two meditation types reflect a tension between two different ancient Buddhist traditions regarding the use of dhyāna, one which focused on insight based practice and the other which focused purely on dhyāna.[289][290] However, other scholars such as Analayo and Rupert Gethin have disagreed with this "two paths" thesis, instead seeing both of these practices as complementary.[290][291]
The four immeasurables or four abodes, also called Brahma-viharas, are virtues or directions for meditation in Buddhist traditions, which helps a person be reborn in the heavenly (Brahma) realm.[292][293][294] These are traditionally believed to be a characteristic of the deity Brahma and the heavenly abode he resides in.[295]
The four Brahma-vihara are:
Some Buddhist traditions, especially those associated with Tantric Buddhism (also known as Vajrayana and Secret Mantra) use images and symbols of deities and Buddhas in meditation. This is generally done by mentally visualizing a Buddha image (or some other mental image, like a symbol, a mandala, a syllable, etc.), and using that image to cultivate calm and insight. One may also visualize and identify oneself with the imagined deity.[297][298] While visualization practices have been particularly popular in Vajrayana, they may also found in Mahayana and Theravada traditions.[299]
In Tibetan Buddhism, unique tantric techniques which include visualization (but also mantra recitation, mandalas, and other elements) are considered to be much more effective than non-tantric meditations and they are one of the most popular meditation methods.[300] The methods of Unsurpassable Yoga Tantra, (anuttarayogatantra) are in turn seen as the highest and most advanced. Anuttarayoga practice is divided into two stages, the Generation Stage and the Completion Stage. In the Generation Stage, one meditates on emptiness and visualizes oneself as a deity as well as visualizing its mandala. The focus is on developing clear appearance and divine pride (the understanding that oneself and the deity are one).[301] This method is also known as deity yoga (devata yoga). There are numerous meditation deities (yidam) used, each with a mandala, a circular symbolic map used in meditation.[302]
Prajñā (Sanskrit) or paññā (Pāli) is wisdom, or knowledge of the true nature of existence. Another term which is associated with prajñā and sometimes is equivalent to it is vipassanā (Pāli) or vipaśyanā (Sanskrit), which is often translated as "insight". In Buddhist texts, the faculty of insight is often said to be cultivated through the four establishments of mindfulness.[303] In the early texts, Paññā is included as one of the "five faculties" (indriya) which are commonly listed as important spiritual elements to be cultivated (see for example: AN I 16). Paññā along with samadhi, is also listed as one of the "trainings in the higher states of mind" (adhicittasikkha).[303]
The Buddhist tradition regards ignorance (avidyā), a fundamental ignorance, misunderstanding or mis-perception of the nature of reality, as one of the basic causes of dukkha and samsara. Overcoming this ignorance is part of the path to awakening. This overcoming includes the contemplation of impermanence and the non-self nature of reality,[304][305] and this develops dispassion for the objects of clinging, and liberates a being from dukkha and saṃsāra.[306][307][308]
Prajñā is important in all Buddhist traditions. It is variously described as wisdom regarding the impermanent and not-self nature of dharmas (phenomena), the functioning of karma and rebirth, and knowledge of dependent origination.[309] Likewise, vipaśyanā is described in a similar way, such as in the Paṭisambhidāmagga, where it is said to be the contemplation of things as impermanent, unsatisfactory and not-self.[310]
Most forms of Buddhism "consider saddhā (Skt śraddhā), 'trustful confidence' or 'faith', as a quality which must be balanced by wisdom, and as a preparation for, or accompaniment of, meditation."[311] Because of this devotion (Skt. bhakti; Pali: bhatti) is an important part of the practice of most Buddhists.[312] Devotional practices include ritual prayer, prostration, offerings, pilgrimage, and chanting.[313] Buddhist devotion is usually focused on some object, image or location that is seen as holy or spiritually influential. Examples of objects of devotion include paintings or statues of Buddhas and bodhisattvas, stupas, and bodhi trees.[314] Public group chanting for devotional and ceremonial is common to all Buddhist traditions and goes back to ancient India where chanting aided in the memorization of the orally transmitted teachings.[315] Rosaries called malas are used in all Buddhist traditions to count repeated chanting of common formulas or mantras. Chanting is thus a type of devotional group meditation which leads to tranquility and communicates the Buddhist teachings.[316]
Based on the Indian principle of ahimsa (non-harming), the Buddha's ethics strongly condemn the harming of all sentient beings, including all animals. He thus condemned the animal sacrifice of the Brahmins as well hunting, and killing animals for food.[317] However, early Buddhist texts depict the Buddha as allowing monastics to eat meat. This seems to be because monastics begged for their food and thus were supposed to accept whatever food was offered to them.[318] This was tempered by the rule that meat had to be "three times clean": "they had not seen, had not heard, and had no reason to suspect that the animal had been killed so that the meat could be given to them".[319] Also, while the Buddha did not explicitly promote vegetarianism in his discourses, he did state that gaining one's livelihood from the meat trade was unethical.[320] In contrast to this, various Mahayana sutras and texts like the Mahaparinirvana sutra, Surangama sutra and the Lankavatara sutra state that the Buddha promoted vegetarianism out of compassion.[321] Indian Mahayana thinkers like Shantideva promoted the avoidance of meat.[322] Throughout history, the issue of whether Buddhists should be vegetarian has remained a much debated topic and there is a variety of opinions on this issue among modern Buddhists.
Buddhism, like all Indian religions, was initially an oral tradition in ancient times.[323] The Buddha's words, the early doctrines, concepts, and their traditional interpretations were orally transmitted from one generation to the next. The earliest oral texts were transmitted in Middle Indo-Aryan languages called Prakrits, such as Pali, through the use of communal recitation and other mnemonic techniques.[324] The first Buddhist canonical texts were likely written down in Sri Lanka, about 400 years after the Buddha died.[323] The texts were part of the Tripitakas, and many versions appeared thereafter claiming to be the words of the Buddha. Scholarly Buddhist commentary texts, with named authors, appeared in India, around the 2nd century CE.[323] These texts were written in Pali or Sanskrit, sometimes regional languages, as palm-leaf manuscripts, birch bark, painted scrolls, carved into temple walls, and later on paper.[323]
Unlike what the Bible is to Christianity and the Quran is to Islam, but like all major ancient Indian religions, there is no consensus among the different Buddhist traditions as to what constitutes the scriptures or a common canon in Buddhism.[323] The general belief among Buddhists is that the canonical corpus is vast.[325][326][327] This corpus includes the ancient Sutras organised into Nikayas or Agamas, itself the part of three basket of texts called the Tripitakas.[328] Each Buddhist tradition has its own collection of texts, much of which is translation of ancient Pali and Sanskrit Buddhist texts of India. The Chinese Buddhist canon, for example, includes 2184 texts in 55 volumes, while the Tibetan canon comprises 1108 texts – all claimed to have been spoken by the Buddha – and another 3461 texts composed by Indian scholars revered in the Tibetan tradition.[329] The Buddhist textual history is vast; over 40,000 manuscripts – mostly Buddhist, some non-Buddhist – were discovered in 1900 in the Dunhuang Chinese cave alone.[329]

The Early Buddhist Texts refers to the literature which is considered by modern scholars to be the earliest Buddhist material. The first four Pali Nikayas, and the corresponding Chinese Āgamas are generally considered to be among the earliest material.[330][331][332] Apart from these, there are also fragmentary collections of EBT materials in other languages such as Sanskrit, Khotanese, Tibetan and Gāndhārī. The modern study of early Buddhism often relies on comparative scholarship using these various early Buddhist sources to identify parallel texts and common doctrinal content.[333] One feature of these early texts are literary structures which reflect oral transmission, such as widespread repetition.[334]After the development of the different early Buddhist schools, these schools began to develop their own textual collections, which were termed Tripiṭakas (Triple Baskets).[335]
Many early Tripiṭakas, like the Pāli Tipitaka, were divided into three sections: Vinaya Pitaka (focuses on monastic rule), Sutta Pitaka (Buddhist discourses) and Abhidhamma Pitaka, which contain expositions and commentaries on the doctrine. The Pāli Tipitaka (also known as the Pali Canon) of the Theravada School constitutes the only complete collection of Buddhist texts in an Indic language which has survived until today.[336] However, many Sutras, Vinayas and Abhidharma works from other schools survive in Chinese translation, as part of the Chinese Buddhist Canon. According to some sources, some early schools of Buddhism had five or seven pitakas.[337]
The Mahāyāna sūtras are a very broad genre of Buddhist scriptures that the Mahāyāna Buddhist tradition holds are original teachings of the Buddha. Modern historians generally hold that the first of these texts were composed probably around the 1st century BCE or 1st century CE.[338][339][340] In Mahāyāna, these texts are generally given greater authority than the early Āgamas and Abhidharma literature, which are called "Śrāvakayāna" or "Hinayana" to distinguish them from Mahāyāna sūtras.[341] Mahāyāna traditions mainly see these different classes of texts as being designed for different types of persons, with different levels of spiritual understanding. The Mahāyāna sūtras are mainly seen as being for those of "greater" capacity.[342][better source needed] Mahāyāna also has a very large literature of philosophical and exegetical texts. These are often called śāstra (treatises) or vrittis (commentaries). Some of this literature was also written in verse form (karikās), the most famous of which is the Mūlamadhyamika-karikā (Root Verses on the Middle Way) by Nagarjuna, the foundational text of the Madhyamika school.
During the Gupta Empire, a new class of Buddhist sacred literature began to develop, which are called the Tantras.[343] By the 8th century, the tantric tradition was very influential in India and beyond. Besides drawing on a Mahāyāna Buddhist framework, these texts also borrowed deities and material from other Indian religious traditions, such as the Śaiva and Pancharatra traditions, local god/goddess cults, and local spirit worship (such as yaksha or nāga spirits).[344][345]
Some features of these texts include the widespread use of mantras, meditation on the subtle body, worship of fierce deities, and antinomian and transgressive practices such as ingesting alcohol and performing sexual rituals.[346][347][348]
Historically, the roots of Buddhism lie in the religious thought of Iron Age India around the middle of the first millennium BCE.[349] This was a period of great intellectual ferment and socio-cultural change known as the "Second urbanisation", marked by the growth of towns and trade, the composition of the Upanishads and the historical emergence of the Śramaṇa traditions.[350][351][note 23]
New ideas developed both in the Vedic tradition in the form of the Upanishads, and outside of the Vedic tradition through the Śramaṇa movements.[354][355][356] The term Śramaṇa refers to several Indian religious movements parallel to but separate from the historical Vedic religion, including Buddhism, Jainism and others such as Ājīvika.[357]
Several Śramaṇa movements are known to have existed in India before the 6th century BCE (pre-Buddha, pre-Mahavira), and these influenced both the āstika and nāstika traditions of Indian philosophy.[358] According to Martin Wilshire, the Śramaṇa tradition evolved in India over two phases, namely Paccekabuddha and Savaka phases, the former being the tradition of individual ascetic and the latter of disciples, and that Buddhism and Jainism ultimately emerged from these.[359] Brahmanical and non-Brahmanical ascetic groups shared and used several similar ideas,[360] but the Śramaṇa traditions also drew upon already established Brahmanical concepts and philosophical roots, states Wiltshire, to formulate their own doctrines.[358][361] Brahmanical motifs can be found in the oldest Buddhist texts, using them to introduce and explain Buddhist ideas.[362] For example, prior to Buddhist developments, the Brahmanical tradition internalised and variously reinterpreted the three Vedic sacrificial fires as concepts such as Truth, Rite, Tranquility or Restraint.[363] Buddhist texts also refer to the three Vedic sacrificial fires, reinterpreting and explaining them as ethical conduct.[364]
The Śramaṇa religions challenged and broke with the Brahmanic tradition on core assumptions such as Atman (soul, self), Brahman, the nature of afterlife, and they rejected the authority of the Vedas and Upanishads.[365][366][367] Buddhism was one among several Indian religions that did so.[367]
Early buddhist positions in the Theravada tradition had not established any deities, but were epistemologically cautious rather than directly atheist. Later buddhist traditions were more influenced by the critique of deities within Hinduism and therefore more committed to a strongly atheist stance. These developments were historic and epistemological as documented in verses from Śāntideva's Bodhicaryāvatāra, and supplemented by reference to suttas and jātakas from the Pali canon.[368]
The history of Indian Buddhism may be divided into five periods:[369] Early Buddhism (occasionally called pre-sectarian Buddhism), Nikaya Buddhism or Sectarian Buddhism: The period of the early Buddhist schools, Early Mahayana Buddhism, Late Mahayana, and the era of Vajrayana or the "Tantric Age".
According to Lambert Schmithausen Pre-sectarian Buddhism is "the canonical period prior to the development of different schools with their different positions."[370]
The early Buddhist Texts include the four principal Pali Nikāyas [note 24] (and their parallel Agamas found in the Chinese canon) together with the main body of monastic rules, which survive in the various versions of the patimokkha.[371][372][373] However, these texts were revised over time, and it is unclear what constitutes the earliest layer of Buddhist teachings. One method to obtain information on the oldest core of Buddhism is to compare the oldest extant versions of the Theravadin Pāli Canon and other texts.[note 25] The reliability of the early sources, and the possibility to draw out a core of oldest teachings, is a matter of dispute.[376] According to Vetter, inconsistencies remain, and other methods must be applied to resolve those inconsistencies.[374][note 26]
According to Schmithausen, three positions held by scholars of Buddhism can be distinguished:[380]
According to Mitchell, certain basic teachings appear in many places throughout the early texts, which has led most scholars to conclude that Gautama Buddha must have taught something similar to the Four Noble Truths, the Noble Eightfold Path, Nirvana, the three marks of existence, the five aggregates, dependent origination, karma and rebirth.[386]
According to N. Ross Reat, all of these doctrines are shared by the Theravada Pali texts and the Mahasamghika school's Śālistamba Sūtra.[387] A recent study by Bhikkhu Analayo concludes that the Theravada Majjhima Nikaya and Sarvastivada Madhyama Agama contain mostly the same major doctrines.[388] Richard Salomon, in his study of the Gandharan texts (which are the earliest manuscripts containing early discourses), has confirmed that their teachings are "consistent with non-Mahayana Buddhism, which survives today in the Theravada school of Sri Lanka and Southeast Asia, but which in ancient times was represented by eighteen separate schools."[389]
However, some scholars argue that critical analysis reveals discrepancies among the various doctrines found in these early texts, which point to alternative possibilities for early Buddhism.[390][391][392] The authenticity of certain teachings and doctrines have been questioned. For example, some scholars think that karma was not central to the teaching of the historical Buddha, while other disagree with this position.[393][394] Likewise, there is scholarly disagreement on whether insight was seen as liberating in early Buddhism or whether it was a later addition to the practice of the four jhānas.[377][395][396] Scholars such as Bronkhorst also think that the four noble truths may not have been formulated in earliest Buddhism, and did not serve in earliest Buddhism as a description of "liberating insight".[397] According to Vetter, the description of the Buddhist path may initially have been as simple as the term "the middle way".[132] In time, this short description was elaborated, resulting in the description of the eightfold path.[132]
According to numerous Buddhist scriptures, soon after the parinirvāṇa (from Sanskrit: "highest extinguishment") of Gautama Buddha, the first Buddhist council was held to collectively recite the teachings to ensure that no errors occurred in oral transmission. Many modern scholars question the historicity of this event.[398] However, Richard Gombrich states that the monastic assembly recitations of the Buddha's teaching likely began during Buddha's lifetime, and they served a similar role of codifying the teachings.[399]
The so called Second Buddhist council resulted in the first schism in the Sangha. Modern scholars believe that this was probably caused when a group of reformists called Sthaviras ("elders") sought to modify the Vinaya (monastic rule), and this caused a split with the conservatives who rejected this change, they were called Mahāsāṃghikas.[400][401] While most scholars accept that this happened at some point, there is no agreement on the dating, especially if it dates to before or after the reign of Ashoka.[402]
Buddhism may have spread only slowly throughout India until the time of the Mauryan emperor Ashoka (304–232 BCE), who was a public supporter of the religion. The support of Aśoka and his descendants led to the construction of more stūpas (such as at Sanchi and Bharhut), temples (such as the Mahabodhi Temple) and to its spread throughout the Maurya Empire and into neighbouring lands such as Central Asia and to the island of Sri Lanka.
During and after the Mauryan period (322–180 BCE), the Sthavira community gave rise to several schools, one of which was the Theravada school which tended to congregate in the south and another which was the Sarvāstivāda school, which was mainly in north India. Likewise, the Mahāsāṃghika groups also eventually split into different Sanghas. Originally, these schisms were caused by disputes over monastic disciplinary codes of various fraternities, but eventually, by about 100 CE if not earlier, schisms were being caused by doctrinal disagreements too.[403]
Following (or leading up to) the schisms, each Saṅgha started to accumulate their own version of Tripiṭaka (triple basket of texts).[60][404] In their Tripiṭaka, each school included the Suttas of the Buddha, a Vinaya basket (disciplinary code) and some schools also added an Abhidharma basket which were texts on detailed scholastic classification, summary and interpretation of the Suttas.[60][405] The doctrine details in the Abhidharmas of various Buddhist schools differ significantly, and these were composed starting about the third century BCE and through the 1st millennium CE.[406][407][408]
According to the edicts of Aśoka, the Mauryan emperor sent emissaries to various countries west of India to spread "Dharma", particularly in eastern provinces of the neighbouring Seleucid Empire, and even farther to Hellenistic kingdoms of the Mediterranean. It is a matter of disagreement among scholars whether or not these emissaries were accompanied by Buddhist missionaries.[409]
In central and west Asia, Buddhist influence grew, through Greek-speaking Buddhist monarchs and ancient Asian trade routes, a phenomenon known as Greco-Buddhism. An example of this is evidenced in Chinese and Pali Buddhist records, such as Milindapanha and the Greco-Buddhist art of Gandhāra. The Milindapanha describes a conversation between a Buddhist monk and the 2nd-century BCE Greek king Menander, after which Menander abdicates and himself goes into monastic life in the pursuit of nirvana.[410][411] Some scholars have questioned the Milindapanha version, expressing doubts whether Menander was Buddhist or just favourably disposed to Buddhist monks.[412]
The Kushan empire (30–375 CE) came to control the Silk Road trade through Central and South Asia, which brought them to interact with Gandharan Buddhism and the Buddhist institutions of these regions. The Kushans patronised Buddhism throughout their lands, and many Buddhist centers were built or renovated (the Sarvastivada school was particularly favored), especially by Emperor Kanishka (128–151 CE).[413][414] Kushan support helped Buddhism to expand into a world religion through their trade routes.[415] Buddhism spread to Khotan, the Tarim Basin, and China, eventually to other parts of the far east.[414] Some of the earliest written documents of the Buddhist faith are the Gandharan Buddhist texts, dating from about the 1st century CE, and connected to the Dharmaguptaka school.[416][417][418]
The Islamic conquest of the Iranian Plateau in the 7th-century, followed by the Muslim conquests of Afghanistan and the later establishment of the Ghaznavid kingdom with Islam as the state religion in Central Asia between the 10th- and 12th-century led to the decline and disappearance of Buddhism from most of these regions.[419]
The origins of Mahāyāna ("Great Vehicle") Buddhism are not well understood and there are various competing theories about how and where this movement arose. Theories include the idea that it began as various groups venerating certain texts or that it arose as a strict forest ascetic movement.[420]
The first Mahāyāna works were written sometime between the 1st century BCE and the 2nd century CE.[339][420] Much of the early extant evidence for the origins of Mahāyāna comes from early Chinese translations of Mahāyāna texts, mainly those of Lokakṣema. (2nd century CE).[note 30] Some scholars have traditionally considered the earliest Mahāyāna sūtras to include the first versions of the Prajnaparamita series, along with texts concerning Akṣobhya, which were probably composed in the 1st century BCE in the south of India.[422][note 31]
There is no evidence that Mahāyāna ever referred to a separate formal school or sect of Buddhism, with a separate monastic code (Vinaya), but rather that it existed as a certain set of ideals, and later doctrines, for bodhisattvas.[424][425] Records written by Chinese monks visiting India indicate that both Mahāyāna and non-Mahāyāna monks could be found in the same monasteries, with the difference that Mahāyāna monks worshipped figures of Bodhisattvas, while non-Mahayana monks did not.[426]
Mahāyāna initially seems to have remained a small minority movement that was in tension with other Buddhist groups, struggling for wider acceptance.[427] However, during the fifth and sixth centuries CE, there seems to have been a rapid growth of Mahāyāna Buddhism, which is shown by a large increase in epigraphic and manuscript evidence in this period. However, it still remained a minority in comparison to other Buddhist schools.[428]
Mahāyāna Buddhist institutions continued to grow in influence during the following centuries, with large monastic university complexes such as Nalanda (established by the 5th-century CE Gupta emperor, Kumaragupta I) and Vikramashila (established under Dharmapala c. 783 to 820) becoming quite powerful and influential. During this period of Late Mahāyāna, four major types of thought developed: Mādhyamaka, Yogācāra, Buddha-nature (Tathāgatagarbha), and the epistemological tradition of Dignaga and Dharmakirti.[429] According to Dan Lusthaus, Mādhyamaka and Yogācāra have a great deal in common, and the commonality stems from early Buddhism.[430]
During the Gupta period (4th–6th centuries) and the empire of Harṣavardana (c. 590–647 CE), Buddhism continued to be influential in India, and large Buddhist learning institutions such as Nalanda and Valabahi Universities were at their peak.[431] Buddhism also flourished under the support of the Pāla Empire (8th–12th centuries). Under the Guptas and Palas, Tantric Buddhism or Vajrayana developed and rose to prominence. It promoted new practices such as the use of mantras, dharanis, mudras, mandalas and the visualization of deities and Buddhas and developed a new class of literature, the Buddhist Tantras. This new esoteric form of Buddhism can be traced back to groups of wandering yogi magicians called mahasiddhas.[432][433]
The question of the origins of early Vajrayana has been taken up by various scholars. David Seyfort Ruegg has suggested that Buddhist tantra employed various elements of a "pan-Indian religious substrate" which is not specifically Buddhist, Shaiva or Vaishnava.[434]
According to Indologist Alexis Sanderson, various classes of Vajrayana literature developed as a result of royal courts sponsoring both Buddhism and Saivism. Sanderson has argued that Buddhist tantras can be shown to have borrowed practices, terms, rituals and more form Shaiva tantras. He argues that Buddhist texts even directly copied various Shaiva tantras, especially the Bhairava Vidyapitha tantras.[435][436] Ronald M. Davidson meanwhile, argues that Sanderson's claims for direct influence from Shaiva Vidyapitha texts are problematic because "the chronology of the Vidyapitha tantras is by no means so well established"[437] and that the Shaiva tradition also appropriated non-Hindu deities, texts and traditions. Thus while "there can be no question that the Buddhist tantras were heavily influenced by Kapalika and other Saiva movements" argues Davidson, "the influence was apparently mutual."[438]
Already during this later era, Buddhism was losing state support in other regions of India, including the lands of the Karkotas, the Pratiharas, the Rashtrakutas, the Pandyas and the Pallavas. This loss of support in favor of Hindu faiths like Vaishnavism and Shaivism, is the beginning of the long and complex period of the Decline of Buddhism in the Indian subcontinent.[439] The Islamic invasions and conquest of India (10th to 12th century), further damaged and destroyed many Buddhist institutions, leading to its eventual near disappearance from India by the 1200s.[440]
The Silk Road transmission of Buddhism to China is most commonly thought to have started in the late 2nd or the 1st century CE, though the literary sources are all open to question.[441][note 32] The first documented translation efforts by foreign Buddhist monks in China were in the 2nd century CE, probably as a consequence of the expansion of the Kushan Empire into the Chinese territory of the Tarim Basin.[443]
The first documented Buddhist texts translated into Chinese are those of the Parthian An Shigao (148–180 CE).[444] The first known Mahāyāna scriptural texts are translations into Chinese by the Kushan monk Lokakṣema in Luoyang, between 178 and 189 CE.[445] From China, Buddhism was introduced into its neighbours Korea (4th century), Japan (6th–7th centuries), and Vietnam (c. 1st–2nd centuries).[446][447]
During the Chinese Tang dynasty (618–907), Chinese Esoteric Buddhism was introduced from India and Chan Buddhism (Zen) became a major religion.[448][449] Chan continued to grow in the Song dynasty (960–1279) and it was during this era that it strongly influenced Korean Buddhism and Japanese Buddhism.[450] Pure Land Buddhism also became popular during this period and was often practised together with Chan.[451] It was also during the Song that the entire Chinese canon was printed using over 130,000 wooden printing blocks.[452]
During the Indian period of Esoteric Buddhism (from the 8th century onwards), Buddhism spread from India to Tibet and Mongolia. Johannes Bronkhorst states that the esoteric form was attractive because it allowed both a secluded monastic community as well as the social rites and rituals important to laypersons and to kings for the maintenance of a political state during succession and wars to resist invasion.[453] During the Middle Ages, Buddhism slowly declined in India,[454] while it vanished from Persia and Central Asia as Islam became the state religion.[455][456]
The Theravada school arrived in Sri Lanka sometime in the 3rd century BCE. Sri Lanka became a base for its later spread to Southeast Asia after the 5th century CE (Myanmar, Malaysia, Indonesia, Thailand, Cambodia and coastal Vietnam).[457][458] Theravada Buddhism was the dominant religion in Burma during the Mon Hanthawaddy Kingdom (1287–1552).[459] It also became dominant in the Khmer Empire during the 13th and 14th centuries and in the Thai Sukhothai Kingdom during the reign of Ram Khamhaeng (1237/1247–1298).[460][461]
Buddhists generally classify themselves as either Theravāda or Mahāyāna.[462] This classification is also used by some scholars[463] and is the one ordinarily used in the English language.[web 6] An alternative scheme used by some scholars divides Buddhism into the following three traditions or geographical or cultural areas: Theravāda (or "Southern Buddhism", "South Asian Buddhism"), East Asian Buddhism (or just "Eastern Buddhism") and Indo-Tibetan Buddhism (or "Northern Buddhism").[note 33]
Some scholars[note 34] use other schemes. Buddhists themselves have a variety of other schemes. Hinayana (literally "lesser or inferior vehicle") is sometimes used by Mahāyāna followers to name the family of early philosophical schools and traditions from which contemporary Theravāda emerged, but as the Hinayana term is considered derogatory, a variety of other terms are used instead, including: Śrāvakayāna, Nikaya Buddhism, early Buddhist schools, sectarian Buddhism and conservative Buddhism.[464][465]
Not all traditions of Buddhism share the same philosophical outlook, or treat the same concepts as central. Each tradition, however, does have its own core concepts, and some comparisons can be drawn between them:[466][467]
Buddhist institutions are often housed and centered around monasteries (Sanskrit:viharas) and temples. Buddhist monastics originally followed a life of wandering, never staying in one place for long. During the three month rainy season (vassa) they would gather together in one place for a period of intense practice and then depart again.[472][473] Some of the earliest Buddhist monasteries were at groves (vanas) or woods (araññas), such as Jetavana and Sarnath's Deer Park. There originally seems to have been two main types of monasteries, monastic settlements (sangharamas) were built and supported by donors, and woodland camps (avasas) were set up by monks. Whatever structures were built in these locales were made out of wood and were sometimes temporary structures built for the rainy season.[474][475] Over time, the wandering community slowly adopted more settled cenobitic forms of monasticism.[476] 
There are many different forms of Buddhist structures. Classic Indian Buddhist institutions mainly made use of the following structures: monasteries, rock-hewn cave complexes (such as the Ajanta Caves), stupas (funerary mounds which contained relics), and temples such as the Mahabodhi Temple.[477] In Southeast Asia, the most widespread institutions are centered on wats. East Asian Buddhist institutions also use various structures including monastic halls, temples, lecture halls, bell towers and pagodas. In Japanese Buddhist temples, these different structures are usually grouped together in an area termed the garan. In Indo-Tibetan Buddhism, Buddhist institutions are generally housed in gompas. They include monastic quarters, stupas and prayer halls with Buddha images. In the modern era, the Buddhist "meditation centre", which is mostly used by laypersons and often also staffed by them, has also become widespread.[478]
Buddhism has faced various challenges and changes during the colonisation of Buddhist states by Christian countries and its persecution under modern states. Like other religions, the findings of modern science has challenged its basic premises. One response to some of these challenges has come to be called Buddhist modernism. Early Buddhist modernist figures such as the American convert Henry Olcott (1832–1907) and Anagarika Dharmapala  (1864–1933) reinterpreted and promoted Buddhism as a scientific and rational religion which they saw as compatible with modern science.[479]
East Asian Buddhism meanwhile suffered under various wars which ravaged China during the modern era, such as the Taiping rebellion and World War II (which also affected Korean Buddhism). During the Republican period (1912–49), a new movement called Humanistic Buddhism was developed by figures such as Taixu (1899–1947), and though Buddhist institutions were destroyed during the Cultural Revolution (1966–76), there has been a revival of the religion in China after 1977.[480] Japanese Buddhism also went through a period of modernisation during the Meiji period.[481] In Central Asia meanwhile, the arrival of Communist repression to Tibet (1966–1980) and Mongolia  (between 1924 and 1990) had a strong negative impact on Buddhist institutions, though the situation has improved somewhat since the 80s and 90s.[482]
While there were some encounters of Western travellers or missionaries such as St. Francis Xavier and Ippolito Desideri with Buddhist cultures, it was not until the 19th century that Buddhism began to be studied by Western scholars. It was the work of pioneering scholars such as Eugène Burnouf, Max Müller, Hermann Oldenberg and Thomas William Rhys Davids that paved the way for modern Buddhist studies in the West. The English words such as Buddhism, "Boudhist", "Bauddhist" and Buddhist were coined in the early 19th-century in the West,[483] while in 1881, Rhys Davids founded the Pali Text Society – an influential Western resource of Buddhist literature in the Pali language and one of the earliest publisher of a journal on Buddhist studies.[484] It was also during the 19th century that Asian Buddhist immigrants (mainly from China and Japan) began to arrive in Western countries such as the United States and Canada, bringing with them their Buddhist religion. This period also saw the first Westerners to formally convert to Buddhism, such as Helena Blavatsky and Henry Steel Olcott.[485] An important event in the introduction of Buddhism to the West was the 1893 World Parliament of Religions, which for the first time saw well-publicized speeches by major Buddhist leaders alongside other religious leaders.
The 20th century saw a prolific growth of new Buddhist institutions in Western countries, including the Buddhist Society, London (1924), Das Buddhistische Haus (1924) and Datsan Gunzechoinei in St Petersburg. The publication and translations of Buddhist literature in Western languages thereafter accelerated. After the second world war, further immigration from Asia, globalisation, the secularisation on Western culture as well a renewed interest in Buddhism among the 60s counterculture led to further growth in Buddhist institutions.[486] Influential figures on post-war Western Buddhism include Shunryu Suzuki, Jack Kerouac, Alan Watts, Thích Nhất Hạnh, and the 14th Dalai Lama. While Buddhist institutions have grown, some of the central premises of Buddhism such as the cycles of rebirth and Four Noble Truths have been problematic in the West.[487][488][489] In contrast, states Christopher Gowans, for "most ordinary [Asian] Buddhists, today as well as in the past, their basic moral orientation is governed by belief in karma and rebirth".[490] Most Asian Buddhist laypersons, states Kevin Trainor, have historically pursued Buddhist rituals and practices seeking better rebirth,[491] not nirvana or freedom from rebirth.[492]
Buddhism has spread across the world,[494][495] and Buddhist texts are increasingly translated into local languages. While Buddhism in the West is often seen as exotic and progressive, in the East it is regarded as familiar and traditional. In countries such as Cambodia and Bhutan, it is recognised as the state religion and receives government support.
In certain regions such as Afghanistan and Pakistan, militants have targeted violence and destruction of historic Buddhist monuments.[496][497]
A number of modern movements in Buddhism emerged during the second half of the 20th century.[498][499] These new forms of Buddhism are diverse and significantly depart from traditional beliefs and practices.[500]
In India, B.R. Ambedkar launched the Navayana tradition – literally, "new vehicle". Ambedkar's Buddhism rejects the foundational doctrines and historic practices of traditional Theravada and Mahayana traditions, such as monk lifestyle after renunciation, karma, rebirth, samsara, meditation, nirvana, Four Noble Truths and others.[501][502][503] Ambedkar's Navayana Buddhism considers these as superstitions and re-interprets the original Buddha as someone who taught about class struggle and social equality.[504][505] Ambedkar urged low caste Indian Dalits to convert to his Marxism-inspired[503] reinterpretation called the Navayana Buddhism, also known as Bhimayana Buddhism. Ambedkar's effort led to the expansion of Navayana Buddhism in India.[506][504]
The Thai King Mongkut (r. 1851–68), and his son Chulalongkorn (r. 1868–1910), were responsible for modern reforms of Thai Buddhism.[507] Modern Buddhist movements include Secular Buddhism in many countries, Won Buddhism in Korea, the Dhammakaya movement in Thailand and several Japanese organisations, such as Shinnyo-en, Risshō Kōsei Kai or Soka Gakkai.
Some of these movements have brought internal disputes and strife within regional Buddhist communities. For example, the Dhammakaya movement in Thailand teaches a "true self" doctrine, which traditional Theravada monks consider as heretically denying the fundamental anatta (not-self) doctrine of Buddhism.[508][509][510]
Buddhism has not been immune from sexual abuse and misconduct scandals, with victims coming forward in various Buddhist schools such as Zen and Tibetan.[511][512][513][514] "There are huge cover ups in the Catholic church, but what has happened within Tibetan Buddhism is totally along the same lines," says Mary Finnigan, an author and journalist who has been chronicling such alleged abuses since the mid-80s.[515] One notably covered case in media of various Western countries was that of Sogyal Rinpoche which began in 1994,[516] and ended with his retirement from his position as Rigpa's spiritual director in 2017.[517]
Buddhism has had a profound influence on various cultures, especially in Asia. Buddhist philosophy, Buddhist art, Buddhist architecture, Buddhist cuisine and Buddhist festivals continue to be influential elements of the modern Culture of Asia, especially in East Asia and the Sinosphere as well as in Southeast Asia and the Indosphere. According to Litian Fang, Buddhism has "permeated a wide range of fields, such as politics, ethics, philosophy, literature, art and customs," in these Asian regions.[518] Buddhist teachings influenced the development of modern Hinduism as well as other Asian religions like Taoism and Confucianism. Buddhist philosophers like Dignaga and Dharmakirti were very influential in the development of Indian logic and epistemology.[519] Buddhist educational institutions like Nalanda and Vikramashila preserved various disciplines of classical Indian knowledge such as grammar, astronomy/astrology and medicine and taught foreign students from Asia.[520]
In the Western world, Buddhism has had a strong influence on modern New Age spirituality and other alternative spiritualities. This began with its influence on 20th century Theosophists such as Helena Blavatsky, which were some of the first Westerners to take Buddhism seriously as a spiritual tradition.[521] More recently, Buddhist meditation practices have influenced the development of modern psychology, particularly the practice of Mindfulness-based stress reduction (MBSR) and other similar mindfulness based modalities.[522][523] The influence of Buddhism on psychology can also be seen in certain forms of modern psychoanalysis.[524][525]
Shamanism is a widespread practice in some Buddhist societies. Buddhist monasteries have long existed alongside local shamanic traditions. Lacking an institutional orthodoxy, Buddhists adapted to the local cultures, blending their own traditions with pre-existing shamanic culture. Research into Himalayan religion has shown that Buddhist and shamanic traditions overlap in many respects: the worship of localized deities, healing rituals and exorcisms. The shamanic Gurung people have adopted some of the Buddhist beliefs such and rebirth but maintain the shamanic rites of "guiding the soul" after death.
Buddhism is practised by an estimated 488 million,[7] 495 million,[526] or 535 million[527] people as of the 2010s, representing 7% to 8% of the world's total population. China is the country with the largest population of Buddhists, approximately 244 million or 18% of its total population.[7][note 35] They are mostly followers of Chinese schools of Mahayana, making this the largest body of Buddhist traditions. Mahayana, also practised in broader East Asia, is followed by over half of world Buddhists.[7]
Buddhism is the dominant religion in Thailand, Cambodia, Tibet, Myanmar, Sri Lanka, Bhutan, Laos, Mongolia, Japan,[529] Hong Kong,[530] Macau,[531] Singapore,[532] and Vietnam. [533] Large Buddhist populations live in Mainland China, Taiwan, North Korea, Nepal and South Korea.[534] The Indian state of Maharashtra account for 77% of all Buddhists in India.[535] In Russia, Buddhists form majority in Tuva (52%) and Kalmykia (53%). Buryatia (20%) and Zabaykalsky Krai (15%) also have significant Buddhist populations.[536]
Buddhism is also growing by conversion. In India, more than  85% of the total Buddhists have converted from Hinduism to Buddhism,[537][538] and they are called neo-Buddhists or Ambedkarite Buddhists.[537][538] In New Zealand, about 25–35% of the total Buddhists are converts to Buddhism.[539][540] Buddhism has also spread to the Nordic countries; for example, the Burmese Buddhists founded in the city of Kuopio in North Savonia the first Buddhist monastery of Finland, named the Buddha Dhamma Ramsi monastery.[541]



A man is an adult male human.[1][2] Prior to adulthood, a male human is referred to as a boy (a male child or adolescent).
Like most other male mammals, a man's genome usually inherits an X chromosome from the mother and a Y chromosome from the father. Sex differentiation of the male fetus is governed by the SRY gene on the Y chromosome. During puberty, hormones which stimulate androgen production result in the development of secondary sexual characteristics, thus exhibiting greater differences between the sexes. These include greater muscle mass, the growth of facial hair and a lower body fat composition. Male anatomy is distinguished from female anatomy by the male reproductive system, which includes the penis, testicles, sperm duct, prostate gland and the epididymis, and by secondary sex characteristics, including a narrower pelvis, narrower hips, and smaller breasts.
Throughout human history, traditional gender roles have often defined and limited men's activities and opportunities. Men often face conscription into military service or are directed into professions with high mortality rates. Many religious doctrines stipulate certain rules for men, such as religious circumcision. Men are over-represented as both perpetrators and victims of violence.
Trans men have a gender identity that does not align with their female sex assignment at birth, while intersex men may have sex characteristics that do not fit typical notions of male biology.
The English term "man" is derived from the Proto-Indo-European root *man- (see Sanskrit/Avestan manu-, Slavic mǫž "man, male").[3] More directly, the word derives from Old English mann. The Old English form primarily meant "person" or "human being" and referred to men, women, and children alike. The Old English word for "man" as distinct from "woman" or "child" was wer. Mann only came to mean "man" in Middle English, replacing wer, which survives today only in the compounds "werewolf" (from Old English werwulf, literally "man-wolf"), and "wergild", literally "man-payment".[4][5][6]
In humans, sperm cells carry either an X or a Y sex chromosome. If a sperm cell carrying a Y chromosome fertilizes the female ovum, the offspring will have a male karyotype (XY). The SRY gene is typically found on the Y chromosome and causes the development of the testes, which in turn govern other aspects of male sex differentiation. Sex differentiation in males proceeds in a testes-dependent way while female differentiation is not gonad dependent.[7]
Primary sex characteristics (or sex organs) are characteristics that are present at birth and are integral to the reproductive process. For men, primary sex characteristics include the penis and testicles.
Adult humans exhibit sexual dimorphism in many other characteristics, many of which have no direct link to reproductive ability. Humans are sexually dimorphic in body size, body structure, and body composition.  Men tend to be taller and heavier than women, and adjusted for height, men tend to have greater lean and bone mass than women, and lower fat mass.[8] 
Secondary sex characteristics are features that appear during puberty in humans.[9][10] Such features are especially evident in the sexually dimorphic phenotypic traits that distinguish between the sexes, but—unlike the primary sex characteristics—are not directly part of the reproductive system.[11][12][13] Secondary sexual characteristics that are specific to men include:
Men weigh more than women.[15] On average, men are taller than women by about 10%.[15] On average, men have a larger waist in comparison to their hips (see waist–hip ratio) than women. In women, the index and ring fingers tend to be either more similar in size or their index finger is slightly longer than their ring finger, whereas men's ring finger tends to be longer.[16]
The male reproductive system includes external and internal genitalia. The male external genitalia consist of the penis, the male urethra, and the scrotum, while the male internal genitalia consist of the testes, the prostate, the epididymis, the seminal vesicle, the vas deferens, the ejaculatory duct, and the bulbourethral gland.[17]
The male reproductive system's function is to produce semen, which carries sperm and thus genetic information that can unite with an egg within a woman. Since sperm that enters a woman's uterus and then fallopian tubes goes on to fertilize an egg which develops into a fetus or child, the male reproductive system plays no necessary role during the gestation. The study of male reproduction and associated organs is called andrology.[18]
Testosterone stimulates the development of the Wolffian ducts, the penis, and closure of the labioscrotal folds into the scrotum. Another significant hormone in sexual differentiation is the anti-Müllerian hormone, which inhibits the development of the Müllerian ducts. For males during puberty, testosterone, along with gonadotropins released by the pituitary gland, stimulates spermatogenesis.[19]
While a majority of the global health gender disparities is weighted against women, there are situations in which men tend to fare poorer. One such instance is armed conflicts, where men are often the immediate victims. A study of conflicts in 13 countries from 1955 to 2002 found that 81% of all violent war deaths were male.[20] Apart from armed conflicts, areas with high incidence of violence, such as regions controlled by drug cartels, also see men experiencing higher mortality rates.[21] This stems from social beliefs that associate ideals of masculinity with aggressive, confrontational behavior.[22] Lastly, sudden and drastic changes in economic environments and the loss of social safety nets, in particular social subsidies and food stamps, have also been linked to higher levels of alcohol consumption and psychological stress among men, leading to a spike in male mortality rates. This is because such situations often makes it harder for men to provide for their family, a task that has been long regarded as the "essence of masculinity."[23] A retrospective analyses of people infected with the common cold found that doctors underrate the symptoms of men, and are more willing to attribute symptoms and illness to women than men.[24] Women live longer than men in all countries, and across all age groups, for which reliable records exist.[25] In the United States, men are less healthy than women across all social classes. Non-white men are especially unhealthy. Men are over-represented in dangerous occupations and represent a majority of on the job deaths. Further, medical doctors provide men with less service, less advice, and spend less time with men than they do with women per medical encounter.[26]
Male sexuality and attraction are variable, and a man's sexual behavior can be affected by many factors, including evolved predispositions, personality, upbringing, and culture. While the majority of men are heterosexual, significant minorities are homosexual or bisexual.[27]
Most cultures use a gender binary in which man is one of the two genders, the other being woman.[28][29][30]
Trans men have a male gender identity that does not align with their female sex assignment at birth and may undergo masculinizing hormone replacement therapy and/or sex reassignment surgery,[31] while intersex men may have sex characteristics that do not fit typical notions of male biology.[32] A 2016 systemic review estimated that 0.256% of people self-identify as female-to-male transgender.[33] A 2017 survey of 80,929 Minnesota students found that roughly twice as many female-assigned adolescents self-identified as transgender, compared to adolescents with a male sex assignment.[34]
Masculinity (also sometimes called manhood or manliness) is the set of personality traits and attributes associated with boys and men. Although masculinity is socially constructed,[35] some research indicates that some behaviors considered masculine are biologically influenced.[36] To what extent masculinity is biologically or socially influenced is subject to debate.[36] It is distinct from the definition of the biological male sex, as both males and females can exhibit masculine traits.[37] Men generally face social stigma for embodying feminine traits, more so than women do for embodying masculine traits.[38] This can also manifest as homophobia.[39]
Standards of manliness or masculinity vary across different cultures and historical periods.[40] While the outward signs of masculinity look different in different cultures, there are some common aspects to its definition across cultures. In all cultures in the past, and still among traditional and non-Western cultures, getting married is the most common and definitive distinction between boyhood and manhood.[41] In the late 20th century, some qualities traditionally associated with marriage (such as the "triple Ps" of protecting, providing, and procreating) were still considered signs of having achieved manhood.[41][42]
Platonic relationships are not significantly different between men and women, though some differences do exist. Friendships involving men tend to be based more on shared activities than self-disclosure and personal connection. Perceptions of friendship involving men varies among cultures and time periods.[43] In heterosexual romantic relationships, men are typically expected to take a proactive role, initiate the relationship, plan dates, and propose marriage.[44]
Anthropology has shown that masculinity itself has social status, just like wealth, race and social class. In Western culture, for example, greater masculinity usually brings greater social status.[citation needed] Many English words such as virtue and virile (from the Indo-European root vir meaning man) reflect this.[45][46] In most cultures, male privilege allows men more rights and privileges than women. In societies where men are not given special legal privileges, they typically hold more positions of power, and men are seen as being taken more seriously in society.[47] This is associated with a "gender-role strain" in which men face increased societal pressure to conform to gender roles.[48]
The earliest known recorded name of a man in writing is potentially Kushim, who would have lived sometime between 3400 and 3000 BC in the Sumerian city of Uruk; though his name may have been a title rather than his actual name.[49] The earliest confirmed names are that of Gal-Sal and his two slaves named En-pap X and Sukkalgir, from c. 3100 BC.[50]
Men may have children, whether biological or adopted; such men are called fathers. The role of men in the family has shifted considerably in the 20th and 21st centuries, taking on a more active role in raising children in most societies.[51][52][53][54] Men would traditionally marry a woman when raising children, but in modern times many countries now allow for same-sex marriage, and for those couples to raise children either via adoption or surrogacy. Men may be single parents, and are increasingly so in modern times, though women are three times more likely to be single parents than men.[55] In paternal societies, men have typically have been regarded as the "head of household" and held additional social privileges.[56]
The men's rights movement claims men face disadvantages when claiming child custody, however, empirical research does not support the notion of a judicial bias against men.[57] Mothers do have custody the majority of the time, but fathers do not seek custody the majority of the time, and custody is settled out of court.[58][59]
Men have traditionally held jobs that were not available to women. Such jobs tended to be either more strenuous, more prestigious, or more dangerous. Modern men increasingly take untraditional career paths, such as staying home and raising children while their partner works.[60] Modern men tend to work longer than women, which impacts their ability to spend time with their families.[61] Even in modern times, some jobs remain available only to men, such as military service.[62] Conscription is overwhelmingly sexist, currently only ten countries include women in their conscription programs.[63][64] Men continue to hold more dangerous jobs than women, even in developed countries. In the United States in 2020, ten times as many men died on the job as women, and a man was ten times more likely to die on the job than a woman.[65]
Media portrayals of men often replicate traditional understanding of masculinity. Men are portrayed more frequently in television than women and most commonly appear as leads in action and drama programming. Men are typically more active in television programming than women and typically hold more power and status. Due to their prominence, men are more likely to be both the objects and instigators of humorous or disparaging content. Fathers are often portrayed in television as either idealized and caring or clumsy and inept. In advertising, men are disproportionately featured in advertisements for alcohol, vehicles, and business products.[66]
Men's clothing typically encompasses a range of garments designed for various occasions, seasons, and styles. Fundamental items of a man's wardrobe include shirts, trousers, suits, and jackets, which are designed to provide both comfort and style while prioritizing functionality. Men's fashion also encompasses more casual garments such as t-shirts, sweatshirts, jeans, shorts, and swimwear, which are typically intended for informal settings. Cultural and regional traditions often influence men's fashion, resulting in diverse styles and garments that reflect the unique characteristics of different parts of the world.[67]
Men traditionally received more education than women as a result of single-sex education. Universal education, meaning state-provided primary and secondary education independent of gender, is not yet a global norm, even if it is assumed in most developed countries.[68][69] In the 21st century, the balance has shifted in many developed nations, and men now lag behind women in education.[70] 
Men are more likely than women to be faculty at universities.[71]
In 2020, 90% of the world's men were literate, compared to 87% of women. But sub-Saharan Africa, and southwest Asia lagged behind the rest of the world; only 72% of men in sub-Saharan Africa were literate.[72]
In most societies, men have more legal and cultural rights than women,[47] and misogyny is far more prevalent than misandry in society.[73][74] Men typically receive less support after being victims of sexual assault, and rape of males is stigmatized.[75] Domestic violence against men is similarly stigmatized.[76] Opponents of circumcision describe it as a human rights violation.[77] The fathers' rights movement seeks to support separated fathers that do not receive equal rights to care for their children.[78] The men's movement is the response to issues faced by men in Western countries. It includes pro-feminist groups such as the men's liberation movement and anti-feminist groups such as the manosphere and the men's rights movement.
The Mars symbol (♂) is a common symbol that represents the male sex.[79] The symbol is identical to the planetary symbol of Mars.[80] It was first used to denote sex by Carl Linnaeus in 1751.[81] The symbol is sometimes seen as a stylized representation of the shield and spear of the Roman god Mars. According to Stearn, however, this derivation is "fanciful" and all the historical evidence favours "the conclusion of the French classical scholar Claude de Saumaise" that it is derived from θρ, the contraction of a Greek name for the planet Mars, which is Thouros.[82]



Socialism is a political philosophy and movement encompassing a wide range of economic and social systems[1] which are characterised by social ownership of the means of production,[2][3][4] as opposed to private ownership.[5][6][4] As a term, it describes the economic, political, and social theories and movements associated with the implementation of such systems.[7] Social ownership can be public, community, collective, cooperative,[8][9][10] or employee.[11][12] While no single definition encapsulates the many types of socialism,[13] social ownership is the one common element,[6][14] and is considered left-wing.[15] Different types of socialism vary based on the role of markets and planning in resource allocation, on the structure of management in organizations, and from below or from above approaches, with some socialists favouring a party, state, or technocratic-driven approach. Socialists disagree on whether government, particularly existing government, is the correct vehicle for change.[16][17]
Socialist systems are divided into non-market and market forms.[18] Non-market socialism substitutes factor markets and often money with integrated economic planning and engineering or technical criteria based on calculation performed in-kind, thereby producing a different economic mechanism that functions according to different economic laws and dynamics than those of capitalism.[19] A non-market socialist system seeks to eliminate the perceived inefficiencies, irrationalities, unpredictability, and crises that socialists traditionally associate with capital accumulation and the profit system in capitalism.[20] By contrast, market socialism retains the use of monetary prices, factor markets and in some cases the profit motive, with respect to the operation of socially owned enterprises and the allocation of capital goods between them. Profits generated by these firms would be controlled directly by the workforce of each firm or accrue to society at large in the form of a social dividend.[21][22][23] Anarchism and libertarian socialism oppose the use of the state as a means to establish socialism, favouring decentralisation above all, whether to establish non-market socialism or market socialism.[24][25] Socialist parties and ideas remain a political force with varying degrees of power and influence on all continents, heading national governments in many countries around the world.
Socialist politics have been both internationalist and nationalist; organised through political parties and opposed to party politics; at times overlapping with trade unions and at other times independent and critical of them, and present in both industrialised and developing nations.[26] Social democracy originated within the socialist movement,[27] supporting economic and social interventions to promote social justice.[28][29] While retaining socialism as a long-term goal,[30] since the post-war period it came to embrace a mixed economy based on Keynesianism within a predominantly developed capitalist market economy and liberal democratic polity that expands state intervention to include income redistribution, regulation, and a welfare state.[31] Economic democracy proposes a sort of market socialism, with more democratic control of companies, currencies, investments, and natural resources.[32]
The socialist political movement includes a set of political philosophies that originated in the revolutionary movements of the mid-to-late 18th century and out of concern for the social problems that were associated with capitalism.[13] By the late 19th century, after the work of Karl Marx and his collaborator Friedrich Engels, socialism had come to signify anti-capitalism and advocacy for a post-capitalist system based on some form of social ownership of the means of production.[33][34] By the early 1920s, communism and social democracy had become the two dominant political tendencies within the international socialist movement,[35] with socialism itself becoming the most influential secular movement of the 20th century.[36] Many socialists also adopted the causes of other social movements, such as feminism, environmentalism, and progressivism.[37]
While the emergence of the Soviet Union as the world's first nominally socialist state led to socialism's widespread association with the Soviet economic model, several scholars posit that in practice, the model functioned as a form of state capitalism.[38][39][40] Several academics, political commentators, and scholars have distinguished between authoritarian socialist and democratic socialist states, with the first representing the Eastern Bloc and the latter representing Western Bloc countries which have been democratically governed by socialist parties such as Britain, France, Sweden, and Western countries in general, among others.[41] Following the end of the Cold War, many of these countries have moved away from socialism as a neoliberal consensus replaced the social democratic consensus in the advanced capitalist world.[42]
For Andrew Vincent, "[t]he word 'socialism' finds its root in the Latin sociare, which means to combine or to share. The related, more technical term in Roman and then medieval law was societas. This latter word could mean companionship and fellowship as well as the more legalistic idea of a consensual contract between freemen".[43]
Initial use of socialism was claimed by Pierre Leroux, who alleged he first used the term in the Parisian journal Le Globe in 1832.[44][45] Leroux was a follower of Henri de Saint-Simon, one of the founders of what would later be labelled utopian socialism. Socialism contrasted with the liberal doctrine of individualism that emphasized the moral worth of the individual whilst stressing that people act or should act as if they are in isolation from one another. The original utopian socialists condemned this doctrine of individualism for failing to address social concerns during the Industrial Revolution, including poverty, oppression, and vast wealth inequality. They viewed their society as harming community life by basing society on competition. They presented socialism as an alternative to liberal individualism based on the shared ownership of resources.[46] Saint-Simon proposed economic planning, scientific administration and the application of scientific understanding to the organisation of society. By contrast, Robert Owen proposed to organise production and ownership via cooperatives.[46][47] Socialism is also attributed in France to Marie Roch Louis Reybaud while in Britain it is attributed to Owen, who became one of the fathers of the cooperative movement.[48][49]
The definition and usage of socialism settled by the 1860s, replacing associationist, co-operative, and mutualist that had been used as synonyms while communism fell out of use during this period.[50] An early distinction between communism and socialism was that the latter aimed to only socialise production while the former aimed to socialise both production and consumption (in the form of free access to final goods).[51] By 1888, Marxists employed socialism in place of communism as the latter had come to be considered an old-fashioned synonym for socialism. It was not until after the Bolshevik Revolution that socialism was appropriated by Vladimir Lenin to mean a stage between capitalism and communism. He used it to defend the Bolshevik program from Marxist criticism that Russia's productive forces were not sufficiently developed for communism.[52] The distinction between communism and socialism became salient in 1918 after the Russian Social Democratic Labour Party renamed itself to the All-Russian Communist Party, interpreting communism specifically to mean socialists who supported the politics and theories of Bolshevism, Leninism and later that of Marxism–Leninism,[53] although communist parties continued to describe themselves as socialists dedicated to socialism.[54] According to The Oxford Handbook of Karl Marx, "Marx used many terms to refer to a post-capitalist society—positive humanism, socialism, Communism, realm of free individuality, free association of producers, etc. He used these terms completely interchangeably. The notion that 'socialism' and 'Communism' are distinct historical stages is alien to his work and only entered the lexicon of Marxism after his death".[55]
In Christian Europe, communists were believed to have adopted atheism. In Protestant England, communism was too close to the Roman Catholic communion rite, hence socialist was the preferred term.[56] Engels wrote that in 1848, when The Communist Manifesto was published, socialism was respectable in Europe while communism was not. The Owenites in England and the Fourierists in France were considered respectable socialists while working-class movements that "proclaimed the necessity of total social change" denoted themselves communists.[57] This branch of socialism produced the communist work of Étienne Cabet in France and Wilhelm Weitling in Germany.[58] British moral philosopher John Stuart Mill discussed a form of economic socialism within a liberal context that would later be known as liberal socialism. In later editions of his Principles of Political Economy (1848), Mill posited that "as far as economic theory was concerned, there is nothing in principle in economic theory that precludes an economic order based on socialist policies"[59][60] and promoted substituting capitalist businesses with worker cooperatives.[61] While democrats looked to the Revolutions of 1848 as a democratic revolution which in the long run ensured liberty, equality, and fraternity, Marxists denounced it as a betrayal of working-class ideals by a bourgeoisie indifferent to the proletariat.[62]
Socialist models and ideas espousing common or public ownership have existed since antiquity. The economy of the 3rd century BCE Mauryan Empire of India, an absolute monarchy, has been described by some scholars as "a socialized monarchy" and "a sort of state socialism" due to "nationalisation of industries".[63][64] Other scholars have suggested that elements of socialist thought were present in the politics of classical Greek philosophers Plato[65] and Aristotle.[66] Mazdak the Younger (died c. 524 or 528 CE), a Persian communal proto-socialist,[67] instituted communal possessions and advocated the public good. Abu Dharr al-Ghifari, a Companion of Muhammad, is credited by multiple authors as a principal antecedent of Islamic socialism.[68][69] The teachings of Jesus are frequently described as socialist, especially by Christian socialists.[70] However, both socialists and non-socialists have disputed claims of Jesus being a socialist.[71][72] In the Bible, Acts 4:32 records that in the early church in Jerusalem "[n]o one claimed that any of their possessions was their own",[73] although the pattern soon disappears from church history except within monasticism. Christian socialism was one of the founding threads of the British Labour Party and is claimed to begin with the uprising of Wat Tyler and John Ball in the 14th century CE.[74] After the French Revolution, activists and theorists such as François-Noël Babeuf, Étienne-Gabriel Morelly, Philippe Buonarroti and Auguste Blanqui influenced the early French labour and socialist movements.[75] In Britain, Thomas Paine proposed a detailed plan to tax property owners to pay for the needs of the poor in Agrarian Justice[76] while Charles Hall wrote The Effects of Civilization on the People in European States, denouncing capitalism's effects on the poor of his time.[77] This work influenced the utopian schemes of Thomas Spence.[78]
The first self-conscious socialist movements developed in the 1820s and 1830s. Groups such as the Fourierists, Owenites and Saint-Simonians provided a series of analyses and interpretations of society. Especially the Owenites overlapped with other working-class movements such as the Chartists in the United Kingdom.[79] This was also the first time that the term socialism itself applies in a fashion recognisably similar to its modern meaning; the word was first used in 1827 in the London Cooperative Magazine in the UK and later in 1832 in the French periodical Le Globe.[80] An earlier usage of the word socialism appears in the Italian language in 1803, but not with the modern meaning of the term.[81]
The Chartists gathered significant numbers around the People's Charter of 1838 which sought democratic reforms focused on the extension of suffrage to all male adults. Leaders in the movement called for a more equitable distribution of income and better living conditions for the working classes. The first trade unions and consumer cooperative societies followed the Chartist movement.[82] Pierre-Joseph Proudhon proposed his philosophy of mutualism in which "everyone had an equal claim, either alone or as part of a small cooperative, to possess and use land and other resources as needed to make a living".[83] Other currents inspired Christian socialism "often in Britain and then usually coming out of left liberal politics and a romantic anti-industrialism"[75] which produced theorists such as Edward Bellamy, Charles Kingsley and Frederick Denison Maurice.[84]
The first advocates of socialism favoured social levelling in order to create a meritocratic or technocratic society based on individual talent.[85] Henri de Saint-Simon was fascinated by the potential of science and technology and advocated a socialist society that would eliminate the disorderly aspects of capitalism based on equal opportunities.[86][unreliable source?] He sought a society in which each person was ranked according to his or her capacities and rewarded according to his or her work.[85] His key focus was on administrative efficiency and industrialism and a belief that science was essential to progress.[87] This was accompanied by a desire for a rationally organised economy based on planning and geared towards large-scale scientific and material progress.[85]
West European social critics, including Louis Blanc, Charles Fourier, Charles Hall, Robert Owen, Pierre-Joseph Proudhon and Saint-Simon were the first modern socialists who criticised the poverty and inequality of the Industrial Revolution. They advocated reform, Owen advocating the transformation of society to small communities without private property. Owen's contribution to modern socialism was his claim that individual actions and characteristics were largely determined by their social environment.[87] On the other hand, Fourier advocated Phalanstères (communities that respected individual desires, including sexual preferences), affinities and creativity and saw that work has to be made enjoyable for people.[88] Owen and Fourier's ideas were practiced in intentional communities around Europe and North America in the mid-19th century.
The Paris Commune was a government that ruled Paris from 18 March (formally, from 28 March) to 28 May 1871. The Commune was the result of an uprising in Paris after France was defeated in the Franco-Prussian War. The Commune elections were held on 26 March. They elected a Commune council of 92 members, one member for each 20,000 residents.[89]
Because the Commune was able to meet on fewer than 60 days in total, only a few decrees were actually implemented. These included the separation of church and state; the remission of rents owed for the period of the siege (during which payment had been suspended); the abolition of night work in the hundreds of Paris bakeries; the granting of pensions to the unmarried companions and children of National Guards killed on active service; and the free return of all workmen's tools and household items valued up to 20 francs that had been pledged during the siege.[90]
In 1864, the First International was founded in London. It united diverse revolutionary currents, including socialists such as the French followers of Proudhon,[91] Blanquists, Philadelphes, English trade unionists and social democrats. In 1865 and 1866, it held a preliminary conference and had its first congress in Geneva, respectively. Due to their wide variety of philosophies, conflict immediately erupted. The first objections to Marx came from the mutualists who opposed state socialism. Shortly after Mikhail Bakunin and his followers joined in 1868, the First International became polarised into camps headed by Marx and Bakunin.[92] The clearest differences between the groups emerged over their proposed strategies for achieving their visions. The First International became the first major international forum for the promulgation of socialist ideas.
Bakunin's followers were called collectivists and sought to collectivise ownership of the means of production while retaining payment proportional to the amount and kind of labour of each individual. Like Proudhonists, they asserted the right of each individual to the product of his labour and to be remunerated for his particular contribution to production. By contrast, anarcho-communists sought collective ownership of both the means and the products of labour. As Errico Malatesta put it, "instead of running the risk of making a confusion in trying to distinguish what you and I each do, let us all work and put everything in common. In this way each will give to society all that his strength permits until enough is produced for every one; and each will take all that he needs, limiting his needs only in those things of which there is not yet plenty for every one".[93] Anarcho-communism as a coherent economic-political philosophy was first formulated in the Italian section of the First International by Malatesta, Carlo Cafiero, Emilio Covelli, Andrea Costa and other ex-Mazzinian republicans.[94] Out of respect for Bakunin, they did not make their differences with collectivist anarchism explicit until after his death.[95]
Syndicalism emerged in France inspired in part by Proudhon and later by Pelloutier and Georges Sorel.[96] It developed at the end of the 19th century out of the French trade-union movement (syndicat is the French word for trade union). It was a significant force in Italy and Spain in the early 20th century until it was crushed by the fascist regimes in those countries. In the United States, syndicalism appeared in the guise of the Industrial Workers of the World, or "Wobblies", founded in 1905.[96] Syndicalism is an economic system that organises industries into confederations (syndicates)[97] and the economy is managed by negotiation between specialists and worker representatives of each field, comprising multiple non-competitive categorised units.[98] Syndicalism is a form of communism and economic corporatism, but also refers to the political movement and tactics used to bring about this type of system. An influential anarchist movement based on syndicalist ideas is anarcho-syndicalism.[99] The International Workers Association is an international anarcho-syndicalist federation of various labour unions.
The Fabian Society is a British socialist organisation established to advance socialism via gradualist and reformist means.[100] The society laid many foundations of the Labour Party and subsequently affected the policies of states emerging from the decolonisation of the British Empire, most notably India and Singapore. Originally, the Fabian Society was committed to the establishment of a socialist economy, alongside a commitment to British imperialism as a progressive and modernising force.[101] Later, the society functioned primarily as a think tank and is one of fifteen socialist societies affiliated with the Labour Party. Similar societies exist in Australia (the Australian Fabian Society), in Canada (the Douglas-Coldwell Foundation and the now disbanded League for Social Reconstruction) and in New Zealand.
Guild socialism is a political movement advocating workers' control of industry through the medium of trade-related guilds "in an implied contractual relationship with the public".[102] It originated in the United Kingdom and was at its most influential in the first quarter of the 20th century. Inspired by medieval guilds, theorists such as Samuel George Hobson and G. D. H. Cole advocated the public ownership of industries and their workforces' organisation into guilds, each of which under the democratic control of its trade union. Guild socialists were less inclined than Fabians to invest power in a state.[96] At some point, like the American Knights of Labor, guild socialism wanted to abolish the wage system.[103]
As the ideas of Marx and Engels gained acceptance, particularly in central Europe, socialists sought to unite in an international organisation. In 1889 (the centennial of the French Revolution), the Second International was founded, with 384 delegates from twenty countries representing about 300 labour and socialist organisations.[104] Engels was elected honorary president at the third congress in 1893. Anarchists were banned, mainly due to pressure from Marxists.[105] It has been argued that at some point the Second International turned "into a battleground over the issue of libertarian versus authoritarian socialism. Not only did they effectively present themselves as champions of minority rights; they also provoked the German Marxists into demonstrating a dictatorial intolerance which was a factor in preventing the British labour movement from following the Marxist direction indicated by such leaders as H. M. Hyndman".[105]
Reformism arose as an alternative to revolution. Eduard Bernstein was a leading social democrat in Germany who proposed the concept of evolutionary socialism. Revolutionary socialists quickly targeted reformism: Rosa Luxemburg condemned Bernstein's Evolutionary Socialism in her 1900 essay Social Reform or Revolution? Revolutionary socialism encompasses multiple social and political movements that may define "revolution" differently. The Social Democratic Party of Germany (SPD) became the largest and most powerful socialist party in Europe, despite working illegally until the anti-socialist laws were dropped in 1890. In the 1893 elections, it gained 1,787,000 votes, a quarter of the total votes cast, according to Engels. In 1895, the year of his death, Engels emphasised The Communist Manifesto's emphasis on winning, as a first step, the "battle of democracy".[106]
In South America, the Socialist Party of Argentina was established in the 1890s led by Juan B. Justo and Nicolás Repetto, among others. It was the first mass party in the country and in Latin America. The party affiliated itself with the Second International.[107]
For four months in 1904, Australian Labor Party leader Chris Watson was the Prime Minister of the country. Watson thus became the head of the world's first socialist or social democratic parliamentary government.[108] Australian historian Geoffrey Blainey argues that the Labor Party was not socialist at all in the 1890s, and that socialist and collectivist elements only made their way in the party's platform in the early 20th century.[109]
In 1909, the first Kibbutz was established in Palestine[110] by Russian Jewish Immigrants. The Kibbutz Movement expanded through the 20th century following a doctrine of Zionist socialism.[111] The British Labour Party first won seats in the House of Commons in 1902.
By 1917, the patriotism of World War I changed into political radicalism in Australia, most of Europe and the United States. Other socialist parties from around the world who were beginning to gain importance in their national politics in the early 20th century included the Italian Socialist Party, the French Section of the Workers' International, the Spanish Socialist Workers' Party, the Swedish Social Democratic Party, the Russian Social Democratic Labour Party and the Socialist Party in Argentina, the Socialist Workers' Party in Chile and the Socialist Party of America in the United States.
In February 1917, a revolution occurred in Russia. Workers, soldiers and peasants established soviets (councils), the monarchy fell and a provisional government convened pending the election of a constituent assembly. In April of that year, Vladimir Lenin, leader of the Bolshevik faction of socialists in Russia and known for his profound and controversial expansions of Marxism, was allowed to cross Germany to return from exile in Switzerland.
Lenin had published essays on his analysis of imperialism, the monopoly and globalisation phase of capitalism, as well as analyses on social conditions. He observed that as capitalism had further developed in Europe and America, the workers remained unable to gain class consciousness so long as they were too busy working to pay their expenses. He therefore proposed that the social revolution would require the leadership of a vanguard party of class-conscious revolutionaries from the educated and politically active part of the population.[112]
Upon arriving in Petrograd, Lenin declared that the revolution in Russia had only begun, and that the next step was for the workers' soviets to take full authority. He issued a thesis outlining the Bolshevik programme, including rejection of any legitimacy in the provisional government and advocacy for state power to be administered through the soviets. The Bolsheviks became the most influential force. On 7 November, the capitol of the provisional government was stormed by Bolshevik Red Guards in what later was officially known in the Soviet Union as the Great October Socialist Revolution. The provisional government ended and the Russian Socialist Federative Soviet Republic—the world's first constitutionally socialist state—was established. On 25 January 1918, Lenin declared "Long live the world socialist revolution!" at the Petrograd Soviet[113] and proposed an immediate armistice on all fronts and transferred the land of the landed proprietors, the crown and the monasteries to the peasant committees without compensation.[114]
The day after assuming executive power on 25 January, Lenin wrote Draft Regulations on Workers' Control, which granted workers control of businesses with more than five workers and office employees and access to all books, documents and stocks and whose decisions were to be "binding upon the owners of the enterprises".[115] Governing through the elected soviets and in alliance with the peasant-based Left Socialist-Revolutionaries, the Bolshevik government began nationalising banks and industry; and disavowed the national debts of the deposed Romanov royal régime. It sued for peace, withdrawing from World War I and convoked a Constituent Assembly in which the peasant Socialist-Revolutionary Party (SR) won a majority.[116]
The Constituent Assembly elected SR leader Victor Chernov President of a Russian republic, but rejected the Bolshevik proposal that it endorse the Soviet decrees on land, peace and workers' control and acknowledge the power of the Soviets of Workers', Soldiers' and Peasants' Deputies. The next day, the Bolsheviks declared that the assembly was elected on outdated party lists[117] and the All-Russian Central Executive Committee of the Soviets dissolved it.[118][119] In March 1919, world communist parties formed Comintern (also known as the Third International) at a meeting in Moscow.[120]
In the interwar period, Soviet Union experienced two major famines.[undue weight?  – discuss] The First famine occurred in 1921-1922 with death estimates varying between 1 and 10 million dead. It was caused by a combination of factors - severe drought and failed harvests, continuous war since 1914, forced collectivisation of farms and requisition of grain and seed from peasants (preventing the sowing of crops) by the Soviet authorities, and an economic blockade of the Soviet Union by the Allies. The experience with the famine led Lenin to replace war communism with the New Economic Policy (NEP) in 1921 to alleviate the extreme shortages.[121] Under the NEP, private ownership was allowed for small and medium-sized enterprises. While large industry remained state-controlled.
A second major famine occurred in 1930-1933, resulting in millions of deaths.
The Soviet economy was the modern world's first centrally planned economy. It adopted state ownership of industry managed through Gosplan (the State Planning Commission), Gosbank (the State Bank) and the Gossnab (State Commission for Materials and Equipment Supply). Economic planning was conducted through serial Five-Year Plans. The emphasis was on development of heavy industry at expense of agriculture. Rapid industrialization served two purposes: to bring largely agrarian societies into the modern age, and to establish a politically loyal working class. Modernization brought about a general increase in the standard of living in the 1950s and 60's.[122]
The Bolshevik Russian Revolution of January 1918 launched Communist parties in many countries and a wave of revolutions until the mid-1920s. Few communists doubted that the Russian experience depended on successful, working-class socialist revolutions in developed capitalist countries.[123][124] In 1919, Lenin and Leon Trotsky organised the world's Communist parties into an international association of workers—the Communist International (Comintern), also called the Third International.
The Russian Revolution influenced uprisings in other countries. The German Revolution of 1918–1919 replaced Germany's imperial government with a republic. The revolution lasted from November 1918 until the establishment of the Weimar Republic in August 1919. It included an episode known as the Bavarian Soviet Republic[125][126][127][128] and the Spartacist uprising. A short lived Hungarian Soviet Republic was set up in Hungary March 21 to August 1, 1919. It was led by Béla Kun.[129][130][131][page needed] It instituted a Red Terror.[132][page needed] After the regime was put down, an even more brutal White Terror followed. Kun managed to escape to the Soviet Union, where he co-led murder of tens of thousands of White Russians.[133][134] He was killed in the 1930 Soviet purges.[135][136]
In Italy, the events known as the Biennio Rosso[137][138] were characterised by mass strikes, worker demonstrations and self-management experiments through land and factory occupations. In Turin and Milan, workers' councils were formed and many factory occupations took place led by anarcho-syndicalists organised around the Unione Sindacale Italiana.[139]
There was a short-lived Persian Socialist Soviet Republic in 1920–21. Patagonia Rebelde was a syndicalist-led revolution in Argentina lasting for a year and a half from in 1920–21. The anarchist-led Guangzhou City Commune in China lasted six years from 1921. In 1924, the Mongolian People's Republic was established and was ruled by the Mongolian People's Party. The Shinmin Prefecture in Manchuria lasted two years from 1929. Many of these revolutions initiated societies and economic models that have been described as socialist.[140]
In 1922, the fourth congress of the Communist International took up the policy of the united front. It urged communists to work with rank and file social democrats while remaining critical of their leaders. They criticised those leaders for betraying the working class by supporting the capitalists' war efforts. The social democrats pointed to the dislocation caused by revolution and later the growing authoritarianism of the communist parties. The Labour Party rejected the Communist Party of Great Britain's application to affiliate to them in 1920.
On seeing the Soviet State's growing coercive power in 1923, a dying Lenin said Russia had reverted to "a bourgeois tsarist machine ... barely varnished with socialism".[141] After Lenin's death in January 1924, the Communist Party of the Soviet Union—then increasingly under the control of Joseph Stalin—rejected the theory that socialism could not be built solely in the Soviet Union in favour of the concept of socialism in one country. Despite the marginalised Left Opposition's demand for the restoration of Soviet democracy,[disputed  – discuss] Stalin developed a bureaucratic, authoritarian government that was condemned by democratic socialists and anarchists for undermining the Revolution's ideals.[142][143]
The Russian Revolution and its aftermath motivated national Communist parties elsewhere that gained political and social influence, in France, the United States, Italy, China, Mexico, the Brazil, Chile and Indonesia.
Left-wing groups which did not agree to the centralisation and abandonment of the soviets by the Bolshevik Party (see anti-Stalinist left) led left-wing uprisings against the Bolsheviks. Such groups included Socialist Revolutionaries,[144] Left Socialist Revolutionaries, Mensheviks and anarchists.[145] Within this left-wing discontent, the most large-scale events were the Kronstadt rebellion[146][147][148] and the Makhnovist movement.[149][150][151]
The International Socialist Commission (ISC, also known as Berne International) was formed in February 1919 at a meeting in Bern by parties that wanted to resurrect the Second International.[152] Centrist socialist parties which did not want to be a part of the resurrected Second International (ISC) or Comintern formed the International Working Union of Socialist Parties (IWUSP, also known as Vienna International, Vienna Union, or Two-and-a-Half International) on 27 February 1921 at a conference in Vienna.[153] The ISC and the IWUSP joined to form the Labour and Socialist International (LSI) in May 1923 at a meeting in Hamburg.[154]
The 1920s and 1930s were marked by an increasing divergence between democratic and reformists socialists (mainly affiliated with the Labour and Socialist International) and revolutionary socialists (mainly affiliated with the Communist International), but also by tension within the Communist movement between the dominant Stalinists and dissidents such as Trotsky's followers in the Left Opposition. Trotsky's Fourth International was established in France in 1938 when Trotskyists argued that the Comintern or Third International had become irretrievably "lost to Stalinism" and thus incapable of leading the working class to power.[155]
In the Spanish Civil War (1936–1939), socialists (including the democratic socialist Spanish Socialist Workers' Party and the Marxist Workers' Party of Marxist Unification) participated on the Republican side, loyal to the left-leaning Popular Front government of the Second Spanish Republic, in alliance with anarchists of the communist and syndicalist variety and supported by the socialist Workers' General Union.[156]
The Spanish Revolution of 1936 was a workers' social revolution during the war, that is often seen as a model of socialism from below.[157][158] An anarchist-inspired movement of peasants and workers, supported by armed militias, took control of Barcelona and of large areas of rural Spain where they collectivised the land.[159] The Spanish Revolution was a workers' social revolution that began with the Spanish Civil War in 1936 and resulted in the widespread implementation of anarchist and more broadly libertarian socialist organisational principles in some areas for two to three years, primarily Catalonia, Aragon, Andalusia and parts of Levante. Much of Spain's economy came under worker control. In anarchist strongholds like Catalonia the figure was as high as 75%, but lower in areas with heavy Communist Party influence, which actively resisted attempts at collectivisation. Factories were run through worker committees, agrarian areas became collectivised and run as libertarian communes. Anarchist historian Sam Dolgoff estimated that about eight million people participated directly or indirectly in the Spanish Revolution.[160]
The rise of Nazism and the start of World War II led to the dissolution of the LSI in 1940. After the War, the Socialist International was formed in Frankfurt in July 1951 as its successor.[161]
After World War II, social democratic governments introduced social reform and wealth redistribution via welfare and taxation. Social democratic parties dominated post-war politics in countries such as France, Italy, Czechoslovakia, Belgium and Norway. At one point, France claimed to be the world's most state-controlled capitalist country. It nationalised public utilities including Charbonnages de France (CDF), Électricité de France (EDF), Gaz de France (GDF), Air France, Banque de France and Régie Nationale des Usines Renault.[162]
In 1945, the British Labour Party led by Clement Attlee was elected based on a radical socialist programme. The Labour government nationalised industries including mines, gas, coal, electricity, rail, iron, steel and the Bank of England. British Petroleum was officially nationalised in 1951.[163] Anthony Crosland said that in 1956 25% of British industry was nationalised and that public employees, including those in nationalised industries, constituted a similar proportion of the country's workers.[164] The Labour Governments of 1964–1970 and 1974–1979 intervened further.[165] It re-nationalised British Steel (1967) after the Conservatives had denationalised it and nationalised British Leyland (1976).[166] The National Health Service provided taxpayer-funded health care to everyone, free at the point of service.[167] Working-class housing was provided in council housing estates and university education became available via a school grant system.[168]
During most of the post-war era, Sweden was governed by the Swedish Social Democratic Party largely in cooperation with trade unions and industry.[169] The party held power from 1936 to 1976, 1982 to 1991, 1994 to 2006 and 2014 to 2022, most often in minority governments. Party leader Tage Erlander led the government from 1946 to 1969, the longest uninterrupted parliamentary government. These governments substantially expanded the welfare state.[170] Swedish Prime Minister Olof Palme identified as a "democratic socialist"[171] and was described as a "revolutionary reformist".[172]
The Norwegian Labour Party was established in 1887 and was largely a trade union federation. The party did not proclaim a socialist agenda, elevating universal suffrage and dissolution of the union with Sweden as its top priorities. In 1899, the Norwegian Confederation of Trade Unions separated from the Labour Party. Around the time of the Russian Revolution, the Labour Party moved to the left and joined the Communist International from 1919 through 1923. Thereafter, the party still regarded itself as revolutionary, but the party's left-wing broke away and established the Communist Party of Norway while the Labour Party gradually adopted a reformist line around 1930. In 1935, Johan Nygaardsvold established a coalition that lasted until 1945.[173]
From 1946 to 1962, the Norwegian Labour Party held an absolute majority in the parliament led by Einar Gerhardsen, who remained Prime Minister for seventeen years. Although the party abandoned most of its pre-war socialist ideas, the welfare state was expanded under Gerhardsen to ensure the universal provision of basic human rights and stabilise the economy.[174] In the 1945 Norwegian parliamentary election, the Communist Party took 12% of the votes, but it largely vanished during the Cold War.[175] In the 1950s, popular socialism emerged in Nordic countries. It placed itself between communism and social democracy.[176] In the early 1960s, the Socialist Left Party challenged the Labour Party from the left.[173] Also in the 1960s, Gerhardsen established a planning agency and tried to establish a planned economy.[174] In the 1970s, a more radical socialist party, the Worker's Communist Party (AKP), broke from the Socialist Left Party and had notable influence in student associations and some trade unions. The AKP identified with Communist China and Albania rather than the Soviet Union.[177]
In countries such as Sweden, the Rehn–Meidner model[178] allowed capitalists owning productive and efficient firms to retain profits at the expense of the firms' workers, exacerbating inequality and causing workers to agitate for a share of the profits in the 1970s. At that time, women working in the state sector began to demand better wages. Rudolf Meidner established a study committee that came up with a 1976 proposal to transfer excess profits into worker-controlled investment funds, with the intention that firms would create jobs and pay higher wages rather than reward company owners and managers.[87] Capitalists immediately labeled this proposal as socialism and launched an unprecedented opposition—including calling off the class compromise established in the 1938 Saltsjöbaden Agreement.[179] Social democratic parties are some of the oldest such parties and operate in all Nordic countries. Countries or political systems that have long been dominated by social democratic parties are often labelled social democratic.[180][181] Those countries fit the social democratic type of "high socialism" which is described as favouring "a high level of decommodification and a low degree of stratification".[182]
The Nordic model is a form of economic-political system common to the Nordic countries (Denmark, Finland, Iceland, Norway and Sweden). It has three main ingredients, namely peaceful, institutionalised negotiation between employers and trade unions; active, predictable and measured macroeconomic policy; and universal welfare and free education. The welfare system is governmental in Norway and Sweden whereas trade unions play a greater role in Denmark, Finland and Iceland.[183][184][185][186][187] The Nordic universal welfare model is often labelled social democratic and contrasted with the selective continental model and the residual Anglo-American model.[188] Major reforms in the Nordic countries are the results of consensus and compromise across the political spectrum. Key reforms were implemented under social democratic cabinets in Denmark, Norway and Sweden while centre-right parties dominated during the implementation of the model in Finland and Iceland. Since World War II, Nordic countries have largely maintained a social democratic mixed economy, characterised by labour force participation, gender equality, egalitarian and universal benefits, redistribution of wealth and expansionary fiscal policy.[174][183][189] In 2015, then-Prime Minister of Denmark Lars Løkke Rasmussen denied that Denmark is socialist, saying "I know that some people in the US associate the Nordic model with some sort of socialism. Therefore I would like to make one thing clear. Denmark is far from a socialist planned economy. Denmark is a market economy".[190]
In Norway, the first mandatory social insurances were introduced by conservative cabinets in 1895 (Francis Hagerups's cabinet) and 1911 (Konow's Cabinet). During the 1930s, the Labour Party adopted the conservatives' welfare state project. After World War II, all political parties agreed that the welfare state should be expanded. Universal social security (Folketrygden) was introduced by the conservative Borten's Cabinet.[191][192] Norway's economy is open to the international or European market for most products and services, joining the European Union's internal market in 1994 through European Economic Area. Some of the mixed economy institutions from the post-war period were relaxed by the conservative cabinet of the 1980s and the finance market was deregulated.[193] Within the Varieties of Capitalism-framework, Finland, Norway and Sweden are identified as coordinated market economies.[194]
The Soviet era saw competition between the Soviet-led Eastern Bloc and the United States-led Western Bloc. The Soviet system was seen as a rival of and a threat to Western capitalism for most of the 20th century.[195][page needed]
The Eastern Bloc was the group of Communist states of Central and Eastern Europe, including the Soviet Union and the countries of the Warsaw Pact,[196][197][198] including Poland, the German Democratic Republic, the Hungary, Bulgaria, Czechoslovakia, Romania, Albania, and initially Yugoslavia. In the Informbiro period from 1948, Yugoslavia under Josip Broz Tito pursued a different, more decentralised form of state socialism than the rest of the Eastern Bloc, known as Socialist self-management.
The Hungarian Revolution of 1956, a spontaneous nationwide revolt against the Communist government brutally suppressed by Soviet forces, and USSR leader Nikita Khrushchev's denunciation of the excesses of Stalin's regime during the Twentieth Communist Party Congress the same year[199] produced disunity within Western European Communist parties,[200][201][202][203] leading to the emergence of the New Left (see below). Over a decade later, Czechoslovakia under Alexander Dubček also attempted to pursue a more democratic model of state socialism, under the name "Socialism with a human face", during the Prague Spring; this was also brutally suppressed by the Soviet Union.
In the post-war years, socialism became increasingly influential in many then-developing countries. Embracing Third World socialism, countries in Africa, Asia and Latin America often nationalised industries. During India's freedom movement and fight for independence, many figures in the left-wing faction of the Indian National Congress organised themselves as the Congress Socialist Party. Their politics and those of the early and intermediate periods of Jayaprakash Narayan's career combined a commitment to the socialist transformation of society with a principled opposition to the one-party authoritarianism they perceived in the Stalinist model.[204]
The Chinese Communist Revolution was the second stage in the Chinese Civil War, which ended with the establishment of the People's Republic of China led by the Chinese Communist Party. The then-Chinese Kuomintang Party in the 1920s incorporated Chinese socialism as part of its ideology.[205][206] Between 1958 and 1962 during the Great Leap Forward in the People's Republic of China, some 30 million people starved to death[207] and at least 45 million died overall.[208]
The emergence of this new political entity in the frame of the Cold War was complex and painful. Several tentative efforts were made to organise newly independent states in order to establish a common front to limit the United States' and the Soviet Union's influence on them. This led to the Sino-Soviet split. The Non-Aligned Movement gathered around the figures of Jawaharlal Nehru of India, Sukarno of Indonesia, Josip Broz Tito of Yugoslavia and Gamal Abdel Nasser of Egypt. After the 1954 Geneva Conference which ended the French war in Vietnam, the 1955 Bandung Conference gathered Nasser, Nehru, Tito, Sukarno and Chinese Premier Zhou Enlai.[209] As many African countries gained independence during the 1960s, some of them rejected capitalism in favour of African socialism as defined by Julius Nyerere of Tanzania, Léopold Senghor of Senegal, Kwame Nkrumah of Ghana and Sékou Touré of Guinea.[210]
The Cuban Revolution (1953–1959) was an armed revolt conducted by Fidel Castro's 26th of July Movement and its allies against the government of Fulgencio Batista. Castro's government eventually adopted communism, becoming the Communist Party of Cuba in October 1965.[211][212][213]
In Indonesia in the mid-1960s, a coup attempt blamed on the Communist Party of Indonesia (PKI) was countered by an anti-communist purge led by Suharto, which mainly targeted the growing influence of the PKI and other leftist groups, with significant support from the United States, which culminated in the overthrow of Sukarno.[214] These events resulted not only in the total destruction of the PKI but also the political left in Indonesia, and paved the way for a major shift in the balance of power in Southeast Asia towards the West, a significant turning point in the global Cold War.[215][216][217]
The New Left was a term used mainly in the United Kingdom and United States in reference to activists, educators and others in the 1960s and 1970s who sought to implement a broad range of reforms on issues such as gay rights, abortion, gender roles and drugs[218] in contrast to earlier leftist or Marxist movements that had taken a more vanguardist approach to social justice and focused mostly on labour unionisation and questions of social class.[219][220][221] The New Left rejected involvement with the labour movement and Marxism's historical theory of class struggle.[222]
In the United States, the New Left was associated with the Hippie movement and anti-war college campus protest movements as well as the black liberation movements such as the Black Panther Party.[223] While initially formed in opposition to the "Old Left" Democratic Party, groups composing the New Left gradually became central players in the Democratic coalition.[218]
The protests of 1968 represented a worldwide escalation of social conflicts, predominantly characterised by popular rebellions against military, capitalist and bureaucratic elites who responded with an escalation of political repression. These protests marked a turning point for the civil rights movement in the United States which produced revolutionary movements like the Black Panther Party. The prominent civil rights leader Martin Luther King Jr. organised the "Poor People's Campaign" to address issues of economic justice,[224] while personally showing sympathy with democratic socialism.[225] In reaction to the Tet Offensive, protests also sparked a broad movement in opposition to the Vietnam War all over the United States and even into London, Paris, Berlin and Rome. In 1968, the International of Anarchist Federations was founded during a conference held in Carrara by the three existing European federations of France, the Italian and the Iberian Anarchist Federation as well as the Bulgarian federation in French exile.
Mass socialist movements grew not only in the United States, but also in most European countries. In many other capitalist countries, struggles against dictatorships, state repression and colonisation were also marked by protests in 1968, such as the Tlatelolco massacre in Mexico City and the escalation of guerrilla warfare against the military dictatorship in Brazil.
Countries governed by Communist parties saw protests against bureaucratic and military elites too. In Eastern Europe, widespread protests escalated particularly in the Prague Spring in Czechoslovakia. In response, Soviet Union occupied Czechoslovakia. The occupation was denounced by the Italian and French[226] Communist parties and the Communist Party of Finland, but defended by the Portuguese Communist Party secretary-general Álvaro Cunhal[227] the Communist Party of Luxembourg[226] and conservative factions of the Communist Party of Greece.[226]
In the Chinese Cultural Revolution, a socio-political youth movement mobilised against "bourgeois" elements which were seen to be infiltrating the government and society at large, aiming to restore capitalism. This movement motivated Maoism-inspired movements around the world in the context of the Sino-Soviet split.
In the 1960s, a socialist tendency within the Latin American Catholic church appeared and was known as liberation theology[229][230] It motivated the Colombian priest Camilo Torres Restrepo to enter the ELN guerrilla. In Chile, Salvador Allende, a physician and candidate for the Socialist Party of Chile, was elected president in 1970. In 1973, his government was ousted by the United States-backed military dictatorship of Augusto Pinochet, which lasted until the late 1980s.[231] In Jamaica, the democratic socialist[232] Michael Manley served as the fourth Prime Minister of Jamaica from 1972 to 1980 and from 1989 to 1992. According to opinion polls, he remains one of Jamaica's most popular Prime Ministers since independence.[233] The Nicaraguan Revolution encompassed the rising opposition to the Somoza dictatorship in the 1960s and 1970s, the campaign led by the Sandinista National Liberation Front (FSLN) to violently oust the dictatorship in 1978–1979, the subsequent efforts of the FSLN to govern Nicaragua from 1979 until 1990[234] and the socialist measures which included wide-scale agrarian reform[235][236] and educational programs.[237] The People's Revolutionary Government was proclaimed on 13 March 1979 in Grenada which was overthrown by armed forces of the United States in 1983. The Salvadoran Civil War (1979–1992) was a conflict between the military-led government of El Salvador and the Farabundo Martí National Liberation Front (FMLN), a coalition or umbrella organisation of five socialist guerrilla groups. A coup on 15 October 1979 led to the killings of anti-coup protesters by the government as well as anti-disorder protesters by the guerrillas, and is widely seen as the tipping point towards the civil war.[238]
In 1982, the newly elected French socialist government of François Mitterrand nationalised parts of a few key industries, including banks and insurance companies.[239] Eurocommunism was a trend in the 1970s and 1980s in various Western European Communist parties to develop a theory and practice of social transformation that was more relevant for a Western European country and less aligned to the influence or control of the Communist Party of the Soviet Union. Outside Western Europe, it is sometimes called neocommunism.[240]
Some Communist parties with strong popular support, notably the Italian Communist Party (PCI) and the Communist Party of Spain (PCE). adopted Eurocommunism most enthusiastically and the Communist Party of Finland was dominated by Eurocommunists. The French Communist Party (PCF) and many smaller parties strongly opposed Eurocommunism and stayed aligned with the Communist Party of the Soviet Union until the end of the Soviet Union. Also emerging from the Communist movement but moving in a more left-wing direction, in Italy Autonomia Operaia was particularly active from 1976 to 1978; it took an important role in the autonomist movement in the 1970s, alongside earlier organisations such as Potere Operaio (created after May 1968) and Lotta Continua, promoting a radical form of socialism based on working class self-activity rather than vanguard parties and state planning.[241][242]
Until its 1976 Geneva Congress, the Socialist International (SI) had few members outside Europe and no formal involvement with Latin America.[243] In the late 1970s and in the 1980s, the SI had extensive contacts and discussion with the two powers of the Cold War, the United States and the Soviet Union, about east–west relations and arms control, and admitted as member parties the Nicaraguan FSLN, the left-wing Puerto Rican Independence Party, as well as former Communist parties such as the Democratic Party of the Left of Italy and the Front for the Liberation of Mozambique (FRELIMO). The SI aided social democratic parties in re-establishing themselves when dictatorship gave way to democracy in Portugal (1974) and Spain (1975).
After Mao Zedong's death in 1976 and the arrest of the faction known as the Gang of Four, who were blamed for the excesses of the Cultural Revolution, Deng Xiaoping took power and led the People's Republic of China to significant economic reforms. The Chinese Communist Party (CCP) loosened governmental control over citizens' personal lives and the communes were disbanded in favour of private land leases, thus China's transition from a planned economy to a mixed economy named as "socialism with Chinese characteristics"[244] which maintained state ownership rights over land, state or cooperative ownership of much of the heavy industrial and manufacturing sectors and state influence in the banking and financial sectors. China adopted its current constitution on 4 December 1982. Chinese Communist Party General Secretary Jiang Zemin, Premiers Li Peng and Zhu Rongji led the nation in the 1990s. Under their administration, China sustained an average annual gross domestic product growth rate of 11.2%.[245][better source needed] At the Sixth National Congress of the Communist Party of Vietnam in December 1986, reformist politicians replaced the "old guard" government with new leadership.[246][247] The reformers were led by 71-year-old Nguyen Van Linh, who became the party's new general secretary.[246][247] Linh and the reformers implemented a series of free market reforms—known as Đổi Mới ("Renovation")—which carefully managed the transition from a planned economy to a "socialist-oriented market economy".[248][249]
The Soviet Union experienced continued increases in mortality rate (particularly among men) as far back as 1965.[250][better source needed] Mikhail Gorbachev wished to move the Soviet Union towards of Nordic-style social democracy, calling it "a socialist beacon for all mankind".[251][252] Prior to its dissolution in 1991, the economy of the Soviet Union was by some measures the second largest in the world after the United States.[253][254][255] This economy was beset by economic stagnation, an inflationary spiral, shortages of consumer goods, and fiscal mismanagement.[254] With the collapse of the Soviet Union, the economic integration of the Soviet republics was dissolved and overall industrial activity declined substantially.[256]
A lasting legacy of Communism in Soviet Union remains in the physical infrastructure created during decades of combined industrial production practices, and widespread environmental destruction.[257] The transition to capitalist market economies in the former Soviet Union and Eastern Bloc was accompanied by Washington Consensus-inspired "shock therapy",[258] advocated by Western institutions and economists with the intent to replace state socialism with capitalism and integrate these countries into the capitalist western world.[259] Following a transition to free-market capitalism there has been a steep fall in the standard of living. Post-Communist Russia experienced rising economic inequality and poverty[260][261] a surge in excess mortality amongst men,[259][250][262] and a decline in life expectancy,[263] which was accompanied by the entrenchment of a newly established business oligarchy.[260] By contrast, the Central European states of the former Eastern Bloc–Poland, Hungary, the Czech Republic and Slovakia–showed healthy increases in life expectancy from the 1990s onward, compared to nearly thirty years of stagnation under socialism.[264][265][266][267][268] Bulgaria and Romania followed this trend after the introduction of more serious economic reforms in the late 1990s.[269][270] The average post-Communist country had returned to 1989 levels of per-capita GDP by 2005,[271] and as of 2015, some countries were still behind that.[272] These economic developments led to increased nationalist sentiment and nostalgia for the Communist era.[273][274][275] However, by 2019, the majority of people in most Eastern European countries approved of the shift to multiparty democracy and a market economy, with approval being highest among residents of Poland and residents in the territory of what was once East Germany, and disapproval being the highest among residents of Russia and Ukraine.[276]
Many social democratic parties, particularly after the Cold War, adopted neoliberal market policies including privatisation, deregulation and financialisation. They abandoned their pursuit of moderate socialism in favour of economic liberalism.[277] By the 1980s, with the rise of conservative neoliberal politicians such as Ronald Reagan in the United States, Margaret Thatcher in Britain, Brian Mulroney in Canada and Augusto Pinochet in Chile, the Western welfare state was dismantled from within, but state support for the corporate sector was maintained.[278] In the United Kingdom, Labour Party leader Neil Kinnock expelled some Trotskyist members and refused to support the 1984–1985 miners' strike over pit closures. In 1989, the 18th Congress of the SI adopted a new Declaration of Principles, stating: "Democratic socialism is an international movement for freedom, social justice, and solidarity. Its goal is to achieve a peaceful world where these basic values can be enhanced and where each individual can live a meaningful life with the full development of his or her personality and talents, and with the guarantee of human and civil rights in a democratic framework of society."[279]
In the 1990s, the British Labour Party under Tony Blair enacted policies based on the free-market economy to deliver public services via the private finance initiative. Influential in these policies was the idea of a Third Way between Old Left state socialism and New Right market capitalism, and a re-evaluation of welfare state policies.[280][281][282] In 1995, the Labour Party re-defined its stance on socialism by re-wording Clause IV of its constitution, defining socialism in ethical terms and removing all references to public, direct worker or municipal ownership of the means of production. The Labour Party stated: "The Labour Party is a democratic socialist party. It believes that, by the strength of our common endeavour we achieve more than we achieve alone, so as to create, for each of us, the means to realise our true potential, and, for all of us, a community in which power, wealth, and opportunity are in the hands of the many, not the few."[283] Left-wing critics of the Third Way argued that it reduced equality to an equal opportunity to compete in an economy in which the rich were growing richer and the poor were becoming more disadvantaged, which the leftists argue is not socialist.[83]
Starting in the late 20th century, the development of a post-industrial economy in which information and knowledge matter more than material production and labor raised doubts about the continued relevance of socialism, since socialism emerged in response to industrialization under capitalism.[83] Several scholars argued that socialism was dead in the immediate aftermath of the end of the Cold War. German sociologist and liberal politician Ralf Dahrendorf declared that "socialism is dead, and none of its variants can be revived for a world awakening from the double nightmare of Stalinism and Brezhnevism." Andre Gorz, a left-wing philosopher, also declared that "As a system, socialism is dead. As a movement and an organized political force, it is on its last legs. All the goals it once proclaimed are out of date."[284] However there was also a counter-argument put forward by the socialist political scholars Antonio Negri and Felix Guattari, who argued that "whether perestroika succeeds in the present form or in a second wave that will inevitably follow, whether the Russian empire endures or not - these are all problems that concern
only the Soviets," arguing that the role of socialism in global politics was not tied to the fate of the Soviet Union.[285]
In 1990, the São Paulo Forum was launched by the Workers' Party (Brazil), linking left-wing socialist parties in Latin America. Its members were associated with the Pink tide of left-wing governments on the continent in the early 21st century. Member parties ruling countries included the Front for Victory in Argentina, the PAIS Alliance in Ecuador, Farabundo Martí National Liberation Front in El Salvador, Peru Wins in Peru, and the United Socialist Party of Venezuela, whose leader Hugo Chávez initiated what he called "Socialism of the 21st century".
Many mainstream democratic socialist and social democratic parties continued to drift right-wards. On the right of the socialist movement, the Progressive Alliance was founded in 2013 by current or former members of the Socialist International. The organisation states the aim of becoming the global network of "the progressive, democratic, social-democratic, socialist and labour movement".[286][287] Mainstream social democratic and socialist parties are also networked in Europe in the Party of European Socialists formed in 1992. Many of these parties lost large parts of their electoral base in the early 21st century. This phenomenon is known as Pasokification[288][289] from the Greek party PASOK, which saw a declining share of the vote in national elections—from 43.9% in 2009 to 13.2% in May 2012, to 12.3% in June 2012 and 4.7% in 2015—due to its poor handling of the Greek government-debt crisis and implementation of harsh austerity measures.[290][291]
In Europe, the share of votes for such socialist parties was at its 70-year lowest in 2015. For example, the Socialist Party, after winning the 2012 French presidential election, rapidly lost its vote share, the Social Democratic Party of Germany's fortunes declined rapidly from 2005 to 2019, and outside Europe the Israeli Labor Party fell from being the dominant force in Israeli politics to 4.43% of the vote in the April 2019 Israeli legislative election, and the Peruvian Aprista Party went from ruling party in 2011 to a minor party. The decline of these mainstream parties opened space for more radical and populist left parties in some countries, such as Spain's Podemos, Greece's Syriza (in government, 2015–19), Germany's Die Linke, and France's La France Insoumise. In other countries, left-wing revivals have taken place within mainstream democratic socialist and centrist parties, as with Jeremy Corbyn in the United Kingdom and Bernie Sanders in the United States. Few of these radical left parties have won national government in Europe, while some more mainstream socialist parties have managed to, such as Portugal's Socialist Party.[292]
Bhaskar Sunkara, the founding editor of the Jacobin socialist magazine, argued that the appeal of socialism persists due to inequality under current global capitalism, the utilization of wage labor "which rests on the exploitation and domination of humans by other humans," and ecological crises, such as climate change.[284] Some in the scientific community have suggested that a contemporary radical response to social and ecological problems could be seen in the emergence of movements associated with degrowth, eco-socialism and eco-anarchism.[293][294]
Early socialist thought took influences from a diverse range of philosophies such as civic republicanism, Enlightenment rationalism, romanticism, forms of materialism, Christianity (both Catholic and Protestant), natural law and natural rights theory, utilitarianism and liberal political economy.[295] Another philosophical basis for a great deal of early socialism was the emergence of positivism during the European Enlightenment. Positivism held that both the natural and social worlds could be understood through scientific knowledge and be analysed using scientific methods. This core outlook influenced early social scientists and different types of socialists ranging from anarchists like Peter Kropotkin to technocrats like Saint Simon.[citation needed]
The fundamental objective of socialism is to attain an advanced level of material production and therefore greater productivity, efficiency and rationality as compared to capitalism and all previous systems, under the view that an expansion of human productive capability is the basis for the extension of freedom and equality in society.[296] Many forms of socialist theory hold that human behaviour is largely shaped by the social environment. In particular, socialism holds that social mores, values, cultural traits and economic practices are social creations and not the result of an immutable natural law.[297][298] The object of their critique is thus not human avarice or human consciousness, but the material conditions and man-made social systems (i.e. the economic structure of society) which give rise to observed social problems and inefficiencies. Bertrand Russell, often considered to be the father of analytic philosophy, identified as a socialist. Russell opposed the class struggle aspects of Marxism, viewing socialism solely as an adjustment of economic relations to accommodate modern machine production to benefit all of humanity through the progressive reduction of necessary work time.[299]
Socialists view creativity as an essential aspect of human nature and define freedom as a state of being where individuals are able to express their creativity unhindered by constraints of both material scarcity and coercive social institutions.[300] The socialist concept of individuality is intertwined with the concept of individual creative expression. Karl Marx believed that expansion of the productive forces and technology was the basis for the expansion of human freedom and that socialism, being a system that is consistent with modern developments in technology, would enable the flourishing of "free individualities" through the progressive reduction of necessary labour time. The reduction of necessary labour time to a minimum would grant individuals the opportunity to pursue the development of their true individuality and creativity.[301]
Socialists argue that the accumulation of capital generates waste through externalities that require costly corrective regulatory measures. They also point out that this process generates wasteful industries and practices that exist only to generate sufficient demand for products such as high-pressure advertisement to be sold at a profit, thereby creating rather than satisfying economic demand.[302][303] Socialists argue that capitalism consists of irrational activity, such as the purchasing of commodities only to sell at a later time when their price appreciates, rather than for consumption, even if the commodity cannot be sold at a profit to individuals in need and therefore a crucial criticism often made by socialists is that "making money", or accumulation of capital, does not correspond to the satisfaction of demand (the production of use-values).[302] The fundamental criterion for economic activity in capitalism is the accumulation of capital for reinvestment in production, but this spurs the development of new, non-productive industries that do not produce use-value and only exist to keep the accumulation process afloat (otherwise the system goes into crisis), such as the spread of the financial industry, contributing to the formation of economic bubbles.[304]
Socialists view private property relations as limiting the potential of productive forces in the economy. According to socialists, private property becomes obsolete when it concentrates into centralised, socialised institutions based on private appropriation of revenue—but based on cooperative work and internal planning in allocation of inputs—until the role of the capitalist becomes redundant.[305] With no need for capital accumulation and a class of owners, private property in the means of production is perceived as being an outdated form of economic organisation that should be replaced by a free association of individuals based on public or common ownership of these socialised assets.[306][307] Private ownership imposes constraints on planning, leading to uncoordinated economic decisions that result in business fluctuations, unemployment and a tremendous waste of material resources during crisis of overproduction.[308]
Excessive disparities in income distribution lead to social instability and require costly corrective measures in the form of redistributive taxation, which incurs heavy administrative costs while weakening the incentive to work, inviting dishonesty and increasing the likelihood of tax evasion while (the corrective measures) reduce the overall efficiency of the market economy.[309] These corrective policies limit the incentive system of the market by providing things such as minimum wages, unemployment insurance, taxing profits and reducing the reserve army of labour, resulting in reduced incentives for capitalists to invest in more production. In essence, social welfare policies cripple capitalism and its incentive system and are thus unsustainable in the long-run.[310] Marxists argue that the establishment of a socialist mode of production is the only way to overcome these deficiencies. Socialists and specifically Marxian socialists argue that the inherent conflict of interests between the working class and capital prevent optimal use of available human resources and leads to contradictory interest groups (labour and business) striving to influence the state to intervene in the economy in their favour at the expense of overall economic efficiency. Early socialists (utopian socialists and Ricardian socialists) criticised capitalism for concentrating power and wealth within a small segment of society.[311] In addition, they complained that capitalism does not use available technology and resources to their maximum potential in the interests of the public.[307]
At a certain stage of development, the material productive forces of society come into conflict with the existing relations of production or—this merely expresses the same thing in legal terms—with the property relations within the framework of which they have operated hitherto. Then begins an era of social revolution. The changes in the economic foundation lead sooner or later to the transformation of the whole immense superstructure.[312]
—Karl Marx, Critique of the Gotha Program
Karl Marx and Friedrich Engels argued that socialism would emerge from historical necessity as capitalism rendered itself obsolete and unsustainable from increasing internal contradictions emerging from the development of the productive forces and technology. It was these advances in the productive forces combined with the old social relations of production of capitalism that would generate contradictions, leading to working-class consciousness.[313]
Marx and Engels held the view that the consciousness of those who earn a wage or salary (the working class in the broadest Marxist sense) would be moulded by their conditions of wage slavery, leading to a tendency to seek their freedom or emancipation by overthrowing ownership of the means of production by capitalists and consequently, overthrowing the state that upheld this economic order. For Marx and Engels, conditions determine consciousness and ending the role of the capitalist class leads eventually to a classless society in which the state would wither away.
Marx and Engels used the terms socialism and communism interchangeably, but many later Marxists defined socialism as a specific historical phase that would displace capitalism and precede communism.[52][55]
The major characteristics of socialism (particularly as conceived by Marx and Engels after the Paris Commune of 1871) are that the proletariat would control the means of production through a workers' state erected by the workers in their interests. Economic activity would still be organised through the use of incentive systems and social classes would still exist, but to a lesser and diminishing extent than under capitalism.[citation needed]
For orthodox Marxists, socialism is the lower stage of communism based on the principle of "from each according to his ability, to each according to his contribution" while upper stage communism is based on the principle of "from each according to his ability, to each according to his need", the upper stage becoming possible only after the socialist stage further develops economic efficiency and the automation of production has led to a superabundance of goods and services.[314][315] Marx argued that the material productive forces (in industry and commerce) brought into existence by capitalism predicated a cooperative society since production had become a mass social, collective activity of the working class to create commodities but with private ownership (the relations of production or property relations). This conflict between collective effort in large factories and private ownership would bring about a conscious desire in the working class to establish collective ownership commensurate with the collective efforts their daily experience.[312]
Socialists have taken different perspectives on the state and the role it should play in revolutionary struggles, in constructing socialism and within an established socialist economy.
In the 19th century, the philosophy of state socialism was first explicitly expounded by the German political philosopher Ferdinand Lassalle. In contrast to Karl Marx's perspective of the state, Lassalle rejected the concept of the state as a class-based power structure whose main function was to preserve existing class structures. Lassalle also rejected the Marxist view that the state was destined to "wither away". Lassalle considered the state to be an entity independent of class allegiances and an instrument of justice that would therefore be essential for achieving socialism.[316]
Preceding the Bolshevik-led revolution in Russia, many socialists including reformists, orthodox Marxist currents such as council communism, anarchists and libertarian socialists criticised the idea of using the state to conduct central planning and own the means of production as a way to establish socialism. Following the victory of Leninism in Russia, the idea of "state socialism" spread rapidly throughout the socialist movement and eventually state socialism came to be identified with the Soviet economic model.[317]
Joseph Schumpeter rejected the association of socialism and social ownership with state ownership over the means of production because the state as it exists in its current form is a product of capitalist society and cannot be transplanted to a different institutional framework. Schumpeter argued that there would be different institutions within socialism than those that exist within modern capitalism, just as feudalism had its own distinct and unique institutional forms. The state, along with concepts like property and taxation, were concepts exclusive to commercial society (capitalism) and attempting to place them within the context of a future socialist society would amount to a distortion of these concepts by using them out of context.[318]
Utopian socialism is a term used to define the first currents of modern socialist thought as exemplified by the work of Henri de Saint-Simon, Charles Fourier and Robert Owen which inspired Karl Marx and other early socialists.[319] Visions of imaginary ideal societies, which competed with revolutionary social democratic movements, were viewed as not being grounded in the material conditions of society and as reactionary.[320] Although it is technically possible for any set of ideas or any person living at any time in history to be a utopian socialist, the term is most often applied to those socialists who lived in the first quarter of the 19th century who were ascribed the label "utopian" by later socialists as a negative term in order to imply naivete and dismiss their ideas as fanciful or unrealistic.[87]
Religious sects whose members live communally such as the Hutterites are not usually called "utopian socialists", although their way of living is a prime example. They have been categorised as religious socialists by some. Similarly, modern intentional communities based on socialist ideas could also be categorised as "utopian socialist". For Marxists, the development of capitalism in Western Europe provided a material basis for the possibility of bringing about socialism because according to The Communist Manifesto "[w]hat the bourgeoisie produces above all is its own grave diggers",[321] namely the working class, which must become conscious of the historical objectives set it by society.
Revolutionary socialists believe that a social revolution is necessary to effect structural changes to the socioeconomic structure of society. Among revolutionary socialists there are differences in strategy, theory and the definition of revolution. Orthodox Marxists and left communists take an impossibilist stance, believing that revolution should be spontaneous as a result of contradictions in society due to technological changes in the productive forces. Lenin theorised that under capitalism the workers cannot achieve class consciousness beyond organising into trade unions and making demands of the capitalists. Therefore, Leninists argue that it is historically necessary for a vanguard of class conscious revolutionaries to take a central role in coordinating the social revolution to overthrow the capitalist state and eventually the institution of the state altogether.[322] Revolution is not necessarily defined by revolutionary socialists as violent insurrection,[323] but as a complete dismantling and rapid transformation of all areas of class society led by the majority of the masses: the working class.
Reformism is generally associated with social democracy and gradualist democratic socialism. Reformism is the belief that socialists should stand in parliamentary elections within capitalist society and if elected use the machinery of government to pass political and social reforms for the purposes of ameliorating the instabilities and inequities of capitalism. Within socialism, reformism is used in two different ways. One has no intention of bringing about socialism or fundamental economic change to society and is used to oppose such structural changes. The other is based on the assumption that while reforms are not socialist in themselves, they can help rally supporters to the cause of revolution by popularizing the cause of socialism to the working class.[324]
The debate on the ability for social democratic reformism to lead to a socialist transformation of society is over a century old. Reformism is criticized for being paradoxical as it seeks to overcome the existing economic system of capitalism while trying to improve the conditions of capitalism, thereby making it appear more tolerable to society. According to Rosa Luxemburg, capitalism is not overthrown, "but is on the contrary strengthened by the development of social reforms".[325] In a similar vein, Stan Parker of the Socialist Party of Great Britain argues that reforms are a diversion of energy for socialists and are limited because they must adhere to the logic of capitalism.[324] French social theorist Andre Gorz criticized reformism by advocating a third alternative to reformism and social revolution that he called "non-reformist reforms", specifically focused on structural changes to capitalism as opposed to reforms to improve living conditions within capitalism or to prop it up through economic interventions.[326]
The economic anarchy of capitalist society as it exists today is, in my opinion, the real source of the evil. ... I am convinced there is only one way to eliminate these grave evils, namely through the establishment of a socialist economy, accompanied by an educational system which would be oriented toward social goals. In such an economy, the means of production are owned by society itself and are utilised in a planned fashion. A planned economy, which adjusts production to the needs of the community, would distribute the work to be done among all those able to work and would guarantee a livelihood to every man, woman, and child. The education of the individual, in addition to promoting his own innate abilities, would attempt to develop in him a sense of responsibility for his fellow men in place of the glorification of power and success in our present society.[327]
—Albert Einstein, "Why Socialism?", 1949
Socialist economics starts from the premise that "individuals do not live or work in isolation but live in cooperation with one another. Furthermore, everything that people produce is in some sense a social product, and everyone who contributes to the production of a good is entitled to a share in it. Society as whole, therefore, should own or at least control property for the benefit of all its members".[96]
The original conception of socialism was an economic system whereby production was organised in a way to directly produce goods and services for their utility (or use-value in classical and Marxian economics), with the direct allocation of resources in terms of physical units as opposed to financial calculation and the economic laws of capitalism (see law of value), often entailing the end of capitalistic economic categories such as rent, interest, profit and money.[328] In a fully developed socialist economy, production and balancing factor inputs with outputs becomes a technical process to be undertaken by engineers.[329]
Market socialism refers to an array of different economic theories and systems that use the market mechanism to organise production and to allocate factor inputs among socially owned enterprises, with the economic surplus (profits) accruing to society in a social dividend as opposed to private capital owners.[330] Variations of market socialism include libertarian proposals such as mutualism, based on classical economics, and neoclassical economic models such as the Lange Model. Some economists, such as Joseph Stiglitz, Mancur Olson, and others not specifically advancing anti-socialists positions have shown that prevailing economic models upon which such democratic or market socialism models might be based have logical flaws or unworkable presuppositions.[331][332] These criticisms have been incorporated into the models of market socialism developed by John Roemer and Nicholas Vrousalis.[333][334][when?]
The ownership of the means of production can be based on direct ownership by the users of the productive property through worker cooperative; or commonly owned by all of society with management and control delegated to those who operate/use the means of production; or public ownership by a state apparatus. Public ownership may refer to the creation of state-owned enterprises, nationalisation, municipalisation or autonomous collective institutions. Some socialists feel that in a socialist economy, at least the "commanding heights" of the economy must be publicly owned.[335] Economic liberals and right libertarians view private ownership of the means of production and the market exchange as natural entities or moral rights which are central to their conceptions of freedom and liberty and view the economic dynamics of capitalism as immutable and absolute, therefore they perceive public ownership of the means of production, cooperatives and economic planning as infringements upon liberty.[336][337]
Management and control over the activities of enterprises are based on self-management and self-governance, with equal power-relations in the workplace to maximise occupational autonomy. A socialist form of organisation would eliminate controlling hierarchies so that only a hierarchy based on technical knowledge in the workplace remains. Every member would have decision-making power in the firm and would be able to participate in establishing its overall policy objectives. The policies/goals would be carried out by the technical specialists that form the coordinating hierarchy of the firm, who would establish plans or directives for the work community to accomplish these goals.[338]
The role and use of money in a hypothetical socialist economy is a contested issue. Nineteenth century socialists including Karl Marx, Robert Owen, Pierre-Joseph Proudhon and John Stuart Mill advocated various forms of labour vouchers or labour credits, which like money would be used to acquire articles of consumption, but unlike money they are unable to become capital and would not be used to allocate resources within the production process. Bolshevik revolutionary Leon Trotsky argued that money could not be arbitrarily abolished following a socialist revolution. Money had to exhaust its "historic mission", meaning it would have to be used until its function became redundant, eventually being transformed into bookkeeping receipts for statisticians and only in the more distant future would money not be required for even that role.[339]
A planned economy is a type of economy consisting of a mixture of public ownership of the means of production and the coordination of production and distribution through economic planning. A planned economy can be either decentralised or centralised. Enrico Barone provided a comprehensive theoretical framework for a planned socialist economy. In his model, assuming perfect computation techniques, simultaneous equations relating inputs and outputs to ratios of equivalence would provide appropriate valuations in order to balance supply and demand.[340]
The most prominent example of a planned economy was the economic system of the Soviet Union and as such the centralised-planned economic model is usually associated with the communist states of the 20th century, where it was combined with a single-party political system. In a centrally planned economy, decisions regarding the quantity of goods and services to be produced are planned in advance by a planning agency (see also the analysis of Soviet-type economic planning). The economic systems of the Soviet Union and the Eastern Bloc are further classified as "command economies", which are defined as systems where economic coordination is undertaken by commands, directives and production targets.[341] Studies by economists of various political persuasions on the actual functioning of the Soviet economy indicate that it was not actually a planned economy. Instead of conscious planning, the Soviet economy was based on a process whereby the plan was modified by localised agents and the original plans went largely unfulfilled. Planning agencies, ministries and enterprises all adapted and bargained with each other during the formulation of the plan as opposed to following a plan passed down from a higher authority, leading some economists to suggest that planning did not actually take place within the Soviet economy and that a better description would be an "administered" or "managed" economy.[342]
Although central planning was largely supported by Marxist–Leninists, some factions within the Soviet Union before the rise of Stalinism held positions contrary to central planning. Leon Trotsky rejected central planning in favour of decentralised planning. He argued that central planners, regardless of their intellectual capacity, would be unable to coordinate effectively all economic activity within an economy because they operated without the input and tacit knowledge embodied by the participation of the millions of people in the economy. As a result, central planners would be unable to respond to local economic conditions.[343] State socialism is unfeasible in this view because information cannot be aggregated by a central body and effectively used to formulate a plan for an entire economy, because doing so would result in distorted or absent price signals.[344]
Socialism, you see, is a bird with two wings. The definition is 'social ownership and democratic control of the instruments and means of production.'[345]
—Upton Sinclair
A self-managed, decentralised economy is based on autonomous self-regulating economic units and a decentralised mechanism of resource allocation and decision-making. This model has found support in notable classical and neoclassical economists including Alfred Marshall, John Stuart Mill and Jaroslav Vanek. There are numerous variations of self-management, including labour-managed firms and worker-managed firms. The goals of self-management are to eliminate exploitation and reduce alienation.[346] Guild socialism is a political movement advocating workers' control of industry through the medium of trade-related guilds "in an implied contractual relationship with the public".[347] It originated in the United Kingdom and was at its most influential in the first quarter of the 20th century.[347] It was strongly associated with G. D. H. Cole and influenced by the ideas of William Morris.
One such system is the cooperative economy, a largely free market economy in which workers manage the firms and democratically determine remuneration levels and labour divisions. Productive resources would be legally owned by the cooperative and rented to the workers, who would enjoy usufruct rights.[348] Another form of decentralised planning is the use of cybernetics, or the use of computers to manage the allocation of economic inputs. The socialist-run government of Salvador Allende in Chile experimented with Project Cybersyn, a real-time information bridge between the government, state enterprises and consumers.[349] Another, more recent variant is participatory economics, wherein the economy is planned by decentralised councils of workers and consumers. Workers would be remunerated solely according to effort and sacrifice, so that those engaged in dangerous, uncomfortable and strenuous work would receive the highest incomes and could thereby work less.[350] A contemporary model for a self-managed, non-market socialism is Pat Devine's model of negotiated coordination. Negotiated coordination is based upon social ownership by those affected by the use of the assets involved, with decisions made by those at the most localised level of production.[351]
Michel Bauwens identifies the emergence of the open software movement and peer-to-peer production as a new alternative mode of production to the capitalist economy and centrally planned economy that is based on collaborative self-management, common ownership of resources and the production of use-values through the free cooperation of producers who have access to distributed capital.[352]
Anarcho-communism is a theory of anarchism which advocates the abolition of the state, private property and capitalism in favour of common ownership of the means of production.[353][354] Anarcho-syndicalism was practised in Catalonia and other places in the Spanish Revolution during the Spanish Civil War. Sam Dolgoff estimated that about eight million people participated directly or at least indirectly in the Spanish Revolution.[355]
The economy of the former Socialist Federal Republic of Yugoslavia established a system based on market-based allocation, social ownership of the means of production and self-management within firms. This system substituted Yugoslavia's Soviet-type central planning with a decentralised, self-managed system after reforms in 1953.[356]
The Marxian economist Richard D. Wolff argues that "re-organising production so that workers become collectively self-directed at their work-sites" not only moves society beyond both capitalism and state socialism of the last century, but would also mark another milestone in human history, similar to earlier transitions out of slavery and feudalism.[357] As an example, Wolff claims that Mondragon is "a stunningly successful alternative to the capitalist organisation of production".[358]
State socialism can be used to classify any variety of socialist philosophies that advocates the ownership of the means of production by the state apparatus, either as a transitional stage between capitalism and socialism, or as an end-goal in itself. Typically, it refers to a form of technocratic management, whereby technical specialists administer or manage economic enterprises on behalf of society and the public interest instead of workers' councils or workplace democracy.
A state-directed economy may refer to a type of mixed economy consisting of public ownership over large industries, as promoted by various Social democratic political parties during the 20th century. This ideology influenced the policies of the British Labour Party during Clement Attlee's administration. In the biography of the 1945 United Kingdom Labour Party Prime Minister Clement Attlee, Francis Beckett states: "[T]he government ... wanted what would become known as a mixed economy."[359]
Nationalisation in the United Kingdom was achieved through compulsory purchase of the industry (i.e. with compensation). British Aerospace was a combination of major aircraft companies British Aircraft Corporation, Hawker Siddeley and others. British Shipbuilders was a combination of the major shipbuilding companies including Cammell Laird, Govan Shipbuilders, Swan Hunter and Yarrow Shipbuilders, whereas the nationalisation of the coal mines in 1947 created a coal board charged with running the coal industry commercially so as to be able to meet the interest payable on the bonds which the former mine owners' shares had been converted into.[360][361]
Market socialism consists of publicly owned or cooperatively owned enterprises operating in a market economy. It is a system that uses the market and monetary prices for the allocation and accounting of the means of production, thereby retaining the process of capital accumulation. The profit generated would be used to directly remunerate employees, collectively sustain the enterprise or finance public institutions.[362] In state-oriented forms of market socialism, in which state enterprises attempt to maximise profit, the profits can be used to fund government programs and services through a social dividend, eliminating or greatly diminishing the need for various forms of taxation that exist in capitalist systems. Neoclassical economist Léon Walras believed that a socialist economy based on state ownership of land and natural resources would provide a means of public finance to make income taxes unnecessary.[363] Yugoslavia implemented a market socialist economy based on cooperatives and worker self-management.[364]
Mutualism is an economic theory and anarchist school of thought that advocates a society where each person might possess a means of production, either individually or collectively, with trade representing equivalent amounts of labour in the free market.[365] Integral to the scheme was the establishment of a mutual-credit bank that would lend to producers at a minimal interest rate, just high enough to cover administration.[366] Mutualism is based on a labour theory of value that holds that when labour or its product is sold, in exchange it ought to receive goods or services embodying "the amount of labour necessary to produce an article of exactly similar and equal utility".[367]
The current economic system in China is formally referred to as a socialist market economy with Chinese characteristics. It combines a large state sector that comprises the commanding heights of the economy, which are guaranteed their public ownership status by law,[368] with a private sector mainly engaged in commodity production and light industry responsible from anywhere between 33%[369] to over 70% of GDP generated in 2005.[370] Although there has been a rapid expansion of private-sector activity since the 1980s, privatisation of state assets was virtually halted and were partially reversed in 2005.[371] The current Chinese economy consists of 150 corporatised state-owned enterprises that report directly to China's central government.[372] By 2008, these state-owned corporations had become increasingly dynamic and generated large increases in revenue for the state,[373][374] resulting in a state-sector led recovery during the 2009 financial crises while accounting for most of China's economic growth.[375] The Chinese economic model is widely cited as a contemporary form of state capitalism, the major difference between Western capitalism and the Chinese model being the degree of state-ownership of shares in publicly listed corporations. The Socialist Republic of Vietnam has adopted a similar model after the Doi Moi economic renovation but slightly differs from the Chinese model in that the Vietnamese government retains firm control over the state sector and strategic industries, but allows for private-sector activity in commodity production.[376]
While major socialist political movements include anarchism, communism, the labour movement, Marxism, social democracy, and syndicalism, independent socialist theorists, utopian socialist authors, and academic supporters of socialism may not be represented in these movements. Some political groups have called themselves socialist while holding views that some consider antithetical to socialism. Socialist has been used by the political right as an epithet, including against individuals who do not consider themselves to be socialists and against policies that are not considered socialist by their proponents. While there are many variations of socialism, and there is no single definition encapsulating all of socialism, there have been common elements identified by scholars.[377]
In his Dictionary of Socialism (1924), Angelo S. Rappoport analysed forty definitions of socialism to conclude that common elements of socialism include general criticism of the social effects of private ownership and control of capital—as being the cause of poverty, low wages, unemployment, economic and social inequality and a lack of economic security; a general view that the solution to these problems is a form of collective control over the means of production, distribution and exchange (the degree and means of control vary amongst socialist movements); an agreement that the outcome of this collective control should be a society based upon social justice, including social equality, economic protection of people and should provide a more satisfying life for most people.[378]
In The Concepts of Socialism (1975), Bhikhu Parekh identifies four core principles of socialism and particularly socialist society, namely sociality, social responsibility, cooperation and planning.[379] In his study Ideologies and Political Theory (1996), Michael Freeden states that all socialists share five themes: the first is that socialism posits that society is more than a mere collection of individuals; second, that it considers human welfare a desirable objective; third, that it considers humans by nature to be active and productive; fourth, it holds the belief of human equality; and fifth, that history is progressive and will create positive change on the condition that humans work to achieve such change.[379]
Anarchism advocates stateless societies often defined as self-governed voluntary institutions,[380][381][382][383] but that several authors have defined as more specific institutions based on non-hierarchical free associations.[384][385][386][387] While anarchism holds the state to be undesirable, unnecessary or harmful,[388][389] it is not the central aspect.[390] Anarchism entails opposing authority or hierarchical organisation in the conduct of human relations, including the state system.[384][391][392][393][394][395][396] Mutualists support market socialism, collectivist anarchists favour workers cooperatives and salaries based on the amount of time contributed to production, anarcho-communists advocate a direct transition from capitalism to libertarian communism and a gift economy and anarcho-syndicalists prefer workers' direct action and the general strike.[397]
The authoritarian–libertarian struggles and disputes within the socialist movement go back to the First International and the expulsion in 1872 of the anarchists, who went on to lead the Anti-authoritarian International and then founded their own libertarian international, the Anarchist St. Imier International.[398] In 1888, the individualist anarchist Benjamin Tucker, who proclaimed himself to be an anarchistic socialist and libertarian socialist in opposition to the authoritarian state socialism and the compulsory communism, included the full text of a "Socialistic Letter" by Ernest Lesigne[399] in his essay on "State Socialism and Anarchism". According to Lesigne, there are two types of socialism: "One is dictatorial, the other libertarian".[400] Tucker's two socialisms were the authoritarian state socialism which he associated to the Marxist school and the libertarian anarchist socialism, or simply anarchism, that he advocated. Tucker noted that the fact that the authoritarian "State Socialism has overshadowed other forms of Socialism gives it no right to a monopoly of the Socialistic idea".[401] According to Tucker, what those two schools of socialism had in common was the labor theory of value and the ends, by which anarchism pursued different means.[402]
According to anarchists such as the authors of An Anarchist FAQ, anarchism is one of the many traditions of socialism. For anarchists and other anti-authoritarian socialists, socialism "can only mean a classless and anti-authoritarian (i.e. libertarian) society in which people manage their own affairs, either as individuals or as part of a group (depending on the situation). In other words, it implies self-management in all aspects of life", including at the workplace.[397] Michael Newman includes anarchism as one of many socialist traditions.[87] Peter Marshall argues that "[i]n general anarchism is closer to socialism than liberalism. ... Anarchism finds itself largely in the socialist camp, but it also has outriders in liberalism. It cannot be reduced to socialism, and is best seen as a separate and distinctive doctrine."[403]
You can't talk about ending the slums without first saying profit must be taken out of slums. You're really tampering and getting on dangerous ground because you are messing with folk then. You are messing with captains of industry. Now this means that we are treading in difficult water, because it really means that we are saying that something is wrong with capitalism. There must be a better distribution of wealth, and maybe America must move toward a democratic socialism.[404][405]
—Martin Luther King Jr., 1966
Democratic socialism represents any socialist movement that seeks to establish an economy based on economic democracy by and for the working class. Democratic socialism is difficult to define and groups of scholars have radically different definitions for the term. Some definitions simply refer to all forms of socialism that follow an electoral, reformist or evolutionary path to socialism rather than a revolutionary one.[406] According to Christopher Pierson, "[i]f the contrast which 1989 highlights is not that between socialism in the East and liberal democracy in the West, the latter must be recognised to have been shaped, reformed and compromised by a century of social democratic pressure". Pierson further claims that "social democratic and socialist parties within the constitutional arena in the West have almost always been involved in a politics of compromise with existing capitalist institutions (to whatever far distant prize its eyes might from time to time have been lifted)". For Pierson, "if advocates of the death of socialism accept that social democrats belong within the socialist camp, as I think they must, then the contrast between socialism (in all its variants) and liberal democracy must collapse. For actually existing liberal democracy is, in substantial part, a product of socialist (social democratic) forces".[407]
Social democracy is a socialist tradition of political thought.[408][409] Many social democrats refer to themselves as socialists or democratic socialists and some such as Tony Blair employ these terms interchangeably.[410][411][412] Others found "clear differences" between the three terms and prefer to describe their own political beliefs by using the term social democracy.[413] The two main directions were to establish democratic socialism or to build first a welfare state within the capitalist system. The first variant advances democratic socialism through reformist and gradualist methods.[414] In the second variant, social democracy is a policy regime involving a welfare state, collective bargaining schemes, support for publicly financed public services and a mixed economy. It is often used in this manner to refer to Western and Northern Europe during the later half of the 20th century.[415][416] It was described by Jerry Mander as "hybrid economics", an active collaboration of capitalist and socialist visions.[417] Numerous studies and surveys indicate that people tend to live happier and healthier lives in social democratic societies rather than neoliberal ones.[418][419][420][421][422]
Social democrats advocate a peaceful, evolutionary transition of the economy to socialism through progressive social reform.[423][424] It asserts that the only acceptable constitutional form of government is representative democracy under the rule of law.[425] It promotes extending democratic decision-making beyond political democracy to include economic democracy to guarantee employees and other economic stakeholders sufficient rights of co-determination.[425] It supports a mixed economy that opposes inequality, poverty and oppression while rejecting both a totally unregulated market economy or a fully planned economy.[426] Common social democratic policies include universal social rights and universally accessible public services such as education, health care, workers' compensation and other services, including child care and elder care.[427] Social democracy supports the trade union labour movement and supports collective bargaining rights for workers.[428] Most social democratic parties are affiliated with the Socialist International.[414]
Modern democratic socialism is a broad political movement that seeks to promote the ideals of socialism within the context of a democratic system. Some democratic socialists support social democracy as a temporary measure to reform the current system while others reject reformism in favour of more revolutionary methods. Modern social democracy emphasises a program of gradual legislative modification of capitalism in order to make it more equitable and humane while the theoretical end goal of building a socialist society is relegated to the indefinite future. According to Sheri Berman, Marxism is loosely held to be valuable for its emphasis on changing the world for a more just, better future.[429]
The two movements are widely similar both in terminology and in ideology, although there are a few key differences. The major difference between social democracy and democratic socialism is the object of their politics in that contemporary social democrats support a welfare state and unemployment insurance as well as other practical, progressive reforms of capitalism and are more concerned to administrate and humanise it. On the other hand, democratic socialists seek to replace capitalism with a socialist economic system, arguing that any attempt to humanise capitalism through regulations and welfare policies would distort the market and create economic contradictions.[430]
Ethical socialism appeals to socialism on ethical and moral grounds as opposed to economic, egoistic and consumeristic grounds. It emphasizes the need for a morally conscious economy based upon the principles of altruism, cooperation and social justice while opposing possessive individualism.[431] Ethical socialism has been the official philosophy of mainstream socialist parties.[432]
Liberal socialism incorporates liberal principles to socialism.[433] It has been compared to post-war social democracy[434] for its support of a mixed economy that includes both public and private capital goods.[435][436] While democratic socialism and social democracy are anti-capitalist positions insofar as criticism of capitalism is linked to the private ownership of the means of production,[378] liberal socialism identifies artificial and legalistic monopolies to be the fault of capitalism[437] and opposes an entirely unregulated market economy.[438] It considers both liberty and social equality to be compatible and mutually dependent.[433]
Principles that can be described as ethical or liberal socialist have been based upon or developed by philosophers such as John Stuart Mill, Eduard Bernstein, John Dewey, Carlo Rosselli, Norberto Bobbio and Chantal Mouffe.[439] Other important liberal socialist figures include Guido Calogero, Piero Gobetti, Leonard Trelawny Hobhouse, John Maynard Keynes and R. H. Tawney.[438] Liberal socialism has been particularly prominent in British and Italian politics.[438]
Blanquism is a conception of revolution named for Louis Auguste Blanqui. It holds that socialist revolution should be carried out by a relatively small group of highly organised and secretive conspirators.[440] Upon seizing power, the revolutionaries introduce socialism.[441] Rosa Luxemburg and Eduard Bernstein[442] criticised Lenin, stating that his conception of revolution was elitist and Blanquist.[443] Marxism–Leninism combines Marx's scientific socialist concepts and Lenin's anti-imperialism, democratic centralism and vanguardism.[444]
Hal Draper defined socialism from above as the philosophy which employs an elite administration to run the socialist state. The other side of socialism is a more democratic socialism from below.[445] The idea of socialism from above is much more frequently discussed in elite circles than socialism from below—even if that is the Marxist ideal—because it is more practical.[446] Draper viewed socialism from below as being the purer, more Marxist version of socialism.[447] According to Draper, Karl Marx and Friedrich Engels were devoutly opposed to any socialist institution that was "conducive to superstitious authoritarianism". Draper makes the argument that this division echoes the division between "reformist or revolutionary, peaceful or violent, democratic or authoritarian, etc." and further identifies six major varieties of socialism from above, among them "Philanthropism", "Elitism", "Pannism", "Communism", "Permeationism" and "Socialism-from-Outside".[448]
According to Arthur Lipow, Marx and Engels were "the founders of modern revolutionary democratic socialism", described as a form of "socialism from below" that is "based on a mass working-class movement, fighting from below for the extension of democracy and human freedom". This type of socialism is contrasted to that of the "authoritarian, antidemocratic creed" and "the various totalitarian collectivist ideologies which claim the title of socialism" as well as "the many varieties of 'socialism from above' which have led in the twentieth century to movements and state forms in which a despotic 'new class' rules over a statified economy in the name of socialism", a division that "runs through the history of the socialist movement". Lipow identifies Bellamyism and Stalinism as two prominent authoritarian socialist currents within the history of the socialist movement.[449]
Libertarian socialism, sometimes called left-libertarianism,[452][453] social anarchism[454][455] and socialist libertarianism,[456] is an anti-authoritarian, anti-statist and libertarian[457] tradition within socialism that rejects centralised state ownership and control[458] including criticism of wage labour relationships (wage slavery)[459] as well as the state itself.[460] It emphasises workers' self-management and decentralised structures of political organisation.[460][461] Libertarian socialism asserts that a society based on freedom and equality can be achieved through abolishing authoritarian institutions that control production.[462] Libertarian socialists generally prefer direct democracy and federal or confederal associations such as libertarian municipalism, citizens' assemblies, trade unions and workers' councils.[463][464]
Anarcho-syndicalist Gaston Leval explained:
We therefore foresee a Society in which all activities will be coordinated, a structure that has, at the same time, sufficient flexibility to permit the greatest possible autonomy for social life, or for the life of each enterprise, and enough cohesiveness to prevent all disorder. ... In a well-organised society, all of these things must be systematically accomplished by means of parallel federations, vertically united at the highest levels, constituting one vast organism in which all economic functions will be performed in solidarity with all others and that will permanently preserve the necessary cohesion".[465]All of this is typically done within a general call for libertarian[466] and voluntary free associations[467] through the identification, criticism and practical dismantling of illegitimate authority in all aspects of human life.[391][468][394]
As part of the larger socialist movement, it seeks to distinguish itself from Bolshevism, Leninism and Marxism–Leninism as well as social democracy.[469] Past and present political philosophies and movements commonly described as libertarian socialist include anarchism (anarcho-communism, anarcho-syndicalism,[citation needed] collectivist anarchism, individualist anarchism[470][471][472] and mutualism),[473] autonomism, Communalism, participism, libertarian Marxism (council communism and Luxemburgism),[474][475] revolutionary syndicalism and utopian socialism (Fourierism).[476]
Christian socialism is a broad concept involving an intertwining of Christian religion with socialism.[477]
Islamic socialism is a more spiritual form of socialism. Muslim socialists believe that the teachings of the Quran and Muhammad are not only compatible with, but actively promoting the principles of equality and public ownership, drawing inspiration from the early Medina welfare state he established. Muslim socialists are more conservative than their Western contemporaries and find their roots in anti-imperialism, anti-colonialism[478][479] and sometimes, if in an Arab speaking country, Arab nationalism. Islamic socialists believe in deriving legitimacy from political mandate as opposed to religious texts.
Socialist feminism is a branch of feminism that argues that liberation can only be achieved by working to end both economic and cultural sources of women's oppression.[480] Marxist feminism's foundation was laid by Engels in The Origin of the Family, Private Property, and the State (1884). August Bebel's Woman under Socialism (1879), is the "single work dealing with sexuality most widely read by rank-and-file members of the Social Democratic Party of Germany (SPD)".[481] In the late 19th and early 20th centuries, both Clara Zetkin and Eleanor Marx were against the demonisation of men and supported a proletariat revolution that would overcome as many male-female inequalities as possible.[482] As their movement already had the most radical demands in women's equality, most Marxist leaders, including Clara Zetkin[483][484] and Alexandra Kollontai,[485][486] counterposed Marxism against liberal feminism rather than trying to combine them. Anarcha-feminism began with late 19th- and early 20th-century authors and theorists such as anarchist feminists Goldman and Voltairine de Cleyre[487] In the Spanish Civil War, an anarcha-feminist group, Mujeres Libres ("Free Women") linked to the Federación Anarquista Ibérica, organised to defend both anarchist and feminist ideas.[488] In 1972, the Chicago Women's Liberation Union published "Socialist Feminism: A Strategy for the Women's Movement", which is believed to be the first published use of the term "socialist feminism".[489]
Many socialists were early advocates of LGBT rights. For early socialist Charles Fourier, true freedom could only occur without suppressing passions, as the suppression of passions is not only destructive to the individual, but to society as a whole. Writing before the advent of the term "homosexuality", Fourier recognised that both men and women have a wide range of sexual needs and preferences which may change throughout their lives, including same-sex sexuality and androgénité. He argued that all sexual expressions should be enjoyed as long as people are not abused and that "affirming one's difference" can actually enhance social integration.[490][491] In Oscar Wilde's The Soul of Man Under Socialism, he advocates an egalitarian society where wealth is shared by all, while warning of the dangers of social systems that crush individuality.[492] Edward Carpenter actively campaigned for homosexual rights. His work The Intermediate Sex: A Study of Some Transitional Types of Men and Women was a 1908 book arguing for gay liberation.[493] who was an influential personality in the foundation of the Fabian Society and the Labour Party. After the Russian Revolution under the leadership of Lenin and Trotsky, the Soviet Union abolished previous laws against homosexuality.[494] Harry Hay was an early leader in the American LGBT rights movement as well as a member of the Communist Party USA. He is known for his roles in helping to found gay organisations, including the Mattachine Society, the first sustained gay rights group in the United States which in its early days reflected a strong Marxist influence. The Encyclopedia of Homosexuality reports that "[a]s Marxists the founders of the group believed that the injustice and oppression which they suffered stemmed from relationships deeply embedded in the structure of American society".[495] Emerging from events such as the May 1968 insurrection in France, the anti-Vietnam war movement in the US and the Stonewall riots of 1969, militant gay liberation organisations began to spring up around the world. Many sprang from left radicalism more than established homophile groups,[496] although the Gay Liberation Front took an anti-capitalist stance and attacked the nuclear family and traditional gender roles.[497]
Eco-socialism is a political strain merging aspects of socialism, Marxism or libertarian socialism with green politics, ecology and alter-globalisation. Eco-socialists generally claim that the expansion of the capitalist system is the cause of social exclusion, poverty, war and environmental degradation through globalisation and imperialism under the supervision of repressive states and transnational structures.[498] Contrary to the depiction of Karl Marx by some environmentalists,[499] social ecologists[500] and fellow socialists[501] as a productivist who favoured the domination of nature, eco-socialists revisited Marx's writings and believe that he "was a main originator of the ecological world-view".[502] Marx discussed a "metabolic rift" between man and nature, stating that "private ownership of the globe by single individuals will appear quite absurd as private ownership of one man by another" and his observation that a society must "hand it [the planet] down to succeeding generations in an improved condition".[503] English socialist William Morris is credited with developing principles of what was later called eco-socialism.[504] During the 1880s and 1890s, Morris promoted his ideas within the Social Democratic Federation and Socialist League.[505] Green anarchism blends anarchism with environmental issues. An important early influence was Henry David Thoreau and his book Walden[506] as well as Élisée Reclus.[507][508]
In the late 19th century, anarcho-naturism fused anarchism and naturist philosophies within individualist anarchist circles in France, Spain, Cuba[509] and Portugal.[510] Murray Bookchin's first book Our Synthetic Environment[511] was followed by his essay "Ecology and Revolutionary Thought" which introduced ecology as a concept in radical politics.[512] In the 1970s, Barry Commoner, claimed that capitalist technologies were chiefly responsible for environmental degradation as opposed to population pressures.[513] In the 1990s socialist/feminists Mary Mellor[514] and Ariel Salleh[515] adopt an eco-socialist paradigm. An "environmentalism of the poor" combining ecological awareness and social justice has also become prominent.[516] Pepper critiqued the current approach of many within green politics, particularly deep ecologists.[517]
Syndicalism operates through industrial trade unions. It rejects state socialism and the use of establishment politics. Syndicalists reject state power in favour of strategies such as the general strike. Syndicalists advocate a socialist economy based on federated unions or syndicates of workers who own and manage the means of production. Some Marxist currents advocate syndicalism, such as De Leonism. Anarcho-syndicalism views syndicalism as a method for workers in capitalist society to gain control of an economy. The Spanish Revolution was largely orchestrated by the anarcho-syndicalist trade union CNT.[518] The International Workers' Association is an international federation of anarcho-syndicalist labour unions and initiatives.[519]
According to analytical Marxist sociologist Erik Olin Wright, "The Right condemned socialism as violating individual rights to private property and unleashing monstrous forms of state oppression", while "the Left saw it as opening up new vistas of social equality, genuine freedom and the development of human potentials."[520]
Because of socialism's many varieties, most critiques have focused on a specific approach. Proponents of one approach typically criticise others. Socialism has been criticised in terms of its models of economic organization as well as its political and social implications. Other critiques are directed at the socialist movement, parties, or existing states.
Some forms of criticism occupy theoretical grounds, such as in the economic calculation problem presented by proponents of the Austrian School as part of the socialist calculation debate, while others support their criticism by examining historical attempts to establish socialist societies. The economic calculation problem concerns the feasibility and methods of resource allocation for a planned socialist system.[521][522][523] Central planning is also criticized by elements of the radical left. Libertarian socialist economist Robin Hahnel notes that even if central planning overcame its inherent inhibitions of incentives and innovation, it would nevertheless be unable to maximize economic democracy and self-management, which he believes are concepts that are more intellectually coherent, consistent and just than mainstream notions of economic freedom.[524]
Economic liberals and right-libertarians argue that private ownership of the means of production and market exchange are natural entities or moral rights which are central to freedom and liberty and argue that the economic dynamics of capitalism are immutable and absolute. As such, they also argue that public ownership of the means of production and economic planning are infringements upon liberty.[525][526]
Critics of socialism have argued that in any society where everyone holds equal wealth, there can be no material incentive to work because one does not receive rewards for a work well done. They further argue that incentives increase productivity for all people and that the loss of those effects would lead to stagnation. Some critics of socialism argue that income sharing reduces individual incentives to work and therefore incomes should be individualized as much as possible.[527]
Some philosophers have also criticized the aims of socialism, arguing that equality erodes away at individual diversities and that the establishment of an equal society would have to entail strong coercion.[528]
Many commentators on the political right point to the mass killings under communist regimes, claiming them as an indictment of socialism.[529][530][531] Opponents of this view, including supporters of socialism, state that these killings were aberrations caused by specific authoritarian regimes, and not caused by socialism itself, and point to mass deaths in famines, wars and massacres that they claim were caused by colonialism, capitalism and anti-communism as a counterpoint to those killings.[532]



The Commonwealth of Nations, simply referred to as the Commonwealth,[4] is a political association of 56 member states, the vast majority of which are former territories of the British Empire.[5] The chief institutions of the organisation are the Commonwealth Secretariat, which focuses on intergovernmental aspects, and the Commonwealth Foundation, which focuses on non-governmental relations among member states.[6] Numerous organisations are associated with and operate within the Commonwealth.[7]
The Commonwealth dates back to the first half of the 20th century with the decolonisation of the British Empire through increased self-governance of its territories. It was originally created as the British Commonwealth of Nations[8] through the Balfour Declaration at the 1926 Imperial Conference, and formalised by the United Kingdom through the Statute of Westminster in 1931. The current Commonwealth of Nations was formally constituted by the London Declaration in 1949, which modernised the community and established the member states as "free and equal".[9]
The head of the Commonwealth is Charles III.  He is king of 15 member states, known as the Commonwealth realms, while 36 other members are republics, and five others have different monarchs.[10]
Member states have no legal obligations to one another but are connected through their use of the English language and historical ties. Citizenship of a Commonwealth nation affords benefits in some member countries, particularly in the United Kingdom. The Commonwealth Charter defines their shared values of democracy, human rights, and the rule of law,[11] as promoted by the quadrennial Commonwealth Games.
Queen Elizabeth II, in her address to Canada on Dominion Day in 1959, pointed out that the Confederation of Canada on 1 July 1867 had been the birth of the "first independent country within the British Empire". She declared: "So, it also marks the beginning of that free association of independent states which is now known as the Commonwealth of Nations."[12] As long ago as 1884 Lord Rosebery, while visiting Australia, had described the changing British Empire, as some of its colonies became more independent, as a "Commonwealth of Nations".[13] Conferences of British and colonial prime ministers occurred periodically from the first one in 1887, leading to the creation of the Imperial Conferences in 1911.[14]
The Commonwealth developed from the imperial conferences. A specific proposal was presented by Jan Smuts in 1917 when he coined the term "the British Commonwealth of Nations" and envisioned the "future constitutional relations and readjustments in essence"[15] at the Paris Peace Conference of 1919, attended by delegates from the Dominions as well as the United Kingdom.[16][17] The term first received imperial statutory recognition in the Anglo-Irish Treaty of 1921, when the term British Commonwealth of Nations was substituted for British Empire in the wording of the oath taken by members of parliament of the Irish Free State.[18]
In the Balfour Declaration at the 1926 Imperial Conference, the United Kingdom and its dominions agreed they were "equal in status, in no way subordinate one to another in any aspect of their domestic or external affairs, though united by common allegiance to the Crown, and freely associated as members of the British Commonwealth of Nations". The term "Commonwealth" was officially adopted to describe the community.[19]
These aspects to the relationship were formalised by the Statute of Westminster in 1931, which applied to Canada without the need for ratification, but Australia, New Zealand, and Newfoundland had to ratify the statute for it to take effect. Newfoundland never did as due to economic hardship and the need for financial assistance from London, Newfoundland voluntarily accepted the suspension of self-government in 1934 and governance reverted to direct control from London. Newfoundland later joined Canada as its tenth province in 1949.[20] Australia and New Zealand ratified the Statute in 1942 and 1947 respectively.[21][22]
Although the Union of South Africa was not among the Dominions that needed to adopt the Statute of Westminster for it to take effect, two laws—the Status of the Union Act, 1934, and the Royal Executive Functions and Seals Act of 1934—were passed to confirm South Africa's status as a sovereign state.[23]
After the Second World War ended, the British Empire was gradually dismantled.  Most of its components have become independent countries, whether Commonwealth realms or republics, and members of the Commonwealth. There remain the 14 mainly self-governing British overseas territories which retain some political association with the United Kingdom. In April 1949, following the London Declaration, the word "British" was dropped from the title of the Commonwealth to reflect its changing nature.[24]
Burma (Myanmar since 1989) and Aden (now part of the Republic of Yemen) are the only states that were British colonies at the time of the war not to have joined the Commonwealth upon independence. Former British protectorates and mandates that did not become members of the Commonwealth are Egypt (independent in 1922), Iraq (1932), Transjordan (1946), Palestine (part of which became the state of Israel in 1948), Sudan (1956), British Somaliland (which united with the former Italian Somaliland in 1960 to form the Somali Republic), Kuwait (1961), Bahrain (1971), Oman (1971), Qatar (1971), and the United Arab Emirates (1971).[25]
The postwar Commonwealth was given a fresh mission by Queen Elizabeth II in her Christmas Day 1953 broadcast, in which she envisioned the Commonwealth as "an entirely new conception – built on the highest qualities of the Spirit of Man: friendship, loyalty, and the desire for freedom and peace".[26] Hoped-for success was reinforced by such achievements as climbing Mount Everest in 1953, breaking the four-minute mile in 1954, and a solo circumnavigation of the globe in 1966.[27]
After the Second World War, the British treasury was so weak that it could not operate independently of the United States. The loss of defence and financial roles, furthermore, undermined Joseph Chamberlain's early 20th-century vision of a world empire that could combine Imperial preference, mutual defence, and social growth. In addition, the United Kingdom's cosmopolitan role in world affairs became increasingly limited, especially with the losses of India and Singapore.[28] While British politicians at first hoped that the Commonwealth would preserve and project British influence, they gradually lost their enthusiasm, argues Krishnan Srinivasan. Early enthusiasm waned as British policies came under fire at Commonwealth meetings. Public opinion became troubled as immigration from non-white member states became large-scale.[29]
On 18 April 1949, Ireland formally became a republic in accordance with the Irish Republic of Ireland Act 1948; in doing so, it also formally left the Commonwealth.[30] While Ireland had not actively participated in the Commonwealth since the early 1930s, other dominions wished to become republics without losing Commonwealth ties. The issue came to a head in April 1949 at a Commonwealth prime ministers' meeting in London. Under the London Declaration, India agreed that, when it became a republic in January 1950, it would remain in the Commonwealth and accept the British Sovereign as a "symbol of the free association of its independent member nations and as such the Head of the Commonwealth". Upon hearing this, King George VI told the Indian politician Krishna Menon: "So, I've become 'as such'".[31] Some other Commonwealth countries that have since become republics have chosen to leave, while others, such as Guyana, Mauritius and Dominica, have remained members.[32]
The London Declaration is often seen as marking the beginning of the modern Commonwealth. Following India's precedent, other nations became republics, or constitutional monarchies with their own monarchs. While some countries retained the same monarch as the United Kingdom, their monarchies developed differently and soon became essentially independent of the British monarchy. The monarch is regarded as a separate legal personality in each realm, even though the same person is monarch of each realm.[33][34][35][36]
Planners in the interwar period, like Lord Davies, who had also taken "a prominent part in building up the League of Nations Union" in the United Kingdom, in 1932 founded the New Commonwealth Society, of whose British section Winston Churchill became the president.[37]
The term 'New Commonwealth' gained usage in the UK (especially in the 1960s and 1970s) to refer to recently decolonised countries, predominantly non-white and developing. It was often used in debates about immigration from these countries.[38] The United Kingdom and the pre-1945 dominions became informally known as the 'Old Commonwealth', or more pointedly as the 'white Commonwealth',[39] in reference to what had been known as the 'White Dominions'.
At a time when Germany and France, together with Belgium, Italy, Luxembourg, and the Netherlands, were planning what later became the European Union, and newly independent African countries were joining the Commonwealth, new ideas were floated to prevent the United Kingdom from becoming isolated in economic affairs. British trade with the Commonwealth was four times larger than its trade with Europe. In 1956 and 1957 the British government under Prime Minister Anthony Eden considered a "Plan G" to create a European free trade zone while also protecting the favoured status of the Commonwealth.[40][41][42] The United Kingdom also considered inviting Scandinavian and other European countries to join the Commonwealth, so that it would become a major economic common market.
At the time of the Suez Crisis in 1956, in the face of colonial unrest and international tensions, French prime minister Guy Mollet proposed to British prime minister Anthony Eden that their two countries be joined in a "union". When that proposal was turned down, Mollet suggested that France join the Commonwealth, possibly with "a common citizenship arrangement on the Irish basis". These ideas faded away with the end of the Suez Crisis.[43][44][45]
Under the formula of the London Declaration, Charles III is the head of the Commonwealth.[2][46] When the monarch dies, the successor to the crown does not automatically become the new head of the Commonwealth.[47] However, at their meeting in April 2018, Commonwealth leaders agreed that Prince Charles should succeed his mother Elizabeth II as head after her death.[48] The position is symbolic, representing the free association of independent members,[46] the majority of which (36) are republics, and five have monarchs of different royal houses (Brunei, Eswatini, Lesotho, Malaysia, and Tonga).
The main decision-making forum of the organisation is the biennial Commonwealth Heads of Government Meeting (CHOGM), where Commonwealth heads of government, including (amongst others) prime ministers and presidents, assemble for several days to discuss matters of mutual interest. CHOGM is the successor to the Meetings of Commonwealth Prime Ministers and, earlier, the Imperial Conferences and Colonial Conferences, dating back to 1887. There are also regular meetings of finance ministers, law ministers, health ministers, etc. Members in arrears, as special members before them, are not invited to send representatives to either ministerial meetings or CHOGMs.[46]
The head of government hosting the CHOGM is called the chair-in-office (CIO) and retains the position until the following CHOGM. Since the most recent CHOGM, in Rwanda in 2022, the chair-in-office has been the president of Rwanda.[49]
The 26th CHOGM was initially to be held in Kigali, Rwanda, in June 2020. Owing to the COVID-19 pandemic, it was rescheduled to be held there in the week of 21 June 2021 before again being postponed to 25-26 June 2022. It was accompanied by meetings of a Commonwealth Youth Forum, a Commonwealth Women's Forum and a Commonwealth People's Forum.[50]
The Commonwealth Secretariat, established in 1965, is the main intergovernmental agency of the Commonwealth, facilitating consultation and co-operation among member governments and countries.[51] It is responsible to member governments collectively. The Commonwealth of Nations is represented in the United Nations General Assembly by the secretariat as an observer. The secretariat organises Commonwealth summits, meetings of ministers, consultative meetings and technical discussions; it assists policy development and provides policy advice, and facilitates multilateral communication among the member governments. It also provides technical assistance to help governments in the social and economic development of their countries and in support of the Commonwealth's fundamental political values.[52]
The secretariat is headed by the Commonwealth secretary-general, who is elected by the Commonwealth heads of government for no more than two four-year terms. The secretary-general and two deputy secretaries-general direct the divisions of the Secretariat. The present secretary-general is Patricia Scotland, Baroness Scotland of Asthal, from Dominica, who took office on 1 April 2016, succeeding Kamalesh Sharma of India (2008–2016). The first secretary-general was Arnold Smith of Canada (1965–75), followed by Sir Shridath Ramphal of Guyana (1975–90), Chief Emeka Anyaoku of Nigeria (1990–99), and Don McKinnon of New Zealand (2000–2008).[52]
Initially, Commonwealth countries were not considered to be "foreign" to each other as their citizens were British subjects.[53][54][55] Citizenship laws have evolved independently in each Commonwealth country. For example, in Australia, for the purpose of considering certain constitutional and legal provisions no distinction is made between Commonwealth and foreign countries: in the High Court case of Sue v Hill, other Commonwealth countries (specifically, the United Kingdom) were held to be 'foreign powers'; similarly, in Nolan v Minister for Immigration and Ethnic Affairs, the nationals of other Commonwealth realms were held to be 'aliens'.
Nevertheless, some members treat resident citizens of other Commonwealth countries preferentially to citizens of non-Commonwealth countries (see Commonwealth citizen). The United Kingdom and several others, mostly in the Caribbean, grant the right to vote to Commonwealth citizens who reside in those countries.
The closer association amongst Commonwealth countries is reflected in the diplomatic protocols of the Commonwealth countries. For example, when engaging bilaterally with one another, Commonwealth governments exchange high commissioners instead of ambassadors.[56] In non-Commonwealth countries in which their own country is not represented, Commonwealth citizens may seek consular assistance at the British embassy although it is for the embassy to decide, in its discretion, whether to provide any.[57] Other alternatives can also occur such as the consular services agreement between Canada and Australia that began in 1986.[58]
The criteria for membership of the Commonwealth of Nations have developed over time from a series of separate documents. The Statute of Westminster 1931, as a fundamental founding document of the organisation, laid out that membership required dominionhood. The 1949 London Declaration ended this, allowing republican and indigenous monarchic members on the condition that they recognised King George VI as "Head of the Commonwealth".[59] In the wake of the wave of decolonisation in the 1960s, these constitutional principles were augmented by political, economic, and social principles. The first of these was set out in 1961, when it was decided that respect for racial equality would be a requirement for membership, leading directly to the withdrawal of South Africa's re-application (which they were required to make under the formula of the London Declaration upon becoming a republic). The 14 points of the 1971 Singapore Declaration dedicated all members to the principles of world peace, liberty, human rights, equality, and free trade.[60]
These criteria were unenforceable for two decades,[61] until, in 1991, the Harare Declaration was issued, dedicating the leaders to applying the Singapore principles to the completion of decolonisation, the end of the Cold War, and the end of apartheid in South Africa.[62] The mechanisms by which these principles would be applied were created, and the manner clarified, by the 1995 Millbrook Commonwealth Action Programme, which created the Commonwealth Ministerial Action Group (CMAG), which has the power to rule on whether members meet the requirements for membership under the Harare Declaration.[63] Also in 1995, an Inter-Governmental Group was created to finalise and codify the full requirements for membership. Upon reporting in 1997, as adopted under the Edinburgh Declaration, the Inter-Governmental Group ruled that any future members would "as a rule" have to have a direct constitutional link with an existing member.[64]
In addition to this new rule, the former rules were consolidated into a single document. These requirements are that members must accept and comply with the Harare principles, be fully sovereign states, recognise King Charles III as head of the Commonwealth, accept the English language as the means of Commonwealth communication, and respect the wishes of the general population with regard to Commonwealth membership.[64] These requirements had undergone review, and a report on potential amendments was presented by the Committee on Commonwealth Membership at the 2007 Commonwealth Heads of Government Meeting.[65] New members were not admitted at this meeting, though applications for admission were considered at the 2009 CHOGM.[66]
New members must "as a general rule" have a direct constitutional link to an existing member. In most cases, this is due to being a former colony of the United Kingdom, but some have links to other countries, either exclusively or more directly (e.g., Samoa to New Zealand, Papua New Guinea to Australia, and Singapore to Malaysia). The first member to be admitted without having any constitutional link to the British Empire was Mozambique in 1995 following its first democratic elections. Mozambique was a former Portuguese colony and the Edinburgh Declaration and the current membership guidelines came after its entry.[67]
In 2009, Rwanda became the second Commonwealth member admitted not to have any constitutional links to Britain. It was a Belgian trust territory that had been a district of German East Africa until World War I.[68] Consideration for Rwanda's admission was considered an "exceptional circumstance" by the Commonwealth Secretariat.[69] Rwanda was permitted to join despite the Commonwealth Human Rights Initiative (CHRI) finding that "the state of governance and human rights in Rwanda does not satisfy Commonwealth standards", and that it "does not therefore qualify for admission".[70] CHRI commented that: "It does not make sense to admit a state that already does not satisfy Commonwealth standards. This would tarnish the reputation of the Commonwealth and confirm the opinion of many people and civic organisations that the leaders of its governments do not really care for democracy and human rights, and that its periodic, solemn declarations are merely hot air."[70]
In 2022, Togo, a former French mandate territory, and Gabon, a former French colony, joined the Commonwealth, despite never having been under British rule.[71] 
The Commonwealth comprises 56 countries, across all inhabited continents.[72] The members have a combined population of 2.4 billion people, almost a third of the world population, with 1.4 billion living in India, and 94% living in either Asia or Africa.[73] After India, the next-largest Commonwealth countries by population are Pakistan (227 million), Nigeria (213 million), Bangladesh (167 million), and the United Kingdom (68 million). Tuvalu is the smallest member, with about 12,000 people.[74]
The land area of the Commonwealth nations is about 31,500,000 km2 (12,200,000 sq mi), or about 21% of the total world land area. The two largest Commonwealth nations by area are Canada at 9,984,670 km2 (3,855,100 sq mi) and Australia at 7,617,930 km2 (2,941,300 sq mi).[75]
The status of "Member in Arrears" is used to denote those that are in arrears in paying subscription dues. The status was originally known as "special membership", but was renamed on the Committee on Commonwealth Membership's recommendation.[76] There are currently no Members in Arrears. The most recent Member in Arrears, Nauru, returned to full membership in June 2011.[77] Nauru has alternated between special and full membership since joining the Commonwealth, depending on its financial situation.[78]
In 2019, the Commonwealth members had a combined gross domestic product of over $9 trillion, 78% of which is accounted for by the four largest economies: India ($3.737 trillion), United Kingdom($3.124 trillion), Canada ($1.652 trillion), and Australia ($1.379 trillion).[79]
In 1997 the Commonwealth Heads of Government agreed that, to become a member of the Commonwealth, an applicant country should, as a rule, have had a constitutional association with an existing Commonwealth member; that it should comply with Commonwealth values, principles and priorities as set out in the Harare Declaration; and that it should accept Commonwealth norms and conventions.[80]
South Sudanese politicians have expressed interest in joining the Commonwealth.[81] A senior Commonwealth source stated in 2006 that "many people have assumed an interest from Israel, but there has been no formal approach".[82] Israel and Palestine are both potential candidates for membership.[82]
President Yahya Jammeh unilaterally withdrew the Gambia from the Commonwealth in October 2013.[83] However, newly elected president Adama Barrow returned the country to the organisation in February 2018.[84]
Other eligible applicants could be any of the remaining inhabited British Overseas Territories, Crown Dependencies, Australian external territories and the Associated States of New Zealand if they become fully independent.[85] Many such jurisdictions are already directly represented within the Commonwealth, particularly through the Commonwealth Family.[86] There are also former British possessions that have not become independent. Although Hong Kong has become part of China, it continues to participate in some of the institutions within the Commonwealth Family, including the Commonwealth Lawyers Association, the Commonwealth Parliamentary Association, the Association of Commonwealth Universities, the Commonwealth Association of Legislative Counsel[87][88] and the Commonwealth War Graves Commission (CWGC).
All three of the Crown dependencies regard their existing situation as unsatisfactory and have lobbied for change. The States of Jersey have called on the UK foreign secretary to request that the Commonwealth heads of government "consider granting associate membership to Jersey and the other Crown Dependencies as well as any other territories at a similarly advanced stage of autonomy". Jersey has proposed that it be accorded "self-representation in all Commonwealth meetings; full participation in debates and procedures, with a right to speak where relevant and the opportunity to enter into discussions with those who are full members; and no right to vote in the Ministerial or Heads of Government meetings, which is reserved for full members".[89] The States of Guernsey and the Government of the Isle of Man have made calls of a similar nature for a more integrated relationship with the Commonwealth,[90] including more direct representation and enhanced participation in Commonwealth organisations and meetings, including Commonwealth Heads of Government Meetings.[91] The Chief Minister of the Isle of Man has said: "A closer connection with the Commonwealth itself would be a welcome further development of the Island's international relationships".[92]
Members can be suspended "from the Councils of the Commonwealth" for "serious or persistent violations" of the Harare Declaration, particularly in abrogating their responsibility to have democratic government.[93] Suspensions are agreed by the Commonwealth Ministerial Action Group (CMAG), which meets regularly to address potential breaches of the Harare Declaration. Suspended members are not represented at meetings of Commonwealth leaders and ministers, although they remain members of the organisation.
Nigeria was suspended between 11 November 1995 and 29 May 1999,[94] following its execution of Ken Saro-Wiwa on the eve of the 1995 CHOGM.[95] Pakistan was the second country to be suspended, on 18 October 1999, following the military coup by Pervez Musharraf.[96] The Commonwealth's longest suspension came to an end on 22 May 2004, when Pakistan's suspension was lifted following the restoration of the country's constitution.[97] Pakistan was suspended for a second time, far more briefly, for six months from 22 November 2007, when Musharraf called a state of emergency.[98] Zimbabwe was suspended in 2002 over concerns regarding the electoral and land reform policies of Robert Mugabe's ZANU-PF government,[99] before it withdrew from the organisation in 2003.[100] On 15 May 2018, Zimbabwe applied to rejoin the Commonwealth.[101]
The declaration of a Republic in Fiji in 1987, after military coups designed to deny Indo-Fijians political power, was not accompanied by an application to remain. Commonwealth membership was held to have lapsed until 1997, after discriminatory provisions in the republican constitution were repealed and reapplication for membership made.[102][103] Fiji has since been suspended twice, with the first imposed from 6 June 2000[104] to 20 December 2001 after another coup.[99] Fiji was suspended yet again in December 2006, following the most recent coup. At first, the suspension applied only to membership on the Councils of the Commonwealth.[102][105] After failing to meet a Commonwealth deadline for setting a date for national elections by 2010, Fiji was "fully suspended" on 1 September 2009.[102][105] The secretary-general of the Commonwealth, Kamalesh Sharma, confirmed that full suspension meant that Fiji would be excluded from Commonwealth meetings, sporting events and the technical assistance programme (with an exception for assistance in re-establishing democracy). Sharma stated that Fiji would remain a member of the Commonwealth during its suspension, but would be excluded from emblematic representation by the secretariat.[102] On 19 March 2014 Fiji's full suspension was amended to a suspension from councils of the Commonwealth by the Commonwealth Ministerial Action Group, permitting Fiji to join a number of Commonwealth activities, including the Commonwealth Games.[106] Fiji's suspension was lifted in September 2014.[107] The Commonwealth Ministerial Action Group fully reinstated Fiji as a member following elections in September 2014.[108]
Most recently, during 2013 and 2014, international pressure mounted to suspend Sri Lanka from the Commonwealth, citing grave human rights violations by the government of President Mahinda Rajapaksa. There were also calls to change the Commonwealth Heads of Government Meeting 2013 from Sri Lanka to another member country. Canadian prime minister Stephen Harper threatened to boycott the event, but was instead represented at the meeting by Deepak Obhrai. UK Prime Minister David Cameron also chose to attend.[109][110] These concerns were rendered moot by the election of opposition leader Maithripala Sirisena as president in 2015.[111]
As membership is purely voluntary, member governments can choose at any time to leave the Commonwealth. The first state to do so was Ireland in 1948 following its decision to declare itself a republic. At the time, all members accepted the British monarch as head of state as a condition of membership. This rule was changed after Ireland's departure to allow India to retain membership when it became a republic, although Ireland did not rejoin. Pakistan left on 30 January 1972 in protest at the Commonwealth's recognition of breakaway Bangladesh, but rejoined on 2 August 1989. Zimbabwe's membership was suspended in 2002 on the grounds of alleged human rights violations and deliberate misgovernment, and Zimbabwe's government terminated its membership in 2003.[112] The Gambia left the Commonwealth on 3 October 2013,[83] and rejoined on 8 February 2018.[84]
The Maldives withdrew from the Commonwealth on 13 October 2016,[113][114] citing Commonwealth's "punitive actions against the Maldives since 2012" after the allegedly forced resignation of Maldivian President Mohamed Nasheed among the reasons for withdrawal.[114] Following the election of Ibrahim Mohamed Solih as president in November 2018, the Maldives announced its intention to reapply to join the Commonwealth.[115] They rejoined on 1 February 2020.[116]
Although heads of government have the power to suspend member states from active participation, the Commonwealth has no provision for the expulsion of members.
Until 1948, there was a consensus among the existing half-dozen Commonwealth members that Commonwealth realms that became a republic would cease to be members but the situation changed in 1948 when newly independent India announced its intention to become a republic on 1 January 1950 although it wished to remain in the Commonwealth. This was granted. Now, the majority of the Commonwealth members, including all those from Africa, are republics or have their own native monarch.
Ireland withdrew from participation in the Commonwealth in the 1930s, attending its last Commonwealth governmental heads' meeting in 1932. For some years Ireland considered itself to be a republic outside the Commonwealth but the Commonwealth considered Ireland to still be a Commonwealth member. Its treatment as a member ended on 18 April 1949 when Irish legislation that the Commonwealth chose to regard as having caused Ireland to become a republic became law. It is the only country whose membership terminated without any declaration withdrawing from the organisation. Instead, it was (with its own tacit support) excluded from the organisation.
South Africa was barred from continuing as a member after it became a republic in 1961, due to hostility from many members, particularly those in Africa and Asia as well as Canada, to its policy of racial apartheid. The South African government withdrew its application to remain in the organisation as a republic when it became clear at the 1961 Commonwealth Prime Ministers' Conference that any such application would be rejected. South Africa was re-admitted to the Commonwealth in 1994, following its first multiracial elections that year.[117]
The transfer of sovereignty over Hong Kong in 1997 ended the territory's status as a part of the Commonwealth through the United Kingdom. Non-sovereign states or regions are not permitted to become members of the Commonwealth. The government of the People's Republic of China has not pursued membership. Hong Kong has nevertheless continued to participate in some of the organisations of the Commonwealth Family, such as the Commonwealth Lawyers Association (hosted the Commonwealth Lawyers Conference in 1983 and 2009), the Commonwealth Parliamentary Association (and the Westminster Seminar on Parliamentary Practice and Procedures), the Association of Commonwealth Universities and the Commonwealth Association of Legislative Counsel,[87][88] as well as the Commonwealth War Graves Commission (CWGC).
The Commonwealth's objectives were first outlined in the 1971 Singapore Declaration, which committed the Commonwealth to the institution of world peace; promotion of representative democracy and individual liberty; the pursuit of equality and opposition to racism; the fight against poverty, ignorance, and disease; and free trade.[118] To these were added opposition to discrimination on the basis of gender by the Lusaka Declaration of 1979,[60] and environmental sustainability by the Langkawi Declaration of 1989.[119] These objectives were reinforced by the Harare Declaration in 1991.[120]
The Commonwealth's current highest-priority aims are on the promotion of democracy and development, as outlined in the 2003 Aso Rock Declaration,[121] which built on those in Singapore and Harare and clarified their terms of reference, stating, "We are committed to democracy, good governance, human rights, gender equality, and a more equitable sharing of the benefits of globalisation."[122] The Commonwealth website lists its areas of work as: democracy, economics, education, gender, governance, human rights, law, small states, sport, sustainability, and youth.[123]
In October 2010, a leaked memo from the Secretary General instructing staff not to speak out on human rights was published, leading to accusations that the Commonwealth was not being vocal enough on its core values.[124]
The Commonwealth Heads of Government Meeting 2011 considered a report by a Commonwealth Eminent Persons Group (EPG) panel which asserted that the organisation had lost its relevance and was decaying due to the lack of a mechanism to censure member countries when they violated human rights or democratic norms.[125] The panel made 106 "urgent" recommendations including the adoption of a Charter of the Commonwealth, the creation of a new commissioner on the rule of law, democracy and human rights to track persistent human rights abuses and allegations of political repression by Commonwealth member states, recommendations for the repeal of laws against homosexuality in 41 Commonwealth states and a ban on forced marriage.[126][127] The failure to release the report, or accept its recommendations for reforms in the area of human rights, democracy and the rule of law, was decried as a "disgrace" by former British Foreign Secretary Sir Malcolm Rifkind, a member of the EPG, who told a press conference: "The Commonwealth faces a very significant problem. It's not a problem of hostility or antagonism, it's more of a problem of indifference. Its purpose is being questioned, its relevance is being questioned and part of that is because its commitment to enforce the values for which it stands is becoming ambiguous in the eyes of many member states. The Commonwealth is not a private club of the governments or the secretariat. It belongs to the people of the Commonwealth."[127]
In the end, two-thirds of the EPG's 106 urgently recommended reforms were referred to study groups, an act described by one EPG member as having them "kicked into the long grass". There was no agreement to create the recommended position of human rights commissioner, instead a ministerial management group was empowered with enforcement: the group includes alleged human rights offenders. It was agreed to develop a charter of values for the Commonwealth without any decision on how compliance with its principles would be enforced.[125]
The result of the effort was that a new Charter of the Commonwealth was signed by Queen Elizabeth II on 11 March 2013 at Marlborough House, which opposes "all forms of discrimination, whether rooted in gender, race, colour, creed, political belief or other grounds".[128][129]
During the Second World War, the British Empire played a major role in supporting British finances.  Foreign exchange reserves were pooled in London, to be used to fight the war.  In effect the United Kingdom procured £2.3 billion, of which £1.3 billion was from India.  The debt was held in the form of British government securities and became known as "sterling balances". By 1950, India, Pakistan and Ceylon had spent much of their sterling, while other countries accumulated more. The sterling area included all of the Commonwealth except for Canada, together with some smaller countries especially in the Persian Gulf.  They held their foreign-exchange in sterling, protecting that currency from runs and facilitating trade and investment inside the Commonwealth.  It was a formal relationship with fixed exchange rates, periodic meetings at Commonwealth summits to coordinate trade policy, and domestic economic policies. The United Kingdom ran a trade surplus, and the other countries were mostly producers of raw materials sold to the United Kingdom.  The commercial rationale was gradually less attractive to the Commonwealth; however, access to the growing London capital market remained an important advantage to the newly independent nations.  As the United Kingdom moved increasingly close to Europe, however, the long-term ties began to be in doubt.[137]
By 1961, with a sluggish economy, the United Kingdom attempted to join the European Economic Community, but this was repeatedly vetoed by Charles de Gaulle.[138] Entry was finally achieved in 1973. Queen Elizabeth was one of the few remaining links between the UK and the Commonwealth.  Historian Ben Pimlott argues that joining Europe "constituted the most decisive step yet in the progress of severance of familial ties between the United Kingdom and its former Empire... It reduced the remaining links to sentimental and cultural ones, and legal niceties."[139]
The newly independent countries of Africa and Asia concentrated on their own internal political and economic development, and sometimes their role in the Cold War.  The United States, international agencies, and the Soviet Union became important players, and the British role receded. While there was opposition to British entry into the EEC from many countries, such as Australia, others preferred the economic advantages brought by British access to the Common Market.[140] The historic ties between the former dominion nations and the United Kingdom were rapidly fraying. The Canadian economy increasingly focused on trade with the United States, and not on trade with the United Kingdom or other Commonwealth nations.  Internal Canadian disputes revolved around the growing American cultural and economic presence, and the strong force of Quebec nationalism. In 1964 the Maple Leaf flag replaced the Canadian Ensign, to the sorrow of many Anglophiles, with Gregory Johnson describing it as "the last gasp of empire".[141] Australia and New Zealand were generally opposed to the United Kingdom's entry and exerted considerable influence on the eventual terms of accession in 1972, for which the United Kingdom agreed to transitional arrangements and monetary compensation to protect important export markets.[142][143] Russell Ward summarises the period in economic terms: 
"In fact the United Kingdom, as Australia's chief trading partner, was being very rapidly replaced just at this time by the United States and an economically resurgent Japan, but most people were scarcely aware of this.... It was feared that British entry into the Common Market was bound to mean abolition, or at least scaling down, of preferential tariff arrangements for Australians goods."[144]
Although the Commonwealth does not have a multilateral trade agreement, research by the Royal Commonwealth Society has shown that trade with another Commonwealth member is up to 50% more than with a non-member on average, with smaller and less wealthy states having a higher propensity to trade within the Commonwealth.[145] At the 2005 Summit in Malta, the heads of government endorsed pursuing free trade among Commonwealth members on a bilateral basis.[146]
Following its vote in June 2016 to leave the EU,[147] some in the United Kingdom suggested the Commonwealth as an alternative to its membership in the European Union;[148] however, it is far from clear that this would either offer sufficient economic benefit to replace the impact of leaving the EU or be acceptable to other member states.[149] Although the EU is already in the process of negotiating free trade agreements with many Commonwealth countries such as India and Canada, it took the EU almost ten years to come to an agreement with Canada,[150][151] due to the challenge associated with achieving the necessary EU-wide approvals.
On 17 December 2021, following the United Kingdom's exit from the European Union, Australia and the United Kingdom signed the Australia–United Kingdom Free Trade Agreement, which on ratification will eliminate tariffs and increase opportunities for movement between the two countries.[152][153]
Commonwealth countries share many links outside government, with over a hundred non-governmental organisations, notably for sport, culture, education, law, and charity claiming to operate on a Commonwealth-wide basis.
The Commonwealth Secretariat regulates formal accreditation with the Commonwealth through its Accreditation Committee. The admittance criteria includes upholding a commitment to the Commonwealth Charter. There are currently approximately 80 organisations holding formal accreditation.[154] These include the Association of Commonwealth Universities which manages the Commonwealth Scholarship allowing students to study in other Commonwealth countries, and the Commonwealth Parliamentary Association which links together over 180 Commonwealth parliaments.
The Commonwealth Foundation is an intergovernmental organisation, resourced by and reporting to Commonwealth governments, and guided by Commonwealth values and priorities. Its mandate is to strengthen civil society in the achievement of Commonwealth priorities: democracy and good governance, respect for human rights and gender equality, poverty eradication, people-centred and sustainable development, and to promote arts and culture.[155]
The Foundation was established in 1965 by the Heads of Government. Admittance is open to all members of the Commonwealth, and in December 2008, stood at 46 out of the 53 member countries. Associate Membership, which is open to associated states or overseas territories of member governments, has been granted to Gibraltar. 2005 saw celebrations for the Foundation's 40th Anniversary. The Foundation is headquartered in Marlborough House, Pall Mall, London. Regular liaison and co-operation between the Secretariat and the Foundation is in place. The Foundation continues to serve the broad purposes for which it was established as written in the Memorandum of Understanding.[155]
The Commonwealth Games, a multi-sport event, is held every four years; the 2018 Commonwealth Games were held in Gold Coast, Australia and 2022 Commonwealth Games in Birmingham; the 2026 Commonwealth Games are to be held across Victoria, Australia. As well as the usual athletic disciplines, as at the Summer Olympic Games, the games include sports particularly popular in the Commonwealth, such as bowls, netball, and rugby sevens. Started in 1930 as the Empire Games, the games were founded on the Olympic model of amateurism, but were deliberately designed to be "the Friendly Games",[156] with the goal of promoting relations between Commonwealth countries and celebrating their shared sporting and cultural heritage.[157]
The games are the Commonwealth's most visible activity[156] and interest in the operation of the Commonwealth increases greatly when the Games are held.[158] There is controversy over whether the games—and sport generally—should be involved in the Commonwealth's wider political concerns.[157] The 1977 Gleneagles Agreement was signed to commit Commonwealth countries to combat apartheid through discouraging sporting contact with South Africa (which was not then a member), whilst the 1986 games were boycotted by most African, Asian, and Caribbean countries for the failure of other countries to enforce the Gleneagles Agreement.[159]
The Commonwealth War Graves Commission (CWGC) is responsible for maintaining the war graves of 1.7 million service personnel who died in the First and Second World Wars fighting for Commonwealth member states. Founded in 1917 (as the Imperial War Graves Commission), the commission has constructed 2,500 war cemeteries, and maintains individual graves at another 20,000 sites around the world.[160] The vast majority of the latter are civilian cemeteries in the United Kingdom. In 1998, the CWGC made the records of its buried available online to facilitate easier searching.[161]
Commonwealth war cemeteries often feature similar horticulture and architecture, with larger cemeteries being home to a Cross of Sacrifice and Stone of Remembrance. The CWGC is notable for marking the graves identically, regardless of the rank, country of origin, race, or religion of the buried.[161][note 1]  It is funded by voluntary agreement by six Commonwealth members, in proportion to the nationality of the casualties in the graves maintained,[160] with 75% of the funding coming from the United Kingdom.[161]
The Commonwealth of Learning (COL) is an intergovernmental organisation created by the Heads of Government to encourage the development and sharing of open learning/distance education knowledge, resources and technologies. COL is helping developing nations improve access to quality education and training.[163]
The Commonwealth Local Government Forum (CLGF) is a global local government organisation, bringing together local authorities, their national associations and the ministries responsible for local government in the member countries of the Commonwealth. CLGF works with national and local governments to support the development of democratic values and good local governance and is the associated organisation officially recognised by Commonwealth Heads of Government as the representative body for local government in the Commonwealth.[164]
CLGF is unique in bringing together central, provincial and local spheres of government involved in local government policy and decision-making. CLGF members include local government associations, individual local authorities, ministries dealing with local government, and research and professional organisations who work with local government. Practitioner to practitioner support is at the core of CLGF's work across the Commonwealth and within the region, using CLGF's own members to support others both within and between regions. CLGF is a member of the Global Taskforce of Local and Regional Governments, the formal partner of the UN Major Group of Local Authorities.[165]
Many Commonwealth nations play similar sports that are considered quintessentially British in character, rooted in and developed under British rule or hegemony, including cricket, football, rugby, field hockey and netball. These ties are particularly strong between the United Kingdom, Australia, New Zealand and South Africa across rugby union, cricket, netball, and field hockey, with Australia in rugby league, with the Caribbean nations in cricket and netball, and with the Indian subcontinent in cricket and hockey. Canada, by contrast, is dominated by North American sports, including baseball instead of cricket, basketball rather than netball, ice hockey rather than field hockey and Canadian football, rather than rugby union or league. Canada does, however, maintain small enthusiastic communities in all the more traditional Commonwealth sports, having reached the World Cup in each of them, and is the homeplace of the Commonwealth Games, hosting the inaugural edition in Hamilton in 1930.[166] 
This shared sporting landscape has led to the development of friendly national rivalries between the main sporting nations that have often defined their relations with each other, and in the cases of India, Australia and New Zealand, have played a major part in defining their emerging national character (in cricket, cricket and rugby league, and rugby union respectively). Indeed, said rivalries preserved close ties by providing a constant in international relationships, even as the Empire transformed into the Commonwealth.[167] Externally, playing these sports is seen to be a sign of sharing a certain Commonwealth culture; the adoption of cricket at schools in Rwanda is seen as symbolic of the country's move towards Commonwealth membership.[168][169]
The Commonwealth Games, a quadrennial multi-sports event held in the middle year of an Olympic cycle is the most visible demonstration of these sporting ties. The Games include standard multi-sports disciplines like athletics, swimming, gymnastics, and cycling, but also includes sports popular in the Commonwealth that are distinct to the Games such as netball, squash and lawn bowls. They are also more avowedly political than events like the Olympics, promoting what are seen as Commonwealth values; historically, a history of shared military endeavor was celebrated and promoted, parasport and disability sport is fully integrated, and the Commonwealth Games Federation has publicly backed the rights of LGBT people, despite the continuing criminalisation of homosexuality in many Commonwealth countries.
The most recent edition of the games was held in 2022 in Birmingham, England. The 2026 Commonwealth Games will be held in the state of Victoria, Australia.
The shared history of British presence has produced a substantial body of writing in many languages, known as Commonwealth literature.[170][171] The Association for Commonwealth Literature and Language Studies has 11 branches worldwide and holds an international conference every three years.[172]
In 1987, the Commonwealth Foundation established the annual Commonwealth Writers' Prize "to encourage and reward the upsurge of new Commonwealth fiction and ensure that works of merit reach a wider audience outside their country of origin". Prizes are awarded for the best book and best first book in the Commonwealth; there are also regional prizes for the best book and best first book in each of four regions. Although not officially affiliated with the Commonwealth, the prestigious annual Man Booker Prize, one of the highest honours in literature,[173] used to be awarded only to authors from Commonwealth countries or former members such as Ireland and Zimbabwe. Since 2014, however, writers of any nationality have been eligible for the prize providing that they write originally in English and their novels are published by established publishers in the United Kingdom.[174]
There had been a few important works in English prior to 1950 from the then British Empire. From 1950 on, a significant number of writers from the countries of the Commonwealth began gaining international recognition, including some who migrated to the United Kingdom.
The South African writer Olive Schreiner's famous novel The Story of an African Farm was published in 1883 and New Zealander Katherine Mansfield published her first collection of short stories, In a German Pension, in 1911. The first major novelist, writing in English, from the Indian sub-continent, R. K. Narayan, began publishing in England in the 1930s, thanks to the encouragement of English novelist Graham Greene.[175] Caribbean writer Jean Rhys's writing career began as early as 1928, though her most famous work, Wide Sargasso Sea, was not published until 1966. South Africa's Alan Paton's famous Cry, the Beloved Country dates from 1948. Doris Lessing from Southern Rhodesia, now Zimbabwe, was a dominant presence in the English literary scene, frequently publishing from 1950 on throughout the 20th century. She won the Nobel Prize in Literature in 2007.[176]
Salman Rushdie is another post-Second World War writer from the former British colonies who permanently settled in the United Kingdom. Rushdie achieved fame with Midnight's Children (1981). His most controversial novel, The Satanic Verses (1989), was inspired in part by the life of Muhammad. V. S. Naipaul (born 1932), born in Trinidad, was another immigrant, who wrote among other things A Bend in the River (1979). Naipaul won the Nobel Prize in Literature in 2001.[177]
Many other Commonwealth writers have achieved an international reputation for works in English, including Nigerian novelist Chinua Achebe, and playwright Wole Soyinka. Soyinka won the Nobel Prize in Literature in 1986, as did South African novelist Nadine Gordimer in 1995. Other South African writers in English are novelist J. M. Coetzee (Nobel Prize 2003) and playwright Athol Fugard. Kenya's most internationally renowned author is Ngũgĩ wa Thiong'o, who has written novels, plays and short stories in English. Poet Derek Walcott, from Saint Lucia in the Caribbean, was another Nobel Prize winner in 1992. An Australian, Patrick White, a major novelist in this period, whose first work was published in 1939, won in 1973. Other noteworthy Australian writers at the end of this period are poet Les Murray, and novelist Peter Carey, who is one of only four writers to have won the Booker Prize twice.[178]
Due to their shared constitutional histories, most countries in the Commonwealth have similar legal and political systems. The Commonwealth requires its members to be functioning democracies that respect human rights and the rule of law. Most Commonwealth countries have the bicameral Westminster system of parliamentary democracy. The Commonwealth Parliamentary Association facilitates co-operation between legislatures across the Commonwealth, and the Commonwealth Local Government Forum promotes good governance amongst local government officials. Most Commonwealth members use common law, modelled on English law. The Latimer House Principles adopted in 2003 are aware of the separation of powers. The Judicial Committee of the Privy Council is the supreme court of 14 Commonwealth members.[179]
The Commonwealth has adopted a number of symbols that represent the association of its members. The English language is recognised as a symbol of the members' heritage; as well as being considered a symbol of the Commonwealth, recognition of it as "the means of Commonwealth communication" is a prerequisite for Commonwealth membership. The flag of the Commonwealth consists of the symbol of the Commonwealth Secretariat, a gold globe surrounded by emanating rays, on a dark blue field; it was designed for the second CHOGM in 1973, and officially adopted on 26 March 1976. 1976 also saw the organisation agree to a common date on which to commemorate Commonwealth Day, the second Monday in March, having developed separately on different dates from Empire Day celebrations.[180]
In 2009, to mark the 60th anniversary of the founding of the Commonwealth, the Royal Commonwealth Society commissioned a poll of public opinion in seven of the member states: Australia, Canada, India, Jamaica, Malaysia, South Africa and the United Kingdom.  It found that most people in these countries were largely ignorant of the Commonwealth's activities, aside from the Commonwealth Games, and indifferent toward its future.  Support for the Commonwealth was twice as high in developing countries as in developed countries; it was lowest in the United Kingdom.[181][182][183][184]
Also to mark the 60th anniversary (Diamond Jubilee) of the Commonwealth in 2009, the Commonwealth Secretariat commissioned Paul Carroll to compose "The Commonwealth Anthem". The lyrics of the Anthem are taken from the 1948 Universal Declaration of Human Rights.[185] The Commonwealth has published the Anthem, performed by the Commonwealth Youth Orchestra, with and without an introductory narrative.[186][187]


Racism is discrimination and prejudice towards people based on their race or ethnicity. Racism can be present in social actions, practices, or political systems (e.g. apartheid) that support the expression of prejudice or aversion in discriminatory practices. The ideology underlying racist practices often assumes that humans can be subdivided into distinct groups that are different in their social behavior and innate capacities and that can be ranked as inferior or superior. Racist ideology can become manifest in many aspects of social life. Associated social actions may include nativism, xenophobia, otherness, segregation, hierarchical ranking, supremacism, and related social phenomena.
While the concepts of race and ethnicity are considered to be separate in contemporary social science, the two terms have a long history of equivalence in popular usage and older social science literature. "Ethnicity" is often used in a sense close to one traditionally attributed to "race", the division of human groups based on qualities assumed to be essential or innate to the group (e.g. shared ancestry or shared behavior). Racism and racial discrimination are often used to describe discrimination on an ethnic or cultural basis, independent of whether these differences are described as racial. According to the United Nations's Convention on the Elimination of All Forms of Racial Discrimination, there is no distinction between the terms "racial" and "ethnic" discrimination. It further concludes that superiority based on racial differentiation is scientifically false, morally condemnable, socially unjust, and dangerous. The convention also declared that there is no justification for racial discrimination, anywhere, in theory or in practice.[1]
Racism is a relatively modern concept, arising in the European age of imperialism, the subsequent growth of capitalism, and especially the Atlantic slave trade,[2][3] of which it was a major driving force.[4] It was also a major force behind racial segregation in the United States in the 19th and early 20th centuries, and of apartheid in South Africa; 19th and 20th-century racism in Western culture is particularly well documented and constitutes a reference point in studies and discourses about racism.[5] Racism has played a role in genocides such as the Holocaust, the Armenian genocide, the Rwandan genocide, and the Genocide of Serbs in the Independent State of Croatia, as well as colonial projects including the European colonization of the Americas, Africa, Asia, and the population transfer in the Soviet Union including deportations of indigenous minorities.[6] Indigenous peoples have been—and are—often subject to racist attitudes.
In the 19th century, many scientists subscribed to the belief that the human population can be divided into races. The term racism is a noun describing the state of being racist, i.e., subscribing to the belief that the human population can or should be classified into races with differential abilities and dispositions, which in turn may motivate a political ideology in which rights and privileges are differentially distributed based on racial categories. The term "racist" may be an adjective or a noun, the latter describing a person who holds those beliefs.[7] The origin of the root word "race" is not clear. Linguists generally agree that it came to the English language from Middle French, but there is no such agreement on how it generally came into Latin-based languages. A recent proposal is that it derives from the Arabic ra's, which means "head, beginning, origin" or the Hebrew rosh, which has a similar meaning.[8] Early race theorists generally held the view that some races were inferior to others and they consequently believed that the differential treatment of races was fully justified.[9][10][11] These early theories guided pseudo-scientific research assumptions; the collective endeavors to adequately define and form hypotheses about racial differences are generally termed scientific racism, though this term is a misnomer, due to the lack of any actual science backing the claims.
Most biologists, anthropologists, and sociologists reject a taxonomy of races in favor of more specific and/or empirically verifiable criteria, such as geography, ethnicity, or a history of endogamy.[12] Human genome research indicates that race is not a meaningful genetic classification of humans.[13][14][15][16]
An entry in the Oxford English Dictionary (2008) defines racialism as "[a]n earlier term than racism, but now largely superseded by it", and cites the term "racialism" in a 1902 quote.[17] The revised Oxford English Dictionary cites the shorter term "racism" in a quote from the year 1903.[18] It was defined by the Oxford English Dictionary (2nd edition 1989) as "[t]he theory that distinctive human characteristics and abilities are determined by race"; the same dictionary termed racism a synonym of racialism: "belief in the superiority of a particular race". By the end of World War II, racism had acquired the same supremacist connotations formerly associated with racialism: racism by then implied racial discrimination, racial supremacism, and a harmful intent. The term "race hatred" had also been used by sociologist Frederick Hertz in the late 1920s.
As its history indicates, the popular use of the word racism is relatively recent. The word came into widespread usage in the Western world in the 1930s, when it was used to describe the social and political ideology of Nazism, which treated "race" as a naturally given political unit.[19] It is commonly agreed that racism existed before the coinage of the word, but there is not a wide agreement on a single definition of what racism is and what it is not.[9] Today, some scholars of racism prefer to use the concept in the plural racisms, in order to emphasize its many different forms that do not easily fall under a single definition. They also argue that different forms of racism have characterized different historical periods and geographical areas.[20] Garner (2009: p. 11) summarizes different existing definitions of racism and identifies three common elements contained in those definitions of racism. First, a historical, hierarchical power relationship between groups; second, a set of ideas (an ideology) about racial differences; and, third, discriminatory actions (practices).[9]
Though many countries around the globe have passed laws related to race and discrimination, the first significant international human rights instrument developed by the United Nations (UN) was the Universal Declaration of Human Rights (UDHR),[21] which was adopted by the United Nations General Assembly in 1948. The UDHR recognizes that if people are to be treated with dignity, they require economic rights, social rights including education, and the rights to cultural and political participation and civil liberty. It further states that everyone is entitled to these rights "without distinction of any kind, such as race, colour, sex, language, religion, political or other opinion, national or social origin, property, birth or other status".
The UN does not define "racism"; however, it does define "racial discrimination". According to the 1965 UN International Convention on the Elimination of All Forms of Racial Discrimination,[22]
The term "racial discrimination" shall mean any distinction, exclusion, restriction, or preference based on race, colour, descent, or national or ethnic origin that has the purpose or effect of nullifying or impairing the recognition, enjoyment or exercise, on an equal footing, of human rights and fundamental freedoms in the political, economic, social, cultural or any other field of public life.
In their 1978 United Nations Educational, Scientific, and Cultural Organization (UNESCO) Declaration on Race and Racial Prejudice (Article 1), the UN states, "All human beings belong to a single species and are descended from a common stock. They are born equal in dignity and rights and all form an integral part of humanity."[23]
The UN definition of racial discrimination does not make any distinction between discrimination based on ethnicity and race, in part because the distinction between the two has been a matter of debate among academics, including anthropologists.[24] Similarly, in British law, the phrase racial group means "any group of people who are defined by reference to their race, colour, nationality (including citizenship) or ethnic or national origin".[25]
In Norway, the word "race" has been removed from national laws concerning discrimination because the use of the phrase is considered problematic and unethical.[26][27] The Norwegian Anti-Discrimination Act bans discrimination based on ethnicity, national origin, descent, and skin color.[28]
Sociologists, in general, recognize "race" as a social construct. This means that, although the concepts of race and racism are based on observable biological characteristics, any conclusions drawn about race on the basis of those observations are heavily influenced by cultural ideologies. Racism, as an ideology, exists in a society at both the individual and institutional level.
While much of the research and work on racism during the last half-century or so has concentrated on "white racism" in the Western world, historical accounts of race-based social practices can be found across the globe.[29] Thus, racism can be broadly defined to encompass individual and group prejudices and acts of discrimination that result in material and cultural advantages conferred on a majority or a dominant social group.[30] So-called "white racism" focuses on societies in which white populations are the majority or the dominant social group. In studies of these majority white societies, the aggregate of material and cultural advantages is usually termed "white privilege".
Race and race relations are prominent areas of study in sociology and economics. Much of the sociological literature focuses on white racism. Some of the earliest sociological works on racism were written by sociologist W. E. B. Du Bois, the first African American to earn a doctoral degree from Harvard University. Du Bois wrote, "[t]he problem of the twentieth century is the problem of the color line."[31] Wellman (1993) defines racism as "culturally sanctioned beliefs, which, regardless of intentions involved, defend the advantages whites have because of the subordinated position of racial minorities".[32] In both sociology and economics, the outcomes of racist actions are often measured by the inequality in income, wealth, net worth, and access to other cultural resources (such as education), between racial groups.[33]
In sociology and social psychology, racial identity and the acquisition of that identity, is often used as a variable in racism studies. Racial ideologies and racial identity affect individuals' perception of race and discrimination. Cazenave and Maddern (1999) define racism as "a highly organized system of 'race'-based group privilege that operates at every level of society and is held together by a sophisticated ideology of color/'race' supremacy. Racial centrality (the extent to which a culture recognizes individuals' racial identity) appears to affect the degree of discrimination African-American young adults perceive whereas racial ideology may buffer the detrimental emotional effects of that discrimination."[34] Sellers and Shelton (2003) found that a relationship between racial discrimination and emotional distress was moderated by racial ideology and social beliefs.[35]
Some sociologists also argue that, particularly in the West, where racism is often negatively sanctioned in society, racism has changed from being a blatant to a more covert expression of racial prejudice. The "newer" (more hidden and less easily detectable) forms of racism—which can be considered embedded in social processes and structures—are more difficult to explore and challenge. It has been suggested that, while in many countries overt or explicit racism has become increasingly taboo, even among those who display egalitarian explicit attitudes, an implicit or aversive racism is still maintained subconsciously.[36]
This process has been studied extensively in social psychology as implicit associations and implicit attitudes, a component of implicit cognition. Implicit attitudes are evaluations that occur without conscious awareness towards an attitude object or the self. These evaluations are generally either favorable or unfavorable. They come about from various influences in the individual experience.[37] Implicit attitudes are not consciously identified (or they are inaccurately identified) traces of past experience that mediate favorable or unfavorable feelings, thoughts, or actions towards social objects.[36] These feelings, thoughts, or actions have an influence on behavior of which the individual may not be aware.[38]
Therefore, subconscious racism can influence our visual processing and how our minds work when we are subliminally exposed to faces of different colors. In thinking about crime, for example, social psychologist Jennifer L. Eberhardt (2004) of Stanford University holds that, "blackness is so associated with crime you're ready to pick out these crime objects."[39] Such exposures influence our minds and they can cause subconscious racism in our behavior towards other people or even towards objects. Thus, racist thoughts and actions can arise from stereotypes and fears of which we are not aware.[40] For example, scientists and activists have warned that the use of the stereotype "Nigerian Prince" for referring to advance-fee scammers is racist, i.e. "reducing Nigeria to a nation of scammers and fraudulent princes, as some people still do online, is a stereotype that needs to be called out".[41]
Language, linguistics, and discourse are active areas of study in the humanities, along with literature and the arts. Discourse analysis seeks to reveal the meaning of race and the actions of racists through careful study of the ways in which these factors of human society are described and discussed in various written and oral works. For example, Van Dijk (1992) examines the different ways in which descriptions of racism and racist actions are depicted by the perpetrators of such actions as well as by their victims.[42] He notes that when descriptions of actions have negative implications for the majority, and especially for white elites, they are often seen as controversial and such controversial interpretations are typically marked with quotation marks or they are greeted with expressions of distance or doubt. The previously cited book, The Souls of Black Folk by W.E.B. Du Bois, represents early African-American literature that describes the author's experiences with racism when he was traveling in the South as an African American.
Much American fictional literature has focused on issues of racism and the black "racial experience" in the US, including works written by whites, such as Uncle Tom's Cabin, To Kill a Mockingbird, and Imitation of Life, or even the non-fiction work Black Like Me. These books, and others like them, feed into what has been called the "white savior narrative in film", in which the heroes and heroines are white even though the story is about things that happen to black characters. Textual analysis of such writings can contrast sharply with black authors' descriptions of African Americans and their experiences in US society. African-American writers have sometimes been portrayed in African-American studies as retreating from racial issues when they write about "whiteness", while others identify this as an African-American literary tradition called "the literature of white estrangement", part of a multi-pronged effort to challenge and dismantle white supremacy in the US.[43]
According to dictionaries, the word is commonly used to describe prejudice and discrimination based on race.[44][45]
Racism can also be said to describe a condition in society in which a dominant racial group benefits from the oppression of others, whether that group wants such benefits or not.[46] Foucauldian scholar Ladelle McWhorter, in her 2009 book, Racism and Sexual Oppression in Anglo-America: A Genealogy, posits modern racism similarly, focusing on the notion of a dominant group, usually whites, vying for racial purity and progress, rather than an overt or obvious ideology focused on the oppression of nonwhites.[47]
In popular usage, as in some academic usage, little distinction is made between "racism" and "ethnocentrism". Often, the two are listed together as "racial and ethnic" in describing some action or outcome that is associated with prejudice within a majority or dominant group in society. Furthermore, the meaning of the term racism is often conflated with the terms prejudice, bigotry, and discrimination. Racism is a complex concept that can involve each of those; but it cannot be equated with, nor is it synonymous, with these other terms.[citation needed]
The term is often used in relation to what is seen as prejudice within a minority or subjugated group, as in the concept of reverse racism. "Reverse racism" is a concept often used to describe acts of discrimination or hostility against members of a dominant racial or ethnic group while favoring members of minority groups.[48][49] This concept has been used especially in the United States in debates over color-conscious policies (such as affirmative action) intended to remedy racial inequalities.[50] However, many experts and other commenters view reverse racism as a myth rather than a reality.[51][52][53][54] Scholars commonly define racism not only in terms of individual prejudice, but also in terms of a power structure that protects the interests of the dominant culture and actively discriminates against ethnic minorities.[48][49] From this perspective, while members of ethnic minorities may be prejudiced against members of the dominant culture, they lack the political and economic power to actively oppress them, and they are therefore not practicing "racism".[2][48][55]
The ideology underlying racism can manifest in many aspects of social life. Such aspects are described in this section, although the list is not exhaustive.
Aversive racism is a form of implicit racism, in which a person's unconscious negative evaluations of racial or ethnic minorities are realized by a persistent avoidance of interaction with other racial and ethnic groups. As opposed to traditional, overt racism, which is characterized by overt hatred for and explicit discrimination against racial/ethnic minorities, aversive racism is characterized by more complex, ambivalent expressions and attitudes.[56] Aversive racism is similar in implications to the concept of symbolic or modern racism (described below), which is also a form of implicit, unconscious, or covert attitude which results in unconscious forms of discrimination.
The term was coined by Joel Kovel to describe the subtle racial behaviors of any ethnic or racial group who rationalize their aversion to a particular group by appeal to rules or stereotypes.[56] People who behave in an aversively racial way may profess egalitarian beliefs, and will often deny their racially motivated behavior; nevertheless they change their behavior when dealing with a member of another race or ethnic group than the one they belong to. The motivation for the change is thought to be implicit or subconscious. Experiments have provided empirical support for the existence of aversive racism. Aversive racism has been shown to have potentially serious implications for decision making in employment, in legal decisions and in helping behavior.[57][58]
In relation to racism, color blindness is the disregard of racial characteristics in social interaction, for example in the rejection of affirmative action, as a way to address the results of past patterns of discrimination. Critics of this attitude argue that by refusing to attend to racial disparities, racial color blindness in fact unconsciously perpetuates the patterns that produce racial inequality.[59]
Eduardo Bonilla-Silva argues that color blind racism arises from an "abstract liberalism, biologization of culture, naturalization of racial matters, and minimization of racism".[60] Color blind practices are "subtle, institutional, and apparently nonracial"[61] because race is explicitly ignored in decision-making. If race is disregarded in predominantly white populations, for example, whiteness becomes the normative standard, whereas people of color are othered, and the racism these individuals experience may be minimized or erased.[62][63] At an individual level, people with "color blind prejudice" reject racist ideology, but also reject systemic policies intended to fix institutional racism.[63]
Cultural racism manifests as societal beliefs and customs that promote the assumption that the products of a given culture, including the language and traditions of that culture, are superior to those of other cultures. It shares a great deal with xenophobia, which is often characterized by fear of, or aggression toward, members of an outgroup by members of an ingroup.[citation needed]
In that sense it is also similar to communalism as used in South Asia.[64]
Cultural racism exists when there is a widespread acceptance of stereotypes concerning diverse ethnic or population groups.[65] Whereas racism can be characterised by the belief that one race is inherently superior to another, cultural racism can be characterised by the belief that one culture is inherently superior to another.[66]
Historical economic or social disparity is alleged to be a form of discrimination caused by past racism and historical reasons, affecting the present generation through deficits in the formal education and kinds of preparation in previous generations, and through primarily unconscious racist attitudes and actions on members of the general population. Economic discrimination may lead to choices that perpetuate racism. For example, color photographic film was tuned for white skin[67] as are automatic soap dispensers[68] and facial recognition systems.[69]
In 2011, Bank of America agreed to pay $335 million to settle a federal government claim that its mortgage division, Countrywide Financial, discriminated against black and Hispanic homebuyers.[70]
Institutional racism (also known as structural racism, state racism or systemic racism) is racial discrimination by governments, corporations, religions, or educational institutions or other large organizations with the power to influence the lives of many individuals. Stokely Carmichael is credited for coining the phrase institutional racism in the late 1960s. He defined the term as "the collective failure of an organization to provide an appropriate and professional service to people because of their colour, culture or ethnic origin".[71]
Maulana Karenga argued that racism constituted the destruction of culture, language, religion, and human possibility and that the effects of racism were "the morally monstrous destruction of human possibility involved redefining African humanity to the world, poisoning past, present and future relations with others who only know us through this stereotyping and thus damaging the truly human relations among peoples".[72]
Othering is the term used by some to describe a system of discrimination whereby the characteristics of a group are used to distinguish them as separate from the norm.[73]
Othering plays a fundamental role in the history and continuation of racism. To objectify a culture as something different, exotic or underdeveloped is to generalize that it is not like 'normal' society. Europe's colonial attitude towards the Orientals exemplifies this as it was thought that the East was the opposite of the West; feminine where the West was masculine, weak where the West was strong and traditional where the West was progressive.[74] By making these generalizations and othering the East, Europe was simultaneously defining herself as the norm, further entrenching the gap.[75]
Much of the process of othering relies on imagined difference, or the expectation of difference. Spatial difference can be enough to conclude that "we" are "here" and the "others" are over "there".[74] Imagined differences serve to categorize people into groups and assign them characteristics that suit the imaginer's expectations.[76]
Racial discrimination refers to discrimination against someone on the basis of their race.
Racial segregation is the separation of humans into socially-constructed racial groups in daily life. It may apply to activities such as eating in a restaurant, drinking from a water fountain, using a bathroom, attending school, going to the movies, or in the rental or purchase of a home.[77] Segregation is generally outlawed, but may exist through social norms, even when there is no strong individual preference for it, as suggested by Thomas Schelling's models of segregation and subsequent work.
Centuries of European colonialism in the Americas, Africa and Asia were often justified by white supremacist attitudes.[78] During the early 20th century, the phrase "The White Man's Burden" was widely used to justify an imperialist policy as a noble enterprise.[79][80] A justification for the policy of conquest and subjugation of Native Americans emanated from the stereotyped perceptions of the indigenous people as "merciless Indian savages", as they are described in the United States Declaration of Independence.[81] Sam Wolfson of The Guardian writes that "the declaration's passage has often been cited as an encapsulation of the dehumanizing attitude toward indigenous Americans that the US was founded on."[82] In an 1890 article about colonial expansion onto Native American land, author L. Frank Baum wrote: "The Whites, by law of conquest, by justice of civilization, are masters of the American continent, and the best safety of the frontier settlements will be secured by the total annihilation of the few remaining Indians."[83] In his Notes on the State of Virginia, published in 1785, Thomas Jefferson wrote: "blacks, whether originally a distinct race, or made distinct by time or circumstances, are inferior to the whites in the endowments of both body and mind."[84] Attitudes of black supremacy, Arab supremacy, and East Asian supremacy also exist.
Some scholars argue that in the US, earlier violent and aggressive forms of racism have evolved into a more subtle form of prejudice in the late 20th century. This new form of racism is sometimes referred to as "modern racism" and it is characterized by outwardly acting unprejudiced while inwardly maintaining prejudiced attitudes, displaying subtle prejudiced behaviors such as actions informed by attributing qualities to others based on racial stereotypes, and evaluating the same behavior differently based on the race of the person being evaluated.[85] This view is based on studies of prejudice and discriminatory behavior, where some people will act ambivalently towards black people, with positive reactions in certain, more public contexts, but more negative views and expressions in more private contexts. This ambivalence may also be visible for example in hiring decisions where job candidates that are otherwise positively evaluated may be unconsciously disfavored by employers in the final decision because of their race.[86][87][88] Some scholars consider modern racism to be characterized by an explicit rejection of stereotypes, combined with resistance to changing structures of discrimination for reasons that are ostensibly non-racial, an ideology that considers opportunity at a purely individual basis denying the relevance of race in determining individual opportunities and the exhibition of indirect forms of micro-aggression toward and/or avoidance of people of other races.[89]
Recent research has shown that individuals who consciously claim to reject racism may still exhibit race-based subconscious biases in their decision-making processes. While such "subconscious racial biases" do not fully fit the definition of racism, their impact can be similar, though typically less pronounced, not being explicit, conscious or deliberate.[90]
In 1919, a proposal to include a racial equality provision in the Covenant of the League of Nations was supported by a majority, but not adopted in the Paris Peace Conference in 1919. In 1943, Japan and its allies declared work for the abolition of racial discrimination to be their aim at the Greater East Asia Conference.[91] Article 1 of the 1945 UN Charter includes "promoting and encouraging respect for human rights and for fundamental freedoms for all without distinction as to race" as UN purpose.
In 1950, UNESCO suggested in The Race Question—a statement signed by 21 scholars such as Ashley Montagu, Claude Lévi-Strauss, Gunnar Myrdal, Julian Huxley, etc.—to "drop the term race altogether and instead speak of ethnic groups". The statement condemned scientific racism theories that had played a role in the Holocaust. It aimed both at debunking scientific racist theories, by popularizing modern knowledge concerning "the race question", and morally condemned racism as contrary to the philosophy of the Enlightenment and its assumption of equal rights for all. Along with Myrdal's An American Dilemma: The Negro Problem and Modern Democracy (1944), The Race Question influenced the 1954 U.S. Supreme Court desegregation decision in Brown v. Board of Education.[92] Also, in 1950, the European Convention on Human Rights was adopted, which was widely used on racial discrimination issues.[93]
The United Nations use the definition of racial discrimination laid out in the International Convention on the Elimination of All Forms of Racial Discrimination, adopted in 1966:[94]
... any distinction, exclusion, restriction or preference based on race, color, descent, or national or ethnic origin that has the purpose or effect of nullifying or impairing the recognition, enjoyment or exercise, on an equal footing, of human rights and fundamental freedoms in the political, economic, social, cultural or any other field of public life. (Part 1 of Article 1 of the U.N. International Convention on the Elimination of All Forms of Racial Discrimination)
In 2001, the European Union explicitly banned racism, along with many other forms of social discrimination, in the Charter of Fundamental Rights of the European Union, the legal effect of which, if any, would necessarily be limited to Institutions of the European Union: "Article 21 of the charter prohibits discrimination on any ground such as race, color, ethnic or social origin, genetic features, language, religion or belief, political or any other opinion, membership of a national minority, property, disability, age or sexual orientation and also discrimination on the grounds of nationality."[95]
Racism existed during the 19th century as scientific racism, which attempted to provide a racial classification of humanity.[96] In 1775 Johann Blumenbach divided the world's population into five groups according to skin color (Caucasians, Mongols, etc.), positing the view that the non-Caucasians had arisen through a process of degeneration. Another early view in scientific racism was the polygenist view, which held that the different races had been separately created. Polygenist Christoph Meiners for example, split mankind into two divisions which he labeled the "beautiful White race" and the "ugly Black race". In Meiners' book, The Outline of History of Mankind, he claimed that a main characteristic of race is either beauty or ugliness. He viewed only the white race as beautiful. He considered ugly races to be inferior, immoral and animal-like.
Anders Retzius demonstrated that neither Europeans nor others are one "pure race", but of mixed origins. While discredited, derivations of Blumenbach's taxonomy are still widely used for the classification of the population in the United States. Hans Peder Steensby, while strongly emphasizing that all humans today are of mixed origins, in 1907 claimed that the origins of human differences must be traced extraordinarily far back in time, and conjectured that the "purest race" today would be the Australian Aboriginals.[97]
Scientific racism fell strongly out of favor in the early 20th century, but the origins of fundamental human and societal differences are still researched within academia, in fields such as human genetics including paleogenetics, social anthropology, comparative politics, history of religions, history of ideas, prehistory, history, ethics, and psychiatry. There is widespread rejection of any methodology based on anything similar to Blumenbach's races. It is more unclear to which extent and when ethnic and national stereotypes are accepted.
Although after World War II and the Holocaust, racist ideologies were discredited on ethical, political and scientific grounds, racism and racial discrimination have remained widespread around the world.
Du Bois observed that it is not so much "race" that we think about, but culture: "... a common history, common laws and religion, similar habits of thought and a conscious striving together for certain ideals of life".[98] Late 19th century nationalists were the first to embrace contemporary discourses on "race", ethnicity, and "survival of the fittest" to shape new nationalist doctrines. Ultimately, race came to represent not only the most important traits of the human body, but was also regarded as decisively shaping the character and personality of the nation.[99] According to this view, culture is the physical manifestation created by ethnic groupings, as such fully determined by racial characteristics. Culture and race became considered intertwined and dependent upon each other, sometimes even to the extent of including nationality or language to the set of definition. Pureness of race tended to be related to rather superficial characteristics that were easily addressed and advertised, such as blondness. Racial qualities tended to be related to nationality and language rather than the actual geographic distribution of racial characteristics. In the case of Nordicism, the denomination "Germanic" was equivalent to superiority of race.
Bolstered by some nationalist and ethnocentric values and achievements of choice, this concept of racial superiority evolved to distinguish from other cultures that were considered inferior or impure. This emphasis on culture corresponds to the modern mainstream definition of racism: "[r]acism does not originate from the existence of 'races'. It creates them through a process of social division into categories: anybody can be racialised, independently of their somatic, cultural, religious differences."[100]
This definition explicitly ignores the biological concept of race, which is still subject to scientific debate. In the words of David C. Rowe, "[a] racial concept, although sometimes in the guise of another name, will remain in use in biology and in other fields because scientists, as well as lay persons, are fascinated by human diversity, some of which is captured by race."[101]
Racial prejudice became subject to international legislation. For instance, the Declaration on the Elimination of All Forms of Racial Discrimination, adopted by the United Nations General Assembly on November 20, 1963, addresses racial prejudice explicitly next to discrimination for reasons of race, colour or ethnic origin (Article I).[102]
Debates over the origins of racism often suffer from a lack of clarity over the term. Many use the term "racism" to refer to more general phenomena, such as xenophobia and ethnocentrism, although scholars attempt to clearly distinguish those phenomena from racism as an ideology or from scientific racism, which has little to do with ordinary xenophobia. Others conflate recent forms of racism with earlier forms of ethnic and national conflict. In most cases, ethno-national conflict seems to owe itself to conflict over land and strategic resources. In some cases, ethnicity and nationalism were harnessed in order to rally combatants in wars between great religious empires (for example, the Muslim Turks and the Catholic Austro-Hungarians).
Notions of race and racism have often played central roles in ethnic conflicts. Throughout history, when an adversary is identified as "other" based on notions of race or ethnicity (in particular when "other" is interpreted to mean "inferior"), the means employed by the self-presumed "superior" party to appropriate territory, human chattel, or material wealth often have been more ruthless, more brutal, and less constrained by moral or ethical considerations. According to historian Daniel Richter, Pontiac's Rebellion saw the emergence on both sides of the conflict of "the novel idea that all Native people were 'Indians,' that all Euro-Americans were 'Whites,' and that all on one side must unite to destroy the other".[103] Basil Davidson states in his documentary, Africa: Different but Equal, that racism, in fact, only just recently surfaced as late as the 19th century, due to the need for a justification for slavery in the Americas.
Historically, racism was a major driving force behind the Transatlantic slave trade.[104] It was also a major force behind racial segregation, especially in the United States in the nineteenth and early twentieth centuries, and South Africa under apartheid; 19th and 20th century racism in the Western world is particularly well documented and constitutes a reference point in studies and discourses about racism.[5] Racism has played a role in genocides such as the Armenian genocide, and the Holocaust, and colonial projects like the European colonization of the Americas, Africa, and Asia. Indigenous peoples have been—and are—often subject to racist attitudes. Practices and ideologies of racism are condemned by the United Nations in the Declaration of Human Rights.[105]
After the Napoleonic Wars, Europe was confronted with the new "nationalities question", leading to reconfigurations of the European map, on which the frontiers between the states had been delineated during the 1648 Peace of Westphalia. Nationalism had made its first appearance with the invention of the levée en masse by the French Revolutionaries, thus inventing mass conscription in order to be able to defend the newly founded Republic against the Ancien Régime order represented by the European monarchies. This led to the French Revolutionary Wars (1792–1802) and then to the conquests of Napoleon, and to the subsequent European-wide debates on the concepts and realities of nations, and in particular of nation-states. The Westphalia Treaty had divided Europe into various empires and kingdoms (such as the Ottoman Empire, the Holy Roman Empire, the Swedish Empire, the Kingdom of France, etc.), and for centuries wars were waged between princes (Kabinettskriege in German).
Modern nation-states appeared in the wake of the French Revolution, with the formation of patriotic sentiments for the first time in Spain during the Peninsula War (1808–1813, known in Spain as the Independence War). Despite the restoration of the previous order with the 1815 Congress of Vienna, the "nationalities question" became the main problem of Europe during the Industrial Era, leading in particular to the 1848 Revolutions, the Italian unification completed during the 1871 Franco-Prussian War, which itself culminated in the proclamation of the German Empire in the Hall of Mirrors in the Palace of Versailles, thus achieving the German unification.
Meanwhile, the Ottoman Empire, the "sick man of Europe", was confronted with endless nationalist movements, which, along with the dissolving of the Austrian-Hungarian Empire, would lead to the creation, after World War I, of the various nation-states of the Balkans, with "national minorities" in their borders.[106]
Ethnic nationalism, which advocated the belief in a hereditary membership of the nation, made its appearance in the historical context surrounding the creation of the modern nation-states.
One of its main influences was the Romantic nationalist movement at the turn of the 19th century, represented by figures such as Johann Herder (1744–1803), Johan Fichte (1762–1814) in the Addresses to the German Nation (1808), Friedrich Hegel (1770–1831), or also, in France, Jules Michelet (1798–1874). It was opposed to liberal nationalism, represented by authors such as Ernest Renan (1823–1892), who conceived of the nation as a community, which, instead of being based on the Volk ethnic group and on a specific, common language, was founded on the subjective will to live together ("the nation is a daily plebiscite", 1882) or also John Stuart Mill (1806–1873).[107]
Ethnic nationalism blended with scientific racist discourses, as well as with "continental imperialist" (Hannah Arendt, 1951[108]) discourses, for example in the pan-Germanism discourses, which postulated the racial superiority of the German Volk (people/folk). The Pan-German League (Alldeutscher Verband), created in 1891, promoted German imperialism and "racial hygiene", and was opposed to intermarriage with Jews. Another popular current, the Völkisch movement, was also an important proponent of the German ethnic nationalist discourse, and it combined Pan-Germanism with modern racial antisemitism. Members of the Völkisch movement, in particular the Thule Society, would participate in the founding of the German Workers' Party (DAP) in Munich in 1918, the predecessor of the Nazi Party. Pan-Germanism played a decisive role in the interwar period of the 1920s–1930s.[108]
These currents began to associate the idea of the nation with the biological concept of a "master race" (often the "Aryan race" or the "Nordic race") issued from the scientific racist discourse. They conflated nationalities with ethnic groups, called "races", in a radical distinction from previous racial discourses that posited the existence of a "race struggle" inside the nation and the state itself. Furthermore, they believed that political boundaries should mirror these alleged racial and ethnic groups, thus justifying ethnic cleansing, in order to achieve "racial purity" and also to achieve ethnic homogeneity in the nation-state.
Such racist discourses, combined with nationalism, were not, however, limited to pan-Germanism. In France, the transition from Republican liberal nationalism, to ethnic nationalism, which made nationalism a characteristic of far-right movements in France, took place during the Dreyfus Affair at the end of the 19th century. During several years, a nationwide crisis affected French society, concerning the alleged treason of Alfred Dreyfus, a French Jewish military officer. The country polarized itself into two opposite camps, one represented by Émile Zola, who wrote J'Accuse…! in defense of Alfred Dreyfus, and the other represented by the nationalist poet, Maurice Barrès (1862–1923), one of the founders of the ethnic nationalist discourse in France.[109] At the same time, Charles Maurras (1868–1952), founder of the monarchist Action française movement, theorized the "anti-France", composed of the "four confederate states of Protestants, Jews, Freemasons and foreigners" (his actual word for the latter being the pejorative métèques). Indeed, to him the first three were all "internal foreigners", who threatened the ethnic unity of the French people.
Bernard Lewis has cited the Greek philosopher Aristotle who, in his discussion of slavery, stated that while Greeks are free by nature, "barbarians" (non-Greeks) are slaves by nature, in that it is in their nature to be more willing to submit to a despotic government.[111] Though Aristotle does not specify any particular races, he argues that people from nations outside Greece are more prone to the burden of slavery than those from Greece.[112] While Aristotle makes remarks about the most natural slaves being those with strong bodies and slave souls (unfit for rule, unintelligent) which would seem to imply a physical basis for discrimination, he also explicitly states that the right kind of souls and bodies do not always go together, implying that the greatest determinate for inferiority and natural slaves versus natural masters is the soul, not the body.[113] This proto-racism is seen as an important precursor to modern racism by classicist Benjamin Isaac.
Such proto-racism and ethnocentrism must be looked at within context, because a modern understanding of racism based on hereditary inferiority (with modern racism based on eugenics and scientific racism) was not yet developed and it is unclear whether Aristotle believed the natural inferiority of Barbarians was caused by environment and climate (like many of his contemporaries) or by birth.[114]
Historian Dante A. Puzzo, in his discussion of Aristotle, racism, and the ancient world writes that:[115]
Racism rests on two basic assumptions: that a correlation exists between physical characteristics and moral qualities; that mankind is divisible into superior and inferior stocks. Racism, thus defined, is a modern conception, for prior to the XVIth century there was virtually nothing in the life and thought of the West that can be described as racist. To prevent misunderstanding a clear distinction must be made between racism and ethnocentrism ... The Ancient Hebrews, in referring to all who were not Hebrews as Gentiles, were indulging in ethnocentrism, not in racism. ... So it was with the Hellenes who denominated all non-Hellenes—whether the wild Scythians or the Egyptians whom they acknowledged as their mentors in the arts of civilization—Barbarians, the term denoting that which was strange or foreign.Bernard Lewis has also cited historians and geographers of the Middle East and North Africa region,[116] including Al-Muqaddasi, Al-Jahiz, Al-Masudi, Abu Rayhan Biruni, Nasir al-Din al-Tusi, and Ibn Qutaybah.[116] Though the Qur'an expresses no racial prejudice, Lewis argues that ethnocentric prejudice later developed among Arabs, for a variety of reasons:[116] their extensive conquests and slave trade; the influence of Aristotelian ideas regarding slavery, which some Muslim philosophers directed towards Zanj (Bantu[117]) and Turkic peoples;[111] and the influence of Judeo-Christian ideas regarding divisions among humankind.[118] By the eighth century, anti-black prejudice among Arabs resulted in discrimination. A number of medieval Arabic authors argued against this prejudice, urging respect for all black people and especially Ethiopians.[119] By the 14th century, a significant number of slaves came from sub-Saharan Africa; Lewis argues that this led to the likes of Egyptian historian Al-Abshibi (1388–1446) writing that "[i]t is said that when the [black] slave is sated, he fornicates, when he is hungry, he steals."[120] According to Lewis, the 14th-century Tunisian scholar Ibn Khaldun also wrote:[116][121]
...beyond [known peoples of black West Africa] to the south there is no civilization in the proper sense. There are only humans who are closer to dumb animals than to rational beings. They live in thickets and caves, and eat herbs and unprepared grain. They frequently eat each other. They cannot be considered human beings. Therefore, the Negro nations are, as a rule, submissive to slavery, because (Negroes) have little that is (essentially) human and possess attributes that are quite similar to those of dumb animals, as we have stated.According to Wesleyan University professor Abdelmajid Hannoum, French Orientalists projected racist and colonialist views of the 19th century into their translations of medieval Arabic writings, including those of Ibn Khaldun. This resulted in the translated texts racializing Arabs and Berber people, when no such distinction was made in the originals.[122] James E. Lindsay argues that the concept of an Arab identity itself did not exist until modern times,[123] though others like Robert Hoyland have argued that a common sense of Arab identity already existed by the 9th century.[124]
With the Umayyad Caliphate's conquest of Hispania, Muslim Arabs and Berbers overthrew the previous Visigothic rulers and created Al-Andalus,[125] which contributed to the Golden age of Jewish culture, and lasted for six centuries.[126] It was followed by the centuries-long Reconquista,[127] during which Christian Iberian kingdoms contested Al-Andalus and progressively conquered the divided Muslim kingdoms, culminating in the fall of the Nasrid kingdom of Granada in 1492 and the rise of Ferdinand V and Isabella I as Catholic monarchs of Spain. The legacy Catholic Spaniards then formulated the limpieza de sangre ("cleanliness of blood") doctrine. It was during this time in history that the Western concept of aristocratic "blue blood" emerged in a racialized, religious and feudal context,[128] so as to stem the upward social mobility of the converted New Christians. Robert Lacey explains:[129]
It was the Spaniards who gave the world the notion that an aristocrat's blood is not red but blue. The Spanish nobility started taking shape around the ninth century in classic military fashion, occupying land as warriors on horseback. They were to continue the process for more than five hundred years, clawing back sections of the peninsula from its Moorish occupiers, and a nobleman demonstrated his pedigree by holding up his sword arm to display the filigree of blue-blooded veins beneath his pale skin—proof that his birth had not been contaminated by the dark-skinned enemy. Sangre azul, blue blood, was thus a euphemism for being a white man—Spain's own particular reminder that the refined footsteps of the aristocracy through history carry the rather less refined spoor of racism.
Following the expulsion of the Arabic Moors and most of the Sephardic Jews from the Iberian peninsula, the remaining Jews and Muslims were forced to convert to Roman Catholicism, becoming "New Christians", who were sometimes discriminated against by the "Old Christians" in some cities (including Toledo), despite condemnations by the Church and the State, which both welcomed the new flock.[128] The Inquisition was carried out by members of the Dominican Order in order to weed out the converts who still practiced Judaism and Islam in secret. The system and ideology of the limpieza de sangre ostracized false Christian converts from society in order to protect it against treason.[130] The remnants of such legislation persevered into the 19th century in military contexts.[131]
In Portugal, the legal distinction between New and Old Christian was only ended through a legal decree issued by the Marquis of Pombal in 1772, almost three centuries after the implementation of the racist discrimination. The limpieza de sangre legislation was common also during the colonization of the Americas, where it led to the racial and feudal separation of peoples and social strata in the colonies. It was however often ignored in practice, as the new colonies needed skilled people.[132]
At the end of the Renaissance, the Valladolid debate (1550–1551), concerning the treatment of the natives of the "New World" pitted the Dominican friar and Bishop of Chiapas, Bartolomé de Las Casas, to another Dominican and Humanist philosopher, Juan Ginés de Sepúlveda. The latter argued that the Indians practiced human sacrifice of innocents, cannibalism, and other such "crimes against nature"; they were unacceptable and should be suppressed by any means possible including war,[133] thus reducing them to slavery or serfdom was in accordance with Catholic theology and natural law. To the contrary, Bartolomé de Las Casas argued that the Amerindians were free men in the natural order and deserved the same treatment as others, according to Catholic theology. It was one of the many controversies concerning racism, slavery, religion, and European morality that would arise in the following centuries and which resulted in the legislation protecting the natives.[134] The marriage between Luisa de Abrego, a free black domestic servant from Seville and Miguel Rodríguez, a white segovian conquistador in 1565 in St. Augustine (Spanish Florida), is the first known and recorded Christian marriage anywhere in the continental United States.[135]
In the Spanish colonies, Spaniards developed a complex caste system based on race, which was used for social control, and which also determined a person's importance in society.[136] While many Latin American countries have long since rendered the system officially illegal through legislation, usually at the time of their independence, prejudice based on degrees of perceived racial distance from European ancestry combined with one's socioeconomic status remain, an echo of the colonial caste system.[137]
Racism is sometimes described as a modern phenomenon. In the view of the French philosopher and historian Michel Foucault, the first formulation of racism emerged in the Early Modern period as the "discourse of race struggle", and a historical and political discourse, which Foucault opposed to the philosophical and juridical discourse of sovereignty.[138]
This European discourse, which first appeared in Great Britain, was then carried on in France by such people as Boulainvilliers (1658–1722), Nicolas Fréret (1688–1749), and then, during the 1789 French Revolution, Sieyès, and afterwards, Augustin Thierry and Cournot. Boulainvilliers, who created the matrix of such racist discourse in France, conceived of the "race" as being something closer to the sense of a "nation", that is, in his time, the "race" meant the "people".
He conceived of France as being divided between various nations—the unified nation-state is an anachronism here—which themselves formed different "races". Boulainvilliers opposed the absolute monarchy, which tried to bypass the aristocracy by establishing a direct relationship to the Third Estate. Thus, he developed the theory that the French aristocrats were the descendants of foreign invaders, whom he called the "Franks", while according to him, the Third Estate constituted the autochthonous, vanquished Gallo-Romans, who were dominated by the Frankish aristocracy as a consequence of the right of conquest. Early modern racism was opposed to nationalism and the nation-state: the Comte de Montlosier, in exile during the French Revolution, who borrowed Boulainvilliers' discourse on the "Nordic race" as being the French aristocracy that invaded the plebeian "Gauls", thus showed his contempt for the Third Estate, calling it "this new people born of slaves ... mixture of all races and of all times".
While 19th-century racism became closely intertwined with nationalism,[139] leading to the ethnic nationalist discourse that identified the "race" with the "folk", leading to such movements as pan-Germanism, pan-Turkism, pan-Arabism, and pan-Slavism, medieval racism precisely divided the nation into various non-biological "races", which were thought to be the consequence of historical conquests and social conflicts. Michel Foucault traced the genealogy of modern racism to this medieval "historical and political discourse of race struggle". According to him, it divided itself in the 19th century according to two rival lines: on one hand, it was incorporated by racists, biologists and eugenicists, who gave it the modern sense of "race", and they also transformed this popular discourse into a "state racism" (e.g., Nazism). On the other hand, Marxism also seized this discourse founded on the assumption of a political struggle that provided the real engine of history and continued to act underneath the apparent peace. Thus, Marxists transformed the essentialist notion of "race" into the historical notion of "class struggle", defined by socially structured positions: capitalist or proletarian. In The Will to Knowledge (1976), Foucault analyzed another opponent of the "race struggle" discourse: Sigmund Freud's psychoanalysis, which opposed the concept of "blood heredity", prevalent in the 19th century racist discourse.
Authors such as Hannah Arendt, in her 1951 book The Origins of Totalitarianism, have said that the racist ideology (popular racism) which developed at the end of the 19th century helped legitimize the imperialist conquests of foreign territories and the atrocities that sometimes accompanied them (such as the Herero and Namaqua Genocide of 1904–1907 or the Armenian genocide of 1915–1917). Rudyard Kipling's poem, The White Man's Burden (1899), is one of the more famous illustrations of the belief in the inherent superiority of the European culture over the rest of the world, though it is also thought to be a satirical appraisal of such imperialism. Racist ideology thus helped legitimize the conquest and incorporation of foreign territories into an empire, which were regarded as a humanitarian obligation partially as a result of these racist beliefs.
However, during the 19th century, Western European colonial powers were involved in the suppression of the Arab slave trade in Africa,[140] as well as in the suppression of the slave trade in West Africa.[141] Some Europeans during the time period objected to injustices that occurred in some colonies and lobbied on behalf of aboriginal peoples. Thus, when the Hottentot Venus was displayed in England in the beginning of the 19th century, the African Association publicly opposed itself to the exhibition. The same year that Kipling published his poem, Joseph Conrad published Heart of Darkness (1899), a clear criticism of the Congo Free State, which was owned by Leopold II of Belgium.
Examples of racial theories used include the creation of the Hamitic theory during the European exploration of Africa. The term Hamite was applied to different populations within North Africa, mainly comprising Ethiopians, Eritreans, Somalis, Berbers, and the ancient Egyptians. Hamites were regarded as Caucasoid peoples who probably originated in either Arabia or Asia on the basis of their cultural, physical and linguistic similarities with the peoples of those areas.[142][143][144] Europeans considered Hamites to be more civilized than Sub-Saharan Africans, and more akin to themselves and Semitic peoples.[145] In the first two-thirds of the 20th century, the Hamitic race was, in fact, considered one of the branches of the Caucasian race, along with the Indo-Europeans, Semites, and the Mediterraneans.
However, the Hamitic peoples themselves were often deemed to have failed as rulers, which was usually ascribed to interbreeding with Negroes. In the mid-20th century, the German scholar Carl Meinhof (1857–1944) claimed that the Bantu race was formed by a merger of Hamitic and Negro races. The Hottentots (Nama or Khoi) were formed by the merger of Hamitic and Bushmen (San) races—both being termed nowadays as Khoisan peoples.
In the United States in the early 19th century, the American Colonization Society was established as the primary vehicle for proposals to return black Americans to greater freedom and equality in Africa.[146] The colonization effort resulted from a mixture of motives with its founder Henry Clay stating that "unconquerable prejudice resulting from their color, they never could amalgamate with the free whites of this country. It was desirable, therefore, as it respected them, and the residue of the population of the country, to drain them off".[147] Racism spread throughout the New World in the late 19th century and early 20th century. Whitecapping, which started in Indiana in the late 19th century, soon spread throughout all of North America, causing many African laborers to flee from the land they worked on. In the US, during the 1860s, racist posters were used during election campaigns. In one of these racist posters (see above), a black man is depicted lounging idly in the foreground as one white man ploughs his field and another chops wood. Accompanying labels are: "In the sweat of thy face shalt thou eat thy bread", and "The white man must work to keep his children and pay his taxes." The black man wonders, "Whar is de use for me to work as long as dey make dese appropriations." Above in a cloud is an image of the "Freedman's Bureau! Negro Estimate of Freedom!" The bureau is pictured as a large domed building resembling the U.S. Capitol and is inscribed "Freedom and No Work". Its columns and walls are labeled, "Candy", "Rum, Gin, Whiskey", "Sugar Plums", "Indolence", "White Women", "Apathy", "White Sugar", "Idleness", and so on.
On June 5, 1873, Sir Francis Galton, distinguished English explorer and cousin of Charles Darwin, wrote in a letter to The Times:
My proposal is to make the encouragement of Chinese settlements of Africa a part of our national policy, in the belief that the Chinese immigrants would not only maintain their position, but that they would multiply and their descendants supplant the inferior Negro race ... I should expect that the African seaboard, now sparsely occupied by lazy, palavering savages, might in a few years be tenanted by industrious, order-loving Chinese, living either as a semidetached dependency of China, or else in perfect freedom under their own law.[148]The Nazi party, which seized power in the 1933 German elections and maintained a dictatorship over much of Europe until the End of World War II on the European continent, deemed the Germans to be part of an Aryan "master race" (Herrenvolk), who therefore had the right to expand their territory and enslave or kill members of other races deemed inferior.[149]
The racial ideology conceived by the Nazis graded humans on a scale of pure Aryan to non-Aryan, with the latter viewed as subhuman. At the top of the scale of pure Aryans were Germans and other Germanic peoples including the Dutch, Scandinavians, and the English as well as other peoples such as some northern Italians and the French, who were said to have a suitable admixture of Germanic blood.[150] Nazi policies labeled Romani people, people of color, and Slavs (mainly Poles, Serbs, Russians, Belarusians, Ukrainians and Czechs) as inferior non-Aryan subhumans.[151][152] Jews were at the bottom of the hierarchy, considered inhuman and thus unworthy of life.[152][153][154][155][156][157][158] In accordance with Nazi racial ideology, approximately six million Jews were killed in the Holocaust. 2.5 million ethnic Poles, 0.5 million ethnic Serbs and 0.2–0.5 million Romani were killed by the regime and its collaborators.[159]
The Nazis considered most Slavs to be non-Aryan Untermenschen. The Nazi Party's chief racial theorist, Alfred Rosenberg, adopted the term from Klansman Lothrop Stoddard's 1922 book The Revolt Against Civilization: The Menace of the Under-man.[160] In the secret plan Generalplan Ost ("Master Plan East") the Nazis resolved to expel, enslave, or exterminate most Slavic people to provide "living space" for Germans,[161] but Nazi policy towards Slavs changed during World War II due to manpower shortages which necessitated limited Slavic participation in the Waffen-SS.[162] Significant war crimes were committed against Slavs, particularly Poles, and Soviet POWs had a far higher mortality rate than their American and British counterparts due to deliberate neglect and mistreatment. Between June 1941 and January 1942, the Nazis killed an estimated 2.8 million Red Army POWs, whom they viewed as "subhuman".[163]
In the years 1943–1945, around 120,000 Polish people, mostly women and children, became the victims of ethnicity-based massacres by the Ukrainian Insurgent Army, which was then operating in the territory of occupied Poland.[164] In addition to Poles who represented the vast majority of the murdered people, the victims also included Jews, Armenians, Russians, and Ukrainians who were married to Poles or attempted to help them.[165]
During the intensification of ties with Nazi Germany in the 1930s, Ante Pavelić and the Ustaše and their idea of the Croatian nation became increasingly race-oriented.[166][167][168] The Ustaše view of national and racial identity, as well as the theory of Serbs as an inferior race, was influenced by Croatian nationalists and intellectuals from the end of the 19th and the beginning of the 20th century.[166][169][170][171] Serbs were primary targets of racial laws and murders in the puppet Independent State of Croatia (NDH); Jews and Roma were also targeted.[172] The Ustaše introduced laws to strip Serbs of their citizenship, livelihoods, and possessions.[173] During the genocide in the NDH, Serbs suffered among the highest casualty rates in Europe during the World War II, and the NDH was one of the most lethal regimes in the 20th century.[174][175][168]
German praise for America's institutional racism was continuous throughout the early 1930s, and Nazi lawyers were advocates of the use of American models.[176] Race based U.S. citizenship laws and anti-miscegenation laws (no race mixing) directly inspired the Nazi's two principal Nuremberg racial laws—the Citizenship Law and the Blood Law.[176] Hitler's 1925 memoir Mein Kampf was full of admiration for America's treatment of "coloreds".[177] Nazi expansion eastward was accompanied with invocation of America's colonial expansion westward, with the accompanying actions toward the Native Americans.[178] In 1928, Hitler praised Americans for having "gunned down the millions of Redskins to a few hundred thousand, and now keeps the modest remnant under observation in a cage."[179] On Nazi Germany's expansion eastward, in 1941 Hitler stated, "Our Mississippi [the line beyond which Thomas Jefferson wanted all Indians expelled] must be the Volga."[178]
White supremacy was dominant in the U.S. from its founding up to the civil rights movement.[180] On the U.S. immigration laws prior to 1965, sociologist Stephen Klineberg cited the law as clearly declaring "that Northern Europeans are a superior subspecies of the white race."[181] While anti-Asian racism was embedded in U.S. politics and culture in the early 20th century, Indians were also racialized for their anticolonialism, with U.S. officials, casting them as a "Hindu" menace, pushing for Western imperial expansion abroad.[182] The Naturalization Act of 1790 limited U.S. citizenship to whites only, and in the 1923 case, United States v. Bhagat Singh Thind, the Supreme Court ruled that high caste Hindus were not "white persons" and were therefore racially ineligible for naturalized citizenship.[183][184] It was after the Luce–Celler Act of 1946 that a quota of 100 Indians per year could immigrate to the U.S. and become citizens.[185] The Immigration and Nationality Act of 1965 dramatically opened entry to the U.S. to immigrants other than traditional Northern European and Germanic groups, and as a result would significantly alter the demographic mix in the U.S.[181]
Serious race riots in Durban between Indians and Zulus erupted in 1949.[186] Ne Win's rise to power in Burma in 1962 and his relentless persecution of "resident aliens" led to an exodus of some 300,000 Burmese Indians.[187] They migrated to escape racial discrimination and wholesale nationalisation of private enterprises a few years later, in 1964.[188] The Zanzibar Revolution of January 12, 1964, put an end to the local Arab dynasty.[189] Thousands of Arabs and Indians in Zanzibar were massacred in riots, and thousands more were detained or fled the island.[190] In August 1972, Ugandan President Idi Amin started the expropriation of properties owned by Asians and Europeans.[191][192] In the same year, Amin ethnically cleansed Uganda's Asians, giving them 90 days to leave the country.[193]
Shortly after World War II, the South African National Party took control of the government in South Africa. Between 1948 and 1994, the apartheid regime took place. This regime based its ideology on the racial separation of whites and non-whites, including the unequal rights of non-whites. Several protests and violence occurred during the struggle against apartheid, the most famous of these include the Sharpeville Massacre in 1960, the Soweto uprising in 1976, the Church Street bombing of 1983, and the Cape Town peace march of 1989.[194]
During the Congo Civil War (1998–2003), Pygmy people were hunted down like game animals and eaten. Both sides in the war regarded them as "subhuman" and some say their flesh can confer magical powers. UN human rights activists reported in 2003 that rebels had carried out acts of cannibalism. Sinafasi Makelo, a representative of the Mbuti pygmies, has asked the UN Security Council to recognise cannibalism as both a crime against humanity and an act of genocide.[196] A report released by the United Nations Committee on the Elimination of Racial Discrimination condemns Botswana's treatment of the 'Bushmen' as racist.[197] In 2008, the tribunal of the 15-nation Southern African Development Community (SADC) accused Zimbabwean President Robert Mugabe of having a racist attitude towards white people.[198][199]
The mass demonstrations and riots against African students in Nanjing, China, lasted from December 1988 to January 1989.[200] In November 2009, British newspaper The Guardian reported that Lou Jing, of mixed Chinese and African parentage, had emerged as the most famous talent show contestant in China and has become the subject of intense debate because of her skin color.[201] Her attention in the media opened serious debates about racism in China and racial prejudice.[202]
Some 70,000 black African Mauritanians were expelled from Mauritania in the late 1980s.[203] In the Sudan, black African captives in the civil war were often enslaved, and female prisoners were often sexually abused.[204] The Darfur conflict has been described by some as a racial matter.[205] In October 2006, Niger announced that it would deport the approximately 150,000[206] Arabs living in the Diffa region of eastern Niger to Chad.[207] While the government collected Arabs in preparation for the deportation, two girls died, reportedly after fleeing Government forces, and three women suffered miscarriages.[208]
The Jakarta riots of May 1998 targeted many Chinese Indonesians.[209] The anti-Chinese legislation was in the Indonesian constitution until 1998. Resentment against Chinese workers has led to violent confrontations in Africa[210][211][212] and Oceania.[213][214] Anti-Chinese rioting, involving tens of thousands of people,[215] broke out in Papua New Guinea in May 2009.[216] Indo-Fijians suffered violent attacks after the Fiji coup in 2000.[217] Non-indigenous citizens of Fiji are subject to discrimination.[218][219] Racial divisions also exist in Guyana,[220] Malaysia,[221] Trinidad and Tobago,[222] Madagascar,[223] and South Africa.[224] In Malaysia such racist state policies are codified on many levels,[225][226] see Bumiputera.
Peter Bouckaert, the Human Rights Watch's emergencies director, said in an interview that "racist hatred" is the chief motivation behind the violence against Rohingya Muslims in Myanmar.[227]
One form of racism in the United States was enforced racial segregation, which existed until the 1960s, when it was outlawed in the Civil Rights Act of 1964. It has been argued that this separation of races continues to exist de facto today in different forms, such as lack of access to loans and resources or discrimination by police and other government officials.[228][229]
The 2016 Pew Research poll found that Italians, in particular, hold strong anti-Romani views, with 82% of Italians expressing negative opinions about Romani. In Greece, there are 67%, in Hungary, 64%, in France, 61%, in Spain, 49%, in Poland, 47%, in the UK, 45%, in Sweden, 42%, in Germany, 40%, and in the Netherlands, 37%, that have an unfavourable view of Roma.[230] A survey conducted by Harvard University found the Czech Republic, Lithuania, Belarus and Ukraine had the strongest racial bias against black people in Europe, while Serbia, Slovenia and Bosnia and Herzegovina had the weakest racial bias, followed by Croatia and Ireland.[231][232]
The modern biological definition of race developed in the 19th century with scientific racist theories. The term scientific racism refers to the use of science to justify and support racist beliefs, which goes back to the early 18th century, though it gained most of its influence in the mid-19th century, during the New Imperialism period. Also known as academic racism, such theories first needed to overcome the Church's resistance to positivist accounts of history and its support of monogenism, the concept that all human beings were originated from the same ancestors, in accordance with creationist accounts of history.
These racist theories put forth on scientific hypothesis were combined with unilineal theories of social progress, which postulated the superiority of the European civilization over the rest of the world. Furthermore, they frequently made use of the idea of "survival of the fittest", a term coined by Herbert Spencer in 1864, associated with ideas of competition, which were named social Darwinism in the 1940s. Charles Darwin himself opposed the idea of rigid racial differences in The Descent of Man (1871), in which he argued that humans were all of one species, sharing common descent. He recognised racial differences as varieties of humanity, and emphasised the close similarities between people of all races in mental faculties, tastes, dispositions and habits, while still contrasting the culture of the "lowest savages" with European civilization.[233][234]
At the end of the 19th century, proponents of scientific racism intertwined themselves with eugenics discourses of "degeneration of the race" and "blood heredity".[citation needed] Henceforth, scientific racist discourses could be defined as the combination of polygenism, unilinealism, social Darwinism, and eugenism. They found their scientific legitimacy on physical anthropology, anthropometry, craniometry, phrenology, physiognomy, and others now discredited disciplines in order to formulate racist prejudices.
Before being disqualified in the 20th century by the American school of cultural anthropology (Franz Boas, etc.), the British school of social anthropology (Bronisław Malinowski, Alfred Radcliffe-Brown, etc.), the French school of ethnology (Claude Lévi-Strauss, etc.), as well as the discovery of the neo-Darwinian synthesis, such sciences, in particular anthropometry, were used to deduce behaviours and psychological characteristics from outward, physical appearances.
The neo-Darwinian synthesis, first developed in the 1930s, eventually led to a gene-centered view of evolution in the 1960s. According to the Human Genome Project, the most complete mapping of human DNA to date indicates that there is no clear genetic basis to racial groups. While some genes are more common in certain populations, there are no genes that exist in all members of one population and no members of any other.[235]
The first theory of eugenics was developed in 1869 by Francis Galton (1822–1911), who used the then-popular concept of degeneration. He applied statistics to study human differences and the alleged "inheritance of intelligence", foreshadowing future uses of "intelligence testing" by the anthropometry school. Such theories were vividly described by the writer Émile Zola (1840–1902), who started publishing in 1871, a twenty-novel cycle, Les Rougon-Macquart, where he linked heredity to behavior. Thus, Zola described the high-born Rougons as those involved in politics (Son Excellence Eugène Rougon) and medicine (Le Docteur Pascal) and the low-born Macquarts as those fatally falling into alcoholism (L'Assommoir), prostitution (Nana), and homicide (La Bête humaine).
During the rise of Nazism in Germany, some scientists in Western nations worked to debunk the regime's racial theories. A few argued against racist ideologies and discrimination, even if they believed in the alleged existence of biological races. However, in the fields of anthropology and biology, these were minority positions until the mid-20th century.[236] According to the 1950 UNESCO statement, The Race Question, an international project to debunk racist theories had been attempted in the mid-1930s. However, this project had been abandoned. Thus, in 1950, UNESCO declared that it had resumed:
...up again, after a lapse of fifteen years, a project that the International Committee on Intellectual Cooperation has wished to carry through but that it had to abandon in deference to the appeasement policy of the pre-war period. The race question had become one of the pivots of Nazi ideology and policy. Masaryk and Beneš took the initiative of calling for a conference to re-establish in the minds and consciences of men everywhere the truth about race ... Nazi propaganda was able to continue its baleful work unopposed by the authority of an international organisation.
The Third Reich's racial policies, its eugenics programs and the extermination of Jews in the Holocaust, as well as the Romani people in the Porrajmos (the Romani Holocaust) and others minorities led to a change in opinions about scientific research into race after the war.[citation needed] Changes within scientific disciplines, such as the rise of the Boasian school of anthropology in the United States contributed to this shift. These theories were strongly denounced in the 1950 UNESCO statement, signed by internationally renowned scholars, and titled The Race Question.
Works such as Arthur de Gobineau's An Essay on the Inequality of the Human Races (1853–1855) may be considered one of the first theorizations of this new racism, founded on an essentialist notion of race, which opposed the former racial discourse, of Boulainvilliers for example, which saw in races a fundamentally historical reality, which changed over time. Gobineau, thus, attempted to frame racism within the terms of biological differences among humans, giving it the legitimacy of biology.
Gobineau's theories would be expanded in France by Georges Vacher de Lapouge (1854–1936)'s typology of races, who published in 1899 The Aryan and his Social Role, in which he claimed that the white "Aryan race" "dolichocephalic", was opposed to the "brachycephalic" race, of whom the "Jew" was the archetype. Vacher de Lapouge thus created a hierarchical classification of races, in which he identified the "Homo europaeus (Teutonic, Protestant, etc.), the "Homo alpinus" (Auvergnat, Turkish, etc.), and finally the "Homo mediterraneus" (Neapolitan, Andalus, etc.) He assimilated races and social classes, considering that the French upper class was a representation of the Homo europaeus, while the lower class represented the Homo alpinus. Applying Galton's eugenics to his theory of races, Vacher de Lapouge's "selectionism" aimed first at achieving the annihilation of trade unionists, considered to be a "degenerate"; second, creating types of man each destined to one end, in order to prevent any contestation of labour conditions. His "anthroposociology" thus aimed at blocking social conflict by establishing a fixed, hierarchical social order.[237]
The same year, William Z. Ripley used identical racial classification in The Races of Europe (1899), which would have a great influence in the United States. Other scientific authors include H.S. Chamberlain at the end of the 19th century (a British citizen who naturalized himself as German because of his admiration for the "Aryan race") and Madison Grant, a eugenicist and author of The Passing of the Great Race (1916). Madison Grant provided statistics for the Immigration Act of 1924, which severely restricted immigration of Jews, Slavs, and Southern Europeans, who were subsequently hindered in seeking to escape Nazi Germany.[238]
Human zoos (called "People Shows"), were an important means of bolstering popular racism by connecting it to scientific racism: they were both objects of public curiosity and of anthropology and anthropometry.[239][240] Joice Heth, an African-American slave, was displayed by P.T. Barnum in 1836, a few years after the exhibition of Saartjie Baartman, the "Hottentot Venus", in England. Such exhibitions became common in the New Imperialism period, and remained so until World War II. Carl Hagenbeck, inventor of the modern zoos, exhibited animals beside humans who were considered "savages".[241][242]
Congolese pygmy Ota Benga was displayed in 1906 by eugenicist Madison Grant, head of the Bronx Zoo, as an attempt to illustrate the "missing link" between humans and orangutans: thus, racism was tied to Darwinism, creating a social Darwinist ideology that tried to ground itself in Darwin's scientific discoveries. The 1931 Paris Colonial Exhibition displayed Kanaks from New Caledonia.[243] A "Congolese village" was on display as late as 1958 at the Brussels' World Fair.
Evolutionary psychologists John Tooby and Leda Cosmides were puzzled by the fact that in the US, race is one of the three characteristics most often used in brief descriptions of individuals (the others are age and sex). They reasoned that natural selection would not have favoured the evolution of an instinct for using race as a classification, because for most of human history, humans almost never encountered members of other races. Tooby and Cosmides hypothesized that modern people use race as a proxy (rough-and-ready indicator) for coalition membership, since a better-than-random guess about "which side" another person is on will be helpful if one does not actually know in advance.
Their colleague Robert Kurzban designed an experiment whose results appeared to support this hypothesis. Using the memory confusion protocol, they presented subjects with pictures of individuals and sentences, allegedly spoken by these individuals, which presented two sides of a debate. The errors that the subjects made in recalling who said what indicated that they sometimes mis-attributed a statement to a speaker of the same race as the "correct" speaker, although they also sometimes mis-attributed a statement to a speaker "on the same side" as the "correct" speaker. In a second run of the experiment, the team also distinguished the "sides" in the debate by clothing of similar colors; and in this case the effect of racial similarity in causing mistakes almost vanished, being replaced by the color of their clothing. In other words, the first group of subjects, with no clues from clothing, used race as a visual guide to guessing who was on which side of the debate; the second group of subjects used the clothing color as their main visual clue, and the effect of race became very small.[244]
Some research suggests that ethnocentric thinking may have actually contributed to the development of cooperation. Political scientists Ross Hammond and Robert Axelrod created a computer simulation wherein virtual individuals were randomly assigned one of a variety of skin colors, and then one of a variety of trading strategies: be color-blind, favor those of your own color, or favor those of other colors. They found that the ethnocentric individuals clustered together, then grew, until all the non-ethnocentric individuals were wiped out.[245]
In The Selfish Gene, evolutionary biologist Richard Dawkins writes that "Blood-feuds and inter-clan warfare are easily interpretable in terms of Hamilton's genetic theory." Dawkins writes that racial prejudice, while not evolutionarily adaptive, "could be interpreted as an irrational generalization of a kin-selected tendency to identify with individuals physically resembling oneself, and to be nasty to individuals different in appearance."[246] Simulation-based experiments in evolutionary game theory have attempted to provide an explanation for the selection of ethnocentric-strategy phenotypes.[247]
Despite support for evolutionary theories relating to an innate origin of racism, various studies have suggested racism is associated with lower intelligence and less diverse peer groups during childhood. A neuroimaging study on amygdala activity during racial matching activities found increased activity to be associated with adolescent age as well as less racially diverse peer groups, which the author conclude suggest a learned aspect of racism.[248] A meta-analysis of neuroimaging studies found amygdala activity correlated to increased scores on implicit measures of racial bias. It was also argued amygdala activity in response to racial stimuli represents increased threat perception rather than the traditional theory of the amygdala activity represented ingroup-outgroup processing.[249] Racism has also been associated with lower childhood IQ in an analysis of 15,000 people in the UK.[250]
A 2017 study in the American Political Science Review found that prejudice towards marginalized groups, such as refugees, could be explained by a failure to take the perspective of the marginalized group.[251] The study found that young Hungarian adults who played a perspective-taking game (a game intended to reduce prejudice towards marginalized groups by having players assume the role of a member of a marginalized group) showed reduced prejudice towards Romani people and refugees, as well as reduced their vote intentions for Hungary's overtly racist, far right party by 10%.[251]
State racism—the institutions and practices of a nation-state that are grounded in racist ideology—has played a major role in all instances of settler colonialism, from the United States to Australia.[citation needed] It also played a prominent role in the Nazi German regime, in fascist regimes throughout Europe, and during the early years of Japan's Shōwa period. These governments advocated and implemented ideologies and policies that were racist, xenophobic, and, in the case of Nazism, genocidal.[252][253]
The Nuremberg Race Laws of 1935 prohibited sexual relations between any Aryan and Jew, considering it Rassenschande, "racial pollution". The Nuremberg Laws stripped all Jews, even quarter- and half-Jews (second and first degree Mischlings), of their German citizenship. This meant that they had no basic citizens' rights, e.g., the right to vote. In 1936, Jews were banned from all professional jobs, effectively preventing them from having any influence in education, politics, higher education, and industry. On 15 November 1938, Jewish children were banned from going to normal schools. By April 1939, nearly all Jewish companies had either collapsed under financial pressure and declining profits, or had been persuaded to sell out to the Nazi government. This further reduced their rights as human beings; they were in many ways officially separated from the German populace. Similar laws existed in Bulgaria—The Law for protection of the nation, Hungary, Romania, and Austria.
Legislative state racism is known to have been enforced by the National Party of South Africa during its Apartheid regime between 1948 and 1994. Here, a series of Apartheid legislation was passed through the legal systems to make it legal for white South Africans to have rights which were superior to those of non-white South Africans. Non-white South Africans were not allowed involvement in any governing matters, including voting; access to quality healthcare; the provision of basic services, including clean water; electricity; as well as access to adequate schooling. Non-white South Africans were also prevented from accessing certain public areas, from using certain public transportation, and were required to live only in certain designated areas. Non-white South Africans were taxed differently than white South Africans and they were also required to carry on them at all times additional documentation, which later became known as "dom passes", to certify their non-white South African citizenship. All of these legislative racial laws were abolished through a series of equal human rights laws which were passed at the end of the Apartheid era in the early 1990s.
Anti-racism includes beliefs, actions, movements, and policies which are adopted or developed in order to oppose racism. In general, it promotes an egalitarian society in which people are not discriminated against on the basis of race. Examples of anti-racist movements include the civil rights movement, the Anti-Apartheid Movement and Black Lives Matter. Nonviolent resistance is sometimes embraced as an element of anti-racist movements, although this was not always the case. Hate crime laws, affirmative action, and bans on racist speech are also examples of government policy which is intended to suppress racism.

Turkish (Türkçe (listen), Türk dili; also Türkiye Türkçesi 'Turkish of Turkey'[15]) is the most widely spoken of the Turkic languages, with around 80 to 90 million speakers. It is the national language of Turkey and Northern Cyprus. Significant smaller groups of Turkish speakers also exist in Germany, Austria, Bulgaria, North Macedonia,[16] Greece,[17] Cyprus, other parts of Europe, the Caucasus, and some parts of Central Asia, Iraq, and Syria. Cyprus has requested the European Union to add Turkish as an official language, even though Turkey is not a member state.[18] Turkish is the 13th most spoken language in the world.
To the west, the influence of Ottoman Turkish—the variety of the Turkish language that was used as the administrative and literary language of the Ottoman Empire—spread as the Ottoman Empire expanded. In 1928, as one of Atatürk's Reforms in the early years of the Republic of Turkey, the Ottoman Turkish alphabet was replaced with a Latin alphabet.
The distinctive characteristics of the Turkish language are vowel harmony and extensive agglutination. The basic word order of Turkish is subject–object–verb. Turkish has no noun classes or grammatical gender. The language makes usage of honorifics and has a strong T–V distinction which distinguishes varying levels of politeness, social distance, age, courtesy or familiarity toward the addressee. The plural second-person pronoun and verb forms are used referring to a single person out of respect.
Turkish is a member of the Oghuz group of the Turkic family. Other members include Azerbaijani, spoken in Azerbaijan and north-west Iran, Gagauz of Gagauzia, Qashqai of south Iran and the Turkmen of Turkmenistan.[19]
Classification of the Turkic languages is complicated. The migrations of the Turkic peoples and their consequent intermingling with one another and with peoples who spoke non-Turkic languages, have created a linguistic situation of vast complexity.[19]
There is ongoing debate about whether the Turkic family is itself a branch of a larger Altaic family, including Japanese, Korean, Mongolian and Tungusic.[20] The nineteenth-century Ural-Altaic theory, which grouped Turkish with Finnish, Hungarian and Altaic languages, is controversial.[21] The theory was based mostly on the fact these languages share three features: agglutination, vowel harmony and lack of grammatical gender.[21]
The earliest known Old Turkic inscriptions are the three monumental Orkhon inscriptions found in modern Mongolia. Erected in honour of the prince Kul Tigin and his brother Emperor Bilge Khagan, these date back to the Second Turkic Khaganate (dated 682–744 CE).[22] After the discovery and excavation of these monuments and associated stone slabs by Russian archaeologists in the wider area surrounding the Orkhon Valley between 1889 and 1893, it became established that the language on the inscriptions was the Old Turkic language written using the Old Turkic alphabet, which has also been referred to as "Turkic runes" or "runiform" due to a superficial similarity to the Germanic runic alphabets.[23]
With the Turkic expansion during Early Middle Ages (c. 6th–11th centuries), peoples speaking Turkic languages spread across Central Asia, covering a vast geographical region stretching from Siberia all the way to Europe and the Mediterranean. The Seljuqs of the Oghuz Turks, in particular, brought their language, Oghuz—the direct ancestor of today's Turkish language—into Anatolia during the 11th century.[24] Also during the 11th century, an early linguist of the Turkic languages, Mahmud al-Kashgari from the Kara-Khanid Khanate, published the first comprehensive Turkic language dictionary and map of the geographical distribution of Turkic speakers in the Dīwān Lughāt al-Turk (ديوان لغات الترك).[25]
Following the adoption of Islam around the year 950 by the Kara-Khanid Khanate and the Seljuq Turks, who are both regarded as the ethnic and cultural ancestors of the Ottomans, the administrative language of these states acquired a large collection of loanwords from Arabic and Persian. Turkish literature during the Ottoman period, particularly Divan poetry, was heavily influenced by Persian, including the adoption of poetic meters and a great quantity of imported words. The literary and official language during the Ottoman Empire period (c. 1299–1922) is termed Ottoman Turkish, which was a mixture of Turkish, Persian, and Arabic that differed considerably and was largely unintelligible to the period's everyday Turkish. The everyday Turkish, known as kaba Türkçe or "vulgar Turkish", spoken by the less-educated lower and also rural members of society, contained a higher percentage of native vocabulary and served as basis for the modern Turkish language.[26]
While visiting the region between Adıyaman and Adana, Evliya Çelebi recorded the "Turkman language" and compared it with his own Turkish:
After the foundation of the modern state of Turkey and the script reform, the Turkish Language Association (TDK) was established in 1932 under the patronage of Mustafa Kemal Atatürk, with the aim of conducting research on Turkish. One of the tasks of the newly established association was to initiate a language reform to replace loanwords of Arabic and Persian origin with Turkish equivalents.[28] By banning the usage of imported words in the press,[clarification needed] the association succeeded in removing several hundred foreign words from the language. While most of the words introduced to the language by the TDK were newly derived from Turkic roots, it also opted for reviving Old Turkish words which had not been used for centuries.[29] In 1935, the TDK published a bilingual Ottoman-Turkish/Pure Turkish dictionary that documents the results of the language reform.[30]   
Owing to this sudden change in the language, older and younger people in Turkey started to differ in their vocabularies. While the generations born before the 1940s tend to use the older terms of Arabic or Persian origin, the younger generations favor new expressions. It is considered particularly ironic that Atatürk himself, in his lengthy speech to the new Parliament in 1927, used a style of Ottoman which sounded so alien to later listeners that it had to be "translated" three times into modern Turkish: first in 1963, again in 1986, and most recently in 1995.[31]
The past few decades have seen the continuing work of the TDK to coin new Turkish words to express new concepts and technologies as they enter the language, mostly from English. Many of these new words, particularly information technology terms, have received widespread acceptance. However, the TDK is occasionally criticized for coining words which sound contrived and artificial. Some earlier changes—such as bölem to replace fırka, "political party"—also failed to meet with popular approval (fırka has been replaced by the French loanword parti). Some words restored from Old Turkic have taken on specialized meanings; for example betik (originally meaning "book") is now used to mean "script" in computer science.[32]
Some examples of modern Turkish words and the old loanwords are:
Turkish is natively spoken by the Turkish people in Turkey and by the Turkish diaspora in some 30 other countries. Turkish language is mutually intelligible with Azerbaijani and other Turkic languages. In particular, Turkish-speaking minorities exist in countries that formerly (in whole or part) belonged to the Ottoman Empire, such as Iraq[34], Bulgaria, Cyprus, Greece (primarily in Western Thrace), the Republic of North Macedonia, Romania, and Serbia. More than two million Turkish speakers live in Germany; and there are significant Turkish-speaking communities in the United States, France, the Netherlands, Austria, Belgium, Switzerland, and the United Kingdom.[1] Due to the cultural assimilation of Turkish immigrants in host countries, not all ethnic members of the diaspora speak the language with native fluency.[35]
In 2005 93% of the population of Turkey were native speakers of Turkish,[36] about 67 million at the time, with Kurdish languages making up most of the remainder.[37]
Azerbaijani language, official in Azerbaijan, is mutually intelligible with Turkish and speakers of both languages can understand them without noticeable difficulty, especially when discussion comes on ordinary, daily language. Turkey has very good relations with Azerbaijan, with a multitude of Turkish companies and authorities investing there, while the influence of Turkey in the country is very high. The rising presence of this very similar language in Azerbaijan and the fact that many children use Turkish words instead of Azerbaijani words due to satellite TV has caused concern that the dinstictive features of the language will be eroded. Many bookstores sell books in Turkish language along Azerbaijani language ones, with Agalar Mahmadov, a leading intellectual, voicing his concern that Turkish language has "already started to take over the national and natural dialects of Azerbaijan". However, the presence of Turkish as foreign language is not as high as Russian.[38] In Uzbekistan, the second most populated Turkic country, a new TV channel Foreign Languages TV was established in 2022. This channel has been broadcasting Turkish lessons along with English, French, German and Russian lessons.
Turkish is the official language of Turkey and is one of the official languages of Cyprus. Turkish has official status in 38 municipalities in Kosovo, including Mamusha,[39][40], two in the Republic of North Macedonia and in Kirkuk Governorate in Iraq.[41][42]
In Turkey, the regulatory body for Turkish is the Turkish Language Association (Türk Dil Kurumu or TDK), which was founded in 1932 under the name Türk Dili Tetkik Cemiyeti ("Society for Research on the Turkish Language"). The Turkish Language Association was influenced by the ideology of linguistic purism: indeed one of its primary tasks was the replacement of loanwords and of foreign grammatical constructions with equivalents of Turkish origin.[43] These changes, together with the adoption of the new Turkish alphabet in 1928, shaped the modern Turkish language spoken today. The TDK became an independent body in 1951, with the lifting of the requirement that it should be presided over by the Minister of Education. This status continued until August 1983, when it was again made into a governmental body in the constitution of 1982, following the military coup d'état of 1980.[29]
Modern standard Turkish is based on the dialect of Istanbul.[44] This Istanbul Turkish (İstanbul Türkçesi) constitutes the model of written and spoken Turkish, as recommended by Ziya Gökalp, Ömer Seyfettin and others.[45]
Dialectal variation persists, in spite of the levelling influence of the standard used in mass media and in the Turkish education system since the 1930s.[46] Academic researchers from Turkey often refer to Turkish dialects as ağız or şive, leading to an ambiguity with the linguistic concept of accent, which is also covered with these words. Several universities, as well as a dedicated work-group of the Turkish Language Association, carry out projects investigating Turkish dialects. As of 2002[update] work continued on the compilation and publication of their research as a comprehensive dialect-atlas of the Turkish language.[47][48]
Some immigrants to Turkey from Rumelia speak Rumelian Turkish, which includes the distinct dialects of Ludogorie, Dinler, and Adakale, which show the influence of the theoretized Balkan sprachbund. Kıbrıs Türkçesi is the name for Cypriot Turkish and is spoken by the Turkish Cypriots. Edirne is the dialect of Edirne. Ege is spoken in the Aegean region, with its usage extending to Antalya. The nomadic Yörüks of the Mediterranean Region of Turkey also have their own dialect of Turkish.[49] This group is not to be confused with the Yuruk nomads of Macedonia, Greece, and European Turkey, who speak Balkan Gagauz Turkish.
The Meskhetian Turks who live in Kazakhstan, Azerbaijan and Russia as well as in several Central Asian countries, also speak an Eastern Anatolian dialect of Turkish, originating in the areas of Kars, Ardahan, and Artvin and sharing similarities with Azerbaijani, the language of Azerbaijan.[50]
The Central Anatolia Region speaks Orta Anadolu. Karadeniz, spoken in the Eastern Black Sea Region and represented primarily by the Trabzon dialect, exhibits substratum influence from Greek in phonology and syntax;[51] it is also known as Laz dialect (not to be confused with the Laz language). Kastamonu is spoken in Kastamonu and its surrounding areas. Karamanli Turkish is spoken in Greece, where it is called Kαραμανλήδικα. It is the literary standard for the Karamanlides.[52]
At least one source claims Turkish consonants are laryngeally-specified three-way fortis-lenis (aspirated/neutral/voiced) like Armenian.[54]
The phoneme that is usually referred to as yumuşak g ("soft g"), written ⟨ğ⟩ in Turkish orthography, represents a vowel sequence or a rather weak bilabial approximant between rounded vowels, a weak palatal approximant between unrounded front vowels, and a vowel sequence elsewhere. It never occurs at the beginning of a word or a syllable, but always follows a vowel. When word-final or preceding another consonant, it lengthens the preceding vowel.[55]
In native Turkic words, the sounds [c], [ɟ], and [l] are in complementary distribution with [k], [ɡ], and [ɫ]; the former set occurs adjacent to front vowels and the latter adjacent to back vowels. The distribution of these phonemes is often unpredictable, however, in foreign borrowings and proper nouns. In such words, [c], [ɟ], and [l] often occur with back vowels:[56] some examples are given below.
Turkish orthography reflects final-obstruent devoicing, a form of consonant mutation whereby a voiced obstruent, such as /b d dʒ ɡ/, is devoiced to [p t tʃ k] at the end of a word or before a consonant, but retains its voicing before a vowel. In loan words, the voiced equivalent of /k/ is /g/; in native words, it is /ğ/.[57][58]
This is analogous to languages such as German and Russian, but in the case of Turkish it only applies, as the above examples demonstrate, to stops and affricates, not to fricatives. The spelling is usually made to match the sound. However, in a few cases, such as ad 'name' (dative ada), the underlying form is retained in the spelling (cf. at 'horse', dative ata). Other exceptions are od 'fire' vs. ot 'herb', sac 'sheet metal', saç 'hair'. Most loanwords, such as kitap above, are spelled as pronounced, but a few such as hac 'hajj', şad 'happy', and yad 'strange' or 'stranger' also show their underlying forms.[citation needed]
Native nouns of two or more syllables that end in /k/ in dictionary form are nearly all //ğ// in underlying form. However, most verbs and monosyllabic nouns are underlyingly //k//.[59]
The vowels of the Turkish language are, in their alphabetical order, ⟨a⟩, ⟨e⟩, ⟨ı⟩, ⟨i⟩, ⟨o⟩, ⟨ö⟩, ⟨u⟩, ⟨ü⟩.[60] The Turkish vowel system can be considered as being three-dimensional, where vowels are characterised by how and where they are articulated focusing on three key features: front and back, rounded and unrounded and vowel height.[61] Vowels are classified [±back], [±round] and [±high].[62]
The only diphthongs in the language are found in loanwords and may be categorised as falling diphthongs usually analyzed as a sequence of /j/ and a vowel.[55]
The principle of vowel harmony, which permeates Turkish word-formation and suffixation, is due to the natural human tendency towards economy of muscular effort.[63] This principle is expressed in Turkish through three rules:
The second and third rules minimize muscular effort during speech. More specifically, they are related to the phenomenon of labial assimilation:[65] if the lips are rounded (a process that requires muscular effort) for the first vowel they may stay rounded for subsequent vowels.[64] If they are unrounded for the first vowel, the speaker does not make the additional muscular effort to round them subsequently.[63]
Grammatical affixes have "a chameleon-like quality",[66] and obey one of the following patterns of vowel harmony:
Practically, the twofold pattern (also referred to as the e-type vowel harmony) means that in the environment where the vowel in the word stem is formed in the front of the mouth, the suffix will take the e-form, while if it is formed in the back it will take the a-form. The fourfold pattern (also called the i-type) accounts for rounding as well as for front/back.[68] The following examples, based on the copula -dir4 ("[it] is"), illustrate the principles of i-type vowel harmony in practice: Türkiye'dir ("it is Turkey"),[69] kapıdır ("it is the door"), but gündür ("it is the day"), paltodur ("it is the coat").[70]
These are four word-classes that are exceptions to the rules of vowel harmony:
The road sign in the photograph above illustrates several of these features:
The rules of vowel harmony may vary by regional dialect. The dialect of Turkish spoken in the Trabzon region of northeastern Turkey follows the reduced vowel harmony of Old Anatolian Turkish, with the additional complication of two missing vowels (ü and ı), thus there is no palatal harmony. It's likely that elün meant "your hand" in Old Anatolian. While the 2nd person singular possessive would vary between back and front vowel, -ün or -un, as in elün for "your hand" and kitabun for "your book", the lack of ü vowel in the Trabzon dialect means -un would be used in both of these cases — elun and kitabun.[71]
With the exceptions stated below, Turkish words are oxytone (accented on the last syllable).
Turkish has two groups of sentences: verbal and nominal sentences. In the case of a verbal sentence, the predicate is a finite verb, while the predicate in nominal sentence will have either no overt verb or a verb in the form of the copula ol or y (variants of "be"). Examples of both are given below:[72]
The two groups of sentences have different ways of forming negation. A nominal sentence can be negated with the addition of the word değil. For example, the sentence above would become Necla öğretmen değil ('Necla is not a teacher'). However, the verbal sentence requires the addition of a negative suffix -me to the verb (the suffix comes after the stem but before the tense): Necla okula gitmedi ('Necla did not go to school').[73]
In the case of a verbal sentence, an interrogative clitic mi is added after the verb and stands alone, for example Necla okula gitti mi? ('Did Necla go to school?'). In the case of a nominal sentence, then mi comes after the predicate but before the personal ending, so for example Necla, siz öğretmen misiniz? ('Necla, are you [formal, plural] a teacher?').[73]
Word order in simple Turkish sentences is generally subject–object–verb, as in Korean and Latin, but unlike English, for verbal sentences and subject-predicate for nominal sentences. However, as Turkish possesses a case-marking system, and most grammatical relations are shown using morphological markers, often the SOV structure has diminished relevance and may vary. The SOV structure may thus be considered a "pragmatic word order" of language, one that does not rely on word order for grammatical purposes.[74]
Consider the following simple sentence which demonstrates that the focus in Turkish is on the element that immediately precedes the verb:[75]
The postpredicate position signifies what is referred to as background information in Turkish- information that is assumed to be known to both the speaker and the listener, or information that is included in the context. Consider the following examples:[72]
There has been some debate among linguists whether Turkish is a subject-prominent (like English) or topic-prominent (like Japanese and Korean) language, with recent scholarship implying that it is indeed both subject and topic-prominent.[76] This has direct implications for word order as it is possible for the subject to be included in the verb-phrase in Turkish. There can be S/O inversion in sentences where the topic is of greater importance than the subject.
Turkish is an agglutinative language and frequently uses affixes, and specifically suffixes, or endings.[77] One word can have many affixes and these can also be used to create new words, such as creating a verb from a noun, or a noun from a verbal root (see the section on Word formation). Most affixes indicate the grammatical function of the word.[78]
The only native prefixes are alliterative intensifying syllables used with adjectives or adverbs: for example sımsıcak ("boiling hot" < sıcak) and masmavi ("bright blue" < mavi).[79]
The extensive use of affixes can give rise to long words, e.g. Çekoslovakyalılaştıramadıklarımızdanmışsınızcasına, meaning "In the manner of you being one of those that we apparently couldn't manage to convert to Czechoslovakian". While this case is contrived, long words frequently occur in normal Turkish, as in this heading of a newspaper obituary column: Bayramlaşamadıklarımız (Bayram [festival]-Recipr-Impot-Partic-Plur-PossPl1; "Those of our number with whom we cannot exchange the season's greetings").[80] Another example can be seen in the final word of this heading of the online Turkish Spelling Guide (İmlâ Kılavuzu): Dilde birlik, ulusal birliğin vazgeçilemezlerindendir ("Unity in language is among the indispensables [dispense-Pass-Impot-Plur-PossS3-Abl-Copula] of national unity ~ Linguistic unity is a sine qua non of national unity").[81]
Turkish does not have grammatical gender and the sex of persons do not affect the forms of words. The third-person pronoun o may refer to "he," "she" or "it." Despite this lack, Turkish still has ways of indicating gender in nouns:
There is no definite article in Turkish, but definiteness of the object is implied when the accusative ending is used (see below). Turkish nouns decline by taking case endings. There are six noun cases in Turkish, with all the endings following vowel harmony (shown in the table using the shorthand superscript notation). Since the postposition ile often gets suffixed onto the noun, some analyze it as an instrumental case, although it takes the genitive with personal pronouns, singular demonstratives, and interrogative kim. The plural marker -ler ² immediately follows the noun before any case or other affixes (e.g. köylerin "of the villages").[citation needed]
The accusative case marker is used only for definite objects; compare (bir) ağaç gördük "we saw a tree" with ağacı gördük "we saw the tree".[82] The plural marker -ler ² is generally not used when a class or category is meant: ağaç gördük can equally well mean "we saw trees [as we walked through the forest]"—as opposed to ağaçları gördük "we saw the trees [in question]".[citation needed]
The declension of ağaç illustrates two important features of Turkish phonology: consonant assimilation in suffixes (ağaçtan, ağaçta) and voicing of final consonants before vowels (ağacın, ağaca, ağacı).[citation needed]
Additionally, nouns can take suffixes that assign person: for example -imiz 4, "our". With the addition of the copula (for example -im 4, "I am") complete sentences can be formed. The interrogative particle mi 4 immediately follows the word being questioned, and also follows vowel harmony: köye mi? "[going] to the village?", ağaç mı? "[is it a] tree?".[citation needed]
The Turkish personal pronouns in the nominative case are ben (1s), sen (2s), o (3s), biz (1pl), siz (2pl, or 2h), and onlar (3pl). They are declined regularly with some exceptions: benim (1s gen.); bizim (1pl gen.); bana (1s dat.); sana (2s dat.); and the oblique forms of o use the root on. As mentioned before, all demonstrative singular and personal pronouns take the genitive when ile is affixed onto it: benimle (1s ins.), bizimle (1pl ins.); but onunla (3s ins.), onlarla (3pl ins.). All other pronouns (reflexive kendi and so on) are declined regularly.[citation needed]
Two nouns, or groups of nouns, may be joined in either of two ways:
The following table illustrates these principles.[84] In some cases the constituents of the compounds are themselves compounds; for clarity these subsidiary compounds are marked with [square brackets]. The suffixes involved in the linking are underlined. If the second noun group already had a possessive suffix (because it is a compound by itself), no further suffix is added.
As the last example shows, the qualifying expression may be a substantival sentence rather than a noun or noun group.[88]
There is a third way of linking the nouns where both nouns take no suffixes (takısız tamlama). However, in this case the first noun acts as an adjective,[89] e.g. Demir kapı (iron gate), elma yanak ("apple cheek", i.e. red cheek), kömür göz ("coal eye", i.e. black eye) :
Turkish adjectives are not declined. However most adjectives can also be used as nouns, in which case they are declined: e.g. güzel ("beautiful") → güzeller ("(the) beautiful ones / people"). Used attributively, adjectives precede the nouns they modify. The adjectives var ("existent") and yok ("non-existent") are used in many cases where English would use "there is" or "have", e.g. süt yok ("there is no milk", lit. "(the) milk (is) non-existent"); the construction "noun 1-GEN noun 2-POSS var/yok" can be translated "noun 1 has/doesn't have noun 2"; imparatorun elbisesi yok "the emperor has no clothes" ("(the) emperor-of clothes-his non-existent"); kedimin ayakkabıları yoktu ("my cat had no shoes", lit. "cat-my-of shoe-plur.-its non-existent-past tense").[citation needed]
Turkish verbs indicate person. They can be made negative, potential ("can"), or non-potential ("cannot"). Furthermore, Turkish verbs show tense (present, past, future, and aorist), mood (conditional, imperative, inferential, necessitative, and optative), and aspect. Negation is expressed by the suffix -me²- immediately following the stem.
(Note. For the sake of simplicity the term "tense" is used here throughout, although for some forms "aspect" or "mood" might be more appropriate.) There are 9 simple and 20 compound tenses in Turkish. 9 simple tenses are simple past (di'li geçmiş), inferential past (miş'li geçmiş), present continuous, simple present (aorist), future, optative, subjunctive, necessitative ("must") and imperative.[90] There are three groups of compound forms. Story (hikaye) is the witnessed past of the above forms (except command), rumor (rivayet) is the unwitnessed past of the above forms (except simple past and command), conditional (koşul) is the conditional form of the first five basic tenses.[91] In the example below the second person singular of the verb gitmek ("go"), stem gid-/git-, is shown.
There are also so-called combined verbs, which are created by suffixing certain verb stems (like bil or ver) to the original stem of a verb. Bil is the suffix for the sufficiency mood. It is the equivalent of the English auxiliary verbs "able to", "can" or "may". Ver is the suffix for the swiftness mood, kal for the perpetuity mood and yaz for the approach ("almost") mood.[92] Thus, while gittin means "you went", gidebildin means "you could go" and gidiverdin means "you went swiftly". The tenses of the combined verbs are formed the same way as for simple verbs.
Turkish verbs have attributive forms, including present,[93] similar to the English present participle (with the ending -en2); future (-ecek2); indirect/inferential past (-miş4); and aorist (-er2 or -ir4).
The most important function of some of these attributive verbs is to form modifying phrases equivalent to the relative clauses found in most European languages. The subject of the verb in an -en2 form is (possibly implicitly) in the third person (he/she/it/they); this form, when used in a modifying phrase, does not change according to number. The other attributive forms used in these constructions are the future (-ecek2) and an older form (-dik4), which covers both present and past meanings.[94] These two forms take "personal endings", which have the same form as the possessive suffixes but indicate the person and possibly number of the subject of the attributive verb; for example, yediğim means "what I eat", yediğin means "what you eat", and so on. The use of these "personal or relative participles" is illustrated in the following table, in which the examples are presented according to the grammatical case which would be seen in the equivalent English relative clause.[95]
Latest 2011 edition of Güncel Türkçe Sözlük (Current Turkish Dictionary), the official dictionary of the Turkish language published by Turkish Language Association, contains 117,000 vocabularies and 93,000 articles.[98]
Turkish extensively uses agglutination to form new words from nouns and verbal stems. The majority of Turkish words originate from the application of derivative suffixes to a relatively small set of core vocabulary.[99]
Turkish obeys certain principles when it comes to suffixation. Most suffixes in Turkish will have more than one form, depending on the vowels and consonants in the root- vowel harmony rules will apply; consonant-initial suffixes will follow the voiced/ voiceless character of the consonant in the final unit of the root; and in the case of vowel-initial suffixes an additional consonant may be inserted if the root ends in a vowel, or the suffix may lose its initial vowel. There is also a prescribed order of affixation of suffixes- as a rule of thumb, derivative suffixes precede inflectional suffixes which are followed by clitics, as can be seen in the example set of words derived from a substantive root below:
Another example, starting from a verbal root:
New words are also frequently formed by compounding two existing words into a new one, as in German. Compounds can be of two types- bare and (s)I. The bare compounds, both nouns and adjectives are effectively two words juxtaposed without the addition of suffixes for example the word for girlfriend kızarkadaş (kız+arkadaş) or black pepper karabiber (kara+biber). A few examples of compound words are given below:
However, the majority of compound words in Turkish are (s)I compounds, which means that the second word will be marked by the 3rd person possessive suffix. A few such examples are given in the table below (note vowel harmony):
Turkish is written using a Latin alphabet introduced in 1928 by Atatürk to replace the Ottoman Turkish alphabet, a version of Perso-Arabic alphabet. The Ottoman alphabet marked only three different vowels—long ā, ū and ī—and included several redundant consonants, such as variants of z (which were distinguished in Arabic but not in Turkish). The omission of short vowels in the Arabic script was claimed to make it particularly unsuitable for Turkish, which has eight vowels.[100]
The reform of the script was an important step in the cultural reforms of the period. The task of preparing the new alphabet and selecting the necessary modifications for sounds specific to Turkish was entrusted to a Language Commission composed of prominent linguists, academics, and writers. The introduction of the new Turkish alphabet was supported by public education centers opened throughout the country, cooperation with publishing companies, and encouragement by Atatürk himself, who toured the country teaching the new letters to the public.[101] As a result, there was a dramatic increase in literacy from its original, pre-modern levels.[102][need quotation to verify]
The Latin alphabet was applied to the Turkish language for educational purposes even before the 20th-century reform. Instances include a 1635 Latin-Albanian dictionary by Frang Bardhi, who also incorporated several sayings in the Turkish language, as an appendix to his work (e.g. alma agatsdan irak duschamas[103]—"An apple does not fall far from its tree").
Turkish now has an alphabet suited to the sounds of the language: the spelling is largely phonemic, with one letter corresponding to each phoneme.[104] Most of the letters are used approximately as in English, the main exceptions being ⟨c⟩, which denotes [dʒ] (⟨j⟩ being used for the [ʒ] found in Persian and European loans); and the undotted ⟨ı⟩, representing [ɯ]. As in German, ⟨ö⟩ and ⟨ü⟩ represent [ø] and [y]. The letter ⟨ğ⟩, in principle, denotes [ɣ] but has the property of lengthening the preceding vowel and assimilating any subsequent vowel. The letters ⟨ş⟩ and ⟨ç⟩ represent [ʃ] and [tʃ], respectively. A circumflex is written over back vowels following ⟨k⟩ and ⟨g⟩ when these consonants represent [c] and [ɟ]—almost exclusively in Arabic and Persian loans.[105]
The Turkish alphabet consists of 29 letters (q, w, x omitted and ç, ş, ğ, ı, ö, ü added); the complete list is:
The specifically Turkish letters and spellings described above are illustrated in this table:
Dostlar Beni Hatırlasın by Âşık Veysel Şatıroğlu (1894–1973), a minstrel and highly regarded poet in the Turkish folk literature tradition.
Turkish language uses two standardised keyboard layouts, known as Turkish Q (QWERTY) and Turkish F, with Turkish Q being the most common.
On-line sources


The Tale of Genji (源氏物語, Genji monogatari, pronounced [ɡeɲdʑi monoɡaꜜtaɾi]) is a classic work of Japanese literature written in the early 11th century by the noblewoman and lady-in-waiting Murasaki Shikibu. The original manuscript, created around the peak of the Heian period, no longer exists. It was made in "concertina" or orihon style:[1] several sheets of paper pasted together and folded alternately in one direction then the other.
The work is a unique depiction of the lifestyles of high courtiers during the Heian period. It is written in archaic language and a poetic and complex style that make it unreadable without specialized study.[2] It was not until the early 20th century that Genji was translated into modern Japanese by the poet Akiko Yosano. The first English translation was attempted in 1882 by Suematsu Kencho, but was of poor quality and incomplete.
The work recounts the life of Hikaru Genji, or "Shining Genji", who is the son of an ancient Japanese emperor (known to readers as Emperor Kiritsubo) and a low-ranking concubine called Kiritsubo Consort. For political reasons, the emperor removes Genji from the line of succession, demoting him to a commoner by giving him the surname Minamoto, and he pursues a career as an imperial officer. The tale concentrates on Genji's romantic life and describes the customs of the aristocratic society of the time. It may be the world's first novel,[3] the first psychological novel, and the first novel still to be considered a classic particularly in the context of Japanese literature.
Murasaki was writing at the height of the Fujiwara clan's power—Fujiwara no Michinaga was the Regent in all but name, and the most significant political figure of his day. Consequently, Murasaki is believed to have partially informed the character of Genji through her experience of Michinaga.
The Tale of Genji may have been written chapter by chapter in installments, as Murasaki delivered the tale to aristocratic women (ladies-in-waiting). It has many elements found in a modern novel: a central character and a very large number of major and minor characters, well-developed characterization of all the major players, a sequence of events covering the central character's lifetime and beyond. There is no specified plot, but events happen and characters simply grow older. Despite a dramatis personæ of some four hundred characters, it maintains internal consistency; for instance, all characters age in step, and both family and feudal relationships stay intact throughout.
One complication for readers and translators of the Genji is that almost none of the characters in the original text is given an explicit name. The characters are instead referred to by their function or role (e.g. Minister of the Left), an honorific (e.g. His Excellency), or their relation to other characters (e.g. Heir Apparent), which changes as the novel progresses. This lack of names stems from Heian-era court manners that would have made it unacceptably familiar and blunt to freely mention a person's given name. Modern readers and translators have used various nicknames to keep track of the many characters.
There is debate over how much of Genji was actually written by Murasaki Shikibu. Debates over the novel's authorship have gone on for centuries, and are unlikely to ever be settled unless some major archival discovery is made.
It is generally accepted that the tale was finished in its present form by 1021, when the author of the Sarashina Nikki wrote a diary entry about her joy at acquiring a complete copy of the tale. She writes that there are over 50 chapters and mentions a character introduced at the end of the work, so if other authors besides Murasaki did work on the tale, the work was finished very near to the time of her writing. Murasaki's own diary includes a reference to the tale, and indeed the application to herself of the name 'Murasaki' in an allusion to the main female character. That entry confirms that some if not all of the diary was available in 1008 when internal evidence convincingly suggests that the entry was written.[4]
Murasaki is said to have written the character of Genji based on the Minister on the Left at the time she was at court. Other translators, such as Tyler, believe the character Murasaki no Ue, whom Genji marries, is based on Murasaki Shikibu herself.
Yosano Akiko, the first author to make a modern Japanese translation of Genji, believed that Murasaki had written only chapters 1 to 33, and that chapters 35 to 54 were written by her daughter, Daini no Sanmi.[5] Other scholars have also doubted the authorship of chapters 42 to 54 (particularly 44, which contains rare examples of continuity mistakes).[5] According to Royall Tyler's introduction to his English translation of the work, recent[when?] computer analysis has turned up "statistically significant" discrepancies of style between chapters 45–54 and the rest, and also among the early chapters.[5]
Genji's mother dies when he is three years old, and the Emperor cannot forget her. The Emperor Kiritsubo then hears of a woman (Lady Fujitsubo), formerly a princess of the preceding emperor, who resembles his deceased concubine, and later she becomes one of his wives. Genji loves her first as a stepmother, but later as a woman, and they fall in love with each other. Genji is frustrated by his forbidden love for the Lady Fujitsubo and is on bad terms with his own wife (Aoi no Ue, the Lady Aoi). He engages in a series of love affairs with many other women. These are however unfulfilling, as in most cases his advances are rebuffed, or his lover dies suddenly, or he becomes bored.
Genji visits Kitayama, a rural hilly area north of Kyoto, where he finds a beautiful ten-year-old girl. He is fascinated by this little girl (Murasaki), and discovers that she is a niece of the Lady Fujitsubo. Finally he kidnaps her, brings her to his own palace and educates her to be like the Lady Fujitsubo, who is his womanly ideal. During this time Genji also meets Lady Fujitsubo secretly, and she bears his son, Reizei. Everyone except the two lovers believes the father of the child is the Emperor Kiritsubo. Later the boy becomes the Crown Prince and Lady Fujitsubo becomes the Empress, but Genji and Lady Fujitsubo swear to keep the child's true parentage secret.
Genji and his wife, Lady Aoi, reconcile. She gives birth to a son but dies soon after. Genji is sorrowful but finds consolation in Murasaki, whom he marries. Genji's father, the Emperor Kiritsubo, dies. He is succeeded by his son Suzaku, whose mother (Kokiden), together with Kiritsubo's political enemies, take power in the court. Then another of Genji's secret love affairs is exposed: Genji and a concubine of the Emperor Suzaku are discovered while meeting in secret. The Emperor Suzaku confides his personal amusement at Genji's exploits with the woman (Oborozukiyo), but is duty-bound to punish Genji even though he is his half-brother. He exiles Genji to the town of Suma in rural Harima Province (now part of Kobe in Hyōgo Prefecture). There, a prosperous man known as the Akashi Novice (because he is from Akashi in Settsu Province) entertains Genji, and Genji has an affair with Akashi's daughter. She gives birth to Genji's only daughter, who will later become the Empress.
In the capital the Emperor Suzaku is troubled by dreams of his late father, Kiritsubo, and something begins to affect his eyes. Meanwhile, his mother, Kokiden, grows ill, which weakens her influence over the throne, and leads to the Emperor ordering Genji to be pardoned. Genji returns to Kyoto. His son by Lady Fujitsubo, Reizei, becomes the emperor. The new Emperor Reizei knows Genji is his real father, and raises Genji's rank to the highest possible.
However, when Genji turns 40 years old, his life begins to decline. His political status does not change, but his love and emotional life begin to incrementally diminish as middle age takes hold. He marries another wife, the Third Princess (known as Onna san no miya in the Seidensticker version, or Nyōsan in Waley's). Genji's nephew, Kashiwagi, later forces himself on the Third Princess, and she bears Kaoru (who, in a similar situation to that of Reizei, is legally known as the son of Genji). Genji's new marriage changes his relationship with Murasaki, who had expressed her wish of becoming a nun (bikuni) though the wish was rejected by Genji.
Genji's beloved Murasaki dies. In the following chapter, Maboroshi ("Illusion"), Genji contemplates how fleeting life is. Immediately after the chapter titled Maboroshi, there is a chapter titled Kumogakure ("Vanished into the Clouds"), which is left blank, but implies the death of Genji.
Chapter 45–54 are known as the "Uji Chapters". These chapters follow Kaoru and his best friend, Niou. Niou is an imperial prince, the son of Genji's daughter, the current Empress now that Reizei has abdicated the throne, while Kaoru is known to the world as Genji's son but is in fact fathered by Genji's nephew. The chapters involve Kaoru and Niou's rivalry over several daughters of an imperial prince who lives in Uji, a place some distance away from the capital. The tale ends abruptly, with Kaoru wondering if Niou is hiding Kaoru's former lover away from him. Kaoru has sometimes been called the first anti-hero in literature.[6]
The tale has an abrupt ending. Opinions vary on whether this was intended by the author. Arthur Waley, who made the first English translation of the whole of The Tale of Genji, believed that the work as we have it was finished. Ivan Morris, however, author of The World of the Shining Prince, believed that it was not complete and that later chapters were missing.  Edward Seidensticker, who made the second translation of the Genji, believed that Murasaki Shikibu had not had a planned story structure with an ending as such but would simply have continued writing as long as she could.
Because it was written to entertain the Japanese court of the 11th century, the work presents many difficulties to modern readers. First and foremost, Murasaki's language, Heian-period court Japanese, was highly inflected and had very complex grammar. Another problem is that naming people was considered rude in Heian court society, so none of the characters are named within the work. Instead, the narrator refers to men often by their rank or their station in life, and to women often by the color of their clothing, or by the words used at a meeting, or by the rank of a prominent male relative. This results in different appellations for the same character, depending on the chapter.
Another aspect of the language is the importance of using poetry in conversations.[7] Modifying or rephrasing a classic poem according to the current situation was expected behavior in Heian court life, and often served to communicate thinly veiled allusions. The poems in the Genji are often in the classic Japanese tanka form. Many of the poems were well known to the intended audience, so usually only the first few lines are given, and the reader is supposed to complete the thought themselves, leaving the rest – which the reader would be expected to know – unspoken.
As with most Heian literature, Genji was probably written mostly (or perhaps entirely) in kana (Japanese phonetic script) and not in kanji, because it was written by a woman for a female audience. Writing in kanji was at the time a masculine pursuit. Women were generally discreet when using kanji, confining themselves mostly to native Japanese words (yamato kotoba).
Outside of vocabulary related to politics and Buddhism, Genji contains remarkably few Chinese loan words (kango). This has the effect of giving the story a very even smooth flow. However it also introduces confusion: there are a number of homophones (words with the same pronunciation but different meanings); and for modern readers context is not always sufficient to determine which meaning was intended.
The novel is traditionally divided into three parts, the first two dealing with the life of Genji and the last with the early years of two of Genji's prominent descendants, Niou and Kaoru. There are also several short transitional chapters which are usually grouped separately and whose authorships are sometimes questioned.
The 54th and last chapter, "The Floating Bridge of Dreams", is sometimes argued by modern scholars to be a separate part from the Uji part. It seems to continue the story from the previous chapters but has an unusually abstract chapter title.  It is the only chapter whose title has no clear reference within the text, although this may be due to the chapter being unfinished. This question is made more difficult by the fact that we do not know exactly when the chapters acquired their titles.
The English translations here are taken from the Arthur Waley, the Edward Seidensticker, the Royall Tyler, and the Dennis Washburn translations. It is not known for certain when the chapters acquired their titles. Early mentions of the Tale refer to chapter numbers, or contain alternate titles for some of the chapters. This may suggest that the titles were added later. The titles are largely derived from poetry that is quoted within the text, or allusions to various characters.
The additional chapter between 41 and 42 in some manuscripts is called Kumogakure (雲隠) which means "Vanished into the Clouds"—the chapter is a title only, and is probably intended to evoke Genji's death. Some scholars have posited the earlier existence of a chapter between 1 and 2 which would have introduced some characters that seem to appear very abruptly in the book as it stands.
The Waley translation completely omits the 38th chapter.
Later authors have composed additional chapters, most often either between 41 and 42, or after the end.
The original manuscript written by Murasaki Shikibu no longer exists. Numerous copies, totaling around 300 according to Ikeda Kikan, exist with differences between each. It is thought that Shikibu often went back and edited early manuscripts introducing discrepancies with earlier copies.[8]
The various manuscripts are classified into three categories:[9][10]
In the 13th century, two major attempts by Minamoto no Chikayuki and Fujiwara Teika were made to edit and revise the differing manuscripts. The Chikayuki manuscript is known as the Kawachibon; edits were many beginning in 1236 and completing in 1255. The Teika manuscript is known as the Aobyōshibon; its edits are more conservative and thought to better represent the original. These two manuscripts were used as the basis for many future copies.
The Beppon category represents all other manuscripts not belonging to either Kawachibon or Aobyōshibon. This includes older but incomplete manuscripts, mixed manuscripts derived from both Kawachibon and Aobyōshibon, and commentaries.
On March 10, 2008, it was announced that a late Kamakura period (1192–1333) manuscript had been found in Kyoto,[11][12] containing the sixth chapter, Suetsumuhana; the manuscript was 65 pages in length. Most remaining manuscripts are based on copies of the Teika manuscript which introduced revisions in the original; this manuscript, however, belongs to a different lineage and was not influenced by Teika. Professor Yamamoto Tokurō, who examined the manuscript, said, "This is a precious discovery as Kamakura manuscripts are so rare." Professor Katō Yōsuke said, "This is an important discovery as it asserts that non-Teika manuscripts were being read during the Kamakura period."
On October 29, 2008, Konan Women's University announced that a mid-Kamakura period manuscript had been found,[13][14][15]
containing the 32nd chapter, Umegae. The manuscript was recognized as the oldest extant copy of this chapter, dating to between 1240 and 1280. The manuscript, considered to be of the Beppon category, is 74 pages in length and differs from Aobyōshi manuscripts in at least four places, raising the "possibility that the contents may be closer to the undiscovered Murasaki Shikibu original manuscript".[13]
On October 9, 2019, it was announced that an original copy of Teika's Aobyōshibon had been found in Tokyo at the home of the current head of the Okochi-Matsudaira clan, who ran the Yoshida Domain. The manuscript is the 5th chapter, Wakamurasaki (若紫), and is the oldest version of the chapter. Blue ink common in Teika's manuscript and handwriting analysis confirmed that the manuscript was written by Teika, making it among the 5 original versions of the Aobyōshibon known to exist.[16]
Numerous illustrations of scenes from Genji have been produced, most notably a 12th-century scroll, the Genji Monogatari Emaki, containing illustrated scenes from Genji together with handwritten sōgana text. This scroll is the earliest extant example of a Japanese "picture scroll": collected illustrations and calligraphy of a single work.  The original scroll is believed to have comprised 10–20 rolls and covered all 54 chapters. The extant pieces include only 19 illustrations and 65 pages of text, plus nine pages of fragments. This is estimated at 15% of the envisioned original.
The Tokugawa Art Museum in Nagoya has three of the scrolls handed down in the Owari branch of the Tokugawa clan and one scroll held by the Hachisuka family is now in the Gotoh Museum in Tokyo. The scrolls are designated National Treasures of Japan. The scrolls are so fragile that they normally are not shown in public. The original scrolls in the Tokugawa Museum were shown from November 21 to November 29 in 2009. Since 2001, they have been displayed in the Tokugawa Museum annually for around one week in November. An oversize English photoreproduction and translation was published in limited edition in 1971 by Kodansha International.[17]
Other notable illustrated scrolls of Genji are by Tosa Mitsuoki, who lived from 1617 to 1691. His paintings are closely based on Heian style from the existing scrolls from the 12th century and are fully complete. The tale was also a popular theme in ukiyo-e prints from the Edo period.
The Tale of Genji was written in an archaic court language, and a century after its completion it was difficult to read without specialized study. Annotated and illustrated versions existed as early as the 12th century.[18] It was not until the early 20th century that Genji was translated into modern Japanese by the poet Akiko Yosano.[19]  Translations into modern Japanese have made it easier to read though changed some meaning, and has given names to the characters, usually the traditional names used by academics. This gives rise to anachronisms; for instance, Genji's first wife is named Aoi because she is known as the lady of the Aoi chapter, in which she dies.
Other known translations were done by the novelists Jun'ichirō Tanizaki and Fumiko Enchi.
Because of the cultural differences, reading an annotated version of the Genji is quite common, even among Japanese readers. There are several annotated versions by novelists, including Seiko Tanabe, Jakucho Setouchi and Osamu Hashimoto.[20] Many works, including a manga series and different television dramas, are derived from The Tale of Genji. There have been at least five manga adaptations of Genji.[21] A manga version was created by Waki Yamato, Asakiyumemishi (The Tale of Genji in English), and a current version by Sugimura Yoshimitsu[22][better source needed] is in progress. Another manga, Genji Monogatari, by Miyako Maki, won the Shogakukan Manga Award in 1989.[23]
The first partial translation of Genji into English was by Suematsu Kenchō, published in 1882. Arthur Waley published a six-volume translation of all but one chapter, with the first volume published in 1925 and the last in 1933.[24] In 1976, Edward Seidensticker published the first complete translation into English, made using a self-consciously "stricter" approach with regards to content if not form.[25] The English translation published in 2001 by Royall Tyler aims at fidelity in content and form to the original text.[5][26]
The major translations into English are each slightly different, mirroring the personal choices of the translator and the period in which the translation was made. Each version has its merits, its detractors and its advocates, and each is distinguished by the name of the translator. For example, the version translated by Arthur Waley would typically be referred to as "the Waley Genji".
The Tale of Genji is an important work of Japanese literature, and modern authors have cited it as inspiration, such as Jorge Luis Borges who said of it, "The Tale of Genji, as translated by Arthur Waley, is written with an almost miraculous naturalness, and what interests us is not the exoticism—the horrible word—but rather the human passions of the novel. Such interest is just: Murasaki's work is what one would quite precisely call a psychological novel ... I dare to recommend this book to those who read me. The English translation that has inspired this brief insufficient note is called The Tale of Genji."[36] It is noted for its internal consistency, psychological depiction, and characterization. The novelist Yasunari Kawabata said in his Nobel Prize acceptance speech: "The Tale of Genji in particular is the highest pinnacle of Japanese literature. Even down to our day there has not been a piece of fiction to compare with it."
The Genji is also often referred to as "the first novel",[37] though there is considerable debate over this; other texts that predate Genji, such as the 7th-century Sanskrit Kādambari, or the Greek and Roman novels from classical antiquity, such as Daphnis and Chloe and the Satyricon, are considered to be novels, and there is debate around whether Genji can even be considered a "novel". Some[who?] consider the psychological insight, complexity and unity of the work to qualify it for "novel" status while simultaneously disqualifying earlier works of prose fiction.[38] Others[who?] see these arguments as subjective and unconvincing.
Related claims, perhaps in an attempt to sidestep these debates, are that Genji is the "first psychological novel" or "historical novel",[39] "the first novel still considered to be a classic" or other more qualified terms. However, critics have almost consistently described The Tale of Genji as the oldest, first, and/or greatest novel in Japanese literature,[40][41] though enthusiastic proponents may have later neglected the qualifying category of 'in Japanese literature', leading to the debates over the book's place in world literature. Even in Japan, the Tale of Genji is not universally embraced; the lesser-known Ochikubo Monogatari has been proposed as the "world's first full-length novel", even though its author is unknown.[42]  Despite these debates, The Tale of Genji enjoys solid respect among the works of literature, and its influence on Japanese literature has been compared to that of Philip Sidney's Arcadia on English literature.[40]
The novel and other works by Lady Murasaki are staple reading material in the curricula of Japanese schools. The Bank of Japan issued the 2000 yen banknote in her honor, featuring a scene from the novel based on the 12th-century illustrated handscroll. Since a 1 November 1008 entry in The Diary of Lady Murasaki is the oldest date on which a reference to The Tale of Genji has appeared, November 1 was designated as the official day to celebrate Japanese classics. According to Act on Classics Day, the "classics" that are honored not only include literature, but encompass a wide range of arts such as music, art, traditional performing arts, entertainment, lifestyle art including tea ceremony and flower arrangement and other cultural products.[43]
The names of the chapters became a central element in a incense-based game called Genjikō, part of the larger practice of  Monkō popular among the nobility. In Genjikō, players must match the scents of a series of five incense samples without being told the names of said samples. Each possible combination was matched to a symbol, called a genji-mon, that represented a chapter from the story.[44]




Mercury is the first planet from the Sun and the smallest planet in the Solar System. It is a terrestrial planet with a heavily cratered surface due to the planet having no geological activity and an extremely tenuous atmosphere (called an exosphere). Despite being the smallest planet in the Solar System with a mean diameter of 4,880 km (3,030 mi), 38% of that of Earth's, Mercury is dense enough to have roughly the same surface gravity as Mars. Mercury has a dynamic magnetic field with a strength about 1% of that of Earth's and has no natural satellites. 
According to current theories, Mercury may have a solid silicate crust and mantle overlying a solid outer core, a deeper liquid core layer, and a solid inner core. Having almost no atmosphere to retain heat, Mercury has surface temperatures that change wildly during the day, ranging from 100 K (−173 °C; −280 °F) at night to 700 K (427 °C; 800 °F) during sunlight across the equator regions.[19] At Mercury's poles though, there are large reservoirs of water ices that are never exposed to direct sunlight, which has an estimated mass of about 0.025–0.25% the Antarctic ice sheet.[20] There are many competing hypothesis about Mercury's origins and development, some of which incorporate collision with planetesimal and rock vaporization. 
Because Mercury is very close to the Sun, the intensity of sunlight on its surface is between 4.59 and 10.61 times the solar constant (amount of the Sun's energy received at 1 astronomical unit, which is roughly the distance between Earth and the Sun). Mercury orbits the Sun in a 3:2 spin–orbit resonance, meaning that relative to the background stars, it rotates on its axis exactly three times for every two revolutions it makes around the Sun.[a][21] Counterintuitively, due to Mercury's slow rotation, an observer on the planet would see only one Mercurian solar day (176 Earth days) every two Mercurian solar years (88 Earth days each).[4] Mercury's axis has the smallest tilt of any of the Solar System's planets (about 1⁄30 of a degree), and its orbital eccentricity is the largest of all known planets in the Solar System.[b] 
Like Venus, Mercury orbits the Sun within Earth's orbit, making it appear in Earth's sky only as a "morning star" or "evening star" that's relatively close to the Sun. In English, it is named after the Roman god Mercurius (Mercury), god of commerce, communication and the messenger of gods. Mercury is the most difficult planet to reach from Earth because it requires the greatest change in spacecraft's velocity. Only two spacecraft have visited Mercury as of 2023: Mariner 10 flew by in 1974 and 1975, and MESSENGER launched in 2004 and orbited Mercury over 4,000 times in four years. The BepiColombo spacecraft is planned to arrive at Mercury in 2025.
The ancients knew Mercury by different names depending on whether it was an evening star or a morning star. By about 350 BC, the ancient Greeks had realized the two stars were one.[22] They knew the planet as Στίλβων Stilbōn, meaning "twinkling", and Ἑρμής Hermēs, for its fleeting motion,[23] a name that is retained in modern Greek (Ερμής Ermis).[24] The Romans named the planet after the swift-footed Roman messenger god, Mercury (Latin Mercurius), which they equated with the Greek Hermes, because it moves across the sky faster than any other planet.[22][25] The astronomical symbol for Mercury is a stylized version of Hermes' caduceus; a Christian cross was added in the 16th century: .[26][27]
Mercury is one of four terrestrial planets in the Solar System, and is a rocky body like Earth. It is the smallest planet in the Solar System, with an equatorial radius of 2,439.7 kilometres (1,516.0 mi).[4] Mercury is also smaller—albeit more massive—than the largest natural satellites in the Solar System, Ganymede and Titan. Mercury consists of approximately 70% metallic and 30% silicate material.[28]
Mercury appears to have a solid silicate crust and mantle overlying a solid, iron sulfide outer core layer, a deeper liquid core layer, and a solid inner core.[29][30] The planet's density is the second highest in the Solar System at 5.427 g/cm3, only slightly less than Earth's density of 5.515 g/cm3.[4] If the effect of gravitational compression were to be factored out from both planets, the materials of which Mercury is made would be denser than those of Earth, with an uncompressed density of 5.3 g/cm3 versus Earth's 4.4 g/cm3.[31] Mercury's density can be used to infer details of its inner structure. Although Earth's high density results appreciably from gravitational compression, particularly at the core, Mercury is much smaller and its inner regions are not as compressed. Therefore, for it to have such a high density, its core must be large and rich in iron.[32]
The radius of Mercury's core is estimated to be 2,020 ± 30 km (1,255 ± 19 mi), based on interior models constrained to be consistent with the value of the moment of inertia factor given in the infobox.[9][33] Hence, Mercury's core occupies about 57% of its volume; for Earth this proportion is 17%. Research published in 2007 suggests that Mercury has a molten core.[34][35] The mantle-crust layer is in total 420 km (260 mi) thick.[36] Based on data from the Mariner 10 and MESSENGER missions, in addition to Earth-based observation, Mercury's crust is estimated to be 35 km (22 mi) thick.[37][38] However, this model may be an overestimate and the crust could be 26 ± 11 km (16.2 ± 6.8 mi) thick based on an Airy isostacy model.[39] One distinctive feature of Mercury's surface is the presence of numerous narrow ridges, extending up to several hundred kilometers in length. It is thought that these were formed as Mercury's core and mantle cooled and contracted at a time when the crust had already solidified.[40][41][42]
Mercury's core has a higher iron content than that of any other major planet in the Solar System, and several theories have been proposed to explain this. The most widely accepted theory is that Mercury originally had a metal–silicate ratio similar to common chondrite meteorites, thought to be typical of the Solar System's rocky matter, and a mass approximately 2.25 times its current mass.[43] Early in the Solar System's history, Mercury may have been struck by a planetesimal of approximately 1/6 Mercury's mass and several thousand kilometers across.[43] The impact would have stripped away much of the original crust and mantle, leaving the core behind as a relatively major component.[43] A similar process, known as the giant impact hypothesis, has been proposed to explain the formation of the Moon.[43]
Alternatively, Mercury may have formed from the solar nebula before the Sun's energy output had stabilized. It would initially have had twice its present mass, but as the protosun contracted, temperatures near Mercury could have been between 2,500 and 3,500 K and possibly even as high as 10,000 K.[44] Much of Mercury's surface rock could have been vaporized at such temperatures, forming an atmosphere of "rock vapor" that could have been carried away by the solar wind.[44] A third hypothesis proposes that the solar nebula caused drag on the particles from which Mercury was accreting, which meant that lighter particles were lost from the accreting material and not gathered by Mercury.[45]
Each hypothesis predicts a different surface composition, and two space missions have been tasked with making observations of this composition. The first MESSENGER, which ended in 2015, found higher-than-expected potassium and sulfur levels on the surface, suggesting that the giant impact hypothesis and vaporization of the crust and mantle did not occur because said potassium and sulfur would have been driven off by the extreme heat of these events.[46] BepiColombo, which will arrive at Mercury in 2025, will make observations to test these hypotheses.[47] The findings so far would seem to favor the third hypothesis; however, further analysis of the data is needed.[48]
Mercury's surface is similar in appearance to that of the Moon, showing extensive mare-like plains and heavy cratering, indicating that it has been geologically inactive for billions of years. It is more heterogeneous than the surface of Mars or the Moon, both of which contain significant stretches of similar geology, such as maria and plateaus.[49] Albedo features are areas of markedly different reflectivity, which include impact craters, the resulting ejecta, and ray systems. Larger albedo features correspond to higher reflectivity plains.[50] Mercury has dorsa (also called "wrinkle-ridges"), Moon-like highlands, montes (mountains), planitiae (plains), rupes (escarpments), and valles (valleys).[51][52]
The planet's mantle is chemically heterogeneous, suggesting the planet went through a magma ocean phase early in its history. Crystallization of minerals and convective overturn resulted in layered, chemically heterogeneous crust with large-scale variations in chemical composition observed on the surface. The crust is low in iron but high in sulfur, resulting from the stronger early chemically reducing conditions than is found in the other terrestrial planets. The surface is dominated by iron-poor pyroxene and olivine, as represented by enstatite and forsterite, respectively, along with sodium-rich plagioclase and minerals of mixed magnesium, calcium, and iron-sulfide. The less reflective regions of the crust are high in carbon, most likely in the form of graphite.[53]
Names for features on Mercury come from a variety of sources. Names coming from people are limited to the deceased. Craters are named for artists, musicians, painters, and authors who have made outstanding or fundamental contributions to their field. Ridges, or dorsa, are named for scientists who have contributed to the study of Mercury. Depressions or fossae are named for works of architecture. Montes are named for the word "hot" in a variety of languages. Plains or planitiae are named for Mercury in various languages. Escarpments or rupēs are named for ships of scientific expeditions. Valleys or valles are named for abandoned cities, towns, or settlements of antiquity.[54]
Mercury was heavily bombarded by comets and asteroids during and shortly following its formation 4.6 billion years ago, as well as during a possibly separate subsequent episode called the Late Heavy Bombardment that ended 3.8 billion years ago.[55] Mercury received impacts over its entire surface during this period of intense crater formation,[52] facilitated by the lack of any atmosphere to slow impactors down.[56] During this time Mercury was volcanically active; basins were filled by magma, producing smooth plains similar to the maria found on the Moon.[57][58] One of the most unusual craters is Apollodorus, or "the Spider", which hosts a serious of radiating troughs extending outwards from its impact site.[59]
Craters on Mercury range in diameter from small bowl-shaped cavities to multi-ringed impact basins hundreds of kilometers across. They appear in all states of degradation, from relatively fresh rayed craters to highly degraded crater remnants. Mercurian craters differ subtly from lunar craters in that the area blanketed by their ejecta is much smaller, a consequence of Mercury's stronger surface gravity.[60] According to International Astronomical Union rules, each new crater must be named after an artist who was famous for more than fifty years, and dead for more than three years, before the date the crater is named.[61]
The largest known crater is Caloris Planitia, or Caloris Basin, with a diameter of 1,550 km.[62] The impact that created the Caloris Basin was so powerful that it caused lava eruptions and left a concentric mountainous ring ~2 km tall surrounding the impact crater. The floor of the Caloris Basin is filled by a geologically distinct flat plain, broken up by ridges and fractures in a roughly polygonal pattern. It is not clear whether they are volcanic lava flows induced by the impact or a large sheet of impact melt.[60]
At the antipode of the Caloris Basin is a large region of unusual, hilly terrain known as the "Weird Terrain". One hypothesis for its origin is that shock waves generated during the Caloris impact traveled around Mercury, converging at the basin's antipode (180 degrees away). The resulting high stresses fractured the surface.[63] Alternatively, it has been suggested that this terrain formed as a result of the convergence of ejecta at this basin's antipode.[64]
Overall, 46 impact basins have been identified.[65] A notable basin is the 400 km wide, multi-ring Tolstoj Basin that has an ejecta blanket extending up to 500 km from its rim and a floor that has been filled by smooth plains materials. Beethoven Basin has a similar-sized ejecta blanket and a 625 km diameter rim.[60] Like the Moon, the surface of Mercury has likely incurred the effects of space weathering processes, including solar wind and micrometeorite impacts.[66]
There are two geologically distinct plains regions on Mercury.[60][67] Gently rolling, hilly plains in the regions between craters are Mercury's oldest visible surfaces,[60] predating the heavily cratered terrain. These inter-crater plains appear to have obliterated many earlier craters, and show a general paucity of smaller craters below about 30 km in diameter.[67]
Smooth plains are widespread flat areas that fill depressions of various sizes and bear a strong resemblance to the lunar maria. Unlike lunar maria, the smooth plains of Mercury have the same albedo as the older inter-crater plains. Despite a lack of unequivocally volcanic characteristics, the localisation and rounded, lobate shape of these plains strongly support volcanic origins.[60] All the smooth plains of Mercury formed significantly later than the Caloris basin, as evidenced by appreciably smaller crater densities than on the Caloris ejecta blanket.[60]
One unusual feature of Mercury's surface is the numerous compression folds, or rupes, that crisscross the plains. These also exist on the moon, but are much more prominent on Mercury.[68] As Mercury's interior cooled, it contracted and its surface began to deform, creating wrinkle ridges and lobate scarps associated with thrust faults. The scarps can reach lengths of 1000 km and heights of 3 km.[69] These compressional features can be seen on top of other features, such as craters and smooth plains, indicating they are more recent.[70] Mapping of the features has suggested a total shrinkage of Mercury's radius in the range of ~1 to 7 km.[71] Most activity along the major thrust systems probably ended about 3.6–3.7 billion years ago.[72] Small-scale thrust fault scarps have been found, tens of meters in height and with lengths in the range of a few km, that appear to be less than 50 million years old, indicating that compression of the interior and consequent surface geological activity continue to the present.[69][71]
There is evidence for pyroclastic flows on Mercury from low-profile shield volcanoes.[73][74][75] 51 pyroclastic deposits have been identified,[76] where 90% of them are found within impact craters.[76] A study of the degradation state of the impact craters that host pyroclastic deposits suggests that pyroclastic activity occurred on Mercury over a prolonged interval.[76]
A "rimless depression" inside the southwest rim of the Caloris Basin consists of at least nine overlapping volcanic vents, each individually up to 8 km in diameter. It is thus a "compound volcano".[77] The vent floors are at least 1 km below their brinks and they bear a closer resemblance to volcanic craters sculpted by explosive eruptions or modified by collapse into void spaces created by magma withdrawal back down into a conduit.[77] Scientists could not quantify the age of the volcanic complex system but reported that it could be on the order of a billion years.[77]
The surface temperature of Mercury ranges from 100 to 700 K (−173 to 427 °C; −280 to 800 °F)[19] at the most extreme places: 0°N, 0°W, or 180°W. It never rises above 180 K at the poles,[14] due to the absence of an atmosphere and a steep temperature gradient between the equator and the poles. The subsolar point reaches about 700 K during perihelion (0°W or 180°W), but only 550 K at aphelion (90° or 270°W).[79]
On the dark side of the planet, temperatures average 110 K.[14][80]
The intensity of sunlight on Mercury's surface ranges between 4.59 and 10.61 times the solar constant (1,370 W·m−2).[81]
Although the daylight temperature at the surface of Mercury is generally extremely high, observations strongly suggest that ice (frozen water) exists on Mercury. The floors of deep craters at the poles are never exposed to direct sunlight, and temperatures there remain below 102 K, far lower than the global average.[82] This creates a cold trap where ice can accumulate. Water ice strongly reflects radar, and observations by the 70-meter Goldstone Solar System Radar and the VLA in the early 1990s revealed that there are patches of high radar reflection near the poles.[83] Although ice was not the only possible cause of these reflective regions, astronomers think it was the most likely.[84]
The icy regions are estimated to contain about 1014–1015 kg of ice,[20] and may be covered by a layer of regolith that inhibits sublimation.[85] By comparison, the Antarctic ice sheet on Earth has a mass of about 4×1018 kg, and Mars's south polar cap contains about 1016 kg of water.[20] The origin of the ice on Mercury is not yet known, but the two most likely sources are from outgassing of water from the planet's interior or deposition by impacts of comets.[20]
Mercury is too small and hot for its gravity to retain any significant atmosphere over long periods of time; it does have a tenuous surface-bounded exosphere[86] containing hydrogen, helium, oxygen, sodium, calcium, potassium and others[17][18] at a surface pressure of less than approximately 0.5 nPa (0.005 picobars).[4] This exosphere is not stable—atoms are continuously lost and replenished from a variety of sources. Hydrogen atoms and helium atoms probably come from the solar wind, diffusing into Mercury's magnetosphere before later escaping back into space. Radioactive decay of elements within Mercury's crust is another source of helium, as well as sodium and potassium. MESSENGER found high proportions of calcium, helium, hydroxide, magnesium, oxygen, potassium, silicon and sodium. Water vapor is present, released by a combination of processes such as: comets striking its surface, sputtering creating water out of hydrogen from the solar wind and oxygen from rock, and sublimation from reservoirs of water ice in the permanently shadowed polar craters. The detection of high amounts of water-related ions like O+, OH−, and H3O+ was a surprise.[87][88] Because of the quantities of these ions that were detected in Mercury's space environment, scientists surmise that these molecules were blasted from the surface or exosphere by the solar wind.[89][90]
Sodium, potassium and calcium were discovered in the atmosphere during the 1980–1990s, and are thought to result primarily from the vaporization of surface rock struck by micrometeorite impacts[91] including presently from Comet Encke.[92] In 2008, magnesium was discovered by MESSENGER.[93] Studies indicate that, at times, sodium emissions are localized at points that correspond to the planet's magnetic poles. This would indicate an interaction between the magnetosphere and the planet's surface.[94]
On November 29, 2012, NASA confirmed that images from MESSENGER had detected that craters at the north pole contained water ice. MESSENGER's principal investigator Sean Solomon is quoted in The New York Times estimating the volume of the ice to be large enough to "encase Washington, D.C., in a frozen block two and a half miles deep".[78]
According to NASA, Mercury is not a suitable planet for Earth-like life. It has a surface boundary exosphere instead of a layered atmosphere, extreme temperatures, and high solar radiation. It is unlikely that any living beings can withstand those conditions.[95] Some parts of the subsurface of Mercury may have been habitable, and perhaps life forms, albeit likely primitive microorganisms, may have existed on the planet.[96][97][98]
Despite its small size and slow 59-day-long rotation, Mercury has a significant, and apparently global, magnetic field. According to measurements taken by Mariner 10, it is about 1.1% the strength of Earth's. The magnetic-field strength at Mercury's equator is about 300 nT.[99][100] Like that of Earth, Mercury's magnetic field is dipolar.[94] Unlike Earth's, Mercury's poles are nearly aligned with the planet's spin axis.[101] Measurements from both the Mariner 10 and MESSENGER space probes have indicated that the strength and shape of the magnetic field are stable.[101]
It is likely that this magnetic field is generated by a dynamo effect, in a manner similar to the magnetic field of Earth.[102][103] This dynamo effect would result from the circulation of the planet's iron-rich liquid core. Particularly strong tidal heating effects caused by the planet's high orbital eccentricity would serve to keep part of the core in the liquid state necessary for this dynamo effect.[104][105]
Mercury's magnetic field is strong enough to deflect the solar wind around the planet, creating a magnetosphere. The planet's magnetosphere, though small enough to fit within Earth,[94] is strong enough to trap solar wind plasma. This contributes to the space weathering of the planet's surface.[101] Observations taken by the Mariner 10 spacecraft detected this low energy plasma in the magnetosphere of the planet's nightside. Bursts of energetic particles in the planet's magnetotail indicate a dynamic quality to the planet's magnetosphere.[94]
During its second flyby of the planet on October 6, 2008, MESSENGER discovered that Mercury's magnetic field can be extremely "leaky". The spacecraft encountered magnetic "tornadoes" – twisted bundles of magnetic fields connecting the planetary magnetic field to interplanetary space – that were up to 800 km wide or a third of the radius of the planet. These twisted magnetic flux tubes, technically known as flux transfer events, form open windows in the planet's magnetic shield through which the solar wind may enter and directly impact Mercury's surface via magnetic reconnection[106] This also occurs in Earth's magnetic field. The MESSENGER observations showed the reconnection rate is ten times higher at Mercury, but its proximity to the Sun only accounts for about a third of the reconnection rate observed by MESSENGER.[106]
Mercury has the most eccentric orbit of all the planets in the Solar System; its eccentricity is 0.21 with its distance from the Sun ranging from 46,000,000 to 70,000,000 km (29,000,000 to 43,000,000 mi). It takes 87.969 Earth days to complete an orbit. The diagram illustrates the effects of the eccentricity, showing Mercury's orbit overlaid with a circular orbit having the same semi-major axis. Mercury's higher velocity when it is near perihelion is clear from the greater distance it covers in each 5-day interval. In the diagram, the varying distance of Mercury to the Sun is represented by the size of the planet, which is inversely proportional to Mercury's distance from the Sun. This varying distance to the Sun leads to Mercury's surface being flexed by tidal bulges raised by the Sun that are about 17 times stronger than the Moon's on Earth.[107] Combined with a 3:2 spin–orbit resonance of the planet's rotation around its axis, it also results in complex variations of the surface temperature.[28]
The resonance makes a single solar day (the length between two meridian transits of the Sun) on Mercury last exactly two Mercury years, or about 176 Earth days.[108]
Mercury's orbit is inclined by 7 degrees to the plane of Earth's orbit (the ecliptic), the largest of all eight known solar planets.[109] As a result, transits of Mercury across the face of the Sun can only occur when the planet is crossing the plane of the ecliptic at the time it lies between Earth and the Sun, which is in May or November. This occurs about every seven years on average.[110]
Mercury's axial tilt is almost zero,[111] with the best measured value as low as 0.027 degrees.[112] This is significantly smaller than that of Jupiter, which has the second smallest axial tilt of all planets at 3.1 degrees. This means that to an observer at Mercury's poles, the center of the Sun never rises more than 2.1 arcminutes above the horizon.[112]
At certain points on Mercury's surface, an observer would be able to see the Sun peek up a little more than two-thirds of the way over the horizon, then reverse and set before rising again, all within the same Mercurian day.[c] This is because approximately four Earth days before perihelion, Mercury's angular orbital velocity equals its angular rotational velocity so that the Sun's apparent motion ceases; closer to perihelion, Mercury's angular orbital velocity then exceeds the angular rotational velocity. Thus, to a hypothetical observer on Mercury, the Sun appears to move in a retrograde direction. Four Earth days after perihelion, the Sun's normal apparent motion resumes.[28] A similar effect would have occurred if Mercury had been in synchronous rotation: the alternating gain and loss of rotation over revolution would have caused a libration of 23.65° in longitude.[113]
For the same reason, there are two points on Mercury's equator, 180 degrees apart in longitude, at either of which, around perihelion in alternate Mercurian years (once a Mercurian day), the Sun passes overhead, then reverses its apparent motion and passes overhead again, then reverses a second time and passes overhead a third time, taking a total of about 16 Earth-days for this entire process. In the other alternate Mercurian years, the same thing happens at the other of these two points. The amplitude of the retrograde motion is small, so the overall effect is that, for two or three weeks, the Sun is almost stationary overhead, and is at its most brilliant because Mercury is at perihelion, its closest to the Sun. This prolonged exposure to the Sun at its brightest makes these two points the hottest places on Mercury. Maximum temperature occurs when the Sun is at an angle of about 25 degrees past noon due to diurnal temperature lag, at 0.4 Mercury days and 0.8 Mercury years past sunrise.[114] Conversely, there are two other points on the equator, 90 degrees of longitude apart from the first ones, where the Sun passes overhead only when the planet is at aphelion in alternate years, when the apparent motion of the Sun in Mercury's sky is relatively rapid. These points, which are the ones on the equator where the apparent retrograde motion of the Sun happens when it is crossing the horizon as described in the preceding paragraph, receive much less solar heat than the first ones described above.[115]
Mercury attains inferior conjunction (nearest approach to Earth) every 116 Earth days on average,[4] but this interval can range from 105 days to 129 days due to the planet's eccentric orbit. Mercury can come as near as 82,200,000 km (0.549 astronomical units; 51.1 million miles) to Earth, and that is slowly declining: The next approach to within 82,100,000 km (51 million mi) is in 2679, and to within 82,000,000 km (51 million mi) in 4487, but it will not be closer to Earth than 80,000,000 km (50 million mi) until 28,622.[116] Its period of retrograde motion as seen from Earth can vary from 8 to 15 days on either side of inferior conjunction. This large range arises from the planet's high orbital eccentricity.[28] Essentially because Mercury is closest to the Sun, when taking an average over time, Mercury is most often the closest planet to the Earth,[117][118] and—in that measure—it is the closest planet to each of the other planets in the Solar System.[119][120][121][d]
The longitude convention for Mercury puts the zero of longitude at one of the two hottest points on the surface, as described above. However, when this area was first visited, by Mariner 10, this zero meridian was in darkness, so it was impossible to select a feature on the surface to define the exact position of the meridian. Therefore, a small crater further west was chosen, called Hun Kal, which provides the exact reference point for measuring longitude.[122][123] The center of Hun Kal defines the 20° west meridian. A 1970 International Astronomical Union resolution suggests that longitudes be measured positively in the westerly direction on Mercury.[124] The two hottest places on the equator are therefore at longitudes 0° W and 180° W, and the coolest points on the equator are at longitudes 90° W and 270° W. However, the MESSENGER project uses an east-positive convention.[125]
For many years it was thought that Mercury was synchronously tidally locked with the Sun, rotating once for each orbit and always keeping the same face directed towards the Sun, in the same way that the same side of the Moon always faces Earth. Radar observations in 1965 proved that the planet has a 3:2 spin-orbit resonance, rotating three times for every two revolutions around the Sun. The eccentricity of Mercury's orbit makes this resonance stable—at perihelion, when the solar tide is strongest, the Sun is nearly still in Mercury's sky.[126]
The rare 3:2 resonant tidal locking is stabilized by the variance of the tidal force along Mercury's eccentric orbit, acting on a permanent dipole component of Mercury's mass distribution.[127] In a circular orbit there is no such variance, so the only resonance stabilized in such an orbit is at 1:1 (e.g., Earth–Moon), when the tidal force, stretching a body along the "center-body" line, exerts a torque that aligns the body's axis of least inertia (the "longest" axis, and the axis of the aforementioned dipole) to point always at the center. However, with noticeable eccentricity, like that of Mercury's orbit, the tidal force has a maximum at perihelion and therefore stabilizes resonances, like 3:2, ensuring that the planet points its axis of least inertia roughly at the Sun when passing through perihelion.[127]
The original reason astronomers thought it was synchronously locked was that, whenever Mercury was best placed for observation, it was always nearly at the same point in its 3:2 resonance, hence showing the same face. This is because, coincidentally, Mercury's rotation period is almost exactly half of its synodic period with respect to Earth. Due to Mercury's 3:2 spin-orbit resonance, a solar day lasts about 176 Earth days.[28] A sidereal day (the period of rotation) lasts about 58.7 Earth days.[28]
Simulations indicate that the orbital eccentricity of Mercury varies chaotically from nearly zero (circular) to more than 0.45 over millions of years due to perturbations from the other planets.[28][128]
This was thought to explain Mercury's 3:2 spin-orbit resonance (rather than the more usual 1:1), because this state is more likely to arise during a period of high eccentricity.[129]
However, accurate modeling based on a realistic model of tidal response has demonstrated that Mercury was captured into the 3:2 spin-orbit state at a very early stage of its history, within 20 (more likely, 10) million years after its formation.[130]
Numerical simulations show that a future secular orbital resonant perihelion interaction with Jupiter may cause the eccentricity of Mercury's orbit to increase to the point where there is a 1% chance that the orbit will be destabilized in the next five billion years. If this happens, Mercury may fall into the Sun, collide with Venus, be ejected from the Solar System, or even disrupt the rest of the inner Solar System.[131][132]
In 1859, the French mathematician and astronomer Urbain Le Verrier reported that the slow precession of Mercury's orbit around the Sun could not be completely explained by Newtonian mechanics and perturbations by the known planets. He suggested, among possible explanations, that another planet (or perhaps instead a series of smaller "corpuscules") might exist in an orbit even closer to the Sun than that of Mercury, to account for this perturbation.[133] (Other explanations considered included a slight oblateness of the Sun.) The success of the search for Neptune based on its perturbations of the orbit of Uranus led astronomers to place faith in this possible explanation, and the hypothetical planet was named Vulcan, but no such planet was ever found.[134]
The observed perihelion precession of Mercury is 5,600 arcseconds (1.5556°) per century relative to Earth, or 574.10±0.65 arcseconds per century[135] relative to the inertial ICRF. Newtonian mechanics, taking into account all the effects from the other planets (included 0.0254 arcsecond per century due to the "flatteness" of the Sun), predicts a precession of 5,557 arcseconds (1.5436°) per century relative to Earth, or 531.63±0.69 arcseconds per century relative to ICRF.[135] In the early 20th century, Albert Einstein's general theory of relativity provided the explanation for the observed precession, by formalizing gravitation as being mediated by the curvature of spacetime. The effect is small: just 42.980±0.001 arcseconds per century (or 0.43 arcsecond per year, or 0.1035 arcsecond per orbital period) for Mercury; it therefore requires a little over 12.5 million orbits, or 3 million years, for a full excess turn. Similar, but much smaller, effects exist for other Solar System bodies: 8.6247 arcseconds per century for Venus, 3.8387 for Earth, 1.351 for Mars, and 10.05 for 1566 Icarus.[136][137]
Mercury's apparent magnitude is calculated to vary between −2.48 (brighter than Sirius) around superior conjunction and +7.25 (below the limit of naked-eye visibility) around inferior conjunction.[15] The mean apparent magnitude is 0.23 while the standard deviation of 1.78 is the largest of any planet. The mean apparent magnitude at superior conjunction is −1.89 while that at inferior conjunction is +5.93.[15] Observation of Mercury is complicated by its proximity to the Sun, as it is lost in the Sun's glare for much of the time. Mercury can be observed for only a brief period during either morning or evening twilight.[138] 
But in some cases Mercury can better be observed in daylight with a telescope when the position is known because it is higher in the sky and less atmospheric effects affect the view of the planet. When proper safety precautions are taken to prevent inadvertently pointing the telescope at the Sun (and thus blinding the user), Mercury can be viewed as close as 4° to the Sun when near superior conjunction when it is almost at its brightest.
Mercury can, like several other planets and the brightest stars, be seen during a total solar eclipse.[139]
Like the Moon and Venus, Mercury exhibits phases as seen from Earth. It is "new" at inferior conjunction and "full" at superior conjunction. The planet is rendered invisible from Earth on both of these occasions because of its being obscured by the Sun,[138] except its new phase during a transit.
Mercury is technically brightest as seen from Earth when it is at a full phase. Although Mercury is farthest from Earth when it is full, the greater illuminated area that is visible and the opposition brightness surge more than compensates for the distance.[140] The opposite is true for Venus, which appears brightest when it is a crescent, because it is much closer to Earth than when gibbous.[140][141]
Nonetheless, the brightest (full phase) appearance of Mercury is an essentially impossible time for practical observation, because of the extreme proximity of the Sun. Mercury is best observed at the first and last quarter, although they are phases of lesser brightness. The first and last quarter phases occur at greatest elongation east and west of the Sun, respectively. At both of these times Mercury's separation from the Sun ranges anywhere from 17.9° at perihelion to 27.8° at aphelion.[142][143] At greatest western elongation, Mercury rises at its earliest before sunrise, and at greatest eastern elongation, it sets at its latest after sunset.[144]
Mercury is more often and easily visible from the Southern Hemisphere than from the Northern. This is because Mercury's maximum western elongation occurs only during early autumn in the Southern Hemisphere, whereas its greatest eastern elongation happens only during late winter in the Southern Hemisphere.[144] In both of these cases, the angle at which the planet's orbit intersects the horizon is maximized, allowing it to rise several hours before sunrise in the former instance and not set until several hours after sundown in the latter from southern mid-latitudes, such as Argentina and South Africa.[144]
An alternate method for viewing Mercury involves observing the planet during daylight hours when conditions are clear, ideally when it is at its greatest elongation. This allows the planet to be found easily, even when using telescopes with 8 cm (3.1 in) apertures. However, great care must be taken to obstruct the Sun from sight because of the extreme risk for eye damage.[145] This method bypasses the limitation of twilight observing when the ecliptic is located at a low elevation (e.g. on autumn evenings).
Ground-based telescope observations of Mercury reveal only an illuminated partial disk with limited detail. The first of two spacecraft to visit the planet was Mariner 10, which mapped about 45% of its surface from 1974 to 1975. The second is the MESSENGER spacecraft, which after three Mercury flybys between 2008 and 2009, attained orbit around Mercury on March 17, 2011,[146] to study and map the rest of the planet.[147]
The Hubble Space Telescope cannot observe Mercury at all, due to safety procedures that prevent its pointing too close to the Sun.[148]
Because the shift of 0.15 revolutions in a year makes up a seven-year cycle (0.15 × 7 ≈ 1.0), in the seventh year Mercury follows almost exactly (earlier by 7 days) the sequence of phenomena it showed seven years before.[142]
The earliest known recorded observations of Mercury are from the MUL.APIN tablets. These observations were most likely made by an Assyrian astronomer around the 14th century BC.[149] The cuneiform name used to designate Mercury on the MUL.APIN tablets is transcribed as UDU.IDIM.GU\U4.UD ("the jumping planet").[e][150] Babylonian records of Mercury date back to the 1st millennium BC. The Babylonians called the planet Nabu after the messenger to the gods in their mythology.[151]
The Greco-Egyptian[152] astronomer Ptolemy wrote about the possibility of planetary transits across the face of the Sun in his work Planetary Hypotheses. He suggested that no transits had been observed either because planets such as Mercury were too small to see, or because the transits were too infrequent.[153]
In ancient China, Mercury was known as "the Hour Star" (Chen-xing 辰星). It was associated with the direction north and the phase of water in the Five Phases system of metaphysics.[154] Modern Chinese, Korean, Japanese and Vietnamese cultures refer to the planet literally as the "water star" (水星), based on the Five elements.[155][156][157] Hindu mythology used the name Budha for Mercury, and this god was thought to preside over Wednesday.[158] The god Odin (or Woden) of Germanic paganism was associated with the planet Mercury and Wednesday.[159] The Maya may have represented Mercury as an owl (or possibly four owls; two for the morning aspect and two for the evening) that served as a messenger to the underworld.[160]
In medieval Islamic astronomy, the Andalusian astronomer Abū Ishāq Ibrāhīm al-Zarqālī in the 11th century described the deferent of Mercury's geocentric orbit as being oval, like an egg or a pignon, although this insight did not influence his astronomical theory or his astronomical calculations.[161][162] In the 12th century, Ibn Bajjah observed "two planets as black spots on the face of the Sun", which was later suggested as the transit of Mercury and/or Venus by the Maragha astronomer Qotb al-Din Shirazi in the 13th century.[163] (Note that most such medieval reports of transits were later taken as observations of sunspots.[164])
In India, the Kerala school astronomer Nilakantha Somayaji in the 15th century developed a partially heliocentric planetary model in which Mercury orbits the Sun, which in turn orbits Earth, similar to the Tychonic system later proposed by Tycho Brahe in the late 16th century.[165]
The first telescopic observations of Mercury were made by Thomas Harriot and Galileo from 1610. In 1612, Simon Marius observed the brightness of Mercury varied with the planet's orbital position and concluded it had phases "in the same way as Venus and the Moon".[166] In 1631, Pierre Gassendi made the first telescopic observations of the transit of a planet across the Sun when he saw a transit of Mercury predicted by Johannes Kepler. In 1639, Giovanni Zupi used a telescope to discover that the planet had orbital phases similar to Venus and the Moon. The observation demonstrated conclusively that Mercury orbited around the Sun.[28]
A rare event in astronomy is the passage of one planet in front of another (occultation), as seen from Earth. Mercury and Venus occult each other every few centuries, and the event of May 28, 1737, is the only one historically observed, having been seen by John Bevis at the Royal Greenwich Observatory.[167] The next occultation of Mercury by Venus will be on December 3, 2133.[168]
The difficulties inherent in observing Mercury mean that it was far less studied than the other planets. In 1800, Johann Schröter made observations of surface features, claiming to have observed 20-kilometre-high (12 mi) mountains. Friedrich Bessel used Schröter's drawings to erroneously estimate the rotation period as 24 hours and an axial tilt of 70°.[169] In the 1880s, Giovanni Schiaparelli mapped the planet more accurately, and suggested that Mercury's rotational period was 88 days, the same as its orbital period due to tidal locking.[170] This phenomenon is known as synchronous rotation. The effort to map the surface of Mercury was continued by Eugenios Antoniadi, who published a book in 1934 that included both maps and his own observations.[94] Many of the planet's surface features, particularly the albedo features, take their names from Antoniadi's map.[171]
In June 1962, Soviet scientists at the Institute of Radio-engineering and Electronics of the USSR Academy of Sciences, led by Vladimir Kotelnikov, became the first to bounce a radar signal off Mercury and receive it, starting radar observations of the planet.[172][173][174] Three years later, radar observations by Americans Gordon H. Pettengill and Rolf B. Dyce, using the 300-meter Arecibo radio telescope in Puerto Rico, showed conclusively that the planet's rotational period was about 59 days.[175][176] The theory that Mercury's rotation was synchronous had become widely held, and it was a surprise to astronomers when these radio observations were announced. If Mercury were tidally locked, its dark face would be extremely cold, but measurements of radio emission revealed that it was much hotter than expected. Astronomers were reluctant to drop the synchronous rotation theory and proposed alternative mechanisms such as powerful heat-distributing winds to explain the observations.[177]
Italian astronomer Giuseppe Colombo noted that the rotation value was about two-thirds of Mercury's orbital period, and proposed that the planet's orbital and rotational periods were locked into a 3:2 rather than a 1:1 resonance.[178] Data from Mariner 10 subsequently confirmed this view.[179] This means that Schiaparelli's and Antoniadi's maps were not "wrong". Instead, the astronomers saw the same features during every second orbit and recorded them, but disregarded those seen in the meantime, when Mercury's other face was toward the Sun, because the orbital geometry meant that these observations were made under poor viewing conditions.[169]
Ground-based optical observations did not shed much further light on Mercury, but radio astronomers using interferometry at microwave wavelengths, a technique that enables removal of the solar radiation, were able to discern physical and chemical characteristics of the subsurface layers to a depth of several meters.[180][181] Not until the first space probe flew past Mercury did many of its most fundamental morphological properties become known. Moreover, recent technological advances have led to improved ground-based observations. In 2000, high-resolution lucky imaging observations were conducted by the Mount Wilson Observatory 1.5 meter Hale telescope. They provided the first views that resolved surface features on the parts of Mercury that were not imaged in the Mariner 10 mission.[182] Most of the planet has been mapped by the Arecibo radar telescope, with 5 km (3.1 mi) resolution, including polar deposits in shadowed craters of what may be water ice.[183]
Reaching Mercury from Earth poses significant technical challenges, because it orbits so much closer to the Sun than Earth. A Mercury-bound spacecraft launched from Earth must travel over 91 million kilometres (57 million miles) into the Sun's gravitational potential well. Mercury has an orbital speed of 47.4 km/s (29.5 mi/s), whereas Earth's orbital speed is 29.8 km/s (18.5 mi/s).[109] Therefore, the spacecraft must make a large change in velocity (delta-v) to get to Mercury and then enter orbit,[185] as compared to the delta-v required for, say, Mars planetary missions.
The potential energy liberated by moving down the Sun's potential well becomes kinetic energy, requiring a delta-v change to do anything other than pass by Mercury. Some portion of this delta-v budget can be provided from a gravity assist during one or more fly-bys of Venus.[186] To land safely or enter a stable orbit the spacecraft would rely entirely on rocket motors. Aerobraking is ruled out because Mercury has a negligible atmosphere. A trip to Mercury requires more rocket fuel than that required to escape the Solar System completely. As a result, only three space probes have visited it so far.[187] A proposed alternative approach would use a solar sail to attain a Mercury-synchronous orbit around the Sun.[188]
The first spacecraft to visit Mercury was NASA's Mariner 10 (1974–1975).[22] The spacecraft used the gravity of Venus to adjust its orbital velocity so that it could approach Mercury, making it both the first spacecraft to use this gravitational "slingshot" effect and the first NASA mission to visit multiple planets.[189] Mariner 10 provided the first close-up images of Mercury's surface, which immediately showed its heavily cratered nature, and revealed many other types of geological features, such as the giant scarps that were later ascribed to the effect of the planet shrinking slightly as its iron core cools.[190] Unfortunately, the same face of the planet was lit at each of Mariner 10's close approaches. This made close observation of both sides of the planet impossible,[191] and resulted in the mapping of less than 45% of the planet's surface.[192]
The spacecraft made three close approaches to Mercury, the closest of which took it to within 327 km (203 mi) of the surface.[193] At the first close approach, instruments detected a magnetic field, to the great surprise of planetary geologists—Mercury's rotation was expected to be much too slow to generate a significant dynamo effect. The second close approach was primarily used for imaging, but at the third approach, extensive magnetic data were obtained. The data revealed that the planet's magnetic field is much like Earth's, which deflects the solar wind around the planet. For many years after the Mariner 10 encounters, the origin of Mercury's magnetic field remained the subject of several competing theories.[194][195]
On March 24, 1975, just eight days after its final close approach, Mariner 10 ran out of fuel. Because its orbit could no longer be accurately controlled, mission controllers instructed the probe to shut down.[196] Mariner 10 is thought to be still orbiting the Sun, passing close to Mercury every few months.[197]
A second NASA mission to Mercury, named MESSENGER (MErcury Surface, Space ENvironment, GEochemistry, and Ranging), was launched on August 3, 2004. It made a fly-by of Earth in August 2005, and of Venus in October 2006 and June 2007 to place it onto the correct trajectory to reach an orbit around Mercury.[198] A first fly-by of Mercury occurred on January 14, 2008, a second on October 6, 2008,[199] and a third on September 29, 2009.[200] Most of the hemisphere not imaged by Mariner 10 was mapped during these fly-bys. The probe successfully entered an elliptical orbit around the planet on March 18, 2011. The first orbital image of Mercury was obtained on March 29, 2011. The probe finished a one-year mapping mission,[199] and then entered a one-year extended mission into 2013. In addition to continued observations and mapping of Mercury, MESSENGER observed the 2012 solar maximum.[201]
The mission was designed to clear up six key issues: Mercury's high density, its geological history, the nature of its magnetic field, the structure of its core, whether it has ice at its poles, and where its tenuous atmosphere comes from. To this end, the probe carried imaging devices that gathered much-higher-resolution images of much more of Mercury than Mariner 10, assorted spectrometers to determine abundances of elements in the crust, and magnetometers and devices to measure velocities of charged particles. Measurements of changes in the probe's orbital velocity were expected to be used to infer details of the planet's interior structure.[202] MESSENGER's final maneuver was on April 24, 2015, and it crashed into Mercury's surface on April 30, 2015.[203][204][205] The spacecraft's impact with Mercury occurred near 3:26 pm EDT on April 30, 2015, leaving a crater estimated to be 16 m (52 ft) in diameter.[206]
The European Space Agency and the Japanese Space Agency developed and launched a joint mission called BepiColombo, which will orbit Mercury with two probes: one to map the planet and the other to study its magnetosphere.[207] Launched on October 20, 2018, BepiColombo is expected to reach Mercury in 2025.[208] It will release a magnetometer probe into an elliptical orbit, then chemical rockets will fire to deposit the mapper probe into a circular orbit. Both probes will operate for one terrestrial year.[207] The mapper probe carries an array of spectrometers similar to those on MESSENGER, and will study the planet at many different wavelengths including infrared, ultraviolet, X-ray and gamma ray.[209] BepiColombo conducted the first of its six planned Mercury flybys on October 1, 2021.[210]





In biology, a species is often defined as the largest group of organisms in which any two individuals of the appropriate sexes or mating types can produce fertile offspring, typically by sexual reproduction. It is the basic unit of classification and a taxonomic rank of an organism, as well as a unit of biodiversity. Other ways of defining species include their karyotype, DNA sequence, morphology, behaviour, or ecological niche. In addition, paleontologists use the concept of the chronospecies since fossil reproduction cannot be examined.
The most recent rigorous estimate for the total number of species of eukaryotes is between 8 and 8.7 million.[1][2][3] However, only about 14% of these had been described by 2011.[3]
All species (except viruses) are given a two-part name, a "binomial". The first part of a binomial is the genus to which the species belongs. The second part is called the specific name or the specific epithet (in botanical nomenclature, also sometimes in zoological nomenclature). For example, Boa constrictor is one of the species of the genus Boa, with constrictor being the species' epithet.
While the definitions given above may seem adequate at first glance, when looked at more closely they represent problematic species concepts. For example, the boundaries between closely related species become unclear with hybridisation, in a species complex of hundreds of similar microspecies, and in a ring species. Also, among organisms that reproduce only asexually, the concept of a reproductive species breaks down, and each clone is potentially a microspecies. Although none of these are entirely satisfactory definitions, and while the concept of species may not be a perfect model of life, it is still a useful tool to scientists and conservationists for studying life on Earth, regardless of the theoretical difficulties. If species were fixed and clearly distinct from one another, there would be no problem, but evolutionary processes cause species to change. This obliges taxonomists to decide, for example, when enough change has occurred to declare that a lineage should be divided into multiple chronospecies, or when populations have diverged to have enough distinct character states to be described as cladistic species.
Species were seen from the time of Aristotle until the 18th century as fixed categories that could be arranged in a hierarchy, the great chain of being. In the 19th century, biologists grasped that species could evolve given sufficient time. Charles Darwin's 1859 book On the Origin of Species explained how species could arise by natural selection. That understanding was greatly extended in the 20th century through genetics and population ecology. Genetic variability arises from mutations and recombination, while organisms themselves are mobile, leading to geographical isolation and genetic drift with varying selection pressures. Genes can sometimes be exchanged between species by horizontal gene transfer; new species can arise rapidly through hybridisation and polyploidy; and species may become extinct for a variety of reasons. Viruses are a special case, driven by a balance of mutation and selection, and can be treated as quasispecies.
Biologists and taxonomists have made many attempts to define species, beginning from morphology and moving towards genetics. Early taxonomists such as Linnaeus had no option but to describe what they saw: this was later formalised as the typological or morphological species concept. Ernst Mayr emphasised reproductive isolation, but this, like other species concepts, is hard or even impossible to test.[4][5] Later biologists have tried to refine Mayr's definition with the recognition and cohesion concepts, among others.[6] Many of the concepts are quite similar or overlap, so they are not easy to count: the biologist R. L. Mayden recorded about 24 concepts,[7] and the philosopher of science John Wilkins counted 26.[4] Wilkins further grouped the species concepts into seven basic kinds of concepts: (1) agamospecies for asexual organisms (2) biospecies for reproductively isolated sexual organisms (3) ecospecies based on ecological niches (4) evolutionary species based on lineage (5) genetic species based on gene pool (6) morphospecies based on form or phenotype and (7) taxonomic species, a species as determined by a taxonomist.[8]

A typological species is a group of organisms in which individuals conform to certain fixed properties (a type), so that even pre-literate people often recognise the same taxon as do modern taxonomists.[10][11] The clusters of variations or phenotypes within specimens (such as longer or shorter tails) would differentiate the species. This method was used as a "classical" method of determining species, such as with Linnaeus, early in evolutionary theory. However, different phenotypes are not necessarily different species (e.g. a four-winged Drosophila born to a two-winged mother is not a different species). Species named in this manner are called morphospecies.[12][13] 
In the 1970s, Robert R. Sokal, Theodore J. Crovello and Peter Sneath proposed a variation on the morphological species concept, a phenetic species, defined as a set of organisms with a similar phenotype to each other, but a different phenotype from other sets of organisms.[14] It differs from the morphological species concept in including a numerical measure of distance or similarity to cluster entities based on multivariate comparisons of a reasonably large number of phenotypic traits.[15]

A mate-recognition species is a group of sexually reproducing organisms that recognise one another as potential mates.[16][17] Expanding on this to allow for post-mating isolation, a cohesion species is the most inclusive population of individuals having the potential for phenotypic cohesion through intrinsic cohesion mechanisms; no matter whether populations can hybridise successfully, they are still distinct cohesion species if the amount of hybridisation is insufficient to completely mix their respective gene pools.[18] A further development of the recognition concept is provided by the biosemiotic concept of species.[19]

In microbiology, genes can move freely even between distantly related bacteria, possibly extending to the whole bacterial domain. As a rule of thumb, microbiologists have assumed that members of Bacteria or Archaea with 16S ribosomal RNA gene sequences more similar than 97% to each other need to be checked by DNA–DNA hybridisation to decide if they belong to the same species.[20] This concept was narrowed in 2006 to a similarity of 98.7%.[21] 
The average nucleotide identity method quantifies genetic distance between entire genomes, using regions of about 10,000 base pairs. With enough data from genomes of one genus, algorithms can be used to categorize species, as for Pseudomonas avellanae in 2013,[22] and for all sequenced bacteria and archaea since 2020.[23]
DNA barcoding has been proposed as a way to distinguish species suitable even for non-specialists to use.[24] One of the barcodes is a region of mitochondrial DNA within the gene for cytochrome c oxidase. A database, Barcode of Life Data System, contains DNA barcode sequences from over 190,000 species.[25][26] However, scientists such as Rob DeSalle have expressed concern that classical taxonomy and DNA barcoding, which they consider a misnomer, need to be reconciled, as they delimit species differently.[27] Genetic introgression mediated by endosymbionts and other vectors can further make barcodes ineffective in the identification of species.[28]


A phylogenetic or cladistic species is "the smallest aggregation of populations (sexual) or lineages (asexual) diagnosable by a unique combination of character states in comparable individuals (semaphoronts)".[29] The empirical basis – observed character states – provides the evidence to support hypotheses about evolutionarily divergent lineages that have maintained their hereditary integrity through time and space.[30][31][32][33] Molecular markers may be used to determine diagnostic genetic differences in the nuclear or mitochondrial DNA of various species.[34][29][35] For example, in a study done on fungi, studying the nucleotide characters using cladistic species produced the most accurate results in recognising the numerous fungi species of all the concepts studied.[35][36] Versions of the phylogenetic species concept that emphasise monophyly or diagnosability[37] may lead to splitting of existing species, for example in Bovidae, by recognising old subspecies as species, despite the fact that there are no reproductive barriers, and populations may intergrade morphologically.[38] Others have called this approach taxonomic inflation, diluting the species concept and making taxonomy unstable.[39] Yet others defend this approach, considering "taxonomic inflation" pejorative and labelling the opposing view as "taxonomic conservatism"; claiming it is politically expedient to split species and recognise smaller populations at the species level, because this means they can more easily be included as endangered in the IUCN red list and can attract conservation legislation and funding.[40]
Unlike the biological species concept, a cladistic species does not rely on reproductive isolation – its criteria are independent of processes that are integral in other concepts.[29] Therefore, it applies to asexual lineages.[34][35] However, it does not always provide clear cut and intuitively satisfying boundaries between taxa, and may require multiple sources of evidence, such as more than one polymorphic locus, to give plausible results.[35]
An evolutionary species, suggested by George Gaylord Simpson in 1951, is "an entity composed of organisms which maintains its identity from other such entities through time and over space, and which has its own independent evolutionary fate and historical tendencies".[7][41] This differs from the biological species concept in embodying persistence over time. Wiley and Mayden stated that they see the evolutionary species concept as "identical" to Willi Hennig's species-as-lineages concept, and asserted that the biological species concept, "the several versions" of the phylogenetic species concept, and the idea that species are of the same kind as higher taxa are not suitable for biodiversity studies (with the intention of estimating the number of species accurately). They further suggested that the concept works for both asexual and sexually-reproducing species.[42] A version of the concept is Kevin de Queiroz's "General Lineage Concept of Species".[43]
An ecological species is a set of organisms adapted to a particular set of resources, called a niche, in the environment. According to this concept, populations form the discrete phenetic clusters that we recognise as species because the ecological and evolutionary processes controlling how resources are divided up tend to produce those clusters.[44]
A genetic species as defined by Robert Baker and Robert Bradley is a set of genetically isolated interbreeding populations. This is similar to Mayr's Biological Species Concept, but stresses genetic rather than reproductive isolation.[45] In the 21st century, a genetic species can be established by comparing DNA sequences, but other methods were available earlier, such as comparing karyotypes (sets of chromosomes) and allozymes (enzyme variants).[46]
An evolutionarily significant unit (ESU) or "wildlife species"[47] is a population of organisms considered distinct for purposes of conservation.[48]
In palaeontology, with only comparative anatomy (morphology) from fossils as evidence, the concept of a chronospecies can be applied. During anagenesis (evolution, not necessarily involving branching), palaeontologists seek to identify a sequence of species, each one derived from the phyletically extinct one before through continuous, slow and more or less uniform change. In such a time sequence, palaeontologists assess how much change is required for a morphologically distinct form to be considered a different species from its ancestors.[49][50][51][52]
Viruses have enormous populations, are doubtfully living since they consist of little more than a string of DNA or RNA in a protein coat, and mutate rapidly. All of these factors make conventional species concepts largely inapplicable.[53] A viral quasispecies is a group of genotypes related by similar mutations, competing within a highly mutagenic environment, and hence governed by a mutation–selection balance. It is predicted that a viral quasispecies at a low but evolutionarily neutral and highly connected (that is, flat) region in the fitness landscape will outcompete a quasispecies located at a higher but narrower fitness peak in which the surrounding mutants are unfit, "the quasispecies effect" or the "survival of the flattest". There is no suggestion that a viral quasispecies resembles a traditional biological species.[54][55][56] The International Committee on Taxonomy of Viruses has since 1962 developed a universal taxonomic scheme for viruses; this has stabilised viral taxonomy.[57][58][59]
Most modern textbooks make use of Ernst Mayr's 1942 definition,[60][61] known as the Biological Species Concept as a basis for further discussion on the definition of species. It is also called a reproductive or isolation concept. This defines a species as[62]
groups of actually or potentially interbreeding natural populations, which are reproductively isolated from other such groups.[62]It has been argued that this definition is a natural consequence of the effect of sexual reproduction on the dynamics of natural selection.[63][64][65][66] Mayr's use of the adjective "potentially" has been a point of debate; some interpretations exclude unusual or artificial matings that occur only in captivity, or that involve animals capable of mating but that do not normally do so in the wild.[62]
It is difficult to define a species in a way that applies to all organisms.[67] The debate about species concepts is called the species problem.[62][68][69][70] The problem was recognised even in 1859, when Darwin wrote in On the Origin of Species:
No one definition has satisfied all naturalists; yet every naturalist knows vaguely what he means when he speaks of a species. Generally the term includes the unknown element of a distinct act of creation.[71]A simple textbook definition, following Mayr's concept, works well for most multi-celled organisms, but breaks down in several situations:
Species identification is made difficult by discordance between molecular and morphological investigations; these can be categorised as two types: (i) one morphology, multiple lineages (e.g. morphological convergence, cryptic species) and (ii) one lineage, multiple morphologies (e.g. phenotypic plasticity, multiple life-cycle stages).[81] In addition, horizontal gene transfer (HGT) makes it difficult to define a species.[82] All species definitions assume that an organism acquires its genes from one or two parents very like the "daughter" organism, but that is not what happens in HGT.[83] There is strong evidence of HGT between very dissimilar groups of prokaryotes, and at least occasionally between dissimilar groups of eukaryotes,[82] including some crustaceans and echinoderms.[84]
The evolutionary biologist James Mallet concludes that
there is no easy way to tell whether related geographic or temporal forms belong to the same or different species. Species gaps can be verified only locally and at a point of time. One is forced to admit that Darwin's insight is correct: any local reality or integrity of species is greatly reduced over large geographic ranges and time periods.[18]Wilkins writes that biologists such as the botanist Brent Mishler[85] have argued that the species concept is not valid, and that "if we were being true to evolution and the consequent phylogenetic approach to taxa, we should replace it with a 'smallest clade' idea" (a phylogenetic species concept).[86] Wilkins states that he concurs[87] with this approach, while noting the difficulties it would cause to taxonomy. He cites the ichthyologist Charles Tate Regan's early 20th century remark that "a species is whatever a suitably qualified biologist chooses to call a species".[86] Wilkins notes that the philosopher Philip Kitcher called this the "cynical species concept",[88] and arguing that far from being cynical, it usefully leads to an empirical taxonomy for any given group, based on taxonomists' experience.[86]
The species concept is further weakened by the existence of microspecies, groups of organisms, including many plants, with very little genetic variability, usually forming species aggregates.[89] For example, the dandelion Taraxacum officinale and the blackberry Rubus fruticosus are aggregates with many microspecies—perhaps 400 in the case of the blackberry and over 200 in the dandelion,[90] complicated by hybridisation, apomixis and polyploidy, making gene flow between populations difficult to determine, and their taxonomy debatable.[91][92][93] Species complexes occur in insects such as Heliconius butterflies,[94] vertebrates such as Hypsiboas treefrogs,[95] and fungi such as the fly agaric.[96]
Blackberries belong to any of hundreds of microspecies of the Rubus fruticosus species aggregate.
The butterfly genus Heliconius contains many similar species.
The Hypsiboas calcaratus–fasciatus species complex contains at least six species of treefrog.
Natural hybridisation presents a challenge to the concept of a reproductively isolated species, as fertile hybrids permit gene flow between two populations. For example, the carrion crow Corvus corone and the hooded crow Corvus cornix appear and are classified as separate species, yet they can hybridise where their geographical ranges overlap.[97]
Carrion crow
Hybrid with dark belly
Hooded crow
A ring species is a connected series of neighbouring populations, each of which can sexually interbreed with adjacent related populations, but for which there exist at least two "end" populations in the series, which are too distantly related to interbreed, though there is a potential gene flow between each "linked" population.[98] Such non-breeding, though genetically connected, "end" populations may co-exist in the same region thus closing the ring. Ring species thus present a difficulty for any species concept that relies on reproductive isolation.[99] However, ring species are at best rare. Proposed examples include the herring gull–lesser black-backed gull complex around the North pole, the Ensatina eschscholtzii group of 19 populations of salamanders in America,[100] and the greenish warbler in Asia,[101] but many so-called ring species have turned out to be the result of misclassification leading to questions on whether there really are any ring species.[102][103][104][105]
Seven "species" of Larus gulls interbreed in a ring around the Arctic.
Opposite ends of the ring: a herring gull (Larus argentatus) (front) and a lesser black-backed gull (Larus fuscus) in Norway
A greenish warbler, Phylloscopus trochiloides
Presumed evolution of five "species"  of greenish warblers around the Himalayas
The commonly used names for kinds of organisms are often ambiguous: "cat" could mean the domestic cat, Felis catus, or the cat family, Felidae. Another problem with common names is that they often vary from place to place, so that puma, cougar, catamount, panther, painter and mountain lion all mean Puma concolor in various parts of America, while "panther" may also mean the jaguar (Panthera onca) of Latin America or the leopard (Panthera pardus) of Africa and Asia. In contrast, the scientific names of species are chosen to be unique and universal; they are in two parts used together: the genus as in Puma, and the specific epithet as in concolor.[106][107]
A species is given a taxonomic name when a type specimen is described formally, in a publication that assigns it a unique scientific name. The description typically provides means for identifying the new species, which may not be based solely on morphology[108] (see cryptic species), differentiating it from other previously described and related or confusable species and provides a validly published name (in botany) or an available name (in zoology) when the paper is accepted for publication. The type material is usually held in a permanent repository, often the research collection of a major museum or university, that allows independent verification and the means to compare specimens.[109][110][111] Describers of new species are asked to choose names that, in the words of the International Code of Zoological Nomenclature, are "appropriate, compact, euphonious, memorable, and do not cause offence".[112]
Books and articles sometimes intentionally do not identify species fully, using the abbreviation "sp." in the singular or "spp." (standing for species pluralis, Latin for "multiple species") in the plural in place of the specific name or epithet (e.g. Canis sp.). This commonly occurs when authors are confident that some individuals belong to a particular genus but are not sure to which exact species they belong, as is common in paleontology.[113]
Authors may also use "spp." as a short way of saying that something applies to many species within a genus, but not to all. If scientists mean that something applies to all species within a genus, they use the genus name without the specific name or epithet. The names of genera and species are usually printed in italics. However, abbreviations such as "sp." should not be italicised.[113]
When a species's identity is not clear, a specialist may use "cf." before the epithet to indicate that confirmation is required. The abbreviations "nr." (near) or "aff." (affine) may be used when the identity is unclear but when the species appears to be similar to the species mentioned after.[113]
With the rise of online databases, codes have been devised to provide identifiers for species that are already defined, including:
The naming of a particular species, including which genus (and higher taxa) it is placed in, is a hypothesis about the evolutionary relationships and distinguishability of that group of organisms. As further information comes to hand, the hypothesis may be corroborated or refuted. Sometimes, especially in the past when communication was more difficult, taxonomists working in isolation have given two distinct names to individual organisms later identified as the same species. When two species names are discovered to apply to the same species, the older species name is given priority and usually retained, and the newer name considered as a junior synonym, a process called synonymy. Dividing a taxon into multiple, often new, taxa is called splitting. Taxonomists are often referred to as "lumpers" or "splitters" by their colleagues, depending on their personal approach to recognising differences or commonalities between organisms.[118][119][113] The circumscription of taxa, considered a taxonomic decision at the discretion of cognizant specialists, is not governed by the Codes of Zoological or Botanical Nomenclature.
The nomenclatural codes that guide the naming of species, including the ICZN for animals and the ICN for plants, do not make rules for defining the boundaries of the species. Research can change the boundaries, also known as circumscription, based on new evidence. Species may then need to be distinguished by the boundary definitions used, and in such cases the names may be qualified with sensu stricto ("in the narrow sense") to denote usage in the exact meaning given by an author such as the person who named the species, while the antonym sensu lato ("in the broad sense") denotes a wider usage, for instance including other subspecies. Other abbreviations such as "auct." ("author"), and qualifiers such as "non" ("not") may be used to further clarify the sense in which the specified authors delineated or described the species.[113][120][121]



Species are subject to change, whether by evolving into new species,[122] exchanging genes with other species,[123] merging with other species or by becoming extinct.[124]
The evolutionary process by which biological populations evolve to become distinct or reproductively isolated as species is called speciation.[125][126] Charles Darwin was the first to describe the role of natural selection in speciation in his 1859 book The Origin of Species.[127] Speciation depends on a measure of reproductive isolation, a reduced gene flow. This occurs most easily in allopatric speciation, where populations are separated geographically and can diverge gradually as mutations accumulate. Reproductive isolation is threatened by hybridisation, but this can be selected against once a pair of populations have incompatible alleles of the same gene, as described in the Bateson–Dobzhansky–Muller model.[122] A different mechanism, phyletic speciation, involves one lineage gradually changing over time into a new and distinct form, without increasing the number of resultant species.[128]
Horizontal gene transfer between organisms of different species, either through hybridisation, antigenic shift, or reassortment, is sometimes an important source of genetic variation. Viruses can transfer genes between species. Bacteria can exchange plasmids with bacteria of other species, including some apparently distantly related ones in different phylogenetic domains, making analysis of their relationships difficult, and weakening the concept of a bacterial species.[129][82][130][123]
Louis-Marie Bobay and Howard Ochman suggest, based on analysis of the genomes of many types of bacteria, that they can often be grouped "into communities that regularly swap genes", in much the same way that plants and animals can be grouped into reproductively isolated breeding populations. Bacteria may thus form species, analogous to Mayr's biological species concept, consisting of asexually reproducing populations that exchange genes by homologous recombination.[131][132]
A species is extinct when the last individual of that species dies, but it may be functionally extinct well before that moment. It is estimated that over 99 percent of all species that ever lived on Earth, some five billion species, are now extinct. Some of these were in mass extinctions such as those at the ends of the Ordovician, Devonian, Permian, Triassic and Cretaceous periods. Mass extinctions had a variety of causes including volcanic activity, climate change, and changes in oceanic and atmospheric chemistry, and they in turn had major effects on Earth's ecology, atmosphere, land surface and waters.[133][134] Another form of extinction is through the assimilation of one species by another through hybridization. The resulting single species has been termed as a "compilospecies".[135]
Biologists and conservationists need to categorise and identify organisms in the course of their work. Difficulty assigning organisms reliably to a species constitutes a threat to the validity of research results, for example making measurements of how abundant a species is in an ecosystem moot. Surveys using a phylogenetic species concept reported 48% more species and accordingly smaller populations and ranges than those using nonphylogenetic concepts; this was termed "taxonomic inflation",[136] which could cause a false appearance of change to the number of endangered species and consequent political and practical difficulties.[137][138] Some observers claim that there is an inherent conflict between the desire to understand the processes of speciation and the need to identify and to categorise.[138]
Conservation laws in many countries make special provisions to prevent species from going extinct. Hybridization zones between two species, one that is protected and one that is not, have sometimes led to conflicts between lawmakers, land owners and conservationists. One of the classic cases in North America is that of the protected northern spotted owl which hybridises with the unprotected California spotted owl and the barred owl; this has led to legal debates.[139] It has been argued that the species problem is created by the varied uses of the concept of species, and that the solution is to abandon it and all other taxonomic ranks, and use unranked monophyletic groups instead. It has been argued, too, that since species are not comparable, counting them is not a valid measure of biodiversity; alternative measures of phylogenetic biodiversity have been proposed.[140][141]
In his biology, Aristotle used the term γένος (génos) to mean a kind, such as a bird or fish, and εἶδος (eidos) to mean a specific form within a kind, such as (within the birds) the crane, eagle, crow, or sparrow. These terms were translated into Latin as "genus" and "species", though they do not correspond to the Linnean terms thus named; today the birds are a class, the cranes are a family, and the crows a genus. A kind was distinguished by its attributes; for instance, a bird has feathers, a beak, wings, a hard-shelled egg, and warm blood. A form was distinguished by being shared by all its members, the young inheriting any variations they might have from their parents. Aristotle believed all kinds and forms to be distinct and unchanging. His approach remained influential until the Renaissance.[142]
When observers in the Early Modern period began to develop systems of organization for living things, they placed each kind of animal or plant into a context. Many of these early delineation schemes would now be considered whimsical: schemes included consanguinity based on colour (all plants with yellow flowers) or behaviour (snakes, scorpions and certain biting ants). John Ray, an English naturalist, was the first to attempt a biological definition of species in 1686, as follows:
No surer criterion for determining species has occurred to me than the distinguishing features that perpetuate themselves in propagation from seed. Thus, no matter what variations occur in the individuals or the species, if they spring from the seed of one and the same plant, they are accidental variations and not such as to distinguish a species ... Animals likewise that differ specifically preserve their distinct species permanently; one species never springs from the seed of another nor vice versa.[143]In the 18th century, the Swedish scientist Carl Linnaeus classified organisms according to shared physical characteristics, and not simply based upon differences.[144] He established the idea of a taxonomic hierarchy of classification based upon observable characteristics and intended to reflect natural relationships.[145][146] At the time, however, it was still widely believed that there was no organic connection between species, no matter how similar they appeared. This view was influenced by European scholarly and religious education, which held that the categories of life are dictated by God, forming an Aristotelian hierarchy, the scala naturae or great chain of being. However, whether or not it was supposed to be fixed, the scala (a ladder) inherently implied the possibility of climbing.[147]
In viewing evidence of hybridisation, Linnaeus recognised that species were not fixed and could change; he did not consider that new species could emerge and maintained a view of divinely fixed species that may alter through processes of hybridisation or acclimatisation.[148] By the 19th century, naturalists understood that species could change form over time, and that the history of the planet provided enough time for major changes. Jean-Baptiste Lamarck, in his 1809 Zoological Philosophy, described the transmutation of species, proposing that a species could change over time, in a radical departure from Aristotelian thinking.[149]
In 1859, Charles Darwin and Alfred Russel Wallace provided a compelling account of evolution and the formation of new species. Darwin argued that it was populations that evolved, not individuals, by natural selection from naturally occurring variation among individuals.[150] This required a new definition of species. Darwin concluded that species are what they appear to be: ideas, provisionally useful for naming groups of interacting individuals, writing:
I look at the term species as one arbitrarily given for the sake of convenience to a set of individuals closely resembling each other ... It does not essentially differ from the word variety, which is given to less distinct and more fluctuating forms. The term variety, again, in comparison with mere individual differences, is also applied arbitrarily, and for convenience sake.[151]
Muscle is a soft tissue, one of the animal tissues that makes up the three different types of muscle.  Muscle tissue gives skeletal muscles the ability to contract. Muscle is formed during embryonic development, in a process known as myogenesis. Muscle tissue contains special contractile proteins called actin and myosin which interact to cause movement. Among many other muscle proteins present are two regulatory proteins, troponin and tropomyosin.
Muscle tissue varies with function and location in the body. In vertebrates the three types are: skeletal or striated; smooth muscle (non-striated) muscle; and cardiac muscle.[1] Skeletal muscle tissue consists of elongated, multinucleate muscle cells called muscle fibers, and is responsible for movements of the body. Other tissues in skeletal muscle include tendons and perimysium.[citation needed] Smooth and cardiac muscle contract involuntarily, without conscious intervention. These muscle types may be activated both through the interaction of the central nervous system as well as by receiving innervation from peripheral plexus or endocrine (hormonal) activation. Striated or skeletal muscle only contracts voluntarily, upon the influence of the central nervous system. Reflexes are a form of non-conscious activation of skeletal muscles, but nonetheless arise through activation of the central nervous system, albeit not engaging cortical structures until after the contraction has occurred.[citation needed]
The different muscle types vary in their response to neurotransmitters and hormones such as acetylcholine, noradrenaline, adrenaline, and nitric oxide depending on muscle type and the exact location of the muscle.[citation needed]
Sub-categorization of muscle tissue is also possible, depending on among other things the content of myoglobin, mitochondria, and myosin ATPase etc.[citation needed]
There are three types of muscle tissue in vertebrates: skeletal, cardiac, and smooth. Skeletal and cardiac muscle are types of striated muscle tissue.[1] Smooth muscle is non-striated.
There are three types of muscle tissue in invertebrates that are based on their pattern of striation: transversely striated, obliquely striated, and smooth muscle. In arthropods there is no smooth muscle. The transversely striated type is the most similar to the skeletal muscle in vertebrates.[2] 
Vertebrate skeletal muscle tissue is an elongated striated muscle tissue with the fibres ranging in width from three to eight micrometers and in length from 18 to 200 micrometers. In the uterine wall during pregnancy they enlarge in length from 70 to 500 micrometers.[3] Skeletal striated muscle tissue is arranged in regular, parallel bundles of myofibrils containing the many contractile units known as sarcomeres, which give the tissue its striated (striped) appearance. Skeletal muscle, is voluntary muscle anchored by tendons or sometimes by aponeuroses to bones, and is used to effect skeletal movement such as locomotion and to maintain posture. Postural control is generally maintained as an unconscious reflex, but the muscles responsible can also react to conscious control. An average adult man is made up of 42% of skeletal muscle as a percentage of body mass, and an average adult woman is made up of 36%.[4]
Cardiac muscle tissue, is found only in the walls of the heart as myocardium, and is an involuntary muscle controlled by the autonomic nervous system. Cardiac muscle tissue is striated like skeletal muscle, containing contractile units called sarcomeres in highly regular arrangements of bundles. While skeletal muscles are arranged in regular, parallel bundles, cardiac muscle connects at branching, irregular angles known as intercalated discs.
Smooth muscle tissue is non-striated and involuntary. Smooth muscle is found within the walls of organs and structures such as the esophagus, stomach, intestines, bronchi, uterus, urethra, bladder, blood vessels, and the arrector pili in the skin which controls the erection of body hair.
Skeletal muscle is broadly classified into two fiber types: Type I slow-twitch, and Type II fast-twitch muscle.    
The density of mammalian skeletal muscle tissue is about 1.06 kg/liter.[8] This can be contrasted with the density of adipose tissue (fat), which is 0.9196 kg/liter.[9] This makes muscle tissue approximately 15% denser than fat tissue.
Smooth muscle is involuntary and non-striated. It is divided into two subgroups: the single-unit (unitary) and multiunit smooth muscle. Within single-unit cells, the whole bundle or sheet contracts as a syncytium (i.e. a multinucleate mass of cytoplasm that is not separated into cells). Multiunit smooth muscle tissues innervate individual cells; as such, they allow for fine control and gradual responses, much like motor unit recruitment in skeletal muscle.
Smooth muscle is found within the walls of blood vessels (such smooth muscle specifically being termed vascular smooth muscle) such as in the tunica media layer of large (aorta) and small arteries, arterioles and veins. Smooth muscle is also found in lymphatic vessels, the urinary bladder, uterus (termed uterine smooth muscle), male and female reproductive tracts, gastrointestinal tract, respiratory tract, arrector pili of skin, the ciliary muscle, and iris of the eye. The structure and function is basically the same in smooth muscle cells in different organs, but the inducing stimuli differ substantially, in order to perform individual effects in the body at individual times. In addition, the glomeruli of the kidneys contain smooth muscle-like cells called mesangial cells.
Cardiac muscle is involuntary, striated muscle that is found in the walls and histological foundation of the heart, specifically the myocardium. The cardiac muscle cells, (also called cardiomyocytes or myocardiocytes), predominantly contain only one nucleus, although populations with two to four nuclei do exist.[10][11][page needed] The myocardium is the muscle tissue of the heart and forms a thick middle layer between the outer epicardium layer and the inner endocardium layer.
Coordinated contractions of cardiac muscle cells in the heart propel blood out of the atria and ventricles to the blood vessels of the left/body/systemic and right/lungs/pulmonary circulatory systems. This complex mechanism illustrates systole of the heart.
Cardiac muscle cells, unlike most other tissues in the body, rely on an available blood and electrical supply to deliver oxygen and nutrients and remove waste products such as carbon dioxide. The coronary arteries help fulfill this function.
All muscles are derived from paraxial mesoderm. The paraxial mesoderm is divided along the embryo's length into somites, corresponding to the segmentation of the body (most obviously seen in the vertebral column.[12] Each somite has three divisions, sclerotome (which forms vertebrae), dermatome (which forms skin), and myotome (which forms muscle). The myotome is divided into two sections, the epimere and hypomere, which form epaxial and hypaxial muscles, respectively. The only epaxial muscles in humans are the erector spinae and small intervertebral muscles, and are innervated by the dorsal rami of the spinal nerves. All other muscles, including those of the limbs are hypaxial, and innervated by the ventral rami of the spinal nerves.[12]
During development, myoblasts (muscle progenitor cells) either remain in the somite to form muscles associated with the vertebral column or migrate out into the body to form all other muscles. Myoblast migration is preceded by the formation of connective tissue frameworks, usually formed from the somatic lateral plate mesoderm. Myoblasts follow chemical signals to the appropriate locations, where they fuse into elongate skeletal muscle cells.[12]
The primary function of muscle tissue is contraction. The three types of muscle tissue (skeletal, cardiac and smooth) have significant differences. However, all three use the movement of actin against myosin to create contraction. 
In skeletal muscle, contraction is stimulated by electrical impulses transmitted by the motor nerves. Cardiac and smooth muscle contractions are stimulated by internal pacemaker cells which regularly contract, and propagate contractions to other muscle cells they are in contact with. All skeletal muscle and many smooth muscle contractions are facilitated by the neurotransmitter acetylcholine.
Smooth muscle is found in almost all organ systems such as hollow organs including the stomach, and bladder; in tubular structures such as blood and lymph vessels, and bile ducts; in sphincters such as in the uterus, and the eye. In addition, it plays an important role in the ducts of exocrine glands. It fulfills various tasks such as sealing orifices (e.g. pylorus, uterine os) or the transport of the chyme through wavelike contractions of the intestinal tube. Smooth muscle cells contract more slowly than skeletal muscle cells, but they are stronger, more sustained and require less energy. Smooth muscle is also involuntary, unlike skeletal muscle, which requires a stimulus.
Cardiac muscle is the muscle of the heart. It is self-contracting, autonomically regulated and must continue to contract in a rhythmic fashion for the whole life of the organism. Hence it has special features.
There are three types of muscle tissue in invertebrates that are based on their pattern of striation: transversely striated, obliquely striated, and smooth muscle. In arthropods there is no smooth muscle. The transversely striated type is the most similar to the skeletal muscle in vertebrates.[2]

Deafness has varying definitions in cultural and medical contexts. In medical contexts, the meaning of deafness is hearing loss that precludes a person from understanding spoken language, an audiological condition.[1] In this context it is written with a lower case d. It later came to be used in a cultural context to refer to those who primarily communicate through sign language regardless of hearing ability, often capitalized as Deaf and referred to as "big D Deaf" in speech and sign.[2][3] The two definitions overlap but are not identical, as hearing loss includes cases that are not severe enough to impact spoken language comprehension, while cultural Deafness includes hearing people who use sign language, such as children of deaf adults.
In a medical context, deafness is defined as a degree of hearing difference such that a person is unable to understand speech, even in the presence of amplification.[1] In profound deafness, even the highest intensity sounds produced by an audiometer (an instrument used to measure hearing by producing pure tone sounds through a range of frequencies) may not be detected. In total deafness, no sounds at all, regardless of amplification or method of production, can be heard.
Neurologically, language is processed in the same areas of the brain whether one is deaf or hearing. The left hemisphere of the brain processes linguistic patterns whether by signed languages or by spoken languages.[5]
Deafness can be broken down into four different types of hearing loss: conductive hearing loss, sensorineural hearing loss, mixed hearing loss, and auditory neuropathy spectrum disorder. All of these forms of hearing loss cause an impairment in a person's hearing where they are not able to hear sounds correctly. These different types of hearing loss occur in different parts of the ear, which make it difficult for the information being heard to get sent to the brain properly. To break it down even further, there are three different levels of hearing loss. According to the CDC, the first level is mild hearing loss. This is when someone is still able to hear noises, but it is more difficult to hear the softer sounds. The second level is moderate hearing loss and this is when someone can hear almost nothing when someone is talking to them at a normal volume. The next level is severe hearing loss. Severe hearing loss is when someone can not hear any sounds when they are being produced at a normal level and they can only hear minimum sounds that are being produced at a loud level. The final level is profound hearing loss, which is when someone is not able to hear any sounds except for very loud ones.[6]
There are millions of people in the world who are living with deafness or hearing impairments. Survey of Income and Program Participation (SIPP) indicate that fewer than 1 in 20 Americans are currently deaf or hard of hearing.[7]  There are a lot of solutions available for people with hearing impairments. Some examples of solutions would be blinking lights on different things like their phones, alarms, and things that are important to alert them. Cochlear implants are an option too.[8] Cochlear implants are surgically placed devices that stimulate the cochlear nerve in order to help the person hear. A cochlear implant is used instead of hearing aids in order to help when someone has difficulties understanding speech.[9]
In a cultural context, Deaf culture refers to a tight-knit cultural group of people whose primary language is signed, and who practice social and cultural norms which are distinct from those of the surrounding hearing community. This community does not automatically include all those who are clinically or legally deaf, nor does it exclude every hearing person. According to Baker and Padden, it includes any person who "identifies him/herself as a member of the Deaf community, and other members accept that person as a part of the community",[10] an example being children of deaf adults with normal hearing ability. It includes the set of social beliefs, behaviors, art, literary traditions, history, values, and shared institutions of communities that are influenced by deafness and which use sign languages as the main means of communication.[2][3] Members of the Deaf community tend to view deafness as a difference in human experience rather than a disability or disease.[11][12]
Many non-disabled people continue to assume that deaf people have no autonomy and fail to provide people with support beyond hearing aids, which is something that must be addressed. Different non-governmental organizations around the world have created programs towards closing the gap between deaf and non-disabled people in developing countries. The Quota International organization with headquarters in the United States provided immense educational support in the Philippines, where it started providing free education to deaf children in the Leganes Resource Center for the Deaf. The Sounds Seekers British organization also provided support by offering audiology maintenance technology, to better assist those who are deaf in hard-to-reach places. The Nippon Foundation also supports deaf students at Gallaudet University and the National Technical Institute for the Deaf, through sponsoring international scholarships programs to encourage students to become future leaders in the deaf community. The more aid these organizations give to the deaf people, the more opportunities and resources disabled people must speak up about their struggles and goals that they aim to achieve. When more people understand how to leverage their privilege for the marginalized groups in the community, then we can build a more inclusive and tolerant environment for the generations that are yet to come.[2][3]
The first known record of sign language in history comes from Plato's Cratylus, written in the fifth century BCE. In a dialogue on the "correctness of names", Socrates says, "Suppose that we had no voice or tongue, and wanted to communicate with one another, should we not, like the deaf and dumb, make signs with the hands and head and the rest of the body?"[13] His belief that deaf people possessed an innate intelligence for language put him at odds with his student Aristotle, who said, "Those who are born deaf all become senseless and incapable of reason," and that "it is impossible to reason without the ability to hear".
This pronouncement would reverberate through the ages and it was not until the 17th century when manual alphabets began to emerge, as did various treatises on deaf education, such as Reducción de las letras y arte para enseñar a hablar a los mudos ('Reduction of letters and art for teaching mute people to speak'), written by Juan Pablo Bonet in Madrid in 1620, and Didascalocophus, or, The deaf and dumb mans tutor, written by George Dalgarno in 1680.
In 1760, French philanthropic educator Charles-Michel de l'Épée opened the world's first free school for the deaf. The school won approval for government funding in 1791 and became known as the "Institution Nationale des Sourds-Muets à Paris."[14] The school inspired the opening of what is today known as the American School for the Deaf, the oldest permanent school for the deaf in the United States, and indirectly, Gallaudet University, the world's first school for the advanced education of the deaf and hard of hearing, and to date, the only higher education institution in which all programs and services are specifically designed to accommodate deaf and hard of hearing students.




Tuberculosis (TB) is an infectious disease usually caused by Mycobacterium tuberculosis (MTB) bacteria.[1] Tuberculosis generally affects the lungs, but it can also affect other parts of the body.[1] Most infections show no symptoms, in which case it is known as latent tuberculosis.[1] Around 10% of latent infections progress to active disease which, if left untreated, kill about half of those affected.[1] Typical symptoms of active TB are chronic cough with blood-containing mucus, fever, night sweats, and weight loss.[1] It was historically referred to as consumption due to the weight loss associated with the disease.[8] Infection of other organs can cause a wide range of symptoms.[9]
Tuberculosis is spread from one person to the next through the air when people who have active TB in their lungs cough, spit, speak, or sneeze.[1][10] People with latent TB do not spread the disease.[1] Active infection occurs more often in people with HIV/AIDS and in those who smoke.[1] Diagnosis of active TB is based on chest X-rays, as well as microscopic examination and culture of body fluids.[11] Diagnosis of Latent TB relies on the tuberculin skin test (TST) or blood tests.[11]
Prevention of TB involves screening those at high risk, early detection and treatment of cases, and vaccination with the bacillus Calmette-Guérin (BCG) vaccine.[3][4][5] Those at high risk include household, workplace, and social contacts of people with active TB.[4] Treatment requires the use of multiple antibiotics over a long period of time.[1] Antibiotic resistance is a growing problem, with increasing rates of multiple drug-resistant tuberculosis (MDR-TB).[1]
In 2018, one quarter of the world's population was thought to have a latent infection of TB.[6] New infections occur in about 1% of the population each year.[12] In 2020, an estimated 10 million people developed active TB, resulting in 1.5 million deaths, making it the second leading cause of death from an infectious disease after COVID-19.[13] As of 2018, most TB cases occurred in the regions of South-East Asia (44%), Africa (24%), and the Western Pacific (18%), with more than 50% of cases being diagnosed in seven countries: India (27%), China (9%), Indonesia (8%), the Philippines (6%), Pakistan (6%), Nigeria (4%), and Bangladesh (4%).[14] By 2021, the number of new cases each year was decreasing by around 2% annually.[13][1] About 80% of people in many Asian and African countries test positive, while 5–10% of people in the United States test positive via the tuberculin test.[15] Tuberculosis has been present in humans since ancient times.[16]
Tuberculosis may infect any part of the body, but most commonly occurs in the lungs (known as pulmonary tuberculosis).[9] Extrapulmonary TB occurs when tuberculosis develops outside of the lungs, although extrapulmonary TB may coexist with pulmonary TB.[9]
General signs and symptoms include fever, chills, night sweats, loss of appetite, weight loss, and fatigue.[9] Significant nail clubbing may also occur.[18]
If a tuberculosis infection does become active, it most commonly involves the lungs (in about 90% of cases).[16][19] Symptoms may include chest pain and a prolonged cough producing sputum. About 25% of people may not have any symptoms (i.e., they remain asymptomatic).[16] Occasionally, people may cough up blood in small amounts, and in very rare cases, the infection may erode into the pulmonary artery or a Rasmussen's aneurysm, resulting in massive bleeding.[9][20] Tuberculosis may become a chronic illness and cause extensive scarring in the upper lobes of the lungs. The upper lung lobes are more frequently affected by tuberculosis than the lower ones.[9] The reason for this difference is not clear.[15] It may be due to either better air flow,[15] or poor lymph drainage within the upper lungs.[9]
In 15–20% of active cases, the infection spreads outside the lungs, causing other kinds of TB.[21] These are collectively denoted as extrapulmonary tuberculosis.[22] Extrapulmonary TB occurs more commonly in people with a weakened immune system and young children. In those with HIV, this occurs in more than 50% of cases.[22] Notable extrapulmonary infection sites include the pleura (in tuberculous pleurisy), the central nervous system (in tuberculous meningitis), the lymphatic system (in scrofula of the neck), the genitourinary system (in urogenital tuberculosis), and the bones and joints (in Pott disease of the spine), among others. A potentially more serious, widespread form of TB is called "disseminated tuberculosis", it is also known as miliary tuberculosis.[9] Miliary TB currently makes up about 10% of extrapulmonary cases.[23]
The main cause of TB is Mycobacterium tuberculosis (MTB), a small, aerobic, nonmotile bacillus.[9] The high lipid content of this pathogen accounts for many of its unique clinical characteristics.[24] It divides every 16 to 20 hours, which is an extremely slow rate compared with other bacteria, which usually divide in less than an hour.[25] Mycobacteria have an outer membrane lipid bilayer.[26] If a Gram stain is performed, MTB either stains very weakly "Gram-positive" or does not retain dye as a result of the high lipid and mycolic acid content of its cell wall.[27] MTB can withstand weak disinfectants and survive in a dry state for weeks. In nature, the bacterium can grow only within the cells of a host organism, but M. tuberculosis can be cultured in the laboratory.[28]
Using histological stains on expectorated samples from phlegm (also called sputum), scientists can identify MTB under a microscope. Since MTB retains certain stains even after being treated with acidic solution, it is classified as an acid-fast bacillus.[15][27] The most common acid-fast staining techniques are the Ziehl–Neelsen stain[29] and the Kinyoun stain, which dye acid-fast bacilli a bright red that stands out against a blue background.[30] Auramine-rhodamine staining[31] and fluorescence microscopy[32] are also used.
The M. tuberculosis complex (MTBC) includes four other TB-causing mycobacteria: M. bovis, M. africanum, M. canetti, and M. microti.[33] M. africanum is not widespread, but it is a significant cause of tuberculosis in parts of Africa.[34][35] M. bovis was once a common cause of tuberculosis, but the introduction of pasteurized milk has almost eliminated this as a public health problem in developed countries.[15][36] M. canetti is rare and seems to be limited to the Horn of Africa, although a few cases have been seen in African emigrants.[37][38] M. microti is also rare and is seen almost only in immunodeficient people, although its prevalence may be significantly underestimated.[39]

Other known pathogenic mycobacteria include M. leprae, M. avium, and M. kansasii. The latter two species are classified as "nontuberculous mycobacteria" (NTM) or atypical mycobacteria. NTM cause neither TB nor leprosy, but they do cause lung diseases that resemble TB.[40]When people with active pulmonary TB cough, sneeze, speak, sing, or spit, they expel infectious aerosol droplets 0.5 to 5.0 µm in diameter. A single sneeze can release up to 40,000 droplets.[41] Each one of these droplets may transmit the disease, since the infectious dose of tuberculosis is very small (the inhalation of fewer than 10 bacteria may cause an infection).[42]
People with prolonged, frequent, or close contact with people with TB are at particularly high risk of becoming infected, with an estimated 22% infection rate.[43] A person with active but untreated tuberculosis may infect 10–15 (or more) other people per year.[44] Transmission should occur from only people with active TB – those with latent infection are not thought to be contagious.[15] The probability of transmission from one person to another depends upon several factors, including the number of infectious droplets expelled by the carrier, the effectiveness of ventilation, the duration of exposure, the virulence of the M. tuberculosis strain, the level of immunity in the uninfected person, and others.[45] The cascade of person-to-person spread can be circumvented by segregating those with active ("overt") TB and putting them on anti-TB drug regimens. After about two weeks of effective treatment, subjects with nonresistant active infections generally do not remain contagious to others.[43] If someone does become infected, it typically takes three to four weeks before the newly infected person becomes infectious enough to transmit the disease to others.[46]
A number of factors make individuals more susceptible to TB infection and/or disease.[47]
The most important risk factor globally for developing active TB is concurrent HIV infection; 13% of those with TB are also infected with HIV.[48] This is a particular problem in sub-Saharan Africa, where HIV infection rates are high.[49][50] Of those without HIV infection who are infected with tuberculosis, about 5–10% develop active disease during their lifetimes;[18] in contrast, 30% of those co-infected with HIV develop the active disease.[18]
Use of certain medications, such as corticosteroids and infliximab (an anti-αTNF monoclonal antibody), is another important risk factor, especially in the developed world.[16]
Other risk factors include: alcoholism,[16] diabetes mellitus (3-fold increased risk),[51] silicosis (30-fold increased risk),[52] tobacco smoking (2-fold increased risk),[53] indoor air pollution, malnutrition, young age,[47] recently acquired TB infection, recreational drug use, severe kidney disease, low body weight, organ transplant, head and neck cancer,[54] and genetic susceptibility[55] (the overall importance of genetic risk factors remains undefined[16]).
Tobacco smoking increases the risk of infections (in addition to increasing the risk of active disease and death). Additional factors increasing infection susceptibility include young age.[47]
About 90% of those infected with M. tuberculosis have asymptomatic, latent TB infections (sometimes called LTBI),[57] with only a 10% lifetime chance that the latent infection will progress to overt, active tuberculous disease.[58] In those with HIV, the risk of developing active TB increases to nearly 10% a year.[58] If effective treatment is not given, the death rate for active TB cases is up to 66%.[44]
TB infection begins when the mycobacteria reach the alveolar air sacs of the lungs, where they invade and replicate within endosomes of alveolar macrophages.[15][59][60] Macrophages identify the bacterium as foreign and attempt to eliminate it by phagocytosis. During this process, the bacterium is enveloped by the macrophage and stored temporarily in a membrane-bound vesicle called a phagosome. The phagosome then combines with a lysosome to create a phagolysosome. In the phagolysosome, the cell attempts to use reactive oxygen species and acid to kill the bacterium.  However, M. tuberculosis has a thick, waxy mycolic acid capsule that protects it from these toxic substances. M. tuberculosis is able to reproduce inside the macrophage and will eventually kill the immune cell.
The primary site of infection in the lungs, known as the Ghon focus, is generally located in either the upper part of the lower lobe, or the lower part of the upper lobe.[15] Tuberculosis of the lungs may also occur via infection from the blood stream. This is known as a Simon focus and is typically found in the top of the lung.[61] This hematogenous transmission can also spread infection to more distant sites, such as peripheral lymph nodes, the kidneys, the brain, and the bones.[15][62] All parts of the body can be affected by the disease, though for unknown reasons it rarely affects the heart, skeletal muscles, pancreas, or thyroid.[63]
Tuberculosis is classified as one of the granulomatous inflammatory diseases. Macrophages, epithelioid cells, T lymphocytes, B lymphocytes, and fibroblasts  aggregate to form granulomas, with lymphocytes surrounding the infected macrophages. When other macrophages attack the infected macrophage, they fuse together to form a giant multinucleated cell in the alveolar lumen. The granuloma may prevent dissemination of the mycobacteria and provide a local environment for interaction of cells of the immune system.[64] However, more recent evidence suggests that the bacteria use the granulomas to avoid destruction by the host's immune system.  Macrophages and dendritic cells in the granulomas are unable to present antigen to lymphocytes; thus the immune response is suppressed.[65] Bacteria inside the granuloma can become dormant, resulting in latent infection. Another feature of the granulomas is the development of abnormal cell death (necrosis) in the center of tubercles.  To the naked eye, this has the texture of soft, white cheese and is termed caseous necrosis.[64]
If TB bacteria gain entry to the blood stream from an area of damaged tissue, they can spread throughout the body and set up many foci of infection, all appearing as tiny, white tubercles in the tissues.[66] This severe form of TB disease, most common in young children and those with HIV, is called miliary tuberculosis.[67] People with this disseminated TB have a high fatality rate even with treatment (about 30%).[23][68]
In many people, the infection waxes and wanes. Tissue destruction and necrosis are often balanced by healing and fibrosis.[64] Affected tissue is replaced by scarring and cavities filled with caseous necrotic material. During active disease, some of these cavities are joined to the air passages (bronchi) and this material can be coughed up. It contains living bacteria and thus can spread the infection. Treatment with appropriate antibiotics kills bacteria and allows healing to take place. Upon cure, affected areas are eventually replaced by scar tissue.[64]
Diagnosing active tuberculosis based only on signs and symptoms is difficult,[69] as is diagnosing the disease in those who have a weakened immune system.[70] A diagnosis of TB should, however, be considered in those with signs of lung disease or constitutional symptoms lasting longer than two weeks.[70] A chest X-ray and multiple sputum cultures for acid-fast bacilli are typically part of the initial evaluation.[70] Interferon-γ release assays (IGRA) and tuberculin skin tests are of little use in most of the developing world.[71][72] IGRA have similar limitations in those with HIV.[72][73]
A definitive diagnosis of TB is made by identifying M. tuberculosis in a clinical sample (e.g., sputum, pus, or a tissue biopsy). However, the difficult culture process for this slow-growing organism can take two to six weeks for blood or sputum culture.[74] Thus, treatment is often begun before cultures are confirmed.[75]
Nucleic acid amplification tests and adenosine deaminase testing may allow rapid diagnosis of TB.[69] Blood tests to detect antibodies are not specific or sensitive, so they are not recommended.[76]
The Mantoux tuberculin skin test is often used to screen people at high risk for TB.[70] Those who have been previously immunized with the Bacille Calmette-Guerin vaccine may have a false-positive test result.[77] The test may be falsely negative in those with sarcoidosis, Hodgkin's lymphoma, malnutrition, and most notably, active tuberculosis.[15] Interferon gamma release assays, on a blood sample, are recommended in those who are positive to the Mantoux test.[75] These are not affected by immunization or most environmental mycobacteria, so they generate fewer false-positive results.[78] However, they are affected by M. szulgai, M. marinum, and M. kansasii.[79] IGRAs may increase sensitivity when used in addition to the skin test, but may be less sensitive than the skin test when used alone.[80]
The US Preventive Services Task Force (USPSTF) has recommended screening people who are at high risk for latent tuberculosis with either tuberculin skin tests or interferon-gamma release assays.[81] While some have recommend testing health care workers, evidence of benefit for this is poor as of 2019[update].[82] The Centers for Disease Control and Prevention (CDC) stopped recommending yearly testing of health care workers without known exposure in 2019.[83]
Tuberculosis prevention and control efforts rely primarily on the vaccination of infants and the detection and appropriate treatment of active cases.[16] The World Health Organization (WHO) has achieved some success with improved treatment regimens, and a small decrease in case numbers.[16] Some countries have legislation to involuntarily detain or examine those suspected to have tuberculosis, or involuntarily treat them if infected.[84]
The only available vaccine as of 2021[update] is bacillus Calmette-Guérin (BCG).[85][86] In children it decreases the risk of getting the infection by 20% and the risk of infection turning into active disease by nearly 60%.[87]
It is the most widely used vaccine worldwide, with more than 90% of all children being vaccinated.[16] The immunity it induces decreases after about ten years.[16] As tuberculosis is uncommon in most of Canada, Western Europe, and the United States, BCG is administered to only those people at high risk.[88][89][90] Part of the reasoning against the use of the vaccine is that it makes the tuberculin skin test falsely positive, reducing the test's usefulness as a screening tool.[90] Several vaccines are being developed.[16]
Intradermal MVA85A vaccine in addition to BCG injection is not effective in preventing tuberculosis.[91]
Public health campaigns which have focused on overcrowding, public spitting and regular sanitation (including hand washing) during the 1800s helped to either interrupt or slow spread which when combined with contact tracing, isolation and treatment helped to dramatically curb the transmission of both tuberculosis and other airborne diseases which led to the elimination of tuberculosis as a major public health issue in most developed economies.[92][93] Other risk factors which worsened TB spread such as malnutrition were also ameliorated, but since the emergence of HIV a new population of immunocompromised individuals was available for TB to infect.
The World Health Organization (WHO) declared TB a "global health emergency" in 1993,[16] and in 2006, the Stop TB Partnership developed a Global Plan to Stop Tuberculosis that aimed to save 14 million lives between its launch and 2015.[94] A number of targets they set were not achieved by 2015, mostly due to the increase in HIV-associated tuberculosis and the emergence of multiple drug-resistant tuberculosis.[16] A tuberculosis classification system developed by the American Thoracic Society is used primarily in public health programs.[95] In 2015, it launched the End TB Strategy to reduce deaths by 95% and incidence by 90% before 2035. The goal of tuberculosis elimination is hampered by the lack of rapid testing, of short and effective treatment courses, and of completely effective vaccines.[96]
The benefits and risks of giving anti-tubercular drugs in those exposed to MDR-TB is unclear.[97] Making HAART therapy available to HIV-positive individuals significantly reduces the risk of progression to an active TB infection by up to 90% and can mitigate the spread through this population.[98]
Treatment of TB uses antibiotics to kill the bacteria. Effective TB treatment is difficult, due to the unusual structure and chemical composition of the mycobacterial cell wall, which hinders the entry of drugs and makes many antibiotics ineffective.[99]
Active TB is best treated with combinations of several antibiotics to reduce the risk of the bacteria developing antibiotic resistance.[16] The routine use of rifabutin instead of rifampicin in HIV-positive people with tuberculosis is of unclear benefit as of 2007[update].[100]
Latent TB is treated with either isoniazid or rifampin alone, or a combination of isoniazid with either rifampicin or rifapentine.[101][102][103]
The treatment takes three to nine months depending on the medications used.[45][101][104][103] People with latent infections are treated to prevent them from progressing to active TB disease later in life.[105]
Education or counselling may improve the latent tuberculosis treatment completion rates.[106]
The recommended treatment of new-onset pulmonary tuberculosis, as of 2010[update], is six months of a combination of antibiotics containing rifampicin, isoniazid, pyrazinamide, and ethambutol for the first two months, and only rifampicin and isoniazid for the last four months.[16] Where resistance to isoniazid is high, ethambutol may be added for the last four months as an alternative.[16] Treatment with anti-TB drugs for at least 6 months results in higher success rates when compared with treatment less than 6 months, even though the difference is small. Shorter treatment regimen may be recommended for those with compliance issues.[107] There is also no evidence to support shorter anti-tuberculosis treatment regimens when compared to a 6-month treatment regimen.[108] However recently, results from an international, randomized, controlled clinical trial indicate that a four-month daily treatment regimen containing high-dose, or "optimized", rifapentine with moxifloxacin (2PHZM/2PHM) is as safe and effective as the existing standard six-month daily regimen at curing drug-susceptible tuberculosis (TB) disease.[109]
If tuberculosis recurs, testing to determine which antibiotics it is sensitive to is important before determining treatment.[16] If multiple drug-resistant TB (MDR-TB) is detected, treatment with at least four effective antibiotics for 18 to 24 months is recommended.[16]
Directly observed therapy, i.e., having a health care provider watch the person take their medications, is recommended by the World Health Organization (WHO) in an effort to reduce the number of people not appropriately taking antibiotics.[110] The evidence to support this practice over people simply taking their medications independently is of poor quality.[111] There is no strong evidence indicating that directly observed therapy improves the number of people who were cured or the number of people who complete their medicine.[111] Moderate quality evidence suggests that there is also no difference if people are observed at home versus at a clinic, or by a family member versus a health care worker.[111] Methods to remind people of the importance of treatment and appointments may result in a small but important improvement.[112] There is also not enough evidence to support intermittent rifampicin-containing therapy given two to three times a week has equal effectiveness as daily dose regimen on improving cure rates and reducing relapsing rates.[113] There is also not enough evidence on effectiveness of giving intermittent twice or thrice weekly short course regimen compared to daily dosing regimen in treating children with tuberculosis.[114]
Primary resistance occurs when a person becomes infected with a resistant strain of TB. A person with fully susceptible MTB may develop secondary (acquired) resistance during therapy because of inadequate treatment, not taking the prescribed regimen appropriately (lack of compliance), or using low-quality medication.[115] Drug-resistant TB is a serious public health issue in many developing countries, as its treatment is longer and requires more expensive drugs. MDR-TB is defined as resistance to the two most effective first-line TB drugs: rifampicin and isoniazid. Extensively drug-resistant TB is also resistant to three or more of the six classes of second-line drugs.[116] Totally drug-resistant TB is resistant to all currently used drugs.[117] It was first observed in 2003 in Italy,[118] but not widely reported until 2012,[117][119] and has also been found in Iran and India.[120] There is some efficacy for linezolid to treat those with XDR-TB but side effects and discontinuation of medications were common.[121][122] Bedaquiline is tentatively supported for use in multiple drug-resistant TB.[123]
XDR-TB is a term sometimes used to define extensively resistant TB, and constitutes one in ten cases of MDR-TB. Cases of XDR TB have been identified in more than 90% of countries.[120]
For those with known rifampicin or MDR-TB, molecular tests such as the Genotype MTBDRsl Assay (performed on culture isolates or smear positive specimens) may be useful to detect second-line anti-tubercular drug resistance.[124][125]
Progression from TB infection to overt TB disease occurs when the bacilli overcome the immune system defenses and begin to multiply. In primary TB disease (some 1–5% of cases), this occurs soon after the initial infection.[15] However, in the majority of cases, a latent infection occurs with no obvious symptoms.[15] These dormant bacilli produce active tuberculosis in 5–10% of these latent cases, often many years after infection.[18]
The risk of reactivation increases with immunosuppression, such as that caused by infection with HIV. In people coinfected with M. tuberculosis and HIV, the risk of reactivation increases to 10% per year.[15] Studies using DNA fingerprinting of M. tuberculosis strains have shown reinfection contributes more substantially to recurrent TB than previously thought,[127] with estimates that it might account for more than 50% of reactivated cases in areas where TB is common.[128] The chance of death from a case of tuberculosis is about 4% as of 2008[update], down from 8% in 1995.[16]
In people with smear-positive pulmonary TB (without HIV co-infection), after 5 years without treatment, 50-60% die while 20-25% achieve spontaneous resolution (cure). TB is almost always fatal in those with untreated HIV co-infection and death rates are increased even with antiretroviral treatment of HIV.[129]
Roughly one-quarter of the world's population has been infected with M. tuberculosis,[6] with new infections occurring in about 1% of the population each year.[12] However, most infections with M. tuberculosis do not cause disease,[130] and 90–95% of infections remain asymptomatic.[57] In 2012, an estimated 8.6 million chronic cases were active.[131] In 2010, 8.8 million new cases of tuberculosis were diagnosed, and 1.20–1.45 million deaths occurred (most of these occurring in developing countries).[48][132] Of these, about 0.35 million occur in those also infected with HIV.[133] In 2018, tuberculosis was the leading cause of death worldwide from a single infectious agent.[134] The total number of tuberculosis cases has been decreasing since 2005, while new cases have decreased since 2002.[48]
Tuberculosis[clarification needed] incidence is seasonal, with peaks occurring every spring and summer.[135][136][137][138] The reasons for this are unclear, but may be related to vitamin D deficiency during the winter.[138][139] There are also studies linking tuberculosis to different weather conditions like low temperature, low humidity and low rainfall. It has been suggested that tuberculosis incidence rates may be connected to climate change.[140]
Tuberculosis is closely linked to both overcrowding and malnutrition, making it one of the principal diseases of poverty.[16] Those at high risk thus include: people who inject illicit drugs, inhabitants and employees of locales where vulnerable people gather (e.g., prisons and homeless shelters), medically underprivileged and resource-poor communities, high-risk ethnic minorities, children in close contact with high-risk category patients, and health-care providers serving these patients.[141]
The rate of tuberculosis varies with age. In Africa, it primarily affects adolescents and young adults.[142] However, in countries where incidence rates have declined dramatically (such as the United States), tuberculosis is mainly a disease of the elderly and immunocompromised (risk factors are listed above).[15][143] Worldwide, 22 "high-burden" states or countries together experience 80% of cases as well as 83% of deaths.[120]
In Canada and Australia, tuberculosis is many times more common among the Indigenous peoples, especially in remote areas.[144][145] Factors contributing to this include higher prevalence of predisposing health conditions and behaviours, and overcrowding and poverty. In some Canadian Indigenous groups, genetic susceptibility may play a role.[47]
Socioeconomic status (SES) strongly affects TB risk. People of low SES are both more likely to contract TB and to be more severely affected by the disease. Those with low SES are more likely to be affected by risk factors for developing TB (e.g., malnutrition, indoor air pollution, HIV co-infection, etc.), and are additionally more likely to be exposed to crowded and poorly ventilated spaces. Inadequate healthcare also means that people with active disease who facilitate spread are not diagnosed and treated promptly; sick people thus remain in the infectious state and (continue to) spread the infection.[47]
The distribution of tuberculosis is not uniform across the globe; about 80% of the population in many African, Caribbean, South Asian, and eastern European countries test positive in tuberculin tests, while only 5–10% of the U.S. population test positive.[15] Hopes of totally controlling the disease have been dramatically dampened because of many factors, including the difficulty of developing an effective vaccine, the expensive and time-consuming diagnostic process, the necessity of many months of treatment, the increase in HIV-associated tuberculosis, and the emergence of drug-resistant cases in the 1980s.[16]
In developed countries, tuberculosis is less common and is found mainly in urban areas. In Europe, deaths from TB fell from 500 out of 100,000 in 1850 to 50 out of 100,000 by 1950. Improvements in public health were reducing tuberculosis even before the arrival of antibiotics, although the disease remained a significant threat to public health, such that when the Medical Research Council was formed in Britain in 1913 its initial focus was tuberculosis research.[146]
In 2010, rates per 100,000 people in different areas of the world were: globally 178, Africa 332, the Americas 36, Eastern Mediterranean 173, Europe 63, Southeast Asia 278, and Western Pacific 139.[133]
Russia has achieved particularly dramatic progress with a decline in its TB mortality rate—from 61.9 per 100,000 in 1965 to 2.7 per 100,000 in 1993;[147][148] however, mortality rate increased to 24 per 100,000 in 2005 and then recoiled to 11 per 100,000 by 2015.[149]
China has achieved particularly dramatic progress, with about an 80% reduction in its TB mortality rate between 1990 and 2010.[133] The number of new cases has declined by 17% between 2004 and 2014.[120]
In 2007, the country with the highest estimated incidence rate of TB was Eswatini, with 1,200 cases per 100,000 people. In 2017, the country with the highest estimated incidence rate as a % of the population was Lesotho, with 665 cases per 100,000 people.[150]
As of 2017, India had the largest total incidence, with an estimated 2,740,000 cases.[150] According to the World Health Organization (WHO), in 2000–2015, India's estimated mortality rate dropped from 55 to 36 per 100,000 population per year with estimated 480 thousand people died of TB in 2015.[151][152] In India a major proportion of tuberculosis patients are being treated by private partners and private hospitals. Evidence indicates that the tuberculosis national survey does not represent the number of cases that are diagnosed and recorded by private clinics and hospitals in India.[153]
In the United States, Native Americans have a fivefold greater mortality from TB,[154] and racial and ethnic minorities accounted for 84% of all reported TB cases.[155]
In the United States, the overall tuberculosis case rate was 3 per 100,000 persons in 2017.[150] In Canada, tuberculosis is still endemic in some rural areas.[156]
In 2017, in the United Kingdom, the national average was 9 per 100,000 and the highest incidence rates in Western Europe were 20 per 100,000 in Portugal.
Number of new cases of tuberculosis per 100,000 people in 2016[157]
Tuberculosis deaths per million persons in 2012
Tuberculosis deaths by region, 1990 to 2017[158]
Tuberculosis has existed since antiquity.[16] The oldest unambiguously detected M. tuberculosis gives evidence of the disease in the remains of bison in Wyoming dated to around 17,000 years ago.[159] However, whether tuberculosis originated in bovines, then transferred to humans, or whether both bovine and human tuberculosis diverged from a common ancestor, remains unclear.[160] A comparison of the genes of M. tuberculosis complex (MTBC) in humans to MTBC in animals suggests humans did not acquire MTBC from animals during animal domestication, as researchers previously believed. Both strains of the tuberculosis bacteria share a common ancestor, which could have infected humans even before the Neolithic Revolution.[161] Skeletal remains show some prehistoric humans (4000 BC) had TB, and researchers have found tubercular decay in the spines of Egyptian mummies dating from 3000 to 2400 BC.[162] Genetic studies suggest the presence of TB in the Americas from about AD 100.[163]
Before the Industrial Revolution, folklore often associated tuberculosis with vampires. When one member of a family died from the disease, the other infected members would lose their health slowly. People believed this was caused by the original person with TB draining the life from the other family members.[164]
Although Richard Morton established the pulmonary form associated with tubercles as a pathology in 1689,[165][166] due to the variety of its symptoms, TB was not identified as a single disease until the 1820s.  Benjamin Marten conjectured in 1720 that consumptions were caused by microbes which were spread by people living close to each other.[167] In 1819, René Laennec claimed that tubercles were the cause of pulmonary tuberculosis.[168] J. L. Schönlein first published the name "tuberculosis" (German:  Tuberkulose) in 1832.[169][170] Between 1838 and 1845, John Croghan, the owner of Mammoth Cave in Kentucky from 1839 onwards, brought a number of people with tuberculosis into the cave in the hope of curing the disease with the constant temperature and purity of the cave air; each died within a year.[171] Hermann Brehmer opened the first TB sanatorium in 1859 in Görbersdorf (now Sokołowsko) in Silesia.[172] In 1865, Jean Antoine Villemin demonstrated that tuberculosis could be transmitted, via inoculation, from humans to animals and among animals.[173] (Villemin's findings were confirmed in 1867 and 1868 by John Burdon-Sanderson.[174])
Robert Koch identified and described the bacillus causing tuberculosis, M. tuberculosis, on 24 March 1882.[175][176] In 1905, he was awarded the Nobel Prize in Physiology or Medicine for this discovery.[177] Koch did not believe the cattle and human tuberculosis diseases were similar, which delayed the recognition of infected milk as a source of infection. During the first half of the 1900s, the risk of transmission from this source was dramatically reduced after the application of the pasteurization process. Koch announced a glycerine extract of the tubercle bacilli as a "remedy" for tuberculosis in 1890, calling it "tuberculin". Although it was not effective, it was later successfully adapted as a screening test for the presence of pre-symptomatic tuberculosis.[178] World Tuberculosis Day is marked on 24 March each year, the anniversary of Koch's original scientific announcement.
Albert Calmette and Camille Guérin achieved the first genuine success in immunization against tuberculosis in 1906, using attenuated bovine-strain tuberculosis. It was called bacille Calmette–Guérin (BCG). The BCG vaccine was first used on humans in 1921 in France,[179] but achieved widespread acceptance in the US, Great Britain, and Germany only after World War II.[180]
Tuberculosis caused widespread public concern in the 19th and early 20th centuries as the disease became common among the urban poor. In 1815, one in four deaths in England was due to "consumption". By 1918, TB still caused one in six deaths in France.[citation needed] After TB was determined to be contagious, in the 1880s, it was put on a notifiable-disease list in Britain; campaigns started to stop people from spitting in public places, and the infected poor were "encouraged" to enter sanatoria that resembled prisons (the sanatoria for the middle and upper classes offered excellent care and constant medical attention).[172] Whatever the benefits of the "fresh air" and labor in the sanatoria, even under the best conditions, 50% of those who entered died within five years (c. 1916).[172] When the Medical Research Council formed in Britain in 1913, it initially focused on tuberculosis research.[181]
In Europe, rates of tuberculosis began to rise in the early 1600s to a peak level in the 1800s, when it caused nearly 25% of all deaths.[182] In the 18th and 19th century, tuberculosis had become epidemic in Europe, showing a seasonal pattern.[183][184] By the 1950s mortality in Europe had decreased about 90%.[185] Improvements in sanitation, vaccination, and other public-health measures began significantly reducing rates of tuberculosis even before the arrival of streptomycin and other antibiotics, although the disease remained a significant threat.[185] In 1946, the development of the antibiotic streptomycin made effective treatment and cure of TB a reality. Prior to the introduction of this medication, the only treatment was surgical intervention, including the "pneumothorax technique", which involved collapsing an infected lung to "rest" it and to allow tuberculous lesions to heal.[186]
Because of the emergence of multidrug-resistant tuberculosis (MDR-TB), surgery has been re-introduced for certain cases of TB infections. It involves the removal of infected chest cavities ("bullae") in the lungs to reduce the number of bacteria and to increase exposure of the remaining bacteria to antibiotics in the bloodstream.[187] Hopes of eliminating TB ended with the rise of drug-resistant strains in the 1980s. The subsequent resurgence of tuberculosis resulted in the declaration of a global health emergency by the World Health Organization (WHO) in 1993.[188]
Tuberculosis has been known by many names from the technical to the familiar.[189] Phthisis (Φθισις) is a Greek word for consumption, an old term for pulmonary tuberculosis;[8] around 460 BCE, Hippocrates described phthisis as a disease of dry seasons.[190] The abbreviation TB is short for tubercle bacillus. Consumption was the most common nineteenth century English word for the disease, and was also in use well into the twentieth century. The Latin root con meaning 'completely' is linked to sumere meaning 'to take up from under'.[191] In The Life and Death of Mr Badman by John Bunyan, the author calls consumption "the captain of all these men of death."[192] "Great white plague" has also been used.[189]
Tuberculosis was for centuries associated with poetic and artistic qualities among those infected, and was also known as "the romantic disease".[189][193] Major artistic figures such as the poets John Keats, Percy Bysshe Shelley, and Edgar Allan Poe, the composer Frédéric Chopin,[194] the playwright Anton Chekhov, the novelists Franz Kafka, Katherine Mansfield,[195] Charlotte Brontë, Fyodor Dostoevsky, Thomas Mann, W. Somerset Maugham,[196] George Orwell,[197] and Robert Louis Stevenson, and the artists Alice Neel,[198] Jean-Antoine Watteau, Elizabeth Siddal, Marie Bashkirtseff, Edvard Munch, Aubrey Beardsley and Amedeo Modigliani either had the disease or were surrounded by people who did. A widespread belief was that tuberculosis assisted artistic talent. Physical mechanisms proposed for this effect included the slight fever and toxaemia that it caused, allegedly helping them to see life more clearly and to act decisively.[199][200][201]
Tuberculosis formed an often-reused theme in literature, as in Thomas Mann's The Magic Mountain, set in a sanatorium;[202] in music, as in Van Morrison's song "T.B. Sheets";[203] in opera, as in Puccini's La bohème and Verdi's La Traviata;[201] in art, as in Monet's painting of his first wife Camille on her deathbed;[204] and in film, such as the 1945 The Bells of St. Mary's starring Ingrid Bergman as a nun with tuberculosis.[205]
In 2014, the WHO adopted the "End TB" strategy which aims to reduce TB incidence by 80% and TB deaths by 90% by 2030.[206] The strategy contains a milestone to reduce TB incidence by 20% and TB deaths by 35% by 2020.[207] However, by 2020 only a 9% reduction in incidence per population was achieved globally, with the European region achieving 19% and the African region achieving 16% reductions.[207] Similarly, the number of deaths only fell by 14%, missing the 2020 milestone of a 35% reduction, with some regions making better progress (31% reduction in Europe and 19% in Africa).[207] Correspondingly, also treatment, prevention and funding milestones were missed in 2020, for example only 6.3 million people were started on TB prevention short of the target of 30 million.[207]
The World Health Organization (WHO), the Bill and Melinda Gates Foundation, and the U.S. government are subsidizing a fast-acting diagnostic tuberculosis test for use in low- and middle-income countries as of 2012.[208][209][210] In addition to being fast-acting, the test can determine if there is resistance to the antibiotic rifampicin which may indicate multi-drug resistant tuberculosis and is accurate in those who are also infected with HIV.[208][211] Many resource-poor places as of 2011[update] have access to only sputum microscopy.[212]
India had the highest total number of TB cases worldwide in 2010, in part due to poor disease management within the private and public health care sector.[213] Programs such as the Revised National Tuberculosis Control Program are working to reduce TB levels among people receiving public health care.[214][215]
A 2014 EIU-healthcare report finds there is a need to address apathy and urges for increased funding. The report cites among others Lucica Ditui "[TB] is like an orphan. It has been neglected even in countries with a high burden and often forgotten by donors and those investing in health interventions."[120]
Slow progress has led to frustration, expressed by the executive director of the Global Fund to Fight AIDS, Tuberculosis and Malaria – Mark Dybul: "we have the tools to end TB as a pandemic and public health threat on the planet, but we are not doing it."[120] Several international organizations are pushing for more transparency in treatment, and more countries are implementing mandatory reporting of cases to the government as of 2014, although adherence is often variable. Commercial treatment providers may at times overprescribe second-line drugs as well as supplementary treatment, promoting demands for further regulations.[120] The government of Brazil provides universal TB care, which reduces this problem.[120] Conversely, falling rates of TB infection may not relate to the number of programs directed at reducing infection rates but may be tied to an increased level of education, income, and health of the population.[120] Costs of the disease, as calculated by the World Bank in 2009 may exceed US$150 billion per year in "high burden" countries.[120] Lack of progress eradicating the disease may also be due to lack of patient follow-up – as among the 250 million rural migrants in China.[120]
There is insufficient data to show that active contact tracing helps to improve case detection rates for tuberculosis.[216] Interventions such as house-to-house visits, educational leaflets, mass media strategies, educational sessions may increase tuberculosis detection rates in short-term.[217] There is no study that compares new methods of contact tracing such as social network analysis with existing contact tracing methods.[218]
Slow progress in preventing the disease may in part be due to stigma associated with TB.[120] Stigma may be due to the fear of transmission from affected individuals. This stigma may additionally arise due to links between TB and poverty, and in Africa, AIDS.[120] Such stigmatization may be both real and perceived; for example, in Ghana, individuals with TB are banned from attending public gatherings.[219]
Stigma towards TB may result in delays in seeking treatment,[120] lower treatment compliance, and family members keeping cause of death secret[219] – allowing the disease to spread further.[120] In contrast, in Russia stigma was associated with increased treatment compliance.[219] TB stigma also affects socially marginalized individuals to a greater degree and varies between regions.[219]
One way to decrease stigma may be through the promotion of "TB clubs", where those infected may share experiences and offer support, or through counseling.[219] Some studies have shown TB education programs to be effective in decreasing stigma, and may thus be effective in increasing treatment adherence.[219] Despite this, studies on the relationship between reduced stigma and mortality are lacking as of 2010[update], and similar efforts to decrease stigma surrounding AIDS have been minimally effective.[219] Some have claimed the stigma to be worse than the disease, and healthcare providers may unintentionally reinforce stigma, as those with TB are often perceived as difficult or otherwise undesirable.[120] A greater understanding of the social and cultural dimensions of tuberculosis may also help with stigma reduction.[220]
The BCG vaccine has limitations, and research to develop new TB vaccines is ongoing.[221] A number of potential candidates are currently in phase I and II clinical trials.[221][222] Two main approaches are used to attempt to improve the efficacy of available vaccines. One approach involves adding a subunit vaccine to BCG, while the other strategy is attempting to create new and better live vaccines.[221] MVA85A, an example of a subunit vaccine, is in trials in South Africa as of 2006, is based on a genetically modified vaccinia virus.[223] Vaccines are hoped to play a significant role in treatment of both latent and active disease.[224]
To encourage further discovery, researchers and policymakers are promoting new economic models of vaccine development as of 2006, including prizes, tax incentives, and advance market commitments.[225][226] A number of groups, including the Stop TB Partnership,[227] the South African Tuberculosis Vaccine Initiative, and the Aeras Global TB Vaccine Foundation, are involved with research.[228] Among these, the Aeras Global TB Vaccine Foundation received a gift of more than $280 million (US) from the Bill and Melinda Gates Foundation to develop and license an improved vaccine against tuberculosis for use in high burden countries.[229][230]
A number of medications are being studied as of 2012 for multidrug-resistant tuberculosis, including bedaquiline and delamanid.[231] Bedaquiline received U.S. Food and Drug Administration (FDA) approval in late 2012.[232] The safety and effectiveness of these new agents are uncertain as of 2012, because they are based on the results of relatively small studies.[231][233] However, existing data suggest that patients taking bedaquiline in addition to standard TB therapy are five times more likely to die than those without the new drug,[234] which has resulted in medical journal articles raising health policy questions about why the FDA approved the drug and whether financial ties to the company making bedaquiline influenced physicians' support for its use.[233][235]
Steroids add-on therapy has not shown any benefits for active pulmonary tuberculosis infection.[236]
Mycobacteria infect many different animals, including birds,[237] fish, rodents,[238] and reptiles.[239] The subspecies Mycobacterium tuberculosis, though, is rarely present in wild animals.[240] An effort to eradicate bovine tuberculosis caused by Mycobacterium bovis from the cattle and deer herds of New Zealand has been relatively successful.[241] Efforts in Great Britain have been less successful.[242][243]
As of 2015[update], tuberculosis appears to be widespread among captive elephants in the US. It is believed that the animals originally acquired the disease from humans, a process called reverse zoonosis. Because the disease can spread through the air to infect both humans and other animals, it is a public health concern affecting circuses and zoos.[244][245]






A mammal (from Latin  mamma 'breast')[1] is a vertebrate animal of the class Mammalia (/məˈmeɪli.ə/). Mammals are characterized by the presence of milk-producing mammary glands for feeding their young, a neocortex region of the brain, fur or hair, and three middle ear bones. These characteristics distinguish them from reptiles and birds, from which their ancestors diverged in the Carboniferous Period over 300 million years ago. Around 6,400 extant species of mammals have been described and divided into 29 orders.
The largest orders of mammals, by number of species, are the rodents, bats, and Eulipotyphla (including hedgehogs, moles and shrews). The next three are the Primates (including humans, monkeys and lemurs), the even-toed ungulates (including pigs, camels, and whales), and the Carnivora (including cats, dogs, and seals).
Mammals are the only living members of Synapsida; this clade, together with Sauropsida (reptiles and birds), constitutes the larger Amniota clade. The early synapsids were sphenacodonts, a group that included the famous Dimetrodon. The synapsids split into several diverse groups of non-mammalian synapsids—traditionally and incorrectly referred to as mammal-like reptiles or by the term pelycosaurs, and now known as stem mammals or protomammals—before giving rise to therapsids during the beginning of the Middle Permian period. Mammals originated from cynodonts, an advanced group of therapsids, during the Late Triassic-Early Jurassic. The modern mammalian orders arose in the Paleogene and Neogene periods of the Cenozoic era, after the extinction of non-avian dinosaurs, and have been the dominant terrestrial animal group from 66 million years ago to the present.
The basic mammalian body type is quadruped, and most mammals use their four extremities for terrestrial locomotion; but in some, the extremities are adapted for life at sea, in the air, in trees, underground, or on two legs. Mammals range in size from the 30–40 mm (1.2–1.6 in) bumblebee bat to the 30 m (98 ft) blue whale—possibly the largest animal to have ever lived. Maximum lifespan varies from two years for the shrew to 211 years for the bowhead whale. All modern mammals give birth to live young, except the five species of monotremes, which are egg-laying mammals. The most species-rich group of mammals, the cohort called placentals, have a placenta, which enables the feeding of the fetus during gestation.
Most mammals are intelligent, with some possessing large brains, self-awareness, and tool use. Mammals can communicate and vocalize in several ways, including the production of ultrasound, scent-marking, alarm signals, singing, echolocation; and, in the case of humans, complex language. Mammals can organize themselves into fission-fusion societies, harems, and hierarchies—but can also be solitary and territorial. Most mammals are polygynous, but some can be monogamous or polyandrous.
Domestication of many types of mammals by humans played a major role in the Neolithic Revolution, and resulted in farming replacing hunting and gathering as the primary source of food for humans. This led to a major restructuring of human societies from nomadic to sedentary, with more co-operation among larger and larger groups, and ultimately the development of the first civilizations. Domesticated mammals provided, and continue to provide, power for transport and agriculture, as well as food (meat and dairy products), fur, and leather. Mammals are also hunted and raced for sport, and are used as model organisms in science. Mammals have been depicted in art since Paleolithic times, and appear in literature, film, mythology, and religion. Decline in numbers and extinction of many mammals is primarily driven by human poaching and habitat destruction, primarily deforestation.
Mammal classification has been through several revisions since Carl Linnaeus initially defined the class, and at present, no classification system is universally accepted. McKenna & Bell (1997) and Wilson & Reeder (2005) provide useful recent compendiums.[2] Simpson (1945)[3] provides systematics of mammal origins and relationships that had been taught universally until the end of the 20th century.
However, since 1945, a large amount of new and more detailed information has gradually been found: The paleontological record has been recalibrated, and the intervening years have seen much debate and progress concerning the theoretical underpinnings of systematization itself, partly through the new concept of cladistics. Though fieldwork and lab work progressively outdated Simpson's classification, it remains the closest thing to an official classification of mammals, despite its known issues.[4]
Most mammals, including the six most species-rich orders, belong to the placental group. The three largest orders in numbers of species are Rodentia: mice, rats, porcupines, beavers, capybaras, and other gnawing mammals; Chiroptera: bats; and Soricomorpha: shrews, moles, and solenodons. The next three biggest orders, depending on the biological classification scheme used, are the Primates: apes, monkeys, and lemurs; the Cetartiodactyla: whales and even-toed ungulates; and the Carnivora which includes cats, dogs, weasels, bears, seals, and allies.[5] According to Mammal Species of the World, 5,416 species were identified in 2006. These were grouped into 1,229 genera, 153 families and 29 orders.[5] In 2008, the International Union for Conservation of Nature (IUCN) completed a five-year Global Mammal Assessment for its IUCN Red List, which counted 5,488 species.[6] According to research published in the Journal of Mammalogy in 2018, the number of recognized mammal species is 6,495, including 96 recently extinct.[7]
The word "mammal" is modern, from the scientific name Mammalia coined by Carl Linnaeus in 1758, derived from the Latin mamma ("teat, pap"). In an influential 1988 paper, Timothy Rowe defined Mammalia phylogenetically as the crown group of mammals, the clade consisting of the most recent common ancestor of living monotremes (echidnas and platypuses) and Therian mammals (marsupials and placentals) and all descendants of that ancestor.[8] Since this ancestor lived in the Jurassic period, Rowe's definition excludes all animals from the earlier Triassic, despite the fact that Triassic fossils in the Haramiyida have been referred to the Mammalia since the mid-19th century.[9] If Mammalia is considered as the crown group, its origin can be roughly dated as the first known appearance of animals more closely related to some extant mammals than to others. Ambondro is more closely related to monotremes than to therian mammals while Amphilestes and Amphitherium are more closely related to the therians; as fossils of all three genera are dated about 167 million years ago in the Middle Jurassic, this is a reasonable estimate for the appearance of the crown group.[10]
T. S. Kemp has provided a more traditional definition: "Synapsids that possess a dentary–squamosal jaw articulation and occlusion between upper and lower molars with a transverse component to the movement" or, equivalently in Kemp's view, the clade originating with the last common ancestor of Sinoconodon and living mammals.[11] The earliest known synapsid satisfying Kemp's definitions is Tikitherium, dated 225 Ma, so the appearance of mammals in this broader sense can be given this Late Triassic date.[12][13]
In 1997, the mammals were comprehensively revised by Malcolm C. McKenna and Susan K. Bell, which has resulted in the McKenna/Bell classification. The authors worked together as paleontologists at the American Museum of Natural History. McKenna inherited the project from Simpson and, with Bell, constructed a completely updated hierarchical system, covering living and extinct taxa, that reflects the historical genealogy of Mammalia.[4] Their 1997 book, Classification of Mammals above the Species Level,[14] is a comprehensive work on the systematics, relationships and occurrences of all mammal taxa, living and extinct, down through the rank of genus, though molecular genetic data challenge several of the groupings.
In the following list, extinct groups are labelled with a dagger (†).
Class Mammalia
As of the early 21st century, molecular studies based on DNA analysis have suggested new relationships among mammal families. Most of these findings have been independently validated by retrotransposon presence/absence data.[17] Classification systems based on molecular studies reveal three major groups or lineages of placental mammals—Afrotheria, Xenarthra and Boreoeutheria—which diverged in the Cretaceous. The relationships between these three lineages is contentious, and all three possible hypotheses have been proposed with respect to which group is basal. These hypotheses are Atlantogenata (basal Boreoeutheria), Epitheria (basal Xenarthra) and Exafroplacentalia (basal Afrotheria).[18] Boreoeutheria in turn contains two major lineages—Euarchontoglires and Laurasiatheria.
Estimates for the divergence times between these three placental groups range from 105 to 120 million years ago, depending on the type of DNA used (such as nuclear or mitochondrial)[19] and varying interpretations of paleogeographic data.[18]
The cladogram above is based on Tarver et al.. (2016)[20]
Synapsida, a clade that contains mammals and their extinct relatives, originated during the Pennsylvanian subperiod (~323 million to ~300 million years ago), when they split from the reptile lineage. Crown group mammals evolved from earlier mammaliaforms during the Early Jurassic. The cladogram takes Mammalia to be the crown group.[21]
The first fully terrestrial vertebrates were amniotes. Like their amphibious early tetrapod predecessors, they had lungs and limbs. Amniotic eggs, however, have internal membranes that allow the developing embryo to breathe but keep water in. Hence, amniotes can lay eggs on dry land, while amphibians generally need to lay their eggs in water.
The first amniotes apparently arose in the Pennsylvanian subperiod of the Carboniferous. They descended from earlier reptiliomorph amphibious tetrapods,[22] which lived on land that was already inhabited by insects and other invertebrates as well as ferns, mosses and other plants. Within a few million years, two important amniote lineages became distinct: the synapsids, which would later include the common ancestor of the mammals; and the sauropsids, which now include turtles, lizards, snakes, crocodilians and dinosaurs (including birds).[23] Synapsids have a single hole (temporal fenestra) low on each side of the skull. Primitive synapsids included the largest and fiercest animals of the early Permian such as Dimetrodon.[24] Nonmammalian synapsids were traditionally—and incorrectly—called "mammal-like reptiles" or pelycosaurs; we now know they were neither reptiles nor part of reptile lineage.[25][26]
Therapsids, a group of synapsids, evolved in the Middle Permian, about 265 million years ago, and became the dominant land vertebrates.[25] They differ from basal eupelycosaurs in several features of the skull and jaws, including: larger skulls and incisors which are equal in size in therapsids, but not for eupelycosaurs.[25] The therapsid lineage leading to mammals went through a series of stages, beginning with animals that were very similar to their early synapsid ancestors and ending with probainognathian cynodonts, some of which could easily be mistaken for mammals. Those stages were characterized by:[27]
The Permian–Triassic extinction event about 252 million years ago, which was a prolonged event due to the accumulation of several extinction pulses, ended the dominance of carnivorous therapsids.[29] In the early Triassic, most medium to large land carnivore niches were taken over by archosaurs[30] which, over an extended period (35 million years), came to include the crocodylomorphs,[31] the pterosaurs and the dinosaurs;[32] however, large cynodonts like Trucidocynodon and traversodontids still occupied large sized carnivorous and herbivorous niches respectively. By the Jurassic, the dinosaurs had come to dominate the large terrestrial herbivore niches as well.[33]
The first mammals (in Kemp's sense) appeared in the Late Triassic epoch (about 225 million years ago), 40 million years after the first therapsids. They expanded out of their nocturnal insectivore niche from the mid-Jurassic onwards;[34] The Jurassic Castorocauda, for example, was a close relative of true mammals that had adaptations for swimming, digging and catching fish.[35] Most, if not all, are thought to have remained nocturnal (the nocturnal bottleneck), accounting for much of the typical mammalian traits.[36] The majority of the mammal species that existed in the Mesozoic Era were multituberculates, eutriconodonts and spalacotheriids.[37] The earliest known metatherian is Sinodelphys, found in 125 million-year-old Early Cretaceous shale in China's northeastern Liaoning Province. The fossil is nearly complete and includes tufts of fur and imprints of soft tissues.[38]
The oldest known fossil among the Eutheria ("true beasts") is the small shrewlike Juramaia sinensis, or "Jurassic mother from China", dated to 160 million years ago in the late Jurassic.[39] A later eutherian relative, Eomaia, dated to 125 million years ago in the early Cretaceous, possessed some features in common with the marsupials but not with the placentals, evidence that these features were present in the last common ancestor of the two groups but were later lost in the placental lineage.[40] In particular, the epipubic bones extend forwards from the pelvis. These are not found in any modern placental, but they are found in marsupials, monotremes, other nontherian mammals and Ukhaatherium, an early Cretaceous animal in the eutherian order Asioryctitheria. This also applies to the multituberculates.[41] They are apparently an ancestral feature, which subsequently disappeared in the placental lineage. These epipubic bones seem to function by stiffening the muscles during locomotion, reducing the amount of space being presented, which placentals require to contain their fetus during gestation periods. A narrow pelvic outlet indicates that the young were very small at birth and therefore pregnancy was short, as in modern marsupials. This suggests that the placenta was a later development.[42]
One of the earliest known monotremes was Teinolophos, which lived about 120 million years ago in Australia.[43] Monotremes have some features which may be inherited from the original amniotes such as the same orifice to urinate, defecate and reproduce (cloaca)—as lizards and birds also do—[44] and they lay eggs which are leathery and uncalcified.[45]
Hadrocodium, whose fossils date from approximately 195 million years ago, in the early Jurassic, provides the first clear evidence of a jaw joint formed solely by the squamosal and dentary bones; there is no space in the jaw for the articular, a bone involved in the jaws of all early synapsids.[46]
The earliest clear evidence of hair or fur is in fossils of Castorocauda and Megaconus, from 164 million years ago in the mid-Jurassic. In the 1950s, it was suggested that the foramina (passages) in the maxillae and premaxillae (bones in the front of the upper jaw) of cynodonts were channels which supplied blood vessels and nerves to vibrissae (whiskers) and so were evidence of hair or fur;[47][48] it was soon pointed out, however, that foramina do not necessarily show that an animal had vibrissae, as the modern lizard Tupinambis has foramina that are almost identical to those found in the nonmammalian cynodont Thrinaxodon.[26][49] Popular sources, nevertheless, continue to attribute whiskers to Thrinaxodon.[50] Studies on Permian coprolites suggest that non-mammalian synapsids of the epoch already had fur, setting the evolution of hairs possibly as far back as dicynodonts.[51]
When endothermy first appeared in the evolution of mammals is uncertain, though it is generally agreed to have first evolved in non-mammalian therapsids.[51][52] Modern monotremes have lower body temperatures and more variable metabolic rates than marsupials and placentals,[53] but there is evidence that some of their ancestors, perhaps including ancestors of the therians, may have had body temperatures like those of modern therians.[54] Likewise, some modern therians like afrotheres and xenarthrans have secondarily developed lower body temperatures.[55]
The evolution of erect limbs in mammals is incomplete—living and fossil monotremes have sprawling limbs. The parasagittal (nonsprawling) limb posture appeared sometime in the late Jurassic or early Cretaceous; it is found in the eutherian Eomaia and the metatherian Sinodelphys, both dated to 125 million years ago.[56] Epipubic bones, a feature that strongly influenced the reproduction of most mammal clades, are first found in Tritylodontidae, suggesting that it is a synapomorphy between them and mammaliformes. They are omnipresent in non-placental mammaliformes, though Megazostrodon and Erythrotherium appear to have lacked them.[57]
It has been suggested that the original function of lactation (milk production) was to keep eggs moist. Much of the argument is based on monotremes, the egg-laying mammals.[58][59] In human females, mammary glands become fully developed during puberty, regardless of pregnancy.[60]
Therian mammals took over the medium- to large-sized ecological niches in the Cenozoic, after the Cretaceous–Paleogene extinction event approximately 66 million years ago emptied ecological space once filled by non-avian dinosaurs and other groups of reptiles, as well as various other mammal groups,[62] and underwent an exponential increase in body size (megafauna).[63] Then mammals diversified very quickly; both birds and mammals show an exponential rise in diversity.[62] For example, the earliest known bat dates from about 50 million years ago, only 16 million years after the extinction of the non-avian dinosaurs.[64]
Molecular phylogenetic studies initially suggested that most placental orders diverged about 100 to 85 million years ago and that modern families appeared in the period from the late Eocene through the Miocene.[65] However, no placental fossils have been found from before the end of the Cretaceous.[66] The earliest undisputed fossils of placentals come from the early Paleocene, after the extinction of the non-avian dinosaurs.[66] (Scientists identified an early Paleocene animal named Protungulatum donnae as one of the first placental mammals,[67] but it has since been reclassified as a non-placental eutherian.)[68] Recalibrations of genetic and morphological diversity rates have suggested a Late Cretaceous origin for placentals, and a Paleocene origin for most modern clades.[69]
The earliest known ancestor of primates is Archicebus achilles[70] from around 55 million years ago.[70] This tiny primate weighed 20–30 grams (0.7–1.1 ounce) and could fit within a human palm.[70]

Living mammal species can be identified by the presence of sweat glands, including those that are specialized to produce milk to nourish their young.[71] In classifying fossils, however, other features must be used, since soft tissue glands and many other features are not visible in fossils.[72]
Many traits shared by all living mammals appeared among the earliest members of the group:
For the most part, these characteristics were not present in the Triassic ancestors of the mammals.[78] Nearly all mammaliaforms possess an epipubic bone, the exception being modern placentals.[79]
On average, male mammals are larger than females, with males being at least 10% larger than females in over 45% of investigated species. Most mammalian orders also exhibit male-biased sexual dimorphism, although some orders do not show any bias or are significantly female-biased (Lagomorpha). Sexual size dimorphism increases with body size across mammals (Rensch's rule), suggesting that there are parallel selection pressures on both male and female size. Male-biased dimorphism relates to sexual selection on males through male–male competition for females, as there is a positive correlation between the degree of sexual selection, as indicated by mating systems, and the degree of male-biased size dimorphism. The degree of sexual selection is also positively correlated with male and female size across mammals. Further, parallel selection pressure on female mass is identified in that age at weaning is significantly higher in more polygynous species, even when correcting for body mass. Also, the reproductive rate is lower for larger females, indicating that fecundity selection selects for smaller females in mammals. Although these patterns hold across mammals as a whole, there is considerable variation across orders.[80]
The majority of mammals have seven cervical vertebrae (bones in the neck). The exceptions are the manatee and the two-toed sloth, which have six, and the three-toed sloth which has nine.[81] All mammalian brains possess a neocortex, a brain region unique to mammals.[82] Placental brains have a corpus callosum, unlike monotremes and marsupials.[83]
The lungs of mammals are spongy and honeycombed. Breathing is mainly achieved with the diaphragm, which divides the thorax from the abdominal cavity, forming a dome convex to the thorax. Contraction of the diaphragm flattens the dome, increasing the volume of the lung cavity. Air enters through the oral and nasal cavities, and travels through the larynx, trachea and bronchi, and expands the alveoli. Relaxing the diaphragm has the opposite effect, decreasing the volume of the lung cavity, causing air to be pushed out of the lungs. During exercise, the abdominal wall contracts, increasing pressure on the diaphragm, which forces air out quicker and more forcefully. The rib cage is able to expand and contract the chest cavity through the action of other respiratory muscles. Consequently, air is sucked into or expelled out of the lungs, always moving down its pressure gradient.[84][85] This type of lung is known as a bellows lung due to its resemblance to blacksmith bellows.[85]
The mammalian heart has four chambers, two upper atria, the receiving chambers, and two lower ventricles, the discharging chambers.[86] The heart has four valves, which separate its chambers and ensures blood flows in the correct direction through the heart (preventing backflow). After gas exchange in the pulmonary capillaries (blood vessels in the lungs), oxygen-rich blood returns to the left atrium via one of the four pulmonary veins. Blood flows nearly continuously back into the atrium, which acts as the receiving chamber, and from here through an opening into the left ventricle. Most blood flows passively into the heart while both the atria and ventricles are relaxed, but toward the end of the ventricular relaxation period, the left atrium will contract, pumping blood into the ventricle. The heart also requires nutrients and oxygen found in blood like other muscles, and is supplied via coronary arteries.[87]
The integumentary system (skin) is made up of three layers: the outermost epidermis, the dermis and the hypodermis. The epidermis is typically 10 to 30 cells thick; its main function is to provide a waterproof layer. Its outermost cells are constantly lost; its bottommost cells are constantly dividing and pushing upward. The middle layer, the dermis, is 15 to 40 times thicker than the epidermis. The dermis is made up of many components, such as bony structures and blood vessels. The hypodermis is made up of adipose tissue, which stores lipids and provides cushioning and insulation. The thickness of this layer varies widely from species to species;[88]: 97  marine mammals require a thick hypodermis (blubber) for insulation, and right whales have the thickest blubber at 20 inches (51 cm).[89] Although other animals have features such as whiskers, feathers, setae, or cilia that superficially resemble it, no animals other than mammals have hair. It is a definitive characteristic of the class, though some mammals have very little.[88]: 61 
Herbivores have developed a diverse range of physical structures to facilitate the consumption of plant material. To break up intact plant tissues, mammals have developed teeth structures that reflect their feeding preferences. For instance, frugivores (animals that feed primarily on fruit) and herbivores that feed on soft foliage have low-crowned teeth specialized for grinding foliage and seeds. Grazing animals that tend to eat hard, silica-rich grasses, have high-crowned teeth, which are capable of grinding tough plant tissues and do not wear down as quickly as low-crowned teeth.[90] Most carnivorous mammals have carnassialiforme teeth (of varying length depending on diet), long canines and similar tooth replacement patterns.[91]
The stomach of even-toed ungulates (Artiodactyla) is divided into four sections: the rumen, the reticulum, the omasum and the abomasum (only ruminants have a rumen). After the plant material is consumed, it is mixed with saliva in the rumen and reticulum and separates into solid and liquid material. The solids lump together to form a bolus (or cud), and is regurgitated. When the bolus enters the mouth, the fluid is squeezed out with the tongue and swallowed again. Ingested food passes to the rumen and reticulum where cellulolytic microbes (bacteria, protozoa and fungi) produce cellulase, which is needed to break down the cellulose in plants.[92] Perissodactyls, in contrast to the ruminants, store digested food that has left the stomach in an enlarged cecum, where it is fermented by bacteria.[93] Carnivora have a simple stomach adapted to digest primarily meat, as compared to the elaborate digestive systems of herbivorous animals, which are necessary to break down tough, complex plant fibers. The caecum is either absent or short and simple, and the large intestine is not sacculated or much wider than the small intestine.[94]
The mammalian excretory system involves many components. Like most other land animals, mammals are ureotelic, and convert ammonia into urea, which is done by the liver as part of the urea cycle.[95] Bilirubin, a waste product derived from blood cells, is passed through bile and urine with the help of enzymes excreted by the liver.[96] The passing of bilirubin via bile through the intestinal tract gives mammalian feces a distinctive brown coloration.[97] Distinctive features of the mammalian kidney include the presence of the renal pelvis and renal pyramids, and of a clearly distinguishable cortex and medulla, which is due to the presence of elongated loops of Henle. Only the mammalian kidney has a bean shape, although there are some exceptions, such as the multilobed reniculate kidneys of pinnipeds, cetaceans and bears.[98][99] Most adult placental mammals have no remaining trace of the cloaca. In the embryo, the embryonic cloaca divides into a posterior region that becomes part of the anus, and an anterior region that has different fates depending on the sex of the individual: in females, it develops into the vestibule that receives the urethra and vagina, while in males it forms the entirety of the penile urethra.[99] However, the tenrecs, golden moles, and some shrews retain a cloaca as adults.[100] In marsupials, the genital tract is separate from the anus, but a trace of the original cloaca does remain externally.[99] Monotremes, which translates from Greek into "single hole", have a true cloaca.[101]
As in all other tetrapods, mammals have a larynx that can quickly open and close to produce sounds, and a supralaryngeal vocal tract which filters this sound. The lungs and surrounding musculature provide the air stream and pressure required to phonate. The larynx controls the pitch and volume of sound, but the strength the lungs exert to exhale also contributes to volume. More primitive mammals, such as the echidna, can only hiss, as sound is achieved solely through exhaling through a partially closed larynx. Other mammals phonate using vocal folds. The movement or tenseness of the vocal folds can result in many sounds such as purring and screaming. Mammals can change the position of the larynx, allowing them to breathe through the nose while swallowing through the mouth, and to form both oral and nasal sounds; nasal sounds, such as a dog whine, are generally soft sounds, and oral sounds, such as a dog bark, are generally loud.[102]
Some mammals have a large larynx and thus a low-pitched voice, namely the hammer-headed bat (Hypsignathus monstrosus) where the larynx can take up the entirety of the thoracic cavity while pushing the lungs, heart, and trachea into the abdomen.[103] Large vocal pads can also lower the pitch, as in the low-pitched roars of big cats.[104] The production of infrasound is possible in some mammals such as the African elephant (Loxodonta spp.) and baleen whales.[105][106] Small mammals with small larynxes have the ability to produce ultrasound, which can be detected by modifications to the middle ear and cochlea. Ultrasound is inaudible to birds and reptiles, which might have been important during the Mesozoic, when birds and reptiles were the dominant predators. This private channel is used by some rodents in, for example, mother-to-pup communication, and by bats when echolocating. Toothed whales also use echolocation, but, as opposed to the vocal membrane that extends upward from the vocal folds, they have a melon to manipulate sounds. Some mammals, namely the primates, have air sacs attached to the larynx, which may function to lower the resonances or increase the volume of sound.[102]
The vocal production system is controlled by the cranial nerve nuclei in the brain, and supplied by the recurrent laryngeal nerve and the superior laryngeal nerve, branches of the vagus nerve. The vocal tract is supplied by the hypoglossal nerve and facial nerves. Electrical stimulation of the periaqueductal gray (PEG) region of the mammalian midbrain elicit vocalizations. The ability to learn new vocalizations is only exemplified in humans, seals, cetaceans, elephants and possibly bats; in humans, this is the result of a direct connection between the motor cortex, which controls movement, and the motor neurons in the spinal cord.[102]
The primary function of the fur of mammals is thermoregulation. Others include protection, sensory purposes, waterproofing, and camouflage.[107] Different types of fur serve different purposes:[88]: 99 
Hair length is not a factor in thermoregulation: for example, some tropical mammals such as sloths have the same length of fur length as some arctic mammals but with less insulation; and, conversely, other tropical mammals with short hair have the same insulating value as arctic mammals. The denseness of fur can increase an animal's insulation value, and arctic mammals especially have dense fur; for example, the musk ox has guard hairs measuring 30 cm (12 in) as well as a dense underfur, which forms an airtight coat, allowing them to survive in temperatures of −40 °C (−40 °F).[88]: 162–163  Some desert mammals, such as camels, use dense fur to prevent solar heat from reaching their skin, allowing the animal to stay cool; a camel's fur may reach 70 °C (158 °F) in the summer, but the skin stays at 40 °C (104 °F).[88]: 188  Aquatic mammals, conversely, trap air in their fur to conserve heat by keeping the skin dry.[88]: 162–163 
Mammalian coats are colored for a variety of reasons, the major selective pressures including camouflage, sexual selection, communication, and thermoregulation. Coloration in both the hair and skin of mammals is mainly determined by the type and amount of melanin; eumelanins for brown and black colors and pheomelanin for a range of yellowish to reddish colors, giving mammals an earth tone.[108][109] Some mammals have more vibrant colors; certain monkeys such mandrills and vervet monkeys, and opossums such as the Mexican mouse opossums and Derby's woolly opossums, have blue skin due to light diffraction in collagen fibers.[110] Many sloths appear green because their fur hosts green algae; this may be a symbiotic relation that affords camouflage to the sloths.[111]
Camouflage is a powerful influence in a large number of mammals, as it helps to conceal individuals from predators or prey.[112] In arctic and subarctic mammals such as the arctic fox (Alopex lagopus), collared lemming (Dicrostonyx groenlandicus), stoat (Mustela erminea), and snowshoe hare (Lepus americanus), seasonal color change between brown in summer and white in winter is driven largely by camouflage.[113] Some arboreal mammals, notably primates and marsupials, have shades of violet, green, or blue skin on parts of their bodies, indicating some distinct advantage in their largely arboreal habitat due to convergent evolution.[110]
Aposematism, warning off possible predators, is the most likely explanation of the black-and-white pelage of many mammals which are able to defend themselves, such as in the foul-smelling skunk and the powerful and aggressive honey badger.[114] Coat color is sometimes sexually dimorphic, as in many primate species.[115] Differences in female and male coat color may indicate nutrition and hormone levels, important in mate selection.[116] Coat color may influence the ability to retain heat, depending on how much light is reflected. Mammals with a darker colored coat can absorb more heat from solar radiation, and stay warmer, and some smaller mammals, such as voles, have darker fur in the winter. The white, pigmentless fur of arctic mammals, such as the polar bear, may reflect more solar radiation directly onto the skin.[88]: 166–167 [107] The dazzling black-and-white striping of zebras appear to provide some protection from biting flies.[117]
Mammals are solely gonochoric (an animal is born with either male or female genitalia, as opposed to hermaphrodites where there is no such schism).[118] In male placentals, the penis is used both for urination and copulation. Depending on the species, an erection may be fueled by blood flow into vascular, spongy tissue or by muscular action. A penis may be contained in a prepuce when not erect, and some placentals also have a penis bone (baculum).[119] Marsupials typically have forked penises,[120] while the echidna penis generally has four heads with only two functioning.[121] The testes of most mammals descend into the scrotum which is typically posterior to the penis but is often anterior in marsupials. Female mammals generally have a clitoris, labia majora and labia minora on the outside, while the internal system contains paired oviducts, 1–2 uteri, 1–2 cervices and a vagina. Marsupials have two lateral vaginas and a medial vagina. The "vagina" of monotremes is better understood as a "urogenital sinus". The uterine systems of placental mammals can vary between a duplex, where there are two uteri and cervices which open into the vagina, a bipartite, where two uterine horns have a single cervix that connects to the vagina, a bicornuate, which consists where two uterine horns that are connected distally but separate medially creating a Y-shape, and a simplex, which has a single uterus.[122][123][88]: 220–221, 247 
The ancestral condition for mammal reproduction is the birthing of relatively undeveloped, either through direct vivipary or a short period as soft-shelled eggs. This is likely due to the fact that the torso could not expand due to the presence of epipubic bones. The oldest demonstration of this reproductive style is with Kayentatherium, which produced undeveloped perinates, but at much higher litter sizes than any modern mammal, 38 specimens.[124] Most modern mammals are viviparous, giving birth to live young. However, the five species of monotreme, the platypus and the four species of echidna, lay eggs. The monotremes have a sex determination system different from that of most other mammals.[125] In particular, the sex chromosomes of a platypus are more like those of a chicken than those of a therian mammal.[126]
Viviparous mammals are in the subclass Theria; those living today are in the marsupial and placental infraclasses. Marsupials have a short gestation period, typically shorter than its estrous cycle and generally giving birth to a number of undeveloped newborns that then undergo further development; in many species, this takes place within a pouch-like sac, the marsupium, located in the front of the mother's abdomen. This is the plesiomorphic condition among viviparous mammals; the presence of epipubic bones in all non-placental mammals prevents the expansion of the torso needed for full pregnancy.[79] Even non-placental eutherians probably reproduced this way.[41] The placentals give birth to relatively complete and developed young, usually after long gestation periods.[127] They get their name from the placenta, which connects the developing fetus to the uterine wall to allow nutrient uptake.[128] In placental mammals, the epipubic is either completely lost or converted into the baculum; allowing the torso to be able to expand and thus birth developed offspring.[124]
The mammary glands of mammals are specialized to produce milk, the primary source of nutrition for newborns. The monotremes branched early from other mammals and do not have the nipples seen in most mammals, but they do have mammary glands. The young lick the milk from a mammary patch on the mother's belly.[129] Compared to placental mammals, the milk of marsupials changes greatly in both production rate and in nutrient composition, due to the underdeveloped young. In addition, the mammary glands have more autonomy allowing them to supply separate milks to young at different development stages.[130] Lactose is the main sugar in placental mammal milk while monotreme and marsupial milk is dominated by oligosaccharides.[131] Weaning is the process in which a mammal becomes less dependent on their mother's milk and more on solid food.[132]
Nearly all mammals are endothermic ("warm-blooded"). Most mammals also have hair to help keep them warm. Like birds, mammals can forage or hunt in weather and climates too cold for ectothermic ("cold-blooded") reptiles and insects. Endothermy requires plenty of food energy, so mammals eat more food per unit of body weight than most reptiles.[133] Small insectivorous mammals eat prodigious amounts for their size. A rare exception, the naked mole-rat produces little metabolic heat, so it is considered an operational poikilotherm.[134] Birds are also endothermic, so endothermy is not unique to mammals.[135]
Among mammals, species maximum lifespan varies significantly (for example the shrew has a lifespan of two years, whereas the oldest bowhead whale is recorded to be 211 years).[136] Although the underlying basis for these lifespan differences is still uncertain, numerous studies indicate that the ability to repair DNA damage is an important determinant of mammalian lifespan. In a 1974 study by Hart and Setlow,[137] it was found that DNA excision repair capability increased systematically with species lifespan among seven mammalian species. Species lifespan was observed to be robustly correlated with the capacity to recognize DNA double-strand breaks as well as the level of the DNA repair protein Ku80.[136] In a study of the cells from sixteen mammalian species, genes employed in DNA repair were found to be up-regulated in the longer-lived species.[138] The cellular level of the DNA repair enzyme poly ADP ribose polymerase was found to correlate with species lifespan in a study of 13 mammalian species.[139] Three additional studies of a variety of mammalian species also reported a correlation between species lifespan and DNA repair capability.[140][141][142]
Most vertebrates—the amphibians, the reptiles and some mammals such as humans and bears—are plantigrade, walking on the whole of the underside of the foot. Many mammals, such as cats and dogs, are digitigrade, walking on their toes, the greater stride length allowing more speed. Digitigrade mammals are also often adept at quiet movement.[citation needed] Some animals such as horses are unguligrade, walking on the tips of their toes. This even further increases their stride length and thus their speed.[143] A few mammals, namely the great apes, are also known to walk on their knuckles, at least for their front legs. Giant anteaters[144] and platypuses[145] are also knuckle-walkers. Some mammals are bipeds, using only two limbs for locomotion, which can be seen in, for example, humans and the great apes. Bipedal species have a larger field of vision than quadrupeds, conserve more energy and have the ability to manipulate objects with their hands, which aids in foraging. Instead of walking, some bipeds hop, such as kangaroos and kangaroo rats.[146][147]
Animals will use different gaits for different speeds, terrain and situations. For example, horses show four natural gaits, the slowest horse gait is the walk, then there are three faster gaits which, from slowest to fastest, are the trot, the canter and the gallop. Animals may also have unusual gaits that are used occasionally, such as for moving sideways or backwards. For example, the main human gaits are bipedal walking and running, but they employ many other gaits occasionally, including a four-legged crawl in tight spaces.[148] Mammals show a vast range of gaits, the order that they place and lift their appendages in locomotion. Gaits can be grouped into categories according to their patterns of support sequence. For quadrupeds, there are three main categories: walking gaits, running gaits and leaping gaits.[149] Walking is the most common gait, where some feet are on the ground at any given time, and found in almost all legged animals. Running is considered to occur when at some points in the stride all feet are off the ground in a moment of suspension.[148]
Arboreal animals frequently have elongated limbs that help them cross gaps, reach fruit or other resources, test the firmness of support ahead and, in some cases, to brachiate (swing between trees).[150] Many arboreal species, such as tree porcupines, silky anteaters, spider monkeys, and possums, use prehensile tails to grasp branches. In the spider monkey, the tip of the tail has either a bare patch or adhesive pad, which provides increased friction. Claws can be used to interact with rough substrates and reorient the direction of forces the animal applies. This is what allows squirrels to climb tree trunks that are so large to be essentially flat from the perspective of such a small animal. However, claws can interfere with an animal's ability to grasp very small branches, as they may wrap too far around and prick the animal's own paw. Frictional gripping is used by primates, relying upon hairless fingertips. Squeezing the branch between the fingertips generates frictional force that holds the animal's hand to the branch. However, this type of grip depends upon the angle of the frictional force, thus upon the diameter of the branch, with larger branches resulting in reduced gripping ability. To control descent, especially down large diameter branches, some arboreal animals such as squirrels have evolved highly mobile ankle joints that permit rotating the foot into a 'reversed' posture. This allows the claws to hook into the rough surface of the bark, opposing the force of gravity. Small size provides many advantages to arboreal species: such as increasing the relative size of branches to the animal, lower center of mass, increased stability, lower mass (allowing movement on smaller branches) and the ability to move through more cluttered habitat.[150] Size relating to weight affects gliding animals such as the sugar glider.[151] Some species of primate, bat and all species of sloth achieve passive stability by hanging beneath the branch. Both pitching and tipping become irrelevant, as the only method of failure would be losing their grip.[150]
Bats are the only mammals that can truly fly. They fly through the air at a constant speed by moving their wings up and down (usually with some fore-aft movement as well). Because the animal is in motion, there is some airflow relative to its body which, combined with the velocity of the wings, generates a faster airflow moving over the wing. This generates a lift force vector pointing forwards and upwards, and a drag force vector pointing rearwards and upwards. The upwards components of these counteract gravity, keeping the body in the air, while the forward component provides thrust to counteract both the drag from the wing and from the body as a whole.[152]
The wings of bats are much thinner and consist of more bones than those of birds, allowing bats to maneuver more accurately and fly with more lift and less drag.[153][154] By folding the wings inwards towards their body on the upstroke, they use 35% less energy during flight than birds.[155] The membranes are delicate, ripping easily; however, the tissue of the bat's membrane is able to regrow, such that small tears can heal quickly.[156] The surface of their wings is equipped with touch-sensitive receptors on small bumps called Merkel cells, also found on human fingertips. These sensitive areas are different in bats, as each bump has a tiny hair in the center, making it even more sensitive and allowing the bat to detect and collect information about the air flowing over its wings, and to fly more efficiently by changing the shape of its wings in response.[157]
A fossorial (from Latin fossor, meaning "digger") is an animal adapted to digging which lives primarily, but not solely, underground. Some examples are badgers, and naked mole-rats. Many rodent species are also considered fossorial because they live in burrows for most but not all of the day. Species that live exclusively underground are subterranean, and those with limited adaptations to a fossorial lifestyle sub-fossorial. Some organisms are fossorial to aid in temperature regulation while others use the underground habitat for protection from predators or for food storage.[158]
Fossorial mammals have a fusiform body, thickest at the shoulders and tapering off at the tail and nose. Unable to see in the dark burrows, most have degenerated eyes, but degeneration varies between species; pocket gophers, for example, are only semi-fossorial and have very small yet functional eyes, in the fully fossorial marsupial mole the eyes are degenerated and useless, talpa moles have vestigial eyes and the cape golden mole has a layer of skin covering the eyes. External ears flaps are also very small or absent. Truly fossorial mammals have short, stout legs as strength is more important than speed to a burrowing mammal, but semi-fossorial mammals have cursorial legs. The front paws are broad and have strong claws to help in loosening dirt while excavating burrows, and the back paws have webbing, as well as claws, which aids in throwing loosened dirt backwards. Most have large incisors to prevent dirt from flying into their mouth.[159]
Many fossorial mammals such as shrews, hedgehogs, and moles were classified under the now obsolete order Insectivora.[160]
Fully aquatic mammals, the cetaceans and sirenians, have lost their legs and have a tail fin to propel themselves through the water. Flipper movement is continuous. Whales swim by moving their tail fin and lower body up and down, propelling themselves through vertical movement, while their flippers are mainly used for steering. Their skeletal anatomy allows them to be fast swimmers. Most species have a dorsal fin to prevent themselves from turning upside-down in the water.[161][162] The flukes of sirenians are raised up and down in long strokes to move the animal forward, and can be twisted to turn. The forelimbs are paddle-like flippers which aid in turning and slowing.[163]
Semi-aquatic mammals, like pinnipeds, have two pairs of flippers on the front and back, the fore-flippers and hind-flippers. The elbows and ankles are enclosed within the body.[164][165] Pinnipeds have several adaptions for reducing drag. In addition to their streamlined bodies, they have smooth networks of muscle bundles in their skin that may increase laminar flow and make it easier for them to slip through water. They also lack arrector pili, so their fur can be streamlined as they swim.[166] They rely on their fore-flippers for locomotion in a wing-like manner similar to penguins and sea turtles.[167] Fore-flipper movement is not continuous, and the animal glides between each stroke.[165] Compared to terrestrial carnivorans, the fore-limbs are reduced in length, which gives the locomotor muscles at the shoulder and elbow joints greater mechanical advantage;[164] the hind-flippers serve as stabilizers.[166] Other semi-aquatic mammals include beavers, hippopotamuses, otters and platypuses.[168] Hippos are very large semi-aquatic mammals, and their barrel-shaped bodies have graviportal skeletal structures,[169] adapted to carrying their enormous weight, and their specific gravity allows them to sink and move along the bottom of a river.[170]
Many mammals communicate by vocalizing. Vocal communication serves many purposes, including in mating rituals, as warning calls,[172] to indicate food sources, and for social purposes. Males often call during mating rituals to ward off other males and to attract females, as in the roaring of lions and red deer.[173] The songs of the humpback whale may be signals to females;[174] they have different dialects in different regions of the ocean.[175] Social vocalizations include the territorial calls of gibbons, and the use of frequency in greater spear-nosed bats to distinguish between groups.[176] The vervet monkey gives a distinct alarm call for each of at least four different predators, and the reactions of other monkeys vary according to the call. For example, if an alarm call signals a python, the monkeys climb into the trees, whereas the eagle alarm causes monkeys to seek a hiding place on the ground.[171] Prairie dogs similarly have complex calls that signal the type, size, and speed of an approaching predator.[177] Elephants communicate socially with a variety of sounds including snorting, screaming, trumpeting, roaring and rumbling. Some of the rumbling calls are infrasonic, below the hearing range of humans, and can be heard by other elephants up to 6 miles (9.7 km) away at still times near sunrise and sunset.[178]
Mammals signal by a variety of means. Many give visual anti-predator signals, as when deer and gazelle stot, honestly indicating their fit condition and their ability to escape,[179][180] or when white-tailed deer and other prey mammals flag with conspicuous tail markings when alarmed, informing the predator that it has been detected.[181] Many mammals make use of scent-marking, sometimes possibly to help defend territory, but probably with a range of functions both within and between species.[182][183][184] Microbats and toothed whales including oceanic dolphins vocalize both socially and in echolocation.[185][186][187]
To maintain a high constant body temperature is energy expensive—mammals therefore need a nutritious and plentiful diet. While the earliest mammals were probably predators, different species have since adapted to meet their dietary requirements in a variety of ways. Some eat other animals—this is a carnivorous diet (and includes insectivorous diets). Other mammals, called herbivores, eat plants, which contain complex carbohydrates such as cellulose. An herbivorous diet includes subtypes such as granivory (seed eating), folivory (leaf eating), frugivory (fruit eating), nectarivory (nectar eating), gummivory (gum eating) and mycophagy (fungus eating). The digestive tract of an herbivore is host to bacteria that ferment these complex substances, and make them available for digestion, which are either housed in the multichambered stomach or in a large cecum.[92] Some mammals are coprophagous, consuming feces to absorb the nutrients not digested when the food was first ingested.[88]: 131–137  An omnivore eats both prey and plants. Carnivorous mammals have a simple digestive tract because the proteins, lipids and minerals found in meat require little in the way of specialized digestion. Exceptions to this include baleen whales who also house gut flora in a multi-chambered stomach, like terrestrial herbivores.[188]
The size of an animal is also a factor in determining diet type (Allen's rule). Since small mammals have a high ratio of heat-losing surface area to heat-generating volume, they tend to have high energy requirements and a high metabolic rate. Mammals that weigh less than about 18 ounces (510 g; 1.1 lb) are mostly insectivorous because they cannot tolerate the slow, complex digestive process of an herbivore. Larger animals, on the other hand, generate more heat and less of this heat is lost. They can therefore tolerate either a slower collection process (carnivores that feed on larger vertebrates) or a slower digestive process (herbivores).[189] Furthermore, mammals that weigh more than 18 ounces (510 g; 1.1 lb) usually cannot collect enough insects during their waking hours to sustain themselves. The only large insectivorous mammals are those that feed on huge colonies of insects (ants or termites).[190]
Some mammals are omnivores and display varying degrees of carnivory and herbivory, generally leaning in favor of one more than the other. Since plants and meat are digested differently, there is a preference for one over the other, as in bears where some species may be mostly carnivorous and others mostly herbivorous.[192] They are grouped into three categories: mesocarnivory (50–70% meat), hypercarnivory (70% and greater of meat), and hypocarnivory (50% or less of meat). The dentition of hypocarnivores consists of dull, triangular carnassial teeth meant for grinding food. Hypercarnivores, however, have conical teeth and sharp carnassials meant for slashing, and in some cases strong jaws for bone-crushing, as in the case of hyenas, allowing them to consume bones; some extinct groups, notably the Machairodontinae, had saber-shaped canines.[191]
Some physiological carnivores consume plant matter and some physiological herbivores consume meat. From a behavioral aspect, this would make them omnivores, but from the physiological standpoint, this may be due to zoopharmacognosy. Physiologically, animals must be able to obtain both energy and nutrients from plant and animal materials to be considered omnivorous. Thus, such animals are still able to be classified as carnivores and herbivores when they are just obtaining nutrients from materials originating from sources that do not seemingly complement their classification.[193] For example, it is well documented that some ungulates such as giraffes, camels, and cattle, will gnaw on bones to consume particular minerals and nutrients.[194] Also, cats, which are generally regarded as obligate carnivores, occasionally eat grass to regurgitate indigestible material (such as hairballs), aid with hemoglobin production, and as a laxative.[195]
Many mammals, in the absence of sufficient food requirements in an environment, suppress their metabolism and conserve energy in a process known as hibernation.[196] In the period preceding hibernation, larger mammals, such as bears, become polyphagic to increase fat stores, whereas smaller mammals prefer to collect and stash food.[197] The slowing of the metabolism is accompanied by a decreased heart and respiratory rate, as well as a drop in internal temperatures, which can be around ambient temperature in some cases. For example, the internal temperatures of hibernating arctic ground squirrels can drop to −2.9 °C (26.8 °F), however the head and neck always stay above 0 °C (32 °F).[198] A few mammals in hot environments aestivate in times of drought or extreme heat, for example the fat-tailed dwarf lemur (Cheirogaleus medius).[199]
In intelligent mammals, such as primates, the cerebrum is larger relative to the rest of the brain. Intelligence itself is not easy to define, but indications of intelligence include the ability to learn, matched with behavioral flexibility. Rats, for example, are considered to be highly intelligent, as they can learn and perform new tasks, an ability that may be important when they first colonize a fresh habitat. In some mammals, food gathering appears to be related to intelligence: a deer feeding on plants has a brain smaller than a cat, which must think to outwit its prey.[190]
Tool use by animals may indicate different levels of learning and cognition. The sea otter uses rocks as essential and regular parts of its foraging behaviour (smashing abalone from rocks or breaking open shells), with some populations spending 21% of their time making tools.[200] Other tool use, such as chimpanzees using twigs to "fish" for termites, may be developed by watching others use tools and may even be a true example of animal teaching.[201] Tools may even be used in solving puzzles in which the animal appears to experience a "Eureka moment".[202] Other mammals that do not use tools, such as dogs, can also experience a Eureka moment.[203]
Brain size was previously considered a major indicator of the intelligence of an animal. Since most of the brain is used for maintaining bodily functions, greater ratios of brain to body mass may increase the amount of brain mass available for more complex cognitive tasks. Allometric analysis indicates that mammalian brain size scales at approximately the 2⁄3 or 3⁄4 exponent of the body mass. Comparison of a particular animal's brain size with the expected brain size based on such allometric analysis provides an encephalisation quotient that can be used as another indication of animal intelligence.[204] Sperm whales have the largest brain mass of any animal on earth, averaging 8,000 cubic centimetres (490 in3) and 7.8 kilograms (17 lb) in mature males.[205]
Self-awareness appears to be a sign of abstract thinking. Self-awareness, although not well-defined, is believed to be a precursor to more advanced processes such as metacognitive reasoning. The traditional method for measuring this is the mirror test, which determines if an animal possesses the ability of self-recognition.[206] Mammals that have passed the mirror test include Asian elephants (some pass, some do not);[207] chimpanzees;[208] bonobos;[209] orangutans;[210] humans, from 18 months (mirror stage);[211] bottlenose dolphins[a][212] killer whales;[213] and false killer whales.[213]
Eusociality is the highest level of social organization. These societies have an overlap of adult generations, the division of reproductive labor and cooperative caring of young. Usually insects, such as bees, ants and termites, have eusocial behavior, but it is demonstrated in two rodent species: the naked mole-rat[214] and the Damaraland mole-rat.[215]
Presociality is when animals exhibit more than just sexual interactions with members of the same species, but fall short of qualifying as eusocial. That is, presocial animals can display communal living, cooperative care of young, or primitive division of reproductive labor, but they do not display all of the three essential traits of eusocial animals. Humans and some species of Callitrichidae (marmosets and tamarins) are unique among primates in their degree of cooperative care of young.[216] Harry Harlow set up an experiment with rhesus monkeys, presocial primates, in 1958; the results from this study showed that social encounters are necessary in order for the young monkeys to develop both mentally and sexually.[217]
A fission-fusion society is a society that changes frequently in its size and composition, making up a permanent social group called the "parent group". Permanent social networks consist of all individual members of a community and often varies to track changes in their environment. In a fission–fusion society, the main parent group can fracture (fission) into smaller stable subgroups or individuals to adapt to environmental or social circumstances. For example, a number of males may break off from the main group in order to hunt or forage for food during the day, but at night they may return to join (fusion) the primary group to share food and partake in other activities. Many mammals exhibit this, such as primates (for example orangutans and spider monkeys),[218] elephants,[219] spotted hyenas,[220] lions,[221] and dolphins.[222]
Solitary animals defend a territory and avoid social interactions with the members of its species, except during breeding season. This is to avoid resource competition, as two individuals of the same species would occupy the same niche, and to prevent depletion of food.[223] A solitary animal, while foraging, can also be less conspicuous to predators or prey.[224]
In a hierarchy, individuals are either dominant or submissive. A despotic hierarchy is where one individual is dominant while the others are submissive, as in wolves and lemurs,[225] and a pecking order is a linear ranking of individuals where there is a top individual and a bottom individual. Pecking orders may also be ranked by sex, where the lowest individual of a sex has a higher ranking than the top individual of the other sex, as in hyenas.[226] Dominant individuals, or alphas, have a high chance of reproductive success, especially in harems where one or a few males (resident males) have exclusive breeding rights to females in a group.[227] Non-resident males can also be accepted in harems, but some species, such as the common vampire bat (Desmodus rotundus), may be more strict.[228]
Some mammals are perfectly monogamous, meaning that they mate for life and take no other partners (even after the original mate's death), as with wolves, Eurasian beavers, and otters.[229][230] There are three types of polygamy: either one or multiple dominant males have breeding rights (polygyny), multiple males that females mate with (polyandry), or multiple males have exclusive relations with multiple females (polygynandry). It is much more common for polygynous mating to happen, which, excluding leks, are estimated to occur in up to 90% of mammals.[231] Lek mating occurs when males congregate around females and try to attract them with various courtship displays and vocalizations, as in harbor seals.[232]
All higher mammals (excluding monotremes) share two major adaptations for care of the young: live birth and lactation. These imply a group-wide choice of a degree of parental care. They may build nests and dig burrows to raise their young in, or feed and guard them often for a prolonged period of time. Many mammals are K-selected, and invest more time and energy into their young than do r-selected animals. When two animals mate, they both share an interest in the success of the offspring, though often to different extremes. Mammalian females exhibit some degree of maternal aggression, another example of parental care, which may be targeted against other females of the species or the young of other females; however, some mammals may "aunt" the infants of other females, and care for them. Mammalian males may play a role in child rearing, as with tenrecs, however this varies species to species, even within the same genus. For example, the males of the southern pig-tailed macaque (Macaca nemestrina) do not participate in child care, whereas the males of the Japanese macaque (M. fuscata) do.[233]
Non-human mammals play a wide variety of roles in human culture. They are the most popular of pets, with tens of millions of dogs, cats and other animals including rabbits and mice kept by families around the world.[234][235][236] Mammals such as mammoths, horses and deer are among the earliest subjects of art, being found in Upper Paleolithic cave paintings such as at Lascaux.[237] Major artists such as Albrecht Dürer, George Stubbs and Edwin Landseer are known for their portraits of mammals.[238] Many species of mammals have been hunted for sport and for food; deer and wild boar are especially popular as game animals.[239][240][241] Mammals such as horses and dogs are widely raced for sport, often combined with betting on the outcome.[242][243] There is a tension between the role of animals as companions to humans, and their existence as individuals with rights of their own.[244] Mammals further play a wide variety of roles in literature,[245][246][247] film,[248] mythology, and religion.[249][250][251]
The domestication of mammals was instrumental in the Neolithic development of agriculture and of civilization, causing farmers to replace hunter-gatherers around the world.[b][253] This transition from hunting and gathering to herding flocks and growing crops was a major step in human history. The new agricultural economies, based on domesticated mammals, caused "radical restructuring of human societies, worldwide alterations in biodiversity, and significant changes in the Earth's landforms and its atmosphere... momentous outcomes".[254]
Domestic mammals form a large part of the livestock raised for meat across the world. They include (2009) around 1.4 billion cattle, 1 billion sheep, 1 billion domestic pigs,[255][256] and (1985) over 700 million rabbits.[257] Working domestic animals including cattle and horses have been used for work and transport from the origins of agriculture, their numbers declining with the arrival of mechanised transport and agricultural machinery. In 2004 they still provided some 80% of the power for the mainly small farms in the third world, and some 20% of the world's transport, again mainly in rural areas. In mountainous regions unsuitable for wheeled vehicles, pack animals continue to transport goods.[258] Mammal skins provide leather for shoes, clothing and upholstery. Wool from mammals including sheep, goats and alpacas has been used for centuries for clothing.[259][260]
Mammals serve a major role in science as experimental animals, both in fundamental biological research, such as in genetics,[262] and in the development of new medicines, which must be tested exhaustively to demonstrate their safety.[263] Millions of mammals, especially mice and rats, are used in experiments each year.[264] A knockout mouse is a genetically modified mouse with an inactivated gene, replaced or disrupted with an artificial piece of DNA. They enable the study of sequenced genes whose functions are unknown.[265] A small percentage of the mammals are non-human primates, used in research for their similarity to humans.[266][267][268]
Despite the benefits domesticated mammals had for human development, humans have an increasingly detrimental effect on wild mammals across the world. It has been estimated that the mass of all wild mammals has declined to only 4% of all mammals, with 96% of mammals being humans and their livestock now (see figure). In fact, terrestrial wild mammals make up only 2% of all mammals.[269][261]
Hybrids are offspring resulting from the breeding of two genetically distinct individuals, which usually will result in a high degree of heterozygosity, though hybrid and heterozygous are not synonymous. The deliberate or accidental hybridizing of two or more species of closely related animals through captive breeding is a human activity which has been in existence for millennia and has grown for economic purposes.[270] Hybrids between different subspecies within a species (such as between the Bengal tiger and Siberian tiger) are known as intra-specific hybrids. Hybrids between different species within the same genus (such as between lions and tigers) are known as interspecific hybrids or crosses. Hybrids between different genera (such as between sheep and goats) are known as intergeneric hybrids.[271] Natural hybrids will occur in hybrid zones, where two populations of species within the same genera or species living in the same or adjacent areas will interbreed with each other. Some hybrids have been recognized as species, such as the red wolf (though this is controversial).[272]
Artificial selection, the deliberate selective breeding of domestic animals, is being used to breed back recently extinct animals in an attempt to achieve an animal breed with a phenotype that resembles that extinct wildtype ancestor. A breeding-back (intraspecific) hybrid may be very similar to the extinct wildtype in appearance, ecological niche and to some extent genetics, but the initial gene pool of that wild type is lost forever with its extinction. As a result, bred-back breeds are at best vague look-alikes of extinct wildtypes, as Heck cattle are of the aurochs.[273]
Purebred wild species evolved to a specific ecology can be threatened with extinction[274] through the process of genetic pollution, the uncontrolled hybridization, introgression genetic swamping which leads to homogenization or out-competition from the heterosic hybrid species.[275] When new populations are imported or selectively bred by people, or when habitat modification brings previously isolated species into contact, extinction in some species, especially rare varieties, is possible.[276] Interbreeding can swamp the rarer gene pool and create hybrids, depleting the purebred gene pool. For example, the endangered wild water buffalo is most threatened with extinction by genetic pollution from the domestic water buffalo. Such extinctions are not always apparent from a morphological standpoint. Some degree of gene flow is a normal evolutionary process, nevertheless, hybridization threatens the existence of rare species.[277][278]
The loss of species from ecological communities, defaunation, is primarily driven by human activity.[279] This has resulted in empty forests, ecological communities depleted of large vertebrates.[280][281] In the Quaternary extinction event, the mass die-off of megafaunal variety coincided with the appearance of humans, suggesting a human influence. One hypothesis is that humans hunted large mammals, such as the woolly mammoth, into extinction.[282][283] The 2019 Global Assessment Report on Biodiversity and Ecosystem Services by IPBES states that the total biomass of wild mammals has declined by 82 percent since the beginning of human civilization.[284][285] Wild animals make up just 4% of mammalian biomass on earth, while humans and their domesticated animals make up 96%.[261]
Various species are predicted to become extinct in the near future,[286] among them the rhinoceros,[287] giraffes,[288] and species of primates[289] and pangolins.[290] According to the WWF's 2020 Living Planet Report, vertebrate wildlife populations have declined by 68% since 1970 as a result of human activities, particularly overconsumption, population growth and intensive farming, which is evidence that humans have triggered a sixth mass extinction event.[291][292] Hunting alone threatens hundreds of mammalian species around the world.[293][294] Scientists claim that the growing demand for meat is contributing to biodiversity loss as this is a significant driver of deforestation and habitat destruction; species-rich habitats, such as significant portions of the Amazon rainforest, are being converted to agricultural land for meat production.[295][296][297] Another influence is over-hunting and poaching, which can reduce the overall population of game animals,[298] especially those located near villages,[299] as in the case of peccaries.[300] The effects of poaching can especially be seen in the ivory trade with African elephants.[301] Marine mammals are at risk from entanglement from fishing gear, notably cetaceans, with discard mortalities ranging from 65,000 to 86,000 individuals annually.[302]
Attention is being given to endangered species globally, notably through the Convention on Biological Diversity, otherwise known as the Rio Accord, which includes 189 signatory countries that are focused on identifying endangered species and habitats.[303] Another notable conservation organization is the IUCN, which has a membership of over 1,200 governmental and non-governmental organizations.[304]
Recent extinctions can be directly attributed to human influences.[305][279] The IUCN characterizes 'recent' extinction as those that have occurred past the cut-off point of 1500,[306] and around 80 mammal species have gone extinct since that time and 2015.[307] Some species, such as the Père David's deer[308] are extinct in the wild, and survive solely in captive populations. Other species, such as the Florida panther, are ecologically extinct, surviving in such low numbers that they essentially have no impact on the ecosystem.[309]: 318  Other populations are only locally extinct (extirpated), still existing elsewhere, but reduced in distribution,[309]: 75–77  as with the extinction of gray whales in the Atlantic.[310]
As of this edit, this article uses content from "Anatomy and Physiology", which is licensed in a way that permits reuse under the Creative Commons Attribution-ShareAlike 3.0 Unported License, but not under the GFDL. All relevant terms must be followed.





In botany, a tree is a perennial plant with an elongated stem, or trunk, usually supporting branches and leaves. In some usages, the definition of a tree may be narrower, including only woody plants with secondary growth, plants that are usable as lumber or plants above a specified height. In wider definitions, the taller palms, tree ferns, bananas, and bamboos are also trees. 
Trees are not a monophyletic taxonomic group but consist of a wide variety of plant species that have independently evolved a trunk and branches as a way to tower above other plants to compete for sunlight. The majority of tree species are angiosperms or hardwoods; of the rest, many are gymnosperms or softwoods. Trees tend to be long-lived, some reaching several thousand years old. Trees have been in existence for 370 million years. It is estimated that there are around three trillion mature trees in the world.
A tree typically has many secondary branches supported clear of the ground by the trunk. This trunk typically contains woody tissue for strength, and vascular tissue to carry materials from one part of the tree to another. For most trees it is surrounded by a layer of bark which serves as a protective barrier. Below the ground, the roots branch and spread out widely; they serve to anchor the tree and extract moisture and nutrients from the soil. Above ground, the branches divide into smaller branches and shoots. The shoots typically bear leaves, which capture light energy and convert it into sugars by photosynthesis, providing the food for the tree's growth and development.
Trees usually reproduce using seeds. Flowers and fruit may be present, but some trees, such as conifers, instead have pollen cones and seed cones. Palms, bananas, and bamboos also produce seeds, but tree ferns produce spores instead.
Trees play a significant role in reducing erosion and moderating the climate. They remove carbon dioxide from the atmosphere and store large quantities of carbon in their tissues. Trees and forests provide a habitat for many species of animals and plants. Tropical rainforests are among the most biodiverse habitats in the world. Trees provide shade and shelter, timber for construction, fuel for cooking and heating, and fruit for food as well as having many other uses. In much of the world, forests are shrinking as trees are cleared to increase the amount of land available for agriculture. Because of their longevity and usefulness, trees have always been revered, with sacred groves in various cultures, and they play a role in many of the world's mythologies.
Although "tree" is a term of common parlance, there is no universally recognised precise definition of what a tree is, either botanically or in common language.[1][2] In its broadest sense, a tree is any plant with the general form of an elongated stem, or trunk, which supports the photosynthetic leaves or branches at some distance above the ground.[3] Trees are also typically defined by height,[4] with smaller plants from 0.5 to 10 m (1.6 to 32.8 ft) being called shrubs,[5] so the minimum height of a tree is only loosely defined.[4] Large herbaceous plants such as papaya and bananas are trees in this broad sense.[2][6]
A commonly applied narrower definition is that a tree has a woody trunk formed by secondary growth, meaning that the trunk thickens each year by growing outwards, in addition to the primary upwards growth from the growing tip.[4][7] Under such a definition, herbaceous plants such as palms, bananas and papayas are not considered trees regardless of their height, growth form or stem girth. Certain monocots may be considered trees under a slightly looser definition;[8] while the Joshua tree, bamboos and palms do not have secondary growth and never produce true wood with growth rings,[9][10] they may produce "pseudo-wood" by lignifying cells formed by primary growth.[11] Tree species in the genus Dracaena, despite also being monocots, do have secondary growth caused by meristem in their trunk, but it is different from the thickening meristem found in dicotyledonous trees.[12]
Aside from structural definitions, trees are commonly defined by use; for instance, as those plants which yield lumber.[13]
The tree growth habit is an evolutionary adaptation found in different groups of plants: by growing taller, trees are able to compete better for sunlight.[14] Trees tend to be tall and long-lived,[15] some reaching several thousand years old.[16] Several trees are among the oldest organisms now living.[17] Trees have modified structures such as thicker stems composed of specialised cells that add structural strength and durability, allowing them to grow taller than many other plants and to spread out their foliage. They differ from shrubs, which have a similar growth form, by usually growing larger and having a single main stem;[5] but there is no consistent distinction between a tree and a shrub,[18] made more confusing by the fact that trees may be reduced in size under harsher environmental conditions such as on mountains and subarctic areas. The tree form has evolved separately in unrelated classes of plants in response to similar environmental challenges, making it a classic example of parallel evolution. With an estimated 60,000-100,000 species, the number of trees worldwide might total twenty-five per cent of all living plant species.[19][20] The greatest number of these grow in tropical regions; many of these areas have not yet been fully surveyed by botanists, making tree diversity and ranges poorly known.[21]
The majority of tree species are angiosperms or hardwoods. Of the rest, many are gymnosperms or softwood trees;[22] these include conifers, cycads, ginkgophytes and gnetales, which produce seeds which are not enclosed in fruits, but in open structures such as pine cones, and many have tough waxy leaves, such as pine needles.[23] Most angiosperm trees are eudicots, the "true dicotyledons", so named because the seeds contain two cotyledons or seed leaves. There are also some trees among the old lineages of flowering plants called basal angiosperms or paleodicots; these include Amborella, Magnolia, nutmeg and avocado,[24] while trees such as bamboo, palms and bananas are monocots.
Wood gives structural strength to the trunk of most types of tree; this supports the plant as it grows larger. The vascular system of trees allows water, nutrients and other chemicals to be distributed around the plant, and without it trees would not be able to grow as large as they do. Trees, as relatively tall plants, need to draw water up the stem through the xylem from the roots by the suction produced as water evaporates from the leaves. If insufficient water is available the leaves will die.[25] The three main parts of trees include the root, stem, and leaves; they are integral parts of the vascular system which interconnects all the living cells. In trees and other plants that develop wood, the vascular cambium allows the expansion of vascular tissue that produces woody growth. Because this growth ruptures the epidermis of the stem, woody plants also have a cork cambium that develops among the phloem. The cork cambium gives rise to thickened cork cells to protect the surface of the plant and reduce water loss. Both the production of wood and the production of cork are forms of secondary growth.[26]
Trees are either evergreen, having foliage that persists and remains green throughout the year,[27] or deciduous, shedding their leaves at the end of the growing season and then having a dormant period without foliage.[28] Most conifers are evergreens, but larches (Larix and Pseudolarix) are deciduous, dropping their needles each autumn, and some species of cypress (Glyptostrobus, Metasequoia and Taxodium) shed small leafy shoots annually in a process known as cladoptosis.[5] The crown is the spreading top of a tree including the branches and leaves,[29] while the uppermost layer in a forest, formed by the crowns of the trees, is known as the canopy.[30] A sapling is a young tree.[31]
Many tall palms are herbaceous[32] monocots; these do not undergo secondary growth and never produce wood.[9][10] In many tall palms, the terminal bud on the main stem is the only one to develop, so they have unbranched trunks with large spirally arranged leaves. Some of the tree ferns, order Cyatheales, have tall straight trunks, growing up to 20 metres (66 ft), but these are composed not of wood but of rhizomes which grow vertically and are covered by numerous adventitious roots.[33]
The number of trees in the world, according to a 2015 estimate, is 3.04 trillion, of which 1.39 trillion (46%) are in the tropics or sub-tropics, 0.61 trillion (20%) in the temperate zones, and 0.74 trillion (24%) in the coniferous boreal forests. The estimate is about eight times higher than previous estimates, and is based on tree densities measured on over 400,000 plots. It remains subject to a wide margin of error, not least because the samples are mainly from Europe and North America. The estimate suggests that about 15 billion trees are cut down annually and about 5 billion are planted. In the 12,000 years since the start of human agriculture, the number of trees worldwide has decreased by 46%.[34][35][36][37] There are approximately 64,100 known tree species in the world. With 43% of all tree species, South America has the highest biodiversity, followed by Eurasia (22%), Africa (16%), North America (15%), and Oceania (11%).[38]
In suitable environments, such as the Daintree Rainforest in Queensland, or the mixed podocarp and broadleaf forest of Ulva Island, New Zealand, forest is the more-or-less stable climatic climax community at the end of a plant succession, where open areas such as grassland are colonised by taller plants, which in turn give way to trees that eventually form a forest canopy.[39][40]
In cool temperate regions, conifers often predominate; a widely distributed climax community in the far north of the northern hemisphere is moist taiga or northern coniferous forest (also called boreal forest).[41][42] Taiga is the world's largest land biome, forming 29% of the world's forest cover.[43] The long cold winter of the far north is unsuitable for plant growth and trees must grow rapidly in the short summer season when the temperature rises and the days are long. Light is very limited under their dense cover and there may be little plant life on the forest floor, although fungi may abound.[44] Similar woodland is found on mountains where the altitude causes the average temperature to be lower thus reducing the length of the growing season.[45]
Where rainfall is relatively evenly spread across the seasons in temperate regions, temperate broadleaf and mixed forest typified by species like oak, beech, birch and maple is found.[46] Temperate forest is also found in the southern hemisphere, as for example in the Eastern Australia temperate forest, characterised by Eucalyptus forest and open acacia woodland.[47]
In tropical regions with a monsoon or monsoon-like climate, where a drier part of the year alternates with a wet period as in the Amazon rainforest, different species of broad-leaved trees dominate the forest, some of them being deciduous.[48] In tropical regions with a drier savanna climate and insufficient rainfall to support dense forests, the canopy is not closed, and plenty of sunshine reaches the ground which is covered with grass and scrub. Acacia and baobab are well adapted to living in such areas.[49]
The roots of a tree serve to anchor it to the ground and gather water and nutrients to transfer to all parts of the tree. They are also used for reproduction, defence, survival, energy storage and many other purposes. The radicle or embryonic root is the first part of a seedling to emerge from the seed during the process of germination. This develops into a taproot which goes straight downwards. Within a few weeks lateral roots branch out of the side of this and grow horizontally through the upper layers of the soil. In most trees, the taproot eventually withers away and the wide-spreading laterals remain. Near the tip of the finer roots are single cell root hairs. These are in immediate contact with the soil particles and can absorb water and nutrients such as potassium in solution. The roots require oxygen to respire and only a few species such as mangroves and the pond cypress (Taxodium ascendens) can live in permanently waterlogged soil.[50]
In the soil, the roots encounter the hyphae of fungi. Many of these are known as mycorrhiza and form a mutualistic relationship with the tree roots. Some are specific to a single tree species, which will not flourish in the absence of its mycorrhizal associate. Others are generalists and associate with many species. The tree acquires minerals such as phosphorus from the fungus, while the fungus obtains the carbohydrate products of photosynthesis from the tree.[51] The hyphae of the fungus can link different trees and a network is formed, transferring nutrients and signals from one place to another.[52] The fungus promotes growth of the roots and helps protect the trees against predators and pathogens. It can also limit damage done to a tree by pollution as the fungus accumulate heavy metals within its tissues.[53] Fossil evidence shows that roots have been associated with mycorrhizal fungi since the early Paleozoic, four hundred million years ago, when the first vascular plants colonised dry land.[54]
Some trees such as Alder (Alnus species) have a symbiotic relationship with Frankia species, a filamentous bacterium that can fix nitrogen from the air, converting it into ammonia. They have actinorhizal root nodules on their roots in which the bacteria live. This process enables the tree to live in low nitrogen habitats where they would otherwise be unable to thrive.[55] The plant hormones called cytokinins initiate root nodule formation, in a process closely related to mycorrhizal association.[56]
It has been demonstrated that some trees are interconnected through their root system, forming a colony. The interconnections are made by the inosculation process, a kind of natural grafting or welding of vegetal tissues. The tests to demonstrate this networking are performed by injecting chemicals, sometimes radioactive, into a tree, and then checking for its presence in neighbouring trees.[57]
The roots are, generally, an underground part of the tree, but some tree species have evolved roots that are aerial. The common purposes for aerial roots may be of two kinds, to contribute to the mechanical stability of the tree, and to obtain oxygen from air. An instance of mechanical stability enhancement is the red mangrove that develops prop roots that loop out of the trunk and branches and descend vertically into the mud.[58] A similar structure is developed by the Indian banyan.[59] Many large trees have buttress roots which flare out from the lower part of the trunk. These brace the tree rather like angle brackets and provide stability, reducing sway in high winds. They are particularly prevalent in tropical rainforests where the soil is poor and the roots are close to the surface.[60]
Some tree species have developed root extensions that pop out of soil, in order to get oxygen, when it is not available in the soil because of excess water. These root extensions are called pneumatophores, and are present, among others, in black mangrove and pond cypress.[58]
The main purpose of the trunk is to raise the leaves above the ground, enabling the tree to overtop other plants and outcompete them for light.[61] It also transports water and nutrients from the roots to the aerial parts of the tree, and distributes the food produced by the leaves to all other parts, including the roots.[62]
In the case of angiosperms and gymnosperms, the outermost layer of the trunk is the bark, mostly composed of dead cells of phellem (cork).[63] It provides a thick, waterproof covering to the living inner tissue. It protects the trunk against the elements, disease, animal attack and fire. It is perforated by a large number of fine breathing pores called lenticels, through which oxygen diffuses. Bark is continually replaced by a living layer of cells called the cork cambium or phellogen.[63] The London plane (Platanus × acerifolia) periodically sheds its bark in large flakes. Similarly, the bark of the silver birch (Betula pendula) peels off in strips. As the tree's girth expands, newer layers of bark are larger in circumference, and the older layers develop fissures in many species. In some trees such as the pine (Pinus species) the bark exudes sticky resin which deters attackers whereas in rubber trees (Hevea brasiliensis) it is a milky latex that oozes out. The quinine bark tree (Cinchona officinalis) contains bitter substances to make the bark unpalatable.[62] Large tree-like plants with lignified trunks in the Pteridophyta, Arecales, Cycadophyta and Poales such as the tree ferns, palms, cycads and bamboos have different structures and outer coverings.[64]
Although the bark functions as a protective barrier, it is itself attacked by boring insects such as beetles. These lay their eggs in crevices and the larvae chew their way through the cellulose tissues leaving a gallery of tunnels. This may allow fungal spores to gain admittance and attack the tree. Dutch elm disease is caused by a fungus (Ophiostoma species) carried from one elm tree to another by various beetles. The tree reacts to the growth of the fungus by blocking off the xylem tissue carrying sap upwards and the branch above, and eventually the whole tree, is deprived of nourishment and dies. In Britain in the 1990s, 25 million elm trees were killed by this disease.[65]
The innermost layer of bark is known as the phloem and this is involved in the transport of the sap containing the sugars made by photosynthesis to other parts of the tree. It is a soft spongy layer of living cells, some of which are arranged end to end to form tubes. These are supported by parenchyma cells which provide padding and include fibres for strengthening the tissue.[66] Inside the phloem is a layer of undifferentiated cells one cell thick called the vascular cambium layer. The cells are continually dividing, creating phloem cells on the outside and wood cells known as xylem on the inside.[67]
The newly created xylem is the sapwood. It is composed of water-conducting cells and associated cells which are often living, and is usually pale in colour. It transports water and minerals from the roots to the upper parts of the tree. The oldest, inner part of the sapwood is progressively converted into heartwood as new sapwood is formed at the cambium. The conductive cells of the heartwood are blocked in some species. Heartwood is usually darker in colour than the sapwood. It is the dense central core of the trunk giving it rigidity. Three quarters of the dry mass of the xylem is cellulose, a polysaccharide, and most of the remainder is lignin, a complex polymer. A transverse section through a tree trunk or a horizontal core will show concentric circles of lighter or darker wood – tree rings.[68] These rings are the annual growth rings[69][70] There may also be rays running at right angles to growth rings. These are vascular rays which are thin sheets of living tissue permeating the wood.[68] Many older trees may become hollow but may still stand upright for many years.[71]
Trees do not usually grow continuously throughout the year but mostly have spurts of active expansion followed by periods of rest. This pattern of growth is related to climatic conditions; growth normally ceases when conditions are either too cold or too dry. In readiness for the inactive period, trees form buds to protect the meristem, the zone of active growth. Before the period of dormancy, the last few leaves produced at the tip of a twig form scales. These are thick, small and closely wrapped and enclose the growing point in a waterproof sheath. Inside this bud there is a rudimentary stalk and neatly folded miniature leaves, ready to expand when the next growing season arrives. Buds also form in the axils of the leaves ready to produce new side shoots. A few trees, such as the eucalyptus, have "naked buds" with no protective scales and some conifers, such as the Lawson's cypress, have no buds but instead have little pockets of meristem concealed among the scale-like leaves.[72]
When growing conditions improve, such as the arrival of warmer weather and the longer days associated with spring in temperate regions, growth starts again. The expanding shoot pushes its way out, shedding the scales in the process. These leave behind scars on the surface of the twig. The whole year's growth may take place in just a few weeks. The new stem is unlignified at first and may be green and downy. The Arecaceae (palms) have their leaves spirally arranged on an unbranched trunk.[72] In some tree species in temperate climates, a second spurt of growth, a Lammas growth may occur which is believed to be a strategy to compensate for loss of early foliage to insect predators.[73]
Primary growth is the elongation of the stems and roots. Secondary growth consists of a progressive thickening and strengthening of the tissues as the outer layer of the epidermis is converted into bark and the cambium layer creates new phloem and xylem cells. The bark is inelastic.[74] Eventually the growth of a tree slows down and stops and it gets no taller. If damage occurs the tree may in time become hollow.[75]
Leaves are structures specialised for photosynthesis and are arranged on the tree in such a way as to maximise their exposure to light without shading each other.[76] They are an important investment by the tree and may be thorny or contain phytoliths, lignins, tannins or poisons to discourage herbivory. Trees have evolved leaves in a wide range of shapes and sizes, in response to environmental pressures including climate and predation. They can be broad or needle-like, simple or compound, lobed or entire, smooth or hairy, delicate or tough, deciduous or evergreen. The needles of coniferous trees are compact but are structurally similar to those of broad-leaved trees. They are adapted for life in environments where resources are low or water is scarce. Frozen ground may limit water availability and conifers are often found in colder places at higher altitudes and higher latitudes than broad leaved trees. In conifers such as fir trees, the branches hang down at an angle to the trunk, enabling them to shed snow. In contrast, broad leaved trees in temperate regions deal with winter weather by shedding their leaves. When the days get shorter and the temperature begins to decrease, the leaves no longer make new chlorophyll and the red and yellow pigments already present in the blades become apparent.[76] Synthesis in the leaf of a plant hormone called auxin also ceases. This causes the cells at the junction of the petiole and the twig to weaken until the joint breaks and the leaf floats to the ground. In tropical and subtropical regions, many trees keep their leaves all year round. Individual leaves may fall intermittently and be replaced by new growth but most leaves remain intact for some time. Other tropical species and those in arid regions may shed all their leaves annually, such as at the start of the dry season.[77] Many deciduous trees flower before the new leaves emerge.[78] A few trees do not have true leaves but instead have structures with similar external appearance such as Phylloclades – modified stem structures[79] – as seen in the genus Phyllocladus.[80]
Trees can be pollinated either by wind or by animals, mostly insects. Many angiosperm trees are insect pollinated. Wind pollination may take advantage of increased wind speeds high above the ground.[81] Trees use a variety of methods of seed dispersal. Some rely on wind, with winged or plumed seeds. Others rely on animals, for example with edible fruits. Others again eject their seeds (ballistic dispersal), or use gravity so that seeds fall and sometimes roll.[82]
Seeds are the primary way that trees reproduce and their seeds vary greatly in size and shape. Some of the largest seeds come from trees, but the largest tree, Sequoiadendron giganteum, produces one of the smallest tree seeds.[83] The great diversity in tree fruits and seeds reflects the many different ways that tree species have evolved to disperse their offspring.
For a tree seedling to grow into an adult tree it needs light. If seeds only fell straight to the ground, competition among the concentrated saplings and the shade of the parent would likely prevent it from flourishing. Many seeds such as birch are small and have papery wings to aid dispersal by the wind. Ash trees and maples have larger seeds with blade shaped wings which spiral down to the ground when released. The kapok tree has cottony threads to catch the breeze.[84]
The seeds of conifers, the largest group of gymnosperms, are enclosed in a cone and most species have seeds that are light and papery that can be blown considerable distances once free from the cone.[85] Sometimes the seed remains in the cone for years waiting for a trigger event to liberate it. Fire stimulates release and germination of seeds of the jack pine, and also enriches the forest floor with wood ash and removes competing vegetation.[86] Similarly, a number of angiosperms including Acacia cyclops and Acacia mangium have seeds that germinate better after exposure to high temperatures.[87]
The flame tree Delonix regia does not rely on fire but shoots its seeds through the air when the two sides of its long pods crack apart explosively on drying.[84] The miniature cone-like catkins of alder trees produce seeds that contain small droplets of oil that help disperse the seeds on the surface of water. Mangroves often grow in water and some species have propagules, which are buoyant fruits with seeds that start germinating before becoming detached from the parent tree.[88][89] These float on the water and may become lodged on emerging mudbanks and successfully take root.[84]
Other seeds, such as apple pips and plum stones, have fleshy receptacles and smaller fruits like hawthorns have seeds enclosed in edible tissue; animals including mammals and birds eat the fruits and either discard the seeds, or swallow them so they pass through the gut to be deposited in the animal's droppings well away from the parent tree. The germination of some seeds is improved when they are processed in this way.[90] Nuts may be gathered by animals such as squirrels that cache any not immediately consumed.[91] Many of these caches are never revisited, the nut-casing softens with rain and frost, and the seed germinates in the spring.[92] Pine cones may similarly be hoarded by red squirrels, and grizzly bears may help to disperse the seed by raiding squirrel caches.[93]
The single extant species of Ginkgophyta (Ginkgo biloba) has fleshy seeds produced at the ends of short branches on female trees,[94] and Gnetum, a tropical and subtropical group of gymnosperms produce seeds at the tip of a shoot axis.[95]
The earliest trees were tree ferns, horsetails and lycophytes, which grew in forests in the Carboniferous period. The first tree may have been Wattieza, fossils of which have been found in New York state in 2007 dating back to the Middle Devonian (about 385 million years ago). Prior to this discovery, Archaeopteris was the earliest known tree.[96] Both of these reproduced by spores rather than seeds and are considered to be links between ferns and the gymnosperms which evolved in the Triassic period. The gymnosperms include conifers, cycads, gnetales and ginkgos and these may have appeared as a result of a whole genome duplication event which took place about 319 million years ago.[97] Ginkgophyta was once a widespread diverse group[98] of which the only survivor is the maidenhair tree Ginkgo biloba. This is considered to be a living fossil because it is virtually unchanged from the fossilised specimens found in Triassic deposits.[99]
During the Mesozoic (245 to 66 million years ago) the conifers flourished and became adapted to live in all the major terrestrial habitats. Subsequently, the tree forms of flowering plants evolved during the Cretaceous period. These began to displace the conifers during the Tertiary era (66 to 2 million years ago) when forests covered the globe.[100] When the climate cooled 1.5 million years ago and the first of four ice ages occurred, the forests retreated as the ice advanced. In the interglacials, trees recolonised the land that had been covered by ice, only to be driven back again in the next ice age.[100]
Trees are an important part of the terrestrial ecosystem,[101] providing essential habitats including many kinds of forest for communities of organisms. Epiphytic plants such as ferns, some mosses, liverworts, orchids and some species of parasitic plants (e.g., mistletoe) hang from branches;[102] these along with arboreal lichens, algae, and fungi provide micro-habitats for themselves and for other organisms, including animals. Leaves, flowers and fruits are seasonally available. On the ground underneath trees there is shade, and often there is undergrowth, leaf litter, and decaying wood that provide other habitat.[103][104] Trees stabilise the soil, prevent rapid run-off of rain water, help prevent desertification, have a role in climate control and help in the maintenance of biodiversity and ecosystem balance.[105]
Many species of tree support their own specialised invertebrates. In their natural habitats, 284 different species of insect have been found on the English oak (Quercus robur)[106] and 306 species of invertebrate on the Tasmanian oak (Eucalyptus obliqua).[107] Non-native tree species provide a less biodiverse community, for example in the United Kingdom the sycamore (Acer pseudoplatanus), which originates from southern Europe, has few associated invertebrate species, though its bark supports a wide range of lichens, bryophytes and other epiphytes.[108] Trees differ ecologically in the ease with which they can be found by herbivores. Tree apparency varies with a tree's size and semiochemical content, and with the extent to which it is concealed by nonhost neighbours from its insect pests.[109]
In ecosystems such as mangrove swamps, trees play a role in developing the habitat, since the roots of the mangrove trees reduce the speed of flow of tidal currents and trap water-borne sediment, reducing the water depth and creating suitable conditions for further mangrove colonisation. Thus mangrove swamps tend to extend seawards in suitable locations.[110] Mangrove swamps also provide an effective buffer against the more damaging effects of cyclones and tsunamis.[111]
Trees are the source of many of the world's best known fleshy fruits. Apples, pears, plums, cherries and citrus are all grown commercially in temperate climates and a wide range of edible fruits are found in the tropics. Other commercially important fruit include dates, figs and olives. Palm oil is obtained from the fruits of the oil palm (Elaeis guineensis). The fruits of the cocoa tree (Theobroma cacao) are used to make cocoa and chocolate and the berries of coffee trees, Coffea arabica and Coffea canephora, are processed to extract the coffee beans. In many rural areas of the world, fruit is gathered from forest trees for consumption.[112] Many trees bear edible nuts which can loosely be described as being large, oily kernels found inside a hard shell. These include coconuts (Cocos nucifera), Brazil nuts (Bertholletia excelsa), pecans (Carya illinoinensis), hazel nuts (Corylus), almonds (Prunus dulcis), walnuts (Juglans regia), pistachios (Pistacia vera) and many others. They are high in nutritive value and contain high-quality protein, vitamins and minerals as well as dietary fibre.[113] A variety of nut oils are extracted by pressing for culinary use; some such as walnut, pistachio and hazelnut oils are prized for their distinctive flavours, but they tend to spoil quickly.[114]
In temperate climates there is a sudden movement of sap at the end of the winter as trees prepare to burst into growth. In North America, the sap of the sugar maple (Acer saccharum) is most often used in the production of a sweet liquid, maple syrup. About 90% of the sap is water, the remaining 10% being a mixture of various sugars and certain minerals. The sap is harvested by drilling holes in the trunks of the trees and collecting the liquid that flows out of the inserted spigots. It is piped to a sugarhouse where it is heated to concentrate it and improve its flavour. Similarly in northern Europe the spring rise in the sap of the silver birch (Betula pendula) is tapped and collected, either to be drunk fresh or fermented into an alcoholic drink. In Alaska, the sap of the sweet birch (Betula lenta) is made into a syrup with a sugar content of 67%. Sweet birch sap is more dilute than maple sap; a hundred litres are required to make one litre of birch syrup.[115]
Various parts of trees are used as spices. These include cinnamon, made from the bark of the cinnamon tree (Cinnamomum zeylanicum) and allspice, the dried small fruits of the pimento tree (Pimenta dioica). Nutmeg is a seed found in the fleshy fruit of the nutmeg tree (Myristica fragrans) and cloves are the unopened flower buds of the clove tree (Syzygium aromaticum).[116]
Many trees have flowers rich in nectar which are attractive to bees. The production of forest honey is an important industry in rural areas of the developing world where it is undertaken by small-scale beekeepers using traditional methods.[117] The flowers of the elder (Sambucus) are used to make elderflower cordial and petals of the plum (Prunus spp.) can be candied.[118] Sassafras oil is a flavouring obtained from distilling bark from the roots of the sassafras tree (Sassafras albidum).
The leaves of trees are widely gathered as fodder for livestock and some can be eaten by humans but they tend to be high in tannins which makes them bitter. Leaves of the curry tree (Murraya koenigii) are eaten, those of kaffir lime (Citrus × hystrix) (in Thai food)[119] and Ailanthus (in Korean dishes such as bugak) and those of the European bay tree (Laurus nobilis) and the California bay tree (Umbellularia californica) are used for flavouring food.[116] Camellia sinensis, the source of tea, is a small tree but seldom reaches its full height, being heavily pruned to make picking the leaves easier.[120]
Wood smoke can be used to preserve food. In the hot smoking process the food is exposed to smoke and heat in a controlled environment. The food is ready to eat when the process is complete, having been tenderised and flavoured by the smoke it has absorbed. In the cold process, the temperature is not allowed to rise above 100 °F (38 °C). The flavour of the food is enhanced but raw food requires further cooking. If it is to be preserved, meat should be cured before cold smoking.[121]
Wood has traditionally been used for fuel, especially in rural areas. In less developed nations it may be the only fuel available and collecting firewood is often a time-consuming task as it becomes necessary to travel further and further afield in the search for fuel.[122] It is often burned inefficiently on an open fire. In more developed countries other fuels are available and burning wood is a choice rather than a necessity. Modern wood-burning stoves are very fuel efficient and new products such as wood pellets are available to burn.[123]
Charcoal can be made by slow pyrolysis of wood by heating it in the absence of air in a kiln. The carefully stacked branches, often oak, are burned with a very limited amount of air. The process of converting them into charcoal takes about fifteen hours. Charcoal is used as a fuel in barbecues and by blacksmiths and has many industrial and other uses.[124]
Timber, "trees that are grown in order to produce wood"[125] is cut into lumber (sawn wood) for use in construction. Wood has been an important, easily available material for construction since humans started building shelters. Engineered wood products are available which bind the particles, fibres or veneers of wood together with adhesives to form composite materials. Plastics have taken over from wood for some traditional uses.[126]
Wood is used in the construction of buildings, bridges, trackways, piles, poles for power lines, masts for boats, pit props, railway sleepers, fencing, hurdles, shuttering for concrete, pipes, scaffolding and pallets. In housebuilding it is used in joinery, for making joists, roof trusses, roofing shingles, thatching, staircases, doors, window frames, floor boards, parquet flooring, panelling and cladding.[127]
Wood is used to construct carts, farm implements, boats, dugout canoes and in shipbuilding. It is used for making furniture, tool handles, boxes, ladders, musical instruments, bows, weapons, matches, clothes pegs, brooms, shoes, baskets, turnery, carving, toys, pencils, rollers, cogs, wooden screws, barrels, coffins, skittles, veneers, artificial limbs, oars, skis, wooden spoons, sports equipment and wooden balls.[127]
Wood is pulped for paper and used in the manufacture of cardboard and made into engineered wood products for use in construction such as fibreboard, hardboard, chipboard and plywood.[127] The wood of conifers is known as softwood while that of broad-leaved trees is hardwood.[128]
Besides inspiring artists down the centuries, trees have been used to create art. Living trees have been used in bonsai and in tree shaping, and both living and dead specimens have been sculpted into sometimes fantastic shapes.[129]
Bonsai (盆栽, lit. "Tray planting")[130] is the practice of hòn non bộ originated in China and spread to Japan more than a thousand years ago, there are similar practices in other cultures like the living miniature landscapes of Vietnam hòn non bộ. The word bonsai is often used in English as an umbrella term for all miniature trees in containers or pots.[131]
The purposes of bonsai are primarily contemplation (for the viewer) and the pleasant exercise of effort and ingenuity (for the grower).[132] Bonsai practice focuses on long-term cultivation and shaping of one or more small trees growing in a container, beginning with a cutting, seedling, or small tree of a species suitable for bonsai development. Bonsai can be created from nearly any perennial woody-stemmed tree or shrub species[133] that produces true branches and can be cultivated to remain small through pot confinement with crown and root pruning. Some species are popular as bonsai material because they have characteristics, such as small leaves or needles, that make them appropriate for the compact visual scope of bonsai and a miniature deciduous forest can even be created using such species as Japanese maple, Japanese zelkova or hornbeam.[134]
Tree shaping is the practice of changing living trees and other woody plants into man made shapes for art and useful structures. There are a few different methods[135] of shaping a tree. There is a gradual method and there is an instant method. The gradual method slowly guides the growing tip along predetermined pathways over time whereas the instant method bends and weaves saplings 2 to 3 m (6.6 to 9.8 ft) long into a shape that becomes more rigid as they thicken up.[136] Most artists use grafting of living trunks, branches, and roots, for art or functional structures and there are plans to grow "living houses" with the branches of trees knitting together to give a solid, weatherproof exterior combined with an interior application of straw and clay to provide a stucco-like inner surface.[136]
Tree shaping has been practised for at least several hundred years, the oldest known examples being the living root bridges built and maintained by the Khasi people of Meghalaya, India using the roots of the rubber tree (Ficus elastica).[137][138]
Cork is produced from the thick bark of the cork oak (Quercus suber). It is harvested from the living trees about once every ten years in an environmentally sustainable industry.[139] More than half the world's cork comes from Portugal and is largely used to make stoppers for wine bottles.[140] Other uses include floor tiles, bulletin boards, balls, footwear, cigarette tips, packaging, insulation and joints in woodwind instruments.[140]
The bark of other varieties of oak has traditionally been used in Europe for the tanning of hides though bark from other species of tree has been used elsewhere. The active ingredient, tannin, is extracted and after various preliminary treatments, the skins are immersed in a series of vats containing solutions in increasing concentrations. The tannin causes the hide to become supple, less affected by water and more resistant to bacterial attack.[141]
At least 120 drugs come from plant sources, many of them from the bark of trees.[142] Quinine originates from the cinchona tree (Cinchona) and was for a long time the remedy of choice for the treatment of malaria.[143] Aspirin was synthesised to replace the sodium salicylate derived from the bark of willow trees (Salix) which had unpleasant side effects.[144] The anti-cancer drug Paclitaxel is derived from taxol, a substance found in the bark of the Pacific yew (Taxus brevifolia).[145] Other tree based drugs come from the paw-paw (Carica papaya), the cassia (Cassia spp.), the cocoa tree (Theobroma cacao), the tree of life (Camptotheca acuminata) and the downy birch (Betula pubescens).[142]
The papery bark of the white birch tree (Betula papyrifera) was used extensively by Native Americans. Wigwams were covered by it and canoes were constructed from it. Other uses included food containers, hunting and fishing equipment, musical instruments, toys and sledges.[146] Nowadays, bark chips, a by-product of the timber industry, are used as a mulch and as a growing medium for epiphytic plants that need a soil-free compost.[147]
Trees create a visual impact in the same way as do other landscape features and give a sense of maturity and permanence to park and garden. They are grown for the beauty of their forms, their foliage, flowers, fruit and bark and their siting is of major importance in creating a landscape. They can be grouped informally, often surrounded by plantings of bulbs, laid out in stately avenues or used as specimen trees. As living things, their appearance changes with the season and from year to year.[148]
Trees are often planted in town environments where they are known as street trees or amenity trees. They can provide shade and cooling through evapotranspiration, absorb greenhouse gases and pollutants, intercept rainfall, and reduce the risk of flooding. Scientific studies show that street trees help cities be more sustainable, and improve the physical and mental wellbeing of the citizens.[149] It has been shown that they are beneficial to humans in creating a sense of well-being and reducing stress. Many towns have initiated tree-planting programmes.[150] In London for example, there is an initiative to plant 20,000 new street trees and to have an increase in tree cover of 5% by 2025, equivalent to one tree for every resident.[151]
Latex is a sticky defensive secretion that protects plants against herbivores. Many trees produce it when injured but the main source of the latex used to make natural rubber is the Pará rubber tree (Hevea brasiliensis). Originally used to create bouncy balls and for the waterproofing of cloth, natural rubber is now mainly used in tyres for which synthetic materials have proved less durable.[152] The latex exuded by the balatá tree (Manilkara bidentata) is used to make golf balls and is similar to gutta-percha, made from the latex of the "getah perca" tree Palaquium. This is also used as an insulator, particularly of undersea cables, and in dentistry, walking sticks and gun butts. It has now largely been replaced by synthetic materials.[153]
Resin is another plant exudate that may have a defensive purpose. It is a viscous liquid composed mainly of volatile terpenes and is produced mostly by coniferous trees. It is used in varnishes, for making small castings and in ten-pin bowling balls. When heated, the terpenes are driven off and the remaining product is called "rosin" and is used by stringed instrumentalists on their bows. Some resins contain essential oils and are used in incense and aromatherapy. Fossilised resin is known as amber and was mostly formed in the Cretaceous (145 to 66 million years ago) or more recently. The resin that oozed out of trees sometimes trapped insects or spiders and these are still visible in the interior of the amber.[154]
The camphor tree (Cinnamomum camphora) produces an essential oil[116] and the eucalyptus tree (Eucalyptus globulus) is the main source of eucalyptus oil which is used in medicine, as a fragrance and in industry.[155]
Dead trees pose a safety risk, especially during high winds and severe storms, and removing dead trees involves a financial burden, whereas the presence of healthy trees can clean the air, increase property values, and reduce the temperature of the built environment and thereby reduce building cooling costs. During times of drought, trees can fall into water stress, which may cause a tree to become more susceptible to disease and insect problems, and ultimately may lead to a tree's death. Irrigating trees during dry periods can reduce the risk of water stress and death.[156]
About a third of all tree species, some twenty thousand, are included in the IUCN Red List of Threatened Species. Of those, over eight thousand are globally threatened, including at least 1400 which are classed as "critically endangered".[157]
Trees have been venerated since time immemorial. To the ancient Celts, certain trees, especially the oak, ash and thorn, held special significance[158] as providing fuel, building materials, ornamental objects and weaponry. Other cultures have similarly revered trees, often linking the lives and fortunes of individuals to them or using them as oracles. In Greek mythology, dryads were believed to be shy nymphs who inhabited trees.
The Oubangui people of west Africa plant a tree when a child is born. As the tree flourishes, so does the child but if the tree fails to thrive, the health of the child is considered at risk. When it flowers it is time for marriage. Gifts are left at the tree periodically and when the individual dies, their spirit is believed to live on in the tree.[159]
Trees have their roots in the ground and their trunk and branches extended towards the sky. This concept is found in many of the world's religions as a tree which links the underworld and the earth and holds up the heavens. In Norse mythology, Yggdrasil is a central cosmic tree whose roots and branches extend to various worlds. Various creatures live on it.[160] In India, Kalpavriksha is a wish-fulfilling tree, one of the nine jewels that emerged from the primitive ocean. Icons are placed beneath it to be worshipped, tree nymphs inhabit the branches and it grants favours to the devout who tie threads round the trunk.[161] Democracy started in North America when the Great Peacemaker formed the Iroquois Confederacy, inspiring the warriors of the original five American nations to bury their weapons under the Tree of Peace, an eastern white pine (Pinus strobus).[162] In the creation story in the Bible, the tree of life and the knowledge of good and evil was planted by God in the Garden of Eden.[163]
Sacred groves exist in China, India, Africa and elsewhere. They are places where the deities live and where all the living things are either sacred or are companions of the gods. Folklore lays down the supernatural penalties that will result if desecration takes place for example by the felling of trees. Because of their protected status, sacred groves may be the only relicts of ancient forest and have a biodiversity much greater than the surrounding area.[164] Some Ancient Indian tree deities, such as Puliyidaivalaiyamman, the Tamil deity of the tamarind tree, or Kadambariyamman, associated with the cadamba tree, were seen as manifestations of a goddess who offers her blessings by giving fruits in abundance.[165]
Trees have a theoretical maximum height of 130 m (430 ft),[166] but the tallest known specimen on earth is believed to be a coast redwood (Sequoia sempervirens) at Redwood National Park, California. It has been named Hyperion and is 115.85 m (380.1 ft) tall.[167] In 2006, it was reported to be 379.1 ft (115.5 m) tall.[168] The tallest known broad-leaved tree is a mountain ash (Eucalyptus regnans) growing in Tasmania with a height of 99.8 m (327 ft).[169]
The largest tree by volume is believed to be a giant sequoia (Sequoiadendron giganteum) known as the General Sherman Tree in the Sequoia National Park in Tulare County, California. Only the trunk is used in the calculation and the volume is estimated to be 1,487 m3 (52,500 cu ft).[170]
The oldest living tree with a verified age is also in California. It is a Great Basin bristlecone pine (Pinus longaeva) growing in the White Mountains. It has been dated by drilling a core sample and counting the annual rings. It is estimated to currently be 5,077 years old.[a][171]
A little farther south, at Santa Maria del Tule, Oaxaca, Mexico, is the tree with the broadest trunk. It is a Montezuma cypress (Taxodium mucronatum) known as Árbol del Tule and its diameter at breast height is 11.62 m (38.1 ft) giving it a girth of 36.2 m (119 ft). The tree's trunk is far from round and the exact dimensions may be misleading as the circumference includes much empty space between the large buttress roots.[172]



Tin is a chemical element with the symbol Sn (from Latin  stannum) and atomic number 50. A silvery-coloured metal, tin is soft enough to be cut with little force,[8] and a bar of tin can be bent by hand with little effort. When bent, the so-called "tin cry" can be heard as a result of twinning in tin crystals;[9] this trait is shared by indium, cadmium, zinc, and mercury in its solid state.
Pure tin after solidifying presents a mirror-like appearance similar to most metals. In most tin alloys (e.g. pewter) the metal solidifies with a dull grey colour.
Tin is a post-transition metal in group 14 of the periodic table of elements. It is obtained chiefly from the mineral cassiterite, which contains stannic oxide, SnO2. Tin shows a chemical similarity to both of its neighbors in group 14, germanium and lead, and has two main oxidation states, +2 and the slightly more stable +4. Tin is the 49th-most abundant element on Earth and has, with 10 stable isotopes, the largest number of stable isotopes in the periodic table, due to its magic number of protons.
It has two main allotropes: at room temperature, the stable allotrope is β-tin, a silvery-white, malleable metal; at low temperatures it is less dense grey α-tin, which has the diamond cubic structure. Metallic tin does not easily oxidize in air and water.
The first tin alloy used on a large scale was bronze, made of 1⁄8 tin and 7⁄8 copper, from as early as 3000 BC. After 600 BC, pure metallic tin was produced. Pewter, which is an alloy of 85–90% tin with the remainder commonly consisting of copper, antimony, bismuth, and sometimes lead and silver, has been used for flatware since the Bronze Age. In modern times, tin is used in many alloys, most notably tin-lead soft solders, which are typically 60% or more tin, and in the manufacture of transparent, electrically conducting films of indium tin oxide in optoelectronic applications. Another large application is corrosion-resistant tin plating of steel. Because of the low toxicity of inorganic tin, tin-plated steel is widely used for food packaging as tin cans. Some organotin compounds can be extremely toxic.
Tin is a soft, malleable, ductile and highly crystalline silvery-white metal. When a bar of tin is bent a crackling sound known as the "tin cry" can be heard from the twinning of the crystals.[9] Tin melts at about 232 °C (450 °F), the lowest in group 14. The melting point is further lowered to 177.3 °C (351.1 °F) for 11 nm particles.[10][11]
β-tin, also called white tin, is the allotrope (structural form) of elemental tin that is stable at and above room temperature. It is metallic and malleable, and has body-centered tetragonal crystal structure. α-tin, or gray tin, is the nonmetallic form. It is stable below 13.2 °C (55.8 °F) and is brittle. α-tin has a diamond cubic crystal structure, as do diamond and silicon. α-tin does not have metallic properties because its atoms form a covalent structure in which electrons cannot move freely. α-tin is a dull-gray powdery material with no common uses other than specialized semiconductor applications.[9] γ-tin and σ-tin exist at temperatures above 161 °C (322 °F)  and pressures above several GPa.[12]
In cold conditions β-tin tends to transform spontaneously into α-tin, a phenomenon known as "tin pest" or "tin disease".[13] Some unverifiable sources also say that, during Napoleon's Russian campaign of 1812, the temperatures became so cold that the tin buttons on the soldiers' uniforms disintegrated over time, contributing to the defeat of the Grande Armée,[14] a persistent legend.[15][16][17]
The α-β transformation temperature is 13.2 °C (55.8 °F), but impurities (e.g. Al, Zn, etc.) lower it well below 0 °C (32 °F). With the addition of antimony or bismuth the transformation might not occur at all, increasing durability.[18]
Commercial grades of tin (99.8% tin content) resist transformation because of the inhibiting effect of small amounts of bismuth, antimony, lead, and silver present as impurities. Alloying elements such as copper, antimony, bismuth, cadmium, and silver increase the hardness of tin.[19] Tin easily forms hard, brittle intermetallic phases that are typically undesirable. It does not mix into a solution with most metals and elements so tin does not have much solid solubility. Tin mixes well with bismuth, gallium, lead, thallium and zinc, forming simple eutectic systems.[18]
Tin becomes a superconductor below 3.72 K[20] and was one of the first superconductors to be studied.[21] The Meissner effect, one of the characteristic features of superconductors, was first discovered in superconducting tin crystals.[21]
Tin resists corrosion from water, but can be corroded by acids and alkalis. Tin can be highly polished and is used as a protective coat for other metals,[9] a protective oxide (passivation) layer prevents further oxidation.[22] Tin acts as a catalyst triggering a chemical reaction of a solution containing oxygen and helps to increase the speed of the chemical reaction that results.[23]
Tin has ten stable isotopes, the greatest number of any element. The isotopes of tin have atomic masses of 112, 114, 115, 116, 117, 118, 119, 120, 122, and 124. 120Sn makes up almost a third of all tin; 118Sn, and 116Sn are also common, while 115Sn is the least common stable isotope. The isotopes with even mass numbers have no nuclear spin, while those with odd mass numbers have a spin of 1/2. Tin is among the easiest elements to detect and analyze by NMR spectroscopy which relies on molecular weight and its chemical shifts are referenced against SnMe4.[notes 1][24] The large number of stable isotopes is thought to be a direct result of tin having the atomic number 50, a "magic number" in nuclear physics.  Of the stable isotopes Tin-115 has a high capture cross section for fast neutron energies at 30 Barns. Two other isotopes Tin-117 ranks next with a cross section of 2.3 Barn while isotope Tin-119 has a slightly smaller cross section of 2.2 Barn. [25] Before these cross sections were well known it was proposed to use Tin-Lead solder as a reactor coolant for fast reactors because of its low melting point. Current studies are for Lead or Lead-Bismuth reactor coolants because both heavy metals are nearly transparent to fast neutrons with very low capture cross sections. [26] In order to use a Tin or Tin-Lead coolant the Tin would first have to go through isotopes separation to remove the 115, 117 and 119 isotopes from the material. Combined these three isotopes make up about 17% of the entire mass of natural Tin but represent nearly all of the capture cross section. Of the remaining seven isotopes Tin-112 has a capture cross section of 1 Barn. The other six isotopes forming 82.7% of all Tin have capture cross sections of 0.3 Barn or less making them effectively transparent to neutrons like Lead and Bismuth.
Tin has 31 unstable isotopes, ranging in mass number from 99 to 139. The unstable tin isotopes have a half-life of less than a year except 126Sn which has a half-life of 230,000 years. 
100Sn and 132Sn are two of the few nuclides with a "doubly magic" nucleus which despite being unstable, as they have very uneven neutron–proton ratios, are the endpoints beyond which tin isotopes lighter than 100Sn and heavier than 132Sn are much less stable.[27] Another 30 metastable isomers have been identified for tin isotopes between 111 and 131, the most stable being 121mSn, with a half-life of 43.9 years.[28]
The relative differences in the number of tin's stable isotopes can be explained by how they are formed during stellar nucleosynthesis. 116Sn through 120Sn are formed in the s-process (slow neutron capture) in most stars which leads to them being the most common tin isotopes, while 122Sn and 124Sn are only formed in the r-process (rapid neutron capture) in supernovae and are less common. Tin isotopes 117Sn through 120Sn are also produced in the r-process.[citation needed] 112Sn, 114Sn, and 115Sn, cannot be made in significant amounts in the s- or r-processes and are among the p-nuclei whose origins are not well understood. Some ideas about for their formation include proton capture and photodisintegration, 115Sn might be partially produced in the s-process both directly and as the daughter of long-lived 115In.[29]
The word tin is shared among Germanic languages and can be traced back to reconstructed Proto-Germanic *tin-om; cognates include German Zinn, Swedish tenn and Dutch tin. It is not found in other branches of Indo-European, except by borrowing from Germanic (e.g., Irish tinne from English).[30][31]
The Latin name for tin, stannum, originally meant an alloy of silver and lead, and came to mean 'tin' in the fourth century[32]—the earlier Latin word for it was plumbum candidum, or "white lead". Stannum apparently came from an earlier stāgnum (meaning the same substance),[30] the origin of the Romance and Celtic terms for tin, such as French étain, Spanish estaño, Italian stagno, and Irish stán.[30][33] The origin of stannum/stāgnum is unknown; it may be pre-Indo-European.[34]
The Meyers Konversations-Lexikon suggests instead that stannum came from Cornish stean, and is evidence that Cornwall in the first centuries AD was the main source of tin.[citation needed]
Tin extraction and use can be dated to the beginnings of the Bronze Age around 3000 BC, when it was observed that copper objects formed of polymetallic ores with different metal contents had different physical properties.[35] The earliest bronze objects had a tin or arsenic content of less than 2% and are believed to be the result of unintentional alloying due to trace metal content in the copper ore.[36] The addition of a second metal to copper increases its hardness, lowers the melting temperature, and improves the casting process by producing a more fluid melt that cools to a denser, less spongy metal.[36] This was an important innovation that allowed for the much more complex shapes cast in closed molds of the Bronze Age. Arsenical bronze objects appear first in the Near East where arsenic is commonly found with copper ore, but the health risks were quickly realized and the quest for sources of the much less hazardous tin ores began early in the Bronze Age.[37] This created the demand for rare tin metal and formed a trade network that linked the distant sources of tin to the markets of Bronze Age cultures.[citation needed]
Cassiterite (SnO2), the oxide form of tin, was most likely the original source of tin. Other tin ores are less common sulfides such as stannite that require a more involved smelting process. Cassiterite often accumulates in alluvial channels as placer deposits because it is harder, heavier, and more chemically resistant than the accompanying granite.[36] Cassiterite is usually black or dark in color, and these deposits can be easily seen in river banks. Alluvial (placer) deposits may incidentally have been collected and separated by methods similar to gold panning.[38]
In the great majority of its compounds, tin has the oxidation state II or IV. Compounds containing bivalent tin are called stannous while those containing tetravalent tin are termed stannic.
Halide compounds are known for both oxidation states. For Sn(IV), all four halides are well known: SnF4, SnCl4, SnBr4, and SnI4. The three heavier members are volatile molecular compounds, whereas the tetrafluoride is polymeric. All four halides are known for Sn(II) also: SnF2, SnCl2, SnBr2, and SnI2. All are polymeric solids. Of these eight compounds, only the iodides are colored.[39]
Tin(II) chloride (also known as stannous chloride) is the most important commercial tin halide. Illustrating the routes to such compounds, chlorine reacts with tin metal to give SnCl4 whereas the reaction of hydrochloric acid and tin produces SnCl2 and hydrogen gas. Alternatively SnCl4 and Sn combine to stannous chloride by a process called comproportionation:[40]
Tin can form many oxides, sulfides, and other chalcogenide derivatives. The dioxide SnO2 (cassiterite) forms when tin is heated in the presence of air.[39] Sn)2 is amphoteric, which means that it dissolves in both acidic and basic solutions.[41] Stannates with the structure [Sn(OH)6]2−, like K2[Sn(OH)6], are also known, though the free stannic acid H2[Sn(OH)6] is unknown.
Sulfides of tin exist in both the +2 and +4 oxidation states: tin(II) sulfide and tin(IV) sulfide (mosaic gold).
Stannane (SnH4), with tin in the +4 oxidation state, is unstable. Organotin hydrides are however well known, e.g. tributyltin hydride (Sn(C4H9)3H).[9] These compound release transient tributyl tin radicals, which are rare examples of compounds of tin(III).[43]
Organotin compounds, sometimes called stannanes, are chemical compounds with tin–carbon bonds.[44] Of the tin compounds, the organic derivatives are commercially the most useful.[45] Some organotin compounds are highly toxic and have been used as biocides. The first organotin compound to be reported was diethyltin diiodide ((C2H5)2SnI2), reported by Edward Frankland in 1849.[46]
Most organotin compounds are colorless liquids or solids that are stable to air and water. They adopt tetrahedral geometry. Tetraalkyl- and tetraaryltin compounds can be prepared using Grignard reagents:[45]
The mixed halide-alkyls, which are more common and more important commercially than the tetraorgano derivatives, are prepared by redistribution reactions:
Divalent organotin compounds are uncommon, although more common than related divalent organogermanium and organosilicon compounds. The greater stabilization enjoyed by Sn(II) is attributed to the "inert pair effect". Organotin(II) compounds include both stannylenes (formula: R2Sn, as seen for singlet carbenes) and distannylenes (R4Sn2), which are roughly equivalent to alkenes. Both classes exhibit unusual reactions.[47]
Tin is generated via the long s-process in low-to-medium mass stars (with masses of 0.6 to 10 times that of the Sun), and finally by beta decay of the heavy isotopes of indium.[48]
Tin is the 49th most abundant element in Earth's crust, representing 2 ppm compared with 75 ppm for zinc, 50 ppm for copper, and 14 ppm for lead.[49]
Tin does not occur as the native element but must be extracted from various ores. Cassiterite (SnO2) is the only commercially important source of tin, although small quantities of tin are recovered from complex sulfides such as stannite, cylindrite, franckeite, canfieldite, and teallite. Minerals with tin are almost always associated with granite rock, usually at a level of 1% tin oxide content.[50]
Because of the higher specific gravity of tin dioxide, about 80% of mined tin is from secondary deposits found downstream from the primary lodes. Tin is often recovered from granules washed downstream in the past and deposited in valleys or the sea. The most economical ways of mining tin are by dredging, hydraulicking, or open pits. Most of the world's tin is produced from placer deposits, which can contain as little as 0.015% tin.[51]
About 253,000 tonnes of tin were mined in 2011, mostly in China (110,000 t), Indonesia (51,000 t), Peru (34,600 t), Bolivia (20,700 t) and Brazil (12,000 t).[52] Estimates of tin production have historically varied with the market and mining technology. It is estimated that, at current consumption rates and technologies, the Earth will run out of mine-able tin in 40 years.[53] In 2006 Lester Brown suggested tin could run out within 20 years based on conservative estimates of 2% annual growth.[54]
Scrap tin is an important source of the metal. Recovery of tin through recycling is increasing rapidly.[when?][citation needed] Whereas the United States has neither mined (since 1993) nor smelted (since 1989) tin, it was the largest secondary producer, recycling nearly 14,000 tonnes in 2006.[52]
New deposits are reported in Mongolia,[55] and in 2009, new deposits of tin were discovered in Colombia.[56]
Tin is produced by carbothermic reduction of the oxide ore with carbon or coke. Both reverberatory furnace and electric furnace can be used:[57][58][59]
The ten largest companies produced most of the world's tin in 2007.
Most of the world's tin is traded on LME, from 8 countries, under 17 brands.[60]
International Tin Council was established in 1947 to control the price of tin. It collapsed in 1985. In 1984, Association of Tin Producing Countries was created, with Australia, Bolivia, Indonesia, Malaysia, Nigeria, Thailand, and Zaire as members.[63]
Tin is unique among mineral commodities because of the complex agreements between producer countries and consumer countries dating back to 1921. Earlier agreements tended to be somewhat informal and led to the "First International Tin Agreement" in 1956, the first of a series that effectively collapsed in 1985. Through these agreements, the International Tin Council (ITC) had a considerable effect on tin prices. ITC supported the price of tin during periods of low prices by buying tin for its buffer stockpile and was able to restrain the price during periods of high prices by selling from the stockpile. This was an anti-free-market approach, designed to assure a sufficient flow of tin to consumer countries and a profit for producer countries. However, the buffer stockpile was not sufficiently large, and during most of those 29 years tin prices rose, sometimes sharply, especially from 1973 through 1980 when rampant inflation plagued many world economies.[64]
During the late 1970s and early 1980s, the U.S. reduced its strategic tin stockpile, partly to take advantage of historically high tin prices. The 1981–82 recession damaged the tin industry. Tin consumption declined dramatically. ITC was able to avoid truly steep declines through accelerated buying for its buffer stockpile; this activity required extensive borrowing. ITC continued to borrow until late 1985 when it reached its credit limit. Immediately, a major "tin crisis" ensued — tin was delisted from trading on the London Metal Exchange for about three years.  ITC dissolved soon afterward, and the price of tin, now in a free-market environment, fell to $4 per pound and remained around that level through the 1990s.[64] The price increased again by 2010 with a rebound in consumption following the 2007–2008 economic crisis, accompanying restocking and continued growth in consumption.[52]
London Metal Exchange (LME) is tin's principal trading site.[52] Other tin contract markets are Kuala Lumpur Tin Market (KLTM) and Indonesia Tin Exchange (INATIN).[65]
Due to factors involved in the 2021 global supply chain crisis, tin prices almost doubled between 2020—21 and have had their largest annual rise in over 30 years. The International Tin Association estimated that global refined tin consumption will grow 7.2 percent in 2021, after losing 1.6 percent in 2020 as the COVID-19 pandemic disrupted global manufacturing industries.[66]
In 2018, just under half of all tin produced was used in solder. The rest was divided between tin plating, tin chemicals, brass and bronze alloys, and niche uses.[67]
Tin has long been used in alloys with lead as solder, in amounts of 5 to 70% w/w. Tin with lead forms a eutectic mixture at the weight proportion of 61.9% tin and 38.1% lead (the atomic proportion: 73.9% tin and 26.1% lead), with melting temperature of 183 °C (361.4 °F). Such solders are primarily used for joining pipes or electric circuits. Since the European Union Waste Electrical and Electronic Equipment Directive (WEEE Directive) and Restriction of Hazardous Substances Directive came into effect on 1 July 2006, the lead content in such alloys has decreased. While lead exposure is associated with serious health problems, lead-free solder is not without its challenges, including a higher melting point, and the formation of tin whiskers that cause electrical problems. Tin pest can occur in lead-free solders, leading to loss of the soldered joint. Replacement alloys are being found, but the problems of joint integrity remain.[68]
Tin bonds readily to iron and is used for coating lead, zinc, and steel to prevent corrosion. Tin-plated (or tinning) steel containers is widely used for food preservation, and this forms a large part of the market for metallic tin. A tinplate canister for preserving food was first manufactured in London in 1812.[69] Speakers of British English call such containers "tins", while speakers of U.S. English call them "cans" or "tin cans". One derivation of such use is the slang term "tinnie" or "tinny", meaning "can of beer" in Australia. The tin whistle is so called because it was mass-produced first in tin-plated steel.[70][71] 
Copper cooking vessels such as saucepans and frying pans are frequently lined with a thin plating of tin, by electroplating or by traditional chemical methods, since use of copper cookware with acidic foods can be toxic.
Tin in combination with other elements forms a wide variety of useful alloys. Tin is most commonly alloyed with copper. Pewter is 85–99% tin;[72] bearing metal has a high percentage of tin as well.[73][74] Bronze is mostly copper with 12% tin, while the addition of phosphorus yields phosphor bronze. Bell metal is also a copper–tin alloy, containing 22% tin. Tin has sometimes been used in coinage; it once formed a single-digit percentage (usually five percent or less) of American[75] and Canadian[76] pennies. Because copper is often the major metal in such coins, sometimes including zinc, these could be called bronze, or brass alloys.
The niobium–tin compound Nb3Sn is commercially used in coils of superconducting magnets for its high critical temperature (18 K) and critical magnetic field (25 T). A superconducting magnet weighing as little as two kilograms is capable of producing the magnetic field of a conventional electromagnet weighing tons.[77]
A small percentage of tin is added to zirconium alloys for the cladding of nuclear fuel.[78]
Most metal pipes in a pipe organ are of a tin/lead alloy, with 50/50 as the most common composition. The proportion of tin in the pipe defines the pipe's tone, since tin has a desirable tonal resonance. When a tin/lead alloy cools, the lead phase solidifies first, then when the eutectic temperature is reached, the remaining liquid forms the layered tin/lead eutectic structure, which is shiny; contrast with the lead phase produces a mottled or spotted effect. This metal alloy is referred to as spotted metal. Major advantages of using tin for pipes include its appearance, workability, and resistance to corrosion.[79][80]
The oxides of indium and tin are electrically conductive and transparent, and are used to make transparent electrically conducting films with applications in optoelectronics devices such as liquid crystal displays.[81]
Punched tin-plated steel, also called pierced tin, is an artisan technique originating in central Europe for creating functional and decorative housewares. Decorative piercing designs exist in a wide variety, based on local tradition and the artisan. Punched tin lanterns are the most common application of this artisan technique. The light of a candle shining through the pierced design creates a decorative light pattern in the room where it sits. Lanterns and other punched tin articles were created in the New World from the earliest European settlement. A well-known example is the Revere lantern, named after Paul Revere.[82]
Before the modern era, in some areas of the Alps, a goat or sheep's horn would be sharpened and a tin panel would be punched out using the alphabet and numbers from one to nine. This learning tool was known appropriately as a "tin horn". Modern reproductions are decorated with such motifs as hearts and tulips.
In America, pie safes and food safes were in use in the days before refrigeration. These were wooden cupboards of various styles and sizes – either floor standing or hanging cupboards meant to discourage vermin and insects and to keep dust from perishable foodstuffs. These cabinets had tinplate inserts in the doors and sometimes in the sides, punched out by the homeowner, cabinetmaker, or a tinsmith in varying designs to allow for air circulation while excluding flies. Modern reproductions of these articles remain popular in North America.[83]
Window glass is most often made by floating molten glass on molten tin (float glass), resulting in a flat and flawless surface. This is also called the "Pilkington process".[84]
Tin is used as a negative electrode in advanced Li-ion batteries. Its application is somewhat limited by the fact that some tin surfaces[which?] catalyze decomposition of carbonate-based electrolytes used in Li-ion batteries.[85]
Tin(II) fluoride is added to some dental care products[86] as stannous fluoride (SnF2). Tin(II) fluoride can be mixed with calcium abrasives while the more common sodium fluoride gradually becomes biologically inactive in the presence of calcium compounds.[87] It has also been shown to be more effective than sodium fluoride in controlling gingivitis.[88]
Tin is used as a target to create laser-induced plasmas that act as the light source for extreme ultraviolet lithography.
The organotin compounds are most heavily used. Worldwide industrial production probably exceeds 50,000 tonnes.[89]
The major commercial application of organotin compounds is in the stabilization of PVC plastics. In the absence of such stabilizers, PVC would rapidly degrade under heat, light, and atmospheric oxygen, resulting in discolored, brittle products. Tin scavenges labile chloride ions (Cl−), which would otherwise strip HCl from the plastic material.[90] Typical tin compounds are carboxylic acid derivatives of dibutyltin dichloride, such as the dilaurate.[91]
Some organotin compounds are relatively toxic, with both advantages and problems. They are used for biocidal properties as fungicides, pesticides, algaecides, wood preservatives, and antifouling agents.[90] Tributyltin oxide is used as a wood preservative.[92] Tributyltin is also used for various industrial purposes such as slime control in paper mills and disinfection of circulating industrial cooling waters.[93] Tributyltin was used as additive for ship paint to prevent growth of fouling organisms on ships, with use declining after organotin compounds were recognized as persistent organic pollutants with high toxicity for some marine organisms (the dog whelk, for example).[94] The EU banned the use of organotin compounds in 2003,[95] while concerns over the toxicity of these compounds to marine life and damage to the reproduction and growth of some marine species[90] (some reports describe biological effects to marine life at a concentration of 1 nanogram per liter) have led to a worldwide ban by the International Maritime Organization.[96] Many nations now restrict the use of organotin compounds to vessels greater than 25 m (82 ft) long.[90] The persistence of tributyltin in the aquatic environment is dependent upon the nature of the ecosystem.[97] Because of this persistence and its use as an additive in ship paint, high concentrations of tributyltin have been found in marine sediments located near naval docks.[98] Tributyltin has been used as a biomarker for imposex in neogastropods, with at least 82 known species.[99] With the high levels of TBT in the local inshore areas, due to shipping activities, the shellfish had an adverse effect.[97] Imposex is the imposition of male sexual characteristics on female specimens where they grow a penis and a pallial vas deferens.[99][100] A high level of TBT can damage mammalian endocrine glands, reproductive and central nervous systems, bone structure and gastrointestinal tract.[100] Not only does tributyltin affect mammals, it affects sea otters, whales, dolphins, and humans.[100]
Some tin reagents are useful in organic chemistry. In the largest application, stannous chloride is a common reducing agent for the conversion of nitro and oxime groups to amines. The Stille reaction couples organotin compounds with organic halides or pseudohalides.[101]
Tin forms several inter-metallic phases with lithium metal, making it a potentially attractive material for battery applications. Large volumetric expansion of tin upon alloying with lithium and instability of the tin-organic electrolyte interface at low electrochemical potentials are the greatest challenges to employment in commercial cells.[102] Tin inter-metallic compound with cobalt and carbon was implemented by Sony in its Nexelion cells released in the late 2000s. The composition of the active material is approximately Sn0.3Co0.4C0.3. Research showed that only some crystalline facets of tetragonal (beta) Sn are responsible for undesirable electrochemical activity.[103]
Cases of poisoning from tin metal, its oxides, and its salts are almost unknown. On the other hand, certain organotin compounds are almost as toxic as cyanide.[45]
Exposure to tin in the workplace can occur by inhalation, skin contact, and eye contact. The US Occupational Safety and Health Administration (OSHA) set the permissible exposure limit for tin exposure in the workplace as 2 mg/m3 over an 8-hour workday. The National Institute for Occupational Safety and Health (NIOSH) determined a recommended exposure limit (REL) of 2 mg/m3 over an 8-hour workday. At levels of 100 mg/m3, tin is immediately dangerous to life and health.[104]



Weather is the state of the atmosphere, describing for example the degree to which it is hot or cold, wet or dry, calm or stormy, clear or cloudy.[1] On Earth, most weather phenomena occur in the lowest layer of the planet's atmosphere, the troposphere,[2][3] just below the stratosphere. Weather refers to day-to-day temperature, precipitation, and other atmospheric conditions, whereas climate is the term for the averaging of atmospheric conditions over longer periods of time.[4] When used without qualification, "weather" is generally understood to mean the weather of Earth.
Weather is driven by air pressure, temperature, and moisture differences between one place and another. These differences can occur due to the Sun's angle at any particular spot, which varies with latitude. The strong temperature contrast between polar and tropical air gives rise to the largest scale atmospheric circulations: the Hadley cell, the Ferrel cell, the polar cell, and the jet stream.  Weather systems in the middle latitudes, such as extratropical cyclones, are caused by instabilities of the jet streamflow.  Because Earth's axis is tilted relative to its orbital plane (called the ecliptic), sunlight is incident at different angles at different times of the year. On Earth's surface, temperatures usually range ±40 °C (−40 °F to 104 °F) annually. Over thousands of years, changes in Earth's orbit can affect the amount and distribution of solar energy received by Earth, thus influencing long-term climate and global climate change.
Surface temperature differences in turn cause pressure differences.  Higher altitudes are cooler than lower altitudes, as most atmospheric heating is due to contact with the Earth's surface while radiative losses to space are mostly constant. Weather forecasting is the application of science and technology to predict the state of the atmosphere for a future time and a given location. Earth's weather system is a chaotic system; as a result, small changes to one part of the system can grow to have large effects on the system as a whole. Human attempts to control the weather have occurred throughout history, and there is evidence that human activities such as agriculture and industry have modified weather patterns
Studying how the weather works on other planets has been helpful in understanding how weather works on Earth. A famous landmark in the Solar System, Jupiter's Great Red Spot, is an anticyclonic storm known to have existed for at least 300 years.  However, the weather is not limited to planetary bodies.  A star's corona is constantly being lost to space, creating what is essentially a very thin atmosphere throughout the Solar System.  The movement of mass ejected from the Sun is known as the solar wind.
On Earth, the common weather phenomena include wind, cloud, rain, snow, fog and dust storms. Less common events include natural disasters such as tornadoes, hurricanes, typhoons and ice storms. Almost all familiar weather phenomena occur in the troposphere (the lower part of the atmosphere).[3] Weather does occur in the stratosphere and can affect weather lower down in the troposphere, but the exact mechanisms are poorly understood.[5]
Weather occurs primarily due to air pressure, temperature and moisture differences between one place to another.  These differences can occur due to the sun angle at any particular spot, which varies by latitude from the tropics. In other words, the farther from the tropics one lies, the lower the sun angle is, which causes those locations to be cooler due to the spread of the sunlight over a greater surface.[6]   The strong temperature contrast between polar and tropical air gives rise to the large scale atmospheric circulation cells and the jet stream.[7]  Weather systems in the mid-latitudes, such as extratropical cyclones, are caused by instabilities of the jet stream flow (see baroclinity).[8]  Weather systems in the tropics, such as monsoons or organized thunderstorm systems, are caused by different processes.
Because the Earth's axis is tilted relative to its orbital plane, sunlight is incident at different angles at different times of the year. In June the Northern Hemisphere is tilted towards the sun, so at any given Northern Hemisphere latitude sunlight falls more directly on that spot than in December (see Effect of sun angle on climate).[10] This effect causes seasons. Over thousands to hundreds of thousands of years, changes in Earth's orbital parameters affect the amount and distribution of solar energy received by the Earth and influence long-term climate. (See Milankovitch cycles).[11]
The uneven solar heating (the formation of zones of temperature and moisture gradients, or frontogenesis) can also be due to the weather itself in the form of cloudiness and precipitation.[12]  Higher altitudes are typically cooler than lower altitudes, which the result of higher surface temperature and radiational heating, which produces the adiabatic lapse rate.[13][14] In some situations, the temperature actually increases with height. This phenomenon is known as an inversion and can cause mountaintops to be warmer than the valleys below. Inversions can lead to the formation of fog and often act as a cap that suppresses thunderstorm development. On local scales, temperature differences can occur because different surfaces (such as oceans, forests, ice sheets, or human-made objects) have differing physical characteristics such as reflectivity, roughness, or moisture content.
Surface temperature differences in turn cause pressure differences. A hot surface warms the air above it causing it to expand and lower the density and the resulting surface air pressure.[15] The resulting horizontal pressure gradient moves the air from higher to lower pressure regions, creating a wind, and the Earth's rotation then causes deflection of this airflow due to the Coriolis effect.[16] The simple systems thus formed can then display emergent behaviour to produce more complex systems and thus other weather phenomena.  Large scale examples include the Hadley cell while a smaller scale example would be coastal breezes.
The atmosphere is a chaotic system. As a result, small changes to one part of the system can accumulate and magnify to cause large effects on the system as a whole.[17] This atmospheric instability makes weather forecasting less predictable than tides or eclipses.[18] Although it is difficult to accurately predict weather more than a few days in advance, weather forecasters are continually working to extend this limit through meteorological research and refining current methodologies in weather prediction. However, it is theoretically impossible to make useful day-to-day predictions more than about two weeks ahead, imposing an upper limit to potential for improved prediction skill.[19]
Weather is one of the fundamental processes that shape the Earth. The process of weathering breaks down the rocks and soils into smaller fragments and then into their constituent substances.[20] During rains precipitation, the water droplets absorb and dissolve carbon dioxide from the surrounding air. This causes the rainwater to be slightly acidic, which aids the erosive properties of water. The released sediment and chemicals are then free to take part in chemical reactions that can affect the surface further (such as acid rain), and sodium and chloride ions (salt) deposited in the seas/oceans. The sediment may reform in time and by geological forces into other rocks and soils. In this way, weather plays a major role in erosion of the surface.[21]
Weather, seen from an anthropological perspective, is something all humans in the world constantly experience through their senses, at least while being outside. There are socially and scientifically constructed understandings of what weather is, what makes it change, the effect it has on humans in different situations, etc.[22] Therefore, weather is something people often communicate about. The National Weather Service has an annual report for fatalities, injury, and total damage costs which include crop and property. They gather this data via National Weather Service offices located throughout the 50 states in the United States as well as Puerto Rico, Guam, and the Virgin Islands. As of 2019, tornadoes have had the greatest impact on humans with 42 fatalities while costing crop and property damage over 3 billion dollars.[23]
The weather has played a large and sometimes direct part in human history. Aside from climatic changes that have caused the gradual drift of populations (for example the desertification of the Middle East, and the formation of land bridges during glacial periods), extreme weather events have caused smaller scale population movements and intruded directly in historical events. One such event is the saving of Japan from invasion by the Mongol fleet of Kublai Khan by the Kamikaze winds in 1281.[24]  French claims to Florida came to an end in 1565 when a hurricane destroyed the French fleet, allowing Spain to conquer Fort Caroline.[25]  More recently, Hurricane Katrina redistributed over one million people from the central Gulf coast elsewhere across the United States, becoming the largest diaspora in the history of the United States.[26]
The Little Ice Age caused crop failures and famines in Europe. During the period known as the Grindelwald Fluctuation (1560–1630), volcanic forcing events[27] seem to have led to more extreme weather events.[28]  These included droughts, storms and unseasonal blizzards, as well as causing the Swiss Grindelwald Glacier to expand. The 1690s saw the worst famine in France since the Middle Ages. Finland suffered a severe famine in 1696–1697, during which about one-third of the Finnish population died.[29]
Weather forecasting is the application of science and technology to predict the state of the atmosphere for a future time and a given location. Human beings have attempted to predict the weather informally for millennia, and formally since at least the nineteenth century.[30]  Weather forecasts are made by collecting quantitative data about the current state of the atmosphere and using scientific understanding of atmospheric processes to project how the atmosphere will evolve.[31]
Once an all-human endeavor based mainly upon changes in barometric pressure, current weather conditions, and sky condition,[32][33] forecast models are now used to determine future conditions. On the other hand, human input is still required to pick the best possible forecast model to base the forecast upon, which involves many disciplines such as pattern recognition skills, teleconnections, knowledge of model performance, and knowledge of model biases.
The chaotic nature of the atmosphere, the massive computational power required to solve the equations that describe the atmosphere, the error involved in measuring the initial conditions, and an incomplete understanding of atmospheric processes mean that forecasts become less accurate as of the difference in current time and the time for which the forecast is being made (the range of the forecast) increases. The use of ensembles and model consensus helps to narrow the error and pick the most likely outcome.[34][35][36]
There are a variety of end users to weather forecasts. Weather warnings are important forecasts because they are used to protect life and property.[37][38]  Forecasts based on temperature and precipitation are important to agriculture,[39][40][41][42] and therefore to commodity traders within stock markets. Temperature forecasts are used by utility companies to estimate demand over coming days.[43][44][45]
In some areas, people use weather forecasts to determine what to wear on a given day. Since outdoor activities are severely curtailed by heavy rain, snow and the wind chill, forecasts can be used to plan activities around these events and to plan ahead to survive through them.
Tropical weather forecasting is different from that at higher latitudes. The sun shines more directly on the tropics than on higher latitudes (at least on average over a year), which makes the tropics warm (Stevens 2011). And, the vertical direction (up, as one stands on the Earth's surface) is perpendicular to the Earth's axis of rotation at the equator, while the axis of rotation and the vertical are the same at the pole; this causes the Earth's rotation to influence the atmospheric circulation more strongly at high latitudes than low. Because of these two factors, clouds and rainstorms in the tropics can occur more spontaneously compared to those at higher latitudes, where they are more tightly controlled by larger-scale forces in the atmosphere. Because of these differences, clouds and rain are more difficult to forecast in the tropics than at higher latitudes. On the other hand, the temperature is easily forecast in the tropics, because it doesn't change much.[46]
The aspiration to control the weather is evident throughout human history: from ancient rituals intended to bring rain for crops to the U.S. Military Operation Popeye, an attempt to disrupt supply lines by lengthening the North Vietnamese monsoon. The most successful attempts at influencing weather involve cloud seeding; they include the fog- and low stratus dispersion techniques employed by major airports, techniques used to increase winter precipitation over mountains, and techniques to suppress hail.[47] A recent example of weather control was China's preparation for the 2008 Summer Olympic Games. China shot 1,104 rain dispersal rockets from 21 sites in the city of Beijing in an effort to keep rain away from the opening ceremony of the games on 8 August 2008. Guo Hu, head of the Beijing Municipal Meteorological Bureau (BMB), confirmed the success of the operation with 100 millimeters falling in Baoding City of Hebei Province, to the southwest and Beijing's Fangshan District recording a rainfall of 25 millimeters.[48]
Whereas there is inconclusive evidence for these techniques' efficacy, there is extensive evidence that human activity such as agriculture and industry results in inadvertent weather modification:[47]
The effects of inadvertent weather modification may pose serious threats to many aspects of civilization, including ecosystems, natural resources, food and fiber production, economic development, and human health.[51]
Microscale meteorology is the study of short-lived atmospheric phenomena smaller than mesoscale, about 1 km or less. These two branches of meteorology are sometimes grouped together as "mesoscale and microscale meteorology" (MMM) and together study all phenomena smaller than synoptic scale; that is they study features generally too small to be depicted on a weather map. These include small and generally fleeting cloud "puffs" and other small cloud features.[52]
On Earth, temperatures usually range ±40 °C (100 °F to −40 °F) annually.  The range of climates and latitudes across the planet can offer extremes of temperature outside this range.  The coldest air temperature ever recorded on Earth is −89.2 °C (−128.6 °F), at Vostok Station, Antarctica on 21 July 1983.  The hottest air temperature ever recorded was 57.7 °C (135.9 °F)  at 'Aziziya, Libya, on 13 September 1922,[54] but that reading is queried.  The highest recorded average annual temperature was 34.4 °C (93.9 °F) at Dallol, Ethiopia.[55]  The coldest recorded average annual temperature was −55.1 °C (−67.2 °F) at Vostok Station, Antarctica.[56]
The coldest average annual temperature in a permanently inhabited location is at Eureka, Nunavut, in Canada, where the annual average temperature is −19.7 °C (−3.5 °F).[57]
The windiest place ever recorded is in Antarctica, Commonwealth Bay (George V Coast).[citation needed] Here the gales reach 199 mph (320 km/h).[citation needed] Furthermore, the greatest snowfall in a period of twelve months occurred in Mount Rainier, Washington, USA. It was recorded as 31,102 mm (102.04 ft) of snow.[58]
Studying how the weather works on other planets has been seen as helpful in understanding how it works on Earth.[59] Weather on other planets follows many of the same physical principles as weather on Earth, but occurs on different scales and in atmospheres having different chemical composition. The Cassini–Huygens mission to Titan discovered clouds formed from methane or ethane which deposit rain composed of liquid methane and other organic compounds.[60]  Earth's atmosphere includes six latitudinal circulation zones, three in each hemisphere.[61]  In contrast, Jupiter's banded appearance shows many such zones,[62] Titan has a single jet stream near the 50th parallel north latitude,[63] and Venus has a single jet near the equator.[64]
One of the most famous landmarks in the Solar System, Jupiter's Great Red Spot, is an anticyclonic storm known to have existed for at least 300 years.[65]  On other gas giants, the lack of a surface allows the wind to reach enormous speeds: gusts of up to 600 metres per second (about 2,100 km/h or 1,300 mph) have been measured on the planet Neptune.[66]  This has created a puzzle for planetary scientists.  The weather is ultimately created by solar energy and the amount of energy received by Neptune is only about 1⁄900 of that received by Earth, yet the intensity of weather phenomena on Neptune is far greater than on Earth.[67] The strongest planetary winds discovered so far are on the extrasolar planet HD 189733 b, which is thought to have easterly winds moving at more than 9,600 kilometres per hour (6,000 mph).[68]
Weather is not limited to planetary bodies.  Like all stars, the Sun's corona is constantly being lost to space, creating what is essentially a very thin atmosphere throughout the Solar System.  The movement of mass ejected from the Sun is known as the solar wind.  Inconsistencies in this wind and larger events on the surface of the star, such as coronal mass ejections, form a system that has features analogous to conventional weather systems (such as pressure and wind) and is generally known as space weather.  Coronal mass ejections have been tracked as far out in the Solar System as Saturn.[69]  The activity of this system can affect planetary atmospheres and occasionally surfaces.  The interaction of the solar wind with the terrestrial atmosphere can produce spectacular aurorae,[70] and can play havoc with electrically sensitive systems such as electricity grids and radio signals.[71]





A magnetic field is a vector field that describes the magnetic influence on moving electric charges, electric currents,[1]: ch1 [2] and magnetic materials. A moving charge in a magnetic field experiences a force perpendicular to its own velocity and to the magnetic field.[1]: ch13 [3]: 278  A permanent magnet's magnetic field pulls on ferromagnetic materials such as iron, and attracts or repels other magnets.   In addition, a nonuniform magnetic field exerts minuscule forces on "nonmagnetic" materials by three other magnetic effects:  paramagnetism, diamagnetism, and antiferromagnetism, although these forces are usually so small they can only be detected by laboratory equipment.  Magnetic fields surround magnetized materials, and are created by electric currents such as those used in electromagnets, and by electric fields varying in time. Since both strength and direction of a magnetic field may vary with location, it is described mathematically by a function assigning a vector to each point of space, called a vector field.
In electromagnetics, the term "magnetic field" is used for two distinct but closely related vector fields denoted by the symbols B and H. In the International System of Units, the unit of H, magnetic field strength, is the ampere per meter (A/m).[4]: 22  The unit of B, the magnetic flux density, is the tesla (in SI base units: kilogram per second2 per ampere),[4]: 21  which is equivalent to newton per meter per ampere. H and B differ in how they account for magnetization. In vacuum, the two fields are related through the vacuum permeability, ; but in a magnetized material, the quantities on each side of this equation differ by the magnetization field of the material.
Magnetic fields are produced by moving electric charges and the intrinsic magnetic moments of elementary particles associated with a fundamental quantum property, their spin.[5][1]: ch1  Magnetic fields and electric fields are interrelated and are both components of the electromagnetic force, one of the four fundamental forces of nature.
Magnetic fields are used throughout modern technology, particularly in electrical engineering and electromechanics. Rotating magnetic fields are used in both electric motors and generators. The interaction of magnetic fields in electric devices such as transformers is conceptualized and investigated as magnetic circuits. Magnetic forces give information about the charge carriers in a material through the Hall effect. The Earth produces its own magnetic field, which shields the Earth's ozone layer from the solar wind and is important in navigation using a compass.
The force on an electric charge depends on its location, speed, and direction; two vector fields are used to describe this force.[1]: ch1  The first is the electric field, which describes the force acting on a stationary charge and gives the component of the force that is independent of motion. The magnetic field, in contrast, describes the component of the force that is proportional to both the speed and direction of charged particles.[1]: ch13  The field is defined by the Lorentz force law and is, at each instant, perpendicular to both the motion of the charge and the force it experiences.
There are two different, but closely related vector fields which are both sometimes called the "magnetic field" written B and H.[note 1] While both the best names for these fields and exact interpretation of what these fields represent has been the subject of long running debate, there is wide agreement about how the underlying physics work.[6] Historically, the term "magnetic field" was reserved for H while using other terms for B, but many recent textbooks use the term "magnetic field" to describe B as well as or in place of H.[note 2]
There are many alternative names for both (see sidebar).
The magnetic field vector B at any point can be defined as the vector that, when used in the Lorentz force law, correctly predicts the force on a charged particle at that point:[9][10]: 204 

Here F is the force on the particle, q is the particle's electric charge, v, is the particle's velocity, and × denotes the cross product. The direction of force on the charge can be determined by a mnemonic known as the right-hand rule (see the figure).[note 3] Using the right hand, pointing the thumb in the direction of the current, and the fingers in the direction of the magnetic field, the resulting force on the charge points outwards from the palm. The force on a negatively charged particle is in the opposite direction. If both the speed and the charge are reversed then the direction of the force remains the same. For that reason a magnetic field measurement (by itself) cannot distinguish whether there is a positive charge moving to the right or a negative charge moving to the left. (Both of these cases produce the same current.) On the other hand, a magnetic field combined with an electric field can distinguish between these, see Hall effect below.
The first term in the Lorentz equation is from the theory of electrostatics, and says that a particle of charge q in an electric field E experiences an electric force:

The second term is the magnetic force:[10]

Using the definition of the cross product, the magnetic force can also be written as a scalar equation:[9]: 357 

where Fmagnetic, v, and B are the scalar magnitude of their respective vectors, and θ is the angle between the velocity of the particle and the magnetic field. The vector B is defined as the vector field necessary to make the Lorentz force law correctly describe the motion of a charged particle. In other words,[9]: 173–4 
[T]he command, "Measure the direction and magnitude of the vector B at such and such a place," calls for the following operations: Take a particle of known charge q. Measure the force on q at rest, to determine E. Then measure the force on the particle when its velocity is v; repeat with v in some other direction. Now find a B that makes the Lorentz force law fit all these results—that is the magnetic field at the place in question.The B field can also be defined by the torque on a magnetic dipole, m.[11]: 174 

The SI unit of B is tesla (symbol: T).[note 4] The Gaussian-cgs unit of B is the gauss (symbol: G). (The conversion is 1 T ≘ 10000 G.[12][13]) One nanotesla corresponds to 1 gamma (symbol: γ).[13]
The magnetic H field is defined:[10]: 269 [11]: 192 [1]: ch36 

Where  is the vacuum permeability, and M is the magnetization vector. In a vacuum, B and H are proportional to each other. Inside a material they are different (see H and B inside and outside magnetic materials). The SI unit of the H-field is the ampere per metre (A/m),[14] and the CGS unit is the oersted (Oe).[12][9]: 286 
An instrument used to measure the local magnetic field is known as a magnetometer. Important classes of magnetometers include using induction magnetometers (or search-coil magnetometers) which measure only varying magnetic fields, rotating coil magnetometers, Hall effect magnetometers, NMR magnetometers, SQUID magnetometers, and fluxgate magnetometers. The magnetic fields of distant astronomical objects are measured through their effects on local charged particles. For instance, electrons spiraling around a field line produce synchrotron radiation that is detectable in radio waves. The finest precision for a magnetic field measurement was attained by Gravity Probe B at 5 aT (5×10−18 T).[15]
The field can be visualized by a set of magnetic field lines, that follow the direction of the field at each point. The lines can be constructed by measuring the strength and direction of the magnetic field at a large number of points (or at every point in space). Then, mark each location with an arrow (called a vector) pointing in the direction of the local magnetic field with its magnitude proportional to the strength of the magnetic field. Connecting these arrows then forms a set of magnetic field lines. The direction of the magnetic field at any point is parallel to the direction of nearby field lines, and the local density of field lines can be made proportional to its strength. Magnetic field lines are like streamlines in fluid flow, in that they represent a continuous distribution, and a different resolution would show more or fewer lines.
An advantage of using magnetic field lines as a representation is that many laws of magnetism (and electromagnetism) can be stated completely and concisely using simple concepts such as the "number" of field lines through a surface. These concepts can be quickly "translated" to their mathematical form. For example, the number of field lines through a given surface is the surface integral of the magnetic field.[9]: 237 
Various phenomena "display" magnetic field lines as though the field lines were physical phenomena. For example, iron filings placed in a magnetic field form lines that correspond to "field lines".[note 5] Magnetic field "lines" are also visually displayed in polar auroras, in which plasma particle dipole interactions create visible streaks of light that line up with the local direction of Earth's magnetic field.
Field lines can be used as a qualitative tool to visualize magnetic forces. In ferromagnetic substances like iron and in plasmas, magnetic forces can be understood by imagining that the field lines exert a tension, (like a rubber band) along their length, and a pressure perpendicular to their length on neighboring field lines. "Unlike" poles of magnets attract because they are linked by many field lines; "like" poles repel because their field lines do not meet, but run parallel, pushing on each other.
Permanent magnets are objects that produce their own persistent magnetic fields. They are made of ferromagnetic materials, such as iron and nickel, that have been magnetized, and they have both a north and a south pole.
The magnetic field of permanent magnets can be quite complicated, especially near the magnet. The magnetic field of a small[note 6] straight magnet is proportional to the magnet's strength (called its magnetic dipole moment m). The equations are non-trivial and also depend on the distance from the magnet and the orientation of the magnet. For simple magnets, m points in the direction of a line drawn from the south to the north pole of the magnet. Flipping a bar magnet is equivalent to rotating its m by 180 degrees.
The magnetic field of larger magnets can be obtained by modeling them as a collection of a large number of small magnets called dipoles each having their own m. The magnetic field produced by the magnet then is the net magnetic field of these dipoles; any net force on the magnet is a result of adding up the forces on the individual dipoles.
There were two simplified models for the nature of these dipoles. These two models produce two different magnetic fields, H and B. Outside a material, though, the two are identical (to a multiplicative constant) so that in many cases the distinction can be ignored. This is particularly true for magnetic fields, such as those due to electric currents, that are not generated by magnetic materials.
A realistic model of magnetism is more complicated than either of these models; neither model fully explains why materials are magnetic. The monopole model has no experimental support. Ampere's model explains some, but not all of a material's magnetic moment. Like Ampere's model predicts, the motion of electrons within an atom are connected to those electrons' orbital magnetic dipole moment, and these orbital moments do contribute to the magnetism seen at the macroscopic level. However, the motion of electrons is not classical, and the spin magnetic moment of electrons (which is not explained by either model) is also a significant contribution to the total moment of magnets.
Historically, early physics textbooks would model the force and torques between two magnets as due to magnetic poles repelling or attracting each other in the same manner as the Coulomb force between electric charges. At the microscopic level, this model contradicts the experimental evidence, and the pole model of magnetism is no longer the typical way to introduce the concept.[10]: 204  However, it is still sometimes used as a macroscopic model for ferromagnetism due to its mathematical simplicity.[16]
In this model, a magnetic H-field is produced by fictitious magnetic charges that are spread over the surface of each pole. These magnetic charges are in fact related to the magnetization field M. The H-field, therefore, is analogous to the electric field E, which starts at a positive electric charge and ends at a negative electric charge. Near the north pole, therefore, all H-field lines point away from the north pole (whether inside the magnet or out) while near the south pole all H-field lines point toward the south pole (whether inside the magnet or out). Too, a north pole feels a force in the direction of the H-field while the force on the south pole is opposite to the H-field.
In the magnetic pole model, the elementary magnetic dipole m is formed by two opposite magnetic poles of pole strength qm separated by a small distance vector d, such that m = qm d. The magnetic pole model predicts correctly the field H both inside and outside magnetic materials, in particular the fact that H is opposite to the magnetization field M inside a permanent magnet.
Since it is based on the fictitious idea of a magnetic charge density, the pole model has limitations. Magnetic poles cannot exist apart from each other as electric charges can, but always come in north–south pairs. If a magnetized object is divided in half, a new pole appears on the surface of each piece, so each has a pair of complementary poles. The magnetic pole model does not account for magnetism that is produced by electric currents, nor the inherent connection between angular momentum and magnetism.
The pole model usually treats magnetic charge as a mathematical abstraction, rather than a physical property of particles. However, a magnetic monopole is a hypothetical particle (or class of particles) that physically has only one magnetic pole (either a north pole or a south pole). In other words, it would possess a "magnetic charge" analogous to an electric charge. Magnetic field lines would start or end on magnetic monopoles, so if they exist, they would give exceptions to the rule that magnetic field lines neither start nor end. Some theories (such as Grand Unified Theories) have predicted the existence of magnetic monopoles, but so far, none have been observed.
In the model developed by Ampere, the elementary magnetic dipole that makes up all magnets is a sufficiently small Amperian loop with current I and loop area A. The dipole moment of this loop is m = IA.
These magnetic dipoles produce a magnetic B-field.
The magnetic field of a magnetic dipole is depicted in the figure. From outside, the ideal magnetic dipole is identical to that of an ideal electric dipole of the same strength. Unlike the electric dipole, a magnetic dipole is properly modeled as a current loop having a current I and an area a. Such a current loop has a magnetic moment of

where the direction of m is perpendicular to the area of the loop and depends on the direction of the current using the right-hand rule. An ideal magnetic dipole is modeled as a real magnetic dipole whose area a has been reduced to zero and its current I increased to infinity such that the product m = Ia is finite. This model clarifies the connection between angular momentum and magnetic moment, which is the basis of the Einstein–de Haas effect rotation by magnetization and its inverse, the Barnett effect or magnetization by rotation.[17] Rotating the loop faster (in the same direction) increases the current and therefore the magnetic moment, for example.
Specifying the force between two small magnets is quite complicated because it depends on the strength and orientation of both magnets and their distance and direction relative to each other. The force is particularly sensitive to rotations of the magnets due to magnetic torque. The force on each magnet depends on its magnetic moment and the magnetic field[note 7] of the other.
To understand the force between magnets, it is useful to examine the magnetic pole model given above. In this model, the H-field of one magnet pushes and pulls on both poles of a second magnet. If this H-field is the same at both poles of the second magnet then there is no net force on that magnet since the force is opposite for opposite poles. If, however, the magnetic field of the first magnet is nonuniform (such as the H near one of its poles), each pole of the second magnet sees a different field and is subject to a different force. This difference in the two forces moves the magnet in the direction of increasing magnetic field and may also cause a net torque.
This is a specific example of a general rule that magnets are attracted (or repulsed depending on the orientation of the magnet) into regions of higher magnetic field. Any non-uniform magnetic field, whether caused by permanent magnets or electric currents, exerts a force on a small magnet in this way.
The details of the Amperian loop model are different and more complicated but yield the same result: that magnetic dipoles are attracted/repelled into regions of higher magnetic field. Mathematically, the force on a small magnet having a magnetic moment m due to a magnetic field B is:[18]: Eq. 11.42 

where the gradient ∇ is the change of the quantity m · B per unit distance and the direction is that of maximum increase of m · B. The dot product m · B = mBcos(θ), where m and B represent the magnitude of the m and B vectors and θ is the angle between them. If m is in the same direction as B then the dot product is positive and the gradient points "uphill" pulling the magnet into regions of higher B-field (more strictly larger m · B). This equation is strictly only valid for magnets of zero size, but is often a good approximation for not too large magnets. The magnetic force on larger magnets is determined by dividing them into smaller regions each having their own m then summing up the forces on each of these very small regions.
If two like poles of two separate magnets are brought near each other, and one of the magnets is allowed to turn, it promptly rotates to align itself with the first. In this example, the magnetic field of the stationary magnet creates a magnetic torque on the magnet that is free to rotate. This magnetic torque τ tends to align a magnet's poles with the magnetic field lines. A compass, therefore, turns to align itself with Earth's magnetic field.
 In terms of the pole model, two equal and opposite magnetic charges experiencing the same H also experience equal and opposite forces. Since these equal and opposite forces are in different locations, this produces a torque proportional to the distance (perpendicular to the force) between them. With the definition of m as the pole strength times the distance between the poles, this leads to τ = μ0 m H sin θ, where μ0 is a constant called the vacuum permeability, measuring 4π×10−7 V·s/(A·m) and θ is the angle between H and m.
Mathematically, the torque τ on a small magnet is proportional both to the applied magnetic field and to the magnetic moment m of the magnet:

where × represents the vector cross product. This equation includes all of the qualitative information included above. There is no torque on a magnet if m is in the same direction as the magnetic field, since the cross product is zero for two vectors that are in the same direction. Further, all other orientations feel a torque that twists them toward the direction of magnetic field.
Currents of electric charges both generate a magnetic field and feel a force due to magnetic B-fields.
All moving charged particles produce magnetic fields. Moving point charges, such as electrons, produce complicated but well known magnetic fields that depend on the charge, velocity, and acceleration of the particles.[19]
Magnetic field lines form in concentric circles around a cylindrical current-carrying conductor, such as a length of wire. The direction of such a magnetic field can be determined by using the "right-hand grip rule" (see figure at right). The strength of the magnetic field decreases with distance from the wire. (For an infinite length wire the strength is inversely proportional to the distance.)
Bending a current-carrying wire into a loop concentrates the magnetic field inside the loop while weakening it outside. Bending a wire into multiple closely spaced loops to form a coil or "solenoid" enhances this effect. A device so formed around an iron core may act as an electromagnet, generating a strong, well-controlled magnetic field. An infinitely long cylindrical electromagnet has a uniform magnetic field inside, and no magnetic field outside. A finite length electromagnet produces a magnetic field that looks similar to that produced by a uniform permanent magnet, with its strength and polarity determined by the current flowing through the coil.
The magnetic field generated by a steady current I (a constant flow of electric charges, in which charge neither accumulates nor is depleted at any point)[note 8] is described by the Biot–Savart law:[20]: 224 

where the integral sums over the wire length where vector dℓ is the vector line element with direction in the same sense as the current I, μ0 is the magnetic constant, r is the distance between the location of dℓ and the location where the magnetic field is calculated, and r̂ is a unit vector in the direction of r. For example, in the case of a sufficiently long, straight wire, this becomes:

where r = |r|. The direction is tangent to a circle perpendicular to the wire according to the right hand rule.[20]: 225 
A slightly more general[21][note 9] way of relating the current  to the B-field is through Ampère's law:

where the line integral is over any arbitrary loop and  is the current enclosed by that loop. Ampère's law is always valid for steady currents and can be used to calculate the B-field for certain highly symmetric situations such as an infinite wire or an infinite solenoid.
In a modified form that accounts for time varying electric fields, Ampère's law is one of four Maxwell's equations that describe electricity and magnetism.
A charged particle moving in a B-field experiences a sideways force that is proportional to the strength of the magnetic field, the component of the velocity that is perpendicular to the magnetic field and the charge of the particle. This force is known as the Lorentz force, and is given by

where F is the force, q is the electric charge of the particle, v is the instantaneous velocity of the particle, and B is the magnetic field (in teslas).
The Lorentz force is always perpendicular to both the velocity of the particle and the magnetic field that created it. When a charged particle moves in a static magnetic field, it traces a helical path in which the helix axis is parallel to the magnetic field, and in which the speed of the particle remains constant. Because the magnetic force is always perpendicular to the motion, the magnetic field can do no work on an isolated charge.[22][23] It can only do work indirectly, via the electric field generated by a changing magnetic field. It is often claimed that the magnetic force can do work to a non-elementary magnetic dipole, or to charged particles whose motion is constrained by other forces, but this is incorrect[24] because the work in those cases is performed by the electric forces of the charges deflected by the magnetic field.
The force on a current carrying wire is similar to that of a moving charge as expected since a current carrying wire is a collection of moving charges. A current-carrying wire feels a force in the presence of a magnetic field. The Lorentz force on a macroscopic current is often referred to as the Laplace force.
Consider a conductor of length ℓ, cross section A, and charge q due to electric current i. If this conductor is placed in a magnetic field of magnitude B that makes an angle θ with the velocity of charges in the conductor, the force exerted on a single charge q is

so, for N charges where

the force exerted on the conductor is

where i = nqvA.
The formulas derived for the magnetic field above are correct when dealing with the entire current. A magnetic material placed inside a magnetic field, though, generates its own bound current, which can be a challenge to calculate. (This bound current is due to the sum of atomic sized current loops and the spin of the subatomic particles such as electrons that make up the material.) The H-field as defined above helps factor out this bound current; but to see how, it helps to introduce the concept of magnetization first.
The magnetization vector field M represents how strongly a region of material is magnetized. It is defined as the net magnetic dipole moment per unit volume of that region. The magnetization of a uniform magnet is therefore a material constant, equal to the magnetic moment m of the magnet divided by its volume. Since the SI unit of magnetic moment is A⋅m2, the SI unit of magnetization M is ampere per meter, identical to that of the H-field.
The magnetization M field of a region points in the direction of the average magnetic dipole moment in that region. Magnetization field lines, therefore, begin near the magnetic south pole and ends near the magnetic north pole. (Magnetization does not exist outside the magnet.)
In the Amperian loop model, the magnetization is due to combining many tiny Amperian loops to form a resultant current called bound current. This bound current, then, is the source of the magnetic B field due to the magnet. Given the definition of the magnetic dipole, the magnetization field follows a similar law to that of Ampere's law:[25]

where the integral is a line integral over any closed loop and Ib is the bound current enclosed by that closed loop.
In the magnetic pole model, magnetization begins at and ends at magnetic poles. If a given region, therefore, has a net positive "magnetic pole strength" (corresponding to a north pole) then it has more magnetization field lines entering it than leaving it. Mathematically this is equivalent to:

where the integral is a closed surface integral over the closed surface S and qM is the "magnetic charge" (in units of magnetic flux) enclosed by S. (A closed surface completely surrounds a region with no holes to let any field lines escape.) The negative sign occurs because the magnetization field moves from south to north.
In SI units, the H-field is related to the B-field by

In terms of the H-field, Ampere's law is

where If represents the 'free current' enclosed by the loop so that the line integral of H does not depend at all on the bound currents.[26]
For the differential equivalent of this equation see Maxwell's equations. Ampere's law leads to the boundary condition

where Kf is the surface free current density and the unit normal  points in the direction from medium 2 to medium 1.[27]
Similarly, a surface integral of H over any closed surface is independent of the free currents and picks out the "magnetic charges" within that closed surface:

which does not depend on the free currents.
The H-field, therefore, can be separated into two[note 10] independent parts:

where H0 is the applied magnetic field due only to the free currents and Hd is the demagnetizing field due only to the bound currents.
The magnetic H-field, therefore, re-factors the bound current in terms of "magnetic charges". The H field lines loop only around "free current" and, unlike the magnetic B field, begins and ends near magnetic poles as well.
Most materials respond to an applied B-field by producing their own magnetization M and therefore their own B-fields. Typically, the response is weak and exists only when the magnetic field is applied. The term magnetism describes how materials respond on the microscopic level to an applied magnetic field and is used to categorize the magnetic phase of a material. Materials are divided into groups based upon their magnetic behavior:
In the case of paramagnetism and diamagnetism, the magnetization M is often proportional to the applied magnetic field such that:

where μ is a material dependent parameter called the permeability. In some cases the permeability may be a second rank tensor so that H may not point in the same direction as B. These relations between B and H are examples of constitutive equations. However, superconductors and ferromagnets have a more complex B-to-H relation; see magnetic hysteresis.
Energy is needed to generate a magnetic field both to work against the electric field that a changing magnetic field creates and to change the magnetization of any material within the magnetic field. For non-dispersive materials, this same energy is released when the magnetic field is destroyed so that the energy can be modeled as being stored in the magnetic field.
For linear, non-dispersive, materials (such that B = μH where μ is frequency-independent), the energy density is:

If there are no magnetic materials around then μ can be replaced by μ0. The above equation cannot be used for nonlinear materials, though; a more general expression given below must be used.
In general, the incremental amount of work per unit volume δW needed to cause a small change of magnetic field δB is:

Once the relationship between H and B is known this equation is used to determine the work needed to reach a given magnetic state. For hysteretic materials such as ferromagnets and superconductors, the work needed also depends on how the magnetic field is created. For linear non-dispersive materials, though, the general equation leads directly to the simpler energy density equation given above.
Like all vector fields, a magnetic field has two important mathematical properties that relates it to its sources. (For B the sources are currents and changing electric fields.) These two properties, along with the two corresponding properties of the electric field, make up Maxwell's Equations. Maxwell's Equations together with the Lorentz force law form a complete description of classical electrodynamics including both electricity and magnetism.
The first property is the divergence of a vector field A, ∇ · A, which represents how A "flows" outward from a given point. As discussed above, a B-field line never starts or ends at a point but instead forms a complete loop. This is mathematically equivalent to saying that the divergence of B is zero. (Such vector fields are called solenoidal vector fields.) This property is called Gauss's law for magnetism and is equivalent to the statement that there are no isolated magnetic poles or magnetic monopoles.
The second mathematical property is called the curl, such that ∇ × A represents how A curls or "circulates" around a given point. The result of the curl is called a "circulation source". The equations for the curl of B and of E are called the Ampère–Maxwell equation and Faraday's law respectively.
One important property of the B-field produced this way is that magnetic B-field lines neither start nor end (mathematically, B is a solenoidal vector field); a field line may only extend to infinity, or wrap around to form a closed curve, or follow a never-ending (possibly chaotic) path.[33] Magnetic field lines exit a magnet near its north pole and enter near its south pole, but inside the magnet B-field lines continue through the magnet from the south pole back to the north.[note 11] If a B-field line enters a magnet somewhere it has to leave somewhere else; it is not allowed to have an end point.
More formally, since all the magnetic field lines that enter any given region must also leave that region, subtracting the "number"[note 12] of field lines that enter the region from the number that exit gives identically zero. Mathematically this is equivalent to Gauss's law for magnetism:

where the integral is a surface integral over the closed surface S (a closed surface is one that completely surrounds a region with no holes to let any field lines escape). Since dA points outward, the dot product in the integral is positive for B-field pointing out and negative for B-field pointing in.
A changing magnetic field, such as a magnet moving through a conducting coil, generates an electric field (and therefore tends to drive a current in such a coil). This is known as Faraday's law and forms the basis of many electrical generators and electric motors. Mathematically, Faraday's law is:

where  is the electromotive force (or EMF, the voltage generated around a closed loop) and Φ is the magnetic flux—the product of the area times the magnetic field normal to that area. (This definition of magnetic flux is why B is often referred to as magnetic flux density.)[34]: 210  The negative sign represents the fact that any current generated by a changing magnetic field in a coil produces a magnetic field that opposes the change in the magnetic field that induced it. This phenomenon is known as Lenz's law. This integral formulation of Faraday's law can be converted[note 13] into a differential form, which applies under slightly different conditions.

Similar to the way that a changing magnetic field generates an electric field, a changing electric field generates a magnetic field. This fact is known as Maxwell's correction to Ampère's law and is applied as an additive term to Ampere's law as given above. This additional term is proportional to the time rate of change of the electric flux and is similar to Faraday's law above but with a different and positive constant out front. (The electric flux through an area is proportional to the area times the perpendicular part of the electric field.)
The full law including the correction term is known as the Maxwell–Ampère equation. It is not commonly given in integral form because the effect is so small that it can typically be ignored in most cases where the integral form is used.
The Maxwell term is critically important in the creation and propagation of electromagnetic waves. Maxwell's correction to Ampère's Law together with Faraday's law of induction describes how mutually changing electric and magnetic fields interact to sustain each other and thus to form electromagnetic waves, such as light: a changing electric field generates a changing magnetic field, which generates a changing electric field again. These, though, are usually described using the differential form of this equation given below.

where J is the complete microscopic current density.
As discussed above, materials respond to an applied electric E field and an applied magnetic B field by producing their own internal "bound" charge and current distributions that contribute to E and B but are difficult to calculate. To circumvent this problem, H and D fields are used to re-factor Maxwell's equations in terms of the free current density Jf:

These equations are not any more general than the original equations (if the "bound" charges and currents in the material are known). They also must be supplemented by the relationship between B and H as well as that between E and D. On the other hand, for simple relationships between these quantities this form of Maxwell's equations can circumvent the need to calculate the bound charges and currents.
According to the special theory of relativity, the partition of the electromagnetic force into separate electric and magnetic components is not fundamental, but varies with the observational frame of reference: An electric force perceived by one observer may be perceived by another (in a different frame of reference) as a magnetic force, or a mixture of electric and magnetic forces.
The magnetic field existing as electric field in other frames can be shown by consistency of equations obtained from Lorentz transformation of four force from Coulomb's Law in particle's rest frame with Maxwell's laws considering definition of fields from Lorentz force and for non accelerating condition. The form of magnetic field hence obtained by Lorentz transformation of four-force from the form of Coulomb's law in source's initial frame is given by:[35]where  is the charge of the point source,  is the position vector from the point source to the point in space,  is the velocity vector of the charged particle,  is the ratio of speed of the charged particle divided by the speed of light and  is the angle between  and . This form of magnetic field can be shown to satisfy maxwell's laws within the constraint of particle being non accelerating.[36] The above reduces to Biot-Savart law for non relativistic stream of current ().
Formally, special relativity combines the electric and magnetic fields into a rank-2 tensor, called the electromagnetic tensor. Changing reference frames mixes these components. This is analogous to the way that special relativity mixes space and time into spacetime, and mass, momentum, and energy into four-momentum.[37] Similarly, the energy stored in a magnetic field is mixed with the energy stored in an electric field in the electromagnetic stress–energy tensor.
In advanced topics such as quantum mechanics and relativity it is often easier to work with a potential formulation of electrodynamics rather than in terms of the electric and magnetic fields. In this representation, the magnetic vector potential A, and the electric scalar potential φ, are defined using gauge fixing such that:
.
The vector potential, A given by this form may be interpreted as a generalized potential momentum per unit charge [38] just as φ is interpreted as a generalized potential energy per unit charge. There are multiple choices one can make for the potential fields that satisfy the above condition. However, the choice of potentials is represented by its respective gauge condition.
Maxwell's equations when expressed in terms of the potentials in Lorentz gauge can be cast into a form that agrees with special relativity.[39] In relativity, A together with φ forms a four-potential regardless of the gauge condition, analogous to the four-momentum that combines the momentum and energy of a particle. Using the four potential instead of the electromagnetic tensor has the advantage of being much simpler—and it can be easily modified to work with quantum mechanics.
Special theory of relativity imposes the condition for events related by cause and effect to be time-like separated, that is that causal efficacy propagates no faster than light.[40] Maxwell's equations for electromagnetism are found to be in favor of this as electric and magnetic disturbances are found to travel at the speed of light in space. Electric and magnetic fields from classical electrodynamics obey the principle of locality in physics and are expressed in terms of retarded time or the time at which the cause of a measured field originated given that the influence of field travelled at speed of light. The retarded time for a point particle is given as solution of:

where  is retarded time or the time at which the source's contribution of the field originated,  is the position vector of the particle as function of time,  is the point in space,  is the time at which fields are measured and  is the speed of light. The equation subtracts the time taken for light to travel from particle to the point in space from the time of measurement to find time of origin of the fields. The uniqueness of solution for  for given ,  and  is valid for charged particles moving slower than speed of light.[41]
The solution of maxwell's equations for electric and magnetic field of a point charge is expressed in terms of retarded time or the time at which the particle in the past causes the field at the point, given that the influence travels across space at the speed of light.
Any arbitrary motion of point charge causes electric and magnetic fields found by solving maxwell's equations using green's function for retarded potentials and hence finding the fields to be as follows:


where and  are electric scalar potential and magnetic vector potential in Lorentz gauge,  is the charge of the point source,  is a unit vector pointing from charged particle to the point in space,  is the velocity of the particle divided by the speed of light and  is the corresponding Lorentz factor. Hence by the principle of superposition, the fields of a system of charges also obey principle of locality.
In modern physics, the electromagnetic field is understood to be not a classical field, but rather a quantum field; it is represented not as a vector of three numbers at each point, but as a vector of three quantum operators at each point. The most accurate modern description of the electromagnetic interaction (and much else) is quantum electrodynamics (QED),[42] which is incorporated into a more complete theory known as the Standard Model of particle physics.
In QED, the magnitude of the electromagnetic interactions between charged particles (and their antiparticles) is computed using perturbation theory. These rather complex formulas produce a remarkable pictorial representation as Feynman diagrams in which virtual photons are exchanged.
Predictions of QED agree with experiments to an extremely high degree of accuracy: currently about 10−12 (and limited by experimental errors); for details see precision tests of QED. This makes QED one of the most accurate physical theories constructed thus far.
All equations in this article are in the classical approximation, which is less accurate than the quantum description mentioned here. However, under most everyday circumstances, the difference between the two theories is negligible.
The Earth's magnetic field is produced by convection of a liquid iron alloy in the outer core. In a dynamo process, the movements drive a feedback process in which electric currents create electric and magnetic fields that in turn act on the currents.[43]
The field at the surface of the Earth is approximately the same as if a giant bar magnet were positioned at the center of the Earth and tilted at an angle of about 11° off the rotational axis of the Earth (see the figure).[44] The north pole of a magnetic compass needle points roughly north, toward the North Magnetic Pole. However, because a magnetic pole is attracted to its opposite, the North Magnetic Pole is actually the south pole of the geomagnetic field. This confusion in terminology arises because the pole of a magnet is defined by the geographical direction it points.[45]
Earth's magnetic field is not constant—the strength of the field and the location of its poles vary.[46] Moreover, the poles periodically reverse their orientation in a process called geomagnetic reversal. The most recent reversal occurred 780,000 years ago.[47]
The rotating magnetic field is a key principle in the operation of alternating-current motors. A permanent magnet in such a field rotates so as to maintain its alignment with the external field. This effect was conceptualized by Nikola Tesla, and later utilized in his and others' early AC (alternating current) electric motors.
Magnetic torque is used to drive electric motors. In one simple motor design, a magnet is fixed to a freely rotating shaft and subjected to a magnetic field from an array of electromagnets. By continuously switching the electric current through each of the electromagnets, thereby flipping the polarity of their magnetic fields, like poles are kept next to the rotor; the resultant torque is transferred to the shaft.
A rotating magnetic field can be constructed using two orthogonal coils with 90 degrees phase difference in their AC currents. However, in practice such a system would be supplied through a three-wire arrangement with unequal currents.
This inequality would cause serious problems in standardization of the conductor size and so, to overcome it, three-phase systems are used where the three currents are equal in magnitude and have 120 degrees phase difference. Three similar coils having mutual geometrical angles of 120 degrees create the rotating magnetic field in this case. The ability of the three-phase system to create a rotating field, utilized in electric motors, is one of the main reasons why three-phase systems dominate the world's electrical power supply systems.
Synchronous motors use DC-voltage-fed rotor windings, which lets the excitation of the machine be controlled—and induction motors use short-circuited rotors (instead of a magnet) following the rotating magnetic field of a multicoiled stator. The short-circuited turns of the rotor develop eddy currents in the rotating field of the stator, and these currents in turn move the rotor by the Lorentz force.
In 1882, Nikola Tesla identified the concept of the rotating magnetic field. In 1885, Galileo Ferraris independently researched the concept. In 1888, Tesla gained U.S. Patent 381,968 for his work. Also in 1888, Ferraris published his research in a paper to the Royal Academy of Sciences in Turin.
The charge carriers of a current-carrying conductor placed in a transverse magnetic field experience a sideways Lorentz force; this results in a charge separation in a direction perpendicular to the current and to the magnetic field. The resultant voltage in that direction is proportional to the applied magnetic field. This is known as the Hall effect.
The Hall effect is often used to measure the magnitude of a magnetic field. It is used as well to find the sign of the dominant charge carriers in materials such as semiconductors (negative electrons or positive holes).
An important use of H is in magnetic circuits where B = μH inside a linear material. Here, μ is the magnetic permeability of the material. This result is similar in form to Ohm's law J = σE, where J is the current density, σ is the conductance and E is the electric field. Extending this analogy, the counterpart to the macroscopic Ohm's law (I = V⁄R) is:

where  is the magnetic flux in the circuit,  is the magnetomotive force applied to the circuit, and Rm is the reluctance of the circuit. Here the reluctance Rm is a quantity similar in nature to resistance for the flux. Using this analogy it is straightforward to calculate the magnetic flux of complicated magnetic field geometries, by using all the available techniques of circuit theory.
As of October 2018[update], the largest magnetic field produced over a macroscopic volume outside a lab setting is 2.8 kT (VNIIEF in Sarov, Russia, 1998).[48][49] As of October 2018, the largest magnetic field produced in a laboratory over a macroscopic volume was 1.2 kT by researchers at the University of Tokyo in 2018.[49]
The largest magnetic fields produced in a laboratory occur in particle accelerators, such as RHIC, inside the collisions of heavy ions, where microscopic fields reach 1014 T.[50][51] Magnetars have the strongest known magnetic fields of any naturally occurring object, ranging from 0.1 to 100 GT (108 to 1011 T).[52]
While magnets and some properties of magnetism were known to ancient societies, the research of magnetic fields began in 1269 when French scholar Petrus Peregrinus de Maricourt mapped out the magnetic field on the surface of a spherical magnet using iron needles. Noting the resulting field lines crossed at two points he named those points "poles" in analogy to Earth's poles. He also articulated the principle that magnets always have both a north and south pole, no matter how finely one slices them.[53][note 14]
Almost three centuries later, William Gilbert of Colchester replicated Petrus Peregrinus's work and was the first to state explicitly that Earth is a magnet.[54]: 34  Published in 1600, Gilbert's work, De Magnete, helped to establish magnetism as a science.
In 1750, John Michell stated that magnetic poles attract and repel in accordance with an inverse square law[54]: 56  Charles-Augustin de Coulomb experimentally verified this in 1785 and stated explicitly that north and south poles cannot be separated.[54]: 59  Building on this force between poles, Siméon Denis Poisson (1781–1840) created the first successful model of the magnetic field, which he presented in 1824.[54]: 64  In this model, a magnetic H-field is produced by magnetic poles and magnetism is due to small pairs of north–south magnetic poles.
Three discoveries in 1820 challenged this foundation of magnetism. Hans Christian Ørsted demonstrated that a current-carrying wire is surrounded by a circular magnetic field.[note 15][55] Then André-Marie Ampère showed that parallel wires with currents attract one another if the currents are in the same direction and repel if they are in opposite directions.[54]: 87 [56] Finally, Jean-Baptiste Biot and Félix Savart announced empirical results about the forces that a current-carrying long, straight wire exerted on a small magnet, determining the forces were inversely proportional to the perpendicular distance from the wire to the magnet.[57][54]: 86  Laplace later deduced a law of force based on the differential action of a differential section of the wire,[57][58] which became known as the Biot–Savart law, as Laplace did not publish his findings.[59]
Extending these experiments, Ampère published his own successful model of magnetism in 1825. In it, he showed the equivalence of electrical currents to magnets[54]: 88  and proposed that magnetism is due to perpetually flowing loops of current instead of the dipoles of magnetic charge in Poisson's model.[note 16] Further, Ampère derived both Ampère's force law describing the force between two currents and Ampère's law, which, like the Biot–Savart law, correctly described the magnetic field generated by a steady current. Also in this work, Ampère introduced the term electrodynamics to describe the relationship between electricity and magnetism.[54]: 88–92 
In 1831, Michael Faraday discovered electromagnetic induction when he found that a changing magnetic field generates an encircling electric field, formulating what is now known as Faraday's law of induction.[54]: 189–192  Later, Franz Ernst Neumann proved that, for a moving conductor in a magnetic field, induction is a consequence of Ampère's force law.[54]: 222  In the process, he introduced the magnetic vector potential, which was later shown to be equivalent to the underlying mechanism proposed by Faraday.[54]: 225 
In 1850, Lord Kelvin, then known as William Thomson, distinguished between two magnetic fields now denoted H and B. The former applied to Poisson's model and the latter to Ampère's model and induction.[54]: 224  Further, he derived how H and B relate to each other and coined the term permeability.[54]: 245 [60]
Between 1861 and 1865, James Clerk Maxwell developed and published Maxwell's equations, which explained and united all of classical electricity and magnetism. The first set of these equations was published in a paper entitled On Physical Lines of Force in 1861. These equations were valid but incomplete. Maxwell completed his set of equations in his later 1865 paper A Dynamical Theory of the Electromagnetic Field and demonstrated the fact that light is an electromagnetic wave. Heinrich Hertz published papers in 1887 and 1888 experimentally confirming this fact.[61][62]
In 1887, Tesla developed an induction motor that ran on alternating current. The motor used polyphase current, which generated a rotating magnetic field to turn the motor (a principle that Tesla claimed to have conceived in 1882).[63][64][65] Tesla received a patent for his electric motor in May 1888.[66][67] In 1885, Galileo Ferraris independently researched rotating magnetic fields and subsequently published his research in a paper to the Royal Academy of Sciences in Turin, just two months before Tesla was awarded his patent, in March 1888.[68]
The twentieth century showed that classical electrodynamics is already consistent with special relativity, and extended classical electrodynamics to work with quantum mechanics. Albert Einstein, in his paper of 1905 that established relativity, showed that both the electric and magnetic fields are part of the same phenomena viewed from different reference frames. Finally, the emergent field of quantum mechanics was merged with electrodynamics to form quantum electrodynamics, which first formalized the notion that electromagnetic field energy is quantized in the form of photons.





The speed of light in vacuum, commonly denoted c, is a universal physical constant that is exactly equal to 299,792,458 metres per second (approximately 300,000 kilometres per second; 186,000 miles per second; 671 million miles per hour).[Note 3] According to the special theory of relativity, c is the upper limit for the speed at which conventional matter or energy (and thus any signal carrying information) can travel through space.[4][5][6]
All forms of electromagnetic radiation, including visible light, travel at the speed of light. For many practical purposes, light and other electromagnetic waves will appear to propagate instantaneously, but for long distances and very sensitive measurements, their finite speed has noticeable effects. Starlight viewed on Earth are from the past, allowing humans to study the history of the universe by viewing distant objects. When communicating with distant space probes, it can take minutes to hours for signals to travel. In computing, the speed of light fixes the ultimate minimum communication delay. The speed of light can be used in time of flight measurements to measure large distances to extremely high precision.
Ole Rømer first demonstrated in 1676 that light does not travel instantaneously by studying the apparent motion of Jupiter's moon Io. Progressively more accurate measurements of its speed came over the following centuries. In a paper published in 1865, James Clerk Maxwell proposed that light was an electromagnetic wave and, therefore, travelled at speed c.[7] In 1905, Albert Einstein postulated that the speed of light c with respect to any inertial frame of reference is a constant and is independent of the motion of the light source.[8] He explored the consequences of that postulate by deriving the theory of relativity and, in doing so, showed that the parameter c had relevance outside of the context of light and electromagnetism.
Massless particles and field perturbations, such as gravitational waves, also travel at speed c in vacuum. Such particles and waves travel at c regardless of the motion of the source or the inertial reference frame of the observer. Particles with nonzero rest mass can be accelerated to approach c but can never reach it, regardless of the frame of reference in which their speed is measured. In the special and general theories of relativity, c interrelates space and time and also appears in the famous equation of mass–energy equivalence, E = mc2.[9]
In some cases, objects or waves may appear to travel faster than light (e.g., phase velocities of waves, the appearance of certain high-speed astronomical objects, and particular quantum effects). The expansion of the universe is understood to exceed the speed of light beyond a certain boundary.
The speed at which light propagates through transparent materials, such as glass or air, is less than c; similarly, the speed of electromagnetic waves in wire cables is slower than c. The ratio between c and the speed v at which light travels in a material is called the refractive index n of the material (n = c/v). For example, for visible light, the refractive index of glass is typically around 1.5, meaning that light in glass travels at c/1.5 ≈ 200000 km/s (124000 mi/s); the refractive index of air for visible light is about 1.0003, so the speed of light in air is about 90 km/s (56 mi/s) slower than c.
The speed of light in vacuum is usually denoted by a lowercase c, for "constant" or the Latin celeritas (meaning 'swiftness, celerity'). In 1856, Wilhelm Eduard Weber and Rudolf Kohlrausch had used c for a different constant that was later shown to equal √2 times the speed of light in vacuum. Historically, the symbol V was used as an alternative symbol for the speed of light, introduced by James Clerk Maxwell in 1865. In 1894, Paul Drude redefined c with its modern meaning. Einstein used V in his original German-language papers on special relativity in 1905, but in 1907 he switched to c, which by then had become the standard symbol for the speed of light.[10][11]
Sometimes c is used for the speed of waves in any material medium, and c0 for the speed of light in vacuum.[12] This subscripted notation, which is endorsed in official SI literature,[13] has the same form as related electromagnetic constants: namely, μ0 for the vacuum permeability or magnetic constant, ε0 for the vacuum permittivity or electric constant, and Z0 for the impedance of free space. This article uses c exclusively for the speed of light in vacuum.
Since 1983, the constant c has been defined in the International System of Units (SI) as exactly 299792458 m/s; this relationship is used to define the metre as exactly the distance that light travels in vacuum in 1⁄299792458 of a second. By using the value of c, as well as an accurate measurement of the second, one can thus establish a standard for the metre.[14] As a dimensional physical constant, the numerical value of c is different for different unit systems. For example, in imperial units, the speed of light is approximately 186282 miles per second,[Note 4] or roughly 1 foot per nanosecond.[Note 5][15][16] 
In branches of physics in which c appears often, such as in relativity, it is common to use systems of natural units of measurement or the geometrized unit system where c = 1.[17][18] Using these units, c does not appear explicitly because multiplication or division by 1 does not affect the result. Its unit of light-second per second is still relevant, even if omitted.
The speed at which light waves propagate in vacuum is independent both of the motion of the wave source and of the inertial frame of reference of the observer.[Note 6] This invariance of the speed of light was postulated by Einstein in 1905,[8] after being motivated by Maxwell's theory of electromagnetism and the lack of evidence for the luminiferous aether;[19] it has since been consistently confirmed by many experiments.[Note 7] It is only possible to verify experimentally that the two-way speed of light (for example, from a source to a mirror and back again) is frame-independent, because it is impossible to measure the one-way speed of light (for example, from a source to a distant detector) without some convention as to how clocks at the source and at the detector should be synchronized. However, by adopting Einstein synchronization for the clocks, the one-way speed of light becomes equal to the two-way speed of light by definition.[20][21] The special theory of relativity explores the consequences of this invariance of c with the assumption that the laws of physics are the same in all inertial frames of reference.[22][23] One consequence is that c is the speed at which all massless particles and waves, including light, must travel in vacuum.[24][Note 8]
Special relativity has many counterintuitive and experimentally verified implications.[26] These include the equivalence of mass and energy (E = mc2), length contraction (moving objects shorten),[Note 9] and time dilation (moving clocks run more slowly). The factor γ by which lengths contract and times dilate is known as the Lorentz factor and is given by γ = (1 − v2/c2)−1/2, where v is the speed of the object. The difference of γ from 1 is negligible for speeds much slower than c, such as most everyday speeds – in which case special relativity is closely approximated by Galilean relativity – but it increases at relativistic speeds and diverges to infinity as v approaches c. For example, a time dilation factor of γ = 2 occurs at a relative velocity of 86.6% of the speed of light (v = 0.866 c). Similarly, a time dilation factor of γ = 10 occurs at 99.5% the speed of light (v = 0.995 c).
The results of special relativity can be summarized by treating space and time as a unified structure known as spacetime (with c relating the units of space and time), and requiring that physical theories satisfy a special symmetry called Lorentz invariance, whose mathematical formulation contains the parameter c.[29] Lorentz invariance is an almost universal assumption for modern physical theories, such as quantum electrodynamics, quantum chromodynamics, the Standard Model of particle physics, and general relativity. As such, the parameter c is ubiquitous in modern physics, appearing in many contexts that are unrelated to light. For example, general relativity predicts that c is also the speed of gravity and of gravitational waves,[30] and observations of gravitational waves have been consistent with this prediction.[31] In non-inertial frames of reference (gravitationally curved spacetime or accelerated reference frames), the local speed of light is constant and equal to c, but the speed of light can differ from c when measured from a remote frame of reference, depending on how measurements are extrapolated to the region.[32]
It is generally assumed that fundamental constants such as c have the same value throughout spacetime, meaning that they do not depend on location and do not vary with time. However, it has been suggested in various theories that the speed of light may have changed over time.[33][34] No conclusive evidence for such changes has been found, but they remain the subject of ongoing research.[35][36]
It also is generally assumed that the speed of light is isotropic, meaning that it has the same value regardless of the direction in which it is measured. Observations of the emissions from nuclear energy levels as a function of the orientation of the emitting nuclei in a magnetic field (see Hughes–Drever experiment), and of rotating optical resonators (see Resonator experiments) have put stringent limits on the possible two-way anisotropy.[37][38]
According to special relativity, the energy of an object with rest mass m and speed v is given by γmc2, where γ is the Lorentz factor defined above. When v is zero, γ is equal to one, giving rise to the famous E = mc2 formula for mass–energy equivalence. The γ factor approaches infinity as v approaches c, and it would take an infinite amount of energy to accelerate an object with mass to the speed of light. The speed of light is the upper limit for the speeds of objects with positive rest mass, and individual photons cannot travel faster than the speed of light.[39] This is experimentally established in many tests of relativistic energy and momentum.[40]
More generally, it is impossible for signals or energy to travel faster than c. One argument for this follows from the counter-intuitive implication of special relativity known as the relativity of simultaneity. If the spatial distance between two events A and B is greater than the time interval between them multiplied by c then there are frames of reference in which A precedes B, others in which B precedes A, and others in which they are simultaneous. As a result, if something were travelling faster than c relative to an inertial frame of reference, it would be travelling backwards in time relative to another frame, and causality would be violated.[Note 10][43] In such a frame of reference, an "effect" could be observed before its "cause". Such a violation of causality has never been recorded,[21] and would lead to paradoxes such as the tachyonic antitelephone.[44]
There are situations in which it may seem that matter, energy, or information-carrying signal travels at speeds greater than c, but they do not. For example, as is discussed in the propagation of light in a medium section below, many wave velocities can exceed c. The phase velocity of X-rays through most glasses can routinely exceed c,[45] but phase velocity does not determine the velocity at which waves convey information.[46]
If a laser beam is swept quickly across a distant object, the spot of light can move faster than c, although the initial movement of the spot is delayed because of the time it takes light to get to the distant object at the speed c. However, the only physical entities that are moving are the laser and its emitted light, which travels at the speed c from the laser to the various positions of the spot. Similarly, a shadow projected onto a distant object can be made to move faster than c, after a delay in time.[47] In neither case does any matter, energy, or information travel faster than light.[48]
The rate of change in the distance between two objects in a frame of reference with respect to which both are moving (their closing speed) may have a value in excess of c. However, this does not represent the speed of any single object as measured in a single inertial frame.[48]
Certain quantum effects appear to be transmitted instantaneously and therefore faster than c, as in the EPR paradox. An example involves the quantum states of two particles that can be entangled. Until either of the particles is observed, they exist in a superposition of two quantum states. If the particles are separated and one particle's quantum state is observed, the other particle's quantum state is determined instantaneously. However, it is impossible to control which quantum state the first particle will take on when it is observed, so information cannot be transmitted in this manner.[48][49]
Another quantum effect that predicts the occurrence of faster-than-light speeds is called the Hartman effect: under certain conditions the time needed for a virtual particle to tunnel through a barrier is constant, regardless of the thickness of the barrier.[50][51] This could result in a virtual particle crossing a large gap faster than light. However, no information can be sent using this effect.[52]
So-called superluminal motion is seen in certain astronomical objects,[53] such as the relativistic jets of radio galaxies and quasars. However, these jets are not moving at speeds in excess of the speed of light: the apparent superluminal motion is a projection effect caused by objects moving near the speed of light and approaching Earth at a small angle to the line of sight: since the light which was emitted when the jet was farther away took longer to reach the Earth, the time between two successive observations corresponds to a longer time between the instants at which the light rays were emitted.[54]
A 2011 experiment where neutrinos were observed to travel faster than light turned out to be due to experimental error.[55][56]
In models of the expanding universe, the farther galaxies are from each other, the faster they drift apart. This receding is not due to motion through space, but rather to the expansion of space itself.[48] For example, galaxies far away from Earth appear to be moving away from the Earth with a speed proportional to their distances. Beyond a boundary called the Hubble sphere, the rate at which their distance from Earth increases becomes greater than the speed of light.[57]
In classical physics, light is described as a type of electromagnetic wave. The classical behaviour of the electromagnetic field is described by Maxwell's equations, which predict that the speed c with which electromagnetic waves (such as light) propagate in vacuum is related to the distributed capacitance and inductance of vacuum, otherwise respectively known as the electric constant ε0 and the magnetic constant μ0, by the equation[58]
In modern quantum physics, the electromagnetic field is described by the theory of quantum electrodynamics (QED). In this theory, light is described by the fundamental excitations (or quanta) of the electromagnetic field, called photons. In QED, photons are massless particles and thus, according to special relativity, they travel at the speed of light in vacuum.[24]
Extensions of QED in which the photon has a mass have been considered. In such a theory, its speed would depend on its frequency, and the invariant speed c of special relativity would then be the upper limit of the speed of light in vacuum.[32] No variation of the speed of light with frequency has been observed in rigorous testing, putting stringent limits on the mass of the photon.[59] The limit obtained depends on the model used: if the massive photon is described by Proca theory,[60] the experimental upper bound for its mass is about 10−57 grams;[61] if photon mass is generated by a Higgs mechanism, the experimental upper limit is less sharp, m ≤ 10−14 eV/c2  (roughly 2 × 10−47 g).[60]
Another reason for the speed of light to vary with its frequency would be the failure of special relativity to apply to arbitrarily small scales, as predicted by some proposed theories of quantum gravity. In 2009, the observation of gamma-ray burst GRB 090510 found no evidence for a dependence of photon speed on energy, supporting tight constraints in specific models of spacetime quantization on how this speed is affected by photon energy for energies approaching the Planck scale.[62]
In a medium, light usually does not propagate at a speed equal to c; further, different types of light wave will travel at different speeds. The speed at which the individual crests and troughs of a plane wave (a wave filling the whole space, with only one frequency) propagate is called the phase velocity vp. A physical signal with a finite extent (a pulse of light) travels at a different speed. The overall envelope of the pulse travels at the group velocity vg, and its earliest part travels at the front velocity vf.[63]
The phase velocity is important in determining how a light wave travels through a material or from one material to another. It is often represented in terms of a refractive index. The refractive index of a material is defined as the ratio of c to the phase velocity vp in the material: larger indices of refraction indicate lower speeds. The refractive index of a material may depend on the light's frequency, intensity, polarization, or direction of propagation; in many cases, though, it can be treated as a material-dependent constant. The refractive index of air is approximately 1.0003.[64] Denser media, such as water,[65] glass,[66] and diamond,[67] have refractive indexes of around 1.3, 1.5 and 2.4, respectively, for visible light. In exotic materials like Bose–Einstein condensates near absolute zero, the effective speed of light may be only a few metres per second. However, this represents absorption and re-radiation delay between atoms, as do all slower-than-c speeds in material substances. As an extreme example of light "slowing" in matter, two independent teams of physicists claimed to bring light to a "complete standstill" by passing it through a Bose–Einstein condensate of the element rubidium. However, the popular description of light being "stopped" in these experiments refers only to light being stored in the excited states of atoms, then re-emitted at an arbitrarily later time, as stimulated by a second laser pulse. During the time it had "stopped", it had ceased to be light. This type of behaviour is generally microscopically true of all transparent media which "slow" the speed of light.[68]
In transparent materials, the refractive index generally is greater than 1, meaning that the phase velocity is less than c. In other materials, it is possible for the refractive index to become smaller than 1 for some frequencies; in some exotic materials it is even possible for the index of refraction to become negative.[69] The requirement that causality is not violated implies that the real and imaginary parts of the dielectric constant of any material, corresponding respectively to the index of refraction and to the attenuation coefficient, are linked by the Kramers–Kronig relations.[70][71] In practical terms, this means that in a material with refractive index less than 1, the wave will be absorbed quickly.[72]
A pulse with different group and phase velocities (which occurs if the phase velocity is not the same for all the frequencies of the pulse) smears out over time, a process known as dispersion. Certain materials have an exceptionally low (or even zero) group velocity for light waves, a phenomenon called slow light.[73]
The opposite, group velocities exceeding c, was proposed theoretically in 1993 and achieved experimentally in 2000.[74] It should even be possible for the group velocity to become infinite or negative, with pulses travelling instantaneously or backwards in time.[63]

None of these options, however, allow information to be transmitted faster than c. It is impossible to transmit information with a light pulse any faster than the speed of the earliest part of the pulse (the front velocity). It can be shown that this is (under certain assumptions) always equal to c.[63] It is possible for a particle to travel through a medium faster than the phase velocity of light in that medium (but still slower than c). When a charged particle does that in a dielectric material, the electromagnetic equivalent of a shock wave, known as Cherenkov radiation, is emitted.[75]
The speed of light is of relevance to communications: the one-way and  round-trip delay time are greater than zero. This applies from small to astronomical scales. On the other hand, some techniques depend on the finite speed of light, for example in distance measurements.
In computers, the speed of light imposes a limit on how quickly data can be sent between processors. If a processor operates at 1 gigahertz, a signal can travel only a maximum of about 30 centimetres (1 ft) in a single clock cycle — in practice, this distance is even shorter since the printed circuit board itself has a refractive index and slows down signals. Processors must therefore be placed close to each other, as well as memory chips, to minimize communication latencies, and care must be exercised when routing wires between them to ensure signal integrity. If clock frequencies continue to increase, the speed of light may eventually become a limiting factor for the internal design of single chips.[76][77]
Given that the equatorial circumference of the Earth is about 40075 km and that c is about 300000 km/s, the theoretical shortest time for a piece of information to travel half the globe along the surface is about 67 milliseconds. When light is traveling in optical fibre (a transparent material) the actual transit time is longer, in part because the speed of light is slower by about 35% in optical fibre, depending on its refractive index n.[Note 11] Furthermore, straight lines are rare in global communications and the travel time increases when signals pass through electronic switches or signal regenerators.[79]
Although this distance is largely irrelevant for most applications, latency becomes important in fields such as high-frequency trading, where traders seek to gain minute advantages by delivering their trades to exchanges fractions of a second ahead of other traders. For example, traders have been switching to microwave communications between trading hubs, because of the advantage which radio waves travelling at near to the speed of light through air have over comparatively slower fibre optic signals.[80][81]
Similarly, communications between the Earth and spacecraft are not instantaneous. There is a brief delay from the source to the receiver, which becomes more noticeable as distances increase. This delay was significant for communications between ground control and Apollo 8 when it became the first crewed spacecraft to orbit the Moon: for every question, the ground control station had to wait at least three seconds for the answer to arrive.[82] The communications delay between Earth and Mars can vary between five and twenty minutes depending upon the relative positions of the two planets. As a consequence of this, if a robot on the surface of Mars were to encounter a problem, its human controllers would not be aware of it until 5–20 minutes later. It would then take a further 5–20 minutes for commands to travel from Earth to Mars.[83]
Receiving light and other signals from distant astronomical sources takes much longer. For example, it takes 13 billion (13×109) years for light to travel to Earth from the faraway galaxies viewed in the Hubble Ultra Deep Field images.[84][85] Those photographs, taken today, capture images of the galaxies as they appeared 13 billion years ago, when the universe was less than a billion years old.[84] The fact that more distant objects appear to be younger, due to the finite speed of light, allows astronomers to infer the evolution of stars, of galaxies, and of the universe itself.[86]
Astronomical distances are sometimes expressed in light-years, especially in popular science publications and media.[87] A light-year is the distance light travels in one Julian year, around 9461 billion kilometres, 5879 billion miles, or 0.3066 parsecs. In round figures, a light year is nearly 10 trillion kilometres or nearly 6 trillion miles. Proxima Centauri, the closest star to Earth after the Sun, is around 4.2 light-years away.[88]
Radar systems measure the distance to a target by the time it takes a radio-wave pulse to return to the radar antenna after being reflected by the target: the distance to the target is half the round-trip transit time multiplied by the speed of light. A Global Positioning System (GPS) receiver measures its distance to GPS satellites based on how long it takes for a radio signal to arrive from each satellite, and from these distances calculates the receiver's position. Because light travels about 300000 kilometres (186000 mi) in one second, these measurements of small fractions of a second must be very precise. The Lunar Laser Ranging Experiment, radar astronomy and the Deep Space Network determine distances to the Moon,[89] planets[90] and spacecraft,[91] respectively, by measuring round-trip transit times.
There are different ways to determine the value of c. One way is to measure the actual speed at which light waves propagate, which can be done in various astronomical and Earth-based setups. However, it is also possible to determine c from other physical laws where it appears, for example, by determining the values of the electromagnetic constants ε0 and μ0 and using their relation to c. Historically, the most accurate results have been obtained by separately determining the frequency and wavelength of a light beam, with their product equalling c. This is described in more detail in the "Interferometry" section below.
In 1983 the metre was defined as "the length of the path travelled by light in vacuum during a time interval of 1⁄299792458 of a second",[92] fixing the value of the speed of light at 299792458 m/s by definition, as described below. Consequently, accurate measurements of the speed of light yield an accurate realization of the metre rather than an accurate value of c.
Outer space is a convenient setting for measuring the speed of light because of its large scale and nearly perfect vacuum. Typically, one measures the time needed for light to traverse some reference distance in the Solar System, such as the radius of the Earth's orbit. Historically, such measurements could be made fairly accurately, compared to how accurately the length of the reference distance is known in Earth-based units.
Ole Christensen Rømer used an astronomical measurement to make the first quantitative estimate of the speed of light in the year 1676.[93][94] When measured from Earth, the periods of moons orbiting a distant planet are shorter when the Earth is approaching the planet than when the Earth is receding from it. The distance travelled by light from the planet (or its moon) to Earth is shorter when the Earth is at the point in its orbit that is closest to its planet than when the Earth is at the farthest point in its orbit, the difference in distance being the diameter of the Earth's orbit around the Sun. The observed change in the moon's orbital period is caused by the difference in the time it takes light to traverse the shorter or longer distance. Rømer observed this effect for Jupiter's innermost major moon Io and deduced that light takes 22 minutes to cross the diameter of the Earth's orbit.[93]
Another method is to use the aberration of light, discovered and explained by James Bradley in the 18th century.[95] This effect results from the vector addition of the velocity of light arriving from a distant source (such as a star) and the velocity of its observer (see diagram on the right). A moving observer thus sees the light coming from a slightly different direction and consequently sees the source at a position shifted from its original position. Since the direction of the Earth's velocity changes continuously as the Earth orbits the Sun, this effect causes the apparent position of stars to move around. From the angular difference in the position of stars (maximally 20.5 arcseconds)[96] it is possible to express the speed of light in terms of the Earth's velocity around the Sun, which with the known length of a year can be converted to the time needed to travel from the Sun to the Earth. In 1729, Bradley used this method to derive that light travelled 10210 times faster than the Earth in its orbit (the modern figure is 10066 times faster) or, equivalently, that it would take light 8 minutes 12 seconds to travel from the Sun to the Earth.[95]
An astronomical unit (AU) is approximately the average distance between the Earth and Sun. It was redefined in 2012 as exactly 149597870700 m.[97][98] Previously the AU was not based on the International System of Units but in terms of the gravitational force exerted by the Sun in the framework of classical mechanics.[Note 12] The current definition uses the recommended value in metres for the previous definition of the astronomical unit, which was determined by measurement.[97] This redefinition is analogous to that of the metre and likewise has the effect of fixing the speed of light to an exact value in astronomical units per second (via the exact speed of light in metres per second).[100]
Previously, the inverse of c expressed in seconds per astronomical unit was measured by comparing the time for radio signals to reach different spacecraft in the Solar System, with their position calculated from the gravitational effects of the Sun and various planets. By combining many such measurements, a best fit value for the light time per unit distance could be obtained. For example, in 2009, the best estimate, as approved by the International Astronomical Union (IAU), was:[101][102]
The relative uncertainty in these measurements is 0.02 parts per billion (2×10−11), equivalent to the uncertainty in Earth-based measurements of length by interferometry.[103] Since the metre is defined to be the length travelled by light in a certain time interval, the measurement of the light time in terms of the previous definition of the astronomical unit can also be interpreted as measuring the length of an AU (old definition) in metres.[Note 13]
A method of measuring the speed of light is to measure the time needed for light to travel to a mirror at a known distance and back. This is the working principle behind experiments by Hippolyte Fizeau and Léon Foucault.
The setup as used by Fizeau consists of a beam of light directed at a mirror 8 kilometres (5 mi) away. On the way from the source to the mirror, the beam passes through a rotating cogwheel. At a certain rate of rotation, the beam passes through one gap on the way out and another on the way back, but at slightly higher or lower rates, the beam strikes a tooth and does not pass through the wheel. Knowing the distance between the wheel and the mirror, the number of teeth on the wheel, and the rate of rotation, the speed of light can be calculated.[104]
The method of Foucault replaces the cogwheel with a rotating mirror. Because the mirror keeps rotating while the light travels to the distant mirror and back, the light is reflected from the rotating mirror at a different angle on its way out than it is on its way back. From this difference in angle, the known speed of rotation and the distance to the distant mirror the speed of light may be calculated.[105] Foucault used this apparatus to measure the speed of light in air versus water, based on a suggestion by François Arago.[106]
Today, using oscilloscopes with time resolutions of less than one nanosecond, the speed of light can be directly measured by timing the delay of a light pulse from a laser or an LED reflected from a mirror. This method is less precise (with errors of the order of 1%) than other modern techniques, but it is sometimes used as a laboratory experiment in college physics classes.[107]
An option for deriving c that does not directly depend on a measurement of the propagation of electromagnetic waves is to use the relation between c and the vacuum permittivity ε0 and vacuum permeability μ0 established by Maxwell's theory: c2 = 1/(ε0μ0). The vacuum permittivity may be determined by measuring the capacitance and dimensions of a capacitor, whereas the value of the vacuum permeability was historically fixed at exactly 4π×10−7 H⋅m−1 through the definition of the ampere. Rosa and Dorsey used this method in 1907 to find a value of 299710±22 km/s. Their method depended upon having a standard unit of electrical resistance, the "international ohm", and so its accuracy was limited by how this standard was defined.[108][109]
Another way to measure the speed of light is to independently measure the frequency f and wavelength λ of an electromagnetic wave in vacuum. The value of c can then be found by using the relation c = fλ. One option is to measure the resonance frequency of a cavity resonator. If the dimensions of the resonance cavity are also known, these can be used to determine the wavelength of the wave. In 1946, Louis Essen and A.C. Gordon-Smith established the frequency for a variety of normal modes of microwaves of a microwave cavity of precisely known dimensions. The dimensions were established to an accuracy of about ±0.8 μm using gauges calibrated by interferometry.[108] As the wavelength of the modes was known from the geometry of the cavity and from electromagnetic theory, knowledge of the associated frequencies enabled a calculation of the speed of light.[108][110]
The Essen–Gordon-Smith result, 299792±9 km/s, was substantially more precise than those found by optical techniques.[108] By 1950, repeated measurements by Essen established a result of 299792.5±3.0 km/s.[111]
A household demonstration of this technique is possible, using a microwave oven and food such as marshmallows or margarine: if the turntable is removed so that the food does not move, it will cook the fastest at the antinodes (the points at which the wave amplitude is the greatest), where it will begin to melt. The distance between two such spots is half the wavelength of the microwaves; by measuring this distance and multiplying the wavelength by the microwave frequency (usually displayed on the back of the oven, typically 2450 MHz), the value of c can be calculated, "often with less than 5% error".[112][113]
Interferometry is another method to find the wavelength of electromagnetic radiation for determining the speed of light.[Note 14] A coherent beam of light (e.g. from a laser), with a known frequency (f), is split to follow two paths and then recombined. By adjusting the path length while observing the interference pattern and carefully measuring the change in path length, the wavelength of the light (λ) can be determined. The speed of light is then calculated using the equation c = λf.
Before the advent of laser technology, coherent radio sources were used for interferometry measurements of the speed of light.[115] However interferometric determination of wavelength becomes less precise with wavelength and the experiments were thus limited in precision by the long wavelength (~4 mm (0.16 in)) of the radiowaves. The precision can be improved by using light with a shorter wavelength, but then it becomes difficult to directly measure the frequency of the light. One way around this problem is to start with a low frequency signal of which the frequency can be precisely measured, and from this signal progressively synthesize higher frequency signals whose frequency can then be linked to the original signal. A laser can then be locked to the frequency, and its wavelength can be determined using interferometry.[116] This technique was due to a group at the National Bureau of Standards (which later became the National Institute of Standards and Technology). They used it in 1972 to measure the speed of light in vacuum with a fractional uncertainty of 3.5×10−9.[116][117]
Until the early modern period, it was not known whether light travelled instantaneously or at a very fast finite speed. The first extant recorded examination of this subject was in ancient Greece. The ancient Greeks, Arabic scholars, and classical European scientists long debated this until Rømer provided the first calculation of the speed of light. Einstein's theory of special relativity postulates that the speed of light is constant regardless of one's frame of reference. Since then, scientists have provided increasingly accurate measurements.
Empedocles (c. 490–430 BCE) was the first to propose a theory of light[124] and claimed that light has a finite speed.[125] He maintained that light was something in motion, and therefore must take some time to travel. Aristotle argued, to the contrary, that "light is due to the presence of something, but it is not a movement".[126] Euclid and Ptolemy advanced Empedocles' emission theory of vision, where light is emitted from the eye, thus enabling sight. Based on that theory, Heron of Alexandria argued that the speed of light must be infinite because distant objects such as stars appear immediately upon opening the eyes.[127]
Early Islamic philosophers initially agreed with the Aristotelian view that light had no speed of travel. In 1021, Alhazen (Ibn al-Haytham) published the Book of Optics, in which he presented a series of arguments dismissing the emission theory of vision in favour of the now accepted intromission theory, in which light moves from an object into the eye.[128] This led Alhazen to propose that light must have a finite speed,[126][129][130] and that the speed of light is variable, decreasing in denser bodies.[130][131] He argued that light is substantial matter, the propagation of which requires time, even if this is hidden from the senses.[132] Also in the 11th century, Abū Rayhān al-Bīrūnī agreed that light has a finite speed, and observed that the speed of light is much faster than the speed of sound.[133]
In the 13th century, Roger Bacon argued that the speed of light in air was not infinite, using philosophical arguments backed by the writing of Alhazen and Aristotle.[134][135] In the 1270s, Witelo considered the possibility of light travelling at infinite speed in vacuum, but slowing down in denser bodies.[136]
In the early 17th century, Johannes Kepler believed that the speed of light was infinite since empty space presents no obstacle to it. René Descartes argued that if the speed of light were to be finite, the Sun, Earth, and Moon would be noticeably out of alignment during a lunar eclipse. (Although this argument fails when aberration of light is taken into account, the latter was not recognized until the following century.[137]) Since such misalignment had not been observed, Descartes concluded the speed of light was infinite. Descartes speculated that if the speed of light were found to be finite, his whole system of philosophy might be demolished.[126] Despite this, in his derivation of Snell's law, Descartes assumed that some kind of motion associated with light was faster in denser media.[138][139] Pierre de Fermat derived Snell's law using the opposing assumption, the denser the medium the slower light travelled. Fermat also argued in support of a finite speed of light.[140]
In 1629, Isaac Beeckman proposed an experiment in which a person observes the flash of a cannon reflecting off a mirror about one mile (1.6 km) away. In 1638, Galileo Galilei proposed an experiment, with an apparent claim to having performed it some years earlier, to measure the speed of light by observing the delay between uncovering a lantern and its perception some distance away. He was unable to distinguish whether light travel was instantaneous or not, but concluded that if it were not, it must nevertheless be extraordinarily rapid.[118][119] In 1667, the Accademia del Cimento of Florence reported that it had performed Galileo's experiment, with the lanterns separated by about one mile, but no delay was observed.[141] The actual delay in this experiment would have been about 11 microseconds.
The first quantitative estimate of the speed of light was made in 1676 by Ole Rømer.[93][94] From the observation that the periods of Jupiter's innermost moon Io appeared to be shorter when the Earth was approaching Jupiter than when receding from it, he concluded that light travels at a finite speed, and estimated that it takes light 22 minutes to cross the diameter of Earth's orbit. Christiaan Huygens combined this estimate with an estimate for the diameter of the Earth's orbit to obtain an estimate of speed of light of 220000 km/s, which is 27% lower than the actual value.[122]
In his 1704 book Opticks, Isaac Newton reported Rømer's calculations of the finite speed of light and gave a value of "seven or eight minutes" for the time taken for light to travel from the Sun to the Earth (the modern value is 8 minutes 19 seconds).[142] Newton queried whether Rømer's eclipse shadows were coloured; hearing that they were not, he concluded the different colours travelled at the same speed. In 1729, James Bradley discovered stellar aberration.[95] From this effect he determined that light must travel 10,210 times faster than the Earth in its orbit (the modern figure is 10,066 times faster) or, equivalently, that it would take light 8 minutes 12 seconds to travel from the Sun to the Earth.[95]
In the 19th century Hippolyte Fizeau developed a method to determine the speed of light based on time-of-flight measurements on Earth and reported a value of 315000 km/s.[143] His method was improved upon by Léon Foucault who obtained a value of 298000 km/s in 1862.[104] In the year 1856, Wilhelm Eduard Weber and Rudolf Kohlrausch measured the ratio of the electromagnetic and electrostatic units of charge, 1/√ε0μ0, by discharging a Leyden jar, and found that its numerical value was very close to the speed of light as measured directly by Fizeau. The following year Gustav Kirchhoff calculated that an electric signal in a resistanceless wire travels along the wire at this speed.[144] In the early 1860s, Maxwell showed that, according to the theory of electromagnetism he was working on, electromagnetic waves propagate in empty space[145] at a speed equal to the above Weber/Kohlrausch ratio, and drawing attention to the numerical proximity of this value to the speed of light as measured by Fizeau, he proposed that light is in fact an electromagnetic wave.[146]
It was thought at the time that empty space was filled with a background medium called the luminiferous aether in which the electromagnetic field existed. Some physicists thought that this aether acted as a preferred frame of reference for the propagation of light and therefore it should be possible to measure the motion of the Earth with respect to this medium, by measuring the isotropy of the speed of light. Beginning in the 1880s several experiments were performed to try to detect this motion, the most famous of which is the experiment performed by Albert A. Michelson and Edward W. Morley in 1887.[147][148] The detected motion was always less than the observational error. Modern experiments indicate that the two-way speed of light is isotropic (the same in every direction) to within 6 nanometres per second.[149]
Because of this experiment Hendrik Lorentz proposed that the motion of the apparatus through the aether may cause the apparatus to contract along its length in the direction of motion, and he further assumed that the time variable for moving systems must also be changed accordingly ("local time"), which led to the formulation of the Lorentz transformation. Based on Lorentz's aether theory, Henri Poincaré (1900) showed that this local time (to first order in v/c) is indicated by clocks moving in the aether, which are synchronized under the assumption of constant light speed. In 1904, he speculated that the speed of light could be a limiting velocity in dynamics, provided that the assumptions of Lorentz's theory are all confirmed. In 1905, Poincaré brought Lorentz's aether theory into full observational agreement with the principle of relativity.[150][151]
In 1905 Einstein postulated from the outset that the speed of light in vacuum, measured by a non-accelerating observer, is independent of the motion of the source or observer. Using this and the principle of relativity as a basis he derived the special theory of relativity, in which the speed of light in vacuum c featured as a fundamental constant, also appearing in contexts unrelated to light. This made the concept of the stationary aether (to which Lorentz and Poincaré still adhered) useless and revolutionized the concepts of space and time.[152][153]
In the second half of the 20th century, much progress was made in increasing the accuracy of measurements of the speed of light, first by cavity resonance techniques and later by laser interferometer techniques. These were aided by new, more precise, definitions of the metre and second. In 1950, Louis Essen determined the speed as 299792.5±3.0 km/s, using cavity resonance.[111] This value was adopted by the 12th General Assembly of the Radio-Scientific Union in 1957. In 1960, the metre was redefined in terms of the wavelength of a particular spectral line of krypton-86, and, in 1967, the second was redefined in terms of the hyperfine transition frequency of the ground state of caesium-133.[154]
In 1972, using the laser interferometer method and the new definitions, a group at the US National Bureau of Standards in Boulder, Colorado determined the speed of light in vacuum to be c = 299792456.2±1.1 m/s. This was 100 times less uncertain than the previously accepted value. The remaining uncertainty was mainly related to the definition of the metre.[Note 16][117] As similar experiments found comparable results for c, the 15th General Conference on Weights and Measures in 1975 recommended using the value 299792458 m/s for the speed of light.[157]
In 1983 the 17th meeting of the General Conference on Weights and Measures (CGPM) found that wavelengths from frequency measurements and a given value for the speed of light are more reproducible than the previous standard. They kept the 1967 definition of second, so the caesium hyperfine frequency would now determine both the second and the metre. To do this, they redefined the metre as "the length of the path traveled by light in vacuum during a time interval of 1/299792458 of a second."[92] As a result of this definition, the value of the speed of light in vacuum is exactly 299792458 m/s[158][159] and has become a defined constant in the SI system of units.[14] Improved experimental techniques that, prior to 1983, would have measured the speed of light no longer affect the known value of the speed of light in SI units, but instead allow a more precise realization of the metre by more accurately measuring the wavelength of krypton-86 and other light sources.[160][161]
In 2011, the CGPM stated its intention to redefine all seven SI base units using what it calls "the explicit-constant formulation", where each "unit is defined indirectly by specifying explicitly an exact value for a well-recognized fundamental constant", as was done for the speed of light. It proposed a new, but completely equivalent, wording of the metre's definition: "The metre, symbol m, is the unit of length; its magnitude is set by fixing the numerical value of the speed of light in vacuum to be equal to exactly 299792458 when it is expressed in the SI unit m s−1."[162]  This was one of the changes that was incorporated in the 2019 redefinition of the SI base units, also termed the New SI.[163]



Food is any substance consumed by an organism for nutritional support. Food is usually of plant, animal, or fungal origin, and contains essential nutrients, such as carbohydrates, fats, proteins, vitamins, or minerals. The substance is ingested by an organism and assimilated by the organism's cells to provide energy, maintain life, or stimulate growth. Different species of animals have different feeding behaviours that satisfy the needs of their metabolisms that have evolved to fill a specific ecological niche within specific geographical contexts.
Omnivorous humans are highly adaptable and have adapted to obtain food in many different ecosystems. The majority of the food energy required is supplied by the industrial food industry, which produces food with intensive agriculture and distributes it through complex food processing and food distribution systems. This system of conventional agriculture relies heavily on fossil fuels, which means that the food and agricultural system is one of the major contributors to climate change, accountable for as much as 37% of total greenhouse gas emissions.[1]
The food system has significant impacts on a wide range of other social and political issues including: sustainability, biological diversity, economics, population growth, water supply, and food security. Food safety and security are monitored by international agencies like the International Association for Food Protection, World Resources Institute, World Food Programme, Food and Agriculture Organization, and International Food Information Council.
Food is any substance consumed to provide nutritional support and energy to an organism.[2][3] It can be raw, processed or formulated and is consumed orally by animals for growth, health or pleasure. Food is mainly composed of water, lipids, proteins and carbohydrates. Minerals (e.g. salts) and organic substances (e.g. vitamins) can also be found in food.[4] Plants, algae and some microorganisms use photosynthesis to make their own food molecules.[5] Water is found in many foods and has been defined as a food by itself.[6] Water and fiber have low energy densities, or calories, while fat is the most energy dense component.[3] Some inorganic (non-food) elements are also essential for plant and animal functioning.[7]
Human food can be classified in various ways, either by related content or by how the food is processed.[8] The number and composition of food groups can vary. Most systems include four basic groups that describe their origin and relative nutritional function: Vegetables and Fruit, Cereals and Bread, Dairy, and Meat.[9] Studies that look into diet quality group food into whole grains/cereals, refined grains/cereals, vegetables, fruits, nuts, legumes, eggs, dairy products, fish, red meat, processed meat, and sugar-sweetened beverages.[10][11][12] The Food and Agriculture Organization and World Health Organization use a system with nineteen food classifications: cereals, roots, pulses and nuts, milk, eggs, fish and shellfish, meat, insects, vegetables, fruits, fats and oils, sweets and sugars, spices and condiments, beverages, foods for nutritional uses, food additives, composite dishes and savoury snacks.[13]
In a given ecosystem, food forms a web of interlocking chains with primary producers at the bottom and apex predators at the top.[14] Other aspects of the web include detrovores (that eat detritis) and decomposers (that break down dead organisms).[14] Primary producers include algae, plants, bacteria and protists that acquire their energy from sunlight.[15] Primary consumers are the herbivores that consume the plants, and secondary consumers are the carnivores that consume those herbivores. Some organisms, including most mammals and birds, diet consists of both animals and plants, and they are considered omnivores.[16] The chain ends with the apex predators, the animals that have no known predators in its ecosystem.[17] Humans are considered apex predators.[18]
Humans are omnivores, finding sustenance in vegetables, fruits, cooked meat, milk, eggs, mushrooms and seaweed.[16] Cereal grain is a staple food that provides more food energy worldwide than any other type of crop.[19] Corn (maize), wheat, and rice account for 87% of all grain production worldwide.[20][21][22] Just over half of the world's crops are used to feed humans (55 percent), with 36 percent grown as animal feed and 9 percent for biofuels.[23] Fungi and bacteria are also used in the preparation of fermented foods like bread, wine, cheese and yogurt.[24]
Photosynthesis is the ultimate source of energy and food for nearly all life on earth.[25] It is the main food source for plants, algae and certain bacteria.[26] Without this, all organisms which depend on these organisms further up the food chain would be unable to exist, from coral to lions.[27] Energy from the sun is absorbed and used to transform water and carbon dioxide in the air or soil into oxygen and glucose. The oxygen is then released, and the glucose stored as an energy reserve.[28]
Plants also absorb important nutrients and minerals from the air, water and soil.[29] Carbon, oxygen and hydrogen are absorbed from the air or water and are the basic nutrients needed for plant survival.[30] The three main nutrients absorbed from the soil for plant growth are nitrogen, phosphorus and potassium, with other important nutrients including calcium, sulfur, magnesium, iron boron, chlorine, manganese, zinc, copper molybdenum and nickel.[30]
Plants as a food source are divided into seeds, fruits, vegetables, legumes, grains and nuts.[31] Where plants fall within these categories can vary, with botanically described fruits such as the tomato, squash, pepper and eggplant or seeds like peas commonly considered vegetables.[32] Food is a fruit if the part eaten is derived from the reproductive tissue, so seeds, nuts and grains are technically fruit.[33][34] From a culinary perspective, fruits are generally considered the remains of botanically described fruits after grains, nuts, seeds and fruits used as vegetables are removed.[35] Grains can be defined as seeds that humans eat or harvest, with cereal grains (oats, wheat, rice, corn, barley, rye, sorghum and millet) belonging to the Poaceae (grass) family[36] and pulses coming from the Fabaceae (legume) family.[37] Whole grains are foods that contain all the elements of the original seed (bran, germ, and endosperm).[38] Nuts are dry fruits, distinguishable by their woody shell.[35]
Fleshy fruits (distinguishable from dry fruits like grain, seeds and nuts) can be further classified as stone fruits (cherries and peaches), pome fruits (apples, pears), berries (blackberry, strawberry), citrus (oranges, lemon), melons (watermelon, cantaloupe), Mediterranean fruits (grapes, fig), tropical fruits (banana, pineapple).[35] Vegetables refer to any other part of the plant that can be eaten, including roots, stems, leaves, flowers, bark or the entire plant itself.[39] These include root vegetables (potatoes and carrots), bulbs (onion family), flowers (cauliflower and broccoli), leaf vegetables (spinach and lettuce) and stem vegetables (celery and asparagus).[40][39]
Plants have high carbohydrate, protein and lipid content, with carbohydrates mainly in the form of starch, fructose, glucose and other sugars.[31] Most vitamins are found from plant sources, with exceptions of vitamin D and vitamin B12. Minerals are also plentiful, although the presence of phytates can prevent their release.[31] Fruit can consist of up to 90% water, contain high levels of simple sugars that contribute to their sweet taste, and have a high vitamin C content.[31][35] Compared to fleshy fruit (excepting Bananas) vegetables are high in starch,[41] potassium, dietary fiber, folate and vitamins and low in fat and calories.[42] Grains are more starch based[31] and nuts have a high protein, fibre, vitamin E and B content.[35] Seeds are a good source of food for animals because they are abundant and contain fibre and healthful fats, such as omega-3 fats.[43][44]
Animals that only eat plants are called herbivores, with those that mostly just eat fruits known as frugivores,[45] leaves, while shoot eaters are folivores (pandas) and wood eaters termed xylophages (termites).[46] Frugivores include a diverse range of species from annelids to elephants, chimpanzees and many birds.[47][48][49] About 182 fish consume seeds or fruit.[50] Animals (domesticated and wild) use as many types of grasses that have adapted to different locations as their main source of nutrients.[51]
Humans only eat about 200 out of the worlds 400 000 plant species, despite at least half of them being edible.[52] Most human plant-based food comes from maize, rice, and wheat.[52] Plants can be processed into breads, pasta, cereals, juices and jams or raw ingredients such as sugar, herbs, spices and oils can be extracted.[31] Oilseeds are pressed to produce rich oils – ⁣sunflower, flaxseed, rapeseed (including canola oil) and sesame.[53]
Many plants and animals have coevolved in such a way that the fruit is a good source of nutrition to the animal who then excretes the seeds some distance away, allowing greater dispersal.[54] Even seed predation can be mutually beneficial, as some seeds can survive the digestion process.[55][56] Insects are major eaters of seeds,[43] with ants being the only real seed dispersers.[57] Birds, although being major dispersers,[58] only rarely eat seeds as a source of food and can be identified by their thick beak that is used to crack open the seed coat.[59] Mammals eat a more diverse range of seeds, as they are able to crush harder and larger seeds with their teeth.[60]
Animals are used as food either directly or indirectly. This includes meat, eggs, shellfish and dairy products like milk and cheese.[61] They are an important source of protein and are considered complete proteins for human consumption as they contain all the essential amino acids that the human body needs.[62] One 4-ounce (110 g) steak, chicken breast or pork chop contains about 30 grams of protein. One large egg has 7 grams of protein. A 4-ounce (110 g) serving of cheese has about 15 grams of protein. And 1 cup of milk has about 8 grams of protein.[62] Other nutrients found in animal products include calories, fat, essential vitamins (including B12) and minerals (including zinc, iron, calcium, magnesium).[62]
Food products produced by animals include milk produced by mammary glands, which in many cultures is drunk or processed into dairy products (cheese, butter, etc.). Eggs laid by birds and other animals are eaten and bees produce honey, a reduced nectar from flowers that is used as a popular sweetener in many cultures. Some cultures consume blood, such as in blood sausage, as a thickener for sauces, or in a cured, salted form for times of food scarcity, and others use blood in stews such as jugged hare.[63]
Animals, specifically humans, typically have five different types of tastes: sweet, sour, salty, bitter, and umami. The differing tastes are important for distinguishing between foods that are nutritionally beneficial and those which may contain harmful toxins.[64] As animals have evolved, the tastes that provide the most energy are the most pleasant to eat while others are not enjoyable,[65] although humans in particular can acquire a preference for some substances which are initially unenjoyable.[64] Water, while important for survival, has no taste.[66]
Sweetness is almost always caused by a type of simple sugar such as glucose or fructose, or disaccharides such as sucrose, a molecule combining glucose and fructose.[67] Sourness is caused by acids, such as vinegar in alcoholic beverages. Sour foods include citrus, specifically lemons and limes. Sour is evolutionarily significant as it can signal a food that may have gone rancid due to bacteria.[68] Saltiness is the taste of alkali metal ions such as sodium and potassium. It is found in almost every food in low to moderate proportions to enhance flavor. Bitter taste is a sensation considered unpleasant characterised by having a sharp, pungent taste. Unsweetened dark chocolate, caffeine, lemon rind, and some types of fruit are known to be bitter. Umami, commonly described as savory, is a marker of proteins and characteristic of broths and cooked meats.[69] Foods that have a strong umami flavor include cheese, meat and mushrooms.[70]
While most animals taste buds are located in their mouth, some insects taste receptors are located on their legs and some fish have taste buds along their entire body.[71][72] Dogs, cats and birds have relatively few taste buds (chickens have about 30),[73] adult humans have between 2000 and 4000,[74] while catfish can have more than a million.[72] Herbivores generally have more than carnivores as they need to tell which plants may be poisonous.[73] Not all mammals share the same tastes: some rodents can taste starch, cats cannot taste sweetness, and several carnivores (including hyenas, dolphins, and sea lions) have lost the ability to sense up to four of the five taste modalities found in humans.[75]
Food is broken into nutrient components through digestive process.[76] Proper digestion consists of mechanical processes (chewing, peristalsis) and chemical processes (digestive enzymes and microorganisms).[77][78] The digestive systems of herbivores and carnivores are very different as plant matter is harder to digest. Carnivores mouths are designed for tearing and biting compared to the grinding action found in herbivores.[79] Herbivores however have comparatively longer digestive tracts and larger stomachs to aid in digesting the cellulose in plants.[80][81]




Sugar is the generic name for sweet-tasting, soluble carbohydrates, many of which are used in food. Simple sugars, also called monosaccharides, include glucose, fructose, and galactose. Compound sugars, also called disaccharides or double sugars, are molecules made of two bonded monosaccharides; common examples are sucrose (glucose + fructose), lactose (glucose + galactose), and maltose (two molecules of glucose). White sugar is a refined form of sucrose. In the body, compound sugars are hydrolysed into simple sugars.
Longer chains of monosaccharides (>2) are not regarded as sugars, and are called oligosaccharides or polysaccharides. Starch is a glucose polymer found in plants, the most abundant source of energy in human food. Some other chemical substances, such as ethylene glycol, glycerol and sugar alcohols, may have a sweet taste, but are not classified as sugar.
Sugars are found in the tissues of most plants. Honey and fruits are abundant natural sources of simple sugars. Sucrose is especially concentrated in sugarcane and sugar beet, making them ideal for efficient commercial extraction to make refined sugar. In 2016, the combined world production of those two crops was about two billion tonnes. Maltose may be produced by malting grain. Lactose is the only sugar that cannot be extracted from plants. It can only be found in milk, including human breast milk, and in some dairy products. A cheap source of sugar is corn syrup, industrially produced by converting corn starch into sugars, such as maltose, fructose and glucose.
Sucrose is used in prepared foods (e.g. cookies and cakes), is sometimes added to commercially available processed food and beverages, and may be used by people as a sweetener for foods (e.g. toast and cereal) and beverages (e.g. coffee and tea). The average person consumes about 24 kilograms (53 pounds) of sugar each year, with North and South Americans consuming up to 50 kg (110 lb) and Africans consuming under 20 kg (44 lb).[1]
As sugar consumption grew in the latter part of the 20th century, researchers began to examine whether a diet high in sugar, especially refined sugar, was damaging to human health. Excessive consumption of sugar is associated with obesity, diabetes, cardiovascular disease, cancer and tooth decay.[2] In 2015, the World Health Organization strongly recommended that adults and children reduce their intake of free sugars to less than 10%, and encouraged a reduction to below 5%, of their total energy intake.[3]
The etymology reflects the spread of the commodity. From Sanskrit (śarkarā), meaning "ground or candied sugar", came Persian shakar, then to 12th century French sucre and the English sugar.[4]
The English word jaggery, a coarse brown sugar made from date palm sap or sugarcane juice, has a similar etymological origin: Portuguese jágara from the Malayalam cakkarā, which is from the Sanskrit śarkarā.[5]
Sugar has been produced in the Indian subcontinent[6] since ancient times and its cultivation spread from there into modern-day Afghanistan through the Khyber Pass.[7] It was not plentiful or cheap in early times, and in most parts of the world, honey was more often used for sweetening.[8] Originally, people chewed raw sugarcane to extract its sweetness. Even after refined sugarcane became more widely available during the European colonial era,[9] palm sugar was preferred in Java and other sugar producing parts of southeast Asia, and along with coconut sugar, is still used locally to make desserts today.[10][11]
Sugarcane is native of tropical areas such as the Indian subcontinent (South Asia) and Southeast Asia.[6][12] Different species seem to have originated from different locations with Saccharum barberi originating in India and S. edule and S. officinarum coming from New Guinea.[12][13] One of the earliest historical references to sugarcane is in Chinese manuscripts dating to 8th century BCE, which state that the use of sugarcane originated in India.[14]
In the tradition of Indian medicine (āyurveda), the sugarcane is known by the name Ikṣu and the sugarcane juice is known as Phāṇita. Its varieties, synonyms and characteristics are defined in nighaṇṭus such as the Bhāvaprakāśa (1.6.23, group of sugarcanes).[15]
Sugar remained relatively unimportant until the Indians discovered methods of turning sugarcane juice into granulated crystals that were easier to store and to transport.[16] Crystallized sugar was discovered by the time of the Imperial Guptas, around the 5th century CE.[16] In the local Indian language, these crystals were called khanda (Devanagari: खण्ड, Khaṇḍa), which is the source of the word candy.[17] Indian sailors, who carried clarified butter and sugar as supplies, introduced knowledge of sugar along the various trade routes they travelled.[16] Traveling Buddhist monks took sugar crystallization methods to China.[18] During the reign of Harsha (r. 606–647) in North India, Indian envoys in Tang China taught methods of cultivating sugarcane after Emperor Taizong of Tang (r. 626–649) made known his interest in sugar. China established its first sugarcane plantations in the seventh century.[19] Chinese documents confirm at least two missions to India, initiated in 647 CE, to obtain technology for sugar refining.[20] In the Indian subcontinent,[6] the Middle East and China, sugar became a staple of cooking and desserts.
Nearchus, admiral of Alexander of Macedonia, knew of sugar during the year 325 BC, because of his participation in the campaign of India led by Alexander (Arrian, Anabasis).[21][22] The Greek physician Pedanius Dioscorides in the 1st century CE described sugar in his medical treatise De Materia Medica,[23] and Pliny the Elder, a 1st-century CE Roman, described sugar in his Natural History: "Sugar is made in Arabia as well, but Indian sugar is better. It is a kind of honey found in cane, white as gum, and it crunches between the teeth. It comes in lumps the size of a hazelnut. Sugar is used only for medical purposes."[24] Crusaders brought sugar back to Europe after their campaigns in the Holy Land, where they encountered caravans carrying "sweet salt". Early in the 12th century, Venice acquired some villages near Tyre and set up estates to produce sugar for export to Europe. It supplemented the use of honey, which had previously been the only available sweetener.[25] Crusade chronicler William of Tyre, writing in the late 12th century, described sugar as "very necessary for the use and health of mankind".[26] In the 15th century, Venice was the chief sugar refining and distribution center in Europe.[14]
There was a drastic change in the mid-15th century, when Madeira and the Canary Islands were settled from Europe and sugar introduced there.[27][28] After this an "all-consuming passion for sugar ... swept through society" as it became far more easily available, though initially still very expensive.[29] By 1492, Madeira was producing over 1,400,000 kilograms (3,000,000 lb) of sugar annually.[30] Genoa, one of the centers of distribution, became known for candied fruit, while Venice specialized in pastries, sweets (candies), and sugar sculptures. Sugar was considered to have "valuable medicinal properties" as a "warm" food under prevailing categories, being "helpful to the stomach, to cure cold diseases, and sooth lung complaints".[31]
A feast given in Tours in 1457 by Gaston de Foix, which is "probably the best and most complete account we have of a late medieval banquet" includes the first mention of sugar sculptures, as the final food brought in was "a heraldic menagerie sculpted in sugar: lions, stags, monkeys ... each holding in paw or beak the arms of the Hungarian king".[32]  Other recorded grand feasts in the decades following included similar pieces.[33]  Originally the sculptures seem to have been eaten in the meal, but later they become merely table decorations, the most elaborate called triomfi. Several significant sculptors are known to have produced them; in some cases their preliminary drawings survive. Early ones were in brown sugar, partly cast in molds, with the final touches carved. They continued to be used until at least the Coronation Banquet for Edward VII of the United Kingdom in 1903; among other sculptures every guest was given a sugar crown to take away.[34]
In August 1492, Christopher Columbus collected sugar cane samples in La Gomera in the Canary Islands, and introduced it to the New World.[35] The cuttings were planted and the first sugar-cane harvest in Hispaniola took place in 1501. Many sugar mills had been constructed in Cuba and Jamaica by the 1520s.[36] The Portuguese took sugar cane to Brazil. By 1540, there were 800 cane-sugar mills in Santa Catarina Island and another 2,000 on the north coast of Brazil, Demarara, and Surinam. It took until 1600 for Brazilian sugar production to exceed that of São Tomé, which was the main center of sugar production in sixteenth century.[28]
Sugar was a luxury in Europe until the early 19th century, when it became more widely available, due to the rise of beet sugar in Prussia, and later in France under Napoleon.[37] Beet sugar was a German invention, since, in 1747, Andreas Sigismund Marggraf announced the discovery of sugar in beets and devised a method using alcohol to extract it.[38] Marggraf's student, Franz Karl Achard, devised an economical industrial method to extract the sugar in its pure form in the late 18th century.[39][40] Achard first produced beet sugar in 1783 in Kaulsdorf, and in 1801, the world's first beet sugar production facility was established in Cunern, Silesia (then part of Prussia, now Poland).[41] The works of Marggraf and Achard were the starting point for the sugar industry in Europe,[42] and for the modern sugar industry in general, since sugar was no longer a luxury product and a product almost only produced in warmer climates.[43]
Sugar became highly popular and by the 19th century, was found in every household. This evolution of taste and demand for sugar as an essential food ingredient resulted in major economic and social changes.[44] Demand drove, in part, the colonization of tropical islands and areas where labor-intensive sugarcane plantations and sugar manufacturing facilities could be successful.[44] World consumption increased more than 100 times from 1850 to 2000, led by Britain, where it increased from about 2 pounds per head per year in 1650 to 90 pounds by the early 20th century. In the late 18th century Britain consumed about half the sugar which reached Europe.[45]
After slavery was abolished, the demand for workers in European colonies in the Caribbean was filled by indentured laborers from the Indian subcontinent.[46][47][48] Millions of enslaved or indentured laborers were brought to various European colonies in the Americas, Africa and Asia (as a result of demand in Europe for among other commodities, sugar), influencing the ethnic mixture of numerous nations around the globe.[49][50][51]
Sugar also led to some industrialization of areas where sugar cane was grown. For example, in the 1790s Lieutenant J. Paterson, of the Bengal Presidency promoted to the British parliament the idea that sugar cane could grow in British India, where it had started, with many advantages and at less expense than in the West Indies. As a result, sugar factories were established in Bihar in eastern India.[52][53]
During the Napoleonic Wars, sugar-beet production increased in continental Europe because of the difficulty of importing sugar when shipping was subject to blockade. By 1880 the sugar beet was the main source of sugar in Europe. It was also cultivated in Lincolnshire and other parts of England, although the United Kingdom continued to import the main part of its sugar from its colonies.[54]
Until the late nineteenth century, sugar was purchased in loaves, which had to be cut using implements called sugar nips.[55] In later years, granulated sugar was more usually sold in bags. Sugar cubes were produced in the nineteenth century. The first inventor of a process to produce sugar in cube form was Jakob Christof Rad, director of a sugar refinery in Dačice. In 1841, he produced the first sugar cube in the world.[56] He began sugar-cube production after being granted a five-year patent for the process on 23 January 1843. Henry Tate of Tate & Lyle was another early manufacturer of sugar cubes at his refineries in Liverpool and London. Tate purchased a patent for sugar-cube manufacture from German Eugen Langen, who in 1872 had invented a different method of processing of sugar cubes.[57]
Sugar was rationed during World War I, though it was said that "No previous war in history has been fought so largely on sugar and so little on alcohol",[58]  and more sharply during World War II.[59][60][61][62][63] Rationing led to the development and use of various artificial sweeteners.[59][64]
Scientifically, sugar loosely refers to a number of carbohydrates, such as monosaccharides, disaccharides, or oligosaccharides. Monosaccharides are also called "simple sugars", the most important being glucose. Most monosaccharides have a formula that conforms to CnH2nOn with n between 3 and 7 (deoxyribose being an exception). Glucose has the molecular formula C6H12O6. The names of typical sugars end with -ose, as in "glucose" and "fructose". Sometimes such words may also refer to any types of carbohydrates soluble in water. The acyclic mono- and disaccharides contain either aldehyde groups or ketone groups. These carbon-oxygen double bonds (C=O) are the reactive centers. All saccharides with more than one ring in their structure result from two or more monosaccharides joined by glycosidic bonds with the resultant loss of a molecule of water (H2O) per bond.[65]
Monosaccharides in a closed-chain form can form glycosidic bonds with other monosaccharides, creating disaccharides (such as sucrose) and polysaccharides (such as starch or cellulose). Enzymes must hydrolyze or otherwise break these glycosidic bonds before such compounds become metabolized. After digestion and absorption the principal monosaccharides present in the blood and internal tissues include glucose, fructose, and galactose. Many pentoses and hexoses can form ring structures. In these closed-chain forms, the aldehyde or ketone group remains non-free, so many of the reactions typical of these groups cannot occur. Glucose in solution exists mostly in the ring form at equilibrium, with less than 0.1% of the molecules in the open-chain form.[65]
In November 2019, scientists reported detecting, for the first time, sugar molecules, including ribose, in meteorites, suggesting that chemical processes on asteroids can produce some fundamentally essential bio-ingredients important to life, and supporting the notion of an RNA World prior to a DNA-based origin of life on Earth, and possibly, as well, the notion of panspermia.[66][67]
Biopolymers of sugars are common in nature. Through photosynthesis, plants produce glyceraldehyde-3-phosphate (G3P), a phosphated 3-carbon sugar that is used by the cell to make monosaccharides such as glucose (C6H12O6) or (as in cane and beet) sucrose (C12H22O11). Monosaccharides may be further converted into structural polysaccharides such as cellulose and pectin for cell wall construction or into energy reserves in the form of storage polysaccharides such as starch or inulin. Starch, consisting of two different polymers of glucose, is a readily degradable form of chemical energy stored by cells, and can be converted to other types of energy.[65] Another polymer of glucose is cellulose, which is a linear chain composed of several hundred or thousand glucose units. It is used by plants as a structural component in their cell walls. Humans can digest cellulose only to a very limited extent, though ruminants can do so with the help of symbiotic bacteria in their gut.[68] DNA and RNA are built up of the monosaccharides deoxyribose and ribose, respectively. Deoxyribose has the formula C5H10O4 and ribose the formula C5H10O5.[69]
Because sugars burn easily when exposed to flame, the handling of sugars risks dust explosion. The risk of explosion is higher when the sugar has been milled to superfine texture, such as for use in chewing gum.[70] The 2008 Georgia sugar refinery explosion, which killed 14 people and injured 36, and destroyed most of the refinery, was caused by the ignition of sugar dust.[71]
In its culinary use, exposing sugar to heat causes caramelization. As the process occurs, volatile chemicals such as diacetyl are released, producing the characteristic caramel flavor.[72]
Fructose, galactose, and glucose are all simple sugars, monosaccharides, with the general formula C6H12O6. They have five hydroxyl groups (−OH) and a carbonyl group (C=O) and are cyclic when dissolved in water. They each exist as several isomers with dextro- and laevo-rotatory forms that cause polarized light to diverge to the right or the left.[73]
Lactose, maltose, and sucrose are all compound sugars, disaccharides, with the general formula C12H22O11. They are formed by the combination of two monosaccharide molecules with the exclusion of a molecule of water.[73]
The sugar contents of common fruits and vegetables are presented in Table 1.
Due to rising demand, sugar production in general increased some 14% over the period 2009 to 2018.[84] The largest importers were China, Indonesia, and the United States.[84]
Global production of sugarcane in 2020 was 1.9 billion tonnes, with Brazil producing 40% of the world total and India 20% (table).
Sugarcane refers to any of several species, or their hybrids, of giant grasses in the genus Saccharum in the family Poaceae. They have been cultivated in tropical climates in the Indian subcontinent and Southeast Asia over centuries for the sucrose found in their stems.[6] A great expansion in sugarcane production took place in the 18th century with the establishment of slave plantations in the Americas. The use of slavery for the labor-intensive process resulted in sugar production, enabling prices cheap enough for most people to buy. Mechanization reduced some labor needs, but in the 21st century, cultivation and production relied on low-wage laborers.
Sugar cane requires a frost-free climate with sufficient rainfall during the growing season to make full use of the plant's substantial growth potential. The crop is harvested mechanically or by hand, chopped into lengths and conveyed rapidly to the processing plant (commonly known as a sugar mill) where it is either milled and the juice extracted with water or extracted by diffusion.[87] The juice is clarified with lime and heated to destroy enzymes. The resulting thin syrup is concentrated in a series of evaporators, after which further water is removed. The resulting supersaturated solution is seeded with sugar crystals, facilitating crystal formation and drying.[87] Molasses is a by-product of the process and the fiber from the stems, known as bagasse,[87] is burned to provide energy for the sugar extraction process. The crystals of raw sugar have a sticky brown coating and either can be used as they are, can be bleached by sulfur dioxide, or can be treated in a carbonatation process to produce a whiter product.[87] About 2,500 litres (660 US gal) of irrigation water is needed for every one kilogram (2.2 pounds) of sugar produced.[88]
In 2020, global production of sugar beets was 253 million tonnes, led by Russia with 13% of the world total (table).
The sugar beet became a major source of sugar in the 19th century when methods for extracting the sugar became available. It is a biennial plant,[90] a cultivated variety of Beta vulgaris in the family Amaranthaceae, the tuberous root of which contains a high proportion of sucrose. It is cultivated as a root crop in temperate regions with adequate rainfall and requires a fertile soil. The crop is harvested mechanically in the autumn and the crown of leaves and excess soil removed. The roots do not deteriorate rapidly and may be left in the field for some weeks before being transported to the processing plant where the crop is washed and sliced, and the sugar extracted by diffusion.[91] Milk of lime is added to the raw juice with calcium carbonate. After water is evaporated by boiling the syrup under a vacuum, the syrup is cooled and seeded with sugar crystals. The white sugar that crystallizes can be separated in a centrifuge and dried, requiring no further refining.[91]
Refined sugar is made from raw sugar that has undergone a refining process to remove the molasses.[92][93] Raw sugar is sucrose which is extracted from sugarcane or sugar beet. While raw sugar can be consumed, the refining process removes unwanted tastes and results in refined sugar or white sugar.[94][95]
The sugar may be transported in bulk to the country where it will be used and the refining process often takes place there. The first stage is known as affination and involves immersing the sugar crystals in a concentrated syrup that softens and removes the sticky brown coating without dissolving them. The crystals are then separated from the liquor and dissolved in water. The resulting syrup is treated either by a carbonatation or by a phosphatation process. Both involve the precipitation of a fine solid in the syrup and when this is filtered out, many of the impurities are removed at the same time. Removal of color is achieved by using either a granular activated carbon or an ion-exchange resin. The sugar syrup is concentrated by boiling and then cooled and seeded with sugar crystals, causing the sugar to crystallize out. The liquor is spun off in a centrifuge and the white crystals are dried in hot air and ready to be packaged or used. The surplus liquor is made into refiners' molasses.[96]
The International Commission for Uniform Methods of Sugar Analysis sets standards for the measurement of the purity of refined sugar, known as ICUMSA numbers; lower numbers indicate a higher level of purity in the refined sugar.[97]
Refined sugar is widely used for industrial needs for higher quality. Refined sugar is purer (ICUMSA below 300) than raw sugar (ICUMSA over 1,500).[98] The level of purity associated with the colors of sugar, expressed by standard number ICUMSA, the smaller ICUMSA numbers indicate the higher purity of sugar.[98]
Brown sugars are granulated sugars, either containing residual molasses, or with the grains deliberately coated with molasses to produce a light- or dark-colored sugar. They are used in baked goods, confectionery, and toffees.[101] Their darkness is due to the amount of molasses they contain. They may be classified based on their darkness or country of origin. For instance:[99]
In most parts of the world, sugar is an important part of the human diet,[citation needed] making food more palatable and providing food energy. After cereals and vegetable oils, sugar derived from sugarcane and beet provided more kilocalories per capita per day on average than other food groups.[105] In 1750 the average Briton got 72 calories a day from sugar. In 1913 this had risen to 395. In 2015 it still provided around 14% of the calories in British diets.[106] According to one source, per capita consumption of sugar in 2016 was highest in the United States, followed by Germany and the Netherlands.[107]
Brown and white granulated sugar are 97% to nearly 100% carbohydrates, respectively, with less than 2% water, and no dietary fiber, protein or fat (table). Brown sugar contains a moderate amount of iron (15% of the Reference Daily Intake in a 100 gram amount, see table), but a typical serving of 4 grams (one teaspoon), would provide 15 calories and a negligible amount of iron or any other nutrient.[108] Because brown sugar contains 5–10% molasses reintroduced during processing, its value to some consumers is a richer flavor than white sugar.[109]
Sugar refiners and manufacturers of sugary foods and drinks have sought to influence medical research and public health recommendations,[110][111] with substantial and largely clandestine spending documented from the 1960s to 2016.[112][113][114][115] The results of research on the health effects of sugary food and drink differ significantly, depending on whether the researcher has financial ties to the food and drink industry.[116][117][118] A 2013 medical review concluded that "unhealthy commodity industries should have no role in the formation of national or international NCD [non-communicable disease] policy".[119]
There have been similar efforts to steer coverage of sugar-related health information in popular media, including news media and social media.[120][121][122]
A 2003 technical report by the World Health Organization (WHO) provides evidence that high intake of sugary drinks (including fruit juice) increases the risk of obesity by adding to overall energy intake.[123] By itself, sugar is not a factor causing obesity and metabolic syndrome, but rather – when over-consumed – is a component of unhealthy dietary behavior.[123][needs update] Meta-analyses showed that excessive consumption of sugar-sweetened beverages increased the risk of developing type 2 diabetes and metabolic syndrome – including weight gain[124] and obesity – in adults and children.[125][126]
A 2019 meta-analysis found that sugar consumption does not improve mood, but can lower alertness and increase fatigue within an hour of consumption.[127] Some studies report evidence of causality between high consumption of refined sugar and hyperactivity.[128] One review of low-quality studies of children consuming high amounts of energy drinks showed association with higher rates of unhealthy behaviors, including smoking and excessive alcohol use, and with hyperactivity and insomnia, although such effects could not be specifically attributed to sugar over other components of those drinks such as caffeine.[129]
The 2003 WHO report stated that "Sugars are undoubtedly the most important dietary factor in the development of dental caries".[123] A review of human studies showed that the incidence of caries is lower when sugar intake is less than 10% of total energy consumed.[130]
The "empty calories" argument states that a diet high in added (or 'free') sugars will reduce consumption of foods that contain essential nutrients.[131] This nutrient displacement occurs if sugar makes up more than 25% of daily energy intake,[132] a proportion associated with poor diet quality and risk of obesity.[133] Displacement may occur at lower levels of consumption.[132]
The WHO recommends that both adults and children reduce the intake of free sugars to less than 10% of total energy intake, and suggests a reduction to below 5%. "Free sugars" include monosaccharides and disaccharides added to foods, and sugars found in fruit juice and concentrates, as well as in honey and syrups. According to the WHO, "[t]hese recommendations were based on the totality of available evidence reviewed regarding the relationship between free sugars intake and body weight (low and moderate quality evidence) and dental caries (very low and moderate quality evidence)."[3]
On 20 May 2016, the U.S. Food and Drug Administration announced changes to the Nutrition Facts panel displayed on all foods, to be effective by July 2018. New to the panel is a requirement to list "added sugars" by weight and as a percent of Daily Value (DV). For vitamins and minerals, the intent of DVs is to indicate how much should be consumed. For added sugars, the guidance is that 100% DV should not be exceeded. 100% DV is defined as 50 grams. For a person consuming 2000 calories a day, 50 grams is equal to 200 calories and thus 10% of total calories—the same guidance as the WHO.[134] To put this in context, most 355 mL (12 US fl oz) cans of soda contain 39 grams of sugar. In the United States, a government survey on food consumption in 2013–2014 reported that, for men and women aged 20 and older, the average total sugar intakes—naturally occurring in foods and added—were, respectively, 125 and 99 g/day.[135]
Various culinary sugars have different densities due to differences in particle size and inclusion of moisture.
Domino Sugar gives the following weight to volume conversions (in United States customary units):[136]
The "Engineering Resources – Bulk Density Chart" published in Powder and Bulk gives different values for the bulk densities:[137]

Manufacturers of sugary products, such as soft drinks and candy, and the Sugar Research Foundation have been accused of trying to influence consumers and medical associations in the 1960s and 1970s by creating doubt about the potential health hazards of sucrose overconsumption, while promoting saturated fat as the main dietary risk factor in cardiovascular diseases.[112] In 2016, the criticism led to recommendations that diet policymakers emphasize the need for high-quality research that accounts for multiple biomarkers on development of cardiovascular diseases.[112]
Brown sugar crystals
Whole date sugar
Whole cane sugar (grey), vacuum-dried
Whole cane sugar (brown), vacuum-dried
Raw crystals of unrefined, unbleached sugar



Geometry (from Ancient Greek  γεωμετρία (geōmetría) 'land measurement'; from  γῆ (gê) 'earth, land', and  μέτρον (métron) 'a measure')[citation needed] is a branch of mathematics concerned with properties of space such as the distance, shape, size, and relative position of figures.[1] Geometry is, along with arithmetic, one of the oldest branches of mathematics. A mathematician who works in the field of geometry is called a geometer. Until the 19th century, geometry was almost exclusively devoted to Euclidean geometry,[a] which includes the notions of point, line, plane, distance, angle, surface, and curve, as fundamental concepts.[2]
Originally developed to model the physical world, geometry has applications in almost all sciences, and also in art, architecture, and other activities that are related to graphics.[3] Geometry also has applications in areas of mathematics that are apparently unrelated. For example, methods of algebraic geometry are fundamental in Wiles's proof of Fermat's Last Theorem, a problem that was stated in terms of elementary arithmetic, and remained unsolved for several centuries.
During the 19th century several discoveries enlarged dramatically the scope of geometry. One of the oldest such discoveries is Carl Friedrich Gauss' Theorema Egregiumcode: lat promoted to code: la  ("remarkable theorem") that asserts roughly that the Gaussian curvature of a surface is independent from any specific embedding in a Euclidean space. This implies that surfaces can be studied intrinsically, that is, as stand-alone spaces, and has been expanded into the theory of manifolds and Riemannian geometry. Later in the 19th century, it appeared that geometries without the parallel postulate (non-Euclidean geometries) can be developed without introducing any contradiction. The geometry that underlies general relativity is a famous application of non-Euclidean geometry.
Since the late 19th century, the scope of geometry has been greatly expanded, and the field has been split in many subfields that depend on the underlying methods—differential geometry, algebraic geometry, computational geometry, algebraic topology, discrete geometry (also known as combinatorial geometry), etc.—or on the properties of Euclidean spaces that are disregarded—projective geometry that consider only alignment of points but not distance and parallelism, affine geometry that omits the concept of angle and distance, finite geometry that omits continuity, and others. This enlargement of the scope of geometry led to a change of meaning of the word "space", which originally referred to the three-dimensional space of the physical world and its model provided by Euclidean geometry; presently a geometric space, or simply a space is a mathematical structure on which some geometry is defined.
The earliest recorded beginnings of geometry can be traced to ancient Mesopotamia and Egypt in the 2nd millennium BC.[4][5] Early geometry was a collection of empirically discovered principles concerning lengths, angles, areas, and volumes, which were developed to meet some practical need in surveying, construction, astronomy, and various crafts. The earliest known texts on geometry are the Egyptian Rhind Papyrus (2000–1800 BC) and Moscow Papyrus (c. 1890 BC), and the Babylonian clay tablets, such as Plimpton 322 (1900 BC). For example, the Moscow Papyrus gives a formula for calculating the volume of a truncated pyramid, or frustum.[6] Later clay tablets (350–50 BC) demonstrate that Babylonian astronomers implemented trapezoid procedures for computing Jupiter's position and motion within time-velocity space. These geometric procedures anticipated the Oxford Calculators, including the mean speed theorem, by 14 centuries.[7] South of Egypt the ancient Nubians established a system of geometry including early versions of sun clocks.[8][9]
In the 7th century BC, the Greek mathematician Thales of Miletus used geometry to solve problems such as calculating the height of pyramids and the distance of ships from the shore.  He is credited with the first use of deductive reasoning applied to geometry, by deriving four corollaries to Thales's theorem.[10] Pythagoras established the Pythagorean School, which is credited with the first proof of the Pythagorean theorem,[11] though the statement of the theorem has a long history.[12][13] Eudoxus (408–c. 355 BC) developed the method of exhaustion, which allowed the calculation of areas and volumes of curvilinear figures,[14] as well as a theory of ratios that avoided the problem of incommensurable magnitudes, which enabled subsequent geometers to make significant advances. Around 300 BC, geometry was revolutionized by Euclid, whose Elements, widely considered the most successful and influential textbook of all time,[15] introduced mathematical rigor through the axiomatic method and is the earliest example of the format still used in mathematics today, that of definition, axiom, theorem, and proof. Although most of the contents of the Elements were already known, Euclid arranged them into a single, coherent logical framework.[16] The Elements was known to all educated people in the West until the middle of the 20th century and its contents are still taught in geometry classes today.[17] Archimedes (c. 287–212 BC) of Syracuse, Italy used the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, and gave remarkably accurate approximations of pi.[18] He also studied the spiral bearing his name and obtained formulas for the volumes of surfaces of revolution.
Indian mathematicians also made many important contributions in geometry. The Shatapatha Brahmana (3rd century BC) contains rules for ritual geometric constructions that are similar to the Sulba Sutras.[19] According to (Hayashi 2005, p. 363), the Śulba Sūtras contain "the earliest extant verbal expression of the Pythagorean Theorem in the world, although it had already been known to the Old Babylonians. They contain lists of Pythagorean triples,[20] which are particular cases of Diophantine equations.[21]
In the Bakhshali manuscript, there are a handful of geometric problems (including problems about volumes of irregular solids). The Bakhshali manuscript also "employs a decimal place value system with a dot for zero."[22] Aryabhata's Aryabhatiya (499) includes the computation of areas and volumes.
Brahmagupta wrote his astronomical work Brāhmasphuṭasiddhānta in 628. Chapter 12, containing 66 Sanskrit verses, was divided into two sections: "basic operations" (including cube roots, fractions, ratio and proportion, and barter) and "practical mathematics" (including mixture, mathematical series, plane figures, stacking bricks, sawing of timber, and piling of grain).[23] In the latter section, he stated his famous theorem on the diagonals of a cyclic quadrilateral. Chapter 12 also included a formula for the area of a cyclic quadrilateral (a generalization of Heron's formula), as well as a complete description of rational triangles (i.e. triangles with rational sides and rational areas).[23]
In the Middle Ages, mathematics in medieval Islam contributed to the development of geometry, especially algebraic geometry.[24][25] Al-Mahani (b. 853) conceived the idea of reducing geometrical problems such as duplicating the cube to problems in algebra.[26] Thābit ibn Qurra (known as Thebit in Latin) (836–901) dealt with arithmetic operations applied to ratios of geometrical quantities, and contributed to the development of analytic geometry.[27] Omar Khayyam (1048–1131) found geometric solutions to cubic equations.[28]  The theorems of Ibn al-Haytham (Alhazen), Omar Khayyam and Nasir al-Din al-Tusi on quadrilaterals, including the Lambert quadrilateral and Saccheri quadrilateral, were early results in hyperbolic geometry, and along with their alternative postulates, such as Playfair's axiom, these works had a considerable influence on the development of non-Euclidean geometry among later European geometers, including Vitello (c. 1230 – c. 1314), Gersonides (1288–1344), Alfonso, John Wallis, and Giovanni Girolamo Saccheri.[dubious  – discuss][29]
In the early 17th century, there were two important developments in geometry. The first was the creation of analytic geometry, or geometry with coordinates and equations, by René Descartes (1596–1650) and Pierre de Fermat (1601–1665).[30] This was a necessary precursor to the development of calculus and a precise quantitative science of physics.[31] The second geometric development of this period was the systematic study of projective geometry by Girard Desargues (1591–1661).[32] Projective geometry studies properties of shapes which are unchanged under projections and sections, especially as they relate to artistic perspective.[33]
Two developments in geometry in the 19th century changed the way it had been studied previously.[34] These were the discovery of non-Euclidean geometries by Nikolai Ivanovich Lobachevsky, János Bolyai and Carl Friedrich Gauss and of the formulation of symmetry as the central consideration in the Erlangen programme of Felix Klein (which generalized the Euclidean and non-Euclidean geometries). Two of the master geometers of the time were Bernhard Riemann (1826–1866), working primarily with tools from mathematical analysis, and introducing the Riemann surface, and Henri Poincaré, the founder of algebraic topology and the geometric theory of dynamical systems. As a consequence of these major changes in the conception of geometry, the concept of "space" became something rich and varied, and the natural background for theories as different as complex analysis and classical mechanics.[35]
The following are some of the most important concepts in geometry.[2][36][37]
Euclid took an abstract approach to geometry in his Elements,[38] one of the most influential books ever written.[39] Euclid introduced certain axioms, or postulates, expressing primary or self-evident properties of points, lines, and planes.[40] He proceeded to rigorously deduce other properties by mathematical reasoning. The characteristic feature of Euclid's approach to geometry was its rigor, and it has come to be known as axiomatic or synthetic geometry.[41] At the start of the 19th century, the discovery of non-Euclidean geometries by Nikolai Ivanovich Lobachevsky (1792–1856), János Bolyai (1802–1860), Carl Friedrich Gauss (1777–1855) and others[42] led to a revival of interest in this discipline, and in the 20th century, David Hilbert (1862–1943) employed axiomatic reasoning in an attempt to provide a modern foundation of geometry.[43]
Points are generally considered fundamental objects for building geometry. They may be defined by the properties that they must have, as in Euclid's definition as "that which has no part",[44] or in synthetic geometry. In modern mathematics, they are generally defined as elements of a set called space, which is itself axiomatically defined.
With these modern definitions, every geometric shape is defined as a set of points; this is not the case in synthetic geometry, where a line is another fundamental object that is not viewed as the set of the points through which it passes.
However, there are modern geometries in which points are not primitive objects, or even without points.[45][46] One of the oldest such geometries is Whitehead's point-free geometry, formulated by Alfred North Whitehead in 1919–1920.
Euclid described a line as "breadthless length" which "lies equally with respect to the points on itself".[44] In modern mathematics, given the multitude of geometries, the concept of a line is closely tied to the way the geometry is described. For instance, in analytic geometry, a line in the plane is often defined as the set of points whose coordinates satisfy a given linear equation,[47] but in a more abstract setting, such as incidence geometry, a line may be an independent object, distinct from the set of points which lie on it.[48] In differential geometry, a geodesic is a generalization of the notion of a line to curved spaces.[49]
In Euclidean geometry a plane is a flat, two-dimensional surface that extends infinitely;[44] the definitions for other types of geometries are generalizations of that. Planes are used in many areas of geometry. For instance, planes can be studied as a topological surface without reference to distances or angles;[50] it can be studied as an affine space, where collinearity and ratios can be studied but not distances;[51] it can be studied as the complex plane using techniques of complex analysis;[52] and so on.
Euclid defines a plane angle as the inclination to each other, in a plane, of two lines which meet each other, and do not lie straight with respect to each other.[44] In modern terms, an angle is the figure formed by two rays, called the sides of the angle, sharing a common endpoint, called the vertex of the angle.[53]
In Euclidean geometry, angles are used to study polygons and triangles, as well as forming an object of study in their own right.[44] The study of the angles of a triangle or of angles in a unit circle forms the basis of trigonometry.[54]
In differential geometry and calculus, the angles between plane curves or space curves or surfaces can be calculated using the derivative.[55][56]
A curve is a 1-dimensional object that may be straight (like a line) or not; curves in 2-dimensional space are called plane curves and those in 3-dimensional space are called space curves.[57]
In topology, a curve is defined by a function from an interval of the real numbers to another space.[50] In differential geometry, the same definition is used, but the defining function is required to be differentiable [58] Algebraic geometry studies algebraic curves, which are defined as algebraic varieties of dimension one.[59]
A surface is a two-dimensional object, such as a sphere or paraboloid.[60] In differential geometry[58] and topology,[50] surfaces are described by two-dimensional 'patches' (or neighborhoods) that are assembled by diffeomorphisms or homeomorphisms, respectively. In algebraic geometry, surfaces are described by polynomial equations.[59]
A manifold is a generalization of the concepts of curve and surface. In topology, a manifold is a topological space where every point has a neighborhood that is homeomorphic to Euclidean space.[50] In differential geometry, a differentiable manifold is a space where each neighborhood is diffeomorphic to Euclidean space.[58]
Manifolds are used extensively in physics, including in general relativity and string theory.[61]
Length, area, and volume describe the size or extent of an object in one dimension, two dimension, and three dimensions respectively.[62]
In Euclidean geometry and analytic geometry, the length of a line segment can often be calculated by the Pythagorean theorem.[63]
Area and volume can be defined as fundamental quantities separate from length, or they can be described and calculated in terms of lengths in a plane or 3-dimensional space.[62] Mathematicians have found many explicit formulas for area and formulas for volume of various geometric objects. In calculus, area and volume can be defined in terms of integrals, such as the Riemann integral[64] or the Lebesgue integral.[65]
The concept of length or distance can be generalized, leading to the idea of metrics.[66] For instance, the Euclidean metric measures the distance between points in the Euclidean plane, while the hyperbolic metric measures the distance in the hyperbolic plane. Other important examples of metrics include the Lorentz metric of special relativity and the semi-Riemannian metrics of general relativity.[67]
In a different direction, the concepts of length, area and volume are extended by measure theory, which studies methods of assigning a size or measure to sets, where the measures follow rules similar to those of classical area and volume.[68]
Congruence and similarity are concepts that describe when two shapes have similar characteristics.[69] In Euclidean geometry, similarity is used to describe objects that have the same shape, while congruence is used to describe objects that are the same in both size and shape.[70] Hilbert, in his work on creating a more rigorous foundation for geometry, treated congruence as an undefined term whose properties are defined by axioms.
Congruence and similarity are generalized in transformation geometry, which studies the properties of geometric objects that are preserved by different kinds of transformations.[71]
Classical geometers paid special attention to constructing geometric objects that had been described in some other way. Classically, the only instruments used in most geometric constructions are the compass and straightedge.[b] Also, every construction had to be complete in a finite number of steps. However, some problems turned out to be difficult or impossible to solve by these means alone, and ingenious constructions using neusis, parabolas and other curves, or mechanical devices, were found.
Where the traditional geometry allowed dimensions 1 (a line), 2 (a plane) and 3 (our ambient world conceived of as three-dimensional space), mathematicians and physicists have used higher dimensions for nearly two centuries.[72] One example of a mathematical use for higher dimensions is the configuration space of a physical system, which has a dimension equal to the system's degrees of freedom. For instance, the configuration of a screw can be described by five coordinates.[73]
In general topology, the concept of dimension has been extended from natural numbers, to infinite dimension (Hilbert spaces, for example) and  positive real numbers (in fractal geometry).[74] In algebraic geometry, the dimension of an algebraic variety has received a number of apparently different definitions, which are all equivalent in the most common cases.[75]
The theme of symmetry in geometry is nearly as old as the science of geometry itself.[76] Symmetric shapes such as the circle, regular polygons and platonic solids held deep significance for many ancient philosophers[77] and were investigated in detail before the time of Euclid.[40] Symmetric patterns occur in nature and were artistically rendered in a multitude of forms, including the graphics of Leonardo da Vinci, M. C. Escher, and others.[78] In the second half of the 19th century, the relationship between symmetry and geometry came under intense scrutiny. Felix Klein's Erlangen program proclaimed that, in a very precise sense, symmetry, expressed via the notion of a transformation group, determines what geometry is.[79] Symmetry in classical Euclidean geometry is represented by congruences and rigid motions, whereas in projective geometry an analogous role is played by collineations, geometric transformations that take straight lines into straight lines.[80] However it was in the new geometries of Bolyai and Lobachevsky, Riemann, Clifford and Klein, and Sophus Lie that Klein's idea to 'define a geometry via its symmetry group' found its inspiration.[81] Both discrete and continuous symmetries play prominent roles in geometry, the former in topology and geometric group theory,[82][83] the latter in Lie theory and Riemannian geometry.[84][85]
A different type of symmetry is the principle of duality  in projective geometry, among other fields. This meta-phenomenon can roughly be described as follows: in any theorem, exchange point with plane, join with meet, lies in with contains, and the result is an equally true theorem.[86] A similar and closely related form of duality exists between a vector space and its dual space.[87]
Euclidean geometry is geometry in its classical sense.[88] As it models the space of the physical world, it is used in many scientific areas, such as mechanics, astronomy, crystallography,[89] and many technical fields, such as engineering,[90] architecture,[91] geodesy,[92] aerodynamics,[93] and navigation.[94] The mandatory educational curriculum of the majority of nations includes the study of Euclidean concepts such as points, lines, planes, angles, triangles, congruence, similarity, solid figures, circles, and analytic geometry.[36]
Differential geometry uses techniques of calculus and linear algebra to study problems in geometry.[95] It has applications in physics,[96] econometrics,[97] and bioinformatics,[98] among others.
In particular, differential geometry is of importance to mathematical physics due to Albert Einstein's general relativity postulation that the universe is curved.[99] Differential geometry can either be intrinsic (meaning that the spaces it considers are smooth manifolds whose geometric structure is governed by a Riemannian metric, which determines how distances are measured near each point) or extrinsic (where the object under study is a part of some ambient flat Euclidean space).[100]
Topology is the field concerned with the properties of continuous mappings,[101] and can be considered a generalization of Euclidean geometry.[102] In practice, topology often means dealing with large-scale properties of spaces, such as connectedness and compactness.[50]
The field of topology, which saw massive development in the 20th century, is in a technical sense a type of transformation geometry, in which transformations are homeomorphisms.[103] This has often been expressed in the form of the saying 'topology is rubber-sheet geometry'. Subfields of topology include geometric topology, differential topology, algebraic topology and general topology.[104]
The field of algebraic geometry developed from the Cartesian geometry of co-ordinates.[105] It underwent periodic periods of growth, accompanied by the creation and study of projective geometry, birational geometry, algebraic varieties, and commutative algebra, among other topics.[106] From the late 1950s through the mid-1970s it had undergone major foundational development, largely due to work of Jean-Pierre Serre and Alexander Grothendieck.[106] This led to the introduction of schemes and greater emphasis on topological methods, including various cohomology theories. One of seven Millennium Prize problems, the Hodge conjecture, is a question in algebraic geometry.[107] Wiles' proof of Fermat's Last Theorem uses advanced methods of algebraic geometry for solving a long-standing problem of number theory.
In general, algebraic geometry studies geometry through the use of concepts in commutative algebra such as multivariate polynomials.[108] It has applications in many areas, including cryptography[109] and string theory.[110]
Complex geometry studies the nature of geometric structures modelled on, or arising out of, the complex plane.[111][112][113] Complex geometry lies at the intersection of differential geometry, algebraic geometry, and analysis of several complex variables, and has found applications to string theory and mirror symmetry.[114]
Complex geometry first appeared as a distinct area of study in the work of Bernhard Riemann in his study of Riemann surfaces.[115][116][117] Work in the spirit of Riemann was carried out by the Italian school of algebraic geometry in the early 1900s. Contemporary treatment of complex geometry began with the work of Jean-Pierre Serre, who introduced the concept of sheaves to the subject, and illuminated the relations between complex geometry and algebraic geometry.[118][119]
The primary objects of study in complex geometry are complex manifolds, complex algebraic varieties, and complex analytic varieties, and holomorphic vector bundles and coherent sheaves over these spaces. Special examples of spaces studied in complex geometry include Riemann surfaces, and Calabi–Yau manifolds, and these spaces find uses in string theory. In particular, worldsheets of strings are modelled by Riemann surfaces, and superstring theory predicts that the extra 6 dimensions of 10 dimensional spacetime may be modelled by Calabi–Yau manifolds.
Discrete geometry is a subject that has close connections with convex geometry.[120][121][122] It is concerned mainly with questions of relative position of simple geometric objects, such as points, lines and circles. Examples include the study of sphere packings, triangulations, the Kneser-Poulsen conjecture, etc.[123][124] It shares many methods and principles with combinatorics.
Computational geometry deals with algorithms and their implementations for manipulating geometrical objects. Important problems historically have included the travelling salesman problem, minimum spanning trees, hidden-line removal, and linear programming.[125]
Although being a young area of geometry, it has many applications in computer vision, image processing, computer-aided design, medical imaging, etc.[126]
Geometric group theory uses large-scale geometric techniques to study finitely generated groups.[127] It is closely connected to low-dimensional topology, such as in Grigori Perelman's proof of the Geometrization conjecture, which included the proof of the Poincaré conjecture, a Millennium Prize Problem.[128]
Geometric group theory often revolves around the Cayley graph, which is a geometric representation of a group. Other important topics include quasi-isometries, Gromov-hyperbolic groups, and right angled Artin groups.[127][129]
Convex geometry investigates convex shapes in the Euclidean space and its more abstract analogues, often using techniques of real analysis and discrete mathematics.[130] It has close connections to convex analysis, optimization and functional analysis and important applications in number theory.
Convex geometry dates back to antiquity.[130] Archimedes gave the first known precise definition of convexity. The isoperimetric problem, a recurring concept in convex geometry, was studied by the Greeks as well, including Zenodorus. Archimedes, Plato, Euclid, and later Kepler and Coxeter all studied convex polytopes and their properties. From the 19th century on, mathematicians have studied other areas of convex mathematics, including higher-dimensional polytopes, volume and surface area of convex bodies, Gaussian curvature, algorithms, tilings and lattices.
Geometry has found applications in many fields, some of which are described below.
Mathematics and art are related in a variety of ways. For instance, the theory of perspective showed that there is more to geometry than just the metric properties of figures: perspective is the origin of projective geometry.[131]
Artists have long used concepts of proportion in design. Vitruvius developed a complicated theory of ideal proportions for the human figure.[132] These concepts have been used and adapted by artists from Michelangelo to modern comic book artists.[133]
The golden ratio is a particular proportion that has had a controversial role in art. Often claimed to be the most aesthetically pleasing ratio of lengths, it is frequently stated to be incorporated into famous works of art, though the most reliable and unambiguous examples were made deliberately by artists aware of this legend.[134]
Tilings, or tessellations, have been used in art throughout history. Islamic art makes frequent use of tessellations, as did the art of M. C. Escher.[135] Escher's work also made use of hyperbolic geometry.
Cézanne advanced the theory that all images can be built up from the sphere, the cone, and the cylinder. This is still used in art theory today, although the exact list of shapes varies from author to author.[136][137]
Geometry has many applications in architecture. In fact, it has been said that geometry lies at the core of architectural design.[138][139] Applications of geometry to architecture include the use of projective geometry to create forced perspective,[140] the use of conic sections in constructing domes and similar objects,[91] the use of tessellations,[91] and the use of symmetry.[91]
The field of astronomy, especially as it relates to mapping the positions of stars and planets on the celestial sphere and describing the relationship between movements of celestial bodies, have served as an important source of geometric problems throughout history.[141]
Riemannian geometry and pseudo-Riemannian geometry are used in general relativity.[142] String theory makes use of several variants of geometry,[143] as does quantum information theory.[144]
Calculus was strongly influenced by geometry.[30] For instance, the introduction of coordinates by René Descartes and the concurrent developments of algebra marked a new stage for geometry, since geometric figures such as plane curves could now be represented analytically in the form of functions and equations. This played a key role in the emergence of infinitesimal calculus in the 17th century. Analytic geometry continues to be a mainstay of pre-calculus and calculus curriculum.[145][146]
Another important area of application is number theory.[147] In ancient Greece the Pythagoreans considered the role of numbers in geometry. However, the discovery of incommensurable lengths contradicted their philosophical views.[148] Since the 19th century, geometry has been used for solving problems in number theory, for example through the geometry of numbers or, more recently, scheme theory, which is used in Wiles's proof of Fermat's Last Theorem.[149]

A screw and a bolt (see Differentiation between bolt and screw below) are similar types of fastener typically made of metal and characterized by a helical ridge, called a male thread (external thread). Screws and bolts are used to fasten materials by the engagement of the screw thread with a similar female thread (internal thread) in a matching part.
Screws are often self-threading (also known as self-tapping) where the thread cuts into the material when the screw is turned, creating an internal thread that helps pull fastened materials together and prevents pull-out. There are many screws for a variety of materials; materials commonly fastened by screws include wood, sheet metal, and plastic.
A screw is a combination of simple machines: it is, in essence, an inclined plane wrapped around a central shaft, but the inclined plane (thread) also comes to a sharp edge around the outside, which acts as a wedge as it pushes into the fastened material, and the shaft and helix also form a wedge at the point. Some screw threads are designed to mate with a complementary thread, called a female thread (internal thread), often in the form of a nut object with an internal thread. Other screw threads are designed to cut a helical groove in a softer material as the screw is inserted. The most common uses of screws are to hold objects together and to position objects.
A screw will usually have a head on one end that allows it to be turned with a tool. Common tools for driving screws include screwdrivers and wrenches. The head is usually larger than the body of the screw, which keeps the screw from being driven deeper than the length of the screw and to provide a bearing surface. There are exceptions. A carriage bolt has a domed head that is not designed to be driven. A set screw may have a head the same size or smaller than the outer diameter of the screws thread; a set screw without a head is sometimes called a grub screw. A J-bolt has a J-shaped head that is sunk into concrete to serve as an anchor bolt.
The cylindrical portion of the screw from the underside of the head to the tip is called the shank; it may be fully or partially threaded.[1] The distance between each thread is called the pitch.[2]
Most screws and bolts are tightened by clockwise rotation, which is called a right-hand thread.[3][4] Screws with a left-hand thread are used in exceptional cases, such as where the screw will be subject to counterclockwise torque, which would tend to loosen a right-hand screw. For this reason, the left-side pedal of a bicycle has a left-hand thread.
There is no universally accepted distinction between a screw and a bolt. A simple distinction that is often true, although not always, is that a bolt passes through a substrate and takes a nut on the other side, whereas a screw takes no nut because it threads directly into the substrate (a screw screws into something, a bolt bolts several things together). So, as a general rule, when buying a packet of "screws", nuts would not be expected to be included, but bolts are often sold with matching nuts. Part of the confusion over this is likely due to regional or dialectical differences. Machinery's Handbook describes the distinction as follows:
A bolt is an externally threaded fastener designed for insertion through holes in assembled parts, and is normally intended to be tightened or released by torquing a nut. A screw is an externally threaded fastener capable of being inserted into holes in assembled parts, of mating with a preformed internal thread or forming its own thread, and of being tightened or released by torquing the head. An externally threaded fastener which is prevented from being turned during assembly and which can be tightened or released only by torquing a nut is a bolt. (Example: round head bolts, track bolts, plow bolts.) An externally threaded fastener that has thread form which prohibits assembly with a nut having a straight thread of multiple pitch length is a screw. (Example: wood screws, tapping screws.)[5]This distinction is consistent with ASME B18.2.1 and some dictionary definitions for screw[6][7] and bolt.[8][9][10]
The issue of what is a screw and what is a bolt is not completely resolved with Machinery's Handbook distinction, however, because of confounding terms, the ambiguous nature of some parts of the distinction, and usage variations.[11][failed verification] Some of these issues are discussed below:
The federal government of the United States made an effort to formalize the difference between a bolt and a screw, because different tariffs apply to each.[12] The document seems to have no significant effect on common usage and does not eliminate the ambiguous nature of the distinction between screws and bolts for some threaded fasteners. The document also reflects (although it probably did not originate) significant confusion of terminology usage that differs between the legal/statutory/regulatory community and the fastener industry. The legal/statutory/regulatory wording uses the terms "coarse" and "fine" to refer to the tightness of the tolerance range, referring basically to "high-quality" or "low-quality", but this is a poor choice of terms, because those terms in the fastener industry have a different meaning (referring to the steepness of the helix's lead).
The distinctions above are enforced in the controlled vocabulary of standards organizations. Nevertheless, there are sometimes differences between the controlled vocabulary and the natural language use of the words by machinists, auto mechanics and others. These differences reflect linguistic evolution shaped by the changing of technology over centuries. The words bolt and screw have both existed since before today's modern mix of fastener types existed, and the natural usage of those words has evolved retronymously in response to the technological change. (That is, the use of words as names for objects changes as the objects change.) Non-threaded fasteners predominated until the advent of practical, inexpensive screw-cutting in the early 19th century. The basic meaning of the word screw has long involved the idea of a helical screw thread, but the Archimedes screw and the screw gimlet (like a corkscrew) preceded the fastener.
The word bolt is also a very old word, and it was used for centuries to refer to metal rods that passed through the substrate to be fastened on the other side, often via nonthreaded means (clinching, forge welding, pinning, wedging, etc.). The connection of this sense to the sense of a door bolt or the crossbow bolt is apparent. In the 19th century, bolts fastened via screw threads were often called screw bolts in contradistinction to clench bolts.
In common usage, the distinction (not rigorous) is often that screws are smaller than bolts, and that screws are generally tapered while bolts are not. For example, cylinder head bolts are called "bolts" (at least in North American usage) despite the fact that by some definitions they ought to be called "screws". Their size and their similarity to a bolt that would take a nut seem linguistically to overrule any other factors in this natural word choice proclivity.
Old USS and SAE standards defined cap screws as fasteners with shanks that were threaded to the head and bolts as fasteners with shanks that were partially unthreaded.[13] The relationship of this rule to the idea that a bolt by definition takes a nut is clear (because the unthreaded section of the shank, which is called the grip, was expected to pass through the substrate without threading into it). This is now an obsolete distinction, although large bolts still often have unthreaded sections of shank.
Although there is no reason to consider this definition obsolete, because it is far from clear that "a bolt by definition takes a nut". Using a coach "bolt" as an example (and it has been a 'bolt' for a very long time). It was not originally intended to receive a nut, but did have a shank. Its purpose was not to pass through the entire substrate but only one piece of it, while the threaded portion bit into the other in order to draw, and clamp the materials together. The 'carriage' bolt was derived from this and was employed more to speed up manufacturing than achieve a different function. The carriage bolt passes through both pieces of materials and employs a nut to provide the clamping force. Both are still, however, bolts.
Bolts have been defined as headed fasteners having external threads that meet an exacting, uniform bolt thread specification (such as ISO metric screw thread M, MJ, Unified Thread Standard UN, UNR, and UNJ) such that they can accept a non-tapered nut. Screws are then defined as headed, externally threaded fasteners that do not meet the above definition of bolts.[citation needed] These definitions of screw and bolt eliminate the ambiguity of the Machinery's handbook distinction. And it is for that reason, perhaps, that some people favor them. However, they are neither compliant with common usage of the two words nor are they compliant with formal specifications.
A possible distinction is that a screw is designed to cut its own thread; it has no need for access from or exposure to the opposite side of the component being fastened to. This definition of screw is further reinforced by the consideration of the developments of fasteners such as Tek Screws, with either round or hex heads, for roof cladding, self-drilling and self-tapping screws for various metal fastening applications, roof batten screws to reinforce the connection between the roof batten and the rafter, decking screws etc. On the other hand, a bolt is the male part of a fastener system designed to be accepted by a pre-equipped socket (or nut) of exactly the same thread design.[citation needed]
Threaded fasteners either have a tapered shank or a non-tapered shank. Fasteners with tapered shanks are designed to either be driven into a substrate directly or into a pilot hole in a substrate, and most are classed as screws. Mating threads are formed in the substrate as these fasteners are driven in. Fasteners with a non-tapered shank are generally designed to mate with a nut or to be driven into a tapped hole, and most would be classed as bolts, although some are thread-forming (eg. taptite) and some authorities would treat some as screws when they are used with a female threaded fastener other than a nut.
A fastener with a built in washer is called a SEM or SEMS, short for pre-asSEMbled.[14][15] It could be fitted on either a tapered or non-tapered shank.
Early wood screws were made by hand, with a series of files, chisels, and other cutting tools, and these can be spotted easily by noting the irregular spacing and shape of the threads, as well as file marks remaining on the head of the screw and in the area between threads. Many of these screws had a blunt end, completely lacking the sharp tapered point on nearly all modern wood screws.[16] Some wood screws were made with cutting dies as early as the late 1700s (possibly even before 1678 when the book content was first published in parts).[17] Eventually, lathes were used to manufacture wood screws, with the earliest patent being recorded in 1760 in England.[16] During the 1850s, swaging tools were developed to provide a more uniform and consistent thread. Screws made with these tools have rounded valleys with sharp and rough threads.[18][19]
Once screw turning machines were in common use, most commercially available wood screws were produced with this method. These cut wood screws are almost invariably tapered, and even when the tapered shank is not obvious, they can be discerned because the threads do not extend past the diameter of the shank. Such screws are best installed after drilling a pilot hole with a tapered drill bit. The majority of modern wood screws, except for those made of brass, are formed on thread rolling machines. These screws have a constant diameter, threads with a larger diameter than the shank, and are stronger because the rolling process does not cut the grain of the metal.[citation needed]
ASME standards specify a variety of machine screws[20] in diameters ranging up to 0.75 in (19.05 mm). In practice, they tend to be mostly available in smaller sizes and the smaller sizes are referred to as screws or less ambiguously as machine screws, although some kinds of machine screw can be referred to as stove bolts.[citation needed]
ASME standard B18.2.1-1996 specifies hex cap screws whose size range is 0.25–3 in (6.35–76.20 mm) in diameter. These fasteners are very similar to hex bolts. They differ mostly in that they are manufactured to tighter tolerances than the corresponding bolts. Machinery's Handbook refers parenthetically to these fasteners as finished hex bolts.[21] Reasonably, these fasteners might be referred to as bolts, but based on the US government document Distinguishing Bolts from Screws, the US government might classify them as screws because of the tighter tolerance.[22] In 1991 responding to an influx of counterfeit fasteners Congress passed PL 101-592[23] "Fastener Quality Act" This resulted in the rewriting of specifications by the ASME B18 committee. B18.2.1[24] was re-written and as a result they eliminated the finished hex bolts and renamed them the hex cap screw—a term that had existed in common usage long before, but was now also being codified as an official name for the ASME B18 standard.
These terms refer to fasteners that are designed to be threaded into a tapped hole that is in part of the assembly and so based on the Machinery's Handbook distinction they would be screws. Here common terms are at variance with Machinery's Handbook distinction.[25][26]
Lag screws (US) or coach screws (UK, Australia, and New Zealand) (also referred to as lag bolts or coach bolts, although this is a misnomer) or French wood screw (Scandinavia) are large wood screws. The head is typically an external hex. Metric hex-headed lag screws are covered by DIN 571. Inch square-headed and hex-headed lag screws are covered by ASME B18.2.1. A typical lag screw can range in diameter from 4 to 20 mm or #10 to 1.25 in (4.83 to 31.75 mm), and lengths from 16 to 200 mm or 1⁄4 to 6 in (6.35 to 152.40 mm) or longer, with the coarse threads of a wood-screw or sheet-metal-screw threadform (but larger).
The materials are usually carbon steel substrate with a coating of zinc galvanization (for corrosion resistance). The zinc coating may be bright  yellow (electroplated), or dull gray (hot-dip galvanized). Lag screws are used to lag together lumber framing, to lag machinery feet to wood floors, and for other heavy carpentry applications. The attributive modifier lag came from an early principal use of such fasteners: the fastening of lags such as barrel staves and other similar parts.[27]
These fasteners are "screws" according to the Machinery's Handbook criteria, and the obsolescent term "lag bolt" has been replaced by "lag screw" in the Handbook.[28] However, based on tradition many tradesmen continue to refer to them as "bolts", because, like head bolts, they are large, with hex or square heads that require a wrench, socket, or specialized bit to turn.
A superbolt, or multi-jackbolt tensioner is an alternative type of fastener that retrofits or replaces existing nuts, bolts, or studs. Tension in the bolt is developed by torquing individual jackbolts, which are threaded through the body of the nut and push against a hardened washer. Because of this, the amount of torque required to achieve a given preload is reduced. Installation and removal of any size tensioner is achieved with hand tools, which can be advantageous when dealing with large diameter bolting applications.
The field of screws and other hardware for internal fixation within the body is huge and diverse. Like prosthetics, it integrates the industrial and medicosurgical fields, causing manufacturing technologies (such as machining, CAD/CAM, and 3D printing) to intersect with the art and science of medicine. Like aerospace and nuclear power, this field involves some of the highest technology for fasteners, as well as some of the highest prices, for the simple reason that performance, longevity, and quality have to be excellent in such applications. Bone screws tend to be made of stainless steel or titanium, and they often have high-end features such as conical threads, multistart threads, cannulation (hollow core), and proprietary screw drive types (some not seen outside of these applications).
These abbreviations have jargon currency among fastener specialists (who, working with many screw types all day long, have need to abbreviate repetitive mentions). The smaller basic ones can be built up into the longer ones; for example, knowing that "FH" means "flat head", it may be possible to parse the rest of a longer abbreviation containing "FH".
These abbreviations are not universally standardized across corporations; each corporation can coin their own. The more obscure ones may not be listed here.
The extra spacing between linked terms below helps the reader to see the correct parsing at a glance.
Screws and bolts are usually made of steel. 
Where great resistance to weather or corrosion is required, like in very small screws or medical implants, materials such as stainless steel, brass, titanium, bronze, silicon bronze or monel may be used.
Galvanic corrosion of dissimilar metals can be prevented (using aluminum screws for double-glazing tracks for example) by a careful choice of material. 
Some types of plastic, such as nylon or polytetrafluoroethylene (PTFE), can be threaded and used for fastenings requiring moderate strength and great resistance to corrosion or for the purpose of electrical insulation.
Often a surface coating is used to protect the fastener from corrosion (e.g. bright zinc plating for steel screws), to impart a decorative finish (e.g. japanning) or otherwise alter the surface properties of the base material.
Selection criteria of the screw materials include: size, required strength, resistance to corrosion, joint material, cost and temperature.
The numbers stamped on the head of the bolt are referred to the grade of the bolt used in certain application with the strength of a bolt. High-strength steel bolts usually have a hexagonal head with an ISO strength rating (called property class) stamped on the head. And the absence of marking/number indicates a lower grade bolt with low strength. The property classes most often used are 5.8, 8.8, and 10.9. The number before the point is the ultimate tensile strength in MPa divided by 100. The number after the point is the multiplier ratio of yield strength to ultimate tensile strength. For example, a property class 5.8 bolt has a nominal (minimum) ultimate tensile strength of 500 MPa, and a tensile yield strength of 0.8 times ultimate tensile strength or 0.8 (500) = 400 MPa.
Ultimate tensile strength is the tensile stress at which the bolt fails. Tensile yield strength is the stress at which the bolt will yield in tension across the entire section of the bolt and receive a permanent set (an elongation from which it will not recover when the force is removed) of 0.2% offset strain. Proof strength is the usable strength of the fastener. Tension testing of a bolt up to the proof load should not cause permanent set of the bolt and should be conducted on actual fasteners rather than calculated.[30] If a bolt is tensioned beyond the proof load, it may behave in plastic manner due to yielding in the threads and the tension preload may be lost due to the permanent plastic deformations. When elongating a fastener prior to reaching the yield point, the fastener is said to be operating in the elastic region; whereas elongation beyond the yield point is referred to as operating in the plastic region of the bolt material. If a bolt is loaded in tension beyond its proof strength, the yielding at the net root section of the bolt will continue until the entire section begins to yield and it has exceeded its yield strength. If tension increases, the bolt fractures at its ultimate strength.
Mild steel bolts have property class 4.6, which is 400 MPa ultimate strength and 0.6*400=240 MPa yield strength. High-strength steel bolts have property class 8.8, which is 800 MPa ultimate strength and 0.8*800=640 MPa yield strength or above.
The same type of screw or bolt can be made in many different grades of material. For critical high-tensile-strength applications, low-grade bolts may fail, resulting in damage or injury. On SAE-standard bolts, a distinctive pattern of marking is impressed on the heads to allow inspection and validation of the strength of the bolt.[31] However, low-cost counterfeit fasteners may be found with actual strength far less than indicated by the markings. Such inferior fasteners are a danger to life and property when used in aircraft, automobiles, heavy trucks, and similar critical applications.[32]
The international standards for metric externally threaded fasteners are ISO 898-1 for property classes produced from carbon steels and ISO 3506-1 for property classes produced from corrosion resistant steels. 
There are many standards governing the material and mechanical properties of imperial sized externally threaded fasteners. Some of the most common consensus standards for grades produced from carbon steels are ASTM A193, ASTM A307, ASTM A354, ASTM F3125, and SAE J429. Some of the most common consensus standards for grades produced from corrosion resistant steels are ASTM F593 & ASTM A193.
Some varieties of screw are manufactured with a break-away head, which snaps off when adequate torque is applied. This prevents tampering and also provides an easily inspectable joint to guarantee proper assembly. An example of this is the shear bolts used on vehicle steering columns, to secure the ignition switch.
Modern screws employ a wide variety of drive designs, each requiring a different kind of tool to drive in or extract them. The most common screw drives are the slotted and Phillips in the US; hex, Robertson, and Torx are also common in some applications, and Pozidriv has almost completely replaced Phillips in Europe. Some types of drive are intended for automatic assembly in mass-production of such items as automobiles. More exotic screw drive types may be used in situations where tampering is undesirable, such as in electronic appliances that should not be serviced by the home repair person.
The hand tool used to drive in most screws is called a screwdriver. A power tool that does the same job is a power screwdriver; power drills may also be used with screw-driving attachments. Where the holding power of the screwed joint is critical, torque-measuring and torque-limiting screwdrivers are used to ensure sufficient but not excessive force is developed by the screw. The hand tool for driving hex head threaded fasteners is a spanner (UK usage) or wrench (US usage), while a nut setter is used with a power screw driver.
There are many systems for specifying the dimensions of screws, but in much of the world the ISO metric screw thread preferred series has displaced the many older systems. Other relatively common systems include the British Standard Whitworth, BA system (British Association), and the Unified Thread Standard.
The basic principles of the ISO metric screw thread are defined in international standard ISO 68-1 and preferred combinations of diameter and pitch are listed in ISO 261. The smaller subset of diameter and pitch combinations commonly used in screws, nuts and bolts is given in ISO 262. The most commonly used pitch value for each diameter is the coarse pitch. For some diameters, one or two additional fine pitch variants are also specified, for special applications such as threads in thin-walled pipes. ISO metric screw threads are designated by the letter M followed by the major diameter of the thread in millimetres (e.g. M8). If the thread does not use the normal coarse pitch (e.g. 1.25 mm in the case of M8), then the pitch in millimeters is also appended with a multiplication sign (e.g. "M8×1" if the screw thread has an outer diameter of 8 mm and advances by 1 mm per 360° rotation).
The nominal diameter of a metric screw is the outer diameter of the thread. The tapped hole (or nut) into which the screw fits, has an internal diameter which is the size of the screw minus the pitch of the thread. Thus, an M6 screw, which has a pitch of 1 mm, is made by threading a 6 mm shank, and the nut or threaded hole is made by tapping threads into a hole of 5 mm diameter (6 mm - 1 mm).
Metric hexagon bolts, screws and nuts are specified, for example, in International Standards ISO 4014, ISO 4017, and ISO 4032. The following table lists the relationship given in these standards between the thread size and the maximum width across the hexagonal flats (wrench size):
In addition, the following non-preferred intermediate sizes are specified:
Bear in mind that these are just examples and the width across flats is different for structural bolts, flanged bolts, and also varies by standards organization.
The first person to create a standard (in about 1841) was the English engineer Sir Joseph Whitworth. Whitworth screw sizes are still used, both for repairing old machinery and where a coarser thread than the metric fastener thread is required. Whitworth became British Standard Whitworth, abbreviated to BSW (BS 84:1956) and the British Standard Fine (BSF) thread was introduced in 1908 because the Whitworth thread was too coarse for some applications. The thread angle was 55°, and the depth and pitch varied with the diameter of the thread (i.e., the bigger the bolt, the coarser the thread). Spanners for Whitworth bolts are marked with the size of the bolt, not the distance across the flats of the screw head.
The most common use of a Whitworth pitch nowadays is in all UK scaffolding. Additionally, the standard photographic tripod thread, which for small cameras is 1/4" Whitworth (20 tpi) and for medium/large format cameras is 3/8" Whitworth (16 tpi). It is also used for microphone stands and their appropriate clips, again in both sizes, along with "thread adapters" to allow the smaller size to attach to items requiring the larger thread. Note that while 1/4" UNC bolts fit 1/4" BSW camera tripod bushes, yield strength is reduced by the different thread angles of 60° and 55° respectively.
British Association (BA) screw threads, named after the British Association for Advancement of Science, were devised in 1884 and standardised in 1903. Screws were described as "2BA", "4BA" etc., the odd numbers being rarely used, except in equipment made prior to the 1970s for telephone exchanges in the UK. This equipment made extensive use of odd-numbered BA screws, in order—it may be suspected—to reduce theft. BA threads are specified by British Standard BS 93:1951 "Specification for British Association (B.A.) screw threads with tolerances for sizes 0 B.A. to 16 B.A."
While not related to ISO metric screws, the sizes were actually defined in metric terms, a 0BA thread having a 6 mm diameter and 1 mm pitch. Other threads in the BA series are related to 0BA in a geometric series with the common factors 0.9 and 1.2. For example, a 4BA thread has pitch  mm (0.65 mm) and diameter  mm (3.62 mm). Although 0BA has the same diameter and pitch as ISO M6, the threads have different forms and are not compatible.
BA threads are still common in some niche applications. Certain types of fine machinery, such as moving-coil meters and clocks, tend to have BA threads wherever they are manufactured. BA sizes were also used extensively in aircraft, especially those manufactured in the United Kingdom. BA sizing is still used in railway signalling, mainly for the termination of electrical equipment and cabling.
BA threads are extensively used in Model Engineering where the smaller hex head sizes make scale fastenings easier to represent. As a result, many UK Model Engineering suppliers still carry stocks of BA fasteners up to typically 8BA and 10BA. 5BA is also commonly used as it can be threaded onto 1/8 rod.[48]
The Unified Thread Standard (UTS) is most commonly used in the United States, but is also extensively used in Canada and occasionally in other countries. The size of a UTS screw is described using the following format: X-Y, where X is the nominal size (the hole or slot size in standard manufacturing practice through which the shank of the screw can easily be pushed) and Y is the threads per inch (TPI). For sizes 1⁄4 inch and larger the size is given as a fraction; for sizes less than this an integer is used, ranging from 0 to 16. The integer sizes can be converted to the actual diameter by using the formula 0.060 + (0.013 × number). For example, a #4 screw is 0.060 + (0.013 × 4) = 0.060 + 0.052 = 0.112 inches in diameter. There are also screw sizes smaller than "0" (zero or ought). The sizes are 00, 000, 0000 which are usually referred to as two ought, three ought, and four ought. Most eyeglasses have the bows screwed to the frame with 00-72 (pronounced double ought – seventy two) size screws. To calculate the major diameter of "ought" size screws count the number of 0's and multiply this number by 0.013 and subtract from 0.060. For example, the major diameter of a 000-72 screw thread is .060 – (3 x .013) = 0.060 - 0.039 = .021 inches. For most size screws there are multiple TPI available, with the most common being designated a Unified Coarse Thread (UNC or UN) and Unified Fine Thread (UNF or UF). Note: In countries other than the United States and Canada, the ISO Metric Screw Thread System is primarily used today. Unlike most other countries the United States and Canada still use the Unified (Inch) Thread System. However, both are moving over to the ISO Metric System.[citation needed] It is estimated that approximately 60% of screw threads in use in the United States are still inch based.[49]
There are three steps in manufacturing a screw: heading, thread rolling, and coating. Screws are normally made from wire, which is supplied in large coils, or round bar stock for larger screws. The wire or rod is then cut to the proper length for the type of screw being made; this workpiece is known as a blank. It is then cold headed, which is a cold working process. Heading produces the head of the screw. The shape of the die in the machine dictates what features are pressed into the screw head; for example a flat head screw uses a flat die. For more complicated shapes two heading processes are required to get all of the features into the screw head. This production method is used because heading has a very high production rate, and produces virtually no waste material. Slotted head screws require an extra step to cut the slot in the head; this is done on a slotting machine. These machines are essentially stripped down milling machines designed to process as many blanks as possible.
The blanks are then polished[citation needed] again prior to threading. The threads are usually produced via thread rolling; however, some are cut. The workpiece is then tumble finished with wood and leather media to do final cleaning and polishing.[citation needed] For most screws, a coating, such as electroplating with zinc (galvanizing) or applying black oxide, is applied to prevent corrosion.
While a recent hypothesis attributes the Archimedes' screw to Sennacherib, King of Assyria (r. 705 BC – 681 BC), archaeological finds and pictorial evidence only appear in the Hellenistic period (after 323 BC), and the standard view regards the  screw machine as a Greek invention, most probably by the 3rd-century BC polymath Archimedes.[50][dubious  – discuss] Though resembling a screw, the screw mechanism associated with the name of Archimedes is not a screw in the usual ("fastening") sense of the word.
Earlier, the screw had been described by the Greek mathematician  Archytas of Tarentum (428–350 BC). By the 1st century BC, wooden screws (screws made of wood) were commonly used throughout the  Mediterranean world in screw presses for pressing olive oil from olives and for pressing juice from grapes in winemaking. Metal screws used as fasteners were rare in Europe before the 15th century, if known at all.[51]
Handheld screwdrivers (formerly called "turnscrews" in English, in direct parallel to their original French name, tournevis[52]) have existed since medieval times (the 1580s at the latest). However they probably did not become truly widespread until after 1800, once threaded fasteners had become commodified.[53]
There were many forms of fastening in use before threaded fasteners became widespread. They tended to involve carpentry and smithing rather than machining, and they involved concepts such as dowels and pins, wedging,  mortises and tenons,  dovetails, nailing (with or without clenching the nail ends), forge welding, and many kinds of binding with cord made of leather or fiber, using many kinds of knots. Prior to the mid-19th century, ship-builders used  cotter pins or pin bolts, and "clinch bolts" (now called  "rivets"). Glues also existed, although not in the profusion used today.
The metal screw did not become a common fastener until machine tools for mass production developed toward the end of the 18th century. This development blossomed in the 1760s and 1770s[54] along two separate paths that soon converged:[55] the mass production of wood screws (meaning screws made of metal to be used in working with wood) in a specialized, single-purpose, high-volume-production machine tool; and the low-count, toolroom-style production of machine screws (V-thread) with easy selection among various pitches (whatever the machinist happened to need on any given day).
The first path was pioneered by brothers Job and William Wyatt of Staffordshire, UK,[56] who patented in 1760 a machine that one might today best call a screw machine of an early and prescient sort. It made use of a leadscrew to guide the cutter to produce the desired pitch,[56] and the slot was cut with a rotary file while the main spindle held still (presaging live tools on lathes 250 years later). Not until 1776 did the Wyatt brothers have a wood-screw factory up and running.[56] Their enterprise failed, but new owners soon made it prosper, and in the 1780s they were producing 16,000 screws a day with only 30 employees[57]—the kind of industrial productivity and output volume that would later become characteristic of modern industry but which was revolutionary at the time.
Meanwhile, English instrument-maker Jesse Ramsden (1735–1800) was working on the  toolmaking and  instrument-making end of the screw-cutting problem, and in 1777 he invented the first satisfactory screw-cutting lathe.[49] The British engineer Henry Maudslay (1771–1831) gained fame by popularizing such lathes with his screw-cutting lathes of 1797 and 1800, containing the trifecta of leadscrew, slide rest, and change-gear gear train, all in the right proportions for industrial machining. In a sense he unified the paths of the Wyatts and Ramsden and did for machine screws what had already been done for wood screws, i.e., significant easing of production spurring commodification. His firm would remain a leader in machine tools for decades afterward. A misquoting of James Nasmyth popularized the notion that Maudslay had invented the slide rest, but this was incorrect; however, his lathes helped to popularize it.
These developments of the 1760–1800 era, with the Wyatts and Maudslay as arguably the most important drivers, caused great increase in the use of threaded fasteners.  Standardization of threadforms began almost immediately, but it was not quickly completed; it has been an evolving process ever since. Further improvements to the mass production of screws continued to push unit prices lower and lower for decades to come, throughout the 19th century.[58]
In 1821 Hardman Philips built the first screw factory in the United States - on Moshannon Creek, near Philipsburg - for the manufacture of blunt metal screws. An expert in screw manufacture, Thomas Lever, was brought over from England to run the factory. The mill used steam and water power, with hardwood charcoal as fuel. The screws were made from wire prepared by "rolling and wire drawing apparatus" from iron manufactured at a nearby forge. The screw mill was not a commercial success. It eventually failed due to competition from the lower-cost, gimlet-pointed screw, and ceased operations in 1836.[59]
The American development of the turret lathe (1840s) and of automatic screw machines derived from it (1870s) drastically reduced the unit cost of threaded fasteners by increasingly automating the machine-tool control. This cost reduction spurred ever greater use of screws.
Throughout the 19th century, the most commonly used forms of screw head (that is, drive types) were simple internal-wrenching straight slots and external-wrenching squares and hexagons. These were easy to machine and served most applications adequately. Rybczynski describes a flurry of patents for alternative drive types in the 1860s through 1890s,[60] but explains that these were patented but not manufactured due to the difficulties and expense of doing so at the time. In 1908, Canadian P. L. Robertson was the first to make the internal-wrenching square socket drive a practical reality by developing just the right design (slight taper angles and overall proportions) to allow the head to be stamped easily but successfully, with the metal cold forming as desired rather than being sheared or displaced in unwanted ways.[60] Practical manufacture of the internal-wrenching hexagon drive (hex socket) shortly followed in 1911.[61][62]
In the early 1930s American Henry F. Phillips popularized the Phillips-head screw.[63]
Threadform standardization further improved in the late 1940s, when the ISO metric screw thread and the Unified Thread Standard were defined.
Precision screws, for controlling motion rather than fastening, developed around the turn of the 19th century, and represented one of the central technical advances, along with flat surfaces, that enabled the industrial revolution.[64] They are key components of  micrometers and lathes.
Alternative fastening methods are:


A transistor is a semiconductor device used to amplify or switch electrical signals and power. It is one of the basic building blocks of modern electronics.[1] It is composed of semiconductor material, usually with at least three terminals for connection to an electronic circuit. A voltage or current applied to one pair of the transistor's terminals controls the current through another pair of terminals. Because the controlled (output) power can be higher than the controlling (input) power, a transistor can amplify a signal. Some transistors are packaged individually, but many more in miniature form are found embedded in integrated circuits.
Physicist Julius Edgar Lilienfeld proposed the concept of a field-effect transistor in 1926, but it was not possible to construct a working device at that time.[2] The first working device was a point-contact transistor invented in 1947 by physicists John Bardeen, Walter Brattain, and William Shockley at Bell Labs; the three shared the 1956 Nobel Prize in Physics for their achievement.[3] The most widely used type of transistor is the metal–oxide–semiconductor field-effect transistor (MOSFET), invented by Mohamed Atalla and Dawon Kahng at Bell Labs in 1959.[4][5][6] Transistors revolutionized the field of electronics and paved the way for smaller and cheaper radios, calculators, computers, and other electronic devices.
Most transistors are made from very pure silicon, and some from germanium, but certain other semiconductor materials are sometimes used. A transistor may have only one kind of charge carrier, in a field-effect transistor, or may have two kinds of charge carriers in  bipolar junction transistor devices. Compared with the vacuum tube, transistors are generally smaller and require less power to operate. Certain vacuum tubes have advantages over transistors at very high operating frequencies or high operating voltages. Many types of transistors are made to standardized specifications by multiple manufacturers.
The thermionic triode, a vacuum tube invented in 1907, enabled amplified radio technology and long-distance telephony. The triode, however, was a fragile device that consumed a substantial amount of power. In 1909, physicist William Eccles discovered the crystal diode oscillator.[7] Physicist Julius Edgar Lilienfeld filed a patent for a field-effect transistor (FET) in Canada in 1925,[8] intended as a solid-state replacement for the triode.[9][10] He filed identical patents in the United States in 1926[11] and 1928.[12][13] However, he did not publish any research articles about his devices nor did his patents cite any specific examples of a working prototype. Because the production of high-quality semiconductor materials was still decades away, Lilienfeld's solid-state amplifier ideas would not have found practical use in the 1920s and 1930s, even if such a device had been built.[14] In 1934, inventor Oskar Heil patented a similar device in Europe.[15]
From November 17 to December 23, 1947, John Bardeen and Walter Brattain at AT&T's Bell Labs in Murray Hill, New Jersey performed experiments and observed that when two gold point contacts were applied to a crystal of germanium, a signal was produced with the output power greater than the input.[16] Solid State Physics Group leader William Shockley saw the potential in this, and over the next few months worked to greatly expand the knowledge of semiconductors. The term transistor was coined by John R. Pierce as a contraction of the term transresistance.[17][18][19] According to Lillian Hoddeson and Vicki Daitch, Shockley proposed that Bell Labs' first patent for a transistor should be based on the field-effect and that he be named as the inventor. Having unearthed Lilienfeld's patents that went into obscurity years earlier, lawyers at Bell Labs advised against Shockley's proposal because the idea of a field-effect transistor that used an electric field as a "grid" was not new. Instead, what Bardeen, Brattain, and Shockley invented in 1947 was the first point-contact transistor.[14] To acknowledge this accomplishment, Shockley, Bardeen and Brattain jointly received the 1956 Nobel Prize in Physics "for their researches on semiconductors and their discovery of the transistor effect".[20][21]
Shockley's team initially attempted to build a field-effect transistor (FET) by trying to modulate the conductivity of a semiconductor, but was unsuccessful, mainly due to problems with the surface states, the dangling bond, and the germanium and copper compound materials. Trying to understand the mysterious reasons behind this failure led them instead to invent the bipolar point-contact and junction transistors.[22][23]
In 1948, the point-contact transistor was independently invented by physicists Herbert Mataré and Heinrich Welker while working at the Compagnie des Freins et Signaux Westinghouse, a Westinghouse subsidiary in Paris. Mataré had previous experience in developing crystal rectifiers from silicon and germanium in the German radar effort during World War II. With this knowledge, he began researching the phenomenon of "interference" in 1947. By June 1948, witnessing currents flowing through point-contacts, he produced consistent results using samples of germanium produced by Welker, similar to what Bardeen and Brattain had accomplished earlier in December 1947. Realizing that Bell Labs' scientists had already invented the transistor, the company rushed to get its "transition" into production for amplified use in France's telephone network, filing his first transistor patent application on August 13, 1948.[24][25][26]
The first bipolar junction transistors were invented by Bell Labs' William Shockley, who applied for patent (2,569,347) on June 26, 1948. On April 12, 1950, Bell Labs chemists Gordon Teal and Morgan Sparks successfully produced a working bipolar NPN junction amplifying germanium transistor. Bell announced the discovery of this new "sandwich" transistor in a press release on July 4, 1951.[27][28]
The first high-frequency transistor was the surface-barrier germanium transistor developed by Philco in 1953, capable of operating at frequencies up to 60 MHz.[29] They were made by etching depressions into an n-type germanium base from both sides with jets of Indium(III) sulfate until it was a few ten-thousandths of an inch thick. Indium electroplated into the depressions formed the collector and emitter.[30][31]
AT&T first used transistors in telecommunications equipment in the No. 4A Toll Crossbar Switching System in 1953, for selecting trunk circuits from routing information encoded on translator cards.[32] Its predecessor, the Western Electric No. 3A phototransistor, read the mechanical encoding from punched metal cards.
The first prototype pocket transistor radio was shown by INTERMETALL, a company founded by Herbert Mataré in 1952, at the Internationale Funkausstellung Düsseldorf from August 29 to September 6, 1953.[33][34] The first production-model pocket transistor radio was the Regency TR-1, released in October 1954.[21] Produced as a joint venture between the Regency Division of Industrial Development Engineering Associates, I.D.E.A. and Texas Instruments of Dallas, Texas, the TR-1 was manufactured in Indianapolis, Indiana. It was a near pocket-sized radio with four transistors and one germanium diode. The industrial design was outsourced to the Chicago firm of Painter, Teague and Petertil. It was initially released in one of six colours: black, ivory, mandarin red, cloud grey, mahogany and olive green. Other colours shortly followed.[35][36][37]
The first production all-transistor car radio was developed by Chrysler and Philco corporations and was announced in the April 28, 1955 edition of the Wall Street Journal. Chrysler made the Mopar model 914HR available as an option starting in fall 1955 for its new line of 1956 Chrysler and Imperial cars, which reached dealership showrooms on October 21, 1955.[38][39]
The Sony TR-63, released in 1957, was the first mass-produced transistor radio, leading to the widespread adoption of transistor radios.[40] Seven million TR-63s were sold worldwide by the mid-1960s.[41] Sony's success with transistor radios led to transistors replacing vacuum tubes as the dominant electronic technology in the late 1950s.[42]
The first working silicon transistor was developed at Bell Labs on January 26, 1954, by Morris Tanenbaum. The first production commercial silicon transistor was announced by Texas Instruments in May 1954. This was the work of Gordon Teal, an expert in growing crystals of high purity, who had previously worked at Bell Labs.[43][44][45]
The basic principle of the field-effect transistor (FET) was first proposed by physicist Julius Edgar Lilienfeld when he filed a patent for a device similar to MESFET in 1926, and for an insulated-gate field-effect transistor in 1928.[46][47] The FET concept was later also theorized by engineer Oskar Heil in the 1930s and by William Shockley in the 1940s.
In 1945 JFET was patented by Heinrich Welker.[48] Following Shockley's theoretical treatment on JFET in 1952, a working practical JFET was made in 1953 by George C. Dacey and Ian M. Ross.[49]
In 1948, Bardeen patented the progenitor of MOSFET, an insulated-gate FET (IGFET) with an inversion layer. Bardeen's patent, and the concept of an inversion layer, forms the basis of CMOS technology today.[50]
In the early years of the semiconductor industry, companies focused on the junction transistor, a relatively bulky device that was difficult to mass-produce, limiting it to several specialized applications. Field-effect transistors (FETs) were theorized as potential alternatives, but researchers could not get them to work properly, largely due to the surface state barrier that prevented the external electric field from penetrating the material.[51]
In 1957, Bell Labs engineer Mohamed Atalla proposed a new method of semiconductor device fabrication: coating a silicon wafer with an insulating layer of silicon oxide so electricity could overcome the surface state and reliably penetrate to the semiconducting silicon below. The process, known as surface passivation, became critical to the semiconductor industry, as it enabled the mass-production of silicon integrated circuits.[52][53][54] Building on the method, he developed the metal–oxide–semiconductor (MOS) process,[52] and proposed that it could be used to build the first working silicon FET. 
Atalla and his Korean colleague Dawon Kahng developed the metal–oxide–semiconductor field-effect transistor (MOSFET), or MOS transistor, in 1959,[52][4][5] the first transistor that could be miniaturized and mass-produced for a wide range of uses.[51] In a self-aligned CMOS process, a transistor is formed wherever the gate layer (polysilicon or metal) crosses a diffusion layer.[55]: p.1 (see Fig. 1.1)  With its high scalability,[56] much lower power consumption, and higher density than bipolar junction transistors,[57] the MOSFET made it possible to build high-density integrated circuits,[6] allowing the integration of more than 10,000 transistors in a single IC.[58]
CMOS (complementary MOS) was invented by Chih-Tang Sah and Frank Wanlass at Fairchild Semiconductor in 1963.[59] The first report of a floating-gate MOSFET was made by Dawon Kahng and Simon Sze in 1967.[60] A double-gate MOSFET was first demonstrated in 1984 by Electrotechnical Laboratory researchers Toshihiro Sekigawa and Yutaka Hayashi.[61][62] FinFET (fin field-effect transistor), a type of 3D non-planar multi-gate MOSFET, originated from the research of Digh Hisamoto and his team at Hitachi Central Research Laboratory in 1989.[63][64]
Because transistors are the key active components in practically all modern electronics, many people consider them one of the 20th century's greatest inventions.[65]
The invention of the first transistor at Bell Labs was named an IEEE Milestone in 2009.[66] Other Milestones include the inventions of the junction transistor in 1948 and the MOSFET in 1959.[67]
The MOSFET is by far the most widely used transistor, in applications ranging from computers and electronics[53] to communications technology such as smartphones.[68] It has been considered the most important transistor,[69] possibly the most important invention in electronics,[70] and the device that enabled modern electronics.[71] It has been the basis of modern digital electronics since the late 20th century, paving the way for the digital age.[72] The US Patent and Trademark Office calls it a "groundbreaking invention that transformed life and culture around the world".[68] Its ability to be mass-produced by a highly automated process (semiconductor device fabrication), from relatively basic materials, allows astonishingly low per-transistor costs. MOSFETs are the most numerously produced artificial objects in history, with more than 13 sextillion manufactured by 2018.[73]
Although several companies each produce over a billion individually packaged (known as discrete) MOS transistors every year,[74] the vast majority are produced in integrated circuits (also known as IC's, microchips, or simply chips), along with diodes, resistors, capacitors and other electronic components, to produce complete electronic circuits. A logic gate consists of up to about 20 transistors, whereas an advanced microprocessor, as of 2022, may contain as many as 57 billion MOSFETs.[75]
The transistor's low cost, flexibility and reliability have made it ubiquitous. Transistorized mechatronic circuits have replaced electromechanical devices in controlling appliances and machinery. It is often easier and cheaper to use a standard microcontroller and write a computer program to carry out a control function than to design an equivalent mechanical system.
A transistor can use a small signal applied between one pair of its terminals to control a much larger signal at another pair of terminals, a property called gain. It can produce a stronger output signal, a voltage or current, proportional to a weaker input signal, acting as an amplifier. It can also be used as an electrically controlled switch, where the amount of current is determined by other circuit elements.[76]
There are two types of transistors, with slight differences in how they are used:
The image represents a typical bipolar transistor in a circuit. A charge flows between emitter and collector terminals depending on the current in the base. Because the base and emitter connections behave like a semiconductor diode, a voltage drop develops between them. The amount of this drop, determined by the transistor's material, is referred to as VBE.[77]
Transistors are commonly used in digital circuits as electronic switches which can be either in an on or off state, both for high-power applications such as switched-mode power supplies and for low-power applications such as logic gates. Important parameters for this application include the current switched, the voltage handled, and the switching speed, characterized by the rise and fall times.[77]
In a switching circuit, the goal is to simulate, as near as possible, the ideal switch having the properties of an open circuit when off, the short circuit when on, and an instantaneous transition between the two states. Parameters are chosen such that the "off" output is limited to leakage currents too small to affect connected circuitry, the resistance of the transistor in the "on" state is too small to affect circuitry, and the transition between the two states is fast enough not to have a detrimental effect.[77]
In a grounded-emitter transistor circuit, such as the light-switch circuit shown, as the base voltage rises, the emitter and collector currents rise exponentially. The collector voltage drops because of reduced resistance from the collector to the emitter. If the voltage difference between the collector and emitter were zero (or near zero), the collector current would be limited only by the load resistance (light bulb) and the supply voltage. This is called saturation because the current is flowing from collector to emitter freely. When saturated, the switch is said to be on.[78]
The use of bipolar transistors for switching applications requires biasing the transistor so that it operates between its cut-off region in the off-state and the saturation region (on). This requires sufficient base drive current. As the transistor provides current gain, it facilitates the switching of a relatively large current in the collector by a much smaller current into the base terminal. The ratio of these currents varies depending on the type of transistor, and even for a particular type, varies depending on the collector current. In the example of a light-switch circuit, as shown, the resistor is chosen to provide enough base current to ensure the transistor is saturated.[77] The base resistor value is calculated from the supply voltage, transistor C-E junction voltage drop, collector current, and amplification factor beta.[79]
The common-emitter amplifier is designed so that a small change in voltage (Vin) changes the small current through the base of the transistor whose current amplification combined with the properties of the circuit means that small swings in Vin produce large changes in Vout.[77]
Various configurations of single transistor amplifiers are possible, with some providing current gain, some voltage gain, and some both.
From mobile phones to televisions, vast numbers of products include amplifiers for sound reproduction, radio transmission, and signal processing. The first discrete-transistor audio amplifiers barely supplied a few hundred milliwatts, but power and audio fidelity gradually increased as better transistors became available and amplifier architecture evolved.[77]
Modern transistor audio amplifiers of up to a few hundred watts are common and relatively inexpensive.
Before transistors were developed, vacuum (electron) tubes (or in the UK "thermionic valves" or just "valves") were the main active components in electronic equipment.
The key advantages that have allowed transistors to replace vacuum tubes in most applications are
Transistors may have the following limitations:
Transistors are categorized by
Hence, a particular transistor may be described as silicon, surface-mount, BJT, NPN, low-power, high-frequency switch.
Convenient mnemonic to remember the type of transistor (represented by an electrical symbol) involves the direction of the arrow. For the BJT, on an n-p-n transistor symbol, the arrow will "Not Point iN". On a p-n-p transistor symbol, the arrow "Points iN Proudly". This however does not apply to MOSFET-based transistor symbols as the arrow is typically reversed (i.e. the arrow for the n-p-n points inside).
The field-effect transistor, sometimes called a unipolar transistor, uses either electrons (in n-channel FET) or holes (in p-channel FET) for conduction. The four terminals of the FET are named source, gate, drain, and body (substrate). On most FETs, the body is connected to the source inside the package, and this will be assumed for the following description.
In a FET, the drain-to-source current flows via a conducting channel that connects the source region to the drain region. The conductivity is varied by the electric field that is produced when a voltage is applied between the gate and source terminals, hence the current flowing between the drain and source is controlled by the voltage applied between the gate and source. As the gate–source voltage (VGS) is increased, the drain–source current (IDS) increases exponentially for VGS below threshold, and then at a roughly quadratic rate:  (IDS ∝  (VGS − VT)2, where VT is the threshold voltage at which drain current begins)[83] in the "space-charge-limited" region above threshold. A quadratic behavior is not observed in modern devices, for example, at the 65 nm technology node.[84]
For low noise at narrow bandwidth, the higher input resistance of the FET is advantageous.
FETs are divided into two families: junction FET (JFET) and insulated gate FET (IGFET). The IGFET is more commonly known as a metal–oxide–semiconductor FET (MOSFET), reflecting its original construction from layers of metal (the gate), oxide (the insulation), and semiconductor. Unlike IGFETs, the JFET gate forms a p–n diode with the channel which lies between the source and drains. Functionally, this makes the n-channel JFET the solid-state equivalent of the vacuum tube triode which, similarly, forms a diode between its grid and cathode. Also, both devices operate in the depletion-mode, they both have a high input impedance, and they both conduct current under the control of an input voltage.
Metal–semiconductor FETs (MESFETs) are JFETs in which the reverse biased p–n junction is replaced by a metal–semiconductor junction. These, and the HEMTs (high-electron-mobility transistors, or HFETs), in which a two-dimensional electron gas with very high carrier mobility is used for charge transport, are especially suitable for use at very high frequencies (several GHz).
FETs are further divided into depletion-mode and enhancement-mode types, depending on whether the channel is turned on or off with zero gate-to-source voltage. For enhancement mode, the channel is off at zero bias, and a gate potential can "enhance" the conduction. For the depletion mode, the channel is on at zero bias, and a gate potential (of the opposite polarity) can "deplete" the channel, reducing conduction. For either mode, a more positive gate voltage corresponds to a higher current for n-channel devices and a lower current for p-channel devices. Nearly all JFETs are depletion-mode because the diode junctions would forward bias and conduct if they were enhancement-mode devices, while most IGFETs are enhancement-mode types.
The metal–oxide–semiconductor field-effect transistor (MOSFET, MOS-FET, or MOS FET), also known as the metal–oxide–silicon transistor (MOS transistor, or MOS),[6] is a type of field-effect transistor that is fabricated by the controlled oxidation of a semiconductor, typically silicon. It has an insulated gate, whose voltage determines the conductivity of the device. This ability to change conductivity with the amount of applied voltage can be used for amplifying or switching electronic signals. The MOSFET is by far the most common transistor, and the basic building block of most modern electronics.[72] The MOSFET accounts for 99.9% of all transistors in the world.[85]
Bipolar transistors are so named because they conduct by using both majority and minority carriers. The bipolar junction transistor, the first type of transistor to be mass-produced, is a combination of two junction diodes and is formed of either a thin layer of p-type semiconductor sandwiched between two n-type semiconductors (an n–p–n transistor), or a thin layer of n-type semiconductor sandwiched between two p-type semiconductors (a p–n–p transistor). This construction produces two p–n junctions: a base-emitter junction and a base-collector junction, separated by a thin region of semiconductor known as the base region. (Two junction diodes wired together without sharing an intervening semiconducting region will not make a transistor).
BJTs have three terminals, corresponding to the three layers of semiconductor—an emitter, a base, and a collector. They are useful in amplifiers because the currents at the emitter and collector are controllable by a relatively small base current.[86] In an n–p–n transistor operating in the active region, the emitter-base junction is forward biased (electrons and holes recombine at the junction), and the base-collector junction is reverse biased (electrons and holes are formed at, and move away from the junction), and electrons are injected into the base region. Because the base is narrow, most of these electrons will diffuse into the reverse-biased base-collector junction and be swept into the collector; perhaps one-hundredth of the electrons will recombine in the base, which is the dominant mechanism in the base current. As well, as the base is lightly doped (in comparison to the emitter and collector regions), recombination rates are low, permitting more carriers to diffuse across the base region. By controlling the number of electrons that can leave the base, the number of electrons entering the collector can be controlled.[86] Collector current is approximately β (common-emitter current gain) times the base current. It is typically greater than 100 for small-signal transistors but can be smaller in transistors designed for high-power applications.
Unlike the field-effect transistor (see below), the BJT is a low-input-impedance device. Also, as the base-emitter voltage (VBE) is increased the base-emitter current and hence the collector-emitter current (ICE) increase exponentially according to the Shockley diode model and the Ebers-Moll model. Because of this exponential relationship, the BJT has a higher transconductance than the FET.
Bipolar transistors can be made to conduct by exposure to light because the absorption of photons in the base region generates a photocurrent that acts as a base current; the collector current is approximately β times the photocurrent. Devices designed for this purpose have a transparent window in the package and are called phototransistors.
The MOSFET is by far the most widely used transistor for both digital circuits as well as analog circuits,[87] accounting for 99.9% of all transistors in the world.[85] The bipolar junction transistor (BJT) was previously the most commonly used transistor during the 1950s to 1960s. Even after MOSFETs became widely available in the 1970s, the BJT remained the transistor of choice for many analog circuits such as amplifiers because of their greater linearity, up until MOSFET devices (such as power MOSFETs, LDMOS and RF CMOS) replaced them for most power electronic applications in the 1980s. In integrated circuits, the desirable properties of MOSFETs allowed them to capture nearly all market share for digital circuits in the 1970s. Discrete MOSFETs (typically power MOSFETs) can be applied in transistor applications, including analog circuits, voltage regulators, amplifiers, power transmitters, and motor drivers.
Three major identification standards are used for designating transistor devices. In each, the alphanumeric prefix provides clues to the type of the device.
The JEDEC part numbering scheme evolved in the 1960s in the United States. The JEDEC EIA-370 transistor device numbers usually start with 2N, indicating a three-terminal device. Dual-gate field-effect transistors are four-terminal devices, and begin with 3N. The prefix is followed by a two-, three- or four-digit number with no significance as to device properties, although early devices with low numbers tend to be germanium devices. For example, 2N3055 is a silicon n–p–n power transistor, 2N1301 is a p–n–p germanium switching transistor. A letter suffix, such as "A", is sometimes used to indicate a newer variant, but rarely gain groupings.
In Japan, the JIS semiconductor designation (|JIS-C-7012), labels transistor devices starting with 2S,[95] e.g., 2SD965, but sometimes the "2S" prefix is not marked on the package–a 2SD965 might only be marked D965 and a 2SC1815 might be listed by a supplier as simply C1815. This series sometimes has suffixes, such as R, O, BL, standing for red, orange, blue, etc., to denote variants, such as tighter hFE (gain) groupings.
The European Electronic Component Manufacturers Association (EECA) uses a numbering scheme that was inherited from Pro Electron when it merged with EECA in 1983. This scheme begins with two letters: the first gives the semiconductor type (A for germanium, B for silicon, and C for materials like GaAs); the second letter denotes the intended use (A for diode, C for general-purpose transistor, etc.). A three-digit sequence number (or one letter and two digits, for industrial types) follows. With early devices this indicated the case type. Suffixes may be used, with a letter (e.g. "C" often means high hFE, such as in: BC549C[96]) or other codes may follow to show gain (e.g. BC327-25) or voltage rating (e.g. BUK854-800A[97]). The more common prefixes are:
Manufacturers of devices may have their proprietary numbering system, for example CK722. Since devices are second-sourced, a manufacturer's prefix (like "MPF" in MPF102, which originally would denote a Motorola FET) now is an unreliable indicator of who made the device. Some proprietary naming schemes adopt parts of other naming schemes, for example, a PN2222A is a (possibly Fairchild Semiconductor) 2N2222A in a plastic case (but a PN108 is a plastic version of a BC108, not a 2N108, while the PN100 is unrelated to other xx100 devices).
Military part numbers sometimes are assigned their codes, such as the British Military CV Naming System.
Manufacturers buying large numbers of similar parts may have them supplied with "house numbers", identifying a particular purchasing specification and not necessarily a device with a standardized registered number. For example, an HP part 1854,0053 is a (JEDEC) 2N2218 transistor[98][99] which is also assigned the CV number: CV7763[100]
With so many independent naming schemes, and the abbreviation of part numbers when printed on the devices, ambiguity sometimes occurs. For example, two different devices may be marked "J176" (one the J176 low-power JFET, the other the higher-powered MOSFET 2SJ176).
As older "through-hole" transistors are given surface-mount packaged counterparts, they tend to be assigned many different part numbers because manufacturers have their systems to cope with the variety in pinout arrangements and options for dual or matched n–p–n + p–n–p devices in one pack. So even when the original device (such as a 2N3904) may have been assigned by a standards authority, and well known by engineers over the years, the new versions are far from standardized in their naming.
The first BJTs were made from germanium (Ge). Silicon (Si) types currently predominate but certain advanced microwave and high-performance versions now employ the compound semiconductor material gallium arsenide (GaAs) and the semiconductor alloy silicon-germanium (SiGe). Single element semiconductor material (Ge and Si) is described as elemental.
Rough parameters for the most common semiconductor materials used to make transistors are given in the adjacent table. These parameters will vary with an increase in temperature, electric field, impurity level, strain, and sundry other factors.
The junction forward voltage is the voltage applied to the emitter-base junction of a BJT to make the base conduct a specified current. The current increases exponentially as the junction forward voltage is increased. The values given in the table are typical for a current of 1 mA (the same values apply to semiconductor diodes). The lower the junction forward voltage the better, as this means that less power is required to "drive" the transistor. The junction forward voltage for a given current decreases with an increase in temperature. For a typical silicon junction, the change is −2.1 mV/°C.[101] In some circuits special compensating elements (sensistors) must be used to compensate for such changes.
The density of mobile carriers in the channel of a MOSFET is a function of the electric field forming the channel and of various other phenomena such as the impurity level in the channel. Some impurities, called dopants, are introduced deliberately in making a MOSFET, to control the MOSFET electrical behavior.
The electron mobility and hole mobility columns show the average speed that electrons and holes diffuse through the semiconductor material with an electric field of 1 volt per meter applied across the material. In general, the higher the electron mobility the faster the transistor can operate. The table indicates that Ge is a better material than Si in this respect. However, Ge has four major shortcomings compared to silicon and gallium arsenide:
Because the electron mobility is higher than the hole mobility for all semiconductor materials, a given bipolar n–p–n transistor tends to be swifter than an equivalent p–n–p transistor. GaAs has the highest electron mobility of the three semiconductors. It is for this reason that GaAs is used in high-frequency applications. A relatively recent[when?] FET development, the high-electron-mobility transistor (HEMT), has a heterostructure (junction between different semiconductor materials) of aluminium gallium arsenide (AlGaAs)-gallium arsenide (GaAs) which has twice the electron mobility of a GaAs-metal barrier junction. Because of their high speed and low noise, HEMTs are used in satellite receivers working at frequencies around 12 GHz. HEMTs based on gallium nitride and aluminum gallium nitride (AlGaN/GaN HEMTs) provide still higher electron mobility and are being developed for various applications.
Maximum junction temperature values represent a cross-section taken from various manufacturers' datasheets. This temperature should not be exceeded or the transistor may be damaged.
Al–Si junction refers to the high-speed (aluminum-silicon) metal–semiconductor barrier diode, commonly known as a Schottky diode. This is included in the table because some silicon power IGFETs have a parasitic reverse Schottky diode formed between the source and drain as part of the fabrication process. This diode can be a nuisance, but sometimes it is used in the circuit.
Discrete transistors can be individually packaged transistors or unpackaged transistor chips.
Transistors come in many different semiconductor packages (see image). The two main categories are through-hole (or leaded), and surface-mount, also known as surface-mount device (SMD). The ball grid array (BGA) is the latest surface-mount package. It has solder "balls" on the underside in place of leads. Because they are smaller and have shorter interconnections, SMDs have better high-frequency characteristics but lower power ratings.
Transistor packages are made of glass, metal, ceramic, or plastic. The package often dictates the power rating and frequency characteristics. Power transistors have larger packages that can be clamped to heat sinks for enhanced cooling. Additionally, most power transistors have the collector or drain physically connected to the metal enclosure. At the other extreme, some surface-mount microwave transistors are as small as grains of sand.
Often a given transistor type is available in several packages. Transistor packages are mainly standardized, but the assignment of a transistor's functions to the terminals is not: other transistor types can assign other functions to the package's terminals. Even for the same transistor type the terminal assignment can vary (normally indicated by a suffix letter to the part number, q.e. BC212L and BC212K).
Nowadays most transistors come in a wide range of SMT packages, in comparison, the list of available through-hole packages is relatively small, here is a shortlist of the most common through-hole transistors packages in alphabetical order:
ATV, E-line, MRT, HRT, SC-43, SC-72, TO-3, TO-18, TO-39, TO-92, TO-126, TO220, TO247, TO251, TO262, ZTX851.
Unpackaged transistor chips (die) may be assembled into hybrid devices.[102]  The IBM SLT module of the 1960s is one example of such a hybrid circuit module using glass passivated transistor (and diode) die.  Other packaging techniques for discrete transistors as chips include direct chip attach (DCA) and chip-on-board (COB).[102]
Researchers have made several kinds of flexible transistors, including organic field-effect transistors.[103][104][105] Flexible transistors are useful in some kinds of flexible displays and other flexible electronics.



An internal combustion engine (ICE or IC engine) is a pressure engine in which the combustion of a fuel occurs with an oxidizer (usually air) in a combustion chamber that is an integral part of the working fluid flow circuit. In an internal combustion engine, the expansion of the high-temperature and high-pressure gases produced by combustion applies direct force to some component of the engine. The force is typically applied to pistons (piston engine), turbine blades (gas turbine), a rotor (Wankel engine), or a nozzle (jet engine). This force moves the component over a distance, transforming chemical energy into kinetic energy which is used to propel, move or power whatever the engine is attached to. 
The first commercially successful internal combustion engine was created by Étienne Lenoir around 1860,[1] and the first modern internal combustion engine, known as the Otto engine, was created in 1876 by Nicolaus Otto. The term internal combustion engine usually refers to an engine in which combustion is intermittent, such as the more familiar two-stroke and four-stroke piston engines, along with variants, such as the six-stroke piston engine and the Wankel rotary engine. A second class of internal combustion engines use continuous combustion: gas turbines, jet engines and most rocket engines, each of which are internal combustion engines on the same principle as previously described.[1][2] Firearms are also a form of internal combustion engine,[2] though of a type so specialized that they are commonly treated as a separate category, along with weaponry such as mortars and anti-aircraft cannons. In contrast, in external combustion engines, such as steam or Stirling engines, energy is delivered to a working fluid not consisting of, mixed with, or contaminated by combustion products. Working fluids for external combustion engines include air, hot water, pressurized water or even boiler-heated liquid sodium.
While there are many stationary applications, most ICEs are used in mobile applications and are the primary power supply for vehicles such as cars, aircraft and boats. ICEs are typically powered by hydrocarbon-based fuels like natural gas, gasoline, diesel fuel, or ethanol. Renewable fuels like biodiesel are used in compression ignition (CI) engines and bioethanol or ETBE (ethyl tert-butyl ether) produced from bioethanol in spark ignition (SI) engines. As early as 1900 the inventor of the diesel engine, Rudolf Diesel, was using peanut oil to run his engines.[3] Renewable fuels are commonly blended with fossil fuels.  Hydrogen, which is rarely used, can be obtained from either fossil fuels or renewable energy.
Various scientists and engineers contributed to the development of internal combustion engines. In 1791, John Barber developed the gas turbine. In 1794 Thomas Mead patented a gas engine. Also in 1794, Robert Street patented an internal combustion engine, which was also the first to use liquid fuel, and built an engine around that time. In 1798, John Stevens built the first American internal combustion engine. In 1807, French engineers Nicéphore Niépce (who went on to invent photography) and Claude Niépce ran a prototype internal combustion engine, using controlled dust explosions, the Pyréolophore, which was granted a patent by Napoleon Bonaparte. This engine powered a boat on the Saône river in France.[4][5] In the same year, Swiss engineer François Isaac de Rivaz invented a hydrogen-based internal combustion engine and powered the engine by electric spark. In 1808, De Rivaz fitted his invention to a primitive working vehicle – "the world's first internal combustion powered automobile".[6] In 1823, Samuel Brown patented the first internal combustion engine to be applied industrially.
In 1854 in the UK, the Italian inventors Eugenio Barsanti and Felice Matteucci obtained the certification: "Obtaining Motive Power by the Explosion of Gases". In 1857 the Great Seal Patent Office conceded them patent No.1655 for the invention of an "Improved Apparatus for Obtaining Motive Power from Gases".[7][8][9][10] Barsanti and Matteucci obtained other patents for the same invention in France, Belgium and Piedmont between 1857 and 1859.[11][12] In 1860, Belgian engineer Jean Joseph Etienne Lenoir produced a gas-fired internal combustion engine.[13] In 1864, Nicolaus Otto patented the first atmospheric gas engine. In 1872, American George Brayton invented the first commercial liquid-fueled internal combustion engine. In 1876, Nicolaus Otto began working with Gottlieb Daimler and Wilhelm Maybach, patented the compressed charge, four-cycle engine. In 1879, Karl Benz patented a reliable two-stroke gasoline engine. Later, in 1886, Benz began the first commercial production of motor vehicles with an internal combustion engine, in which a three-wheeled, four-cycle engine and chassis formed a single unit.[14] In 1892, Rudolf Diesel developed the first compressed charge, compression ignition engine. In 1926, Robert Goddard launched the first liquid-fueled rocket. In 1939, the Heinkel He 178 became the world's first jet aircraft.
At one time, the word engine (via Old French, from Latin ingenium, "ability") meant any piece of machinery—a sense that persists in expressions such as siege engine. A "motor" (from Latin motor, "mover") is any machine that produces mechanical power. Traditionally, electric motors are not referred to as "engines"; however, combustion engines are often referred to as "motors". (An electric engine refers to a locomotive operated by electricity.)
In boating, an internal combustion engine that is installed in the hull is referred to as an engine, but the engines that sit on the transom are referred to as motors.[15]
Reciprocating piston engines are by far the most common power source for land and water vehicles, including automobiles, motorcycles, ships and to a lesser extent, locomotives (some are electrical but most use diesel engines[16][17]). Rotary engines of the Wankel design are used in some automobiles, aircraft and motorcycles.  These are collectively known as internal-combustion-engine vehicles (ICEV).[18]
Where high power-to-weight ratios are required, internal combustion engines appear in the form of combustion turbines, or sometimes Wankel engines. Powered aircraft typically use an ICE which may be a reciprocating engine. Airplanes can instead use jet engines and helicopters can instead employ turboshafts; both of which are types of turbines. In addition to providing propulsion, airliners may employ a separate ICE as an auxiliary power unit. Wankel engines are fitted to many unmanned aerial vehicles.
ICEs drive large electric generators that power electrical grids. They are found in the form of combustion turbines with a typical electrical output in the range of some 100 MW. Combined cycle power plants use the high temperature exhaust to boil and superheat water steam to run a steam turbine. Thus, the efficiency is higher because more energy is extracted from the fuel than what could be extracted by the combustion engine alone.
Combined cycle power plants achieve efficiencies in the range of 50–60%. In a smaller scale, stationary engines like gas engines or diesel generators are used for backup or for providing electrical power to areas not connected to an electric grid.
Small engines (usually 2‐stroke gasoline/petrol engines) are a common power source for lawnmowers, string trimmers, chain saws, leafblowers, pressure washers, snowmobiles, jet skis, outboard motors, mopeds, and motorcycles.
There are several possible ways to classify internal combustion engines.
By number of strokes:
By type of ignition:
By mechanical/thermodynamic cycle (these cycles are infrequently used but are commonly found in hybrid vehicles, along with other vehicles manufactured for fuel efficiency[20]):
The base of a reciprocating internal combustion engine is the engine block, which is typically made of cast iron (due to its good wear resistance and low cost)[22] or aluminum. In the latter case, the cylinder liners are made of cast iron or steel,[23] or a coating such as nikasil or alusil. The engine block contains the cylinders. In engines with more than one cylinder they are usually arranged either in 1 row (straight engine) or 2 rows (boxer engine or V engine); 3 rows are occasionally used (W engine) in contemporary engines, and other engine configurations are possible and have been used. Single cylinder engines (or thumpers) are common for motorcycles and other small engines found in light machinery. On the outer side of the cylinder, passages that contain cooling fluid are cast into the engine block whereas, in some heavy duty engines, the passages are the types of removable cylinder sleeves which can be replaceable.[22] Water-cooled engines contain passages in the engine block where cooling fluid circulates (the water jacket). Some small engines are air-cooled, and instead of having a water jacket the cylinder block has fins protruding away from it to cool the engine by directly transferring heat to the air. The cylinder walls are usually finished by honing to obtain a cross hatch, which is able to retain more oil. A too rough surface would quickly harm the engine by excessive wear on the piston.
The pistons are short cylindrical parts which seal one end of the cylinder from the high pressure of the compressed air and combustion products and slide continuously within it while the engine is in operation. In smaller engines, the pistons are made of aluminum; while in larger applications, they are typically made of cast iron.[22] The top wall of the piston is termed its crown and is typically flat or concave. Some two-stroke engines use pistons with a deflector head. Pistons are open at the bottom and hollow except for an integral reinforcement structure (the piston web). When an engine is working, the gas pressure in the combustion chamber exerts a force on the piston crown which is transferred through its web to a gudgeon pin. Each piston has rings fitted around its circumference that mostly prevent the gases from leaking into the crankcase or the oil into the combustion chamber.[24] A ventilation system drives the small amount of gas that escapes past the pistons during normal operation (the blow-by gases) out of the crankcase so that it does not accumulate contaminating the oil and creating corrosion.[22] In two-stroke gasoline engines the crankcase is part of the air–fuel path and due to the continuous flow of it, two-stroke engines do not need a separate crankcase ventilation system.
The cylinder head is attached to the engine block by numerous bolts or studs. It has several functions. The cylinder head seals the cylinders on the side opposite to the pistons; it contains short ducts (the ports) for intake and exhaust and the associated intake valves that open to let the cylinder be filled with fresh air and exhaust valves that open to allow the combustion gases to escape. However, 2-stroke crankcase scavenged engines connect the gas ports directly to the cylinder wall without poppet valves; the piston controls their opening and occlusion instead. The cylinder head also holds the spark plug in the case of spark ignition engines and the injector for engines that use direct injection. All CI (compression ignition) engines use fuel injection, usually direct injection but some engines instead use indirect injection. SI (spark ignition) engines can use a carburetor or fuel injection as port injection or direct injection. Most SI engines have a single spark plug per cylinder but some have 2. A head gasket prevents the gas from leaking between the cylinder head and the engine block. The opening and closing of the valves is controlled by one or several camshafts and springs—or in some engines—a desmodromic mechanism that uses no springs. The camshaft may press directly the stem of the valve or may act upon a rocker arm, again, either directly or through a pushrod.
The crankcase is sealed at the bottom with a sump that collects the falling oil during normal operation to be cycled again. The cavity created between the cylinder block and the sump houses a crankshaft that converts the reciprocating motion of the pistons to rotational motion. The crankshaft is held in place relative to the engine block by main bearings, which allow it to rotate. Bulkheads in the crankcase form a half of every main bearing; the other half is a detachable cap. In some cases a single main bearing deck is used rather than several smaller caps. A connecting rod is connected to offset sections of the crankshaft (the crankpins) in one end and to the piston in the other end through the gudgeon pin and thus transfers the force and translates the reciprocating motion of the pistons to the circular motion of the crankshaft. The end of the connecting rod attached to the gudgeon pin is called its small end, and the other end, where it is connected to the crankshaft, the big end. The big end has a detachable half to allow assembly around the crankshaft. It is kept together to the connecting rod by removable bolts.
The cylinder head has an intake manifold and an exhaust manifold attached to the corresponding ports. The intake manifold connects to the air filter directly, or to a carburetor when one is present, which is then connected to the air filter. It distributes the air incoming from these devices to the individual cylinders. The exhaust manifold is the first component in the exhaust system. It collects the exhaust gases from the cylinders and drives it to the following component in the path. The exhaust system of an ICE may also include a catalytic converter and muffler. The final section in the path of the exhaust gases is the tailpipe.
The top dead center (TDC) of a piston is the position where it is nearest to the valves; bottom dead center (BDC) is the opposite position where it is furthest from them. A stroke is the movement of a piston from TDC to BDC or vice versa, together with the associated process. While an engine is in operation, the crankshaft rotates continuously at a nearly constant speed. In a 4-stroke ICE, each piston experiences 2 strokes per crankshaft revolution in the following order. Starting the description at TDC, these are:[25][26]
The defining characteristic of this kind of engine is that each piston completes a cycle every crankshaft revolution. The 4 processes of intake, compression, power and exhaust take place in only 2 strokes so that it is not possible to dedicate a stroke exclusively for each of them. Starting at TDC the cycle consists of:
While a 4-stroke engine uses the piston as a positive displacement pump to accomplish scavenging taking 2 of the 4 strokes, a 2-stroke engine uses the last part of the power stroke and the first part of the compression stroke for combined intake and exhaust. The work required to displace the charge and exhaust gases comes from either the crankcase or a separate blower. For scavenging, expulsion of burned gas and entry of fresh mix, two main approaches are described: Loop scavenging, and Uniflow scavenging. SAE news published in the 2010s that 'Loop Scavenging' is better under any circumstance than Uniflow Scavenging.[19]
Some SI engines are crankcase scavenged and do not use poppet valves. Instead, the crankcase and the part of the cylinder below the piston is used as a pump. The intake port is connected to the crankcase through a reed valve or a rotary disk valve driven by the engine. For each cylinder, a transfer port connects in one end to the crankcase and in the other end to the cylinder wall. The exhaust port is connected directly to the cylinder wall. The transfer and exhaust port are opened and closed by the piston. The reed valve opens when the crankcase pressure is slightly below intake pressure, to let it be filled with a new charge; this happens when the piston is moving upwards. When the piston is moving downwards the pressure in the crankcase increases and the reed valve closes promptly, then the charge in the crankcase is compressed. When the piston is moving downwards, it also uncovers the exhaust port and the transfer port and the higher pressure of the charge in the crankcase makes it enter the cylinder through the transfer port, blowing the exhaust gases. Lubrication is accomplished by adding 2-stroke oil to the fuel in small ratios. Petroil refers to the mix of gasoline with the aforesaid oil. This kind of 2-stroke engine has a lower efficiency than comparable 4-strokes engines and releases more polluting exhaust gases for the following conditions:
The main advantage of 2-stroke engines of this type is mechanical simplicity and a higher power-to-weight ratio than their 4-stroke counterparts. Despite having twice as many power strokes per cycle, less than twice the power of a comparable 4-stroke engine is attainable in practice.
In the US, 2-stroke engines were banned for road vehicles due to the pollution. Off-road only motorcycles are still often 2-stroke but are rarely road legal. However, many thousands of 2-stroke lawn maintenance engines are in use.[citation needed]
Using a separate blower avoids many of the shortcomings of crankcase scavenging, at the expense of increased complexity which means a higher cost and an increase in maintenance requirement. An engine of this type uses ports or valves for intake and valves for exhaust, except opposed piston engines, which may also use ports for exhaust. The blower is usually of the Roots-type but other types have been used too. This design is commonplace in CI engines, and has been occasionally used in SI engines.
CI engines that use a blower typically use uniflow scavenging. In this design the cylinder wall contains several intake ports placed uniformly spaced along the circumference just above the position that the piston crown reaches when at BDC. An exhaust valve or several like that of 4-stroke engines is used. The final part of the intake manifold is an air sleeve that feeds the intake ports. The intake ports are placed at a horizontal angle to the cylinder wall (I.e: they are in plane of the piston crown) to give a swirl to the incoming charge to improve combustion. The largest reciprocating IC are low speed CI engines of this type; they are used for marine propulsion (see marine diesel engine) or electric power generation and achieve the highest thermal efficiencies among internal combustion engines of any kind. Some Diesel-electric locomotive engines operate on the 2-stroke cycle. The most powerful of them have a brake power of around 4.5 MW or 6,000 HP. The EMD SD90MAC class of locomotives are an example of such. The comparable class GE AC6000CW whose prime mover has almost the same brake power uses a 4-stroke engine.
An example of this type of engine is the Wärtsilä-Sulzer RT-flex96-C turbocharged 2-stroke Diesel, used in large container ships. It is the most efficient and powerful reciprocating internal combustion engine in the world with a thermal efficiency over 50%.[27][28][29] For comparison, the most efficient small four-stroke engines are around 43% thermally-efficient (SAE 900648);[citation needed] size is an advantage for efficiency due to the increase in the ratio of volume to surface area.
See the external links for an in-cylinder combustion video in a 2-stroke, optically accessible motorcycle engine.
Dugald Clerk developed the first two-cycle engine in 1879. It used a separate cylinder which functioned as a pump in order to transfer the fuel mixture to the cylinder.[19]
In 1899 John Day simplified Clerk's design into the type of 2 cycle engine that is very widely used today.[30]
Day cycle engines are crankcase scavenged and port timed. The crankcase and the part of the cylinder below the exhaust port is used as a pump. The operation of the Day cycle engine begins when the crankshaft is turned so that the piston moves from BDC upward (toward the head) creating a vacuum in the crankcase/cylinder area. The carburetor then feeds the fuel mixture into the crankcase through a reed valve or a rotary disk valve (driven by the engine). There are cast in ducts from the crankcase to the port in the cylinder to provide for intake and another from the exhaust port to the exhaust pipe.  The height of the port in relationship to the length of the cylinder is called the "port timing".
On the first upstroke of the engine there would be no fuel inducted into the cylinder as the crankcase was empty. On the downstroke, the piston now compresses the fuel mix, which has lubricated the piston in the cylinder and the bearings due to the fuel mix having oil added to it. As the piston moves downward it first uncovers the exhaust, but on the first stroke there is no burnt fuel to exhaust. As the piston moves downward further, it uncovers the intake port which has a duct that runs to the crankcase. Since the fuel mix in the crankcase is under pressure, the mix moves through the duct and into the cylinder.
Because there is no obstruction in the cylinder of the fuel to move directly out of the exhaust port prior to the piston rising far enough to close the port, early engines used a high domed piston to slow down the flow of fuel. Later the fuel was "resonated" back into the cylinder using an expansion chamber design. When the piston rose close to TDC, a spark ignited the fuel. As the piston is driven downward with power, it first uncovers the exhaust port where the burned fuel is expelled under high pressure and then the intake port where the process has been completed and will keep repeating.
Later engines used a type of porting devised by the Deutz company to improve performance. It was called the Schnurle Reverse Flow system. DKW licensed this design for all their motorcycles. Their DKW RT 125 was one of the first motor vehicles to achieve over 100 mpg as a result.[31]
Internal combustion engines require ignition of the mixture, either by spark ignition (SI) or compression ignition (CI). Before the invention of reliable electrical methods, hot tube and flame methods were used. Experimental engines with laser ignition have been built.[32]
The spark-ignition engine was a refinement of the early engines which used Hot Tube ignition. When Bosch developed the magneto it became the primary system for producing electricity to energize a spark plug.[33] Many small engines still use magneto ignition. Small engines are started by hand cranking using a recoil starter or hand crank. Prior to Charles F. Kettering of Delco's development of the automotive starter all gasoline engined automobiles used a hand crank.[34]
Larger engines typically power their starting motors and ignition systems using the electrical energy stored in a lead–acid battery.  The battery's charged state is maintained by an automotive alternator or (previously) a generator which uses engine power to create electrical energy storage.
The battery supplies electrical power for starting when the engine has a starting motor system, and supplies electrical power when the engine is off. The battery also supplies electrical power during rare run conditions where the alternator cannot maintain more than 13.8 volts (for a common 12V automotive electrical system). As alternator voltage falls below 13.8 volts, the lead-acid storage battery increasingly picks up electrical load. During virtually all running conditions, including normal idle conditions, the alternator supplies primary electrical power.
Some systems disable alternator field (rotor) power during wide-open throttle conditions. Disabling the field reduces alternator pulley mechanical loading to nearly zero, maximizing crankshaft power. In this case, the battery supplies all primary electrical power.
Gasoline engines take in a mixture of air and gasoline and compress it by the movement of the piston from bottom dead center to top dead center when the fuel is at maximum compression. The reduction in the size of the swept area of the cylinder and taking into account the volume of the combustion chamber is described by a ratio. Early engines had compression ratios of 6 to 1. As compression ratios were increased, the efficiency of the engine increased as well.
With early induction and ignition systems the compression ratios had to be kept low. With advances in fuel technology and combustion management, high-performance engines can run reliably at 12:1 ratio. With low octane fuel, a problem would occur as the compression ratio increased as the fuel was igniting due to the rise in temperature that resulted. Charles Kettering developed a lead additive which allowed higher compression ratios, which was progressively abandoned for automotive use from the 1970s onward, partly due to lead poisoning concerns.
The fuel mixture is ignited at different progressions of the piston in the cylinder. At low rpm, the spark is timed to occur close to the piston achieving top dead center. In order to produce more power, as rpm rises the spark is advanced sooner during piston movement. The spark occurs while the fuel is still being compressed progressively more as rpm rises.[35]
The necessary high voltage, typically 10,000 volts, is supplied by an induction coil or transformer. The induction coil is a fly-back system, using interruption of electrical primary system current through some type of synchronized interrupter. The interrupter can be either contact points or a power transistor. The problem with this type of ignition is that as RPM increases the availability of electrical energy decreases. This is especially a problem, since the amount of energy needed to ignite a more dense fuel mixture is higher. The result was often a high RPM misfire.
Capacitor discharge ignition was developed. It produces a rising voltage that is sent to the spark plug. CD system voltages can reach 60,000 volts.[36] CD ignitions use step-up transformers. The step-up transformer uses energy stored in a capacitance to generate electric spark. With either system, a mechanical or electrical control system provides a carefully timed high-voltage to the proper cylinder. This spark, via the spark plug, ignites the air-fuel mixture in the engine's cylinders.
While gasoline internal combustion engines are much easier to start in cold weather than diesel engines, they can still have cold weather starting problems under extreme conditions. For years, the solution was to park the car in heated areas. In some parts of the world, the oil was actually drained and heated overnight and returned to the engine for cold starts. In the early 1950s, the gasoline Gasifier unit was developed, where, on cold weather starts, raw gasoline was diverted to the unit where part of the fuel was burned causing the other part to become a hot vapor sent directly to the intake valve manifold.  This unit was quite popular until electric engine block heaters became standard on gasoline engines sold in cold climates.[37]
For ignition, diesel, PPC and HCCI engines rely solely on the high temperature and pressure created by the engine in its compression process. The compression level that occurs is usually twice or more than a gasoline engine. Diesel engines take in air only, and shortly before peak compression, spray a small quantity of diesel fuel into the cylinder via a fuel injector that allows the fuel to instantly ignite. HCCI type engines take in both air and fuel, but continue to rely on an unaided auto-combustion process, due to higher pressures and temperature. This is also why diesel and HCCI engines are more susceptible to cold-starting issues, although they run just as well in cold weather once started. Light duty diesel engines with indirect injection in automobiles and light trucks employ glowplugs (or other pre-heating: see Cummins ISB#6BT) that pre-heat the combustion chamber just before starting to reduce no-start conditions in cold weather. Most diesels also have a battery and charging system; nevertheless, this system is secondary and is added by manufacturers as a luxury for the ease of starting, turning fuel on and off (which can also be done via a switch or mechanical apparatus), and for running auxiliary electrical components and accessories. Most new engines rely on electrical and electronic engine control units (ECU) that also adjust the combustion process to increase efficiency and reduce emissions.
Surfaces in contact and relative motion to other surfaces require lubrication to reduce wear, noise and increase efficiency by reducing the power wasting in overcoming friction, or to make the mechanism work at all. Also, the lubricant used can reduce excess heat and provide additional cooling to components. At the very least, an engine requires lubrication in the following parts:
In 2-stroke crankcase scavenged engines, the interior of the crankcase, and therefore the crankshaft, connecting rod and bottom of the pistons are sprayed by the 2-stroke oil in the air-fuel-oil mixture which is then burned along with the fuel. The valve train may be contained in a compartment flooded with lubricant so that no oil pump is required.
In a splash lubrication system no oil pump is used. Instead the crankshaft dips into the oil in the sump and due to its high speed, it splashes the crankshaft, connecting rods and bottom of the pistons. The connecting rod big end caps may have an attached scoop to enhance this effect. The valve train may also be sealed in a flooded compartment, or open to the crankshaft in a way that it receives splashed oil and allows it to drain back to the sump. Splash lubrication is common for small 4-stroke engines.
In a forced (also called pressurized) lubrication system, lubrication is accomplished in a closed-loop which carries motor oil to the surfaces serviced by the system and then returns the oil to a reservoir. The auxiliary equipment of an engine is typically not serviced by this loop; for instance, an alternator may use ball bearings sealed with their own lubricant. The reservoir for the oil is usually the sump, and when this is the case, it is called a wet sump system. When there is a different oil reservoir the crankcase still catches it, but it is continuously drained by a dedicated pump; this is called a dry sump system.
On its bottom, the sump contains an oil intake covered by a mesh filter which is connected to an oil pump then to an oil filter outside the crankcase. From there it is diverted to the crankshaft main bearings and valve train. The crankcase contains at least one oil gallery (a conduit inside a crankcase wall) to which oil is introduced from the oil filter. The main bearings contain a groove through all or half its circumference; the oil enters these grooves from channels connected to the oil gallery. The crankshaft has drillings that take oil from these grooves and deliver it to the big end bearings. All big end bearings are lubricated this way. A single main bearing may provide oil for 0, 1 or 2 big end bearings. A similar system may be used to lubricate the piston, its gudgeon pin and the small end of its connecting rod; in this system, the connecting rod big end has a groove around the crankshaft and a drilling connected to the groove which distributes oil from there to the bottom of the piston and from then to the cylinder.
Other systems are also used to lubricate the cylinder and piston. The connecting rod may have a nozzle to throw an oil jet to the cylinder and bottom of the piston. That nozzle is in movement relative to the cylinder it lubricates, but always pointed towards it or the corresponding piston.
Typically forced lubrication systems have a lubricant flow higher than what is required to lubricate satisfactorily, in order to assist with cooling. Specifically, the lubricant system helps to move heat from the hot engine parts to the cooling liquid (in water-cooled engines) or fins (in air-cooled engines) which then transfer it to the environment. The lubricant must be designed to be chemically stable and maintain suitable viscosities within the temperature range it encounters in the engine.
Common cylinder configurations include the straight or inline configuration, the more compact V configuration, and the wider but smoother flat or boxer configuration. Aircraft engines can also adopt a radial configuration, which allows more effective cooling. More unusual configurations such as the H, U, X, and W have also been used.
Multiple cylinder engines have their valve train and crankshaft configured so that pistons are at different parts of their cycle. It is desirable to have the pistons' cycles uniformly spaced (this is called even firing) especially in forced induction engines; this reduces torque pulsations[38] and makes inline engines with more than 3 cylinders statically balanced in its primary forces. However, some engine configurations require odd firing to achieve better balance than what is possible with even firing. For instance, a 4-stroke I2 engine has better balance when the angle between the crankpins is 180° because the pistons move in opposite directions and inertial forces partially cancel, but this gives an odd firing pattern where one cylinder fires 180° of crankshaft rotation after the other, then no cylinder fires for 540°. With an even firing pattern, the pistons would move in unison and the associated forces would add.
Multiple crankshaft configurations do not necessarily need a cylinder head at all because they can instead have a piston at each end of the cylinder called an opposed piston design. Because fuel inlets and outlets are positioned at opposed ends of the cylinder, one can achieve uniflow scavenging, which, as in the four-stroke engine is efficient over a wide range of engine speeds. Thermal efficiency is improved because of a lack of cylinder heads. This design was used in the Junkers Jumo 205 diesel aircraft engine, using two crankshafts at either end of a single bank of cylinders, and most remarkably in the Napier Deltic diesel engines. These used three crankshafts to serve three banks of double-ended cylinders arranged in an equilateral triangle with the crankshafts at the corners. It was also used in single-bank locomotive engines, and is still used in marine propulsion engines and marine auxiliary generators.
Most truck and automotive diesel engines use a cycle reminiscent of a four-stroke cycle, but with temperature increase by compression causing ignition, rather than needing a separate ignition system. This variation is called the diesel cycle. In the diesel cycle, diesel fuel is injected directly into the cylinder so that combustion occurs at constant pressure, as the piston moves.
The Otto cycle is the most common cycle for most cars' internal combustion engines that use gasoline as a fuel. It consists of the same major steps as described for the four-stroke engine: Intake, compression, ignition, expansion and exhaust.
In 1879, Nicolaus Otto manufactured and sold a double expansion engine (the double and triple expansion principles had ample usage in steam engines), with two small cylinders at both sides of a low-pressure larger cylinder, where a second expansion of exhaust stroke gas took place; the owner returned it, alleging poor performance. In 1906, the concept was incorporated in a car built by EHV (Eisenhuth Horseless Vehicle Company);[39] and in the 21st century Ilmor designed and successfully tested a 5-stroke double expansion internal combustion engine, with high power output and low SFC (Specific Fuel Consumption).[40]
The six-stroke engine was invented in 1883. Four kinds of six-stroke engines use a regular piston in a regular cylinder (Griffin six-stroke, Bajulaz six-stroke, Velozeta six-stroke and Crower six-stroke), firing every three crankshaft revolutions. These systems capture the waste heat of the four-stroke Otto cycle with an injection of air or water.
The Beare Head and "piston charger" engines operate as opposed-piston engines, two pistons in a single cylinder, firing every two revolutions rather than every four like a four-stroke engine.
The very first internal combustion engines did not compress the mixture. The first part of the piston downstroke drew in a fuel-air mixture, then the inlet valve closed and, in the remainder of the down-stroke, the fuel-air mixture fired. The exhaust valve opened for the piston upstroke. These attempts at imitating the principle of a steam engine were very inefficient.
There are a number of variations of these cycles, most notably the Atkinson and Miller cycles.
Split-cycle engines separate the four strokes of intake, compression, combustion and exhaust into two separate but paired cylinders. The first cylinder is used for intake and compression. The compressed air is then transferred through a crossover passage from the compression cylinder into the second cylinder, where combustion and exhaust occur. A split-cycle engine is really an air compressor on one side with a combustion chamber on the other.
Previous split-cycle engines have had two major problems—poor breathing (volumetric efficiency) and low thermal efficiency. However, new designs are being introduced that seek to address these problems.
The Scuderi Engine addresses the breathing problem by reducing the clearance between the piston and the cylinder head through various turbocharging techniques. The Scuderi design requires the use of outwardly opening valves that enable the piston to move very close to the cylinder head without the interference of the valves. Scuderi addresses the low thermal efficiency via firing after top dead center (ATDC).
Firing ATDC can be accomplished by using high-pressure air in the transfer passage to create sonic flow and high turbulence in the power cylinder.
Jet engines use a number of rows of fan blades to compress air which then enters a combustor where it is mixed with fuel (typically JP fuel) and then ignited. The burning of the fuel raises the temperature of the air which is then exhausted out of the engine creating thrust. A modern turbofan engine can operate at as high as 48% efficiency.[41]
There are six sections to a turbofan engine:
A gas turbine compresses air and uses it to turn a turbine. It is essentially a jet engine which directs its output to a shaft.  There are three stages to a turbine: 1) air is drawn through a compressor where the temperature rises due to compression, 2) fuel is added in the combuster, and 3) hot air is exhausted through turbine blades which rotate a shaft connected to the compressor.
A gas turbine is a rotary machine similar in principle to a steam turbine and it consists of three main components: a compressor, a combustion chamber, and a turbine. The temperature of the air, after being compressed in the compressor, is increased by burning fuel in it. The heated air and the products of combustion expand in a turbine, producing work output. About 2⁄3 of the work drives the compressor: the rest (about 1⁄3) is available as useful work output.[43]
Gas turbines are among the most efficient internal combustion engines. The General Electric 7HA and 9HA turbine combined cycle electrical plants are rated at over 61% efficiency.[44]
A gas turbine is a rotary machine somewhat similar in principle to a steam turbine. It consists of three main components: compressor, combustion chamber, and turbine. The air is compressed by the compressor where a temperature rise occurs. The temperature of the compressed air is further increased by combustion of injected fuel in the combustion chamber which expands the air. This energy rotates the turbine which powers the compressor via a mechanical coupling. The hot gases are then exhausted to provide thrust.
Gas turbine cycle engines employ a continuous combustion system where compression, combustion, and expansion occur simultaneously at different places in the engine—giving continuous power. Notably, the combustion takes place at constant pressure, rather than with the Otto cycle, constant volume.
The Wankel engine (rotary engine) does not have piston strokes. It operates with the same separation of phases as the four-stroke engine with the phases taking place in separate locations in the engine. In thermodynamic terms it follows the Otto engine cycle, so may be thought of as a "four-phase" engine. While it is true that three power strokes typically occur per rotor revolution, due to the 3:1 revolution ratio of the rotor to the eccentric shaft, only one power stroke per shaft revolution actually occurs. The drive (eccentric) shaft rotates once during every power stroke instead of twice (crankshaft), as in the Otto cycle, giving it a greater power-to-weight ratio than piston engines. This type of engine was most notably used in the Mazda RX-8, the earlier RX-7, and other vehicle models. The engine is also used in unmanned aerial vehicles, where the small size and weight and the high power-to-weight ratio are advantageous.
Forced induction is the process of delivering compressed air to the intake of an internal combustion engine. A forced induction engine uses a gas compressor to increase the pressure, temperature and density of the air. An engine without forced induction is considered a naturally aspirated engine.
Forced induction is used in the automotive and aviation industry to increase engine power and efficiency. It particularly helps aviation engines, as they need to operate at high altitude.
Forced induction is achieved by a supercharger, where the compressor is directly powered from the engine shaft or, in the turbocharger, from a turbine powered by the engine exhaust.
All internal combustion engines depend on combustion of a chemical fuel, typically with oxygen from the air (though it is possible to inject nitrous oxide to do more of the same thing and gain a power boost). The combustion process typically results in the production of a great quantity of thermal energy, as well as the production of steam and carbon dioxide and other chemicals at very high temperature; the temperature reached is determined by the chemical make up of the fuel and oxidizers (see stoichiometry), as well as by the compression and other factors.
The most common modern fuels are made up of hydrocarbons and are derived mostly from fossil fuels (petroleum). Fossil fuels include diesel fuel, gasoline and petroleum gas, and the rarer use of propane. Except for the fuel delivery components, most internal combustion engines that are designed for gasoline use can run on natural gas or liquefied petroleum gases without major modifications. Large diesels can run with air mixed with gases  and a pilot diesel fuel ignition injection. Liquid and gaseous biofuels, such as ethanol and biodiesel (a form of diesel fuel that is produced from crops that yield triglycerides such as soybean oil), can also be used. Engines with appropriate modifications can also run on hydrogen gas, wood gas, or charcoal gas, as well as from so-called producer gas made from other convenient biomass. Experiments have also been conducted using powdered solid fuels, such as the magnesium injection cycle.
Presently, fuels used include:
Even fluidized metal powders and explosives have seen some use. Engines that use gases for fuel are called gas engines and those that use liquid hydrocarbons are called oil engines; however, gasoline engines are also often colloquially referred to as "gas engines" ("petrol engines" outside North America).
The main limitations on fuels are that it must be easily transportable through the fuel system to the combustion chamber, and that the fuel releases sufficient energy in the form of heat upon combustion to make practical use of the engine.
Diesel engines are generally heavier, noisier, and more powerful at lower speeds than gasoline engines. They are also more fuel-efficient in most circumstances and are used in heavy road vehicles, some automobiles (increasingly so for their increased fuel efficiency over gasoline engines), ships, railway locomotives, and light aircraft. Gasoline engines are used in most other road vehicles including most cars, motorcycles, and mopeds. Note that in Europe, sophisticated diesel-engined cars have taken over about 45% of the market since the 1990s. There are also engines that run on hydrogen, methanol, ethanol, liquefied petroleum gas (LPG), biodiesel, paraffin and tractor vaporizing oil (TVO).
Hydrogen could eventually replace conventional fossil fuels in traditional internal combustion engines. Alternatively fuel cell technology may come to deliver its promise and the use of the internal combustion engines could even be phased out.
Although there are multiple ways of producing free hydrogen, those methods require converting combustible molecules into hydrogen or consuming electric energy. Unless that electricity is produced from a renewable source—and is not required for other purposes—hydrogen does not solve any energy crisis. In many situations the disadvantage of hydrogen, relative to carbon fuels, is its storage. Liquid hydrogen has extremely low density (14 times lower than water) and requires extensive insulation—whilst gaseous hydrogen requires heavy tankage. Even when liquefied, hydrogen has a higher specific energy but the volumetric energetic storage is still roughly five times lower than gasoline. However, the energy density of hydrogen is considerably higher than that of electric batteries, making it a serious contender as an energy carrier to replace fossil fuels. The 'Hydrogen on Demand' process (see direct borohydride fuel cell) creates hydrogen as needed, but has other issues, such as the high price of the sodium borohydride that is the raw material.
Since air is plentiful at the surface of the earth, the oxidizer is typically atmospheric oxygen, which has the advantage of not being stored within the vehicle. This increases the power-to-weight and power-to-volume ratios. Other materials are used for special purposes, often to increase power output or to allow operation under water or in space.
Cooling is required to remove excessive heat—high temperature can cause engine failure, usually from wear (due to high-temperature-induced failure of lubrication), cracking or warping. Two most common forms of engine cooling are air-cooled and water-cooled. Most modern automotive engines are both water and air-cooled, as the water/liquid-coolant is carried to air-cooled fins and/or fans, whereas larger engines may be singularly water-cooled as they are stationary and have a constant supply of water through water-mains or fresh-water, while most power tool engines and other small engines are air-cooled. Some engines (air or water-cooled) also have an oil cooler. In some engines, especially for turbine engine blade cooling and liquid rocket engine cooling, fuel is used as a coolant, as it is simultaneously preheated before injecting it into a combustion chamber.
Internal combustion engines must have their cycles started. In reciprocating engines this is accomplished by turning the crankshaft (Wankel Rotor Shaft) which induces the cycles of intake, compression, combustion, and exhaust.  The first engines were started with a turn of their flywheels, while the first vehicle (the Daimler Reitwagen) was started with a hand crank.  All ICE engined automobiles were started with hand cranks until Charles Kettering developed the electric starter for automobiles.[47] This method is now the most widely used, even among non-automobiles.
As diesel engines have become larger and their mechanisms heavier, air starters have come into use.[48] This is due to the lack of torque in electric starters. Air starters work by pumping compressed air into the cylinders of an engine to start it turning.
Two-wheeled vehicles may have their engines started in one of four ways:
There are also starters where a spring is compressed by a crank motion and then used to start an engine.
Some small engines use a pull-rope mechanism called "recoil starting", as the rope rewinds itself after it has been pulled out to start the engine. This method is commonly used in  pushed lawn mowers and other settings where only a small amount of torque is needed to turn an engine over.
Turbine engines are frequently started by an electric motor or by compressed air.
Engine types vary greatly in a number of different ways:
Once ignited and burnt, the combustion products—hot gases—have more available thermal energy than the original compressed fuel-air mixture (which had higher chemical energy). This available energy is manifested as a higher temperature and pressure that can be converted into kinetic energy by the engine. In a reciprocating engine, the high-pressure gases inside the cylinders drive the engine's pistons.
Once the available energy has been removed, the remaining hot gases are vented (often by opening a valve or exposing the exhaust outlet) and this allows the piston to return to its previous position (top dead center, or TDC). The piston can then proceed to the next phase of its cycle, which varies between engines. Any thermal energy that is not translated into work is normally considered a waste product and is removed from the engine either by an air or liquid cooling system.
Internal combustion engines are considered heat engines (since the release of chemical energy in combustion has the same effect as heat transfer into the engine) and as such their theoretical efficiency can be approximated by idealized thermodynamic cycles. The thermal efficiency of a theoretical cycle cannot exceed that of the Carnot cycle, whose efficiency is determined by the difference between the lower and upper operating temperatures of the engine. The upper operating temperature of an engine is limited by two main factors; the thermal operating limits of the materials, and the auto-ignition resistance of the fuel. All metals and alloys have a thermal operating limit, and there is significant research into ceramic materials that can be made with greater thermal stability and desirable structural properties. Higher thermal stability allows for a greater temperature difference between the lower (ambient) and upper operating temperatures, hence greater thermodynamic efficiency. Also, as the cylinder temperature rises, the fuel becomes more prone to auto-ignition. This is caused when the cylinder temperature nears the flash point of the charge. At this point, ignition can spontaneously occur before the spark plug fires, causing excessive cylinder pressures. Auto-ignition can be mitigated by using fuels with high auto-ignition resistance (octane rating), however it still puts an upper bound on the allowable peak cylinder temperature.
The thermodynamic limits assume that the engine is operating under ideal conditions: a frictionless world, ideal gases, perfect insulators, and operation for infinite time. Real world applications introduce complexities that reduce efficiency. For example, a real engine runs best at a specific load, termed its power band. The engine in a car cruising on a highway is usually operating significantly below its ideal load, because it is designed for the higher loads required for rapid acceleration.[citation needed] In addition, factors such as wind resistance reduce overall system efficiency.  Vehicle fuel economy is measured in miles per gallon or in liters per 100 kilometers. The volume of hydrocarbon assumes a standard energy content.
Even when aided with turbochargers and stock efficiency aids, most engines retain an average efficiency of about 18–20%.[49] However, the latest technologies in Formula One engines have seen a boost in thermal efficiency past 50%.[50]
There are many inventions aimed at increasing the efficiency of IC engines. In general, practical engines are always compromised by trade-offs between different properties such as efficiency, weight, power, heat, response, exhaust emissions, or noise. Sometimes economy also plays a role in not only the cost of manufacturing the engine itself, but also manufacturing and distributing the fuel. Increasing the engine's efficiency brings better fuel economy but only if the fuel cost per energy content is the same.
For stationary and shaft engines including propeller engines, fuel consumption is measured by calculating the brake specific fuel consumption, which measures the mass flow rate of fuel consumption divided by the power produced.
For internal combustion engines in the form of jet engines, the power output varies drastically with airspeed and a less variable measure is used: thrust specific fuel consumption (TSFC), which is the mass of propellant needed to generate impulses that is measured in either pound force-hour or the grams of propellant needed to generate an impulse that measures one kilonewton-second.
For rockets, TSFC can be used, but typically other equivalent measures are traditionally used, such as specific impulse and effective exhaust velocity.
Internal combustion engines such as reciprocating internal combustion engines produce air pollution emissions, due to incomplete combustion of carbonaceous fuel. The main derivatives of the process are carbon dioxide CO2, water and some soot—also called particulate matter (PM). The effects of inhaling particulate matter have been studied in humans and animals and include asthma, lung cancer, cardiovascular issues, and premature death. There are, however, some additional products of the combustion process that include nitrogen oxides and sulfur and some uncombusted hydrocarbons, depending on the operating conditions and the fuel-air ratio.
Carbon dioxide emissions from internal combustion engines (particularly ones using fossil fuels such as gasoline and diesel) contribute to human-induced climate change.  Increasing the engine's fuel efficiency can reduce, but not eliminate, the amount of CO2 emissions as carbon-based fuel combustion produces CO2.  Since removing CO2 from engine exhaust is impractical, there is increasing interest in alternatives.  Sustainable fuels such as biofuels, synfuels, and electric motors powered by batteries are examples.
Not all of the fuel is completely consumed by the combustion process. A small amount of fuel is present after combustion, and some of it reacts to form oxygenates, such as formaldehyde or acetaldehyde, or hydrocarbons not originally present in the input fuel mixture. Incomplete combustion usually results from insufficient oxygen to achieve the perfect stoichiometric ratio. The flame is "quenched" by the relatively cool cylinder walls, leaving behind unreacted fuel that is expelled with the exhaust. When running at lower speeds, quenching is commonly observed in diesel (compression ignition) engines that run on natural gas. Quenching reduces efficiency and increases knocking, sometimes causing the engine to stall. Incomplete combustion also leads to the production of carbon monoxide (CO). Further chemicals released are benzene and 1,3-butadiene that are also hazardous air pollutants.
Increasing the amount of air in the engine reduces emissions of incomplete combustion products, but also promotes reaction between oxygen and nitrogen in the air to produce nitrogen oxides (NOx). NOx is hazardous to both plant and animal health, and leads to the production of ozone (O3). Ozone is not emitted directly; rather, it is a secondary air pollutant, produced in the atmosphere by the reaction of NOx and volatile organic compounds in the presence of sunlight. Ground-level ozone is harmful to human health and the environment. Though the same chemical substance, ground-level ozone should not be confused with stratospheric ozone, or the ozone layer, which protects the earth from harmful ultraviolet rays.
Carbon fuels containing sulfur produce sulfur monoxides (SO) and sulfur dioxide (SO2) contributing to acid rain.
In the United States, nitrogen oxides, PM, carbon monoxide, sulfur dioxide, and ozone, are regulated as criteria air pollutants under the Clean Air Act to levels where human health and welfare are protected. Other pollutants, such as benzene and 1,3-butadiene, are regulated as hazardous air pollutants whose emissions must be lowered as much as possible depending on technological and practical considerations.
NOx, carbon monoxide and other pollutants are frequently controlled via exhaust gas recirculation which returns some of the exhaust back into the engine intake.  Catalytic converters are used to convert exhaust chemicals to CO2 (a greenhouse gas), H2O (water vapour, also a greenhouse gas) and N2 (nitrogen).
The emission standards used by many countries have special requirements for non-road engines which are used by equipment and vehicles that are not operated on the public roadways. The standards are separated from the road vehicles.[51]
Significant contributions to noise pollution are made by internal combustion engines. Automobile and truck traffic operating on highways and street systems produce noise, as do aircraft flights due to jet noise, particularly supersonic-capable aircraft. Rocket engines create the most intense noise.
Internal combustion engines continue to consume fuel and emit pollutants while idling. Idling is reduced by stop-start systems.
A good way to estimate the mass of carbon dioxide that is released when one litre of diesel fuel (or gasoline) is combusted can be found as follows:[52]
As a good approximation the chemical formula of diesel is CnH2n. Note that in reality diesel is a mixture of different molecules. As carbon has a molar mass of 12 g/mol and hydrogen (atomic) has a molar mass of about 1 g/mol, the fraction by weight of carbon in diesel is roughly 12⁄14.
The reaction of diesel combustion is given by:
2CnH2n + 3nO2 ⇌ 2nCO2 + 2nH2O
Carbon dioxide has a molar mass of 44 g/mol as it consists of 2 atoms of oxygen (16 g/mol) and 1 atom of carbon (12 g/mol). So 12 g of carbon yields 44 g of carbon dioxide.
Diesel has a density of 0.838 kg per litre.
Putting everything together the mass of carbon dioxide that is produced by burning 1 litre of diesel can be calculated as:

The figure obtained with this estimation is close to the values found in the literature.
For gasoline, with a density of 0.75 kg/L and a ratio of carbon to hydrogen atoms of about 6 to 14, the estimated value of carbon dioxide emission from burning 1 litre of gasoline is:

The term parasitic loss is often applied to devices that take energy from the engine in order to enhance the engine's ability to create more energy or convert energy to motion. In the internal combustion engine, almost every mechanical component, including the drivetrain, causes parasitic loss and could thus be characterized as a parasitic load.
Bearings, oil pumps, piston rings, valve springs, flywheels, transmissions, driveshafts, and differentials all act as parasitic loads that rob the system of power. These parasitic loads can be divided into two categories: those inherent to the working of the engine and those drivetrain losses incurred in the systems that transfer power from the engine to the road (such as the transmission, driveshaft, differentials and axles).
For example, the former category (engine parasitic loads) includes the oil pump used to lubricate the engine, which is a necessary parasite that consumes power from the engine (its host). Another example of an engine parasitic load is a supercharger, which derives its power from the engine and creates more power for the engine. The power that the supercharger consumes is parasitic loss and is usually expressed in kilowatt or horsepower. While the power that the supercharger consumes in comparison to what it generates is small, it is still measurable or calculable. One of the desirable features of a turbocharger over a supercharger is the lower parasitic loss of the former.[53]
Drivetrain parasitic losses include both steady state and dynamic loads. Steady state loads occur at constant speeds and may originate in discrete components such as the torque converter, the transmission oil pump, and/or clutch drag, and in seal/bearing drag, churning of lubricant and gear windage/friction found throughout the system. Dynamic loads occur under acceleration and are caused by inertia of rotating components and/or increased friction.[54]
While rules of thumb such as a 15% power loss from drivetrain parasitic loads have been commonly repeated, the actual loss of energy due to parasitic loads varies between systems. It can be influenced by powertrain design, lubricant type and temperature and many other factors.[54][55] In automobiles, drivetrain loss can be quantified by measuring the difference between power measured by an engine dynamometer and a chassis dynamometer. However, this method is primarily useful for measuring steady state loads and may not accurately reflect losses due to dynamic loads.[54] More advanced methods can be used in a laboratory setting, such as measuring in-cylinder pressure measurements, flow rate and temperature at certain points, and testing of individual parts or sub-assemblies to determine friction and pumping losses.[56]
For example, in a dynamometer test by Hot Rod magazine, a Ford Mustang equipped with a modified 357ci small-block Ford V8 engine and an automatic transmission had a measured drivetrain power loss averaging 33%. In the same test, a Buick equipped with a modified 455ci V8 engine and a 4-speed manual transmission was measured to have an average drivetrain power loss of 21%.[57]
Laboratory testing of a heavy-duty diesel engine determined that 1.3% of the fuel energy input was lost to parasitic loads of engine accessories such as water and oil pumps.[56]
Automotive engineers and tuners commonly make design choices that reduce parasitic loads in order to improve efficiency and power output.  These may involve the choice of major engine components or systems, such as the use of dry sump lubrication system over a wet sump system. Alternately, this can be effected through substitution of minor components available as aftermarket modifications, such as exchanging a directly engine-driven fan for one equipped with a fan clutch or an electric fan.[57] Another modification to reduce parasitic loss, usually seen in track-only cars, is the replacement of an engine-driven water pump for an electrical water pump.[58] The reduction in parasitic loss from these changes may be due to reduced friction or many other variables that cause the design to be more efficient.[citation needed]

Gunpowder, also commonly known as black powder to distinguish it from modern smokeless powder, is the earliest known chemical explosive. It consists of a mixture of sulfur, carbon (in the form of charcoal) and potassium nitrate (saltpeter). The sulfur and carbon act as fuels while the saltpeter is an oxidizer.[1][2] Gunpowder has been widely used as a propellant in firearms, artillery, rocketry, and pyrotechnics, including use as a blasting agent for explosives in quarrying, mining, building pipelines and road building.
Gunpowder is classified as a low explosive because of its relatively slow decomposition rate and consequently low brisance. Low explosives deflagrate (i.e., burn at subsonic speeds), whereas high explosives detonate, producing a supersonic shockwave. Ignition of gunpowder packed behind a projectile generates enough pressure to force the shot from the muzzle at high speed, but usually not enough force to rupture the gun barrel. It thus makes a good propellant but is less suitable for shattering rock or fortifications with its low-yield explosive power. Nonetheless, it was widely used to fill fused artillery shells (and used in mining and civil engineering projects) until the second half of the 19th century, when the first high explosives were put into use.
Gunpowder is one of the Four Great Inventions of China.[3] Originally developed by the Taoists for medicinal purposes, it was first used for warfare around 904 AD.[4] Its use in weapons has declined due to smokeless powder replacing it, and it is no longer used for industrial purposes due to its relative inefficiency compared to newer alternatives such as dynamite and ammonium nitrate/fuel oil.[5]
Gunpowder is a low explosive: it does not detonate, but rather deflagrates (burns quickly). This is an advantage in a propellant device, where one does not desire a shock that would shatter the gun and potentially harm the operator; however, it is a drawback when an explosion is desired. In that case, the propellant (and most importantly, gases produced by its burning) must be confined. Since it contains its own oxidizer and additionally burns faster under pressure, its combustion is capable of bursting containers such as a shell, grenade, or improvised "pipe bomb" or "pressure cooker" casings to form shrapnel
In quarrying, high explosives are generally preferred for shattering rock. However, because of its low brisance, gunpowder causes fewer fractures and results in more usable stone compared to other explosives, making it useful for blasting slate, which is fragile,[6] or monumental stone such as granite and marble. Gunpowder is well suited for blank rounds, signal flares, burst charges, and rescue-line launches. It is also used in fireworks for lifting shells, in rockets as fuel, and in certain special effects.
Combustion converts less than half the mass of gunpowder to gas; most of it turns into particulate matter. Some of it is ejected, wasting propelling power, fouling the air, and generally being a nuisance (giving away a soldier's position, generating fog that hinders vision, etc.). Some of it ends up as a thick layer of soot inside the barrel, where it also is a nuisance for subsequent shots, and a cause of jamming an automatic weapon. Moreover, this residue is hygroscopic, and with the addition of moisture absorbed from the air forms a corrosive substance. The soot contains potassium oxide or sodium oxide that turns into potassium hydroxide, or sodium hydroxide, which corrodes wrought iron or steel gun barrels. Gunpowder arms therefore require thorough and regular cleaning to remove the residue.[7]
Gunpowder loads can be used in modern firearms as long as they are not gas-operated.[note 1] The most compatible modern guns are smoothbore-barreled shotguns that are long-recoil operated. Combined with chrome-plated essential parts such as barrels and bores, these elements heavily reduce fouling and corrosion; the combination of these factors makes the shotgun easier to clean.[14]
The first confirmed reference to what can be considered gunpowder in China occurred in the 9th century AD during the Tang dynasty, first in a formula contained in the Taishang Shengzu Jindan Mijue (太上聖祖金丹秘訣) in 808, and then about 50 years later in a Taoist text known as the Zhenyuan miaodao yaolüe (真元妙道要略).[15] The Taishang Shengzu Jindan Mijue mentions a formula composed of six parts sulfur to six parts saltpeter to one part birthwort herb.[15] According to the Zhenyuan miaodao yaolüe, "Some have heated together sulfur, realgar and saltpeter with honey; smoke and flames result, so that their hands and faces have been burnt, and even the whole house where they were working burned down."[16] Based on these Taoist texts, the invention of gunpowder by Chinese alchemists was likely an accidental byproduct from experiments seeking to create the elixir of life.[17] This experimental medicine origin is reflected in its Chinese name huoyao (Chinese: 火药/火藥; pinyin: huǒ yào /xuo yɑʊ/), which means "fire medicine".[18] Saltpeter was known to the Chinese by the mid-1st century AD and was primarily produced in the provinces of Sichuan, Shanxi, and Shandong.[19] There is strong evidence of the use of saltpeter and sulfur in various medicinal combinations.[20] A Chinese alchemical text dated 492 noted saltpeter burnt with a purple flame, providing a practical and reliable means of distinguishing it from other inorganic salts, thus enabling alchemists to evaluate and compare purification techniques; the earliest Latin accounts of saltpeter purification are dated after 1200.[21]
The earliest chemical formula for gunpowder appeared in the 11th century Song dynasty text, Wujing Zongyao (Complete Essentials from the Military Classics), written by Zeng Gongliang between 1040 and 1044.[22] The Wujing Zongyao provides encyclopedia references to a variety of mixtures that included petrochemicals—as well as garlic and honey. A slow match for flame-throwing mechanisms using the siphon principle and for fireworks and rockets is mentioned. The mixture formulas in this book contain at most 50% saltpeter; not enough to create an explosion, they produce an incendiary instead.[22] The Essentials was written by a Song dynasty court bureaucrat and there is little evidence that it had any immediate impact on warfare; there is no mention of its use in the chronicles of the wars against the Tanguts in the 11th century, and China was otherwise mostly at peace during this century. However, it had already been used for fire arrows since at least the 10th century. Its first recorded military application dates its use to the year 904 in the form of incendiary projectiles.[4] In the following centuries various gunpowder weapons such as bombs, fire lances, and the gun appeared in China.[23][24] Explosive weapons such as bombs have been discovered in a shipwreck off the shore of Japan dated from 1281, during the Mongol invasions of Japan.[25]
By 1083 the Song court was producing hundreds of thousands of fire arrows for their garrisons.[26] Bombs and the first proto-guns, known as "fire lances", became prominent during the 12th century and were used by the Song during the Jin-Song Wars. Fire lances were first recorded to have been used at the Siege of De'an in 1132 by Song forces against the Jin.[27] In the early 13th century the Jin used iron-casing bombs.[28] Projectiles were added to fire lances, and re-usable fire lance barrels were developed, first out of hardened paper, and then metal. By 1257 some fire lances were firing wads of bullets.[29][30] In the late 13th-century metal fire lances became 'eruptors', proto-cannons firing co-viative projectiles (mixed with the propellant, rather than seated over it with a wad), and by 1287 at the latest, had become true guns, the hand cannon.[31]
According to Iqtidar Alam Khan, it was invading Mongols who introduced gunpowder to the Islamic world.[32] The Muslims acquired knowledge of gunpowder sometime between 1240 and 1280, by which point the Syrian Hasan al-Rammah had written recipes, instructions for the purification of saltpeter, and descriptions of gunpowder incendiaries. It is implied by al-Rammah's usage of "terms that suggested he derived his knowledge from Chinese sources" and his references to saltpeter as "Chinese snow" (Arabic: ثلج الصين thalj al-ṣīn), fireworks as "Chinese flowers", and rockets as "Chinese arrows" that knowledge of gunpowder arrived from China.[33] However, because al-Rammah attributes his material to "his father and forefathers", al-Hassan argues that gunpowder became prevalent in Syria and Egypt by "the end of the twelfth century or the beginning of the thirteenth".[34] In Persia saltpeter was known as "Chinese salt" (Persian: نمک چینی) namak-i chīnī)[35][36] or "salt from Chinese salt marshes" (نمک شوره چینی namak-i shūra-yi chīnī).[37][38]
Hasan al-Rammah included 107 gunpowder recipes in his text al-Furusiyyah wa al-Manasib al-Harbiyya (The Book of Military Horsemanship and Ingenious War Devices), 22 of which are for rockets. If one takes the median of 17 of these 22 compositions for rockets (75% nitrates, 9.06% sulfur, and 15.94% charcoal), it is nearly identical to the modern reported ideal recipe of 75% potassium nitrate, 10% sulfur, and 15% charcoal.[34] The text also mentions fuses, incendiary bombs, naphtha pots, fire lances, and an illustration and description of the earliest torpedo. The torpedo was called the "egg which moves itself and burns".[39] Two iron sheets were fastened together and tightened using felt. The flattened pear-shaped vessel was filled with gunpowder, metal filings, "good mixtures", two rods, and a large rocket for propulsion. Judging by the illustration, it was evidently supposed to glide across the water.[39][40][41] Fire lances were used in battles between the Muslims and Mongols in 1299 and 1303.[42]
Al-Hassan claims that in the Battle of Ain Jalut of 1260, the Mamluks used against the Mongols, in "the first cannon in history", formula with near-identical ideal composition ratios for explosive gunpowder.[34] Other historians urge caution regarding claims of Islamic firearms use in the 1204–1324 period as late medieval Arabic texts used the same word for gunpowder, naft, that they used for an earlier incendiary, naphtha.[43][44]
The earliest surviving documentary evidence for cannons in the Islamic world is from an Arabic manuscript dated to the early 14th century.[45][46] The author's name is uncertain but may have been Shams al-Din Muhammad, who died in 1350.[39] Dating from around 1320-1350, the illustrations show gunpowder weapons such as gunpowder arrows, bombs, fire tubes, and fire lances or proto-guns.[41] The manuscript describes a type of gunpowder weapon called a midfa which uses gunpowder to shoot projectiles out of a tube at the end of a stock.[47] Some consider this to be a cannon while others do not. The problem with identifying cannons in early 14th century Arabic texts is the term midfa, which appears from 1342 to 1352 but cannot be proven to be true hand-guns or bombards. Contemporary accounts of a metal-barrel cannon in the Islamic world do not occur until 1365.[48] Needham believes that in its original form the term midfa refers to the tube or cylinder of a naphtha projector (flamethrower), then after the invention of gunpowder it meant the tube of fire lances, and eventually it applied to the cylinder of hand-gun and cannon.[49]
According to Paul E. J. Hammer, the Mamluks certainly used cannons by 1342.[50] According to J. Lavin, cannons were used by Moors at the siege of Algeciras in 1343. A metal cannon firing an iron ball was described by Shihab al-Din Abu al-Abbas al-Qalqashandi between 1365 and 1376.[48]
The musket appeared in the Ottoman Empire by 1465.[51] In 1598, Chinese writer Zhao Shizhen described Turkish muskets as being superior to European muskets.[52] The Chinese military book Wu Pei Chih (1621) later described Turkish muskets that used a rack-and-pinion mechanism, which was not known to have been used in European or Chinese firearms at the time.[53]
The state-controlled manufacture of gunpowder by the Ottoman Empire through early supply chains to obtain nitre, sulfur and high-quality charcoal from oaks in Anatolia contributed significantly to its expansion between the 15th and 18th century. It was not until later in the 19th century when the syndicalist production of Turkish gunpowder was greatly reduced, which coincided with the decline of its military might.[54]
The earliest Western accounts of gunpowder appears in texts written by English philosopher Roger Bacon in 1267 called Opus Majus and Opus Tertium.[55] The oldest written recipes in continental Europe were recorded under the name Marcus Graecus or Mark the Greek between 1280 and 1300 in the Liber Ignium, or Book of Fires.[56]
Some sources mention possible gunpowder weapons being deployed by the Mongols against European forces at the Battle of Mohi in 1241.[57][58][59] Professor Kenneth Warren Chase credits the Mongols for introducing into Europe gunpowder and its associated weaponry.[60] However, there is no clear route of transmission,[61] and while the Mongols are often pointed to as the likeliest vector, Timothy May points out that "there is no concrete evidence that the Mongols used gunpowder weapons on a regular basis outside of China."[62] However, Timothy May also points out "However... the Mongols used the gunpowder weapon in their wars against the Jin, the Song and in their invasions of Japan."[62]
Records show that, in England, gunpowder was being made in 1346 at the Tower of London; a powder house existed at the Tower in 1461; and in 1515 three King's gunpowder makers worked there.[63] Gunpowder was also being made or stored at other royal castles, such as Portchester.[64] The English Civil War (1642–1645) led to an expansion of the gunpowder industry, with the repeal of the Royal Patent in August 1641.[63]
In late 14th century Europe, gunpowder was improved by corning, the practice of drying it into small clumps to improve combustion and consistency.[65] During this time, European manufacturers also began regularly purifying saltpeter, using wood ashes containing potassium carbonate to precipitate calcium from their dung liquor, and using ox blood, alum, and slices of turnip to clarify the solution.[65]
During the Renaissance, two European schools of pyrotechnic thought emerged, one in Italy and the other at Nuremberg, Germany.[66] In Italy, Vannoccio Biringuccio, born in 1480, was a member of the guild Fraternita di Santa Barbara but broke with the tradition of secrecy by setting down everything he knew in a book titled De la pirotechnia, written in vernacular.  It was published posthumously in 1540, with 9 editions over 138 years, and also reprinted by MIT Press in 1966.[65]
By the mid-17th century fireworks were used for entertainment on an unprecedented scale in Europe, being popular even at resorts and public gardens.[67] With the publication of Deutliche Anweisung zur Feuerwerkerey (1748), methods for creating fireworks were sufficiently well-known and well-described that "Firework making has become an exact science."[68] In 1774 Louis XVI ascended to the throne of France at age 20.  After he discovered that France was not self-sufficient in gunpowder, a Gunpowder Administration was established; to head it, the lawyer Antoine Lavoisier was appointed. Although from a bourgeois family, after his degree in law Lavoisier became wealthy from a company set up to collect taxes for the Crown; this allowed him to pursue experimental natural science as a hobby.[69]
Without access to cheap saltpeter (controlled by the British), for hundreds of years France had relied on saltpetremen with royal warrants, the droit de fouille or "right to dig", to seize nitrous-containing soil and demolish walls of barnyards, without compensation to the owners.[70] This caused farmers, the wealthy, or entire villages to bribe the petermen and the associated bureaucracy to leave their buildings alone and the saltpeter uncollected.  Lavoisier instituted a crash program to increase saltpeter production, revised (and later eliminated) the droit de fouille, researched best refining and powder manufacturing methods, instituted management and record-keeping, and established pricing that encouraged private investment in works.  Although saltpeter from new Prussian-style putrefaction works had not been produced yet (the process taking about 18 months), in only a year France had gunpowder to export. A chief beneficiary of this surplus was the American Revolution.  By careful testing and adjusting the proportions and grinding time, powder from mills such as at Essonne outside Paris became the best in the world by 1788, and inexpensive.[70][71]
Two British physicists, Andrew Noble and Frederick Abel, worked to improve the properties of gunpowder during the late 19th century. This formed the basis for the Noble-Abel gas equation for internal ballistics.[72]
The introduction of smokeless powder in the late 19th century led to a contraction of the gunpowder industry. After the end of World War I, the majority of the British gunpowder manufacturers merged into a single company, "Explosives Trades limited"; and a number of sites were closed down, including those in Ireland. This company became Nobel Industries Limited; and in 1926 became a founding member of Imperial Chemical Industries. The Home Office removed gunpowder from its list of Permitted Explosives; and shortly afterwards, on 31 December 1931, the former Curtis & Harvey's Glynneath gunpowder factory at Pontneddfechan, in Wales, closed down, and it was demolished by fire in 1932.[73] The last remaining gunpowder mill at the Royal Gunpowder Factory, Waltham Abbey was damaged by a German parachute mine in 1941 and it never reopened.[63] This was followed by the closure of the gunpowder section at the Royal Ordnance Factory, ROF Chorley, the section was closed and demolished at the end of World War II; and ICI Nobel's Roslin gunpowder factory, which closed in 1954.[63][74] This left ICI Nobel's Ardeer site in Scotland, which included a gunpowder factory, as the only factory in Great Britain producing gunpowder. The gunpowder area of the Ardeer site closed in October 1976.[63]
Gunpowder and gunpowder weapons were transmitted to India through the Mongol invasions of India.[75][76] The Mongols were defeated by Alauddin Khalji of the Delhi Sultanate, and some of the Mongol soldiers remained in northern India after their conversion to Islam.[76] It was written in the Tarikh-i Firishta (1606–1607) that Nasiruddin Mahmud the ruler of the Delhi Sultanate presented the envoy of the Mongol ruler Hulegu Khan with a dazzling pyrotechnics display upon his arrival in Delhi in 1258. Nasiruddin Mahmud tried to express his strength as a ruler and tried to ward off any Mongol attempt similar to the Siege of Baghdad (1258).[77] Firearms known as top-o-tufak also existed in many Muslim kingdoms in India by as early as 1366.[77] From then on the employment of gunpowder warfare in India was prevalent, with events such as the "Siege of Belgaum" in 1473 by Sultan Muhammad Shah Bahmani.[78]
The shipwrecked Ottoman Admiral Seydi Ali Reis is known to have introduced the earliest type of matchlock weapons, which the Ottomans used against the Portuguese during the Siege of Diu (1531). After that, a diverse variety of firearms, large guns in particular, became visible in Tanjore, Dacca, Bijapur, and Murshidabad.[79] Guns made of bronze were recovered from Calicut (1504)- the former capital of the Zamorins[80]
The Mughal emperor Akbar mass-produced matchlocks for the Mughal Army. Akbar is personally known to have shot a leading Rajput commander during the Siege of Chittorgarh.[81] The Mughals began to use bamboo rockets (mainly for signalling) and employ sappers: special units that undermined heavy stone fortifications to plant gunpowder charges.
The Mughal Emperor Shah Jahan is known to have introduced much more advanced matchlocks, their designs were a combination of Ottoman and Mughal designs. Shah Jahan also countered the British and other Europeans in his province of Gujarāt, which supplied Europe saltpeter for use in gunpowder warfare during the 17th century.[82] Bengal and Mālwa participated in saltpeter production.[82] The Dutch, French, Portuguese, and English used Chhapra as a center of saltpeter refining.[82]
Ever since the founding of the Sultanate of Mysore by Hyder Ali, French military officers were employed to train the Mysore Army. Hyder Ali and his son Tipu Sultan were the first to introduce modern cannons and muskets, their army was also the first in India to have official uniforms. During the Second Anglo-Mysore War Hyder Ali and his son Tipu Sultan unleashed the Mysorean rockets at their British opponents effectively defeating them on various occasions. The Mysorean rockets inspired the development of the Congreve rocket, which the British widely used during the Napoleonic Wars and the War of 1812.[83]
Cannons were introduced to Majapahit when Kublai Khan's Chinese army under the leadership of Ike Mese sought to invade Java in 1293. History of Yuan mentioned that the Mongol used cannons (Chinese: 炮—Pào) against Daha forces.[84]: 1–2 [85][86]: 220  Cannons were used by the Ayutthaya Kingdom in 1352 during its invasion of the Khmer Empire.[87] Within a decade large quantities of gunpowder could be found in the Khmer Empire.[87] By the end of the century firearms were also used by the Trần dynasty.[88]
Even though the knowledge of making gunpowder-based weapon has been known after the failed Mongol invasion of Java, and the predecessor of firearms, the pole gun (bedil tombak), was recorded as being used by Java in 1413,[89][90]: 245  the knowledge of making "true" firearms came much later, after the middle of the 15th century. It was brought by the Islamic nations of West Asia, most probably the Arabs. The precise year of introduction is unknown, but it may be safely concluded to be no earlier than 1460.[91]: 23  Before the arrival of the Portuguese in Southeast Asia, the natives already possessed primitive firearms, the Java arquebus.[92] Portuguese influence to local weaponry after the capture of Malacca (1511) resulted in a new type of hybrid tradition matchlock firearm, the istinggar.[93][94]: 53 
Portuguese and Spanish invaders were unpleasantly surprised and even outgunned on occasion.[95] Circa 1540, the Javanese, always alert for new weapons found the newly arrived Portuguese weaponry superior to that of the locally made variants. Majapahit-era cetbang cannons were further improved and used in the Demak Sultanate period during the Demak invasion of Portuguese Malacca. During this period, the iron for manufacturing Javanese cannons was imported from Khorasan in northern Persia. The material was known by Javanese as wesi kurasani (Khorasan iron).[96] When the Portuguese came to the archipelago, they referred to it as berço, which was also used to refer to any breech-loading swivel gun, while the Spaniards call it verso.[97]: 151  By the early 16th century, the Javanese already locally producing large guns, some of them still survived until the present day and dubbed as "sacred cannon" or "holy cannon". These cannons varied between 180- and 260-pounders, weighing anywhere between 3 and 8 tons, length of them between 3 and 6 m.[98]
Saltpeter harvesting was recorded by Dutch and German travelers as being common in even the smallest villages and was collected from the decomposition process of large dung hills specifically piled for the purpose. The Dutch punishment for possession of non-permitted gunpowder appears to have been amputation.[99]: 180–181  Ownership and manufacture of gunpowder was later prohibited by the colonial Dutch occupiers.[100] According to colonel McKenzie quoted in Sir Thomas Stamford Raffles', The History of Java (1817), the purest sulfur was supplied from a crater from a mountain near the straits of Bali.[99]: 180–181 
On the origins of gunpowder technology, historian Tonio Andrade remarked, "Scholars today overwhelmingly concur that the gun was invented in China."[101] Gunpowder and the gun are widely believed by historians to have originated from China due to the large body of evidence that documents the evolution of gunpowder from a medicine to an incendiary and explosive, and the evolution of the gun from the fire lance to a metal gun, whereas similar records do not exist elsewhere.[102] As Andrade explains, the large amount of variation in gunpowder recipes in China relative to Europe is "evidence of experimentation in China, where gunpowder was at first used as an incendiary and only later became an explosive and a propellant... in contrast, formulas in Europe diverged only very slightly from the ideal proportions for use as an explosive and a propellant, suggesting that gunpowder was introduced as a mature technology."[61]
However, the history of gunpowder is not without controversy. A major problem confronting the study of early gunpowder history is ready access to sources close to the events described. Often the first records potentially describing use of gunpowder in warfare were written several centuries after the fact, and may well have been colored by the contemporary experiences of the chronicler.[103] Translation difficulties have led to errors or loose interpretations bordering on artistic licence. Ambiguous language can make it difficult to distinguish gunpowder weapons from similar technologies that do not rely on gunpowder. A commonly cited example is a report of the Battle of Mohi in Eastern Europe that mentions a "long lance" sending forth "evil-smelling vapors and smoke", which has been variously interpreted by different historians as the "first-gas attack upon European soil" using gunpowder, "the first use of cannon in Europe", or merely a "toxic gas" with no evidence of gunpowder.[104] It is difficult to accurately translate original Chinese alchemical texts, which tend to explain phenomena through metaphor, into modern scientific language with rigidly defined terminology in English. [33] Early texts potentially mentioning gunpowder are sometimes marked by a linguistic process where semantic change occurred.[105] For instance, the Arabic word naft transitioned from denoting naphtha to denoting gunpowder, and the Chinese word pào changed in meaning from trebuchet to a cannon.[106] This has led to arguments on the exact origins of gunpowder based on etymological foundations. Science and technology historian Bert S. Hall makes the observation that, "It goes without saying, however, that historians bent on special pleading, or simply with axes of their own to grind, can find rich material in these terminological thickets."[105]
Another major area of contention in modern studies of the history of gunpowder is regarding the transmission of gunpowder. While the literary and archaeological evidence supports a Chinese origin for gunpowder and guns, the manner in which gunpowder technology was transferred from China to the West is still under debate.[101] It is unknown why the rapid spread of gunpowder technology across Eurasia took place over several decades whereas other technologies such as paper, the compass, and printing did not reach Europe until centuries after they were invented in China.[61]
Gunpowder is a granular mixture of:
Potassium nitrate is the most important ingredient in terms of both bulk and function because the combustion process releases oxygen from the potassium nitrate, promoting the rapid burning of the other ingredients.[107] To reduce the likelihood of accidental ignition by static electricity, the granules of modern gunpowder are typically coated with graphite, which prevents the build-up of electrostatic charge.
Charcoal does not consist of pure carbon; rather, it consists of partially pyrolyzed cellulose, in which the wood is not completely decomposed. Carbon differs from ordinary charcoal. Whereas charcoal's autoignition temperature is relatively low, carbon's is much greater. Thus, a gunpowder composition containing pure carbon would burn similarly to a match head, at best.[108]
The current standard composition for the gunpowder manufactured by pyrotechnicians was adopted as long ago as 1780. Proportions by weight are 75% potassium nitrate (known as saltpeter or saltpetre), 15% softwood charcoal, and 10% sulfur.[109] These ratios have varied over the centuries and by country, and can be altered somewhat depending on the purpose of the powder. For instance, power grades of black powder, unsuitable for use in firearms but adequate for blasting rock in quarrying operations, are called blasting powder rather than gunpowder with standard proportions of 70% nitrate, 14% charcoal, and 16% sulfur; blasting powder may be made with the cheaper sodium nitrate substituted for potassium nitrate and proportions may be as low as 40% nitrate, 30% charcoal, and 30% sulfur.[110] In 1857, Lammot du Pont solved the main problem of using cheaper sodium nitrate formulations when he patented DuPont "B" blasting powder. After manufacturing grains from press-cake in the usual way, his process tumbled the powder with graphite dust for 12 hours. This formed a graphite coating on each grain that reduced its ability to absorb moisture.[111]
Neither the use of graphite nor sodium nitrate was new. Glossing gunpowder corns with graphite was already an accepted technique in 1839,[112] and sodium nitrate-based blasting powder had been made in Peru for many years using the sodium nitrate mined at Tarapacá (now in Chile).[113] Also, in 1846, two plants were built in south-west England to make blasting powder using this sodium nitrate.[114] The idea may well have been brought from Peru by Cornish miners returning home after completing their contracts. Another suggestion is that it was William Lobb, the plant collector, who recognised the possibilities of sodium nitrate during his travels in South America. Lammot du Pont would have known about the use of graphite and probably also knew about the plants in south-west England. In his patent he was careful to state that his claim was for the combination of graphite with sodium nitrate-based powder, rather than for either of the two individual technologies.
French war powder in 1879 used the ratio 75% saltpeter, 12.5% charcoal, 12.5% sulfur. English war powder in 1879 used the ratio 75% saltpeter, 15% charcoal, 10% sulfur.[115] The British Congreve rockets used 62.4% saltpeter, 23.2% charcoal and 14.4% sulfur, but the British Mark VII gunpowder was changed to 65% saltpeter, 20% charcoal and 15% sulfur.[citation needed] The explanation for the wide variety in formulation relates to usage. Powder used for rocketry can use a slower burn rate since it accelerates the projectile for a much longer time—whereas powders for weapons such as flintlocks, cap-locks, or matchlocks need a higher burn rate to accelerate the projectile in a much shorter distance. Cannons usually used lower burn-rate powders, because most would burst with higher burn-rate powders.
Besides black powder, there are other historically important types of gunpowder. "Brown gunpowder" is cited as composed of 79% nitre, 3% sulfur, and 18% charcoal per 100 of dry powder, with about 2% moisture. Prismatic Brown Powder is a large-grained product the Rottweil Company introduced in 1884 in Germany, which was adopted by the British Royal Navy shortly thereafter. The French navy adopted a fine, 3.1 millimeter, not prismatic grained product called Slow Burning Cocoa (SBC) or "cocoa powder". These brown powders reduced burning rate even further by using as little as 2 percent sulfur and using charcoal made from rye straw that had not been completely charred, hence the brown color.[116]
Lesmok powder was a product developed by DuPont in 1911,[117] one of several semi-smokeless products in the industry containing a mixture of black and nitrocellulose powder. It was sold to Winchester and others primarily for .22 and .32 small calibers. Its advantage was that it was believed at the time to be less corrosive than smokeless powders then in use.  It was not understood in the U.S. until the 1920s that the actual source of corrosion was the potassium chloride residue from potassium chlorate sensitized primers.  The bulkier black powder fouling better disperses primer residue.  Failure to mitigate primer corrosion by dispersion caused the false impression that nitrocellulose-based powder caused corrosion.[118] Lesmok had some of the bulk of black powder for dispersing primer residue, but somewhat less total bulk than straight black powder, thus requiring less frequent bore cleaning.[119] It was last sold by Winchester in 1947.
The development of smokeless powders, such as cordite, in the late 19th century created the need for a spark-sensitive priming charge, such as gunpowder. However, the sulfur content of traditional gunpowders caused corrosion problems with Cordite Mk I and this led to the introduction of a range of sulfur-free gunpowders, of varying grain sizes.[63] They typically contain 70.5 parts of saltpeter and 29.5 parts of charcoal.[63] Like black powder, they were produced in different grain sizes. In the United Kingdom, the finest grain was known as sulfur-free mealed powder (SMP). Coarser grains were numbered as sulfur-free gunpowder (SFG n): 'SFG 12', 'SFG 20', 'SFG 40' and 'SFG 90', for example; where the number represents the smallest BSS sieve mesh size, which retained no grains.
Sulfur's main role in gunpowder is to decrease the ignition temperature. A sample reaction for sulfur-free gunpowder would be:
The term black powder was coined in the late 19th century, primarily in the United States, to distinguish prior gunpowder formulations from the new smokeless powders and semi-smokeless powders. Semi-smokeless powders featured bulk volume properties that approximated black powder, but had significantly reduced amounts of smoke and combustion products. Smokeless powder has different burning properties (pressure vs. time) and can generate higher pressures and work per gram. This can rupture older weapons designed for black powder. Smokeless powders ranged in color from brownish tan to yellow to white. Most of the bulk semi-smokeless powders ceased to be manufactured in the 1920s.[120][119][121]
The original dry-compounded powder used in 15th-century Europe was known as "Serpentine", either a reference to Satan[36] or to a common artillery piece that used it.[122] The ingredients were ground
together with a mortar and pestle, perhaps for 24 hours,[122] resulting in a fine flour.  Vibration during transportation could cause the components to separate again, requiring remixing in the field. Also if the quality of the saltpeter was low (for instance if it was contaminated with highly hygroscopic calcium nitrate), or if the powder was simply old (due to the mildly hygroscopic nature of potassium nitrate), in humid weather it would need to be re-dried. The dust from "repairing" powder in the field was a major hazard.
Loading cannons or bombards before the powder-making advances of the Renaissance was a skilled art. Fine powder loaded haphazardly or too tightly would burn incompletely or too slowly. Typically, the breech-loading powder chamber in the rear of the piece was filled only about half full, the serpentine powder neither too compressed nor too loose, a wooden bung pounded in to seal the chamber from the barrel when assembled, and the projectile placed on.  A carefully determined empty space was necessary for the charge to burn effectively. When the cannon was fired through the touchhole, turbulence from the initial surface combustion caused the rest of the powder to be rapidly exposed to the flame.[122]
The advent of much more powerful and easy to use corned powder changed this procedure, but serpentine was used with older guns into the 17th century.[123]
For propellants to oxidize and burn rapidly and effectively, the combustible ingredients must be reduced to the smallest possible particle sizes, and be as thoroughly mixed as possible. Once mixed, however, for better results in a gun, makers discovered that the final product should be in the form of individual dense grains that spread the fire quickly from grain to grain, much as straw or twigs catch fire more quickly than a pile of sawdust.
In late 14th century Europe and China,[124] gunpowder was improved by wet grinding; liquid, such as distilled spirits[65] was added during the grinding-together of the ingredients and the moist paste dried afterwards. The principle of wet mixing to prevent the separation of dry ingredients, invented for gunpowder, is used today in the pharmaceutical industry.[125] It was discovered that if the paste was rolled into balls before drying the resulting gunpowder absorbed less water from the air during storage and traveled better. The balls were then crushed in a mortar by the gunner immediately before use, with the old problem of uneven particle size and packing causing unpredictable results. If the right size particles were chosen, however, the result was a great improvement in power.  Forming the damp paste into corn-sized clumps by hand or with the use of a sieve instead of larger balls produced a product after drying that loaded much better, as each tiny piece provided its own surrounding air space that allowed much more rapid combustion than a fine powder.  This "corned" gunpowder was from 30% to 300% more powerful. An example is cited where 15 kilograms (34 lb) of serpentine was needed to shoot a 21-kilogram (47 lb) ball, but only 8.2 kilograms (18 lb) of corned powder.[65]
Because the dry powdered ingredients must be mixed and bonded together for extrusion and cut into grains to maintain the blend, size reduction and mixing is done while the ingredients are damp, usually with water. After 1800, instead of forming grains by hand or with sieves, the damp mill-cake was pressed in molds to increase its density and extract the liquid, forming press-cake. The pressing took varying amounts of time, depending on conditions such as atmospheric humidity.  The hard, dense product was broken again into tiny pieces, which were separated with sieves to produce a uniform product for each purpose: coarse powders for cannons, finer grained powders for muskets, and the finest for small hand guns and priming.[123] Inappropriately fine-grained powder often caused cannons to burst before the projectile could move down the barrel, due to the high initial spike in pressure.[126] Mammoth powder with large grains, made for Rodman's 15-inch cannon, reduced the pressure to only 20 percent as high as ordinary cannon powder would have produced.[127]
In the mid-19th century, measurements were made determining that the burning rate within a grain of black powder (or a tightly packed mass) is about 6 cm/s (0.20 feet/s), while the rate of ignition propagation from grain to grain is around 9 m/s (30 feet/s), over two orders of magnitude faster.[123]
Modern corning first compresses the fine black powder meal into blocks with a fixed density (1.7 g/cm3).[128] In the United States, gunpowder grains were designated F (for fine) or C (for coarse). Grain diameter decreased with a larger number of Fs and increased with a larger number of Cs, ranging from about 2 mm (1⁄16 in) for 7F to 15 mm (9⁄16 in) for 7C. Even larger grains were produced for artillery bore diameters greater than about 17 cm (6.7 in). The standard DuPont Mammoth powder developed by Thomas Rodman and Lammot du Pont for use during the American Civil War had grains averaging 15 mm (0.6 in) in diameter with edges rounded in a glazing barrel.[127] Other versions had grains the size of golf and tennis balls for use in 20-inch (51 cm) Rodman guns.[129] In 1875 DuPont introduced Hexagonal powder for large artillery, which was pressed using shaped plates with a small center core—about 38 mm (1+1⁄2 in) diameter, like a wagon wheel nut, the center hole widened as the grain burned.[116] By 1882 German makers also produced hexagonal grained powders of a similar size for artillery.[116]
By the late 19th century manufacturing focused on standard grades of black powder from Fg used in large bore rifles and shotguns, through FFg (medium and small-bore arms such as muskets and fusils), FFFg (small-bore rifles and pistols), and FFFFg (extreme small bore, short pistols and most commonly for priming flintlocks).[130] A coarser grade for use in military artillery blanks was designated A-1. These grades were sorted on a system of screens with oversize retained on a mesh of 6 wires per inch, A-1 retained on 10 wires per inch, Fg retained on 14, FFg on 24, FFFg on 46, and FFFFg on 60. Fines designated FFFFFg were usually reprocessed to minimize explosive dust hazards.[131] In the United Kingdom, the main service gunpowders were classified RFG (rifle grained fine) with diameter of one or two millimeters and RLG (rifle grained large) for grain diameters between two and six millimeters.[129] Gunpowder grains can alternatively be categorized by mesh size: the BSS sieve mesh size, being the smallest mesh size, which retains no grains. Recognized grain sizes are Gunpowder G 7, G 20, G 40, and G 90.
Owing to the large market of antique and replica black-powder firearms in the US, modern black powder substitutes like Pyrodex, Triple Seven and Black Mag3[119] pellets have been developed since the 1970s. These products, which should not be confused with smokeless powders, aim to produce less fouling (solid residue), while maintaining the traditional volumetric measurement system for charges. Claims of less corrosiveness of these products have been controversial however. New cleaning products for black-powder guns have also been developed for this market.[130]
A simple, commonly cited, chemical equation for the combustion of gunpowder is:
A balanced, but still simplified, equation is:[132]
The exact percentages of ingredients varied greatly through the medieval period as the recipes were developed by trial and error, and needed to be updated for changing military technology.[133]
Gunpowder does not burn as a single reaction, so the byproducts are not easily predicted. One study[134] showed that it produced (in order of descending quantities) 55.91% solid products: potassium carbonate, potassium sulfate, potassium sulfide, sulfur, potassium nitrate, potassium thiocyanate, carbon, ammonium carbonate and 42.98% gaseous products: carbon dioxide, nitrogen, carbon monoxide, hydrogen sulfide, hydrogen, methane, 1.11% water.
Gunpowder made with less-expensive and more plentiful sodium nitrate instead of potassium nitrate (in appropriate proportions) works just as well. However, it is more hygroscopic than powders made from potassium nitrate. Muzzleloaders have been known to fire after hanging on a wall for decades in a loaded state, provided they remained dry. By contrast, gunpowder made with sodium nitrate must be kept sealed to remain stable.[original research?]
Gunpowder releases 3 megajoules per kilogram and contains its own oxidant.[citation needed] This is less than TNT (4.7 megajoules per kilogram), or gasoline (47.2 megajoules per kilogram in combustion, but gasoline requires an oxidant; for instance, an optimized gasoline and O2 mixture releases 10.4 megajoules per kilogram, taking into account the mass of the oxygen).
Gunpowder also has a low energy density[how much?] compared to modern "smokeless" powders, and thus to achieve high energy loadings, large amounts are needed with heavy projectiles.[135]
For the most powerful black powder, meal powder, a wood charcoal, is used. The best wood for the purpose is Pacific willow,[136] but others such as alder or buckthorn can be used. In Great Britain between the 15th and 19th centuries charcoal from alder buckthorn was greatly prized for gunpowder manufacture; cottonwood was used by the American Confederate States.[137] The ingredients are reduced in particle size and mixed as intimately as possible. Originally, this was with a mortar-and-pestle or a similarly operating stamping-mill, using copper, bronze or other non-sparking materials, until supplanted by the rotating ball mill principle with non-sparking bronze or lead. Historically, a marble or limestone edge runner mill, running on a limestone bed, was used in Great Britain; however, by the mid 19th century this had changed to either an iron-shod stone wheel or a cast iron wheel running on an iron bed.[109] The mix was dampened with alcohol or water during grinding to prevent accidental ignition. This also helps the extremely soluble saltpeter to mix into the microscopic pores of the very high surface-area charcoal.
Around the late 14th century, European powdermakers first began adding liquid during grinding to improve mixing, reduce dust, and with it the risk of explosion.[138] The powder-makers would then shape the resulting paste of dampened gunpowder, known as mill cake, into corns, or grains, to dry. Not only did corned powder keep better because of its reduced surface area, gunners also found that it was more powerful and easier to load into guns. Before long, powder-makers standardized the process by forcing mill cake through sieves instead of corning powder by hand.
The improvement was based on reducing the surface area of a higher density composition. At the beginning of the 19th century, makers increased density further by static pressing. They shoveled damp mill cake into a two-foot square box, placed this beneath a screw press and reduced it to half its volume. "Press cake" had the hardness of slate. They broke the dried slabs with hammers or rollers, and sorted the granules with sieves into different grades.  In the United States, Eleuthere Irenee du Pont, who had learned the trade from Lavoisier, tumbled the dried grains in rotating barrels to round the edges and increase durability during shipping and handling. (Sharp grains rounded off in transport, producing fine "meal dust" that changed the burning properties.)
Another advance was the manufacture of kiln charcoal by distilling wood in heated iron retorts instead of burning it in earthen pits. Controlling the temperature influenced the power and consistency of the finished gunpowder.  In 1863, in response to high prices for Indian saltpeter, DuPont chemists developed a process using potash or mined potassium chloride to convert plentiful Chilean sodium nitrate to potassium nitrate.[139]
The following year (1864) the Gatebeck Low Gunpowder Works in Cumbria (Great Britain) started a plant to manufacture potassium nitrate by essentially the same chemical process.[140] This is nowadays called the 'Wakefield Process', after the owners of the company. It would have used potassium chloride from the Staßfurt mines, near Magdeburg, Germany, which had recently become available in industrial quantities.[141]
During the 18th century, gunpowder factories became increasingly dependent on mechanical energy.[142] Despite mechanization, production difficulties related to humidity control, especially during the pressing, were still present in the late 19th century. A paper from 1885 laments that "Gunpowder is such a nervous and sensitive spirit, that in almost every process of manufacture it changes under our hands as the weather changes." Pressing times to the desired density could vary by a factor of three depending on the atmospheric humidity.[143]
The United Nations Model Regulations on the Transportation of Dangerous Goods and national transportation authorities, such as United States Department of Transportation, have classified gunpowder (black powder) as a Group A: Primary explosive substance for shipment because it ignites so easily. Complete manufactured devices containing black powder are usually classified as Group D: Secondary detonating substance, or black powder, or article containing secondary detonating substance, such as firework, class D model rocket engine, etc., for shipment because they are harder to ignite than loose powder. As explosives, they all fall into the category of Class 1 .
Besides its use as a propellant in firearms and artillery, black powder's other main use has been as a blasting powder in quarrying, mining, and road construction (including railroad construction). During the 19th century, outside of war emergencies such as the Crimean War or the American Civil War, more black powder was used in these industrial uses than in firearms and artillery. Dynamite gradually replaced it for those uses. Today, industrial explosives for such uses are still a huge market, but most of the market is in newer explosives rather than black powder.
Beginning in the 1930s, gunpowder or smokeless powder was used in rivet guns, stun guns for animals, cable splicers and other industrial construction tools.[144] The "stud gun", a powder-actuated tool, drove nails or screws into solid concrete, a function not possible with hydraulic tools, and today is still an important part of various industries, but the cartridges usually use smokeless powders. Industrial shotguns have been used to eliminate persistent material rings in operating rotary kilns (such as those for cement, lime, phosphate, etc.) and clinker in operating furnaces, and commercial tools make the method more reliable.[145]
Gunpowder has occasionally been employed for other purposes besides weapons, mining, fireworks and construction:




The Colosseum (/ˌkɒləˈsiːəm/ KOL-ə-SEE-əm; Italian: Colosseo [kolosˈsɛːo]) is an elliptical amphitheatre in the centre of the city of Rome, Italy, just east of the Roman Forum. It is the largest ancient amphitheatre ever built, and is still the largest standing amphitheatre in the world, despite its age. Construction began under the emperor Vespasian (r. 69–79 AD) in 72[1] and was completed in 80 AD under his successor and heir, Titus (r. 79–81).[2] Further modifications were made during the reign of Domitian (r. 81–96).[3] The three emperors who were patrons of the work are known as the Flavian dynasty, and the amphitheatre was named the Flavian Amphitheatre (Latin: Amphitheatrum Flavium; Italian: Anfiteatro Flavio [aɱfiteˈaːtro ˈflaːvjo]) by later classicists and archaeologists for its association with their family name (Flavius).
The Colosseum is built of travertine limestone, tuff (volcanic rock), and brick-faced concrete. It could hold an estimated 50,000 to 80,000 spectators at various points in its history,[4][5] having an average audience of some 65,000;[6] it was used for gladiatorial contests and public spectacles including animal hunts, executions, re-enactments of famous battles, and dramas based on Roman mythology, and briefly mock sea battles. The building ceased to be used for entertainment in the early medieval era. It was later reused for such purposes as housing, workshops, quarters for a religious order, a fortress, a quarry, and a Christian shrine.
Although substantially ruined by earthquakes and stone robbers taking spolia, the Colosseum is still a renowned symbol of Imperial Rome and was listed as one of the New 7 Wonders of the World.[7] It is one of Rome's most popular tourist attractions and also has links to the Roman Catholic Church, as each Good Friday the Pope leads a torchlit "Way of the Cross" procession that starts in the area around the Colosseum.[8] The Colosseum is depicted on the Italian version of the five-cent euro coin.
Originally, the building's Latin name was simply amphitheatrum, 'amphitheatre'.[9] Though the modern name Flavian Amphitheatre (Latin: Amphitheatrum Flavium) is often used, there is no evidence it was used in classical antiquity.[9] This name refers to the patronage of the Flavian dynasty, during whose reigns the building was constructed, but the structure is better known as the Colosseum.[9] In antiquity, Romans may have referred to the Colosseum by the unofficial name Amphitheatrum Caesareum (with Caesareum an adjective pertaining to the title Caesar), but this name may have been strictly poetic[10][11] as it was not exclusive to the Colosseum; Vespasian and Titus, builders of the Colosseum, also constructed a Flavian Amphitheatre in Puteoli (modern Pozzuoli).[12]
The name Colosseum is believed to be derived from a colossal statue of Nero on the model of the Colossus of Rhodes.[9][3] The giant bronze sculpture of Nero as a solar deity was moved to its position beside the amphitheatre by the emperor Hadrian (r. 117–138).[9] The word colosseum is a neuter Latin noun formed from the adjective colosseus, meaning "gigantic" or "colossean".[9] By the year 1000 the Latin name "Colosseum" had been coined to refer to the amphitheatre from the nearby "Colossus Solis".[13]
The spelling was sometimes altered in Medieval Latin: coloseum and coliseum are attested from the 12th and 14th centuries respectively.[9] In the 12th century, the structure was recorded as the amphitheatrum colisei, 'Amphitheatre of the Colossus'.[9] In the High Middle Ages, the Flavian amphitheatre is attested as the late 13th-century Old French: colosé, and in Middle French as: colisée by the early 16th century, by which time the word could be applied to any amphitheatre.[9] From Middle French: colisée derived the Middle English: colisee, in use by the middle of the 15th century and employed by John Capgrave in his Solace of Pilgrims, in which he remarked: Middle English: collise eke is a meruelous place … þe moost part of it stant at þis day.[14] An English translation by John Bourchier, 2nd Baron Berners, of Antonio de Guevara's biography of Marcus Aurelius (r. 161–180) in about 1533 referred to Middle English: this Emperour, beynge with the Senate at Collisee ....[14] Similarly, the Italian: colosseo, or coliseo, are attested as referring first to the amphitheatre in Rome, and then to any amphitheatre (as Italian: culiseo in 1367).[14][9] By 1460, an equivalent existed in Catalan: coliseu; by 1495 had appeared the Spanish: coliseo, and by 1548 the Portuguese: coliseu.[9]
The earliest citation for the name Colosseum in Early Modern English is the 1600 translation, by Philemon Holland, of the Urbis Romae topographia of Bartolomeo Marliani, which he used in the preparation of his translation of Livy's Augustan era Ab Urbe Condita Libri.[9] The text states: "This Amphitheatre was commonly called Colosseum, of Neroes Colossus, which was set up in the porch of Neroes house."[9] Similarly, John Evelyn, translating the Middle French name: le Colisée used by the architectural theorist Roland Fréart de Chambray, wrote "And 'tis indeed a kind of miracle to see that the Colosseum … and innumerable other Structures which seemed to have been built for Eternity, should be at present so ruinous and dilapidated".[9] 
After Nero's suicide and the civil wars of the Year of the Four Emperors, the Colossus of Nero statue was remodeled by the condemned emperor's successors into the likeness of Helios (Sol) or Apollo, the sun god, by adding the appropriate solar crown. It was then commonly referred to as the "Colossus solis".[15] Nero's head was also replaced several times with the heads of succeeding emperors.[16] Despite its pagan links, the statue remained standing well into the medieval era and was credited with magical powers. The emperor Constantine the Great remodeled the statue's face as his own.[17]
In the 8th century, an epigram attributed to the Venerable Bede celebrated the symbolic significance of the statue in a prophecy that is variously quoted: Quamdiu stat Colisæus, stat et Roma; quando cadet colisæus, cadet et Roma; quando cadet Roma, cadet et mundus ("as long as the Colossus stands, so shall Rome; when the Colossus falls, Rome shall fall; when Rome falls, so falls the world").[18] This is often mistranslated to refer to the Colosseum rather than the Colossus (as in, for instance, Byron's poem Childe Harold's Pilgrimage). However, at the time that the Pseudo-Bede wrote, the masculine noun coliseus was applied to the statue rather than to the amphitheatre.[19]
The Colossus did eventually fall, possibly being pulled down to reuse its bronze. The statue itself was largely forgotten and only its base survives, between the Colosseum and the nearby Temple of Venus and Roma.[20]

The site chosen was a flat area on the floor of a low valley between the Caelian, Esquiline and Palatine Hills, through which a canalised stream ran as well as an artificial lake/marsh.[21] By the 2nd century BC the area was densely inhabited. It was devastated by the Great Fire of Rome in 64 AD, following which Nero seized much of the area to add to his personal domain. He built the grandiose Domus Aurea on the site, in front of which he created an artificial lake surrounded by pavilions, gardens and porticoes. The existing Aqua Claudia aqueduct was extended to supply water to the area and the gigantic bronze Colossus of Nero was set up nearby at the entrance to the Domus Aurea.[20]  Although the Colossus was preserved, much of the Domus Aurea was torn down. The lake was filled in and the land reused as the location for the new Flavian Amphitheatre. Gladiatorial schools and other support buildings were constructed nearby within the former grounds of the Domus Aurea. Vespasian's decision to build the Colosseum on the site of Nero's lake can be seen as a populist gesture of returning to the people an area of the city which Nero had appropriated for his own use. In contrast to many other amphitheatres, which were on the outskirts of a city, the Colosseum was constructed in the city centre, in effect, placing it both symbolically and precisely at the heart of Rome.
Construction was funded by the opulent spoils taken from the Jewish Temple after the First Jewish–Roman War in 70 AD led to the Siege of Jerusalem. According to a reconstructed inscription found on the site, "the emperor Vespasian ordered this new amphitheatre to be erected from his general's share of the booty." It is often assumed that Jewish prisoners of war were brought back to Rome and contributed to the massive workforce needed for the construction of the amphitheatre, but there is no ancient evidence for that; it would, nonetheless, be commensurate with Roman practice to add humiliation to the defeated population.[22] Along with this free source of unskilled labor, teams of professional Roman builders, engineers, artists, painters and decorators undertook the more specialized tasks necessary for building the Colosseum. The Colosseum was constructed with several different materials: wood, limestone, tuff, tiles, cement, and mortar.
Construction of the Colosseum began under the rule of Vespasian[3] in around 70–72 AD (73–75 AD according to some sources). The Colosseum had been completed up to the third story by the time of Vespasian's death in 79. The top level was finished by his son, Titus, in 80,[3] and the inaugural games were held in 80 or 81 AD.[23] Dio Cassius recounts that over 9,000 wild animals were killed during the inaugural games of the amphitheatre. Commemorative coinage was issued celebrating the inauguration.[24] The building was remodelled further under Vespasian's younger son, the newly designated Emperor Domitian, who constructed the hypogeum, a series of tunnels used to house animals and slaves. He also added a gallery to the top of the Colosseum to increase its seating capacity.[25]
In 217, the Colosseum was badly damaged by a major fire (caused by lightning, according to Dio Cassius[26]), which destroyed the wooden upper levels of the amphitheatre's interior. It was not fully repaired until about 240 and underwent further repairs in 250 or 252 and again in 320. Honorius banned the practice of gladiator fights in 399 and again in 404. Gladiatorial fights are last mentioned around 435.[20] An inscription records the restoration of various parts of the Colosseum under Theodosius II and Valentinian III (reigned 425–455), possibly to repair damage caused by a major earthquake in 443; more work followed in 484[27] and 508. The arena continued to be used for contests well into the 6th century. Animal hunts continued until at least 523, when Anicius Maximus celebrated his consulship with some venationes, criticised by King Theodoric the Great for their high cost.[20]
The Colosseum underwent several radical changes of use. By the late 6th century a small chapel had been built into the structure of the amphitheater, though this apparently did not confer any particular religious significance on the building as a whole. The arena was converted into a cemetery. The numerous vaulted spaces in the arcades under the seating were converted into housing and workshops, and are recorded as still being rented out as late as the 12th century. Around 1200 the Frangipani family took over the Colosseum and fortified it, apparently using it as a castle. In the early to mid 14th century, the Pope's relocation to Avignon caused a population decline in Rome that left the region insecure. The colosseum was largely abandoned by the public and became a popular den for bandits.[28]
Severe damage was inflicted on the Colosseum by the great earthquake in 1349, causing the outer south side, lying on a less stable alluvial terrain, to collapse. Much of the tumbled stone was reused to build palaces, churches, hospitals and other buildings elsewhere in Rome. In 1377, after the Pope's return to Rome, the Colosseum was restored by a religious order called Arciconfraternita del SS. Salvatore ad Sancta Sanctorum, who then inhabited a northern portion of it until as late as the early 19th century.[29][30] The interior of the amphitheater was extensively stripped of stone, which was reused elsewhere, or (in the case of the marble façade) was burned to make quicklime.[20] The iron clamps[20] which held the stonework together were pried or hacked out of the walls, leaving numerous pockmarks which still scar the building today.
During the 16th and 17th century, Church officials sought a productive role for the Colosseum. Pope Sixtus V (1585–1590) planned to turn the building into a wool factory to provide employment for Rome's prostitutes, though this proposal fell through with his premature death.[31] In 1671 Cardinal Altieri authorized its use for bullfights; a public outcry caused the idea to be hastily abandoned.
In 1749, Pope Benedict XIV endorsed the view that the Colosseum was a sacred site where early Christians had been martyred. He forbade the use of the Colosseum as a quarry and consecrated the building to the Passion of Christ and installed Stations of the Cross, declaring it sanctified by the blood of the Christian martyrs who perished there (see Significance in Christianity). However, there is no historical evidence to support Benedict's claim, nor is there even any evidence that anyone before the 16th century suggested this might be the case; the Catholic Encyclopedia concludes that there are no historical grounds for the supposition, other than the reasonably plausible conjecture that some of the many martyrs may well have been.[32]
Later popes initiated various stabilization and restoration projects, removing the extensive vegetation which had overgrown the structure and threatened to damage it further. The façade was reinforced with triangular brick wedges in 1807 and 1827, and the interior was repaired in 1831, 1846 and in the 1930s. The arena substructure was partly excavated in 1810–1814 and 1874 and was fully exposed under Benito Mussolini in the 1930s.[20]
The Colosseum is today one of Rome's most popular tourist attractions, receiving millions of visitors annually. The effects of pollution and general deterioration over time prompted a major restoration programme carried out between 1993 and 2000, at a cost of Lire 40 billion ($19.3m / €20.6m at 2000 prices).
In recent years, the Colosseum has become a symbol of the international campaign against capital punishment, which was abolished in Italy in 1948. Several anti–death penalty demonstrations took place in front of the Colosseum in 2000. Since that time, as a gesture against the death penalty, the local authorities of Rome change the color of the Colosseum's night time illumination from white to gold whenever a person condemned to the death penalty anywhere in the world gets their sentence commuted or is released,[33] or if a jurisdiction abolishes the death penalty. Most recently, the Colosseum was illuminated in gold in November 2012 following the abolishment of capital punishment in the American state of Connecticut in April 2012.[34]
Because of the ruined state of the interior, it is impractical to use the Colosseum to host large events; only a few hundred spectators can be accommodated in temporary seating. However, much larger concerts have been held just outside, using the Colosseum as a backdrop. Performers who have played at the Colosseum in recent years have included Ray Charles (May 2002),[35] Paul McCartney (May 2003),[36] Elton John (September 2005),[37] and Billy Joel (July 2006).
Unlike Roman theatres that were built into hillsides, the Colosseum is an entirely free-standing structure. It derives its basic exterior and interior architecture from that of two theatres back to back. It is elliptical in plan and is 189 meters (615 ft / 640 Roman feet) long, and 156 meters (510 ft / 528 Roman feet) wide, with a base area of 24,000 square metres (6 acres). The height of the outer wall is 48 meters (157 ft / 165 Roman feet). The perimeter originally measured 545 meters (1,788 ft / 1,835 Roman feet). The central arena is an ellipse 87 m (287 ft) long and 55 m (180 ft) wide, surrounded by a wall 5 m (15 ft) high, above which rose tiers of seating.
The outer wall is estimated to have required over 100,000 cubic metres (3.5 million cubic feet) of travertine stone which were set without mortar; they were held together by 300 tons of iron clamps.[20] However, it has suffered extensive damage over the centuries, with large segments having collapsed following earthquakes. The north side of the perimeter wall is still standing; the distinctive triangular brick wedges at each end are modern additions, having been constructed in the early 19th century to shore up the wall. The remainder of the present-day exterior of the Colosseum is in fact the original interior wall.
The surviving part of the outer wall's monumental façade comprises three superposed storeys surmounted by a podium on which stands a tall attic, both of which are pierced by windows interspersed at regular intervals. The arcades are framed by half-columns of the Doric, Ionic, and Corinthian orders, while the attic is decorated with Corinthian pilasters.[38] Each of the arches in the second- and third-floor arcades framed statues, probably honoring divinities and other figures from Classical mythology.
Two hundred and forty mast corbels were positioned around the top of the attic. They originally supported a retractable awning, known as the velarium, that kept the sun and rain off spectators. This consisted of a canvas-covered, net-like structure made of ropes, with a hole in the center.[3] It covered two-thirds of the arena, and sloped down towards the center to catch the wind and provide a breeze for the audience. Sailors, specially enlisted from the Roman naval headquarters at Misenum and housed in the nearby Castra Misenatium, were used to work the velarium.[39]
The Colosseum's huge crowd capacity made it essential that the venue could be filled or evacuated quickly. Its architects adopted solutions very similar to those used in modern stadia to deal with the same problem. The amphitheatre was ringed by eighty entrances at ground level, 76 of which were used by ordinary spectators.[3] Each entrance and exit was numbered, as was each staircase. The northern main entrance was reserved for the Roman Emperor and his aides, whilst the other three axial entrances were most likely used by the elite. All four axial entrances were richly decorated with painted stucco reliefs, of which fragments survive. Many of the original outer entrances have disappeared with the collapse of the perimeter wall, but entrances XXIII (23) to LIIII (54) survive.[20]
Spectators were given tickets in the form of numbered pottery shards, which directed them to the appropriate section and row. They accessed their seats via vomitoria (singular vomitorium), passageways that opened into a tier of seats from below or behind. These quickly dispersed people into their seats and, upon conclusion of the event or in an emergency evacuation, could permit their exit within only a few minutes. The name vomitoria derived from the Latin word for a rapid discharge, from which English derives the word vomit.
According to the Codex-Calendar of 354, the Colosseum could accommodate 87,000 people, although modern estimates put the figure at around 50,000. They were seated in a tiered arrangement that reflected the rigidly stratified nature of Roman society. Special boxes were provided at the north and south ends respectively for the Emperor and the Vestal Virgins, providing the best views of the arena. Flanking them at the same level was a broad platform or podium for the senatorial class, who were allowed to bring their own chairs. The names of some 5th century senators can still be seen carved into the stonework, presumably reserving areas for their use.
The tier above the senators, known as the maenianum primum, was occupied by the non-senatorial noble class or knights (equites). The next level up, the maenianum secundum, was originally reserved for ordinary Roman citizens (plebeians) and was divided into two sections. The lower part (the immum) was for wealthy citizens, while the upper part (the summum) was for poor citizens. Specific sectors were provided for other social groups: for instance, boys with their tutors, soldiers on leave, foreign dignitaries, scribes, heralds, priests and so on. Stone (and later marble) seating was provided for the citizens and nobles, who presumably would have brought their own cushions with them. Inscriptions identified the areas reserved for specific groups.
Another level, the maenianum secundum in legneis, was added at the very top of the building during the reign of Domitian. This comprised a gallery for the common poor, slaves and women. It would have been either standing room only, or would have had very steep wooden benches. Some groups were banned altogether from the Colosseum, notably gravediggers, actors and former gladiators.[20]
Each tier was divided into sections (maeniana) by curved passages and low walls (praecinctiones or baltei), and were subdivided into cunei, or wedges, by the steps and aisles from the vomitoria. Each row (gradus) of seats was numbered, permitting each individual seat to be exactly designated by its gradus, cuneus, and number.[40]
The arena itself was 83 meters by 48 meters (272 ft by 157 ft / 280 by 163 Roman feet).[20] It comprised a wooden floor covered by sand (the Latin word for sand is harena or arena), covering an elaborate underground structure called the hypogeum (literally meaning "underground"). The hypogeum was not part of the original construction but was ordered to be built by Emperor Domitian. Little now remains of the original arena floor, but the hypogeum is still clearly visible. It consisted of a two-level subterranean network of tunnels and cages beneath the arena where gladiators and animals were held before contests began. Eighty vertical shafts provided instant access to the arena for caged animals and scenery pieces concealed underneath; larger hinged platforms, called hegmata, provided access for elephants and the like. It was restructured on numerous occasions; at least twelve different phases of construction can be seen.[20]
The hypogeum was connected by tunnels to a number of points outside the Colosseum. Animals and performers were brought through the tunnel from nearby stables, with the gladiators' barracks at the Ludus Magnus to the east also being connected by tunnels. Separate tunnels were provided for the Emperor and the Vestal Virgins to permit them to enter and exit the Colosseum without needing to pass through the crowds.[20]
Substantial quantities of machinery also existed in the hypogeum. Elevators and pulleys raised and lowered scenery and props, as well as lifting caged animals to the surface for release. There is evidence for the existence of major hydraulic mechanisms[20] and according to ancient accounts, it was possible to flood the arena rapidly, presumably via a connection to a nearby aqueduct. However, the construction of the hypogeum at Domitian's behest put an end to the practise of flooding, and thus also to naval battles, early in the Colosseum's existence.
The Colosseum and its activities supported a substantial industry in the area. In addition to the amphitheatre itself, many other buildings nearby were linked to the games. Immediately to the east is the remains of the Ludus Magnus, a training school for gladiators. This was connected to the Colosseum by an underground passage, to allow easy access for the gladiators. The Ludus Magnus had its own miniature training arena, which was itself a popular attraction for Roman spectators. Other training schools were in the same area, including the Ludus Matutinus (Morning School), where fighters of animals were trained, plus the Dacian and Gallic Schools.
Also nearby were the Armamentarium, comprising an armory to store weapons; the Summum Choragium, where machinery was stored; the Sanitarium, which had facilities to treat wounded gladiators; and the Spoliarium, where bodies of dead gladiators were stripped of their armor and disposed of.
Around the perimeter of the Colosseum, at a distance of 18 m (59 ft) from the perimeter, was a series of tall stone posts, with five remaining on the eastern side. Various explanations have been advanced for their presence; they may have been a religious boundary, or an outer boundary for ticket checks, or an anchor for the velarium or awning.[20]

The Colosseum was used to host gladiatorial shows as well as a variety of other events. The shows, called munera, were always given by private individuals rather than the state. They had a strong religious element but were also demonstrations of power and family prestige and were immensely popular. Another major attraction was the animal hunt, or venatio. This utilized a great variety of wild beasts, mainly imported from Africa and the Middle East, and included creatures such as rhinoceros, hippopotamuses, elephants, giraffes, aurochs, wisents, Barbary lions, panthers, leopards, bears, Caspian tigers, crocodiles and ostriches. Battles and hunts were often staged amid elaborate sets with movable trees and buildings. These events could be huge in scale; Trajan is said to have celebrated his victories in Dacia in 107 with contests involving 11,000 animals and 10,000 gladiators over the course of 123 days. During lunch intervals, executions ad bestias would be staged. Those condemned to death would be sent into the arena, naked and unarmed, to face the beasts of death which would literally tear them to pieces.  Acrobats and magicians would also perform, usually during the intervals.During the early days of the Colosseum, ancient writers recorded that the building was used for naumachiae (more properly known as navalia proelia) or simulated sea battles. Accounts of the inaugural games held by Titus in AD 80 describe it being filled with water for a display of specially trained swimming horses and bulls. There is also an account of a re-enactment of a famous sea battle between the Corcyrean (Corfiot) Greeks and the Corinthians. This has been the subject of some debate among historians; although providing the water would not have been a problem, it is unclear how the arena could have been waterproofed, nor would there have been enough space in the arena for the warships to move around. It has been suggested that the reports either have the location wrong or that the Colosseum originally featured a wide floodable channel down its central axis (which would later have been replaced by the hypogeum).[20]
Sylvae or recreations of natural scenes were also held in the arena. Painters, technicians and architects would construct a simulation of a forest with real trees and bushes planted in the arena's floor, and animals would then be introduced. Such scenes might be used simply to display a natural environment for the urban population, or could otherwise be used as the backdrop for hunts or dramas depicting episodes from mythology. They were also occasionally used for executions in which the hero of the story – played by a condemned person – was killed in one of various gruesome but mythologically authentic ways, such as being mauled by beasts or burned to death.
The Colosseum today is a major tourist attraction in Rome with thousands of tourists each year entering to view the interior arena.[41] There is now a museum dedicated to Eros in the upper floor of the outer wall of the building. Part of the arena floor has been reinstated. Beneath the Colosseum, a network of subterranean passageways once used to transport wild animals and gladiators to the arena opened to the public in summer 2010.[42]
The Colosseum is also the site of Roman Catholic ceremonies in the 20th and 21st centuries. For instance, Pope Benedict XVI led the Stations of the Cross called the Scriptural Way of the Cross (which calls for more meditation) at the Colosseum[43][44] on Good Fridays.[8]
In 2011 Diego Della Valle, head of the shoe firm Tod's, entered into an agreement with local officials to sponsor a €25 million restoration of the Colosseum. Work was planned to begin at the end of 2011, taking up to two and a half years.[45] Due to the controversial nature of using a public–private partnership to fund the restoration, work was delayed and began in 2013. The restoration is the first full cleaning and repair in the Colosseum's history.[46] The first stage is to clean and restore the Colosseum's arcaded façade and replace the metal enclosures that block the ground-level arches. After three years, the work was completed on 1 July 2016, when the Italian minister of culture, Dario Franceschini, also announced that the funds have been committed to replace the floors by the end of 2018. These will provide a stage that Franceschini says will be used for "cultural events of the highest level."[47] The project also includes creating a services center and restoring the galleries and underground spaces inside the Colosseum.[48] Since 1 November 2017, the top two levels have been opened for guided visits. The fourth level held the marketplace, and the top fifth tier is where the poorest citizens, the plebeians, gathered and watched the show, bringing picnics for the day-long event.[49]
The Colosseum is generally regarded by Christians as a site of the martyrdom of large numbers of believers during the persecution of Christians in the Roman Empire, as evidenced by Church history and tradition.[50][51][52] On the other hand, other scholars believe that the majority of martyrdoms may have occurred at other venues within the city of Rome, rather than at the Colosseum, citing a lack of still-intact physical evidence or historical records.[53][54][55] These scholars assert that "some Christians were executed as common criminals in the Colosseum—their crime being refusal to reverence the Roman gods", but most Christian martyrs of the early Church were executed for their faith at the Circus Maximus.[56][57] According to Irenaeus (died about 202), Ignatius of Antioch was fed to the lions in Rome around 107 A.D and although Irenaeus says nothing about this happening at the Colosseum, tradition ascribes it to that place.[58][59][60][61]
In the Middle Ages, the Colosseum was not regarded as a monument, and was used as what some modern sources label a "quarry,"[62] which is to say that stones from the Colosseum were taken for the building of other sacred sites.[63] This fact is used to support the idea that, at a time when sites associated with martyrs were highly venerated the Colosseum was not being treated as a sacred site.[64] It was not included in the itineraries compiled for the use of pilgrims nor in works such as the 12th century Mirabilia Urbis Romae ("Marvels of the City of Rome"), which claims the Circus Flaminius – but not the Colosseum – as the site of martyrdoms.[65] Part of the structure was inhabited by a Christian religious order, but it is not known whether this was for any particular religious reason.
Pope Pius V (1566–1572) is said to have recommended that pilgrims gather sand from the arena of the Colosseum to serve as a relic, on the grounds that it was impregnated with the blood of martyrs, although some of his contemporaries did not share his conviction.[66] A century later Fioravante Martinelli listed the Colosseum at the head of a list of places sacred to the martyrs in his 1653 book Roma ex ethnica sacra. Martinelli's book evidently had an effect on public opinion; in response to Cardinal Altieri's proposal some years later to turn the Colosseum into a bullring, Carlo Tomassi published a pamphlet in protest against what he regarded as an act of desecration. The ensuing controversy persuaded Pope Clement X to close the Colosseum's external arcades and declare it a sanctuary.[67]
At the insistence of St. Leonard of Port Maurice, Pope Benedict XIV (1740–1758) forbade the quarrying of the Colosseum and erected Stations of the Cross around the arena, which remained until February 1874.[68] Benedict Joseph Labre spent the later years of his life within the walls of the Colosseum, living on alms, before he died in 1783.[68] Several 19th century popes funded repair and restoration work on the Colosseum, and it still retains its Christian connection today. A Christian cross stands in the Colosseum, with a plaque, stating:
The amphitheater, one consecrated to triumphs, entertainments, and the impious worship of pagan gods, is now dedicated to the sufferings of the martyrs purified from impious superstitions.[58]Other Christian crosses stand in several points around the arena and every Good Friday the Pope leads a Via Crucis procession to the amphitheater.
The Colosseum has a wide and well-documented history of flora ever since Domenico Panaroli made the first catalogue of its plants in 1643. Since then, 684 species have been identified there. The peak was in 1855 (420 species). Attempts were made in 1871 to eradicate the vegetation, because of concerns over the damage that was being caused to the masonry, but much of it has returned.[20] 242 species have been counted today and of the species first identified by Panaroli, 200 remain.
The variation of plants can be explained by the change of climate in Rome through the centuries. Additionally, bird migration, flower blooming, and the growth of Rome that caused the Colosseum to become embedded within the modern city centre rather than on the outskirts of the ancient city, as well as deliberate transport of species, are also contributing causes. Another reason often given is their seeds being unwittingly transported either on the fur or in the feces of animals brought there from all corners of the empire.[69]
The Colosseum has appeared in numerous films, artworks and games. It is featured in movies such as Roman Holiday, Gladiator, The Way of the Dragon, The Core and Jumper and games like Assassin's Creed: Brotherhood, Ryse: Son of Rome and Forge of Empires.
Several architectural works have also been modelled on or inspired by, the Colosseum. These include:
The Colosseum in 2021
Colosseum and the Arch of Constantine seen from Palatine
Interior
Interior
Colosseum at night
Seating tiers at the east entrance
Colosseum 2013
 Media related to Colosseum at Wikimedia Commons


Opera  is a form of theatre in which music is a fundamental component and dramatic roles are taken by singers. Such a "work" (the literal translation of the Italian word "opera") is typically a collaboration between a composer and a librettist[1] and incorporates a number of the performing arts, such as acting, scenery, costume, and sometimes dance or ballet. The performance is typically given in an opera house, accompanied by an orchestra or smaller musical ensemble, which since the early 19th century has been led by a conductor. Although musical theatre is closely related to opera, the two are considered to be distinct from one another.[2]
Opera is a key part of the Western classical music tradition.[3] Originally understood as an entirely sung piece, in contrast to a play with songs, opera has come to include numerous genres, including some that include spoken dialogue such as Singspiel and Opéra comique. In traditional number opera, singers employ two styles of singing: recitative, a speech-inflected style,[4] and self-contained arias. The 19th century saw the rise of the continuous music drama.
Opera originated in Italy at the end of the 16th century (with Jacopo Peri's mostly lost Dafne, produced in Florence in 1598) especially from works by Claudio Monteverdi, notably L'Orfeo, and soon spread through the rest of Europe: Heinrich Schütz in Germany, Jean-Baptiste Lully in France, and Henry Purcell in England all helped to establish their national traditions in the 17th century. In the 18th century, Italian opera continued to dominate most of Europe (except France), attracting foreign composers such as George Frideric Handel. Opera seria was the most prestigious form of Italian opera, until Christoph Willibald Gluck reacted against its artificiality with his "reform" operas in the 1760s. The most renowned figure of late 18th-century opera is Wolfgang Amadeus Mozart, who began with opera seria but is most famous for his Italian comic operas, especially The Marriage of Figaro (Le nozze di Figaro), Don Giovanni, and Così fan tutte, as well as Die Entführung aus dem Serail (The Abduction from the Seraglio), and The Magic Flute (Die Zauberflöte), landmarks in the German tradition.
The first third of the 19th century saw the high point of the bel canto style, with Gioachino Rossini, Gaetano Donizetti and Vincenzo Bellini all creating signature works of that style. It also saw the advent of grand opera typified by the works of Daniel Auber and Giacomo Meyerbeer as well as Carl Maria von Weber's introduction of German Romantische Oper (German Romantic Opera). The mid-to-late 19th century was a golden age of opera, led and dominated by Giuseppe Verdi in Italy and Richard Wagner in Germany. The popularity of opera continued through the verismo era in Italy and contemporary French opera through to Giacomo Puccini and Richard Strauss in the early 20th century. During the 19th century, parallel operatic traditions emerged in central and eastern Europe, particularly in Russia and Bohemia. The 20th century saw many experiments with modern styles, such as atonality and serialism (Arnold Schoenberg and Alban Berg), neoclassicism (Igor Stravinsky), and minimalism (Philip Glass and John Adams). With the rise of recording technology, singers such as Enrico Caruso and Maria Callas became known to much wider audiences that went beyond the circle of opera fans. Since the invention of radio and television, operas were also performed on (and written for) these media. Beginning in 2006, a number of major opera houses began to present live high-definition video transmissions of their performances in cinemas all over the world. Since 2009, complete performances can be downloaded and are live streamed.
The words of an opera are known as the libretto (meaning "small book"). Some composers, notably Wagner, have written their own libretti; others have worked in close collaboration with their librettists, e.g. Mozart with Lorenzo Da Ponte. Traditional opera, often referred to as "number opera", consists of two modes of singing: recitative, the plot-driving passages sung in a style designed to imitate and emphasize the inflections of speech,[4] and aria (an "air" or formal song) in which the characters express their emotions in a more structured melodic style. Vocal duets, trios and other ensembles often occur, and choruses are used to comment on the action. In some forms of opera, such as singspiel, opéra comique, operetta, and semi-opera, the recitative is mostly replaced by spoken dialogue. Melodic or semi-melodic passages occurring in the midst of, or instead of, recitative, are also referred to as arioso. The terminology of the various kinds of operatic voices is described in detail below.[5] 
During both the Baroque and Classical periods, recitative could appear in two basic forms, each of which was accompanied by a different instrumental ensemble: secco (dry) recitative, sung with a free rhythm dictated by the accent of the words, accompanied only by basso continuo, which was usually a harpsichord and a cello; or accompagnato (also known as strumentato) in which the orchestra provided accompaniment. Over the 18th century, arias were increasingly accompanied by the orchestra. By the 19th century, accompagnato had gained the upper hand, the orchestra played a much bigger role, and Wagner revolutionized opera by abolishing almost all distinction between aria and recitative in his quest for what Wagner termed "endless melody". Subsequent composers have tended to follow Wagner's example, though some, such as Stravinsky in his The Rake's Progress have bucked the trend. The changing role of the orchestra in opera is described in more detail below.
The Italian word opera means "work", both in the sense of the labour done and the result produced. The Italian word derives from the Latin word opera, a singular noun meaning "work" and also the plural of the noun opus. According to the Oxford English Dictionary, the Italian word was first used in the sense "composition in which poetry, dance, and music are combined" in 1639; the first recorded English usage in this sense dates to 1648.[6]
Dafne by Jacopo Peri was the earliest composition considered opera, as understood today. It was written around 1597, largely under the inspiration of an elite circle of literate Florentine humanists who gathered as the "Camerata de' Bardi". Significantly, Dafne was an attempt to revive the classical Greek drama, part of the wider revival of antiquity characteristic of the Renaissance. The members of the Camerata considered that the "chorus" parts of Greek dramas were originally sung, and possibly even the entire text of all roles; opera was thus conceived as a way of "restoring" this situation. Dafne, however, is lost. A later work by Peri, Euridice, dating from 1600, is the first opera score to have survived until the present day. However, the honour of being the first opera still to be regularly performed goes to Claudio Monteverdi's L'Orfeo, composed for the court of Mantua in 1607.[7] The Mantua court of the Gonzagas, employers of Monteverdi, played a significant role in the origin of opera employing not only court singers of the concerto delle donne (till 1598), but also one of the first actual "opera singers", Madama Europa.[8]
Opera did not remain confined to court audiences for long. In 1637, the idea of a "season" (often during the carnival) of publicly attended operas supported by ticket sales emerged in Venice. Monteverdi had moved to the city from Mantua and composed his last operas, Il ritorno d'Ulisse in patria and L'incoronazione di Poppea, for the Venetian theatre in the 1640s. His most important follower Francesco Cavalli helped spread opera throughout Italy. In these early Baroque operas, broad comedy was blended with tragic elements in a mix that jarred some educated sensibilities, sparking the first of opera's many reform movements, sponsored by the Arcadian Academy, which came to be associated with the poet Metastasio, whose libretti helped crystallize the genre of opera seria, which became the leading form of Italian opera until the end of the 18th century. Once the Metastasian ideal had been firmly established, comedy in Baroque-era opera was reserved for what came to be called opera buffa. Before such elements were forced out of opera seria, many libretti had featured a separately unfolding comic plot as sort of an "opera-within-an-opera". One reason for this was an attempt to attract members of the growing merchant class, newly wealthy, but still not as cultured as the nobility, to the public opera houses. These separate plots were almost immediately resurrected in a separately developing tradition that partly derived from the commedia dell'arte, a long-flourishing improvisatory stage tradition of Italy. Just as intermedi had once been performed in between the acts of stage plays, operas in the new comic genre of intermezzi, which developed largely in Naples in the 1710s and 1720s, were initially staged during the intermissions of opera seria. They became so popular, however, that they were soon being offered as separate productions.
Opera seria was elevated in tone and highly stylised in form, usually consisting of secco recitative interspersed with long da capo arias. These afforded great opportunity for virtuosic singing and during the golden age of opera seria the singer really became the star. The role of the hero was usually written for the high-pitched male castrato voice, which was produced by castration of the singer before puberty, which prevented a boy's larynx from being transformed at puberty. Castrati such as Farinelli and Senesino, as well as female sopranos such as Faustina Bordoni, became in great demand throughout Europe as opera seria ruled the stage in every country except France. Farinelli was one of the most famous singers of the 18th century. Italian opera set the Baroque standard. Italian libretti were the norm, even when a German composer like Handel found himself composing the likes of Rinaldo and Giulio Cesare for London audiences. Italian libretti remained dominant in the classical period as well, for example in the operas of Mozart, who wrote in Vienna near the century's close. Leading Italian-born composers of opera seria include Alessandro Scarlatti, Antonio Vivaldi and Nicola Porpora.[9]
Opera seria had its weaknesses and critics. The taste for embellishment on behalf of the superbly trained singers, and the use of spectacle as a replacement for dramatic purity and unity drew attacks. Francesco Algarotti's Essay on the Opera (1755) proved to be an inspiration for Christoph Willibald Gluck's reforms. He advocated that opera seria had to return to basics and that all the various elements—music (both instrumental and vocal), ballet, and staging—must be subservient to the overriding drama. In 1765 Melchior Grimm published "Poème lyrique", an influential article for the Encyclopédie on lyric and opera librettos.[10][11][12][13][14] Several composers of the period, including Niccolò Jommelli and Tommaso Traetta, attempted to put these ideals into practice. The first to succeed however, was Gluck. Gluck strove to achieve a "beautiful simplicity". This is evident in his first reform opera, Orfeo ed Euridice, where his non-virtuosic vocal melodies are supported by simple harmonies and a richer orchestra presence throughout.
Gluck's reforms have had resonance throughout operatic history. Weber, Mozart, and Wagner, in particular, were influenced by his ideals. Mozart, in many ways Gluck's successor, combined a superb sense of drama, harmony, melody, and counterpoint to write a series of comic operas with libretti by Lorenzo Da Ponte, notably Le nozze di Figaro, Don Giovanni, and Così fan tutte, which remain among the most-loved, popular and well-known operas. But Mozart's contribution to opera seria was more mixed; by his time it was dying away, and in spite of such fine works as Idomeneo and La clemenza di Tito, he would not succeed in bringing the art form back to life again.[15]
The bel canto opera movement flourished in the early 19th century and is exemplified by the operas of Rossini, Bellini, Donizetti, Pacini, Mercadante and many others. Literally "beautiful singing", bel canto opera derives from the Italian stylistic singing school of the same name. Bel canto lines are typically florid and intricate, requiring supreme agility and pitch control. Examples of famous operas in the bel canto style include Rossini's Il barbiere di Siviglia and La Cenerentola, as well as Bellini's Norma, La sonnambula and I puritani and Donizetti's Lucia di Lammermoor, L'elisir d'amore and Don Pasquale.
Following the bel canto era, a more direct, forceful style was rapidly popularized by Giuseppe Verdi, beginning with his biblical opera Nabucco. This opera, and the ones that would follow in Verdi's career, revolutionized Italian opera, changing it from merely a display of vocal fireworks, with Rossini's and Donizetti's works, to dramatic story-telling. Verdi's operas resonated with the growing spirit of Italian nationalism in the post-Napoleonic era, and he quickly became an icon of the patriotic movement for a unified Italy. In the early 1850s, Verdi produced his three most popular operas: Rigoletto, Il trovatore and La traviata. The first of these, Rigoletto, proved the most daring and revolutionary. In it, Verdi blurs the distinction between the aria and recitative as it never before was, leading the opera to be "an unending string of duets". La traviata was also novel. It tells the story of courtesan, and is often cited as one of the first "realistic" operas,[citation needed] because rather than featuring great kings and figures from literature, it focuses on the tragedies of ordinary life and society. After these, he continued to develop his style, composing perhaps the greatest French grand opera, Don Carlos, and ending his career with two Shakespeare-inspired works, Otello and Falstaff, which reveal how far Italian opera had grown in sophistication since the early 19th century. These final two works showed Verdi at his most masterfully orchestrated, and are both incredibly influential, and modern. In Falstaff, Verdi sets the preeminent standard for the form and style that would dominate opera throughout the twentieth century. Rather than long, suspended melodies, Falstaff contains many little motifs and mottos, that, rather than being expanded upon, are introduced and subsequently dropped, only to be brought up again later. These motifs never are expanded upon, and just as the audience expects a character to launch into a long melody, a new character speaks, introducing a new phrase. This fashion of opera directed opera from Verdi, onward, exercising tremendous influence on his successors Giacomo Puccini, Richard Strauss, and Benjamin Britten.[16]
After Verdi, the sentimental "realistic" melodrama of verismo appeared in Italy. This was a style introduced by Pietro Mascagni's Cavalleria rusticana and Ruggero Leoncavallo's Pagliacci that came to dominate the world's opera stages with such popular works as Giacomo Puccini's La bohème, Tosca, and Madama Butterfly. Later Italian composers, such as Berio and Nono, have experimented with modernism.[17]
The first German opera was Dafne, composed by Heinrich Schütz in 1627, but the music score has not survived. Italian opera held a great sway over German-speaking countries until the late 18th century. Nevertheless, native forms would develop in spite of this influence. In 1644, Sigmund Staden produced the first Singspiel, Seelewig, a popular form of German-language opera in which singing alternates with spoken dialogue. In the late 17th century and early 18th century, the Theater am Gänsemarkt in Hamburg presented German operas by Keiser, Telemann and Handel. Yet most of the major German composers of the time, including Handel himself, as well as Graun, Hasse and later Gluck, chose to write most of their operas in foreign languages, especially Italian. In contrast to Italian opera, which was generally composed for the aristocratic class, German opera was generally composed for the masses and tended to feature simple folk-like melodies, and it was not until the arrival of Mozart that German opera was able to match its Italian counterpart in musical sophistication.[18] The theatre company of Abel Seyler pioneered serious German-language opera in the 1770s, marking a break with the previous simpler musical entertainment.[19][20]
Mozart's Singspiele, Die Entführung aus dem Serail (1782) and Die Zauberflöte (1791) were an important breakthrough in achieving international recognition for German opera. The tradition was developed in the 19th century by Beethoven with his Fidelio (1805), inspired by the climate of the French Revolution. Carl Maria von Weber established German Romantic opera in opposition to the dominance of Italian bel canto. His Der Freischütz (1821) shows his genius for creating a supernatural atmosphere. Other opera composers of the time include Marschner, Schubert and Lortzing, but the most significant figure was undoubtedly Wagner.
Wagner was one of the most revolutionary and controversial composers in musical history. Starting under the influence of Weber and Meyerbeer, he gradually evolved a new concept of opera as a Gesamtkunstwerk (a "complete work of art"), a fusion of music, poetry and painting. He greatly increased the role and power of the orchestra, creating scores with a complex web of leitmotifs, recurring themes often associated with the characters and concepts of the drama, of which prototypes can be heard in his earlier operas such as Der fliegende Holländer, Tannhäuser and Lohengrin; and he was prepared to violate accepted musical conventions, such as tonality, in his quest for greater expressivity. In his mature music dramas, Tristan und Isolde, Die Meistersinger von Nürnberg, Der Ring des Nibelungen and Parsifal, he abolished the distinction between aria and recitative in favour of a seamless flow of "endless melody". Wagner also brought a new philosophical dimension to opera in his works, which were usually based on stories from Germanic or Arthurian legend. Finally, Wagner built his own opera house at Bayreuth with part of the patronage from Ludwig II of Bavaria, exclusively dedicated to performing his own works in the style he wanted.
Opera would never be the same after Wagner and for many composers his legacy proved a heavy burden. On the other hand, Richard Strauss accepted Wagnerian ideas but took them in wholly new directions, along with incorporating the new form introduced by Verdi. He first won fame with the scandalous Salome and the dark tragedy Elektra, in which tonality was pushed to the limits. Then Strauss changed tack in his greatest success, Der Rosenkavalier, where Mozart and Viennese waltzes became as important an influence as Wagner. Strauss continued to produce a highly varied body of operatic works, often with libretti by the poet Hugo von Hofmannsthal. Other composers who made individual contributions to German opera in the early 20th century include Alexander von Zemlinsky, Erich Korngold, Franz Schreker, Paul Hindemith, Kurt Weill and the Italian-born Ferruccio Busoni. The operatic innovations of Arnold Schoenberg and his successors are discussed in the section on modernism.[21]
During the late 19th century, the Austrian composer Johann Strauss II, an admirer of the French-language operettas composed by Jacques Offenbach, composed several German-language operettas, the most famous of which was Die Fledermaus.[22] Nevertheless, rather than copying the style of Offenbach, the operettas of Strauss II had distinctly Viennese flavor to them.
In rivalry with imported Italian opera productions, a separate French tradition was founded by the Italian-born French composer Jean-Baptiste Lully at the court of King Louis XIV. Despite his foreign birthplace, Lully established an Academy of Music and monopolised French opera from 1672. Starting with Cadmus et Hermione, Lully and his librettist Quinault created tragédie en musique, a form in which dance music and choral writing were particularly prominent. Lully's operas also show a concern for expressive recitative which matched the contours of the French language. In the 18th century, Lully's most important successor was Jean-Philippe Rameau, who composed five tragédies en musique as well as numerous works in other genres such as opéra-ballet, all notable for their rich orchestration and harmonic daring. Despite the popularity of Italian opera seria throughout much of Europe during the Baroque period, Italian opera never gained much of a foothold in France, where its own national operatic tradition was more popular instead.[23] After Rameau's death, the German Gluck was persuaded to produce six operas for the Parisian stage in the 1770s. They show the influence of Rameau, but simplified and with greater focus on the drama. At the same time, by the middle of the 18th century another genre was gaining popularity in France: opéra comique. This was the equivalent of the German singspiel, where arias alternated with spoken dialogue. Notable examples in this style were produced by Monsigny, Philidor and, above all, Grétry. During the Revolutionary and Napoleonic period, composers such as Étienne Méhul, Luigi Cherubini and Gaspare Spontini, who were followers of Gluck, brought a new seriousness to the genre, which had never been wholly "comic" in any case. Another phenomenon of this period was the 'propaganda opera' celebrating revolutionary successes, e.g. Gossec's Le triomphe de la République (1793).
By the 1820s, Gluckian influence in France had given way to a taste for Italian bel canto, especially after the arrival of Rossini in Paris. Rossini's Guillaume Tell helped found the new genre of grand opera, a form whose most famous exponent was another foreigner, Giacomo Meyerbeer. Meyerbeer's works, such as Les Huguenots, emphasised virtuoso singing and extraordinary stage effects. Lighter opéra comique also enjoyed tremendous success in the hands of Boïeldieu, Auber, Hérold and Adam. In this climate, the operas of the French-born composer Hector Berlioz struggled to gain a hearing. Berlioz's epic masterpiece Les Troyens, the culmination of the Gluckian tradition, was not given a full performance for almost a hundred years.
In the second half of the 19th century, Jacques Offenbach created operetta with witty and cynical works such as Orphée aux enfers, as well as the opera Les Contes d'Hoffmann; Charles Gounod scored a massive success with Faust; and Georges Bizet composed Carmen, which, once audiences learned to accept its blend of Romanticism and realism, became the most popular of all opéra comiques. Jules Massenet, Camille Saint-Saëns and Léo Delibes all composed works which are still part of the standard repertory, examples being Massenet's Manon, Saint-Saëns' Samson et Dalila and Delibes' Lakmé. Their operas formed another genre, the Opera Lyrique, combined opera comique and grand opera. It is less grandiose than grand opera, but without the spoken dialogue of opera comique.  At the same time, the influence of Richard Wagner was felt as a challenge to the French tradition. Many French critics angrily rejected Wagner's music dramas while many French composers closely imitated them with variable success. Perhaps the most interesting response came from Claude Debussy. As in Wagner's works, the orchestra plays a leading role in Debussy's unique opera Pelléas et Mélisande (1902) and there are no real arias, only recitative. But the drama is understated, enigmatic and completely un-Wagnerian.
Other notable 20th-century names include Ravel, Dukas, Roussel, Honegger and Milhaud. Francis Poulenc is one of the very few post-war composers of any nationality whose operas (which include Dialogues des Carmélites) have gained a foothold in the international repertory. Olivier Messiaen's lengthy sacred drama Saint François d'Assise (1983) has also attracted widespread attention.[24]
In England, opera's antecedent was the 17th-century jig. This was an afterpiece that came at the end of a play. It was frequently libellous and scandalous and consisted in the main of dialogue set to music arranged from popular tunes. In this respect, jigs anticipate the ballad operas of the 18th century. At the same time, the French masque was gaining a firm hold at the English Court, with even more lavish splendour and highly realistic scenery than had been seen before. Inigo Jones became the quintessential designer of these productions, and this style was to dominate the English stage for three centuries. These masques contained songs and dances. In Ben Jonson's Lovers Made Men (1617), "the whole masque was sung after the Italian manner, stilo recitativo".[25] The approach of the English Commonwealth closed theatres and halted any developments that may have led to the establishment of English opera. However, in 1656, the dramatist Sir William Davenant produced The Siege of Rhodes. Since his theatre was not licensed to produce drama, he asked several of the leading composers (Lawes, Cooke, Locke, Coleman and Hudson) to set sections of it to music. This success was followed by The Cruelty of the Spaniards in Peru (1658) and The History of Sir Francis Drake (1659). These pieces were encouraged by Oliver Cromwell because they were critical of Spain. With the English Restoration, foreign (especially French) musicians were welcomed back. In 1673, Thomas Shadwell's Psyche, patterned on the 1671 'comédie-ballet' of the same name produced by Molière and Jean-Baptiste Lully. William Davenant produced The Tempest in the same year, which was the first musical adaption of a Shakespeare play (composed by Locke and Johnson).[25] About 1683, John Blow composed Venus and Adonis, often thought of as the first true English-language opera.
Blow's immediate successor was the better known Henry Purcell. Despite the success of his masterwork Dido and Aeneas (1689), in which the action is furthered by the use of Italian-style recitative, much of Purcell's best work was not involved in the composing of typical opera, but instead, he usually worked within the constraints of the semi-opera format, where isolated scenes and masques are contained within the structure of a spoken play, such as Shakespeare in Purcell's The Fairy-Queen (1692) and Beaumont and Fletcher in The Prophetess (1690) and Bonduca (1696). The main characters of the play tend not to be involved in the musical scenes, which means that Purcell was rarely able to develop his characters through song. Despite these hindrances, his aim (and that of his collaborator John Dryden) was to establish serious opera in England, but these hopes ended with Purcell's early death at the age of 36.
Following Purcell, the popularity of opera in England dwindled for several decades. A revived interest in opera occurred in the 1730s which is largely attributed to Thomas Arne, both for his own compositions and for alerting Handel to the commercial possibilities of large-scale works in English. Arne was the first English composer to experiment with Italian-style all-sung comic opera, with his greatest success being Thomas and Sally in 1760. His opera Artaxerxes (1762) was the first attempt to set a full-blown opera seria in English and was a huge success, holding the stage until the 1830s. Although Arne imitated many elements of Italian opera, he was perhaps the only English composer at that time who was able to move beyond the Italian influences and create his own unique and distinctly English voice. His modernized ballad opera, Love in a Village (1762), began a vogue for pastiche opera that lasted well into the 19th century. Charles Burney wrote that Arne introduced "a light, airy, original, and pleasing melody, wholly different from that of Purcell or Handel, whom all English composers had either pillaged or imitated".
Besides Arne, the other dominating force in English opera at this time was George Frideric Handel, whose opera serias filled the London operatic stages for decades and influenced most home-grown composers, like John Frederick Lampe, who wrote using Italian models. This situation continued throughout the 18th and 19th centuries, including in the work of Michael William Balfe, and the operas of the great Italian composers, as well as those of Mozart, Beethoven, and Meyerbeer, continued to dominate the musical stage in England.
The only exceptions were ballad operas, such as John Gay's The Beggar's Opera (1728), musical burlesques, European operettas, and late Victorian era light operas, notably the Savoy Operas of W. S. Gilbert and Arthur Sullivan, all of which types of musical entertainments frequently spoofed operatic conventions. Sullivan wrote only one grand opera, Ivanhoe (following the efforts of a number of young English composers beginning about 1876),[25] but he claimed that even his light operas constituted part of a school of "English" opera, intended to supplant the French operettas (usually performed in bad translations) that had dominated the London stage from the mid-19th century into the 1870s. London's Daily Telegraph agreed, describing The Yeomen of the Guard as "a genuine English opera, forerunner of many others, let us hope, and possibly significant of an advance towards a national lyric stage".[26] Sullivan produced a few light operas in the 1890s that were of a more serious nature than those in the G&S series, including Haddon Hall and The Beauty Stone, but Ivanhoe (which ran for 155 consecutive performances, using alternating casts—a record until Broadway's La bohème) survives as his only grand opera.
In the 20th century, English opera began to assert more independence, with works of Ralph Vaughan Williams and in particular Benjamin Britten, who in a series of works that remain in standard repertory today, revealed an excellent flair for the dramatic and superb musicality. More recently Sir Harrison Birtwistle has emerged as one of Britain's most significant contemporary composers from his first opera Punch and Judy to his most recent critical success in The Minotaur. In the first decade of the 21st century, the librettist of an early Birtwistle opera, Michael Nyman, has been focusing on composing operas, including Facing Goya, Man and Boy: Dada, and Love Counts. Today composers such as Thomas Adès continue to export English opera abroad.[27]
Also in the 20th century, American composers like George Gershwin (Porgy and Bess), Scott Joplin (Treemonisha), Leonard Bernstein (Candide), Gian Carlo Menotti, Douglas Moore, and Carlisle Floyd began to contribute English-language operas infused with touches of popular musical styles. They were followed by composers such as Philip Glass (Einstein on the Beach), Mark Adamo, John Corigliano (The Ghosts of Versailles), Robert Moran, John Adams (Nixon in China), André Previn and Jake Heggie. Many contemporary 21st century opera composers have emerged such as Missy Mazzoli, Kevin Puts, Tom Cipullo, Huang Ruo, David T. Little, Terence Blanchard, Jennifer Higdon, Tobias Picker, Michael Ching, Anthony Davis, and Ricky Ian Gordon.
Opera was brought to Russia in the 1730s by the Italian operatic troupes and soon it became an important part of entertainment for the Russian Imperial Court and aristocracy. Many foreign composers such as Baldassare Galuppi, Giovanni Paisiello, Giuseppe Sarti, and Domenico Cimarosa (as well as various others) were invited to Russia to compose new operas, mostly in the Italian language. Simultaneously some domestic musicians like Maksym Berezovsky and Dmitry Bortniansky were sent abroad to learn to write operas. The first opera written in Russian was Tsefal i Prokris by the Italian composer Francesco Araja (1755). The development of Russian-language opera was supported by the Russian composers Vasily Pashkevich, Yevstigney Fomin and Alexey Verstovsky.
However, the real birth of Russian opera came with Mikhail Glinka and his two great operas A Life for the Tsar (1836) and Ruslan and Lyudmila (1842). After him, during the 19th century in Russia, there were written such operatic masterpieces as Rusalka and The Stone Guest by Alexander Dargomyzhsky, Boris Godunov and Khovanshchina by Modest Mussorgsky, Prince Igor by Alexander Borodin, Eugene Onegin and The Queen of Spades by Pyotr Tchaikovsky, and The Snow Maiden and Sadko by Nikolai Rimsky-Korsakov. These developments mirrored the growth of Russian nationalism across the artistic spectrum, as part of the more general Slavophilism movement.
In the 20th century, the traditions of Russian opera were developed by many composers including Sergei Rachmaninoff in his works The Miserly Knight and Francesca da Rimini, Igor Stravinsky in Le Rossignol, Mavra, Oedipus rex, and The Rake's Progress, Sergei Prokofiev in The Gambler, The Love for Three Oranges, The Fiery Angel, Betrothal in a Monastery, and War and Peace; as well as Dmitri Shostakovich in The Nose and Lady Macbeth of the Mtsensk District, Edison Denisov in L'écume des jours, and Alfred Schnittke in Life with an Idiot and Historia von D. Johann Fausten.[28]
Czech composers also developed a thriving national opera movement of their own in the 19th century, starting with Bedřich Smetana, who wrote eight operas including the internationally popular The Bartered Bride. Smetana's eight operas created the bedrock of the Czech opera repertory, but of these only The Bartered Bride is performed regularly outside the composer's homeland. After reaching Vienna in 1892 and London in 1895 it rapidly became part of the repertory of every major opera company worldwide.
Antonín Dvořák's nine operas, except his first, have librettos in Czech and were intended to convey the Czech national spirit, as were some of his choral works. By far the most successful of the operas is Rusalka which contains the well-known aria "Měsíčku na nebi hlubokém" ("Song to the Moon"); it is played on contemporary opera stages frequently outside the Czech Republic. This is attributable to their uneven invention and libretti, and perhaps also their staging requirements – The Jacobin, Armida, Vanda and Dimitrij need stages large enough to portray invading armies.
Leoš Janáček gained international recognition in the 20th century for his innovative works. His later, mature works incorporate his earlier studies of national folk music in a modern, highly original synthesis, first evident in the opera Jenůfa, which was premiered in 1904 in Brno. The success of Jenůfa (often called the "Moravian national opera") at Prague in 1916 gave Janáček access to the world's great opera stages. Janáček's later works are his most celebrated. They include operas such as Káťa Kabanová and The Cunning Little Vixen, the Sinfonietta and the Glagolitic Mass.
Spain also produced its own distinctive form of opera, known as zarzuela, which had two separate flowerings: one from the mid-17th century through the mid-18th century, and another beginning around 1850. During the late 18th century up until the mid-19th century, Italian opera was immensely popular in Spain, supplanting the native form.
In Russian Eastern Europe, several national operas began to emerge. Ukrainian opera was developed by Semen Hulak-Artemovsky (1813–1873) whose most famous work Zaporozhets za Dunayem (A Cossack Beyond the Danube) is regularly performed around the world. Other Ukrainian opera composers include Mykola Lysenko (Taras Bulba and Natalka Poltavka), Heorhiy Maiboroda, and Yuliy Meitus. At the turn of the century, a distinct national opera movement also began to emerge in Georgia under the leadership Zacharia Paliashvili, who fused local folk songs and stories with 19th-century Romantic classical themes.
The key figure of Hungarian national opera in the 19th century was Ferenc Erkel, whose works mostly dealt with historical themes. Among his most often performed operas are Hunyadi László and Bánk bán. The most famous modern Hungarian opera is Béla Bartók's Duke Bluebeard's Castle.
Stanisław Moniuszko's opera Straszny Dwór (in English The Haunted Manor) (1861–64) represents a nineteenth-century peak of Polish national opera.[29] In the 20th century, other operas created by Polish composers included King Roger by Karol Szymanowski and Ubu Rex by Krzysztof Penderecki.
The first known opera from Turkey (the Ottoman Empire) was Arshak II, which was an Armenian opera composed by an ethnic Armenian composer Tigran Chukhajian in 1868 and partially performed in 1873. It was fully staged in 1945 in Armenia.
The first years of the Soviet Union saw the emergence of new national operas, such as the Koroğlu (1937) by the Azerbaijani composer Uzeyir Hajibeyov. The first Kyrgyz opera, Ai-Churek, premiered in Moscow at the Bolshoi Theatre on 26 May 1939, during Kyrgyz Art Decade. It was composed by Vladimir Vlasov, Abdylas Maldybaev and Vladimir Fere. The libretto was written by Joomart Bokonbaev, Jusup Turusbekov, and Kybanychbek Malikov. The opera is based on the Kyrgyz heroic epic Manas.[30][31]
In Iran, opera gained more attention after the introduction of Western classical music in the late 19th century. However, it took until mid 20th century for Iranian composers to start experiencing with the field, especially as the construction of the Roudaki Hall in 1967, made possible staging of a large variety of works for stage. Perhaps, the most famous Iranian opera is Rostam and Sohrab by Loris Tjeknavorian premiered not until the early 2000s.
Chinese contemporary classical opera, a Chinese language form of Western style opera that is distinct from traditional Chinese opera, has had operas dating back to The White Haired Girl in 1945.[32][33][34]
In Latin America, opera started as a result of European colonisation. The first opera ever written in the Americas was La púrpura de la rosa, by Tomás de Torrejón y Velasco, although Partenope, by the Mexican Manuel de Zumaya, was the first opera written from a composer born in Latin America (music now lost). The first Brazilian opera for a libretto in Portuguese was A Noite de São João, by Elias Álvares Lobo. However, Antônio Carlos Gomes is generally regarded as the most outstanding Brazilian composer, having a relative success in Italy with its Brazilian-themed operas with Italian librettos, such as Il Guarany. Opera in Argentina developed in the 20th century after the inauguration of Teatro Colón in Buenos Aires—with the opera Aurora, by Ettore Panizza, being heavily influenced by the Italian tradition, due to immigration. Other important composers from Argentina include Felipe Boero and Alberto Ginastera.
Perhaps the most obvious stylistic manifestation of modernism in opera is the development of atonality. The move away from traditional tonality in opera had begun with Richard Wagner, and in particular the Tristan chord. Composers such as Richard Strauss, Claude Debussy, Giacomo Puccini,[35] Paul Hindemith, Benjamin Britten and Hans Pfitzner pushed Wagnerian harmony further with a more extreme use of chromaticism and greater use of dissonance. Another aspect of modernist opera is the shift away from long, suspended melodies, to short quick mottos, as first illustrated by Giuseppe Verdi in his Falstaff. Composers such as Strauss, Britten, Shostakovich and Stravinsky adopted and expanded upon this style.
Operatic modernism truly began in the operas of two Viennese composers, Arnold Schoenberg and his student Alban Berg, both composers and advocates of atonality and its later development (as worked out by Schoenberg), dodecaphony. Schoenberg's early musico-dramatic works, Erwartung (1909, premiered in 1924) and Die glückliche Hand display heavy use of chromatic harmony and dissonance in general. Schoenberg also occasionally used Sprechstimme.
The two operas of Schoenberg's pupil Alban Berg, Wozzeck (1925) and Lulu (incomplete at his death in 1935) share many of the same characteristics as described above, though Berg combined his highly personal interpretation of Schoenberg's twelve-tone technique with melodic passages of a more traditionally tonal nature (quite Mahlerian in character) which perhaps partially explains why his operas have remained in standard repertory, despite their controversial music and plots. Schoenberg's theories have influenced (either directly or indirectly) significant numbers of opera composers ever since, even if they themselves did not compose using his techniques.
Composers thus influenced include the Englishman Benjamin Britten, the German Hans Werner Henze, and the Russian Dmitri Shostakovich. (Philip Glass also makes use of atonality, though his style is generally described as minimalist, usually thought of as another 20th-century development.)[36]
However, operatic modernism's use of atonality also sparked a backlash in the form of neoclassicism. An early leader of this movement was Ferruccio Busoni, who in 1913 wrote the libretto for his neoclassical number opera Arlecchino (first performed in 1917).[37] Also among the vanguard was the Russian Igor Stravinsky. After composing music for the Diaghilev-produced ballets Petrushka (1911) and The Rite of Spring (1913), Stravinsky turned to neoclassicism, a development culminating in his opera-oratorio Oedipus Rex (1927).  Stravinsky had already turned away from the modernist trends of his early ballets to produce small-scale works that do not fully qualify as opera, yet certainly contain many operatic elements, including Renard (1916: "a burlesque in song and dance") and The Soldier's Tale (1918: "to be read, played, and danced"; in both cases the descriptions and instructions are those of the composer). In the latter, the actors declaim portions of speech to a specified rhythm over instrumental accompaniment, peculiarly similar to the older German genre of Melodrama.  Well after his Rimsky-Korsakov-inspired works The Nightingale (1914), and Mavra (1922), Stravinsky continued to ignore serialist technique and eventually wrote a full-fledged 18th-century-style diatonic number opera The Rake's Progress (1951). His resistance to serialism (an attitude he reversed following Schoenberg's death) proved to be an inspiration for many[who?] other composers.[38]
A common trend throughout the 20th century, in both opera and general orchestral repertoire, is the use of smaller orchestras as a cost-cutting measure; the grand Romantic-era orchestras with huge string sections, multiple harps, extra horns, and exotic percussion instruments were no longer feasible. As government and private patronage of the arts decreased throughout the 20th century, new works were often commissioned and performed with smaller budgets, very often resulting in chamber-sized works, and short, one-act operas. Many of Benjamin Britten's operas are scored for as few as 13 instrumentalists; Mark Adamo's two-act realization of Little Women is scored for 18 instrumentalists.
Another feature of late 20th-century opera is the emergence of contemporary historical operas, in contrast to the tradition of basing operas on more distant history, the re-telling of contemporary fictional stories or plays, or on myth or legend. The Death of Klinghoffer, Nixon in China, and Doctor Atomic by John Adams, Dead Man Walking by Jake Heggie, and Anna Nicole by Mark-Anthony Turnage exemplify the dramatisation onstage of events in recent living memory, where characters portrayed in the opera were alive at the time of the premiere performance.
The Metropolitan Opera in the US (often known as the Met) reported in 2011 that the average age of its audience was 60.[39] Many opera companies attempted to attract a younger audience to halt the larger trend of greying audiences for classical music since the last decades of the 20th century.[40] Efforts resulted in lowering the average age of the Met's audience to 58 in 2018, the average age at Berlin State Opera was reported as 54, and Paris Opera reported an average age of 48.[41] New York Times critic Anthony Tommasini has suggested that "companies inordinately beholden to standard repertory" are not reaching younger, more curious audiences.[42]
Smaller companies in the US have a more fragile existence, and they usually depend on a "patchwork quilt" of support from state and local governments, local businesses, and fundraisers. Nevertheless, some smaller companies have found ways of drawing new audiences. In addition to radio and television broadcasts of opera performances, which have had some success in gaining new audiences, broadcasts of live performances to movie theatres have shown the potential to reach new audiences.[43]
By the late 1930s, some musicals began to be written with a more operatic structure. These works include complex polyphonic ensembles and reflect musical developments of their times. Porgy and Bess (1935), influenced by jazz styles, and Candide (1956), with its sweeping, lyrical passages and farcical parodies of opera, both opened on Broadway but became accepted as part of the opera repertory. Popular musicals such as Show Boat, West Side Story, Brigadoon, Sweeney Todd, Passion, Evita, The Light in the Piazza, The Phantom of the Opera and others tell dramatic stories through complex music and in the 2010s they are sometimes seen in opera houses.[44] The Most Happy Fella (1952) is quasi-operatic and has been revived by the New York City Opera. Other rock-influenced musicals, such as Tommy (1969) and Jesus Christ Superstar (1971), Les Misérables (1980), Rent (1996), Spring Awakening (2006), and Natasha, Pierre & The Great Comet of 1812 (2012) employ various operatic conventions, such as through composition, recitative instead of dialogue, and leitmotifs.
A subtle type of sound electronic reinforcement called acoustic enhancement is used in some modern concert halls and theatres where operas are performed. Although none of the major opera houses "...use traditional, Broadway-style sound reinforcement, in which most if not all singers are equipped with radio microphones mixed to a series of unsightly loudspeakers scattered throughout the theatre", many use a sound reinforcement system for acoustic enhancement and for subtle boosting of offstage voices, child singers, onstage dialogue, and sound effects (e.g., church bells in Tosca or thunder effects in Wagnerian operas).[45]
Operatic vocal technique evolved, in a time before electronic amplification, to allow singers to produce enough volume to be heard over an orchestra, without the instrumentalists having to substantially compromise their volume.
Singers and the roles they play are classified by voice type, based on the tessitura, agility, power and timbre of their voices. Male singers can be classified by vocal range as bass, bass-baritone, baritone, baritenor, tenor and countertenor, and female singers as contralto, mezzo-soprano and soprano. (Men sometimes sing in the "female" vocal ranges, in which case they are termed sopranist or countertenor. The countertenor is commonly encountered in opera, sometimes singing parts written for castrati—men neutered at a young age specifically to give them a higher singing range.) Singers are then further classified by size—for instance, a soprano can be described as a lyric soprano, coloratura, soubrette, spinto, or dramatic soprano. These terms, although not fully describing a singing voice, associate the singer's voice with the roles most suitable to the singer's vocal characteristics.
Yet another sub-classification can be made according to acting skills or requirements, for example the basso buffo who often must be a specialist in patter as well as a comic actor. This is carried out in detail in the Fach system of German speaking countries, where historically opera and spoken drama were often put on by the same repertory company.
A particular singer's voice may change drastically over his or her lifetime, rarely reaching vocal maturity until the third decade, and sometimes not until middle age. Two French voice types, premiere dugazon and deuxieme dugazon, were named after successive stages in the career of Louise-Rosalie Lefebvre (Mme. Dugazon). Other terms originating in the star casting system of the Parisian theatres are baryton-martin and soprano falcon.
The soprano voice has typically been used as the voice of choice for the female protagonist of the opera since the latter half of the 18th century. Earlier, it was common for that part to be sung by any female voice, or even a castrato. The current emphasis on a wide vocal range was primarily an invention of the Classical period. Before that, the vocal virtuosity, not range, was the priority, with soprano parts rarely extending above a high A (Handel, for example, only wrote one role extending to a high C), though the castrato Farinelli was alleged to possess a top D (his lower range was also extraordinary, extending to tenor C). The mezzo-soprano, a term of comparatively recent origin, also has a large repertoire, ranging from the female lead in Purcell's Dido and Aeneas to such heavyweight roles as Brangäne in Wagner's Tristan und Isolde (these are both roles sometimes sung by sopranos; there is quite a lot of movement between these two voice-types). For the true contralto, the range of parts is more limited, which has given rise to the insider joke that contraltos only sing "witches, bitches, and britches" roles. In recent years many of the "trouser roles" from the Baroque era, originally written for women, and those originally sung by castrati, have been reassigned to countertenors.
The tenor voice, from the Classical era onwards, has traditionally been assigned the role of male protagonist. Many of the most challenging tenor roles in the repertory were written during the bel canto era, such as Donizetti's sequence of 9 Cs above middle C during La fille du régiment. With Wagner came an emphasis on vocal heft for his protagonist roles, with this vocal category described as Heldentenor; this heroic voice had its more Italianate counterpart in such roles as Calaf in Puccini's Turandot. Basses have a long history in opera, having been used in opera seria in supporting roles, and sometimes for comic relief (as well as providing a contrast to the preponderance of high voices in this genre). The bass repertoire is wide and varied, stretching from the comedy of Leporello in Don Giovanni to the nobility of Wotan in Wagner's Ring Cycle, to the conflicted King Phillip of Verdi's Don Carlos. In between the bass and the tenor is the baritone, which also varies in weight from say, Guglielmo in Mozart's Così fan tutte to Posa in Verdi's Don Carlos; the actual designation "baritone" was not standard until the mid-19th century.
Early performances of opera were too infrequent for singers to make a living exclusively from the style, but with the birth of commercial opera in the mid-17th century, professional performers began to emerge. The role of the male hero was usually entrusted to a castrato, and by the 18th century, when Italian opera was performed throughout Europe, leading castrati who possessed extraordinary vocal virtuosity, such as Senesino and Farinelli, became international stars. The career of the first major female star (or prima donna), Anna Renzi, dates to the mid-17th century. In the 18th century, a number of Italian sopranos gained international renown and often engaged in fierce rivalry, as was the case with Faustina Bordoni and Francesca Cuzzoni, who started a fistfight with one another during a performance of a Handel opera. The French disliked castrati, preferring their male heroes to be sung by an haute-contre (a high tenor), of which Joseph Legros (1739–1793) was a leading example.[46]
Though opera patronage has decreased in the last century in favor of other arts and media (such as musicals, cinema, radio, television and recordings), mass media and the advent of recording have supported the popularity of many famous singers including Maria Callas, Enrico Caruso, Amelita Galli-Curci, Kirsten Flagstad, Mario Del Monaco, Renata Tebaldi, Risë Stevens, Alfredo Kraus, Franco Corelli, Montserrat Caballé, Joan Sutherland, Birgit Nilsson, Nellie Melba, Rosa Ponselle, Beniamino Gigli, Jussi Björling, Feodor Chaliapin, Cecilia Bartoli, Renée Fleming, Marilyn Horne, Bryn Terfel, Dmitri Hvorostovsky and The Three Tenors (Luciano Pavarotti, Plácido Domingo, José Carreras).
Before the 1700s, Italian operas used a small string orchestra, but it rarely played to accompany the singers. Opera solos during this period were accompanied by the basso continuo group, which consisted of the harpsichord, "plucked instruments" such as lute and a bass instrument.[47] The string orchestra typically only played when the singer was not singing, such as during a singer's "...entrances and exits, between vocal numbers, [or] for [accompanying] dancing". Another role for the orchestra during this period was playing an orchestral ritornello to mark the end of a singer's solo.[47] During the early 1700s, some composers began to use the string orchestra to mark certain aria or recitatives "...as special"; by 1720, most arias were accompanied by an orchestra. Opera composers such as Domenico Sarro, Leonardo Vinci, Giambattista Pergolesi, Leonardo Leo, and Johann Adolf Hasse added new instruments to the opera orchestra and gave the instruments new roles. They added wind instruments to the strings and used orchestral instruments to play instrumental solos, as a way to mark certain arias as special.[47]
The orchestra has also provided an instrumental overture before the singers come onstage since the 1600s. Peri's Euridice opens with a brief instrumental ritornello, and Monteverdi's L'Orfeo (1607) opens with a toccata, in this case a fanfare for muted trumpets. The French overture as found in Jean-Baptiste Lully's operas[48] consist of a slow introduction in a marked "dotted rhythm", followed by a lively movement in fugato style. The overture was frequently followed by a series of dance tunes before the curtain rose. This overture style was also used in English opera, most notably in Henry Purcell's Dido and Aeneas. Handel also uses the French overture form in some of his Italian operas such as Giulio Cesare.[49]
In Italy, a distinct form called "overture" arose in the 1680s, and became established particularly through the operas of Alessandro Scarlatti, and spread throughout Europe, supplanting the French form as the standard operatic overture by the mid-18th century.[50] It uses three generally homophonic movements: fast–slow–fast. The opening movement was normally in duple metre and in a major key; the slow movement in earlier examples was short, and could be in a contrasting key; the concluding movement was dance-like, most often with rhythms of the gigue or minuet, and returned to the key of the opening section. As the form evolved, the first movement may incorporate fanfare-like elements and took on the pattern of so-called "sonatina form" (sonata form without a development section), and the slow section became more extended and lyrical.[50]
In Italian opera after about 1800, the "overture" became known as the sinfonia.[51] Fisher also notes the term Sinfonia avanti l'opera (literally, the "symphony before the opera") was "an early term for a sinfonia used to begin an opera, that is, as an overture as opposed to one serving to begin a later section of the work".[51] In 19th-century opera, in some operas, the overture, Vorspiel, Einleitung, Introduction, or whatever else it may be called, was the portion of the music which takes place before the curtain rises; a specific, rigid form was no longer required for the overture.
The role of the orchestra in accompanying the singers changed over the 19th century, as the Classical style transitioned to the Romantic era. In general, orchestras got bigger, new instruments were added, such as additional percussion instruments (e.g., bass drum, cymbals, snare drum, etc.). The orchestration of orchestra parts also developed over the 19th century. In Wagnerian operas, the forefronting of the orchestra went beyond the overture. In Wagnerian operas such as the Ring Cycle, the orchestra often played the recurrent musical themes or leitmotifs, a role which gave a prominence to the orchestra which "...elevated its status to that of a prima donna".[52] Wagner's operas were scored with unprecedented scope and complexity, adding more brass instruments and huge ensemble sizes: indeed, his score to Das Rheingold calls for six harps. In Wagner and the work of subsequent composers, such as Benjamin Britten, the orchestra "often communicates facts about the story that exceed the levels of awareness of the characters therein." As a result, critics began to regard the orchestra as performing a role analogous to that of a literary narrator."[53]
As the role of the orchestra and other instrumental ensembles changed over the history of opera, so did the role of leading the musicians. In the Baroque era, the musicians were usually directed by the harpsichord player, although the French composer Lully is known to have conducted with a long staff. In the 1800s, during the Classical period, the first violinist, also known as the concertmaster, would lead the orchestra while sitting. Over time, some directors began to stand up and use hand and arm gestures to lead the performers. Eventually this role of music director became termed the conductor, and a podium was used to make it easier for all the musicians to see him or her. By the time Wagnerian operas were introduced, the complexity of the works and the huge orchestras used to play them gave the conductor an increasingly important role. Modern opera conductors have a challenging role: they have to direct both the orchestra in the orchestra pit and the singers on stage.
Since the days of Handel and Mozart, many composers have favored Italian as the language for the libretto of their operas. From the Bel Canto era to Verdi, composers would sometimes supervise versions of their operas in both Italian and French. Because of this, operas such as Lucia di Lammermoor or Don Carlos are today deemed canonical in both their French and Italian versions.[54]
Until the mid-1950s, it was acceptable to produce operas in translations even if these had not been authorized by the composer or the original librettists. For example, opera houses in Italy routinely staged Wagner in Italian.[55] After World War II, opera scholarship improved, artists refocused on the original versions, and translations fell out of favor. Knowledge of European languages, especially Italian, French, and German, is today an important part of the training for professional singers. "The biggest chunk of operatic training is in linguistics and musicianship", explains mezzo-soprano Dolora Zajick. "[I have to understand] not only what I'm singing, but what everyone else is singing. I sing Italian, Czech, Russian, French, German, English."[56]
In the 1980s, supertitles (sometimes called surtitles) began to appear. Although supertitles were first almost universally condemned as a distraction,[57] today many opera houses provide either supertitles, generally projected above the theatre's proscenium arch, or individual seat screens where spectators can choose from more than one language. TV broadcasts typically include subtitles even if intended for an audience who knows well the language (for example, a RAI broadcast of an Italian opera). These subtitles target not only the hard of hearing but the audience generally, since a sung discourse is much harder to understand than a spoken one—even in the ears of native speakers. Subtitles in one or more languages have become standard in opera broadcasts, simulcasts, and DVD editions.
Today, operas are only rarely performed in translation. Exceptions include the English National Opera, the Opera Theatre of Saint Louis, Opera Theater of Pittsburgh, and Opera South East,[58] which favor English translations.[59] Another exception are opera productions intended for a young audience, such as Humperdinck's Hansel and Gretel[60] and some productions of Mozart's The Magic Flute.[61]
Outside the US, and especially in Europe, most opera houses receive public subsidies from taxpayers.[62] In Milan, Italy, 60% of La Scala's annual budget of €115 million is from ticket sales and private donations, with the remaining 40% coming from public funds.[63] In 2005, La Scala received 25% of Italy's total state subsidy of €464 million for the performing arts.[64] In the UK, Arts Council England provides funds to Opera North, the Royal Opera House, Welsh National Opera, and English National Opera. Between 2012 and 2015, these four opera companies along with the English National Ballet, Birmingham Royal Ballet and Northern Ballet accounted for 22% of the funds in the Arts Council's national portfolio. During that period, the Council undertook an analysis of its funding for large-scale opera and ballet companies, setting recommendations and targets for the companies to meet prior to the 2015–2018 funding decisions.[65] In February 2015, concerns over English National Opera's business plan led to the Arts Council placing it "under special funding arrangements" in what The Independent termed "the unprecedented step" of threatening to withdraw public funding if the council's concerns were not met by 2017.[66] European public funding to opera has led to a disparity between the number of year-round opera houses in Europe and the United States. For example, "Germany has about 80 year-round opera houses [as of 2004], while the U.S., with more than three times the population, does not have any. Even the Met only has a seven-month season."[67]
A milestone for opera broadcasting in the U.S. was achieved on 24 December 1951, with the live broadcast of Amahl and the Night Visitors, an opera in one act by Gian Carlo Menotti. It was the first opera specifically composed for television in America.[68] Another milestone occurred in Italy in 1992 when Tosca was broadcast live from its original Roman settings and times of the day: the first act came from the 16th-century Church of Sant'Andrea della Valle at noon on Saturday; the 16th-century Palazzo Farnese was the setting for the second at 8:15 pm; and on Sunday at 6 am, the third act was broadcast from Castel Sant'Angelo. The production was transmitted via satellite to 105 countries.[69]
Major opera companies have begun presenting their performances in local cinemas throughout the United States and many other countries. The Metropolitan Opera began a series of live high-definition video transmissions to cinemas around the world in 2006.[70] In 2007, Met performances were shown in over 424 theaters in 350 U.S. cities. La bohème went out to 671 screens worldwide. San Francisco Opera began prerecorded video transmissions in March 2008. As of June 2008, approximately 125 theaters in 117 U.S. cities carry the showings. The HD video opera transmissions are presented via the same HD digital cinema projectors used for major Hollywood films.[71] European opera houses and festivals including the Royal Opera in London, La Scala in Milan, the Salzburg Festival, La Fenice in Venice, and the Maggio Musicale in Florence have also transmitted their productions to theaters in cities around the world since 2006, including 90 cities in the U.S.[72][73]
The emergence of the Internet has also affected the way in which audiences consume opera. In 2009 the British Glyndebourne Festival Opera offered for the first time an online digital video download of its complete 2007 production of Tristan und Isolde. In the 2013 season, the festival streamed all six of its productions online.[74][75] In July 2012, the first online community opera was premiered at the Savonlinna Opera Festival. Titled Free Will, it was created by members of the Internet group Opera By You. Its 400 members from 43 countries wrote the libretto, composed the music, and designed the sets and costumes using the Wreckamovie web platform. Savonlinna Opera Festival provided professional soloists, an 80-member choir, a symphony orchestra, and the stage machinery. It was performed live at the festival and streamed live on the internet.[76]
Sources



Karate (空手) (/kəˈrɑːti/; Japanese pronunciation: [kaɾate] (listen); Okinawan pronunciation: [kaɽati]), also Karate-do (空手道, Karate-dō) is a martial art developed in the Ryukyu Kingdom. It developed from the indigenous Ryukyuan martial arts (called te (手), "hand"; tii in Okinawan) under the influence of Chinese martial arts, particularly Fujian White Crane.[1][2] Karate is now predominantly a striking art using punching, kicking, knee strikes, elbow strikes, and open-hand techniques such as knife-hands, spear-hands, and palm-heel strikes. Historically, and in some modern styles, grappling, throws, joint locks, restraints, and vital-point strikes are also taught.[3] A karate practitioner is called a karate-ka (空手家).
The Empire of Japan annexed the Ryukyu Kingdom in 1879. Karate came to mainland Japan in the early 20th century during a time of migration as Ryukyuans, especially from Okinawa, looked for work in the main islands of Japan.[4] It was systematically taught in Japan after the Taishō era of 1912–1926.[5] In 1922, the Japanese Ministry of Education invited Gichin Funakoshi to Tokyo to give a karate demonstration. In 1924, Keio University established the first university karate club in mainland Japan, and by 1932 major Japanese universities had karate clubs.[6] In this era of escalating Japanese militarism,[7] the name was changed from 唐手 ("Chinese hand" or "Tang hand")[8] to 空手 ("empty hand") – both of which are pronounced karate in Japanese – to indicate that the Japanese wished to develop the combat form in Japanese style.[9] After World War II, Okinawa became (1945) an important United States military site and karate became popular among servicemen stationed there.[10][11]
The martial arts movies of the 1960s and 1970s served to greatly increase the popularity of martial arts around the world, and English-speakers began to use the word karate in a generic way to refer to all striking-based Asian martial arts.[12] Karate schools (dōjōs) began appearing around the world, catering to those with casual interest as well as those seeking a deeper study of the art.
Shigeru Egami, Chief Instructor of the Shotokan dōjō, opined that "the majority of followers of karate in overseas countries pursue karate only for its fighting techniques ... Movies and television ... depict karate as a mysterious way of fighting capable of causing death or injury with a single blow ... the mass media present a pseudo art far from the real thing."[13] Shōshin Nagamine said: "Karate may be considered as the conflict within oneself or as a life-long marathon which can be won only through self-discipline, hard training and one's own creative efforts."[14]
On 28 September 2015 karate featured on a shortlist (along with baseball, softball, skateboarding, surfing, and sport climbing) for consideration for inclusion in the 2020 Summer Olympics. On 1 June 2016 the International Olympic Committee's executive board announced they were supporting the inclusion of all five sports (counting baseball and softball as only one sport) for inclusion in the 2020 Games.
Web Japan (sponsored by the Japanese Ministry of Foreign Affairs) claims that karate has 50 million practitioners worldwide,[15] while the World Karate Federation claims there are 100 million practitioners around the world.[16]
Karate was originally written as "Chinese hand" (唐手, literally "Tang dynasty hand") in kanji. It was changed to a homophone meaning empty hand (空手) in 1935. The original use of the word "karate" in print is attributed to Ankō Itosu; he wrote it as "唐手". The Tang Dynasty of China ended in AD 907, but the kanji representing it remains in use in Japanese language referring to China generally, in such words as "唐人街" meaning Chinatown. Thus the word "karate" was originally a way of expressing "martial art from China."
Since there are no written records it is not known definitely whether the kara in karate was originally written with the character 唐 meaning China or the character 空 meaning empty. During the time when admiration for China and things Chinese was at its height in the Ryūkyūs it was the custom to use the former character when referring to things of fine quality. Influenced by this practice, in recent times karate has begun to be written with the character 唐 to give it a sense of class or elegance.The first documented use of a homophone of the logogram pronounced kara by replacing the Chinese character meaning "Tang Dynasty" with the character meaning "empty"  took place in Karate Kumite written in August 1905 by Chōmo Hanashiro (1869–1945). Sino-Japanese relations have never been very good and especially at the time of the Japanese invasion of Manchuria, referring to the Chinese origins of karate was considered politically incorrect.[18]
In 1933, the Okinawan art of karate was recognized as a Japanese martial art by the Japanese Martial Arts Committee known as the "Butoku Kai". Until 1935, "karate" was written as "唐手" (Chinese hand). But in 1935, the masters of the various styles of Okinawan karate conferred to decide a new name for their art. They decided to call their art "karate"  written in Japanese characters as "空手" (empty hand).[19]Another nominal development is the addition of dō (道:どう) to the end of the word karate. Dō is a suffix having numerous meanings including road, path, route and way. It is used in many martial arts that survived Japan's transition from feudal culture to modern times. It implies that these arts are not just fighting systems but contain spiritual elements when promoted as disciplines. In this context dō is usually translated as "the way of ___". Examples include aikido, judo, kyūdō and kendo. Thus karatedō is more than just empty hand techniques. It is "The Way of the Empty Hand".
Karate began as a common fighting system known as te (Okinawan: ti) among the Pechin class of the Ryukyuans. After trade relationships were established with the Ming dynasty of China in 1372 by King Satto of Chūzan, some forms of Chinese martial arts were introduced to the Ryukyu Islands by the visitors from China, particularly Fujian Province. A large group of Chinese families moved to Okinawa around 1392 for the purpose of cultural exchange, where they established the community of Kumemura and shared their knowledge of a wide variety of Chinese arts and sciences, including the Chinese martial arts. The political centralization of Okinawa by King Shō Hashi in 1429 and the policy of banning weapons by King Shō Shin in 1477, later enforced in Okinawa after the invasion by the Shimazu clan in 1609, are also factors that furthered the development of unarmed combat techniques in Okinawa.[2]
There were few formal styles of te, but rather many practitioners with their own methods. One surviving example is the Motobu-ryū school passed down from the Motobu family by Seikichi Uehara.[20] Early styles of karate are often generalized as Shuri-te, Naha-te, and Tomari-te, named after the three cities from which they emerged.[19] Each area and its teachers had particular kata, techniques, and principles that distinguished their local version of te from the others.
Members of the Okinawan upper classes were sent to China regularly to study various political and practical disciplines. The incorporation of empty-handed Chinese Kung Fu into Okinawan martial arts occurred partly because of these exchanges and partly because of growing legal restrictions on the use of weaponry. Traditional karate kata bear a strong resemblance to the forms found in Fujian martial arts such as Fujian White Crane, Five Ancestors, and Gangrou-quan (Hard Soft Fist; pronounced "Gōjūken" in Japanese).[21] Many Okinawan weapons such as the sai, tonfa, and nunchaku may have originated in and around Southeast Asia.[citation needed]
Sakukawa Kanga (1786–1867) had studied pugilism and staff (bo) fighting in China (according to one legend, under the guidance of Kosokun, originator of kusanku kata). In 1806, he started teaching a fighting art in the city of Shuri that he called "Tudi Sakukawa," which meant "Sakukawa of China Hand." This was the first known recorded reference to the art of "Tudi," written as 唐手. Around the 1820s Sakukawa's most significant student Matsumura Sōkon (1809–1899) taught a synthesis of te (Shuri-te and Tomari-te) and Shaolin (Chinese 少林) styles.[22] Matsumura's style would later become the Shōrin-ryū style.
Matsumura taught his art to Itosu Ankō (1831–1915) among others. Itosu adapted two forms he had learned from Matsumura. These are kusanku and chiang nan.[23] He created the ping'an forms ("heian" or "pinan" in Japanese) which are simplified kata for beginning students. In 1901, Itosu helped to get karate introduced into Okinawa's public schools. These forms were taught to children at the elementary school level. Itosu's influence in karate is broad. The forms he created are common across nearly all styles of karate. His students became some of the most well-known karate masters, including Gichin Funakoshi, Kenwa Mabuni, and Chōki Motobu. Itosu is sometimes referred to as "the Grandfather of Modern Karate."[24]
In 1881, Higaonna Kanryō returned from China after years of instruction with Ryu Ryu Ko and founded what would become Naha-te. One of his students was the founder of Gojū-ryū, Chōjun Miyagi. Chōjun Miyagi taught such well-known karateka as Seko Higa (who also trained with Higaonna), Meitoku Yagi, Miyazato Ei'ichi, and Seikichi Toguchi, and for a very brief time near the end of his life, An'ichi Miyagi (a teacher claimed by Morio Higaonna).

In addition to the three early te styles of karate a fourth Okinawan influence is that of Uechi Kanbun (1877–1948). At the age of 20 he went to Fuzhou in Fujian Province, China, to escape Japanese military conscription. While there he studied under Shū Shiwa (Chinese: Zhou Zihe 周子和 1874–1926).[25] He was a leading figure of Chinese Nanpa Shorin-ken style at that time.[26] He later developed his own style of Uechi-ryū karate based on the Sanchin, Seisan, and Sanseiryu kata that he had studied in China.[27]Gichin Funakoshi, the founder of Shotokan karate, is generally credited with having introduced and popularized karate on the main islands of Japan. In addition, many Okinawans were actively teaching, and are thus also responsible for the development of karate on the main islands. Funakoshi was a student of both Asato Ankō and Itosu Ankō (who had worked to introduce karate to the Okinawa Prefectural School System in 1902). During this time period, prominent teachers who also influenced the spread of karate in Japan included Kenwa Mabuni, Chōjun Miyagi, Chōki Motobu, Kanken Tōyama, and Kanbun Uechi. This was a turbulent period in the history of the region. It includes Japan's annexation of the Okinawan island group in 1872, the First Sino-Japanese War (1894–1895), the Russo-Japanese War (1904–1905), the annexation of Korea, and the rise of Japanese militarism (1905–1945).
Japan was invading China at the time, and Funakoshi knew that the art of Tang/China hand would not be accepted; thus the change of the art's name to "way of the empty hand." The dō suffix implies that karatedō is a path to self-knowledge, not just a study of the technical aspects of fighting. Like most martial arts practised in Japan, karate made its transition from -jutsu to -dō around the beginning of the 20th century. The "dō" in "karate-dō" sets it apart from karate-jutsu, as aikido is distinguished from aikijutsu, judo from jujutsu, kendo from kenjutsu and iaido from iaijutsu.
Funakoshi changed the names of many kata and the name of the art itself (at least on mainland Japan), doing so to get karate accepted by the Japanese budō organization Dai Nippon Butoku Kai. Funakoshi also gave Japanese names to many of the kata. The five pinan forms became known as heian, the three naihanchi forms became known as tekki, seisan as hangetsu, Chintō as gankaku, wanshu as enpi, and so on. These were mostly political changes, rather than changes to the content of the forms, although Funakoshi did introduce some such changes. Funakoshi had trained in two of the popular branches of Okinawan karate of the time, Shorin-ryū and Shōrei-ryū. In Japan he was influenced by kendo, incorporating some ideas about distancing and timing into his style. He always referred to what he taught as simply karate, but in 1936 he built a dōjō in Tokyo and the style he left behind is usually called Shotokan after this dōjō. Shoto, meaning "pine wave", was Funakoshi's pen name and kan meaning "hall".
The modernization and systemization of karate in Japan also included the adoption of the white uniform that consisted of the kimono and the dogi or keikogi—mostly called just karategi—and coloured belt ranks. Both of these innovations were originated and popularized by Jigoro Kano, the founder of judo and one of the men Funakoshi consulted in his efforts to modernize karate.
A new form of karate called Kyokushin was formally founded in 1957 by Masutatsu Oyama (who was born a Korean, Choi Yeong-Eui 최영의). Kyokushin is largely a synthesis of Shotokan and Gōjū-ryū. It teaches a curriculum that emphasizes aliveness, physical toughness, and full contact sparring. Because of its emphasis on physical, full-force sparring, Kyokushin is now often called "full contact karate", or "Knockdown karate" (after the name for its competition rules). Many other karate organizations and styles are descended from the Kyokushin curriculum.
Karate can be practiced as an art (budō), self defense or as a combat sport. Traditional karate places emphasis on self-development (budō).[28] Modern Japanese style training emphasizes the psychological elements incorporated into a proper kokoro (attitude) such as perseverance, fearlessness, virtue, and leadership skills. Sport karate places emphasis on exercise and competition. Weapons are an important training activity in some styles of karate.
Karate training is commonly divided into kihon (basics or fundamentals), kata (forms), and kumite (sparring).
Kihon means basics and these form the base for everything else in the style including stances, strikes, punches, kicks and blocks. Karate styles place varying importance on kihon. Typically this is training in unison of a technique or a combination of techniques by a group of karateka. Kihon may also be prearranged drills in smaller groups or in pairs.
Kata (型:かた) means literally "shape" or "model." Kata is a formalized sequence of movements which represent various offensive and defensive postures. These postures are based on idealized combat applications. The applications when applied in a demonstration with real opponents is referred to as a Bunkai. The Bunkai shows how every stance and movement is used. Bunkai is a useful tool to understand a kata.
To attain a formal rank the karateka must demonstrate competent performance of specific required kata for that level. The Japanese terminology for grades or ranks is commonly used. Requirements for examinations vary among schools.
Sparring in Karate is called kumite (組手:くみて). It literally means "meeting of hands." Kumite is practiced both as a sport and as self-defense training.
Levels of physical contact during sparring vary considerably. Full contact karate has several variants. Knockdown karate (such as Kyokushin) uses full power techniques to bring an opponent to the ground. Sparring in armour, bogu kumite, allows full power techniques with some safety. Sport kumite in many international competition under the World Karate Federation is free or structured with light contact or semi contact and points are awarded by a referee.
In structured kumite (yakusoku, prearranged), two participants perform a choreographed series of techniques with one striking while the other blocks. The form ends with one devastating technique (hito tsuki).
In free sparring (Jiyu Kumite), the two participants have a free choice of scoring techniques. The allowed techniques and contact level are primarily determined by sport or style organization policy, but might be modified according to the age, rank and sex of the participants. Depending upon style, take-downs, sweeps and in some rare cases even time-limited grappling on the ground are also allowed.
Free sparring is performed in a marked or closed area. The bout runs for a fixed time (2 to 3 minutes.) The time can run continuously (iri kume) or be stopped for referee judgment. In light contact or semi contact kumite, points are awarded based on the criteria: good form, sporting attitude, vigorous application, awareness/zanshin, good timing and correct distance.
In full contact karate kumite, points are based on the results of the impact, rather than the formal appearance of the scoring technique.
In the bushidō tradition dōjō kun is a set of guidelines for karateka to follow. These guidelines apply both in the dōjō (training hall) and in everyday life.
Okinawan karate uses supplementary training known as hojo undo. This utilizes simple equipment made of wood and stone. The makiwara is a striking post. The nigiri game is a large jar used for developing grip strength. These supplementary exercises are designed to increase strength, stamina, speed, and muscle coordination.[29] Sport Karate emphasizes aerobic exercise, anaerobic exercise, power, agility, flexibility, and stress management.[30] All practices vary depending upon the school and the teacher.
Gichin Funakoshi (船越 義珍) said, "There are no contests in karate."[31] In pre–World War II Okinawa, kumite was not part of karate training.[32] Shigeru Egami relates that, in 1940, some karateka were ousted from their dōjō because they adopted sparring after having learned it in Tokyo.[33]
Karate is divided into style organizations.[34] These organizations sometimes cooperate in non-style specific sport karate organizations or federations. Examples of sport organizations include AAKF/ITKF, AOK, TKL, AKA, WKF, NWUKO, WUKF and WKC.[35] Organizations hold competitions (tournaments) from local to international level. Tournaments are designed to match members of opposing schools or styles against one another in kata, sparring and weapons demonstration. They are often separated by age, rank and sex with potentially different rules or standards based on these factors. The tournament may be exclusively for members of a particular style (closed) or one in which any martial artist from any style may participate within the rules of the tournament (open).
The World Karate Federation (WKF) is the largest sport karate organization and is recognized by the International Olympic Committee (IOC) as being responsible for karate competition in the Olympic Games.[36] The WKF has developed common rules governing all styles. The national WKF organizations coordinate with their respective National Olympic Committees. WKF has appointed Shin Koyamada as its first Goodwill Ambassador in 2022 to promote Karate globally.[37]
WKF karate competition has two disciplines: sparring (kumite) and forms (kata).[38] Competitors may enter either as individuals or as part of a team. Evaluation for kata and kobudō is performed by a panel of judges, whereas sparring is judged by a head referee, usually with assistant referees at the side of the sparring area. Sparring matches are typically divided by weight, age, gender, and experience.[39]
WKF only allows membership through one national organization/federation per country to which clubs may join. The World Union of Karate-do Federations (WUKF)[40] offers different styles and federations a world body they may join, without having to compromise their style or size. The WUKF accepts more than one federation or association per country.
Sport organizations use different competition rule systems.[34][39][41][42][43] Light contact rules are used by the WKF, WUKO, IASK and WKC. Full contact karate rules used by Kyokushinkai, Seidokaikan and other organizations. Bogu kumite (full contact with protective shielding of targets) rules are used in the World Koshiki Karate-Do Federation organization.[44] Shinkaratedo Federation use boxing gloves.[45] Within the United States, rules may be under the jurisdiction of state sports authorities, such as the boxing commission.
In August 2016, the International Olympic Committee approved karate as an Olympic sport beginning at the 2020 Summer Olympics.[46][47] Karate also debuted at the 2018 Summer Youth Olympics. During this debut of Karate in the Summer Olympics, sixty competitors from around the world competed in the Kumite competition, and twenty competed in the Kata competition. In September 2015, karate was included in a shortlist along with baseball, softball, skateboarding, surfing, and sport climbing to be considered for inclusion in the 2020 Summer Olympics;[48] and in June 2016, the Executive Board of the International Olympic Committee (IOC) announced that they would support the proposal to include all of the shortlisted sports in the 2020 Games.[49] Finally, on 3 August 2016, all five sports (counting baseball and softball together as one sport) were approved for inclusion in the 2020 Olympic program.[50] Karate will not be included in the 2024 Olympic Games and there is no word yet if it will be included in a future Olympic Games.[51]
Karate, although not widely used in mixed martial arts, has been effective for some MMA practitioners.[52][53]
Various styles of karate are practiced in MMA: Lyoto Machida and John Makdessi practice Shotokan;[54] Bas Rutten and Georges St-Pierre train in Kyokushin;[55] Michelle Waterson holds a black belt in American Free Style Karate;[56] Stephen Thompson practices American Kenpo Karate;[57] and Robert Whittaker practices Gōjū-ryū.[58]
In 1924, Gichin Funakoshi, founder of Shotokan Karate, adopted the Dan system from the judo founder Jigoro Kano[59] using a rank scheme with a limited set of belt colors. Other Okinawan teachers also adopted this practice. In the Kyū/Dan system the beginner grades start with a higher numbered kyū (e.g., 10th Kyū or Jukyū) and progress toward a lower numbered kyū. The Dan progression continues from 1st Dan (Shodan, or 'beginning dan') to the higher dan grades. Kyū-grade karateka are referred to as "color belt" or mudansha ("ones without dan/rank"). Dan-grade karateka are referred to as yudansha (holders of dan/rank). Yudansha typically wear a black belt. Normally, the first five to six dans are given by examination by superior dan holders, while the subsequent (7 and up) are honorary, given for special merits and/or age reached.
Requirements of rank differ among styles, organizations, and schools. Kyū ranks stress stance, balance, and coordination. Speed and power are added at higher grades.
Minimum age and time in rank are factors affecting promotion. Testing consists of demonstration of techniques before a panel of examiners. This will vary by school, but testing may include everything learned at that point, or just new information. The demonstration is an application for new rank (shinsa) and may include kata, bunkai, self-defense, routines, tameshiwari (breaking), and kumite (sparring).
In Karate-Do Kyohan, Funakoshi quoted from the Heart Sutra, which is prominent in Shingon Buddhism: "Form is emptiness, emptiness is form itself" (shiki zokuze kū kū zokuze shiki).[60]
He interpreted the "kara" of Karate-dō to mean "to purge oneself of selfish and evil thoughts ... for only with a clear mind and conscience can the practitioner understand the knowledge which he receives."  Funakoshi believed that one should be "inwardly humble and outwardly gentle." Only by behaving humbly can one be open to Karate's many lessons. This is done by listening and being receptive to criticism. He considered courtesy of prime importance. He said that "Karate is properly applied only in those rare situations in which one really must either down another or be downed by him." Funakoshi did not consider it unusual for a devotee to use Karate in a real physical confrontation no more than perhaps once in a lifetime. He stated that Karate practitioners must "never be easily drawn into a fight." It is understood that one blow from a real expert could mean death. It is clear that those who misuse what they have learned bring dishonor upon themselves. He promoted the character trait of personal conviction. In "time of grave public crisis, one must have the courage ... to face a million and one opponents."  He taught that indecisiveness is a weakness.[61]
Karate is divided into many styles, each with their different training methods, focuses, and cultures; though they mainly originate from the historical Okinawan parent styles of Naha-te, Tomari-te and Shuri-te. In the modern era the major four styles of karate are considered to be Gōjū-ryū, Shotokan, Shitō-ryū, and Wadō-ryū.[62] These four styles are those recognised by the World Karate Federation for international kata competition.[63]
Other internationally recognised styles include but are not limited to:
[64][65]
Karate has grown in popularity in Africa, particularly in South Africa and Ghana.[66][67][68]
Karate began in Canada in the 1930s and 1940s as Japanese people immigrated to the country. Karate was practised quietly without a large amount of organization. During the Second World War, many Japanese-Canadian families were moved to the interior of British Columbia. Masaru Shintani, at the age of 13, began to study Shorin-Ryu karate in the Japanese camp under Kitigawa. In 1956, after 9 years of training with Kitigawa, Shintani travelled to Japan and met Hironori Otsuka (Wado Ryu). In 1958, Otsuka invited Shintani to join his organization Wado Kai, and in 1969 he asked Shintani to officially call his style Wado.[69]
In Canada during this same time, karate was also introduced by Masami Tsuruoka who had studied in Japan in the 1940s under Tsuyoshi Chitose.[70] In 1954, Tsuruoka initiated the first karate competition in Canada and laid the foundation for the National Karate Association.[70]
In the late 1950s Shintani moved to Ontario and began teaching karate and judo at the Japanese Cultural Centre in Hamilton. In 1966, he began (with Otsuka's endorsement) the Shintani Wado Kai Karate Federation. During the 1970s Otsuka appointed Shintani the Supreme Instructor of Wado Kai in North America. In 1979, Otsuka publicly promoted Shintani to hachidan (8th dan) and privately gave him a kudan certificate (9th dan), which was revealed by Shintani in 1995. Shintani and Otsuka visited each other in Japan and Canada several times, the last time in 1980 two years prior to Otsuka's death. Shintani died 7 May 2000.[69]
After World War II, members of the United States military learned karate in Okinawa or Japan and then opened schools in the US. In 1945, Robert Trias opened the first dōjō in the United States in Phoenix, Arizona, a Shuri-ryū karate dōjō.[71] In the 1950s, William J. Dometrich, Ed Parker, Cecil T. Patterson, Gordon Doversola, Harold G. Long, Donald Hugh Nagle, George Mattson and Peter Urban all began instructing in the US.
Tsutomu Ohshima began studying karate under Shotokan's founder, Gichin Funakoshi, while a student at Waseda University, beginning in 1948. In 1957, Ohshima received his godan (fifth-degree black belt), the highest rank awarded by Funakoshi. He founded the first university karate club in the United States at California Institute of Technology in 1957. In 1959, he founded the Southern California Karate Association (SCKA) which was renamed Shotokan Karate of America (SKA) in 1969.
In the 1960s, Anthony Mirakian, Richard Kim, Teruyuki Okazaki, John Pachivas, Allen Steen, Gosei Yamaguchi (son of Gōgen Yamaguchi), Michael G. Foster and Pat Burleson began teaching martial arts around the country.[72]
In 1961, Hidetaka Nishiyama, a co-founder of the Japan Karate Association (JKA) and student of Gichin Funakoshi, began teaching in the United States. He founded the International Traditional Karate Federation (ITKF). Takayuki Mikami was sent to New Orleans by the JKA in 1963.
In 1964, Takayuki Kubota relocated the International Karate Association from Tokyo to California.
Due to past conflict between Korea and Japan, most notably during the Japanese occupation of Korea in the early 20th century, the influence of karate in Korea is a contentious issue.[73] From 1910 until 1945, Korea was annexed by the Japanese Empire. It was during this time that many of the Korean martial arts masters of the 20th century were exposed to Japanese karate. After regaining independence from Japan, many Korean martial arts schools that opened up in the 1940s and 1950s were founded by masters who had trained in karate in Japan as part of their martial arts training.
Won Kuk Lee, a Korean student of Funakoshi, founded the first martial arts school after the Japanese occupation of Korea ended in 1945, called the Chung Do Kwan. Having studied under Gichin Funakoshi at Chuo University, Lee had incorporated taekkyon, kung fu, and karate in the martial art that he taught which he called "Tang Soo Do", the Korean transliteration of the Chinese characters for "Way of Chinese Hand" (唐手道).[74] In the mid-1950s, the martial arts schools were unified under President Rhee Syngman's order, and became taekwondo under the leadership of Choi Hong Hi and a committee of Korean masters. Choi, a significant figure in taekwondo history, had also studied karate under Funakoshi. Karate also provided an important comparative model for the early founders of taekwondo in the formalization of their art including hyung and the belt ranking system. The original taekwondo hyung were identical to karate kata. Eventually, original Korean forms were developed by individual schools and associations. Although the World Taekwondo Federation and International Taekwon-Do Federation are the most prominent among Korean martial arts organizations, tang soo do schools that teach Japanese karate still exist as they were originally conveyed to Won Kuk Lee and his contemporaries from Funakoshi.
Karate appeared in the Soviet Union in the mid-1960s, during Nikita Khrushchev's policy of improved international relations. The first Shotokan clubs were opened in Moscow's universities.[75] In 1973, however, the government banned karate—together with all other foreign martial arts—endorsing only the Soviet martial art of sambo.[76][77] Failing to suppress these uncontrolled groups, the USSR's Sport Committee formed the Karate Federation of USSR in December 1978.[78] On 17 May 1984, the Soviet Karate Federation was disbanded and all karate became illegal again. In 1989, karate practice became legal again, but under strict government regulations, only after the dissolution of the Soviet Union in 1991 did independent karate schools resume functioning, and so federations were formed and national tournaments in authentic styles began.[79][80]
In the 1950s and 1960s, several Japanese karate masters began to teach the art in Europe, but it was not until 1965 that the Japan Karate Association (JKA) sent to Europe four well-trained young Karate instructors Taiji Kase, Keinosuke Enoeda, Hirokazu Kanazawa and Hiroshi Shirai.[citation needed] Kase went to France, Enoeada to England and Shirai in Italy. These Masters maintained always a strong link between them, the JKA and the others JKA masters in the world, especially Hidetaka Nishiyama in the US
France Shotokan Karate was created in 1964 by Tsutomu Ohshima. It is affiliated with another of his organizations, Shotokan Karate of America (SKA). However, in 1965 Taiji Kase came from Japan along with Enoeda and Shirai, who went to England and Italy respectively, and karate came under the influence of the JKA.
Hiroshi Shirai, one of the original instructors sent by the JKA to Europe along with Kase, Enoeda and Kanazawa, moved to Italy in 1965 and quickly established a Shotokan enclave that spawned several instructors who in their turn soon spread the style all over the country. By 1970 Shotokan karate was the most spread martial art in Italy apart from Judo. Other styles such as Wado Ryu, Goju Ryu and Shito Ryu, are present and well established in Italy, while Shotokan remains the most popular.
Vernon Bell, a 3rd Dan Judo instructor who had been instructed by Kenshiro Abbe introduced Karate to England in 1956, having attended classes in Henry Plée's Yoseikan dōjō in Paris. Yoseikan had been founded by Minoru Mochizuki, a master of multiple Japanese martial arts, who had studied Karate with Gichin Funakoshi, thus the Yoseikan style was heavily influenced by Shotokan.[81] Bell began teaching in the tennis courts of his parents' back garden in Ilford, Essex and his group was to become the British Karate Federation. On 19 July 1957, Vietnamese Hoang Nam 3rd Dan, billed as "Karate champion of Indo China", was invited to teach by Bell at Maybush Road, but the first instructor from Japan was Tetsuji Murakami (1927–1987) a 3rd Dan Yoseikan under Minoru Mochizuki and 1st Dan of the JKA, who arrived in England in July 1959.[81] In 1959, Frederick Gille set up the Liverpool branch of the British Karate Federation, which was officially recognised in 1961. The Liverpool branch was based at Harold House Jewish Boys Club in Chatham Street before relocating to the YMCA in Everton where it became known as the Red Triangle. One of the early members of this branch was Andy Sherry who had previously studied Jujutsu with Jack Britten. In 1961, Edward Ainsworth, another blackbelt Judoka, set up the first Karate study group in Ayrshire, Scotland having attended Bell's third 'Karate Summer School' in 1961.[81]
Outside of Bell's organisation, Charles Mack traveled to Japan and studied under Masatoshi Nakayama of the Japan Karate Association who graded Mack to 1st Dan Shotokan on 4 March 1962 in Japan.[81] Shotokai Karate was introduced to England in 1963 by another of Gichin Funakoshi's students, Mitsusuke Harada.[81] Outside of the Shotokan stable of karate styles, Wado Ryu Karate was also an early adopted style in the UK, introduced by Tatsuo Suzuki, a 6th Dan at the time in 1964.
Despite the early adoption of Shotokan in the UK, it was not until 1964 that JKA Shotokan officially came to the UK. Bell had been corresponding with the JKA in Tokyo asking for his grades to be ratified in Shotokan having apparently learnt that Murakami was not a designated representative of the JKA. The JKA obliged, and without enforcing a grading on Bell, ratified his black belt on 5 February 1964, though he had to relinquish his Yoseikan grade. Bell requested a visitation from JKA instructors and the next year Taiji Kase, Hirokazu Kanazawa, Keinosuke Enoeda and Hiroshi Shirai gave the first JKA demo at the old Kensington Town Hall on 21 April 1965. Hirokazu Kanazawa and Keinosuke Enoeda stayed and Murakami left (later re-emerging as a 5th Dan Shotokai under Harada).[81]
In 1966, members of the former British Karate Federation established the Karate Union of Great Britain (KUGB) under Hirokazu Kanazawa as chief instructor[82] and affiliated to JKA. Keinosuke Enoeda came to England at the same time as Kanazawa, teaching at a dōjō in Liverpool. Kanazawa left the UK after 3 years and Enoeda took over. After Enoeda's death in 2003, the KUGB elected Andy Sherry as Chief Instructor. Shortly after this, a new association split off from KUGB, JKA England.
An earlier significant split from the KUGB took place in 1991 when a group led by KUGB senior instructor Steve Cattle formed the English Shotokan Academy (ESA). The aim of this group was to follow the teachings of Taiji Kase, formerly the JKA chief instructor in Europe, who along with Hiroshi Shirai created the World Shotokan Karate-do Academy (WKSA), in 1989 in order to pursue the teaching of "Budo" karate as opposed to what he viewed as "sport karate". Kase sought to return the practice of Shotokan Karate to its martial roots, reintroducing amongst other things open hand and throwing techniques that had been side lined as the result of competition rules introduced by the JKA. Both the ESA and the WKSA (renamed the Kase-Ha Shotokan-Ryu Karate-do Academy (KSKA) after Kase's death in 2004) continue following this path today.
In 1975, Great Britain became the first team ever to take the World male team title from Japan after being defeated the previous year in the final.
The World Karate Federation was first introduced to Oceania as the Oceania Karate Federation 1973.[83]
The Australian Karate Federation, under the World Karate Federation, was first introduced in 1970. In 1972 Frank Novak became the first fully qualified Shotokan instructor to arrive in Australia and teach in the country,[84] establishing the first Shotokan Karate dojo in Australia.[85] At karate's debut in the Olympics at the 2020 Summer Olympics, Tsuneari Yahiro became Australia's first Karate Olympian.[86]
Karate spread rapidly in the West through popular culture. In 1950s popular fiction, karate was at times described to readers in near-mythical terms, and it was credible to show Western experts of unarmed combat as unaware of Eastern martial arts of this kind.[87][better source needed] Following the inclusion of judo at the 1964 Tokyo Olympics, there was growing mainstream Western interest in Japanese martial arts, particularly karate, during the 1960s.[88] By the 1970s, martial arts films (especially kung fu films and Bruce Lee flicks from Hong Kong) had formed a mainstream genre and launched the "kung fu craze" which propelled karate and other Asian martial arts into mass popularity. However, mainstream Western audiences at the time generally did not distinguish between different Asian martial arts such as karate, kung fu and tae kwon do.[57]
In the film series 007 (1953-present), the main protagonist James Bond is exceptionally skillful in martial arts. He is an expert in various types of martial arts including Karate, as well as Judo, Aikido, Brazilian Jiujitsu, Filipino Eskrima and Krav Maga.
During the late 20th century, specifically during the 80s and 90s, karate saw a rise in mainstream popularity. America in the 80s took hold of the martial arts craze and began to produce more homegrown films in the martial arts genre.[89] Films weren't the only popular visual representation of Karate in the 80s, just as arcades grew in popularity, so did Karate in arcade fighting games. The first video game to feature fist fighting was Heavyweight Champ in 1976,[90] but it was Karate Champ that popularized the one-on-one fighting game genre in arcades in 1984. In 1987, Capcom released Street Fighter, featuring multiple Karateka characters.[91][92]
The Karate Kid (1984) and its sequels The Karate Kid, Part II (1986), The Karate Kid, Part III (1989) and The Next Karate Kid (1994) are films relating the fictional story of an American adolescent's introduction into karate.[93][94] Its television sequel, Cobra Kai (2018), has led to similar growing interest in karate.[95] The success of The Karate Kid further popularized karate (as opposed to Asian martial arts more generally) in mainstream American popular culture.[57] Karate Kommandos is an animated children's show, with Chuck Norris appearing to reveal the moral lessons contained in every episode.
Dragon Ball (1984–present) is a Japanese media franchise (Anime) whos characters use a variety and hybrid of east Asian martial arts styles, including Karate[96][97][98] and Wing Chun (Kung fu).[97][98][99] Dragon Ball was originally inspired by the classical 16th-century Chinese novel Journey to the West, combined with elements of Hong Kong martial arts films, with influences of Jackie Chan and Bruce Lee.

In the film series The Matrix, Neo uses a variety of martial arts styles.[100] Neo's skill in martial arts was shown having downloaded into his brain, which granted combat abilities equivalent to a martial artist with decades of experience. Kenpo Karate is one of the many styles Neo learns as part of his computerised combat training.[101] As part of the preparation for the movie, Yuen Woo-ping had Keanu Reeves undertake four months of martial arts training in a variety of different styles.[102]Many other film stars such as Bruce Lee, Chuck Norris, Jackie Chan, Sammo Hung, and Jet Li come from a range of other martial arts.



Ancient Greece (Greek: Ἑλλάς, romanized: Hellás) was a northeastern Mediterranean civilization, existing from the Greek Dark Ages of the 12th–9th centuries BC to the end of classical antiquity (c. 600 AD), that comprised a loose collection of culturally and linguistically related city-states and other territories. Most of these regions were officially unified only once, for 13 years, under Alexander the Great's empire from 336 to 323 BC (though this excludes a number of Greek city-states free from Alexander's jurisdiction in the western Mediterranean, around the Black Sea, Cyprus, and Cyrenaica). In Western history, the era of classical antiquity was immediately followed by the Early Middle Ages and the Byzantine period.[1]
Roughly three centuries after the Late Bronze Age collapse of Mycenaean Greece, Greek urban poleis began to form in the 8th century BC, ushering in the Archaic period and the colonization of the Mediterranean Basin. This was followed by the age of Classical Greece, from the Greco-Persian Wars to the 5th to 4th centuries BC, and which included the Golden Age of Athens. The conquests of Alexander the Great of Macedon spread Hellenistic civilization from the western Mediterranean to Central Asia. The Hellenistic period ended with the conquest of the eastern Mediterranean world by the Roman Republic, and the annexation of the Roman province of Macedonia in Roman Greece, and later the province of Achaea during the Roman Empire.
Classical Greek culture, especially philosophy, had a powerful influence on ancient Rome, which carried a version of it throughout the Mediterranean and much of Europe. For this reason, Classical Greece is generally considered the cradle of Western civilization, the seminal culture from which the modern West derives many of its founding archetypes and ideas in politics, philosophy, science, and art.[2][3][4]
Classical antiquity in the Mediterranean region is commonly considered to have begun in the 8th century BC[5] (around the time of the earliest recorded poetry of Homer) and ended in the 6th century AD.
Classical antiquity in Greece was preceded by the Greek Dark Ages (c. 1200 – c. 800 BC), archaeologically characterised by the protogeometric and geometric styles of designs on pottery. Following the Dark Ages was the Archaic Period, beginning around the 8th century BC, which saw early developments in Greek culture and society leading to the Classical Period[6] from the Persian invasion of Greece in 480 BC until the death of Alexander the Great in 323 BC.[7] The Classical Period is characterized by a "classical" style, i.e. one which was considered exemplary by later observers, most famously in the Parthenon of Athens. Politically, the Classical Period was dominated by Athens and the Delian League during the 5th century, but displaced by Spartan hegemony during the early 4th century BC, before power shifted to Thebes and the Boeotian League and finally to the League of Corinth led by Macedon. This period was shaped by the Greco-Persian Wars, the Peloponnesian War, and the Rise of Macedon.
Following the Classical period was the Hellenistic period (323–146 BC), during which Greek culture and power expanded into the Near and Middle East from the death of Alexander until the Roman conquest. Roman Greece is usually counted from the Roman victory over the Corinthians at the Battle of Corinth in 146 BC to the establishment of Byzantium by Constantine as the capital of the Roman Empire in 330 AD. Finally, Late Antiquity refers to the period of Christianization during the later 4th to early 6th centuries AD, consummated by the closure of the Academy of Athens by Justinian I in 529.[8]
The historical period of ancient Greece is unique in world history as the first period attested directly in comprehensive, narrative historiography, while earlier ancient history or protohistory is known from much more fragmentary documents such as annals, king lists, and pragmatic epigraphy.
Herodotus is widely known as the "father of history": his Histories are eponymous of the entire field. Written between the 450s and 420s BC, Herodotus' work reaches about a century into the past, discussing 6th century BC historical figures such as Darius I of Persia, Cambyses II and Psamtik III, and alluding to some 8th century BC persons such as Candaules. The accuracy of Herodotus' works is debated.[9][10][11][12][13]
Herodotus was succeeded by authors such as Thucydides, Xenophon, Demosthenes, Plato and Aristotle. Most were either Athenian or pro-Athenian, which is why far more is known about the history and politics of Athens than of many other cities.
Their scope is further limited by a focus on political, military and diplomatic history, ignoring economic and social history.[14]
In the 8th century BC, Greece began to emerge from the Dark Ages, which followed the collapse of the Mycenaean civilization. Literacy had been lost and the Mycenaean script forgotten, but the Greeks adopted the Phoenician alphabet, modifying it to create the Greek alphabet. Objects inscribed with Phoenician writing may have been available in Greece from the 9th century BC, but the earliest evidence of Greek writing comes from graffiti on Greek pottery from the mid-8th century.[15] Greece was divided into many small self-governing communities, a pattern largely dictated by its geography: every island, valley and plain is cut off from its neighbors by the sea or mountain ranges.[16]
The Lelantine War (c. 710 – c. 650 BC) is the earliest documented war of the ancient Greek period. It was fought between the important poleis (city-states) of Chalcis and Eretria over the fertile Lelantine plain of Euboea. Both cities seem to have declined as a result of the long war, though Chalcis was the nominal victor.
A mercantile class arose in the first half of the 7th century BC, shown by the introduction of coinage in about 680 BC.[17][where?]This seems to have introduced tension to many city-states, as their aristocratic regimes were threatened by the new wealth of merchants ambitious for political power. From 650 BC onwards, the aristocracies had to fight to maintain themselves against populist tyrants.[a] A growing population and a shortage of land also seem to have created internal strife between rich and poor in many city-states.
In Sparta, the Messenian Wars resulted in the conquest of Messenia and enserfment of the Messenians, beginning in the latter half of the 8th century BC. This was an unprecedented act in ancient Greece, which led to a social revolution[20] in which the subjugated population of helots farmed and labored for Sparta, whilst every Spartan male citizen became a soldier of the Spartan army permanently in arms. Rich and poor citizens alike were obliged to live and train as soldiers, an equality that defused social conflict. These reforms, attributed to Lycurgus of Sparta, were probably complete by 650 BC.
Athens suffered a land and agrarian crisis in the late 7th century BC, again resulting in civil strife. The Archon (chief magistrate) Draco made severe reforms to the law code in 621 BC (hence "draconian"), but these failed to quell the conflict. Eventually, the moderate reforms of Solon (594 BC), improving the lot of the poor but firmly entrenching the aristocracy in power, gave Athens some stability.
By the 6th century BC, several cities had emerged as dominant in Greek affairs: Athens, Sparta, Corinth, and Thebes. Each of them had brought the surrounding rural areas and smaller towns under their control, and Athens and Corinth had become major maritime and mercantile powers as well.
Rapidly increasing population in the 8th and 7th centuries BC had resulted in the emigration of many Greeks to form colonies in Magna Graecia (Southern Italy and Sicily), Asia Minor and further afield. The emigration effectively ceased in the 6th century BC by which time the Greek world had, culturally and linguistically, become much larger than the area of present-day Greece. Greek colonies were not politically controlled by their founding cities, although they often retained religious and commercial links with them.
The Greek colonies of Sicily, especially Syracuse, were soon drawn into prolonged conflicts with the Carthaginians. These conflicts lasted from 600 BC to 265 BC, when the Roman Republic allied with the Mamertines to fend off the new tyrant of Syracuse, Hiero II, and then the Carthaginians. As a result, Rome became the new dominant power against the fading strength of the Sicilian Greek cities and the fading Carthaginian hegemony. One year later, the First Punic War erupted.
In this period, Greece and its overseas colonies enjoyed huge economic development in commerce and manufacturing, with rising general prosperity. Some studies estimate that the average Greek household grew fivefold between 800 and 300 BC, indicating a large increase in average income.[citation needed]
In the second half of the 6th century BC, Athens fell under the tyranny of Pisistratus followed by his sons Hippias and Hipparchus. However, in 510 BC, at the instigation of the Athenian aristocrat Cleisthenes, the Spartan king Cleomenes I helped the Athenians overthrow the tyranny, possibly attracted by silver deposits at Laurion.[21] Sparta and Athens promptly turned on each other, at which point Cleomenes I installed Isagoras as a pro-Spartan archon. Eager to secure Athens' independence from Spartan control, Cleisthenes proposed a political revolution: that all citizens share power, regardless of status, making Athens a "democracy". The democratic enthusiasm of the Athenians swept out Isagoras and threw back the Spartan-led invasion to restore him.[22] The advent of democracy cured many of the social ills of Athens and ushered in the Golden Age.
In 499 BC, the Ionian city states under Persian rule rebelled against their Persian-supported tyrant rulers.[23] Supported by troops sent from Athens and Eretria, they advanced as far as Sardis and burnt the city before being driven back by a Persian counterattack.[24] The revolt continued until 494, when the rebelling Ionians were defeated.[24] Darius did not forget that Athens had assisted the Ionian revolt, and in 490 he assembled an armada to retaliate.[25] Though heavily outnumbered, the Athenians—supported by their Plataean allies—defeated the Persian hordes at the Battle of Marathon, and the Persian fleet turned tail.[26]
Ten years later, a second invasion was launched by Darius' son Xerxes.[27] The city-states of northern and central Greece submitted to the Persian forces without resistance, but a coalition of 31 Greek city states, including Athens and Sparta, determined to resist the Persian invaders.[27] At the same time, Greek Sicily was invaded by a Carthaginian force.[27] In 480 BC, the first major battle of the invasion was fought at Thermopylae, where a small rearguard of Greeks, led by three hundred Spartans, held a crucial pass guarding the heart of Greece for several days; at the same time Gelon, tyrant of Syracuse, defeated the Carthaginian invasion at the Battle of Himera.[28]
The Persians were decisively defeated at sea by a primarily Athenian naval force at the Battle of Salamis, and on land in 479 BC at the Battle of Plataea.[29] The alliance against Persia continued, initially led by the Spartan Pausanias but from 477 by Athens,[30] and by 460 Persia had been driven out of the Aegean.[31] During this long campaign, the Delian League gradually transformed from a defensive alliance of Greek states into an Athenian empire, as Athens' growing naval power intimidated the other league states.[32] Athens ended its campaigns against Persia in 450, after a disastrous defeat in Egypt in 454, and the death of Cimon in action against the Persians on Cyprus in 450.[33]
As the Athenian fight against the Persian empire waned, conflict grew between Athens and Sparta. Suspicious of the increasing Athenian power funded by the Delian League, Sparta offered aid to reluctant members of the League to rebel against Athenian domination. These tensions were exacerbated in 462 BC when Athens sent a force to aid Sparta in overcoming a helot revolt, but this aid was rejected by the Spartans.[34] In the 450s, Athens took control of Boeotia, and won victories over Aegina and Corinth.[33] However, Athens failed to win a decisive victory, and in 447 lost Boeotia again.[33] Athens and Sparta signed the Thirty Years' Peace in the winter of 446/5, ending the conflict.[33]
Despite the treaty, Athenian relations with Sparta declined again in the 430s, and in 431 BC the Peloponnesian War began.[35] The first phase of the war saw a series of fruitless annual invasions of Attica by Sparta, while Athens successfully fought the Corinthian empire in northwest Greece and defended its own empire, despite a plague which killed the leading Athenian statesman Pericles.[36] The war turned after Athenian victories led by Cleon at Pylos and Sphakteria,[36] and Sparta sued for peace, but the Athenians rejected the proposal.[37] The Athenian failure to regain control of Boeotia at Delium and Brasidas' successes in northern Greece in 424 improved Sparta's position after Sphakteria.[37] After the deaths of Cleon and Brasidas, the strongest proponents of war on each side, a peace treaty was negoitiated in 421 by the Athenian general Nicias.[38]
The peace did not last, however. In 418 BC allied forces of Athens and Argos were defeated by Sparta at Mantinea.[39] In 415 Athens launched an ambitious naval expedition to dominate Sicily;[40] the expedition ended in disaster at the harbor of Syracuse, with almost the entire army killed, and the ships destroyed.[41] Soon after the Athenian defeat in Syracuse, Athens' Ionian allies began to rebel against the Delian league, while Persia began to once again involve itself in Greek affairs on the Spartan side.[42] Initially the Athenian position continued relatively strong, with important victories at Cyzicus in 410 and Arginusae in 406.[43] However, in 405 the Spartan Lysander defeated Athens in the Battle of Aegospotami, and began to blockade Athens' harbour;[44] driven by hunger, Athens sued for peace, agreeing to surrender their fleet and join the Spartan-led Peloponnesian League.[45] Following the Athenian surrender, Sparta installed an oligarchic regime, the Thirty Tyrants, in Athens,[44] one of a number of Spartan-backed oligarchies which rose to power after the Peloponnesian war.[46] Spartan predominance did not last: after only a year, the Thirty had been overthrown.[47]
The first half of the fourth century saw the major Greek states attempt to dominate the mainland; none were successful, and their resulting weakness led to a power vacuum which would eventually be filled by Macedon under Philip II and then Alexander the Great.[48] In the immediate aftermath of the Peloponnesian war, Sparta attempted to extend their own power, leading Argos, Athens, Corinth, and Thebes to join against them.[49] Aiming to prevent any single Greek state gaining the dominance that would allow it to challenge Persia, the Persian king initially joined the alliance against Sparta, before imposing the Peace of Antalcidas ("King's Peace") which restored Persia's control over the Anatolian Greeks.[50]
By 371 BC, Thebes was in the ascendancy, defeating Sparta at the Battle of Leuctra, killing the Spartan king Cleombrotus I, and invading Laconia. Further Theban successes against Sparta in 369 led to Messenia gaining independence; Sparta never recovered from the loss of Messenia's fertile land and the helot workforce it provided.[51] The rising power of Thebes led Sparta and Athens to join forces; in 362 they were defeated by Thebes at the Battle of Mantinea. In the aftermath of Mantinea, none of the major Greek states were able to dominate. Though Thebes had won the battle, their general Epaminondas was killed, and they spent the following decades embroiled in wars with their neighbours; Athens, meanwhile, saw its second naval alliance, formed in 377, collapse in the mid-350s.[52]
The power vacuum in Greece after the Battle of Mantinea was filled by Macedon, under Philip II. In 338 BC, he defeated a Greek alliance at the Battle of Chaeronea, and subsequently formed the League of Corinth. Philip planned to lead the League to invade Persia, but was murdered in 336. His son Alexander the Great was left to fulfil his father's ambitions.[53] After campaigns against Macedon's western and northern enemies, and those Greek states that had broken from the League of Corinth following the death of Philip, Alexander began his campaign against Persia in 334.[54] He conquered Persia, defeating Darius III at the Battle of Issus in 333, and after the Battle of Gaugamela in 331 proclaimed himself king of Asia.[55] From 329 he led expeditions to Bactria and then India;[56] further plans to invade Arabia and North Africa were halted by his death in 323.[57]
The period from the death of Alexander the Great in 323 until the death of Cleopatra VII, the last Macedonian ruler of Egypt, is known as the Hellenistic period. In the early part of this period, a new form of kingship developed based on Macedonian and Near Eastern traditions. The first Hellenistic kings were previously Alexander's generals, and took power in the period following his death, though they were not part of existing royal lineages and lacked historic claims to the territories they controlled.[58] The most important of these rulers in the decades after Alexander's death were Antigonus I and his son Demetrius in Macedonia and Greece, Ptolemy in Eygpt, and Seleucus I in Syria and the former Persian empire;[59] smaller Hellenistic kingdoms included the Attalids in Anatolia and the Greco-Bactrian kingdom.[60]
In the early part of the Hellenistic period, the exact borders of the Hellenistic kingdoms were not settled. Antigonus attempted to expand his territory by attacking the other successor kingdoms until they joined against him, and he was killed at the Battle of Ipsus in 301 BC.[61] His son Demetrius spent many years in Seleucid captivity, and his son, Antigonus II, only reclaimed the Macedonian throne around 276.[61] Meanwhile, the Seleucid kingdom gave up territory in the east to the Indian king Chandragupta Maurya in exchange for war elephants, and later lost large parts of Persia to the Parthian empire.[61] By the mid-third century, the kingdoms of Alexander's successors was mostly stable, though there continued to be disputes over border areas.[60]
During the Hellenistic period, the importance of "Greece proper" (the territory of modern Greece) within the Greek-speaking world declined sharply. The great capitals of Hellenistic culture were Alexandria in the Ptolemaic Kingdom and Antioch in the Seleucid Empire.
The conquests of Alexander had numerous consequences for the Greek city-states. It greatly widened the horizons of the Greeks and led to a steady emigration of the young and ambitious to the new Greek empires in the east.[62] Many Greeks migrated to Alexandria, Antioch and the many other new Hellenistic cities founded in Alexander's wake, as far away as present-day Afghanistan and Pakistan, where the Greco-Bactrian Kingdom and the Indo-Greek Kingdom survived until the end of the first century BC.
The city-states within Greece formed themselves into two leagues; the Achaean League (including Thebes, Corinth and Argos) and the Aetolian League (including Sparta and Athens). For much of the period until the Roman conquest, these leagues were at war, often participating in the conflicts between the Diadochi (the successor states to Alexander's empire).
The Antigonid Kingdom became involved in a war with the Roman Republic in the late 3rd century. Although the First Macedonian War was inconclusive, the Romans, in typical fashion, continued to fight Macedon until it was completely absorbed into the Roman Republic (by 149 BC). In the east, the unwieldy Seleucid Empire gradually disintegrated, although a rump survived until 64 BC, whilst the Ptolemaic Kingdom continued in Egypt until 30 BC when it too was conquered by the Romans. The Aetolian league grew wary of Roman involvement in Greece, and sided with the Seleucids in the Roman–Seleucid War; when the Romans were victorious, the league was effectively absorbed into the Republic. Although the Achaean league outlasted both the Aetolian league and Macedon, it was also soon defeated and absorbed by the Romans in 146 BC, bringing Greek independence to an end.
The Greek peninsula came under Roman rule during the 146 BC conquest of Greece after the Battle of Corinth. Macedonia became a Roman province while southern Greece came under the surveillance of Macedonia's prefect; however, some Greek poleis managed to maintain a partial independence and avoid taxation. The Aegean islands were added to this territory in 133 BC. Athens and other Greek cities revolted in 88 BC, and the peninsula was crushed by the Roman general Sulla. The Roman civil wars devastated the land even further, until Augustus organized the peninsula as the province of Achaea in 27 BC.
Greece was a key eastern province of the Roman Empire, as the Roman culture had long been in fact Greco-Roman. The Greek language served as a lingua franca in the East and in Italy, and many Greek intellectuals such as Galen would perform most of their work in Rome.
The territory of Greece is mountainous, and as a result, ancient Greece consisted of many smaller regions, each with its own dialect, cultural peculiarities, and identity. Regionalism and regional conflicts were prominent features of ancient Greece. Cities tended to be located in valleys between mountains, or on coastal plains, and dominated a certain area around them.
In the south lay the Peloponnese, consisting of the regions of Laconia (southeast), Messenia (southwest), Elis (west), Achaia (north), Korinthia (northeast), Argolis (east), and Arcadia (center). These names survive to the present day as regional units of modern Greece, though with somewhat different boundaries. Mainland Greece to the north, nowadays known as Central Greece, consisted of Aetolia and Acarnania in the west, Locris, Doris, and Phocis in the center, while in the east lay Boeotia, Attica, and Megaris. Northeast lay Thessaly, while Epirus lay to the northwest. Epirus stretched from the Ambracian Gulf in the south to the Ceraunian mountains and the Aoos river in the north, and consisted of Chaonia (north), Molossia (center), and Thesprotia (south). In the northeast corner was Macedonia,[63] originally consisting Lower Macedonia and its regions, such as Elimeia, Pieria, and Orestis. Around the time of Alexander I of Macedon, the Argead kings of Macedon started to expand into Upper Macedonia, lands inhabited by independent Macedonian tribes like the Lyncestae, Orestae and the Elimiotae and to the west, beyond the Axius river, into Eordaia, Bottiaea, Mygdonia, and Almopia, regions settled by Thracian tribes.[64] To the north of Macedonia lay various non-Greek peoples such as the Paeonians due north, the Thracians to the northeast, and the Illyrians, with whom the Macedonians were frequently in conflict, to the northwest. Chalcidice was settled early on by southern Greek colonists and was considered part of the Greek world, while from the late 2nd millennium BC substantial Greek settlement also occurred on the eastern shores of the Aegean, in Anatolia.
During the Archaic period, the Greek population grew beyond the capacity of the limited arable land of Greece proper, resulting in the large-scale establishment of colonies elsewhere: according to one estimate, the population of the widening area of Greek settlement increased roughly tenfold from 800 BC to 400 BC, from 800,000 to as many as 7+1⁄2-10 million.[65]
From about 750 BC the Greeks began 250 years of expansion, settling colonies in all directions. To the east, the Aegean coast of Asia Minor was colonized first, followed by Cyprus and the coasts of Thrace, the Sea of Marmara and south coast of the Black Sea.
Eventually, Greek colonization reached as far northeast as present-day Ukraine and Russia (Taganrog). To the west the coasts of Illyria, Sicily and Southern Italy were settled, followed by Southern France, Corsica, and even eastern Spain. Greek colonies were also founded in Egypt and Libya.
Modern Syracuse, Naples, Marseille and Istanbul had their beginnings as the Greek colonies Syracusae (Συράκουσαι), Neapolis (Νεάπολις), Massalia (Μασσαλία) and Byzantion (Βυζάντιον). These colonies played an important role in the spread of Greek influence throughout Europe and also aided in the establishment of long-distance trading networks between the Greek city-states, boosting the economy of ancient Greece.
Ancient Greece consisted of several hundred relatively independent city-states (poleis). This was a situation unlike that in most other contemporary societies, which were either tribal or kingdoms ruling over relatively large territories. Undoubtedly, the geography of Greece—divided and sub-divided by hills, mountains, and rivers—contributed to the fragmentary nature of ancient Greece. On the one hand, the ancient Greeks had no doubt that they were "one people"; they had the same religion, same basic culture, and same language. Furthermore, the Greeks were very aware of their tribal origins; Herodotus was able to extensively categorise the city-states by tribe. Yet, although these higher-level relationships existed, they seem to have rarely had a major role in Greek politics. The independence of the poleis was fiercely defended; unification was something rarely contemplated by the ancient Greeks. Even when, during the second Persian invasion of Greece, a group of city-states allied themselves to defend Greece, the vast majority of poleis remained neutral, and after the Persian defeat, the allies quickly returned to infighting.[67]
Thus, the major peculiarities of the ancient Greek political system were its fragmentary nature (and that this does not particularly seem to have tribal origin), and the particular focus on urban centers within otherwise tiny states. The peculiarities of the Greek system are further evidenced by the colonies that they set up throughout the Mediterranean Sea, which, though they might count a certain Greek polis as their 'mother' (and remain sympathetic to her), were completely independent of the founding city.
Inevitably smaller poleis might be dominated by larger neighbors, but conquest or direct rule by another city-state appears to have been quite rare. Instead the poleis grouped themselves into leagues, membership of which was in a constant state of flux. Later in the Classical period, the leagues would become fewer and larger, be dominated by one city (particularly Athens, Sparta and Thebes); and often poleis would be compelled to join under threat of war (or as part of a peace treaty). Even after Philip II of Macedon "conquered" the heartlands of ancient Greece, he did not attempt to annex the territory or unify it into a new province, but compelled most of the poleis to join his own Corinthian League.
Initially many Greek city-states seem to have been petty kingdoms; there was often a city official carrying some residual, ceremonial functions of the king (basileus), e.g., the archon basileus in Athens.[68] However, by the Archaic period and the first historical consciousness, most had already become aristocratic oligarchies. It is unclear exactly how this change occurred. For instance, in Athens, the kingship had been reduced to a hereditary, lifelong chief magistracy (archon) by c. 1050 BC; by 753 BC this had become a decennial, elected archonship; and finally by 683 BC an annually elected archonship. Through each stage, more power would have been transferred to the aristocracy as a whole, and away from a single individual.
Inevitably, the domination of politics and concomitant aggregation of wealth by small groups of families was apt to cause social unrest in many poleis. In many cities a tyrant (not in the modern sense of repressive autocracies), would at some point seize control and govern according to their own will; often a populist agenda would help sustain them in power. In a system wracked with class conflict, government by a 'strongman' was often the best solution.
Athens fell under a tyranny in the second half of the 6th century BC. When this tyranny was ended, the Athenians founded the world's first democracy as a radical solution to prevent the aristocracy regaining power. A citizens' assembly (the Ecclesia), for the discussion of city policy, had existed since the reforms of Draco in 621 BC; all citizens were permitted to attend after the reforms of Solon (early 6th century), but the poorest citizens could not address the assembly or run for office. With the establishment of the democracy, the assembly became the de jure mechanism of government; all citizens had equal privileges in the assembly. However, non-citizens, such as metics (foreigners living in Athens) or slaves, had no political rights at all.
After the rise of democracy in Athens, other city-states founded democracies. However, many retained more traditional forms of government. As so often in other matters, Sparta was a notable exception to the rest of Greece, ruled through the whole period by not one, but two hereditary monarchs. This was a form of diarchy. The Kings of Sparta belonged to the Agiads and the Eurypontids, descendants respectively of Eurysthenes and Procles. Both dynasties' founders were believed to be twin sons of Aristodemus, a Heraclid ruler. However, the powers of these kings were held in check by both a council of elders (the Gerousia) and magistrates specifically appointed to watch over the kings (the Ephors).
Only free, land-owning, native-born men could be citizens entitled to the full protection of the law in a city-state. In most city-states, unlike the situation in Rome, social prominence did not allow special rights. Sometimes families controlled public religious functions, but this ordinarily did not give any extra power in the government. In Athens, the population was divided into four social classes based on wealth. People could change classes if they made more money. In Sparta, all male citizens were called homoioi, meaning "peers". However, Spartan kings, who served as the city-state's dual military and religious leaders, came from two families.[69]
Slaves had no power or status. They had the right to have a family and own property, subject to their master's goodwill and permission, but they had no political rights. By 600 BC, chattel slavery had spread in Greece. By the 5th century BC, slaves made up one-third of the total population in some city-states. Between 40-80% of the population of Classical Athens were slaves.[70] Slaves outside of Sparta almost never revolted because they were made up of too many nationalities and were too scattered to organize. However, unlike later Western culture, the ancient Greeks did not think in terms of race.[71]
Most families owned slaves as household servants and laborers, and even poor families might have owned a few slaves. Owners were not allowed to beat or kill their slaves. Owners often promised to free slaves in the future to encourage slaves to work hard. Unlike in Rome, freedmen did not become citizens. Instead, they were mixed into the population of metics, which included people from foreign countries or other city-states who were officially allowed to live in the state.
City-states legally owned slaves. These public slaves had a larger measure of independence than slaves owned by families, living on their own and performing specialized tasks. In Athens, public slaves were trained to look out for counterfeit coinage, while temple slaves acted as servants of the temple's deity and Scythian slaves were employed in Athens as a police force corralling citizens to political functions.
Sparta had a special type of slaves called helots. Helots were Messenians enslaved en masse during the Messenian Wars by the state and assigned to families where they were forced to stay. Helots raised food and did household chores so that women could concentrate on raising strong children while men could devote their time to training as hoplites. Their masters treated them harshly, and helots revolted against their masters several times. In 370/69 BC, as a result of Epaminondas' liberation of Messenia from Spartan rule, the helot system there came to an end and the helots won their freedom.[72] However, it did continue to persist in Laconia until the 2nd century BC.
For most of Greek history, education was private, except in Sparta. During the Hellenistic period, some city-states established public schools. Only wealthy families could afford a teacher. Boys learned how to read, write and quote literature. They also learned to sing and play one musical instrument and were trained as athletes for military service. They studied not for a job but to become an effective citizen. Girls also learned to read, write and do simple arithmetic so they could manage the household. They almost never received education after childhood.[73]
Boys went to school at the age of seven, or went to the barracks, if they lived in Sparta. The three types of teachings were: grammatistes for arithmetic, kitharistes for music and dancing, and Paedotribae for sports.
Boys from wealthy families attending the private school lessons were taken care of by a paidagogos, a household slave selected for this task who accompanied the boy during the day. Classes were held in teachers' private houses and included reading, writing, mathematics, singing, and playing the lyre and flute. When the boy became 12 years old the schooling started to include sports such as wrestling, running, and throwing discus and javelin. In Athens, some older youths attended academy for the finer disciplines such as culture, sciences, music, and the arts. The schooling ended at age 18, followed by military training in the army usually for one or two years.[74]
Only a small number of boys continued their education after childhood, as in the Spartan agoge. A crucial part of a wealthy teenager's education was a mentorship with an elder, which in a few places and times may have included pederasty.[citation needed] The teenager learned by watching his mentor talking about politics in the agora, helping him perform his public duties, exercising with him in the gymnasium and attending symposia with him. The richest students continued their education by studying with famous teachers. Some of Athens' greatest such schools included the Lyceum (the so-called Peripatetic school founded by Aristotle of Stageira) and the Platonic Academy (founded by Plato of Athens). The education system of the wealthy ancient Greeks is also called Paideia.[citation needed]
At its economic height in the 5th and 4th centuries BC, the free citizenry of Classical Greece represented perhaps the most prosperous society in the ancient world, some economic historians considering Greece one of the most advanced pre-industrial economies. In terms of wheat, wages reached an estimated 7–12 kg daily for an unskilled worker in urban Athens, 2-3 times the 3.75 kg of an unskilled rural labourer in Roman Egypt, though Greek farm incomes too were on average lower than those available to urban workers.[75]
While slave conditions varied widely, the institution served to sustain the incomes of the free citizenry: an estimate of economic development drawn from the latter (or derived from urban incomes alone) is therefore likely to overstate the true overall level despite widespread evidence for high living standards.
At least in the Archaic Period, the fragmentary nature of ancient Greece, with many competing city-states, increased the frequency of conflict but conversely limited the scale of warfare. Unable to maintain professional armies, the city-states relied on their own citizens to fight. This inevitably reduced the potential duration of campaigns, as citizens would need to return to their own professions (especially in the case of, for example, farmers). Campaigns would therefore often be restricted to summer. When battles occurred, they were usually set piece and intended to be decisive. Casualties were slight compared to later battles, rarely amounting to more than five percent of the losing side, but the slain often included the most prominent citizens and generals who led from the front.
The scale and scope of warfare in ancient Greece changed dramatically as a result of the Greco-Persian Wars. To fight the enormous armies of the Achaemenid Empire was effectively beyond the capabilities of a single city-state. The eventual triumph of the Greeks was achieved by alliances of city-states (the exact composition changing over time), allowing the pooling of resources and division of labor. Although alliances between city-states occurred before this time, nothing on this scale had been seen before. The rise of Athens and Sparta as pre-eminent powers during this conflict led directly to the Peloponnesian War, which saw further development of the nature of warfare, strategy and tactics. Fought between leagues of cities dominated by Athens and Sparta, the increased manpower and financial resources increased the scale and allowed the diversification of warfare. Set-piece battles during the Peloponnesian war proved indecisive and instead there was increased reliance on attritionary strategies, naval battles and blockades and sieges. These changes greatly increased the number of casualties and the disruption of Greek society.
Athens owned one of the largest war fleets in ancient Greece. It had over 200 triremes each powered by 170 oarsmen who were seated in 3 rows on each side of the ship. The city could afford such a large fleet—it had over 34,000 oarsmen—because it owned a lot of silver mines that were worked by slaves.
According to Josiah Ober, Greek city-states faced approximately a one-in-three chance of destruction during the archaic and classical period.[76]
Ancient Greek philosophy focused on the role of reason and inquiry. In many ways, it had an important influence on modern philosophy, as well as modern science. Clear unbroken lines of influence lead from ancient Greek and Hellenistic philosophers, to medieval Muslim philosophers and Islamic scientists, to the European Renaissance and Enlightenment, to the secular sciences of the modern day.
Neither reason nor inquiry began with the ancient Greeks. Defining the difference between the Greek quest for knowledge and the quests of the elder civilizations, such as the ancient Egyptians and Babylonians, has long been a topic of study by theorists of civilization.
The first known philosophers of Greece were the pre-Socratics, who attempted to provide naturalistic, non-mythical descriptions of the world. They were followed by Socrates, one of the first philosophers based in Athens during its golden age whose ideas, despite being known by second-hand accounts instead of writings of his own, laid the basis of Western philosophy. Socrates' disciple Plato, who wrote The Republic and established a radical difference between ideas and the concrete world, and Plato's disciple Aristotle, who wrote extensively about nature and ethics, are also immensely influential in Western philosophy to this day. The later Hellenistic philosophy, also originating in Greece, is defined by names such as Antisthenes (cynicism), Zeno of Citium (stoicism) and Plotinus (Neoplatonism).
The earliest Greek literature was poetry and was composed for performance rather than private consumption.[77] The earliest Greek poet known is Homer, although he was certainly part of an existing tradition of oral poetry.[78] Homer's poetry, though it was developed around the same time that the Greeks developed writing, would have been composed orally; the first poet to certainly compose their work in writing was Archilochus, a lyric poet from the mid-seventh century BC.[79] Tragedy developed around the end of the archaic period, taking elements from across the pre-existing genres of late archaic poetry.[80] Towards the beginning of the classical period, comedy began to develop—the earliest date associated with the genre is 486 BC, when a competition for comedy became an official event at the City Dionysia in Athens, though the first preserved ancient comedy is Aristophanes' Acharnians, produced in 425.[81]
Like poetry, Greek prose had its origins in the archaic period, and the earliest writers of Greek philosophy, history, and medical literature all date to the sixth century BC.[82] Prose first emerged as the writing style adopted by the presocratic philosophers Anaximander and Anaximenes—though Thales of Miletus, considered the first Greek philosopher, apparently wrote nothing.[83] Prose as a genre reached maturity in the classical era,[82] and the major Greek prose genres—philosophy, history, rhetoric, and dialogue—developed in this period.[84]
The Hellenistic period saw the literary centre of the Greek world move from Athens, where it had been in the classical period, to Alexandria. At the same time, other Hellenistic kings such as the Antigonids and the Attalids were patrons of scholarship and literature, turning Pella and Pergamon respectively into cultural centres.[85] It was thanks to this cultural patronage by Hellenistic kings, and especially the Museum at Alexandria, that so much ancient Greek literature has survived.[86] The Library of Alexandria, part of the Museum, had the previously unenvisaged aim of collecting together copies of all known authors in Greek. Almost all of the surviving non-technical Hellenistic literature is poetry,[86] and Hellenistic poetry tended to be highly intellectual,[87] blending different genres and traditions, and avoiding linear narratives.[88] The Hellenistic period also saw a shift in the ways literature was consumed—while in the archaic and classical periods literature had typically been experienced in public performance, in the Hellenistic period it was more commonly read privately.[89] At the same time, Hellenistic poets began to write for private, rather than public, consumption.[90]
With Octavian's victory at Actium in 31 BC, Rome began to become a major centre of Greek literature, as important Greek authors such as Strabo and Dionysius of Halicarnassus came to Rome.[91] The period of greatest innovation in Greek literature under Rome was the "long second century" from approximately 80 AD to around 230 AD.[92] This innovation was especially marked in prose, with the development of the novel and a revival of prominence for display oratory both dating to this period.[92]
Music was present almost universally in Greek society, from marriages and funerals to religious ceremonies, theatre, folk music and the ballad-like reciting of epic poetry. There are significant fragments of actual Greek musical notation as well as many literary references to ancient Greek music. Greek art depicts musical instruments and dance. The word music derives from the name of the Muses, the daughters of Zeus who were patron goddesses of the arts.
Ancient Greek mathematics contributed many important developments to the field of mathematics, including the basic rules of geometry, the idea of formal mathematical proof, and discoveries in number theory, mathematical analysis, applied mathematics, and approached close to establishing integral calculus. The discoveries of several Greek mathematicians, including Pythagoras, Euclid, and Archimedes, are still used in mathematical teaching today.
The Greeks developed astronomy, which they treated as a branch of mathematics, to a highly sophisticated level. The first geometrical, three-dimensional models to explain the apparent motion of the planets were developed in the 4th century BC by Eudoxus of Cnidus and Callippus of Cyzicus. Their younger contemporary Heraclides Ponticus proposed that the Earth rotates around its axis. In the 3rd century BC, Aristarchus of Samos was the first to suggest a heliocentric system. Archimedes in his treatise The Sand Reckoner revives Aristarchus' hypothesis that "the fixed stars and the Sun remain unmoved, while the Earth revolves about the Sun on the circumference of a circle". Otherwise, only fragmentary descriptions of Aristarchus' idea survive.[93] Eratosthenes, using the angles of shadows created at widely separated regions, estimated the circumference of the Earth with great accuracy.[94] In the 2nd century BC Hipparchus of Nicea made a number of contributions, including the first measurement of precession and the compilation of the first star catalog in which he proposed the modern system of apparent magnitudes.
The Antikythera mechanism, a device for calculating the movements of planets, dates from about 80 BC and was the first ancestor of the astronomical computer. It was discovered in an ancient shipwreck off the Greek island of Antikythera, between Kythera and Crete. The device became famous for its use of a differential gear, previously believed to have been invented in the 16th century, and the miniaturization and complexity of its parts, comparable to a clock made in the 18th century. The original mechanism is displayed in the Bronze collection of the National Archaeological Museum of Athens, accompanied by a replica.
The ancient Greeks also made important discoveries in the medical field. Hippocrates was a physician of the Classical period, and is considered one of the most outstanding figures in the history of medicine. He is referred to as the "father of medicine"[95][96] in recognition of his lasting contributions to the field as the founder of the Hippocratic school of medicine. This intellectual school revolutionized medicine in ancient Greece, establishing it as a discipline distinct from other fields that it had traditionally been associated with (notably theurgy and philosophy), thus making medicine a profession.[97][98]
The art of ancient Greece has exercised an enormous influence on the culture of many countries from ancient times to the present day, particularly in the areas of sculpture and architecture. In the West, the art of the Roman Empire was largely derived from Greek models. In the East, Alexander the Great's conquests initiated several centuries of exchange between Greek, Central Asian and Indian cultures, resulting in Greco-Buddhist art, with ramifications as far as Japan. Following the Renaissance in Europe, the humanist aesthetic and the high technical standards of Greek art inspired generations of European artists. Well into the 19th century, the classical tradition derived from Greece dominated the art of the Western world.
Religion was a central part of ancient Greek life.[99] Though the Greeks of different cities and tribes worshipped similar gods, religious practices were not uniform and the gods were thought of differently in different places. The Greeks were polytheistic, worshipping many gods, but as early as the sixth century BC a pantheon of twelve Olympians began to develop.[100] Greek religion was influenced by the practices of the Greeks' near eastern neighbours at least as early as the archaic period, and by the Hellenistic period this influence was seen in both directions.[101]
The most important religious act in ancient Greece was animal sacrifice, most commonly of sheep and goats.[102] Sacrifice was accompanied by public prayer,[103] and prayer and hymns were themselves a major part of ancient Greek religious life.[104]
The civilization of ancient Greece has been immensely influential on language, politics, educational systems, philosophy, science, and the arts. It became the Leitkultur of the Roman Empire to the point of marginalizing native Italic traditions. As Horace put it,
Via the Roman Empire, Greek culture came to be foundational to Western culture in general.
The Byzantine Empire inherited Classical Greek-Hellenistic culture directly, without Latin intermediation, and the preservation of Classical Greek learning in medieval Byzantine tradition further exerted a strong influence on the Slavs and later on the Islamic Golden Age and the Western European Renaissance. A modern revival of Classical Greek learning took place in the Neoclassicism movement in 18th- and 19th-century Europe and the Americas.




Apartheid (/əˈpɑːrt(h)aɪt/, especially South African English: /əˈpɑːrt(h)eɪt/, Afrikaans: [aˈpartɦɛit]; transl. "separateness", lit. 'aparthood') was a system of institutionalised racial segregation that existed in South Africa and South West Africa (now Namibia) from 1948 to the early 1990s.[note 1] Apartheid was characterised by an authoritarian political culture based on baasskap (lit. 'boss-hood' or 'boss-ship'), which ensured that South Africa was dominated politically, socially, and economically through minoritarianism by the nation's dominant minority white population.[4] According to this system of social stratification, white citizens had the highest status, followed by Indians and Coloureds, then Black Africans.[4] The economic legacy and social effects of apartheid continue to the present day, particularly inequality.[5][6][7][better source needed]
Broadly speaking, apartheid was delineated into petty apartheid, which entailed the segregation of public facilities and social events, and grand apartheid, which dictated housing and employment opportunities by race.[8] The first apartheid law was the Prohibition of Mixed Marriages Act, 1949, followed closely by the Immorality Amendment Act of 1950, which made it illegal for most South African citizens to marry or pursue sexual relationships across racial lines.[9] The Population Registration Act, 1950 classified all South Africans into one of four racial groups based on appearance, known ancestry, socioeconomic status, and cultural lifestyle: "Black", "White", "Coloured", and "Indian", the last two of which included several sub-classifications.[10] Places of residence were determined by racial classification.[9] Between 1960 and 1983, 3.5 million black Africans were removed from their homes and forced into segregated neighbourhoods as a result of apartheid legislation, in some of the largest mass evictions in modern history.[11] Most of these targeted removals were intended to restrict the black population to ten designated "tribal homelands", also known as bantustans, four of which became nominally independent states.[9] The government announced that relocated persons would lose their South African citizenship as they were absorbed into the bantustans.[8]
Apartheid sparked significant international and domestic opposition, resulting in some of the most influential global social movements of the 20th century.[12] It was the target of frequent condemnation in the United Nations and brought about extensive international sanctions, including arms embargoes and economic sanctions on South Africa.[13] During the 1970s and 1980s, internal resistance to apartheid became increasingly militant, prompting brutal crackdowns by the National Party ruling government and protracted sectarian violence that left thousands dead or in detention.[14] Some reforms of the apartheid system were undertaken, including allowing for Indian and Coloured political representation in parliament, but these measures failed to appease most activist groups.[15]
Between 1987 and 1993, the National Party entered into bilateral negotiations with the African National Congress (ANC), the leading anti-apartheid political movement, for ending segregation and introducing majority rule.[15][16] In 1990, prominent ANC figures such as Nelson Mandela were released from prison.[17] Apartheid legislation was repealed on 17 June 1991,[2] leading to multiracial elections in April 1994.[18]
Apartheid is an Afrikaans[19] word meaning "separateness", or "the state of being apart", literally "apart-hood" (from the Afrikaans suffix -heid).[20][21] Its first recorded use was in 1929.[9]
Racial discrimination and inequality against blacks in South Africa dates to the beginning of large-scale European colonization of South Africa with the Dutch East India Company's establishment of a trading post in the Cape of Good Hope in 1652, which eventually expanded into the Dutch Cape Colony. The company began the Khoikhoi–Dutch Wars in which it displaced the local Khoikhoi people, replaced them with farms worked by white settlers, and imported black slaves from across the Dutch Empire.[22] In the days of slavery, slaves required passes to travel away from their masters.
In 1797, the Landdrost and Heemraden, local officials, of Swellendam and Graaff-Reinet extended pass laws beyond slaves and ordained that all Khoikhoi (designated as Hottentots) moving about the country for any purpose should carry passes.[23] This was confirmed by the British Colonial government in 1809 by the Hottentot Proclamation, which decreed that if a Khoikhoi were to move they would need a pass from their master or a local official.[23] Ordinance No. 49 of 1828 decreed that prospective black immigrants were to be granted passes for the sole purpose of seeking work.[23] These passes were to be issued for Coloureds and Khoikhoi but not for other Africans, who were still forced to carry passes.
During the French Revolutionary and Napoleonic Wars, the British Empire captured and annexed the Dutch Cape Colony.[24] Under the 1806 Cape Articles of Capitulation the new British colonial rulers were required to respect previous legislation enacted under Roman-Dutch law,[25] and this led to a separation of the law in South Africa from English Common Law and a high degree of legislative autonomy. The governors and assemblies that governed the legal process in the various colonies of South Africa were launched on a different and independent legislative path from the rest of the British Empire.
The United Kingdom's Slavery Abolition Act 1833 abolished slavery throughout the British Empire and overrode the Cape Articles of Capitulation. To comply with the act, the South African legislation was expanded to include Ordinance 1 in 1835, which effectively changed the status of slaves to indentured labourers. This was followed by Ordinance 3 in 1848, which introduced an indenture system for Xhosa that was little different from slavery.
The various South African colonies passed legislation throughout the rest of the 19th century to limit the freedom of unskilled workers, to increase the restrictions on indentured workers and to regulate the relations between the races. The discoveries of diamonds and gold in South Africa also raised racial inequality between whites and blacks.[26]
In the Cape Colony, which previously had a liberal and multi-racial constitution and a system of Cape Qualified Franchise open to men of all races, the Franchise and Ballot Act of 1892 raised the property franchise qualification and added an educational element, disenfranchising a disproportionate number of the Cape's non-white voters,[27] and the Glen Grey Act of 1894 instigated by the government of Prime Minister Cecil Rhodes limited the amount of land Africans could hold. Similarly, in Natal, the Natal Legislative Assembly Bill of 1894 deprived Indians of the right to vote.[28] In 1896 the South African Republic brought in two pass laws requiring Africans to carry a badge. Only those employed by a master were permitted to remain on the Rand, and those entering a "labour district" needed a special pass.[29] During the Second Boer War the British Empire used racial exploitation of blacks as a cause for its war against the Boer republics. However, the peace negotiations for the Treaty of Vereeniging demanded "the just predominance of the white race" in South Africa as a precondition for the Boer republics unifying with the British Empire.[30]
In 1905 the General Pass Regulations Act denied blacks the vote and limited them to fixed areas,[31] and in 1906 the Asiatic Registration Act of the Transvaal Colony required all Indians to register and carry passes.[32] Beginning in 1906 the South African Native Affairs Commission under Godfrey Lagden began implementing a more openly segregationist policy towards nonwhites.[33] The latter was repealed by the British government but re-enacted in 1908. In 1910, the Union of South Africa was created as a self-governing dominion, which continued the legislative program: the South Africa Act (1910) enfranchised white people, giving them complete political control over all other racial groups while removing the right of black people to sit in parliament;[34] the Native Land Act (1913) prevented blacks, except those in the Cape, from buying land outside "reserves";[34] the Natives in Urban Areas Bill (1918) was designed to force black people into "locations";[35] the Urban Areas Act (1923) introduced residential segregation and provided cheap labour for industry led by white people; the Colour Bar Act (1926) prevented black mine workers from practicing skilled trades; the Native Administration Act (1927) made the British Crown rather than paramount chiefs the supreme head over all African affairs;[36][better source needed] the Native Land and Trust Act (1936) complemented the 1913 Native Land Act and, in the same year, the Representation of Natives Act removed previous black voters from the Cape voters' roll and allowed them to elect three whites to Parliament.[37][better source needed]
The United Party government of Jan Smuts began to move away from the rigid enforcement of segregationist laws during World War II, but faced growing opposition from Afrikaner nationalists who wanted stricter segregation.[38][39] Post-war, one of the first pieces of segregating legislation enacted by Smuts' government was the Asiatic Land Tenure Bill (1946), which banned land sales to Indians and Indian descendent South Africans.[40] The same year, the government established the Fagan Commission. Amid fears integration would eventually lead to racial assimilation, the Opposition Herenigde Nasionale Party (HNP) established the Sauer Commission to investigate the effects of the United Party's policies. The commission concluded that integration would bring about a "loss of personality" for all racial groups. The HNP incorporated the commission's findings into its campaign platform for the 1948 South African general election, which it won.[citation needed]
South Africa had allowed social custom and law to govern the consideration of multiracial affairs and of the allocation, in racial terms, of access to economic, social, and political status.[41] Most white South Africans, regardless of their own differences, accepted the prevailing pattern.[citation needed] Nevertheless, by 1948 it remained apparent that there were gaps in the social structure, whether legislated or otherwise, concerning the rights and opportunities of nonwhites. The rapid economic development of World War II attracted black migrant workers in large numbers to chief industrial centres, where they compensated for the wartime shortage of white labour. However, this escalated rate of black urbanisation went unrecognised by the South African government, which failed to accommodate the influx with parallel expansion in housing or social services.[41] Overcrowding, increasing crime rates, and disillusionment resulted; urban blacks came to support a new generation of leaders influenced by the principles of self-determination and popular freedoms enshrined in such statements as the Atlantic Charter. Black political organizations and leaders such as Alfred Xuma, James Mpanza, the African National Congress, and the Council of Non-European Trade Unions began demanding political rights, land reform, and the right to unionise.[42]
Whites reacted negatively to the changes, allowing the Herenigde Nasionale Party (or simply the National Party) to convince a large segment of the voting bloc that the impotence of the United Party in curtailing the evolving position of nonwhites indicated that the organisation had fallen under the influence of Western liberals.[41] Many Afrikaners resented what they perceived as disempowerment by an underpaid black workforce and the superior economic power and prosperity of white English speakers.[43] Smuts, as a strong advocate of the United Nations, lost domestic support when South Africa was criticised for its colour bar and the continued mandate of South West Africa by other UN member states.[44]
Afrikaner nationalists proclaimed that they offered the voters a new policy to ensure continued white domination.[45] This policy was initially expounded from a theory drafted by Hendrik Verwoerd and was presented to the National Party by the Sauer Commission.[41] It called for a systematic effort to organise the relations, rights, and privileges of the races as officially defined through a series of parliamentary acts and administrative decrees. Segregation had thus far been pursued only in major matters, such as separate schools, and local society rather than law had been depended upon to enforce most separation; it should now be extended to everything.[41] The commission's goal was to completely remove blacks from areas designated for whites, including cities, with the exception of temporary migrant labor. Blacks would then be encouraged to create their own political units in land reserved for them.[46] The party gave this policy a name – apartheid. Apartheid was to be the basic ideological and practical foundation of Afrikaner politics for the next quarter of a century.[45]
The National Party's election platform stressed that apartheid would preserve a market for white employment in which nonwhites could not compete. On the issues of black urbanisation, the regulation of nonwhite labour, influx control, social security, farm tariffs and nonwhite taxation, the United Party's policy remained contradictory and confused.[44] Its traditional bases of support not only took mutually exclusive positions, but found themselves increasingly at odds with each other. Smuts' reluctance to consider South African foreign policy against the mounting tensions of the Cold War also stirred up discontent, while the nationalists promised to purge the state and public service of communist sympathisers.[44]
First to desert the United Party were Afrikaner farmers, who wished to see a change in influx control due to problems with squatters, as well as higher prices for their maize and other produce in the face of the mineowners' demand for cheap food policies. Always identified with the affluent and capitalist, the party also failed to appeal to its working class constituents.[44]
Populist rhetoric allowed the National Party to sweep eight constituencies in the mining and industrial centres of the Witwatersrand and five more in Pretoria. Barring the predominantly English-speaking landowner electorate of the Natal, the United Party was defeated in almost every rural district. Its urban losses in the nation's most populous province, the Transvaal, proved equally devastating.[44] As the voting system was disproportionately weighted in favour of rural constituencies and the Transvaal in particular, the 1948 election catapulted the Herenigde Nasionale Party from a small minority party to a commanding position with an eight-vote parliamentary lead.[47][48] Daniel François Malan became the first nationalist prime minister, with the aim of implementing the apartheid philosophy and silencing liberal opposition.[41]
When the National Party came to power in 1948, there were factional differences in the party about the implementation of systemic racial segregation. The "baasskap" (white domination or supremacist) faction, which was the dominant faction in the NP, and state institutions, favoured systematic segregation, but also favoured the participation of black Africans in the economy with black labour controlled to advance the economic gains of Afrikaners. A second faction were the "purists", who believed in "vertical segregation", in which blacks and whites would be entirely separated, with blacks living in native reserves, with separate political and economic structures, which, they believed, would entail severe short-term pain, but would also lead to independence of white South Africa from black labour in the long term. A third faction, which included Hendrik Verwoerd, sympathised with the purists, but allowed for the use of black labour, while implementing the purist goal of vertical separation.[49] Verwoerd would refer to this policy as a policy of "good neighbourliness" as a means of justifying such segregation.[50]
NP leaders argued that South Africa did not comprise a single nation, but was made up of four distinct racial groups: white, black, Coloured and Indian. Such groups were split into 13 nations or racial federations. White people encompassed the English and Afrikaans language groups; the black populace was divided into ten such groups.
The state passed laws that paved the way for "grand apartheid", which was centred on separating races on a large scale, by compelling people to live in separate places defined by race. This strategy was in part adopted from "left-over" British rule that separated different racial groups after they took control of the Boer republics in the Anglo-Boer war. This created the black-only "townships" or "locations", where blacks were relocated to their own towns. As the NP government's minister of native affairs from 1950, Hendrik Verwoerd had a significant role in crafting such laws, which led to him being regarded as the 'Architect of Apartheid'.[51][50][52] In addition, "petty apartheid" laws were passed. The principal apartheid laws were as follows.[53]
The first grand apartheid law was the Population Registration Act of 1950, which formalised racial classification and introduced an identity card for all persons over the age of 18, specifying their racial group.[54] Official teams or boards were established to come to a conclusion on those people whose race was unclear.[55] This caused difficulty, especially for Coloured people, separating their families when members were allocated different races.[56]
The second pillar of grand apartheid was the Group Areas Act of 1950.[57] Until then, most settlements had people of different races living side by side. This Act put an end to diverse areas and determined where one lived according to race. Each race was allotted its own area, which was used in later years as a basis of forced removal.[58] The Prevention of Illegal Squatting Act of 1951 allowed the government to demolish black shanty town slums and forced white employers to pay for the construction of housing for those black workers who were permitted to reside in cities otherwise reserved for whites.[59] The Native Laws Amendment Act, 1952 centralised and tightened pass laws so that blacks could not stay in urban areas longer than 72 hours without a permit.[60]
The Prohibition of Mixed Marriages Act of 1949 prohibited marriage between persons of different races, and the Immorality Act of 1950 made sexual relations with a person of a different race a criminal offence.
Under the Reservation of Separate Amenities Act of 1953, municipal grounds could be reserved for a particular race, creating, among other things, separate beaches, buses, hospitals, schools and universities. Signboards such as "whites only" applied to public areas, even including park benches.[61] Black South Africans were provided with services greatly inferior to those of whites, and, to a lesser extent, to those of Indian and Coloured people.[62]
Further laws had the aim of suppressing resistance, especially armed resistance, to apartheid. The Suppression of Communism Act of 1950 banned the Communist Party of South Africa and any party subscribing to Communism. The act defined Communism and its aims so sweepingly that anyone who opposed government policy risked being labelled as a Communist. Since the law specifically stated that Communism aimed to disrupt racial harmony, it was frequently used to gag opposition to apartheid. Disorderly gatherings were banned, as were certain organisations that were deemed threatening to the government. It also empowered the Ministry of Justice to impose banning orders.[63]
After the Defiance Campaign, the government used the act for the mass arrests and banning of leaders of dissent groups such as the African National Congress (ANC), the South African Indian Congress (SAIC), and the South African Congress of Trade Unions (SACTU). After the release of the Freedom Charter, 156 leaders of these groups were charged in the 1956 Treason Trial. It established censorship of film, literature, and the media under the Customs and Excise Act 1955 and the Official Secrets Act 1956. The same year, the Native Administration Act 1956 allowed the government to banish blacks.[63]
The Bantu Authorities Act of 1951 created separate government structures for blacks and whites and was the first piece of legislation to support the government's plan of separate development in the bantustans. The Bantu Education Act, 1953 established a separate education system for blacks emphasizing African culture and vocational training under the Ministry of Native Affairs and defunded most mission schools.[64] The Promotion of Black Self-Government Act of 1959 entrenched the NP policy of nominally independent "homelands" for blacks. So-called "self–governing Bantu units" were proposed, which would have devolved administrative powers, with the promise later of autonomy and self-government. It also abolished the seats of white representatives of black South Africans and removed from the rolls the few blacks still qualified to vote. The Bantu Investment Corporation Act of 1959 set up a mechanism to transfer capital to the homelands to create employment there. Legislation of 1967 allowed the government to stop industrial development in "white" cities and redirect such development to the "homelands". The Black Homeland Citizenship Act of 1970 marked a new phase in the Bantustan strategy. It changed the status of blacks to citizens of one of the ten autonomous territories. The aim was to ensure a demographic majority of white people within South Africa by having all ten Bantustans achieve full independence.
Interracial contact in sport was frowned upon, but there were no segregatory sports laws.
The government tightened pass laws compelling blacks to carry identity documents, to prevent the immigration of blacks from other countries. To reside in a city, blacks had to be in employment there. Until 1956 women were for the most part excluded from these pass requirements, as attempts to introduce pass laws for women were met with fierce resistance.[65]
In 1950, D. F. Malan announced the NP's intention to create a Coloured Affairs Department.[66] J.G. Strijdom, Malan's successor as Prime Minister, moved to strip voting rights from black and Coloured residents of the Cape Province. The previous government had introduced the Separate Representation of Voters Bill into Parliament in 1951, turning it to be an Act on 18 June 1951; however, four voters, G Harris, W D Franklin, W D Collins and Edgar Deane, challenged its validity in court with support from the United Party.[67] The Cape Supreme Court upheld the act, but reversed by the Appeal Court, finding the act invalid because a two-thirds majority in a joint sitting of both Houses of Parliament was needed to change the entrenched clauses of the Constitution.[68] The government then introduced the High Court of Parliament Bill (1952), which gave Parliament the power to overrule decisions of the court.[69] The Cape Supreme Court and the Appeal Court declared this invalid too.[70]
In 1955 the Strijdom government increased the number of judges in the Appeal Court from five to 11, and appointed pro-Nationalist judges to fill the new places.[71] In the same year they introduced the Senate Act, which increased the Senate from 49 seats to 89.[72] Adjustments were made such that the NP controlled 77 of these seats.[73] The parliament met in a joint sitting and passed the Separate Representation of Voters Act in 1956, which transferred Coloured voters from the common voters' roll in the Cape to a new Coloured voters' roll.[74] Immediately after the vote, the Senate was restored to its original size. The Senate Act was contested in the Supreme Court, but the recently enlarged Appeal Court, packed with government-supporting judges, upheld the act, and also the Act to remove Coloured voters.[75]
The 1956 law allowed Coloureds to elect four people to Parliament, but a 1969 law abolished those seats and stripped Coloureds of their right to vote. Since Indians had never been allowed to vote, this resulted in whites being the sole enfranchised group.
A 2016 study in The Journal of Politics suggests that disenfranchisement in South Africa had a significant negative effect on basic service delivery to the disenfranchised.[76]
Before South Africa became a republic in 1961, politics among white South Africans was typified by the division between the mainly Afrikaner pro-republic conservative and the largely English anti-republican liberal sentiments,[77] with the legacy of the Boer War still a factor for some people. Once South Africa became a republic, Prime Minister Hendrik Verwoerd called for improved relations and greater accord between people of British descent and the Afrikaners.[78] He claimed that the only difference was between those in favour of apartheid and those against it. The ethnic division would no longer be between Afrikaans and English speakers, but between blacks and whites.[citation needed]
Most Afrikaners supported the notion of unanimity of white people to ensure their safety. White voters of British descent were divided. Many had opposed a republic, leading to a majority "no" vote in Natal.[79] Later, some of them recognised the perceived need for white unity, convinced by the growing trend of decolonisation elsewhere in Africa, which concerned them. British Prime Minister Harold Macmillan's "Wind of Change" speech left the British faction feeling that the United Kingdom had abandoned them.[80] The more conservative English speakers supported Verwoerd;[81] others were troubled by the severing of ties with the UK and remained loyal to the Crown.[82] They were displeased by having to choose between British and South African nationalities. Although Verwoerd tried to bond these different blocs, the subsequent voting illustrated only a minor swell of support,[83] indicating that a great many English speakers remained apathetic and that Verwoerd had not succeeded in uniting the white population.
Under the homeland system, the government attempted to divide South Africa and South West Africa into a number of separate states, each of which was supposed to develop into a separate nation-state for a different ethnic group.[84]
Territorial separation was hardly a new institution. There were, for example, the "reserves" created under the British government in the nineteenth century. Under apartheid, 13 percent of the land was reserved for black homelands, a small amount relative to its total population, and generally in economically unproductive areas of the country. The Tomlinson Commission of 1954 justified apartheid and the homeland system, but stated that additional land ought to be given to the homelands, a recommendation that was not carried out.[85]
When Verwoerd became Prime Minister in 1958, the policy of "separate development" came into being, with the homeland structure as one of its cornerstones. Verwoerd came to believe in the granting of independence to these homelands. The government justified its plans on the ostensible basis that "(the) government's policy is, therefore, not a policy of discrimination on the grounds of race or colour, but a policy of differentiation on the ground of nationhood, of different nations, granting to each self-determination within the borders of their homelands – hence this policy of separate development".[86] Under the homelands system, blacks would no longer be citizens of South Africa, becoming citizens of the independent homelands who worked in South Africa as foreign migrant labourers on temporary work permits. In 1958 the Promotion of Black Self-Government Act was passed, and border industries and the Bantu Investment Corporation were established to promote economic development and the provision of employment in or near the homelands. Many black South Africans who had never resided in their identified homeland were forcibly removed from the cities to the homelands.
The vision of a South Africa divided into multiple ethnostates appealed to the reform-minded Afrikaner intelligentsia, and it provided a more coherent philosophical and moral framework for the National Party's policies, while also providing a veneer of intellectual respectability to the controversial policy of so-called baasskap.[87][88][89]
In total, 20 homelands were allocated to ethnic groups, ten in South Africa proper and ten in South West Africa. Of these 20 homelands, 19 were classified as black, while one, Basterland, was set aside for a sub-group of Coloureds known as Basters, who are closely related to Afrikaners. Four of the homelands were declared independent by the South African government: Transkei in 1976, Bophuthatswana in 1977, Venda in 1979, and Ciskei in 1981 (known as the TBVC states). Once a homeland was granted its nominal independence, its designated citizens had their South African citizenship revoked and replaced with citizenship in their homeland. These people were then issued passports instead of passbooks. Citizens of the nominally autonomous homelands also had their South African citizenship circumscribed, meaning they were no longer legally considered South African.[90] The South African government attempted to draw an equivalence between their view of black citizens of the homelands and the problems which other countries faced through entry of illegal immigrants.
Bantustans within the borders of South Africa and South West Africa were classified by degree of nominal self-rule: 6 were "non-self-governing", 10 were "self-governing", and 4 were "independent". In theory, self-governing Bantustans had control over many aspects of their internal functioning but were not yet sovereign nations. Independent Bantustans (Transkei, Bophutatswana, Venda and Ciskei; also known as the TBVC states) were intended to be fully sovereign. In reality, they had no significant economic infrastructure and with few exceptions encompassed swaths of disconnected territory. This meant all the Bantustans were little more than puppet states controlled by South Africa.
Throughout the existence of the independent Bantustans, South Africa remained the only country to recognise their independence. Nevertheless, internal organisations of many countries, as well as the South African government, lobbied for their recognition. For example, upon the foundation of Transkei, the Swiss-South African Association encouraged the Swiss government to recognise the new state. In 1976, leading up to a United States House of Representatives resolution urging the President to not recognise Transkei, the South African government intensely lobbied lawmakers to oppose the bill.[91] Each TBVC state extended recognition to the other independent Bantustans while South Africa showed its commitment to the notion of TBVC sovereignty by building embassies in the TBVC capitals.
During the 1960s, 1970s and early 1980s, the government implemented a policy of "resettlement", to force people to move to their designated "group areas". Millions of people were forced to relocate. These removals included people relocated due to slum clearance programmes, labour tenants on white-owned farms, the inhabitants of the so-called "black spots" (black-owned land surrounded by white farms), the families of workers living in townships close to the homelands, and "surplus people" from urban areas, including thousands of people from the Western Cape (which was declared a "Coloured Labour Preference Area")[92] who were moved to the Transkei and Ciskei homelands. The best-publicised forced removals of the 1950s occurred in Johannesburg, when 60,000 people were moved to the new township of Soweto (an abbreviation for South Western Townships).[93][94]
Until 1955, Sophiatown had been one of the few urban areas where black people were allowed to own land, and was slowly developing into a multiracial slum. As industry in Johannesburg grew, Sophiatown became the home of a rapidly expanding black workforce, as it was convenient and close to town. It had the only swimming pool for black children in Johannesburg.[95] As one of the oldest black settlements in Johannesburg, it held an almost symbolic importance for the 50,000 black people it contained. Despite a vigorous ANC protest campaign and worldwide publicity, the removal of Sophiatown began on 9 February 1955 under the Western Areas Removal Scheme. In the early hours, heavily armed police forced residents out of their homes and loaded their belongings onto government trucks. The residents were taken to a large tract of land 19 kilometres (12 mi) from the city centre, known as Meadowlands, which the government had purchased in 1953. Meadowlands became part of a new planned black city called Soweto. Sophiatown was destroyed by bulldozers, and a new white suburb named Triomf (Triumph) was built in its place. This pattern of forced removal and destruction was to repeat itself over the next few years, and was not limited to black South Africans alone. Forced removals from areas like Cato Manor (Mkhumbane) in Durban, and District Six in Cape Town, where 55,000 Coloured and Indian people were forced to move to new townships on the Cape Flats, were carried out under the Group Areas Act of 1950. Nearly 600,000 Coloured, Indian and Chinese people were moved under the Group Areas Act. Some 40,000 whites were also forced to move when land was transferred from "white South Africa" into the black homelands.[96] In South-West Africa, the apartheid plan that instituted Bantustans was as a result of the so-called Odendaal Plan, a set of proposals from the Odendaal Commission of 1962–1964.[97]
The NP passed a string of legislation that became known as petty apartheid. The first of these was the Prohibition of Mixed Marriages Act 55 of 1949, prohibiting marriage between whites and people of other races. The Immorality Amendment Act 21 of 1950 (as amended in 1957 by Act 23) forbade "unlawful racial intercourse" and "any immoral or indecent act" between a white and a black, Indian or Coloured person.
Black people were not allowed to run businesses or professional practices in areas designated as "white South Africa" unless they had a permit – such being granted only exceptionally. They were required to move to the black "homelands" and set up businesses and practices there. Trains, hospitals and ambulances were segregated.[98] Because of the smaller numbers of white patients and the fact that white doctors preferred to work in white hospitals, conditions in white hospitals were much better than those in often overcrowded and understaffed, significantly underfunded black hospitals.[99] Residential areas were segregated and blacks were allowed to live in white areas only if employed as a servant and even then only in servants' quarters. Black people were excluded from working in white areas, unless they had a pass, nicknamed the dompas, also spelt dompass or dom pass. The most likely origin of this name is from the Afrikaans "verdomde pas" (meaning accursed pass),[100] although some commentators ascribe it to the Afrikaans words meaning "dumb pass". Only black people with "Section 10" rights (those who had migrated to the cities before World War II) were excluded from this provision. A pass was issued only to a black person with approved work. Spouses and children had to be left behind in black homelands. A pass was issued for one magisterial district (usually one town) confining the holder to that area only. Being without a valid pass made a person subject to arrest and trial for being an illegal migrant. This was often followed by deportation to the person's homeland and prosecution of the employer for employing an illegal migrant. Police vans patrolled white areas to round up blacks without passes. Black people were not allowed to employ whites in white South Africa.[101]
Although trade unions for black and Coloured workers had existed since the early 20th century, it was not until the 1980s reforms that a mass black trade union movement developed. Trade unions under apartheid were racially segregated, with 54 unions being white only, 38 for Indian and Coloured and 19 for black people. The Industrial Conciliation Act (1956) legislated against the creation of multi-racial trade unions and attempted to split existing multi-racial unions into separate branches or organisations along racial lines.[102]
Each black homeland controlled its own education, health and police systems. Blacks were not allowed to buy hard liquor. They were able to buy only state-produced poor quality beer (although this law was relaxed later). Public beaches, swimming pools, some pedestrian bridges, drive-in cinema parking spaces, graveyards, parks, and public toilets were segregated. Cinemas and theatres in white areas were not allowed to admit blacks. There were practically no cinemas in black areas. Most restaurants and hotels in white areas were not allowed to admit blacks except as staff. Blacks were prohibited from attending white churches under the Churches Native Laws Amendment Act of 1957, but this was never rigidly enforced, and churches were one of the few places races could mix without the interference of the law. Blacks earning 360 rand a year or more had to pay taxes while the white threshold was more than twice as high, at 750 rand a year. On the other hand, the taxation rate for whites was considerably higher than that for blacks.[citation needed]
Blacks could not acquire land in white areas. In the homelands, much of the land belonged to a "tribe", where the local chieftain would decide how the land had to be used. This resulted in whites owning almost all the industrial and agricultural lands and much of the prized residential land. Most blacks were stripped of their South African citizenship when the "homelands" became "independent", and they were no longer able to apply for South African passports. Eligibility requirements for a passport had been difficult for blacks to meet, the government contending that a passport was a privilege, not a right, and the government did not grant many passports to blacks. Apartheid pervaded culture as well as the law, and was entrenched by most of the mainstream media.
The population was classified into four groups: African, White, Indian and Coloured (capitalised to denote their legal definitions in South African law). The Coloured group included people regarded as being of mixed descent, including of Bantu, Khoisan, European and Malay ancestry. Many were descended from people brought to South Africa from other parts of the world, such as India, Sri Lanka, Madagascar and China as slaves and indentured workers.[103]
The Population Registration Act, (Act 30 of 1950), defined South Africans as belonging to one of three races: White, Black or Coloured. People of Indian ancestry were considered Coloured under this act. Appearance, social acceptance and descent were used to determine the qualification of an individual into one of the three categories. A white person was described by the act as one whose parents were both white and possessed the "habits, speech, education, deportment and demeanour" of a white person. Blacks were defined by the act as belonging to an African race or tribe. Lastly, Coloureds were those who could not be classified as black or white.[104]
The apartheid bureaucracy devised complex (and often arbitrary) criteria at the time that the Population Registration Act was implemented to determine who was Coloured. Minor officials would administer tests to determine if someone should be categorised either Coloured or White, or if another person should be categorised either Coloured or Black. The tests included the pencil test, in which a pencil was shoved into the subjects' curly hair and the subjects made to shake their head. If the pencil stuck they were deemed to be Black; if dislodged they were pronounced Coloured. Other tests involved examining the shapes of jaw lines and buttocks and pinching people to see what language they would say "Ouch" in.[105] As a result of these tests, different members of the same family found themselves in different race groups. Further tests determined membership of the various sub-racial groups of the Coloureds.
Discriminated against by apartheid, Coloureds were as a matter of state policy forced to live in separate townships, as defined in the Group Areas Act (1950),[106] in some cases leaving homes their families had occupied for generations, and received an inferior education, though better than that provided to Africans. They played an important role in the anti-apartheid movement: for example the African Political Organization established in 1902 had an exclusively Coloured membership.
Voting rights were denied to Coloureds in the same way that they were denied to Blacks from 1950 to 1983. However, in 1977 the NP caucus approved proposals to bring Coloureds and Indians into central government. In 1982, final constitutional proposals produced a referendum among Whites, and the Tricameral Parliament was approved. The Constitution was reformed the following year to allow the Coloured and Indian minorities participation in separate Houses in a Tricameral Parliament, and Botha became the first Executive State President. The idea was that the Coloured minority could be granted voting rights, but the Black majority were to become citizens of independent homelands.[104][106] These separate arrangements continued until the abolition of apartheid. The Tricameral reforms led to the formation of the (anti-apartheid) United Democratic Front as a vehicle to try to prevent the co-option of Coloureds and Indians into an alliance with Whites. The battles between the UDF and the NP government from 1983 to 1989 were to become the most intense period of struggle between left-wing and right-wing South Africans.
Education was segregated by the 1953 Bantu Education Act, which crafted a separate system of education for black South African students and was designed to prepare black people for lives as a labouring class.[107] In 1959 separate universities were created for black, Coloured and Indian people. Existing universities were not permitted to enroll new black students. The Afrikaans Medium Decree of 1974 required the use of Afrikaans and English on an equal basis in high schools outside the homelands.[108]
In the 1970s, the state spent ten times more per child on the education of white children than on black children within the Bantu Education system (the education system in black schools within white South Africa). Higher education was provided in separate universities and colleges after 1959. Eight black universities were created in the homelands. Fort Hare University in the Ciskei (now Eastern Cape) was to register only Xhosa-speaking students. Sotho, Tswana, Pedi and Venda speakers were placed at the newly founded University College of the North at Turfloop, while the University College of Zululand was launched to serve Zulu students. Coloureds and Indians were to have their own establishments in the Cape and Natal respectively.[109]
Each black homeland controlled its own education, health and police systems.
By 1948, before formal Apartheid, 10 universities existed in South Africa: four were Afrikaans, four for English, one for Blacks and a Correspondence University open to all ethnic groups. By 1981, under apartheid government, 11 new universities were built: seven for Blacks, one for Coloureds, one for Indians, one for Afrikaans and one dual-language medium Afrikaans and English.
Colonialism and apartheid had a major effect on Black and Coloured women, since they suffered both racial and gender discrimination.[110][111] Judith Nolde argues that in general, South African women were "deprive[d] [...] of their human rights as individuals" under the apartheid system.[112] Jobs were often hard to find. Many Black and Coloured women worked as agricultural or domestic workers, but wages were extremely low, if existent.[113] Children developed diseases caused by malnutrition and sanitation problems, and mortality rates were therefore high. The controlled movement of black and Coloured workers within the country through the Natives Urban Areas Act of 1923 and the pass laws separated family members from one another, because men could prove their employment in urban centres while most women were merely dependents; consequently, they risked being deported to rural areas.[114] Even in rural areas there were legal hurdles for women to own land, and outside the cities jobs were scarce.[115]
By the 1930s, association football mirrored the balkanised society of South Africa; football was divided into numerous institutions based on race: the (White) South African Football Association, the South African Indian Football Association (SAIFA), the South African African Football Association (SAAFA) and its rival the South African Bantu Football Association, and the South African Coloured Football Association (SACFA). Lack of funds to provide proper equipment would be noticeable in regards to black amateur football matches; this revealed the unequal lives black South Africans were subject to, in contrast to Whites, who were much better off financially.[116] Apartheid's social engineering made it more difficult to compete across racial lines. Thus, in an effort to centralise finances, the federations merged in 1951, creating the South African Soccer Federation (SASF), which brought Black, Indian, and Coloured national associations into one body that opposed apartheid. This was generally opposed more and more by the growing apartheid government, and – with urban segregation being reinforced with ongoing racist policies – it was harder to play football along these racial lines. In 1956, the Pretoria regime – the administrative capital of South Africa – passed the first apartheid sports policy; by doing so, it emphasised the White-led government's opposition to inter-racialism.
While football was plagued by racism, it also played a role in protesting apartheid and its policies. With the international bans from FIFA and other major sporting events, South Africa would be in the spotlight internationally. In a 1977 survey, white South Africans ranked the lack of international sport as one of the three most damaging consequences of apartheid.[117] By the mid-1950s, Black South Africans would also use media to challenge the "racialisation" of sports in South Africa; anti-apartheid forces had begun to pinpoint sport as the "weakness" of white national morale. Black journalists for the Johannesburg Drum magazine were the first to give the issue public exposure, with an intrepid special issue in 1955 that asked, "Why shouldn't our blacks be allowed in the SA team?"[117] As time progressed, international standing with South Africa would continue to be strained. In the 1980s, as the oppressive system was slowly collapsing the ANC and National Party started negotiations on the end of apartheid, football associations also discussed the formation of a single, non-racial controlling body. This unity process accelerated in the late 1980s and led to the creation, in December 1991, of an incorporated South African Football Association. On 3 July 1992, FIFA finally welcomed South Africa back into international football.
Sport has long been an important part of life in South Africa, and the boycotting of games by international teams had a profound effect on the white population, perhaps more so than the trade embargoes did. After the re-acceptance of South Africa's sports teams by the international community, sport played a major unifying role between the country's diverse ethnic groups. Mandela's open support of the predominantly white rugby fraternity during the 1995 Rugby World Cup was considered instrumental in bringing together South African sports fans of all races.[118]
Activities in the sport of professional boxing were also affected, as there were 44 recorded professional boxing fights for national titles as deemed "for Whites only" between 1955 and 1979,[119] and 397 fights as deemed "for non-Whites" between 1901 and 1978.[120]
The first fight for a national "White" title was held on April 9, 1955, between Flyweights Jerry Jooste and Tiny Corbett at the City Hall in Johannesburg; it was won by Jooste by a twelve rounds points decision.[121] The last one was between national "White" Light-Heavyweight champion Gerrie Bodenstein and challenger Mervin Smit on February 5, 1979, at the Joekies Ice Rink in Welkom, Free State. it was won by the champion by a fifth-round technical knockout.[122]
The first "non Whites" South African national championship bout on record apparently (the date appears as "uncertain" on the records) took place on May 1, 1901, between Andrew Jephtha and Johnny Arendse for the vacant Lightweight belt, Jephtha winning by knockout in round nineteen of a twenty rounds-scheduled match, in Cape Town.[123]
The last "non White" title bout took place on December 18, 1978, between Sipho Mange and Chris Kid Dlamini; Mange-Dlamini was the culminating fight of a boxing program that included several other "non White" championship contests. Mange won the vacant non-White Super Bantamweight title by outpointing Dlamini over twelve rounds at the Goodwood Showgrounds in Cape Town.[120]
Defining its Asian population, a minority that did not appear to belong to any of the initial three designated non-white groups, was a constant dilemma for the apartheid government.
The classification of "honorary white" (a term which would be ambiguously used throughout apartheid) was granted to immigrants from Japan, South Korea and Taiwan – countries with which South Africa maintained diplomatic and economic relations[124] – and to their descendants.
Indian South Africans during apartheid were classified many ranges of categories from "Asian" to "black"[clarification needed] to "Coloured"[clarification needed] and even the mono-ethnic category of "Indian", but never as white, having been considered "nonwhite" throughout South Africa's history. The group faced severe discrimination during the apartheid regime and were subject to numerous racialist policies.
In 2005, a retrospect study was done by Josephine C. Naidoo and Devi Moodley Rajab, where they interviewed a series of Indian South Africans about their experience living through apartheid; their study highlighted education, the workplace, and general day to day living. One participant who was a doctor said that it was considered the norm for Non-White and White doctors to mingle while working at the hospital but when there was any down time or breaks, they were to go back to their segregated quarters. Not only was there severe segregation for doctors, non-white, more specifically Indians, were paid three to four times less than their white counterparts. Many described being treated as a "third class citizen" due to the humiliation of the standard of treatment for non-white employees across many professions. Many Indians described a sense of justified superiority from whites due to the apartheid laws that, in the minds of White South Africans, legitimised those feelings. Another finding of this study was the psychological damage done to Indians living in South Africa during apartheid. One of the biggest long-term effects on Indians was the distrust of white South Africans. There was a strong degree of alienation that left a strong psychological feeling of inferiority.[125]
Chinese South Africans – who were descendants of migrant workers who came to work in the gold mines around Johannesburg in the late 19th century – were initially either classified as "Coloured" or "Other Asian" and were subject to numerous forms of discrimination and restriction.[126] It was not until 1984 that South African Chinese, increased to about 10,000, were given the same official rights as the Japanese, to be treated as whites in terms of the Group Areas Act, although they still faced discrimination and did not receive all the benefits/rights of their newly obtained honorary white status such as voting.[citation needed][127]
Indonesians arrived at the Cape of Good Hope as slaves until the abolishment of slavery during the 19th century. They were predominantly Muslim, were allowed religious freedom and formed their own ethnic group/community known as Cape Malays. They were classified as part of the Coloured racial group.[128] This was the same for South Africans of Malaysian descent who were also classified as part of the Coloured race and thus considered "not-white".[103] South Africans of Filipino descent were classified as "black" due to historical outlook on Filipinos by White South Africans, and many of them lived in Bantustans.[103]
The Lebanese population were somewhat of an anomaly during the apartheid era. Lebanese immigration to South Africa was chiefly Christian, and the group was originally classified as non-white; however, a court case in 1913 ruled that because Lebanese and Syrians originated from the Canaan region (the birthplace of Christianity and Judaism), they could not be discriminated against by race laws which targeted non-believers, and thus, were classified as white. The Lebanese community maintained their white status after the Population Registration Act came into effect; however, further immigration from the Middle East was restricted.[129]
Alongside apartheid, the National Party implemented a programme of social conservatism. Pornography,[130] gambling[131] and works from Marx, Lenin and other socialist thinkers[132] were banned. Cinemas, shops selling alcohol and most other businesses were forbidden from opening on Sundays.[133] Abortion,[134] homosexuality[135] and sex education were also restricted; abortion was legal only in cases of rape or if the mother's life was threatened.[134]
Television was not introduced until 1976 because the government viewed English programming as a threat to the Afrikaans language.[136] Television was run on apartheid lines – TV1 broadcast in Afrikaans and English (geared to a White audience), TV2 in Zulu and Xhosa, TV3 in Sotho, Tswana and Pedi (both geared to a Black audience), and TV4 mostly showed programmes for an urban Black audience.
Apartheid sparked significant internal resistance.[13] The government responded to a series of popular uprisings and protests with police brutality, which in turn increased local support for the armed resistance struggle.[137]
Internal resistance to the apartheid system in South Africa came from several sectors of society and saw the creation of organisations dedicated variously to peaceful protests, passive resistance and armed insurrection.
In 1949, the youth wing of the African National Congress (ANC) took control of the organisation and started advocating a radical black nationalist programme. The new young leaders proposed that white authority could only be overthrown through mass campaigns. In 1950 that philosophy saw the launch of the Programme of Action, a series of strikes, boycotts and civil disobedience actions that led to occasional violent clashes with the authorities.
In 1959, a group of disenchanted ANC members formed the Pan Africanist Congress (PAC), which organised a demonstration against pass books on 21 March 1960. One of those protests was held in the township of Sharpeville, where 69 people were killed by police in the Sharpeville massacre.
In the wake of Sharpeville, the government declared a state of emergency. More than 18,000 people were arrested, including leaders of the ANC and PAC, and both organisations were banned. The resistance went underground, with some leaders in exile abroad and others engaged in campaigns of domestic sabotage and terrorism.
In May 1961, before the declaration of South Africa as a Republic, an assembly representing the banned ANC called for negotiations between the members of the different ethnic groupings, threatening demonstrations and strikes during the inauguration of the Republic if their calls were ignored.
When the government overlooked them, the strikers (among the main organisers was a 42-year-old, Thembu-origin Nelson Mandela) carried out their threats. The government countered swiftly by giving police the authority to arrest people for up to twelve days and detaining many strike leaders amid numerous cases of police brutality.[138] Defeated, the protesters called off their strike. The ANC then chose to launch an armed struggle through a newly formed military wing, Umkhonto we Sizwe (MK), which would perform acts of sabotage on tactical state structures. Its first sabotage plans were carried out on 16 December 1961, the anniversary of the Battle of Blood River.
In the 1970s, the Black Consciousness Movement (BCM) was created by tertiary students influenced by the Black Power movement in the US. BCM endorsed black pride and African customs and did much to alter the feelings of inadequacy instilled among black people by the apartheid system. The leader of the movement, Steve Biko, was taken into custody on 18 August 1977 and was beaten to death in detention.
In 1976, secondary students in Soweto took to the streets in the Soweto uprising to protest against the imposition of Afrikaans as the only language of instruction. On 16 June, police opened fire on students protesting peacefully. According to official reports 23 people were killed, but the number of people who died is usually given as 176, with estimates of up to 700.[139][140][141] In the following years several student organisations were formed to protest against apartheid, and these organisations were central to urban school boycotts in 1980 and 1983 and rural boycotts in 1985 and 1986.
In parallel with student protests, labour unions started protest action in 1973 and 1974. After 1976 unions and workers are considered to have played an important role in the struggle against apartheid, filling the gap left by the banning of political parties. In 1979 black trade unions were legalised and could engage in collective bargaining, although strikes were still illegal. Economist Thomas Sowell wrote that basic supply and demand led to violations of Apartheid "on a massive scale" throughout the nation, simply because there were not enough white South African business owners to meet the demand for various goods and services. Large portions of the garment industry and construction of new homes, for example, were effectively owned and operated by blacks, who either worked surreptitiously or who circumvented the law with a white person as a nominal, figurehead manager.[142]
In 1983, anti-apartheid leaders determined to resist the tricameral parliament assembled to form the United Democratic Front (UDF) in order to coordinate anti-apartheid activism inside South Africa. The first presidents of the UDF were Archie Gumede, Oscar Mpetha and Albertina Sisulu; patrons were Archbishop Desmond Tutu, Dr Allan Boesak, Helen Joseph, and Nelson Mandela. Basing its platform on abolishing apartheid and creating a nonracial democratic South Africa, the UDF provided a legal way for domestic human rights groups and individuals of all races to organise demonstrations and campaign against apartheid inside the country. Churches and church groups also emerged as pivotal points of resistance. Church leaders were not immune to prosecution, and certain faith-based organisations were banned, but the clergy generally had more freedom to criticise the government than militant groups did. The UDF, coupled with the protection of the church, accordingly permitted a major role for Archbishop Desmond Tutu, who served both as a prominent domestic voice and international spokesperson denouncing apartheid and urging the creation of a shared nonracial state.[143]
Although the majority of whites supported apartheid, some 20 percent did not. Parliamentary opposition was galvanised by Helen Suzman, Colin Eglin and Harry Schwarz, who formed the Progressive Federal Party. Extra-parliamentary resistance was largely centred in the South African Communist Party and women's organisation the Black Sash. Women were also notable in their involvement in trade union organisations and banned political parties. The public intellectuals too, such as Nadine Gordimer the eminent author and winner of the Nobel Prize in Literature (1991), vehemently opposed the Apartheid regime and accordingly bolstered the movement against it.
South Africa's policies were subject to international scrutiny in 1960, when British Prime Minister Harold Macmillan criticised them during his Wind of Change speech in Cape Town. Weeks later, tensions came to a head in the Sharpeville massacre, resulting in more international condemnation. Soon afterwards, Prime Minister Hendrik Verwoerd announced a referendum on whether the country should become a republic. Verwoerd lowered the voting age for Whites to eighteen years of age and included Whites in South West Africa on the roll. The referendum on 5 October that year asked Whites; "Are you in favour of a Republic for the Union?", and 52 percent voted "Yes".[144]
As a consequence of this change of status, South Africa needed to reapply for continued membership of the Commonwealth, with which it had privileged trade links. India had become a republic within the Commonwealth in 1950, but it became clear that African and South and Southeast Asian member states would oppose South Africa due to its apartheid policies. As a result, South Africa withdrew from the Commonwealth on 31 May 1961, the day that the Republic came into existence.
We stand here today to salute the United Nations Organisation and its Member States, both singly and collectively, for joining forces with the masses of our people in a common struggle that has brought about our emancipation and pushed back the frontiers of racism.The apartheid system as an issue was first formally brought to the United Nations attention, in order to advocate for the Indians residing in South Africa. On June 22 of 1946, the Indian government requested that the discriminatory treatment of Indians living in South Africa be included on the agenda of the first General Assembly session.[146] In 1952, apartheid was again discussed in the aftermath of the Defiance Campaign, and the UN set up a task team to keep watch on the progress of apartheid and the racial state of affairs in South Africa. Although South Africa's racial policies were a cause for concern, most countries in the UN concurred that this was a domestic affair, which fell outside the UN's jurisdiction.[147]
In April 1960, the UN's conservative stance on apartheid changed following the Sharpeville massacre, and the Security Council for the first time agreed on concerted action against the apartheid regime. Resolution 134 called upon the nation of South Africa to abandon its policies implementing racial discrimination. The newly founded United Nations Special Committee Against Apartheid, scripted and passed Resolution 181 on August 7, 1963, which called upon all states to cease the sale and shipment of all ammunition and military vehicles to South Africa. This clause was finally declared mandatory on 4 November 1977, depriving South Africa of military aid. From 1964 onwards, the US and the UK discontinued their arms trade with South Africa. The Security Council also condemned the Soweto massacre in Resolution 392. In 1977, the voluntary UN arms embargo became mandatory with the passing of Resolution 418. In addition to isolating South Africa militarily, the United Nations General Assembly, encouraged the boycotting of oil sales to South Africa.[146] Other actions taken by the United Nations General Assembly include the request for all nations and organisations, "to suspend cultural, educational, sporting and other exchanges with the racist regime and with organisations or institutions in South Africa which practice apartheid".[146] Illustrating that over a long period of time, the United Nations was working towards isolating the state of South Africa, by putting pressure on the Apartheid regime.
After much debate, by the late-1980s, the United States, the United Kingdom, and 23 other nations had passed laws placing various trade sanctions on South Africa. A disinvestment from South Africa movement in many countries was similarly widespread, with individual cities and provinces around the world implementing various laws and local regulations forbidding registered corporations under their jurisdiction from doing business with South African firms, factories, or banks.[148]
Pope John Paul II was an outspoken opponent of apartheid. In 1985, while visiting the Netherlands, he gave an impassioned speech at the International Court of Justice condemning apartheid, proclaiming that "no system of apartheid or separate development will ever be acceptable as a model for the relations between peoples or races."[149] In September 1988, he made a pilgrimage to countries bordering South Africa, while demonstratively avoiding South Africa itself. During his visit to Zimbabwe, he called for economic sanctions against the South African government.[150]
The Organisation of African Unity (OAU) was created in 1963. Its primary objectives were to eradicate colonialism and improve social, political and economic situations in Africa. It censured apartheid and demanded sanctions against South Africa. African states agreed to aid the liberation movements in their fight against apartheid.[151] In 1969, fourteen nations from Central and East Africa gathered in Lusaka, Zambia, and formulated the Lusaka Manifesto, which was signed on 13 April by all of the countries in attendance except Malawi.[152] This manifesto was later taken on by both the OAU and the United Nations.[151]
The Lusaka Manifesto summarised the political situations of self-governing African countries, condemning racism and inequity, and calling for Black majority rule in all African nations.[153] It did not rebuff South Africa entirely, though, adopting an appeasing manner towards the apartheid government, and even recognizing its autonomy. Although African leaders supported the emancipation of Black South Africans, they preferred this to be attained through peaceful means.[154]
South Africa's negative response to the Lusaka Manifesto and rejection of a change to its policies brought about another OAU announcement in October 1971. The Mogadishu Declaration stated that South Africa's rebuffing of negotiations meant that its Black people could only be freed through military means, and that no African state should converse with the apartheid government.[155]
In 1966, B. J. Vorster became Prime Minister. He was not prepared to dismantle apartheid, but he did try to redress South Africa's isolation and to revitalise the country's global reputation, even those with Black majority rule in Africa. This he called his "Outward-Looking" policy.[156][157][158]
Vorster's willingness to talk to African leaders stood in contrast to Verwoerd's refusal to engage with leaders such as Abubakar Tafawa Balewa of Nigeria in 1962 and Kenneth Kaunda of Zambia in 1964. In 1966, he met the heads of the neighbouring states of Lesotho, Swaziland and Botswana. In 1967, he offered technological and financial aid to any African state prepared to receive it, asserting that no political strings were attached, aware that many African states needed financial aid despite their opposition to South Africa's racial policies. Many were also tied to South Africa economically because of their migrant labour population working down the South African mines. Botswana, Lesotho and Swaziland remained outspoken critics of apartheid, but were dependent on South African economic assistance.
Malawi was the first non-neighbouring country to accept South African aid. In 1967, the two states set out their political and economic relations. In 1969, Malawi was the only country at the assembly which did not sign the Lusaka Manifesto condemning South Africa's apartheid policy. In 1970, Malawian president Hastings Banda made his first and most successful official stopover in South Africa.
Associations with Mozambique followed suit and were sustained after that country won its sovereignty in 1975. Angola was also granted South African loans. Other countries which formed relationships with South Africa were Liberia, Ivory Coast, Madagascar, Mauritius, Gabon, Zaire (now the Democratic Republic of the Congo) and the Central African Republic. Although these states condemned apartheid (more than ever after South Africa's denunciation of the Lusaka Manifesto), South Africa's economic and military dominance meant that they remained dependent on South Africa to varying degrees[clarification needed].
South Africa's isolation in sport began in the mid-1950s and increased throughout the 1960s. Apartheid forbade multiracial sport, which meant that overseas teams, by virtue of them having players of different races, could not play in South Africa. In 1956, the International Table Tennis Federation severed its ties with the all-White South African Table Tennis Union, preferring the non-racial South African Table Tennis Board. The apartheid government responded by confiscating the passports of the Board's players so that they were unable to attend international games.
In 1959, the non-racial South African Sports Association (SASA) was formed to secure the rights of all players on the global field. After meeting with no success in its endeavours to attain credit by collaborating with White establishments, SASA approached the International Olympic Committee (IOC) in 1962, calling for South Africa's expulsion from the Olympic Games. The IOC sent South Africa a caution to the effect that, if there were no changes, they would be barred from competing at the 1964 Olympic Games in Tokyo. The changes were initiated, and in January 1963, the South African Non-Racial Olympic Committee (SANROC) was set up. The Anti-Apartheid Movement persisted in its campaign for South Africa's exclusion, and the IOC acceded in barring the country from the 1964 Olympic Games. South Africa selected a multi-racial team for the next Olympic Games, and the IOC opted for incorporation in the 1968 Mexico City Olympic Games. Because of protests from AAMs and African nations, however, the IOC was forced to retract the invitation.
Foreign complaints about South Africa's bigoted sports brought more isolation. Racially selected New Zealand sports teams toured South Africa, until the 1970 All Blacks rugby tour allowed Maori to enter the country under the status of "honorary Whites". Huge and widespread protests occurred in New Zealand in 1981 against the Springbok tour – the government spent $8,000,000 protecting games using the army and police force. A planned All Black tour to South Africa in 1985 remobilised the New Zealand protesters and it was cancelled. A "rebel tour" – not government sanctioned – went ahead in 1986, but after that sporting ties were cut, and New Zealand made a decision not to convey an authorised rugby team to South Africa until the end of apartheid.[159]
On 6 September 1966, Verwoerd was fatally stabbed at Parliament House by parliamentary messenger Dimitri Tsafendas. John Vorster took office shortly after, and announced that South Africa would no longer dictate to the international community what their teams should look like. Although this reopened the gate for international sporting meets, it did not signal the end of South Africa's racist sporting policies. In 1968, Vorster went against his policy by refusing to permit Basil D'Oliveira, a Coloured South African-born cricketer, to join the English cricket team on its tour to South Africa. Vorster said that the side had been chosen only to prove a point, and not on merit. D'Oliveira was eventually included in the team as the first substitute, but the tour was cancelled. Protests against certain tours brought about the cancellation of a number of other visits, including that of an England rugby team touring South Africa in 1969–70.
The first of the "White Bans" occurred in 1971 when the Chairman of the Australian Cricketing Association – Sir Don Bradman – flew to South Africa to meet Vorster. Vorster had expected Bradman to allow the tour of the Australian cricket team to go ahead, but things became heated after Bradman asked why Black sportsmen were not allowed to play cricket. Vorster stated that Blacks were intellectually inferior and had no finesse for the game. Bradman – thinking this ignorant and repugnant – asked Vorster if he had heard of a man named Garry Sobers. On his return to Australia, Bradman released a short statement: "We will not play them until they choose a team on a non-racist basis."[160] Bradman's views were in stark contrast to those of Australian tennis great Margaret Court, who had won the grand slam the previous year and commented about apartheid that "South Africans have this thing better organised than any other country, particularly America" and that she would "go back there any time."[161]
In South Africa, Vorster vented his anger publicly against Bradman, while the African National Congress rejoiced. This was the first time a predominantly White nation had taken the side of multiracial sport, producing an unsettling resonance that more "White" boycotts were coming.[162]
Almost twenty years later, on his release from prison, Nelson Mandela asked a visiting Australian statesman if Donald Bradman, his childhood hero, was still alive (Bradman lived until 2001).
In 1971, Vorster altered his policies even further by distinguishing multiracial from multinational sport. Multiracial sport, between teams with players of different races, remained outlawed; multinational sport, however, was now acceptable: international sides would not be subject to South Africa's racial stipulations.
In 1978, Nigeria boycotted the Commonwealth Games because New Zealand's sporting contacts with the South African government were not considered to be in accordance with the 1977 Gleneagles Agreement. Nigeria also led the 32-nation boycott of the 1986 Commonwealth Games because of UK Prime Minister Margaret Thatcher's ambivalent attitude towards sporting links with South Africa, significantly affecting the quality and profitability of the Games and thus thrusting apartheid into the international spotlight.[163]
In the 1960s, the Anti-Apartheid Movements began to campaign for cultural boycotts of apartheid South Africa. Artists were requested not to present or let their works be hosted in South Africa. In 1963, 45 British writers put their signatures to an affirmation approving of the boycott, and, in 1964, American actor Marlon Brando called for a similar affirmation for films. In 1965, the Writers' Guild of Great Britain called for a proscription on the sending of films to South Africa. Over sixty American artists signed a statement against apartheid and against professional links with the state. The presentation of some South African plays in the United Kingdom and the United States was also vetoed.[by whom?][citation needed] After the arrival of television in South Africa in 1975, the British Actors Union, Equity, boycotted the service, and no British programme concerning its associates could be sold to South Africa. Similarly, when home video grew popular in the 1980s, the Australian arm of CBS/Fox Video (now 20th Century Fox Home Entertainment) placed stickers on their VHS and Betamax cassettes which labeled exporting such cassettes to South Africa as "an infringement of copyright".[164] Sporting and cultural boycotts did not have the same effect as economic sanctions,[citation needed] but they did much to lift consciousness amongst normal South Africans of the global condemnation of apartheid.
While international opposition to apartheid grew, the Nordic countries – and Sweden in particular – provided both moral and financial support for the ANC.[165] On 21 February 1986 – a week before he was assassinated – Sweden's Prime Minister Olof Palme made the keynote address to the Swedish People's Parliament Against Apartheid held in Stockholm.[166] In addressing the hundreds of anti-apartheid sympathisers as well as leaders and officials from the ANC and the Anti-Apartheid Movement such as Oliver Tambo, Palme declared: "Apartheid cannot be reformed; it has to be eliminated."[167]
Other Western countries adopted a more ambivalent position. In Switzerland, the Swiss-South African Association lobbied on behalf of the South African government. The Nixon administration implemented a policy known as the Tar Baby Option, pursuant to which the US maintained close relations with the Apartheid South African government.[168] The Reagan administration evaded international sanctions and provided diplomatic support in international forums for the South African government. The United States also increased trade with the Apartheid regime, while describing the ANC as "a terrorist organisation."[169] Like the Reagan administration, the government of Margaret Thatcher termed this policy "constructive engagement" with the apartheid government, vetoing the imposition of UN economic sanctions. U.S. government justification for supporting the Apartheid regime were publicly given as a belief in "free trade" and the perception of the anti-communist South African government as a bastion against Marxist forces in Southern Africa, for example, by the military intervention of South Africa in the Angolan Civil War in support of right-wing insurgents fighting to topple the government. The U.K. government also declared the ANC a terrorist organisation,[170] and in 1987 Thatcher's spokesman, Bernard Ingham, famously said that anyone who believed that the ANC would ever form the government of South Africa was "living in cloud cuckoo land".[171] The American Legislative Exchange Council (ALEC), a conservative lobbying organisation, actively campaigned against divesting from South Africa throughout the 1980s.[172]
By the late-1980s, with no sign of a political resolution in South Africa, Western patience began to run out. By 1989, a bipartisan Republican/Democratic initiative in the US favoured economic sanctions (realised as the Comprehensive Anti-Apartheid Act of 1986), the release of Nelson Mandela and a negotiated settlement involving the ANC. Thatcher too began to take a similar line, but insisted on the suspension of the ANC's armed struggle.[173]
The UK's significant economic involvement in South Africa may have provided some leverage with the South African government, with both the UK and the US applying pressure and pushing for negotiations. However, neither the UK nor the US was willing to apply economic pressure upon their multinational interests in South Africa, such as the mining company Anglo American. Although a high-profile compensation claim against these companies was thrown out of court in 2004,[174] the US Supreme Court in May 2008 upheld an appeal court ruling allowing another lawsuit that seeks damages of more than US$400 billion from major international companies which are accused of aiding South Africa's apartheid system.[175]
During the 1950s, South African military strategy was decisively shaped by fears of communist espionage and a conventional Soviet threat to the strategic Cape trade route between the south Atlantic and Indian Oceans.[176] The apartheid government supported the US-led North Atlantic Treaty Organization (NATO), as well as its policy of regional containment against Soviet-backed regimes and insurgencies worldwide.[177] By the late-1960s, the rise of Soviet client states on the African continent, as well as Soviet aid for militant anti-apartheid movements, was considered one of the primary external threats to the apartheid system.[178] South African officials frequently accused domestic opposition groups of being communist proxies.[179] For its part, the Soviet Union viewed South Africa as a bastion of neocolonialism and a regional Western ally, which helped fuel its support for various anti-apartheid causes.[180]
From 1973 onwards, much of South Africa's white population increasingly looked upon their country as a bastion of the free world besieged militarily, politically, and culturally by Communism and radical black nationalism.[181] The apartheid government perceived itself as being locked in a proxy struggle with the Warsaw Pact and by implication, armed wings of black nationalist forces such as Umkhonto we Sizwe (MK) and the People's Liberation Army of Namibia (PLAN), which often received Soviet arms and training.[180] This was described as "Total Onslaught".[181][182]
Soviet support for militant anti-apartheid movements worked in the government's favour, as its claim to be reacting in opposition to aggressive communist expansion gained greater plausibility, and helped it justify its own domestic militarisation methods, known as "Total Strategy".[181] Total Strategy involved building up a formidable conventional military and counter-intelligence capability.[181] It was formulated on counter-revolutionary tactics as espoused by noted French tactician André Beaufre.[182] Considerable effort was devoted towards circumventing international arms sanctions, and the government even went so far as to develop nuclear weapons,[183] allegedly with covert assistance from Israel.[184] In 2010, The Guardian released South African government documents that revealed an Israeli offer to sell the apartheid regime nuclear weapons.[185][186] Israel denied these allegations and claimed that the documents were minutes from a meeting which did not indicate any concrete offer for a sale of nuclear weapons. Shimon Peres said that The Guardian's article was based on "selective interpretation ... and not on concrete facts."[187]
As a result of "Total Strategy", South African society became increasingly militarised. Many domestic civil organisations were modelled upon military structures, and military virtues such as discipline, patriotism, and loyalty were highly regarded.[188] In 1968, national service for White South African men lasted nine months at minimum, and they could be called up for reserve duty into their late-middle age if necessary.[189] The length of national service was gradually extended to 12 months in 1972 and 24 months in 1978.[189] At state schools, white male students were organised into paramilitary formations and drilled as cadets or as participants in a civil defence or "Youth Preparedness" curriculum.[188] Compulsory military education and in some cases, paramilitary training was introduced for all older white male students at state schools in three South African provinces.[188] These programmes presided over the construction of bomb shelters at schools and drills aimed at simulating mock insurgent raids.[188]
From the late 1970s to the late 1980s, defence budgets in South Africa were raised exponentially.[182] In 1975, Israeli defense minister Shimon Peres signed a security pact with South African defence minister P.W. Botha that led to $200 million in arms deals. In 1988, Israeli arm sales to South Africa totaled over $1.4 billion.[190] Covert operations focused on espionage and domestic counter-subversion became common, the number of special forces units swelled, and the South African Defence Force (SADF) had amassed enough sophisticated conventional weaponry to pose a serious threat to the "front-line states", a regional alliance of neighbouring countries opposed to apartheid.[182]
Total Strategy was advanced in the context of MK, PLAN, and Azanian People's Liberation Army (APLA) guerrilla raids into South Africa or against South African targets in South West Africa; frequent South African reprisal attacks on these movements' external bases in Angola, Zambia, Mozambique, Zimbabwe, and Botswana, often involving collateral damage to foreign infrastructure and civilian populations; and periodic complaints brought before the international community about South African violations of its neighbours' sovereignty.[191]
The apartheid government made judicious use of extraterritorial operations to eliminate its military and political opponents, arguing that neighbouring states, including their civilian populations, which hosted, tolerated on their soil, or otherwise sheltered anti-apartheid insurgent groups could not evade responsibility for provoking retaliatory strikes.[191] While it did focus on militarising the borders and sealing up its domestic territory against insurgent raids, it also relied heavily on an aggressive preemptive and counter-strike strategy, which fulfilled a preventive and deterrent purpose.[192] The reprisals which occurred beyond South Africa's borders involved not only hostile states, but neutral and sympathetic governments as well, often forcing them to react against their will and interests.[193]
External South African military operations were aimed at eliminating the training facilities, safehouses, infrastructure, equipment, and manpower of the insurgents.[192] However, their secondary objective was to dissuade neighbouring states from offering sanctuary to MK, PLAN, APLA, and similar organisations.[192] This was accomplished by deterring the supportive foreign population from cooperating with infiltration and thus undermining the insurgents' external sanctuary areas.[194] It would also send a clear message to the host government that collaborating with insurgent forces involved potentially high costs.[194]
The scale and intensity of foreign operations varied, and ranged from small special forces units carrying out raids on locations across the border which served as bases for insurgent infiltration to major conventional offensives involving armour, artillery, and aircraft.[192] Actions such as Operation Protea in 1981 and Operation Askari in 1983 involved both full scale conventional warfare and a counter-insurgency reprisal operation.[195][196] The insurgent bases were usually situated near military installations of the host government, so that SADF retaliatory strikes hit those facilities as well and attracted international attention and condemnation of what was perceived as aggression against the armed forces of another sovereign state.[197] This would inevitably result in major engagements, in which the SADF's expeditionary units would have to contend with the firepower of the host government's forces.[197] Intensive conventional warfare of this nature carried the risk of severe casualties among white soldiers, which had to be kept to a minimum for political reasons.[192] There were also high economic and diplomatic costs associated with openly deploying large numbers of South African troops into another country.[192] Furthermore, military involvement on that scale had the potential to evolve into wider conflict situations, in which South Africa became entangled.[192] For example, South Africa's activities in Angola, initially limited to containing PLAN, later escalated to direct involvement in the Angolan Civil War.[192]
As it became clearer that full-scale conventional operations could not effectively fulfill the requirements of a regional counter-insurgency effort, South Africa turned to a number of alternative methods. Retributive artillery bombardments were the least sophisticated means of reprisal against insurgent attacks. Between 1978 and 1979 the SADF directed artillery fire against locations in Angola and Zambia from which insurgent rockets were suspected to have been launched.[198][199] This precipitated several artillery duels with the Zambian Army.[199] Special forces raids were launched to harass PLAN and MK by liquidating prominent members of those movements, destroying their offices and safehouses, and seizing valuable records stored at these sites.[200] One example was the Gaborone Raid, carried out in 1985, during which a South African special forces team crossed the border into Botswana and demolished four suspected MK safe houses, severely damaging another four.[200] Other types of special forces operations included the sabotage of economic infrastructure.[201] The SADF sabotaged infrastructure being used for the insurgents' war effort; for example, port facilities in southern Angola's Moçâmedes District, where Soviet arms were frequently offloaded for PLAN, as well as the railway line which facilitated their transport to PLAN headquarters in Lubango, were common targets.[202] Sabotage was also used as a pressure tactic when South Africa was negotiating with a host government to cease providing sanctuary to insurgent forces, as in the case of Operation Argon.[203] Successful sabotage actions of high-profile economic targets undermined a country's ability to negotiate from a position of strength, and made it likelier to accede to South African demands rather than risk the expense of further destruction and war.[203]
Also noteworthy were South African transnational espionage efforts, which included covert assassinations, kidnappings, and attempts to disrupt the overseas influence of anti-apartheid organisations. South African military intelligence agents were known to have abducted and killed anti-apartheid activists and others suspected of having ties to MK in London and Brussels.[204][205]
During the 1980s the government, led by P.W. Botha, became increasingly preoccupied with security. It set up a powerful state security apparatus to "protect" the state against an anticipated upsurge in political violence that the reforms were expected to trigger. The 1980s became a period of considerable political unrest, with the government becoming increasingly dominated by Botha's circle of generals and police chiefs (known as securocrats), who managed the various States of Emergencies.[206]
Botha's years in power were marked also by numerous military interventions in the states bordering South Africa, as well as an extensive military and political campaign to eliminate SWAPO in Namibia. Within South Africa, meanwhile, vigorous police action and strict enforcement of security legislation resulted in hundreds of arrests and bans, and an effective end to the African National Congress' sabotage campaign.
The government punished political offenders brutally. 40,000 people annually were subjected to whipping as a form of punishment.[207] The vast majority had committed political offences and were lashed ten times for their crime.[208] If convicted of treason, a person could be hanged, and the government executed numerous political offenders in this way.[209]
As the 1980s progressed, more and more anti-apartheid organisations were formed and affiliated with the UDF. Led by the Reverend Allan Boesak and Albertina Sisulu, the UDF called for the government to abandon its reforms and instead abolish the apartheid system and eliminate the homelands completely.
Serious political violence was a prominent feature from 1985 to 1989, as Black townships became the focus of the struggle between anti-apartheid organisations and the Botha government. Throughout the 1980s, township people resisted apartheid by acting against the local issues that faced their particular communities. The focus of much of this resistance was against the local authorities and their leaders, who were seen to be supporting the government. By 1985, it had become the ANC's aim to make Black townships "ungovernable" (a term later replaced by "people's power") by means of rent boycotts and other militant action. Numerous township councils were overthrown or collapsed, to be replaced by unofficial popular organisations, often led by militant youth. People's courts were set up, and residents accused of being government agents were dealt extreme and occasionally lethal punishments. Black town councillors and policemen, and sometimes their families, were attacked with petrol bombs, beaten, and murdered by necklacing, where a burning tyre was placed around the victim's neck, after they were restrained by wrapping their wrists with barbed wire. 
On 20 July 1985, Botha declared a State of Emergency in 36 magisterial districts. Areas affected were the Eastern Cape, and the PWV region ("Pretoria, Witwatersrand, Vereeniging").[210] Three months later, the Western Cape was included. An increasing number of organisations were banned or listed (restricted in some way); many individuals had restrictions such as house arrest imposed on them. During this state of emergency, about 2,436 people were detained under the Internal Security Act.[211] This act gave police and the military sweeping powers. The government could implement curfews controlling the movement of people. The president could rule by decree without referring to the constitution or to parliament. It became a criminal offence to threaten someone verbally or possess documents that the government perceived to be threatening, to advise anyone to stay away from work or to oppose the government, and to disclose the name of anyone arrested under the State of Emergency until the government released that name, with up to ten years' imprisonment for these offences. Detention without trial became a common feature of the government's reaction to growing civil unrest and by 1988, 30,000 people had been detained.[212] The media was censored, thousands were arrested and many were interrogated and tortured.[213]
On 12 June 1986, four days before the tenth anniversary of the Soweto uprising, the state of emergency was extended to cover the whole country. The government amended the Public Security Act, including the right to declare "unrest" areas, allowing extraordinary measures to crush protests in these areas. Severe censorship of the press became a dominant tactic in the government's strategy and television cameras were banned from entering such areas. The state broadcaster, the South African Broadcasting Corporation (SABC), provided propaganda in support of the government. Media opposition to the system increased, supported by the growth of a pro-ANC underground press within South Africa.
In 1987, the State of Emergency was extended for another two years. Meanwhile, about 200,000 members of the National Union of Mineworkers commenced the longest strike (three weeks) in South African history. The year 1988 saw the banning of the activities of the UDF and other anti-apartheid organisations.
Much of the violence in the late-1980s and early-1990s was directed at the government, but a substantial amount was between the residents themselves. Many died in violence between members of Inkatha and the UDF-ANC faction. It was later proven that the government manipulated the situation by supporting one side or the other whenever it suited them. Government agents assassinated opponents within South Africa and abroad; they undertook cross-border army and air-force attacks on suspected ANC and PAC bases. The ANC and the PAC in return detonated bombs at restaurants, shopping centres and government buildings such as magistrates courts. Between 1960 and 1994, according to statistics from the Truth and Reconciliation Commission, the Inkatha Freedom Party was responsible for 4,500 deaths, South African security forces were responsible for 2,700 deaths and the ANC was responsible for 1,300 deaths.[214]
The state of emergency continued until 1990 when it was lifted by State President F. W. de Klerk.
Apartheid developed from the racism of colonial factions and due to South Africa's "unique industrialisation".[215] The policies of industrialisation led to the segregation of and classing of people, which was "specifically developed to nurture early industry such as mining".[215] Cheap labour was the basis of the economy and this was taken from what the state classed as peasant groups and the migrants.[216] Furthermore, Philip Bonner highlights the "contradictory economic effects" as the economy did not have a manufacturing sector, therefore promoting short term profitability but limiting labour productivity and the size of local markets. This also led to its collapse as "Clarkes emphasises the economy could not provide and compete with foreign rivals as they failed to master cheap labour and complex chemistry".[217]
The contradictions[clarification needed] in the traditionally capitalist economy of the apartheid state led to considerable debate about racial policy, and division and conflicts in the central state.[218] To a large extent, the political ideology of apartheid had emerged from the colonisation of Africa by European powers which institutionalised racial discrimination and exercised a paternal philosophy of "civilising inferior natives."[218] Some scholars have argued that this can be reflected in Afrikaner Calvinism, with its parallel traditions of racialism;[219] for example, as early as 1933; the executive council of the Broederbond formulated a recommendation for mass segregation.[219]
External Western influence, arising from European experiences in colonisation, may be seen as a factor which greatly influenced political attitudes and ideology. Late twentieth-century South Africa was cited as an "unreconstructed example of western civilisation twisted by racism".[220]
In the 1960s, South Africa experienced economic growth second only to that of Japan.[221] Trade with Western countries grew, and investment from the United States, France, and the United Kingdom poured in.
In 1974, resistance to apartheid was encouraged by Portuguese withdrawal from Mozambique and Angola, after the 1974 Carnation Revolution. South African troops withdrew from Angola early in 1976, failing to prevent the MPLA from gaining power there, and Black students in South Africa celebrated.
The Mahlabatini Declaration of Faith, signed by Mangosuthu Buthelezi and Harry Schwarz in 1974, enshrined the principles of peaceful transition of power and equality for all. Its purpose was to provide a blueprint for South Africa by consent and racial peace in a multi-racial society, stressing opportunity for all, consultation, the federal concept, and a Bill of Rights. It caused a split in the United Party that ultimately realigned oppositional politics in South Africa with the formation of the Progressive Federal Party in 1977. The Declaration was the first of several such joint agreements by acknowledged Black and White political leaders in South Africa.
In 1978, the National Party Defence Minister, Pieter Willem Botha, became Prime Minister. His white minority regime worried about Soviet aid to revolutionaries in South Africa at the same time that South African economic growth had slowed. The South African Government noted that it was spending too much money to maintain segregated homelands created for Blacks, and the homelands were proving to be uneconomical.[222]
Nor was maintaining Blacks as third-class citizens working well. Black labour remained vital to the economy, and illegal Black labour unions were flourishing. Many Blacks remained too poor to contribute significantly to the economy through their purchasing power – although they composed more than 70% of the population. Botha's regime feared that an antidote was needed to prevent the Blacks' being attracted to Communism.[223]
In July 1979, the Nigerian Government alleged that the Shell-BP Petroleum Development Company of Nigeria Limited (SPDC) was selling Nigerian oil to South Africa, though there was little evidence or commercial logic for such sales.[224] The alleged sanctions-breaking was used to justify the seizure of some of BP's assets in Nigeria including their stake in SPDC, although it appears the real reasons were economic nationalism and domestic politics ahead of the Nigerian elections.[225] Many South Africans attended schools in Nigeria,[226] and Nelson Mandela acknowledged the role of Nigeria in the struggle against apartheid on several occasions.[227]
In the 1980s, anti-apartheid movements in the United States and Europe were gaining support for boycotts against South Africa, for the withdrawal of US companies from South Africa, and for release of imprisoned Nelson Mandela. South Africa was sinking to the bottom of the international community. Investment in South Africa was ending and an active policy of disinvestment had begun.
In the early-1980s, Botha's National Party government started to recognise the inevitability of the need to reform the apartheid system.[228] Early reforms were driven by a combination of internal violence, international condemnation, changes within the National Party's constituency, and changing demographics – whites constituted only 16% of the total population, in comparison to 20% fifty years earlier.[229]
In 1983, a new constitution was passed implementing what was called the Tricameral Parliament, giving Coloureds and Indians voting rights and parliamentary representation in separate houses – the House of Assembly (178 members) for Whites, the House of Representatives (85 members) for Coloureds and the House of Delegates (45 members) for Indians.[230] Each House handled laws pertaining to its racial group's "own affairs", including health, education and other community issues.[231] All laws relating to "general affairs" (matters such as defence, industry, taxation and Black affairs) were handled by a Cabinet made up of representatives from all three houses. However, the White chamber had a large majority on this Cabinet, ensuring that effective control of the country remained in the hands of the White minority.[232][233] Blacks, although making up the majority of the population, were excluded from representation; they remained nominal citizens of their homelands.[234] The first Tricameral elections were largely boycotted by Coloured and Indian voters, amid widespread rioting.[235]
Concerned over the popularity of Mandela, Botha denounced him as an arch-Marxist committed to violent revolution, but to appease Black opinion and nurture Mandela as a benevolent leader of Blacks,[222] the government transferred him from the maximum security Robben Island to the lower security Pollsmoor Prison just outside Cape Town; where prison life was more comfortable for him. The government allowed Mandela more visitors, including visits and interviews by foreigners, to let the world know that he was being treated well.[222]
Black homelands were declared nation-states and pass laws were abolished. Black labour unions were legitimised, the government recognised the right of Blacks to live in urban areas permanently and gave Blacks property rights there. Interest was expressed in rescinding the law against interracial marriage and also rescinding the law against sexual relations between different races, which was under ridicule abroad. The spending for Black schools increased, to one-seventh of what was spent per white child, up from on one-sixteenth in 1968. At the same time, attention was given to strengthening the effectiveness of the police apparatus.
In January 1985, Botha addressed the government's House of Assembly and stated that the government was willing to release Mandela on condition that Mandela pledge opposition to acts of violence to further political objectives. Mandela's reply was read in public by his daughter Zinzi – his first words distributed publicly since his sentence to prison 21 years earlier. Mandela described violence as the responsibility of the apartheid regime and said that with democracy there would be no need for violence. The crowd listening to the reading of his speech erupted in cheers and chants. This response helped to further elevate Mandela's status in the eyes of those, both internationally and domestically, who opposed apartheid.
Between 1986 and 1988, some petty apartheid laws were repealed, along with the pass laws.[236] Botha told White South Africans to "adapt or die"[237] and twice he wavered on the eve of what were billed as "rubicon" announcements of substantial reforms, although on both occasions he backed away from substantial changes. Ironically, these reforms served only to trigger intensified political violence through the remainder of the 1980s as more communities and political groups across the country joined the resistance movement. Botha's government stopped short of substantial reforms, such as lifting the ban on the ANC, PAC and SACP and other liberation organisations, releasing political prisoners, or repealing the foundation laws of grand apartheid. The government's stance was that they would not contemplate negotiating until those organisations "renounced violence".
By 1987, South Africa's economy was growing at one of the lowest rates in the world, and the ban on South African participation in international sporting events was frustrating many whites in South Africa. Examples of African states with Black leaders and White minorities existed in Kenya and Zimbabwe. Whispers of South Africa one day having a Black President sent more hardline whites into supporting right-wing political parties. Mandela was moved to a four-bedroom house of his own, with a swimming pool and shaded by fir trees, on a prison farm just outside of Cape Town. He had an unpublicised meeting with Botha. Botha impressed Mandela by walking forward, extending his hand and pouring Mandela's tea. The two had a friendly discussion, with Mandela comparing the African National Congress' rebellion with that of the Afrikaner rebellion and talking about everyone being brothers.
A number of clandestine meetings were held between the ANC-in-exile and various sectors of the internal struggle, such as women and educationalists. More overtly, a group of White intellectuals met the ANC in Senegal for talks known as the Dakar Conference.[238]
Early in 1989, Botha had a stroke; he was prevailed upon to resign in February 1989.[239] He was succeeded as president later that year by F. W. de Klerk. Despite his initial reputation as a conservative, de Klerk moved decisively towards negotiations to end the political stalemate in the country. Prior to his term in office, F. W. de Klerk had already experienced political success as a result of the power base he had built in the Transvaal. During this time, F. W. de Klerk served as chairman to the provincial National Party, which was in favour of the Apartheid regime. The transition of de Klerk's ideology regarding apartheid is seen clearly in his opening address to parliament on 2 February 1990. F. W. de Klerk announced that he would repeal discriminatory laws and lift the 30-year ban on leading anti-apartheid groups such as the African National Congress, the Pan Africanist Congress, the South African Communist Party (SACP) and the United Democratic Front. The Land Act was brought to an end. F. W. de Klerk also made his first public commitment to release Nelson Mandela, to return to press freedom and to suspend the death penalty. Media restrictions were lifted and political prisoners not guilty of common law crimes were released.
On 11 February 1990, Nelson Mandela was released from Victor Verster Prison after more than 27 years behind bars.
Having been instructed by the UN Security Council to end its long-standing involvement in South West Africa/Namibia, and in the face of military stalemate in Southern Angola, and an escalation in the size and cost of the combat with the Cubans, the Angolans, and SWAPO forces and the growing cost of the border war, South Africa negotiated a change of control; Namibia became independent on 21 March 1990.
Apartheid was dismantled in a series of negotiations from 1990 to 1991, culminating in a transitional period which resulted in the country's 1994 general election, the first in South Africa held with universal suffrage.
In 1990, negotiations were earnestly begun, with two meetings between the government and the ANC. The purpose of the negotiations was to pave the way for talks towards a peaceful transition towards majority rule. These meetings were successful in laying down the preconditions for negotiations, despite the considerable tensions still abounding within the country. Apartheid legislation was abolished in 1991.[2]
At the first meeting, the NP and ANC discussed the conditions for negotiations to begin. The meeting was held at Groote Schuur, the President's official residence. They released the Groote Schuur Minute, which said that before negotiations commenced political prisoners would be freed and all exiles allowed to return.
There were fears that the change of power would be violent. To avoid this, it was essential that a peaceful resolution between all parties be reached. In December 1991, the Convention for a Democratic South Africa (CODESA) began negotiations on the formation of a multiracial transitional government and a new constitution extending political rights to all groups. CODESA adopted a Declaration of Intent and committed itself to an "undivided South Africa".
Reforms and negotiations to end apartheid led to a backlash among the right-wing White opposition, leading to the Conservative Party winning a number of by-elections against NP candidates. De Klerk responded by calling a Whites-only referendum in March 1992 to decide whether negotiations should continue. 68% voted in favour, and the victory instilled in de Klerk and the government a lot more confidence, giving the NP a stronger position in negotiations.
When negotiations resumed in May 1992, under the tag of CODESA II, stronger demands were made. The ANC and the government could not reach a compromise on how power should be shared during the transition to democracy. The NP wanted to retain a strong position in a transitional government, and the power to change decisions made by parliament.
Persistent violence added to the tension during the negotiations. This was due mostly to the intense rivalry between the Inkatha Freedom Party (IFP) and the ANC and the eruption of some traditional tribal and local rivalries between the Zulu and Xhosa historical tribal affinities, especially in the Southern Natal provinces. Although Mandela and Buthelezi met to settle their differences, they could not stem the violence. One of the worst cases of ANC-IFP violence was the Boipatong massacre of 17 June 1992, when 200 IFP militants attacked the Gauteng township of Boipatong, killing 45. Witnesses said that the men had arrived in police vehicles, supporting claims that elements within the police and army contributed to the ongoing violence. Subsequent judicial inquiries found the evidence of the witnesses to be unreliable or discredited, and that there was no evidence of National Party or police involvement in the massacre. When de Klerk visited the scene of the incident he was initially warmly welcomed, but he was suddenly confronted by a crowd of protesters brandishing stones and placards. The motorcade sped from the scene as police tried to hold back the crowd. Shots were fired by the police, and the PAC stated that three of its supporters had been gunned down.[240] Nonetheless, the Boipatong massacre offered the ANC a pretext to engage in brinkmanship. Mandela argued that de Klerk, as head of state, was responsible for bringing an end to the bloodshed. He also accused the South African police of inciting the ANC-IFP violence. This formed the basis for ANC's withdrawal from the negotiations, and the CODESA forum broke down completely at this stage.
The Bisho massacre on 7 September 1992 brought matters to a head. The Ciskei Defence Force killed 29 people and injured 200 when they opened fire on ANC marchers demanding the reincorporation of the Ciskei homeland into South Africa. In the aftermath, Mandela and de Klerk agreed to meet to find ways to end the spiralling violence. This led to a resumption of negotiations.
Right-wing violence also added to the hostilities of this period. The assassination of Chris Hani on 10 April 1993 threatened to plunge the country into chaos. Hani, the popular General Secretary of the South African Communist Party (SACP), was assassinated in 1993 in Dawn Park in Johannesburg by Janusz Waluś, an anti-Communist Polish refugee who had close links to the White nationalist Afrikaner Weerstandsbeweging (AWB). Hani enjoyed widespread support beyond his constituency in the SACP and ANC and had been recognised as a potential successor to Mandela; his death brought forth protests throughout the country and across the international community, but ultimately proved a turning point, after which the main parties pushed for a settlement with increased determination.[241] On 25 June 1993, the AWB used an armoured vehicle to crash through the doors of the Kempton Park World Trade Centre where talks were still going ahead under the Negotiating Council, though this did not derail the process.
In addition to the continuing "black-on-black" violence, there were a number of attacks on white civilians by the PAC's military wing, the Azanian People's Liberation Army (APLA). The PAC was hoping to strengthen their standing by attracting the support of the angry, impatient youth. In the St James Church massacre on 25 July 1993, members of the APLA opened fire in a church in Cape Town, killing 11 members of the congregation and wounding 58.
In 1993, de Klerk and Mandela were jointly awarded the Nobel Peace Prize "for their work for the peaceful termination of the apartheid regime, and for laying the foundations for a new democratic South Africa".[242]
Violence persisted right up to the 1994 general election. Lucas Mangope, leader of the Bophuthatswana homeland, declared that it would not take part in the elections. It had been decided that, once the temporary constitution had come into effect, the homelands would be incorporated into South Africa, but Mangope did not want this to happen. There were strong protests against his decision, leading to a coup d'état in Bophuthatswana carried out by the SDF on 10 March that deposed Mangope. AWB militants attempted to intervene in hopes of maintaining Mangope in power. Fighting alongside black paramilitaries loyal to Mangope they were unsuccessful, with 3 AWB militants being killed during this intervention, and harrowing images of the bloodshed shown on national television and in newspapers across the world.
Two days before the election, a car bomb exploded in Johannesburg, killing nine people.[243][244] The day before the elections, another one went off, injuring 13. At midnight on 26–27 April 1994 the previous "orange white blue" flag adopted in 1928 was lowered, and the old (now co-official) national anthem Die Stem ("The Call") was sung, followed by the raising of the new Y shaped flag and singing of the other co-official anthem, Nkosi Sikelel' iAfrika ("God Bless Africa").
Since 2019, publicly displaying the 1928–1994 flag in South Africa is banned and it is classified as hate speech.[245]
The election was held on 27 April 1994 and went off peacefully throughout the country as 20,000,000 South Africans cast their votes. There was some difficulty in organising the voting in rural areas, but people waited patiently for many hours to vote amidst a palpable feeling of goodwill. An extra day was added to give everyone the chance. International observers agreed that the elections were free and fair.[246] The European Union's report on the election compiled at the end of May 1994, published two years after the election, criticised the Independent Electoral Commission's lack of preparedness for the polls, the shortages of voting materials at many voting stations, and the absence of effective safeguards against fraud in the counting process. In particular, it expressed disquiet that "no international observers had been allowed to be present at the crucial stage of the count when party representatives negotiated over disputed ballots." This meant that both the electorate and the world were "simply left to guess at the way the final result was achieved."[247]
The ANC won 62.65% of the vote,[248][249] less than the 66.7 percent that would have allowed it to rewrite the constitution. 252 of the 400 seats went to members of the African National Congress. The NP captured most of the White and Coloured votes and became the official opposition party. As well as deciding the national government, the election decided the provincial governments, and the ANC won in seven of the nine provinces, with the NP winning in the Western Cape and the IFP in KwaZulu-Natal. On 10 May 1994, Mandela was sworn in as the new President of South Africa. The Government of National Unity was established, its cabinet made up of 12 ANC representatives, six from the NP, and three from the IFP. Thabo Mbeki and de Klerk were made deputy presidents.
The anniversary of the elections, 27 April, is celebrated as a public holiday known as Freedom Day.
The following individuals, who had previously supported apartheid, made public apologies:
The South African experience has given rise to the term "apartheid" being used in a number of contexts other than the South African system of racial segregation. For example: 
The "crime of apartheid" is defined in international law, including in the 2007 law that created the International Criminal Court (ICC), which names it as a crime against humanity. Even before the creation of the ICC, the International Convention on the Suppression and Punishment of the Crime of Apartheid of the United Nations, which came into force in 1976, enshrined into law the "crime of apartheid."[256]
The term apartheid has been adopted by Palestinian rights advocates and by leading Israeli and other human rights organizations, referring to occupation in the West Bank, legal treatment of illegal settlements and the West Bank barrier.[257][258][259][260] Within the pre-1967 Israeli borders, Palestinian rights advocates have raised concern over discriminatory housing planning against Palestinian citizens of Israel, likening it to racial segregation.[261]
Social apartheid is segregation on the basis of class or economic status. For example, social apartheid in Brazil refers to the various aspects of economic inequality in Brazil. Social apartheid may fall into various categories. Economic and social discrimination because of gender is sometimes referred to as gender apartheid. Separation of people according to their religion, whether pursuant to official laws or pursuant to social expectations, is sometimes referred to as religious apartheid. Communities in northern Ireland for example, are often housed based on religion in a situation which has been described as "self imposed apartheid".[262] The treatment of non-Muslims and women by the Saudi rulers has also been called apartheid.
The concept in occupational therapy that individuals, groups and communities can be deprived of meaningful and purposeful activity through segregation due to social, political, economic factors and for social status reasons, such as race, disability, age, gender, sexuality, religious preference, political preference, or creed, or due to war conditions, is sometimes known as occupational apartheid.
A 2007 book by Harriet A. Washington on the history of medical experimentation on African Americans is entitled Medical Apartheid.
The disproportionate management and control of the world's economy and resources by countries and companies of the Global North has been referred to as global apartheid. A related phenomenon is technological apartheid, a term used to describe the denial of modern technologies to Third World or developing nations. The last two examples use the term "apartheid" less literally since they are centered on relations between countries, not on disparate treatment of social populations within a country or political jurisdiction.



90°N 0°E﻿ / ﻿90°N 0°E﻿ / 90; 0
The North Pole, also known as the Geographic North Pole, Terrestrial North Pole or 90th Parallel North, is the point in the Northern Hemisphere where the Earth's axis of rotation meets its surface. It is called the True North Pole to distinguish from the Magnetic North Pole.
The North Pole is by definition the northernmost point on the Earth, lying antipodally to the South Pole. It defines geodetic latitude 90° North, as well as the direction of true north. At the North Pole all directions point south; all lines of longitude converge there, so its longitude can be defined as any degree value. No time zone has been assigned to the North Pole, so any time can be used as the local time. Along tight latitude circles, counterclockwise is east and clockwise is west. The North Pole is at the center of the Northern Hemisphere. The nearest land is usually said to be Kaffeklubben Island, off the northern coast of Greenland about 700 km (430 mi) away, though some perhaps semi-permanent gravel banks lie slightly closer. The nearest permanently inhabited place is Alert on Ellesmere Island, Canada, which is located 817 km (508 mi) from the Pole.
While the South Pole lies on a continental land mass, the North Pole is located in the middle of the Arctic Ocean amid waters that are almost permanently covered with constantly shifting sea ice.  The sea depth at the North Pole has been measured at 4,261 m (13,980 ft) by the Russian Mir submersible in 2007[1] and at  4,087 m (13,409 ft) by USS Nautilus in 1958.[2][3] This makes it impractical to construct a permanent station at the North Pole (unlike the South Pole). However, the Soviet Union, and later Russia, constructed a number of manned drifting stations on a generally annual basis since 1937, some of which have passed over or very close to the Pole. Since 2002, a group of Russians have also annually established a private base, Barneo, close to the Pole. This operates for a few weeks during early spring. Studies in the 2000s predicted that the North Pole may become seasonally ice-free because of Arctic ice shrinkage, with timescales varying from 2016[4][5] to the late 21st century or later.
Attempts to reach the North Pole began in the late 19th century, with the record for "Farthest North" being surpassed on numerous occasions. The first undisputed expedition to reach the North Pole was that of the airship Norge, which overflew the area in 1926 with 16 men on board, including expedition leader Roald Amundsen. Three prior expeditions – led by Frederick Cook (1908, land), Robert Peary (1909, land) and Richard E. Byrd (1926, aerial) – were once also accepted as having reached the Pole. However, in each case later analysis of expedition data has cast doubt upon the accuracy of their claims. The first confirmed overland expedition to reach the North Pole was in 1968 by Ralph Plaisted, Walt Pederson, Gerry Pitzl and Jean-Luc Bombardier, using snowmobiles and with air support.[6]
The Earth's axis of rotation – and hence the position of the North Pole – was commonly believed to be fixed (relative to the surface of the Earth) until, in the 18th century, the mathematician Leonhard Euler predicted that the axis might "wobble" slightly. Around the beginning of the 20th century astronomers noticed a small apparent "variation of latitude", as determined for a fixed point on Earth from the observation of stars. Part of this variation could be attributed to a wandering of the Pole across the Earth's surface, by a range of a few metres. The wandering has several periodic components and an irregular component. The component with a period of about 435 days is identified with the eight-month wandering predicted by Euler and is now called the Chandler wobble after its discoverer. The exact point of intersection of the Earth's axis and the Earth's surface, at any given moment, is called the "instantaneous pole", but because of the "wobble" this cannot be used as a definition of a fixed North Pole (or South Pole) when metre-scale precision is required.
It is desirable to tie the system of Earth coordinates (latitude, longitude, and elevations or orography) to fixed landforms. However, given plate tectonics and isostasy, there is no system in which all geographic features are fixed. Yet the International Earth Rotation and Reference Systems Service and the International Astronomical Union have defined a framework called the International Terrestrial Reference System.
As early as the 16th century, many prominent people correctly believed that the North Pole was in a sea, which in the 19th century was called the Polynya or Open Polar Sea.[7] It was therefore hoped that passage could be found through ice floes at favorable times of the year. Several expeditions set out to find the way, generally with whaling ships, already commonly used in the cold northern latitudes.
One of the earliest expeditions to set out with the explicit intention of reaching the North Pole was that of British naval officer William Edward Parry, who in 1827 reached latitude 82°45′ North. In 1871, the Polaris expedition, a US attempt on the Pole led by Charles Francis Hall, ended in disaster. Another British Royal Navy attempt to get to the pole, part of the British Arctic Expedition, by Commander Albert H. Markham reached a then-record 83°20'26" North in May 1876 before turning back. An 1879–1881 expedition commanded by US naval officer George W. De Long ended tragically when their ship, the USS Jeannette, was crushed by ice. Over half the crew, including De Long, were lost.
In April 1895, the Norwegian explorers Fridtjof Nansen and Hjalmar Johansen struck out for the Pole on skis after leaving Nansen's icebound ship Fram. The pair reached latitude 86°14′ North before they abandoned the attempt and turned southwards, eventually reaching Franz Josef Land.
In 1897, Swedish engineer Salomon August Andrée and two companions tried to reach the North Pole in the hydrogen balloon Örnen ("Eagle"), but came down 300 km (190 mi) north of Kvitøya, the northeasternmost part of the Svalbard archipelago. They trekked to Kvitøya but died there three months after their crash.  In 1930 the remains of this expedition were found by the Norwegian Bratvaag Expedition.
The Italian explorer Luigi Amedeo, Duke of the Abruzzi and Captain Umberto Cagni of the Italian Royal Navy (Regia Marina) sailed the converted whaler Stella Polare ("Pole Star") from Norway in 1899. On 11 March 1900, Cagni led a party over the ice and reached latitude 86° 34’ on 25 April, setting a new record by beating Nansen's result of 1895 by 35 to 40 km (22 to 25 mi). Cagni barely managed to return to the camp, remaining there until 23 June. On 16 August, the Stella Polare left Rudolf Island heading south and the expedition returned to Norway.
The US explorer Frederick Cook claimed to have reached the North Pole on 21 April 1908 with two Inuit men, Ahwelah and Etukishook, but he was unable to produce convincing proof and his claim is not widely accepted.[9][10]
The conquest of the North Pole was for many years credited to US Navy engineer Robert Peary, who claimed to have reached the Pole on 6 April 1909, accompanied by Matthew Henson and four Inuit men, Ootah, Seeglo, Egingwah, and Ooqueah. However, Peary's claim remains highly disputed and controversial. Those who accompanied Peary on the final stage of the journey were not trained in [Western] navigation, and thus could not independently confirm his navigational work, which some claim to have been particularly sloppy as he approached the Pole.
The distances and speeds that Peary claimed to have achieved once the last support party turned back seem incredible to many people, almost three times that which he had accomplished up to that point. Peary's account of a journey to the Pole and back while traveling along the direct line – the only strategy that is consistent with the time constraints that he was facing – is contradicted by Henson's account of tortuous detours to avoid pressure ridges and open leads.
The  British explorer Wally Herbert, initially a supporter of Peary, researched Peary's records in 1989 and found that there were significant discrepancies in the explorer's navigational records. He concluded that Peary had not reached the Pole.[11] Support for Peary came again in 2005, however, when British explorer Tom Avery and four companions recreated the outward portion of Peary's journey with replica wooden sleds and Canadian Eskimo Dog teams, reaching the North Pole in 36 days, 22 hours – nearly five hours faster than Peary. However, Avery's fastest 5-day march was 90 nautical miles (170 km), significantly short of the 135 nautical miles (250 km) claimed by Peary. Avery writes on his web site that "The admiration and respect which I hold for Robert Peary, Matthew Henson and the four Inuit men who ventured North in 1909, has grown enormously since we set out from Cape Columbia. Having now seen for myself how he travelled across the pack ice, I am more convinced than ever that Peary did indeed discover the North Pole."[12]
The first claimed flight over the Pole was made on 9 May 1926 by US naval officer Richard E. Byrd and pilot Floyd Bennett in a Fokker tri-motor aircraft. Although verified at the time by a committee of the National Geographic Society, this claim has since been undermined[13] by the 1996 revelation that Byrd's long-hidden diary's solar sextant data (which the NGS never checked) consistently contradict his June 1926 report's parallel data by over 100 mi (160 km).[14] The secret report's alleged en-route solar sextant data were inadvertently so impossibly overprecise that he excised all these alleged raw solar observations out of the version of the report finally sent to geographical societies five months later (while the original version was hidden for 70 years), a realization first published in 2000 by the University of Cambridge after scrupulous refereeing.[15]
The first consistent, verified, and scientifically convincing attainment of the Pole was on 12 May 1926, by Norwegian explorer Roald Amundsen and his US sponsor Lincoln Ellsworth from the airship Norge.[16] Norge, though Norwegian-owned, was designed and piloted by the Italian Umberto Nobile. The flight started from Svalbard in Norway, and crossed the Arctic Ocean to Alaska. Nobile, with several scientists and crew from the Norge, overflew the Pole a second time on 24 May 1928, in the airship Italia. The Italia crashed on its return from the Pole, with the loss of half the crew.
Another transpolar flight [ru] was accomplished in a Tupolev ANT-25 airplane with a crew of Valery Chkalov, Georgy Baydukov and Alexander Belyakov, who flew over the North Pole on 19 June 1937, during their direct flight from the Soviet Union to the USA without any stopover.
In May 1937 the world's first North Pole ice station, North Pole-1, was established by Soviet scientists 20 kilometres (13 mi) from the North Pole after the ever first landing of four heavy and one light aircraft onto the ice at the North Pole. The expedition members — oceanographer Pyotr Shirshov, meteorologist Yevgeny Fyodorov, radio operator Ernst Krenkel, and the leader Ivan Papanin[17] — conducted scientific research at the station for the next nine months. By 19 February 1938, when the group was picked up by the ice breakers Taimyr and Murman, their station had drifted 2850 km to the eastern coast of Greenland.[18][19]
In May 1945 an RAF Lancaster of the Aries expedition became the first Commonwealth aircraft to overfly the North Geographic and North Magnetic Poles. The plane was piloted by David Cecil McKinley of the Royal Air Force. It carried an 11-man crew, with Kenneth C. Maclure of the Royal Canadian Air Force in charge of all scientific observations. In 2006, Maclure was honoured with a spot in Canada's Aviation Hall of Fame.[20]
Discounting Peary's disputed claim, the first men to set foot at the North Pole were a Soviet party[21] including geophysicists Mikhail Ostrekin and Pavel Senko, oceanographers Mikhail Somov and Pavel Gordienko,[22] and other scientists and flight crew (24 people in total)[23] of Aleksandr Kuznetsov's Sever-2 expedition (March–May 1948).[24] It was organized by the Chief Directorate of the Northern Sea Route.[25] The party flew on three planes (pilots Ivan Cherevichnyy, Vitaly Maslennikov and Ilya Kotov) from Kotelny Island to the North Pole and landed there at 4:44pm (Moscow Time, UTC+04:00) on 23 April 1948.[26] They established a temporary camp and for the next two days conducted scientific observations. On 26 April the expedition flew back to the continent.
Next year, on 9 May 1949[27] two other Soviet scientists (Vitali Volovich and Andrei Medvedev)[28] became the first people to parachute onto the North Pole.[29] They jumped from a Douglas C-47 Skytrain, registered CCCP H-369.[30]
On 3 May 1952, U.S. Air Force Lieutenant Colonel Joseph O. Fletcher and Lieutenant William Pershing Benedict, along with scientist Albert P. Crary, landed a modified Douglas C-47 Skytrain at the North Pole. Some Western sources considered this to be the first landing at the Pole[31] until the Soviet landings became widely known.
The United States Navy submarine USS Nautilus (SSN-571) crossed the North Pole on 3 August 1958. On 17 March 1959 USS Skate (SSN-578) surfaced at the Pole, breaking through the ice above it, becoming the first naval vessel to do so.[32]
The first confirmed surface conquest of the North Pole was accomplished by Ralph Plaisted, Walt Pederson, Gerry Pitzl and Jean Luc Bombardier, who traveled over the ice by snowmobile and arrived on 19 April 1968. The United States Air Force independently confirmed their position.
On 6 April 1969 Wally Herbert and companions Allan Gill, Roy Koerner and Kenneth Hedges of the British Trans-Arctic Expedition became the first men to reach the North Pole on foot (albeit with the aid of dog teams and airdrops). They continued on to complete the first surface crossing of the Arctic Ocean – and by its longest axis, Barrow, Alaska, to Svalbard – a feat that has never been repeated.[33][34] Because of suggestions (later proven false) of Plaisted's use of air transport, some sources classify Herbert's expedition as the first confirmed to reach the North Pole over the ice surface by any means.[34][35] In the 1980s Plaisted's pilots Weldy Phipps and Ken Lee signed affidavits asserting that no such airlift was provided.[36] It is also said that Herbert was the first person to reach the pole of inaccessibility.[37]
On 17 August 1977 the Soviet nuclear-powered icebreaker Arktika completed the first surface vessel journey to the North Pole.
In 1982 Ranulph Fiennes and Charles R. Burton became the first people to cross the Arctic Ocean in a single season. They departed from Cape Crozier, Ellesmere Island, on 17 February 1982 and arrived at the geographic North Pole on 10 April 1982. They travelled on foot and snowmobile. From the Pole, they travelled towards Svalbard but, due to the unstable nature of the ice, ended their crossing at the ice edge after drifting south on an ice floe for 99 days. They were eventually able to walk to their expedition ship MV Benjamin Bowring and boarded it on 4 August 1982 at position 80:31N 00:59W. As a result of this journey, which formed a section of the three-year Transglobe Expedition 1979–1982, Fiennes and Burton became the first people to complete a circumnavigation of the world via both North and South Poles, by surface travel alone.[38] This achievement remains unchallenged to this day.  The expedition crew included a Jack Russell Terrier named Bothie who became the first dog to visit both poles.[39]
In 1985 Sir Edmund Hillary (the first man to stand on the summit of Mount Everest) and Neil Armstrong (the first man to stand on the moon) landed at the North Pole in a small twin-engined ski plane.[40] Hillary thus became the first man to stand at both poles and on the summit of Everest.
In 1986 Will Steger, with seven teammates, became the first to be confirmed as reaching the Pole by dogsled and without resupply.
USS Gurnard (SSN-662) operated in the Arctic Ocean under the polar ice cap from September to November 1984 in company with one of her sister ships, the attack submarine USS Pintado (SSN-672). On 12 November 1984 Gurnard and Pintado became the third pair of submarines to surface together at the North Pole. In March 1990, Gurnard deployed to the Arctic region during exercise Ice Ex '90 and completed only the fourth winter submerged transit of the Bering and Seas. Gurnard surfaced at the North Pole on 18 April, in the company of the USS Seahorse (SSN-669).[citation needed]
On 6 May 1986 USS Archerfish (SSN 678), USS Ray (SSN 653) and USS Hawkbill (SSN-666) surfaced at the North Pole, the first tri-submarine surfacing at the North Pole.
On 21 April 1987 Shinji Kazama of Japan became the first person to reach the North Pole on a motorcycle.[41][42]
On 18 May 1987 USS Billfish (SSN 676), USS Sea Devil (SSN 664) and HMS Superb (S 109) surfaced at the North Pole, the first international surfacing at the North Pole.
In 1988 a team of 13 (9 Soviets, 4 Canadians) skied across the arctic from Siberia to northern Canada. One of the Canadians, Richard Weber, became the first person to reach the Pole from both sides of the Arctic Ocean.
On April 16, 1990, a German-Swiss expedition led by a team of the University of Giessen reached the Geographic North Pole for studies on pollution of pack ice, snow and air. Samples taken were analyzed in cooperation with the Geological Survey of Canada and the Alfred Wegener Institute for Polar and Marine Research. Further stops for sample collections were on multi-year sea ice at 86°N, at Cape Columbia and Ward Hunt Island.[43]
On 4 May 1990 Børge Ousland and Erling Kagge became the first explorers ever to reach the North Pole unsupported, after a 58-day ski trek from Ellesmere Island in Canada, a distance of 800 km.[44]
On 7 September 1991 the German research vessel Polarstern and the Swedish icebreaker Oden reached the North Pole as the first conventional powered vessels.[45] Both scientific parties and crew took oceanographic and geological samples and had a common tug of war and a football game on an ice floe. Polarstern again reached the pole exactly 10 years later,[46] with the Healy.
In 1998, 1999, and 2000, Lada Niva Marshs (special very large wheeled versions made by BRONTO, Lada/Vaz's experimental product division) were driven to the North Pole.[47][48] The 1998 expedition was dropped by parachute and completed the track to the North Pole. The 2000 expedition departed from a Russian research base around 114 km from the Pole and claimed an average speed of 20–15 km/h in an average temperature of −30 °C.
Commercial airliner flights on the Polar routes may pass within viewing distance of the North Pole.  For example, a flight from Chicago to Beijing may come close as latitude 89° N, though because of prevailing winds return journeys go over the Bering Strait.  In recent years journeys to the North Pole by air (landing by helicopter or on a runway prepared on the ice) or by icebreaker have become relatively routine, and are even available to small groups of tourists through adventure holiday companies. Parachute jumps have frequently been made onto the North Pole in recent years. The temporary seasonal Russian camp of Barneo has been established by air a short distance from the Pole annually since 2002, and caters for scientific researchers as well as tourist parties. Trips from the camp to the Pole itself may be arranged overland or by helicopter.
The first attempt at underwater exploration of the North Pole was made on 22 April 1998 by Russian firefighter and diver Andrei Rozhkov with the support of the Diving Club of Moscow State University, but ended in fatality. The next attempted dive at the North Pole was organized the next year by the same diving club, and ended in success on 24 April 1999. The divers were Michael Wolff (Austria), Brett Cormick (UK), and Bob Wass (USA).[49]
In 2005 the United States Navy submarine USS Charlotte (SSN-766) surfaced through 155 cm (61 in) of ice at the North Pole and spent 18 hours there.[50]
In July 2007 British endurance swimmer Lewis Gordon Pugh completed a 1 km (0.62 mi) swim at the North Pole. His feat, undertaken to highlight the effects of global warming, took place in clear water that had opened up between the ice floes.[51] His later attempt to paddle a kayak to the North Pole in late 2008, following the erroneous prediction of clear water to the Pole, was stymied when his expedition found itself stuck in thick ice after only three days. The expedition was then abandoned.
By September 2007 the North Pole had been visited 66 times by different surface ships: 54 times by Soviet and Russian icebreakers, 4 times by Swedish Oden, 3 times by German Polarstern, 3 times by USCGC Healy and USCGC Polar Sea, and once by CCGS Louis S. St-Laurent and by Swedish Vidar Viking.[52]
On 2 August 2007 a Russian scientific expedition Arktika 2007 made the first ever manned descent to the ocean floor at the North Pole, to a depth of 4.3 km (2.7 mi), as part of the research programme in support of Russia's 2001 extended continental shelf claim to a large swathe of the Arctic Ocean floor. The descent took place in two MIR submersibles and was led by Soviet and Russian polar explorer Artur Chilingarov. In a symbolic act of visitation, the Russian flag was placed on the ocean floor exactly at the Pole.[53][54][55]
The expedition was the latest in a series of efforts intended to give Russia a dominant influence in the Arctic according to The New York Times.[56] The warming Arctic climate and summer shrinkage of the iced area has attracted the attention of many countries, such as China and the United States, toward the top of the world, where resources and shipping routes may soon be exploitable.[57]
In 2009 the Russian Marine Live-Ice Automobile Expedition (MLAE-2009) with Vasily Elagin as a leader and a team of Afanasy Makovnev, Vladimir Obikhod, Alexey Shkrabkin, Sergey Larin, Alexey Ushakov and Nikolay Nikulshin reached the North Pole on two custom-built 6 x 6 low-pressure-tire ATVs. The vehicles, Yemelya-1 and Yemelya-2, were designed by Vasily Elagin, a Russian mountain climber, explorer and engineer. They reached the North Pole on 26 April 2009, 17:30 (Moscow time). The expedition was partly supported by Russian State Aviation. The Russian Book of Records recognized it as the first successful vehicle trip from land to the Geographical North Pole.
On 1 March 2013 the Russian Marine Live-Ice Automobile Expedition (MLAE 2013) with Vasily Elagin as a leader, and a team of Afanasy Makovnev, Vladimir Obikhod, Alexey Shkrabkin, Andrey Vankov, Sergey Isayev and Nikolay Kozlov on two custom-built 6 x 6 low-pressure-tire ATVs—Yemelya-3 and Yemelya-4—started from Golomyanny Island (the Severnaya Zemlya Archipelago) to the North Pole across drifting ice of the Arctic Ocean. The vehicles reached the Pole on 6 April and then continued to the Canadian coast. The coast was reached on 30 April 2013 (83°08N, 075°59W Ward Hunt Island), and on 5 May 2013 the expedition finished in Resolute Bay, NU. The way between the Russian borderland (Machtovyi Island of the Severnaya Zemlya Archipelago, 80°15N, 097°27E) and the Canadian coast (Ward Hunt Island, 83°08N, 075°59W) took 55 days; it was ~2300 km across drifting ice and about 4000 km in total. The expedition was totally self-dependent and used no external supplies. The expedition was supported by the Russian Geographical Society.[58]
The sun at the North Pole is continuously above the horizon during the summer and continuously below the horizon during the winter. Sunrise is just before the March equinox (around 20 March); the sun then takes three months to reach its highest point of near 23½° elevation at the summer solstice (around 21 June), after which time it begins to sink, reaching sunset just after the September equinox (around 23 September). When the sun is visible in the polar sky, it appears to move in a horizontal circle above the horizon. This circle gradually rises from near the horizon just after the vernal equinox to its maximum elevation (in degrees) above the horizon at summer solstice and then sinks back toward the horizon before sinking below it at the autumnal equinox. Hence the North and South Poles experience the slowest rates of sunrise and sunset on Earth.
The twilight period that occurs before sunrise and after sunset has three different definitions:
These effects are caused by a combination of the Earth's axial tilt and its revolution around the sun. The direction of the Earth's axial tilt, as well as its angle relative to the plane of the Earth's orbit around the sun, remains very nearly constant over the course of a year (both change very slowly over long time periods). At northern midsummer the North Pole is facing towards the sun to its maximum extent. As the year progresses and the Earth moves around the sun, the North Pole gradually turns away from the sun until at midwinter it is facing away from the Sun to its maximum extent. A similar sequence is observed at the South Pole, with a six-month time difference.
In most places on Earth, local time is determined by longitude, such that the time of day is more or less synchronised to the position of the sun in the sky (for example, at midday, the sun is roughly at its highest). This line of reasoning fails at the North Pole, where the sun rises and sets only once per year, and all lines of longitude, and hence all time zones, converge. There is no permanent human presence at the North Pole and no particular time zone has been assigned. Polar expeditions may use any time zone that is convenient, such as Greenwich Mean Time, or the time zone of the country from which they departed.[citation needed]
The North Pole is substantially warmer than the South Pole because it lies at sea level in the middle of an ocean (which acts as a reservoir of heat), rather than at altitude on a continental land mass. Despite being an ice cap, the northernmost weather station in Greenland has a tundra climate (Köppen ET) due to the July and August mean temperatures peaking just above freezing.[a]
Winter temperatures at the northernmost weather station in Greenland can range from about −50 to −13 °C (−58 to 9 °F), averaging around −31 °C (−24 °F), with the North Pole being slightly colder. However, a freak storm caused the temperature to reach 0.7 °C (33.3 °F) for a time at a World Meteorological Organization buoy, located at 87.45°N, on 30 December 2015. It was estimated that the temperature at the North Pole was between −1 and 2 °C (30 and 35 °F) during the storm.[59] Summer temperatures (June, July, and August) average around the freezing point (0 °C (32 °F)). The highest temperature yet recorded is 13 °C (55 °F),[60] much warmer than the South Pole's record high of only −12.3 °C (9.9 °F).[61] A similar[clarification needed] spike in temperatures occurred on 15 November 2016 when temperatures hit freezing.[62] Yet again, February 2018 featured a storm so powerful that temperatures at Cape Morris Jesup, the world's northernmost weather station in Greenland, reached 6.1 °C (43.0 °F) and spent 24 straight hours above freezing.[63] Meanwhile, the pole itself was estimated to reach a high temperature of 1.6 °C (34.9 °F)[clarification needed]. This same temperature of 1.6 °C (34.9 °F) was also recorded at the Hollywood Burbank Airport in Los Angeles at the very same time.[64]
The sea ice at the North Pole is typically around 2 to 3 m (6 ft 7 in to 9 ft 10 in) thick,[65] although ice thickness, its spatial extent, and the fraction of open water within the ice pack can vary rapidly and profoundly in response to weather and climate.[66]  Studies have shown that the average ice thickness has decreased in recent years.[67] It is likely that global warming has contributed to this, but it is not possible to attribute the recent abrupt decrease in thickness entirely to the observed warming in the Arctic.[68] Reports have also predicted that within a few decades the Arctic Ocean will be entirely free of ice in the summer.[69] This may have significant commercial implications; see "Territorial claims", below.
The retreat of the Arctic sea ice will accelerate global warming, as less ice cover reflects less solar radiation, and may have serious climate implications by contributing to Arctic cyclone generation.[70]
Polar bears are believed to travel rarely beyond about 82° North, owing to the scarcity of food, though tracks have been seen in the vicinity of the North Pole, and a 2006 expedition reported sighting a polar bear just 1 mi (1.6 km) from the Pole.[71][72] The ringed seal has also been seen at the Pole, and Arctic foxes have been observed less than 60 km (37 mi) away at 89°40′ N.[73][74]
Birds seen at or very near the Pole include the snow bunting, northern fulmar and black-legged kittiwake, though some bird sightings may be distorted by the tendency of birds to follow ships and expeditions.[75]
Fish have been seen in the waters at the North Pole, but these are probably few in number.[75] A member of the Russian team that descended to the North Pole seabed in August 2007 reported seeing no sea creatures living there.[54] However, it was later reported that a sea anemone had been scooped up from the seabed mud by the Russian team and that video footage from the dive showed unidentified shrimps and amphipods.[76]
Currently, under international law, no country owns the North Pole or the region of the Arctic Ocean surrounding it. The five surrounding Arctic countries, Russia, Canada, Norway, Denmark (via Greenland), and the United States, are limited to a 200-nautical-mile (370 km; 230 mi) exclusive economic zone off their coasts, and the area beyond that is administered by the International Seabed Authority.
Upon ratification of the United Nations Convention on the Law of the Sea, a country has 10 years to make claims to an extended continental shelf beyond its 200-mile exclusive economic zone.  If validated, such a claim gives the claimant state rights to what may be on or beneath the sea bottom within the claimed zone.[77] Norway (ratified the convention in 1996[78]), Russia (ratified in 1997[78]), Canada (ratified in 2003[78]) and Denmark (ratified in 2004[78]) have all launched projects to base claims that certain areas of Arctic continental shelves should be subject to their sole sovereign exploitation.[79][80]
In 1907 Canada invoked a "sector principle" to claim sovereignty over a sector stretching from its coasts to the North Pole. This claim has not been relinquished, but was not consistently pressed until 2013.[81][82]
In some children's Christmas legends and Western folklore, the geographic North Pole is described as the location of Santa Claus' legendary workshop and residence,[83][84] although the depictions have been inconsistent between the geographic and magnetic North Pole.[citation needed] Canada Post has assigned postal code H0H 0H0 to the North Pole (referring to Santa's traditional exclamation of "Ho ho ho!").[85]
This association reflects an age-old esoteric mythology of Hyperborea that posits the North Pole, the otherworldly world-axis, as the abode of God and superhuman beings.[86]
As Henry Corbin has documented, the North Pole plays a key part in the cultural worldview of Sufism and Iranian mysticism. "The Orient sought by the mystic, the Orient that cannot be located on our maps, is in the direction of the north, beyond the north.".[87]
In Mandaean cosmology, the North Pole and Polaris are considered to be auspicious, since they are associated with the World of Light. Mandaeans face north when praying, and temples are also oriented towards the north. On the contrary, South is associated with the World of Darkness.[88]
Owing to its remoteness, the Pole is sometimes identified with a mysterious mountain of ancient Iranian tradition called Mount Qaf (Jabal Qaf), the "farthest point of the earth".[89][90] According to certain authors, the Jabal Qaf of Muslim cosmology is a version of Rupes Nigra, a mountain whose ascent, like Dante's climbing of the Mountain of Purgatory, represents the pilgrim's progress through spiritual states.[91] In Iranian theosophy, the heavenly Pole, the focal point of the spiritual ascent, acts as a magnet to draw beings to its "palaces ablaze with immaterial matter."[92]





Canada is a country in North America. Its ten provinces and three territories extend from the Atlantic Ocean to the Pacific Ocean and northward into the Arctic Ocean, making it the world's second-largest country by total area, with the world's longest coastline. Its border with the United States is the world's longest international land border. The country is characterized by a wide range of both meteorologic and geological regions. It is sparsely inhabited, with the vast majority residing south of the 55th parallel in urban areas. Canada's capital is Ottawa and its three largest metropolitan areas are Toronto, Montreal, and Vancouver.
Indigenous peoples have continuously inhabited what is now Canada for thousands of years. Beginning in the 16th century, British and French expeditions explored and later settled along the Atlantic coast. As a consequence of various armed conflicts, France ceded nearly all of its colonies in North America in 1763. In 1867, with the union of three British North American colonies through Confederation, Canada was formed as a federal dominion of four provinces. This began an accretion of provinces and territories and a process of increasing autonomy from the United Kingdom, highlighted by the Statute of Westminster, 1931, and culminating in the Canada Act 1982, which severed the vestiges of legal dependence on the Parliament of the United Kingdom.
Canada is a parliamentary liberal democracy and a constitutional monarchy in the Westminster tradition. The country's head of government is the prime minister, who holds office by virtue of their ability to command the confidence of the elected House of Commons and is "called upon" by the governor general, representing the monarch of Canada, the ceremonial head of state. The country is a Commonwealth realm and is officially bilingual (English and French) in the federal jurisdiction. It is very highly ranked in international measurements of government transparency, quality of life, economic competitiveness, innovation and education. It is one of the world's most ethnically diverse and multicultural nations, the product of large-scale immigration. Canada's long and complex relationship with the United States has had a significant impact on its history, economy, and culture.
A developed country, Canada has one of the highest nominal per capita income globally and its advanced economy ranks among the largest in the world, relying chiefly upon its abundant natural resources and well-developed international trade networks. Canada is recognized as a middle power for its role in international affairs, with a tendency to pursue multilateral solutions. Canada's peacekeeping role during the 20th century has had a significant influence on its global image. Canada is part of multiple major international and intergovernmental institutions.
While a variety of theories have been postulated for the etymological origins of Canada, the name is now accepted as coming from the St. Lawrence Iroquoian word kanata, meaning "village" or "settlement".[9] In 1535, Indigenous inhabitants of the present-day Quebec City region used the word to direct French explorer Jacques Cartier to the village of Stadacona.[10] Cartier later used the word Canada to refer not only to that particular village but to the entire area subject to Donnacona (the chief at Stadacona);[10] by 1545, European books and maps had begun referring to this small region along the Saint Lawrence River as Canada.[10]
From the 16th to the early 18th century, "Canada" referred to the part of New France that lay along the Saint Lawrence River.[11] In 1791, the area became two British colonies called Upper Canada and Lower Canada. These two colonies were collectively named the Canadas until their union as the British Province of Canada in 1841.[12]
Upon Confederation in 1867, Canada was adopted as the legal name for the new country at the London Conference and the word Dominion was conferred as the country's title.[13] By the 1950s, the term Dominion of Canada was no longer used by the United Kingdom, which considered Canada a "Realm of the Commonwealth".[14] 
The Canada Act 1982, which brought the constitution of Canada fully under Canadian control, referred only to Canada. Later that year, the name of the national holiday was changed from Dominion Day to Canada Day.[15] The term Dominion was used to distinguish the federal government from the provinces, though after the Second World War the term federal had replaced dominion.[16]
Indigenous peoples in present-day Canada include the First Nations, Inuit, and Métis,[17] the last being of mixed descent who originated in the mid-17th century when First Nations people married European settlers and subsequently developed their own identity.[17]
The first inhabitants of North America are generally hypothesized to have migrated from Siberia by way of the Bering land bridge and arrived at least 14,000 years ago.[18][19] The Paleo-Indian archeological sites at Old Crow Flats and Bluefish Caves are two of the oldest sites of human habitation in Canada.[20] The characteristics of Indigenous societies included permanent settlements, agriculture, complex societal hierarchies, and trading networks.[21][22] Some of these cultures had collapsed by the time European explorers arrived in the late 15th and early 16th centuries and have only been discovered through archeological investigations.[23]
The Indigenous population at the time of the first European settlements is estimated to have been between 200,000[24] and two million,[25] with a figure of 500,000 accepted by Canada's Royal Commission on Aboriginal Peoples.[26] As a consequence of European colonization, the Indigenous population declined by forty to eighty percent and several First Nations, such as the Beothuk, disappeared.[27] The decline is attributed to several causes, including the transfer of European diseases, such as influenza, measles, and smallpox, to which they had no natural immunity,[24][28] conflicts over the fur trade, conflicts with the colonial authorities and settlers, and the loss of Indigenous lands to settlers and the subsequent collapse of several nations' self-sufficiency.[29][30]

Although not without conflict, European Canadians' early interactions with First Nations and Inuit populations were relatively peaceful.[31] First Nations and Métis peoples played a critical part in the development of European colonies in Canada, particularly for their role in assisting European coureur des bois and voyageurs in their explorations of the continent during the North American fur trade.[32] These early European interactions with First Nations would change from friendship and peace treaties to the dispossession of Indigenous lands through treaties.[33][34] From the late 18th century, European Canadians forced Indigenous peoples to assimilate into a western Canadian society.[35] These attempts reached a climax in the late 19th and early 20th centuries with forced integration,[36] health-care segregation,[37] and displacement.[38] A period of redress is underway, which started with the formation of the Truth and Reconciliation Commission of Canada by the Government of Canada in 2008.[39] This includes recognition of past colonial injustices and settlement agreements and betterment of racial discrimination issues, such as addressing the plight of missing and murdered Indigenous women.[39][40]It is believed that the first European to explore the east coast of Canada was Norse explorer Leif Erikson.[41][42] In approximately 1000 AD, the Norse built a small short-lived encampment that was occupied sporadically for perhaps 20 years at L'Anse aux Meadows on the northern tip of Newfoundland.[43] No further European exploration occurred until 1497, when Italian seafarer John Cabot explored and claimed Canada's Atlantic coast in the name of King Henry VII of England.[44] In 1534, French explorer Jacques Cartier explored the Gulf of Saint Lawrence where, on July 24, he planted a 10-metre (33 ft) cross bearing the words, "long live the King of France", and took possession of the territory New France in the name of King Francis I.[45] The early 16th century saw European mariners with navigational techniques pioneered by the Basque and Portuguese establish seasonal whaling and fishing outposts along the Atlantic coast.[46] In general, early settlements during the Age of Discovery appear to have been short-lived due to a combination of the harsh climate, problems with navigating trade routes and competing outputs in Scandinavia.[47][48]
In 1583, Sir Humphrey Gilbert, by the royal prerogative of Queen Elizabeth I, founded St John's, Newfoundland, as the first North American English seasonal camp.[49] In 1600, the French established their first seasonal trading post at Tadoussac along the Saint Lawrence.[43] French explorer Samuel de Champlain arrived in 1603 and established the first permanent year-round European settlements at Port Royal (in 1605) and Quebec City (in 1608).[50] Among the colonists of New France, Canadiens extensively settled the Saint Lawrence River valley and Acadians settled the present-day Maritimes, while fur traders and Catholic missionaries explored the Great Lakes, Hudson Bay, and the Mississippi watershed to Louisiana.[51] The Beaver Wars broke out in the mid-17th century over control of the North American fur trade.[52]
The English established additional settlements in Newfoundland in 1610 along with settlements in the Thirteen Colonies to the south.[53][54] A series of four wars erupted in colonial North America between 1689 and 1763; the later wars of the period constituted the North American theatre of the Seven Years' War.[55] Mainland Nova Scotia came under British rule with the 1713 Treaty of Utrecht and Canada and most of New France came under British rule in 1763 after the Seven Years' War.[56]
The Royal Proclamation of 1763 established First Nation treaty rights, created the Province of Quebec out of New France, and annexed Cape Breton Island to Nova Scotia.[15] St John's Island (now Prince Edward Island) became a separate colony in 1769.[57] To avert conflict in Quebec, the British Parliament passed the Quebec Act 1774, expanding Quebec's territory to the Great Lakes and Ohio Valley.[58] More importantly, the Quebec Act afforded Quebec special autonomy and rights of self-administration at a time when the Thirteen Colonies were increasingly agitating against British rule.[59] It re-established the French language, Catholic faith, and French civil law there, staving off the growth of an independence movement in contrast to the Thirteen Colonies.[60] The Proclamation and the Quebec Act in turn angered many residents of the Thirteen Colonies, further fuelling anti-British sentiment in the years prior to the American Revolution.[15]
After the successful American War of Independence, the 1783 Treaty of Paris recognized the independence of the newly formed United States and set the terms of peace, ceding British North American territories south of the Great Lakes and east of the Mississippi River to the new country.[61] The American war of independence also caused a large out-migration of Loyalists, the settlers who had fought against American independence. Many moved to Canada, particularly Atlantic Canada, where their arrival changed the demographic distribution of the existing territories. New Brunswick was in turn split from Nova Scotia as part of a reorganization of Loyalist settlements in the Maritimes, which led to the incorporation of Saint John, New Brunswick, as Canada's first city.[62] To accommodate the influx of English-speaking Loyalists in Central Canada, the Constitutional Act of 1791 divided the province of Canada into French-speaking Lower Canada (later Quebec) and English-speaking Upper Canada (later Ontario), granting each its own elected legislative assembly.[63]
The Canadas were the main front in the War of 1812 between the United States and the United Kingdom. Peace came in 1815; no boundaries were changed.[64] Immigration resumed at a higher level, with over 960,000 arrivals from Britain between 1815 and 1850.[65] New arrivals included refugees escaping the Great Irish Famine as well as Gaelic-speaking Scots displaced by the Highland Clearances.[66] Infectious diseases killed between 25 and 33 percent of Europeans who immigrated to Canada before 1891.[24]
The desire for responsible government resulted in the abortive Rebellions of 1837.[67] The Durham Report subsequently recommended responsible government and the assimilation of French Canadians into English culture.[15] The Act of Union 1840 merged the Canadas into a united Province of Canada and responsible government was established for all provinces of British North America east of Lake Superior by 1855.[68] The signing of the Oregon Treaty by Britain and the United States in 1846 ended the Oregon boundary dispute, extending the border westward along the 49th parallel. This paved the way for British colonies on Vancouver Island (1849) and in British Columbia (1858).[69] The Anglo-Russian Treaty of Saint Petersburg (1825) established the border along the Pacific coast, but, even after the US Alaska Purchase of 1867, disputes continued about the exact demarcation of the Alaska–Yukon and Alaska–BC border.[70]
Following three constitutional conferences, the British North America Act, 1867 officially proclaimed Canadian Confederation on July 1, 1867, initially with four provinces: Ontario, Quebec, Nova Scotia, and New Brunswick.[71][72] Canada assumed control of Rupert's Land and the North-Western Territory to form the Northwest Territories, where the Métis' grievances ignited the Red River Rebellion and the creation of the province of Manitoba in July 1870.[73] British Columbia and Vancouver Island (which had been united in 1866) joined the confederation in 1871 on the promise of a transcontinental railway extending to Victoria in the province within 10 years,[74] while Prince Edward Island joined in 1873.[75] In 1898, during the Klondike Gold Rush in the Northwest Territories, Parliament created the Yukon Territory. Alberta and Saskatchewan became provinces in 1905.[75] Between 1871 and 1896, almost one quarter of the Canadian population emigrated south to the US.[76]
To open the West and encourage European immigration, the Government of Canada sponsored the construction of three transcontinental railways (including the Canadian Pacific Railway), passed the Dominion Lands Act to regulate settlement and established the North-West Mounted Police to assert authority over the territory.[77][78] This period of westward expansion and nation building resulted in the displacement of many Indigenous peoples of the Canadian Prairies to "Indian reserves",[79] clearing the way for ethnic European block settlements.[80] This caused the collapse of the Plains Bison in western Canada and the introduction of European cattle farms and wheat fields dominating the land.[81] The Indigenous peoples saw widespread famine and disease due to the loss of the bison and their traditional hunting lands.[82] The federal government did provide emergency relief, on condition of the Indigenous peoples moving to the reserves.[83] During this time, Canada introduced the Indian Act extending its control over the First Nations to education, government and legal rights.[84]
Because Britain still maintained control of Canada's foreign affairs under the British North America Act, 1867, its declaration of war in 1914 automatically brought Canada into the First World War.[85] Volunteers sent to the Western Front later became part of the Canadian Corps, which played a substantial role in the Battle of Vimy Ridge and other major engagements of the war.[86] Out of approximately 625,000 Canadians who served in World War I, some 60,000 were killed and another 172,000 were wounded.[87] The Conscription Crisis of 1917 erupted when the Unionist Cabinet's proposal to augment the military's dwindling number of active members with conscription was met with vehement objections from French-speaking Quebecers.[88] The Military Service Act brought in compulsory military service, though it, coupled with disputes over French language schools outside Quebec, deeply alienated Francophone Canadians and temporarily split the Liberal Party.[88] In 1919, Canada joined the League of Nations independently of Britain,[86] and the Statute of Westminster, 1931, affirmed Canada's independence.[89]
The Great Depression in Canada during the early 1930s saw an economic downturn, leading to hardship across the country.[90] In response to the downturn, the Co-operative Commonwealth Federation (CCF) in Saskatchewan introduced many elements of a welfare state (as pioneered by Tommy Douglas) in the 1940s and 1950s.[91] On the advice of Prime Minister William Lyon Mackenzie King, war with Germany was declared effective September 10, 1939, by King George VI, seven days after the United Kingdom. The delay underscored Canada's independence.[86]
The first Canadian Army units arrived in Britain in December 1939. In all, over a million Canadians served in the armed forces during the Second World War and approximately 42,000 were killed and another 55,000 were wounded.[92] Canadian troops played important roles in many key battles of the war, including the failed 1942 Dieppe Raid, the Allied invasion of Italy, the Normandy landings, the Battle of Normandy, and the Battle of the Scheldt in 1944.[86] Canada provided asylum for the Dutch monarchy while that country was occupied and is credited by the Netherlands for major contributions to its liberation from Nazi Germany.[93]
The Canadian economy boomed during the war as its industries manufactured military materiel for Canada, Britain, China, and the Soviet Union.[86] Despite another Conscription Crisis in Quebec in 1944, Canada finished the war with a large army and strong economy.[94]
The financial crisis of the Great Depression led the Dominion of Newfoundland to relinquish responsible government in 1934 and become a Crown colony ruled by a British governor.[95] After two referendums, Newfoundlanders voted to join Canada in 1949 as a province.[96]
Canada's post-war economic growth, combined with the policies of successive Liberal governments, led to the emergence of a new Canadian identity, marked by the adoption of the maple leaf flag in 1965,[97] the implementation of official bilingualism (English and French) in 1969,[98] and the institution of official multiculturalism in 1971.[99] Socially democratic programs were also instituted, such as Medicare, the Canada Pension Plan, and Canada Student Loans; though, provincial governments, particularly Quebec and Alberta, opposed many of these as incursions into their jurisdictions.[100]
Finally, another series of constitutional conferences resulted in the Canada Act 1982, the patriation of Canada's constitution from the United Kingdom, concurrent with the creation of the Canadian Charter of Rights and Freedoms.[101][102][103] Canada had established complete sovereignty as an independent country under its own monarchy.[104][105] In 1999, Nunavut became Canada's third territory after a series of negotiations with the federal government.[106]
At the same time, Quebec underwent profound social and economic changes through the Quiet Revolution of the 1960s, giving birth to a secular nationalist movement.[107] The radical Front de libération du Québec (FLQ) ignited the October Crisis with a series of bombings and kidnappings in 1970,[108] and the sovereignist Parti Québécois was elected in 1976, organizing an unsuccessful referendum on sovereignty-association in 1980. Attempts to accommodate Quebec nationalism constitutionally through the Meech Lake Accord failed in 1990.[109] This led to the formation of the Bloc Québécois in Quebec and the invigoration of the Reform Party of Canada in the West.[110][111] A second referendum followed in 1995, in which sovereignty was rejected by a slimmer margin of 50.6 to 49.4 percent.[112] In 1997, the Supreme Court ruled unilateral secession by a province would be unconstitutional, and the Clarity Act was passed by Parliament, outlining the terms of a negotiated departure from Confederation.[109]
In addition to the issues of Quebec sovereignty, a number of crises shook Canadian society in the late 1980s and early 1990s. These included the explosion of Air India Flight 182 in 1985, the largest mass murder in Canadian history;[113] the École Polytechnique massacre in 1989, a university shooting targeting female students;[114] and the Oka Crisis of 1990,[115] the first of a number of violent confrontations between provincial governments and Indigenous groups.[116] Canada also joined the Gulf War in 1990 as part of a United States–led coalition force and was active in several peacekeeping missions in the 1990s, including the UNPROFOR mission in the former Yugoslavia.[117] Canada sent troops to Afghanistan in 2001 but declined to join the United States–led invasion of Iraq in 2003.[118]
In 2011, Canadian forces participated in the NATO-led intervention into the Libyan Civil War[119] and also became involved in battling the Islamic State insurgency in Iraq in the mid-2010s.[120] The country celebrated its sesquicentennial in 2017, three years before the COVID-19 pandemic in Canada began on January 27, 2020, with widespread social and economic disruption.[121] In 2021, the possible graves of hundreds of Indigenous people were discovered near the former sites of Canadian Indian residential schools.[122] Administered by various Christian churches and funded by the Canadian government from 1828 to 1997, these boarding schools attempted to assimilate Indigenous children into Euro-Canadian culture.[36]
By total area (including its waters), Canada is the second-largest country in the world, after Russia.[123] By land area alone, Canada ranks fourth, due to having the world's largest area of fresh water lakes.[124] Stretching from the Atlantic Ocean in the east, along the Arctic Ocean to the north, and to the Pacific Ocean in the west, the country encompasses 9,984,670 km2 (3,855,100 sq mi) of territory.[125] Canada also has vast maritime terrain, with the world's longest coastline of 243,042 kilometres (151,019 mi).[126][127] In addition to sharing the world's largest land border with the United States—spanning 8,891 km (5,525 mi)[a]—Canada shares a land border with Greenland (and hence the Kingdom of Denmark) to the northeast, on Hans Island,[128] and a maritime boundary with France's overseas collectivity of Saint Pierre and Miquelon to the southeast.[129] Canada is also home to the world's northernmost settlement, Canadian Forces Station Alert, on the northern tip of Ellesmere Island—latitude 82.5°N—which lies 817 kilometres (508 mi) from the North Pole.[130]
Canada can be divided into seven physiographic regions: the Canadian Shield, the interior plains, the Great Lakes-St. Lawrence Lowlands, the Appalachian region, the Western Cordillera, Hudson Bay Lowlands, and the Arctic Archipelago.[131] Boreal forests prevail throughout the country, ice is prominent in northern Arctic regions and through the Rocky Mountains, and the relatively flat Canadian Prairies in the southwest facilitate productive agriculture.[125] The Great Lakes feed the St. Lawrence River (in the southeast) where the lowlands host much of Canada's economic output.[125] Canada has over 2,000,000 lakes—563 of which are larger than 100 km2 (39 sq mi)—containing much of the world's fresh water.[132][133] There are also fresh-water glaciers in the Canadian Rockies, the Coast Mountains, and the Arctic Cordillera.[134] Canada is geologically active, having many earthquakes and potentially active volcanoes, notably Mount Meager massif, Mount Garibaldi, Mount Cayley, and the Mount Edziza volcanic complex.[135]
Average winter and summer high temperatures across Canada vary from region to region. Winters can be harsh in many parts of the country, particularly in the interior and Prairie provinces, which experience a continental climate, where daily average temperatures are near −15 °C (5 °F), but can drop below −40 °C (−40 °F) with severe wind chills.[136] In non-coastal regions, snow can cover the ground for almost six months of the year, while in parts of the north snow can persist year-round. Coastal British Columbia has a temperate climate, with a mild and rainy winter. On the east and west coasts, average high temperatures are generally in the low 20s °C (70s °F), while between the coasts, the average summer high temperature ranges from 25 to 30 °C (77 to 86 °F), with temperatures in some interior locations occasionally exceeding 40 °C (104 °F).[137]
Much of Northern Canada is covered by ice and permafrost. The future of the permafrost is uncertain because the Arctic has been warming at three times the global average as a result of climate change in Canada.[138] Canada's annual average temperature over land has risen by 1.7 °C (3.1 °F), with changes ranging from 1.1 to 2.3 °C (2.0 to 4.1 °F) in various regions, since 1948.[125]  The rate of warming has been higher across the North and in the Prairies.[139] In the southern regions of Canada, air pollution from both Canada and the United States—caused by metal smelting, burning coal to power utilities, and vehicle emissions—has resulted in acid rain, which has severely impacted waterways, forest growth, and agricultural productivity in Canada.[140]
Canada is divided into 15 terrestrial and five marine ecozones.[141] These ecozones encompass over 80,000 classified species of Canadian wildlife, with an equal number yet to be formally recognized or discovered.[142] Although Canada has a low percentage of endemic species compared to other countries,[143] due to human activities, invasive species, and environmental issues in the country, there are currently more than 800 species at risk of being lost.[144] About 65 percent of Canada's resident species are considered "Secure".[145] Over half of Canada's landscape is intact and relatively free of human development.[146] The boreal forest of Canada is considered to be the largest intact forest on Earth, with approximately 3,000,000 km2 (1,200,000 sq mi) undisturbed by roads, cities or industry.[147] Since the end of the last glacial period, Canada has consisted of eight distinct forest regions,[148] with 42 percent of its land area covered by forests (approximately 8 percent of the world's forested land).[149]
Approximately 12.1 percent of the nation's landmass and freshwater are conservation areas, including 11.4 percent designated as protected areas.[150] Approximately 13.8 percent of its territorial waters are conserved, including 8.9 percent designated as protected areas.[150] Canada's first National Park, Banff National Park established in 1885, spans 6,641 square kilometres (2,564 sq mi)[151] of mountainous terrain, with many glaciers and ice fields, dense coniferous forest, and alpine landscapes.[152] Canada's oldest provincial park, Algonquin Provincial Park, established in 1893, covers an area of 7,653.45 square kilometres (2,955.01 sq mi). It is dominated by old-growth forest with over 2,400 lakes and 1,200 kilometres of streams and rivers.[153] Lake Superior National Marine Conservation Area is the world's largest freshwater protected area, spanning roughly 10,000 square kilometres (3,900 sq mi) of lakebed, its overlaying freshwater, and associated shoreline on 60 square kilometres (23 sq mi) of islands and mainland.[154] Canada's largest national wildlife region is the Scott Islands Marine National Wildlife Area, which spans 11,570.65 square kilometres (4,467.45 sq mi)[155] and protects critical breeding and nesting habitat for over 40 percent of British Columbia's seabirds.[156] Canada's 18 UNESCO Biosphere Reserves cover a total area of 235,000 square kilometres (91,000 sq mi).[157]
Canada is described as a "full democracy",[158] with a tradition of liberalism,[159] and an egalitarian,[160] moderate political ideology.[161] An emphasis on social justice has been a distinguishing element of Canada's political culture.[162][163] Peace, order, and good government, alongside an Implied Bill of Rights, are founding principles of the Canadian government.[164][165]
At the federal level, Canada has been dominated by two relatively centrist parties practising "brokerage politics":[b] the centre-left leaning Liberal Party of Canada[172][173] and the centre-right leaning Conservative Party of Canada (or its predecessors).[174] Five parties had representatives elected to the Parliament in the 2021 election—the Liberals, who formed a minority government; the Conservatives, who became the Official Opposition; the New Democratic Party (occupying the left[175][176]); the Bloc Québécois; and the Green Party of Canada.[177] Far-right and far-left politics have never been a prominent force in Canadian society.[178][179][180]
Canada has a parliamentary system within the context of a constitutional monarchy—the monarchy of Canada being the foundation of the executive, legislative, and judicial branches.[181][182][183][184] The reigning monarch is also monarch of 14 other Commonwealth countries (though, all are sovereign of one another[185]) and each of Canada's 10 provinces. To carry out most of their federal royal duties in Canada, the monarch appoints a representative, the governor general, on the advice of the prime minister.[186][187]
The monarchy is the source of sovereignty and authority in Canada.[184][188][189] However, while the governor general or monarch may exercise their power without ministerial advice in certain rare crisis situations,[188] the use of the executive powers (or royal prerogative) is otherwise always directed by the Cabinet, a committee of ministers of the Crown responsible to the elected House of Commons and chosen and headed by the prime minister,[190] the head of government. To ensure the stability of government, the governor general will usually appoint as prime minister the individual who is the current leader of the political party that can obtain the confidence of a majority of members in the House of Commons.[191] The Prime Minister's Office (PMO) is thus one of the most powerful institutions in government, initiating most legislation for parliamentary approval and selecting for appointment by the Crown, besides the aforementioned, the governor general, lieutenant governors, senators, federal court judges, and heads of Crown corporations and government agencies.[188] The leader of the party with the second-most seats usually becomes the leader of the Official Opposition and is part of an adversarial parliamentary system intended to keep the government in check.[192]
The Parliament of Canada passes all statute laws within the federal sphere. It comprises the monarch, the House of Commons, and the Senate. While Canada inherited the British concept of parliamentary supremacy, this was later, with the enactment of the Constitution Act, 1982, all but completely superseded by the American notion of the supremacy of the law.[193]
Each of the 338 members of Parliament in the House of Commons is elected by simple plurality in an electoral district or riding. The Constitution Act, 1982, requires that no more than five years pass between elections, although the Canada Elections Act limits this to four years with a "fixed" election date in October; general elections still must be called by the governor general and can be triggered by either the advice of the prime minister or a lost confidence vote in the House.[194][195] The 105 members of the Senate, whose seats are apportioned on a regional basis, serve until age 75.[196]
Canadian federalism divides government responsibilities between the federal government and the 10 provinces. Provincial legislatures are unicameral and operate in parliamentary fashion similar to the House of Commons.[189] Canada's three territories also have legislatures; but, these are not sovereign and have fewer constitutional responsibilities than the provinces.[197] The territorial legislatures also differ structurally from their provincial counterparts.[198]
The Bank of Canada is the central bank of the country.[199] The minister of finance and minister of innovation, science, and industry use the Statistics Canada agency for financial planning and economic policy development.[200] The Bank of Canada is the sole authority authorized to issue currency in the form of Canadian bank notes.[201] The bank does not issue Canadian coins; they are issued by the Royal Canadian Mint.[202]
The constitution of Canada is the supreme law of the country and consists of written text and unwritten conventions.[203] The Constitution Act, 1867 (known as the British North America Act, 1867 prior to 1982), affirmed governance based on parliamentary precedent and divided powers between the federal and provincial governments.[204] The Statute of Westminster, 1931, granted full autonomy, and the Constitution Act, 1982, ended all legislative ties to Britain, as well as adding a constitutional amending formula and the Canadian Charter of Rights and Freedoms.[205] The Charter guarantees basic rights and freedoms that usually cannot be over-ridden by any government; though, a notwithstanding clause allows Parliament and the provincial legislatures to override certain sections of the Charter for a period of five years.[206]
Canada's judiciary plays an important role in interpreting laws and has the power to strike down acts of Parliament that violate the constitution. The Supreme Court of Canada is the highest court, final arbiter, and has been led since December 18, 2017, by Richard Wagner, the Chief Justice of Canada.[207] The governor general appoints the court's nine members on the advice of the prime minister and minister of justice.[208] The federal Cabinet also appoints justices to superior courts in the provincial and territorial jurisdictions.[209]
Common law prevails everywhere, except in Quebec, where civil law predominates.[210] Criminal law is solely a federal responsibility and is uniform throughout Canada.[211] Law enforcement, including criminal courts, is officially a provincial responsibility, conducted by provincial and municipal police forces.[212] In most rural and some urban areas, policing responsibilities are contracted to the federal Royal Canadian Mounted Police.[213]
Canadian Aboriginal law provides certain constitutionally recognized rights to land and traditional practices for Indigenous groups in Canada.[214] Various treaties and case laws were established to mediate relations between Europeans and many Indigenous peoples.[215] Most notably, a series of 11 treaties, known as the Numbered Treaties, were signed between the Indigenous peoples and the reigning monarch of Canada between 1871 and 1921.[216] These treaties are agreements between the Canadian Crown-in-Council, with the duty to consult and accommodate.[217] The role of Aboriginal law and the rights they support were reaffirmed by section 35 of the Constitution Act, 1982.[215] These rights may include provision of services, such as healthcare through the Indian Health Transfer Policy, and exemption from taxation.[218]
Canada is recognized as a middle power for its role in international affairs with a tendency to pursue multilateral solutions.[219] Canada's foreign policy based on international peacekeeping and security is carried out through coalitions, international organizations, and the work of numerous federal institutions.[220][221] Canada's peacekeeping role during the 20th century has played a major role in its global image.[222][223] The strategy of the Canadian government's foreign aid policy reflects an emphasis to meet the Millennium Development Goals, while also providing assistance in response to foreign humanitarian crises.[224]
Canada was a founding member of the United Nations and has membership in the World Trade Organization, the G20, and the Organisation for Economic Co-operation and Development (OECD).[219] Canada is also a member of various other international and regional organizations and forums for economic and cultural affairs.[225] Canada acceded to the International Covenant on Civil and Political Rights in 1976.[226] Canada joined the Organization of American States (OAS) in 1990 and hosted the OAS General Assembly in 2000 and the 3rd Summit of the Americas in 2001.[227] Canada seeks to expand its ties to Pacific Rim economies through membership in the Asia-Pacific Economic Cooperation forum (APEC).[228]
Canada and the United States share the world's longest undefended border, co-operate on military campaigns and exercises, and are each other's largest trading partner.[229][230] Canada nevertheless has an independent foreign policy.[231] For example, it maintains full relations with Cuba and declined to participate in the 2003 invasion of Iraq.[232]
Canada maintains historic ties to the United Kingdom and France and to other former British and French colonies through Canada's membership in the Commonwealth of Nations and the Organisation internationale de la Francophonie.[233] Canada is noted for having a positive relationship with the Netherlands, owing, in part, to its contribution to the Dutch liberation during World War II.[93]
Canada's earlier strong attachment to the British Empire and, later, the Commonwealth led to major participation in British military efforts in the Second Boer War (1899–1902), First World War (1914–1918), and Second World War (1939–1945).[234] Since then, Canada has been an advocate for multilateralism, making efforts to resolve global issues in collaboration with other nations.[235][236] During the Cold War, Canada was a major contributor to UN forces in the Korean War and founded the North American Aerospace Defense Command (NORAD), in cooperation with the United States, to defend against potential aerial attacks from the Soviet Union.[237]
During the Suez Crisis of 1956, future prime minister Lester B. Pearson eased tensions by proposing the inception of the United Nations Peacekeeping Force, for which he was awarded the 1957 Nobel Peace Prize.[238] As this was the first UN peacekeeping mission, Pearson is often credited as the inventor of the concept.[239] Canada has since served in over 50 peacekeeping missions, including every UN peacekeeping effort until 1989,[86] and has since maintained forces in international missions in Rwanda, the former Yugoslavia, and elsewhere. Canada has sometimes faced controversy over its involvement in foreign countries, notably in the 1993 Somalia affair.[240]
In 2001, Canada deployed troops to Afghanistan as part of the US stabilization force and the UN-authorized, NATO-led International Security Assistance Force.[241] In August 2007, Canada's territorial claims in the Arctic were challenged after a Russian underwater expedition to the North Pole; Canada has considered that area to be sovereign territory since 1925.[242]
The unified Canadian Forces (CF) comprise the Royal Canadian Navy, Canadian Army, and Royal Canadian Air Force. The nation employs a professional, volunteer force of approximately 68,000 active personnel and 27,000 reserve personnel—increasing to 71,500 and 30,000 respectively under "Strong, Secure, Engaged"[243]—with a sub-component of approximately 5,000 Canadian Rangers.[244][c] In 2021, Canada's military expenditure totalled approximately $26.4 billion, or around 1.3 percent of the country's gross domestic product (GDP).[246] Canada's total military expenditure is expected to reach $32.7 billion by 2027.[247] Canada's military currently has over 3000 personnel deployed overseas in multiple operations, such as Operation Snowgoose in Cyprus, Operation Unifier supporting Ukraine, Operation Caribbe in the Caribbean Sea, and Operation Impact, a coalition for the military intervention against ISIL.[248]
Canada is a federation composed of 10 federated states, called provinces, and three federal territories. In turn, these may be grouped into four main regions: Western Canada, Central Canada, Atlantic Canada, and Northern Canada (Eastern Canada refers to Central Canada and Atlantic Canada together).[249] Provinces and territories have responsibility for social programs such as healthcare, education, and welfare,[250] as well as administration of justice (but not criminal law). Together, the provinces collect more revenue than the federal government, a rarity among other federations in the world. Using its spending powers, the federal government can initiate national policies in provincial areas such as health and child care; the provinces can opt out of these cost-share programs but rarely do so in practice. Equalization payments are made by the federal government to ensure reasonably uniform standards of services and taxation are kept between the richer and poorer provinces.[251]
The major difference between a Canadian province and a territory is that provinces receive their sovereignty from the Crown[252] and power and authority from the Constitution Act, 1867, whereas territorial governments have powers delegated to them by the Parliament of Canada[253] and the commissioners represent the King in his federal Council,[254] rather than the monarch directly. The powers flowing from the Constitution Act, 1867, are divided between the federal government and the provincial governments to exercise exclusively[255] and any changes to that arrangement require a constitutional amendment, while changes to the roles and powers of the territories may be performed unilaterally by the Parliament of Canada.[256]
Canada has a highly developed mixed-market economy,[258][259] with the world's eighth-largest economy as of 2022[update], and a nominal GDP of approximately US$2.221 trillion.[260] It is one of the world's largest trading nations, with a highly globalized economy.[261] In 2021, Canadian trade in goods and services reached $2.016 trillion.[262] Canada's exports totalled over $637 billion, while its imported goods were worth over $631 billion, of which approximately $391 billion originated from the United States.[262] In 2018, Canada had a trade deficit in goods of $22 billion and a trade deficit in services of $25 billion.[262]  The Toronto Stock Exchange is the ninth-largest stock exchange in the world by market capitalization, listing over 1,500 companies with a combined market capitalization of over US$2 trillion.[263] 
Canada has a strong cooperative banking sector, with the world's highest per-capita membership in credit unions.[264] It ranks low in the Corruption Perceptions Index (14th in 2023)[265] and "is widely regarded as among the least corrupt countries of the world".[266] It ranks high in the Global Competitiveness Report (14th in 2019)[267] and Global Innovation Indexes (15th in 2022).[268] Canada's economy ranks above most Western nations on The Heritage Foundation's Index of Economic Freedom[269] and experiences a relatively low level of income disparity.[270] The country's average household disposable income per capita is "well above" the OECD average.[271] Canada ranks among the lowest of the most developed countries for housing affordability[272][273] and  foreign direct investment.[274][273]
Since the early 20th century, the growth of Canada's manufacturing, mining, and service sectors has transformed the nation from a largely rural economy to an urbanized, industrial one.[275] Like many other developed countries, the Canadian economy is dominated by the service industry, which employs about three-quarters of the country's workforce.[276] Among developed countries, Canada has an unusually important primary sector, of which the forestry and petroleum industries are the most prominent components.[277] Many towns in northern Canada, where agriculture is difficult, are sustained by nearby mines or sources of timber.[278] 
Canada's economic integration with the United States has increased significantly since World War II.[279] The Automotive Products Trade Agreement of 1965 opened Canada's borders to trade in the automobile manufacturing industry.[280]  The Canada – United States Free Trade Agreement (FTA) of 1988 eliminated tariffs between the two countries, while the North American Free Trade Agreement (NAFTA) expanded the free-trade zone to include Mexico in 1994 (later replaced by the Canada–United States–Mexico Agreement).[281] As of 2023, Canada is a signatory to 15 free trade agreements with 51 different countries.[282]
Canada is one of the few developed nations that are net exporters of energy.[277][283] Atlantic Canada possess vast offshore deposits of natural gas,[284] and Alberta hosts the fourth-largest oil reserves in the world.[285] The vast Athabasca oil sands and other oil reserves give Canada 13 percent of global oil reserves, constituting the world's third or fourth-largest.[286] Canada is additionally one of the world's largest suppliers of agricultural products; the Canadian Prairies region is one of the most important global producers of wheat, canola, and other grains.[287] The country is a leading exporter  of zinc, uranium, gold, nickel, platinoids, aluminum, steel, iron ore, coking coal, lead, copper, molybdenum, cobalt, and cadmium.[288][289] Canada has a sizeable manufacturing sector centred in southern Ontario and Quebec, with automobiles and aeronautics representing particularly important industries.[290] The fishing industry is also a key contributor to the economy.[291]
In 2020, Canada spent approximately $41.9 billion on domestic research and development, with supplementary estimates for 2022 at $43.2 billion.[292] As of 2023[update], the country has produced 15 Nobel laureates in physics, chemistry, and medicine,[293] Canada ranks seventh in the worldwide share of articles published in scientific journals, according to the Nature Index,[294] and is home to the headquarters of a number of global technology firms.[295] Canada has one of the highest levels of Internet access in the world, with over 33 million users, equivalent to around 94 percent of its total population.[296]
Canada's developments in science and technology include the creation of the modern alkaline battery,[297] the discovery of insulin,[298] the development of the polio vaccine,[299] and discoveries about the interior structure of the atomic nucleus.[300] Other major Canadian scientific contributions include the artificial cardiac pacemaker, mapping the visual cortex,[301][302] the development of the electron microscope,[303][304] plate tectonics, deep learning, multi-touch technology, and the identification of the first black hole, Cygnus X-1.[305] Canada has a long history of discovery in genetics, which include stem cells, site-directed mutagenesis, T-cell receptor, and the identification of the genes that cause Fanconi anemia, cystic fibrosis, and early-onset Alzheimer's disease, among numerous other diseases.[302][306]
The Canadian Space Agency operates a highly active space program, conducting deep-space, planetary, and aviation research and developing rockets and satellites.[307] Canada was the third country to design and construct a satellite after the Soviet Union and the United States, with the 1962 Alouette 1 launch.[308] Canada is a participant in the International Space Station (ISS), and is a pioneer in space robotics, having constructed the Canadarm, Canadarm2 and Dextre robotic manipulators for the ISS and NASA's Space Shuttle.[309] Since the 1960s, Canada's aerospace industry has designed and built numerous marques of satellite, including Radarsat-1 and 2, ISIS, and MOST.[310] Canada has also produced one of the world's most successful and widely used sounding rockets, the Black Brant.[311]
The 2021 Canadian census enumerated a total population of 36,991,981, an increase of around 5.2 percent over the 2016 figure.[313] The main drivers of population growth are immigration and, to a lesser extent, natural growth.[314] Canada has one of the highest per-capita immigration rates in the world,[315] driven mainly by economic policy and also family reunification.[316][317] A record 405,000 immigrants were admitted to Canada in 2021.[318] New immigrants settle mostly in major urban areas in the country, such as Toronto, Montreal, and Vancouver.[319] Canada also accepts large numbers of refugees, accounting for over 10 percent of annual global refugee resettlements; it resettled more than 28,000 in 2018.[320][321]
Canada's population density, at 4.2 inhabitants per square kilometre (11/sq mi), is among the lowest in the world.[313] Canada spans latitudinally from the 83rd parallel north to the 41st parallel north and approximately 95 percent of the population is found south of the 55th parallel north.[322] About 80 percent of the population lives within 150 kilometres (93 mi) of the border with the contiguous United States.[323] The most densely populated part of the country, accounting for nearly 50 percent, is the Quebec City–Windsor Corridor in Southern Quebec and Southern Ontario along the Great Lakes and the St. Lawrence River.[312][322]
The majority of Canadians (81.1 percent) live in family households, 12.1 percent report living alone, and those living with other relatives or unrelated persons reported at 6.8 percent.[324] Fifty-one percent of households are couples with or without children, 8.7 percent are single-parent households, 2.9 percent are multigenerational households, and 29.3 percent are single-person households.[324]

According to the 2021 Canadian census, over 450 "ethnic or cultural origins" were self-reported by Canadians.[325] The major panethnic groups chosen were: European (52.5 percent), North American (22.9 percent), Asian (19.3 percent), North American Indigenous (6.1 percent), African (3.8 percent), Latin, Central and South American (2.5 percent), Caribbean (2.1 percent), Oceanian (0.3 percent), and other (6 percent).[325][326] Over 60 percent of Canadians reported a single origin, and 36 percent of Canadians reported having multiple ethnic origins, thus the overall total is greater than 100 percent.[325]
The country's ten largest self-reported specific ethnic or cultural origins in 2021 were Canadian[d] (accounting for 15.6 percent of the population), followed by English (14.7 percent), Irish (12.1 percent), Scottish (12.1 percent), French (11.0 percent), German (8.1 percent), Chinese (4.7 percent), Italian (4.3 percent), Indian (3.7 percent), and Ukrainian (3.5 percent).[330]
Of the 36.3 million people enumerated in 2021, approximately 25.4 million reported being "White", representing 69.8 percent of the population.[331] The Indigenous population representing 5 percent or 1.8 million individuals, grew by 9.4 percent compared to the non-Indigenous population, which grew by 5.3 percent from 2016 to 2021.[331] One out of every four Canadians or 26.5 percent of the population belonged to a non-White and non-Indigenous visible minority,[332][e] the largest of which in 2021 were South Asian (2.6 million people; 7.1 percent), Chinese (1.7 million; 4.7 percent), and Black (1.5 million; 4.3 percent).[334]
Between 2011 and 2016, the visible minority population rose by 18.4 percent.[335] In 1961, about 300,000 people, less than two percent of Canada's population, were members of visible minority groups.[336] The 2021 census indicated that 8.3 million people, or almost one-quarter (23.0 percent) of the population, reported themselves as being or having been a landed immigrant or permanent resident in Canada—above the 1921 census previous record of 22.3 percent.[337] In 2021, India, China, and the Philippines were the top three countries of origin for immigrants moving to Canada.[338]
A multitude of languages are used by Canadians, with English and French (the official languages) being the mother tongues of approximately 54 percent and 19 percent of Canadians, respectively.[340] As of the 2021 census, just over 7.8 million Canadians listed a non-official language as their mother tongue. Some of the most common non-official first languages include Mandarin (679,255 first-language speakers), Punjabi (666,585), Cantonese (553,380), Spanish (538,870), Arabic (508,410), Tagalog (461,150), Italian (319,505), and German (272,865).[340] Canada's federal government practises official bilingualism, which is applied by the commissioner of official languages in consonance with section 16 of the Canadian Charter of Rights and Freedoms and the federal Official Languages Act. English and French have equal status in federal courts, Parliament, and in all federal institutions. Citizens have the right, where there is sufficient demand, to receive federal government services in either English or French and official-language minorities are guaranteed their own schools in all provinces and territories.[341]
The 1977 Charter of the French Language established French as the official language of Quebec.[342] Although more than 82 percent of French-speaking Canadians live in Quebec, there are substantial Francophone populations in New Brunswick, Alberta, and Manitoba; Ontario has the largest French-speaking population outside Quebec.[343] New Brunswick, the only officially bilingual province, has a French-speaking Acadian minority constituting 33 percent of the population.[344] There are also clusters of Acadians in southwestern Nova Scotia, on Cape Breton Island, and in central and western Prince Edward Island.[345]
Other provinces have no official languages as such, but French is used as a language of instruction, in courts, and for other government services, in addition to English. Manitoba, Ontario, and Quebec allow for both English and French to be spoken in the provincial legislatures and laws are enacted in both languages. In Ontario, French has some legal status, but is not fully co-official.[346] There are 11 Indigenous language groups, composed of more than 65 distinct languages and dialects.[347] Several Indigenous languages have official status in the Northwest Territories.[348] Inuktitut is the majority language in Nunavut and is one of three official languages in the territory.[349]
Additionally, Canada is home to many sign languages, some of which are Indigenous.[350] American Sign Language (ASL) is used across the country due to the prevalence of ASL in primary and secondary schools.[351] Quebec Sign Language (LSQ) is used primarily in Quebec.[352]
Canada is religiously diverse, encompassing a wide range of beliefs and customs.[354] Although the constitution of Canada refers to God and the monarch carries the title of Defender of the Faith, Canada has no official church and the government is officially committed to religious pluralism.[355] Freedom of religion in Canada is a constitutionally protected right, allowing individuals to assemble and worship without limitation or interference.[356]
The practice of religion is generally considered a private matter throughout society and the state.[357] Rates of religious adherence have steadily decreased from the 1970s to the 2020s.[354] With Christianity in decline after having once been central and integral to Canadian culture and daily life,[358] Canada has become a post-Christian, secular state.[359][360][361][362] The majority of Canadians consider religion to be unimportant in their daily lives,[354][363] but still believe in God.[364]
According to the 2021 census, Christianity is the largest religion in Canada, with Roman Catholics representing 29.9 percent of the population having the most adherents. Christians overall representing 53.3 percent of the population,[f] are followed by people reporting irreligion or having no religion at 34.6 percent.[366] Other faiths include Islam (4.9 percent), Hinduism (2.3 percent), Sikhism (2.1 percent), Buddhism (1.0 percent), Judaism (0.9 percent), and Indigenous spirituality (0.2 percent).[367] Canada has the second-largest national Sikh population, behind India.[368][369]
Healthcare in Canada is delivered through the provincial and territorial systems of publicly funded health care, informally called Medicare.[370][371] It is guided by the provisions of the Canada Health Act of 1984[372] and is universal.[373] Universal access to publicly funded health services "is often considered by Canadians as a fundamental value that ensures national healthcare insurance for everyone wherever they live in the country."[374] Around 30 percent of Canadians' healthcare is paid for through the private sector.[375] This mostly pays for services not covered or partially covered by Medicare, such as prescription drugs, dentistry and optometry.[375] Approximately 65 to 75 percent of Canadians have some form of supplementary health insurance; many receive it through their employers or access secondary social service programs.[376][375]
In common with many other developed countries, Canada is experiencing an increase in healthcare expenditures due to a demographic shift toward an older population, with more retirees and fewer people of working age. In 2021, the average age in Canada was 41.9 years.[324] Life expectancy is 81.1 years.[377] A 2016 report by the chief public health officer found that 88 percent of Canadians, one of the highest proportions of the population among G7 countries, indicated that they "had good or very good health".[378] Eighty percent of Canadian adults self-report having at least one major risk factor for chronic disease: smoking, physical inactivity, unhealthy eating or excessive alcohol use.[379] Canada has one of the highest rates of adult obesity among OECD countries, contributing to approximately 2.7 million cases of diabetes.[379] Four chronic diseases—cancer (leading cause of death), cardiovascular diseases, respiratory diseases, and diabetes—account for 65 percent of deaths in Canada.[380][381]
In 2021, the Canadian Institute for Health Information reported that healthcare spending reached $308 billion, or 12.7 percent of Canada's GDP for that year.[382] Canada's per-capita spending on health expenditures ranked 4th among health-care systems in the OECD.[383] Canada has performed close to, or above the average on the majority of OECD health indicators since the early 2000s, ranking above the average on OECD indicators for wait-times and access to care, with average scores for quality of care and use of resources.[384][385] The Commonwealth Fund's 2021 report comparing the healthcare systems of the 11 most developed countries ranked Canada second-to-last.[386] Identified weaknesses were comparatively higher infant mortality rate, the prevalence of chronic conditions, long wait times, poor availability of after-hours care, and a lack of prescription drugs and dental coverage.[386] An increasing problem in Canada's health system is a lack of healthcare professionals.[387]
Education in Canada is for the most part provided publicly, funded and overseen by federal, provincial, and local governments.[388] Education is within provincial jurisdiction and the curriculum is overseen by the province.[389][390] Education in Canada is generally divided into primary education, followed by secondary education and post-secondary. Education in both English and French is available in most places across Canada.[391] Canada has a large number of universities, almost all of which are publicly funded.[392] Established in 1663, Université Laval is the oldest post-secondary institution in Canada.[393] The largest university is the University of Toronto with over 85,000 students.[394] Four universities are regularly ranked among the top 100 world-wide, namely University of Toronto, University of British Columbia, McGill University, and McMaster University, with a total of 18 universities ranked in the top 500 worldwide.[395]
According to a 2019 report by the OECD, Canada is one of the most educated countries in the world;[396] the country ranks first worldwide in the percentage of adults having tertiary education, with over 56 percent of Canadian adults having attained at least an undergraduate college or university degree.[396] Canada spends about 5.3 percent of its GDP on education.[397] The country invests heavily in tertiary education (more than US$20,000 per student).[398] As of 2014[update], 89 percent of adults aged 25 to 64 have earned the equivalent of a high-school degree, compared to an OECD average of 75 percent.[399]
The mandatory education age ranges between 5–7 to 16–18 years,[400] contributing to an adult literacy rate of 99 percent.[401] Just over 60,000 children are homeschooled in the country as of 2016. The Programme for International Student Assessment indicates Canadian students perform well above the OECD average, particularly in mathematics, science, and reading,[402][403] ranking the overall knowledge and skills of Canadian 15-year-olds as the sixth-best in the world, although these scores have been declining in recent years. Canada is a well-performing OECD country in reading literacy, mathematics, and science, with the average student scoring 523.7, compared with the OECD average of 493 in 2015.[404][405]
Canada's culture draws influences from its broad range of constituent nationalities and policies that promote a "just society" are constitutionally protected.[406][407][408] Since the 1960s, Canada has emphasized equality and inclusiveness for all its people.[409][410][411] The official state policy of multiculturalism is often cited as one of Canada's significant accomplishments[412] and a key distinguishing element of Canadian identity.[413][414] In Quebec, cultural identity is strong and there is a French Canadian culture that is distinct from English Canadian culture.[415]
Canada's approach to governance emphasizing multiculturalism, which is based on selective immigration, social integration, and suppression of far-right politics, has wide public support.[416] Government policies such as publicly funded health care, higher taxation to redistribute wealth, the outlawing of capital punishment, strong efforts to eliminate poverty, strict gun control, a social liberal attitude toward women's rights (like pregnancy termination) and LGBTQ rights, and legalized euthanasia and cannabis use are indicators of Canada's political and cultural values.[417][418][419] Canadians also identify with the country's foreign aid policies, peacekeeping roles, the national park system, and the Canadian Charter of Rights and Freedoms.[420][421]
Historically, Canada has been influenced by British, French, and Indigenous cultures and traditions. Through their language, art, and music, Indigenous peoples continue to influence the Canadian identity.[422] During the 20th century, Canadians with African, Caribbean, and Asian nationalities have added to the Canadian identity and its culture.[423] 
Themes of nature, pioneers, trappers, and traders played an important part in the early development of Canadian symbolism.[425] Modern symbols emphasize the country's geography, cold climate, lifestyles, and the Canadianization of traditional European and Indigenous symbols.[426] The use of the maple leaf as a Canadian symbol dates to the early 18th century. The maple leaf is depicted on Canada's current and previous flags and on the Arms of Canada.[427] Canada's official tartan, known as the "maple leaf tartan", has four colours that reflect the colours of the maple leaf as it changes through the seasons—green in the spring, gold in the early autumn, red at the first frost, and brown after falling.[428] The Arms of Canada are closely modelled after those of the United Kingdom, with French and distinctive Canadian elements replacing or added to those derived from the British version.[429]
Other prominent symbols include the national motto, "A mari usque ad mare" ("From Sea to Sea"),[430] the sports of ice hockey and lacrosse, the beaver, Canada goose, common loon, Canadian horse, the Royal Canadian Mounted Police, the Canadian Rockies,[427] and, more recently, the totem pole and Inuksuk.[431] Canadian beer, maple syrup, tuques, canoes, nanaimo bars, butter tarts, and poutine are defined as uniquely Canadian.[431][432] Canadian coins feature many of these symbols: the loon on the $1 coin, the Arms of Canada on the 50¢ piece, and the beaver on the nickel.[433] An image of the previous monarch, Queen Elizabeth II, appears on $20 bank notes and the obverse of all current Canadian coins.[433]
Canadian literature is often divided into French- and English-language literatures, which are rooted in the literary traditions of France and Britain, respectively.[434] The earliest Canadian narratives were of travel and exploration.[435] This progressed into three major themes that can be found within historical Canadian literature: nature, frontier life, and Canada's position within the world, all three of which tie into the garrison mentality.[436] In recent decades, Canada's literature has been strongly influenced by immigrants from around the world.[437] By the 1990s, Canadian literature was viewed as some of the world's best.[438]
Numerous Canadian authors have accumulated international literary awards,[439] including novelist, poet, and literary critic Margaret Atwood, who received two Booker Prizes;[440] Nobel laureate Alice Munro, who has been called the best living writer of short stories in English;[441] and Booker Prize recipient Michael Ondaatje, who wrote the novel The English Patient, which was adapted as a film of the same name that won the Academy Award for Best Picture.[442] L. M. Montgomery produced a series of children's novels beginning in 1908 with Anne of Green Gables.[443]
Canada's media is highly autonomous, uncensored, diverse, and very regionalized.[444][445] The Broadcasting Act declares "the system should serve to safeguard, enrich, and strengthen the cultural, political, social, and economic fabric of Canada".[446] Canada has a well-developed media sector, but its cultural output—particularly in English films, television shows, and magazines—is often overshadowed by imports from the United States.[447] As a result, the preservation of a distinctly Canadian culture is supported by federal government programs, laws, and institutions such as the Canadian Broadcasting Corporation (CBC), the National Film Board of Canada (NFB), and the Canadian Radio-television and Telecommunications Commission (CRTC).[448]
Canadian mass media, both print and digital, and in both official languages, is largely dominated by a "handful of corporations".[449] The largest of these corporations is the country's national public broadcaster, the Canadian Broadcasting Corporation, which also plays a significant role in producing domestic cultural content, operating its own radio and TV networks in both English and French.[450] In addition to the CBC, some provincial governments offer their own public educational TV broadcast services as well, such as TVOntario and Télé-Québec.[451]
Non-news media content in Canada, including film and television, is influenced both by local creators as well as by imports from the United States, the United Kingdom, Australia, and France.[452] In an effort to reduce the amount of foreign-made media, government interventions in television broadcasting can include both regulation of content and public financing.[453] Canadian tax laws limit foreign competition in magazine advertising.[454]
Art in Canada is marked by thousands of years of habitation by its Indigenous peoples,[455] and, in later times, artists have combined British, French, Indigenous, and American artistic traditions, at times embracing European styles while working to promote nationalism.[456] The nature of Canadian art reflects these diverse origins, as artists have taken their traditions and adapted these influences to reflect the reality of their lives in Canada.[457]
The Canadian government has played a role in the development of Canadian culture through the department of Canadian Heritage, by giving grants to art galleries,[458] as well as establishing and funding art schools and colleges across the country, and through the Canada Council for the Arts, the national public arts funder, helping artists, art galleries and periodicals, and thus contributing to the development of Canada's cultural works.[459] 
Canadian visual art has been dominated by figures, such as painter Tom Thomson and the Group of Seven.[460] The latter were painters with a nationalistic and idealistic focus, who first exhibited their distinctive works in May 1920. Though referred to as having seven members, five artists—Lawren Harris, A. Y. Jackson, Arthur Lismer, J. E. H. MacDonald, and Frederick Varley—were responsible for articulating the group's ideas. They were joined briefly by Frank Johnston and commercial artist Franklin Carmichael. A. J. Casson became part of the group in 1926.[461] Associated with the group was another prominent Canadian artist, Emily Carr, known for her landscapes and portrayals of the Indigenous peoples of the Pacific Northwest Coast.[462]
Canadian music reflects a variety of regional scenes.[463] Canada has developed a vast music infrastructure that includes church halls, chamber halls, conservatories, academies, performing arts centres, record companies, radio stations, and television music video channels.[464] Government support programs, such as the Canada Music Fund, assist a wide range of musicians and entrepreneurs who create, produce and market original and diverse Canadian music.[465] As a result of its cultural importance, as well as government initiatives and regulations, the Canadian music industry is one of the largest in the world,[466] producing internationally renowned composers, musicians, and ensembles.[467] Music broadcasting in the country is regulated by the CRTC.[468] The Canadian Academy of Recording Arts and Sciences presents Canada's music industry awards, the Juno Awards.[469] The Canadian Music Hall of Fame honours Canadian musicians for their lifetime achievements.[470]
Patriotic music in Canada dates back over 200 years. The earliest work of patriotic music in Canada, "The Bold Canadian", was written in 1812.[471] "The Maple Leaf Forever", written in 1866, was a popular patriotic song throughout English Canada and, for many years, served as an unofficial national anthem.[472] "O Canada" was adopted as the official anthem in 1980.[473] Calixa Lavallée wrote the music, which was a setting of a patriotic poem composed by the poet and judge Sir Adolphe-Basile Routhier. The text was originally only in French before it was adapted into English in 1906.[474]
The roots of organized sports in Canada date back to the 1770s,[475] culminating in the development and popularization of the major professional games of ice hockey, lacrosse, curling, basketball, baseball, soccer, and Canadian football.[476] Canada's official national sports are ice hockey and lacrosse.[477] Other sports such as golf, volleyball, skiing, cycling, swimming, badminton, tennis, bowling, and the study of martial arts are all widely enjoyed at the youth and amateur levels.[478] Great achievements in Canadian sports are recognized by Canada's Sports Hall of Fame.[479] There are numerous other sport "halls of fame" in Canada, such as the Hockey Hall of Fame.[479]
Canada shares several major professional sports leagues with the United States.[480] Canadian teams in these leagues include seven franchises in the National Hockey League, as well as three Major League Soccer teams and one team in each of Major League Baseball and the National Basketball Association. Other popular professional competitions include the Canadian Football League, National Lacrosse League, the Canadian Premier League, and the various curling tournaments sanctioned and organized by Curling Canada.[481]
Canada has enjoyed success both at the Winter Olympics and at the Summer Olympics[482]—though, particularly, the Winter Games as a "winter sports nation"—and has hosted several high-profile international sporting events such as the 1976 Summer Olympics,[483] the 1988 Winter Olympics,[484] the 2010 Winter Olympics,[485][486] and the 2015 FIFA Women's World Cup.[487] Most recently, Canada hosted the 2015 Pan American Games and 2015 Parapan American Games in Toronto.[488] The country is scheduled to co-host the 2026 FIFA World Cup alongside Mexico and the United States.[489]
Overview
Culture
Demography and statistics
Economy
Foreign relations and military
Geography and climate
Government and law
History
Social welfare
Overviews
Government
Travel
 
60°N 110°W﻿ / ﻿60°N 110°W﻿ / 60; -110




Russia (Russian: Россия, romanized: Rossiya, [rɐˈsʲijə]), or the Russian Federation,[c] is a transcontinental country spanning Eastern Europe and Northern Asia. It is the largest country in the world, encompassing one-eighth of Earth's inhabitable landmass. Russia extends across eleven time zones and shares land boundaries with fourteen countries.[d] It is the world's ninth-most populous country and Europe's most populous country. The country's capital and largest city is Moscow. Saint Petersburg is Russia's cultural centre and second-largest city. Other major urban areas in the country include Novosibirsk, Yekaterinburg, Nizhny Novgorod, Chelyabinsk, Krasnoyarsk, and Kazan.
The East Slavs emerged as a recognisable group in Europe between the 3rd and 8th centuries CE. The first East Slavic state, Kievan Rus', arose in the 9th century, and in 988, it adopted Orthodox Christianity from the Byzantine Empire. Rus' ultimately disintegrated, with the Grand Duchy of Moscow growing to become the Tsardom of Russia. By the early 18th century, Russia had vastly expanded through conquest, annexation, and the efforts of Russian explorers, developing into the Russian Empire, which remains the third-largest empire in history. However, with the Russian Revolution in 1917, Russia's monarchic rule was abolished and eventually replaced by the Russian SFSR—the world's first constitutionally socialist state. Following the Russian Civil War, the Russian SFSR established the Soviet Union with three other Soviet republics, within which it was the largest and principal constituent. At the expense of millions of lives, the Soviet Union underwent rapid industrialisation in the 1930s, and later played a decisive role for the Allies of World War II by leading large-scale efforts on the Eastern Front. With the onset of the Cold War, it competed with the United States for global ideological influence; the Soviet era of the 20th century saw some of the most significant Russian technological achievements, including the first human-made satellite and the first human expedition into outer space.
In 1991, the Russian SFSR emerged from the dissolution of the Soviet Union as the independent Russian Federation. A new constitution was adopted, which established a federal semi-presidential system. Since the turn of the century, Russia's political system has been dominated by Vladimir Putin, under whom the country has experienced democratic backsliding and a shift towards authoritarianism. Russia has been involved militarily in a number of post-Soviet conflicts, which has included the internationally unrecognised annexations of Crimea in 2014 from neighbouring Ukraine, followed by the further annexation of four other regions in 2022 during an ongoing invasion.
Internationally, Russia ranks amongst the lowest in measurements of democracy, human rights and freedom of the press; the country also has high levels of perceived corruption. The Russian economy ranks 11th by nominal GDP, relying heavily upon its abundant natural resources. Its mineral and energy sources are the world's largest, and its figures for oil production and natural gas production rank high globally. The Russian GDP ranks 65th by per capita, Russia possesses the largest stockpile of nuclear weapons, and has the third-highest military expenditure. The country is a permanent member of the United Nations Security Council; a member state of the G20, the SCO, BRICS, the APEC, the OSCE, and the WTO; and is the leading member state of post-Soviet organizations such as the CIS, the CSTO, and the Eurasian Economic Union (EAEU). Russia is home to 30 UNESCO World Heritage Sites.
The name Russia comes from a Medieval Latin name for Rus', a medieval state populated primarily by the East Slavs.[19][20] In modern historiography, this state is usually denoted as Kievan Rus' after its capital city.[21] The name Rus' itself comes from the early medieval Rus' people, who were originally a group of Norse merchants and warriors who relocated from across the Baltic Sea and first settled in the northern region of Novgorod, and later founded a state centred on Kiev.[22] Another Medieval Latin name for Rus' was Ruthenia.[23]
In Russian, the current name of the country, Россияcode: rus promoted to code: ru  (Rossiyacode: rus promoted to code: ru ), comes from the Byzantine Greek name for Rus', Ρωσία (Rosía).[24] A new form of the name Rus', Росияcode: rus promoted to code: ru  (Rosiyacode: rus promoted to code: ru ), was borrowed from the Greek term and first attested in 1387,[25] before coming into official use by the 15th century, though the country was still often referred to by its inhabitants as Rus' or the Russian land until the end of the 17th century.[26][27] There are two words in Russian which translate to "Russians" in English – русскиеcode: rus promoted to code: ru  (russkiyecode: rus promoted to code: ru ), which refers to ethnic Russians, and россиянеcode: rus promoted to code: ru  (rossiyanecode: rus promoted to code: ru ), which refers to Russian citizens, regardless of ethnicity.[27][28]
The first human settlement on Russia dates back to the Oldowan period in the early Lower Paleolithic. About 2 million years ago, representatives of Homo erectus migrated to the Taman Peninsula in southern Russia.[29] Flint tools, some 1.5 million years old, have been discovered in the North Caucasus.[30] Radiocarbon dated specimens from Denisova Cave in the Altai Mountains estimate the oldest Denisovan specimen lived 195–122,700 years ago.[31] Fossils of Denny, an archaic human hybrid that was half Neanderthal and half Denisovan, and lived some 90,000 years ago, was also found within the latter cave.[32] Russia was home to some of the last surviving Neanderthals, from about 45,000 years ago, found in Mezmaiskaya cave.[33]
The first trace of an early modern human in Russia dates back to 45,000 years, in Western Siberia.[34] The discovery of high concentration cultural remains of anatomically modern humans, from at least 40,000 years ago, was found at Kostyonki–Borshchyovo,[35] and at Sungir, dating back to 34,600 years ago—both in western Russia.[36] Humans reached Arctic Russia at least 40,000 years ago, in Mamontovaya Kurya.[37] Ancient North Eurasian populations from Siberia genetically similar to Mal'ta–Buret' culture and Afontova Gora were an important genetic contributor to Ancient Native Americans and Eastern Hunter-Gatherers.[38]
The Kurgan hypothesis places the Volga-Dnieper region of southern Russia and Ukraine as the urheimat of the Proto-Indo-Europeans.[40] Early Indo-European migrations from the Pontic–Caspian steppe of Ukraine and Russia spread Yamnaya ancestry and Indo-European languages across large parts of Eurasia.[41][42] Nomadic pastoralism developed in the Pontic–Caspian steppe beginning in the Chalcolithic.[43] Remnants of these steppe civilizations were discovered in places such as Ipatovo,[43] Sintashta,[44] Arkaim,[45] and Pazyryk,[46] which bear the earliest known traces of horses in warfare.[44] The genetic makeup of speakers of the Uralic language family in northern Europe was shaped by migration from Siberia that began at least 3,500 years ago.[47] In classical antiquity, the Pontic-Caspian Steppe was known as Scythia.[48] In late 8th century BCE, Ancient Greek traders brought classical civilization to the trade emporiums in Tanais and Phanagoria.[49]
In the 3rd to 4th centuries CE, the Gothic kingdom of Oium existed in southern Russia, which was later overrun by Huns.[50][failed verification] Between the 3rd and 6th centuries CE, the Bosporan Kingdom, which was a Hellenistic polity that succeeded the Greek colonies,[51] was also overwhelmed by nomadic invasions led by warlike tribes such as the Huns and Eurasian Avars.[52] The Khazars, who were of Turkic origin, ruled the steppes between the Caucasus in the south, to the east past the Volga river basin, and west as far as Kyiv on the Dnieper river until the 10th century.[53] After them came the Pechenegs who created a large confederacy, which was subsequently taken over by the Cumans and the Kipchaks.[54]
The ancestors of Russians are among the Slavic tribes that separated from the Proto-Indo-Europeans, who appeared in the northeastern part of Europe c. 1500 years ago.[55] The East Slavs gradually settled western Russia in two waves: one moving from Kiev towards present-day Suzdal and Murom and another from Polotsk towards Novgorod and Rostov. From the 7th century onwards, the East Slavs constituted the bulk of the population in western Russia,[56] and slowly but peacefully assimilated the native Finnic peoples.[50]
The establishment of the first East Slavic states in the 9th century coincided with the arrival of Varangians, the Vikings who ventured along the waterways extending from the eastern Baltic to the Black and Caspian Seas.[57][failed verification] According to the Primary Chronicle, a Varangian from the Rus' people, named Rurik, was elected ruler of Novgorod in 862. In 882, his successor Oleg ventured south and conquered Kiev, which had been previously paying tribute to the Khazars.[50] Rurik's son Igor and Igor's son Sviatoslav subsequently subdued all local East Slavic tribes to Kievan rule, destroyed the Khazar Khaganate,[58] and launched several military expeditions to Byzantium and Persia.[59][60]
In the 10th to 11th centuries, Kievan Rus' became one of the largest and most prosperous states in Europe. The reigns of Vladimir the Great (980–1015) and his son Yaroslav the Wise (1019–1054) constitute the Golden Age of Kiev, which saw the acceptance of Orthodox Christianity from Byzantium, and the creation of the first East Slavic written legal code, the Russkaya Pravda.[50] The age of feudalism and decentralisation had come, marked by constant in-fighting between members of the Rurik dynasty that ruled Kievan Rus' collectively. Kiev's dominance waned, to the benefit of Vladimir-Suzdal in the north-east, the Novgorod Republic in the north, and Galicia-Volhynia in the south-west.[50] By the 12th century, Kiev lost its pre-eminence and Kievan Rus' had fragmented into different principalities.[61] Prince Andrey Bogolyubsky sacked Kiev in 1169 and made Vladimir his base,[61] leading to political power being shifted to the north-east.[50]
Led by Prince Alexander Nevsky, Novgorodians repelled the invading Swedes in the Battle of the Neva in 1240,[62] as well as the Germanic crusaders in the Battle on the Ice in 1242.[63]
Kievan Rus' finally fell to the Mongol invasion of 1237–1240, which resulted in the sacking of Kiev and other cities, as well as the death of a major part of the population.[50] The invaders, later known as Tatars, formed the state of the Golden Horde, which ruled over Russia for the next two centuries.[64] Only the Novgorod Republic escaped foreign occupation after it surrendered and agreed to pay tribute to the Mongols.[50] Galicia-Volhynia would later be absorbed by Lithuania and Poland, while the Novgorod Republic continued to prosper in the north. In the northeast, the Byzantine-Slavic traditions of Kievan Rus' were adapted to form the Russian autocratic state.[50]
The destruction of Kievan Rus' saw the eventual rise of the Grand Duchy of Moscow, initially a part of Vladimir-Suzdal.[65]: 11–20  While still under the domain of the Mongol-Tatars and with their connivance, Moscow began to assert its influence in the region in the early 14th century,[66] gradually becoming the leading force in the "gathering of the Russian lands".[67] When the seat of the Metropolitan of the Russian Orthodox Church moved to Moscow in 1325, its influence increased.[68] Moscow's last rival, the Novgorod Republic, prospered as the chief fur trade centre and the easternmost port of the Hanseatic League.[69]
Led by Prince Dmitry Donskoy of Moscow, the united army of Russian principalities inflicted a milestone defeat on the Mongol-Tatars in the Battle of Kulikovo in 1380.[50] Moscow gradually absorbed its parent duchy and surrounding principalities, including formerly strong rivals such as Tver and Novgorod.[67]
Ivan III ("the Great") finally threw off the control of the Golden Horde and consolidated the whole of northern Rus' under Moscow's dominion, and was the first Russian ruler to take the title "Grand Duke of all Rus'". After the fall of Constantinople in 1453, Moscow claimed succession to the legacy of the Eastern Roman Empire. Ivan III married Sophia Palaiologina, the niece of the last Byzantine emperor Constantine XI, and made the Byzantine double-headed eagle his own, and eventually Russia's, coat-of-arms.[67] Vasili III completed the task of uniting all of Russia by annexing the last few independent Russian states in the early 16th century.[70]
In development of the Third Rome ideas, the grand duke Ivan IV ("the Terrible") was officially crowned the first tsar of Russia in 1547. The tsar promulgated a new code of laws (Sudebnik of 1550), established the first Russian feudal representative body (the Zemsky Sobor), revamped the military, curbed the influence of the clergy, and reorganised local government.[67] During his long reign, Ivan nearly doubled the already large Russian territory by annexing the three Tatar khanates: Kazan and Astrakhan along the Volga,[71] and the Khanate of Sibir in southwestern Siberia. Ultimately, by the end of the 16th century, Russia expanded east of the Ural Mountains.[72] However, the Tsardom was weakened by the long and unsuccessful Livonian War against the coalition of the Kingdom of Poland and the Grand Duchy of Lithuania (later the united Polish–Lithuanian Commonwealth), the Kingdom of Sweden, and Denmark–Norway for access to the Baltic coast and sea trade.[73] In 1572, an invading army of Crimean Tatars were thoroughly defeated in the crucial Battle of Molodi.[74]
The death of Ivan's sons marked the end of the ancient Rurik dynasty in 1598, and in combination with the disastrous famine of 1601–1603, led to a civil war, the rule of pretenders, and foreign intervention during the Time of Troubles in the early 17th century.[75] The Polish–Lithuanian Commonwealth, taking advantage, occupied parts of Russia, extending into the capital Moscow.[76] In 1612, the Poles were forced to retreat by the Russian volunteer corps, led by merchant Kuzma Minin and prince Dmitry Pozharsky.[77] The Romanov dynasty acceded to the throne in 1613 by the decision of the Zemsky Sobor, and the country started its gradual recovery from the crisis.[78]
Russia continued its territorial growth through the 17th century, which was the age of the Cossacks.[79] In 1654, the Ukrainian leader, Bohdan Khmelnytsky, offered to place Ukraine under the protection of the Russian tsar, Alexis; whose acceptance of this offer led to another Russo-Polish War. Ultimately, Ukraine was split along the Dnieper, leaving the eastern part, (Left-bank Ukraine and Kiev) under Russian rule.[80] In the east, the rapid Russian exploration and colonisation of vast Siberia continued, hunting for valuable furs and ivory. Russian explorers pushed eastward primarily along the Siberian River Routes, and by the mid-17th century, there were Russian settlements in eastern Siberia, on the Chukchi Peninsula, along the Amur River, and on the coast of the Pacific Ocean.[79] In 1648, Semyon Dezhnyov became the first European to navigate through the Bering Strait.[81]
Under Peter the Great, Russia was proclaimed an empire in 1721, and established itself as one of the European great powers. Ruling from 1682 to 1725, Peter defeated Sweden in the Great Northern War (1700–1721), securing Russia's access to the sea and sea trade. In 1703, on the Baltic Sea, Peter founded Saint Petersburg as Russia's new capital. Throughout his rule, sweeping reforms were made, which brought significant Western European cultural influences to Russia.[82] The reign of Peter I's daughter Elizabeth in 1741–1762 saw Russia's participation in the Seven Years' War (1756–1763). During the conflict, Russian troops overran East Prussia, reaching Berlin.[83] However, upon Elizabeth's death, all these conquests were returned to the Kingdom of Prussia by pro-Prussian Peter III of Russia.[84]
Catherine II ("the Great"), who ruled in 1762–1796, presided over the Russian Age of Enlightenment. She extended Russian political control over the Polish–Lithuanian Commonwealth and annexed most of its territories into Russia, making it the most populous country in Europe.[85] In the south, after the successful Russo-Turkish Wars against the Ottoman Empire, Catherine advanced Russia's boundary to the Black Sea, by dissolving the Crimean Khanate, and annexing Crimea.[86] As a result of victories over Qajar Iran through the Russo-Persian Wars, by the first half of the 19th century, Russia also conquered the Caucasus.[87] Catherine's successor, her son Paul, was unstable and focused predominantly on domestic issues.[88] Following his short reign, Catherine's strategy was continued with Alexander I's (1801–1825) wresting of Finland from the weakened Sweden in 1809,[89] and of Bessarabia from the Ottomans in 1812.[90] In North America, the Russians became the first Europeans to reach and colonise Alaska.[91] In 1803–1806, the first Russian circumnavigation was made.[92] In 1820, a Russian expedition discovered the continent of Antarctica.[93]
During the Napoleonic Wars, Russia joined alliances with various European powers, and fought against France. The French invasion of Russia at the height of Napoleon's power in 1812 reached Moscow, but eventually failed miserably as the obstinate resistance in combination with the bitterly cold Russian winter led to a disastrous defeat of invaders, in which the pan-European Grande Armée faced utter destruction. Led by Mikhail Kutuzov and Michael Andreas Barclay de Tolly, the Imperial Russian Army ousted Napoleon and drove throughout Europe in the War of the Sixth Coalition, ultimately entering Paris.[94] Alexander I controlled Russia's delegation at the Congress of Vienna, which defined the map of post-Napoleonic Europe.[95]
The officers who pursued Napoleon into Western Europe brought ideas of liberalism back to Russia, and attempted to curtail the tsar's powers during the abortive Decembrist revolt of 1825.[96] At the end of the conservative reign of Nicholas I (1825–1855), a zenith period of Russia's power and influence in Europe, was disrupted by defeat in the Crimean War.[97]
Nicholas's successor Alexander II (1855–1881) enacted significant changes throughout the country, including the emancipation reform of 1861.[98] These reforms spurred industrialisation, and modernised the Imperial Russian Army, which liberated much of the Balkans from Ottoman rule in the aftermath of the 1877–1878 Russo-Turkish War.[99] During most of the 19th and early 20th century, Russia and Britain colluded over Afghanistan and its neighbouring territories in Central and South Asia; the rivalry between the two major European empires came to be known as the Great Game.[100]
The late 19th century saw the rise of various socialist movements in Russia. Alexander II was assassinated in 1881 by revolutionary terrorists.[101] The reign of his son Alexander III (1881–1894) was less liberal but more peaceful.[102]
Under last Russian emperor, Nicholas II (1894–1917), the Revolution of 1905 was triggered by the failure of the humiliating Russo-Japanese War.[103]  The uprising was put down, but the government was forced to concede major reforms (Russian Constitution of 1906), including granting freedoms of speech and assembly, the legalisation of political parties, and the creation of an elected legislative body, the State Duma.[104]
In 1914, Russia entered World War I in response to Austria-Hungary's declaration of war on Russia's ally Serbia,[105] and fought across multiple fronts while isolated from its Triple Entente allies.[106] In 1916, the Brusilov Offensive of the Imperial Russian Army almost completely destroyed the Austro-Hungarian Army.[107] However, the already-existing public distrust of the regime was deepened by the rising costs of war, high casualties, and rumors of corruption and treason. All this formed the climate for the Russian Revolution of 1917, carried out in two major acts.[108] In early 1917, Nicholas II was forced to abdicate; he and his family were imprisoned and later executed in Yekaterinburg during the Russian Civil War.[109] The monarchy was replaced by a shaky coalition of political parties that declared itself the Provisional Government.[110] The Provisional Government proclaimed the Russian Republic in September. On 19 January [O.S. 6 January], 1918, the Russian Constituent Assembly declared Russia a democratic federal republic (thus ratifying the Provisional Government's decision). The next day the Constituent Assembly was dissolved by the All-Russian Central Executive Committee.[108]
An alternative socialist establishment co-existed, the Petrograd Soviet, wielding power through the democratically elected councils of workers and peasants, called soviets. The rule of the new authorities only aggravated the crisis in the country instead of resolving it, and eventually, the October Revolution, led by Bolshevik leader Vladimir Lenin, overthrew the Provisional Government and gave full governing power to the soviets, leading to the creation of the world's first socialist state.[108] The Russian Civil War broke out between the anti-communist White movement and the Bolsheviks with its Red Army.[111] In the aftermath of signing the Treaty of Brest-Litovsk that concluded hostilities with the Central Powers of World War I; Bolshevist Russia surrendered most of its western territories, which hosted 34% of its population, 54% of its industries, 32% of its agricultural land, and roughly 90% of its coal mines.[112]
The Allied powers launched an unsuccessful military intervention in support of anti-communist forces.[113] In the meantime, both the Bolsheviks and White movement carried out campaigns of deportations and executions against each other, known respectively as the Red Terror and White Terror.[114] By the end of the violent civil war, Russia's economy and infrastructure were heavily damaged, and as many as 10 million perished during the war, mostly civilians.[115] Millions became White émigrés,[116] and the Russian famine of 1921–1922 claimed up to five million victims.[117]
On 30 December 1922, Lenin and his aides formed the Soviet Union, by joining the Russian SFSR into a single state with the Byelorussian, Transcaucasian, and Ukrainian republics.[118] Eventually internal border changes and annexations during World War II created a union of 15 republics; the largest in size and population being the Russian SFSR, which dominated the union for its entire history politically, culturally, and economically.[119][failed verification]
Following Lenin's death in 1924, a troika was designated to take charge. Eventually Joseph Stalin, the General Secretary of the Communist Party, managed to suppress all opposition factions and consolidate power in his hands to become the country's dictator by the 1930s.[120] Leon Trotsky, the main proponent of world revolution, was exiled from the Soviet Union in 1929,[121] and Stalin's idea of Socialism in One Country became the official line.[122] The continued internal struggle in the Bolshevik party culminated in the Great Purge.[123]
Under Stalin's leadership, the government launched a command economy, industrialisation of the largely rural country, and collectivisation of its agriculture. During this period of rapid economic and social change, millions of people were sent to penal labour camps, including many political convicts for their suspected or real opposition to Stalin's rule;[124] and millions were deported and exiled to remote areas of the Soviet Union.[125] The transitional disorganisation of the country's agriculture, combined with the harsh state policies and a drought,[126] led to the Soviet famine of 1932–1933; which killed up to 8.7 million, 3.3 million of them in the Russian SFSR.[127] The Soviet Union, ultimately, made the costly transformation from a largely agrarian economy to a major industrial powerhouse within a short span of time.[128]
The Soviet Union entered World War II on 17 September 1939 with its invasion of Poland,[129] in accordance with a secret protocol within the Molotov–Ribbentrop Pact with Nazi Germany.[130] The Soviet Union later invaded Finland,[131] and occupied and annexed the Baltic states,[132] as well as parts of Romania.[133]: 91–95  On 22 June 1941, Germany invaded the Soviet Union,[134] opening the Eastern Front, the largest theater of World War II.[135]: 7 
Eventually, some 5 million Red Army troops were captured by the Nazis;[136]: 272  the latter deliberately starved to death or otherwise killed 3.3 million Soviet POWs, and a vast number of civilians, as the "Hunger Plan" sought to fulfil Generalplan Ost.[137]: 175–186  Although the Wehrmacht had considerable early success, their attack was halted in the Battle of Moscow.[138] Subsequently, the Germans were dealt major defeats first at the Battle of Stalingrad in the winter of 1942–1943,[139] and then in the Battle of Kursk in the summer of 1943.[140] Another German failure was the Siege of Leningrad, in which the city was fully blockaded on land between 1941 and 1944 by German and Finnish forces, and suffered starvation and more than a million deaths, but never surrendered.[141] Soviet forces steamrolled through Eastern and Central Europe in 1944–1945 and captured Berlin in May 1945.[142] In August 1945, the Red Army invaded Manchuria and ousted the Japanese from Northeast Asia, contributing to the Allied victory over Japan.[143]
The 1941–1945 period of World War II is known in Russia as the Great Patriotic War.[144] The Soviet Union, along with the United States, the United Kingdom and China were considered the Big Four of Allied powers in World War II, and later became the Four Policemen, which was the foundation of the United Nations Security Council.[145]: 27  During the war, Soviet civilian and military death were about 26–27 million,[146] accounting for about half of all World War II casualties.[147]: 295  The Soviet economy and infrastructure suffered massive devastation, which caused the Soviet famine of 1946–1947.[148] However, at the expense of a large sacrifice, the Soviet Union emerged as a global superpower.[149]
After World War II, parts of Eastern and Central Europe, including East Germany and eastern parts of Austria were occupied by Red Army according to the Potsdam Conference.[150]  Dependent communist governments were installed in the Eastern Bloc satellite states.[151] After becoming the world's second nuclear power,[152] the Soviet Union established the Warsaw Pact alliance,[153] and entered into a struggle for global dominance, known as the Cold War, with the rivalling United States and NATO.[154]
After Stalin's death in 1953 and a short period of collective rule, the new leader Nikita Khrushchev denounced Stalin and launched the policy of de-Stalinization, releasing many political prisoners from the Gulag labour camps.[155] The general easement of repressive policies became known later as the Khrushchev Thaw.[156] At the same time, Cold War tensions reached its peak when the two rivals clashed over the deployment of the United States Jupiter missiles in Turkey and Soviet missiles in Cuba.[157]
In 1957, the Soviet Union launched the world's first artificial satellite, Sputnik 1, thus starting the Space Age.[158] Russian cosmonaut Yuri Gagarin became the first human to orbit the Earth, aboard the Vostok 1 crewed spacecraft on 12 April 1961.[159]
Following the ousting of Khrushchev in 1964, another period of collective rule ensued, until Leonid Brezhnev became the leader. The era of the 1970s and the early 1980s was later designated as the Era of Stagnation. The 1965 Kosygin reform aimed for partial decentralisation of the Soviet economy.[160] In 1979, after a communist-led revolution in Afghanistan, Soviet forces invaded the country, ultimately starting the Soviet–Afghan War.[161] In May 1988, the Soviets started to withdraw from Afghanistan, due to international opposition, persistent anti-Soviet guerrilla warfare, and a lack of support by Soviet citizens.[162]
From 1985 onwards, the last Soviet leader Mikhail Gorbachev, who sought to enact liberal reforms in the Soviet system, introduced the policies of glasnost (openness) and perestroika (restructuring) in an attempt to end the period of economic stagnation and to democratise the government.[163] This, however, led to the rise of strong nationalist and separatist movements across the country.[164] Prior to 1991, the Soviet economy was the world's second-largest, but during its final years, it went into a crisis.[165]
By 1991, economic and political turmoil began to boil over as the Baltic states chose to secede from the Soviet Union.[166] On 17 March, a referendum was held, in which the vast majority of participating citizens voted in favour of changing the Soviet Union into a renewed federation.[167] In June 1991, Boris Yeltsin became the first directly elected president in Russian history when he was elected president of the Russian SFSR.[168] In August 1991, a coup d'état attempt by members of Gorbachev's government, directed against Gorbachev and aimed at preserving the Soviet Union, instead led to the end of the Communist Party of the Soviet Union.[169] On 25 December 1991, following the dissolution of the Soviet Union, along with contemporary Russia, fourteen other post-Soviet states emerged.[170]
The economic and political collapse of the Soviet Union led Russia into a deep and prolonged depression. During and after the disintegration of the Soviet Union, wide-ranging reforms including privatisation and market and trade liberalisation were undertaken, including radical changes along the lines of "shock therapy".[171] The privatisation largely shifted control of enterprises from state agencies to individuals with inside connections in the government, which led to the rise of the infamous Russian oligarchs.[172] Many of the newly rich moved billions in cash and assets outside of the country in an enormous capital flight.[173] The depression of the economy led to the collapse of social services—the birth rate plummeted while the death rate skyrocketed,[174][175] and millions plunged into poverty;[176] while extreme corruption,[177] as well as criminal gangs and organised crime rose significantly.[178]
In late 1993, tensions between Yeltsin and the Russian parliament culminated in a constitutional crisis which ended violently through military force. During the crisis, Yeltsin was backed by Western governments, and over 100 people were killed.[179]
In December, a referendum was held and approved, which introduced a new constitution, giving the president enormous powers.[180] The 1990s were plagued by armed conflicts in the North Caucasus, both local ethnic skirmishes and separatist Islamist insurrections.[181] From the time Chechen separatists declared independence in the early 1990s, an intermittent guerrilla war was fought between the rebel groups and Russian forces.[182] Terrorist attacks against civilians were carried out by Chechen separatists, claiming the lives of thousands of Russian civilians.[e][183]
After the dissolution of the Soviet Union, Russia assumed responsibility for settling the latter's external debts.[184] In 1992, most consumer price controls were eliminated, causing extreme inflation and significantly devaluing the rouble.[185] High budget deficits coupled with increasing capital flight and inability to pay back debts, caused the 1998 Russian financial crisis, which resulted in a further GDP decline.[186]
On 31 December 1999, president Yeltsin unexpectedly resigned,[187] handing the post to the recently appointed prime minister and his chosen successor, Vladimir Putin.[188] Putin then won the 2000 presidential election,[189] and defeated the Chechen insurgency in the Second Chechen War.[190]
Putin won a second presidential term in 2004.[191] High oil prices and a rise in foreign investment saw the Russian economy and living standards improve significantly.[192] Putin's rule increased stability, while transforming Russia into an authoritarian state.[193] In 2008, Putin took the post of prime minister, while Dmitry Medvedev was elected president for one term, to hold onto power despite legal term limits;[194] this period has been described as a "tandemocracy".[195]
Following a diplomatic crisis with neighbouring Georgia, the Russo-Georgian War took place during 1–12 August 2008, resulting in Russia recognising two separatist states in the territories that it occupies in Georgia.[196] It was the first European war of the 21st century.[197]
In early 2014, following a revolution in Ukraine, Russia occupied and annexed Crimea from neighbouring Ukraine following a disputed referendum,[198] with Russian troops later participating in a war in eastern Ukraine between Russian-backed separatists and Ukrainian troops.[199] In a major escalation of the conflict, Russia launched a full-scale invasion of Ukraine on 24 February 2022.[200] The invasion marked the largest conventional war in Europe since World War II,[201] and was met with international condemnation,[202] as well as expanded sanctions against Russia.[203] As a result, Russia was expelled from the Council of Europe in March,[204] and was suspended from the United Nations Human Rights Council in April.[205] In September, following successful Ukrainian counteroffensives,[206] Putin announced a "partial mobilisation", Russia's first mobilisation since World War II.[207] By the end of September, Putin proclaimed the annexation of four Ukrainian regions, the largest annexation in Europe since World War II.[208] Putin and Russian-installed leaders signed treaties of accession, internationally unrecognized and widely denounced as illegal, despite the fact that Russian forces have been unable to fully occupy any of the four regions.[208] A number of supranational and national parliaments passed resolutions declaring Russia to be a state sponsor of terrorism.[209] In addition, Russia was declared a terrorist state by Latvia, Lithuania and Estonia.[210] Tens of thousands are estimated to have been killed as a result of the invasion.[211][212]
Russia's vast landmass stretches over the easternmost part of Europe and the northernmost part of Asia.[213] It spans the northernmost edge of Eurasia; and has the world's fourth-longest coastline, of over 37,653 km (23,396 mi).[f][215] Russia lies between latitudes 41° and 82° N, and longitudes 19° E and 169° W, extending some 9,000 km (5,600 mi) east to west, and 2,500 to 4,000 km (1,600 to 2,500 mi) north to south.[216] Russia, by landmass, is larger than three continents,[g] and has the same surface area as Pluto.[217]
Russia has nine major mountain ranges, and they are found along the southernmost regions, which share a significant portion of the Caucasus Mountains (containing Mount Elbrus, which at 5,642 m (18,510 ft) is the highest peak in Russia and Europe);[9] the Altai and Sayan Mountains in Siberia; and in the East Siberian Mountains and the Kamchatka Peninsula in the Russian Far East (containing Klyuchevskaya Sopka, which at 4,750 m (15,584 ft) is the highest active volcano in Eurasia).[218][219] The Ural Mountains, running north to south through the country's west, are rich in mineral resources, and form the traditional boundary between Europe and Asia.[220] The lowest point in Russia and Europe, is situated at the head of the Caspian Sea, where the Caspian Depression reaches some 29 metres (95.1 ft) below sea level.[221]
Russia, as one of the world's only three countries bordering three oceans,[213] has links with a great number of seas.[h][222] Its major islands and archipelagos include Novaya Zemlya, Franz Josef Land, Severnaya Zemlya, the New Siberian Islands, Wrangel Island, the Kuril Islands (four of which are disputed with Japan), and Sakhalin.[223][224] The Diomede Islands, administered by Russia and the United States, are just 3.8 km (2.4 mi) apart;[225] and Kunashir Island of the Kuril Islands is merely 20 km (12.4 mi) from Hokkaido, Japan.[2]
Russia, home of over 100,000 rivers,[213] has one of the world's largest surface water resources, with its lakes containing approximately one-quarter of the world's liquid fresh water.[219] Lake Baikal, the largest and most prominent among Russia's fresh water bodies, is the world's deepest, purest, oldest and most capacious fresh water lake, containing over one-fifth of the world's fresh surface water.[226] Ladoga and Onega in northwestern Russia are two of the largest lakes in Europe.[213] Russia is second only to Brazil by total renewable water resources.[227] The Volga in western Russia, widely regarded as Russia's national river, is the longest river in Europe; and forms the Volga Delta, the largest river delta in the continent.[228] The Siberian rivers of Ob, Yenisey, Lena, and Amur are among the world's longest rivers.[229]
The size of Russia and the remoteness of many of its areas from the sea result in the dominance of the humid continental climate throughout most of the country, except for the tundra and the extreme southwest. Mountain ranges in the south and east obstruct the flow of warm air masses from the Indian and Pacific oceans, while the European Plain spanning its west and north opens it to influence from the Atlantic and Arctic oceans.[230] Most of northwest Russia and Siberia have a subarctic climate, with extremely severe winters in the inner regions of northeast Siberia (mostly Sakha, where the Northern Pole of Cold is located with the record low temperature of −71.2 °C or −96.2 °F),[223] and more moderate winters elsewhere. Russia's vast coastline along the Arctic Ocean and the Russian Arctic islands have a polar climate.[230]
The coastal part of Krasnodar Krai on the Black Sea, most notably Sochi, and some coastal and interior strips of the North Caucasus possess a humid subtropical climate with mild and wet winters.[230]  In many regions of East Siberia and the Russian Far East, winter is dry compared to summer; while other parts of the country experience more even precipitation across seasons. Winter precipitation in most parts of the country usually falls as snow. The westernmost parts of Kaliningrad Oblast and some parts in the south of Krasnodar Krai and the North Caucasus have an oceanic climate.[230] The region along the Lower Volga and Caspian Sea coast, as well as some southernmost slivers of Siberia, possess a semi-arid climate.[231]
Throughout much of the territory, there are only two distinct seasons, winter and summer; as spring and autumn are usually brief periods of change between extremely low and extremely high temperatures.[230] The coldest month is January (February on the coastline); the warmest is usually July. Great ranges of temperature are typical. In winter, temperatures get colder both from south to north and from west to east. Summers can be quite hot, even in Siberia.[232] Climate change in Russia is causing more frequent wildfires,[233] and thawing the country's large expanse of permafrost.[234]
Russia, owing to its gigantic size, has diverse ecosystems, including polar deserts, tundra, forest tundra, taiga, mixed and broadleaf forest, forest steppe, steppe, semi-desert, and subtropics.[235] About half of Russia's territory is forested,[9] and it has the world's largest area of forest,[236] which sequester some of the world's highest amounts of carbon dioxide.[236][237]
Russian biodiversity includes 12,500 species of vascular plants, 2,200 species of bryophytes, about 3,000 species of lichens, 7,000–9,000 species of algae, and 20,000–25,000 species of fungi. Russian fauna is composed of 320 species of mammals, over 732 species of birds, 75 species of reptiles, about 30 species of amphibians, 343 species of freshwater fish (high endemism), approximately 1,500 species of saltwater fishes, 9 species of cyclostomata, and approximately 100–150,000 invertebrates (high endemism).[235][238] Approximately 1,100 rare and endangered plant and animal species are included in the Russian Red Data Book.[235]
Russia's entirely natural ecosystems are conserved in nearly 15,000 specially protected natural territories of various statuses, occupying more than 10% of the country's total area.[235] They include 45 biosphere reserves,[239] 64 national parks, and 101 nature reserves.[240] Although in decline, the country still has many ecosystems which are still considered intact forest; mainly in the northern taiga areas, and the subarctic tundra of Siberia.[241] Russia had a Forest Landscape Integrity Index mean score of 9.02 in 2019, ranking 10th out of 172 countries; and the first ranked major nation globally.[242]
Russia, by 1993 constitution, is a symmetric federal republic with a semi-presidential system, wherein the president is the head of state,[243] and the prime minister is the head of government.[9] It is structured as a multi-party representative democracy, with the federal government composed of three branches:[244]
The president is elected by popular vote for a six-year term and may be elected no more than twice.[248][i] Ministries of the government are composed of the premier and his deputies, ministers, and selected other individuals; all are appointed by the president on the recommendation of the prime minister (whereas the appointment of the latter requires the consent of the State Duma). United Russia is the dominant political party in Russia, and has been described as "big tent" and the "party of power".[250][251] Under the administrations of Vladimir Putin, Russia has experienced democratic backsliding,[252][253] and has become an authoritarian state[10] under a dictatorship,[7][254] with Putin's policies being referred to as Putinism.[255]
Russia, by 1993 constitution, is a symmetric (with the possibility of an asymmetric configuration) federation. Unlike the Soviet asymmetric model of the RSFSR, where only republics were "subjects of the federation", the current constitution raised the status of other regions to the level of republics and made all regions equal with the title "subject of the federation". In 2000, it was further confirmed by the decision of the constitutional court that all regions have only one identical status, which is called "subject of the federation", and the naming of some regions as "republic (state)" does not mean recognition as a state in the modern international law meaning of this term and does not mean recognition of the sovereignty of the region. Only the multiethnic people of Russia and the Russian Federation have sovereignty. The regions of Russia have reerved areas of competence, but no regions have sovereignty, do not have the status of a sovereign state, do not have the right to indicate any sovereignty in their constitutions and do not have the right to secede from the country. The laws of the regions cannot contradict federal laws.[256]
The 1993 Russian constitution recognized the possibility of an asymmetric configuration of intergovernmental relations between the regions and
the federal government.[257] Article 11(3) of the Constitution allows the delimitation of jurisdictions and powers between the federal authorities and the region through the conclusion of an agreement. By 1998, such agreements had been concluded with 46 subjects of the federation, including the federal city of Moscow. The most notable asymetric relations and debates were with Tatarstan.[258]
According to the constitution, the Russian Federation is composed of 89 federal subjects.[j] In 1993, when the new constitution was adopted, there were 89 federal subjects listed, but some were later merged. The federal subjects have equal representation—two delegates each—in the Federation Council, the upper house of the Federal Assembly.[259] They do, however, differ in the degree of autonomy they enjoy.[260] The federal districts of Russia were established by Putin in 2000 to facilitate central government control of the federal subjects.[261] Originally seven, currently there are eight federal districts, each headed by an envoy appointed by the president.[262]
Russia had the world's fifth-largest diplomatic network in 2019. It maintains diplomatic relations with 190 United Nations member states, four partially-recognised states, and three United Nations observer states; along with 144 embassies.[269] Russia is one of the five permanent members of the United Nations Security Council. It has historically been a great power,[270] and a former superpower as the leading constituent of the former Soviet Union.[149] Russia is a member of the G20, the OSCE, and the APEC. Russia also takes a leading role in organisations such as the CIS,[271] the EAEU,[272] the CSTO,[273] the SCO,[274] and BRICS.[275]
Russia maintains close relations with neighbouring Belarus, which is a part of the Union State, a supranational confederation of the two states.[276] Serbia has been a historically close ally of Russia, as both countries share a strong mutual cultural, ethnic, and religious affinity.[277] India is the largest customer of Russian military equipment, and the two countries share a strong strategic and diplomatic relationship since the Soviet era.[278] Russia wields influence across the geopolitically important South Caucasus and Central Asia; and the two regions have been described as Russia's "backyard".[279][280]
In the 21st century Russia has pursued an aggressive foreign policy aimed at securing regional dominance and international influence, as well as increasing domestic support for the government. Military intervention in the post-soviet states include a war with Georgia in 2008, and the invasion and destablisation of Ukraine beginning in 2014. Russia has also sought to increase its influence in the Middle East, most significantly through military intervention in the Syrian civil war. Cyberwarfare and airspace violations, along with electoral interference, have been used to increase perceptions of Russian power.[281] Russia's relations with neighbouring Ukraine and the Western world—especially the United States, the European Union, the United Nations and NATO—have collapsed; especially following the start of the Russo-Ukrainian War in 2014 and the consequent escalation in 2022.[282][283] Relations between Russia and China have significantly strengthened bilaterally and economically; due to shared political interests.[284] Turkey and Russia share a complex strategic, energy, and defence relationship.[285] Russia maintains cordial relations with Iran, as it is a strategic and economic ally.[286] Russia has also increasingly pushed to expand its influence across the Arctic,[287] Asia-Pacific,[288] Africa,[289] the Middle East,[290] and Latin America.[291]
The Russian Armed Forces are divided into the Ground Forces, the Navy, and the Aerospace Forces—and there are also two independent arms of service: the Strategic Missile Troops and the Airborne Troops.[9] As of 2021[update], the military have around a million active-duty personnel, which is the world's fifth-largest, and about 2–20 million reserve personnel.[293][294] It is mandatory for all male citizens aged 18–27 to be drafted for a year of service in the Armed Forces.[9]
Russia is among the five recognised nuclear-weapons states, with the world's largest stockpile of nuclear weapons; over half of the world's nuclear weapons are owned by Russia.[295] Russia possesses the second-largest fleet of ballistic missile submarines,[296] and is one of the only three countries operating strategic bombers.[297] Russia maintains the world's third-highest military expenditure, spending $86.4 billion in 2022, corresponding to around 4.1% of its GDP.[298] In 2021 it was the world's second-largest arms exporter, and had a large and entirely indigenous defence industry, producing most of its own military equipment.[299]
Violations of human rights in Russia have been increasingly criticised by leading democracy and human rights groups. In particular, Amnesty International and Human Rights Watch say that Russia is not democratic and allows few political rights and civil liberties to its citizens.[301][302]
Since 2004, Freedom House has ranked Russia as "not free" in its Freedom in the World survey.[303] Since 2011, the Economist Intelligence Unit has ranked Russia as an "authoritarian regime" in its Democracy Index, ranking it 146th out of 167 countries in 2022.[304] In regards to media freedom, Russia was ranked 155th out of 180 countries in Reporters Without Borders' Press Freedom Index for 2022.[305] The Russian government has been widely criticised by political dissidents and human rights activists for unfair elections,[306] crackdowns on opposition political parties and protests,[307][308] persecution of non-governmental organisations and enforced suppression and killings of independent journalists,[309][310][311] and censorship of mass media and internet.[312]
Russia's autocratic[313] political system has been variously described as a kleptocracy,[314] an oligarchy,[315] and a plutocracy.[316] It was the lowest rated European country in Transparency International's Corruption Perceptions Index for 2021, ranking 136th out of 180 countries.[317] Russia has a long history of corruption, which is seen as a significant problem.[318] It impacts various sectors, including the economy,[319] business,[320] public administration,[321] law enforcement,[322] healthcare,[323][324] education,[325] and the military.[326]
Muslims, especially Salafis, have faced persecution in Russia.[327][328] To quash the insurgency in the North Caucasus, Russian authorities have been accused of indiscriminate killings,[329] arrests, forced disappearances, and torture of civilians.[330][331] In Dagestan, some Salafis along with facing government harassment based on their appearance, have had their homes blown up in counterinsurgency operations.[332][333] Chechens and Ingush in Russian prisons reportedly take more abuse than other ethnic groups.[334] During the 2022 invasion of Ukraine, Russia has set up filtration camps where many Ukrainians are subjected to abuses and forcibly sent to Russia; the camps have been compared to those used in the Chechen Wars.[335][336]
The primary and fundamental statement of laws in Russia is the Constitution of the Russian Federation. Statutes, like the Russian Civil Code and the Russian Criminal Code, are the predominant legal sources of Russian law.[337][338][339]
Russia has the world's second largest illegal arms trade market, after the United States, is ranked first in Europe and 32nd globally in the Global Organized Crime Index, and is among the countries with the highest number of people in prison.[340][341][342]
Russia has a market economy, with enormous natural resources, particularly oil and natural gas.[344] It has the world's ninth-largest economy by nominal GDP and the sixth-largest by PPP. The large service sector accounts for 62% of total GDP, followed by the industrial sector (32%), while the agricultural sector is the smallest, making up only 5% of total GDP.[9] Russia has a low official unemployment rate of 4.1%.[345] Its foreign exchange reserves are the world's fifth-largest, worth $540 billion.[346] It has a labour force of roughly 70 million, which is the world's sixth-largest.[347]
Russia is the world's thirteenth-largest exporter and the 21st-largest importer.[348][349] It relies heavily on revenues from oil and gas-related taxes and export tariffs, which accounted for 45% of Russia's federal budget revenues in January 2022,[350] and up to 60% of its exports in 2019.[351] Russia has one of the lowest levels of external debt among major economies,[352] although its inequality of household income and wealth is one of the highest among developed countries.[353] High regional disparity is also an issue.[354][355]
After over a decade of post-Soviet rapid economic growth, backed by high oil-prices and a surge in foreign exchange reserves and investment,[192] Russia's economy was damaged following the start of the Russo-Ukrainian War and the annexation of Crimea in 2014, due to the first wave of Western sanctions being imposed.[356] In the aftermath of the Russian invasion of Ukraine in 2022, the country has faced revamped sanctions and corporate boycotts,[357] becoming the most sanctioned country in the world,[358] in a move described as an "all-out economic and financial war" to isolate the Russian economy from the Western financial system.[203] Due to the impact, the Russian government has stopped publishing a raft of economic data since April 2022.[359] Economists suggest the sanctions will have a long-term effect over the Russian economy.[360]
Railway transport in Russia is mostly under the control of the state-run Russian Railways. The total length of common-used railway tracks is the world's third-longest, and exceeds 87,000 km (54,100 mi).[362] As of 2016[update], Russia has the world's fifth-largest road network, with 1.5 million km of roads,[363] while its road density is among the world's lowest.[364] Russia's inland waterways are the world's longest, and total 102,000 km (63,380 mi).[365] Among Russia's 1,218 airports,[366] the busiest is Sheremetyevo International Airport in Moscow. Russia's largest port is the Port of Novorossiysk in Krasnodar Krai along the Black Sea.[367]
Russia was widely described as an energy superpower.[368] It has the world's largest proven gas reserves,[369] the second-largest coal reserves,[370] the eighth-largest oil reserves,[371] and the largest oil shale reserves in Europe.[372] Russia is also the world's leading natural gas exporter,[373] the second-largest natural gas producer,[374] and the second-largest oil producer and exporter.[375][376] Russia's oil and gas production led to deep economic relationships with the European Union, China, and former Soviet and Eastern Bloc states.[377][378] For example, over the last decade, Russia's share of supplies to total European Union (including the United Kingdom) gas demand increased from 25% in 2009 to 32% in the weeks before the Russian invasion of Ukraine in February 2022.[378]
It is not trivial to estimate the influence of oil and gas in the Russian economy. In the mid-2000s, the share of the oil and gas sector in GDP was around 20%, and in 2013 it was 20–21% of GDP.[379] The share of oil and gas in Russia's exports (about 50%) and federal budget revenues (about 50%) is large, and the dynamics of Russia's GDP are highly dependent on oil and gas prices,[380] but the share in GDP is much less than 50%. According to the first such comprehensive assessment published by the Russian statistics agency Rosstat in 2021, the maximum total share of the oil and gas sector in Russia's GDP, including extraction, refining, transport, sale of oil and gas, all goods and services used, and all supporting activities, amounts to 19.2% in 2019 and 15.2% in 2020. This is comparable to the share of GDP in Norway and Kazakhstan. It is much lower than the share of GDP in Saudi Arabia and the United Arab Emirates.[381][382][383][384][385]
Russia ratified the Paris Agreement in 2019.[386] Greenhouse gas emissions by Russia are the world's fourth-largest.[387] Russia is the world's fourth-largest electricity producer.[388] It was also the world's first country to develop civilian nuclear power, and to construct the world's first nuclear power plant.[389] Russia was also the world's fourth-largest nuclear energy producer in 2019,[390] and was the fifth-largest hydroelectric producer in 2021.[391]
Russia's agriculture sector contributes about 5% of the country's total GDP, although the sector employs about one-eighth of the total labour force.[392] It has the world's third-largest cultivated area, at 1,265,267 square kilometres (488,522 sq mi). However, due to the harshness of its environment, about 13.1% of its land is agricultural,[9] and only 7.4% of its land is arable.[393] The country's agricultural land is considered part of the "breadbasket" of Europe.[394] More than one-third of the sown area is devoted to fodder crops, and the remaining farmland is devoted to industrial crops, vegetables, and fruits.[392] The main product of Russian farming has always been grain, which occupies considerably more than half of the cropland.[392] Russia is the world's largest exporter of wheat,[395][396] the largest producer of barley and buckwheat, among the largest exporters of maize and sunflower oil, and the leading producer of fertilizer.[397]
Various analysts of climate change adaptation foresee large opportunities for Russian agriculture during the rest of the 21st century as arability increases in Siberia, which would lead to both internal and external migration to the region.[398] Owing to its large coastline along three oceans and twelve marginal seas, Russia maintains the world's sixth-largest fishing industry; capturing nearly 5 million tons of fish in 2018.[399] It is home to the world's finest caviar, the beluga; and produces about one-third of all canned fish, and some one-fourth of the world's total fresh and frozen fish.[392]
Russia spent about 1% of its GDP on research and development in 2019, with the world's tenth-highest budget.[400] It also ranked tenth worldwide in the number of scientific publications in 2020, with roughly 1.3 million papers.[401] Since 1904, Nobel Prize were awarded to 26 Soviets and Russians in physics, chemistry, medicine, economy, literature and peace.[402] Russia ranked 45th in the Global Innovation Index in 2021.[403]
Mikhail Lomonosov proposed the conservation of mass in chemical reactions, discovered the atmosphere of Venus, and founded modern geology.[404] Since the times of Nikolay Lobachevsky, who pioneered the non-Euclidean geometry, and Pafnuty Chebyshev, a prominent tutor; Russian mathematicians became among the world's most influential.[405] Dmitry Mendeleev invented the Periodic table, the main framework of modern chemistry.[406] Sofya Kovalevskaya was a pioneer among women in mathematics in the 19th century.[407] Nine Soviet and Russian mathematicians have been awarded with the Fields Medal. Grigori Perelman was offered the first ever Clay Millennium Prize Problems Award for his final proof of the Poincaré conjecture in 2002, as well as the Fields Medal in 2006.[408]
Alexander Popov was among the inventors of radio,[409] while Nikolai Basov and Alexander Prokhorov were co-inventors of laser and maser.[410] Zhores Alferov contributed significantly to the creation of modern heterostructure physics and electronics.[411] Oleg Losev made crucial contributions in the field of semiconductor junctions, and discovered light-emitting diodes.[412] Vladimir Vernadsky is considered one of the founders of geochemistry, biogeochemistry, and radiogeology.[413] Élie Metchnikoff is known for his groundbreaking research in immunology.[414] Ivan Pavlov is known chiefly for his work in classical conditioning.[415] Lev Landau made fundamental contributions to many areas of theoretical physics.[416]
Nikolai Vavilov was best known for having identified the centres of origin of cultivated plants.[417] Trofim Lysenko was known mainly for Lysenkoism.[418] Many famous Russian scientists and inventors were émigrés. Igor Sikorsky was an aviation pioneer.[419] Vladimir Zworykin was the inventor of the iconoscope and kinescope television systems.[420] Theodosius Dobzhansky was the central figure in the field of evolutionary biology for his work in shaping the modern synthesis.[421] George Gamow was one of the foremost advocates of the Big Bang theory.[422] Many foreign scientists lived and worked in Russia for a long period, such as Leonard Euler and Alfred Nobel.[423][424]
Roscosmos is Russia's national space agency. The country's achievements in the field of space technology and space exploration can be traced back to Konstantin Tsiolkovsky, the father of theoretical astronautics, whose works had inspired leading Soviet rocket engineers, such as Sergey Korolyov, Valentin Glushko, and many others who contributed to the success of the Soviet space program in the early stages of the Space Race and beyond.[426]: 6–7, 333 
In 1957, the first Earth-orbiting artificial satellite, Sputnik 1, was launched. In 1961, the first human trip into space was successfully made by Yuri Gagarin. Many other Soviet and Russian space exploration records ensued. In 1963, Valentina Tereshkova became the first and youngest woman in space, having flown a solo mission on Vostok 6.[427] In 1965, Alexei Leonov became the first human to conduct a spacewalk, exiting the space capsule during Voskhod 2.[428]
In 1957, Laika, a Soviet space dog, became the first animal to orbit the Earth, aboard Sputnik 2.[429] In 1966, Luna 9 became the first spacecraft to achieve a survivable landing on a celestial body, the Moon.[430] In 1968, Zond 5 brought the first Earthlings (two tortoises and other life forms) to circumnavigate the Moon.[431] In 1970, Venera 7 became the first spacecraft to land on another planet, Venus.[432] In 1971, Mars 3 became the first spacecraft to land on Mars.[433]: 34–60  During the same period, Lunokhod 1 became the first space exploration rover,[434] while Salyut 1 became the world's first space station.[435] Russia had 172 active satellites in space in April 2022, the world's third-highest.[436]
According to the World Tourism Organization, Russia was the sixteenth-most visited country in the world, and the tenth-most visited country in Europe, in 2018, with over 24.6 million visits.[437] According to Federal Agency for Tourism, the number of inbound trips of foreign citizens to Russia amounted to 24.4 million in 2019.[438] Russia's international tourism receipts in 2018 amounted to $11.6 billion.[437] In 2019, travel and tourism accounted for about 4.8% of country's total GDP.[439]
Major tourist routes in Russia include a journey around the Golden Ring of Russia, a theme route of ancient Russian cities, cruises on large rivers such as the Volga, hikes on mountain ranges such as the Caucasus Mountains,[440] and journeys on the famous Trans-Siberian Railway.[441] Russia's most visited and popular landmarks include Red Square, the Peterhof Palace, the Kazan Kremlin, the Trinity Lavra of St. Sergius and Lake Baikal.[442]
Moscow, the nation's cosmopolitan capital and historic core, is a bustling megacity. It retains its classical and Soviet-era architecture; while boasting high art, world class ballet, and modern skyscrapers.[443] Saint Petersburg, the Imperial capital, is famous for its classical architecture, cathedrals, museums and theatres, white nights, criss-crossing rivers and numerous canals.[444] Russia is famed worldwide for its rich museums, such as the State Russian, the State Hermitage, and the Tretyakov Gallery; and for theatres such as the Bolshoi and the Mariinsky. The Moscow Kremlin and the Saint Basil's Cathedral are among the cultural landmarks of Russia.[445]
Russia is one of the world's most sparsely populated and urbanised countries,[9] with the vast majority of its population concentrated within its western part.[446] It had a population of 142.8 million according to the 2010 census,[447] which rose to roughly 145.5 million as of 2022.[14][clarification needed] Russia is the most populous country in Europe, and the world's ninth most populous country, with a population density of 9 inhabitants per square kilometre (23 per square mile).[448]
Since the 1990s, Russia's death rate has exceeded its birth rate, which some analysts have called a demographic crisis.[449] In 2019, the total fertility rate across Russia was estimated to be 1.5 children born per woman,[450] which is below the replacement rate of 2.1, and is one of the world's lowest fertility rates.[451] Subsequently, the nation has one of the world's oldest populations, with a median age of 40.3 years.[9] In 2009, it recorded annual population growth for the first time in fifteen years, and subsequently experienced annual population growth due to declining death rates, increased birth rates, and increased immigration.[452]
However, since 2020, Russia's population gains have been reversed, as excessive deaths from the COVID-19 pandemic resulted in its largest peacetime decline in history.[453] Following the Russian invasion of Ukraine in 2022, the demographic crisis in the country has deepened,[454] as the country has reportedly suffered high military fatalities while facing renewed brain drain and human capital flight caused by Western mass-sanctions and boycotts.[455]
Russia is a multinational state with many subnational entities associated with different minorities.[456] There are over 193 ethnic groups nationwide. In the 2010 census, roughly 81% of the population were ethnic Russians, and the remaining 19% of the population were ethnic minorities;[457] while over four-fifths of Russia's population was of European descent—of whom the vast majority were Slavs,[458] with a substantial minority of Finnic and Germanic peoples.[459][460] According to the United Nations, Russia's immigrant population is the world's third-largest, numbering over 11.6 million;[461] most of which are from post-Soviet states, mainly Ukrainians.[462]

Russian is the official and the predominantly spoken language in Russia.[3] It is the most spoken native language in Europe, the most geographically widespread language of Eurasia, as well as the world's most widely spoken Slavic language.[465] Russian is one of two official languages aboard the International Space Station,[466] as well as one of the six official languages of the United Nations.[465]
Russia is a multilingual nation; approximately 100–150 minority languages are spoken across the country.[467][468] According to the Russian Census of 2010, 137.5 million across the country spoke Russian, 4.3 million spoke Tatar, and 1.1 million spoke Ukrainian.[469] The constitution gives the country's individual republics the right to establish their own state languages in addition to Russian, as well as guarantee its citizens the right to preserve their native language and to create conditions for its study and development.[470] However, various experts have claimed Russia's linguistic diversity is rapidly declining due to many languages becoming endangered.[471][472]
Russia is a secular state by constitution, and its largest religion is Eastern Orthodox Christianity, chiefly represented by the Russian Orthodox Church.[6] Orthodox Christianity, together with Islam, Buddhism, and Paganism (either preserved or revived), are recognised by Russian law as the traditional religions of the country, part of its "historical heritage".[473][474] The amendments of 2020 to the constitution added, in the Article 67, the continuity of the Russian state in history based on preserving "the memory of the ancestors" and general "ideals and belief in God" which the ancestors conveyed.[475]
After the collapse of the Soviet Union, there was a renewal of religions in Russia, with the revival of the traditional faiths and the emergence of new forms within the traditional faiths as well as many new religious movements.[476][477] Islam is the second-largest religion in Russia, and is the traditional religion among the majority of the peoples of the North Caucasus, and among some Turkic peoples scattered along the Volga-Ural region.[6] Large populations of Buddhists are found in Kalmykia, Buryatia, Zabaykalsky Krai, and they are the vast majority of the population in Tuva.[6] Many Russians practise other religions, including Rodnovery (Slavic Neopaganism),[478] Assianism (Scythian Neopaganism),[479] other ethnic Paganisms, and inter-Pagan movements such as Ringing Cedars' Anastasianism,[480] various movements of Hinduism,[481] Siberian shamanism[482] and Tengrism, various Neo-Theosophical movements such as Roerichism, and other faiths.[483][484] Some religious minorities have faced oppression and some have been banned in the country;[485] notably, in 2017 the Jehovah's Witnesses were outlawed in Russia, facing persecution ever since, after having been declared an "extremist" and "nontraditional" faith.[486]
In 2012, the research organisation Sreda, in cooperation with the Ministry of Justice, published the Arena Atlas, an adjunct to the 2010 census, enumerating in detail the religious populations and nationalities of Russia, based on a large-sample country-wide survey. The results showed that 47.3% of Russians declared themselves Christians—including 41% Russian Orthodox, 1.5% simply Orthodox or members of non-Russian Orthodox churches, 4.1% unaffiliated Christians, and less than 1% Old Believers, Catholics or Protestants—25% were believers without affiliation to any specific religion, 13% were atheists, 6.5% were Muslims,[b] 1.2% were followers of "traditional religions honouring gods and ancestors" (Rodnovery, other Paganisms, Siberian shamanism and Tengrism), 0.5% were Buddhists, 0.1% were religious Jews and 0.1% were Hindus.[6]
Russia has an adult literacy rate of 100%,[488] and has compulsory education for a duration of 11 years, exclusively for children aged 7 to 17–18.[489] It grants free education to its citizens by constitution.[490] The Ministry of Education of Russia is responsible for primary and secondary education, as well as vocational education; while the Ministry of Education and Science of Russia is responsible for science and higher education.[489] Regional authorities regulate education within their jurisdictions within the prevailing framework of federal laws. Russia is among the world's most educated countries, and has the sixth-highest proportion of tertiary-level graduates in terms of percentage of population, at 62.1%.[491] It spent roughly 4.7% of its GDP on education in 2018.[492]
Russia's pre-school education system is highly developed and optional,[493] some four-fifths of children aged 3 to 6 attend day nurseries or kindergartens. Primary school is compulsory for eleven years, starting from age 6 to 7, and leads to a basic general education certificate.[489] An additional two or three years of schooling are required for the secondary-level certificate, and some seven-eighths of Russians continue their education past this level.[494]
Admission to an institute of higher education is selective and highly competitive:[490] first-degree courses usually take five years.[494] The oldest and largest universities in Russia are Moscow State University and Saint Petersburg State University.[495] There are ten highly prestigious federal universities across the country. Russia was the world's fifth-leading destination for international students in 2019, hosting roughly 300 thousand.[496]
Russia, by constitution, guarantees free, universal health care for all Russian citizens, through a compulsory state health insurance program.[498] The Ministry of Health of the Russian Federation oversees the Russian public healthcare system, and the sector employs more than two million people. Federal regions also have their own departments of health that oversee local administration. A separate private health insurance plan is needed to access private healthcare in Russia.[499]
Russia spent 5.65% of its GDP on healthcare in 2019.[500] Its healthcare expenditure is notably lower than other developed nations.[501] Russia has one of the world's most female-biased sex ratios, with 0.859 males to every female,[9] due to its high male mortality rate.[502] In 2019, the overall life expectancy in Russia at birth was 73.2 years (68.2 years for males and 78.0 years for females),[503] and it had a very low infant mortality rate (5 per 1,000 live births).[504]
The principal cause of death in Russia are cardiovascular diseases.[505] Obesity is a prevalent health issue in Russia; most adults are overweight or obese.[506] However, Russia's historically high alcohol consumption rate is the biggest health issue in the country,[507] as it remains one of the world's highest, despite a stark decrease in the last decade.[508] Smoking is another health issue in the country.[509]  The country's high suicide rate, although on the decline,[510] remains a significant social issue.[511]
Russian culture has been formed by the nation's history, its geographical location and its vast expanse, religious and social traditions, and Western influence.[512] Russian writers and philosophers have played an important role in the development of European literature and thought.[513][514] The Russians have also greatly influenced classical music,[515] ballet,[516] sport,[517] painting,[518] and cinema.[519] The nation has also made pioneering contributions to science and technology and space exploration.[520][521]
Russia is home to 30 UNESCO World Heritage Sites, 19 out of which are cultural; while 27 more sites lie on the tentative list.[522] The large global Russian diaspora has also played a major role in spreading Russian culture throughout the world. Russia's national symbol, the double-headed eagle, dates back to the Tsardom period, and is featured in its coat of arms and heraldry.[67] The Russian Bear and Mother Russia are often used as national personifications of the country.[523][524] Matryoshka dolls are considered a cultural icon of Russia.[525]
Russia has eight—public, patriotic, and religious—official holidays.[526] The year starts with New Year's Day on 1 January, soon followed by Russian Orthodox Christmas on 7 January; the two are the country's most popular holidays.[527] Defender of the Fatherland Day, dedicated to men, is celebrated on 23 February.[528] International Women's Day on 8 March, gained momentum in Russia during the Soviet era. The annual celebration of women has become so popular, especially among Russian men, that Moscow's flower vendors often see profits of "15 times"  more than other holidays.[529] Spring and Labour Day, originally a Soviet era holiday dedicated to workers, is celebrated on 1 May.[530]
Victory Day, which honours Soviet victory over Nazi Germany and the End of World War II in Europe, is celebrated as an annual large parade in Moscow's Red Square;[531] and marks the famous Immortal Regiment civil event.[532] Other patriotic holidays include Russia Day on 12 June, celebrated to commemorate Russia's declaration of sovereignty from the collapsing Soviet Union;[533] and Unity Day on 4 November, commemorating the 1612 uprising which marked the end of the Polish occupation of Moscow.[534]
There are many popular non-public holidays. Old New Year is celebrated on 14 January.[535] Maslenitsa is an ancient and popular East Slavic folk holiday.[536] Cosmonautics Day on 12 April, in tribute to the first human trip into space.[537] Two major Christian holidays are Easter and Trinity Sunday.[538]
Early Russian painting is represented in icons and vibrant frescos. In the early 15th-century, the master icon painter Andrei Rublev created some of Russia's most treasured religious art.[539] The Russian Academy of Arts, which was established in 1757, to train Russian artists, brought Western techniques of secular painting to Russia.[82] In the 18th century, academicians Ivan Argunov, Dmitry Levitzky, Vladimir Borovikovsky became influential.[540] The early 19th century saw many prominent paintings by Karl Briullov and Alexander Ivanov, both of whom were known for Romantic historical canvases.[541][542] Ivan Aivazovsky, another Romantic painter, is considered one of the greatest masters of marine art.[543]
In the 1860s, a group of critical realists (Peredvizhniki), led by Ivan Kramskoy, Ilya Repin and Vasiliy Perov broke with the academy, and portrayed the many-sided aspects of social life in paintings.[544] The turn of the 20th century saw the rise of symbolism; represented by Mikhail Vrubel and Nicholas Roerich.[545][546] The Russian avant-garde flourished from approximately 1890 to 1930; and globally influential artists from this era were El Lissitzky,[547] Kazimir Malevich, Natalia Goncharova, Wassily Kandinsky, and Marc Chagall.[548]
The history of Russian architecture begins with early woodcraft buildings of ancient Slavs, and the church architecture of Kievan Rus'.[549] Following the Christianization of Kievan Rus', for several centuries it was influenced predominantly by Byzantine architecture.[550] Aristotle Fioravanti and other Italian architects brought Renaissance trends into Russia.[551] The 16th-century saw the development of the unique tent-like churches; and the onion dome design, which is a distinctive feature of Russian architecture.[552] In the 17th-century, the "fiery style" of ornamentation flourished in Moscow and Yaroslavl, gradually paving the way for the Naryshkin baroque of the 1680s.[553]
After the reforms of Peter the Great, Russia's architecture became influenced by Western European styles. The 18th-century taste for Rococo architecture led to the splendid works of Bartolomeo Rastrelli and his followers. The most influential Russian architects of the eighteenth century; Vasily Bazhenov, Matvey Kazakov, and Ivan Starov, created lasting monuments in Moscow and Saint Petersburg and established a base for the more Russian forms that followed.[539] During the reign of Catherine the Great, Saint Petersburg was transformed into an outdoor museum of Neoclassical architecture.[554] Under Alexander I, Empire style became the de facto architectural style.[555] The second half of the 19th-century was dominated by the Neo-Byzantine and Russian Revival style.[556] In early 20th-century, Russian neoclassical revival became a trend.[557] Prevalent styles of the late 20th-century were Art Nouveau,[558] Constructivism,[559] and Socialist Classicism.[560]
Until the 18th-century, music in Russia consisted mainly of church music and folk songs and dances.[561] In the 19th-century, it was defined by the tension between classical composer Mikhail Glinka along with other members of The Mighty Handful, who were later succeeded by the Belyayev circle,[562] and the Russian Musical Society led by composers Anton and Nikolay Rubinstein.[563] The later tradition of Pyotr Ilyich Tchaikovsky, one of the greatest composers of the Romantic era, was continued into the 20th century by Sergei Rachmaninoff, one of the last great representatives of Romanticism in Russian and European classical music. World-renowned composers of the 20th century include Alexander Scriabin, Alexander Glazunov,[561] Igor Stravinsky, Sergei Prokofiev and Dmitri Shostakovich, and later Edison Denisov, Sofia Gubaidulina,[564] Georgy Sviridov,[565] and Alfred Schnittke.[564]
Soviet and Russian conservatories have turned out generations of world-renowned soloists. Among the best known are violinists David Oistrakh and Gidon Kremer,[566][567] cellist Mstislav Rostropovich,[568] pianists Vladimir Horowitz,[569] Sviatoslav Richter,[570] and Emil Gilels,[571] and vocalist Galina Vishnevskaya.[572]
During the Soviet era, popular music also produced a number of renowned figures, such as the two balladeers—Vladimir Vysotsky and Bulat Okudzhava,[564] and performers such as Alla Pugacheva.[573] Jazz, even with sanctions from Soviet authorities, flourished and evolved into one of the country's most popular musical forms.[564] By the 1980s, rock music became popular across Russia, and produced bands such as Aria, Aquarium,[574] DDT,[575] and Kino;[576] the latter's leader Viktor Tsoi, was in particular, a gigantic figure.[577] Pop music has continued to flourish in Russia since the 1960s, with globally famous acts such as t.A.T.u.[578]
Russian literature is considered to be among the world's most influential and developed.[513] It can be traced to the Middle Ages, when epics and chronicles in Old East Slavic were composed.[581] By the Age of Enlightenment, literature had grown in importance, with works from Mikhail Lomonosov, Denis Fonvizin, Gavrila Derzhavin, and Nikolay Karamzin.[582] From the early 1830s, during the Golden Age of Russian Poetry, literature underwent an astounding golden age in poetry, prose and drama.[583] Romanticism permitted a flowering of poetic talent: Vasily Zhukovsky and later his protégé Alexander Pushkin came to the fore.[584] Following Pushkin's footsteps, a new generation of poets were born, including Mikhail Lermontov, Nikolay Nekrasov, Aleksey Konstantinovich Tolstoy, Fyodor Tyutchev and Afanasy Fet.[582]
The first great Russian novelist was Nikolai Gogol.[585] Then came Ivan Turgenev, who mastered both short stories and novels.[586] Fyodor Dostoevsky and Leo Tolstoy soon became internationally renowned. Ivan Goncharov is remembered mainly for his novel Oblomov.[587] Mikhail Saltykov-Shchedrin wrote prose satire,[588] while Nikolai Leskov is best remembered for his shorter fiction.[589] In the second half of the century Anton Chekhov excelled in short stories and became a leading dramatist.[590] Other important 19th-century developments included the fabulist Ivan Krylov,[591] non-fiction writers such as the critic Vissarion Belinsky,[592] and playwrights such as Aleksandr Griboyedov and Aleksandr Ostrovsky.[593][594] The beginning of the 20th century ranks as the Silver Age of Russian Poetry. This era had poets such as Alexander Blok, Anna Akhmatova, Boris Pasternak, Konstantin Balmont,[595] Marina Tsvetaeva, Vladimir Mayakovsky, and Osip Mandelshtam. It also produced some first-rate novelists and short-story writers, such as Aleksandr Kuprin, Nobel Prize winner Ivan Bunin, Leonid Andreyev, Yevgeny Zamyatin, Dmitry Merezhkovsky and Andrei Bely.[582]
After the Russian Revolution of 1917, Russian literature split into Soviet and white émigré parts. In the 1930s, Socialist realism became the predominant trend in Russia. Its leading figure was Maxim Gorky, who laid the foundations of this style.[596] Mikhail Bulgakov was one of the leading writers of the Soviet era.[597] Nikolay Ostrovsky's novel How the Steel Was Tempered has been among the most successful works of Russian literature. Influential émigré writers include Vladimir Nabokov,[598] and Isaac Asimov; who was considered one of the "Big Three" science fiction writers.[599] Some writers dared to oppose Soviet ideology, such as Nobel Prize-winning novelist Aleksandr Solzhenitsyn, who wrote about life in the Gulag camps.[600]
Russian philosophy has been greatly influential. Alexander Herzen is known as one of the fathers of agrarian populism.[601] Mikhail Bakunin is referred to as the father of anarchism.[602] Peter Kropotkin was the most important theorist of anarcho-communism.[603] Mikhail Bakhtin's writings have significantly inspired scholars.[604] Helena Blavatsky gained international following as the leading theoretician of Theosophy, and co-founded the Theosophical Society.[605] Vladimir Lenin, a major revolutionary, developed a variant of communism known as Leninism.[606] Leon Trotsky, on the other hand, founded Trotskyism.[607] Alexander Zinoviev was a prominent philosopher in the second half of the 20th century.[608] Aleksandr Dugin, known for his fascist views, has been regarded as the "guru of geopolitics".[609]
Russian cuisine has been formed by climate, cultural and religious traditions, and the vast geography of the nation; and it shares similarities with the cuisines of its neighbouring countries. Crops of rye, wheat, barley, and millet provide the ingredients for various breads, pancakes and cereals, as well as for many drinks. Bread, of many varieties,[610] is very popular across Russia.[611] Flavourful soups and stews include shchi, borsch, ukha, solyanka, and okroshka. Smetana (a heavy sour cream) and mayonnaise are often added to soups and salads.[612][613] Pirozhki,[614] blini,[615] and syrniki are native types of pancakes.[616] Beef Stroganoff,[617]: 266  Chicken Kiev,[617]: 320  pelmeni,[618] and shashlyk are popular meat dishes.[619] Other meat dishes include stuffed cabbage rolls (golubtsy) usually filled with meat.[620] Salads include Olivier salad,[621] vinegret,[622] and dressed herring.[623]
Russia's national non-alcoholic drink is kvass,[624] and the national alcoholic drink is vodka; its creation in the nation dates back to the 14th century.[625] The country has the world's highest vodka consumption,[626] while beer is the most popular alcoholic beverage.[627] Wine has become increasingly popular in Russia in the 21st century.[628] Tea has been popular in Russia for centuries.[629]
There are 400 news agencies in Russia, among which the largest internationally operating are TASS, RIA Novosti, Sputnik, and Interfax.[631] Television is the most popular medium in Russia.[632] Among the 3,000 licensed radio stations nationwide, notable ones include Radio Rossii, Vesti FM, Echo of Moscow, Radio Mayak, and Russkoye Radio. Of the 16,000 registered newspapers, Argumenty i Fakty, Komsomolskaya Pravda, Rossiyskaya Gazeta, Izvestia, and Moskovskij Komsomolets are popular. State-run Channel One and Russia-1 are the leading news channels, while RT is the flagship of Russia's international media operations.[632] Russia has the largest video gaming market in Europe, with over 65 million players nationwide.[633]
Russian and later Soviet cinema was a hotbed of invention, resulting in world-renowned films such as The Battleship Potemkin, which was named the greatest film of all time at the Brussels World's Fair in 1958.[634][635] Soviet-era filmmakers, most notably Sergei Eisenstein and Andrei Tarkovsky, would go on to become among of the world's most innovative and influential directors.[636][637] Eisenstein was a student of Lev Kuleshov, who developed the groundbreaking Soviet montage theory of film editing at the world's first film school, the All-Union Institute of Cinematography.[638] Dziga Vertov's "Kino-Eye" theory had a huge impact on the development of documentary filmmaking and cinema realism.[639] Many Soviet socialist realism films were artistically successful, including Chapaev, The Cranes Are Flying, and Ballad of a Soldier.[519]
The 1960s and 1970s saw a greater variety of artistic styles in Soviet cinema.[519] The comedies of Eldar Ryazanov and Leonid Gaidai of that time were immensely popular, with many of the catchphrases still in use today.[640][641] In 1961–68 Sergey Bondarchuk directed an Oscar-winning film adaptation of Leo Tolstoy's epic War and Peace, which was the most expensive film made in the Soviet Union.[519] In 1969, Vladimir Motyl's White Sun of the Desert was released, a very popular film in a genre of ostern; the film is traditionally watched by cosmonauts before any trip into space.[642] After the dissolution of the Soviet Union, the Russian cinema industry suffered large losses—however, since the late 2000s, it has seen growth once again, and continues to expand.[643]
Football is the most popular sport in Russia.[645] The Soviet Union national football team became the first European champions by winning Euro 1960,[646] and reached the finals of Euro 1988.[647] Russian clubs CSKA Moscow and Zenit Saint Petersburg won the UEFA Cup in 2005 and 2008.[648][649] The Russian national football team reached the semi-finals of Euro 2008.[650] Russia was the host nation for the 2017 FIFA Confederations Cup,[651] and the 2018 FIFA World Cup.[652] However, Russian teams are currently suspended from FIFA and UEFA competitions.[653]
Ice hockey is very popular in Russia, and the Soviet national ice hockey team dominated the sport internationally throughout its existence.[517] Bandy is Russia's national sport, and it has historically been the highest-achieving country in the sport.[654] The Russian national basketball team won the EuroBasket 2007,[655] and the Russian basketball club PBC CSKA Moscow is among the most successful European basketball teams.[656] The annual Formula One Russian Grand Prix was held at the Sochi Autodrom in the Sochi Olympic Park, until its termination following the Russian invasion of Ukraine in 2022.[657][658]
Historically, Russian athletes have been one of the most successful contenders in the Olympic Games.[517] Russia is the leading nation in rhythmic gymnastics; and Russian synchronised swimming is considered to be the world's best.[659] Figure skating is another popular sport in Russia, especially pair skating and ice dancing.[660] Russia has produced numerous prominent tennis players.[661] Chess is also a widely popular pastime in the nation, with many of the world's top chess players being Russian for decades.[662] The 1980 Summer Olympic Games were held in Moscow,[663] and the 2014 Winter Olympics and the 2014 Winter Paralympics were hosted in Sochi.[664][665] However, Russia has also had 43 Olympic medals stripped from its athletes due to doping violations, which is the most of any country, and nearly a third of the global total.[666]
 This article incorporates text from a free content work.  Licensed under CC BY 4.0 (license statement/permission). Text taken from Frequently Asked Questions on Energy Security​,  International Energy Agency, the International Energy Agency. To learn how to add open license text to Wikipedia articles, please see this how-to page. For information on reusing text from Wikipedia, please see the terms of use.
Government
General information
Other
66°N 94°E﻿ / ﻿66°N 94°E﻿ / 66; 94



Bangkok,[a] officially known in Thai as Krung Thep Maha Nakhon[b] and colloquially as Krung Thep,[c] is the capital and most populous city of Thailand. The city occupies 1,568.7 square kilometres (605.7 sq mi) in the Chao Phraya River delta in central Thailand and has an estimated population of 10.539 million as of 2020, 15.3 percent of the country's population. Over 14 million people (22.2 percent) lived within the surrounding Bangkok Metropolitan Region at the 2010 census, making Bangkok an extreme primate city, dwarfing Thailand's other urban centres in both size and importance to the national economy.
Bangkok traces its roots to a small trading post during the Ayutthaya Kingdom in the 15th century, which eventually grew and became the site of two capital cities, Thonburi in 1768 and Rattanakosin in 1782. Bangkok was at the heart of the modernization of Siam, later renamed Thailand, during the late-19th century, as the country faced pressures from the West. The city was at the centre of Thailand's political struggles throughout the 20th century, as the country abolished absolute monarchy, adopted constitutional rule, and underwent numerous coups and several uprisings. The city, incorporated as a special administrative area under the Bangkok Metropolitan Administration in 1972, grew rapidly during the 1960s through the 1980s and now exerts a significant impact on Thailand's politics, economy, education, media and modern society.
The Asian investment boom in the 1980s and 1990s led many multinational corporations to locate their regional headquarters in Bangkok. The city is now a regional force in finance, business and pop culture. It is an international hub for transport and health care, and has emerged as a centre for the arts, fashion, and entertainment. The city is known for its street life and cultural landmarks, as well as its red-light districts. The Grand Palace and Buddhist temples including Wat Arun and Wat Pho stand in contrast with other tourist attractions such as the nightlife scenes of Khaosan Road and Patpong. Bangkok is among the world's top tourist destinations, and has been named the world's most visited city consistently in several international rankings.
Bangkok's rapid growth coupled with little urban planning has resulted in a haphazard cityscape and inadequate infrastructure. Despite an extensive expressway network, an inadequate road network and substantial private car usage have led to chronic and crippling traffic congestion, which caused severe air pollution in the 1990s. The city has since turned to public transport in an attempt to solve the problem, operating eight urban rail lines and building other public transit, but congestion still remains a prevalent issue.
The history of Bangkok dates at least back to the early 15th century, to when it was a village on the west bank of the Chao Phraya River, under the rule of Ayutthaya.[9] Because of its strategic location near the mouth of the river, the town gradually increased in importance. Bangkok initially served as a customs outpost with forts on both sides of the river, and was the site of a siege in 1688 in which the French were expelled from Siam. After the fall of Ayutthaya to the Burmese in 1767, the newly crowned King Taksin established his capital at the town, which became the base of the Thonburi Kingdom. In 1782, King Phutthayotfa Chulalok (Rama I) succeeded Taksin, moved the capital to the eastern bank's Rattanakosin Island, thus founding the Rattanakosin Kingdom. The City Pillar was erected on 21 April 1782, which is regarded as the date of foundation of Bangkok as the capital.[10]
Bangkok's economy gradually expanded through international trade, first with China, then with Western merchants returning in the early-to-mid 19th century. As the capital, Bangkok was the centre of Siam's modernization as it faced pressure from Western powers in the late-19th century. The reigns of Kings Mongkut (Rama IV, r. 1851–68) and Chulalongkorn (Rama V, r. 1868–1910) saw the introduction of the steam engine, printing press, rail transport and utilities infrastructure in the city, as well as formal education and healthcare. Bangkok became the centre stage for power struggles between the military and political elite as the country abolished absolute monarchy in 1932.[11]
As Thailand allied with Japan in World War II, Bangkok was subjected to Allied bombing, but rapidly grew in the post-war period as a result of US aid and government-sponsored investment. Bangkok's role as a US military R&R destination boosted its tourism industry as well as firmly establishing it as a sex tourism destination. Disproportionate urban development led to increasing income inequalities and migration from rural areas into Bangkok; its population surged from 1.8 million to 3 million in the 1960s.[11]
Following the US withdrawal from Vietnam in 1973, Japanese businesses took over as leaders in investment, and the expansion of export-oriented manufacturing led to growth of the financial market in Bangkok.[11] Rapid growth of the city continued through the 1980s and early 1990s, until it was stalled by the 1997 Asian financial crisis. By then, many public and social issues had emerged, among them the strain on infrastructure reflected in the city's notorious traffic jams. Bangkok's role as the nation's political stage continues to be seen in strings of popular protests, from the student uprisings in 1973 and 1976, anti-military demonstrations in 1992, and frequent street protests since 2006, including those by groups opposing and supporting former prime minister Thaksin Shinawatra from 2006 to 2013, and a renewed student-led movement in 2020.[12]
Administration of the city was first formalized by King Chulalongkorn in 1906, with the establishment of Monthon Krung Thep Phra Maha Nakhon (มณฑลกรุงเทพพระมหานคร) as a national subdivision. In 1915, the monthon was split into several provinces, the administrative boundaries of which have since further changed. The city in its current form was created in 1972 with the formation of the Bangkok Metropolitan Administration (BMA), following the merger of Phra Nakhon province on the eastern bank of the Chao Phraya and Thonburi province on the west during the previous year.[10]
The origin of the name Bangkok (บางกอก, pronounced in Thai as [bāːŋ kɔ̀ːk] (listen)) is unclear. Bang บาง is a Thai word meaning 'a village on a stream',[13] and the name might have been derived from Bang Ko (บางเกาะ), ko เกาะ meaning 'island', stemming from the city's watery landscape.[9] Another theory suggests that it is shortened from Bang Makok (บางมะกอก), makok มะกอก being the name of Elaeocarpus hygrophilus, a plant bearing olive-like fruit.[d] This is supported by the former name of Wat Arun, a historic temple in the area, that used to be called Wat Makok.[14]
Officially, the town was known as Thonburi Si Mahasamut (ธนบุรีศรีมหาสมุทร, from Pali and Sanskrit, literally 'city of treasures gracing the ocean') or Thonburi, according to the Ayutthaya Chronicles.[15] Bangkok was likely a colloquial name, albeit one widely adopted by foreign visitors, who continued to use it to refer to the city even after the new capital's establishment.
When King Rama I established his new capital on the river's eastern bank, the city inherited Ayutthaya's ceremonial name, of which there were many variants, including Krung Thep Thawarawadi Si Ayutthaya (กรุงเทพทวารวดีศรีอยุธยา) and Krung Thep Maha Nakhon Si Ayutthaya (กรุงเทพมหานครศรีอยุธยา).[16] Edmund Roberts, visiting the city as envoy of the United States in 1833, noted that the city, since becoming capital, was known as Sia-Yut'hia, and this is the name used in international treaties of the period.[17]
Today, the city is known in Thai as Krung Thep Maha Nakhon (กรุงเทพมหานคร) or simply Krung Thep (กรุงเทพฯ), a shortening of the ceremonial name which came into use during the reign of King Mongkut. The full name reads as follows:[e][10]
Krungthepmahanakhon Amonrattanakosin Mahintharayutthaya Mahadilokphop Noppharatratchathaniburirom Udomratchaniwetmahasathan Amonphimanawatansathit Sakkathattiyawitsanukamprasit[f]กรุงเทพมหานคร อมรรัตนโกสินทร์ มหินทรายุธยา มหาดิลกภพ นพรัตนราชธานีบูรีรมย์ อุดมราชนิเวศน์มหาสถาน อมรพิมานอวตารสถิต สักกะทัตติยวิษณุกรรมประสิทธิ์The name, composed of Pali and Sanskrit root words, translates as:
City of angels, great city of immortals, magnificent city of the nine gems, seat of the king, city of royal palaces, home of gods incarnate, erected by Vishvakarman at Indra's behest.[18]The name is listed in Guinness World Records as the world's longest place name, at 168 letters.[19][g] Many Thais who recall the full name do so because of its use in the 1989 song "Krung Thep Maha Nakhon" by Thai rock band Asanee–Wasan, the lyrics of which consist entirely of the city's full name, repeated throughout the song.[20]
The city is now officially known in Thai by a shortened form of the full ceremonial name, Krung Thep Maha Nakhon, which is colloquially further shortened to Krung Thep (city of gods). Krung, กรุง is a Thai word of Mon–Khmer origin, meaning 'capital, king',[21][verification needed] while thep, เทพ is from Pali/Sanskrit, meaning 'deity' or 'god' and corresponding to deva.
The city of Bangkok is locally governed by the Bangkok Metropolitan Administration (BMA). Although its boundaries are at the provincial (changwat) level, unlike the other 76 provinces Bangkok is a special administrative area whose governor is directly elected to serve a four-year term. The governor, together with four appointed deputies, form the executive body, who implement policies through the BMA civil service headed by the Permanent Secretary for the BMA. In separate elections, each district elects one or more city councillors, who form the Bangkok Metropolitan Council. The council is the BMA's legislative body, and has power over municipal ordinances and the city's budget.[22] The latest gubernatorial election took place on 22 May 2022 after an extended lapse following the 2014 Thai coup d'état, and was won by Chadchart Sittipunt.[23]
Bangkok is divided into fifty districts (khet, equivalent to amphoe in the other provinces), which are further subdivided into 180 sub-districts (khwaeng, equivalent to tambon). Each district is managed by a district director appointed by the governor. District councils, elected to four-year terms, serve as advisory bodies to their respective district directors.
The BMA is divided into sixteen departments, each overseeing different aspects of the administration's responsibilities. Most of these responsibilities concern the city's infrastructure, and include city planning, building control, transportation, drainage, waste management and city beautification, as well as education, medical and rescue services.[24] Many of these services are provided jointly with other agencies. The BMA has the authority to implement local ordinances, although civil law enforcement falls under the jurisdiction of the Metropolitan Police Bureau.
The seal of the city shows Hindu god Indra riding in the clouds on Airavata, a divine white elephant known in Thai as Erawan. In his hand Indra holds his weapon, the vajra.[25] The seal is based on a painting done by Prince Naris. The tree symbol of Bangkok is Ficus benjamina.[26] The official city slogan, adopted in 2012, reads:
As built by deities, the administrative centre, dazzling palaces and temples, the capital of Thailand กรุงเทพฯ ดุจเทพสร้าง เมืองศูนย์กลางการปกครอง วัดวังงามเรืองรอง เมืองหลวงของประเทศไทย[27]As the capital of Thailand, Bangkok is the seat of all branches of the national government. The Government House, Parliament House and Supreme, Administrative and Constitutional Courts are all in the city. Bangkok is the site of the Grand Palace and Dusit Palace, respectively the official and de facto residence of the king. Most government ministries also have headquarters and offices in the capital.
Bangkok covers an area of 1,568.7 square kilometres (605.7 sq mi), ranking 69th among the other 76 provinces of Thailand. Of this, about 700 square kilometres (270 sq mi) form the built-up urban area.[1] It is ranked 73rd in the world in terms of land area.[28] The city's urban sprawl reaches into parts of the six other provinces that it borders, namely, in clockwise order from northwest: Nonthaburi, Pathum Thani, Chachoengsao, Samut Prakan, Samut Sakhon, and Nakhon Pathom. With the exception of Chachoengsao, these provinces, together with Bangkok, form the greater Bangkok Metropolitan Region.[2]
Bangkok is situated in the Chao Phraya River delta in Thailand's central plain. The river meanders through the city in a southerly direction, emptying into the Gulf of Thailand approximately 25 kilometres (16 mi) south of city centre. The area is flat and low-lying, with an average elevation of 1.5 metres (4 ft 11 in) above sea level.[3][h] Most of the area was originally swampland, which was gradually drained and irrigated for agriculture by the construction of canals (khlong) which took place from the 16th to 19th centuries. The course of the river as it flows through Bangkok has been modified by the construction of several shortcut canals.
The city's waterway network served as the primary means of transport until the late 19th century, when modern roads began to be built. Up until then, most people lived near or on the water, leading the city to be known during the 19th century as the "Venice of the East".[29] Many of these canals have since been filled in or paved over, but others still criss-cross the city, serving as major drainage channels and transport routes. Most canals are now badly polluted, although the BMA has committed to the treatment and cleaning up of several canals.[30]
The geology of the Bangkok area is characterized by a top layer of soft marine clay, known as "Bangkok clay", averaging 15 metres (49 ft) in thickness, which overlies an aquifer system consisting of eight known units. This feature has contributed to the effects of subsidence caused by extensive groundwater pumping. First recognized in the 1970s, subsidence soon became a critical issue, reaching a rate of 120 millimetres (4.7 in) per year in 1981. Ground water management and mitigation measures have since lessened the severity of the situation, and the rate of subsidence decreased to 10 to 30 millimetres (0.39 to 1.18 in) per year in the early 2000s, though parts of the city are now 1 metre (3 ft 3 in) below sea level.[31]
Subsidence has resulted in increased flood risk, as Bangkok is already prone to flooding due to its low elevation and an inadequate drainage infrastructure,[32][33] often compounded by blockage from rubbish pollution (especially plastic waste).[34] The city now relies on flood barriers and augmenting drainage from canals by pumping and building drain tunnels, but parts of Bangkok and its suburbs are still regularly inundated. Heavy downpours resulting in urban runoff overwhelming drainage systems, and runoff discharge from upstream areas, are major triggering factors.[35] Severe flooding affecting much of the city occurred in 1995 and 2011. In 2011, most of Bangkok's northern, eastern and western districts were flooded, in some places for over two months.
Bangkok's coastal location makes it particularly vulnerable to rising sea levels due to global warming and climate change. A study by the OECD has estimated that 5.138 million people in Bangkok may be exposed to coastal flooding by 2070, the seventh highest figure among the world's port cities.[36]: 8  There are fears that the city may be submerged by 2030.[37][38][39] A study published in October 2019 in Nature Communications corrected earlier models of coastal elevations[40] and concluded that up to 12 million Thais—mostly in the greater Bangkok metropolitan area—face the prospect of annual flooding events.[41][42] This is compounded by coastal erosion, which is an issue in the gulf coastal area, a small length of which lies within Bangkok's Bang Khun Thian District. Tidal flat ecosystems existed on the coast, however, many have been reclaimed for agriculture, aquaculture, and salt works.[43]
There are no mountains in Bangkok. The closest mountain range is the Khao Khiao Massif, about 40 km (25 mi) southeast of the city. Phu Khao Thong, the only hill in the metropolitan area, originated with a very large chedi that King Rama III (1787–1851) built at Wat Saket. The chedi collapsed during construction because the soft soil could not support its weight. Over the next few decades, the abandoned mud-and-brick structure acquired the shape of a natural hill and became overgrown with weeds. The locals called it phu khao (ภูเขา), as if it were a natural feature.[44] In the 1940s, enclosing concrete walls were added to stop the hill from eroding.[45]
Like most of Thailand, Bangkok has a tropical savanna climate (Aw) under the Köppen climate classification and is under the influence of the South Asian monsoon system. The city experiences three seasons: hot, rainy, and cool, although temperatures are fairly hot year-round, ranging from an average low of 22.0 °C (71.6 °F) in December to an average high of 35.4 °C (95.7 °F) in April. The rainy season begins with the arrival of the southwest monsoon around mid-May. September is the wettest month, with an average rainfall of 334.3 millimetres (13.16 in). The rainy season lasts until October, when the dry and cool northeast monsoon takes over until February. The hot season is generally dry, but also sees occasional summer storms.[46] The surface magnitude of Bangkok's urban heat island has been measured at 2.5 °C (4.5 °F) during the day and 8.0 °C (14 °F) at night.[47] The highest recorded temperature of Bangkok metropolis was 41.0 °C (105.8 °F) in May 2023,[48] and the lowest recorded temperature was 9.9 °C (49.8 °F) in January 1955.[49]
The Climate Impact Group at NASA's Goddard Institute for Space Studies projected severe weather impacts on Bangkok caused by climate change. It found that Bangkok in 1960 had 193 days at or above 32 °C. In 2018, Bangkok can expect 276 days at or above 32 °C. The group forecasts a rise by 2100 to, on average, 297 to 344 days at or above 32 °C.[50]
Bangkok's fifty districts serve as administrative subdivisions under the authority of the BMA. Thirty-five of these districts lie to the east of the Chao Phraya, while fifteen are on the western bank, known as the Thonburi side of the city. The fifty districts, arranged by district code, are:[59]
Bangkok's districts often do not accurately represent the functional divisions of its neighbourhoods or land usage. Although urban planning policies date back to the commission of the "Litchfield Plan" in 1960, which set out strategies for land use, transportation and general infrastructure improvements, zoning regulations were not fully implemented until 1992. As a result, the city grew organically throughout the period of its rapid expansion, both horizontally as ribbon developments extended along newly built roads, and vertically, with increasing numbers of high rises and skyscrapers being built in commercial areas.[60]
The city has grown from its original centre along the river into a sprawling metropolis surrounded by swaths of suburban residential development extending north and south into neighbouring provinces. The highly populated and growing cities of Nonthaburi, Pak Kret, Rangsit and Samut Prakan are effectively now suburbs of Bangkok. Nevertheless, large agricultural areas remain within the city proper at its eastern and western fringes, and a small number of forest area is found within the city limits: 3,887 rai (6.2 km2; 2.4 sq mi), amounting to 0.4 percent of city area.[61] Land use in the city consists of 23 percent residential use, 24 percent agriculture, and 30 percent used for commerce, industry, and government.[1] The BMA's City Planning Department (CPD) is responsible for planning and shaping further development. It published master plan updates in 1999 and 2006, and a third revision is undergoing public hearings in 2012.[62]
Bangkok's historic centre remains the Rattanakosin Island in Phra Nakhon District.[63] It is the site of the Grand Palace and the City Pillar Shrine, primary symbols of the city's founding, as well as important Buddhist temples. Phra Nakhon, along with the neighbouring Pom Prap Sattru Phai and Samphanthawong Districts, formed what was the city proper in the late 19th century. Many traditional neighbourhoods and markets are found here, including the Chinese settlement of Sampheng.[63] The city was expanded toward Dusit District in the early 19th century, following King Chulalongkorn's relocation of the royal household to the new Dusit Palace. The buildings of the palace, including the neoclassical Ananta Samakhom Throne Hall, as well as the Royal Plaza and Ratchadamnoen Avenue which leads to it from the Grand Palace, reflect the heavy influence of European architecture at the time. Major government offices line the avenue, as does the Democracy Monument. The area is the site of the country's seat of power as well as the city's most popular tourist landmarks.[63]
In contrast with the low-rise historic areas, the business district on Si Lom and Sathon Roads in Bang Rak and Sathon Districts teems with skyscrapers. It is the site of many of the country's major corporate headquarters, but also of some of the city's red-light districts. The Siam and Ratchaprasong areas in Pathum Wan are home to some of the largest shopping malls in Southeast Asia. Numerous retail outlets and hotels also stretch along Sukhumvit Road leading southeast through Watthana and Khlong Toei Districts. More office towers line the streets branching off Sukhumvit, especially Asok Montri, while upmarket housing is found in many of its sois ('alley' or 'lane').
Bangkok lacks a single distinct central business district. Instead, the areas of Siam and Ratchaprasong serve as a "central shopping district" containing many of the bigger malls and commercial areas in the city, as well as Siam Station, the only transfer point between the city's two elevated train lines.[64] The Victory Monument in Ratchathewi District is among its most important road junctions, serving over 100 bus lines as well as an elevated train station. From the monument, Phahonyothin and Ratchawithi / Din Daeng Roads respectively run north and east linking to major residential areas. Most of the high-density development areas are within the 113-square-kilometre (44 sq mi) area encircled by the Ratchadaphisek inner ring road. Ratchadaphisek is lined with businesses and retail outlets, and office buildings also cluster around Ratchayothin Intersection in Chatuchak District to the north. Farther from the city centre, most areas are primarily mid- or low-density residential. The Thonburi side of the city is less developed, with fewer high rises. With the exception of a few secondary urban centres, Thonburi, in the same manner as the outlying eastern districts, consists mostly of residential and rural areas.
While most of Bangkok's streets are fronted by vernacular shophouses, the largely unrestricted building euphoria of the 1980s has transformed the city into an urban area of skyscrapers and high rises of contrasting and clashing styles.[65] There are 581 skyscrapers over 90 metres (300 feet) tall in the city. Bangkok was ranked as the world's eighth tallest city in 2016.[66] As a result of persistent economic disparity, many slums have emerged in the city. In 2000 there were over one million people living in about 800 informal settlements.[67] Some settlements are squatted such as the large slums in Khlong Toei District. In total there were 125 squatted areas.[67]
Bangkok has several parks, although these amount to a per capita total park area of only 1.82 square metres (19.6 sq ft) in the city proper. Total green space for the entire city is moderate, at 11.8 square metres (127 sq ft) per person. In the more densely built-up areas of the city these numbers are as low as 1.73 and 0.72 square metres (18.6 and 7.8 sq ft) per person.[68] More recent numbers claim that there is 3.3 square metres (36 sq ft) of green space per person,[69] compared to an average of 39 square metres (420 sq ft) in other cities across Asia. In Europe, London has 33.4 m2 of green space per head.[70] Bangkokians thus have 10 times less green space than is standard in the region's urban areas.[71] Green belt areas include about 700 square kilometres (270 sq mi) of rice paddies and orchards on the eastern and western edges of the city, although their primary purpose is to serve as flood detention basins rather than to limit urban expansion.[72] Bang Kachao, a 20-square-kilometre (7.7 sq mi) conservation area on an oxbow of the Chao Phraya, lies just across the southern riverbank districts, in Samut Prakan province. A master development plan has been proposed to increase total park area to 4 square metres (43 sq ft) per person.[68]
Bangkok's largest parks include the centrally located Lumphini Park near the Si Lom–Sathon business district with an area of 57.6 hectares (142 acres), the 80-hectare (200-acre) Suanluang Rama IX in the east of the city, and the Chatuchak–Queen Sirikit–Wachirabenchathat park complex in northern Bangkok, which has a combined area of 92 hectares (230 acres).[73] More parks are expected to be created through the Green Bangkok 2030 project, which aims to leave the city with 10 square metres (110 sq ft) of green space per person, including 30% of the city having tree cover.[74]
The city of Bangkok has a population of 8,305,218 according to the 2010 census, or 12.6 percent of the national population,[4] while 2020 estimates place the figure at 10.539 million (15.3 percent).[5] Roughly half are internal migrants from other Thai provinces;[76] population registry statistics recorded 5,676,648 residents belonging to 2,959,524 households in 2018.[77][i] Much of Bangkok's daytime population commutes from surrounding provinces in the Bangkok Metropolitan Region, the total population of which is 14,626,225 (2010 census).[4] Bangkok is a cosmopolitan city; the census showed that it is home to 567,120 expatriates from Asian countries (including 71,024 Chinese and 63,069 Japanese nationals), 88,177 from Europe, 32,241 from the Americas, 5,856 from Oceania and 5,758 from Africa. Migrants from neighbouring countries include 216,528 Burmese, 72,934 Cambodians and 52,498 Lao.[78] In 2018, numbers show that there are 370,000 international migrants registered with the Department of Employment, more than half of them migrants from Cambodia, Laos, and Myanmar.[76]
Following its establishment as capital city in 1782, Bangkok grew only slightly throughout the 18th and early 19th centuries. British diplomat John Crawfurd, visiting in 1822, estimated its population at no more than 50,000.[79] As a result of Western medicine brought by missionaries as well as increased immigration from both within Siam and overseas, Bangkok's population gradually increased as the city modernized in the late 19th century. This growth became even more pronounced in the 1930s, following the discovery of antibiotics. Although family planning and birth control were introduced in the 1960s, the lowered birth rate was more than offset by increased migration from the provinces as economic expansion accelerated. Only in the 1990s have Bangkok's population growth rates decreased, following the national rate. Thailand had long since become highly centralized around the capital. In 1980, Bangkok's population was fifty-one times that of Hat Yai and Songkhla, the second-largest urban centre at the time, making it the world's most prominent primate city.[80][81]
The majority of Bangkok's population identify as Thai,[j] although details on the city's ethnic make-up are unavailable, as the national census does not document race.[k] Bangkok's cultural pluralism dates back to the early days of its founding: several ethnic communities were formed by immigrants and forced settlers including the Khmer, northern Thai, Lao, Vietnamese, Mon and Malay.[10] Most prominent were the Chinese, who played major roles in the city's trade and became the majority of Bangkok's population—estimates include up to three-fourths in 1828 and almost half in the 1950s.[85][l] Chinese immigration was restricted from the 1930s and effectively ceased after the Chinese Revolution in 1949. Their prominence subsequently declined as younger generations of Thai Chinese integrated and adopted a Thai identity. Bangkok is still nevertheless home to a large Chinese community, with the greatest concentration in Yaowarat, Bangkok's Chinatown.
Religion in Bangkok
The majority (93 percent) of the city's population is Buddhist, according to the 2010 census. Other religions include Islam (4.6 percent), Christianity (1.9 percent), Hinduism (0.3 percent), Sikhism (0.1 percent) and Confucianism (0.1 percent).[87]
Apart from Yaowarat, Bangkok also has several other distinct ethnic neighbourhoods. The Indian community is centred in Phahurat, where the Gurdwara Siri Guru Singh Sabha, founded in 1933, is located. Ban Khrua on Saen Saep Canal is home to descendants of the Cham who settled in the late 18th century. Although the Portuguese who settled during the Thonburi period have ceased to exist as a distinct community, their past is reflected in Santa Cruz Church, on the west bank of the river. Likewise, Assumption Cathedral on Charoen Krung Road is among many European-style buildings in the Old Farang Quarter, where European diplomats and merchants lived in the late 19th to early 20th centuries. Nearby, the Haroon Mosque is the centre of a Muslim community. Newer expatriate communities exist along Sukhumvit Road, including the Japanese community near Soi Phrom Phong and Soi Thong Lo, and the Arab and North African neighbourhood along Soi Nana. Sukhumvit Plaza, a mall on Soi Sukhumvit 12, is popularly known as Korea Town.
Bangkok is the economic centre of Thailand, and the heart of the country's investment and development. In 2010, the city had an economic output of 3.142 trillion baht (US$98.34 billion), contributing 29.1 percent of the gross domestic product (GDP). This amounted to a per-capita GDP value of 456,911 baht ($14,301), almost three times the national average of 160,556 baht ($5,025). The Bangkok Metropolitan Region had a combined output of 4.773 trillion baht ($149.39 billion), or 44.2 percent of GDP.[88] Bangkok's economy ranked as the sixth among Asian cities in terms of per-capita GDP, after Singapore, Hong Kong, Tokyo, Osaka–Kobe and Seoul, as of 2010.[89][needs update]
Wholesale and retail trade is the largest sector in the city's economy, contributing 24 percent of Bangkok's gross provincial product. It is followed by manufacturing (14.3 percent); real estate, renting and business activities (12.4 percent); transport and communications (11.6 percent); and financial intermediation (11.1 percent). Bangkok alone accounts for 48.4 percent of Thailand's service sector, which in turn constitutes 49 percent of GDP. When the Bangkok Metropolitan Region is considered, manufacturing is the most significant contributor at 28.2 percent of the gross regional product, reflecting the density of industry in the Bangkok's neighbouring provinces.[90] The automotive industry based around Greater Bangkok is the largest production hub in Southeast Asia.[91] Tourism is also a significant contributor to Bangkok's economy, generating 427.5 billion baht ($13.38 billion) in revenue in 2010.[92]
The Stock Exchange of Thailand (SET) is on Ratchadaphisek Road in inner Bangkok. The SET, together with the Market for Alternative Investment (MAI) has 648 listed companies as of the end of 2011, with a combined market capitalization of 8.485 trillion baht ($267.64 billion).[93] Due to the large amount of foreign representation, Thailand has for several years been a mainstay of the Southeast Asian economy and a centre of Asian business. The Globalization and World Cities Research Network ranks Bangkok as an "Alpha -" world city, and it is ranked 59th in Z/Yen's Global Financial Centres Index 11.[94][95]
Bangkok is home to the headquarters of all of Thailand's major commercial banks and financial institutions, as well as the country's largest companies. Many multinational corporations base their regional headquarters in Bangkok due to the lower cost of labour and operations relative to other major Asian business centres. Seventeen Thai companies are listed on the Forbes 2000, all of which are based in the capital,[96] including PTT, the only Fortune Global 500 company in Thailand.[97]
Income inequality is a major issue in Bangkok, especially between relatively unskilled lower-income immigrants from rural provinces and neighbouring countries, and middle-class professionals and business elites. Although absolute poverty rates are low—only 0.64 percent of Bangkok's registered residents were living under the poverty line in 2010, compared to a national average of 7.75 percent—economic disparity is still substantial.[98] The city has a Gini coefficient of 0.48, indicating a high level of inequality.[99]
Bangkok is one of the world's top tourist destinations. Of 162 cities worldwide, MasterCard ranked Bangkok as the top destination city by international visitor arrivals in its Global Destination Cities Index 2018, ahead of London, with just over 20 million overnight visitors in 2017.[100] This was a repeat of its 2017 ranking (for 2016).[101][102] Euromonitor International ranked Bangkok fourth in its Top City Destinations Ranking for 2016.[103] Bangkok was also named "World's Best City" by Travel + Leisure magazine's survey of its readers for four consecutive years, from 2010 to 2013.[104]
As the main gateway through which visitors arrive in Thailand, Bangkok is visited by the majority of international tourists to the country. Domestic tourism is also prominent. The Department of Tourism recorded 26,861,095 Thai and 11,361,808 foreign visitors to Bangkok in 2010. Lodgings were made by 15,031,244 guests, who occupied 49.9 percent of the city's 86,687 hotel rooms.[92] Bangkok also topped the list as the world's most popular tourist destinations in 2017 rankings.[105][106][107][108]
Bangkok's multi-faceted sights, attractions and city life appeal to diverse groups of tourists. Royal palaces and temples as well as several museums constitute its major historical and cultural tourist attractions. Shopping and dining experiences offer a wide range of choices and prices. The city is also famous for its dynamic nightlife. Although Bangkok's sex tourism scene is well known to foreigners, it is usually not openly acknowledged by locals or the government.
Among Bangkok's well-known sights are the Grand Palace and major Buddhist temples, including Wat Phra Kaew, Wat Pho, and Wat Arun. The Giant Swing and Erawan Shrine demonstrate Hinduism's deep-rooted influence in Thai culture. Vimanmek Mansion in Dusit Palace is famous as the world's largest teak building, while the Jim Thompson House provides an example of traditional Thai architecture. Other major museums include the Bangkok National Museum and the Royal Barge National Museum. Cruises and boat trips on the Chao Phraya and the canals of Thonburi offer views of some of the city's traditional architecture and ways of life on the waterfront.[109]
Shopping venues, many of which are popular with both tourists and locals, range from the shopping centres and department stores concentrated in Siam and Ratchaprasong to the sprawling Chatuchak Weekend Market. Taling Chan Floating Market is among the few such markets in Bangkok. Yaowarat is known for its shops as well as street-side food stalls and restaurants, which are also found throughout the city. Khao San Road has long been famous as a destination for backpacker tourism, with its budget accommodation, shops and bars attracting visitors from all over the world.
Bangkok has a reputation overseas as a major destination in the sex industry. Although prostitution is technically illegal and is rarely openly discussed in Thailand, it commonly takes place among massage parlours, saunas and hourly hotels, serving foreign tourists as well as locals. Bangkok has acquired the nickname "Sin City of Asia" for its level of sex tourism.[110]
Issues often encountered by foreign tourists include scams, overcharging and dual pricing. In a survey of 616 tourists visiting Thailand, 7.79 percent reported encountering a scam, the most common of which was the gem scam, in which tourists are tricked into buying overpriced jewellery.[111]
Grand Palace
The Giant Swing
Wat Arun
The culture of Bangkok reflects its position as Thailand's centre of wealth and modernisation. The city has long been the portal of entry of Western concepts and material goods, which have been adopted and blended with Thai values to various degrees by its residents. This is most evident in the lifestyles of the expanding middle class. Conspicuous consumption serves as a display of economic and social status, and shopping centres are popular weekend hangouts.[112] Ownership of electronics and consumer products such as mobile phones is ubiquitous. This has been accompanied by a degree of secularism, as religion's role in everyday life has rather diminished. Although such trends have spread to other urban centres, and, to a degree, the countryside, Bangkok remains at the forefront of social change.
A distinct feature of Bangkok is the ubiquity of street vendors selling goods ranging from food items to clothing and accessories. It has been estimated that the city may have over 100,000 hawkers. While the BMA has authorised the practice in 287 sites, the majority of activity in another 407 sites takes place illegally. Although they take up pavement space and block pedestrian traffic, many of the city's residents depend on these vendors for their meals, and the BMA's efforts to curb their numbers have largely been unsuccessful.[113]
In 2015, however, the BMA, with support from the National Council for Peace and Order (Thailand's ruling military junta), began cracking down on street vendors in a bid to reclaim public space. Many famous market neighbourhoods were affected, including Khlong Thom, Saphan Lek, and the flower market at Pak Khlong Talat. Nearly 15,000 vendors were evicted from 39 public areas in 2016.[114] While some applauded the efforts to focus on pedestrian rights, others have expressed concern that gentrification would lead to the loss of the city's character and adverse changes to people's way of life.[115][116]
The residents of Bangkok celebrate many of Thailand's annual festivals. During Songkran on 13–15 April, traditional rituals as well as water fights take place throughout the city. Loi Krathong, usually in November, is accompanied by the Golden Mount Fair. New Year celebrations take place at many venues, the most prominent being the plaza in front of CentralWorld. Observances related to the royal family are held primarily in Bangkok. Wreaths are laid at King Chulalongkorn's equestrian statue in the Royal Plaza on 23 October, which is King Chulalongkorn Memorial Day. The present king's and queen's birthdays, respectively on 5 December and 12 August, are marked as Thailand's national Father's Day and national Mother's Day. These national holidays are celebrated by royal audiences on the day's eve, in which the king or queen gives a speech, and public gatherings on the day of the observance. The king's birthday is also marked by the Royal Guards' parade.
Sanam Luang is the site of the Thai Kite, Sport and Music Festival, usually held in March, and the Royal Ploughing Ceremony which takes place in May. The Red Cross Fair at the beginning of April is held at Suan Amporn and the Royal Plaza, and features numerous booths offering goods, games and exhibits. The Chinese New Year (January–February) and Vegetarian Festival (September–October) are celebrated widely by the Chinese community, especially in Yaowarat.[117]
Bangkok was designated as the World Book Capital for the year 2013 by UNESCO.[118]
Bangkok's first Thai International Gay Pride Festival took place on October 31, 1999.[119] Pride Parades have also been held in Bangkok, with the first official parade held in 2022 under the name "Bangkok Naruemit Pride Parade". Pride Parades were announced to be a part of Bangkok's "12 monthly festivals" in 2022.[120]
Bangkok is the centre of Thailand's media industry. All national newspapers, broadcast media and major publishers are based in the capital. Its 21 national newspapers had a combined daily circulation of about two million in 2002. These include the mass-oriented Thai Rath, Khao Sod and Daily News, the first of which currently prints a million copies per day,[121] as well as the less sensational Matichon and Krungthep Thurakij. The Bangkok Post and The Nation are the two national English language dailies. Foreign publications including The Asian Wall Street Journal, Financial Times, The Straits Times and the Yomiuri Shimbun also have operations in Bangkok.[122] The large majority of Thailand's more than 200 magazines are published in the capital, and include news magazines as well as lifestyle, entertainment, gossip and fashion-related publications.
Bangkok is also the hub of Thailand's broadcast television. All six national terrestrial channels, Channels 3, 5 and 7, Modernine, NBT and Thai PBS, have headquarters and main studios in the capital. GMM Grammy is Thailand's largest mass-media conglomerate is also headquartered in Bangkok as well. With the exception of local news segments broadcast by the NBT, all programming is done in Bangkok and repeated throughout the provinces. However, this centralised model is weakening with the rise of cable television, which has many local providers. There are numerous cable and satellite channels based in Bangkok. TrueVisions is the major subscription television provider in Bangkok and Thailand, and it also carries international programming. Bangkok was home to 40 of Thailand's 311 FM radio stations and 38 of its 212 AM stations in 2002.[122] Broadcast media reform stipulated by the 1997 Constitution has been progressing slowly, although many community radio stations have emerged in the city.
Likewise, Bangkok has dominated the Thai film industry since its inception. Although film settings normally feature locations throughout the country, the city is home to all major film studios in Thailand such as GDH 559 (GMM Grammy's film production subsidiary), Sahamongkol Film International and Five Star Production. Bangkok has dozens of cinemas and multiplexes, and the city hosts two major film festivals annually, the Bangkok International Film Festival and the World Film Festival of Bangkok.
Traditional Thai art, long developed within religious and royal contexts, continues to be sponsored by various government agencies in Bangkok, including the Department of Fine Arts' Office of Traditional Arts. The SUPPORT Foundation in Chitralada Palace sponsors traditional and folk handicrafts. Various communities throughout the city still practice their traditional crafts, including the production of khon masks, alms bowls, and classical musical instruments. The National Gallery hosts permanent collection of traditional and modern art, with temporary contemporary exhibits. Bangkok's contemporary art scene has slowly grown from relative obscurity into the public sphere over the past two decades. Private galleries gradually emerged to provide exposure for new artists, including the Patravadi Theatre and H Gallery. The centrally located Bangkok Art and Culture Centre, opened in 2008 following a fifteen-year lobbying campaign, is now the largest public exhibition space in the city.[123] There are also many other art galleries and museums, including the privately owned Museum of Contemporary Art.
The city's performing arts scene features traditional theatre and dance as well as Western-style plays. Khon and other traditional dances are regularly performed at the National Theatre and Salachalermkrung Royal Theatre, while the Thailand Cultural Centre is a newer multi-purpose venue which also hosts musicals, orchestras and other events. Numerous venues regularly feature a variety of performances throughout the city.
As is the national trend, association football and Muay Thai dominate Bangkok's spectator sport scene.[124] Muangthong United, Bangkok United, BG Pathum United, Port and Police Tero are major Thai League clubs based in the Bangkok Metropolitan Region,[125][126] while the Rajadamnern and Lumpini stadiums are the main kickboxing venues.
While sepak takraw can be seen played in open spaces throughout the city, football and other modern sports are now the norm. Western sports introduced during the reign of King Chulalongkorn were originally only available to the privileged, and such status is still associated with certain sports. Golf is popular among the upwardly mobile, and there are several courses in Bangkok. Horse racing, highly popular at the mid-20th century, still takes place at the Royal Bangkok Sports Club.
There are many public sporting facilities located throughout Bangkok. The two main centres are the National Stadium complex, which dates to 1938, and the newer Hua Mak Sports Complex, which was built for the 1998 Asian Games. Bangkok had also hosted the games in 1966, 1970 and 1978; the most of any city. The city was the host of the inaugural Southeast Asian Games in 1959, the 2007 Summer Universiade and the 2012 FIFA Futsal World Cup.
Although Bangkok's canals historically served as a major mode of transport, they have long since been surpassed in importance by land traffic. Charoen Krung Road, the first to be built by Western techniques, was completed in 1864. Since then, the road network has vastly expanded to accommodate the sprawling city. A complex elevated expressway network and Don Mueang Tollway helps bring traffic into and out of the city centre, but Bangkok's rapid growth has put a large strain on infrastructure, and traffic jams have plagued the city since the 1990s. Although rail transport was introduced in 1893 and trams served the city from 1888 to 1968, it was only in 1999 that Bangkok's first rapid transit system began operation. Older public transport systems include an extensive bus network and boat services which still operate on the Chao Phraya and two canals. Taxis appear in the form of cars, motorcycles, and "tuk-tuk" auto rickshaws.
Bangkok is connected to the rest of the country through the national highway and rail networks, as well as by domestic flights to and from the city's two international airports. Its centuries-old maritime transport of goods is still conducted through Khlong Toei Port.
The BMA is largely responsible for overseeing the construction and maintenance of the road network and transport systems through its Public Works Department and Traffic and Transportation Department. However, many separate government agencies are also in charge of the individual systems, and much of transport-related policy planning and funding is contributed to by the national government.
Road-based transport is the primary mode of travel in Bangkok. Due to the city's organic development, its streets do not follow an organized grid structure. Forty-eight major roads link the different areas of the city, branching into smaller streets and lanes (soi) which serve local neighbourhoods. Eleven bridges over the Chao Phraya link the two sides of the city, while several expressway and motorway routes bring traffic into and out of the city centre and link with nearby provinces. The first expressway in Bangkok is Chaloem Maha Nakhon Expressway, which opened 1981.
Bangkok's rapid growth in the 1980s resulted in sharp increases in vehicle ownership and traffic demand, which have since continued—in 2006 there were 3,943,211 in-use vehicles in Bangkok, of which 37.6 percent were private cars and 32.9 percent were motorcycles.[128] These increases, in the face of limited carrying capacity, caused severe traffic congestion evident by the early 1990s. The extent of the problem is such that the Thai Traffic Police has a unit of officers trained in basic midwifery in order to assist deliveries which do not reach hospital in time.[129] While Bangkok's limited road surface area (8 percent, compared to 20–30 percent in most Western cities) is often cited as a major cause of its traffic jams, other factors, including high vehicle ownership rate relative to income level, inadequate public transport systems, and lack of transportation demand management, also play a role.[130] Efforts to alleviate the problem have included the construction of intersection bypasses and an extensive system of elevated highways, as well as the creation of several new rapid transit systems. The city's overall traffic conditions, however, remain poor.
Traffic has been the main source of air pollution in Bangkok, which reached serious levels in the 1990s. But efforts to improve air quality by improving fuel quality and enforcing emission standards, among others, had visibly ameliorated the problem by the 2000s. Atmospheric particulate matter levels dropped from 81 micrograms per cubic metre in 1997 to 43 in 2007.[131] However, increasing vehicle numbers and a lack of continued pollution-control efforts threatens a reversal of the past success.[132] In January–February 2018, weather conditions caused bouts of haze to cover the city, with particulate matter under 2.5 micrometres (PM2.5) rising to unhealthy levels for several days on end.[133][134]
Although the BMA has created thirty signed bicycle routes along several roads totalling 230 kilometres (140 mi),[135] cycling is still largely impractical, especially in the city centre. Most of these bicycle lanes share the pavement with pedestrians. Poor surface maintenance, encroachment by hawkers and street vendors, and a hostile environment for cyclists and pedestrians, make cycling and walking unpopular methods of getting around in Bangkok.
Bangkok has an extensive bus network providing local transit services within the Greater Bangkok area. The Bangkok Mass Transit Authority (BMTA) operates a monopoly on bus services, with substantial concessions granted to private operators. Buses, minibus vans, and song thaeo operate on a total of 470 routes throughout the region.[136] A separate bus rapid transit system owned by the BMA has been in operation since 2010. Known simply as the BRT, the system currently consists of a single line running from the business district at Sathon to Ratchaphruek on the western side of the city. The Transport Co., Ltd. is the BMTA's long-distance counterpart, with services to all provinces operating out of Bangkok.
Taxis are ubiquitous in Bangkok, and are a popular form of transport. As of August 2012[update], there are 106,050 cars, 58,276 motorcycles and 8,996 tuk-tuk motorized tricycles cumulatively registered for use as taxis.[137] Meters have been required for car taxis since 1992, while tuk-tuk fares are usually negotiated. Motorcycle taxis operate from regulated ranks, with either fixed or negotiable fares, and are usually employed for relatively short journeys.
Despite their popularity, taxis have gained a bad reputation for often refusing passengers when the requested route is not to the driver's convenience.[138] Motorcycle taxis were previously unregulated, and subject to extortion by organized crime gangs. Since 2003, registration has been required for motorcycle taxi ranks, and drivers now wear distinctive numbered vests designating their district of registration and where they are allowed to accept passengers.
Several ride hailing super-apps operate within the city, including Grab (offering car and motorbike options),[139] and AirAsia in 2022.[140][141] The Estonian company Bolt launched airport transfer and ride hailing services in 2020. Ride sharing startup MuvMi launched in 2018, and operates an electric tuk-tuk service in 9 areas across the city.[142][143]
Bangkok is the location of Krung Thep Aphiwat Central Terminal, the new main terminus of the national rail network operated by the State Railway of Thailand (SRT). The older terminus, Bangkok (Hua Lamphong) Railway Station, which was the main station for Bangkok for over a century, remains in use. The SRT operates long-distance intercity services from Krung Thep Aphiwat, while commuter trains running to and from the outskirts of the city during the rush hour continue to operate at Bangkok (Hua Lamphong).
Bangkok is served by four rapid transit systems: the BTS Skytrain, the MRT, the SRT Red Lines, and the elevated Airport Rail Link. Although proposals for the development of rapid transit in Bangkok had been made since 1975,[144] it was only in 1999 that the BTS finally began operation.
The BTS consists of two lines, Sukhumvit and Silom, with 59 stations along 68.25 kilometres (42.41 mi).[145] The MRT opened for use in July 2004, and currently consists of two lines, the Blue Line and Purple Line with 53 stations along 70.6 kilometres (43.9 mi). The Airport Rail Link, opened in August 2010, connects the city centre to Suvarnabhumi Airport to the east. Its eight stations span a distance of 28.6 kilometres (17.8 mi). The SRT Red Lines (Commuter) opened in 2021, and consists of two lines, The SRT Dark Red Line and SRT Light Red Line with currently 14 stations along 41 kilometres (25 mi).
Although initial passenger numbers were low and their service area was limited to the inner city until the 2016 opening of the Purple Line, which serves the Nonthaburi area, these systems have become indispensable to many commuters. The BTS reported an average of 600,000 daily trips in 2012,[146] while the MRT had 240,000 passenger trips per day.[147]
As of September 2020[update], construction work is ongoing to extend the city-wide transit system's reach, including the construction of the Light Red grade-separated commuter rail line. The entire Mass Rapid Transit Master Plan in Bangkok Metropolitan Region consists of eight main lines and four feeder lines totaling 508 kilometres (316 mi) to be completed by 2029. In addition to rapid transit and heavy rail lines, there have been proposals for several monorail systems.
Although much diminished from its past prominence, water-based transport still plays an important role in Bangkok and the immediate upstream and downstream provinces. Several water buses serve commuters daily. The Chao Phraya Express Boat serves thirty-four stops along the river, carrying an average of 35,586 passengers per day in 2010, while the smaller Khlong Saen Saep boat service serves twenty-seven stops on Saen Saep Canal with 57,557 daily passengers. Khlong Phasi Charoen boat service serves twenty stops on the Phasi Charoen Canal. Long-tail boats operate on fifteen regular routes on the Chao Phraya, and passenger ferries at thirty-two river crossings served an average of 136,927 daily passengers in 2010.[148]
Bangkok Port, popularly known by its location as Khlong Toei Port, was Thailand's main international port from its opening in 1947 until it was superseded by the deep-sea Laem Chabang Port in 1991. It is primarily a cargo port, though its inland location limits access to ships of 12,000 deadweight tonnes or less. The port handled 11,936,855 tonnes (13,158,130 tons) of cargo in the first eight months of the 2010 fiscal year, about 22 percent the total of the country's international ports.[149][150]
Bangkok is one of Asia's busiest air transport hubs. Two commercial airports serve the city, the older Don Mueang International Airport and the newer Suvarnabhumi Airport. Suvarnabhumi, which replaced Don Mueang as Bangkok's main airport after its opening in 2006, served 52,808,013 passengers in 2015,[151] making it the world's 20th busiest airport by passenger volume. This volume exceeded its designed capacity of 45 million passengers. Don Mueang reopened for domestic flights in 2007,[152] and resumed international service focusing on low-cost carriers in October 2012.[153] Suvarnabhumi is undergoing expansion to increase its capacity to 60 million passengers by 2019 and 90 million by 2021.[154]
Bangkok has long been the centre of modern education in Thailand. The first schools in the country were established here in the later 19th century, and there are now 1,351 schools in the city.[155] The city is home to the country's five oldest universities, Chulalongkorn, Thammasat, Kasetsart, Mahidol and Silpakorn, founded between 1917 and 1943. The city has since continued its dominance, especially in higher education; the majority of the country's universities, both public and private, are located in Bangkok or the Metropolitan Region. Chulalongkorn and Mahidol are the only Thai universities to appear in the top 500 of the QS World University Rankings.[156] King Mongkut's University of Technology Thonburi, also located in Bangkok, is the only Thai university in the top 400 of the 2012–13 Times Higher Education World University Rankings.[157]
Over the past few decades the general trend of pursuing a university degree has prompted the founding of new universities to meet the needs of Thai students. Bangkok became not only a place where immigrants and provincial Thais go for job opportunities, but also for a chance to receive a university degree. Ramkhamhaeng University emerged in 1971 as Thailand's first open university; it now has the highest enrolment in the country. The demand for higher education has led to the founding of many other universities and colleges, both public and private. While many universities have been established in major provinces, the Greater Bangkok region remains home to the greater majority of institutions, and the city's tertiary education scene remains over-populated with non-Bangkokians. The situation is not limited to higher education, either. In the 1960s, 60 to 70 percent of 10- to 19-year-olds who were in school had migrated to Bangkok for secondary education. This was due to both a lack of secondary schools in the provinces and perceived higher standards of education in the capital.[158] Although this discrepancy has since largely abated, tens of thousands of students still compete for places in Bangkok's leading schools. Education has long been a prime factor in the centralization of Bangkok and will play a vital role in the government's efforts to decentralize the country.
Much of Thailand's medical resources are disproportionately concentrated in the capital. In 2000, Bangkok had 39.6 percent of the country's doctors and a physician-to-population ratio of 1:794, compared to a median of 1:5,667 among all provinces.[159] The city is home to 42 public hospitals, five of which are university hospitals, as well as 98 private hospitals and 4,063 registered clinics.[dead link][160] The BMA operates nine public hospitals through its Medical Service Department, and its Health Department provides primary care through sixty-eight community health centres. Thailand's universal healthcare system is implemented through public hospitals and health centres as well as participating private providers.
Research-oriented medical school affiliates such as Siriraj, King Chulalongkorn Memorial and Ramathibodi Hospitals are among the largest in the country, and act as tertiary care centres, receiving referrals from distant parts of the country. Lately, especially in the private sector, there has been much growth in medical tourism, with hospitals such as Bumrungrad and Bangkok Hospital, among others, providing services specifically catering to foreigners. An estimated 200,000 medical tourists visited Thailand in 2011, making Bangkok the most popular global destination for medical tourism.[161]
Bangkok has a relatively moderate crime rate when compared to urban counterparts around the world.[162] Traffic accidents are a major hazard[163] while natural disasters are rare. Intermittent episodes of political unrest and occasional terrorist attacks have resulted in losses of life.
Although the crime threat in Bangkok is relatively low, non-confrontational crimes of opportunity such as pick-pocketing, purse-snatching, and credit card fraud occur with frequency.[162] Bangkok's growth since the 1960s has been followed by increasing crime rates partly driven by urbanisation, migration, unemployment and poverty. By the late 1980s, Bangkok's crime rates were about four times that of the rest of the country. The police have long been preoccupied with street crimes ranging from housebreaking to assault and murder.[164] The 1990s saw the emergence of vehicle theft and organized crime, particularly by foreign gangs.[165] Drug trafficking, especially that of ya ba methamphetamine pills, is also chronic.[166][167]
According to police statistics, the most common complaint received by the Metropolitan Police Bureau in 2010 was housebreaking, with 12,347 cases. This was followed by 5,504 cases of motorcycle thefts, 3,694 cases of assault and 2,836 cases of embezzlement. Serious offences included 183 murders, 81 gang robberies, 265 robberies, 1 kidnapping and 9 arson cases. Offences against the state were by far more common, and included 54,068 drug-related cases, 17,239 cases involving prostitution and 8,634 related to gambling.[168] The Thailand Crime Victim Survey conducted by the Office of Justice Affairs of the Ministry of Justice found that 2.7 percent of surveyed households reported a member being victim of a crime in 2007. Of these, 96.1 percent were crimes against property, 2.6 percent were crimes against life and body, and 1.4 percent were information-related crimes.[169]
Political demonstrations and protests are common in Bangkok. The historic uprisings of 1973, 1976 and 1992 are infamously known for the deaths from military suppression. Most events since then have been peaceful, but the series of major protests since 2006 have often turned violent. Demonstrations during March–May 2010 ended in a crackdown in which 92 were killed, including armed and unarmed protesters, security forces, civilians and journalists. Terrorist incidents have also occurred in Bangkok, most notably the bombing in 2015 at the Erawan shrine, which killed 20, and also a series of bombings on the 2006–07 New Year's Eve.
Traffic accidents are a major hazard in Bangkok. There were 37,985 accidents in the city in 2010, resulting in 16,602 injuries and 456 deaths as well as 426.42 million baht in damages. However, the rate of fatal accidents is much lower than in the rest of Thailand. While accidents in Bangkok amounted to 50.9 percent of the entire country, only 6.2 percent of fatalities occurred in the city.[170] Another serious public health hazard comes from Bangkok's stray dogs. Up to 300,000 strays are estimated to roam the city's streets,[171] and dog bites are among the most common injuries treated in the emergency departments of the city's hospitals. Rabies is prevalent among the dog population, and treatment for bites pose a heavy public burden.[m]
Bangkok is faced with multiple problems—including congestion, and especially subsidence and flooding—which have raised the issue of moving the nation's capital elsewhere. The idea is not new: during World War II Prime Minister Plaek Phibunsongkhram planned unsuccessfully to relocate the capital to Phetchabun. In the 2000s, the Thaksin Shinawatra administration assigned the Office of the National Economic and Social Development Council (NESDC) to formulate a plan to move the capital to Nakhon Nayok province. The 2011 floods revived the idea of moving government functions from Bangkok. In 2017, the military government assigned NESDC to study the possibility of moving government offices from Bangkok to Chachoengsao province in the east.[173][174][175]
The city's formal international relations are managed by the International Affairs Division of the BMA. Its missions include partnering with other major cities through sister city or friendship agreements, participation and membership in international organizations, and pursuing cooperative activities with the many foreign diplomatic missions based in the city.[176]
Bangkok is a member of several international organizations and regional city government networks, including the Asian Network of Major Cities 21, the Japan-led Asian-Pacific City Summit, the C40 Cities Climate Leadership Group, the ESCAP-sponsored Regional Network of Local Authorities for Management of Human Settlements in Asia and Pacific (CITYNET), Japan's Council of Local Authorities for International Relations, the World Association of the Major Metropolises and Local Governments for Sustainability, among others.[176]
With its location at the heart of mainland Southeast Asia and as one of Asia's hubs of transportation, Bangkok is home to many international and regional organizations. Among others, Bangkok is the seat of the Secretariat of the UN Economic and Social Commission for Asia and the Pacific (ESCAP), as well as the Asia-Pacific regional offices of the Food and Agricultural Organization (FAO), the International Civil Aviation Organization (ICAO), the International Labour Organization (ILO), the International Organization for Migration (IOM), the International Telecommunication Union (ITU), the UN High Commission for Refugees (UNHCR), and the UN Children's Fund (UNICEF).[177]
Bangkok has made sister city or friendship agreements with:[178]



Mecca (/ˈmɛkə/; officially Makkah al-Mukarramah,[a] commonly shortened to Makkah[b]) is the capital of Mecca Province in the Hejaz region of western Saudi Arabia and considered the holiest city in Islam.[2] It is 70 km (43 mi) inland from Jeddah on the Red Sea, in a narrow valley 277 m (909 ft) above sea level. Its last recorded population was 1,578,722 in 2015.[3] Its estimated metro population in 2020 is 2.042 million, making it the third-most populated city in Saudi Arabia after Riyadh and Jeddah.[citation needed] Pilgrims more than triple this number every year during the Ḥajj pilgrimage, observed in the twelfth Hijri month of Dhūl-Ḥijjah.[citation needed]
Mecca is generally considered "the fountainhead and cradle of Islam".[4][5] Mecca is revered in Islam as the birthplace of the Islamic prophet Muhammad. The Hira cave atop the Jabal al-Nur ("Mountain of Light"), just outside the city, is where Muslims believe the Quran was first revealed to Muhammad.[6] Visiting Mecca for the Ḥajj is an obligation upon all able Muslims. The Great Mosque of Mecca, known as the Masjid al-Haram, is home to the Ka'bah, believed by Muslims to have been built by Abraham and Ishmael. It is one of Islam's holiest sites and the direction of prayer for all Muslims (qibla).[7]
Muslim rulers from in and around the region long tried to take the city and keep it in their control, and thus, much like most of the Hejaz region, the city has seen several regime changes. The city was most recently conquered in the Saudi conquest of Hejaz by Ibn Saud and his allies in 1925. Since then, Mecca has seen a tremendous expansion in size and infrastructure, with newer, modern buildings such as the Abraj Al Bait, the world's fourth-tallest building and third-largest by floor area[citation needed], towering over the Great Mosque. The Saudi government has also carried out the destruction of several historical structures and archaeological sites,[8] such as the Ajyad Fortress.[9][10][11] Non-Muslims are strictly prohibited from entering the city.[12][13]
Under the Saudi government, Mecca is governed by the Mecca Regional Municipality, a municipal council of 14 locally elected members headed by the mayor (called Amin in Arabic) appointed by the Saudi government. In 2015, the mayor of the city was Osama bin Fadhel Al-Barr;[14][15] as of January 2022[update], the mayor is Saleh Al-Turki.[16] The City of Mecca amanah, which constitutes Mecca and the surrounding region, is the capital of the Mecca Province, which includes the neighbouring cities of Jeddah and Ta'if, even though Jeddah is considerably larger in population compared to Mecca. The Provincial Governor of the province from 16 May 2007 was Prince Khalid bin Faisal Al Saud.[17]
Mecca has been referred to by many names. As with many Arabic words, its etymology is obscure.[18] Widely believed to be a synonym for Makkah, it is said to be more specifically the early name for the valley located therein, while Muslim scholars generally use it to refer to the sacred area of the city that immediately surrounds and includes the Ka'bah.[19][20]
The Quran refers to the city as Bakkah in Surah Al Imran (3), verse 96: "Indeed the first House [of worship], established for mankind was that at Bakkah". This is said to have been the name of the city at the time of Abraham (Ibrahim in Islamic tradition) and it is also transliterated as Baca, Baka, Bakah, Bakka, Becca and Bekka, among others.[21][22][23] It was a name for the city in the ancient world.[24]
Makkah is the official transliteration used by the Saudi government and is closer to the Arabic pronunciation.[25][26] The government adopted Makkah as the official spelling in the 1980s, but it is not universally known or used worldwide.[25] The full official name is Makkah al-Mukarramah (Arabic: مكة المكرمة, lit. 'Makkah the Honored').[25] Makkah is used to refer to the city in the Quran in Surah Al-Fath (48), verse 24.[18][27]
The word Mecca in English has come to be used to refer to any place that draws large numbers of people, and because of this some English-speaking Muslims have come to regard the use of this spelling for the city as offensive.[25] Nonetheless, Mecca is the familiar form of the English transliteration for the Arabic name of the city.
The consensus in academic scholarship has been that "Macoraba", the place mentioned in Arabia Felix by Claudius Ptolemy, is Mecca.[28] Some studies have questioned this association.[29] Many etymologies have been proposed: the traditional one is that it is derived from the Old South Arabian root M-K-R-B which means "temple".[29]
Another name used for Mecca in the Quran is at 6:92 where it is called Umm al-Qurā[30] (أُمّ ٱلْقُرَى‎, meaning "Mother of all Settlements").[27] The city has been called several other names in both the Quran and ahadith. Another name used historically for Mecca is Tihāmah.[31] According to Arab and Islamic tradition, another name for Mecca, Fārān, is synonymous with the Desert of Paran mentioned in the Old Testament at Genesis 21:21.[32] Arab and Islamic tradition holds that the wilderness of Paran, broadly speaking, is the Tihamah coastal plain and the site where Ishmael settled was Mecca.[32] Yaqut al-Hamawi, the 12th-century Syrian geographer, wrote that Fārān was "an arabized Hebrew word, one of the names of Mecca mentioned in the Torah."[33]
Another ancient name for the city was Macoraba.[24]
In 2010, Mecca and the surrounding area became an important site for paleontology with respect to primate evolution, with the discovery of a Saadanius fossil. Saadanius is considered to be a primate closely related to the common ancestor of the Old World monkeys and apes. The fossil habitat, near what is now the Red Sea in western Saudi Arabia, was a damp forest area between 28 million and 29 million years ago.[34] Paleontologists involved in the research hope to find further fossils in the area.[35]
The early history of Mecca is still largely disputed, as there are no unambiguous references to it in ancient literature prior to the rise of Islam.[36] The first unambiguous reference to Mecca in external literature occurs in 741 CE, in the Byzantine-Arab Chronicle, though here the author places the region in Mesopotamia rather than the Hejaz.[37]
The Greek historian Diodorus Siculus writes about Arabia in the 1st century BCE in his work Bibliotheca historica, describing a holy shrine: "And a temple has been set up there, which is very holy and exceedingly revered by all Arabians".[38] Claims have been made this could be a reference to the Ka'bah in Mecca. However, the geographic location Diodorus describes is located in northwest Arabia, around the area of Leuke Kome, within the former Nabataean Kingdom and the Roman province of Arabia Petraea.[39][40]
Ptolemy lists the names of 50 cities in Arabia, one going by the name of Macoraba. There has been speculation since 1646 that this could be a reference to Mecca.  Historically, there has been a general consensus in scholarship that Macoraba mentioned by Ptolemy in the 2nd century CE is indeed Mecca, but more recently, this has been questioned.[41][42] Bowersock favors the identity of the former, with his theory being that "Macoraba" is the word "Makkah" followed by the aggrandizing Aramaic adjective rabb (great). The Roman historian Ammianus Marcellinus also enumerated many cities of Western Arabia, most of which can be identified. According to Bowersock, he did mention Mecca as "Geapolis" or "Hierapolis", the latter one meaning "holy city" potentially referring to the sanctuary of the Kaaba.[43] Patricia Crone, from the Revisionist school of Islamic studies on the other hand, writes that "the plain truth is that the name Macoraba has nothing to do with that of Mecca [...] if Ptolemy mentions Mecca at all, he calls it Moka, a town in Arabia Petraea".[44]
Recent research suggests that "Mecca was small" and the population of Mecca at the time of Muhammad was around 550.[45]
Procopius' 6th century statement that the Ma'ad tribe possessed the coast of western Arabia between the Ghassanids and the Himyarites of the south supports the Arabic sources tradition that associates Quraysh as a branch of the Ma'add and Muhammad as a direct descendant of Ma'ad ibn Adnan.[46][47]
Historian Patricia Crone has cast doubt on the claim that Mecca was a major historical trading outpost.[48][49] However, other scholars such as Glen W. Bowersock disagree and assert that Mecca was a major trading outpost.[50][51] Crone later on disregarded some of her theories.[52] She argues that Meccan trade relied on skins, hides, manufactured leather goods, clarified butter, Hijazi woollens, and camels. She suggests that most of these goods were destined for the Roman army, which is known to have required colossal quantities of leather and hides for its equipment.
Mecca is mentioned in the following early Quranic manuscripts:
The earliest Muslim inscriptions are from the Mecca-Ta'if area.[53]
Islamic narrative
In the Islamic view, the beginnings of Mecca are attributed to the Biblical figures, Abraham, Hagar and Ishmael. The civilization of Mecca is believed to have started after Ibrāhīm (Abraham) left his son Ismāʿīl (Ishmael) and wife Hājar (Hagar) in the valley at Allah's command.[citation needed] Some people from the Yemeni tribe of Jurhum settled with them, and Isma'il reportedly married two women, one after divorcing the first, on Ibrahim's advice. At least one man of the Jurhum helped Ismāʿīl and his father to construct or according to Islamic narratives, reconstruct, the Ka'bah ('Cube'),[54][19][55] which would have social, religious, political and historical implications for the site and region.[56][57]
Muslims see the mention of a pilgrimage at the Valley of Baca in the Old Testament chapter Psalm 84:3–6 as a reference to Mecca, similar to the Quran at Surah  3:96 In the Sharḥ al-Asāṭīr, a commentary on the Samaritan midrashic chronology of the Patriarchs, of unknown date but probably composed in the 10th century CE, it is claimed that Mecca was built by the sons of Nebaioth, the eldest son of Ismāʿīl or Ishmael.[58][59][60]
Thamudic inscriptions
Some Thamudic inscriptions which were discovered in the south Jordan contained names of some individuals such as ʿAbd Mekkat (عَبْد مَكَّة‎, "Servant of Mecca").[61]
There were also some other inscriptions which contained personal names such as Makki (مَكِّي, "Makkahn"), but Jawwad Ali from the University of Baghdad suggested that there's also a probability of a tribe named "Makkah".[62]
Sometime in the 5th century, the Ka'bah was a place of worship for the deities of Arabia's pagan tribes. Mecca's most important pagan deity was Hubal, which had been placed there by the ruling Quraish tribe.[63][64] and remained until the Conquest of Mecca by Muhammad.[citation needed] In the 5th century, the Quraish took control of Mecca, and became skilled merchants and traders. In the 6th century, they joined the lucrative spice trade, since battles elsewhere were diverting trade routes from dangerous sea routes to more secure overland routes. The Byzantine Empire had previously controlled the Red Sea, but piracy had been increasing.[citation needed] Another previous route that ran through the Persian Gulf via the Tigris and Euphrates rivers was also being threatened by exploitations from the Sassanid Empire, and was being disrupted by the Lakhmids, the Ghassanids, and the Roman–Persian Wars. Mecca's prominence as a trading center also surpassed the cities of Petra and Palmyra.[65][66] The Sassanids however did not always pose a threat to Mecca, as in 575 CE they protected it from a Yemeni invasion, led by its Christian leader Abraha. The tribes of southern Arabia asked the Persian king Khosrau I for aid, in response to which he came south to Arabia with foot-soldiers and a fleet of ships near Mecca.[67]
By the middle of the 6th century, there were three major settlements in northern Arabia, all along the south-western coast that borders the Red Sea, in a habitable region between the sea and the Hejaz mountains to the east. Although the area around Mecca was completely barren, it was the wealthiest of the three settlements with abundant water from the renowned Zamzam Well and a position at the crossroads of major caravan routes.[68]
The harsh conditions and terrain of the Arabian peninsula meant a near-constant state of conflict between the local tribes, but once a year they would declare a truce and converge upon Mecca in an annual pilgrimage. Up to the 7th century, this journey was intended for religious reasons by the pagan Arabs to pay homage to their shrine, and to drink Zamzam. However, it was also the time each year that disputes would be arbitrated, debts would be resolved, and trading would occur at Meccan fairs. These annual events gave the tribes a sense of common identity and made Mecca an important focus for the peninsula.[69]
The Year of the Elephant (570 CE)
The "Year of the Elephant" is the name in Islamic history for the year approximately equating to 570–572 CE, when, according to Islamic sources such as Ibn Ishaq, Abraha descended upon Mecca, riding an elephant, with a large army after building a cathedral at San'aa, named al-Qullays in honor of the Negus of Axum. It gained widespread fame, even gaining attention from the Byzantine Empire.[70] Abraha attempted to divert the pilgrimage of the Arabs from the Ka'bah to al-Qullays, effectively converting them to Christianity. According to Islamic tradition, this was the year of Muhammad's birth.[70] Abraha allegedly sent a messenger named Muhammad ibn Khuza'i to Mecca and Tihamah with a message that al-Qullays was both much better than other houses of worship and purer, having not been defiled by the housing of idols.[70] When Muhammad ibn Khuza'i got as far as the land of Kinana, the people of the lowland, knowing what he had come for, sent a man of Hudhayl called ʿUrwa bin Hayyad al-Milasi, who shot him with an arrow, killing him. His brother Qays who was with him, fled to Abraha and told him the news, which increased his rage and fury and he swore to raid the Kinana tribe and destroy the Ka'bah. Ibn Ishaq further states that one of the men of the Quraysh tribe was angered by this, and going to Sana'a, entering the church at night and defiling it; widely assumed to have done so by defecating in it.[71][72]
Abraha marched upon the Ka'bah with a large army, which included one or more war elephants, intending to demolish it. When news of the advance of his army came, the Arab tribes of Quraysh, Kinanah, Khuza'a and Hudhayl united in the defense of the Ka'bah and the city. A man from the Himyarite Kingdom was sent by Abraha to advise them that Abraha only wished to demolish the Ka'bah and if they resisted, they would be crushed. Abdul Muttalib told the Meccans to seek refuge in the hills while he and some members of the Quraysh remained within the precincts of the Kaaba. Abraha sent a dispatch inviting Abdul-Muttalib to meet with Abraha and discuss matters. When Abdul-Muttalib left the meeting he was heard saying: "The Owner of this House is its Defender, and I am sure he will save it from the attack of the adversaries and will not dishonor the servants of His House."[73][74]
Abraha eventually attacked Mecca. However, the lead elephant, known as Mahmud,[75] is said to have stopped at the boundary around Mecca and refused to enter. It has been theorized that an epidemic such as by smallpox could have caused such a failed invasion of Mecca.[76] The reference to the story in Quran is rather short. According to the 105th Surah of the Quran, Al-Fil, the next day, a dark cloud of small birds sent by Allah appeared. The birds carried small rocks in their beaks, and bombarded the Ethiopian forces, and smashed them to a state like that of eaten straw.[77]
Economy
Camel caravans, said to have first been used by Muhammad's great-grandfather, were a major part of Mecca's bustling economy. Alliances were struck between the merchants in Mecca and the local nomadic tribes, who would bring goods – leather, livestock, and metals mined in the local mountains – to Mecca to be loaded on the caravans and carried to cities in Shaam and Iraq.[78] Historical accounts also provide some indication that goods from other continents may also have flowed through Mecca. Goods from Africa and the Far East passed through en route to Syria including spices, leather, medicine, cloth, and slaves; in return Mecca received money, weapons, cereals and wine, which in turn were distributed throughout Arabia.[citation needed] The Meccans signed treaties with both the Byzantines and the Bedouins, and negotiated safe passages for caravans, giving them water and pasture rights. Mecca became the center of a loose confederation of client tribes, which included those of the Banu Tamim. Other regional powers such as the Abyssinians, Ghassanids, and Lakhmids were in decline leaving Meccan trade to be the primary binding force in Arabia in the late 6th century.[69]
Muhammad was born in Mecca in 570 CE, and thus Islam has been inextricably linked with it ever since. He was born into the faction of Banu Hashim in the ruling tribe of Quraysh. It was in the nearby mountain cave of Hira on Jabal al-Nour that Muhammad began receiving divine revelations from God through the archangel Jibreel in 610 CE, according to Islamic tradition. Advocating his form of Abrahamic monotheism against Meccan paganism, and after enduring persecution from the pagan tribes for 13 years, Muhammad emigrated to Medina (hijrah) in 622 CE with his companions, the Muhajirun, to Yathrib (later renamed Medina). The conflict between the Quraysh and the Muslims is accepted to have begun at this point. Overall, Meccan efforts to annihilate Islam failed and proved to be costly and unsuccessful.[citation needed] During the Battle of the Trench in 627 CE, the combined armies of Arabia were unable to defeat Muhammad's forces (as the trench surrounding Muhammad's forces protected them from harm and a storm was sent to breach the Quraysh tribe).[79]
In 628 CE, Muhammad and his followers wanted to enter Mecca for pilgrimage, but were blocked by the Quraysh. Subsequently, Muslims and Meccans entered into the Treaty of Hudaybiyyah, whereby the Quraysh and their allies promised to cease fighting Muslims and their allies and promised that Muslims would be allowed into the city to perform the pilgrimage the following year. It was meant to be a ceasefire for 10 years; however, just two years later, the Banu Bakr, allies of the Quraish, violated the truce by slaughtering a group of the Banu Khuza'ah, allies of the Muslims. Muhammad and his companions, now 10,000 strong, marched into Mecca and conquered the city. The pagan imagery was destroyed by Muhammad's followers and the location Islamized and rededicated to the worship of Allah alone. Mecca was declared the holiest site in Islam ordaining it as the center of Muslim pilgrimage (Hajj), one of the Islamic faith's Five Pillars.
Muhammad then returned to Medina, after assigning 'Akib ibn Usaid as governor of the city. His other activities in Arabia led to the unification of the Arabian Peninsula under the banner of Islam.[65][79] Muhammad passed away in 632 CE. Within the next few hundred years, the area under the banner of Islam stretched from North Africa into Asia and parts of Europe. As the Islamic realm grew, Mecca continued to attract pilgrims from all across the Muslim world and beyond, as Muslims came to perform the annual Hajj pilgrimage. Mecca also attracted a year-round population of scholars, pious Muslims who wished to live close to the Kaaba, and local inhabitants who served the pilgrims. Due to the difficulty and expense of the Hajj, pilgrims arrived by boat at Jeddah, and came overland, or joined the annual caravans from Syria or Iraq.[citation needed]
Mecca was never the capital of any of the Islamic states. Muslim rulers did contribute to its upkeep, such as during the reigns of 'Umar (r. 634–644 CE) and 'Uthman ibn Affan (r. 644–656 CE) when concerns of flooding caused the caliphs to bring in Christian engineers to build barrages in the low-lying quarters and construct dykes and embankments to protect the area around the Kaaba.[65]
Muhammad's return to Medina shifted the focus away from Mecca and later even further away when 'Ali, the fourth caliph, took power and chose Kufa as his capital. The Umayyad Caliphate moved the capital to Damascus in Syria and the Abbasid Caliphate to Baghdad, in modern-day Iraq, which remained the center of the Islamic Empire for nearly 500 years. Mecca re-entered Islamic political history during the Second Fitna, when it was held by Abdullah ibn az-Zubayr and the Zubayrids.[citation needed] The city was twice besieged by the Umayyads in 683 CE and 692 CE, and for some time thereafter, the city figured little in politics, remaining a city of devotion and scholarship governed by various other factions. In 930 CE, Mecca was attacked and sacked by Qarmatians, a millenarian Shi'a Isma'ili Muslim sect led by Abū-Tāhir Al-Jannābī and centered in eastern Arabia.[80] The Black Death pandemic hit Mecca in 1349 CE.[81]
One of the most famous travelers to Mecca in the 14th century was a Moroccan scholar and traveler, Ibn Battuta. In his rihla (account), he provides a vast description of the city. Around the year 1327 CE or 729 AH, Ibn Battuta arrived at the holy city. Immediately, he says, it felt like a holy sanctuary, and thus he started the rites of the pilgrimage. He remained in Mecca for three years and left in 1330 CE. During his second year in the holy city, he says his caravan arrived "with a great quantity of alms for the support of those who were staying in Mecca and Medina". While in Mecca, prayers were made for (not to) the King of Iraq and also for Salaheddin al-Ayyubi, Sultan of Egypt and Syria at the Ka'bah. Battuta says the Ka'bah was large, but was destroyed and rebuilt smaller than the original and that it contained images of angels and prophets including Jesus (Isa in Islamic tradition), his mother Mary (Maryam in Islamic tradition), and many others. Battuta describes the Ka'bah as an important part of Mecca due to the fact that many people make the pilgrimage to it. Battuta describes the people of the city as being humble and kind, and also willing to give a part of everything they had to someone who had nothing. The inhabitants of Mecca and the village itself, he says, were very clean. There was also a sense of elegance to the village.[82]
Under the OttomansIn 1517, the then Sharif of Mecca, Barakat bin Muhammad, acknowledged the supremacy of the Ottoman Caliph but retained a great degree of local autonomy.[83] In 1803 the city was captured by the First Saudi State,[84] which held Mecca until 1813, destroying some of the historic tombs and domes in and around the city. The Ottomans assigned the task of bringing Mecca back under Ottoman control to their powerful Khedive (viceroy) and Wali of Egypt, Muhammad Ali Pasha. Muhammad Ali Pasha successfully returned Mecca to Ottoman control in 1813. In 1818, the Saud were defeated again but survived and founded the Second Saudi State that lasted until 1891 and led on to the present country of Saudi Arabia. In 1853, Sir Richard Francis Burton undertook the Muslim pilgrimage to Mecca and Medina disguised as a Muslim. Although Burton was certainly not the first non-Muslim European to make the Hajj (Ludovico di Varthema did this in 1503),[85] his pilgrimage remains one of the most famous and documented of modern times. Mecca was regularly hit by cholera outbreaks. Between 1830 and 1930, cholera broke out among pilgrims at Mecca 27 times.[86]
Hashemite Revolt and subsequent control by the Sharifate of Mecca
In World War I, the Ottoman Empire was at war with the Allies. It had successfully repulsed an attack on Istanbul in the Gallipoli campaign and on Baghdad in the Siege of Kut. The British intelligence agent T.E. Lawrence conspired with the Ottoman governor, Hussain bin Ali, the Sharif of Mecca to revolt against the Ottoman Empire and it was the first city captured by his forces in the 1916 Battle of Mecca. Sharif's revolt proved a turning point of the war on the eastern front. Hussein declared a new state, the Kingdom of Hejaz, declaring himself the Sharif of the state and Mecca his capital. News reports in November 1916 via contact in Cairo with returning Hajj pilgrims, stated that with the Ottoman Turkish authorities gone, the Hajj of 1916 was free of the previous massive extortion and monetary demands made by the Turks who were agents of the Ottoman government.[87]
Saudi Arabian conquest and modern history
Following the 1924 Battle of Mecca, the Sharif of Mecca was overthrown by the Saud family, and Mecca was incorporated into Saudi Arabia.[88] Under Saudi rule, much of the historic city has been demolished as a result of the Saudi government fearing these sites might become sites of association in worship besides Allah (shirk). The city has been expanded to include several towns previously considered to be separate from the holy city and now is just a few kilometers outside the main sites of the Hajj, Mina, Muzdalifah and Arafat. Mecca is not served by any airport, due to concerns about the city's safety. It is instead served by the King Abdulaziz International Airport in Jeddah (approx. 70 km away) internationally and the Ta'if Regional Airport (approx. 120 km away) for domestic flights.[citation needed]
The city today is at the junction of the two most important highways in all of the Saudi Arabian highway system, Highway 40, which connects the city to Jeddah in the west and the capital, Riyadh and Dammam in the east and Highway 15, which connects it to Medina, Tabuk and onward to Jordan in the north and Abha and Jizan in the south. The Ottomans had planned to extend their railway network to the holy city, but were forced to abandon this plan due to their entry into the First World War. This plan was later carried out by the Saudi government, which connected the two holy cities of Medina and Mecca with the modern Haramain high-speed railway system which runs at 300 km/h (190 mph) and connects the two cities via Jeddah, King Abdulaziz International Airport and King Abdullah Economic City near Rabigh within two hours.[citation needed]
The haram area of Mecca, in which the entry of non-Muslims is forbidden, is much larger than that of Medina.
1979 Grand Mosque seizureOn 20 November 1979, two hundred armed dissidents led by Juhayman al-Otaibi, seized the Grand Mosque, claiming the Saudi royal family no longer represented pure Islam and that the Masjid al-Haram and the Ka'bah, must be held by those of true faith. The rebels seized tens of thousands of pilgrims as hostages and barricaded themselves in the mosque. The siege lasted two weeks, and resulted in several hundred deaths and significant damage to the shrine, especially the Safa-Marwah gallery. A multinational force was finally able to retake the mosque from the dissidents.[89] Since then, the Grand Mosque has been expanded several times, with many other expansions being undertaken in the present day.
Destruction of Islamic heritage sitesUnder Saudi rule, it has been estimated that since 1985, about 95% of Mecca's historic buildings, most over a thousand years old, have been demolished.[9][90] It has been reported that there are now fewer than 20 structures remaining in Mecca that date back to the time of Muhammad. Some important buildings that have been destroyed include the house of Khadijah, the wife of Muhammad, the house of Abu Bakr, Muhammad's birthplace and the Ottoman-era Ajyad Fortress.[91] The reason for much of the destruction of historic buildings has been for the construction of hotels, apartments, parking lots, and other infrastructure facilities for Hajj pilgrims.[90][92]
Incidents during pilgrimageMecca has been the site of several incidents and failures of crowd control because of the large numbers of people who come to make the Hajj.[93][94][95] For example, on 2 July 1990, a pilgrimage to Mecca ended in tragedy when the ventilation system failed in a crowded pedestrian tunnel and 1,426 people were either suffocated or trampled to death in a stampede.[96] On 24 September 2015, 700 pilgrims were killed in a stampede at Mina during the stoning-the-Devil ritual at Jamarat.[97]
Mecca holds an important place in Islam and is considered the holiest city in all branches of the religion. The city derives its importance from the role it plays in the Hajj and 'Umrah and for its status as the birthplace of the Prophet Muhammad.
The Masjid al-Haram is the site of two of the most important rites of both the Hajj and of the Umrah, the circumambulation around the Ka'bah (tawaf) and the walking between the two mounts of Safa and Marwa (sa'ee). The masjid is also the site of the Zamzam Well. According to Islamic tradition, a prayer in the masjid is equal to 100,000 prayers in any other masjid around the world.[98]
There is a difference of opinion between Islamic scholars upon who first built the Ka'bah, some believe it was built by the angels while others believe it was built by Adam. Regardless, it was built several times before reaching its current state. The Ka'bah is also the common direction of prayer (qibla) for all Muslims. The surface surrounding the Ka'bah on which Muslims circumambulate it is known as the Mataf.
The Black Stone is a stone, considered by scientists to be a meteorite or of similar origin and believed by Muslims to be of divine origin. It is set in the eastern corner of the Ka’bah and it is Sunnah to touch and kiss the stone. The area around the stone is generally always crowded and guarded by policemen to ensure the pilgrims' safety. In Islamic tradition, the stone was sent down from Jannah (Paradise) and used to build the Ka'bah. It used to be a white stone (and was whiter than milk). Because of the worldly sins of man, it slowly changed color to black over the years after it was brought down to Earth.
This is the stone that Ibrahim (Abraham) stood on to build the higher parts of the Ka'bah. It contains two footprints that are comparatively larger than average modern-day human feet. The stone is raised and housed in a golden hexagonal chamber beside the Ka'bah on the Mataf plate.

Muslims believe that in the divine revelation to Muhammad, the Quran, Allah describes the mountains of Safa and Marwah as symbols of His divinity. Walking between the two mountains seven times, 4 times from Safa to Marwah and 3 times from Marwah interchangeably, is considered a mandatory pillar (rukn) of 'Umrah.The Hajj pilgrimage, also called the greater pilgrimage, attracts millions of Muslims from all over the world and almost triples Mecca's population for one week in the twelfth and final Islamic month of Dhu al-Hijjah. In 2019, the Hajj attracted 2,489,406 pilgrims to the holy city.[99] The 'Umrah, or the lesser pilgrimage, can be done at anytime during the year.  Every adult, healthy Muslim who has the financial and physical capacity to travel to Mecca must perform the Hajj at least once in a lifetime. Umrah, the lesser pilgrimage, is not obligatory, but is recommended in the Quran.[100] In addition to the Masjid al-Haram, pilgrims also must visit the nearby towns of Mina/Muna, Muzdalifah and Mount Arafat for various rituals that are part of the Hajj.
This is a mountain believed by Muslims to have been the place where Muhammad spent his time away from the bustling city of Mecca in seclusion.[101][102] The mountain is located on the eastern entrance of the city and is the highest point in the city at 642 meters (2,106 feet).
Situated atop Jabal an-Nur, this is the cave where Muslims believe Muhammad received the first revelation from Allah through the archangel Gabriel (Jibril in Islamic tradition) at the age of 40.[101][102]
 Mecca is located in the Hejaz region, a 200 km (124 mi) wide strip of mountains separating the Nafud desert from the Red Sea. The city is situated in a valley with the same name around 70 km (44 mi) east of the port city of Jeddah. Mecca is one of the lowest cities in elevation in the Hejaz region, located at an elevation of 277 m (909 ft) above sea level at 21º23' north latitude and 39º51' east longitude. Mecca is divided into 34 districts.
The city centers on the al-Haram area, which contains the Masjid al-Haram. The area around the mosque is the old city and contains the most famous district of Mecca, Ajyad. The main street that runs to al-Haram is the Ibrahim al-Khalil Street, named after Ibrahim. Traditional, historical homes built of local rock, two to three stories long are still present within the city's central area, within view of modern hotels and shopping complexes. The total area of modern Mecca is over 1,200 km2 (460 sq mi).[103]
Mecca is at an elevation of 277 m (909 ft) above sea level, and approximately 70 km (44 mi) inland from the Red Sea.[68] It is one of the lowest in the Hejaz region. Although some mountain peaks in Mecca reach 1,000m in height.
The city center lies in a corridor between mountains, which is often called the "Hollow of Mecca". The area contains the valley of al-Taneem, the valley of Bakkah and the valley of Abqar.[65][104] This mountainous location has defined the contemporary expansion of the city.
Due to Mecca's climatic conditions water scarcity has been an issue throughout its history. In pre-modern Mecca, the city used a few chief sources of water. Among them were local wells, such as the Zamzam Well, that produced generally brackish water. Finding a sustainable water source to supply Mecca's permanent population and the large number of annual pilgrims was an undertaking that began in the Abbasid era under the auspices of Zubayda, the wife of the caliph Harun ar-Rashid.[c] She donated funds for the deepening of Zamzam Well and funded a massive construction project likely costing 1.75 Million gold dinars. The project encompassed the construction of an underground aqueduct from the Arabic: عين حنين, romanized: ʿAyn Ḥunayn, lit. 'Spring of Hunayn' and smaller water sources in the area to Mecca in addition to the construction of a waterworks on Mount Arafat called Arabic: عين زبيدة, romanized: ʿAyn Zubayda, lit. 'Spring of Zubayda' using a separate conduit to connect it to Mecca and the Masjid al-Haram. Over time however the system deteriorated and failed to fulfil its function. Thus in 1245 CE, 1361 CE, 1400 CE, 1474 CE, and 1510 CE different rulers invested into extensive repairs of the system. In 1525 CE due to the system's troubles persisting however the Ottoman sultan Suleiman the Magnificent began a construction project to rebuild the aqueduct in its entirety, the project took until 1571 CE to be completed. Its water quality was greatly lacking during the 19th century until a restoration and cleaning project by Osman Pasha began.[105]
Another source which sporadically provided water was rainfall which was stored by the people in small reservoirs or cisterns. According to al-Kurdī, there had been 89 floods by 1965. In the last century, the most severe flood was that of 1942. Since then, dams have been built to ameliorate this problem.[104]
In the modern day water treatment plants and desalination facilities have been constructed and are being constructed to provide suitable amounts of water fit for human consumption to the city.[106][107]

Mecca features a hot desert climate (Köppen: BWh), in three different plant hardiness zones: 10, 11 and 12.[108] Like most Saudi Arabian cities, Mecca retains warm to hot temperatures even in winter, which can range from 19 °C (66 °F) at night to 30 °C (86 °F) in the afternoon. Summer temperatures are extremely hot and consistently break the 40 °C (104 °F) mark in the afternoon, dropping to 30 °C (86 °F) in the evening, but humidity remains relatively low, at 30–40%. Rain usually falls in Mecca in small amounts scattered between November and January, with heavy thunderstorms also common during the winter.The Meccan economy has been heavily dependent on pilgrimages coming for Umrah and Hajj.[111] Income generated through pilgrims not only powers the Meccan economy but has historically had far-reaching effects on the economy of the entire Arabian Peninsula. The income was generated in a number of ways. One method was taxing the pilgrims. Taxes were especially increased during the Great Depression, and many of these taxes existed to as late as 1972. Another way the Hajj generates income is through services to pilgrims. For example, the Saudi flag carrier, Saudia, generates 12% of its income from the pilgrimage. Fares paid by pilgrims to reach Mecca by land also generate income; as do the hotels and lodging companies that house them.[104] The city takes in more than $100 million, while the Saudi government spends about $50 million on services for the Hajj. There are some industries and factories in the city, but Mecca no longer plays a major role in Saudi Arabia's economy, which is mainly based on oil exports.[112] The few industries operating in Mecca include textiles, furniture, and utensils. The majority of the economy is service-oriented.
Nevertheless, many industries have been set up in Mecca. Various types of enterprises that have existed since 1970 in the city include corrugated iron manufacturing, copper extraction, carpentry, upholstery, bakeries, farming and banking.[104] The city has grown substantially in the 20th and 21st centuries, as the convenience and affordability of jet travel has increased the number of pilgrims participating in the Hajj. Thousands of Saudis are employed year-round to oversee the Hajj and staff the hotels and shops that cater to pilgrims; these workers in turn have increased the demand for housing and services. The city is now ringed by freeways, and contains shopping malls and skyscrapers.[113]
Formal education started to be developed in the late Ottoman period continuing slowly into Hashemite times. The first major attempt to improve the situation was made by a Jeddah merchant, Muhammad ʿAlī Zaynal Riḍā, who founded the Madrasat al-Falāḥ in Mecca in 1911–12 that cost £400,000.[104] The school system in Mecca has many public and private schools for both males and females. As of 2005, there were 532 public and private schools for males and another 681 public and private schools for female students.[114] The medium of instruction in both public and private schools is Arabic with emphasis on English as a second language, but some private schools founded by foreign entities such as International schools use the English language as the medium of instruction. Some of these are coeducational while other schools are not. For higher education, the city has only one university, Umm Al-Qura University, which was established in 1949 as a college and became a public university in 1981.
Healthcare is provided by the Saudi government free of charge to all pilgrims. There are ten main hospitals in Mecca:[115]
There are also many walk-in clinics available for both residents and pilgrims. Several temporary clinics are set up during the Hajj to tend to wounded pilgrims.
Mecca's culture has been affected by the large number of pilgrims that arrive annually, and thus boasts a rich cultural heritage. As a result of the vast numbers of pilgrims coming to the city each year, Mecca has become by far the most diverse city in the Muslim world.[citation needed]
Al Baik, a local fast-food chain, is very popular among pilgrims and locals alike. Until 2018, it was available only in Mecca, Medina and Jeddah, and traveling to Jeddah just to get a taste of the fried chicken was common.
In pre-modern Mecca, the most common sports were impromptu wrestling and foot races.[104] Football is now the most popular sport in Mecca and the kingdom, and the city hosts some of the oldest sport clubs in Saudi Arabia such as Al Wahda FC (established in 1945). King Abdulaziz Stadium is the largest stadium in Mecca with a capacity of 38,000.[116]
Mecca is very densely populated. Most long-term residents live in the Old City, the area around the Great Mosque and many work to support pilgrims, known locally as the Hajj industry. 'Iyad Madani, the Saudi Arabian Minister for Hajj, was quoted saying, "We never stop preparing for the Hajj."[117]
Year-round, pilgrims stream into the city to perform the rites of 'Umrah, and during the last weeks of eleventh Islamic month, Dhu al-Qi'dah, on average 2–4 million Muslims arrive in the city to take part in the rites known as Hajj.[118] Pilgrims are from varying ethnicities and backgrounds, mainly South and Southeast Asia, Europe and Africa. Many of these pilgrims have remained and become residents of the city. The Burmese are an older, more established community who number roughly 250,000.[119] Adding to this, the discovery of oil in the past 50 years has brought hundreds of thousands of working immigrants.
Non-Muslims are not permitted to enter Mecca under Saudi law,[12] and using fraudulent documents to do so may result in arrest and prosecution.[120] The prohibition extends to Ahmadis, as they are considered non-Muslims.[121] Nevertheless, many non-Muslims and Ahmadis have visited the city as these restrictions are loosely enforced. The first such recorded example of a non-Muslim entering the city is that of Ludovico di Varthema of Bologna in 1503.[122] Guru Nanak, the founder of Sikhism, is said to have visited Mecca[123] in December 1518.[124] One of the most famous was Richard Francis Burton,[125] who traveled as a Qadiriyya Sufi from Afghanistan in 1853.
Mecca Province is the only province where expatriates outnumber Saudis.[126]
Adorning the southern facade of the Masjid al-Haram, the Abraj al-Bait Complex, which towers over the Great Mosque, is a seven-building complex with the central clock tower having a length of 601 m (1,972 feet), making it the world's fourth-tallest building. All seven buildings in the complex also form the third-largest building by floor area.
The Mecca Gate, known popularly as the Quran Gate, on the western entrance of the city, or from Jeddah. Located on Highway 40, it marks the boundary of the Haram area where non-Muslims are prohibited from entering. The gate was designed in 1979 by an Egyptian architect, Samir Elabd, for the architectural firm IDEA Center. The structure is that of a book, representing the Quran, sitting on a rehal, or bookrest.[127]
The first press was brought to Mecca in 1885 by Osman Nuri Pasha, an Ottoman Wāli. During the Hashemite period, it was used to print the city's official gazette, Al Qibla. The Saudi regime expanded this press into a larger operation, introducing the new Saudi official gazette of Mecca, Umm al-Qurā.[104] Mecca also has its own paper owned by the city, Al Nadwa. However, other Saudi newspapers are also provided in Mecca such as the Saudi Gazette, Al Madinah, Okaz and Al Bilad, in addition to other international newspapers.
Telecommunications in the city were emphasized early under the Saudi reign. King Abdulaziz pressed them forward as he saw them as a means of convenience and better governance. While under Hussein bin Ali, there were about 20 public telephones in the entire city; in 1936, the number jumped to 450, totaling about half the telephones in the country. During that time, telephone lines were extended to Jeddah and Ta’if, but not to the capital, Riyadh. By 1985, Mecca, like other Saudi cities, possessed modern telephone, telex, radio and television communications.[104] Many television stations serving the city area include Saudi TV1, Saudi TV2, Saudi TV Sports, Al-Ekhbariya, Arab Radio and Television Network and various cable, satellite and other specialty television providers.
Limited radio communication was established within the Kingdom under the Hashemites. In 1929, wireless stations were set up in various towns in the region, creating a network that would become fully functional by 1932. Soon after World War II, the existing network was greatly expanded and improved. Since then, radio communication has been used extensively in directing the pilgrimage and addressing the pilgrims. This practice started in 1950, with the initiation of broadcasts on the Day of 'Arafah (9 Dhu al-Hijjah), and increased until 1957, at which time Radio Makkah became the most powerful station in the Middle East at 50 kW. Later, power was increased 9-fold to 450 kW. Music was not immediately broadcast, but gradually folk music was introduced.[104]
The only airport near the city is the Mecca East airport, which is not active. Mecca is primarily served by King Abdulaziz International Airport in Jeddah for international and regional connections and Ta'if Regional Airport for regional connections. To cater the large number of Hajj pilgrims, Jeddah Airport has Hajj Terminal, specifically for use in the Hajj season, which can accommodate 47 planes simultaneously and can receive 3,800 pilgrims per hour during the Hajj season.[128]
Mecca, similar to Medina, lies at the junction of two of the most important highways in Saudi Arabia, Highway 40, connecting it to the important port city of Jeddah in the west and the capital of Riyadh and the other major port city, Dammam, in the east. The other, Highway 15, connects Mecca to the other holy Islamic city of Medina approximately 400 km (250 mi) in the north and onward to Tabuk and Jordan. While in the south, it connects Mecca to Abha and Jizan.[129][130] Mecca is served by four ring roads, and these are very crowded compared to the three ring roads of Medina.
Mecca also has many tunnels.[131]
Al Masha'er Al Muqaddassah Metro
The Al Masha'er Al Muqaddassah Metro is a metro line in Mecca opened on 13 November 2010.[132] The 18.1-kilometer (11.2-mile) elevated metro transports pilgrims to the holy sites of 'Arafat, Muzdalifah and Mina in the city to reduce congestion on the road and is only operational during the Hajj season.[133] It consists of nine stations, three in each of the aforementioned towns.
Mecca Metro
The Mecca Metro, officially known as Makkah Mass Rail Transit, is a planned four-line metro system for the city.[134] This will be in addition to[134] the Al Masha'er Al Muqaddassah Metro which carries pilgrims.
In 2018, a high speed intercity rail line, part of the Haramain High Speed Rail Project, named the Haramain high-speed railway line entered operation, connecting the holy cities of Mecca and Medina together via Jeddah, King Abdulaziz International Airport and King Abdullah Economic City in Rabigh.[135][136] The railway consists of 35 electric trains and is capable of transporting 60 million passengers annually. Each train can achieve speeds of up to 300 kmh (190 mph), traveling a total distance of 450 km (280 mi), reducing the travel time between the two cities to less than two hours.[137][136]

90°N 0°E﻿ / ﻿90°N 0°E﻿ / 90; 0


The Arctic Ocean is the smallest and shallowest of the world's five major oceans.[1] It spans an area of approximately 14,060,000 km2 (5,430,000 sq mi) and is known as one of the coldest of oceans. The International Hydrographic Organization (IHO) recognizes it as an ocean, although some oceanographers call it the Arctic Mediterranean Sea.[2] It has been also been described as an estuary of the Atlantic Ocean.[3][4] It is also seen as the northernmost part of the all-encompassing World Ocean.
The Arctic Ocean includes the North Pole region in the middle of the Northern Hemisphere and extends south to about 60°N. The Arctic Ocean is surrounded by Eurasia and North America, and the borders follow topographic features: the Bering Strait on the Pacific side and the Greenland Scotland Ridge on the Atlantic side. It is mostly covered by sea ice throughout the year and almost completely in winter. The Arctic Ocean's surface temperature and salinity vary seasonally as the ice cover melts and freezes;[5] its salinity is the lowest on average of the five major oceans, due to low evaporation, heavy fresh water inflow from rivers and streams, and limited connection and outflow to surrounding oceanic waters with higher salinities. The summer shrinking of the ice has been quoted at 50%.[1] The US National Snow and Ice Data Center (NSIDC) uses satellite data to provide a daily record of Arctic sea ice cover and the rate of melting compared to an average period and specific past years, showing a continuous decline in sea ice extent.[6] In September 2012, the Arctic ice extent reached a new record minimum. Compared to the average extent (1979–2000), the sea ice had diminished by 49%.[7]
Human habitation in the North American polar region goes back at least 17,000–50,000 years, during the Wisconsin glaciation. At this time, falling sea levels allowed people to move across the Bering land bridge that joined Siberia to northwestern North America (Alaska), leading to the Settlement of the Americas.[8]
Early Paleo-Eskimo groups included the Pre-Dorset (c. 3200–850 BC); the Saqqaq culture of Greenland (2500–800 BC); the Independence I and Independence II cultures of northeastern Canada and Greenland (c. 2400–1800 BC and c. 800–1 BC); and the Groswater of Labrador and Nunavik. The Dorset culture spread across Arctic North America between 500 BC and AD 1500. The Dorset were the last major Paleo-Eskimo culture in the Arctic before the migration east from present-day Alaska of the Thule, the ancestors of the modern Inuit.[9]
The Thule Tradition lasted from about 200 BC to AD 1600, arising around the Bering Strait and later encompassing almost the entire Arctic region of North America. The Thule people were the ancestors of the Inuit, who now live in Alaska, Northwest Territories, Nunavut, northern Quebec, Labrador and Greenland.[10]
For much of European history, the north polar regions remained largely unexplored and their geography conjectural. Pytheas of Massilia recorded an account of a journey northward in 325 BC, to a land he called "Eschate Thule", where the Sun only set for three hours each day and the water was replaced by a congealed substance "on which one can neither walk nor sail". He was probably describing loose sea ice known today as "growlers" or "bergy bits"; his "Thule" was probably Norway, though the Faroe Islands or Shetland have also been suggested.[11]
Early cartographers were unsure whether to draw the region around the North Pole as land (as in Johannes Ruysch's map of 1507, or Gerardus Mercator's map of 1595) or water (as with Martin Waldseemüller's world map of 1507). The fervent desire of European merchants for a northern passage, the Northern Sea Route or the Northwest Passage, to "Cathay" (China) caused water to win out, and by 1723 mapmakers such as Johann Homann featured an extensive "Oceanus Septentrionalis" at the northern edge of their charts.
The few expeditions to penetrate much beyond the Arctic Circle in that era added only small islands, such as Novaya Zemlya (11th century) and Spitzbergen (1596), though, since these were often surrounded by pack-ice, their northern limits were not so clear. The makers of navigational charts, more conservative than some of the more fanciful cartographers, tended to leave the region blank, with only fragments of known coastline sketched in.
This lack of knowledge of what lay north of the shifting barrier of ice gave rise to a number of conjectures. In England and other European nations, the myth of an "Open Polar Sea" was persistent. John Barrow, longtime Second Secretary of the British Admiralty, promoted exploration of the region from 1818 to 1845 in search of this.
In the United States in the 1850s and 1860s, the explorers Elisha Kane and Isaac Israel Hayes both claimed to have seen part of this elusive body of water. Even quite late in the century, the eminent authority Matthew Fontaine Maury included a description of the Open Polar Sea in his textbook The Physical Geography of the Sea (1883). Nevertheless, as all the explorers who travelled closer and closer to the pole reported, the polar ice cap is quite thick and persists year-round.
Fridtjof Nansen was the first to make a nautical crossing of the Arctic Ocean, in the Fram Expedition from 1893 to 1896.
The first surface crossing of the ocean was led by Wally Herbert in 1969, in a dog sled expedition from Alaska to Svalbard, with air support.[12] The first nautical transit of the north pole was made in 1958 by the submarine USS Nautilus, and the first surface nautical transit occurred in 1977 by the icebreaker NS Arktika.
Since 1937, Soviet and Russian manned drifting ice stations have extensively monitored the Arctic Ocean. Scientific settlements were established on the drift ice and carried thousands of kilometres by ice floes.[13]
In World War II, the European region of the Arctic Ocean was heavily contested: the Allied commitment to resupply the Soviet Union via its northern ports was opposed by German naval and air forces.
Since 1954 commercial airlines have flown over the Arctic Ocean (see Polar route).
The Arctic Ocean occupies a roughly circular basin and covers an area of about 14,056,000 km2 (5,427,000 sq mi), almost the size of Antarctica.[14][15] The coastline is 45,390 km (28,200 mi) long.[14][16] It is the only ocean smaller than Russia, which has a land area of 16,377,742 km2 (6,323,482 sq mi).
The Arctic Ocean is surrounded by the land masses of Eurasia (Russia and Norway), North America (Canada and the U.S. state of Alaska), Greenland, and Iceland.
Note: Some parts of the areas listed in the table are located in the Atlantic Ocean. Other consists of Gulfs, Straits, Channels and other parts without specific names and excludes Exclusive Economic Zones.
The Arctic Ocean is connected to the Pacific Ocean by the Bering Strait and to the Atlantic Ocean through the Greenland Sea and Labrador Sea.[1] (The Iceland Sea is sometimes considered part of the Greenland Sea, and sometimes separate.)
The largest seas in the Arctic Ocean:[18][19][20]
Different authorities put various marginal seas in either the Arctic Ocean or the Atlantic Ocean, including: Hudson Bay,[21][22][23][24][25][26][27][28]
Baffin Bay, the Norwegian Sea, and Hudson Strait.

The main islands and archipelagos in the Arctic Ocean are, from the prime meridian west:
There are several ports and harbours on the Arctic Ocean.[29]
The ocean's Arctic shelf comprises a number of continental shelves, including the Canadian Arctic shelf, underlying the Canadian Arctic Archipelago, and the Russian continental shelf, which is sometimes called the "Arctic Shelf" because it is larger. The Russian continental shelf consists of three separate, smaller shelves: the Barents Shelf, Chukchi Sea Shelf and Siberian Shelf. Of these three, the Siberian Shelf is the largest such shelf in the world; it holds large oil and gas reserves. The Chukchi shelf forms the border between Russian and the United States as stated in the USSR–USA Maritime Boundary Agreement. The whole area is subject to international territorial claims.
The Chukchi Plateau extends from the Chukchi Sea Shelf.
An underwater ridge, the Lomonosov Ridge, divides the deep sea North Polar Basin into two oceanic basins: the Eurasian Basin, which is 4,000–4,500 m (13,100–14,800 ft) deep, and the Amerasian Basin (sometimes called the North American or Hyperborean Basin), which is about 4,000 m (13,000 ft) deep. The bathymetry of the ocean bottom is marked by fault block ridges, abyssal plains, ocean deeps, and basins. The average depth of the Arctic Ocean is 1,038 m (3,406 ft).[31] The deepest point is Molloy Hole in the Fram Strait, at about 5,550 m (18,210 ft).[32]
The two major basins are further subdivided by ridges into the Canada Basin (between Beaufort Shelf of North America and the Alpha Ridge), Makarov Basin (between the Alpha and Lomonosov Ridges), Amundsen Basin (between Lomonosov and Gakkel ridges), and Nansen Basin (between the Gakkel Ridge and the continental shelf that includes the Franz Josef Land).
The crystalline basement rocks of mountains around the Arctic Ocean were recrystallized or formed during the Ellesmerian orogeny, the regional phase of the larger Caledonian orogeny in the Paleozoic Era. Regional subsidence in the Jurassic and Triassic periods led to significant sediment deposition, creating many of the reservoirs for current day oil and gas deposits. During the Cretaceous period, the Canadian Basin opened, and tectonic activity due to the assembly of Alaska caused hydrocarbons to migrate toward what is now Prudhoe Bay. At the same time, sediments shed off the rising Canadian Rockies built out the large Mackenzie Delta.
The rifting apart of the supercontinent Pangea, beginning in the Triassic period, opened the early Atlantic Ocean. Rifting then extended northward, opening the Arctic Ocean as mafic oceanic crust material erupted out of a branch of Mid-Atlantic Ridge. The Amerasia Basin may have opened first, with the Chukchi Borderland moved along to the northeast by transform faults. Additional spreading helped to create the "triple-junction" of the Alpha-Mendeleev Ridge in the Late Cretaceous epoch.
Throughout the Cenozoic Era, the subduction of the Pacific plate, the collision of India with Eurasia, and the continued opening of the North Atlantic created new hydrocarbon traps. The seafloor began spreading from the Gakkel Ridge in the Paleocene Epoch and the Eocene Epoch, causing the Lomonosov Ridge to move farther from land and subside.
Because of sea ice and remote conditions, the geology of the Arctic Ocean is still poorly explored. The Arctic Coring Expedition drilling shed some light on the Lomonosov Ridge, which appears to be continental crust separated from the Barents-Kara Shelf in the Paleocene and then starved of sediment. It may contain up to 10 billion barrels of oil. The Gakkel Ridge rift is also poorly understand and may extend into the Laptev Sea.[33][34]
In large parts of the Arctic Ocean, the top layer (about 50 m [160 ft]) is of lower salinity and lower temperature than the rest. It remains relatively stable because the salinity effect on density is bigger than the temperature effect. It is fed by the freshwater input of the big Siberian and Canadian rivers (Ob, Yenisei, Lena, Mackenzie), the water of which quasi floats on the saltier, denser, deeper ocean water. Between this lower salinity layer and the bulk of the ocean lies the so-called halocline, in which both salinity and temperature rise with increasing depth.
Because of its relative isolation from other oceans, the Arctic Ocean has a uniquely complex system of water flow. It resembles some hydrological features of the Mediterranean Sea, referring to its deep waters having only limited communication through the Fram Strait with the Atlantic Basin, "where the circulation is dominated by thermohaline forcing".[35] The Arctic Ocean has a total volume of 18.07 × 106 km3, equal to about 1.3% of the World Ocean.  Mean surface circulation is predominantly cyclonic on the Eurasian side and anticyclonic in the Canadian Basin.[36]
Water enters from both the Pacific and Atlantic Oceans and can be divided into three unique water masses. The deepest water mass is called Arctic Bottom Water and begins around 900 m (3,000 ft) depth.[35] It is composed of the densest water in the World Ocean and has two main sources: Arctic shelf water and Greenland Sea Deep Water. Water in the shelf region that begins as inflow from the Pacific passes through the narrow Bering Strait at an average rate of 0.8 Sverdrups and reaches the Chukchi Sea.[37] During the winter, cold Alaskan winds blow over the Chukchi Sea, freezing the surface water and pushing this newly formed ice out to the Pacific. The speed of the ice drift is roughly 1–4 cm/s.[36] This process leaves dense, salty waters in the sea that sink over the continental shelf into the western Arctic Ocean and create a halocline.[38]
This water is met by Greenland Sea Deep Water, which forms during the passage of winter storms. As temperatures cool dramatically in the winter, ice forms, and intense vertical convection allows the water to become dense enough to sink below the warm saline water below.[35] Arctic Bottom Water is critically important because of its outflow, which contributes to the formation of Atlantic Deep Water. The overturning of this water plays a key role in global circulation and the moderation of climate.
In the depth range of 150–900 m (490–2,950 ft) is a water mass referred to as Atlantic Water. Inflow from the North Atlantic Current enters through the Fram Strait, cooling and sinking to form the deepest layer of the halocline, where it circles the Arctic Basin counter-clockwise. This is the highest volumetric inflow to the Arctic Ocean, equalling about 10 times that of the Pacific inflow, and it creates the Arctic Ocean Boundary Current.[37] It flows slowly, at about 0.02 m/s.[35] Atlantic Water has the same salinity as Arctic Bottom Water but is much warmer (up to 3 °C [37 °F]). In fact, this water mass is actually warmer than the surface water and remains submerged only due to the role of salinity in density.[35] When water reaches the basin, it is pushed by strong winds into a large circular current called the Beaufort Gyre. Water in the Beaufort Gyre is far less saline than that of the Chukchi Sea due to inflow from large Canadian and Siberian rivers.[38]
The final defined water mass in the Arctic Ocean is called Arctic Surface Water and is found in the depth range of 150–200 m (490–660 ft). The most important feature of this water mass is a section referred to as the sub-surface layer. It is a product of Atlantic water that enters through canyons and is subjected to intense mixing on the Siberian Shelf.[35][39] As it is entrained, it cools and acts a heat shield for the surface layer on account of weak mixing between layers.[40][41]
However, over the past couple of decades a combination of the warming[42] and the shoaling of Atlantic water[43] are leading to the increasing influence of Atlantic water heat in melting sea ice in the eastern Arctic. The most recent estimates, for 2016–2018, indicate the oceanic heat flux to the surface has now overtaken the atmospheric flux in the eastern Eurasian Basin.[44] Over the same period the weakening halocline stratification has coincided with increasing upper ocean currents thought to be associated with declining sea ice, indicate increasing mixing in this region.[45] In contrast direct measurements of mixing in the western Arctic indicate the Atlantic water heat remains isolated at intermediate depths even under the 'perfect storm' conditions of the Great Arctic Cyclone of 2012.[46]
Waters originating in the Pacific and Atlantic both exit through the Fram Strait between Greenland and Svalbard Island, which is about 2,700 m (8,900 ft) deep and 350 km (220 mi) wide. This outflow is about 9 Sv.[37] The width of the Fram Strait is what allows for both inflow and outflow on the Atlantic side of the Arctic Ocean. Because of this, it is influenced by the Coriolis force, which concentrates outflow to the East Greenland Current on the western side and inflow to the Norwegian Current on the eastern side.[35] Pacific water also exits along the west coast of Greenland and the Hudson Strait (1–2 Sv), providing nutrients to the Canadian Archipelago.[37]
As noted, the process of ice formation and movement is a key driver in Arctic Ocean circulation and the formation of water masses. With this dependence, the Arctic Ocean experiences variations due to seasonal changes in sea ice cover. Sea ice movement is the result of wind forcing, which is related to a number of meteorological conditions that the Arctic experiences throughout the year. For example, the Beaufort High—an extension of the Siberian High system—is a pressure system that drives the anticyclonic motion of the Beaufort Gyre.[36] During the summer, this area of high pressure is pushed out closer to its Siberian and Canadian sides. In addition, there is a sea level pressure (SLP) ridge over Greenland that drives strong northerly winds through the Fram Strait, facilitating ice export. In the summer, the SLP contrast is smaller, producing weaker winds. A final example of seasonal pressure system movement is the low pressure system that exists over the Nordic and Barents Seas. It is an extension of the Icelandic Low, which creates cyclonic ocean circulation in this area. The low shifts to centre over the North Pole in the summer. These variations in the Arctic all contribute to ice drift reaching its weakest point during the summer months.  There is also evidence that the drift is associated with the phase of the Arctic Oscillation and Atlantic Multidecadal Oscillation.[36]
Much of the Arctic Ocean is covered by sea ice that varies in extent and thickness seasonally. The mean extent of the Arctic sea ice has been continuously decreasing in the last decades, declining at a rate of currently 12.85% per decade since 1980 from the average winter value of 15,600,000 km2 (6,023,200 sq mi).[48] The seasonal variations are about 7,000,000 km2 (2,702,700 sq mi), with the maximum in April and minimum in September. The sea ice is affected by wind and ocean currents, which can move and rotate very large areas of ice. Zones of compression also arise, where the ice piles up to form pack ice.[49][50][51]
Icebergs occasionally break away from northern Ellesmere Island, and icebergs are formed from glaciers in western Greenland and extreme northeastern Canada. Icebergs are not sea ice but may become embedded in the pack ice. Icebergs pose a hazard to ships, of which the Titanic is one of the most famous. The ocean is virtually icelocked from October to June, and the superstructure of ships are subject to icing from October to May.[29] Before the advent of modern icebreakers, ships sailing the Arctic Ocean risked being trapped or crushed by sea ice (although the Baychimo drifted through the Arctic Ocean untended for decades despite these hazards).
The Arctic Ocean is contained in a polar climate characterized by persistent cold and relatively narrow annual temperature ranges. Winters are characterized by the polar night, extreme cold, frequent low-level temperature inversions, and stable weather conditions.[52] Cyclones are only common on the Atlantic side.[53] Summers are characterized by continuous daylight (midnight sun), and air temperatures can rise slightly above 0 °C (32 °F). Cyclones are more frequent in summer and may bring rain or snow.[53] It is cloudy year-round, with mean cloud cover ranging from 60% in winter to over 80% in summer.[54]
The temperature of the surface water of the Arctic Ocean is fairly constant at approximately −1.8 °C (28.8 °F), near the freezing point of seawater.
The density of sea water, in contrast to fresh water, increases as it nears the freezing point and thus it tends to sink. It is generally necessary that the upper 100–150 m (330–490 ft) of ocean water cools to the freezing point for sea ice to form.[55] In the winter, the relatively warm ocean water exerts a moderating influence, even when covered by ice. This is one reason why the Arctic does not experience the extreme temperatures seen on the Antarctic continent.
There is considerable seasonal variation in how much pack ice of the Arctic ice pack covers the Arctic Ocean. Much of the Arctic ice pack is also covered in snow for about 10 months of the year. The maximum snow cover is in March or April—about 20–50 cm (7.9–19.7 in) over the frozen ocean.
The climate of the Arctic region has varied significantly during the Earth's history. During the Paleocene–Eocene Thermal Maximum 55 million years ago, when the global climate underwent a warming of approximately 5–8 °C (9–14 °F), the region reached an average annual temperature of 10–20 °C (50–68 °F).[56][57][58] The surface waters of the northernmost[59] Arctic Ocean warmed, seasonally at least, enough to support tropical lifeforms (the dinoflagellates Apectodinium augustum) requiring surface temperatures of over 22 °C (72 °F).[60]
Currently, the Arctic region is warming twice as fast as the rest of the planet.[61][62]
Due to the pronounced seasonality of 2–6 months of midnight sun and polar night[63] in the Arctic Ocean, the primary production of photosynthesizing organisms such as ice algae and phytoplankton is limited to the spring and summer months (March/April to September).[64] Important consumers of primary producers in the central Arctic Ocean and the adjacent shelf seas include zooplankton, especially copepods (Calanus finmarchicus, Calanus glacialis, and Calanus hyperboreus)[65] and euphausiids,[66] as well as ice-associated fauna (e.g., amphipods).[65] These primary consumers form an important link between the primary producers and higher trophic levels. The composition of higher trophic levels in the Arctic Ocean varies with region (Atlantic side vs. Pacific side) and with the sea-ice cover. Secondary consumers in the Barents Sea, an Atlantic-influenced Arctic shelf sea, are mainly sub-Arctic species including herring, young cod, and capelin.[66] In ice-covered regions of the central Arctic Ocean, polar cod is a central predator of primary consumers. The apex predators in the Arctic Ocean—marine mammals such as seals, whales, and polar bears—prey upon fish.
Endangered marine species in the Arctic Ocean include walruses and whales. The area has a fragile ecosystem, and it is especially exposed to climate change, because it warms faster than the rest of the world. Lion's mane jellyfish are abundant in the waters of the Arctic, and the banded gunnel is the only species of gunnel that lives in the ocean.
Petroleum and natural gas fields, placer deposits, polymetallic nodules, sand and gravel aggregates, fish, seals and whales can all be found in abundance in the region.[29][51]
The political dead zone near the centre of the sea is also the focus of a mounting dispute between the United States, Russia, Canada, Norway, and Denmark.[67] It is significant for the global energy market because it may hold 25% or more of the world's undiscovered oil and gas resources.[68]
The Arctic ice pack is thinning, and a seasonal hole in the ozone layer frequently occurs.[69] Reduction of the area of Arctic sea ice reduces the planet's average albedo, possibly resulting in global warming in a positive feedback mechanism.[51][70] Research shows that the Arctic may become ice-free in the summer for the first time in human history by 2040.[71][72] Estimates vary for when the last time the Arctic was ice-free: 65 million years ago when fossils indicate that plants existed there to as recently as 5,500 years ago; ice and ocean cores going back 8,000 years to the last warm period or 125,000 during the last intraglacial period.[73]
Warming temperatures in the Arctic may cause large amounts of fresh melt-water to enter the north Atlantic, possibly disrupting global ocean current patterns. Potentially severe changes in the Earth's climate might then ensue.[70]
As the extent of sea ice diminishes and sea level rises, the effect of storms such as the Great Arctic Cyclone of 2012 on open water increases, as does possible salt-water damage to vegetation on shore at locations such as the Mackenzie Delta as stronger storm surges become more likely.[74]
Global warming has increased encounters between polar bears and humans. Reduced sea ice due to melting is causing polar bears to search for new sources of food.[75] Beginning in December 2018 and coming to an apex in February 2019, a mass invasion of polar bears into the archipelago of Novaya Zemlya caused local authorities to declare a state of emergency. Dozens of polar bears were seen entering homes, public buildings and inhabited areas.[76][77]
Sea ice, and the cold conditions it sustains, serves to stabilize methane deposits on and near the shoreline,[78] preventing the clathrate breaking down and outgassing methane into the atmosphere, causing further warming. Melting of this ice may release large quantities of methane, a powerful greenhouse gas, into the atmosphere, causing further warming in a strong positive feedback cycle and marine genera and species to become extinct.[78][79]
Other environmental concerns relate to the radioactive contamination of the Arctic Ocean from, for example, Russian radioactive waste dump sites in the Kara Sea,[80] Cold War nuclear test sites such as Novaya Zemlya,[81] Camp Century's contaminants in Greenland,[82] and radioactive contamination from the Fukushima Daiichi nuclear disaster.[83]
On 16 July 2015, five nations (United States, Russia, Canada, Norway, Denmark/Greenland) signed a declaration committing to keep their fishing vessels out of a 1.1 million square mile zone in the central Arctic Ocean near the North Pole. The agreement calls for those nations to refrain from fishing there until there is better scientific knowledge about the marine resources and until a regulatory system is in place to protect those resources.[84][85]



The North Sea lies between Great Britain, Denmark, Norway, Germany, the Netherlands, Belgium and France. An epeiric sea on the European continental shelf, it connects to the Atlantic Ocean through the English Channel in the south and the Norwegian Sea in the north. It is more than 970 kilometres (600 mi) long and 580 kilometres (360 mi) wide, covering 570,000 square kilometres (220,000 sq mi).
It hosts key north European shipping lanes and is a major fishery. The coast is a popular destination for recreation and tourism in bordering countries, and a rich source of energy resources, including wind and wave power.
The North Sea has featured prominently in geopolitical and military affairs, particularly in Northern Europe, from the Middle Ages to the modern era. It was also important globally through the power northern Europeans projected worldwide during much of the Middle Ages and into the modern era. The North Sea was the centre of the Vikings' rise. The Hanseatic League, the Dutch Republic, and the British each sought to gain command of the North Sea and access to the world's markets and resources. As Germany's only outlet to the ocean, the North Sea was strategically important through both world wars.
The coast has diverse geology and geography. In the north, deep fjords and sheer cliffs mark much of its Norwegian and Scottish coastlines respectively, whereas in the south, the coast consists mainly of sandy beaches, estuaries of long rivers and wide mudflats. Due to the dense population, heavy industrialisation, and intense use of the sea and the area surrounding it, various environmental issues affect the sea's ecosystems. Adverse environmental issues – commonly including overfishing, industrial and agricultural runoff, dredging, and dumping, among others –  have led to several efforts to prevent degradation and to safeguard long-term economic benefits.
The North Sea is bounded by the Orkney Islands and east coast of Great Britain to the west[1] and the northern and central European mainland to the east and south, including Norway, Denmark, Germany, the Netherlands, Belgium, and France.[2] In the southwest, beyond the Straits of Dover, the North Sea becomes the English Channel connecting to the Atlantic Ocean.[1][2] In the east, it connects to the Baltic Sea via the Skagerrak and Kattegat,[2] narrow straits that separate Denmark from Norway and Sweden respectively.[1] In the north it is bordered by the Shetland Islands, and connects with the Norwegian Sea, which is a marginal sea in the Arctic Ocean.[1][3]
The North Sea is more than 970 kilometres (600 mi) long and 580 kilometres (360 mi) wide, with an area of 750,000 square kilometres (290,000 sq mi) and a volume of 54,000 cubic kilometres (13,000 cu mi).[4] Around the edges of the North Sea are sizeable islands and archipelagos, including Shetland, Orkney, and the Frisian Islands.[2] The North Sea receives freshwater from a number of European continental watersheds, as well as the British Isles. A large part of the European drainage basin empties into the North Sea, including water from the Baltic Sea. The largest and most important rivers flowing into the North Sea are the Elbe and the Rhine – Meuse.[5] Around 185 million people live in the catchment area of the rivers discharging into the North Sea encompassing some highly industrialized areas.[6]
For the most part, the sea lies on the European continental shelf with a mean depth of 90 metres (300 ft).[1][7] The only exception is the Norwegian trench, which extends parallel to the Norwegian shoreline from Oslo to an area north of Bergen.[1] It is between 20 and 30 kilometres (12 and 19 mi) wide and has a maximum depth of 725 metres (2,379 ft).[8]
The Dogger Bank, a vast moraine, or accumulation of unconsolidated glacial debris, rises to a mere 15 to 30 m (50 to 100 ft) below the surface.[9][10] This feature has produced the finest fishing location of the North Sea.[1] The Long Forties and the Broad Fourteens are large areas with roughly uniform depth in fathoms (forty fathoms and fourteen fathoms or 73 and 26 m or 240 and 85 ft deep, respectively). These great banks and others make the North Sea particularly hazardous to navigate,[11] which has been alleviated by the implementation of satellite navigation systems.[12] The Devil's Hole lies 320 kilometres (200 mi) east of Dundee, Scotland. The feature is a series of asymmetrical trenches between 20 and 30 kilometres (12 and 19 mi) long, one and two kilometres (0.6 and 1.2 mi) wide and up to 230 metres (750 ft) deep.[13]
Other areas which are less deep are Cleaver Bank, Fisher Bank and Noordhinder Bank.
The International Hydrographic Organization defines the limits of the North Sea as follows:[14]
On the Southwest. A line joining the Phare de Walde (Walde Lighthouse, in France, 50°59'37"N, 1°54'53"E) and Leathercoat Point (England, 51°10'01.4"N 1°24'07.8").[15] northeast of Dover.
On the Northwest. From Dunnet Head (58°40'20"N, 3°22'30"W) in Scotland to Tor Ness (58°47'N) in the Island of Hoy, thence through this island to the Kame of Hoy (58°55'N) on to Breck Ness on Mainland (58°58'N) through this island to Costa Head (3°14'W) and Inga Ness (59'17'N) in Westray through Westray, to Bow Head, across to Mull Head (North point of Papa Westray) and on to Seal Skerry (North point of North Ronaldsay) and thence to Horse Island (South point of the Shetland Islands).
On the North. From the North point (Fethaland Point) of the Mainland of the Shetland Islands, across to Graveland Ness (60°39'N) in the Island of Yell, through Yell to Gloup Ness (1°04'W) and across to Spoo Ness (60°45'N) in Unst island, through Unst to Herma Ness (60°51'N), on to the SW point of the Rumblings and to Muckle Flugga (60°51′N 0°53′W﻿ / ﻿60.850°N 0.883°W﻿ / 60.850; -0.883) all these being included in the North Sea area; thence up the meridian of 0°53' West to the parallel of 61°00' North and eastward along this parallel to the coast of Norway, the whole of Viking Bank is thus included in the North Sea.
On the East. The Western limit of the Skagerrak [A line joining Hanstholm (57°07′N 8°36′E﻿ / ﻿57.117°N 8.600°E﻿ / 57.117; 8.600) and the Naze (Lindesnes, 58°N 7°E﻿ / ﻿58°N 7°E﻿ / 58; 7)].The average temperature is 17 °C (63 °F) in the summer and 6 °C (43 °F) in the winter.[4] The average temperatures have been trending higher since 1988, which has been attributed to climate change.[16][17] Air temperatures in January range on average between 0 and 4 °C (32 and 39 °F) and in July between 13 and 18 °C (55 and 64 °F). The winter months see frequent gales and storms.[1]
The salinity averages between 34 and 35 grams per litre (129 and 132 g/US gal) of water.[4] The salinity has the highest variability where there is fresh water inflow, such as at the Rhine and Elbe estuaries, the Baltic Sea exit and along the coast of Norway.[18]
The main pattern to the flow of water in the North Sea is an anti-clockwise rotation along the edges.[19]
The North Sea is an arm of the Atlantic Ocean receiving the majority of ocean current from the northwest opening, and a lesser portion of warm current from the smaller opening at the English Channel. These tidal currents leave along the Norwegian coast.[20] Surface and deep water currents may move in different directions. Low salinity surface coastal waters move offshore, and deeper, denser high salinity waters move inshore.[21]
The North Sea located on the continental shelf has different waves from those in deep ocean water. The wave speeds are diminished and the wave amplitudes are increased. In the North Sea there are two amphidromic systems and a third incomplete amphidromic system.[22][23] In the North Sea the average tide difference in wave amplitude is between zero and eight metres (26 ft).[An average is a single figure, not a range.][4] The presence of fixed-foundation wind turbines in the North Sea was created by action of the tide large visible sea plumes, as photographed by NASA.[24]
The Kelvin tide of the Atlantic Ocean is a semidiurnal wave that travels northward. Some of the energy from this wave travels through the English Channel into the North Sea. The wave continues to travel northward in the Atlantic Ocean, and once past the northern tip of Great Britain, the Kelvin wave turns east and south and once again enters the North Sea.[25]
The eastern and western coasts of the North Sea are jagged, formed by glaciers during the ice ages. The coastlines along the southernmost part are covered with the remains of deposited glacial sediment.[1] The Norwegian mountains plunge into the sea creating deep fjords and archipelagos. South of Stavanger, the coast softens, the islands become fewer.[1] The eastern Scottish coast is similar, though less severe than Norway. From north east of England, the cliffs become lower and are composed of less resistant moraine, which erodes more easily, so that the coasts have more rounded contours.[60][61] In the Netherlands, Belgium and in East Anglia the littoral is low and marshy.[1] The east coast and south-east of the North Sea (Wadden Sea) have coastlines that are mainly sandy and straight owing to longshore drift, particularly along Belgium and Denmark.[62]
The southern coastal areas were originally flood plains and swampy land. In areas especially vulnerable to storm surges, people settled behind elevated levees and on natural areas of high ground such as spits and geestland.[63]: [302, 303]  As early as 500 BC, people were constructing artificial dwelling hills higher than the prevailing flood levels.[63]: [306, 308]  It was only around the beginning of the High Middle Ages, in 1200 AD, that inhabitants began to connect single ring dikes into a dike line along the entire coast, thereby turning amphibious regions between the land and the sea into permanent solid ground.[63]
The modern form of the dikes supplemented by overflow and lateral diversion channels, began to appear in the 17th and 18th centuries, built in the Netherlands.[64] The North Sea Floods of 1953 and 1962 were the impetus for further raising of the dikes as well as the shortening of the coast line so as to present as little surface area as possible to the punishment of the sea and the storms.[65] Currently, 27% of the Netherlands is below sea level protected by dikes, dunes, and beach flats.[66]
Coastal management today consists of several levels.[67] The dike slope reduces the energy of the incoming sea, so that the dike itself does not receive the full impact.[67] Dikes that lie directly on the sea are especially reinforced.[67] The dikes have, over the years, been repeatedly raised, sometimes up to 9 metres (30 ft) and have been made flatter to better reduce wave erosion.[68] Where the dunes are sufficient to protect the land behind them from the sea, these dunes are planted with beach grass (Ammophila arenaria) to protect them from erosion by wind, water, and foot traffic.[69]
Storm surges threaten, in particular, the coasts of the Netherlands, Belgium, Germany, and Denmark and low-lying areas of eastern England particularly around The Wash and Fens.[62]
Storm surges are caused by changes in barometric pressure combined with strong wind created wave action.[70]
The first recorded storm tide flood was the Julianenflut, on 17 February 1164. In its wake, the Jadebusen, (a bay on the coast of Germany), began to form.
A storm tide in 1228 is recorded to have killed more than 100,000 people.[71] In 1362, the Second Marcellus Flood, also known as the Grote Manndrenke, hit the entire southern coast of the North Sea. Chronicles of the time again record more than 100,000 deaths, large parts of the coast were lost permanently to the sea, including the now legendary lost city of Rungholt.[72]
In the 20th century, the North Sea flood of 1953 flooded several nations' coasts and cost more than 2,000 lives.[73]
315 citizens of Hamburg died in the North Sea flood of 1962.[74]: [79, 86] 
Though rare, the North Sea has been the site of a number of historically documented tsunamis. The Storegga Slides were a series of underwater landslides, in which a piece of the Norwegian continental shelf slid into the Norwegian Sea. The immense landslips occurred between 8150 BCE and 6000 BCE, and caused a tsunami up to 20 metres (66 ft) high that swept through the North Sea, having the greatest effect on Scotland and the Faeroe Islands.[75][76]
The Dover Straits earthquake of 1580 is among the first recorded earthquakes in the North Sea measuring between 5.6 and 5.9 on the Richter scale. This event caused extensive damage in Calais both through its tremors and possibly triggered a tsunami, though this has never been confirmed. The theory is a vast underwater landslide in the English Channel was triggered by the earthquake, which in turn caused a tsunami.[77] The tsunami triggered by the 1755 Lisbon earthquake reached Holland, although the waves had lost their destructive power. The largest earthquake ever recorded in the United Kingdom was the 1931 Dogger Bank earthquake, which measured 6.1 on the Richter magnitude scale and caused a small tsunami that flooded parts of the British coast.[77]
Shallow epicontinental seas like the current North Sea have since long existed on the European continental shelf. The rifting that formed the northern part of the Atlantic Ocean during the Jurassic and Cretaceous periods, from about 150 million years ago, caused tectonic uplift in the British Isles.[78] Since then, a shallow sea has almost continuously existed between the uplands of the Fennoscandian Shield and the British Isles.[79] This precursor of the current North Sea has grown and shrunk with the rise and fall of the eustatic sea level during geologic time. Sometimes it was connected with other shallow seas, such as the sea above the Paris Basin to the south-west, the Paratethys Sea to the south-east, or the Tethys Ocean to the south.[80]
During the Late Cretaceous, about 85 million years ago, all of modern mainland Europe except for Scandinavia was a scattering of islands.[81] By the Early Oligocene, 34 to 28 million years ago, the emergence of Western and Central Europe had almost completely separated the North Sea from the Tethys Ocean, which gradually shrank to become the Mediterranean as Southern Europe and South West Asia became dry land.[82] The North Sea was cut off from the English Channel by a narrow land bridge until that was breached by at least two catastrophic floods between 450,000 and 180,000 years ago.[83][84] Since the start of the Quaternary period about 2.6 million years ago, the eustatic sea level has fallen during each glacial period and then risen again. Every time the ice sheet reached its greatest extent, the North Sea became almost completely dry, the dry landmass being known as Doggerland, whose northern regions were themselves known to have been glaciated.[85] The present-day coastline formed after the Last Glacial Maximum when the sea began to flood the European continental shelf.[86]
In 2006 a bone fragment was found while drilling for oil in the North Sea. Analysis indicated that it was a Plateosaurus from 199 to 216 million years ago. This was the deepest dinosaur fossil ever found and the first find for Norway.[87]
Map showing hypothetical extent of Doggerland (c. 8,000 BC), which provided a land bridge between Great Britain and continental Europe
North Sea from De Koog, Texel island
The North Sea between 34 million years ago and 28 million years ago, as Central Europe became dry land
Copepods and other zooplankton are plentiful in the North Sea. These tiny organisms are crucial elements of the food chain supporting many species of fish.[88] Over 230 species of fish live in the North Sea. Cod, haddock, whiting, saithe, plaice, sole, mackerel, herring, pouting, sprat, and sandeel are all very common and are fished commercially.[88][89] Due to the various depths of the North Sea trenches and differences in salinity, temperature, and water movement, some fish such as blue-mouth redfish and rabbitfish reside only in small areas of the North Sea.[90]
Crustaceans are also commonly found throughout the sea. Norway lobster, deep-water prawns, and brown shrimp are all commercially fished, but other species of lobster, shrimp, oyster, mussels and clams all live in the North Sea.[88] Recently non-indigenous species have become established including the Pacific oyster and Atlantic jackknife clam.[89]
The coasts of the North Sea are home to nature reserves including the Ythan Estuary, Fowlsheugh Nature Preserve, and Farne Islands in the UK and the Wadden Sea National Parks in Denmark, Germany and the Netherlands.[88] These locations provide breeding habitat for dozens of bird species. Tens of millions of birds make use of the North Sea for breeding, feeding, or migratory stopovers every year. Populations of black-legged kittiwakes, Atlantic puffins, northern gannets, northern fulmars, and species of petrels, seaducks, loons (divers), cormorants, gulls, auks, and terns, and many other seabirds make these coasts popular for birdwatching.[88][89]
The North Sea is also home to marine mammals. Common seals, grey seals, and harbour porpoises can be found along the coasts, at marine installations, and on islands. The very northern North Sea islands such as the Shetland Islands are occasionally home to a larger variety of pinnipeds including bearded, harp, hooded and ringed seals, and even walrus.[91] North Sea cetaceans include various porpoise, dolphin and whale species.[89][92]
Plant species in the North Sea include species of wrack, among them bladder wrack, knotted wrack, and serrated wrack. Algae, macroalgal, and kelp, such as oarweed and laminaria hyperboria, and species of maerl are found as well.[89] Eelgrass, formerly common in the entirety of the Wadden Sea, was nearly wiped out in the 20th century by a disease.[93] Similarly, sea grass used to coat huge tracts of ocean floor, but have been damaged by trawling and dredging have diminished its habitat and prevented its return.[94] Invasive Japanese seaweed has spread along the shores of the sea clogging harbours and inlets and has become a nuisance.[95]
Due to the heavy human populations and high level of industrialization along its shores, the wildlife of the North Sea has suffered from pollution, overhunting, and overfishing. Flamingos and pelicans were once found along the southern shores of the North Sea, but became extinct over the second millennium.[96] Walruses frequented the Orkney Islands through the mid-16th century, as both Sable Island and Orkney Islands lay within their normal range.[97] Grey whales also resided in the North Sea but were driven to extinction in the Atlantic in the 17th century[98] Other species have dramatically declined in population, though they are still found. North Atlantic right whales, sturgeon, shad, rays, skates, salmon, and other species were common in the North Sea until the 20th century, when numbers declined due to overfishing.[99][100]
Other factors like the introduction of non-indigenous species, industrial and agricultural pollution, trawling and dredging, human-induced eutrophication, construction on coastal breeding and feeding grounds, sand and gravel extraction, offshore construction, and heavy shipping traffic have also contributed to the decline.[89] For example, a resident orca pod was lost in the 1960s, presumably due to the peak in PCB pollution in this time period.[101]
The OSPAR commission manages the OSPAR convention to counteract the harmful effects of human activity on wildlife in the North Sea, preserve endangered species, and provide environmental protection.[102] All North Sea border states are signatories of the MARPOL 73/78 Accords, which preserve the marine environment by preventing pollution from ships.[103] Germany, Denmark, and the Netherlands also have a trilateral agreement for the protection of the Wadden Sea, or mudflats, which run along the coasts of the three countries on the southern edge of the North Sea.[104]
The North Sea has had various names throughout history. One of the earliest recorded names was Septentrionalis Oceanus, or "Northern Ocean", which was cited by Pliny.[105] The name "North Sea" probably came into English, however, via the Dutch "Noordzee", who named it thus either in contrast with the Zuiderzee ("South Sea"), located south of Frisia, or because the sea is generally to the north of the Netherlands. Before the adoption of "North Sea", the names used in English were "German Sea" or "German Ocean", referred to as the Latin names "Mare Germanicum" and "Oceanus Germanicus",[106] and these persisted in use until the First World War.[107] Other common names in use for long periods were the Latin terms "Mare Frisicum", as well as the English equivalent, "Frisian Sea".[108][109] The modern names of the sea in the other local languages are: Danish: Vesterhavet [ˈvestɐˌhɛˀvð̩] ("West Sea") or Nordsøen [ˈnoɐ̯ˌsøˀn̩], Dutch: Noordzee, Dutch Low Saxon: Noordzee, French: Mer du Nord, West Frisian: Noardsee, German: Nordsee, Low German: Noordsee, Northern Frisian: Weestsiie ("West Sea"), Swedish: Nordsjön, Bokmål: Nordsjøen [ˈnûːrˌʂøːn], Nynorsk: Nordsjøen, Scots: North Sea and Scottish Gaelic: An Cuan a Tuath.
A 1482 recreation of a map from Ptolemy's Geography showing the "Oceanus Germanicus"
Edmond Halley's solar eclipse 1715 map showing The German Sea
The North Sea has provided waterway access for commerce and conquest. Many areas have access to the North Sea because of its long coastline and the European rivers that empty it.[1] There is little documentary evidence concerning the North Sea before the Roman conquest of Britain in 43 CE, however, archaeological evidence reveals the diffusion of cultures and technologies from across or along the North Sea to Great Britain and Scandinavia and reliance by some prehistoric cultures on fishing, whaling, and seaborne trade on the North Sea. The Romans established organised ports in Britain, which increased shipping and began sustained trade[110] the diffusion of cultures and technologies from across or along the North Sea to Great Britain and Scandinavia and reliance by some prehistoric cultures on fishing, whaling, and seaborne trade on the North Sea. The Romans established organised ports in Britain, which increased shipping and began sustained trade[110] and many Scandinavian tribes participated in raids and wars against the Romans and Roman coinage and manufacturing were important trade goods. When the Romans abandoned Britain in 410, the Germanic Angles, Frisians, Saxons, and Jutes began the next great migration across the North Sea during the Migration Period. They made successive invasions of the island from what is now the Netherlands, Denmark, and Germany.[111]
The Viking Age began in 793 with the attack on Lindisfarne; for the next quarter-millennium, the Vikings ruled the North Sea. In their superior longships, they raided, traded, and established colonies and outposts along the coasts of the sea. From the Middle Ages through the 15th century, the northern European coastal ports exported domestic goods, dyes, linen, salt, metal goods and wine. The Scandinavian and Baltic areas shipped grain, fish, naval necessities, and timber. In turn, the North Sea countries imported high-grade cloths, spices, and fruits from the Mediterranean region.[112] Commerce during this era was mainly conducted by maritime trade due to underdeveloped roadways.[112]
In the 13th century the Hanseatic League, though centred on the Baltic Sea, started to control most of the trade through important members and outposts on the North Sea.[113] The League lost its dominance in the 16th century, as neighbouring states took control of former Hanseatic cities and outposts. Their internal conflict prevented effective cooperation and defence.[114] As the League lost control of its maritime cities, new trade routes emerged that provided Europe with Asian, American, and African goods.[115][116]
The 17th century Dutch Golden Age saw Dutch maritime power at its zenith.[117][118] Important overseas colonies, a vast merchant marine, a large fishing fleet,[112] powerful navy, and sophisticated financial markets made the Dutch the ascendant power in the North Sea, to be challenged by an ambitious England. This rivalry led to the first three Anglo-Dutch Wars between 1652 and 1673, which ended with Dutch victories.[118] After the Glorious Revolution in 1688, the Dutch prince William ascended to the English throne. With unified leadership, commercial, military, and political power began to shift from Amsterdam to London.[119]
The British did not face a challenge to their dominance of the North Sea until the 20th century.[120]
Tensions in the North Sea were again heightened in 1904 by the Dogger Bank incident. During the Russo-Japanese War, several ships of the Russian Baltic Fleet, which was on its way to the Far East, mistook British fishing boats for Japanese ships and fired on them, and then upon each other, near the Dogger Bank, nearly causing Britain to enter the war on the side of Japan.
During the First World War, Great Britain's Grand Fleet and Germany's Kaiserliche Marine faced each other in the North Sea,[121] which became the main theatre of the war for surface action.[121] Britain's larger fleet and North Sea Mine Barrage were able to establish an effective blockade for most of the war, which restricted the Central Powers' access to many crucial resources.[122] Major battles included the Battle of Heligoland Bight,[123] the Battle of the Dogger Bank,[124] and the Battle of Jutland.[124]
World War I also brought the first extensive use of submarine warfare, and a number of submarine actions occurred in the North Sea.[125]
The Second World War also saw action in the North Sea, though it was restricted more to aircraft reconnaissance and action by fighter/bomber aircraft, submarines and smaller vessels such as minesweepers and torpedo boats.[126][127]
After the war, hundreds of thousands of tons of chemical weapons were disposed of by being dumped in the North Sea.[128]
After the war, the North Sea lost much of its military significance because it is bordered only by NATO member-states. However, it gained significant economic importance in the 1960s as the states around the North Sea began full-scale exploitation of its oil and gas resources.[129] The North Sea continues to be an active trade route.[130]
Countries that border the North Sea all claim the 12 nautical miles (22 km; 14 mi) of territorial waters, within which they have exclusive fishing rights.[131] The Common Fisheries Policy of the European Union (EU) exists to coordinate fishing rights and assist with disputes between EU states and the EU border state of Norway.[132]
After the discovery of mineral resources in the North Sea during the early 1960s, the Convention on the Continental Shelf established country rights largely divided along the median line. The median line is defined as the line "every point of which is equidistant from the nearest points of the baselines from which the breadth of the territorial sea of each State is measured".[133]
The ocean floor border between Germany, the Netherlands, and Denmark was only reapportioned in 1969 after protracted negotiations and a judgment of the International Court of Justice.[131][134]
As early as 1859, oil was discovered in onshore areas around the North Sea and natural gas as early as 1910.[81] Onshore resources, for example the K12-B field in the Netherlands continue to be exploited today.
Offshore test drilling began in 1966 and then, in 1969, Phillips Petroleum Company discovered the Ekofisk oil field[135] distinguished by valuable, low-sulphur oil.[136] Commercial exploitation began in 1971 with tankers and, after 1975, by a pipeline, first to Teesside, England and then, after 1977, also to Emden, Germany.[137]
The exploitation of the North Sea oil reserves began just before the 1973 oil crisis, and the climb of international oil prices made the large investments needed for extraction much more attractive.[138]
The start in 1973 of the oil reserves by the UK allowed them to stop the declining position in international trade in 1974, and a huge increase after the discovery and exploitation of the huge oil field by Phillips group in 1977 as the Brae field.
Although the production costs are relatively high, the quality of the oil, the political stability of the region, and the proximity of important markets in western Europe have made the North Sea an important oil-producing region.[136] The largest single humanitarian catastrophe in the North Sea oil industry was the destruction of the offshore oil platform Piper Alpha in 1988 in which 167 people lost their lives.[139]
Besides the Ekofisk oil field, the Statfjord oil field is also notable as it was the cause of the first pipeline to span the Norwegian trench.[140] The largest natural gas field in the North Sea, Troll gas field, lies in the Norwegian trench, dropping over 300 metres (980 ft), requiring the construction of the enormous Troll A platform to access it.
The price of Brent Crude, one of the first types of oil extracted from the North Sea is used today as a standard price for comparison for crude oil from the rest of the world.[141] The North Sea contains western Europe's largest oil and natural gas reserves and is one of the world's key non-OPEC producing regions.[142]
In the UK sector of the North Sea, the oil industry invested £14.4 billion in 2013 and was on track to spend £13 billion in 2014. Industry body Oil & Gas UK put the decline down to rising costs, lower production, high tax rates, and less exploration.[143]
In January 2018, The North Sea region contained 184 offshore rigs, which made it the region with the highest number of offshore rigs in the world at the time.[144]
The North Sea is Europe's main fishery accounting for over 5% of international commercial fish caught.[1] Fishing in the North Sea is concentrated in the southern part of the coastal waters. The main method of fishing is trawling.[145]
In 1995, the total volume of fish and shellfish caught in the North Sea was approximately 3.5 million tonnes.[146] Besides saleable fish, it is estimated that one million tonnes of unmarketable by-catch is caught and discarded to die each year.[147]
In recent decades, overfishing has left many fisheries unproductive, disturbing marine food chain dynamics and costing jobs in the fishing industry.[148] Herring, cod and plaice fisheries may soon face the same plight as mackerel fishing, which ceased in the 1970s due to overfishing.[149]
The objective of the European Union Common Fisheries Policy is to minimize the environmental impact associated with resource use by reducing fish discards, increasing the productivity of fisheries, stabilising markets of fisheries and fish processing, and supplying fish at reasonable prices for the consumer.[150]
Whaling was an important economic activity from the 9th until the 13th century for Flemish whalers.[151] The medieval Flemish, Basque and Norwegian whalers who were replaced in the 16th century by Dutch, English, Danes, and Germans, took massive numbers of whales and dolphins and nearly depleted the right whales. This activity likely led to the extinction of the Atlantic population of the once common grey whale.[152] By 1902 the whaling had ended.[151] After being absent for 300 years a single grey whale returned,[153] it probably was the first of many more to find its way through the now ice-free Northwest Passage.
In addition to oil, gas, and fish, the states along the North Sea also take millions of cubic metres per year of sand and gravel from the ocean floor. These are used for beach nourishment, land reclamation
and construction.[154]
Rolled pieces of amber may be picked up on the east coast of England.[155]
Due to the strong prevailing winds, and shallow water, countries on the North Sea, particularly Germany and Denmark, have used the shore for wind power since the 1990s.[156] The North Sea is the home of one of the first large-scale offshore wind farms in the world, Horns Rev 1, completed in 2002. Since then many other wind farms have been commissioned in the North Sea (and elsewhere). As of 2013, the 630 megawatt (MW) London Array is the largest offshore wind farm in the world, with the 504 (MW) Greater Gabbard wind farm the second largest, followed by the 367 MW Walney Wind Farm. All are off the coast of the UK. These projects will be dwarfed by subsequent wind farms that are in the pipeline, including Dogger Bank at 4,800 MW, Norfolk Bank (7,200 MW), and Irish Sea (4,200 MW). At the end of June 2013 total European combined offshore wind energy capacity was 6,040 MW. The UK installed 513.5 MW of offshore wind power in the first half-year of 2013.[157] The development of the offshore wind industry in UK-controlled areas of the North Sea is traced to three phases: coastal, off-coastal and deep offshore in the period 2004 - 2021.[158]
The expansion of offshore wind farms has met with some resistance. Concerns have included shipping collisions[159] and environmental effects on ocean ecology and wildlife such as fish and migratory birds,[160] however, these concerns were found to be negligible in a long-term study in Denmark released in 2006 and again in a UK government study in 2009.[161][162]
There are also concerns about reliability,[163] and the rising costs of constructing and maintaining offshore wind farms.[164] Despite these, development of North Sea wind power is continuing, with plans for additional wind farms off the coasts of Germany, the Netherlands, and the UK.[165] There have also been proposals for a transnational power grid in the North Sea[166][167] to connect new offshore wind farms.[168]
Energy production from tidal power is still in a pre-commercial stage. The European Marine Energy Centre has installed a wave testing system at Billia Croo on the Orkney mainland[169] and a tidal power testing station on the nearby island of Eday.[170] Since 2003, a prototype Wave Dragon energy converter has been in operation at Nissum Bredning fjord of northern Denmark.[171]
The beaches and coastal waters of the North Sea are destinations for tourists. The English, Belgian, Dutch, German and Danish coasts[172][173] are developed for tourism. The North Sea coast of the United Kingdom has tourist destinations with beach resorts and links golf courses; the coastal town of St. Andrews in Scotland is renowned as the "Home of Golf".
The North Sea Trail is a long-distance trail linking seven countries around the North Sea.[174] Windsurfing and sailing[175] are popular sports because of the strong winds. Mudflat hiking,[176] recreational fishing and birdwatching[173] are among other activities.
The climatic conditions on the North Sea coast have been claimed to be healthy. As early as the 19th century, travellers visited the North Sea coast for curative and restorative vacations. The sea air, temperature, wind, water, and sunshine are counted among the beneficial conditions that are said to activate the body's defences, improve circulation, strengthen the immune system, and have healing effects on the skin and the respiratory system.[177]
The Wadden Sea in Denmark, Germany and the Netherlands is an UNESCO World Heritage Site.
The North Sea is important for marine transport and its shipping lanes are among the busiest in the world.[131] Major ports are located along its coasts: Rotterdam, the busiest port in Europe and the fourth busiest port in the world by tonnage as of 2013[update], Antwerp (was 16th) and Hamburg (was 27th), Bremen/Bremerhaven and Felixstowe, both in the top 30 busiest container seaports,[178] as well as the Port of Bruges-Zeebrugge, Europe's leading ro-ro port.[179]
Fishing boats, service boats for offshore industries, sport and pleasure craft, and merchant ships to and from North Sea ports and Baltic ports must share routes on the North Sea. The Dover Strait alone sees more than 400 commercial vessels a day.[180] Because of this volume, navigation in the North Sea can be difficult in high traffic zones, so ports have established elaborate vessel traffic services to monitor and direct ships into and out of port.[181]
The North Sea coasts are home to numerous canals and canal systems to facilitate traffic between and among rivers, artificial harbours, and the sea. The Kiel Canal, connecting the North Sea with the Baltic Sea, is the most heavily used artificial seaway in the world reporting an average of 89 ships per day not including sporting boats and other small watercraft in 2009.[182] It saves an average of 250 nautical miles (460 km; 290 mi), instead of the voyage around the Jutland peninsula.[183] The North Sea Canal connects Amsterdam with the North Sea.

