Sarah Bernhardt (francês: [saʁa bɛʁnɑʁt] nascida Henriette-Rosine Bernard; Paris, 22 de outubro de 1844 — 26 de março de 1923), foi uma atriz francesa, considerada por alguns "a mais famosa atriz da história".[1]
Referência nos palcos franceses no final do século XIX e começo do século XX, Sarah tornou-se uma celebridade por sua atuação em A Dama das Camélias de Alexandre Dumas, em Ruy Blas de Victor Hugo,  Fédora e Tosca de Victorien Sardou, e em L'Aiglon de Edmond Rostand. 
Rostand a chamou de "a rainha da pose e a princesa do gesto", enquanto Hugo elogiou sua "voz de ouro". Conquistou a fama de atriz dramática em papéis sérios, ganhando o epíteto de "A Divina Sarah". Sarah fez várias turnês teatrais ao redor do mundo e foi uma das primeiras atrizes de destaque a fazer gravações de som e atuar em filmes.[2]
Era também ligada ao sucesso do artista Alphonse Mucha, cujo trabalho ela ajudou a divulgar. Mucha se tornaria um dos artistas mais procurados desse período por seu estilo Art Nouveau. Visitou o Brasil quatro vezes, as duas primeiras ainda durante o reinado de D. Pedro II. [2]
Henriette-Rosine Bernard[3] nasceu no número 5 da rue de L'École-de-Médicine no Bairro Latino, em Paris, em 1844.[4] Era a filha ilegítima de Judith Bernard (também chamada de Julie e Youle, na França), uma cortesã judia de origem holandesa com um homem não identificado de classe alta da cidade.[5][6][7][8]
O nome de seu pai é desconhecido, mas sabe-se que ele era um advogado em Le Havre. Bernhardt escreveu posteriormente que a família de seu pai pagou por sua educação, insistiu que ela fosse batizada na Igreja Católica e lhe deixou uma grande soma de dinheiro a ser paga quando se tornasse maior de idade.[9] Sua mãe viajava constantemente, então tinha pouco tempo para passar com a filha. Ela a colocou aos cuidados de uma babá na Bretanha, depois em uma casa no subúrbio de Paris, em Neuilly-sur-Seine.[10]
Aos sete anos, sua mãe a mandou para um internato para moças no subúrbio parisiense, pago com os fundos da família de seu pai. Lá pode atuar pela primeira vez nos palcos, na peça Clothilde, onde ela interpretou a rainha das fadas.[10] Enquanto Sarah estudava, sua mãe chegava ao ápice da carreira como cortesã, frequentando palácios, atendendo banqueiros, generais e escritores. Seus patronos e amigos mais próximos incluíam o meio-irmão do imperador Napoleão III e Charles de Morny, duque de Morny.[11]
Aos dez anos, com a recomendação de Morny, Sarah foi aceita na Grandchamp, uma escola de freiras exclusiva, perto de Versalhes.[12] Lá ela atuou em Tobias and the Angel, ópera de Jonathan Dove e David Lan. Sarah declarou sua intenção de se tornar freira, mas nem sempre seguiu as regras do convento; ela foi acusada de sacrilégio quando arranjou um enterro cristão, com procissão e cerimônia, para seu lagarto de estimação.[13]  
Ela recebeu sua primeira comunhão como católica romana em 1856 e, a partir de então, tornou-se fervorosamente religiosa. No entanto, ela nunca esqueceu sua herança judaica. Quando questionada anos depois por um repórter se ela era cristã, ela respondeu: "Não, sou católica romana e membro da grande raça judaica. Estou esperando até que os cristãos se tornem melhores".[14] Tal declaração contradiz sua resposta de "Não, nunca. Sou ateia" a uma pergunta feita pelo compositor e compatriota Charles Gounod se ela já rezou alguma vez antes.[15] Seja como for, Sarah recebeu os rituais católicos pouco antes de sua morte.[16]
Em 1857, Bernhardt soube que seu pai havia morrido no exterior.[17] Sua mãe convocou a família, incluindo seu amigo Morny, para decidir o que fazer com ela. Morny propôs que Sarah se tornasse atriz, ideia que a horrorizou, pois ela nunca havia entrado em um teatro.[18] Morny conseguiu que ela assistisse à sua primeira apresentação teatral no Comédie-Française em uma festa que incluía sua mãe, Morny, e seu amigo Alexandre Dumas, o pai. A peça em apresentação era Britannicus, de Jean Racine, seguida do clássico da comédia Amphitryon, de Plauto. Sarah ficou tão comovida com a emoção da peça que começou a chorar alto, perturbando o resto da plateia.[18] Morny e outros em seu grupo ficaram com raiva dela e foram embora, mas Dumas a confortou e mais tarde disse a Morny que acreditava que ela estava destinada ao palco. Após a apresentação, Dumas a chamou de "minha estrelinha".[19]
Morny usou de sua influência com o compositor Daniel Auber, então diretor do Conservatório de Paris, para conseguir um teste para Sarah. Ela começou a se preparar, como descreveu em suas memórias, "com aquele exagero vívido com que abraço qualquer novo empreendimento".[20] Dumas a instruiu. O júri foi composto por Auber e cinco atores e atrizes principais da Comédie-Française. Ela deveria recitar versos de Racine, mas ninguém lhe disse que ela precisava de alguém para lhe dar dicas enquanto recitava. Sarah disse ao júri que, em vez disso, recitaria a fábula dos "Dois Pombos" de La Fontaine. Os jurados estavam céticos, mas o fervor e o pathos de sua recitação os conquistaram e ela foi convidada para ser estudante.[21]
Sarah estudou atuação no Conservatório de janeiro de 1860 até 1862 com dois atores proeminentes da Comédie-Française, Joseph-Isidore Samson e Jean-Baptiste Provost. Ela escreveu em suas memórias que Provost ensinou sua dicção e grandes gestos, enquanto Samson lhe ensinou o poder da simplicidade.[22] Para os palcos, ela mudou seu nome de "Bernard" para "Bernhardt". Enquanto estudava, ela também recebeu seu primeiro pedido de casamento, de um rico empresário que lhe ofereceu 500 000 francos. Ele chorou quando ela recusou. Bernhardt escreveu que ela estava "confusa, arrependida e encantada - porque ele me amava do jeito que as pessoas amam nas peças de teatro".[23]
Antes do primeiro teste para a aula de tragédia, ela tentou alisar os fartos cabelos crespos, o que os tornava ainda mais incontroláveis, e pegou um forte resfriado, que deixou sua voz tão anasalada que ela mal a reconheceu. Além disso, os papéis designados para sua atuação eram clássicos e exigiam emoções cuidadosamente estilizadas, enquanto ela preferia o romantismo e a expressão plena e natural de suas emoções. Os professores a classificaram em 14º lugar na tragédia e em segundo lugar na comédia.[24] Mais uma vez, Morny veio em seu socorro. Ele falou bem dela com o Ministro Nacional das Artes, Camille Doucet. Doucet então a recomendou a Edouard Thierry, o principal administrador do Comédie-Française, que ofereceu a Sarah uma vaga como pensionária no teatro, com um salário mínimo.[24][25]
Sarah fez sua estreia na companhia em 31 de agosto de 1862 no papel-título da peça Iphigénie' de Racine.[26] Sua estréia não foi um sucesso. Ela teve medo do palco e apressou suas falas. Alguns membros da platéia zombaram de sua figura muito magra. Quando a apresentação terminou, Provost estava esperando nos bastidores e ela pediu perdão por sua atuação. Ele disse a ela: "Posso perdoar você e você um dia vai se perdoar, mas Racine em seu túmulo nunca o fará." Francisque Sarcey, o influente crítico de teatro de L'Opinion Nationale e Le Temps, escreveu: "ela se comporta bem e pronuncia com perfeita precisão. Isso é tudo o que pode ser dito sobre ela no momento."[27]Sarah não ficou muito tempo no Comédie-Française. Ela interpretou Henrietta na peça Les Femmes Savantes, de Molière e Hipólita, em L'Étourdi, além de ser a protagonista em Valérie, de Eugène Scribe, mas não impressionou os críticos nem outros membros da companhia, que se resssentiam dela por sua rápida ascensão na carreria. Conforme as semanas passaram, ela não recebeu novos papéis.[28] Seu temperamento explosivo também a colocou em apuros; quando um porteiro do teatro se dirigiu a ela como "Pequena Bernhardt", ela quebrou o guarda-chuva na cabeça dele. Ela se desculpou profusamente e, quando o porteiro se aposentou, 20 anos depois, ela comprou uma casa de campo para ele na Normandia.[29] 
Em uma cerimônia em homenagem ao aniversário de Molière em 15 de janeiro de 1863, Sarah convidou sua irmã mais nova, Regina, para acompanhá-la. Regina acidentalmente pisou na cauda do vestido de uma atriz principal da companhia, Zaire-Nathalie Martel (1816–1885), conhecida como Madame Nathalie.[30]
Madame Nathalie empurrou Regina, fazendo-a bater em uma coluna de pedra e cortar a testa. Regina e Madame Nathalie começaram a gritar uma com a outra, e Sarah, dando um passo à frente, deu um tapa no rosto de Madame Nathalie. A atriz mais velha caiu em cime de outro ator. Thierry pediu que Sarah se desculpasse com Madame Nathalie. Sarah se recusou a fazê-lo até que Madame Nathalie se desculpasse com Regina. Sarah já havia sido escalada para um novo papel no teatro e tinha até começado os ensaios. Madame Nathalie exigiu que Sarah fosse retirada do papel, a menos que ela se desculpasse. Como nenhuma das duas cederia e Madame Nathalie era um membro sênior da companhia, Thierry foi forçado a pedir a Sarah que fosse embora.[31]
Sua família não conseguia entender sua saída do teatro; era inconcebível para eles que alguém saísse do teatro de maior prestígio de Paris aos 18 anos.[32] Ela então foi para um teatro popular, o Gymnase, onde se tornou substituta de duas das atrizes principais. Ela quase imediatamente causou outro escândalo nos bastidores, quando foi convidada a recitar poesia em uma recepção no Palácio das Tulherias oferecida por Napoleão III e a Imperatriz Eugenie, junto com outros atores do Gymnase. Ela escolheu recitar dois poemas românticos de Victor Hugo, sem saber que Hugo era um crítico ferrenho do imperador. Após o primeiro poema, o Imperador e a Imperatriz levantaram-se e saíram, seguidos pela corte e pelos outros convidados.[33]
Seu próximo papel no Gymnase, como uma tola princesa russa, era totalmente inadequado para ela; sua mãe disse a ela que seu desempenho foi "ridículo".[32] Com tantas experiências ruins, Sarah decidiu abruptamente deixar o teatro para viajar e, como sua mãe, para ter amantes e patrocinadores. Ela foi brevemente para a Espanha, depois, por sugestão de Alexandre Dumas, para a Bélgica.[34]
Sarah chegou a Bruxelas com cartas de apresentação de Dumas e foi admitida nos níveis mais altos da sociedade. De acordo com alguns relatos posteriores, ela compareceu a um baile de máscaras em Bruxelas, onde conheceu o aristocrata belga Henri, Príncipe de Ligne, e teve um caso com ele.[35] Outros relatos dizem que eles se conheceram em Paris, onde o príncipe vinha com frequência para assistir ao teatro. O caso foi interrompido quando ela soube que sua mãe teve um infarto. Sarah voltou a Paris, onde descobriu que sua mãe estava melhor, mas que ela mesma estava grávida de seu caso com o príncipe. Ela não notificou o príncipe a respeito. Sua mãe não queria que o filho órfão nascesse sob seu teto, então ela se mudou para um pequeno apartamento na rue Duphot e, em 22 de dezembro de 1864, a atriz de 20 anos deu à luz seu único filho, Maurice Bernhardt, dramaturgo e diretor de teatro.[36]
Alguns relatos dizem que o príncipe Henri nunca a esqueceu. De acordo com tais versões, ele descobriu o endereço dela no teatro, chegou a Paris e mudou-se para o apartamento de Bernhardt. Depois de um mês, ele voltou para Bruxelas e disse à família que queria se casar com a atriz. A família do príncipe enviou seu tio, general de Ligne, para romper o romance, ameaçando deserdá-lo se ele se casasse com Bernhardt.[37] De acordo com outros relatos, o príncipe negou qualquer responsabilidade pela criança.
Mais tarde, ela chamou o caso de "sua ferida permanente", mas nunca discutiu a paternidade de Maurice com ninguém. Quando questionada sobre quem era seu pai, ela às vezes respondia: "Eu nunca consegui decidir se o pai dele era Léon Gambetta, Victor Hugo ou General Boulanger."[38] Muitos anos depois, em janeiro de 1885, quando Sarah estava bastante famosa, o príncipe veio a Paris e se ofereceu para reconhecer formalmente Maurice como seu filho, mas Maurice recusou educadamente, explicando que estava totalmente satisfeito por ser filho de Sarah Bernhardt.[39]
Para se sustentar após o nascimento de Maurice, Sarah atuou em papéis menores e substitutos no teatro Porte Saint-Martin, um popular teatro de melodrama. No início de 1866, ela fez um teste com Felix Duquesnel, diretor do Odéon-Théâtre de l'Europe (Odéon) na margem esquerda do Sena. Duquesnel descreveu a leitura anos depois, dizendo: "Eu tinha diante de mim uma criatura que era maravilhosamente talentosa, inteligente no nível de um gênio, com enorme energia sob uma aparência frágil e delicada e uma vontade selvagem." O co-diretor do teatro e encarregado das finanças, Charles de Chilly, queria rejeitá-la por não achá-la confiável e por ser muito magra, mas Duquesnel ficou encantado; ele a contratou para o teatro com um modesto salário de 150 francos mensais, que pagava do próprio bolso.[40] 
O Odéon perdia em prestígio apenas para a Comédie-Française e, ao contrário daquele teatro tão tradicional, especializou-se em produções mais modernas. O Odéon era popular entre os estudantes da margem esquerda. Porém, suas primeiras apresentações no teatro não tiveram sucesso. Ela foi escalada para comédias altamente estilizadas e frívolas do século XVIII, enquanto seu ponto forte no palco era sua total sinceridade.[41] Sua figura magra também a fazia parecer ridícula nos trajes ornamentados. Dumas, seu maior apoiador, comentou após uma apresentação, "ela tem a cabeça de uma virgem e o corpo de um cabo de vassoura".[42] Em junho de 1867, ela interpretou dois papéis em Athalie de Jean Racine; o papel de uma jovem e um menino, Zacharie, o primeiro de muitos papéis masculinos que ela interpretou na carreira. O influente crítico Sarcey escreveu "... ela encantou seu público como um pequeno Orfeu."[42]
A performance que seria o ponto de virada em sua carreira foi a peça de Kean de Alexandre Dumas, de 1868, no qual ela interpretou o papel feminino principal de Anna Danby. A peça foi interrompida no início por distúrbios na platéia por jovens espectadores que gritavam: "Abaixo Dumas! Dê-nos Hugo!". Sarah dirigiu-se diretamente ao público: "Amigos, vocês desejam defender a causa da justiça. Estão fazendo isso responsabilizando Monsieur Dumas pelo banimento de Monsieur Hugo?".[43] Com isso, o público riu e aplaudiu e ficou em silêncio. Na cortina final, ela recebeu aplausos de pé e Dumas correu para os bastidores para parabenizá-la. Quando ela saiu do teatro, uma multidão se reuniu e jogou flores para ela. Seu salário foi imediatamente aumentado para 250 francos por mês.[44]
Seu próximo sucesso foi a atuação em Le Passant, de François Coppée, que estreou no Odéon em 14 de janeiro de 1868.[45] O crítico Théophile Gautier descreveu o "encanto delicado e terno" de sua atuação. A peça teve 150 apresentações, além de uma apresentação principal no Palácio das Tulherias para Napoleão III e sua corte. Depois, o imperador enviou a ela um broche com suas iniciais escritas em diamantes.[46]
Em sua biografia, ela escreveu: 
Sarah morava com sua amiga e assistente de longa data, Madame Guérard, e seu filho em uma pequena cabana no subúrbio de Auteuil, e dirigia-se ao teatro em uma pequena carruagem todos os dias. Sarah tinha uma estreita amizade com a escritora George Sand e atuou em duas peças de sua autoria. Era comum receber celebridades em seu camarim, como Gustave Flaubert e Léon Gambetta.[47]
Em 1869, quando ela melhorou de vida, Sarah mudou-se para um apartamento maior de sete cômodos na rue Auber, 16, no centro de Paris. Sua mãe começou a visitá-la pela primeira vez em anos, e sua avó, uma judia ortodoxa muito rígida, mudou-se para o apartamento para cuidar de Maurice. Bernhardt contratou uma empregada e uma cozinheira, assim como começou a ter vários animais de estimação; ela tinha um ou dois cachorros com ela o tempo todo e duas tartarugas se moviam livremente pelo apartamento.[48]
Em 1868, um incêndio destruiu completamente seu apartamento, junto com todos os seus pertences. Ela havia se esquecido de comprar o seguro. O broche presenteado pelo imperador e suas pérolas derreteram, assim como a tiara presenteada por um de seus amantes, Khalid Bey. Ela encontrou os diamantes nas cinzas e os diretores do Odéon organizaram uma apresentação beneficente. A soprano mais famosa da época, Adelina Patti, se apresentou de graça. Além disso, a avó de seu pai doou 120 000 francos. Sarah conseguiu comprar uma residência ainda maior, com dois salões e uma grande sala de jantar, na 4 rue de Rome.[49]
A eclosão da Guerra Franco-Prussiana interrompeu abruptamente sua carreira teatral. A notícia da derrota do exército francês, a rendição de Napoleão III em Sedan e a proclamação da Terceira República Francesa em 4 de setembro de 1870 foram seguidas por um cerco à cidade pelo exército prussiano. Paris foi cortada das notícias e do abastecimento de alimentos, e os teatros foram fechados. Sarah se encarregou de converter o Odéon em um hospital para os soldados feridos nas batalhas fora da cidade.[50]
Ela organizou a colocação de 32 leitos no saguão e nas coxias, trouxe seu chef pessoal para preparar sopa para os pacientes e convenceu seus amigos ricos e admiradores a doar suprimentos para o hospital. Além de organizar o hospital, ela trabalhava como enfermeira, auxiliando o cirurgião-chefe em amputações e operações.[51] Quando o suprimento de carvão da cidade acabou, Sarah usou velhos cenários, bancos e adereços de palco como combustível para aquecer o teatro.[52] No início de janeiro de 1871, após 16 semanas de cerco, os alemães começaram a bombardear a cidade com canhões de longo alcance. Os pacientes tiveram que ser transferidos para o porão e, em pouco tempo, o hospital foi forçado a fechar. Sarah providenciou a transferência de casos graves para outro hospital militar e alugou um apartamento na rue de Provence para abrigar os 20 pacientes restantes. Ao final do cerco, o hospital de Sarah cuidava de mais de 150 pacientes, incluindo um jovem estudante da École Polytechnique, Ferdinand Foch, que posteriormente comandaria os exércitos dos aliados na Primeira Guerra Mundial.[53]
O governo francês assinou um armistício em 19 de janeiro de 1871, e Sarah soube que seu filho e sua família haviam se mudado para Hamburgo. Ela foi até o novo chefe do executivo da República Francesa, Adolphe Thiers, e conseguiu um passe para ir à Alemanha para devolvê-los. Quando ela voltou a Paris várias semanas depois, a cidade estava sob o domínio da Comuna de Paris. Ela se mudou novamente, levando sua família para Saint-Germain-en-Laye. Mais tarde, ela voltou para seu apartamento na rue de Rome em maio, depois que a Comuna foi derrotada pelo exército francês.[53]
Bernhardt se especializou em representar as obras em verso de Jean Racine, tais como Ifigênia, Fedra ou Racine. Destacou-se especialmente, entre muitas outras, n'A Dama das Camélias, de Alexandre Dumas, filho, Théodora, de Sardou, L´Aiglon, de Edmond Rostand, Izéïl, de Silvestre e Morand, Macbeth, de Shakespeare, Jeanne D´Arc, de Jules Barbier.
Em 1879, saiu pela primeira vez da França, e conheceu a Inglaterra, onde esteve 6 semanas fazendo duas apresentações diárias, obtendo grande êxito. Ao chegar lá foi recebida espetacularmente, e acabou por conhecer um jovem escritor chamado Oscar Wilde que, anos mais tarde, em 1893, lhe escreveria o papel principal da peça Salomé. Nesse mesmo ano, Sarah ascendeu a "Sócio Pleno" da Comédie-Française, hierarquia mais alta desta instituição.
Após seu espetacular sucesso na Inglaterra decidiu fazer sua primeira viagem à América. Partiu para os Estados Unidos em 15 de outubro de 1880. O êxito foi total. Bernhardt faria repetidas viagens pelos Estados Unidos (suas famosas "Viagens de despedida") e também percorreu toda América do Sul, chegando a atuar no Brasil, Argentina, Chile. Viajava de trem e de barco e chegou a cruzar o Cabo Horn. Nos Estados Unidos sua fama era tal que disponibilizaram um trem com sete vagões de luxo chamado Sarah Bernhardt Special, para uso exclusivo da atriz. Suas voltas chegaram também a Austrália e visitou o Havaí e as ilhas Sandwich. Atuou no Egito e na Turquia. Também percorreu a Europa, atuando em Moscou, Berlin, Bucareste, Roma, Atenas. Em sua turnê, atuou não somente em grandes teatros mas também em teatros de ínfima categoria. Nunca chegou a pisar na Ásia.
Foi, possivelmente, a atriz mais famosa do século XIX[54] Treinou muitas jovens na arte da atuação, incluindo a também atriz e cortesã Liane de Pougy.
Alphonse Mucha baseou nela diversas de suas obras emblemáticas do art nouveau e foi o responsável por seus cartazes dos espetáculos.[55]
Embora antes de tudo uma atriz de palco, Sarah Bernhardt gravou diversos cilindros e discos de diálogos célebres de várias produções. Um dos primeiros foi a Fedra, de Jean Racine, na casa de Thomas Edison, durante uma visita a Nova Iorque na década de 1880. Dotada de muitos talentos, ela se envolveu com as artes visuais, pintando e esculpindo, chegando a expor suas obras no Salão de Paris várias vezes, entre os anos de 1874 e 1896. Também posou para Antonio de La Gandara.
Em seu tempo, Sarah Bernhardt teve uma grande influência na grand opera, uma influência que continua até hoje. Tosca e Salome, por exemplo, contêm duas das heroínas mais sensacionais, ambas baseadas em peças escritas especialmente para ela.
Escreveu também três livros: sua autobiografia, intitulada Ma double vie, Petite Idole e L´art du Théâtre: la voix, le geste, la prononciation.
Em 1914 foi condecorada, pelo governo francês, com a Légion d'honneur.[56]
Sua vida familiar não foi simples. Teve uma relação tensa e distante com sua mãe, Julie. Sua genitora nunca foi uma mãe carinhosa e interessada e isso fez com que Sarah sempre buscasse sua aprovação e seu carinho. Julie Bernard sentia predileção tão exclusiva por sua filha Jeanne que descuidou totalmente a educação de sua filha menor, Régine. Sarah Bernhardt sentia predileção por sua irmã pequena Régine e quando se tornou independente levou-a para morar consigo para distanciá-la da mãe e das intenções desta em convertê-la também em cortesã. No entanto, devido ao abandono afetivo que sofreu e o ambiente com sua mãe, fez com que Régine se convertesse em prostituta aos 13 anos. Faleceu aos 18, em 1873 de tuberculose. Sua outra irmã, Jeanne também foi cortesã durante uma época, sempre que tinha necessidade de dinheiro. Para apartá-la da má vida, Bernhardt levou-a consigo como sua companhia e acompanhou-a em várias de suas viagens americanas e européias. Era uma atriz medíocre, mas fazia pequenos papéis e vivia uma vida de luxo junto de sua irmã. Se sabe que sofreu crise de neurose por seu vício em morfina e que teve que ser internada no hospital La Pitié-Salpetrière em Paris sob cuidados do doutor Charcot.
Em 1864 Sarah Bernhardt teve um caso romântico passageiro com um aristocrata belga, Charles-Joseph Eugène Henri, Príncipe de Ligne, com quem ela teve seu único filho, Maurice Bernhardt. Envolveu-se com diversos nomes do cenário artístico da época, como Gustave Doré, Georges Clarin, os atores Jean Mounet-Sully e Lou Tellegen, assim como o célebre escritor Victor Hugo. Sua amizade com Louise Abbema, pintora impressionista catorze anos mais nova que ela, era tão íntima e apaixonada que existiam rumores de que as duas fossem amantes.
Em 1882 casou-se, em Londres, com o ex-oficial militar e ator grego Aristides Damala (conhecido na França pelo nome artístico de Jacques Damala). O casamento, embora tenha durado oficialmente até a morte de Damala, com 34 anos, em 1889, entrou rapidamente em colapso graças às infidelidades do marido e ao vício em morfina alimentado por ele. Nos últimos anos de seu casamento, Sarah Bernhardt teria se envolvido com o Príncipe de Gales, que posteriormente veio a ser o rei Eduardo VII da Inglaterra.[57]
Não era uma pessoa religiosa, e teria dito, certa vez: "Eu, rezar? Nunca! Sou uma ateia".[58]
Sarah Bernhardt foi uma das atrizes pioneiras do cinema, debutando em 1900 no papel de Hamlet, no filme Le Duel d'Hamlet.[59] Veio a estrelar posteriormente oito outros filmes, além de dois filmes biográficos, um dos quais Sarah Bernhardt à Belle-Isle (1912), um filme sobre sua vida cotidiana em sua casa.
Em 1906 rodou La Dame aux Camélies, com Lou Tellegem, em 1912 Elisabeth, rainha de Inglaterra, dirigido por Louis Mercanton. Em 1913 filmou Jeanne Doré, dirigido por Tristam Bernard. Esta película se considera a melhor rodada por Bernhardt e onde se pode observar o melhor da sua arte de atuar. O filme está preservado na Cinémathèque de Paris.
Sarah Bernhardt foi também a primeira atriz-empresária do mundo do espetáculo, graças à sua relação muito tensa com o diretor da Comédie-Française, Perrin. Bernhardt rompeu seu contrato e se demitiu da qualidade de "Sócio Pleno" em 18 de Março de 1880. A Comédie pleiteou contra ela, ganhando na justiça: Sarah Bernhardt teve que renunciar à sua pensão de 43 mil francos que teria tido, caso tivesse permanecido por um mínimo de 20 anos a companhia, e além disso foi condenada a pagar 100 mil francos de multa - multa essa que a atriz nunca chegou a pagar.
Depois de sua esplendorosa primeira viagem americana, que lhe havia feito ganhar uma grande fortuna, Bernhardt arrendou o Teatro Porte Saint-Martin, em 1883. Neste teatro produziu e atuou em obras como Frou-Frou e La Dame aux Camélies, entre outras; durante suas viagens, o teatro permanecia aberto e se estreavam obras continuamente, com distinto sucesso comercial. Bernhardt não duvidava em apoiar o teatro de vanguarda, assim e apesar do repertório clássico, no Porte Saint-Martin estrearam obras de novos autores que rompiam com o teatro tradicional. Depois de alguns anos, Bernhardt alugou o Teatro da Renascença, onde representou muitas obras de sucesso. Em 1899 alugou por 25 anos o enorme Theâtre des Nations, único teatro onde atuaria na França durante os últimos 24 anos de sua vida.
O século XX começou para Sarah Bernhardt com um grande sucesso, L´Aiglon, de Edmond Rostand. A obra estreou em 15 de março de 1900 e obteve um sucesso triunfal. Sarah fez 250 representações da peça e, depois disso, fez outra viagem aos Estados Unidos para representá-la. Em Nova Iorque representou a obra na Metropolitan Opera House e também conseguiu um enorme sucesso.
Em 1905, ao encenar a peça La Tosca, de Victorien Sardou, no, hoje inexistente, Teatro Lírico do Rio de Janeiro, Sarah Bernhardt machucou seu joelho direito, durante a cena final em que deveria pular de um alto muro. Sua perna não se recuperou do ferimento. Em 1915 a gangrena havia tomado conta do membro, que teve de ser amputado inteiramente. Confinada por meses a uma cadeira de rodas, Bernhardt teria, supostamente, recusado uma oferta de 10 mil dólares de um empresário de espetáculos circenses que desejava expor sua perna amputada como uma curiosidade médica (embora P. T. Barnum seja citado normalmente como tendo sido o autor da oferta, ele morrera em 1891). Assim mesmo, ela continuou sua carreira, e, ao contrário da crença popular, sem o uso de uma prótese de madeira (ela teria experimentado uma, porém não gostara). Realizou uma turnê de sucesso nos Estados Unidos em 1915, e, ao retornar à França, já iniciada a Primeira Guerra Mundial, a atriz decidiu fazer uma viagem às trincheiras francesas fazendo atuações para animar as tropas. Encenou doravante apenas produções suas, quase que ininterruptamente, até a sua morte. Entre seus últimos sucessos estão Daniel (1920), La Gloire (1921) e Régine Armand (1922). Sua condição física a forçou a ficar praticamente imóvel sobre o palco, porém o charme de sua voz, que teria se alterado muito pouco com o passar dos anos, garantia os seus triunfos. Em 1922, vendeu sua mansão no campo de Belle Isle-en-Mer, onde havia rodado anos atrás Sarah Bernhardt à Belle-Isle, uma película-documental sobre sua vida. Rodava em 1923 um filme, La Voyante, em sua casa, no Boulevard Péreire, devido à sua saúde muito frágil, quando depois de rodar uma cena, desmaiou.
Em 26 de março de 1923, Sarah Bernhardt morreu, de uremia, sob os cuidados de seu filho Maurice. Foi enterrada, perante uma multidão de admiradores, no Cemitério do Père Lachaise, em Paris.[60]
No fim do século XX, recebeu uma estrela na Calçada da Fama, em Hollywood.
Michelangelo di Lodovico Buonarroti Simoni (Caprese, 6 de março de 1475 — Roma, 18 de fevereiro de 1564), mais conhecido simplesmente como Michelangelo ou Miguel Ângelo,[1] foi um pintor, escultor, poeta, anatomista e arquiteto italiano, considerado um dos maiores criadores da história da arte do ocidente.
Ele desenvolveu o seu trabalho artístico por mais de setenta anos entre Florença e Roma, onde viveram seus grandes mecenas, a família Medici de Florença, e vários papas romanos. Iniciou-se como aprendiz dos irmãos Davide e Domenico Ghirlandaio em Florença. Tendo o seu talento logo reconhecido, tornou-se um protegido dos Medici, para quem realizou várias obras. Depois fixou-se em Roma, onde deixou a maior parte de suas obras mais representativas. Sua carreira se desenvolveu na transição do Renascimento para o maneirismo, e seu estilo sintetizou influências da arte da Antiguidade clássica, do primeiro Renascimento, dos ideais do humanismo e do neoplatonismo, centrado na representação da figura humana e em especial no nu masculino, que retratou com enorme pujança. 
Várias de suas criações estão entre as mais célebres da arte do ocidente, destacando-se na escultura o Baco, a Pietà, o David, as duas tumbas Medici e o Moisés; na pintura o vasto ciclo do teto da Capela Sistina e o Juízo Final no mesmo local, e dois afrescos na Capela Paulina; serviu como arquiteto da Basílica de São Pedro implementando grandes reformas em sua estrutura e desenhando a cúpula, remodelou a praça do Capitólio romano e projetou diversos edifícios, e escreveu grande número de poesias.
Ainda em vida foi considerado o maior artista de seu tempo; chamavam-no de o Divino, e ao longo dos séculos, até os dias de hoje, vem sendo tido na mais alta conta, parte do reduzido grupo dos artistas de fama universal, de fato como um dos maiores que já viveram e como o protótipo do gênio. Foi um dos primeiros artistas ocidentais a ter sua biografia publicada ainda em vida. Sua fama era tamanha que, como nenhum artista anterior ou contemporâneo seu, sobrevivem registros numerosos sobre sua carreira e personalidade, e objetos que ele usara ou simples esboços para suas obras eram guardados como relíquias por uma legião de admiradores. Para a posteridade Michelangelo permanece como um dos poucos artistas que foram capazes de expressar a experiência do belo, do trágico e do sublime numa dimensão cósmica e universal.
Michelangelo foi o segundo filho de Lodovico di Lionardo Buonarroti Simoni e Francesca di Neri Buonarroti. Em sua certidão de batismo seu nome consta de duas formas, Michelagnelo e Michelagnolo Buonaroti; aparece na biografia de Vasari como Michelagnolo Bonarroti e na de Condivi como Michelagnolo Buonarroti.[2][3] Quando jovem assinava como Michelagniolo.[4] Essas primeiras biografias foram escritas quando ele ainda vivia e sua fama estava no auge, e seus admiradores, não contentes em estabelecer uma alta estirpe para sua família — cuja genealogia aparece hoje como duvidosa —, trataram de engrandecer eventos relacionados ao seu nascimento e infância, alegadamente proféticos de sua futura glória. Por exemplo, dizia-se que sua mãe caíra de um cavalo enquanto o carregava nos braços mas teriam saído ilesos do acidente; ainda bebê, dormindo no mesmo berço de um irmão, este contraiu grave doença contagiosa, da qual faleceu, mas Michelangelo milagrosamente não foi contaminado. Também diziam que seu mapa astral preconizava um futuro brilhante, por causa de uma conjunção de Vênus, Marte e Júpiter no Ascendente.[5] Condivi disse que sua família era antiga e pertencia à nobreza, o que era aceito como um fato na época em que viveu. Seria descendente dos condes de Canossa, da região de Reggio Emilia, tendo entre seus ancestrais a célebre Matilde de Canossa, e ligados pelo sangue a imperadores. Um membro da família, Simone da Canossa, teria se radicado em Florença em 1250 e sido feito cidadão da República, encarregado da administração de uma das seis divisões florentinas. Ali mais tarde mudara seu sobrenome de Canossa para Buonarroti, em função do prestígio que vários indivíduos da família chamados Buonarroto adquiriram como magistrados, passando este ramo da Casa de Canossa a ser conhecido como Casa de' Buonarroti Simoni.[6][7]
Lodovico na época do nascimento de Michelangelo era administrador das vilas de Caprese e Castello di Chiusi, subordinadas a Florença. Um mês depois, contudo, expirando o seu mandato, a família se transferiu definitivamente para Florença, mas o bebê, como era um hábito, foi entregue a uma ama para ser criado em Settignano, outra vila florentina, numa propriedade familiar. Com três anos voltou a viver na casa paterna, e com seis perdeu a mãe. Teve como irmãos Lionardo, o primogênito, e mais Buonarroto, Giovansimone e Gismondo.[7] O pai, mesmo possuindo algum prestígio, não era rico. Sua família era numerosa e suas rendas, baseadas principalmente na propriedade rural em Settignano, eram insuficientes para manter um elevado padrão de vida. O salário que recebia da República era baixo, 500 liras a cada seis meses, e ficava obrigado a pagar com ele mais dois notários, três servos e um cavalariço. A antiga fortuna da família, adquirida no comércio e no câmbio, começara a se dissipar com seu próprio pai, que teve de prover dotes para suas filhas, pagar dívidas vultosas e não obteve cargos lucrativos, e a situação piorou na geração seguinte, a ponto de estarem perto de perder seu estatuto de patrícios e decair para a plebe.[8]
Reconhecendo que Michelangelo era especialmente dotado, assim que atingiu a idade adequada Lodovico o enviou para ser educado por Francesco da Urbino, esperando que seguisse uma carreira prestigiada. Para a sua frustração, o filho fez pouco progresso na gramática, no latim e na matemática, e roubava tempo dos estudos para procurar a companhia de artistas e desenhar. Tornou-se amigo de Francesco Granacci, discípulo de Domenico Ghirlandaio, que o incentivou nas artes e o levava para frequentar o atelier de seu mestre, com o resultado de ele abandonar o interesse pela instrução regular, e por isso receber repetidas punições de seu pai e irmãos, para quem a carreira artística era indigna da nobreza de sua linhagem. Mesmo assim, conseguiu finalmente vencer a oposição paterna e ser admitido como discípulo de pintura dos irmãos Davide e Domenico Ghirlandaio, através de um contrato com duração estipulada de três anos, assinado em 1 de abril de 1489, ganhando um salário de 24 florins de ouro, o que não era uma prática costumeira naquele tempo. Disse Condivi que a primeira obra acabada de Michelangelo foi a pintura Santo Antônio Abade atormentado por demônios a partir de uma gravura de Martin Schongauer, tão bem feita que teria suscitado a inveja de Domenico. As relações entre ambos já deviam estar tensas, pois Michelangelo tinha o hábito de jactar-se como superior a Domenico e certa vez ousara corrigir os seus desenhos, humilhando-o, o que não foi pouca coisa, dado que era então um dos pintores mais importantes de Florença, e a insolência deve ter repercutido fundo no espírito do mestre. Outra peça que produziu na época, uma cópia de uma cabeça antiga, teria resultado tão bem que o proprietário do original, recebendo em vez a cópia, não conseguiu perceber a troca. Somente pela indiscrição de um companheiro de Michelangelo a artimanha foi descoberta, e comparando-se ambos os trabalhos, o talento de Michelangelo se tornou reconhecido.[5][7][9]
Mas é provável que esses relatos tenham sido muito magnificados — Vasari, na segunda versão de sua biografia, disse que a obra de Condivi tinha muitas inverdades —,[10] pois considerando o reduzido tempo que permaneceu ali, e sabendo-se hoje dos rigorosos hábitos disciplinares do aprendizado artístico da época, que iniciava com as tarefas mais humildes, ele dificilmente teria tido condições de desenvolver tão cedo uma técnica capaz de produzir obras de qualidade tão alta como é declarado. Ainda seria apenas um serviçal, como todos os principiantes, mantendo os materiais e ferramentas dos mestres e dos discípulos mais graduados em ordem e em condições de uso, limpando o espaço, e ficando à disposição dos mestres para atender quaisquer outras demandas para o bom funcionamento da oficina. No pouco tempo que lhes restava era-lhes permitido exercitar o desenho através da cópia de modelos consagrados, mas isso nessa primeira fase era raro, pois além do trabalho servil ser exaustivo o papel era caríssimo e não podia ser gasto à toa com alunos ainda despreparados. Somente quando os alunos dominavam essa parte instrumental e já conheciam em profundidade as propriedades dos materiais da arte era-lhes dado acesso ao conhecimento dos rudimentos mais básicos da criação, servindo então como assistentes diretos dos mestres, mas ainda apenas esticando as telas e preparando os painéis em madeira, dando-lhes as camadas de base, pintando alguns detalhes menos importantes da composição e se aprofundando no estudo do desenho. Entretanto, parece certo que quando ingressou na oficina de Ghirlandaio Michelangelo já havia praticado muito desenho, e assim é difícil determinar com exatidão até onde vai a verdade das biografias primitivas, até porque elas constantemente tendem a exaltar o seu sujeito, mesmo que seja reconhecido que seu talento foi precoce e seu desenvolvimento, muito rápido.[11]
Michelangelo não terminou seu aprendizado com os Ghirlandaio. Um ano depois deixou o atelier e entrou na proteção de Lourenço de Médici. Os autores divergem sobre as circunstâncias desse evento. Talvez por seu temperamento rebelde ele tenha se tornado uma presença irritante para os seus mestres, também ele aparentemente não apreciava tanto a pintura como a escultura; Barbara Somervill disse que seu pai, confiando na força de um parentesco distante com os Médici e na disposição de Lourenço em ajudar seus familiares pobres, apelou para que ele o aceitasse como aprendiz;[12] Vasari e Condivi alegam que foi por solicitação direta de Lourenço a Lodovico.[13][14] Seja como for, com quinze anos de fato ele passou a viver no palácio dos Medici. Lourenço era o chefe de sua ilustre família, então a mais rica da Itália, governava de facto Florença embora não tivesse cargo oficial, e reunira em torno de si uma brilhante corte de humanistas e artistas, sendo ele próprio um poeta e intelectual. Foi uma circunstância afortunada para Michelangelo, pois recebia o atraente salário de cinco ducados por semana, e pôde desfrutar da amizade pessoal com o mecenas, comendo em sua mesa, e da atmosfera erudita do seu círculo, do qual participavam Angelo Poliziano, Pico della Mirandola e Marsilio Ficino, reforçando sua educação precária e entrando em contato com o Neoplatonismo. Fez amigos também entre os filhos da casa, que mais tarde se tornaram seus patronos, e mais importante para sua carreira foi poder frequentar o célebre Jardim de Esculturas que Lourenço organizara com uma importante coleção de fragmentos da Antiguidade clássica, de cujo estudo retirou substancial informação para desenvolver seu estilo pessoal na escultura.[15][16]
Para administrar esse jardim, Lourenço contratara o escultor Bertoldo di Giovanni, que havia sido aluno de Donatello, e com ele Michelangelo teve algo que se aproximou de um professor de escultura, embora aparentemente não tenha seguido seus métodos. Sua primeira obra para Lourenço parece ter sido uma cabeça de fauno, que não sobreviveu, mas segundo consta foi tão bem realizada que com ela Lourenço definitivamente se rendeu ao talento do jovem.[16] Outras obras dessa fase foram um crucifixo para o prior do Hospital do Santo Espírito, que lhe permitia dissecar cadáveres para estudar sua anatomia, um baixo-relevo hoje conhecido como a Madonna da Escada, à maneira de Donatello, e o alto-relevo da Centauromaquia, criado sob o conselho de Poliziano e possivelmente inspirado em um motivo encontrado em um sarcófago romano, que despertou a admiração, até das gerações seguintes, como uma obra já madura, ainda que tenha sido deixado inconcluso.[17]
Pouco depois, em 8 de abril de 1492, Lourenço faleceu, deixando o governo para seu filho Pedro de Médici (Piero), de apenas vinte e um anos de idade. Segundo Condivi, para Michelangelo a morte de seu patrono foi um grande choque, tendo permanecido dias em funda tristeza, incapaz de qualquer ação. Retirou-se para a casa de seu pai, onde esculpiu um Hércules de grandes dimensões, que foi vendido para Francisco I da França, mas do qual não se conhece o paradeiro. Sucedeu então que caísse uma grande nevasca sobre Florença, e então Pedro lembrou-se do amigo. Intimou que ele acorresse ao seu palácio para fazer um boneco de neve, e renovou o convite para que o artista vivesse no palácio Medici a fim de que as coisas continuassem da maneira que eram antes da morte de Lourenço. O convite foi aceito e Michelangelo novamente se tornou um favorito, mas Pedro carecia de toda a sabedoria política de seu pai, era tirânico e completamente inepto para a sua função. Tanto que atraiu a condenação de Savonarola e o descontentamento popular cresceu rápido. Percebendo o rumo fatal que os acontecimentos tomavam, e por causa de sua íntima associação com Pedro, Michelangelo fugiu secretamente primeiro para Bolonha, e depois seguindo para Veneza, poucas semanas antes de Florença ser invadida por Carlos VIII de França e Pedro ser derrubado e expulso de lá junto com toda a sua família.[18][19]
Não conseguindo trabalho em Veneza, voltou para Bolonha, onde encontrou um novo patrono em Gianfrancesco Aldovrandi, em cuja casa permaneceu por um ano. Por sua sugestão produziu figuras para a tumba inacabada de São Domingos, um Anjo segurando um candelabro, um São Proclo e um São Petrônio, além de entreter seu mecenas com leituras de Dante, Petrarca e Boccaccio, apreciadas por seu dialeto toscano ser o mesmo em que haviam sido escritas.[20][21] Entretanto, conheceu obras classicistas de Jacopo della Quercia, que exerceram significativa influência em seu estilo.[22] No inverno de 1495 voltou brevemente a Florença. Condivi e Vasari relataram que Michelangelo encontrou-se com Lorenzo di Pierfrancesco de' Medici, que o encorajou a esculpir um São João, e depois um Cupido adormecido, induzindo o artista a patiná-lo para que pudesse ser vendido como uma antiguidade por um bom preço no mercado romano. Michelangelo o teria enviado para Roma em 1496 e sido adquirido pelo Cardeal Raffaele Riario, mas Clacment alega que a história é muito duvidosa.[23]
De qualquer forma ele viajou para Roma em seguida e hospedou-se por um ano com Riario, mas para ele aparentemente não produziu nada. Sua obra seguinte, um Baco embriagado de grandes dimensões e traços claramente clássicos, foi feita a pedido do banqueiro Jacopo Galli, que solicitou ainda um Cupido em pé, e através de quem Michelangelo conheceu o Cardeal Jean de la Grolaye de Villiers, embaixador da França junto ao Papa, que encomendou a célebre Pietà, um tema raro na Itália mas comum na França, que foi imediatamente aclamada como uma obra-prima, alçando-o à fama. Logo recebeu outras comissões, incluindo quinze estatuetas de santos para o Cardeal Francesco Piccolomini, mas realizou destas apenas quatro, interrompendo o trabalho em 1501 para atender um chamado da Catedral de Florença.[24][25]
A encomenda foi de um David, a ser instalado nos contrafortes da Catedral. Michelangelo escolheu para a obra um enorme bloco de mármore que havia sido trabalhado parcialmente por outros escultores mas permanecia abandonado há quarenta anos, com mais de 5 metros de altura. Talhar uma obra desse vulto ainda hoje é um desafio técnico enorme, e quando pronta em 1504 o resultado foi considerado tão brilhante e magnificente que foi formada uma comissão de notáveis para decidir onde colocá-la, pois se julgou merecer uma posição mais destacada do que a prevista anteriormente. Assim, foi instalada diante do Palácio dos Priores, a sede administrativa da República, como um símbolo das virtudes cívicas florentinas. Durante esses anos envolvido com o David, Michelangelo ainda achou tempo para criar várias Madonnas para patronos privados, uma em forma de estátua, duas em relevo e uma pintura, esta sendo especialmente significativa como um exemplo precursor do Maneirismo florentino.[26] Condivi mencionou mais duas obras, em bronze, um David e uma Madonna, que não são conhecidas.[27] Depois do sucesso absoluto de seu David, Michelangelo foi atraído para projetos monumentais, mas raramente aceitava ajudantes diretos, de forma que muitos deles não foram acabados. Foi o caso da outra empreitada com que os magistrados florentinos o incumbiram, um grande afresco para a Sala do Conselho, representando a Batalha de Cascina, um evento da guerra em que Florença conquistou Pisa. Leonardo da Vinci foi convidado no mesmo momento para fazer outra grande pintura na parede oposta da sala. Nem uma das duas foi terminada, e a de Michelangelo sequer saiu do estudo preparatório. Em 1505, Michelangelo aceitou um pedido de doze grandes Apóstolos em mármore para a Catedral, mas somente um, Mateus, foi começado, e mesmo este foi abandonado antes de acabar, pois o Papa Júlio II o chamara para Roma.[26]
Júlio era tão fascinado pelo grandioso quanto Michelangelo, e era voluntarioso; seus atritos com o artista, cujo temperamento também era forte, se tornaram lendários. Planejara erguer uma portentosa tumba para si mesmo, com quarenta estátuas.[26] Definido o desenho, Michelangelo viajou para as minas de mármore em Carrara para selecionar as pedras, passando lá oito meses. Quando o material chegou a Roma ocupou boa parte da Praça de São Pedro.[28] Mas estando Júlio engajado ao mesmo tempo na reconstrução da vasta Basílica de São Pedro, os fundos para o trabalho logo secaram. Michelangelo supôs que o arquiteto de São Pedro, Bramante, havia envenenado o papa contra ele, e deixou Roma, voltando para Florença. O papa fez pressão sobre as autoridades florentinas exigindo o seu retorno, e em vez de continuar as obras da sua tumba mandou-o criar uma colossal estátua sua em bronze para instalar em Bolonha, que recém havia conquistado em suas expedições militares. Depois de pronta o fez aceitar, a contragosto, o encargo de pintar o enorme teto da Capela Sistina, completado em apenas quatro anos, entre 1508 e 1511. O resultado foi muito além das expectativas papais, e mesmo que Michelangelo não estivesse muito à vontade com a técnica da pintura, preferindo sempre a escultura, deu provas de possuir um gênio pictórico comparável ao que produziu o David e a Pietà.[26] 
Assim que terminou o teto, Júlio mandou que ele voltasse a trabalhar em sua tumba, que jamais foi acabada segundo o plano original. Júlio morreu em 1513 e o projeto então foi revisado várias vezes e sucessivamente reduzido pelos outros papas, transformando-se em uma obra muito mais modesta do que a pretendida. Das quarenta estátuas do plano o monumento atual possui apenas sete, e destas somente o Moisés (1513–15) tem real valor, sendo uma contrapartida escultórica das grandes figuras do teto da Sistina. Seis outras, inacabadas mas também de grande interesse, representando escravos e prisioneiros, originalmente pretendidas como parte do conjunto, foram dispersas e estão hoje no Museu do Louvre em Paris e na Galleria dell'Accademia de Florença. Outra peça importante do período foi um Cristo Redentor nu para a Igreja de Santa Maria sobre Minerva.[26]
O sucessor de Júlio foi um amigo de juventude de Michelangelo, o segundo filho de Lourenço de Médici, Giovanni, que foi sagrado papa com o nome de Leão X. O governante de Florença então era o Cardeal Giulio de' Medici, mais tarde também papa com o nome de Clemente VII. Ambos empregaram o artista principalmente em Florença em obras de glorificação de sua família. Para eles Michelangelo penetrou no terreno da arquitetura, elaborando um plano para a remodelação da fachada da Basílica de São Lourenço, nunca concretizado, mas os seus esforços deram melhores frutos em um projeto menor, a construção e decoração da Sacristia Nova, ligada à Basílica. As obras mais significativas na Sacristia são as originais tumbas de Juliano II de Médici e Lourenço II de Médici, que compreendem cada uma uma estátua idealizada do morto e duas figuras decorativas reclinadas sobre o caixão, nem todas inteiramente acabadas mas de grande pujança, já em um estilo claramente maneirista. No mesmo período Michelangelo projetou outro edifício anexo à Basílica, a Biblioteca Laurenciana, para receber o acervo legado pelo papa Leão X após sua morte. A estrutura é marcante pela sua livre interpretação dos cânones arquitetônicos clássicos, tornando-a o primeiro e um dos mais importantes exemplos do Maneirismo arquitetural.[26]
Em 1527 Roma foi invadida e violentamente saqueada por tropas rebeldes de Carlos V, imperador do Sacro Império. O papa fugiu, e Florença se revoltou novamente contra os Medici, banindo-os. Em seguida a cidade foi assediada, e nesse período Michelangelo foi empregado pelo governo local em obras de engenharia, projetando fortificações. Esta década e a seguinte foram especialmente difíceis para ele. Seu pai morrera em 1521 e em seguida seu irmão favorito. Michelangelo se preocupava com o avanço dos anos e temia a morte, e ainda se envolveu em assuntos familiares para assegurar a perpetuação do nome Buonarroti. Em sua vida afetiva se ligou fortemente a homens jovens, em especial a Tommaso dei Cavalieri, trocando calorosa correspondência e escrevendo-lhes poesias de grande qualidade, tratando do tema do amor na tradição de Petrarca e expressando ideias neoplatônicas. Essas ligações e esses testemunhos materiais têm sido considerados por grande número de estudiosos como evidências de homossexualidade, mas para uma minoria influente, da qual participa Gilbert Creighton, editor da Britannica, é provável que ele estivesse mais preocupado em encontrar um filho adotivo e que seu transbordamento emocional não passasse de retórica literária.[26][29][30] Em 1530 os Medici conseguiram impor definitivamente seu governo em Florença, Michelangelo voltou ao projeto das tumbas da família e produziu duas esculturas, um Gênio da Vitória, que se tornou um protótipo para os escultores maneiristas, e um David, às vezes identificado também como Apolo. Em 1534 deixou a cidade pela última vez, a chamado do novo papa, Paulo III, passando a residir em Roma, embora tenha sempre alimentado a esperança de poder voltar e terminar seus projetos inacabados.[26]
Nessa fase Michelangelo deixou um pouco de lado a escultura e se voltou para a arquitetura, a poesia e a pintura. Paulo III o chamara para pintar a cena do Juízo Final na parede atrás do altar da Capela Sistina. A composição foi outra obra-prima, mas em um estilo muito diverso daquele do teto, e reflete o impacto da Contra-Reforma na cultura da época. A concepção é poderosa e as figuras ainda são grandiosas, mas sua descrição anatômica é menos clara. Por outro lado, a intensidade psicológica e dramática é muito mais impressionante.[26] Uma cena prevista para a parede oposta, mostrando a Queda de Lúcifer, foi desenhada em cartão mas não realizada. Entretanto, de acordo com Vasari o desenho foi aproveitado por um artista menor na Catedral de Todi, com uma execução pobre.[31] Imediatamente depois foi convocado para pintar mais dois grandes painéis na Capela Paulina, ilustrando a Crucificação de São Pedro e a Conversão de Saulo. Nesse período desenvolveu uma profunda ligação afetiva com a patrícia romana Vittoria Colonna, que perdurou até a morte dela em 1547, compartilhando um interesse pela poesia e pela religião. Desenhou a remodelação da Praça do Capitólio, um dos projetos urbanísticos mais notáveis da cidade, e na sua condição de novo arquiteto de São Pedro, cargo aceito também com grande relutância, elaborou os planos para a reforma de sua estrutura a partir das ideias deixadas por Bramante, descartando acréscimos de outros colaboradores e revertendo a planta para cruz grega. Também desenhou a cúpula, uma grande obra de arquitetura, embora construída somente depois que morreu, com ligeiras modificações. Enquanto trabalhava em São Pedro se envolveu em projetos arquitetônicos menores, completando o inacabado Palácio Farnese, dando aconselhamento nas obras da Villa Giulia, da Igreja de São Pedro em Montorio e do Belvedere do Vaticano, além de fornecer um projeto, não utilizado, para a remodelação da Basílica de São João dos Florentinos.[26][32]
Em 1555 Paulo IV ascendeu o papado e de imediato abriu um conflito com o governo espanhol em Nápoles, ao mesmo tempo em que intensificou os procedimentos da Contra-Reforma e apoiou a Inquisição. Cancelou a chancelaria de Rimini que Paulo III havia outorgado a Michelangelo, uma boa fonte de renda para ele, e quis destruir o Juízo Final da Sistina, considerado indecente, o que só não ocorreu graças à firme oposição de vários cardeais; mesmo assim vários nus foram cobertos. O clima em Roma se tornou tenso, tropas francesas entraram nos Estados Papais e Michelangelo, em 1557, buscou refúgio temporário em um mosteiro em Spoleto, deixando as obras na Basílica a cargo de auxiliares. Voltando a Roma pouco depois, passou a se dedicar ao projeto de um túmulo para si mesmo, nunca executado, mas para ele esculpiu a Pietà de Florença, onde se acredita que ele tenha deixado seu auto-retrato na figura de José de Arimateia. Então voltou às obras de São Pedro, mas suas decisões eram continuamente desacatadas pelos assistentes, criando uma situação estressante. Em 1559 o papa morreu. Era tão odiado que o povo romano deu grandes manifestações de júbilo ao saber da notícia, e Duppa diz que deve ter sido um alívio também para o artista.[33]
Pio IV manteve Michelangelo como arquiteto de São Pedro — desta época é o projeto da cúpula — e lhe restituiu parte das rendas de Rimini. Desenhou um monumento em honra ao irmão do papa a ser instalado na Catedral de Milão, executado por outros, construiu a Porta Pia, reformou as Termas de Diocleciano, transformando-as na Basílica de Santa Maria dos Anjos e dos Mártires, e projetou uma capela na Basílica de Santa Maria Maior, terminada postumamente. A nomeação de Michelangelo como arquiteto-chefe de São Pedro nunca agradara aos diretores da obra e aos arquitetos assistentes, as pressões por fim acabaram por triunfar, e em 1562 ele foi removido do cargo. Mas logo a situação reverteu a seu favor, pois Michelangelo solicitou uma entrevista com o papa e lhe expôs as intrigas que haviam levado à situação. O papa mandou examinar o caso, confirmou as alegações de Michelangelo e o reconduziu à chefia das obras, e mais, ordenou que suas diretrizes fossem seguidas à risca.[34]
Em 1563 foi eleito primus inter pares da Accademia del Disegno de Florença, recém fundada por Cosimo I de' Medici e Vasari, e somente depois disso, às portas dos noventa anos de idade, sua saúde e vigor começaram a declinar rápida e visivelmente. Pouco tempo lhe restava, e na passagem de 1563 para 1564 se tornou claro que já não poderia sair à rua a qualquer hora e sob qualquer tempo como costumava, e nem podia mais recusar a ajuda de outros como fora seu hábito perene. Em 14 de fevereiro de 1564 sofreu uma espécie de ataque, e espalhou-se a notícia de que ele estava doente. Não obstante, seu amigo Tiberio Calcagni, que correu visitá-lo, o encontrou na rua debaixo da chuva, dizendo que não encontrava sossego de forma alguma. De acordo com o relato, sua face estava com uma péssima aparência e sua fala era hesitante. Entrando em casa, recolheu-se para descansar. Outros amigos vieram para atendê-lo, no dia seguinte pressentiu a morte e mandou chamar seu sobrinho Lionardo, mas este não chegou a tempo de vê-lo vivo. Faleceu pacificamente pouco antes das cinco da tarde do dia 18, na companhia de Tiberio Calcagni, Diomede Leoni, Tommaso dei Cavalieri e Daniele da Volterra, além dos médicos Federigo Donati e Gherardo Fidelissimi.[35][36]
Por ordem do governador de Roma o corpo foi depositado com grandes honras na Basílica dos Doze Santos Apóstolos,[37] mas Lionardo desejava que ele repousasse em Florença, e teve de roubar o cadáver e despachá-lo para a outra cidade disfarçado como mercadoria, sendo entregue na alfândega local em 11 de março. Dali foi removido para um oratório, e no dia seguinte em segredo foi levado para a Basílica da Santa Cruz, mas o movimento foi percebido por populares e logo uma grande multidão se formou para acompanhar o cortejo, prestando-lhe sua última homenagem. O grupo entrou na Basílica, que ficou completamente lotada, e o lugar-tenente da Accademia ordenou que o caixão fosse aberto. Segundo os registros, após vinte e cinco dias de seu falecimento, o corpo ainda estava intacto e sem qualquer odor. Então foi enterrado atrás do altar dos Cavalcanti. Em 14 de julho uma grande cerimônia pública homenageou sua memória. Os poemas e panegíricos escritos para o dia encheram um volume, que foi publicado em seguida. Sua tumba definitiva foi desenhada por Giorgio Vasari e está na Basílica da Santa Cruz. Mais tarde diversas cidades ergueram-lhe monumentos.[38]
Quando adulto Michelangelo tinha uma estatura mediana e possuía ombros largos e braços fortes, resultado de suas infindáveis horas trabalhando com a pedra. Seu cabelo era escuro e seus olhos pequenos e castanhos, usava a barba dividida em duas, tinha os lábios finos, o nariz quebrado de uma luta na juventude com Pietro Torrigiano, e sua testa era saliente. Não dava a mínima atenção à sua aparência física, vestia-se com roupas velhas, às vezes até esfarrapadas, que estavam invariavelmente sujas. Mesmo assim não raro dormia com elas e com seus sapatos. Da mesma forma, era indiferente quanto à comida, comia pouco e irregularmente, tinha má digestão; ficava tão satisfeito com um pedaço de queijo como com uma refeição de vários pratos, como as que comia quando convidado pelos poderosos. Não fazia caso de onde ia dormir e tinha um sono curto, sofria de dores de cabeça e com o avançar dos anos teve problemas de vesícula e reumatismo nas pernas, mas em geral gozou de boa saúde até seu último ano de vida. Trabalhava incansavelmente, pôde adquirir uma educação geral bastante larga mesmo sem instrução regular, e poucas coisas o interessavam além de sua arte.[39][40] Entre elas, como se depreende de suas cartas, ele tinha preocupações quanto à perpetuação e dignificação do nome familiar. Em várias, dirigidas a seu sobrinho Lionardo, urgiu que ele se casasse com uma jovem da nobreza, digna dos Buonarroti, e encareceu que ele deixasse o campo e morasse em um palacete urbano, o sinal mais evidente do status de um patrício. Em outras expressa sua ambição de "ressuscitar a sua Casa", e seu desejo de glória tanto pessoal como familiar é documentado por outros testemunhos.[41]
Enquanto viveu se formou um folclore a respeito de sua personalidade, descrevendo-o como terribile, ou seja, passional e violento. Também era considerado desconfiado, irritável, antissocial, excêntrico e melancólico, tímido e avarento, e muitos o chamavam de louco. Vasari e Condivi consideraram necessário enfatizar que essas descrições eram caluniosas, mas isso prova que elas eram correntes, mesmo que possam não ter correspondido à toda a verdade. Eles em vez o descreveram como uma pessoa profundamente religiosa, em quem a pregação de Savonarola sobre o despojamento dos bens mundanos exercera duradouro impacto. Lera suas obras até o fim de seus dias e dizia que recordava claramente da sua voz. Disseram ainda que era liberal e generoso, dando obras valiosas de presente para seus amigos e sendo gentil com seus servos. Como professor não escondia seu conhecimento dos discípulos, mas não gostava que fosse divulgado que ele ensinava. Vários de seus alunos o chamavam de pai. Não era desprovido de senso de humor, e às vezes buscava a companhia de pessoas capazes de fazê-lo rir. Entre elas apreciava especialmente os pintores Jacopo Torni, Sebastiano del Piombo e o próprio Vasari, com quem se divertia. Era sensível ao trabalho alheio qualificado, e louvava até o de antigos rivais como Rafael, mas várias vezes expressou seu desprezo pela mediocridade e pela pretensão de outros. Era admirador entre outros de Donatello, Ticiano, Ghiberti e Bramante, e mesmo de artistas pouco conhecidos como Antonio Begarelli e Alessandro Cesari, em quem encontrava qualidades invisíveis para outros. Sobrevivem documentos que atestam sua natureza generosa e benevolente, mas outros em parte confirmam aquele folclore, incluindo sua própria correspondência. Mas é de lembrar em se tratando de um artista tão diferenciado em relação aos seus contemporâneos, uma pessoa submetida a pressões internas e externas desconhecidas pela maioria, obviamente não possuía a mesma natureza que um homem comum e ele por consequência não poderia se comportar como tal. Sem entrar numa apologia do gênio, seu enorme talento, suas ideias artísticas visionárias e de amplitude titânica, sua insatisfação com a conquista ordinária e a sua infatigável capacidade de realização, dons que se por um lado foram reconhecidos universalmente e atraíram a admiração e o assombro gerais e lhe valeram o epíteto de divino, por outro com toda a probabilidade o separaram psicologicamente do resto dos humanos, nem se pode esperar que universos tão distintos pudessem se compreender ou conviver sem tensões importantes.[42][43][44]
É muito difícil fazer uma ideia da evolução de sua riqueza pessoal. Herdou terras em Settignano e foi capaz de torná-las bem mais produtivas do que no tempo de seu pai, e até expandiu sua área. Possuía uma casa-atelier em Roma, duas casas e um atelier em Florença, e se diz que tinha terras em vários locais da Toscana. Suas maiores obras foram pagas regiamente, mas muitas vezes os custos do material, que não eram baixos, estavam incluídos. Além disso, muitas vezes seus patronos lhe pagaram irregularmente, em diversas ocasiões não recebeu o pagamento completo e obras como a tumba de Júlio II representaram despesa e não ganho para ele. Por outro lado, com seus hábitos espartanos de vida fez uma boa economia, e numa carta disse que Paulo III o cumulara de benefícios. Doou altas somas para caridade e sustentou seus familiares quando pôde, e várias vezes ajudou artistas pobres, inclusive seus dois biógrafos.[45] Não confiava em bancos e guardava seu dinheiro em um baú embaixo da cama. Quando morreu este baú continha dez mil ducados de ouro, uma quantia, segundo Forcellino, suficiente para comprar o Palácio Pitti.[46]
Michelangelo nunca se casou e hoje é praticamente um consenso que tenha sido homossexual, a despeito da negação de seus primeiros biógrafos. Tem sido aventado que o artista teve casos amorosos concretos com vários jovens, como Cecchino dei Bracci, para quem desenhou o túmulo, e Giovanni da Pistoia, que conheceu enquanto trabalhava no teto da Capela Sistina, e para quem escreveu alguns sonetos.[47] Mas nenhuma prova concludente se encontrou nessa direção, e é bastante possível que o próprio Michelangelo, refreando seus sentimentos e necessidades, tenha fugido de uma consumação carnal. Vários fatores podem ser considerados para tornar a hipótese plausível. Em sua juventude em Florença ficara profundamente impressionado com a pregação de renúncia ao mundo de Girolamo Savonarola, e expressou sua admiração por ele ao longo de toda a vida. Em segundo lugar se considere a influência da visão humanista-neoplatônica de sua época sobre o amor, outro elemento relevante em seu universo pessoal, que falava do corpo como o cárcere terreno, e ainda que aceitasse o amor entre homens e até o estimulasse, não aprovava o contato físico, lançando a vivência do sentimento num plano espiritual. Além disso, a opinião pública sobre a homossexualidade no século XVI era bastante negativa; em Florença os homossexuais podiam ser castrados ou condenados à morte. O que transparece fortemente de suas poesias é o perene conflito entre o impulso ao amor terreno e ao amor divino, que, como ele mesmo disse, "o mantinha dividido em duas metades", e segundo Harmon, pelo que se sabe sobre sua vida, não há como excluir nenhum dos opostos no estudo de sua personalidade e de sua forma de amar. Ao mesmo tempo em que reiteradas vezes falou do amor dirigido a pessoas como a força dinâmica que o capacitava à transcendência — "o amor nos urge e desperta, dá penas às nossas asas, e a partir daquele primeiro estágio, com o qual a alma não se satisfaz, ela pode voar e subir ao seu Criador" — em outros momentos declarava seu desejo de intimidade física, querendo "abraçar meu tão desejado, meu tão doce senhor, com meus braços indignos", ou imaginando ser um bicho-da-seda para tecer uma túnica preciosa "envolvendo seu belo peito com prazer".[30][48][49][50] Condivi registrou que "muitas vezes ouvi Michelangelo discursar a respeito do amor, mas jamais o ouvi falar qualquer coisa diferente do amor platônico".[51] Dizem Ryan e Ellis, invocando mais outros autores, que a maioria dos historiadores modernos reconhece a inclinação homoerótica de Michelangelo, mas a questão de se isso o levou a uma vida sexualmente ativa permanece uma incógnita.[52][53]
Entre os homens quem ocupou o maior lugar em seus pensamentos foi Tommaso dei Cavalieri, um patrício amante das artes. Na época Cavalieri era um jovem de 17 anos de idade, e Varchi, que também o conheceu, disse que ele que tinha um temperamento calmo e despretensioso, uma fina inteligência e educação, e uma beleza incomparável, e por tais qualidades merecia o amor de quantos o conhecessem. Logo após seu primeiro contato Michelangelo enviou-lhe duas breves cartas.[54] Numa delas disse:
"Percebo agora que não posso esquecer vosso nome assim como não posso esquecer a comida com a qual vivo — não! antes eu poderia esquecer a comida com que vivo, que infelizmente alimenta apenas o corpo, mas não vosso nome, que nutre minha alma e meu corpo, enchendo ambos de tamanho deleite que me torno imune à tristeza e ao medo da morte, isso enquanto vossa memória dura em mim. Imaginai se o meu olho estivesse também fazendo sua parte (uma referência à distância física entre eles) o estado em que eu me encontraria!".[54]Em outra carta, para seu amigo Sebastiano del Piombo, disse:
"Se o vires, imploro-te que me recomendes a ele mil vezes, e quando tu me escreveres diz-me algo a seu respeito para eu ter o que colocar na mente, pois se eu esquecê-lo creio que no mesmo instante cairei morto".[54]Para ele Michelangelo escreveu cerca de quarenta poemas, presenteou-o com desenhos, e foi o único de quem pintou um retrato, uma obra infelizmente perdida. Entre os desenhos que deu a Tommaso estão um Rapto de Ganimedes, a Queda de Phaeton, a Punição de Tytus, e um Bacanal de crianças, cujos temas são sugestivos. Ainda que Cavalieri tenha retribuído o amor do artista em grande medida e o tenha expressado várias vezes, inclusive em cartas, não parece ter sido apaixonado, e o teria cultivado dentro da esfera da amizade, o que segundo Ryan foi fonte de muita angústia e desapontamento para Michelangelo.[55] Entretanto, em uma das cartas que Tommaso enviou a Michelangelo se encontra uma passagem ambígua que reza: "…che Vostra Signoria torni presto, perché tornando liberarete me di prigione: perché io fuggo le male pratiche, e volendo fugirle non posso praticare altri che con voi".[56] Uma tradução direta é "...que Vossa Senhoria volte logo, porque voltando me libertareis da prisão: porque eu fujo das más práticas, e querendo fugir delas não posso praticar com ninguém mais senão convosco". Frederick Hartt traduziu praticare como fazer amor,[57] mas vários dicionários consultados não fazem qualquer associação de praticare com fazer amor, e a traduzem no sentido de fazer amizade, frequentar, visitar com frequência e conhecer,[58][59][60] de modo que a interpretação desta passagem permanece duvidosa. O que é certo é que sua relação se transformou em uma sólida lealdade, sobrevivendo a alguns atritos e à transformação do jovem em um pai de família, perdurando até a morte de Michelangelo.[55] Segue um dos sonetos que lhe dedicou:
A outra figura de grande importância em sua vida pessoal foi Vittoria Colonna. Descendente de uma família nobre, foi uma das mulheres mais notáveis da Itália quinhentista. Ainda jovem casou-se com Fernando de Ávalos, Marquês de Pescara. Tornou-se autora de poesias louvadas como impecáveis, das mais importantes continuadoras da tradição de Petrarca em sua geração, uma mediadora política, reformadora religiosa, e seus méritos próprios foram amplamente reconhecidos ainda em sua vida, mas a historiografia posterior a retratou indevidamente mais como uma figura passiva, à sombra de grandes homens que conheceu, entre eles Michelangelo.[62][63] É possível que tenham se encontrado em torno de 1537, mas sua relação só se estreitou em torno de 1542 quando Michelangelo já era idoso e ela, viúva há dezessete anos. Discutiam arte e religião. Para ela Michelangelo escreveu várias poesias e produziu desenhos, e ela por sua vez dedicou-lhe também uma série de poemas. O afeto de Michelangelo tornou-se intenso, e em seus sonetos meditava se esta não seria mais uma paixão infrutífera, talvez a mais infeliz de todas. Apesar de suas dúvidas, o tom geral de suas poesias sobre ela é calmo e doce, e busca a sublimação de forma mais consistente através da fé. Walter Pater comparou a relação de ambos com a de Dante e Beatriz.[64]
A fé em que Michelangelo se apoiou para enfrentar os dilemas de seus sentimentos foi a da Contra-Reforma, que depositava a responsabilidade pela solução dos problemas espirituais mais na força interior de cada um do que em santos, padres, indulgências e outros auxiliares externos comuns às gerações anteriores. Da parte de Vittoria, Abigail Brundin disse que as poesias que ela dedicou ao seu amigo revelam o mesmo esforço de lidar com essa responsabilidade e de compartilhar os frutos do labor no espírito de uma comunhão evangélica com alguém que passava pelas mesmas dúvidas e agitações de alma.[65] Michelangelo esteve presente em sua agonia, e ela faleceu em seus braços, enquanto ele em lágrimas beijava suas mãos sem cessar. Mais tarde arrependeu-se de não ter ousado beijar-lhe a testa e a face. Condivi registrou que após a morte de Vittoria, Michelangelo passou um período transtornado, como se tivesse perdido a razão. Em um soneto expressou sua tristeza e revolta, e disse que jamais a natureza fizera face tão bela.[66]
Michelangelo viveu ao longo da última fase do Renascimento e na transição para o Maneirismo, uma época de intensos conflitos sociais e profundas mudanças na vida cultural. Quando jovem absorveu as lições do primeiro Renascimento, que estabelecera uma série de cânones técnicos e estéticos para a representação artística. Esses cânones haviam sido estabelecidos sobre uma forte tendência de recuperação na arte e na cultura da tradição clássica da Antiguidade, que se desenvolvia desde séculos antes a partir de uma série de descobertas de textos de filósofos e outros escritores antigos, especialmente neoplatônicos helenistas e oradores, poetas, políticos e historiadores romanos, e de peças de arqueologia. Com essa quantidade de novas informações, no século XV se consolidou o que se chamou de Humanismo, uma síntese eclética dessas fontes pagãs com o pensamento cristão, incorporando ainda elementos das tradições árabes, orientais e egípcias, bem como outros oriundos da magia, das tradições religiosas esotéricas, da mitologia clássica e da astrologia. O resultado foi a colocação do ser humano novamente no centro do universo, enfatizando sua nobreza, sua beleza, sua liberdade, os poderes de seu intelecto e sua natureza divina. Na arte foi criado um sistema de proporções ideais para a arquitetura e para a representação do corpo e se cristalizou o sistema da perspectiva para a definição da representação bidimensional. O Renascimento associou ainda o idealismo clássico com um intenso interesse pelo estudo científico do mundo natural, produzindo uma arte que era uma generalização universal mas capaz de se deter no particular para descrever caracteres individuais. Ao mesmo tempo, era uma arte de índole ética, pois se considerava possuir uma função social da qual não podia escapar, e almejava sobretudo a cura das almas e a instrução do público para a condução da vida pelos caminhos da virtude.[67][68][69]
Michelangelo passou sua juventude em Florença quando ela estava no auge de seu prestígio político, econômico e principalmente cultural, sendo uma referência não só para a Itália mas para boa parte da Europa. Pouco depois, em torno de 1500, Roma tomou-lhe a dianteira em todos esses aspectos, onde os papas fortaleceram seu poder temporal enfraquecido, invocaram para Roma a posição de cabeça do mundo e herdeira do Império Romano, e proclamaram a universalidade de sua autoridade religiosa. Foi a fase chamada de Alta Renascença, quando as ideias artísticas clássicas a respeito de harmonia, equilíbrio, moderação, dignidade, proporção e fidelidade à natureza se tornaram especialmente influentes. Nesta mesma altura, Michelangelo atingia sua primeira maturidade artística e já trabalhava para os papas em Roma, produzindo obras que espelham perfeitamente essas concepções. Entretanto, a Itália já estava sob a mira de grandes potências estrangeiras, e começou a ser invadida em vários pontos. Em 1527 Florença foi posta sob sítio e Roma foi vítima de um terrível saque por tropas do Sacro Império, enquanto na mesma época no norte da Europa os Protestantes conseguiam a sua separação da Igreja romana com severas críticas à doutrina e aos abusos e corrupção do clero. A autoridade papal sofreu sério abalo, o poder político italiano no panorama internacional caiu de imediato, e na Itália se instaurou um clima sociocultural de incerteza, tensão e medo. A reação da Igreja foi lançar nos anos seguintes a Contra-Reforma, estabelecendo uma nova formulação para a doutrina e novas regras para a arte sacra, onde se tornou uma praxe a censura prévia com uma orientação claramente propagandística.[67][68][70][71] Como descreveu Argan, nesse período a religião já não era uma revelação inconteste de verdades eternas, mas uma busca individual; a ciência já não se estabelecia sobre a autoridade dos antigos, mas sobre a livre pesquisa; a política mudava sua base de uma noção de hierarquia emanada de Deus para se lançar na procura de um equilíbrio sempre provisório entre forças contrastantes; a História, como experiência já vivida, perdera seu valor determinante, o que contava então era a experiência de cada um no presente, e a arte deixava para trás os cânones abstratos e coletivos para mergulhar no mundo do julgamento individual, da investigação do próprio processo criativo e da materialidade da obra.[72]
Desta forma, os seus cerca de quarenta ou cinquenta últimos anos, a maior parte da carreira de Michelangelo, transcorreram nesse ambiente agitado, e seu estilo dessa fase deve ser caracterizado como maneirista, exibindo traços típicos desta escola que ele mesmo ajudou a fundar, quais sejam: uma marcada reação ao equilíbrio e harmonia do classicismo e à idealização da Alta Renascença, a distorção das proporções do corpo, uma tendência à estilização de feições, ao exagero e ao drama, o uso de uma paleta de cores pouco naturais, a anulação da perspectiva de ponto central com a criação de uma sensação de vários planos simultâneos, arbitrários e irracionais de espaço, e a preferência por formas espiraladas, contorcidas e bizarras, e por composições apinhadas de personagens.[26][71]
Michelangelo se distinguiu da estética renascentista abandonando a crença de que a Beleza é produzida por uma relação matemática de proporções entre as partes do todo, e confiava antes nos sentidos. Dizia que é mais necessário ter um compasso no olho do que nas mãos, pois as mãos produzem a obra, mas quem a julga é o olho. Não se sentia obrigado a seguir leis estéticas apriorísticas dizendo que o artista não devia ser guiado senão pela ideia que concebera, e considerava possível definir outras proporções igualmente aceitáveis e belas. Sua insistência na sua própria autonomia criativa e na expressão de sua visão pessoal o tornou o primeiro artista do ocidente a ter príncipes e papas a seus pés, decretando por si mesmo como a obra deveria ser realizada, ao contrário da prática anterior à sua geração, quando o artista era um simples artesão obediente à vontade de seus patronos. Isso era tanto um reconhecimento de sua própria capacidade como uma resposta à cultura da época que glorificava a fama pessoal.[73] Compartilhava com os renascentistas e com seus outros contemporâneos o amor pela arte da Antiguidade, mas na sua época os modelos disponíveis eram em sua grande maioria produto do Helenismo ou da era romana, que não são propriamente idealistas e trabalham mais o lado dramático, dinâmico e emotivo da representação. Também foi estimulado nessa direção pela descoberta de uma importante obra helenista, o Grupo de Laocoonte, que causou uma sensação em toda a intelectualidade romana em sua exibição pública no Vaticano em 1508.[74][75]
Apesar de sua inclinação para os modelos romanos e helenistas, Michelangelo aparece como um grande idealista, um herdeiro direto do universalismo da arte do Alto Classicismo grego. O artista não estava mais interessado na observação da natureza além do necessário para criar um protótipo de forma que ignorava o particular e era aplicável indiscriminadamente para todos os sujeitos. Nada em sua arte é específico além da forma geral do corpo humano, e o transformou em algo cuja potência vem causando admiração desde quando o plasmou em imagem. Na mesma tradição alto-clássica, procurou expressar as virtudes heróicas da alma através de corpos poderosos cuja beleza é apoteótica e ideal, e não humana, porém inevitavelmente filtrando o idealismo antigo através da sua eclética interpretação pelo Humanismo renascentista, onde o trágico e o patético também tinham um lugar. Como observou Weinberger, não representou a sua geração, mas uma geração de gigantes vivendo fora do tempo, e os edifícios que ergueu parecem ter-se destinado a esta raça. Mesmo suas obras pequenas têm uma feição monumental. Não caracterizou trajes ou fisionomias de sua época, não produziu retratos além de uns poucos desenhos, suas figuras não estão engajadas em atividades comuns, não aparecem utensílios do cotidiano, nem móveis, nem arquiteturas da época; não parecem afetadas pelas estações, pela paisagem em torno. Quando há alguma paisagem, é surpreendentemente desértica, é apenas um espaço convencional e abstrato onde distribuiu seus personagens sobre-humanos. Não teve outros alicerces para sua arte senão o corpo humano, o amor pela sua beleza e uma ideia de sublime magnificada ao extremo — certa vez buscando mármores em Carrara desejou transformar uma montanha inteira em uma estátua de um gigante.[76][77][78][79]
Até mesmo suas descrições de gênero sexual são ambíguas, em várias de suas pinturas e esculturas as mulheres são quase tão musculosas quanto os seus homens e a única diferença visível é a presença de seios e ausência de um pênis. Por outro lado, algumas de suas figuras masculinas têm uma languidez e afetação postural só encontradas na representação feminina de seu tempo.[76] Mesmo que em várias imagens seja aparente um androginismo, é amplamente reconhecida sua preferência pelo corpo masculino, especialmente nu, que é fio condutor de toda a sua produção artística, e abunda mesmo em suas composições sacras. O nu masculino aparece desde a sua primeira escultura autenticada, a Centauromaquia, numa de suas primeiras pinturas, o Tondo Doni, continua pela sua carreira afora, no Baco, no David, no Cristo Redentor, nos Escravos e nos Cativos, no Gênio da Vitória, no Jovem ajoelhado e várias outras esculturas, toma a vasta maioria de seus desenhos, é tema de suas poesias e se multiplica nas pinturas A Batalha de Cascina, no teto e no Juízo Final da Sistina. Tal frequência desde aquele tempo tem provocado reações negativas em setores da Igreja, a ponto de o Papa Paulo IV mandar cobrir as genitálias expostas de várias figuras nos afrescos da Sistina, e o Cristo Redentor em mármore sofrer o mesmo destino, recebendo um manto de bronze.[80][81]
Seu estilo e iconografia nos últimos dois séculos têm sido objeto do mais acalorado debate entre os críticos e historiadores, a ponto de Barolsky ter dito ironicamente que a copiosa bibliografia produzida sobre ele é ela mesma "michelangelesca",[82] e embora Michelangelo seja em geral contado na mais alta estima, pouco consenso sobre pontos específicos pôde ser conseguido. Entretanto, em sua poesia ele deixou muitas pistas sobre suas ideias artísticas e sobre a vida, e nela, conforme foi sugerido por escritores como Erwin Panofsky e Carlo Argan, parece transpirar o Neoplatonismo como uma influência preponderante, da forma como ele foi interpretado pelos humanistas e poetas cristãos italianos como Marsilio Ficino, Pico della Mirandola, Dante Alighieri e Petrarca. Martin Weinberger, de inclinação formalista, rejeitou a explicação transcendental e assinalou que não se pode atribuir com certeza a uma escola definida de pensamento ideias que pertenciam à cultura renascentista como um todo e estiveram sujeitas a uma multiplicidade de correntes. Também disse que Michelangelo dificilmente teria colocado sua arte acima da natureza, e que sua obra requer um estudo mais dentro do domínio da arte pura — seu tratamento dos materiais, a evolução de suas formas e de sua linguagem plástica. É possível que uma síntese de ambas as visões seja o caminho mais adequado para se compreender melhor sua produção. De fato a interpretação de uma peça específica muito dificilmente pode ser circunscrita a qualquer fonte individual, mas em linhas gerais a explicação transcendental parece permanecer a tendência mais forte entre a crítica para interpretar seu estilo e motivações como um todo. Parece bastante claro que para Michelangelo a busca da transcendência foi uma força propulsora em todo o seu trabalho, como foi documentada de várias maneiras, mas estava firmemente inspirada na beleza que ele via no mundo físico, transformada pelo poder do Amor residente na alma em algo, então sim, divino.[83][84] Num poema escreveu:
"Vejo em tua bela face, meu Senhor,
aquilo que não posso expressar nesta vida.
A alma, ainda vestida de carne,
com aquilo tantas vezes é transportada para Deus."[85]
Em outro, disse:
"Meus olhos, buscando coisas belas,
e minha alma, buscando a salvação
não têm outro poder de ascender ao céu
senão contemplando tudo que é belo".[86]
Por outro lado, não se pode atribuir um peso por demais determinante ao que ele disse de si mesmo e de sua arte, ainda que seu testemunho jamais possa ser descartado. Como lembrou Barolsky, na cultura da época o artificialismo e a estilização eram onipresentes. Mesmo a arte era considerada uma ilusão deliciosa, conforme disse Vasari, e estava sujeita a uma série de convenções, de domínio público. Os textos do século XVI são carregados de recursos puramente retóricos, e o relacionamento social em altas esferas era algo muito próximo de um teatro. Da mesma forma as biografias eram peças laudatórias enganosas, e os escritos poéticos de Michelangelo devem ser analisados levando-se em conta o contexto desse universo de convenções e artifícios, num processo de construção consciente da sua imagem pública e do seu próprio mito que ele levou a um grau hiperbólico, modelando a si mesmo à feição de um colosso, assim como fazia com suas obras. Sua correspondência tampouco é uma fonte exatamente fiel de informação sobre sua vida; em muitos momentos é evasiva, ambígua, exagerada, contraditória e às vezes claramente mentirosa, o que não era, de resto, uma característica exclusiva sua, mas espelhava um comportamento coletivo. Epitomizando esses costumes, em 1532 foi publicado O Príncipe, de Machiavelli, cujas ideias tiveram larga difusão, sacramentando a necessidade do governante de usar o engano e a dissimulação pelo bem da manutenção da ordem pública.[87][88]
Outro aspecto que tem intrigado os historiadores é o estado inacabado de muitas de suas obras. Frequentemente fatores externos, que foram documentados, o levaram a isso, sendo constantemente chamado de um lado para outro pelos papas e príncipes, mas em outros casos não houve qualquer imperativo conhecido que pudesse justificá-lo, e se especula hoje que as tensões entre suas ideias grandiosas e a dificuldade prática de transportá-las satisfatoriamente para uma forma concreta e limitada pela matéria física podem ter sido um elemento importante nesse fenômeno.[89][90]
A seguir são abordadas em mais detalhe as várias técnicas a que se dedicou, mas dada a quantidade de suas obras, apenas as mais importantes serão citadas.
Michelangelo via a si mesmo acima de tudo como um escultor. Participou do debate teórico da época sobre qual das artes seria a mais nobre, a chamada questão do paragone, e se posicionou do lado dos escultores. Em uma carta escrita para Benedetto Varchi disse:
Para Michelangelo o processo escultórico era uma sucessiva remoção do supérfluo para expor a ideia — o concetto — projetada na matéria. Em um de seus poemas comparou o processo com o ato de Deus tirando o homem do barro. Às vezes fazia modelos em argila ou cera como estudos preliminares.[91] Suas influências imediatas foram a escultura romana, Giovanni Pisano, Niccolò dell'Arca, Jacopo della Quercia, Donatello e também Leonardo da Vinci, mas desde o início eximiu-se de uma fidelidade estrita a esses modelos, buscando uma abordagem individual, o que é visível já na Centauromaquia que criou para Lorenzo de' Medici, uma das composições mais avançadas tecnicamente de sua época. Ali o mito é apenas um pretexto, conforme analisou Argan, para uma pesquisa em torno do movimento puro. Sua primeira obra importante foi o Baco, fortemente inspirada em modelos helenistas. A solução formal é criativa, uma figura desequilibrada e sensual que anula a solenidade clássica e a transforma numa figura quase burlesca, em uma contínua interpenetração de curvas e superfícies polidas que captam habilmente a luz. O contraste é provido pela pequena figura do fauno atrás dele, que serve como suporte estrutural e ao mesmo tempo é tratado com outras texturas e construído a partir de blocos básicos bem distintos.[92]
Em seguida, com apenas vinte e três anos, esculpiu sua afamada Pietà, que tem sido louvada desde a origem pelo seu finíssimo acabamento de superfície e pela sua brilhante composição em pirâmide, uma forma de grande estabilidade e perfeita para veicular o pathos melancólico, resignado e meditativo da cena. O Cristo aparece no regaço de sua mãe, com uma face tranquila sem sinal de sofrimento, como se dormisse; suas chagas mal são perceptíveis, o que enfatiza a beleza do seu corpo. A Virgem é uma jovem, e mais parece uma irmã de Cristo e não sua mãe, o que foi criticado pelos seus contemporâneos. Sua resposta foi de que sua castidade tão perfeita teria preservado sua beleza e juventude. A composição é interessante também porque a figura da Virgem, se estivesse de pé, seria bem maior do que a de seu filho, um recurso técnico-ilusionístico que não é notado sem uma medição, mas provê com o seu corpo e o largo manto cheio de dobras um amplo receptáculo para o descanso do mártir, e empresta ao conjunto uma impressão de tranquilidade. O sucesso da obra foi enorme, e foi a única que Michelangelo assinou.[92][93]
Entre 1501 e 1504 criou sua maior escultura, o colossal David para Florença. Usando um bloco único de mármore já parcialmente trabalhado, mandou erguer uma cerca em torno e o escavou sozinho, sem permitir visitas. Quando foi inaugurado causou uma sensação entre os florentinos. Inteiramente nu, é uma imagem de triunfo, na tradição dos nus heroicos do classicismo, mas por pudor foi-lhe aplicada uma guirlanda de bronze sobre o sexo. Apesar do seu tamanho descomunal, o David é ainda um adolescente, e foi representado nos momentos preparatórios do combate com Golias. Sua expressão é tensa, sua mão direita se crispa sobre a coxa, mas não há ação, tudo se resume na concentração da energia antecipando o momento mortal. É tanto um símbolo do civismo republicano de Florença, como foi reconhecido de imediato, como da condição gloriosa do homem no pensamento renascentista.[94][95]
A tumba de Júlio II deveria ter sido sua maior obra de escultura, uma grande estrutura livre dentro da Basílica de São Pedro no estilo de um mausoléu, com 10 x 15 m de área e adornado com 40 estátuas em tamanho natural representando profetas e personificações das artes liberais, com uma grande estátua de Júlio de 3 metros de altura coroando o conjunto. Michelangelo deveria receber pela obra um salário anual de 1 200 ducados — dez vezes mais do que outro artista receberia — e mais um pagamento final de 10 mil ducados, uma quantia bastante expressiva. O mármore foi trazido de Carrara e ocupou noventa carros. Para que o mausoléu pudesse ser instalado na Basílica, esta teve de ser reformada, destruindo-se a veneranda construção anterior erguida por Constantino I entre 326 e 333 d.C., ampliando-se sua planta consideravelmente, e desviando a maior parte dos recursos de Júlio para lá. Desta forma, as obras da tumba se paralisaram, Michelangelo teve de pagar o transporte do mármore por conta própria, reclamou com o papa e foi expulso do Vaticano. Ultrajado, partiu para Florença. O papa mandou cavaleiros em sua perseguição mas só o alcançaram perto de Florença. A despeito das ameaças, recusou-se a voltar e enviou ao papa uma carta protestando contra os maus tratos.[96] Alguns meses depois ocorreu a reconciliação em Roma, e Júlio solicitou que ele esculpisse uma enorme estátua sua em bronze para a cidade de Bolonha, e para lá foi enviado, morando em alojamento precário, tendo de dividir a cama com mais dois ajudantes, que além disso ele considerava incompetentes. A primeira fundição da estátua falhou, e teve de ser refundida, agora com sucesso. Tinha 4 metros de altura e pesava 4,5 toneladas, uma das maiores obras em bronze desde a Antiguidade, sendo instalada em 1508. Quando Bolonha readquiriu sua independência a estátua foi destruída.[97]
Com a morte de Júlio em 1513 a sua tumba se tornou mais do que nunca uma necessidade, mas seus herdeiros não se dispunham a prosseguir na escala que ele pretendera. Um dos primeiros esboços de Michelangelo foi revivido e o monumento foi muito reduzido, a câmara mortuária foi substituída por um simples sarcófago e o monumento deslocado para junto de uma parede lateral, mas ainda haveria diversas estátuas e relevos. Apesar da dedicação com que Michelangelo se voltou para o trabalho, nos três anos seguintes só três estátuas haviam sido iniciadas, o Moisés, e duas outras, de escravos, mas permaneciam inconclusas.[98] Entre 1519 e 1520 fez um Cristo Redentor inteiramente nu para a Igreja de Santa Maria sobre Minerva, inspirado no modelo do nu heroico da Antiguidade clássica.[99] Havia sido encomendado em 1514 pelo patrício romano Metello Vari, e o trabalho iniciara em seguida, mas a meio caminho Michelangelo descobriu um veio negro no mármore e abandonou a peça. Uma segunda versão foi criada rapidamente para cumprir o contrato, e o polimento final foi entregue a seus discípulos. Anos depois sua nudez foi oculta.[100]
Quando iniciou o pontificado de Leão X o artista foi requisitado para trabalhar em Florença. Lá, entre vários projetos arquitetônicos, esculpiu a partir de 1521 um importante par de tumbas para dois duques Medici na Sacristia Nova. Como já se tornava uma experiência comum para ele, houve várias interrupções e não pôde terminá-las, foi chamado para Roma em 1526. As estátuas dos mortos são belas e nobres, os retratam idealizadamente vestidos como antigos generais romanos, mas são especialmente notáveis as que se reclinam sobre os sarcófagos. Na tumba de Juliano, as alegorias do Dia e da Noite, e na de Lorenzo, a Aurora e o Ocaso, com uma possante descrição anatômica e intenso pathos. Deveriam ter sido esculpidas ainda quatro estátuas de deidades fluviais, mas não foram sequer iniciadas. Quando Michelangelo partiu os monumentos ainda não estavam montados, e sua forma final se deveu ao concurso de alguns de seus discípulos, que finalmente arranjaram as tumbas no ano de 1545, na forma como se as vê hoje.[98]
Provavelmente Michelangelo voltou a trabalhar na tumba de Júlio em 1526, produzindo quatro Cativos, maiores do que os dois Escravos, que também não foram acabados mas modernamente são muito apreciados por sua concepção poderosa e por parecerem estar lutando em desespero para se libertar da prisão da matéria amorfa que os rodeia, e Hartt chegou a dizer que dificilmente seu impacto emocional poderia ser mais forte se tivessem sido finalizados. Um quarto projeto para a tumba de Júlio II foi desenhado em 1532, e formalizado em contrato com a família, em dimensões ainda menores, e toda a iconografia foi revista. Criou nesta época mais uma estátua, o Gênio da Vitória, uma das mais originais composições de Michelangelo com sua figura fortemente contorcida, a subjugar um prisioneiro, embora não seja garantido que se destinasse ao túmulo. Também esta não foi finalizada. O trabalho só foi terminado em 1545 após o papa intervir para liberar o artista das obrigações com a família de Júlio. E em vez de se localizar na Basílica, foi montado na Basílica de São Pedro Acorrentado. O Moisés, a única peça que ele completou inteiramente, e que deveria ser apenas uma figura secundária no projeto original, foi instalado no centro da composição, rodeado de estátuas muito menos expressivas esculpidas às pressas e completadas por escultores menores, junto com mais duas, obra integral de outros artistas.[101] 
De interesse ainda restam suas obras finais, duas pietàs. A primeira ele iniciou antes de 1555 como parte de um projeto para uma tumba própria, que não se concretizou. A meio do trabalho exasperou-se com a "indocilidade da pedra" e destruiu parcialmente o que havia conseguido. Seus discípulos tentaram recompô-la e acabar algumas partes, mas sem grande sucesso, e uma das pernas de Cristo foi perdida. O que hoje permanece é um esboço, ainda assim pungente, da morte de Jesus, estruturado de forma a parecer que o peso de seu corpo sem vida é grande demais para ser sustentado pelas figuras de José de Arimateia, Maria Madalena e a Virgem Maria, emprestando à peça uma atmosfera de trágico desalento. Outra das pietàs, a chamada Pietà Rondanini, foi iniciada pouco antes de ele falecer, e permanece apenas com suas formas sugeridas, mas novamente seu aspecto inacabado, junto com a sensível concepção do conjunto, tem grande apelo para o público moderno.[102]
A primeira pintura que se pode atribuir com segurança a Michelangelo é o Tondo Doni (c. 1504), uma imagem da Sagrada Família. Seu tratamento de espaços e volumes é claramente escultórico, com linhas exatas a delimitar as formas, e sua iconografia foi interpretada por Charles de Tolnay como um sumário da evolução da fé. A Virgem e São José pertencem ao mundo do Antigo Testamento, regido pela Lei; Cristo é a Boa Nova, o mundo da Graça; São João Batista é a ponte de ligação entre ambos, e a galeria de nus ao fundo representaria o mundo pagão. O grupo é organizado a partir da forma da pirâmide combinada à espiral, a figura serpentinata que se tornou tão cara aos maneiristas, e o tratamento dos planos cromáticos estabelece limites nítidos entre eles, sem sfumato. Sua obra seguinte de importância teria sido a jamais realizada Batalha de Cascina, mas da qual sobrevive uma cópia do desenho preparatório. A cena escolhida para representação foi a do aviso da chegada das forças pisanas, colhendo os florentinos de surpresa. Michelangelo usou novamente um pretexto temático para realizar um notável estudo de anatomia de corpos humanos, colocando um grande grupo de soldados a se aprontarem para a batalha numa multiplicidade de posições.[103]
Na sequência veio a encomenda do teto da Capela Sistina, um espaço de grande significado simbólico no Vaticano por ser onde se realizam as eleições papais. A Capela já era decorada com uma série de afrescos importantes nas paredes, e a tarefa de Michelangelo foi a de decorar o teto, pintado apenas de um azul pontilhado de estrelas. A ideia inicial de Júlio II era de apenas doze grandes figuras dos Apóstolos, mas Michelangelo concebeu um conjunto de sete Apóstolos e mais as cinco sibilas da mitologia grecorromana, uma escolha bastante incomum mas não inteiramente inédita para um teto de capela. Acrescentou ainda quarenta ancestrais de Cristo, uma longa série de cenas do Genesis, vários nus e outras figuras acessórias, compondo um grupo de trezentas figuras dividido em três grupos: a Criação da Terra por Deus, a Criação da Humanidade e sua queda e, por fim, a Humanidade representada por Noé. As figuras exibem uma força e majestade sem precedentes na pintura ocidental. Todo o tom da obra é monumental, é grandiloquente sem ser puramente retórico, mas possuindo alta poesia, inaugurando uma forma inteiramente nova de representar o trágico, o heroico e o sublime, e também o movimento e o corpo humano. Sua interpretação temática tem sido objeto de intenso debate desde o momento de sua apresentação pública, e muitos a tem comparado com um grande panorama da evolução humana dentro de um escopo cósmico, num entendimento do Antigo Testamento como uma preparação para a vinda de Cristo, ou como uma interpretação neoplatônica dos eventos bíblicos sob uma óptica de relacionamento Deus-Homem particularmente dramática.[26][104][105] Michelangelo anos mais tarde disse que a concepção da iconografia se devia a ele, mas para quem não tinha uma grande erudição nem sabia latim, a complexidade simbólica das cenas parece estar além de sua capacidade de conceituação. Hartt disse que tem sido sugerido que ele teve um conselheiro teológico na elaboração do programa temático do teto na pessoa de Marco Vigerio della Rovere, um franciscano parente do papa. As cenas são compostas sem relação espacial umas com as outras ou com as figuras laterais, e o painel não pode ser observado a partir de um único ponto de vista.[106]
É interessante também porque permanece como um documento da evolução do autor na pintura de afresco em escala monumental, técnica com a qual ele estava pouco familiarizado no início da obra. Ele partiu da figura de Noé sobre a entrada, e foi seguindo em direção ao altar. As primeiras figuras revelam sua inexperiência e usam modelos formais mais ou menos padronizados e pouco dinâmicos, e as cenas guardam uma escala relativamente modesta. Mas em pouco tempo, como é visível, adquiriu confiança e desenvoltura, e estudos recentes têm afirmado que à medida que o trabalho avançava prescindia mais e mais de esboços preparatórios na escala definitiva, até descartá-los por completo, pintando diretamente. A mesma confiança fica patente no tratamento cada vez mais livre das pinceladas e no crescente dinamismo e expressividade das figuras, chegando a dimensões de tragédia em alguns personagens, o que ilustra com clareza a passagem do equilíbrio clássico do Alto Renascimento para o mundo agitado do Maneirismo. O trabalho foi interrompido na metade por cerca de um ano, quando não houve fundos para pagá-lo, e quando foi retomado, curiosamente o mesmo processo evolutivo se observa da segunda metade, na cena da criação de Adão, para o final na figura de Jonas. Existe porém um diferencial na segunda metade, enfatizando as atmosferas reflexivas antes do que as anatomias vitais e exuberantes.[26]
Recentemente uma restauração patrocinada por uma companhia de televisão japonesa e levada a cabo por uma grande equipe de especialistas removeu camadas de fuligem de velas, sujeiras diversas e possíveis restauros anteriores. A opção do responsável pelo trabalho foi remover tudo o que havia acima da camada realizada em buon fresco, o afresco puro, pintado quando a camada de base ainda está úmida, fazendo com que ao secar as cores se fixem permanentemente incorporadas ao reboco. O resultado foi surpreendente, mostrando uma paleta de cores brilhante e variada, muito diferente daquela que por séculos foi associada com a pintura de Michelangelo. Mas o restauro levantou uma turbulenta controvérsia no mundo da arte. Enquanto que um grupo de críticos louvou o resultado como uma revelação, dizendo que obrigava à reformulação de todas as avaliações prévias sobre sua estética, muitos outros peritos igualmente respeitados consideraram a intervenção uma calamidade que destruiu a sua pintura para sempre, acusando os restauradores de remover, além dos detritos acumulados ao longo dos anos, também acréscimos do próprio Michelangelo que teriam sido pintados a seco depois do buon fresco secar, o que realmente era uma prática bastante comum no seu tempo. Comparando-se fotografias dos estados anterior e posterior, parece claro que a adoção de uma solução técnica unificada para todo o painel foi de fato uma atitude temerária, e que o restauro tenha sido radical demais pelo menos em alguns pontos, pois é difícil crer que o artista tivesse, por exemplo, pintado figuras sem olhos, como estão agora algumas delas. Diversos outros detalhes desapareceram, como ornamentações na arquitetura ilusionística que emoldura as cenas, pregas nos mantos e o modelado sutil dos corpos e das sombras, resultando em planos mais achatados e anulando parte do efeito escultórico da pintura. Entretanto, em termos de cores a paleta luminosa que surgiu na Capela Sistina teve uma confirmação quando se restaurou o Tondo Doni, que traz o mesmo espectro de cores.[26][107][108][109][110][111][112]
Outra composição de grande importância foi realizada na mesma Capela Sistina, a cena do Juízo Final sobre a parede do altar, pintada entre 1536 e 1541, encomendada por Paulo III, um tema perfeitamente afinado a um momento em que a Contra-Reforma exercia com força a censura e perseguia visões heterodoxas do Cristianismo. A composição é estruturada em torno da figura monumental de Cristo Juiz, que separa os bons dos maus. Ao contrário da tradição anterior, que estabelecia esta cena em níveis e hierarquias rigidamente compartimentalizados, Michelangelo dissolveu boa parte desses limites, tornando o conjunto muito mais dinâmico e unificado. A própria distinção entre os condenados e o salvos é minimizada, e os próprios santos são em sua maioria despojados de vestimentas e atributos conspícuos, numa massa de corpos nus que se espalha em movimento por toda a superfície. Toda essa nudez imediatamente despertou severas críticas de parte do alto clero, e por pouco o painel não foi destruído.[26][113] Para explicar como uma profusão de nus pôde ser pintada na Capela Sistina, um dos espaços mais importantes do Vaticano, Crompton disse que os dois papas mais fortemente associados a ela eram alvo de rumores. O construtor da Capela, Sisto IV, foi mais de uma vez acusado de sodomia, e sua inclinação era confirmada pelo seu próprio camareiro. Júlio II, que mandou pintar o teto, havia sido condenado pelo Concílio de Pisa como outro "sodomita, coberto de úlceras vergonhosas". O concílio em verdade serviu a fins políticos de seus adversários, mas outros relatos falam de sua atração por homens jovens.[30]
Suas últimas pinturas dignas de nota foram dois grandes afrescos na Capela Paulina do Vaticano. Depois da grande liberdade mostrada no Juízo, ambos foram concebidos com mais rigor e menos dinamismo, ainda que estejam entre suas obras mais expressivas pela poderosa compactação dos grupos e pela intensidade dramática da caracterização dos personagens. O primeiro representa a Conversão de Saulo, realizado entre 1542 e 1545, organizado em torno da eficiente diagonal entre Cristo no céu e Saulo arrojado ao solo, cego pela luz divina, com uma grande figura de cavalo ao centro funcionando como o eixo estrutural que equilibra toda a cena. O segundo afresco retrata a Crucificação de Pedro, e foi terminado em 1550, a mais compacta de todas as pinturas de Michelangelo, organizada também sobre a diagonal formada pela cruz sendo erguida. Todo o senso de perspectiva foi ignorado e Michelangelo reverteu ao uso medieval de representar o que está mais longe numa posição superior, com pouca distinção de proporções entre o primeiro plano e os planos em recuo. A cena é essencialmente estática, com pouca ação, mas possui uma pungente qualidade ritualística.[114]
Vasari disse que os arquitetos do século XV haviam levado a arquitetura a um alto nível, mas careciam de um elemento que os impediu de atingirem a perfeição — a liberdade. Descrevendo a arquitetura como um sistema de regras definidas, declarou que os edifícios novos deviam seguir o exemplo dos antigos mestres clássicos, mantendo o conjunto em boa ordem e evitando mistura de elementos díspares. Nessa linha de ideias, ele acrescentou que a liberdade criativa, apesar de cair fora de algumas regras, não era incompatível com a ordem e a correção, e tinha a vantagem de ser guiada pelo juízo do próprio criador. Michelangelo foi considerado por Vasari o único dos mestres da sua geração a conquistar essa desejada liberdade, e cujo engajamento pessoal e individualista em todas as suas atividades, incomum numa época em que o trabalho coletivo era a regra, abriu caminho para outros arquitetos produzirem obras cada vez mais personalistas, buscando solucionar os problemas da construção dentro da esfera da própria arquitetura sem a antiga tutela dos literatos, dos tratadistas e dos intelectuais, e com um novo senso de profissionalismo. Isso não impediu, contudo, que elementos clássicos continuassem a ser empregados, mas numa abordagem eclética e experimental, e se adequando a novos conceitos de habitabilidade, função e conforto.[115]
Seu primeiro trabalho arquitetônico foi o desenho de uma nova fachada para a Basílica de São Lourenço, em Florença, executado a pedido de Leão X em 1515. O plano possui dois pisos de igual importância, estruturados em dois blocos laterais dispostos simetricamente em torno de um bloco central coroado por um frontão, dentro do esquema clássico. O projeto foi abandonado sem ser realizado. Seu projeto seguinte foi a Sacristia Nova da Basílica, concebida na tradição de Brunelleschi, instalando uma cúpula apoiada em pendentes sobre um volume cúbico, com paredes revestidas de estuque intercalado com seções em pedra. A decoração interna também foi sua, criando tumbas de feição arquitetural para dois príncipes Médici nas paredes laterais e aplicando elementos arquiteturais simplesmente decorativos que subvertem as suas funções primitivas, como tabernáculos vazios sobre as portas, janelas cegas e pilastras sem capitéis. A Biblioteca Laurenciana, também anexa à Basílica, é da mesma forma inovadora, especialmente o espaço do vestíbulo, cuja verticalidade é de todo incomum. Também faz uso de elementos arquiteturais desvinculados de sua função, como as janelas cegas e pilastras sem base, apoiadas apenas sobre consoles, além de agrupar os elementos de forma muito compacta. A peça mas notável no vestíbulo é a escadaria, tratada de maneira escultórica como um volume de grande independência em relação à estrutura do edifício. As salas para a guarda dos livros e para a leitura são convencionais, amplas e espaçosas na tradição das bibliotecas conventuais medievais, demonstrando um entendimento das necessidades funcionais do espaço.[116]
Em 1534, com esses projetos ainda em andamento, Michelangelo mudou-se para Roma. Ali seu primeiro projeto foi a remodelação da praça na colina do Capitólio, que desde o Saque de Roma em 1527 estava em ruínas. O papa Paulo III havia recentemente instalado no centro da praça uma importante relíquia romana, a estátua equestre de Marco Aurélio, e Michelangelo foi incumbido de prover um cenário urbanístico para ela, numa obra que tinha grande importância cívica. Michelangelo encontrou a área já ocupada por dois palácios arruinados dispostos em um ângulo arbitrário, com um outro no fundo da praça. Aproveitou a disposição original dos palácios e criou um espaço intermédio trapezoidal, em cujo centro foi colocada a estátua, organizando os volumes e vazios de forma simétrica. Os palácios foram restaurados e suas fachadas redesenhadas, com um projeto diferenciado para o do fundo e fachadas gêmeas para os laterais. Para a entrada da praça Michelangelo concebeu uma rampa-escadaria monumental, interligando o topo da colina com o nível da cidade. O resultado do conjunto foi brilhante. Ele não chegou a ver o projeto concluído mas seus continuadores seguiram o desenho que deixou.[117]
Sua obra mais ambiciosa na arquitetura foi sua participação na reforma da Basílica de São Pedro. Usou como base o projeto desenvolvido por Bramante, que considerava de alto nível, e, como ele, preferia a planta em cruz grega como a mais adequada para uma igreja. Mesmo nesse terreno sua obsessão pela forma humana se torna evidente; pensava que o edifício era comparável a um corpo humano, significando organizar suas partes em torno de um eixo central, assim como os membros se organizam em torno do tronco. Dizia que quem não dominava a forma do corpo não seria capaz de compreender a arquitetura. Suas modificações no desenho bramantino foram a compactação do conjunto, eliminando o esquema de várias cruzes interligadas e estruturando a planta sobre uma única grande cruz, com uma entrada de colunata dupla sustentando um frontão clássico. A configuração atual da Basílica, todavia, é em cruz latina, tendo sido reformada em anos posteriores. Seu tratamento das fachadas também revela sua tendência de dar mais unidade e coerência ao conjunto, estabelecendo uma série de pilastras externas na ordem colossal, que atravessa dois pisos e os interliga poderosamente sem interromper a fluência do desenvolvimento horizontal. Essa ideia havia sido esboçada por Leon Battista Alberti numa igreja de Mântua, mas Michelangelo levou-a à sua conclusão lógica e em uma escala monumental, tornando-se um modelo para os arquitetos do Maneirismo e do Barroco. Outra contribuição importante para o edifício foi o desenho de sua cúpula. Planejou de início uma cúpula ogival, como a da Catedral de Florença, mas depois o reformulou de forma hemisférica para compensar a verticalidade do bloco inferior e para criar um diálogo entre elementos estáticos e dinâmicos. Contudo, ele não viu a cúpula ser construída, e quando o foi seu segundo desenho foi descartado e Giacomo della Porta reverteu o projeto para sua concepção primitiva, julgando-o, com boas razões, mais estável e fácil de construir. Mesmo assim ainda é uma criação de Michelangelo, e uma das mais importantes em seu gênero em todo o mundo, sendo também ela um modelo para gerações futuras.[118]
A maior parte dos desenhos de Michelangelo que sobrevivem são esboços preparatórios para suas obras em escultura e pintura. Não obstante, possuem qualidades que os tornam obras de arte por si mesmos, e demonstram uma habilidade consumada no tratamento da forma, nos efeitos de luz e na descrição da anatomia e do movimento. Era capaz de obter efeitos de volume muitas vezes com o simples controle da espessura do traço. O estudo dos seus desenhos lança luzes valiosas sobre o seu processo criativo e sobre as mudanças na concepção de uma obra, e existem vários deles que jamais foram transportados para outros formatos, permanecendo como testemunhos únicos de uma dada ideia. Mas nem todos foram concebidos como estudos, presenteou seus amigos várias vezes com obras acabadas, e realizou alguns retratos, o que indica que ele considerava esta técnica como um território autônomo da arte. Dava-lhes grande valor, e os guardava ciosamente. O domínio do desenho era enfatizado quando ensinou seus poucos alunos, recomendando que eles o praticassem sem cessar, muitas vezes provendo desenhos seus para que copiassem. O desenho também lhe serviu como meio de divulgação de suas ideias, e após 1550 realizou vários para serem transportados para a pintura por outros artistas.[119]
Vasari disse que pouco antes de morrer Michelangelo queimou grande quantidade de seus desenhos e esboços, a fim de que ninguém pudesse ver o modo como ele desenvolveu seu trabalho e testou o seu gênio, para que sua imagem pública não parecesse menos do que perfeita. Com isso o acervo remanescente é relativamente reduzido, e as peças que escaparam da destruição foram altamente cobiçadas pelos colecionadores. Seu próprio sobrinho Lionardo teve de desembolsar uma elevada quantia quando quis adquirir alguns no mercado de arte romano. A maior parte deles, cerca de duzentos, está preservada na Casa Buonarroti em Florença, como um legado de seus descendentes. Devido à superexposição à luz e a más condições ambientais ao longo dos séculos, grande parte da coleção sofreu grave prejuízo, mas foram restaurados na década de 1970.[120]
Na opinião de Alma Altizer, Michelangelo foi um poeta extraordinário, e em seus melhores momentos foi capaz de expressar uma poderosa unidade de visão que os torna ao mesmo tempo rústicos e sofisticados, arcaicos e contemporâneos, obscuros e cristalinamente claros. Buscava com eles poder expressar "os movimentos internos de sua alma". Para a pesquisadora sua força deriva de sua capacidade de condensar nos poucos versos de suas formas favoritas, o soneto e o madrigal, uma vasta gama de significados e uma rica pletora de imagens poéticas, penetrando fundo na dialética inerente à vida humana. Nos cerca de trezentos poemas e fragmentos que chegaram aos dias de hoje é recorrente a exploração de antíteses — imaginação e realidade, criação e destruição, sujeito o objeto, espírito e matéria, amante e amado, dor e prazer, vida interior e exterior, beleza e feiura, vida e morte.[121] 
Suas primeiras obras são muitas vezes inacabadas, convencionais e derivativas de Dante Alighieri e Petrarca, e também da filosofia escolástica em sua maneira de lidar com os paradoxos morais e religiosos, mas com o passar dos anos desenvolveu uma técnica sintética original que lhe possibilitou manejar as antíteses em vários planos simultâneos. Girardi apontou como outras características de sua poesia uma tendência a abstrair e interiorizar as imagens e expressões formais que herdou de seus modelos, uma recusa a usar conceitos de maneira simplesmente ornamental como um fim em si mesmos, e uma capacidade de evitar circunlóquios e ir diretamente ao ponto desejado, o que lhes empresta uma grande força persuasiva e os anima como imagens de uma experiência verdadeira.[121] A seguir um madrigal composto para Vittoria Colonna:
Para Altizer este poema exemplifica o melhor da produção de Michelangelo. O uso de palavras em sua forma toscana arcaica, como fora, foco, volta, opra, fratta, tragge, reforça a atemporalidade do conceito de que o amor é uma força ao mesmo tempo criativa e destrutiva, e que a produção de uma obra de arte de prata ou ouro, significando sua alta qualidade, muitas vezes exige o sacrifício do artista, que se rompe em pedaços para trazê-la à luz. Também condensa em poucas linhas a relação da arte com o amor e o desejo insaciável pela beleza infinita, e a noção de que a alma do artista é incompleta sem a presença da sua musa. Essa condensação é possível graças ao uso de formas sintáticas compactas, invertidas ou interpoladas e pela escolha de poucas palavras-chave que por si trazem junto uma série de conceitos associados.[121] Apesar de suas altas qualidades, a maior parte de sua obra poética não foi escrita senão para ele mesmo e para um reduzido círculo de amigos. Vários poemas foram registrados em fragmentos de papel, ou em meio a outros escritos, como se fossem pensamentos paralelos que ele captou de passagem. Uma vez pensou em publicar cerca de uma centena deles, mas seu editor faleceu antes de terminar o trabalho e ele não retornou ao projeto, mas alguns apareceram a público sem seu consentimento, e foram considerados obras preciosas.[123] Berni o elogiou com um terzetto onde dizia:
"Calai todos vós, pálidas violetas,
e líquidos cristais, e bestas pétreas:
ele fala coisas, e vós dizeis só palavras."[123]
Benedetto Varchi nas homenagens fúnebres a Michelangelo igualou sua poesia às suas realizações nos outros campos da arte. Mesmo assim, essa apreciação não era generalizada, e somente em 1623 surgiu uma coletânea, obra de seu sobrinho-neto Michelangelo, o Jovem, mas largamente corrigida, atualizando a linguagem e expurgando-a de alusões homoeróticas e de declarações consideradas inaceitáveis para a moral e a religião. Esta edição foi a única disponível até o século XIX, quando as adulterações do editor foram em boa parte removidas e as poesias restauradas a uma forma bastante próxima da original. O interesse nesse momento era já significativo, mas sua tradução para outras línguas era considerada extremamente difícil.[124]
Uma edição completa só foi conseguida por Cesare Guasti em 1863, mas padecia de problemas editoriais sérios. Em 1897 Carl Frey ofereceu o primeiro trabalho realmente erudito de edição, catapultando a produção poética de Michelangelo para um patamar muito mais elevado de atenção do público, tornando-se canônico por cerca de sessenta anos, embora ainda apresentasse algumas deficiências. Em 1960 Enzo Girardi publicou outra edição completa, muito superior, oferecendo uma versão em italiano moderno ao lado de uma versão restaurada que se tornou referencial, e provendo uma ordenação cronológica baseado na evolução da caligrafia de Michelangelo, o que possibilitou estudar o tema em relação à sua evolução artística como um todo e com os acontecimentos de sua vida pessoal. O quadro formado a partir disso é que ele só começou a escrever depois de 1503, produzindo quatorze poemas até 1520. Desta data até 1531, mais trinta ou quarenta, e entre 1532 e 1547, cerca de duzentos, divididos em três grupos: o primeiro dirigido a Tommaso dei Cavalieri, expressando uma intensidade de amor que tornava seu destinatário o epítome de tudo o que de bom poderia haver no mundo, unindo de maneira sem igual a beleza do corpo e do espírito; o segundo, dirigido a Vittoria Colonna, igualmente intenso afetivamente mas mais inclinado à religiosidade; e um grupo para uma destinatária desconhecida, a dama bela e cruel, possivelmente uma figura simbólica e não uma pessoa real, tratando de temas variados não relacionados ao amor.[124] 
A última fase é a mais eclética, e só pode ser agrupada pela cronologia, mas um tema comum é a religião, expressando seu desejo de paz e perdão por seus pecados. Depois da contribuição de Girardi as edições e traduções se multiplicaram, e até o fim do século XX somente em inglês apareceram cinco edições completas. Entretanto, na própria Itália ele está longe de ser uma unanimidade entre os críticos, e nomes importantes como Benedetto Croce e Giuseppe Toffanin consideraram sua poesia pobre, pouco original e seriamente defeituosa.[124] Curiosamente o próprio autor escreveu comentários ao lado de vários poemas denegrindo seu mérito, mas ele não estendia essa opinião ao conjunto de sua obra poética, e escreveu a Jacob Arcadelt agradecendo-lhe ter musicado um deles, e a Varchi por uma palestra altamente laudatória que proferira em Florença sobre esta faceta de sua carreira; além disso, como se disse antes, ele pelo menos uma vez tencionou publicar uma coletânea substancial.[125] Seus poemas foram musicados várias vezes ao longo da história, por compositores como Costanzo Festa, Bartolomeo Tromboncino, Jacob Arcadelt[126] Dmitri Shostakovitch, Hugo Wolf, Richard Strauss, Luigi Dallapiccola e Benjamin Britten.[127][128]
Michelangelo era um assíduo correspondente; sobrevivem quase quinhentas das incontáveis cartas que escreveu para os mais diferentes destinatários, embora estes tenham sido principalmente seus familiares, amigos, agentes e patronos. Elas são uma fonte da maior relevância para se formar uma ideia mais completa de sua vida, personalidade e obra. Em várias delas se encontram exemplos de sua poesia, e em sua prosa exibem, segundo George Bull, um dialeto toscano robusto e fluente, capaz de transitar entre uma linguagem rica e florida e asserções diretas e objetivas, muitas vezes de dura crítica, expressando uma imaginação viva e complexa, uma posição ambivalente sobre várias coisas e uma sensibilidade refinada e passional, muitas vezes coloridas por um fino senso de humor, mas que às vezes chegava ao grotesco. Não raro abordou temas centrais da vida humana — a morte, a religião, o amor e a ambição. Usava muitas metáforas de grande vivacidade e sua habilidade com os jogos de palavras era grande.[129] Segue uma carta dirigida a Tommaso dei Cavalieri em dezembro de 1532:
"Impulsivamente, senhor Tommaso, meu senhor caríssimo, sou impelido a escrever a Vossa Senhoria, não em resposta a alguma (carta) vossa que houvesse recebido, mas antes por andar como as plantas meio secas à beira de um magro regato, que por pouca água sofrem manifestamente. Mas depois que parti da praia não encontrei pequenos córregos, mas o oceano profundo onde aparecestes, tanto que, se pudesse, para não submergir de todo, à praia de onde parti primeiro voluntariamente retornaria. Mas como estou aqui, faremos pedra do coração e seguiremos; e se não tenho a arte de navegar pelas ondas do mar de vosso valoroso gênio, me desculpareis, nem desdenhareis o que vos digo, nem querereis dar-me o que não possuo: pois quem é sempre solitário, nunca pode ter companhia. Mas Vossa Senhoria, única luz de nosso século, não pode satisfazer-se com a obra alheia, não tendo semelhantes nem alguém igual a si. Mas se entre as minhas coisas, que espero e prometo fazer, alguma vos agradar, a direi muito mais afortunada do que boa; e quando estiver certo de agradar, como disse, em alguma coisa a Vossa Senhoria, o tempo presente, com tudo o que surgir através de mim, nela porei, e pesa-me demais não poder reaver o passado, pois então poderia servi-lo muito mais longamente do que só possuindo o futuro, que será breve, pois já sou muito velho. Não tendes que dizer-me nada. Lede o coração e não a palavra, porque a pena não é fiel à boa vontade. Oh, desculpai-me que antes tenha-me mostrado estupefato com vosso peregrino gênio, pois sei quanto agi mal; pois natural é maravilhar-se que Deus faça milagres, assim como maravilha que Roma produza homens divinos. E disso o universo é testemunha."[130]Outra carta, escrita para seu irmão Lionardo em agosto de 1541:
"Lionardo, me escreves que vens a Roma neste setembro com o Guicciardino. Digo-te que não é uma boa hora, pois não farias senão aumentar as minhas preocupações, além das que já tenho. E digo isso também para Michele, porque estou tão ocupado que não tenho tempo a perder convosco, e todas as outras coisinhas me aborrecem demais: só por isso te escrevo. É preciso preparar a quaresma, te mandarei dinheiro para que te ajeites bem, para que não chegues aqui como uma besta. Escrevi também a Michele, e aconselha-o que também ele se apronte para fazer uma boa quaresma, pois ficarei aliviado; mas talvez haja qualquer coisa que ele precise fazer em Roma em setembro. Isso não sei, mas se não for o caso, de novo aconselho que não venham antes desta quaresma, porque em setembro não terei tempo de qualquer forma para falar convosco, ainda mais que o Urbino que está comigo vai em setembro a Urbino e me deixa aqui sozinho com tanto a fazer. Mas não me faltará alguém que me providencie a comida! Lê esta carta para Michele e pede-lhe que se prepare para esta quaresma como eu disse. E anda a treinar a escrita!, que me parece que tu pioras a cada dia."[131]Michelangelo foi repetidas vezes disputado por várias cidades, que tentaram seduzí-lo com pensões vultosas para que se estabelecesse entre eles. Até mesmo o sultão da Turquia desejou tê-lo em sua corte. Os banqueiros Gondi de Florença puseram à sua disposição quaisquer quantias que ele desejasse. O rei da França, Francisco I, lhe ofereceu 3 mil coroas para lá se radicar, e a Signoria de Veneza, uma pensão vitalícia de 600 coroas e liberdade completa de ação. Foi estimadíssimo por todos os seus patronos; mesmo o turbulento Júlio II, com quem brigou inúmeras vezes, lhe mostrava caloroso afeto. Júlio III, embora não o tenha empregado para nenhuma tarefa definida em consideração à sua idade avançada, constantemente solicitava seu conselho e se mostrava tão atencioso a ponto de dizer que daria seu sangue e anos de sua vida para prolongar a de Michelangelo, queria sempre que ele sentasse ao seu lado e abria caminho quando ele passava.[132]
Para seus contemporâneos e sucessores imediatos, a influência visual de sua arte foi relativamente pequena e não pode ser comparada à influência de sua personalidade como um grande criador, nem guarda uma relação direta com a fama alcançada por suas maiores obras, possivelmente porque o seu modelo formal era considerado grandioso e sublime demais e, por isso, inibidor para a formação de uma verdadeira escola. Os casos em que se assinalou uma influência direta foram poucos e revelam uma dependência quase completa ao mestre, como foi o de Daniele da Volterra, o mais talentoso entre seus discípulos. Entretanto, em aspectos limitados ele continuou por muito tempo sendo considerado um modelo, especialmente no terreno do desenho anatômico. Na escultura ele contribuiu para cristalizar a forma da figura serpentinata, que teve uma grande penetração entre os maneiristas, e artistas importantes do Barroco como Rubens, Borromini, Ticiano, Tintoretto e Bernini devem algo às suas concepções. No século XIX Rodin também se mostrou sensível ao seu tratamento de volumes e superfícies.[26][133][134][135] Na arquitetura sua obra também teve um impacto fertilizador sobre os criadores da geração seguinte, abrindo um caminho para experimentações livres e individuais a partir dos padrões clássicos ortodoxos.[115][117]
Michelangelo foi o primeiro artista ocidental a reivindicar consistentemente sua independência criativa, e o prestígio de que desfrutou em vida, tornando-o um iluminado, um ser tocado pelo divino, desencadeou um processo de inversão das hierarquias do sistema de produção e consumo de arte que culminou na visão romântica do artista como um gênio isolado, incompreendido, semilouco, preocupado apenas com a expressão de si mesmo, atormentado por anelos insatisfeitos pelo infinito, à frente de seu tempo, perseguido por filisteus insensíveis e absolutamente livre de obrigações sociais ou morais para com seu público.[136]
As primeiras análises substanciais da obra de Michelangelo apareceram nas duas biografias que foram escritas sobre ele enquanto ainda estava vivo, embora não se possa a rigor dizer que fossem críticas; são antes elogios efusivos ao seu talento e caráter pessoal. A primeira foi incluída no compêndio biográfico de Giorgio Vasari, As vidas dos mais excelentes pintores, escultores e arquitetos (1550). O texto inicia dizendo que "o benigno Regente dos Céus, vendo quão infrutiferamente os artistas se esforçavam para aperfeiçoar a arte, decidiu enviar à Terra um gênio capaz de, sozinho, levá-las todas à perfeição consumada". Mesmo com todo o elogio, Michelangelo, ao ler o trabalho, não ficou inteiramente satisfeito. Assim, seu discípulo Ascanio Condivi em 1553 escreveu sua Vida de Michelangelo Buonarroti, que contém dados fornecidos pelo próprio artista, mas mesmo esta versão de sua história não foi considerada de todo fiel, e contém muitos erros factuais importantes. Mas o tom da narrativa é o mesmo de Vasari. Por exemplo, ao descrever os afrescos da Capela Sistina, disse que ali estava tudo o que era possível fazer com a forma humana. Por outro lado, possui muita informação valiosa, e foi usada como uma das fontes de Vasari para sua segunda edição das Vidas, de 1568, que se tornou um texto canônico sobre ele, mas não deixou de criticar vários aspectos do trabalho de seu colega. Esses estudos se esforçaram por criar uma imagem pública de Michelangelo sob uma perspectiva heroica, divinizada e exemplar, apagam defeitos de caráter que eram notórios para os outros contemporâneos do artista, e inclusive negam peremptoriamente os boatos que diziam ele ser homossexual; Condivi chegou ao ponto de assegurar que ele era tão casto quanto um monge.[137] Os elogios a ele em seu tempo foram incontáveis, e além do que Vasari e Condivi disseram, acrescentem-se mais alguns a título de ilustração: Benedetto Varchi disse que seu talento incomparável seria reconhecido até entre os bárbaros; Perino del Vaga o chamou de o deus do desenho; Ariosto disse que ele estava além dos mortais, e até Rafael Sanzio, que fora seu rival, falou que dava graças a Deus por ter nascido no tempo de Michelangelo.[138]
Expressões semelhantes foram encontradas amiúde nos séculos seguintes. Goethe, depois de ver a Capela Sistina, disse que já não tinha prazer em observar a natureza, pois não mais encontrava nela a grandeza com que Michelangelo a retratara;[139] foi uma influência sobre Winckelmann na sua conceitualização do Neoclassicismo, considerando-o um dos poucos artistas modernos a igualarem as realizações dos antigos gregos;[140] foi um paradigma para todos os artistas românticos pelo caráter autobiográfico de sua obra, pela sua paixão e ambição;[141] Yeats louvou a sua capacidade de imitar a natureza e viu sua obra como uma confirmação de suas próprias inclinações a valorizar a vida física;[142] Freud disse que nenhuma outra obra de arte o impressionara tanto como o Moisés, deu uma interpretação psicológica para ele relacionando-o a figuras de autoridade e à força da justa indignação, e viu em seu idealismo patriarcal uma expressão concreta da mais alta conquista intelectual possível para a humanidade.[143] Foi admirado até por artistas das vanguardas iconoclastas do século XX, como Henry Moore, que o chamou de sobre-humano.[144]
A despeito da tendência moderna de se estudar a arte dentro de uma óptica acadêmica que tem muito do racionalismo e objetividade da ciência, ainda em tempos recentes são comuns expressões bombásticas para descrever sua vida e obra. Como exemplo, Sir Kenneth Clark disse que Michelangelo foi um dos maiores eventos na história do homem ocidental, e André Malraux o chamou de o inventor do Herói.[145] Antonio Paolucci considerou esse fenômeno como virtualmente impossível de ser evitado, dada a enorme pressão nesse sentido exercida pela reiteração continuada de um processo de deificação acrítica e incondicional ao longo de séculos, em uma dimensão tal que nenhum outro artista experimentou.[146] De uma forma bastante clara, ele foi o primeiro grande artista moderno, e permanece como o protótipo do conceito de gênio até os dias de hoje.[147]
Michelangelo foi um dos poucos artistas do mundo erudito que puderam penetrar na cultura popular e criar um folclore a seu respeito. Ele deu o nome a uma quantidade de pessoas, estabelecimentos de ensino, empresas e produtos comerciais de vários tipos, incluindo uma mão biônica.[148] É nome de um vírus de computador,[149] de um grande transatlântico,[150] de um asteroide,[151] de uma cratera no planeta Mercúrio,[152] de uma das Tartarugas Ninjas,[153] e sua figura foi retratada no cinema, sendo considerado um clássico o filme The Agony and the Ecstasy (1965), dirigido por Carol Reed e com Charlton Heston no papel do artista.[carece de fontes?]
Miguel de Cervantes Saavedra[a] (Alcalá de Henares, 29 de setembro de 1547 – Madrid, 22 de abril de 1616)[3] foi um romancista, dramaturgo e poeta castelhano. A sua obra-prima, Dom Quixote, muitas vezes considerada o primeiro romance moderno,[4] é um clássico da literatura ocidental e é regularmente considerada um dos melhores romances já escritos.[5] O seu trabalho é considerado entre os mais importantes em toda a literatura,[5] e sua influência sobre a língua castelhana tem sido tão grande que o castelhano é frequentemente chamado de La lengua de Cervantes (A língua de Cervantes).[6]
Filho de um cirurgião cujo nome era Rodrigo Cervantes e de Leonor de Cortinas, supõe-se que Miguel de Cervantes tenha nascido em Alcalá de Henares.[7] O dia exato do seu nascimento é desconhecido, ainda que seja provável que tenha nascido no dia 29 de setembro, data em que se celebra a festa do arcanjo San Miguel, pela tradição de receber o nome do santoral. Miguel de Cervantes foi batizado em Castela no dia 9 de outubro de 1547 na paróquia de Santa María la Mayor.[8] A carta do batismo reza:
Em 1569 foge para a Itália depois de um confuso incidente (feriu em duelo Antonio Sigura), tendo publicado já quatro poesias de valor. Sua participação na batalha de Lepanto, no ano de 1571, onde foi ferido na mão e no peito,[9] deixa-lhe inutilizada a mão esquerda que lhe vale o apelido de o maneta de Lepanto.
Em 1575, durante seu regresso de Nápoles a Castela é capturado por corsários de Argel, então parte do Império Otomano. Permanece em Argel até 1580, ano em que é liberado depois de pagar seu resgate (ver: Escravidão branca).
Cervantes passou 2 anos em Lisboa, Portugal, entre a primavera de 1581 e a de 1583.
O escritor tentava conquistar um lugar de favorito na corte do monarca espanhol, aproveitando os primeiros momentos do reinado português do rei. Rodeado pelos seus cortesões, Filipe terá trocado as roupas negras e a gola branca isabelina pelos tecidos ricos e coloridos de Lisboa. Foi neste ambiente de fausto e deslumbramento real que Cervantes chegou à capital portuguesa, onde se terá encantado pela cidade e pelas suas damas. Tendo escrito “Para festas Milão, para amores Lusitânia”.
Cervantes descreve os lisboetas como agraváveis, corteses, liberais e apaixonados, embora discretos. A formosura das mulheres admira e apaixona.[10]
De volta a Castela se casa com Catalina de Salazar em 1584, vivendo algum tempo em Esquivias, povoado de La Mancha de onde era sua esposa, e se dedica ao teatro.
Publica em 1585 A Galatea, o seu primeiro livro de ficção, no novo estilo elegante da novela pastoral. Com a ajuda de um pequeno círculo de amigos, que incluía Luíz Gálvez de Montalvo, com o livro um público sofisticado passou a conhecer Cervantes.
Encarcerado em 1597 depois da quebra do banco onde depositava a arrecadação, "engendra" Dom Quixote de La Mancha, segundo o prólogo a esta obra, sem que se saiba se este termo quer dizer que começou a escrevê-lo na prisão, ou simplesmente que se lhe ocorreu a ideia ou o plano geral ali.
Finalmente, em 1605 publica a primeira parte de sua principal obra: O engenhoso fidalgo dom Quixote de La Mancha. A segunda parte não aparece até 1615: O engenhoso cavaleiro dom Quixote de La Mancha. Num ano antes aparece publicada uma falsa continuação de Alonso Fernández de Avellaneda.
Entre as duas partes de Dom Quixote, aparecem as Novelas exemplares (1613), um conjunto de doze narrações breves, bem como Viagem de Parnaso (1614). Em 1615 publica Oito comédias e oito entremezes novos nunca antes representados, mas seu drama mais popular hoje, A Numancia, além de O trato de Argel, ficou inédito até ao final do século XVIII.
Miguel de Cervantes morreu em 1616, parecendo ter alcançado uma serenidade final de espírito.
Um ano depois de sua morte aparece a novela Os trabalhos de Persiles e Sigismunda.
É bem notória a coincidência das datas de morte de dois dos grandes escritores da humanidade, Cervantes e William Shakespeare, ambos com data de falecimento em 23 de abril de 1616. Porém, é importante notar que o Calendário gregoriano já era utilizado na Castela desde o século XVI, enquanto que na Inglaterra sua adopção somente ocorreu em 1751. Daí, em realidade, William Shakespeare faleceu dez dias depois de Miguel de Cervantes.[carece de fontes?]
Cervantes, por outro lado, teria morrido em 22 de abril de 1616, sexta-feira, tendo sido registada a morte no sábado, dia 23, em sua paróquia, em San Sebastián. Conforme costume da época, no registo constava a data do enterro. Em 23 de abril é comemorado o Dia do Livro na Espanha.[3]
Em 2011, um grupo de investigadores históricos e arqueólogos iniciaram uma busca pelos ossos do autor Miguel de Cervantes na igreja conventual das Trinitarias em Madrid, onde os seus restos mortais foram depositados em 1616, não se sabendo exactamente em que parte do monumento. A iniciativa, que permite reconstruir o rosto do escritor, até agora só conhecido através de uma pintura do artista Juan de Jauregui, conta com o apoio da Academia Espanhola e o aval do arcebispado espanhol.
A igreja foi remodelada no final do século XVII, e apesar das certezas de que os restos do escritor espanhol ali se encontram, ninguém sabia o lugar exacto onde estará a sua campa.
A descoberta e consequente análise das ossadas do autor espanhol poderão ainda ajudar os investigadores a determinar as causas da morte de Cervantes, que se acredita que tenha morrido de cirrose.[11]
Em fevereiro de 2014 a Comunidade Autônoma de Madri autorizou a busca pelos restos mortais de Cervantes e de Catalina, supostamente enterrados no subsolo do Convento de Las Trinitarias Descalzas, com o uso de radar.[12] Em março de 2015 o time multidisciplinar liderado por Francisco Etxeberría confirmou o descobrimento dos restos mortais de Cervantes, identificados pelas iniciais "M.C." em seu caixão. Apesar de uma análise de DNA para confirmar se os restos são de Cervantes não ser possível (devido ao fato de que não são muitos os descendentes vivos do dramaturgo para realizar uma comparação do DNA), o time responsável pela descoberta usou outras informações, como as iniciais no caixão e o fato de que Cervantes pediu para ser enterrado ali, para chegar a conclusão; "São muitas as coincidências e não há discrepâncias. Todos os membros da equipe estão convencidos de que temos entre os fragmentos algo de Cervantes, embora não possamos dizer em termos de certeza absoluta", afirmou Etxeberría.[13][14][15][16]
A única peça teatral trágica a sobreviver de Cervantes é O cerco de Numancia, na qual é encenada a resistência desesperada da população desta cidade ibera contra as forças romanas que querem conquistá-la.
Já o volume Oito comédias e oito entremezes nunca antes representados traz um excelente exemplo do humor cervantino, com seu pleno domínio das convenções da época; curioso notar as diferenças de tratamento existentes entre a novela exemplar O ciumento de Extremadura e o entremez O velho ciumento, que apresentam basicamente a mesma história - a do marido que, para evitar ser traído, tranca a mulher em casa e proíbe-lhe qualquer contacto com o mundo externo: Cervantes sacrifica, na peça, boa parte da sutileza da novela para produzir um efeito cômico mais imediato.
Conforme listado em Complete Works of Miguel de Cervantes:[17]
Geralmente considerado um poeta medíocre, poucos de seus poemas sobrevivem; alguns aparecem em La Galatea, enquanto ele também escreveu Dos Canciones à la Armada Invencible.
Seus sonetos são considerados seu melhor trabalho, especialmente Al Túmulo del Rey Felipe en Sevilla, Canto de Calíope e Epístola a Mateo Vázquez. Viaje del Parnaso, ou Journey to Parnassus, é sua obra de versos mais ambiciosa, uma alegoria que consiste em grande parte em resenhas de poetas contemporâneos.
Ele publicou uma série de obras dramáticas, incluindo dez peças completas existentes:
Ele também escreveu 8 farsas curtas (entremeses):
Calidaça[1] (Devanāgarī: कालिदास "servo de Cáli") foi um renomado poeta e dramaturgo sânscrito clássico, amplamente considerado como o maior poeta e dramaturgo no idioma sânscrito. O período em que viveu não pode ser datado com precisão, mas é mais provável que seja dentro do período Gupta, provavelmente no século IV ou V ou VI.
Seu lugar na literatura sânscrita é semelhante ao de Shakespeare na inglesa.[2] Suas peças de teatro e poesias são principalmente baseadas na mitologia e filosofia hindus.
É o mais notável dos dramaturgos sânscritos e o maior nome da literatura sânscrita depois de Asvagosas (século III a.C. - c. 375 a.c.). A sua vida está oculta por um véu de lendas. Deve ter sido um estrangeiro convertido à via brâmane. Era a mais brilhante das nove gemas na corte de Vicramaditia de Ujaim. Eminente nas artes e ciências do seu tempo, desde a astronomia à política.
A tradução do seu Sacuntala no princípio do século XIX foi entusiasticamente aclamada por Goethe e revelou a toda a Europa as realizações de alto nível  que a literatura sânscrita atingira. Seu estilo é rigoroso, formalmente rebuscado e catarticamente intenso.  
Nada além de suas obras é conhecido com certeza na vida de Calidaça, tais como onde e em que período viveu. Pouco se sabe sobre a vida de Calidaça. Segundo a lenda, o poeta era conhecido por sua beleza que chamou a atenção de uma princesa que se casou com ele. No entanto, como diz a lenda, Calidaça haviam crescido sem muita instrução, e a princesa ficava envergonhada de sua ignorância e grosseria. Um adorador devoto de Cali (por outros relatos, Deusa Saraswati), Calidaça teria pedido ajuda à sua deusa quando ele ia se suicidar em um lago depois de ter sido humilhado por sua esposa, tendo sido recompensado com um repentino e extraordinário dom de inteligência. Ele teria então se tornado o mais brilhante das "nove joias" na corte do rei Vicramaditia de Ujaim. A lenda também diz que ele foi assassinado por uma cortesã no Sri Lanka, durante o reinado de Cumaradaça.
Como limite da data mais recente em que poderia ter vivido pode-se tomar o dado pela Aihole Praxasti de 634 d.C., que faz uma referência a suas habilidades. Um limite de data inicial para sua vida pode ser inferido a partir de sua peça Mālavikāgnimitra, onde o  herói, rei Agnimitra da dinastia Sunga, assumiu o trono da Mágada em 152 a.C. As características linguísticas dos dialetos Prakrit usados por alguns dos personagens menores em suas peças têm sido apresentadas para sugerir que ele não poderia ter vivido antes do século III da era cristã.[3] Tem havido grande ambiguidade na definição da época exata da vida de Calidaça, mas em 1986, o estudioso de sânscrito, Dr. Ramchandra Tiwari de Bhopal, alegou ter realizado uma investigação exaustiva sobre Calidaça e depois de analisar 627 evidências arqueológicas que incluíram 104 esculturas, 30 pinturas e 493 palavras de escrituras determinou que Calidaça viveu no período 370 a 450[carece de fontes?]
Em suas obras, Calidaça não mencionou nenhum rei como seu soberano ou qualquer outra dinastia além da Dinastia Sunga, mas vários historiadores têm tradicionalmente considerado Calidaça como uma das "nove joias" na corte de um rei chamado Vicramaditia. Havia, no entanto, vários reis na antiga Índia com esse nome. Um deles foi o imperador rajapute tuar Vicramaditia de Ujaim, que fundou o Vikrama Samvat (calendário hindu, que é o oficial em Bangladexe e Nepal), após sua vitória sobre os sacas em 56 a.C.[4] Os estudiosos observaram outras associações possíveis com a dinastia Gupta, o que colocaria seu período de vida no intervalo de 300-470 d.C.:
Os estudiosos têm especulado que Calidaça pode ter vivido tanto perto do Himalaia ou nas proximidades de Ujaim. As duas especulações são baseadas, respectivamente, em descrição detalhada de Calidaça sobre o Himalaia em seu Kumārasambhava e a demonstração de seu amor por Ujaim em Megaduta.
Calidaça escreveu três peças. Entre elas, Abhijñānaśākuntalam ("Do reconhecimento de Shakuntala por um símbolo") é geralmente considerado como uma obra-prima. Foi uma das primeiras obras em  sânscrito a serem traduzidas para o inglês e desde então tem sido traduzido para muitas línguas.[5]
Mālavikāgnimitram ("Mālavikā e Agnimitra") conta a história do rei Agnimitra, que apaixona-se pela imagem de uma menina serviçal exilada chamada Mālavikā. Quando a rainha descobre a paixão do marido por essa garota, fica enfurecida e aprisiona Mālavikā, mas como o destino mostraria, Mālavikā nasceu na verdade uma verdadeira princesa, legitimando assim o caso de amor.
Abhijñānaśākuntalam ("Do reconhecimento de Shakuntala por um símbolo") conta a história do rei Dushyanta que, quando em uma caçada, encontra Shakuntalā, filha adotiva de um sábio, e casa-se com ela. Um revés lhes acontece quando ele é convocado de volta à corte: Shakuntala, grávida, inadvertidamente ofende um sábio visitante e incorre em uma maldição, pela qual Dushyanta irá esquecê-la completamente até que ele veja o anel que deixara com ela. Em sua viagem à corte de Dushyanta em avançado estado de gravidez, ela perde o anel e tem de vir embora sem ter sido reconhecida. O anel é encontrado por um pescador que reconhece o selo real e devolve-o a Dushyanta, que recupera a memória de Shakuntala e passa a procurá-la. Depois de mais viagens, eles finalmente se unem novamente.
Vikramōrvaśīyam ("Pertencendo a Vikrama e Urvashi") conta a história do rei mortal Pururavas e da ninfa celeste Urvashi que se apaixonam. Como uma imortal, ela tem que voltar para o céu, onde um infeliz acidente faz com que ela seja enviada de volta para a terra como uma mortal com a maldição de que ela vai morrer (e assim retornar para o céu) no momento que seu amante fixar os olhos na criança que ela vai lhe dar. Após uma série de contratempos, incluindo a transformação temporária de Urvashi em uma videira, a maldição é anulada e os amantes são autorizados a permanecer juntos na terra.
Públio Virgílio Maro[1] ou Marão (em latim: Publius Vergilius Maro; Andes, 15 de outubro de 70 a.C. — Brundísio, 21 de setembro de 19 a.C.) foi um poeta romano clássico, autor de três grandes obras da literatura latina, as Éclogas (ou Bucólicas), as Geórgicas, e a Eneida. Uma série de poemas menores, contidos na Appendix vergiliana, são por vezes atribuídos a ele.
Virgílio é tradicionalmente considerado um dos maiores poetas de Roma, e expoente da literatura latina. Sua obra mais conhecida, a Eneida, é considerada o épico nacional da antiga Roma: segue a história de Eneias, refugiado de Troia, que cumpre o seu destino chegando às margens de Itália — na mitologia romana, o ato de fundação de Roma. A obra de Virgílio foi uma vigorosa expressão das tradições de uma nação que urgia pela afirmação histórica, saída de um período turbulento de cerca de dez anos, durante os quais as revoluções prevaleceram. Virgílio teve uma influência ampla e profunda na literatura ocidental, mais notavelmente na Divina Comédia de Dante, em que Virgílio aparece como guia de Dante pelo inferno e purgatório.
Estima-se que a tradição biográfica de Virgílio venha de uma biografia perdida de autoria de Varius, editor de Virgílio, incorporada na biografia feita por Suetônio e os comentários de Sérvio e Donato, os dois grandes comentadores da poesia de Virgílio. Embora os comentários sem dúvida registrem muitas informações factuais sobre Virgílio, pode mostrar-se que algumas das suas evidências se baseiam em inferências feitas a partir de sua poesia e alegorização. Portanto, a tradição biográfica de Virgílio continua a ser problemática.[2]
A tradição diz que Virgílio nasceu na vila de Andes (atual Virgilio), perto de Mântua,[3] na Gália Cisalpina.[4] Estudiosos sugerem descendência etrusca, úmbria ou até mesmo céltica, examinando os marcos linguísticos ou étnicos da região. A análise do seu nome[5] deu origem a crenças de que ele descenderia dos primeiros colonizadores romanos. Especulações modernas, em última análise, não são apoiadas pela evidência narrativa nem dos seus próprios escritos nem dos seus biógrafos posteriores. Macróbio diz que o pai de Virgílio era de origem humilde; no entanto, os estudiosos em geral acreditam que Virgílio era de uma família de latifundiários equestres que puderam oferecer-lhe uma educação. Frequentou escolas em Cremona, Mediolano, Roma e Nápoles. Após um breve período de reflexão, o jovem Virgílio decide abandonar uma carreira em retórica e lei, e passar a dedicar os seus talentos à poesia.[6]
Amigo de Horácio, como ele protegido por Mecenas, entrou em contato com o imperador pela primeira vez quando Augusto retornava de uma campanha vitoriosa contra Marco Antônio, ainda no porto de Brindes (Brundísio). Mecenas apresentou o jovem escritor com o intuito de fazê-lo instrumento de propaganda do imperador,[7] de quem recebeu o incentivo para escrever a Eneida.
Admirador da cultura helênica, empreendeu uma viagem à Grécia, berço e viveiro da cultura, sonho que há muito acalentava: o destino concedeu-lhe a realização desse anseio, mas morreu no regresso, junto de Brundísio. O seu túmulo encontra-se em Nápoles.
A obra de Virgílio compreende, além de poemas menores, compostos na juventude, as Bucólicas ou Éclogas, em número de dez, em que reflete a influência do gênero pastoril criado por Teócrito.
As Geórgicas, dedicadas ao seu protetor Mecenas, constam de quatro livros, tratando da agricultura. Trata-se de uma obra de implicações políticas indiretas, embora bem definidas: ao fazer a apologia da vida do campo, o poeta serve o ideal político-social da dignificação da classe rural. Reflete a influência de Hesíodo e Lucrécio.
Literariamente, as Geórgicas são consideradas a sua obra mais perfeita. E finalmente, a Eneida, que o poeta considerou inacabada, a ponto de pedir, no leito de morte, que fosse queimada, constitui a epopeia nacional.
Esta refere-se à lenda do guerreiro Eneias, que, após a célebre guerra, teria fugido de Troia, saqueada e incendiada, e chegado à península Itálica, onde se tornou o antepassado do povo romano. Epopeia erudita, a Eneida tem como objetivo dar aos romanos uma ascendência não grega, definindo a cultura latina como original e não tributária da cultura helênica.
O poema consta de doze livros e a sua construção serviu de modelo definitivo às grandes epopeias do renascimento, nomeadamente para Os Lusíadas, de Luís Vaz de Camões, o que se percebe claramente comparando o primeiro verso das duas epopeias:
Segundo os comentaristas, Virgílio recebeu sua primeira educação quando tinha cinco anos de idade, indo mais tarde para Cremona, Milão e finalmente Roma para estudar retórica, medicina, e astronomia, que ele logo abandonou pela filosofia. Da admiração por Virgílio referida pelos escritores neotéricos Gaio Asino Pólio e Élvio Cinna, tem-se inferido que ele foi durante algum tempo associado ao círculo neotérico de Catulo. Segundo a tradição, Virgílio preferia sexo com homens e especialmente amava dois jovens escravos,[8][9] entretanto colegas de Virgílio consideravam-no extremamente tímido e reservado, de acordo com Sérvio, e ele foi apelidado de "Partênias" ou "donzela" por causa de seu distanciamento social. Virgílio parece ter tido pouca saúde em toda a sua vida e em algumas aspectos viveu como um inválido. Segundo o "Catalepton", enquanto que esteve na escola epicurista de Siro, o epicurista, em Nápoles, começou a escrever poesia. Um grupo de pequenas obras atribuídas ao jovem Virgílio pelos comentadores sobrevive coletados sob o título Appendix vergiliana, mas são largamente considerados espúrios pelos estudiosos. Um deles, o Catalepton, consiste em catorze poemas curtos,[10] alguns dos quais podem ser de Virgílio, enquanto outro, um poema narrativo curto intitulado Culex ("O Mosquito"), foi atribuído a Virgílio já no século I da era cristã.
A tradição biográfica afirma que Virgílio começou o hexâmetro Éclogas (ou Bucólicas) em 42 a.C. e acredita-se que a coleção foi publicada em torno de 39-38 a.C., embora isto seja controverso.[10] As Éclogas (do grego "seleções") são um grupo de dez poemas aproximadamente modelados na poesia bucólica hexamétrica ("poesia pastoral") do poeta helenístico Teócrito.
Após sua vitória na Batalha de Filipos em 42 a.C., onde lutara contra o exército liderado pelos assassinos de Júlio César, Otaviano tentou pagar seus veteranos com  terras expropriadas de cidades no norte da península Itálica, supostamente incluindo, segundo a tradição, uma propriedade perto de Mântua pertencente a Virgílio. A perda da fazenda de sua família e a tentativa, por meio de petições poéticas, de recuperar sua propriedade têm sido tradicionalmente consideradas como os motivos de Virgílio para a composição das Éclogas. Hoje isso é visto como uma inferência sem suporte baseada em interpretações das Éclogas. Nas Éclogas 1 e 9, Virgílio de fato dramatiza os sentimentos contrastantes causadas pela brutalidade das desapropriações de terra através da expressão pastoral, mas não oferece provas irrefutáveis do suposto incidente biográfico. Os leitores muitas vezes fazem essa relação e por vezes identificam o próprio poeta com vários personagens e suas vicissitudes, quer a gratidão de um velho rústico a um novo deus (Ecl. 1), o amor frustrado de um cantor rústico por um menino distante (o animal de estimação de seu senhor, Ecl. 2), ou o pedido de um cantor do senhor para compor várias éclogas (Ecl. 5). Os estudiosos modernos em grande parte rejeitam esses esforços para reunir dados biográficos de textos fictícios, preferindo em vez disso interpretar os diversos personagens e temas como a representação das próprias percepções contrastantes do poeta em relação à vida e pensamento de sua época.
Édith Giovanna Gassion, conhecida como Édith Piaf (Paris, 19 de dezembro de 1915 — Grasse, 10 de outubro de 1963),[2][3] foi uma consagrada cantora, compositora e atriz francesa. O seu ritmo musical era concentrado inicialmente em música de salão e as suas variedades, mas ficou reconhecida pelo seu talento com a música de estilo francês chanson. O seu canto dramático expressava claramente os momentos trágicos que permearam sua intensa história de vida. 
Entre seus maiores sucessos estão La vie en rose (1946), Hymne à l'amour (1949), Milord (1959), Non, je ne regrette rien (1960). Participou de peças teatrais e filmes. Em junho de 2007 foi lançado um filme biográfico sobre ela, chegando aos cinemas brasileiros em agosto do mesmo ano com o título Piaf – Um Hino Ao Amor (originalmente La Môme, em inglês La Vie En Rose), direção de Olivier Dahan.
Édith Piaf está sepultada na mais célebre necrópole francesa, o cemitério do Père-Lachaise. O seu funeral foi acompanhado por uma multidão poucas vezes vista na capital francesa. Hoje, o seu túmulo é um dos mais visitados por turistas do mundo inteiro. Segundo a pesquisa da BBC: Le Plus Grand Français, Édith Piaf foi considerada a 10.ª maior personalidade francesa de todos os tempos.
A consagrada cantora nasceu como Édith Giovanna Gassion[4] em Belleville, um distrito cheio de imigrantes em Paris. Uma lenda diz que ela nasceu na calçada da Rue de Belleville 72, mas a sua certidão de nascimento cita o Hospital Tenon,[5] que faz parte de Belleville. Ela recebeu o nome de Édith em homenagem a uma enfermeira britânica da Primeira Guerra Mundial que foi executada por ajudar soldados franceses a escapar dos alemães.[6] Piaf, um nome coloquial francês para um tipo de pardal, foi um apelido dado a ela 20 anos depois.
A sua mãe, Annetta Giovanna Maillard (1895–1945), era pied-noir, mais especificamente de ascendência franco-italiana por parte de pai e cabila-berbere por parte de mãe. Ela trabalhava como cantora em um café com o pseudônimo de Line Marsa, tendo sido obrigada a deixar a profissão contra sua vontade após o casamento. Louis-Alphonse Gassion (1881–1944), o pai de Édith, era normando e acrobata de rua[7] com um passado no teatro. Devido a dificuldades financeiras após a separação conturbada que tiveram em 1916, devido a agressões físicas, traições e humilhações, a mãe de Édith passou a sustentar a menina cantando em restaurantes, cafés, e também nas ruas de Paris, porém a abandonou em janeiro de 1918 com o ex-marido, quando Édith havia recém completado dois anos, pois decidiu ir embora de Paris para tentar seguir carreira artística como cantora, porém sem êxito, terminando por se prostituir pelas ruas parisienses para poder sobreviver, não procurando mais sua única filha, Édith, que carregou a mágoa e a dor do abandono materno por toda a vida. Sem recursos financeiros e sozinho com a filha, o pai de Édith a deixou aos cuidados de sua avó materna, Emma Aïcha Saïd ben Mohammed (1876–1930), que era negligente com Édith, batendo na menina, deixando-a sozinha em uma saleta com fome, e também não cuidava da sua higiene. Ela ficou dezoito meses com a avó, mas quando o seu pai soube desta situação, decidiu levar a filha consigo, mas não pôde ficar com ela, pois precisou se alistar na armada francesa, para lutar na Primeira Guerra Mundial. Seu pai, mesmo relutando, então, levou-a para que ficasse com a mãe dele, a avó paterna de Édith, uma ex-prostituta e atual dona de um bordel em Bernay, na Normandia. Lá, a sua avó e as demais prostitutas cuidaram da pequena Édith, em especial a meretriz Titine, que a criou por três anos como sendo sua própria filha. [8] [9]
Dos cinco aos seis anos, Édith ficou parcialmente cega, devido a uma queratite. De acordo com uma de suas biografias, ela curou-se depois de sua avó, junto com as prostitutas, a terem levado em uma curta viagem até a cidade de Lisieux, para que Édith pudesse orar no túmulo de Santa Teresa de Lisieux, conhecida popularmente como Santa Teresinha. O túmulo é um local de popular peregrinação na França, onde pessoas oram para agradecer ou pedir alguma cura e/ou bênção. Após passar o dia orando, Édith retornou a Paris. Uma semana após esse episódio, milagrosamente voltou a enxergar. Esse episódio a fez tornar-se uma católica fervorosa após adulta, sempre indo a missa aos domingos, lendo diariamente a bíblia, orando de joelhos com seu terço, e sempre usando seu crucifixo no pescoço. Por toda a vida, Édith conservou grande devoção a Santa Teresinha, a quem atribuía todas as suas conquistas.  [10]
Em 1922, aos sete anos, o pai de Édith conseguiu reunir condições financeiras para tirar a filha da companhia da avó, contra a vontade de Édith, que sofreu ao separa-se de Titine, a que mais lhe dava afeto naquele local em que vivia.[10]
Seu pai levou-a de volta consigo, para viverem juntos no circo itinerante em que ele trabalhava, onde a companhia circense viajava por toda a França. No circo, Édith apaixonou-se pelo mundo artístico e decidiu que queria ser artista como o pai, porém o mesmo saiu do circo após três anos, devido a constantes desentendimentos com o patrão. Em 1925, ele voltou para Paris com a filha, e a matriculou numa escola pública no período da manhã, já que Édith não pôde estudar no período que morou no circo, que sempre estava em uma cidade diferente, e então seu pai passou a trabalhar por conta própria fazendo acrobacias nas ruas parisienses. Ao término das aulas, Édith acompanhava o pai nas suas apresentações, recolhendo o dinheiro das mesmas, mas o público sempre questionava qual número circense a menina fazia, porém Édith ficava calada, pois não sabia ainda qual era o seu talento. Um dia, a menina de dez anos, que, tentando improvisar alguma apresentação para ajudar o pai a ganhar mais dinheiro, decidiu cantar o hino nacional francês, A Marselhesa, sendo muito aplaudida pelo pequeno público que se aglomerava a volta deles. Vendo o talento da filha, ambos passaram a sobreviver desse trabalho nas ruas de Paris: Seu pai fazia performances acrobáticas e Édith cantava.[11] [9]
Aos 15 anos, ela e o pai iniciaram diversas brigas. Ele temia que a jovem tivesse o mesmo destino da mãe dela, não aceitando a vocação artística da filha, não querendo que ela tentasse ter uma carreira profissional como cantora e se frustrasse, mas sim, queria lhe arrumar um casamento. Sem o pai saber, Édith já escrevia as suas primeiras canções, e quando não estava se apresentando com o pai nas ruas, fazia pequenas apresentações em estabelecimentos locais e ganhava um pouco de dinheiro. Desesperada, e querendo lutar por seus sonhos, Édith fugiu de casa, deixando uma carta de despedida para seu pai, indo viver em um quarto alugado, no Grand Hôtel de Clermont, na rua Veron, 18, em Paris. Nesta época começou a se apresentar em restaurantes e bares nos bairros nobres de Quartier Pigalle e Ménilmontant, e também nos bairros pobres do subúrbio de Paris. Nessa época, juntou-se à atriz iniciante, que frequentava suas apresentações, Simone Berteaut, apelidada de "Mômone"[5] e as duas tornaram-se amigas inseparáveis, tendo ido morar juntas em um pequeno quarto no subúrbio parisiense, para dividir o aluguel.[11] Já havia um ano que Édith havia saído de casa, e seu pai tentava convencê-la a voltar, o que ela não queria. Nesta época estava com 16 anos quando se apaixonou pela primeira vez, por Louis Dupont, um rapaz três anos mais velho, e entregador de pães, que tornou-se o seu primeiro namorado.[11]
Após três meses de namoro, ela deixou o quarto que vivia com sua amiga e foi morar no quarto alugado que Louis vivia sozinho. Inicialmente, ela o ajudava com as despesas, pois ele não se opôs a sua profissão de cantora, embora Édith fosse alvo de preconceito social por ser uma mulher financeiramente independente. Aos 17 anos, em 11 de fevereiro de 1933, em Paris, Édith deu à luz de parto normal no mesmo hospital em que nasceu, a sua única filha, Marcelle Léontine Gassion Dupont.[12] A partir daí, o seu casamento entrou em crise, pois o marido passou a humilhá-la e agredi-la, impedindo que ela voltasse a cantar. Quando sua filha fez um ano de vida, ela decidiu fugir de casa. Não queria deixar a menina, mas sabia que a noite não era um lugar saudável para uma criança estar. Assim, com as economias que tinha, voltou a morar sozinha mais distante dali, se escondendo de seu ex-marido. A filha adoeceu após a sua partida. Já estabelecida, embora ainda com pouco dinheiro, Édith voltou seis meses depois para buscá-la, mas seu ex-marido não a deixou vê-la, e a justiça nada fez para ajudar a jovem artista a recuperar a menina, pois a forte censura da época era a favor da moral e dos bons costumes, sendo contra os artistas. Impedida de ver sua filha e sofrendo muito, não pôde mais voltar a procurá-la. Marcelle faleceu de meningite, com dois anos de idade, em 7 de julho de 1935, e foi enterrada no Cemitério do Père-Lachaise, onde futuramente Édith também foi enterrada, ao lado de sua filha. Dupont, o pai da menina, criou sua filha até a morte desta. [7] A relação familiar conturbada de Édith com seu pai, que a artista voltou a procurar para uma reconciliação, porém sem êxito, as brigas com sua mãe, que reapareceu, mas só queria o dinheiro da cantora, aliado ao adoecimento e falecimento de sua única filha, causaram danos psicológicos profundos em Édith, que se culpava a todo momento, desenvolvendo uma profunda depressão, com diversas tentativas de suicídio. A cantora levou a dor imensurável da perda de sua filha até o fim de sua vida, o que a levou a dedicar-se de corpo e alma a sua carreira, optando por nunca mais querer ter filhos.[11]
Após reencontrar sua amiga Mômone, e voltarem a morar juntas, Édith começou a cantar em praças, boates, bares e também bordéis. Lá conheceu seu segundo namorado, um cafetão chamado Albert. A relação tornou-se abusiva com o tempo, onde ele passou a ameaçá-la: Em troca de não a forçar a se prostituir, cobrava comissões sobre o dinheiro que ela ganhava cantando. Ele passou a prendê-la no bordel, e impedir que ela cantasse em outros locais. Sua amiga tentou ajudar, mas a polícia tratava cantoras como prostitutas. Após sofrer agressões, humilhações e abusos sexuais, Édith conseguiu terminar definitivamente o namoro, quando fugiu do bordel, após descobrir que ele também era um assassino, e que uma de suas colegas que contavam na noite, chamada Nádia, amante de seu namorado, cometeu suicídio, para não tornar-se prostituta.[11]
Em 1935, Édith foi descoberta cantando na rua da área de Pigalle por Louis Leplée, dono do cabaré Le Gerny's, situado na avenida Champs Élysées, em Paris. Foi ele quem a iniciou na vida artística e a batizou de la Môme Piaf,[4] uma expressão francesa que significa "pequeno pardal" ou "pardalzinho", pois ela tinha uma estatura baixa (1,42 m). Lepleé, vendo quão nervosa Piaf ficava ao cantar, começou a ensinar-lhe como se portar no palco e disse-lhe para começar a usar um vestido preto, quando se apresentasse, vestuário que mais tarde se tornou sua marca registrada como roupa de apresentação.[11] Ele também fez enorme campanha para a noite de estreia de Piaf no Le Gerny's, o que resultou na presença de várias celebridades, como o ator Maurice Chevalier[11] e a grande vedeta do music hall, Mistinguett. Foi durante suas apresentações no Le Gerny's que Piaf conheceu o compositor Raymond Asso e a compositora Marguerite Monnot, que se tornou sua parceira profissional, grande e fiel amiga por toda sua vida.[11] São de Marguerite composições como Mon légionnaire, Hymne à l'amour, Milord e Les Amants d'un jour.
No ano seguinte (1936), Piaf assina contrato com a Polydor e lança seu primeiro disco Les Mômes de la Cloche, que se torna sucesso imediato. Mas, no dia 6 de abril desse mesmo ano, Leplée é assassinado em seu domicílio. Piaf é interrogada, e por conhecer o assassino, é acusada de cúmplice, mas sem provas de sua participação no crime, acabou sendo absolvida mais tarde. Ele foi morto por um assaltante que havia sido namorado de Édith no início de sua carreira, [13] o que gerou uma atenção negativa sobre ela por parte dos mídia, ameaçando assim a sua carreira.[11] Édith havia terminado esse relacionamento após descobrir que ele era um criminoso, mas o mesmo passou a persegui-la e ameaçá-la, e ela nunca soube se realmente foi uma coincidência ele assaltar a casa e depois matar seu empresário, se fez isso para complicar sua vida ou se na verdade pensou que Édith estava na casa e foi lá para matá-la. Esse episódio a fez recair em depressão, abusando do álcool e cigarro. Para reerguer a sua imagem, ela retoma contato com Raymond Asso, com quem, mais tarde, ela também viria a se envolver romanticamente. Raymond passou a ser o seu novo mentor e foi ele quem mudou o nome artístico dela de "La Môme Piaf" para "Édith Piaf". Ele encomendou a Marguerite Monnot canções que tratassem unicamente do passado de Piaf como cantora de rua.[11] A partir deste reencontro, Raymond começou a fazer Piaf trabalhar arduamente para se tornar uma cantora profissional de Music Hall, a orientando para que soubesse usar gestos suaves com as mãos, e figurinos mais belos no palco.
Entre 1936 e 1937, Piaf se apresentou no Bobino, um music hall no bairro Montparnasse. Em março de 1937, ela fez a sua estreia no music hall ABC, onde ela se tornou imediatamente uma imensa vedete da canção francesa, amada pelo público e difundida pela rádio. Em 1940, estreou-se no teatro com uma peça de Jean Cocteau, Le Bel Indifférent,[11] escrita especialmente para ela, e que a fez contracenar com seu então namorado, o ator Paul Meurisse. Ao lado de Paul, ela se estreia em um filme em 1941, Montmartre-sur-Seine de Georges Lacombe.
Durante a ocupação alemã na França, Piaf continuou seus shows. Muitos a consideraram uma traidora, mas, após a guerra, ela declarou que trabalhou a favor da resistência francesa. Na primavera de 1944, recém separada de Paul, Piaf conhece o jovem cantor Yves Montand e passou a ser sua amiga e mentora intelectual.[5] Eles também foram namorados.[13]
Em 1945, Piaf escreveu uma de suas primeiras canções: La Vie en Rose, a canção mais célebre dela e seu grande clássico, que a consagrou mundialmente. Em 1946, Montand estreia no cinema ao lado de Piaf em Étoile sans lumière. Neste ano também, o romance terminaria para Édith e Yves. Piaf acaba desfazendo o relacionamento devido às traições dele, no momento em que ele estava perto de alcançar o mesmo sucesso dela.
Durante esse tempo, Piaf estava fazendo muito sucesso por toda a França. Após a Segunda Guerra Mundial, tornou-se famosa internacionalmente, excursionando pela Europa, Japão, América do Sul e América do Norte. De início, ela conseguiu pouco sucesso entre o público norte-americano. Entretanto, após a publicação de uma brilhante matéria enaltecendo sua carreira, escrita por um proeminente crítico de Nova York,[11] Piaf viu seu sucesso crescer ao ponto de sua popularidade levá-la a se apresentar oito vezes no Ed Sullivan Show e duas vezes no Carnegie Hall (1956 e 1957).
Em 1947, ela faz os seus primeiros shows nos Estados Unidos. Em 1948, durante sua volta aos Estados Unidos, ela conheceu [4] o pugilista Marcel Cerdan. De nacionalidade francesa, mas nascido na Argélia, Marcel era casado ao começar seu tórrido romance com Édith Piaf. Pouco tempo depois de os dois se conhecerem, Marcel tornou-se campeão mundial de boxe. Em 28 de outubro de 1949, Marcel morreu em acidente de avião em um voo de Paris para Nova Iorque,[14] onde a reencontraria. Édith ficou extremamente abalada com a morte de seu amante: Ele ia deixar a esposa para casarem-se. Sofrendo muito por essa perda, suas crises de depressão aumentaram. Édith desenvolveu ansiedade e insônia, e também fibromialgia e artrite reumatoide, que lhe causavam dores físicas insuportáveis. Com o intuito de amenizá-las, Édith Piaf começou a se anestesiar, aplicando fortes doses de morfina. Ela acabou aumentando seu vício em cigarros e bebidas alcoólicas, vícios estes que iniciou na adolescência. Quando tais vícios começaram a prejudicar a sua performance no palco e ameaçaram sua carreira, passou por tratamentos de desintoxicação, realizando tratamento psiquiátrico, tomando ansiolíticos e antidepressivos, que também lhe prejudicavam, a deixando dopada, decidindo só tomar esses remédios quando estava em crise depressiva, os misturando com altas doses de bebidas alcoólicas para cortar o efeito da sedação e poder cantar, sem estar muito bêbada e nem muito dopada. Também passou a misturar álcool e seus remédios com altas doses de barbitúricos para tratar sua insônia. Sentindo-se muito só, acabou isolando-se socialmente em sua mansão. Nesta época de recolhimento e grande introspecção, compôs grandes sucessos que a consagraram, como Hymne à l'amour e Mon Dieu, que foram cantadas por Édith em memória a Marcel.
Em 1951 o jovem cantor Charles Aznavour foi contratado como seu secretário, assistente pessoal e motorista. Com o tempo de amizade, ele tornou-se o seu confidente, e a ajudou a sair de sua crise depressiva. Ela o ajudou a decolar sua carreira, levando-o em turnê pelos Estados Unidos e pela França, e gravando algumas de suas músicas. Nestas viagens, mantiveram um caso amoroso, uma amizade colorida como definiu Édith. No mesmo ano, de volta a França, e já separada de Charles, conheceu e iniciou um namoro com o célebre cantor francês Jacques Pills. Com seis meses de namoro, casaram-se no dia 20 de setembro de 1952. Após muitas brigas porque Jacques queria ser pai e Édith não queria ser mãe, ambos não chegaram a um acordo amigável, e divorciaram-se em 1956.
Após a separação, envolveu-se amorosamente com atores, empresários, cantores e compositores. Nesta época, iniciou uma história de amor com Georges Moustaki, apelidado de "Jo", que Édith lançou no cenário musical. Ao seu lado, sofreu um grave acidente automobilístico em 1958, enquanto viajavam durante uma madrugada chuvosa, o que a fez ficar um mês internada, e deixou como sequela o agravamento de suas dores físicas e constantes desmaios, o que a levou a um uso descontrolado de morfina, devido as dores paralisantes que sentia, precisando realizar diversos tratamentos de desintoxicação. Tudo isto piorou o seu já deteriorado estado de saúde por conta da artrite e da fibromialgia, aumentando consideravelmente sua dependência por álcool, para lidar com o agravamento da depressão, pois ficou temporariamente impossibilitada de cantar. Recuperada em alguns meses, Édith pôde voltar a sorrir ao voltar para os palcos. Nessa época gravou mais um grande sucesso, a canção Millord, da qual Moustaki foi o compositor.
Em 1960 terminou seu namoro com Georges, e iniciou um relacionamento com Théo Sarapo, um cabeleireiro francês de ascendência grega, rapaz vinte anos mais novo do que ela, onde o casal enfrentou juntos os preconceitos sociais por conta da diferença etária. Os dois casaram-se em 1962, e ficaram juntos até o falecimento de Édith, que o ajudou bastante a se tornar um cantor e ator de sucesso.[11]
Édith Piaf faleceu em 10 de outubro de 1963, vítima de câncer no fígado, em decorrência do alcoolismo, em Plascassier, na cidade de Grasse, nos Alpes Marítimos, aos 47 anos. O consumo de bebidas alcoólicas, cigarros, barbitúricos e grande quantidade de morfina para tratar de sua dor intensa causada pelo que hoje se sabe, lúpus eritematoso sistêmico tiveram um severo impacto em sua saúde.
Poucos meses antes de sua morte, ela alugara uma mansão de 25 cômodos de frente à praia, onde passou 2 meses de descanso com amigos e com o marido. Segundo o seu acordeonista, Marc Bonel, foi um período de muitas festas: almoços e jantares para 30 a 40 pessoas, todos os dias, regados a muito espumante, whisky e música.
Por medida de economia, transferiu-se para uma casa em Plascassier, apenas com seus funcionários principais: A enfermeira, o acordeonista, a secretária e o empresário. Mesmo muito ocupado profissionalmente com viagens, o seu marido foi junto, onde na época trabalhava em um filme em Paris para "assegurar o dinheiro do casal", como dizia a própria Piaf.
Édith Piaf faleceu em consequência de uma hemorragia interna no fígado: A cantora estava em coma há duas semanas. Como disse certa vez em um documentário, ela morreu "sem um grito, sem uma palavra..." O transporte de seu corpo para Paris foi feito clandestina e ilegalmente. O seu falecimento foi anunciado oficialmente no final do dia 11 de outubro. Faleceu no mesmo dia que o seu amigo Jean Cocteau e foi enterrada no cemitério do Père-Lachaise (Divisão 97).
Édith Piaf influenciou grande parte dos artistas da sua época. Tornou-se principalmente uma ponte para que estes se conhecessem; geralmente, o seu círculo de amizade se encontrava em sua casa. Foi na sua residência que grandes nomes da música francesa tiveram o primeiro contato. Em diversas vezes, iniciaram maravilhosas parcerias musicais, tais como Gilbert Bécaud, Jacques Pills (célebre cantor francês com quem a intérprete se casou em setembro de 1952 na Igreja de São Vicente de Paulo), Jacques Plante, Louis Amade, Charles Aznavour (com quem também teve um caso amoroso), Jean Broussolle, Yves Montand, Jacques Prévert, Francis Lemarque, entre tantos outros, hoje também consagrados na história fonográfica da França e do mundo.
Segundo Marc Robine, em seu livro Il était une fois la chanson française: Des trouvères à nos jours, é na casa de Piaf que Gilbert Bécaud começa a sua amizade com Charles Aznavour com quem ele escreverá diversas canções como Mé qué me qué ou La Ville, que serão registradas em cada um deles, interpretadas e preparadas de maneira bem diferente. Ainda na casa da cantora, Bécaud reencontra Jean Broussolle – das Compagnons de La Chanson, que lhe escreverá as letras de Alors, raconte. Os Compagnons registraram, no curso de sua longa carreira, uma gama de canções de Bécaud e Aznavour. Também na residência de Piaf, Charles Aznavour conheceu Jacques Plante, que se tornou um de seus grandes colaboradores (For me…formidable, La Bohème, Les Comédiens…). Assim, pouco a pouco, o círculo de relações e de colaborações de Piaf foi se alargando ainda mais. Na época imediata ao pós-guerra, que via nascer toda uma nova geração de artistas, não só a cantora teve apoio incondicional de seus amigos, grandes profissionais de música, mas também ajudou na carreira de muitos deles.
James Huge Cook (Marton, 27 de outubrojul./ 7 de novembro de 1728greg. — Kealakekua, 14 de fevereiro de 1779) foi um explorador, navegador e cartógrafo inglês tendo depois alçado a patente de capitão na Marinha Real Britânica. Cook foi o primeiro a mapear Terra Nova antes de fazer três viagens para o Oceano Pacífico durante a qual ele conseguiu o primeiro contacto europeu com a costa leste da Austrália e o Arquipélago do Havaí, bem como a primeira circum-navegação registrada da Nova Zelândia.[1]
James Cook entrou na marinha mercante britânica quando era adolescente[2] e ingressou na Marinha Real em 1755. Ele participou da Guerra dos Sete Anos, e posteriormente estudou e mapeou grande parte da entrada do Rio São Lourenço durante o cerco de Quebec. Isso permitiu que General Wolfe fizesse o seu famoso ataque nas Planícies de Abraão, e ajudou a levar Cook à atenção da Almirantado Britânico e Royal Society. Esta notícia veio em um momento crucial, tanto na sua carreira pessoal e na direção das explorações ultramarinas britânicas, e levou o seu cargo em 1766 como comandante da HMS Endeavour para a primeira das três viagens do Pacífico.
Cook cartografou muitas áreas e registrou várias ilhas e zonas costeiras nos mapas europeus pela primeira vez. Seus resultados podem ser atribuídos a uma combinação de navegação, superior levantamento cartográfico e competências, a coragem em explorar locais perigosos para confirmar os factos (por exemplo, a imersão no Círculo Polar Antártico repetidamente e explorar ao redor da Grande Barreira de Coral), uma capacidade de conduzir os homens em condições adversas, e ousadia, tanto em relação à medida da sua exploração e sua vontade de ultrapassar as instruções dadas a ele pelo Almirantado Britânico.[2]
No seu livro "Colapso" (2005), o biólogo e biogeógrafo Jared Diamond cita um registro feito por Cook em que o capitão descreve uma breve visita à ilha de Páscoa, em 1744. "Pequenos, magros, tímidos e miseráveis", foi como Cook descreveu os insulares, que já enfrentavam um forte problema ambiental.
Cook morreu na baía havaiana de Kealakekua em 1779, em uma luta com os nativos durante a sua terceira viagem exploratória na região do Pacífico. A casa de Cook na Inglaterra é hoje um memorial. Cook é considerado o pai da Oceania.
James Cook nasceu na vila de Marton em Yorkshire, hoje pertencente a um subúrbio da cidade de Middlesbrough.[3] Cook foi baptizado na igreja local de St. Cuthbert's, em cujo registro pode-se hoje ver o seu nome. Cook foi o segundo dos oito filhos de James Cook, um trabalhador de fazenda escocês, e sua esposa natural do local Grace Pace de Thornaby-on-Tees.[2][3] Em 1736, a sua família mudou para a fazenda Airey Holme em Great Ayton, onde o patrão do seu pai, Thomas Skottowe pagou para ele frequentar a escola local (agora um museu).[1] Em 1741, após 5 anos de escolaridade, ele começou a trabalhar para o seu pai, que havia sido promovido a gerente de fazenda. Para lazer iria subir uma colina, Roseberry Topping, aproveitando a oportunidade para a solidão.[4] Cook's Cottage, a última casa de seus pais, que parece que ele visitou, está agora em Melbourne, tendo sido transferida a partir da Inglaterra e reagrupados tijolo por tijolo em 1934.[5]
Em 1745, aos 16 anos, Cook mudou-se em 32 km para a aldeia pescadora de Staithes para ser aprendiz de vendedor em uma mercearia de William Sanderson.[3] Historiadores têm especulado que foi ali onde Cook sentiu a primeira atração do mar, enquanto olhava para fora da vitrine.[2]
Após 18 meses, não tendo sido aprovado para o trabalho de loja, Cook viajou para a vizinha cidade portuária de Whitby para ser apresentado a uns amigos de Sanderson, Henry e John Walker.[5] Os Walkers eram proeminentes armadores locais e Quakers, e estavam no comércio de carvão. Cook foi aprendiz em sua pequena frota de navios que operavam no transporte de carvão ao longo da costa inglesa. A sua primeira missão foi a bordo do cargueiro Freelove, e ele passou vários anos nesta rota de cabotagem bem como em várias outras entre o rio Tyne e Londres.
Como parte dessa aprendizagem, Cook aplicou-se ao estudo de álgebra, geometria, trigonometria, navegação e astronomia, todas as competências que seriam necessárias no futuro para comandar um navio próprio.[2]
Após seus três anos de aprendizagem concluídos, Cook começou a trabalhar no comércio naval no Mar Báltico. Ele rapidamente progrediu através das fileiras da marinha mercante, começando com a sua promoção em 1752 a Mate (funcionário encarregado de navegação) a bordo do Collier cargueiro Amizade. Em 1755, dentro de um mês, ao lhe ser oferecido o comando deste navio, voluntaria-se para a marinha e é recrutado para o que viria a ser a Guerra dos Sete Anos. Apesar da necessidade de reiniciar na parte inferior da hierarquia naval, Cook iria avançar mais rapidamente sua carreira em serviço militar, e entrou na Marinha em Wapping em 7 de junho de 1755.[6]
Em 1768, no navio HMS Endeavour, Cook foi o comandante escolhido para levar os membros da Royal Society ao Taiti, para observar o trânsito de Vênus, na primeira expedição científica pelo Pacífico. O astrônomo encarregado da observação do evento, Charles Green, faleceu durante a viagem, quando o navio passava por Batávia (antigo nome de Jacarta, capital da Indonésia).[1] Cook esteve em novembro de 1768 no Rio de Janeiro, mas os tripulantes não receberam permissão para aportar, ficando reclusos nas embarcações e sob vigia das autoridades portuguesas. O famoso naturalista Joseph Banks, que participava da expedição, teve momentos fortuitos e conseguiu recolher 320 espécies vegetais nos arredores da cidade, segundo o livro de John Hawkesworth, "An account of the voyages undertaken by the order of his Present Majesty for Making Discoveries" (Londres 1773). As autoridades locais negavam a permanência de estrangeiros na colônia e o francês Louis Antoine de Bougainville enfrentou obstáculos semelhantes quando visitou o Rio de Janeiro.
Com a embarcação HMS Resolution comandada pelo capitão James Cook, e acompanhada da fragata Adventure, a expedição tinha como objetivo identificar a existência de um continente no Polo Sul. A expedição comandada por Cook  para a região Antártica foi uma das pioneiras missões da Europa na região, logo o seu conteúdo científico obtido pelos pesquisadores durante a viagem, desde material coletado a ilustrações, serviu como princípio para o início de pesquisas no continente Antártico.[7]
Em busca da Terra Australis, Cook circunavegou toda a Nova Zelândia e a Austrália, identificando que eles não estavam conectados a uma massa de terra maior ao sul chegou a conclusão de que o continente Antártico estaria em latitude mais alta, ainda mais ao sul desses países. Em 31 de Janeiro de 1774, Cook atingiu a latitude dos 71° Sul, e ao continuar a exploração encontrou o continente Antártico. Após a descoberta ele teve de voltar ao Taiti para reabastecer a embarcação e então voltar a região recém descoberta para que pudesse explorá-la. Foi nessa expedição que ocorreu a descoberta das ilhas Cook, nomeada em homenagem ao capitão.[7]
Já a terceira grande viagem do capitão Cook, e ainda no comando do HMS Resolution, essa expedição objetivava a descoberta de uma possível conexão entre o Oceano Pacífico Norte e o Oceano Atlântico Norte pelo mar Ártico.[8]
Durante essa expedição, Cook chegou a atravessar o Estreito de Bering navegando pelo mar de Chukchi, em agosto de 1778. Chegou a atravessar latitudes superiores a 77 ° Norte, em direção a costa Nordeste do Alasca, até ter seu caminho bloqueado pelo gelo.
Após o sucesso da expedição científica, Cook prosseguiu com a exploração. Durante a viagem, descobre o arquipélago que batiza de Ilhas Sociedade, na Polinésia Francesa, e mapeia toda a Nova Zelândia. No regresso, descobre a costa ocidental da Austrália.[1]
Em 1772, Cook parte para nova circunavegação ao comando das naus Resolution e Adventure. Durante esta viagem chega à mais baixa latitude ao sul alcançada até então (70°10''S), cruzando pela primeira vez o Círculo Polar Antártico. Esta viagem resultou na descoberta das Ilhas Cook.
Em 1776, com os navios Resolution e Discovery, Cook parte para a missão que seria a sua última e descobre o arquipélago do Havaí, que chama de Sandwich. Costeia a América e atravessa o estreito de Bering, chegando ao Ártico. No regresso ao Havaí, é morto pelos nativos ao voltar a Kealakekua para consertar o mastro do Resolution.[9] Cook ficou conhecido pela preocupação com a saúde e a alimentação de sua tripulação. Em sua primeira viagem, nenhum membro da tripulação morre de escorbuto, doença causada pela falta de ácido ascórbico (vitamina C) no organismo e responsável pela morte de muitos marinheiros até o século XVIII.
Como a grande maioria dos capitães da época, Cook também escrevia diários, nos quais eram descritos acontecimentos dessas grandes viagens exploratórias pelos oceanos. Os diários de bordo geralmente trazem descrições diárias da vida no mar, observações e descrições de organismos e espécies marinhas. Também é possível encontrar relatos de curiosidades sobre diferentes culturas, para cada localidade dos portos de desembarque para reabastecimentos de suprimentos para as viagens.[10]
Cópias dos diários de bordo do capitão Cook eram enviados para o governo Britânico durantes as grandes viagens, servindo como relatórios. A primeira cópia enviada por Cook só ocorreu dois anos após o início de sua primeira viagem, sendo enviada de Batavia (hoje chamada de Jacarta), capital da Indonésia no ano de 1770.  Alguns dos diários de navegação do capitão James Cook ainda se encontram preservados e algumas de suas páginas digitalizadas. Volumes originais desses livros podem ser encontrados em museus, como o Museu Nacional da Austrália.[11]
Durante a navegação no oceano Pacífico várias observações sobre o clima das regiões foram feitas, para as ilhas descobertas durante as missões muito se comentou sobre a vegetação e morfologia do local. Regiões tropicais foram descritas como possuindo temperaturas moderadas, o que refletia nas estruturas habitacionais dos nativos dos países quentes. Também foram realizadas descrições de variadas rochas, que variavam de cor, como rochas mais escuras ou mais claras, e características como a dureza desse material, sua facilidade de ser quebrada ou não. Descrições sobre os vegetais presentes nas ilhas também estavam presentes nas observações, como a variedade de plantas, árvores que produziam frutos, além de descrições da grande variedade de sementes dos locais.[12]
Todas essas informações serviram como base de estudos para entender as variedades de plantas e animais em diferentes regiões do planeta, entre diferentes continentes e ilhas.
Além de todo conhecimento biológico obtido nessas viagens, outro grande progresso obtido nessas expedições foi a elaboração de mapas, e mesmo para a época eles eram elaborados com grandiosa precisão de latitude e longitude, calculados a partir da observação de corpos celestes e conhecimentos precisos da superfície da Terra.
Devido a toda contribuição das expedições para a comunidade científica, possibilitada pelas viagens comandadas por James Cook, o capitão foi considerado como uma espécie de herói pelos pesquisadores da época.
 
Stanley Kubrick (Manhattan, Nova Iorque, 26 de julho de 1928 — St Albans, Hertfordshire, 7 de março de 1999) foi um cineasta, roteirista, produtor e fotógrafo estadunidense. Frequentemente apontado como um dos cineastas mais influentes do cinema, seus filmes, que são principalmente adaptações de romances ou contos, cobrem uma ampla variedade de gêneros e são conhecidos por seu realismo, humor sombrio, cinematografia única, extensos cenários e uso evocativo da música.
Kubrick foi criado no Bronx, Nova Iorque, e frequentou a William Howard Taft High School de 1941 a 1945. Ele recebia notas medianas na escola, mas demonstrava um grande interesse em literatura, fotografia e cinema desde tenra idade e aprendeu por si próprio todos os aspectos de produção e direção de filmes depois de terminar o colegial. Depois de trabalhar como fotógrafo da revista Look no final da década de 1940 e no início da década de 1950, começou a fazer curtas-metragens com um orçamento apertado e fez seu primeiro grande filme de Hollywood, The Killing, para a United Artists em 1956. Isso foi seguido por duas colaborações com Kirk Douglas: o filme de guerra Paths of Glory (1957) e o épico histórico Spartacus (1960).
Diferenças criativas decorrentes de seu trabalho com Douglas e os estúdios de cinema, uma aversão à indústria de Hollywood e uma crescente preocupação com o crime nos Estados Unidos levaram Kubrick a se mudar para o Reino Unido em 1961, onde passou a maior parte do resto de sua vida e carreira. Sua casa em Childwickbury Manor, em Hertfordshire, que ele compartilhou com sua esposa Christiane, tornou-se seu local de trabalho, onde escrevia, pesquisava, editava e gerenciava os detalhes da produção. Isso lhe permitiu ter controle artístico quase completo sobre seus filmes, mas com a rara vantagem de ter apoio financeiro dos principais estúdios de Hollywood. Suas primeiras produções britânicas foram dois filmes com Peter Sellers: Lolita (1962), uma adaptação do romance de Vladimir Nabokov, e a comédia negra sobre a Guerra Fria Dr. Strangelove, de 1964.
Perfeccionista exigente, Kubrick assumiu o controle sobre a maioria dos aspectos do processo de filmagem, desde a direção e a redação até a edição, e teve o cuidado de pesquisar seus filmes e encenar cenas, trabalhando em estreita coordenação com seus atores e outros colaboradores. Ele costumava pedir várias dezenas de retomadas da mesma cena em um filme, o que resultava em muitos conflitos com seus elencos. Apesar da notoriedade resultante entre os atores, muitos dos filmes de Kubrick abriram novos caminhos na cinematografia. O realismo científico e os efeitos especiais inovadores de 2001: A Space Odyssey (1968) não tiveram precedentes na história do cinema e o filme lhe rendeu seu único Óscar pessoal, por Melhores Efeitos Visuais. Steven Spielberg se referiu ao filme como o "big bang" de sua geração e é considerado um dos melhores filmes já feitos. Apesar de muitos dos filmes de Kubrick serem controversos e receberem inicialmente críticas mistas após o lançamento — particularmente A Clockwork Orange (1971), que foi retirado de circulação no Reino Unido após um frenesi da mídia de massa — a maioria deles foi indicada ao Óscar, Globo de Ouro ou Prêmio BAFTA, e passou por reavaliações críticas. Para o filme de período do século XVIII, Barry Lyndon (1975), obteve lentes desenvolvidas pela Zeiss para a NASA para filmar cenas sob a luz natural das velas. Com The Shining (1980), ele se tornou um dos primeiros diretores a usar um Steadicam para filmagens de rastreamentos estabilizados e fluidos, uma tecnologia vital para sua produção sobre a Guerra do Vietnã, Full Metal Jacket, de 1987. Seu último filme, Eyes Wide Shut, foi concluído pouco antes de sua morte em 1999, aos 70 anos.
Kubrick nasceu em 26 de julho de 1928, no Lying-In Hospital em Manhattan, Nova Iorque, em uma família judia.[1][2] Foi o primeiro dos dois filhos de Jacob Leonard Kubrick, conhecido como Jack ou Jacques, e sua esposa Sadie Gertrude Kubrick, ou Gert. Sua irmã Barbara Mary Kubrick nasceu em maio de 1934.[3] Jack Kubrick, cujos pais e avós paternos eram de origem judaico polonesa e judaico romena,[1] era médico homeopata,[4] graduado pela New York Homeopathic Medical College em 1927, mesmo ano em que se casou com a mãe de Kubrick, filha de imigrantes austríacos judeus.[5] Seu bisavô Hersh Kubrick chegou a Ilha Ellis através de Liverpool de navio em 27 de dezembro de 1899, aos 47 anos, deixando para trás a esposa e dois filhos adultos, um dos quais seria o avô de Stanley, Elias, para começar uma nova vida com uma mulher mais jovem.[6] Elias Kubrick os acompanhou em 1902.[7] Quando Stanley nasceu, os Kubricks moravam no Bronx.[8] Seus pais se casaram em uma cerimônia judaica, mas ele não teve uma educação religiosa e mais tarde professou uma visão ateísta do universo.[9] Seu pai era médico e, pelos padrões do West Bronx, a família era bastante rica.[10]
Logo após o nascimento de sua irmã, Kubrick começou a estudar na Escola Pública 3 no Bronx e mudou-se para a Escola Pública 90 em junho de 1938. Descobriu-se que seu quociente de inteligência estava acima da média, mas sua frequência era ruim.[2] Desde muito jovem demonstrou interesse pela literatura e começou a ler os mitos gregos e romanos e as fábulas dos irmãos Grimm, que "incutiram nele uma afinidade vitalícia com a Europa".[11] Ele passava a maior parte dos sábados durante o verão assistindo ao New York Yankees e mais tarde fotografou dois meninos assistindo ao jogo num trabalho para a revista Look para imitar sua própria emoção de infância com o beisebol.[10] Quando tinha 12 anos, seu pai Jack lhe ensinou xadrez. O jogo permaneceu um interesse permanente,[12] aparecendo em muitos de seus filmes.[13] Kubrick, que mais tarde se tornou membro da Federação de Xadrez dos Estados Unidos, explicou que o jogo o ajudou a desenvolver "paciência e disciplina" na tomada de decisões.[14] Quando tinha 13 anos, seu pai lhe comprou uma câmera Graflex, despertando o fascínio pela fotografia. Ele fez amizade com um vizinho, Marvin Traub, que compartilhava essa paixão.[15] Traub tinha sua própria sala escura, onde ele e o jovem Kubrick passavam muitas horas examinando fotografias e observando os produtos químicos "criarem imagens magicamente em papel fotográfico".[3] Os dois se entregaram a inúmeros projetos fotográficos para os quais percorriam as ruas em busca de assuntos interessantes para capturar e passavam um tempo nos cinemas locais estudando filmes. O fotógrafo freelance Weegee (Arthur Fellig) teve uma influência considerável no desenvolvimento de Kubrick como fotógrafo; posteriormente, contratou Fellig como fotógrafo especial para Dr. Strangelove (1964).[16] Quando adolescente, também se interessou por jazz e brevemente tentou uma carreira como baterista.[17]
Kubrick estudou na William Howard Taft High School de 1941 a 1945.[18] Embora tenha ingressado no clube de fotografia da escola, que lhe permitia fotografar os eventos em sua revista,[3] era um aluno medíocre, com nota média de 67/D+.[19] Introvertido e tímido, tinha um baixo histórico de frequência e frequentemente faltava à escola para assistir dois filmes.[20] Formou-se em 1945, mas suas notas baixas, combinadas com a demanda de soldados que voltaram da Segunda Guerra Mundial por admissão na faculdade, eliminaram qualquer esperança de educação superior. Posteriormente, Kubrick falou com desdém de sua educação e da educação americana como um todo, afirmando que as escolas eram ineficazes em estimular o pensamento crítico e o interesse dos alunos. Seu pai ficou desapontado com o fracasso do filho em alcançar a excelência na escola da qual ele sabia que Stanley era totalmente capaz. Jack também encorajou o filho a ler a biblioteca da família em casa, enquanto permitia que adotasse a fotografia como um hobby sério.[21]
Enquanto cursava o ensino médio, Kubrick foi escolhido como fotógrafo oficial da escola. Em meados da década de 1940, como não conseguiu ser admitido nas aulas diurnas nas faculdades, frequentou brevemente as aulas noturnas no City College de Nova Iorque.[22] Eventualmente, vendeu uma série de fotografias para a revista Look,[23] sendo impressas em 26 de junho de 1945. Kubrick complementava sua renda jogando xadrez "por quarteirão" no Washington Square Park e em vários clubes de xadrez de Manhattan.[24]
Em 1946, tornou-se aprendiz de fotógrafo da Look e, mais tarde, fotógrafo da equipe em tempo integral. G. Warren Schloat, Jr., outro novo fotógrafo da revista na época, lembrou que achava que faltava a ele personalidade para se tornar um diretor em Hollywood, comentando: "Stanley era um sujeito quieto. Ele não falava muito. Ele era magro, magro e meio pobre — como todos nós éramos."[25] Kubrick rapidamente se tornou conhecido por contar histórias em fotografias. A primeira, publicada em 16 de abril de 1946, intitulava-se "Um conto de uma sacada de cinema" e encenava uma briga entre um homem e uma mulher, durante a qual o homem leva um tapa na cara, pego genuinamente de surpresa.[23] Em outra atribuição, foram tiradas 18 fotos de várias pessoas esperando em um consultório odontológico. Foi dito retrospectivamente que este projeto demonstrou um interesse inicial de Kubrick em capturar indivíduos e seus sentimentos em ambientes mundanos.[26] Em 1948, foi enviado a Portugal para documentar um relato de viagem e cobriu o Ringling Bros. e Barnum & Bailey Circus em Sarasota, Flórida.[27][nota 1]
Entusiasta do boxe, Kubrick eventualmente começou a fotografar lutas do esporte para a revista. Sua primeira, "Prizefighter", foi publicada em 18 de janeiro de 1949 e capturou uma luta de boxe e os eventos que levaram a ela, apresentando Walter Cartier.[29] Em 2 de abril de 1949, publicou na Look o ensaio fotográfico "Chicago-City of Extremes", o qual mostrou desde cedo seu talento para criar uma atmosfera com imagens. No ano seguinte, em julho de 1950, a revista publicou seu ensaio fotográfico "Working Debutante — Betsy von Furstenberg", que trazia ao fundo um retrato de Angel F. de Soto por Pablo Picasso.[30] Kubrick também foi designado para fotografar vários músicos de jazz, de Frank Sinatra e Erroll Garner a George Lewis, Eddie Condon, Phil Napoleon, Papa Celestin, Alphonse Picou, Muggsy Spanier, Sharkey Bonano e outros.[31]
Casou-se com Toba Metz, sua namorada do colégio, em 28 de maio de 1948. Eles moravam juntos em um pequeno apartamento na 36 West 16th Street, na Sexta Avenida, ao norte de Greenwich Village.[32] Durante esse tempo, começou a frequentar exibições de filmes no Museu de Arte Moderna e nos cinemas de Nova Iorque. Ele se inspirou na filmagem complexa e fluida do diretor Max Ophüls, cujos filmes influenciaram seu estilo visual, e do diretor Elia Kazan, a quem descreveu como o "melhor diretor" da América na época, com sua habilidade de "fazer milagres" com seus atores.[33] Amigos começaram a notar que Kubrick havia ficado obcecado com a arte de fazer filmes — um amigo, David Vaughan, observou que ele examinava o filme no cinema quando ficava mudo e voltava a ler seu jornal quando as pessoas começavam a falar.[23] Passou muitas horas lendo livros sobre teoria do cinema e escrevendo notas. Foi particularmente inspirado por Serguei Eisenstein e Arthur Rothstein, o diretor técnico fotográfico da Look.[34][nota 2]
Kubrick compartilhou o amor pelo cinema com seu amigo de escola Alexander Singer, que depois de se formar no colégio tinha a intenção de dirigir uma versão cinematográfica da Ilíada de Homero. Por meio de Singer, que trabalhava nos escritórios da produtora de cinejornais The March of Time, Kubrick soube que poderia custar US$40 mil para fazer um curta-metragem adequado, dinheiro que não podia pagar. Tinha US$1500 em economias e produziu alguns documentários curtos alimentados pelo incentivo do amigo. Começou a aprender tudo o que podia sobre cinema por conta própria, ligando para fornecedores de filmes, laboratórios e locadoras de equipamentos.[35]
Decidiu fazer um documentário curta metragem sobre o boxeador Walter Cartier, a quem havia fotografado e escrito para a revista Look um ano antes. Alugou uma câmera e produziu um documentário em preto e branco de 16 minutos, Day of the Fight. Kubrick conseguiu o dinheiro de forma independente para financiá-lo. Pensou em pedir a Montgomery Clift para narrá-lo, a quem conheceu durante uma sessão fotográfica da revista, mas decidiu pelo locutor veterano da CBS, Douglas Edwards.[36] De acordo com Paul Duncan, o projeto foi "notavelmente feito para um primeiro filme" e usou um tiro de rastreamento reverso para filmar uma cena em que Cartier e seu irmão caminham em direção à câmera, um dispositivo que mais tarde se tornou um dos movimentos de câmera característicos do cineasta.[37] Vincent Cartier, irmão e empresário de Walter, refletiu mais tarde sobre suas observações de Kubrick durante as filmagens. Ele disse que "Stanley era uma pessoa do tipo muito estóico, impassível, mas imaginativo, com pensamentos fortes e imaginativos. Ele impunha respeito de uma forma quieta e tímida. O que quer que ele quisesse, você cumpria, ele apenas te cativava. Qualquer um que trabalhou com Stanley fez exatamente o que Stanley queria."[35] Depois que uma trilha sonora foi acrescentada por Gerald Fried, um amigo de Singer, Kubrick gastou US$3900 para fazê-la e a vendeu para a RKO-Pathé por US$4 mil, o que foi o máximo que a empresa já pagou por um curta na época.[37] Descreveu seu primeiro esforço no cinema como valioso, pois acreditava ter sido forçado a fazer a maior parte do trabalho,[38] e posteriormente declarou que "a melhor educação sobre filmes é fazer um."[3]
Estreou como cineasta de curta-metragens aos 22 anos. Aos 25, obteve uma grande ajuda financeira do pai, que penhorou a casa para a produção de Fear and Desire, de 1953, seu primeiro longa-metragem. Considerou o trabalho amador e, mesmo com algumas boas críticas, logo tratou de retirá-lo de circulação. Até hoje, o filme permanece fora de catálogo, tendo sido exibido poucas vezes em festivais ou distribuído ilegalmente. Logo após, Kubrick realizaria outro longa, Killer's Kiss (br: A Morte Passou Por Perto), de 1955, outro filme pouco divulgado e de difícil acesso. Mas é a partir de The Killing (br: O Grande Golpe), de 1956, que sua carreira começa a funcionar. A trama sobre um plano de assalto ganhou a atenção de alguns produtores. Apesar disso, teve dificuldades com a adaptação da novela Paths of Glory. O filme homônimo foi estrelado pelo astro Kirk Douglas, que ajudou a levantar o projeto após ele ter sido rejeitado pelos estúdios. Kubrick fez um dos filmes antiguerra mais poderosos que o mundo do cinema já viu. Focado não em heróis, mas sim em covardes. Apesar das excelentes críticas, Paths of Glory (br: Glória Feita de Sangue; pt: Horizontes de Glória), de 1957, foi proibido em alguns países, incluindo a França.[39]
Kirk Douglas gostou de trabalhar com Kubrick. Foi ele quem o chamou para dirigir o épico Spartacus (1960) após a tensa demissão do veterano diretor Anthony Mann. Mann já havia filmado boa parte da produção quando Kubrick, com apenas 29 anos, assumiu o seu lugar. A boa relação com Douglas viria por água a baixo quando diferenças criativas se confrontaram. Kubrick perdeu a batalha e se viu obrigado a filmar sem poder colocar algumas de suas ideias em prática. Mesmo com o sucesso do filme, ele decidiu que dali por diante só iria aceitar projetos em que pudesse ter total liberdade criativa. E foi com esse pensamento que se muda para a Inglaterra em 1962. No mesmo ano começa as filmagens de Lolita (1962), clássico da literatura escrita por Vladimir Nabokov. A curiosidade sobre a adaptação da obra de Nabokov dá grande visibilidade ao filme, que mesmo imerso em polêmicas (a relação entre um homem de meia-idade e uma adolescente era a principal delas) se torna outro grande sucesso de crítica. Dois anos depois, o diretor lança outro clássico absoluto: Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1964) (Doutor Fantástico no Brasil; Doutor Estranhoamor em Portugal). Tendo como tema a ameaça nuclear, o filme é uma comédia de humor negro com atuações e roteiro primorosos. Para atuar, Kubrick chamou o comediante inglês Peter Sellers, que se desdobra em três papéis, incluindo o de presidente dos Estados Unidos e George C. Scott (que alguns anos mais tarde se destacaria de vez em Patton, Rebelde ou Herói?). Entre os vários momentos clássicos, está o final, com direito a um major cowboy montado sobre uma bomba atômica no ar. Esse trabalho rendeu a Kubrick sua primeira indicação ao Oscar de melhor diretor.[carece de fontes?]
Cinco anos de produção foram necessários para o desenvolvimento de 2001: A Space Odyssey (br: 2001: Uma Odisseia no Espaço), de 1968, para muitos a melhor ficção científica já filmada.[40] Foi escrito ao mesmo tempo em que o livro homônimo de Arthur C. Clarke estava em produção. Clarke, inclusive, deu assistência na criação do roteiro. 2001 teve uma recepção fria da crítica, mas obteve sucesso junto ao público. Até hoje, possui em sua força maior as músicas de Richard Strauss, Also sprach Zarathustra e Johann Strauss II, Danúbio Azul. Os efeitos especiais, inovadores para a época, garantiram ao filme um Oscar da categoria.[41]
Novamente indicado a melhor diretor, Kubrick vê o seu prêmio escapar. Logo após, chateado com o cancelamento do longa sobre Napoleão Bonaparte, ele segue para mais uma adaptação. Dessa vez o livro é A Clockwork Orange (Laranja Mecânica), de 1962, de Anthony Burgess, focado na violência humana e, principalmente, na da juventude. Causou grande polêmica na época de seu lançamento e foi acusado de incitar a barbárie. Na história, quatro jovens de classe trabalhadora passam as noites cometendo as maiores atrocidades: brigar, roubar, estuprar… são apenas algumas delas. A vida de um deles, Alex, (Malcolm McDowell) toma um rumo diferente quando o mesmo vai para a cadeia. Disparado o trabalho mais controverso do diretor, que lhe rendeu outra indicação ao prêmio da Academia e outra derrota.[carece de fontes?]
Nos anos seguintes, três filmes totalmente diferentes: um longuíssimo filme de época, um terror de gelar a espinha e a sua visão da Guerra do Vietnã. O primeiro é Barry Lyndon (1975). Linda obra saída de uma novela de William Makepeace Thackeray. Apesar de ser pouco conhecido, o filme é considerado por muito dos fãs de Kubrick, entre eles Martin Scorsese, como seu melhor trabalho. Pois nele é perfeitamente visível o seu perfeccionismo, característica marcante em sua carreira. Barry Lyndon, interpretado magistralmente por Ryan O'Neal, é uma espécie de "talentosa fraude" que inevitavelmente é expulso de onde quer que se meta. A fotografia do filme, dirigida por John Alcott — trabalhando sob a orientação técnica de Kubrick — é outro momento inesquecível. Kubrick usou lentes criadas pela NASA para poder filmar alguns interiores. Detalhe: iluminados apenas com velas. Mesmo com todo o cuidado da produção, Barry Lyndon fracassou nos Estados Unidos, mas fez relativo sucesso na Europa. Levou quatro estatuetas douradas, mas de novo o admirável trabalho de Stanley como diretor não foi reconhecido pela maioria votante. Ele voltaria a ter um novo sucesso mundial com o clássico O Iluminado (1980) (The Shining), adaptação da obra de Stephen King. A história de uma família que passa uma temporada em um hotel nas montanhas até hoje faz sucesso onde quer que seja exibida. Quase no final dos anos 1980, Kubrick ressurgiria dando ênfase a guerra. Dessa vez a do Vietnã. Saída do livro de Gustav Hasford, Full Metal Jacket (br: Nascido Para Matar), de 1987, quase que uma versão "kubrickiana" de Apocalypse Now.[42] O filme é praticamente dividido em duas partes: a preparação para a guerra e o ambiente de combate. Kubrick disse que ficou frustrado com o fato de que antes da estreia do filme dois outros longas sobre o tema já tinham sido lançados com sucesso: The Killing Fields (br: Os Gritos do Silêncio), de 1984, e Platoon, de 1986. Ainda como destaque da produção está o imenso set erguido em Londres para a batalha final.[carece de fontes?]
Do seu último longa-metragem até Eyes Wide Shut (br: De Olhos Bem Fechados), de 1999, passou-se um longo período sem nada assinado por Stanley. Lançado em 1999, o filme protagonizado pelo (até então) casal número um dos Estados Unidos, causou uma grande comoção entre os amantes da sétima arte. Tom Cruise e Nicole Kidman interpretam um casal em crise e foi adaptada de romance escrito por Arthur Schnitzler, chamado Traumnovelle. Dois anos foi o período de filmagem, tempo que o perfeccionismo de Kubrick achou necessário para a conclusão do filme, mas não o necessário para agradar à crítica e público.
Kubrick faleceu enquanto dormia, devido a um ataque cardíaco, em 7 de março de 1999,[43] não testemunhando a fria recepção que seu último trabalho obteve. O último projeto cinematográfico em que esteve envolvido, mas que por questões de saúde não dirigiu, foi AI:Inteligência Artificial, de Steven Spielberg.[44] Encontra-se sepultado em Childwickbury Manor, Hertfordshire na Inglaterra.[45]
Kubrick foi eleito o sexto maior diretor de todos os tempos pelos cineastas pesquisados pelo British Film Institute e a revista Sight & Sound em 2002.[46]
 Documentários de curta-metragem:
Leonhard Paul Euler (pronúncia em português: ['ɔjler]; pronúncia em alemão: [ˈɔʏlɐ] (escutar?·info), pronúncia local: [ˈɔɪlr̩] (escutar?·info); Basileia, 15 de abril de 1707  –  São Petersburgo, 18 de setembro de 1783) foi um matemático e físico suíço de língua alemã que passou a maior parte de sua vida na Rússia e na Alemanha.[2] Fez importantes descobertas em várias áreas da matemática como o cálculo e a teoria dos grafos. Também introduziu muitas das terminologias da matemática moderna e da notação matemática, particularmente na análise matemática, assim como no conceito de função matemática.[3] Também é reconhecido por seus trabalhos na mecânica, dinâmica de fluidos, óptica, astronomia e teoria da música.[4]
Euler é considerado um dos mais proeminentes matemáticos do século XVIII e também é considerado como um dos grandes matemáticos de todos os tempos, assim como Isaac Newton, Arquimedes e Carl Friedrich Gauss.[5] Foi um dos mais prolíficos matemáticos, calcula-se que toda a sua obra reunida teria entre 60 e 80 volumes de quartos.[6] Viveu a maior parte da vida em São Petersburgo, na Rússia, e em Berlim, que na época era capital da Prússia.
Uma declaração atribuída a Pierre-Simon Laplace manifestada sobre Euler na sua influência sobre a matemática: "Leiam Euler, leiam Euler, ele é o mestre de todos nós".[7][8]
Leonhard Paul Euler [AFI: [ˈɔʏlɐ], ver pronúncia em alemão?·info] nasceu no dia 15 de abril de 1707 em Basileia, na Suíça, filho do pastor calvinista Paul Euler e Margaret Brucker, filha de um pastor. Teve duas irmãs mais novas, Anna Maria e Maria Magdalena.[9] Depois do nascimento de Leonhard, sua família mudou da cidade de Basiléia para a cidade de Riehen, onde viveu a maior parte de sua infância. Paul Euler era amigo da família Bernoulli; Johann Bernoulli, que era então o matemático mais importante da Europa, foi a influência mais relevante da vida do jovem Leonhard.
Sua educação primeira foi dada por seu pai Paul que lhe ensinou matemática. Em 1720, aos treze anos, Euler ingressou na pequena Universidade de Basileia que possuía um famoso departamento de estudos da matemática liderado por Johann Bernoulli, irmão de Jacob Bernoulli. Johann inicialmente recusou-se a dar aulas particulares a Euler, oferecendo então um valioso conselho de como estudar por conta própria.[10]
Em 1722, recebe o grau de Mestre em Artes, e no seu exame deu um discurso em latim comparando as filosofias de Descartes e Newton.[11] Nesta altura, já recebia, aos sábados à tarde, lições de Johann Bernoulli,[12] que rapidamente descobriu o seu talento para a matemática.
Euler nesta altura estudava teologia, grego e hebraico, pela vontade de seu pai - para mais tarde se tornar pastor. Porém Johann Bernoulli resolveu intervir e convenceu Paul Euler que o seu filho estava destinado a ser um grande matemático.[13]
Em 1726, Euler completou a sua dissertação sobre propagação do som intitulada de De Sono.[14] Na época estava tentando, sem sucesso, obter um cargo na Universidade de Basileia. Em 1727 entrou pela primeira vez na competição premiada da Academia de Paris; o problema do ano era encontrar a melhor maneira de colocar os mastros num navio. Ganhou o segundo lugar, perdendo para Pierre Bouguer, mais tarde conhecido como “o pai da arquitetura naval”. Euler, entretanto, ganharia o prêmio anual doze vezes.[15]
Na época os dois filhos de Johann Bernoulli, Daniel e Nicolaus, foram trabalhar na Academia Russa de Ciências. No dia 10 de julho de 1726, Nicolaus morreu de apendicite após viver um ano na Rússia, e quando Daniel assumiu o cargo do irmão na divisão de matemática e física da universidade, ele indicou a vaga em fisiologia, que ocupava até então, para ser preenchida por seu amigo Euler. Em novembro de 1726 Euler aceitou ansiosamente a oferta, porém atrasou a viagem para São Petersburgo enquanto tentava uma vaga como professor de física na Universidade de Basileia.[16]
Leonhard chegou em São Petersburgo no dia 17 de maio de 1727. Foi promovido a assistente do departamento médico da academia para uma vaga no departamento de matemática. Foi apresentado por Daniel Bernoulli com quem frequentemente trabalhava em uma estreita parceria. Euler aprendeu russo e instalou-se em São Petersburgo. Também aceitou um trabalho adicional como médico na Marinha Russa.[17]
A Academia de S. Petersburgo, sob a política de Pedro I da Rússia, tinha intenção de melhorar a educação na Rússia e corrigir a defasagem no campo das ciências do país em relação à Europa Ocidental. Como resultado, a instituição criou um programa de internacionalização, com o objetivo de atrair estudantes estrangeiros como Euler. A instituição possuía vultosos recursos financeiros e uma biblioteca abrangente, planejada a partir das bibliotecas privadas da nobreza e do príncipe Pedro. Poucos estudantes foram inscritos na academia para diminuir a grade curricular e enfatizar a pesquisa, oferecendo para o corpo docente tempo e liberdade para prosseguir o questionamento científico.[15]
A benfeitora da Academia, Catarina I da Rússia, que tinha continuado a política progressiva da gestão anterior, morreu no dia em que Euler foi viajar. A aristocracia, em seguida, teve mais poder durante os dois anos de mandato do Pedro II. A nobreza, desconfiada dos cientistas estrangeiros da Academia, cortou-lhes o financiamento e causou dificuldades para Euler e seus colegas.
As condições melhoraram um pouco depois da morte de Pedro II, e Euler tornou-se professor de física em 1731, pela sua classificação no ranking da escola. Dois anos mais tarde, Daniel Bernoulli partiu para Basileia depois de ter sido perseguido com a censura e pela hostilidade que enfrentou em St. Petersburgo, e, assim, Euler o substituiu como professor de Matemática.[18]
No dia 7 de janeiro de 1734, Leonhard Euler casou com Katharina Gsell, filha de Georg Gsell, um pintor da Academia Gymnasium.[19] O jovem casal construiu uma casa perto do rio Neva. Tiveram treze filhos, dos quais apenas cinco sobreviveram à infância.[20]
Preocupado com a contínua turbulência na Rússia, Euler deixou São Petersburgo em 19 de Julho de 1741, para assumir uma posição em Berlim que tinha sido ofertada por Frederico II da Prússia. Viveu por vinte e cinco anos em Berlim, onde escreveu 380 artigos. Em Berlim publicou dois de seus mais renomados trabalhos: A Introductio in analysin infinitorum, um texto sobre funções matemáticas publicado em 1748 e o Institutiones calculi differentialis,[21] publicado em 1755 sobre cálculo diferencial.[22] No mesmo ano foi eleito um membro estrangeiro da Academia Real das Ciências da Suécia.
Entretanto, é convidado para ser tutor de Friederike Charlotte of Brandenburg-Schwedt, a Princesa de Anhalt-Dessau e sobrinha de Frederico II, o Grande. Euler escreveu mais de 200 cartas dirigidas à princesa, que mais tarde foram compiladas num volume best-selling intitulado Cartas de Euler sobre diferentes assuntos da Filosofia natural para uma Princesa Alemã.[23] Este trabalho incorpora exposições sobre vários assuntos pertencentes à física e matemática, dando também a conhecer as perspectivas religiosas e a própria personalidade do seu autor. Este livro veio a ser o mais lido de todas as suas obras matemáticas, e foi publicado dentro da Europa e nos Estados Unidos. A popularidade das "Cartas" atesta a capacidade de Euler para comunicar sobre assuntos científicos de maneira eficaz para um público leigo, uma rara habilidade para um cientista dedicado à pesquisa.[22]
Apesar da imensa e impressionante contribuição para a Academia de Berlim, provocou a ira de Frederico II, o que o forçou a abandonar Berlim. O rei da Prússia tinha um grande círculo social de intelectuais em sua corte e ele permaneceu um matemático sem sofisticações e informal, tanto em seu trabalho como na vida pessoal. Euler foi simples, religioso devoto que nunca questionou a existência de ordens ou crenças convencionais, em muitas situações, opositor direto de Voltaire – que tinha uma posição privilegiada na corte de Frederick. Euler não era um debatedor qualificado e muitas vezes fez dela um ponto para discutir assuntos sobre os quais ele sabia pouco, fazendo dele o alvo frequente de sagacidade de Voltaire.[22] Frederick também comentou decepcionado com Euler as habilidades práticas de engenharia:
Citação: Eu quero ter um jato de água em meu jardim: Euler calculava a força necessária das rodas para transportar a água para o reservatório, de onde deve voltar por meio de canais, até finalmente jorrar em Sanssouci. Meu moinho foi geometricamente construído e não poderia levantar um gole de água mais perto do que a quinze passos até o reservatório. A vaidade das vaidades! A vaidade da geometria![24]
A acuidade visual de Euler piorou ao longo de sua carreira matemática. Em 1738, três anos depois de sofrer uma febre quase fatal em 1735, tornou-se quase cego do olho direito, mas, ao invés de se lamentar, apresentou um trabalho meticuloso sobre cartografia para a Academia de São Petersburgo. A visão de Euler se agravou durante a sua estada na Alemanha, na medida em que Frederico II da Prússia se referia a ele como "Cyclops". Euler mais tarde desenvolveu uma catarata no olho esquerdo, deixando-o quase totalmente cego poucas semanas depois de sua descoberta em 1766. No entanto, sua condição parece ter pouco efeito sobre sua produtividade, compensando com suas habilidades de cálculo mental e de memória fotográfica. Por exemplo, Euler conseguia repetir a Eneida de Virgílio, do começo ao fim, sem hesitação. Com a ajuda de seus escribas, a produtividade de Euler em muitas áreas de estudo, na verdade, aumentou. Produziu, em média, um artigo matemático durante todas as semanas do ano 1775.[6]
Em 1760, com o alastramento da Guerra dos Sete Anos, a fazenda de Euler em Charlottenburg foi devastada pelo avanço das tropas russas. Como punição, o general Ivan Petrovich Saltykov indenizou pelos danos causados na propriedade de Euler, depois da tsarina Isabel da Rússia adicionar um pagamento de 4 000 rublos - um valor exorbitante na época.[26] A situação política russa se estabilizaria após a ascensão de Catarina, a Grande ao trono, quando em 1766, Euler aceita um convite para voltar a Academia de St. Petersburgo. Suas condições foram bastante exorbitantes – um salário anual de 3 000 rublos, uma pensão para a sua esposa e a promessa de cargos de alto escalão para os seus filhos. Todas destas condições foram atendidas. Viveu o resto de sua vida na Rússia. Contudo, sua segunda estadia no país foi marcada por uma tragédia: Um incêndio em Santo Petersburgo em 1771 destruiu a sua casa, e quase o matou. Em 1773, faleceu a sua esposa Katharina após 40 anos de casamento.
Três anos depois da morte de sua esposa, Euler casou com sua meia-irmã, Salome Abigail Gsell (1723–1794).[27] Este casamento durou até o fim de sua vida. Em 1782 foi eleito membro honorário estrangeiro da Academia de Artes e Ciências dos Estados Unidos.[28]
Em Santo Petersburgo no dia 18 de setembro de 1783, depois de um almoço com sua família, Leonhard estava discutindo sobre a descoberta de um novo planeta da época, chamado Urano e sua órbita com o também acadêmico Anders Johan Lexell, quando sucumbiu por causa de uma hemorragia cerebral. Morreu algumas horas depois.[29] Jacob von Staehlin-Storcksburg escreveu no obituário de Leonard para a Academia de Santo Petersburgo e o matemático russo Nicolaus Fuss, um de seus discípulos, escreveram uma detalhada eulogia,[30] em que ele encomendou uma assembleia em memória. Em sua eulogia para a Academia Francesa, o filósofo e matemático francês marquês de Condorcet, escreveu:
Citação: il cessa de calculer et de vivre—... ele terminou de calcular e de viver.[31]
Foi enterrado próximo de Katharina no cemitério luterano de Smolensk na Ilha de Vassiliev. Em 1785, a Academia de Ciências da Rússia pôs um busto de mármore de Leonhard Euler em um pedestal próximo à reitoria e, em 1837, esculpiram uma lápide para Euler. Para comemorar os duzentos e cinquenta anos do nascimento dele, a lápide foi transferida em 1956, junto com seus restos mortais, para a necrópole do século XVIII no mosteiro Alexander Nevsky, o cemitério Tikhvin.[32]
Euler trabalhou em quase todas as áreas da matemática: geometria, cálculo infinitesimal, trigonometria, álgebra e teoria dos números, bem como deu continuidade na física newtoniana, teoria lunar e outras áreas da física. É uma figura seminal na história da matemática, e suas obras, muitas das quais são de interesse fundamental, ocupam entre 60 e 80 volumes. O nome de Euler está associado a um grande número de temas. Euler é o único matemático que tem dois números em homenagem a ele: O número e, aproximadamente igual a 2,71828, e a constante de Euler-Mascheroni γ (gama) por vezes referida apenas como "constante de Euler", aproximadamente igual a 0,57721. Não se sabe se γ é racional ou irracional.[33]
Euler introduziu e popularizou várias convenções de notação matemática através de seus numerosos e amplamente divulgados livros didáticos. Mais notavelmente, introduziu o conceito de uma função, e foi o primeiro a escrever f(x) para denotar a função f aplicada ao argumento x. Também introduziu a notação moderna para as funções trigonométricas, a letra e para a base do logaritmo natural (agora também conhecido como número de Euler), a letra grega Σ (sigma maiúsculo) para somatórios e a letra i para representar a unidade imaginária.[34] O uso da letra grega π (pi) para designar a razão entre a circunferência de um círculo e o seu diâmetro também foi popularizado por Euler, embora não tenha se originado com ele.[35]
O desenvolvimento do cálculo infinitesimal estava na vanguarda da pesquisa matemática do século XVIII, e os amigos, de Euler, da Família Bernoulli - foram responsáveis ​​por grande parte do progresso inicial no campo. Graças à sua influência, estudar cálculo tornou-se o foco principal do trabalho de Euler. Embora algumas das provas de Euler não sejam aceitáveis ​​para os padrões modernos de rigor matemático[36] (em particular a sua dependência em relação ao princípio da generalidade da álgebra), suas ideias levaram a muitos grandes avanços. Euler é bem conhecido na análise pela sua utilização frequente e desenvolvimento da série de potência, a expressão de funções como somas de um número infinito de termos, tais como:
Notavelmente, Euler provou diretamente as expansões em séries de potência para  e e a função da tangente inversa. (Prova indireta através da técnica de séries de potência inversa foi dada por Newton e Leibniz. (Entre 1670 e 1680) Seu uso ousado da série de potência lhe permitiu resolver o famoso problema de Basileia em 1735 (ele forneceu um argumento mais elaborado em 1741):[36]
Euler introduziu o uso da função exponencial e logaritmo em provas analíticas. Descobriu maneiras de expressar diversas funções logarítmicas utilizando séries de potência, e conseguiu definir logaritmos para números negativos e complexos, ampliando consideravelmente o leque de aplicações matemáticas de logaritmos.[34] Também definiu a função exponencial para números complexos, e descobriu a sua relação com as funções trigonométricas.
Para qualquer número real φ (tida como radianos), a fórmula de Euler afirma que o complexo satisfaz a função exponencial.
Um caso especial da fórmula acima é conhecida como a identidade de Euler,
chamada de "a fórmula mais notável em matemática", por Richard Feynman,[37] por seus usos individuais das noções de adição, multiplicação, exponenciação, e igualdade, e os usos individuais da constantes 0, 1, e, i e π. Em 1988, os leitores da Mathematical Intelligencer votaram como sendo "a fórmula matemática mais bela de todos os tempos". No total, Euler foi responsável por três das cinco melhores fórmulas nessa enquete.[38]
Pafnuti Tchebychev escreveu: "Euler foi o início de todas as pesquisas que compõem a teoria geral dos números." A maioria dos matemáticos do século XVIII se dedica ao desenvolvimento de análise, mas Euler carregava a antiga paixão aritmética ao longo de sua vida. Por causa de seu interesse na teoria dos números, a mesma foi revivida até o final do século.
Euler continuou com suas pesquisas feitas anteriormente (sob influência de Diophantus), uma série de hipóteses separadas sobre os números naturais. Euler provou rigorosamente essas hipóteses, muito generalizadas, e as combinou em uma interessante teoria dos números. Introduziu na matemática "função de Euler" crítica e formulada com a ajuda do "teorema de Euler." Euler criou a teoria dos resíduos quadráticos e comparações, apontando para o último critério de Euler.
Euler Introduziu a função zeta de Riemann, uma generalização, que mais tarde recebeu o nome de Bernhard Riemann
 verdadeiro. Euler provocou a sua expansão:
onde o produto é feita sobre todos os primos, . Devido a isso, provou que a soma do inverso simples  diverge.[39]
A fórmula de De Moivre é uma conseqüência da fórmula de Euler. Além disso, Euler elaborou a teoria do nível superior das funções transcendentes pela introdução da função gama e introduziu um novo método de solução das funções quárticas. Também descobriu um meio de calcular integral com limites complexos, o prenúncio do desenvolvimento da análise complexa moderna. Ele, igualmente, inventou o cálculo das variações incluindo suas melhores soluções pela Equação de Euler-Lagrange.
Euler iniciou também o uso de métodos de análise para resolução dos problemas da teoria dos números.[40]
O interesse de Euler na teoria dos números pode ser atribuído à influência de Christian Goldbach, seu amigo na Academia de São Petersburgo. Muitos dos primeiros trabalhos de Euler na teoria dos números foram baseadas nas obras de Pierre de Fermat. Euler desenvolveu algumas das ideias de Fermat, e refutou algumas das suas conjeturas.
Euler ligou a natureza da distribuição privilegiada, com ideias de análise. Conseguiu provar que a soma dos recíprocos dos primos divergem. Ao fazer isso, descobriu a conexão entre a função zeta de Riemann e os números primos, o que é conhecido como a fórmula do produto Euler para a função zeta de Riemann.
Euler provou identidades de Newton, Pequeno teorema de Fermat, teorema de Fermat em somas de dois quadrados, e fez contribuições distintas ao Teorema de Fermat-Lagrange. Inventou também a função φ totiente (n). Usando as propriedades desta função, generalizou o teorema de Fermat ao que é hoje conhecido como o teorema de Euler. Contribuiu de forma significativa para a teoria dos números perfeitos, que havia fascinado os matemáticos desde Euclides. Euler também conjeturou a lei da reciprocidade quadrática. O conceito é considerado como um teorema fundamental da teoria dos números, e suas ideias pavimentaram o caminho para o trabalho de Carl Friedrich Gauss.[41]
Em 1736, Euler resolveu o problema conhecido como sete pontes de Königsberg. A cidade de Königsberg, Prússia, foi construída no rio Pregel, e incluiu duas grandes ilhas que estavam conectadas entre si e ao continente por sete pontes. O problema era o de decidir se é possível seguir um caminho que atravessa cada uma das pontes exatamente uma vez e retornar ao ponto de partida. Esta solução é considerada como sendo o primeira teorema da teoria dos grafos, especificamente da teoria gráfica planar.[42]
Euler também descobriu a fórmula V - E + F = 2 relacionando o número de vértices, arestas e faces de um poliedro convexo e, portanto, de um grafo planar. A constante nesta fórmula é agora conhecida como a característica de Euler para o gráfico (ou objeto de cálculo), e está relacionada ao gênero do objeto. O estudo e generalização desta fórmula foram, especificamente através de Augustin-Louis Cauchy, Simon Antoine Jean L'Huillier, estando na origem da topologia.[43][44]
Alguns dos maiores sucessos de Euler foram na resolução de problemas do mundo real analiticamente, e em descrever inúmeras aplicações do números de Bernoulli, série de Fourier, diagramas de Venn, os números de Euler, as constantes e e pi, frações contínuas e integrais. Integrou cálculo diferencial de Leibniz com o de Newton, e as ferramentas que tornaram mais fácil de aplicar o cálculo de problemas físicos desenvolvidos. Fez grandes progressos na melhoria da aproximação numérica de integrais, inventando o que hoje é conhecido como aproximações de Euler. As mais notáveis dessas aproximações são o método de Euler e a fórmula de Euler. Também facilitou o uso de equações diferenciais, em particular, a introdução da constante de Euler-Mascheroni.
Um dos interesses mais incomuns de Euler foi a aplicação de ideias matemáticas na música. Em 1739, escreveu o Tentamen novae theoriae musicae, na esperança de, eventualmente, incorporar a teoria musical como parte da matemática. Esta parte de seu trabalho, no entanto, não recebeu grande atenção e já foi descrita como muito matemática para músicos e demasiado musical para matemáticos.[45]
Euler ajudou a desenvolver o modelo de viga de Euler-Bernoulli, que se tornou um marco da engenharia. Além de aplicar com sucesso as suas ferramentas analíticas para problemas em mecânica clássica, Euler também aplicou essas técnicas para problemas celestes. Seu trabalho em astronomia foi reconhecido por uma série de prêmios da Paris Academy ao longo de sua carreira. Suas realizações incluem determinar com grande precisão as órbitas de cometas e outros corpos celestes, compreender a natureza dos cometas, e calcular a paralaxe do Sol. Seus cálculos também contribuíram para o desenvolvimento de tabelas de longitude precisas.[46]
Além disso, Euler fez importantes contribuições na óptica. Discordou da teoria corpuscular de Newton da luz nos Opticks, que era então a teoria prevalecente. Seus trabalhos sobre óptica em 1740 ajudaram a garantir que a teoria ondulatória da luz proposta por Christiaan Huygens se tornasse o modo dominante de pensamento, pelo menos até o desenvolvimento da teoria quântica da luz.[47]
Em 1757 publicou um importante conjunto de equações, que agora são conhecidas como as Equações de Euler.[48] Na forma diferencial, as equações são:
Onde:
Euler também é conhecido na engenharia pela fórmula de flambagem sob carga crítica de um suporte ideal, calculada a partir do seu comprimento e rigidez à flexão:[49]
Em 1765, em seu livro "A Teoria do movimento dos corpos sólidos", Euler matematicamente descreveu a cinemática de um corpo rígido de tamanho finito. Introduziu na matemática o teorema de Euler de ângulos de rotação. Seu nome também é usado na fórmula de cinemática da distribuição de velocidade em um sólido, conhecido como as equações (Euler - Poisson), dinâmica de corpo rígido, um dos três casos gerais integráveis ​​no problema da dinâmica de um corpo rígido com um ponto fixo.
Euler generalizou o princípio da mínima ação, um conjunto bastante confuso e apontou para a sua importância fundamental na mecânica. Infelizmente, não revelou a natureza do princípio variacional, mas, no entanto, atraiu a atenção de físicos, que mais tarde descobriram que o seu papel fundamental na natureza era válido.
Euler trabalhou no campo da mecânica celeste. Lançou as bases da teoria de perturbações, mais tarde completadas por Pierre Simon Laplace, e desenvolveu uma teoria muito precisa do movimento da Lua. Esta teoria provou ser adequada para resolver o problema urgente de determinar a longitude no mar.
Principais obras de Euler nesta área:
Euler estudou o campo gravitacional não só esférico, mas os corpos elipsoidais, o que representa um significativo passo em frente.[50]
Euler é também creditado por utilizar as curvas fechadas para ilustrar os argumentos do silogismo em 1768. Estes diagramas são conhecidos como diagramas de Euler.[51]
Um Diagrama de Euler é uma forma diagramática de representar conjuntos e suas relações. Este diagrama consiste de curvas fechadas simples (geralmente círculos) no plano que representam os conjuntos. Cada curva de Euler divide o plano dentro de duas regiões ou "zonas": o interior, simbologicamente representa os elementos do conjunto, e o exterior que representa todos os elementos que não são membros do conjunto. As partes das formas das curvas não são importantes: o significado do diagrama é em como eles se coincidem. As relações espaciais entre as fronteiras entre regiões de cada curva (sobrepõem, contenção ou nem um dos casos) correspondem às relações aos conjuntos teóricos (interseção, subconjunto e conjuntos disjuntos).[52] Desde então, eles têm também sido adotados por outros campos curriculares, como leitura.[53]
Porém Euler tinha caído em desgraça junto de Frederico II, que lhe chamava “ciclope” – numa referência ao seu defeito físico. Já desde 1735, Euler sofria de alguns problemas de saúde, como febres altas. Em 1738, perdeu a visão do olho direito, devido ao excesso de trabalho. Mas tal infelicidade não diminuiu em nada a sua produção Matemática.
Euler nunca teve problemas em produzir trabalhos de diferentes géneros, como por exemplo, material para livros-textos para as escolas russas. Geralmente escrevia em latim, mas também em francês, embora a sua língua de origem fosse o alemão. Tinha uma enorme facilidade para línguas, como bom suíço que era, o que lhe facilitava muito a vida nas diversas viagens que fazia, como era costume dos matemáticos do século XVIII. Em 1749, depois de 7 anos de trabalho e quase cem anos após a morte de Fermat, conseguiu provar a teoria de Fermat.
Em 1759, com a morte de Maupertuis (1698-1759), o lugar de diretor da Academia foi dado a Euler. Ao saber que outro cargo, o de presidente, tinha sido oferecido ao matemático d'Alembert, com quem tinha tido algumas divergências sobre questões científicas, Euler ficou bastante perturbado. Apesar de d'Alembert não ter aceite o cargo, Frederico continuou a implicar com Euler, que farto de tal situação, aceitou o convite feito por Catarina, a Grande de voltar para a Academia de S. Petersburgo.
O trabalho entre Euler e d'Alembert sempre convergiu no mesmo sentido. Os seus interesses eram quase os mesmos, apesar de ter havido alguma controvérsia entre eles sobre o problema das membranas vibrantes, em 1757, cuja solução da equação de Bessel, Euler conseguiu obter, o que ocasionou um afastamento. Mas, com a teoria dos números houve um grande apoio por parte de d’Alembert a Euler.
A contribuição de Euler para a teoria dos logaritmos não se restringiu à definição de expoentes, como usamos hoje. Trabalhou, também, no conceito de logaritmo de números negativos.
Enquanto se mantinha ocupado a pesquisar matemática em Berlim, d’Alembert pesquisava em Paris.
Em 1747, Euler escreveu a este matemático explicando corretamente a questão dos logaritmos dos números negativos. Mas ao contrário do que seria de se esperar, a fórmula formulada por Euler, válida para qualquer ângulo (em radianos), não foi compreendida por Bernoulli nem por d'Alembert pois, para estes, os logaritmos de números negativos eram reais, o que não é verdade já que se tratam de números imaginários puros.
Através da sua identidade – mais tarde conhecida como Igualdade de Euler – é possível observar que os logaritmos de números complexos, reais ou imaginários, também são números complexos. Usando as identidades de Euler é também possível expressar quantidades como sen(1 + i) ou cos(i), na forma usual para números complexos. Desta maneira, vê-se que ao efetuar operações transcendentes elementares sobre os números complexos, os resultados são números complexos.
Assim sendo, Euler foi capaz de demonstrar que o sistema de números complexos é fechado sob as operações transcendentes elementares, enquanto d’Alembert sugerira que o sistema de números complexos era algebricamente fechado.
Tanto Fermat como Euler sentiram-se bastante interessados pela teoria dos números. Embora não haja qualquer livro sobre este assunto, Euler escreveu cartas e artigos sobre vários aspetos desta teoria. Entre elas encontram-se as conjeturas apresentadas por Fermat, que foram derrubadas por Euler. Duas dessas conjeturas foram:
A primeira foi derrubada em 1732 com o auxílio do seu domínio em computação, evidenciando que 225 + 1 = 4 294 967 297 é fatorizável em 6 700 417 * 641. No entanto, no recurso a um contra-exemplo para deitar por terra a segunda conjetura, Euler também errou, apesar do erro só ter sido descoberto em 1966, dois séculos depois e com o auxílio de um computador.
Euler também realizou a demonstração de uma conjetura bastante conhecida, denominada como Pequeno teorema de Fermat. Tal demonstração foi apresentada numa publicação em 1736, denominada Commentarii.
Posteriormente, demonstrou uma afirmação mais geral do Pequeno teorema de Fermat, que veio a chamar-se Função de Euler. Mas, contrariando o que seria esperado, Euler não foi capaz de demonstrar o Último Teorema de Fermat, embora provasse a impossibilidade de soluções inteiras de xn + yn = zn para n = 3.
Em 1747, definiu mais 27 números amigáveis, que se juntaram aos três já conhecidos por Fermat. Mais tarde aumentou o número para 60. Euler também provou que todos os números perfeitos pares são da forma dada por Euclides, 2n-1(2n – 1), onde 2n – 1 é primo. Se existe ou não um número ímpar perfeito foi uma questão levantada por Euler e Goldbach, através de correspondência, ainda hoje sem resposta.
Euler foi destaque na sexta série de notas de 10 francos suíços e em numerosos selos suíços, alemães e russos. O asteroide 2002 Euler foi nomeado em sua honra. Também é comemorada pela Igreja Luterana em seu Calendário dos Santos em 24 de maio, ele era um devoto cristão (crente na infalibilidade bíblica), que escreveu apologéticas e argumentou energicamente contra os ateus proeminentes de seu tempo.[54]
Em 15 de abril de 2013, os 306 anos de Euler foi comemorado com um Google Doodle.[55]
Gottfried Wilhelm Leibniz (AFI: [ˈɡɔtfʁiːt ˈvɪlhɛlm ˈlaɪbnɪts],[2] Leipzig, 1 de julho de 1646 — Hanôver, 14 de novembro de 1716) foi um proeminente polímata e filósofo alemão e figura central na história da matemática e na história da filosofia. Sua realização mais notável foi conceber as ideias de cálculo diferencial e integral, independentemente dos desenvolvimentos contemporâneos de Isaac Newton.[3] Trabalhos matemáticos sempre favoreceram a notação de Leibniz como a expressão convencional do cálculo, enquanto a notação de Newton ficou sem uso. Foi apenas no século XX que a lei de continuidade e a lei transcendental da homogeneidade de Leibniz encontraram implementação matemática (por meio da análise não padronizada). Ele se tornou um dos inventores mais prolíficos no campo das calculadoras mecânicas, trabalhou para adicionar a multiplicação automática e a divisão à calculadora de Pascal, e foi o primeiro a descrever uma calculadora cata-vento em 1685,[4] além de inventar a roda de Leibniz, usada no aritmômetro - a primeira calculadora mecânica produzida em massa. Ele também refinou o sistema de números binários que se tornaria a base de todos os computadores digitais.
Em filosofia, Leibniz é mais conhecido por seu otimismo, por sua conclusão de que nosso universo é, num sentido restrito, o melhor de todos os mundos possíveis que Deus poderia ter criado. Essa ideia muitas vezes foi satirizada por outros filósofos, como Voltaire. Leibniz, juntamente com René Descartes e Baruch Spinoza, foi um dos três grandes defensores do racionalismo no século XVII. O trabalho de Leibniz antecipou a lógica moderna e a filosofia analítica, mas sua filosofia também remete à tradição escolástica, na qual as conclusões são produzidas aplicando-se a razão aos primeiros princípios ou definições anteriores, e não à evidências empíricas.
Leibniz fez importantes contribuições para a física e para a tecnologia e antecipou noções que surgiram muito mais tarde na filosofia, na teoria das probabilidades, na biologia, na medicina, na geologia, na psicologia, na linguística e na informática. Ele escreveu obras sobre filosofia, política, direito, ética, teologia, história e filologia. Leibniz também contribuiu para o campo da biblioteconomia. Enquanto servia como superintendente da biblioteca Wolfenbüttel na Alemanha, ele desenvolveu um sistema de catalogação que serviria de guia para muitas das maiores bibliotecas da Europa.[5] As contribuições de Leibniz para esta vasta gama de assuntos foram espalhadas em várias revistas científicas, em dezenas de milhares de cartas e em manuscritos inéditos. Ele escreveu em várias línguas, sobretudo em latim, francês e alemão. A obra completa de Leibniz ainda não foi traduzida para a língua inglesa ou portuguesa.[6]
Gottfried Wilhelm Leibniz nasceu em 1 de julho de 1646, filho de Catharina Schmuck e Friedrich Leibniz, um professor de filosofia moral em Leipzig que morreu em 1652, quando Leibniz tinha apenas seis anos. Em 1663 ingressou na Universidade de Leipzig, como estudante de Direito. Em 1666 obteve o grau de doutor em direito, em Nuremberg, pelo ensaio prenunciando uma das mais importantes doutrinas da posterior filosofia. Nessa época afilia-se à Sociedade Rosacruz, da qual seria secretário durante dois anos.
Foi o primeiro a perceber que a anatomia da lógica - “as leis do pensamento”- é assunto de análise combinatória. Em 1666 escreveu De Arte Combinatória, no qual formulou um modelo científico que é o precursor teórico de computação moderna: todo raciocínio, toda descoberta, verbal ou não, é redutível a uma combinação ordenada de elementos tais como números, palavras, sons ou cores.
Na visão que teve da existência de uma "característica universal", Leibniz encontrava-se dois séculos à frente da época, no que concerne à matemática e à lógica.
Aos 22 anos, foi-lhe recusado o grau de doutor, alegando-se juventude. Tinha vinte e seis anos, quando passou a ter aulas com Christiaan Huygens, cujos melhores trabalhos tratam da teoria ondulatória da luz. A maior parte dos papéis em que rascunhava suas ideias, nunca revisando, muito menos publicando, encontra-se na Biblioteca Real de Hanôver aguardando o paciente trabalho de estudantes. Leibniz criou uma máquina de calcular, superior à que fora criada por Blaise Pascal, fazendo as quatro operações.
Em Londres, compareceu a encontros da Royal Society, em que exibiu a máquina de calcular, sendo eleito membro estrangeiro da Sociedade antes de sua volta a Paris em março de 1673. Em 1676, já tinha desenvolvido algumas fórmulas elementares do cálculo e tinha descoberto o teorema fundamental do cálculo, que só foi publicado em 11 de julho de 1677, onze anos depois da descoberta não publicada de Newton. No período entre  1677 e 1704, o cálculo leibniziano foi desenvolvido como instrumento de real força e fácil aplicabilidade no continente, enquanto na Inglaterra, devido à relutância de Newton em dividir as descobertas matemáticas, o cálculo continuava uma curiosidade relativamente não procurada.
Durante toda a vida, paralelamente à Matemática, Leibniz trabalhou para aristocratas, buscando nas genealogias provas legais do direito ao título, tendo passado os últimos quarenta anos trabalhando exclusivamente para a família Brunswick, chegando a confirmar para os empregadores o direito a metade de todos os tronos da Europa. As pesquisas levaram-no pela Alemanha, Áustria e Itália de 1687 a 1690. Em 1700, Leibniz organizou a Academia de Ciências da Prússia, da qual foi o primeiro presidente. Esta Academia permaneceu como uma das três ou quatro principais do mundo até que os nazistas a eliminaram.
Morreu solitário e esquecido. O funeral foi acompanhado pelo secretário, única testemunha dos últimos dias. Encontra-se sepultado em Neustädter Hof- und Stadtkirche St. Johannis, Hanôver, Baixa Saxônia na Alemanha.
O pensamento filosófico de Leibniz parece fragmentado, porque seus escritos filosóficos consistem principalmente de uma infinidade de escritos curtos: artigos de periódicos, manuscritos publicados muito tempo depois de sua morte, e muitas cartas a muitos correspondentes. Ele escreveu apenas dois tratados filosóficos, dos quais apenas Teodiceia de 1710 foi publicado em sua vida.
Leibniz data o seu começo na historia da filosofia com seu Discurso sobre metafísica, que ele compôs em 1686 como um comentário sobre uma contínua disputa entre Malebranche e Antoine Arnauld. Isto levou a uma extensa e valiosa correspondência com Arnauld; o Discurso sobre metafísica não foi publicado até o século XIX. Em 1695, Leibniz fez sua entrada pública na filosofia europeia, com um artigo de jornal intitulado "Novo Sistema da Natureza e da comunicação das substâncias". Entre 1695 e 1705, compôs o seu Novos ensaios sobre o entendimento humano, um longo comentário sobre John Locke em seu Ensaios sobre o entendimento humano, mas ao saber da morte de Locke, 1704, perdeu o desejo de publicá-lo, Isto aconteceu até que os novos ensaios foram publicados em 1765. A Monadologia, composta em 1714 e publicado postumamente, é constituída por 90 aforismos.
Em 1676, Leibniz teria feito uma curta viagem a Londres, por consequência disso, há interpretações de que o referido erudito teria tido contato com obras não publicadas de Newton. Décadas depois, esse fato foi alegado como uma acusação leviana e infundada de que Leibniz teria aproveitado a concepção do cálculo de Newton, todavia, destaca-se que Leibniz ainda não teria na época do fato o domínio matemático, necessário, para entender determinado conceito. Ademais, cumpre destacar que Leibniz não publicou nada sobre o seu cálculo, até 1684; este assunto é tratado extensamente em um artigo de controvérsia entre Leibniz-Newton.
Neste sentido, evidencia-se que a Leibniz é creditado, juntamente com Isaac Newton, a descoberta do cálculo (cálculo diferencial e integral). De acordo com os cadernos de Leibniz, um avanço crítico ocorreu em 11 de novembro de 1675, quando ele empregou cálculo integral pela primeira vez para encontrar a área sob o gráfico de uma função y = f ( x ). Introduziu várias notações usadas até hoje, por exemplo, o sinal integral ∫, representando um S alongado, da palavra latina summa, e o d usado para diferenciais, a partir da palavra latina differentia. Esta notação inteligente para o cálculo é provavelmente o seu legado matemático mais duradouro.  Leibniz expressa a relação inversa de integração e diferenciação, mais tarde chamado de teorema fundamental do cálculo, por meio de uma figura em seu artigo de 1693 Supplementum Geometriae dimensoriae. No entanto, a James Gregory é creditado a descoberta do teorema em forma geométrica, Isaac Barrow provou uma versão geométrica mais generalizada e Newton apoiou em desenvolver a teoria. Desse modo, o conceito tornou-se mais transparente ao ser  desenvolvido através do formalismo de Leibniz e sua nova notação. Nesses termos, a regra de produto do cálculo diferencial ainda é chamada de "lei de Leibniz". Além disso, o teorema que diz como e quando diferenciar sob o sinal integral é chamado de regra integral de Leibniz.
Leibniz explorou infinitesimais no cálculo em desenvolvimento, manipulando-os de diversas maneiras e sugerindo que eles tinham propriedades algébricas paradoxais, consequentemente, George Berkeley, em um tratado chamado "O Analista" e também em De Motu, criticou estes. Um estudo recente argumenta que o cálculo Leibniziano estava livre de contradições e estava mais bem fundamentado do que as críticas empiristas de Berkeley.
O uso de infinitesimais em matemática foi desaprovada por seguidores de Karl Weierstrass, Mas sobreviveu em ciência e engenharia, e até mesmo em matemática rigorosa, através do dispositivo computacional fundamentais conhecido como o diferencial. A partir de 1960, Abraham Robinson elaborou uma base rigorosa aos infinitesimais de Leibniz, usando a teoria dos modelos, no contexto de um campo de números hiperrealistas. A análise não-padrão resultante pode ser vista como uma vindicação tardia do raciocínio matemático de Leibniz. O princípio de transferência de Robinson é uma implementação matemática de Leibniz.[7]
Da capital inglesa, Leibniz partiu para Hanôver, mas no percurso fez uma parada em Haia, onde conheceu  Leeuwenhoek, o descobridor dos micro-organismos e também passou vários dias em intensos debates com Baruch Espinoza, que recém completara sua principal obra, Ética.[8] Desde então, também foi acusado de se apropriar das ideias de Espinoza - tal acusação foi devido ao enorme apelo ideológico que estava inserido no panorama histórico da época. Embora Leibniz admirasse seu poderoso intelecto, ficou francamente desanimado com as conclusões dele,[9] especialmente por serem insuficientes à ortodoxia cristã.
Ao contrário de Descartes e Espinoza, Leibniz tinha uma formação universitária completa na área de filosofia. Sua carreira começou, ao longo de uma influência escolar e aristotélica traindo a forte influência de um de seus professores de Leipzig, Jakob Thomasius, que também supervisionou a sua tese de Licenciatura em Filosofia. Leibniz leu ansiosamente Francisco Suárez, jesuíta espanhol respeitado, mesmo em universidades Luteranas. Leibniz estava profundamente interessado em novos métodos e nas conclusões de Descartes, Huygens, Newton e Boyle, mas viu estes trabalhos através de uma lente fortemente matizada por noções escolásticas. No entanto, a verdade é que os métodos de Leibniz e suas preocupações, muitas vezes anteciparam a lógica e a analítica, assim como a filosofia da linguagem do século XX.
Há um relato de Leibniz em que ele declara que foi influenciado pela platonista Anne Conway em sua filosofia:[10]
A história da filosofia atribui a Anselmo de Cantuária, ainda no período da Renascimento do século XII, no âmbito das Escolas de Catedral, a primeira formulação de um Argumento ontológico. Tal Argumento pretende demonstrar a existência de Deus de maneira A priori, ou seja,  a partir da mera compreensão lógica do conceito de Deus,  deve-se concluir a necessidade de sua existência. Podemos entender o argumento ontológico como um salto do campo da lógica (conceito de Deus) para o campo da ontologia (existência de Deus). Ao longo da história da Filosofia moderna e Filosofia contemporânea, muitos pensadores dedicaram-se a formular definições sobre 'Deus' e produzir novas maneiras de demonstrar sua existência. Dentre muitos, destacam-se René Descartes, Alvin Plantinga, Kurt Gödel e, especialmente, Leibniz.
Definindo Deus como 'O ser mais perfeito'  - sendo a perfeição, segundo o pensador, a totalidade das qualidades ou atributos afirmativos -,  Leibniz pretende demonstrar a existência de tal ser a partir da compatibilidade entre dois ou mais atributos afirmativos, pois se todos os atributos afirmativos são compatíveis entre si sem gerar contradição, então  deve ser possível conceber um ser com todos estes atributos. Assim, necessariamente tal ser existe, pois se ele possui todos os atributos afirmativos, não lhe pode faltar a existência.[11]
Antes da demonstração é importante fazer algumas definições:
Podemos entender um atributo afirmativo não-analisável como um predicado (qualidade) necessário que não pode ser decomposto em outros predicados.
Podemos chamar dois predicados de Compatíveis quando tanto um predicado A quanto um predicado B podem atuar sobre um mesmo sujeito sem gerar contradição. 
Chamamos de Proposição Analítica uma proposição verdadeira em si própria em virtude do seu significado. (ex.: Todo casado é não-solteiro).  
Dados A e B como atributos afirmativos não-analisáveis, tomamos como hipótese que A e B são incompatíveis; dessa hipótese se segue necessariamente que A e B não podem estar em um mesmo sujeito - devido a própria definição de predicados compatíveis -, ou seja, da suposição da incompatibilidade de A e B,  podemos concluir a seguinte proposição: Se A então não-B ( A → ¬ B ) ( ou ainda: se é o caso que A, então não é o caso que B). 
Podemos categorizar, prima facie, tal proposição como analítica ou demonstrável, no entanto: 
Logo, não é possível demonstrar a hipótese da incompatibilidade de A e B, logo, A e B são compatíveis. Sendo assim, por exercício análogo, podemos provar a afirmatividade e a compatibilidade entre quaisquer outros atributos não-analíticos, logo, é possível conceber um ser com estes atributos. C.Q.D.[11]
Os escritos de Leibniz estão a ser discutidos até os dias de hoje, não apenas por suas antecipações e possíveis descobertas ainda não reconhecidas, mas como formas de avanço do conhecimento atual. Grande parte de seus escritos sobre a física está incluído nos Escritos Matemáticos de Gerhardt.
Leibniz teve grandes contribuições para a estática e a dinâmica emergentes sobre ele, muitas vezes em desacordo com Descartes e Newton. Ele desenvolveu uma nova teoria do movimento (dinâmicas) com base na energia cinética e energia potencial, que postulava o espaço como relativo, enquanto Newton sentira fortemente o espaço como algo absoluto. Um exemplo importante do pensamento maduro de Leibniz na questão da física é seu Specimen Dynamicum, de 1695.
Até a descoberta das partículas subatômicas e da mecânica quântica que os regem, muitas das ideias especulativas de Leibniz sobre aspectos da natureza não redutível a estática e dinâmica faziam pouco sentido. Por exemplo, ele antecipou Albert Einstein, argumentando, contra Newton, que o espaço, tempo e movimento são relativos, não absolutos. As regras de Leibniz são importantes, se muitas vezes esquecidas, provas em diversos campos da física. O princípio da razão suficiente tem sido invocado na cosmologia recente, e sua identidade dos indiscerníveis na mecânica quântica, um campo de algum crédito, mesmo com ele tendo antecipado em algum sentido. Aqueles que defendem a filosofia digital, uma direção recente em cosmologia, alegam Leibniz como precursor.
Confúcio (chinês: 孔子, pinyin: Kǒng Zǐ, Wade-Giles: K'ung-tzŭ, ou chinês: 孔夫子, pinyin: Kǒng Fūzǐ, Wade-Giles: K'ung-fu-tzŭ, literalmente "Mestre Kong"),[nota 1][1] nascido entre 552 a.C. e 489 a.C. foi um pensador e filósofo chinês do Período das Primaveras e Outonos.
A filosofia de Confúcio sublinhava uma moralidade pessoal e governamental, os procedimentos corretos nas relações sociais, a justiça e a sinceridade. Estes valores ganharam predominância na China em relação a outras doutrinas, como o legalismo (法家) e o taoismo (道家), durante a Dinastia Han[2][3][4] (206 a.C. – 220). Os pensamentos de Confúcio foram desenvolvidos num sistema filosófico conhecido por confucionismo (儒家).
Por nenhum texto ser comprovadamente de autoria de Confúcio e as ideias mais comumente atribuídas a ele terem sido redigidas durante o período entre a sua morte e a fundação do primeiro império chinês em 221 a.C., muitos acadêmicos são muito cautelosos em atribuir asserções específicas ao próprio Confúcio. Os seus ensinamentos podem ser encontrados na obra Analectos de Confúcio (論語), uma coleção de aforismos que foi compilada muitos anos após a sua morte. Por cerca de dois mil anos, pensou-se ter sido Confúcio o autor ou editor de todos os Cinco Clássicos (五經),[5][6] como o Clássico dos Ritos (禮記) (editor) e Os Anais de Primavera e Outono (春秋) (autor).
Os princípios de Confúcio tinham base nas tradições e crenças chinesas comuns. Favoreciam uma lealdade familiar forte, veneração dos ancestrais, respeito com os idosos e a família como a base para um governo ideal.
Confúcio, também conhecido como K'ung Ch'iu, K'ung Chung-ni ou Confucius,[7] nasceu em meados do século VI (551 a.C.), em Tsou, uma pequena cidade no estado de Lu, hoje Shantung. Segundo algumas fontes antigas, teria nascido em 27 de agosto de 551 a.C. (ou seja, no vigésimo primeiro ano do duque Hsiang).[8] Esse estado é denominado de "terra santa" pelos chineses. Confúcio estava longe de se originar de uma família abastada, embora seja dito que ele tinha ascendência aristocrática. Seu pai, Shu-Liang He, antes magistrado e guerreiro de certa fama, tinha setenta anos quando se casou com a mãe de Confúcio, uma jovem de quinze anos chamada Yen Cheng Tsai, que diziam ser descendente de Po Chi'in, o filho mais velho do Duque de Chou, cujo sobrenome era Chi.
Dos onze filhos, Confúcio era o mais novo. Seu pai morreu quando ele tinha três anos de idade, o que o obrigou a trabalhar desde muito jovem para ajudar no sustento da família. Aos quinze anos, resolveu dedicar suas energias em busca do aprendizado. Em vários estágios de sua vida, empregou suas habilidades como pastor, vaqueiro, funcionário público e guarda-livros. Aos dezenove anos, se casou com uma jovem chamada Chi-Kuan. Confúcio teve um filho chamado K'ung Li.
Aos 51 anos de idade, Confúcio obteve um posto oficial no estado de Lu. Mas, ele demitiu-se do cargo poucos anos depois, dizendo que não queria confundir-se com aqueles cujas ideias e conceitos de valor ele não podia compartilhar. Assim, ele começou a viajar por diversos reinos, pretendendo persuadir seus governadores a aceitar suas ideias políticas, mas não achou um lugar para realizar seus pensamentos e políticas. Viajando e conversando, atraiu muitos discípulos, impressionados com sua sabedoria e a elevação de seu caráter.
Confúcio viajou por diversos lugares, esteve em íntimo contato com o povo e pregou a necessidade de uma mudança total do sistema de governo por outro que se destinasse a assegurar o bem-estar dos súbditos, pondo, em prática, processos tão simples como a diminuição de contribuições o abrandamento das penalidades.
Suas ideias expandiram-se pelo país e por toda a China. Durante 14 anos, ele viajou por 7 reinos.
Ao completar 68 anos, Confúcio voltou a sua terra natal, Qufu, no estado de Lu, onde ensinava seus estudantes, coligia e ordenava os livros clássicos até sua morte em 479 a.C., aos 73 anos.
A sua ideologia de organização da sociedade procurava recuperar os valores antigos, perdidos pelos homens de sua época. No entanto, em sua busca pelo Tao, ele usava uma abordagem diferente da noção de desprendimento proposta pelos taoistas. A sua teoria baseava-se num critério mais realístico, onde a prática do comportamento ritual daria uma possibilidade real aos praticantes de sua doutrina de viverem em harmonia.
Confúcio não pregava a aceitação plena de um papel definido para os elementos da sociedade, mas sim que cada um cumprisse com seu dever de forma correta. Já o condicionamento dos hábitos serviria para temperar os espíritos e evitar os excessos. Logo, a sua doutrina apregoava a criação de uma sociedade capaz, culturalmente instruída e disposta ao bem-estar comum. A sua escola foi sistematizada nos seguintes princípios:
Cada um desses princípios ligar-se-ia às características que, para ele, se encontravam ausentes ou decadentes na sociedade.
Confúcio não procurou uma definição aprofundada sobre a natureza humana, mas parece ter acreditado sempre no valor da educação para a condicionar. Sua bibliografia consta de três livros básicos, sendo que os dois últimos são atribuídos aos seus discípulos:
Após sua morte, Confúcio recebeu o título de "Senhor Propagador da Cultura, Sábio Supremo e Grande Realizador" (大成至聖文宣王), nome que se encontra registado em seu túmulo.
Ao contrário de profetas de religiões monoteístas, Confúcio não pregava uma teologia que conduzisse a humanidade a uma redenção pessoal. Pregava uma filosofia que buscava a redenção do Estado mediante a correção do comportamento individual. Tratava-se de uma doutrina orientada para esse mundo, pregando um código de conduta social e não um caminho para a vida após a morte.[9]
Discípulos de Confúcio e seu único neto, Zisi, continuaram a sua escola filosófica após sua morte. Estes esforços espalharam os ideais de Confúcio para os estudantes, que, depois, se tornaram funcionários em muitas das cortes reais chinesas, dando, assim, ao confucionismo, o primeiro teste em grande escala de seus dogmas. Apesar de confiar fortemente no sistema ético-político de Confúcio, dois de seus mais famosos seguidores enfatizaram aspectos radicalmente diferentes de seus ensinamentos. Mêncio (século IV a.C.) articulou a bondade inata no ser humano como uma fonte das intuições éticas que guiam as pessoas para rén, yì, e lǐ, enquanto Xun Zi (século III) ressaltou os aspectos realista e materialista do pensamento de Confúcio, salientando que a moralidade é incutida na sociedade através da tradição e, nos indivíduos, através da formação.
Este realinhamento no pensamento de Confúcio foi paralelo ao desenvolvimento do legalismo, que viu a piedade filial como interesse e não como um instrumento útil para um governante criar um Estado eficiente. A divergência entre estas duas filosofias políticas veio à tona em 223 a.C., quando o estado de Qin conquistou toda a China. Li Ssu, o primeiro-ministro da Dinastia Qin, convenceu Qin Shi Huang a abandonar as recomendações confucionistas de distribuir feudos a parentes (o que correspondia a uma volta ao sistema anterior da Dinastia Zhou), que ele via como contrárias à ideia legalista de centralização do Estado em torno do governante. Quando os conselheiros de Confúcio defenderam sua posição, Li Ssu executou muitos estudiosos confucionistas e seus livros foram queimados, o que foi considerado um duro golpe para a filosofia e a sabedoria chinesas.
As ideias de Confúcio foram adotadas como filosofia oficial do Estado durante a Dinastia Han (206 a.C. - 220 d.C.)ː o conhecimento dessas ideias passou a ser uma das principais qualificações exigidas de funcionários públicos, que eram selecionados por meio de concorridos exames e que eram encarregados de manter a harmonia no Império.[9]
Karl Marx[a] RSA (Tréveris, 5 de maio de 1818 – Londres, 14 de março de 1883)[2] foi um filósofo, economista, historiador, sociólogo, teórico político, jornalista, e revolucionário socialista alemão. Nascido em Tréveris, Prússia, Marx estudou direito e filosofia nas universidades de Bona e Berlim. Casou-se com a crítica de teatro e ativista política alemã Jenny von Westphalen em 1843. Devido às suas publicações políticas, Marx tornou-se apátrida e viveu no exílio com a sua mulher e filhos em Londres durante décadas, onde continuou a desenvolver o seu pensamento em colaboração com o pensador alemão Friedrich Engels e a publicar os seus escritos, pesquisando na Sala de Leitura do Museu Britânico. Os seus títulos mais conhecidos são o panfleto Manifesto Comunista de 1848 e o triplo volume O Capital (1867–1883). O pensamento político e filosófico de Marx teve uma enorme influência na história intelectual, económica e política subsequente. O seu nome tem sido usado como adjetivo, substantivo e escola de teoria social.
As teorias críticas de Marx sobre sociedade, economia e política, entendidas coletivamente como marxismo, sustentam que as sociedades humanas se desenvolvem através da luta de classes. No modo capitalista de produção, isto manifesta-se no conflito entre as classes dirigentes (conhecidas como a burguesia) que controlam os meios de produção e as classes trabalhadoras (conhecidas como o proletariado) que permitem a existência destes meios através da venda da sua força de trabalho em troca de salários. Empregando uma abordagem crítica conhecida como materialismo histórico, Marx previu que o capitalismo produzia tensões internas como os sistemas socioeconómicos anteriores e que estas levariam à sua autodestruição e substituição por um novo sistema conhecido como o modo de produção socialista. Para Marx, os antagonismos de classe sob o capitalismo — em parte devido à sua instabilidade e natureza propensa a crises — iriam dar origem ao desenvolvimento da consciência de classe da classe trabalhadora, levando à sua conquista do poder político e, eventualmente, ao estabelecimento de uma sociedade comunista sem classes, constituída por uma livre associação de produtores.[3] Marx insistiu ativamente na sua implementação, argumentando que a classe trabalhadora deveria levar a cabo uma ação proletária revolucionária organizada para derrubar o capitalismo e provocar a emancipação socioeconómica.
Marx é descrito como uma das figuras mais influentes na história da humanidade, e o seu trabalho tem sido elogiado e criticado.[4] Muitos intelectuais, sindicatos, artistas e partidos políticos em todo o mundo foram influenciados pelo trabalho de Marx, com muitos a modificarem ou adaptarem as suas ideias. Marx é tipicamente citado como um dos principais arquitectos da ciência social moderna.[5][6][7]
Marx foi o terceiro de nove filhos,[8] de uma família de origem judaica de classe média da cidade de Tréveris, na época no Reino da Prússia. Sua mãe, Henriette Pressburg (1788–1863), era judia holandesa e seu pai, Herschel Marx (1777–1838), um advogado e conselheiro de Justiça. Herschel descende de uma família de rabinos, mas se converteu ao cristianismo luterano em função das restrições impostas à presença de membros de etnia judaica no serviço público, quando Marx ainda tinha 6 anos de idade.[9] Seus irmãos eram Sophie (1816–1886), Hermann (1819–1842), Henriette (1820–1845), Louise (1821–1893), Emilie (1824–1888 — adotada por seus pais), Caroline (1824–1847) e Eduard (1826–1837).[10]
Em 1830, Marx iniciou seus estudos no Liceu Friedrich Wilhelm, em Tréveris, ano em que eclodiram revoluções em diversos países europeus. Em 1835, Marx, com 17 anos, se prepara para deixar Trier e ingressar na universidade, num ensaio sobre a escolha de uma carreira, conclui:O principal guia que deve nos orientar na escolha de uma profissão é o bem-estar da humanidade e o nosso próprio aperfeiçoamento … a natureza humana é de tal modo constituída que o homem só atinge a própria perfeição trabalhando pelo aperfeiçoamento, pelo bem, de seus semelhantes … Se ele trabalha só para si mesmo, pode vir a ser um erudito famoso, um grande sábio, um excelente poeta, mas jamais será um homem perfeito, um grande homem de verdade …
Se escolhemos a posição na vida em que possamos trabalhar principalmente pela humanidade, nenhum fardo nos há de derrubar, pois serão sacrifícios pelo benefício de todos; de modo que não sentiremos uma alegria mesquinha, limitada e egoísta, mas nossa felicidade será a de milhões, nossas proezas viverão em silêncio mas eternamente atuantes, e sobre nossas cinzas serão derramadas fervorosas lágrimas de pessoas nobres.[11]Ingressou mais tarde na Universidade de Bonn para estudar Direito, transferindo-se no ano seguinte para a Universidade de Berlim, onde o filósofo alemão Georg Wilhelm Friedrich Hegel, cuja obra exerceu grande influência sobre Marx, foi professor e reitor.[9] Em Berlim, Marx ingressou no Clube dos Doutores, que era liderado pelo hegeliano de esquerda Bruno Bauer.[12] Ali perdeu interesse pelo Direito e se voltou para a Filosofia, tendo participado ativamente do movimento dos hegelianos de esquerda ou Jovens Hegelianos.[9] Seu pai faleceu naquele mesmo ano.[9] Em 1841, obteve o título de doutor em Filosofia com uma tese sobre as Diferenças da filosofia da natureza em Demócrito e Epicuro.[9] Impedido de seguir uma carreira acadêmica,[13] tornou-se, em 1842, redator-chefe da Gazeta Renana (Rheinische Zeitung), um jornal da província de Colônia.[14] Conheceu Friedrich Engels naquele mesmo ano, durante visitação deste à redação do jornal.[9]
Em 1843, a Gazeta Renana foi fechada após publicar uma série de ataques ao governo prussiano. Tendo perdido o seu emprego de redator-chefe, Marx mudou-se para Paris. Lá assumiu a direção da publicação Deutsch-Französische Jahrbücher ('Anais Franco-Alemães') e foi apresentado a diversas sociedades secretas de socialistas. Antes ainda da sua mudança para Paris, Marx casou-se, no dia 19 de junho de 1843, com Jenny von Westphalen,[15] a filha de um barão da Prússia com a qual mantinha noivado desde o início dos seus estudos universitários[16] (noivado que foi mantido em sigilo durante anos, pois as famílias Marx e Westphalen não concordavam com a união).
Do casamento de Marx com Jenny von Westphalen, nasceram sete filhos, mas devido às más condições de vida que foram forçados a viver em Londres, apenas três sobreviveram à idade adulta. As crianças eram: Jenny Caroline (1844–1883), Jenny Laura (1845–1911), Edgar (1847–1855), Henry Edward Guy ("Guido"; 18479–1850), Jenny Eveline Frances ("Franziska"; 1851–52), Jenny Julia Eleanor (1855–1898) e mais um que morreu antes de ser nomeado (Julho, 1857). Ao que consta, Franziska, Edgar e Guido morreram na infância, provavelmente pelas péssimas condições materiais a que a família estava submetida,[17] duas das filhas de Marx cometeram suicídio: Eleanor, 15 anos após a morte de Marx, aos 43 anos, após descobrir que seu companheiro havia se casado secretamente com uma atriz bem mais jovem, mas há quem suspeite que ele, na verdade, assassinou-a; e Laura, 28 anos após a morte de Marx, aos 66 anos, junto com o seu marido, Paul Lafargue, por não querer viver na velhice.[18]
Marx também teve um filho, Frederick Demuth (1851–1929),[19] nascido de sua relação amorosa com a militante socialista e empregada da família Marx, Helena Demuth. Solicitado por Marx, Engels assumiu a paternidade da criança, e pagando uma pensão, entregou-o a uma família de um bairro proletário de Londres.[20]
No tratamento pessoal — Leandro Konder ressalta — Marx foi produto de seu tempo: "Antes de poder contestar a sociedade capitalista Marx pertencia a ela, estava espiritualmente mais enraizado no solo da sua cultura do que admitiria, e que diante dos padrões da Inglaterra vitoriana mostrou: traços típicos das limitações de seu tempo". Como moças aristocráticas, suas filhas tinham aulas de piano, canto e desenho, mesmo que não tivessem desenvoltura para tais atividades artísticas.[20]
Também em 1843, Marx conheceu a Liga dos Justos (que mais tarde tornar-se-ia Liga dos Comunistas).[9] Em 1844, Friedrich Engels visitou Marx em Paris por alguns dias. A amizade e o trabalho conjunto entre ambos, que se iniciou nesse período, só seria interrompido com a morte de Marx.[16] Na mesma época, Marx também se encontrou com Proudhon, com quem teve discussões polêmicas e muitas divergências. E conheceu rapidamente Bakunin, então refugiado do czarismo russo e militante socialista. No seu período em Paris, Marx intensificou os seus estudos sobre economia política, os socialistas utópicos franceses e a história da França, produzindo reflexões que resultaram nos Manuscritos de Paris, mais conhecidos como Manuscritos Econômico-Filosóficos. De acordo com Engels, foi nesse período que Marx aderiu às ideias socialistas.[16]
De Paris, Marx ajudou a editar uma publicação de pequena circulação chamada Vorwärts!, que contestava o regime político alemão da época. Por conta disto, Marx foi expulso da França em 1845 a pedido do governo prussiano. Migrou então para Bruxelas, para onde Engels também viajou.[16] Entre outros escritos, a dupla redigiu na Bélgica o Manifesto comunista. Em 1848, Marx foi expulso de Bruxelas pelo governo belga. Junto com Engels, mudou-se para Colônia, onde fundam o jornal Nova Gazeta Renana.[9] Após ataques às autoridades locais publicados no jornal, Marx foi expulso de Colônia em 1849. Até 1848, Marx viveu confortavelmente com a renda oriunda de seus trabalhos, seu salário e presentes de amigos e aliados, além da herança legada por seu pai. Entretanto, em 1849 Marx e sua família enfrentaram grave crise financeira; após superarem dificuldades conseguiram chegar a Paris, mas o governo francês proibiu-os de fixar residência em seu território. Graças, então, a uma campanha de arrecadação de donativos promovida por Ferdinand Lassalle na Alemanha, Marx e família conseguem migrar para Londres, onde fixaram residência definitiva.[9] Trabalhou como correspondente em Londres para o New York Tribune[21] onde declarou seu apoio público o governo de Abraham Lincoln durante a Guerra da Secessão.[22][23]
Deprimido pela morte de sua esposa em dezembro de 1881, Marx desenvolveu, em consequência dos problemas de saúde que suportou ao longo de toda a vida, bronquite e pleurisia, que causaram seu falecimento em 1883. Foi enterrado na condição de apátrida,[24] no Cemitério de Highgate, em Londres.[9]
Muitos dos amigos mais próximos de Marx prestaram-lhe homenagem no seu funeral, incluindo Wilhelm Liebknecht e Friedrich Engels. Este pronunciou as seguintes palavras:[25]
Em 1954, o Partido Comunista Britânico construiu uma lápide com o busto de Marx sobre sua tumba, até então de decoração muito simples.[26] Na lápide, estão inscritos o parágrafo final do Manifesto Comunista ("Proletários de todos os países, uni-vos!") e um trecho extraído das Teses sobre Feuerbach: "Os filósofos apenas interpretaram o mundo de várias maneiras, enquanto que o objetivo é mudá-lo."
Algumas das principais leituras e estudos feitos por Marx são:[27]
Ele estudou profundamente todas essas concepções ao mesmo tempo em que as questionou e desenvolveu novos temas, de modo a produzir uma profunda reorientação no debate intelectual europeu.[27]
Hegel foi professor da Universidade de Jena, a mesma instituição onde Marx cursou o doutorado. E, em Berlim, Marx teve contato prolongado com as ideias dos Jovens Hegelianos (também chamados de "hegelianos de esquerda") e com o pensamento de Jean-Jacques Rousseau.[29] Os dois principais aspectos do sistema de Hegel que influenciaram Marx foram sua filosofia da história e sua concepção dialética.[30]
Para Hegel, nada no mundo é estático, tudo está em constante processo (vir-a-ser); tudo é histórico, portanto. O sujeito desse mundo em movimento é o Espírito do Mundo (também chamado de Superalma ou Consciência Absoluta), que representa a consciência humana geral, comum a todos indivíduos e manifesta na ideia de Deus. A historicidade é concebida enquanto história do progresso da consciência da liberdade. As formas concretas de organização social correspondem a imperativos ditados pela consciência humana, ou seja, a realidade é determinada pelas ideias dos homens, que concebem novas ideias de como deve ser a vida social em função do conflito entre as ideias de liberdade e as ideias de coerção ligadas a condição natural ("selvagem") do homem. O homem se liberta progressivamente de sua condição de existência natural através de um processo de "espiritualização" — reflexão filosófica (ao nível do pensamento, portanto) que conduz o homem a perceber quem é o real sujeito da história.[30][31]
Marx considerou-se um "hegeliano de esquerda" durante certo tempo, mas rompeu com o grupo e efetuou uma revisão bastante crítica dos conceitos de Hegel após tomar contato com as concepções de Ludwig Feuerbach.[b] Manteve o entendimento da história enquanto progressão dialética (ou seja, o mundo está em processo graças ao choque permanente entre os opostos; não é estático), mas eliminou o Espírito do Mundo enquanto sujeito ou essência, porque passou a compreender que a origem da realidade social não reside nas ideias, na consciência que os homens têm dela, mas sim na ação concreta (material, portanto) dos homens, portanto no trabalho humano. A existência material precede qualquer pensamento; inexiste possibilidade de pensamento sem existência concreta. Marx inverte, então, a dialética hegeliana, porque coloca a materialidade — e não as ideias — na gênese do movimento histórico que constitui o mundo. Elabora assim a dialética materialista, construída como uma crítica ao materialismo de Feuerbach[32] e um conceito não desenvolvido por Marx que também costuma ser chamado de materialismo dialético).[30][33]
A respeito da influência de Hegel sobre Marx, escreveu Lenin que "é completamente impossível entender O Capital de Marx, e, em especial, seu primeiro capítulo, sem haver estudado e compreendido a fundo toda a lógica de Hegel."[35]
À época de Marx, "socialismo utópico" designava um conjunto de doutrinas diversas (e até antagônicas entre si) que tinham em comum, entretanto, duas características básicas: (1) a base determinante do comportamento humano residia na esfera moral/ideologia e (2) o desenvolvimento das civilizações ocidentais estava a permitir uma nova era onde iria imperar a harmonia social.
Marx criticou sagazmente as ideias dos socialistas utópicos (principalmente dos franceses, com os quais mais polemizou), acusando-os de muito romantismo ingênuo e pouca ou nenhuma dedicação ao estudo rigoroso da conjuntura social, pois os socialistas utópicos muito diziam sobre como deveria ser a sociedade harmônica ideal, mas nada indicavam sobre como seria possível alcançá-la plenamente. Além de criticar o socialismo utópico, ele também criticou o socialismo pequeno burguês,[36] o "socialismo feudal" reacionário e o "socialismo conservador".[37] Por outro lado, pode-se dizer que, de certa forma, Marx adotou — explícita ou implicitamente — algumas noções contidas nas ideias de alguns dos socialistas utópicos, como a noção de que o aumento da capacidade de produção decorrente da revolução industrial permite condições materiais mais confortáveis à vida humana ou ainda a noção de que as crenças ideológicas do sujeito lhe determinam o comportamento. É importante destacar uma diferença primordial: para os socialistas utópicos em geral, todo o comportamento humano é absolutamente determinado pela moral/ideologia, já para Marx, essa afirmação é parcialmente verdadeira, pois a moral/ideologia encontra-se submetida a uma outra condição anterior que lhe determina — a dimensão material da reprodução da existência.[30]
Marx empreendeu um minucioso estudo de grande parte da teoria econômica ocidental, desde escritos da Grécia antiga até obras que lhe eram contemporâneas. As contribuições que julgou mais fecundas foram as elaboradas por dois economistas políticos britânicos: Adam Smith e David Ricardo (tendo predileção especial por Ricardo, a quem chamava de "o maior dos economistas clássicos"). Na obra deste último, Marx encontrou conceitos — então bastante utilizados no debate britânico — que, após fecunda revisão e reelaboração, adotou em definitivo, como os de valor, divisão social do trabalho, acumulação primitiva e mais-valia. A avaliação do grau de influência da obra de Ricardo sobre Marx é bastante desigual. Estudiosos pertencentes à tradição neorricardiana tendem a considerar que existem poucas diferenças cruciais entre o pensamento econômico de um e outro; já estudiosos ligados à tradição marxista tendem a delimitar diferenças fundamentais entre eles.[38][30][39] Apesar de Marx ter sido influenciado pelo utilitarismo radical de Jeremy Bentham na área econômica, ele admite que a sociedade possa dedicar parte de seu tempo a atividades não produtivas depois de que ela tenha atingido seus objetivos econômicos.[40]
Friedrich Engels exerceu significativa influência sobre as reflexões intelectuais de Marx, principalmente no início da associação entre ambos, período em que dirigiu a atenção de Marx para a economia política e a história econômica da Europa. Após a morte deste, Engels tornou-se não só o organizador dos muitos manuscritos incompletos e/ou inéditos legados, mas também o primeiro intérprete e sistematizador das ideias de Marx. Engels igualmente se ocupou, desde bem antes do falecimento de seu amigo, de redigir exposições em termos populares das ideias de Marx, visando facilitar sua difusão.[41]
A teoria marxista é, substancialmente, uma crítica radical das sociedades capitalistas, mas é uma crítica que não se limita a teoria em si: Marx se posiciona contra qualquer separação drástica entre teoria e prática, entre pensamento e realidade, porque essas dimensões são abstrações mentais (categorias analíticas) que, no plano concreto, real, integram uma mesma totalidade complexa.[42]
O marxismo constitui-se como a concepção materialista da História, longe de qualquer tipo de determinismo, mas compreendendo a predominância da materialidade sobre a ideia, sendo esta possível somente com o desenvolvimento daquela, e a compreensão das coisas em seu movimento, em sua interdeterminação, que é a dialética. Portanto, não é possível entender os conceitos marxianos — como forças produtivas ou capital — sem levar em conta o processo histórico, pois não são conceitos abstratos e sim uma abstração do real, tendo como pressuposto que o real é movimento.[43]
Karl Marx compreende o trabalho como atividade fundante da humanidade.[44][45] E o trabalho, sendo a centralidade da atividade humana, se desenvolve socialmente, sendo o homem um ser social. Sendo os homens seres sociais, a História, isto é, suas relações de produção e suas relações sociais fundam todo processo de formação da humanidade. Esta compreensão e concepção do homem é radicalmente revolucionária em todos os sentidos, pois é a partir dela que Marx irá identificar a alienação do trabalho como a alienação fundante das demais. E com esta base filosófica é que Marx compreende todas as demais ciências, tendo sua compreensão do real influenciado cada dia mais a ciência por sua consistência.[46]
Segundo Marx, Hegel e seus seguidores criaram uma dialética mistificada, que buscava explicar a história mundial a partir da economia[47] e como autodesenvolvimento da Ideia absoluta.
Já os economistas clássicos naturalizavam e desistoricizaram o modo de produção capitalista, concebendo a dominação de classe burguesa como uma ordem natural das relações econômicas, a partir de um conceito abstrato de indivíduo, homo economicus. Por isso, os economistas clássicos recorriam a "robsonadas", isto é, narrativas de trocas de produtos entre caçadores e pescadores primitivos, para ilustrar as suas teorias econômicas. Marx atribuía essa mistificação ao fetichismo da mercadoria, e não a uma intenção consciente.[48]
Em oposição aos filósofos idealistas e aos economistas clássicos, Marx propunha a investigação do desenvolvimento histórico das formas de produção e reprodução social, partindo do concreto para o abstrato e do abstrato para o concreto.[49]
Em razão da divisão social do trabalho e dos meios, a sociedade se extrema entre possuidores e os não detentores dos meios de produção. Surgem, então, a classe dominante e a classe dominada, sendo a classe dominante aquela que mantém poder sobre os meios de produção e a classe dominada a que se sujeita a dominante para obter os bens produzidos. O Estado aparece para representar os interesses da classe dominante[50] e cria, para isso, inúmeros aparatos para manter a estrutura da produção. Esses aparatos são nomeados por Marx de infraestrutura e condicionam o desenvolvimento de ideologias e normas reguladoras, sejam elas políticas, religiosas, culturais ou econômicas, para assegurar os interesses dos proprietários dos meios de produção.[51] Ele defendia a tributação pesada e progressiva de heranças em sua obra até a abolição do direito a herança.[52]
Para Marx a crítica da religião é o pressuposto de toda crítica social, pois crê que as concepções religiosas tendem a desresponsabilizar os homens pelas consequências de seus atos.[30] Marx tornou-se reconhecido como crítico sagaz da religião devido à sentença que profere em um escrito intitulado Crítica da filosofia do direito de Hegel: “A religião é o suspiro da criatura oprimida, o coração de um mundo sem coração, assim como é o espírito de uma situação carente de espírito. É o ópio do povo.”[53] Em verdade, Marx se ocupou muito pouco em criticar sistematicamente a atividade religiosa. Nesse quesito ele basicamente seguiu as opiniões de Ludwig Feuerbach, para quem a religião não expressa a vontade de nenhum Deus ou outro ser metafísico: é criada pela fabulação dos homens.[53]
Apesar de alguns leitores de Marx adjetivarem-no de “teórico da revolução”, inexiste em suas obras qualquer definição conceitual explícita e específica do termo "revolução".[54] O que Marx oferece são descrições e projeções históricas inspiradas nos estudos que fez acerca das revoluções francesa, inglesa e norte-americana.[30] Um exemplo de prognóstico histórico desse tipo encontra-se em Contribuição para a crítica da Economia Política:
Em geral, Marx considerava que toda revolução é necessariamente violenta, ainda que isso dependa, em maior ou menor grau, da constrição ou abertura do Estado. A necessidade de violência se justifica porque o Estado tenderia sempre a empregar a coerção para salvaguardar a manutenção da ordem sobre a qual repousa seu poder político, logo, a insurreição não tem outra possibilidade de se realizar senão atuando também violentamente. Diferente do apregoado pelos pensadores contratualistas, para Marx o poder político do Estado não emana de algum consenso geral, é antes o poder particular de uma classe particular que se afirma em detrimento das demais.[54] A revolução se daria no âmbito da necessidade de sobrevivência, pois segundo ele as forças produtivas em seu ápice passariam a se tornar destrutivas.[56]
Importante notar que Marx não entende revolução enquanto algo como reconstruir a sociedade a partir de um zero absoluto. Na Crítica ao Programa de Gotha, por exemplo, indica claramente que a instauração de um novo regime só é possível mediada pelas instituições do regime anterior. O novo é sempre gestado tendo o velho por ponto de partida.[54] A revolução proletária, que instauraria um novo regime sem classes, só obteria sucesso pleno após a conclusão de um período de transição que Marx denominou socialismo.[30]
Criticou o anarquismo por sua visão tida como ingênua do fim do Estado onde se objetiva acabar com o Estado "por decreto", ao invés de acabar com as condições sociais que fazem do Estado uma necessidade e realidade. Na obra Miséria da Filosofia, elabora suas críticas ao pensamento do anarquista Proudhon. Também criticou o blanquismo com sua visão elitista de partido, por ter uma tendência autoritária e superada. Posicionou-se a favor do liberalismo, não como solução para o proletariado, mas como premissa para maturação das forças produtivas (produtividade do trabalho) das condições positivas e negativas da emancipação proletária, como a da homogeneização da condição proletária internacional gerado pela "globalização" do capital. Sua visão política era profundamente marcada pelas condições que o desenvolvimento econômico ofereceria para a emancipação proletária, tanto em sentido negativo (desemprego), como em sentido positivo (em que o próprio capital centralizaria a economia, exemplo: multinacionais).[57]
Na lógica da concepção materialista da História, não é a realidade que move a si mesma, mas comove os atores, trata-se sempre de um "drama histórico" (termo que Marx usa em O 18 Brumário de Luís Bonaparte) e não de um "determinismo histórico" que cairia num materialismo mecânico (positivismo), oposto ao materialismo dialético de Marx, que poderia ser definido como uma "dialética realidade-idealidade evolutiva". Ou seja, as relações entre a realidade e as ideias se fundem na práxis, e a práxis é o grande fundamento do pensamento de Marx. Pois sendo a história uma produção humana, e sendo as ideias produto das circunstâncias em que tais ideais brotaram, fazer história racionalmente é a grande meta. É o próprio fazer da história que criará suas condições objetivas e subjetivas adjacentes, já que a objetividade histórica é produto da humanidade (dos homens associados, luta política, etc.). E, assim, Marx finaliza as Teses sobre Feuerbach, não se trata de interpretar diferentemente o mundo, mas de transformá-lo, pois a própria interpretação está condicionada ao mundo posto, só a ação revolucionária produz a transcendência do mundo vigente.[58]
O conceito de mais-valia foi empregado por Karl Marx para explicar a obtenção dos lucros no sistema capitalista. Para Marx, o trabalho gera a riqueza, portanto, a mais-valia seria o valor extra da mercadoria, a diferença entre o que o empregado produz e o que ele recebe. Os operários em determinada produção produzem bens (ex: 100 carros num mês). Se dividirmos o valor dos carros pelo trabalho realizado dos operários, teremos o valor do trabalho de cada operário. Entretanto os carros são vendidos por um preço maior: esta diferença é o lucro do proprietário da fábrica. A esta diferença, Marx chama de "valor excedente ou maior", ou mais-valia.[59] Segundo ele, o lucro teria uma tendência decrescente devido a necessidade de se investir na produção, à medida que a remuneração dos trabalhadores estaria submetida a mais-valia.[60]
A grande obra de Marx é O Capital, na qual trata de fazer uma extensa análise da sociedade capitalista. É predominantemente um livro de Economia Política, mas não só. Nesta obra monumental, Marx discorre desde a economia, até a sociedade, cultura, política e filosofia. É uma obra analítica, sintética, crítica, descritiva, científica, filosófica, etc. Uma obra de difícil leitura, ainda que suas categorias não tenham a ambiguidade especulativa própria da obra de Hegel, possui, no entanto, uma linguagem pouco atraente e nem um pouco fácil. Dentro da estrutura do pensamento de Marx, só uma obra como O Capital é o principal conhecimento, tanto para a humanidade em geral, quanto para o proletariado em particular, já que através de uma análise radical da realidade que está submetido, só assim poderá se desviar da ideologia dominante ("a ideologia dominante" é sempre da "classe dominante"), como poderá obter uma base concreta para sua luta política. Sobre o caráter da abordagem econômica das formações societárias humanas, afirmou Alphonse De Waelhens: "O marxismo é um esforço para ler, por trás da pseudoimediaticidade do mundo econômico reificado as relações inter-humanas que o edificaram e se dissimularam por trás de sua obra".[61] Cabe lembrar que O Capital é uma obra incompleta, tendo sido publicado apenas o primeiro volume com Marx vivo. Os demais volumes foram organizados por Engels e publicados posteriormente.[62]
Na obra A Ideologia Alemã, Marx apresenta os pressupostos de seu novo pensamento. No Manifesto Comunista, apresenta sua tese política básica, propondo a construção de uma nova sociedade, derrubando a burguesia através da luta contra a propriedade privada[63] de poucos.[64] No ensaio "Sobre a Questão Judaica", apresenta sua crítica à religião, dizendo que não se deve apresentar questões humanas como teológicas, mas as teológicas como questões humanas, e que afirmar ou negar a existência de Deus, são ambas teologia. Para ele, deve-se sempre ver as religiões como reflexões fantasiosas do ser humano acerca de si mesmo, mas que representam a condição real a qual está submetido o ser humano. Em Crítica ao Programa de Gotha, Marx faz sua mais extensa e sistemática apresentação do que seria uma sociedade socialista. Em A Guerra Civil na França, Marx supera todas as suas tendências jacobinas[65] de antes e defende claramente que só com o fim do Estado o proletariado oferece a si mesmo as condições de manter o próprio poder recém conquistado, e o fim do Estado é literalmente o "povo em armas", ou seja, o fim do "monopólio da violência" que o Estado representa. Em O 18 Brumário de Luís Bonaparte, além da profunda análise sobre o terror da "burocracia", outros aspectos marcantes são a questão do campesinato como aliado da classe operária na revolução iminente, o papel dos partidos políticos na vida social[66] e uma caracterização profunda da essência do bonapartismo. Karl Marx foi um dos poucos ideólogos que acompanharam todo o percurso de instabilidade política francesa pós-Revolução Francesa, revolução industrial e globalização[67] sendo que influenciou muito na obra do autor e contribuiu para alimentar os debates políticos dentro da esquerda.[68]
Durante a vida de Marx, suas ideias receberam pouca atenção de outros estudiosos. Talvez o maior interesse tenha se verificado na Rússia, onde, em 1872, foi publicada a primeira tradução do Tomo I de O Capital. Na Alemanha, a teoria de Marx foi ignorada durante bastante tempo, até que, em 1879, Adolph Wagner, um alemão estudioso da economia política, comentou o trabalho de Marx ao longo de uma obra intitulada Allgemeine oder theoretische Volkswirthschaftslehre. A partir de então, os escritos de Marx começaram a atrair cada vez mais atenção.[41]
Ao final do século XIX, o principal local de debate da teoria de Marx era o Partido Social-Democrata da Alemanha. Contudo, nos primeiros anos após sua morte, sua teoria obteve crescente influência intelectual e política sobre os movimentos operários e, em menor proporção, sobre os círculos acadêmicos ligados às ciências humanas — notadamente na Universidade de Viena e na Universidade de Roma, primeiras instituições acadêmicas a oferecerem cursos voltados para o estudo de Marx.[41]
Marx foi herdeiro da filosofia alemã, considerado ao lado de Kant, Nietzsche e Hegel um de seus grandes representantes. Foi um dos maiores pensadores de todos os tempos, tendo uma produção teórica com a extensão e densidade de um Aristóteles, de quem era um admirador.[69] Marx criticou ferozmente o sistema filosófico idealista de Hegel. Enquanto que, para Hegel, "da realidade se faz filosofia", para Marx, a filosofia precisa incidir sobre a realidade. Para transformar o mundo, é necessário vincular o pensamento à prática revolucionária, união conceitualizada como práxis: união entre teoria e prática.[70]
As ideias de Marx tiveram um profundo impacto na política mundial e pensamento intelectual.[71][72][73][74] Os seguidores de Marx vêm debatendo entre si sobre como interpretar seus escritos e aplicar seus conceitos para o mundo moderno. O legado do pensamento de Marx tornou-se objeto de contestação entre inúmeras tendências, cada uma se vendo como a intérprete mais precisa de Marx. Na esfera política, estas tendências incluem o leninismo, marxismo-leninismo, trotskismo, maoismo, luxemburguismo e o marxismo libertário. Várias correntes também se desenvolveram no marxismo acadêmico, muitas vezes sob influência de outros pontos de vista, resultando no marxismo estruturalista, marxismo histórico, fenomenológica marxista, marxismo analítico e marxismo hegeliano.[75]
Do ponto de vista acadêmico, a obra de Marx contribuiu para o nascimento da sociologia moderna. Ele tem sido citado como um dos três mestres da "escola cínica" do século XIX, ao lado de Friedrich Nietzsche e Sigmund Freud,[76] e como um dos três principais arquitetos da ciência social moderna juntamente com Émile Durkheim e Max Weber.[77] Em contraste com outros filósofos, Marx ofereceu teorias que, muitas vezes, poderiam ser testadas com o método científico.[71] Tanto Marx quanto Auguste Comte começaram a desenvolver ideologias cientificamente fundadas durante a secularização européia e novos desenvolvimentos na filosofia da história e ciência. Trabalhando na tradição hegeliana, Marx rejeitou o positivismo sociológico comtiano na tentativa de desenvolver uma ciência da sociedade.[78] Karl Löwith considerou Marx e Søren Kierkegaard os dois maiores sucessores filosóficos de Hegel.[79] Na teoria sociológica moderna, a sociologia marxista é reconhecida como uma das principais perspectivas clássicas. Isaiah Berlin considera Marx o verdadeiro fundador da sociologia moderna, "na medida em que qualquer um pode reivindicar o título".[80] Além da ciência social, ele também teve um legado duradouro na filosofia, na literatura, nas artes e nas humanidades.[81][82][83][84]
Na teoria social, pensadores do século XX e XXI adotaram duas estratégias principais em resposta a Marx: a primeira, conhecida como marxismo analítico, tende a reduzi-lo ao seu núcleo analítico, e precisa sacrificar suas ideias mais interessantes e intrigantes; a segunda, mais comum, dilui as reivindicações explicativas da teoria social de Marx e enfatiza a "autonomia relativa" dos aspectos da vida social e econômica, não diretamente relacionadas com a narrativa central de Marx: a interação entre o desenvolvimento das forças de produção e a sucessão dos modos de produção. Nesta segunda estratégia, incluem-se, por exemplo, a teorização neomarxista — adotada pelos historiadores inspirados na teoria social de Marx como E. P. Thompson e Eric Hobsbawm — e a linha de pensamento adotada por pensadores e ativistas como Antonio Gramsci, que têm procurado entender as oportunidades e as dificuldades da prática política transformadora vista à luz da teoria social marxista.[85][86][87][88] Gramsci desenvolveu o conceito de revolução passiva,[89] a qual é definida como "revolução sem revolução".[90]
Politicamente, o legado de Marx é mais complexo. Ao longo do século XX, ocorreram revoluções em dezenas de países que se autorotularam de "marxistas", mais notavelmente a Revolução Russa, que levou à fundação da URSS.[91] Líderes mundiais como Vladimir Lenin,[91] Mao Tsé-Tung,[92] Fidel Castro,[93] Salvador Allende,[94] Josip Tito[95] e Kwame Nkrumah[96] citaram Marx como uma influência, e suas ideias estão presentes em vários partidos políticos em todo o mundo, além daqueles onde ocorreram "revoluções marxistas".[97] As ditaduras brutais associadas com algumas nações marxistas levaram oponentes políticos a culpar Marx por milhões de mortes,[98] mas a fidelidade destes líderes, partidos e revoluções à obra de Marx é contestada e rejeitada por muitos marxistas.[99] Atualmente, é comum distinguir entre o legado e a influência de Marx especificamente, e o legado e influência de suas ideias para fins políticos.[100]
Alexandre III da Macedônia (português brasileiro) ou Macedónia (português europeu) (20/21 de julho de 356 a.C. – 10 de junho de 323 a.C.), comumente conhecido como Alexandre, o Grande ou Alexandre Magno (em grego clássico: Ἀλέξανδρος ὁ Μέγας; romaniz.: Aléxandros ho Mégas), foi rei (basileu) do reino grego antigo da Macedônia e um membro da dinastia argéada. Nascido em Pela em 356 a.C., o jovem príncipe sucedeu a seu pai, o rei Filipe II, no trono com vinte anos de idade. Ele passou a maior parte de seus anos no poder em uma série de campanhas militares sem precedentes através da Ásia e nordeste da África. Até os trinta anos havia criado um dos maiores impérios do mundo antigo, que se estendia da Grécia para o Egito e ao noroeste da Índia. Morreu invicto em batalhas e é considerado um dos comandantes militares mais bem-sucedidos da história.
Durante sua juventude, Alexandre foi orientado pelo filósofo Aristóteles até aos 16 anos. Depois que Filipe foi assassinado em 336 a.C., Alexandre sucedeu a seu pai no trono e herdou um reino forte e um exército experiente. Havia sido premiado com o generalato da Grécia e usou essa autoridade para lançar o projeto pan-helênico de seu pai liderando os gregos na conquista da Pérsia. Em 334 a.C., invadiu o Império Aquemênida, governando a Ásia Menor, e começou uma série de campanhas que durou dez anos. Quebrou o poder da Pérsia em uma série de batalhas decisivas, mais notavelmente as batalhas de Isso e Gaugamela. Em seguida, derrubou o rei persa Dario III e conquistou a Pérsia em sua totalidade. Nesse ponto, seu império se estendia do mar Adriático ao rio Indo.
Buscando alcançar os "confins do mundo e do Grande Mar Exterior", invadiu a Índia em 326 a.C., mas foi forçado a voltar pela demanda de suas tropas. Alexandre morreu na Babilônia em 323 a.C., a cidade que planejava estabelecer como sua capital, sem executar uma série de campanhas planejadas que teria começado com uma invasão da Arábia. Nos anos seguintes à sua morte, uma série de guerras civis rasgou seu império em pedaços, resultando em vários estados governados pelos diádocos, sobreviventes e herdeiros generais de Alexandre.
Seu legado inclui a difusão cultural que suas conquistas geraram, como o greco-budismo. Fundou cerca de vinte cidades que levavam o seu nome, principalmente Alexandria, no Egito. Seus assentamentos de colonos gregos e a propagação resultante da cultura grega no leste resultou em uma nova civilização helenística, aspectos que ainda eram evidentes nas tradições do Império Bizantino em meados do século XV e a presença de oradores gregos na região central e noroeste da Anatólia até à década de 1920. Alexandre tornou-se lendário como um herói clássico no molde de Aquiles, aparecendo com destaque na história e mito grego e culturas não gregas. Tornou-se a medida contra a qual os líderes militares se compararam, e academias militares em todo o mundo ainda ensinam suas táticas. É muitas vezes classificado entre as pessoas mais influentes do mundo em todos os tempos, junto com seu professor Aristóteles.
Alexandre nasceu na cidade de Pela, capital do Reino da Macedônia,[1] no sexto dia do mês hecatombeu do antigo calendário grego, o que provavelmente corresponde a 20 de julho de 356 a.C., apesar da data exata ainda não ser sabida com certeza.[2] Era filho do rei Filipe II e de sua quarta esposa, Olímpia, filha do rei Neoptólemo I do Epiro.[3][4] Apesar de Filipe ter sete ou oito esposas, Olímpia foi sua esposa principal por muito tempo, provavelmente devido ao fato dela ter sido aquela que lhe deu um filho homem.[5]
Muitas lendas envolvem o nascimento e infância de Alexandre.[6] De acordo com o biógrafo grego Plutarco, Olímpia, na noite da consumação do seu casamento com Filipe, sonhou que seu útero fora atingido por um raio. É dito que Filipe, em um sonho um tempo após o casamento, viu-se segurando o útero de sua esposa marcando-o com um selo gravado com uma imagem de leão.[7] Plutarco deu várias interpretações a este sonho: talvez que Olímpia estivesse grávida antes do casamento, indicado pelo selo gravado em seu útero; ou que Alexandre fosse filho do deus Zeus. Analistas antigos dizem que uma ambiciosa Olímpia pode ter propagado a história da origem divina de Alexandre ou talvez ela dispensasse essa sugestão como ímpia.[7]
No dia que Alexandre nasceu, Filipe estava preparando um cerco à cidade de Potideia, na península de Calcídica. No mesmo dia, Filipe recebeu notícias que o seu general Parménio tinha derrotado os exércitos combinados da Ilíria e da Peônia, e também que seu cavalo havia vencido uma competição nos Jogos Olímpicos. Também é dito que, neste dia, o Templo de Ártemis, em Éfeso, uma das sete maravilhas do mundo antigo, havia sido queimado. Isso levou Hegésias de Magnésia a dizer que o incêndio tinha ocorrido porque Ártemis estava longe, testemunhando o nascimento de Alexandre.[4] [8] Tais lendas podem ter surgido após Alexandre ter se tornado rei e possivelmente foram instigadas pelo próprio para mostrar que era um super-humano e destinado à grandeza desde sua concepção.[6]
Nos seus primeiros anos de vida Alexandre foi criado por uma enfermeira, Lanice, irmã do futuro general Clito. Mais adiante na sua infância, Alexandre foi tutorado pelo rígido Leônidas de Epiro, um parente de sua mãe, e por Lisímaco, um general de Filipe.[9] Alexandre foi criado como todos os jovens nobres macedônios, aprendendo a lutar, a ler, a tocar lira, a cavalgar e a caçar.[10]
Quando Alexandre tinha dez anos de idade um comerciante da Tessália trouxe um cavalo a Filipe, que procurou vender por treze talentos. O cavalo se recusava a ser montado e Filipe o dispensou. Alexandre, contudo, percebendo que o cavalo parecia ter medo da própria sombra, afirmou que poderia domar o animal, o que posteriormente conseguiu.[6] Plutarco afirmou que Filipe ficou exacerbado pela coragem e ambição do filho, o beijou firmemente e declarou: "Meu filho, você deve encontrar um reino grande o suficiente para a sua ambição. A Macedônia é pequena demais para você". Ele acabou comprando o cavalo para o garoto.[11] Alexandre deu ao animal o nome Bucéfalo, que significa "cabeça de boi". Bucéfalo tornou-se o cavalo principal de Alexandre, acompanhando-o até suas campanhas na Índia. Quando o animal morreu (devido à idade avançada, de acordo com Plutarco, aos 30 anos), nomeou uma cidade com seu nome, Bucéfala.[12][13][14]
Quando Alexandre tinha treze anos, Filipe começou a buscar um tutor para seu filho e considerou acadêmicos como Isócrates e Espeusipo, sendo que este último queria o cargo. No final, Filipe escolheu Aristóteles e lhe ofereceu o Templo das Ninfas em Mieza para ser usado como sala de aula. Em retorno por educar seu filho, Filipe concordou em reconstruir a cidade natal de Aristóteles, Estagira, que o próprio Filipe havia destruído. Ele a repovoaria, libertaria seus cidadãos que haviam sido escravizados e perdoaria os que estavam no exílio.[15][16][17]
Mieza era como um colégio interno para Alexandre e os filhos de outros nobres macedônios que o acompanharam, como Ptolemeu, Heféstio e Cassandro. Muitos destes outros estudantes acabaram se tornando amigos de Alexandre e mais tarde se tornariam generais em seu exército. Aristóteles ensinou a Alexandre e seus companheiros sobre medicina, filosofia, moral, religião, lógica e arte. Sob sua tutela, Alexandre desenvolveu muito interesse pelo autor Homero, em particular com a obra Ilíada; Aristóteles lhe deu uma cópia deste livro, que Alexandre levava em suas campanhas.[18][19][20]
Aos 16 anos de idade a educação de Alexandre sob Aristóteles acabou. Filipe então foi para a guerra contra Bizâncio, deixando Alexandre como regente do seu reino e herdeiro aparente.[6] Na ausência de Filipe, os medos trácios se revoltaram contra a Macedônia. Alexandre respondeu rápido, expulsando-os dos seus territórios. Ele recolonizou a região com gregos e fundou uma cidade chamada Alexandrópolis.[21][22][23]
Quando Filipe retornou, enviou Alexandre e uma pequena força de combate para subjugar uma revolta no sul da Trácia. Logo depois, durante uma campanha contra outros gregos na cidade de Perinto (atual Marmara Ereğlisi), Alexandre teria salvado a vida do seu pai. Enquanto isso, a cidade de Anfissa começou a trabalhar em terras que eram consagradas a Apolo, próximo de Delfos, um sacrilégio que deu a Filipe a oportunidade de mais uma vez interferir em assuntos gregos. Ainda ocupado na Trácia, ordenou a Alexandre que reunisse um exército para uma campanha no sul da Grécia. Preocupado que os estados gregos percebessem e interviessem, Alexandre fez parecer que se estava preparando para atacar a Ilíria. Nesse meio tempo, de fato, os ilírios invadiram a Macedônia, mas foram facilmente repelidos por Alexandre.[24]
Filipe e seu exército se reuniram com Alexandre em 338 a.C., e juntos marcharam para Termópilas, onde derrotaram uma pequena mas obstinada resistência de homens de Tebas. Eles depois avançaram e ocuparam Elateia. Alguns dias depois marcharam sobre Atenas e Tebas. Os atenienses, liderados por Demóstenes, decidiram se aliar aos tebanos contra a Macedônia. Embaixadores atenienses e de Filipe tentaram ganhar o favor de Tebas, mas preferiram ficar do lado de Atenas.[25][26][27] Filipe marchou então até Anfissa (ostensivamente agindo sobre o pedido do Anfictionia), capturando mercenários enviados por Demóstenes e aceitando a rendição desta cidade. Retornou então para Elateia, enviando uma oferta final de paz para Atenas e Tebas, mas foi rejeitado.[28][29][30]
Enquanto Filipe marchava rumo a sul, seus oponentes o bloquearam próximo a Queroneia, na Beócia. Na subsequente batalha de Queroneia, Filipe comandou a ala direita dos exércitos macedônios e Alexandre ficou no flanco esquerdo, acompanhado de alguns dos melhores generais do reino. De acordo com fontes antigas, a luta foi intensa. Filipe recuou propositadamente, forçando os hoplitas atenienses a segui-lo, abrindo assim uma brecha em suas linhas. Alexandre então quebrou a formação do exército de Tebas, seguido pelos generais de Filipe. Com a coesão do inimigo quebrada, Filipe ordenou que suas tropas avançassem. Com os atenienses perdidos, os tebanos foram cercados e derrotados.[31]
Depois da vitória em Queroneia, Filipe e Alexandre marcharam sem oposição pelo Peloponeso, sendo bem recebidos pelas cidades; contudo, quando se aproximaram de Esparta, foram recusados, mas decidiram não partir para a guerra.[32] Em Corinto, Filipe estabeleceu a "Aliança Helênica" (moldada igualmente como a aliança antiPérsia durante as Guerras Greco-Persas), que incluía quase todas as cidades-estado gregas, excluindo Esparta. Filipe foi então proclamado hegemon (que pode ser traduzido como "Comandante Supremo") da Liga (conhecida pelos historiadores modernos como a Liga de Corinto), e anunciou seus planos de invadir o Império Persa.[33][34]
Filipe se casou novamente quando retornou para Pela, desta vez com uma mulher chamada Cleópatra Eurídice, sobrinha do general Átalo.[35] O casamento fez da posição de Alexandre como herdeiro menos segura já que qualquer filho homem que Eurídice e Filipe tivessem seria um macedônio puro, enquanto Alexandre era apenas meio macedônio (sua mãe, Olímpia, era de Epiro).[36] Durante o banquete de casamento, Átalo ficou bêbado e começou a gritar pedindo aos deuses que aquela união produzisse um herdeiro legítimo.[35]
Alexandre fugiu da Macedônia junto com a mãe, deixando-a com seu irmão, o rei Alexandre I de Epiro, em Dodona, capital dos molossos. Ele continuou fugindo até a Ilíria, onde foi aceito como convidado pelo rei local, apesar de tê-lo derrotado em batalha anos antes. Contudo, Filipe nunca teve a intenção de deserdar o seu político e militarmente treinado filho.[38] Seis meses depois, com a mediação de Demarato, os dois fizeram as pazes e Alexandre retornou para casa.[39][40]
No ano seguinte, o sátrapa (governador) de Cária, em Pixodaro, ofereceu a mão de sua filha ao meio-irmão de Alexandre, Filipe Arrideu. Olímpia e vários amigos de Alexandre sugeriram então que isso mostrava que Filipe II iria fazer de Arrideu seu herdeiro. Alexandre reagiu enviando um ator, Téssalo, até Corinto, para dizer a Pixodaro que ele não deveria oferecer sua filha a um ilegítimo, mas deveria o fazer a Alexandre. Quando Filipe ouviu isso, parou as negociações e repreendeu Alexandre por querer se casar com a filha de Cária, afirmando que ele queria uma noiva melhor para ele.[38] Filipe exilou quatro amigos de Alexandre, Hárpalo, Nearco, Ptolemeu e Erígio.[36][41][42]
No verão de 336 a.C., enquanto estava em Egas num casamento da sua filha Cleópatra com o irmão de Olímpia, Alexandre I de Epiro, Filipe foi morto por Pausânias, o próprio capitão de sua guarda. Enquanto Pausânias tentava fugir, ele tropeçou e foi morto por seus perseguidores, incluindo dois companheiros de Alexandre, Pérdicas e Leonato.[43][44] Alexandre foi então proclamado rei pelos nobres macedônios e pelo exército. Tinha ele apenas vinte anos de idade.[45]
Agora Alexandre III, o novo rei começou seu governo eliminando potenciais rivais ao trono. Ele mandou executar seu primo, Amintas IV. Também ordenou a morte de dois príncipes macedônios de Lincéstida, mas poupou um terceiro, Alexandre de Lincéstida. Olímpia mandou queimar vivas Cleópatra Eurídice e sua filha com Filipe, a criança Europa. Quando Alexandre descobriu o que sua mãe fez, ficou furioso. Contudo, ele teve que mandar executar Átalo, tio de Eurídice,[46] que comandava a vanguarda do exército na Ásia Menor.[47]
Átalo, naquela altura, estava negociando com Demóstenes sobre a possibilidade de desertar para Atenas. Ele constantemente insultava Alexandre e depois da morte de Cleópatra, Alexandre deve ter considerado-o perigoso demais para viver.[47] O rei poupou Arrideu, que afirmavam ser mentalmente incapaz na época, possivelmente como resultado do envenenamento feito por Olímpia.[43][45] [48]
A notícia da morte de Filipe fez com que várias cidades gregas se revoltassem contra a Macedônia, incluindo Tebas, Atenas, Tessália e diversas tribos trácias ao norte da fronteira macedônia. Quando notícias das revoltas chegaram a Alexandre, ele respondeu rapidamente. Apesar de ser aconselhado a usar diplomacia, Alexandre reuniu 3 000 cavaleiros e marchou rumo a Tessália. Ele encontrou o exército tessálio em uma passagem entre o monte Olimpo e o monte Ossa, e ordenou que seus homens marchassem para o monte Ossa. Quando os tessalianos acordaram, encontraram Alexandre na sua retaguarda e decidiram se render, comprometendo suas forças ao rei. Ele continuou rumo ao sul, seguindo até o Peloponeso.[49][50][51]
Alexandre parou nas Termópilas, onde foi reconhecido como líder da Liga Anfictionia antes de seguir até Corinto. Atenas decidiu pedir a paz e Alexandre os perdoou. O famoso encontro entre Alexandre e Diógenes de Sinope ocorreu enquanto esses estavam em Corinto. Quando Alexandre perguntou a Diógenes o que ele poderia fazer por si, o filósofo pediu desdenhosamente a Alexandre que se afastasse um pouco, já que estava bloqueando a luz do sol. Alexandre gostou da resposta, e teria dito "mas, na verdade, se eu não fosse Alexandre, eu seria Diógenes."[52] Em Corinto, assim como seu pai, foi nomeado hegemon ("Líder Supremo") da Grécia para a luta contra a Pérsia. Enquanto estava lá recebeu notícias de uma nova rebelião na Trácia.[53][54]
Antes de partir para a Ásia para enfrentar os persas, Alexandre queria garantir a segurança de suas fronteiras no norte. Na primavera de 335 a.C., foi reprimir várias revoltas. Começando em Anfípolis, viajou para o leste para enfrentar os trácios e, no monte Hemo, o exército macedônio atacou e derrotou as forças trácias na região.[55] As tropas de Alexandre então se lançaram sobre Tribálios, derrotando os exércitos locais as margens do rio Ligino.[56] Alexandre então marchou por três dias sobre o Danúbio, encontrando tribos trácias de Getas. Ele não teve muita dificuldade em sobrepujá-las.[57][58]
Notícias então chegaram a Alexandre que Clito, então rei da Ilíria, e Gláucias, líder da Confederação dos Taulâncios, também estavam em revolta. Marchou então até a Ilíria, derrotando todas as forças inimigas no caminho e botando os rebeldes em retirada. Assim a fronteira norte estava segura.[59][60]
Enquanto Alexandre lutava no norte, os tebanos e atenienses mais uma vez se revoltaram. Alexandre marchou para o sul novamente. Outras cidades gregas decidiram hesitar, mas Tebas se precipitou em batalha. Sua resistência foi, contudo, ineficaz, e Alexandre destruiu a cidade e queimou todas as regiões vizinhas. Muitas pessoas morreram e outras milhares foram escravizadas. Atenas e outras cidades gregas, impressionadas e assustadas, buscaram a paz com a Macedônia.[61] Com a Grécia novamente firme sob seu controle, Alexandre voltou sua atenção para a Ásia. Ele deixou seu general Antípatro como regente.[62]
 (em português)A 334 a.C., o exército de Alexandre cruzou o Helesponto com aproximadamente 48 100 soldados de infantaria, 6 100 na cavalaria e uma frota de 120 navios com tripulação de 38 000 homens.[61] Estes combatentes eram, em sua maioria, macedônios, mas também tinham milhares de gregos de diversas cidades-estado, mercenários e tropas conseguidas da Trácia, Peônia e Ilíria.[64] Ele mostrou aos seus homens sua determinação de conquistar a Pérsia ao fincar sua lança em solo asiático e afirmar que aceitaria a Ásia como um presente dos deuses. Isso também mostrava sua vontade de lutar, ao contraste da preferência por diplomacia de seu pai.[61]
O primeiro grande confronto com os persas aconteceu na batalha do Grânico, a 24 de Daisios (8 de abril de 334 a.C.).[65] Alexandre derrotou seus adversários e aceitou a rendição de Sárdis, a capital da província local. Ele então prosseguiu pela costa de Jônia, garantindo a autonomia das cidades da região. A cidade de Mileto, principal foco de resistência persa, foi cercada e conquistada. Indo mais a sul, estava Halicarnasso, em Cária, onde um prolongado cerco foi feito. Alexandre forçou a rendição das tropas persas, capturando o líder mercenário local, forçando assim a fuga do sátrapa de Cária, Orontobates.[66] Alexandre deixou no poder na região uma membra da dinastia hecatômnia, Ada, que o adotou.[67]
De Halicarnasso, Alexandre foi até as montanhas da Lícia e as planícies de Panfília, assumindo o controle das cidades costeiras da Ásia Menor, negando aos persas o uso destas como base para sua marinha. De Panfília e da costa, Alexandre moveu-se terra adentro. Em Termesso, avançou sobre a cidade de Pisídia.[68] Na antiga cidade de Górdio, Alexandre "desfez" o até então insolúvel nó górdio, uma façanha que dizem esperar o futuro "rei da Ásia".[69] De acordo com a história, Alexandre disse que não importava como o nó era desfeito e apenas o destruiu com sua espada.[70]
Na primavera de 333 a.C., Alexandre cruzou de Tauro até à Cilícia. Após uma pausa devido a uma doença, marchou até a Síria. Dario III trouxe um novo exército, bem maior, e flanqueou os macedônios, forçando Alexandre a recuar de volta a Cilícia. Os dois se enfrentaram na Batalha de Isso, que resultou em uma importante vitória para Alexandre. Dario fugiu às pressas, levando ao colapso de suas forças, deixando para trás uma enorme quantidade de tesouros, sua esposa, suas duas filhas e sua mãe Sisigambis. O rei persa então propôs um tratado de paz que incluía a entrega aos macedônios de todos os territórios que eles já haviam conquistado e um resgate de 10 000 talentos por sua família. Alexandre respondeu que agora era o rei da Ásia e que apenas ele decidiria as divisões territoriais.[71]
Alexandre prosseguiu para conquistar a Síria e a costa da região do Levante.[67] No ano seguinte, precisamente a 332 a.C., cercou a cidade de Tiro (atualmente no Líbano), e após um prolongado e difícil sítio forçou a submissão da região.[72][73] Alexandre não mostrou piedade com a cidade, matando todos os homens em idade militar e vendendo as mulheres e crianças como escravos.[74]
Após esmagar a resistência persa em Tiro, a maioria das cidades na linha costeira até o Egito rendeu-se rapidamente. Uma história notória foi reportada quando os macedônios entraram em Jerusalém: de acordo com Josefo, foi mostrado a Alexandre uma profecia do Livro de Daniel, presumidamente no capítulo 8, que descrevia um poderoso rei grego que conquistaria o Império Persa. Ele poupou Jerusalém da destruição e avançou rumo ao Egito.[75] O avanço na região não foi calmo, com Alexandre enfrentando resistência por parte da cidade de Gaza. O local era fortificado e construído perto de montanhas. Os macedônios cercaram a cidade. Os defensores resistiram mas tiveram de ceder após sofrerem pesadas baixas.[76] Durante a batalha, Alexandre foi ferido. Assim como em Tiro, as forças de Alexandre massacraram incontáveis civis e venderam milhares de outros como escravos.[77]
Alexandre entrou no Egito ao fim de 332 a.C., onde foi saudado como libertador pela população local.[78] Ele foi proclamado como filho da divindade Amom pelo Oráculo de Siuá, em território que ficava no antigo deserto da Líbia.[79] Mais adiante, Alexandre passou a ser chamado de filho de Zeus-Amom e após sua morte continuou a ser tratado como uma divindade.[80] Durante sua estadia no Egito, fundou a cidade de Alexandria, que viria a ser um dos centros urbanos mais prósperos da antiguidade e capital do Egito Ptolemaico.[81]
Com o Egito sob seu controle, Alexandre partiu, em 331 a.C., em direção à Mesopotâmia (atual Iraque), o coração do Império Persa. Lá uma vez mais confrontou Dario na crucial batalha de Gaugamela. Novamente, mesmo em menor número, se saiu vitorioso e destruiu o exército inimigo.[82] Dario, assim como fez após outras derrotas sofridas diante de Alexandre, fugiu em desespero. A cidade da Babilônia, capital do império, abriu seus portões para os macedônios (para evitar ser destruída). Alexandre e seus homens adentraram nos seus muros e ocuparam os palácios de Dario.[83]
O rei persa havia fugido e Alexandre o perseguiu, indo até Arbela. Gaugamela acabou se tornando a batalha decisiva da campanha na Pérsia. O governo de Dario entrou em colapso e ele não conseguiu levantar um exército novamente. O antigo rei persa fugiu para Ecbátana (atual Hamadã).[83]
Após conquistar a Babilônia, Alexandre foi para a cidade de Susa, uma das capitais do Império Aquemênida (Pérsia), e capturou seus lendários tesouros.[83] Ele então enviou o grosso do seu exército até Persépolis, usando a Estrada Real Persa. O próprio Alexandre ficou na vanguarda, levando um grupo de soldados e atravessou os Portões Persas (nas cordilheira de Zagros), que eram defendidos por uma tropa comandada pelo sátrapa Ariobarzanes. Alexandre rapidamente superou estas defesas e avançou cidade a dentro em Persépolis, saqueando os seus tesouros.[84]
Em Persépolis, Alexandre deu permissão para que seus soldados saqueassem a cidade e tomassem espólios pessoais.[85] Alexandre ficou na cidade por cinco meses.[86] Durante sua estadia, um incêndio começou no palácio leste de Xerxes I que se espalhou pela cidade. Não se sabe se foi deliberado ou um acidente de um bêbado. Para alguns foi um ato de vingança pela queima da Acrópole de Atenas durante a Segunda Guerra Greco-Persa.[87]
Alexandre continuou sua perseguição implacável a Dario, indo até Medo e a Pártia.[88] Contudo, o rei persa já não controlava mais o seu destino, sendo feito prisioneiro pelo general Besso, que era o sátrapa de Báctria e um dos seus comandantes mais confiáveis.[89] Quando Alexandre se aproximou, Besso matou Dario e se proclamou seu sucessor, com o nome de Artaxerxes V, antes de recuar até a Ásia Central com o intuito de começar uma campanha de guerrilha contra Alexandre.[90]
Alexandre enterrou o corpo de Dario e lhe deu um funeral digno.[91] Ele afirmou que Dario, no seu leito de morte, o nomeou seu sucessor para o trono persa.[92] A morte de Dario é considerado o evento final do Império Aquemênida.[93]
Alexandre viu Besso como um usurpador e partiu em sua perseguição. Sua campanha, inicialmente apenas contra Besso, se tornou uma grande aventura pela Ásia Central. Alexandre sufocou qualquer resistência que via pela frente. No caminho, fundou cidades, chamando-as de Alexandria também, incluindo a moderna Candaar no Afeganistão e Alexandria Escate no Tajiquistão. A campanha levou Alexandre e seu exército até o extremo da região de Medo, Pártia, Ária (oeste do Afeganistão), Drangiana, Aracósia (sul afegão), Báctria e Cítia.[94]
Espitamenes, um senhor que governava uma região da Soguediana, traiu Besso em 329 a.C. e o entregou a Ptolemeu, um dos generais e amigos mais confiáveis de Alexandre. Besso foi então executado.[95] Contudo, enquanto Alexandre estava em Jaxartes repelindo uma invasão de um exército nômade, Espitamenes levantou Soguediana em revolta. Alexandre pessoalmente comandou uma tropa e derrotou os citas na Batalha de Jaxartes e depois se moveu contra Espitamenes, derrotando-o na batalha de Gabai. Então, os próprios comandados de Espitamenes o assassinaram e buscaram a paz com os macedônios logo em seguida.[96]
Durante a conquista final do Império Persa, Alexandre acabou adotando alguns elementos da cultura persa, como vestimentas e costumes na corte, mais notavelmente o prosquínese, que incluía o beijar de mãos ou a reverência, prostrando-se diante de alguém que é hierárquica e socialmente superior.[97] Os gregos aceitavam tais bajulações apenas a deidades e acreditavam que Alexandre queria se declarar ele mesmo um deus. Muitos dos seus compatriotas acabaram por criticá-lo e então ele acabou abandonando estas práticas.[98]
Por volta de 330 a.C., foi descoberto um complô contra Alexandre. Um dos seus oficiais, Filotas, foi executado por não avisar Alexandre de uma possível tentativa de assassinato. Filotas era filho do general Parménio, que estava em Ecbátana. Alexandre acabou por ordenar sua morte também. Em seguida ele ordenou a execução de Clito, um outro general, que era seu amigo e que havia salvado sua vida em Grânico. Os dois teriam brigado bêbados durante uma recepção em Maracanda (atual Samarcanda, no Uzbequistão). Clito teria acusado Alexandre de cometer diversos erros de julgamento e, especialmente, de ter esquecido o jeito macedônio em favor de um estilo de vida oriental corrupto.[99]
Mais tarde, durante uma campanha na Ásia central, um segundo complô contra Alexandre foi revelado, instigado por seus próprios pajens. Seu historiador oficial, Calístenes de Olinto, foi implicado no complô. Ele foi morto logo em seguida vítima de tortura sistemática ou doença.[100]
Quando Alexandre partiu para conquistar a Ásia, ele deixou o general Antípatro, um militar e político experiente e parte da "Velha Guarda" de Filipe II, no comando da Macedônia. A brutal destruição de Tebas garantiu que os gregos não se rebelariam em sua ausência. Não houve incidentes com a exceção de uma pequena revolta feita pelo rei espartano Ágis III em 331 a.C.. Antípatro o derrotou em batalha e o matou em Megalópolis.[62] Os espartanos foram posteriormente perdoados por sua traição.[101] Havia também muita tensão entre Antípatro e a mãe de Alexandre, Olímpia, com um reclamando ao rei a respeito do outro.[102]
Em geral, a Grécia ficou em paz durante boa parte do reinado de Alexandre e prosperou com os espólios da campanha na Ásia.[103] Alexandre enviava tesouros de volta para casa, estimulando a economia e o comércio pelo seu novo império, que agora ia desde as ilhas gregas até a região do Afeganistão na Ásia central.[104] Contudo, os constantes pedidos por tropas de Alexandre e a migração de macedônios para outras regiões conquistadas para o império acabaram por enfraquecer a própria Macedônia, que, décadas após a morte de Alexandre, não teve como resistir à invasão romana.[10]
Após a morte de Espitamenes e o seu casamento com Roxana, que teve o objetivo de sedimentar sua relação com as novas satrapias, Alexandre focou seu olhar no subcontinente indiano. Ele convidou vários chefes tribais da antiga satrapia de Gandara, no agora norte do Paquistão, para vir até ele e se submeter a sua autoridade. Onfis, o governador de Taxila, cujo reino ia do rio Indo até ao rio Jelum, concordou, mas alguns chefes das tribos das montanhas, incluindo os dos aspásios e assacenos, na região norte da Índia, se recusaram. Onfis colocou o seu reino e suas tropas a disposição de Alexandre e também entregou vários presentes. Alexandre devolveu o título de rei a Onfis e lhe presenteou com roupas da Pérsia, ouro, ornamentos de prata, 30 cavalos e 1 000 talentos de ouro. Alexandre dividiu suas forças, enviando Onfis para ajudar Heféstio e Pérdicas para reconstruir as pontes sobre o rio Indo, a fim de manter suas tropas na vanguarda supridas. Onfis então recebeu o rei macedônio em sua casa em Taxila.[105]
Nas campanhas seguintes dos macedônios, Taxiles enviou pelo menos 5 000 homens para apoiá-los. Esse apoio foi importante na sangrenta batalha do rio Hidaspes. A incursão contra o rei indiano Poro tinha como objetivo submeter parte da região de Utar Pradexe. Após a vitória em Hidaspes, Alexandre ordenou então que Onfis perseguisse Poro e quando este foi pego o rei macedônio lhe ofereceu termos favoráveis. Os dois líderes indianos permaneceram rivais e Alexandre teve que mediar as disputas entre eles. Taxiles continuou a ajudar os macedônios, dando-lhes suprimentos e equipamentos para a frota no rio Hidaspes, que em troca recebeu o governo de toda a região até o rio Indo. Quando Alexandre morreu (323 a.C.), Onfis reteve o seu poder e autoridade.[106]
No inverno de 327/326 a.C., Alexandre liderou várias campanhas contra diferentes tribos e clãs indianos; como os aspásios no vale de Cunar, os gureanos nas marges do rio Panjcora e os assacenos no vales de Suate e Buner.[107] Sangrentos confrontos foram travados com os aspásios. Mesmo com o próprio Alexandre sendo ferido em batalha, os aspásios foram derrotados. Alexandre partiu para enfrentar os assacenos, que lutaram para manter as cidades de Mássaga, Ora e Aornos.[105] O forte em Mássaga foi tomado após um curto mas violento combate, onde Alexandre foi novamente ferido (no joelho). De acordo com o historiador Cúrcio, "não só Alexandre massacrou toda a população de Mássaga, mas ele também destruiu todos os prédios".[108] Outro massacre aconteceu em Ora. Aornos (que havia recebido milhares de refugiados) foi o último foco de resistência na região. Alexandre sobrepujou os inimigos por lá também.[105]
Foi logo após conquistar Aornos, que Alexandre cruzou o rio Indo e lutou a dramática batalha de Hidaspes, em 326 a.C., contra o rei Poro (que governava a região de Panjabe).[109] Alexandre havia ficado impressionado com a coragem de Poro e o tornou um aliado. Ele o apontou como um sátrapa e até lhe deu mais território que ele outrora governava. Ter Poro, o rei mais importante da região, era crucial para ajudá-lo a controlar um lugar tão longe da sua base de poder na Macedônia.[110] Alexandre ainda teve tempo de fundar duas cidades de lados opostos do rio Jelum, nomeando uma delas de Bucéfala, em honra ao seu cavalo que morrera naquele período (de velhice).[111] A outra ficava em Niceia (Vitória), atualmente localizada perto da cidade de Mongue, no Panjabe.[112]
Ao leste do reino do rei Poro, próximo ao rio Ganges, estavam o Império Nanda de Mágada e mais a leste ainda estava os gangáridas (onde fica atualmente Bangladexe). Com medo do prospecto de invasões de exércitos do leste e exaustivas campanhas, várias unidades do exército de Alexandre se amotinaram nas proximidades do rio Beás, recusando-se a marchar mais para o leste. De fato este rio marcou a extensão máxima do Império de Alexandre Magno.[113]
Alexandre tentou persuadir os seus soldados a marchar com ele para o leste, mas seu general Ceno lhe implorou para que ele reconsiderasse e retornasse. Os homens, segundo ele, estavam querendo voltar para suas casas, ver seus pais, suas esposas, seus filhos e sua terra natal. Alexandre posteriormente concordou e marchou em direção ao sul, seguindo a margem do rio Indo. Ao longo do caminho ele enfrentou e derrotou uma força inimiga em Máli (atualmente chamada de Multan, no Paquistão) e ainda enfrentou algumas outras tribos indianas.[115]
Alexandre enviou então boa parte do seu exército a Carmânia (atualmente sul do Irã) com o general Crátero e enviou uma frota para explorar a região do Golfo Pérsico, enquanto o próprio Alexandre levou o que sobrou das tropas sob seu comando de volta a Pérsia tomando a difícil rota ao sul através do deserto de Gedrósia e Macrão.[116] Alexandre chegou em Susa em 324 a.C., mas havia perdido muitos soldados na travessia pelo deserto.[117]
Ao retornar do extremo oriente para a Pérsia, Alexandre ficou irritado ao saber que seus sátrapas e governadores militares haviam se comportado mal durante sua ausência. Ele então ordenou a execução de vários deles, para servirem de exemplo, enquanto ia até a cidade de Susa.[118][119] Como um gesto de gratidão, o rei pagou as dívidas dos seus soldados e anunciou que ele mandaria de volta à Macedônia os veteranos mais velhos ou deficientes, liderados por Crátero. Suas tropas duvidaram de suas intenções e se amotinaram na cidade de Ópis. Eles se recusaram a partir e criticaram sua adoção de costumes e vestimentas persas, e ainda a adição de soldados e oficiais persas no seu exército e em unidades macedônias.[120]
Após três dias, não capaz de persuadir seus homens a desistirem, Alexandre deu aos persas postos de comando no exército e conferiu a macedônios títulos militares nas unidades persas. Os soldados macedônios então pediram por perdão, que Alexandre aceitou, e então fez um grande banquete para milhares de seus homens, onde comeu junto com eles.[121] Em uma tentativa de criar mais harmonia entre seus súditos persas e macedônios, Alexandre fez casamentos em massa dos seus oficiais graduados e outros nobres em Susa, mas muitos destes casamentos não duraram muito.[119] Nesse meio tempo, Alexandre também descobriu que os guardas da tumba de Ciro II a haviam profanado e ordenou a execução deles.[122]
Depois que Alexandre viajou para Ecbátana para recuperar boa parte do grande tesouro persa, seu grande companheiro, Heféstio, morreu (de doença ou envenenamento).[123][124] Sua morte foi devastadora para Alexandre e ele ordenou uma cara e grandiosa pira funerária no meio da Babilônia para o amigo, além de ter decretado luto oficial.[123]
Uma vez na Babilônia, Alexandre começou a planejar uma série de novas campanhas militares. Ele pretendia invadir a Arábia e talvez lançar uma incursão na Europa ocidental, mas sua morte prematura impediu que todos os planejamentos fossem adiante.[125]
A 10 ou 11 de junho de 323 a.C., Alexandre morreu no antigo palácio do rei Nabucodonosor II, na Babilônia, aos 32 anos.[126] Existem duas versões a respeito de sua morte. De acordo com Plutarco, cerca de quatorze dias antes de falecer, Alexandre deu uma festa ao almirante Nearco e passou aquela noite e a próxima bebendo.[127] Ele teve então uma febre, que foi piorando até o ponto de não poder falar. Aos soldados comuns, ansiosos por causa da saúde do seu rei, foi permitido passar por ele silenciosamente e acenar.[128] A segunda versão, de Diodoro, afirma que Alexandre passou a sofrer de fortes dores após tomar uma enorme porção de vinho, em uma festa a Héracles. Permaneceu fraco por onze dias; não teve febre e morreu depois de dias de agonia.[129] Plutarco afirmou que esta última versão não seria verdade.[127]
Dada a propensão da aristocracia macedônia ao assassinato,[130] conspirações circulam sobre as histórias de sua morte. Diodoro, Plutarco, Arriano e Justino, todos mencionam a possibilidade de Alexandre ter sido envenenado. Justino afirma que houve uma grande conspiração para envenená-lo, mas Plutarco nega isso,[131] enquanto Diodoro e Arriano apenas mencionam essa possibilidade.[129][132] Relatos afirmam que Antípatro poderia ser o líder do complô, pois havia sido dispensado da posição de vice-rei da Macedônia e estava de briga com Olímpia, mãe de Alexandre. Talvez tenha assumido que o fato dele ter sido convocado para a Babilônia poderia ser uma sentença de morte e resolveu agir.[133] Antípatro teria então arquitetado o envenenamento com seu filho Iolas, que era o homem que servia os vinhos para Alexandre.[132][133] Há quem sugira que até Aristóteles tenha participado.[132]
Um argumento contra a teoria do envenenamento é que houve um espaço de doze dias entre o começo da doença e a morte; venenos que demorassem tanto para matar não estavam disponíveis na época.[134] Contudo, em 2003, o Dr. Leo Schep da The New Zealand National Poisons Centre propôs em um documentário da BBC que sua morte pode ter sido causada por flores brancas de heléboro (Veratrum album), que são usadas como veneno.[135][136] Em 2014, o Dr. Leo Schep publicou sua teoria no jornal médico Clinical Toxicology; em cujo artigo sugere que o vinho de Alexandre continha heléboro, uma planta conhecida pelos antigos gregos, que produzia sintomas similares aos que foram descritos no Romance de Alexandre. Envenenamento por heléboro demora e sugere-se que, se Alexandre realmente tenha sido envenenado, heléboro é a causa mais provável.[137][138] Outra explicação para o envenenamento foi divulgada em 2010, quando foi proposto que as circunstâncias da sua morte eram compatíveis com envenenamento pela água do rio Estige (Mavroneri) que contém caliqueamicina, um composto perigoso produzido por uma bactéria.[139]
Muitas causas naturais (doenças) foram sugeridas para a morte de Alexandre, incluindo malária e febre tifoide. Um artigo de 1998 da New England Journal of Medicine atribuiu sua morte a febre tifoide complicada por uma perfuração gastrointestinal e ascendente paralisia.[140] Outra análise recente indica espondilite piogênica ou meningite.[141] Os sintomas também são similares a outras doenças, incluindo pancreatite aguda e febre do Nilo Ocidental.[142][143] Muitos dizem que a saúde geral de Alexandre havia declinado devido a anos de bebedeiras e feridas pelo corpo devido às batalhas. A agonia que Alexandre sentiu após perder seu grande amigo Heféstio, também lhe pode ter feito mal, segundo alguns.[140]
O corpo de Alexandre foi posto em um sarcófago antropoide de ouro que foi enchido com mel, o qual foi colocado em um caixão de ouro.[144][145] De acordo com Eliano, um vidente chamado Aristandro teve uma visão da terra onde os restos de Alexandre deveriam descansar onde seria "feliz e invencível para sempre".[146] Talvez, mais provavelmente, os sucessores podem ter visto que o local de enterro de Alexandre serviria como um símbolo de legitimidade, já que enterrar o rei que o antecedeu era uma prerrogativa real.[147]
Enquanto o cortejo fúnebre de Alexandre ia até a Macedônia, Ptolomeu o pegou e levou o corpo temporariamente até Mênfis.[144][146] Seu sucessor, Ptolemeu II, transferiu o sarcófago para Alexandria, onde permaneceu até o fim do período conhecido como Antiguidade Tardia. Ptolemeu IX, um dos últimos sucessores de Ptolomeu Sóter, substituiu o sarcófago de Alexandre com um de vidro para que ele pudesse converter o antigo em dinheiro.[148] A recente descoberta de uma grande tumba no norte da Grécia, em Anfípolis, que data do tempo de Alexandre, pode significar que os macedônios tinham intenções de enterrá-lo mesmo em solo grego. Isso é plausível devido ao eventual destino da caravana do cortejo fúnebre de Alexandre.[149]
Pompeu, Júlio César e Augusto visitaram a tumba de Alexandre Magno na cidade de Alexandria. Foi dito que Calígula teria tirado a armadura peitoral usada por Alexandre para seu próprio uso. O também imperador romano Septímio Severo fechou a tumba de Alexandre para visitação pública. Seu filho e sucessor, Caracala, um grande admirador, também visitou sua tumba durante o seu reinado. Após isso, a história do sarcófago de Alexandre ficou nebulosa.[148]
O chamado "Sarcófago de Alexandre", descoberto próximo de Sidom (no Líbano) e agora em amostra no Museus Arqueológicos de Istambul, é chamado assim não necessariamente por suspeitas de ter os restos mortais de Alexandre, mas por causa dos baixos-relevos que mostram Alexandre e seus companheiros lutando contra Persas e caçando. Inicialmente acreditava-se que o sarcófago era na verdade de Abdalônimo (morto em 311 a.C.), o rei de Sidom nomeado por Alexandre imediatamente após a Batalha de Isso, em 331 a.C..[150] Contudo, mais recentemente, esta informação foi desacreditada.[151]
A morte de Alexandre foi tão repentina que quando a notícia chegou na Grécia, muitos não acreditaram.[62] Alexandre não tinha um herdeiro legítimo imediato, já que sua esposa, Roxana, estava apenas grávida no período da sua morte. A criança, Alexandre IV, nasceu após o seu falecimento e veio também a falecer oito anos depois.[152] De acordo com Diodoro, os companheiros de Alexandre perguntaram, no seu leito de morte, para quem ele deixaria o seu império gigantesco; sua resposta lacônica foi tôi kratistôi ("para o mais forte").[129]
Arriano e Plutarco dizem que Alexandre não tinha condições de falar, implicando que a história do "para o mais forte" é apócrifa.[153] Diodoro, Cúrcio e Justino oferecem um fim mais plausível, com Alexandre passando seu anel de sinete para Pérdicas, seu guarda-costas e líder de sua cavalaria pessoal, em frente a testemunhas, o que teoricamente o teria feito seu sucessor.[129][152] Pérdicas não clamou pelo poder inicialmente, sugerindo que o filho de Alexandre com Roxana deveria ser o rei, com ele próprio, Crátero, Leonato e Antípatro como guardiões. Contudo, a infantaria macedônia, sob comando do general Meleagro, rejeitou esta ideia pois eles não teriam um papel a cumprir neste cenário. Em vez disso, eles apoiaram o meio irmão de Alexandre, Filipe Arrideu. Eventualmente, os dois lados se reconciliaram e depois do nascimento de Alexandre IV, ele e Filipe III foram nomeados como reis conjuntos, ainda que apenas no nome.[154]
Dissensão e rivalidade afligiram os macedônios, contudo. As satrapias entregues por Pérdicas na Partição da Babilônia tornaram-se bases de poder de cada general para tentar conseguir mais poder. Após o assassinato de Pérdicas em 321 a.C., a unidade macedônica foi quebrada e seguiram-se quarenta anos de guerra entre "Os Sucessores" (diádocos) até que o mundo helênico alcançou certa estabilidade com uma divisão formal e prática: os Ptolemeus no Egito, os Selêucidas na Mesopotâmia e Ásia Central, os Atálidas na Anatólia e os Antígonos na Macedônia. Nesse meio tempo, tanto Alexandre IV e Filipe III foram assassinados.[155]
Diodoro afirmou que Alexandre deixou instruções em escrito para Crátero algum tempo antes da sua morte.[156] Crátero começou a executar alguns de seus comandos, mas os sucessores do seu império decidiram parar, afirmando que alguns dos pedidos eram impraticáveis e extravagantes.[156] Ainda assim, Pérdicas leu o testamento de Alexandre para as suas tropas.[62]
O texto do testamento dele pedia mais expansão territorial do império, indo para o sul e oeste do mediterrâneo, construção de monumentos e a união das populações do ocidente e do oriente. Também tinha:
Alexandre passou a ser chamado de "o Grande" (Mégas Aléxandros) devido ao seu sucesso sem paralelo como comandante militar. Ele nunca perdeu uma batalha, apesar de quase sempre estar em menor número.[61] Conhecido por usar muito bem o terreno, sua infantaria pesada (as falanges) e táticas de cavalaria, contava com a obediência de suas tropas em suas táticas ousadas. A falange macedônica, armada com longas sarissas (de até seis metros), havia sido aperfeiçoada por seu pai, Filipe II, através de rigoroso treinamento, e Alexandre usou sua força, velocidade e manobrabilidade com grande efeito contra forças inimigas maiores, como a dos persas. Alexandre também conhecia o potencial de desunidade de exércitos diversificados, que continham diferentes línguas e armas. Ele era conhecido por participar pessoalmente das batalhas na linha de frente, à maneira dos reis macedônios.[158][159]
Na sua primeira grande batalha na Ásia, em Grânico, Alexandre usou uma pequena parte das suas forças, aproximadamente 13 000 soldados de infantaria e 5 000 de cavalaria, contra uma força persa de 40 000 homens. Ele colocou as falanges no centro e a cavalaria e os arqueiros nos flancos, para igualar o tamanho das linhas persas, de aproximadamente 3 km. Em contraste, a infantaria persa ficava estacionada atrás de sua cavalaria. Isso garantiu que Alexandre não fosse flanqueado, enquanto sua falange tinha uma clara vantagem sobre as cimitarras e lanças curtas persas. As perdas macedônias foram muito pequenas, comparada com as persas.[160]
Na Batalha de Isso, em 333 a.C., seu primeiro confronto direto com Dario, dispôs suas forças da mesma maneira de Grânico e novamente ordenou que sua falange central avançasse na vanguarda.[160] Alexandre pessoalmente comandou o ataque da infantaria, colocando em retirada o inimigo.[158] Na batalha decisiva em Gaugamela, Dario dispôs várias bigas para quebrar as linhas das falanges. Alexandre dispôs suas tropas em linhas, com o centro avançando em um ângulo mais para frente, o que quebrou a coesão do ataque inicial das bigas, obrigando-as a saírem de formação. Assim, o centro das linhas de Dario foram quebradas e ele novamente teve que fugir para salvar a própria vida.[160]
Quando enfrentou inimigos cujas táticas eram desconhecidas a ele, como na Ásia Central e na Índia, Alexandre rapidamente se adaptava ao novo cenário adverso e empregava novas táticas. Assim, em Báctria e Sogdiana, Alexandre usou lanceiros e arqueiros para impedir que o inimigo afobasse seus flancos, enquanto concentrava sua cavalaria no centro.[158] Na Índia, quando confrontou o rei Poros e seus elefantes de guerra, os macedônios abriram suas linhas para envolver os elefantes e usavam suas sarissas para atacar os animais e os seus condutores.[121]
Segundo o biografo grego Plutarco (c. 46–120) descreveu a aparência de Alexandre como:
O historiador grego Arriano descreveu Alexandre como:
Em Romance de Alexandre é sugerido que Alexandre tinha heterocromia, com dois olhos de cores diferentes cada.[163] O historiador britânico Peter Green descreveu assim a aparência de Alexandre, baseado em sua interpretação de documentos antigos:
Autores da antiguidade afirmavam que Alexandre gostava tanto dos seus retratos feitos por Lísipo que proibiu que outros escultores fizessem retratos dele.[165] Lísipo usava normalmente o esquema contrapposto escultural para reproduzir Alexandre e outros personagens como Apoxiômeno, Hermes e Eros.[166] As esculturas de Lísipo, famosas pela sua naturalidade, se opunham às poses rígidas e estáticas, e são creditadas como sendo as que melhor oferecem uma ideia de como Alexandre era.[167]
Alexandre herdou uma personalidade forte dos seus pais. Sua mãe tinha grandes ambições e encorajava o filho a acreditar que o destino dele era conquistar o Império Persa.[164] A influência de Olímpia incutiu o senso de destino nele,[168] e Plutarco afirmou que "manteve seu espírito sério e sublime com o passar dos anos."[169] Contudo, seu pai, Filipe II, era a principal influência e modelo para Alexandre, enquanto ele observava o pai ir em campanha atrás de campanha em sua infância, conquistando várias vitórias, ignorando ferimentos.[46] A relação pai e filho era competitiva no lado da personalidade; ele precisava sempre superar o pai, as vezes mostrando um comportamento impulsivo demais em batalha.[164]
De acordo com Plutarco, um dos traços mais importantes de Alexandre era seu temperamento violento e imprudente, impulsivo por natureza,[170] o que contribuiu para o seu mecanismo de tomar decisões.[164] Apesar de Alexandre ser teimoso e obstinado, ele não respondia bem as ordens do pai, mas era aberto ao debate.[171] Ele tinha um lado mais calmo-perceptivo, logico e calculista. Ele tinha um desejo por conhecimento, amor por filosofia e era um ávido leitor.[172] Ele ganhou esses interesses através de seu tutor, Aristóteles. Alexandre era inteligente e aprendia rápido.[164] Sua inteligência e lado racional era demonstrado em suas habilidades e sucesso como general.[170] Ele tinha muito autocontrole com os "prazeres do corpo", mas era propenso a beber muito álcool sem qualquer controle.[173]
Alexandre era erudita e um entusiasta e apadrinhador das artes e ciências.[169][172] Contudo, tinha pouco interesse por esportes ou pelos Jogos Olímpicos (ao contrário do pai), buscando apenas a ideia Homérica de honra (timê) e glória (kudos).[46][168] Possuía muito carisma e uma personalidade forte, características que fizeram-no um grande líder.[170] Sua habilidade única é demonstrada pela inabilidade que outros generais macedônios tiveram em unir o país e manter o império após sua morte (algo que Alexandre não teve muita dificuldade de fazer em vida).[152]
Durante seus últimos anos de vida, e especialmente após a morte do amigo Heféstio, Alexandre começa a exibir sinais de megalomania e paranoia.[174] Seus feitos extraordinários, somado ao seu inefável senso de destino e a bajulação de seus companheiros, podem ter contribuído para este efeito.[175] Seus delírios de grandeza ficam óbvios em seu testamento e seu desejo de conquistar o mundo[174] contribuiu para esta conclusão de que sua ambição não tinha limites.[176]
Acredita-se que ele também passou a se ver como uma divindade, ou ao menos ele parecia querer tentar se divinizar.[174] Olímpia constantemente lhe dizia que ele era filho de Zeus,[177] uma teoria que foi reforçada por um oráculo de Amom em Siuá.[178] Ele começa a se identificar como filho de Zeus-Amom.[178] Alexandre adotou elementos da vestimenta e costumes persas em sua corte, mais notavelmente a prosquínese, o que levou a desaprovação de muitos macedônios, os quais relutavam em imitar.[97] Esse comportamento lhe fez perder a simpatia de muitos dos seus compatriotas.[179] Contudo, Alexandre também era um governante pragmático que entendia as dificuldades de reinar sobre povos com culturas tão diferentes. Muitos reinos conquistados tinham a cultura de cultuar seus reis como deuses. Assim, alguns acreditam que seu comportamento não era necessariamente apenas de megalomania, mas talvez uma tentativa prática de fortalecer seu reinado e manter o império unido.[86] [180]
Alexandre foi casado três vezes: com Roxana, filha do nobre Oxiartes de Báctria (um casamento realizado por relações amorosas);[181] e com as princesas Estatira II, filha de Dario III, e Parisátide (casamentos por razões políticas).[182] Acredita-se que tenha tido dois filhos, Alexandre IV (nascido de Roxana) e, possivelmente, Héracles (que teria nascido de sua amante, Barsine). Ele teria tido outro filho com Roxana, porém ela sofreu de um aborto espontâneo na Babilônia.[183]
Alexandre tinha um relacionamento muito próximo com seu amigo, general e guarda-costas Heféstio, que era filho de um nobre macedônio.[123][164][184] A morte de Heféstio foi devastadora para Alexandre.[123][185] Este evento pode ter impulsionado o declínio da saúde física e emocional de Alexandre nos últimos meses da sua vida.[140][174]
A sexualidade de Alexandre é assunto de muita especulação e controvérsia.[186] Nenhuma fonte da antiguidade relata que Alexandre tinha uma relação homossexual com algum homem, ou se a relação dele com Heféstio era sexual. Eliano, contudo, escreveu sobre a visita de Alexandre a Troia onde ele afirma que o rei se via como Aquiles e Heféstio como Pátroclo, sendo que estes personagens possivelmente eram amantes.[187] Eliano diz que Alexandre pode ter sido bissexual, o que na sua época não era algo tão controverso.[188]
O historiador Peter Green afirma que também não há muitas fontes que demonstrem Alexandre tinha muito interesse por mulheres (ele não produziu um herdeiro até ficar mais velho).[164] Contudo, ele morreu relativamente jovem (aos 32 anos), e Ogden sugere que a vida matrimonial de Alexandre era mais impressionante que a do pai, para a sua idade.[189]
Além das esposas, Alexandre também teve várias amantes. De fato, ele tinha um harém de mulheres disponíveis (tais como os reis persas), mas não as visitava tão frequentemente,[190] mostrando auto-controle com os "prazeres do corpo".[173] Ainda assim, Plutarco descreve que Alexandre era devoto à esposa Roxana, não se forçando em cima dela.[191] Green sugere que, no contexto da época, Alexandre forjou várias amizades relativamente fortes com mulheres, incluindo Ada de Cária, a qual o adotou como filho,[67] Taís de Atenas, amante de Ptolomeu e com quem Alexandre pode ter tido um relacionamento mais íntimo, e inclusive até a mãe do seu antigo inimigo Dario, Sisigambis, que teria morrido devido à tristeza profunda que sentiu ao ouvir que Alexandre havia falecido.[164]
O legado de Alexandre vai além de suas habilidades como comandante militar. Suas campanhas aumentaram os contatos e o comércio entre o Ocidente e o Oriente, e vastas áreas orientais foram expostas à civilização grega e sua influência. Algumas cidades que ele fundou se tornariam grandes centros culturais, com muitas sobrevivendo até o século XXI. Seus cronistas registraram valiosas informações durante suas marchas, enquanto os gregos passaram a ter a noção de que eles pertenciam a um mundo maior que o Mediterrâneo.[10]
Talvez o maior legado imediato de Alexandre foi a introdução de um governo macedônio para grandes faixas da Ásia. No período da sua morte, seu império se estendia da península Balcânica até ao subcontinente indiano, somando mais de 5,2 milhões de km²,[193] e era o maior império de sua época. Muitas destas áreas permaneceram sob poder ou influência macedônia ou grega pelos próximos 200–300 anos. Os estados sucessores que emergiram após a sua morte, pelo menos inicialmente, permaneceram a força dominante da região e nos 300 anos seguintes ofereceram ao mundo o chamado "período helenístico".[194]
As fronteiras orientais do império de Alexandre começaram a entrar em colapso ainda durante a sua vida.[152] Contudo, o vácuo de poder deixado no noroeste do subcontinente indiano com sua morte deu a oportunidade de ascensão de uma das mais poderosas dinastias indianas da antiguidade. O governante Chandragupta Máuria (referido em fontes gregas como Sandrócoto), de origem relativamente humilde, tomou controle da região de Panjabe, e se tornou a base de poder do subsequente Império Máuria.[195]
Durante o curso de suas conquistas, Alexandre fundou mais de vinte cidades com seu nome, a maioria a leste do rio Tigre. A primeira (e a maior), na verdade, foi a própria Alexandria do Egito, que se tornou uma das grandes cidades do Mediterrâneo. Estas cidades normalmente ficavam em importantes rotas comerciais ou boas posições defensivas. No início, elas devem ter sido bem inóspitas, um pouco mais do que grandes quartéis. Após a morte de Alexandre, muitos dos gregos que foram assentados lá resolveram voltar para suas regiões de origem.[196] Contudo, nos séculos seguintes, muitas das Alexandrias prosperaram, com elaborados prédios públicos e populações crescentes, que incluía gregos e habitantes de povos nativos da região.[98]
O termo helenização foi cunhado pelo historiador alemão Johann Gustav Droysen para denotar a expansão pelo mundo da língua, cultura e população grega para além das regiões do Império Aquemênida após as conquistas de Alexandre.[194] Que essa exportação de cultura aconteceu é inquestionável e pode ser visto nas grandes cidades helênicas como, por exemplo, Alexandria, Antioquia[197] e Selêucia (ao sul da atual Bagdá).[198] Alexandre queria inserir elementos gregos na cultura persa e tentou hibridizá-la com a cultura grega. Isso fazia parte dos seus esforços de homogenizar a Ásia e a Europa. Contudo, seus sucessores rejeitaram estas ideias. Ainda assim, a helenização se espalhou pela região, acompanhada por uma distinta e oposta orientalização dos Estados sucessores.[199]
O coração da cultura helênica ficava em Atenas.[197] [200] O relacionamento dos homens de toda a Grécia no exército de Alexandre levou ao crescimento do dialeto comum grego (o "Koiné"). O koiné se espalhou pelo mundo helênico, se tornando a língua franca das terras helênicas e, posteriormente, o antepassado do grego moderno.[201] Além disso, planejamento urbano, educação, governo local e a arte no período helênico foram todas baseadas nos ideais da Grécia clássica, evoluindo em novas formas distintas de helenístico.[197] Aspectos da cultura helênica ainda eram evidentes nas tradições do Império Bizantino em meados do século XV.[202][203]
Alguns dos principais efeitos da helenização pode ser visto no Afeganistão e na Índia, especialmente na região do Reino Greco-Báctrio (250 a.C.-125 a.C.) que englobava os territórios afegão, paquistanês e tajiquistanês, além também do Reino Indo-Grego (180 a.C.–10 d.C.), nos territórios afegão e indiano.[204] Na nova "rota da Seda" a cultura grega hibridizou com a indiana, especialmente com a cultura budista. O resultado do sincretismo conhecido como greco-budismo teve muitas influências no desenvolvimento da cultura budista em geral e criou também uma nova cultura de arte greco-budista.[205] Os reinos greco-budistas enviaram os primeiros missionários budistas à China, ao Sri Lanka e até ao Mediterrâneo. Algumas das primeiras e mais influentes imagens de Gautama Buda apareceram neste período; talvez modelados igual a Apolo.[204] Várias tradições budistas podem ter sido influenciadas pelas religiões da Grécia Antiga: o conceito de Bodisatva é uma reminiscência de heróis divinos gregos,[206] e algumas práticas cerimoniais maaianas (a queima de incenso, flores como presentes e comida nos altares) são similares as práticas dos antigos gregos, contudo práticas similares também eram vistas em povos nativos. Um rei grego em particular, Menandro I, provavelmente se tornou budista e foi imortalizado nas escrituras 'Milinda'.[204]
O processo de helenização intensificou o comércio entre o ocidente e o oriente.[207] Por exemplo, instrumentos astronômicos gregos datados do século III antes de cristo foram encontradas na cidade greco-bactriana de Ai-Khanoum, no moderno Afeganistão,[208] enquanto o conceito grego de terra redonda cercada de planetas igualmente redondos contrastava com a crença cosmológica de uma terra esférica com planetas em órbita elipsoide.[207][209]
Alexandre e suas façanhas eram admirados por muitos romanos, especialmente generais, que queriam se associar com seus feitos. Políbio começou sua obra Histórias relembrando aos romanos os feitos de Alexandre. Muitos líderes políticos e militares romanos se comparavam com Alexandre Magno, usando-o como modelo. O general e cônsul Pompeu também adotou o epíteto "Magno" ("Grande") e até tentou copiar o estilo de cabelo de Alexandre. Ele ainda costumava usar uma capa vermelha, assim como Alexandre, como um sinal de grandeza.[210]
Júlio César chegou a construir uma estátua equestre de bronze em honra a Alexandre mas depois substituiu sua cabeça pela dele próprio, enquanto o imperador Augusto chegou a visitar a tumba dele em Alexandria. Trajano também admirava muito Alexandre, assim como Nero e Caracala.[210] Os Macrianos, uma família romana que sob a liderança de Macrino rapidamente ascendeu ao trono imperial romano, usava roupas que lembravam Alexandre e também tinha várias peças dele.[211]
Por outro lado, alguns escritores romanos, particularmente na era republicana, usavam Alexandre como um conto preventivo de como tendências autocráticas podem ser colocadas em xeque com valores republicanos romanos. Mas na maioria dos casos, Alexandre era retratado como um exemplo de líder com valores como "amizade" (amicita) e "clemência" (clementia), mas também "raiva" (iracundia) e "excesso de desejo de glória" (cupiditas gloriae).[212]
Relatos lendários cercaram a vida de Alexandre, provavelmente encorajados por ele mesmo.[213] Seu historiador cortesão Calístenes retratou o mar na Cilícia como desenhado de volta para ele em prosquínese. Escrevendo logo depois da morte de Alexandre, outro participante, Onesícrito de Astipaleia, inventou um cortejo entre Alexandre e Taléstris, a mítica rainha das Amazonas. Quando Onesícrito leu esta passagem para seu patrão, o general de Alexandre e depois rei Lisímaco relatadamente brincou, "Eu me pergunto onde estava naquele momento."[214]
Nos primeiros séculos da morte de Alexandre, provavelmente em Alexandria, certa quantidade de material lendário foi agrupado em um texto conhecido como o Romance de Alexandre, depois falsamente atribuído a Calístenes e portanto conhecido como "Pseudo-Calístenes". Este texto sofreu numerosas expansões e revisões através da Antiguidade e Idade Média, contendo muitas histórias dúbias,[213] e foi traduzido em numerosas línguas.[215]
Os feitos de Alexandre, o Grande e o seu legado são retratados em diversas culturas. Alexandre está na cultura popular desde sua era até os dias atuais. O Romance de Alexandre, em particular, teve um impacto profundo sobre a forma como o rei macedônio é retratado nas culturas, da Pérsia até a da Europa medieval até a Grécia Moderna.[215]
Alexandre já considerava a si mesmo como o "Rei da Ásia" logo após sua vitória em Isso, um conceito fortalecido após seus sucessos posteriores.[216] Nos documentos babilônios, ele era referido como o "Rei do Mundo" (já que "Rei da Ásia" não tinha significado na geografia pelos habitantes da Babilônia).[217] Alexandre também é chamado de Cosmocrátor (kosmokrator, "governador do mundo") na obra Romance de Alexandre.[218]
Alexandre é figura presente no folclore da Grécia moderna, mais do que qualquer outra figura histórica.[219] A forma coloquial do seu nome em grego moderno ("O Megalexandros") é um nome familiar.[220] Santo Agostinho, no seu livro A Cidade de Deus, reafirmou a parábola de Cícero que mostrava Alexandre, o Grande era pouco mais do que um líder de um bando de ladrões:
"E então se a justiça for deixada de fora, o que são reinos além de um bando de ladrões? Pois o que são um bando de ladrões, se não pequenos reinos? O grupo também é um bando de homens governados por ordens de um líder, ligados por um pacto social, e seu espólio é dividido de acordo com uma lei que concordaram. Por repetidamente adicionar homens desesperados, essa praga cresce ao ponto de controlar territórios e estabelecer um local fixo, controlando cidades e subjugando pessoas, em seguida, mais conspicuamente assume o nome de reino e então este nome é dado abertamente a ele, não por qualquer subtração de cupidez, mas pela adição de impunidade. Pois foi uma elegante e verdadeira resposta que fez Alexandre o Grande por um certo pirata que ele havia capturado. Quando o rei perguntou o que ele estava pensando, que ele deveria molestar o mar, ele respondeu com uma independência desafiadora: 'O mesmo que você quando você molesta o mundo! Já que eu faço isso de um pequeno navio eu sou chamado de pirata. Você o faz com uma grande frota e te chamam de imperador'."[221]Na literatura em persa médio pré-islâmica, Alexandre é referido pelo epíteto gujastak, que significa "amaldiçoado", e ele foi acusado de destruir templos e queimar documentos sagrados do zoroastrismo.[222] No Irã islâmico, sob influência da obra Romance de Alexandre (em farsi: اسکندرنامه, lit. 'Iskandarnamah'), uma visão mais positiva de Alexandre emerge.[223] Em Épica dos Reis de Ferdusi cita Alexandre na linhagem de legítimos xás (governantes) do Irã, uma figura mítica que explorou até os cantos do mundo em busca da "fonte da juventude".[224] Escritores persas posteriores associaram ele com filosofia, o retratando como figuras conhecidas como Sócrates, Platão e Aristóteles, na busca por imortalidade.[223]
Na versão siríaca de o Romance de Alexandre o retratam como um conquistador cristão ideal que rezava ao "verdadeiro Deus".[223] No Egito, Alexandre é retratado como um filho de Nectanebo II, o último faraó antes da conquista do país pela Pérsia. A derrota que Alexandre infligiu ao rei Dario III é relatado como a salvação do Egito.[225]
A figura de Dhul-Qarnayn (literalmente "Aquele de dois chifres") mencionado no Corão é acreditado por acadêmicos como uma representação de Alexandre, devido aos paralelos com a obra Romance de Alexandre.[223] Nesta tradição, ele era uma figura histórica que construiu uma muralha para defender contra as nações de Gogue e Magogue. Ele então viajou o mundo em busca da 'Água da Vida e Imortalidade', eventualmente se tornando um profeta.[225]
Nas línguas hindi e urdu, o nome "Sikandar", que deriva do persa, denota o surgimento de um jovem talento. Na Europa medieval, ele é membro dos "Nove da Fama", um grupo de heróis que encapsulavam todas as qualidades ideais de cavalheirismo.[226]
Além de poucas inscrições e fragmentos, textos escritos por contemporâneos de Alexandre, que o conheceram pessoalmente, ou pelos que tomaram como base relatos diretos de seus subordinados estão todos perdidos. Entre os contemporâneos que escreveram os feitos de sua vida estão o historiador das campanhas de Alexandre, Calístenes; os generais Ptolomeu e Nearco; Aristóbulo, um jovem oficial; e ainda Onesícrito, um timoneiro de Alexandre. A maioria do trabalho deles foi perdido com o tempo, mas pesquisas feitas na antiguidade em cima destas fontes sobreviveram. Um dos primeiros historiadores não contemporâneos a escrever sobre Alexandre, citando como fonte trabalhos de pessoas que conheceram ele, foi Diodoro Sículo (século I a.C.), seguido por Quinto Cúrcio Rufo (no século I), Arriano (século I e II), o biografo Plutarco (século I e II), e finalmente Marco Juniano Justino, cujo trabalho foi feito no século IV. Destes, os relatos de Arriano são geralmente considerados os mais confiáveis, já que ele usou textos de Ptolomeu e Aristóbulo como fonte. Diodoro também é citado como uma ótima fonte dos fatos.[10]
Isabel I ou Elizabeth I (Greenwich, 7 de setembro de 1533 – Richmond, 24 de março de 1603), também chamada de  "A Rainha Virgem", "Gloriana" ou "Boa Rainha Bess" ("Bess" era como Roberto Durdley, seu favorito, a chamava) foi Rainha Reinante da Inglaterra e Irlanda de 1558 até sua morte e a quinta e última monarca da Casa de Tudor. Como filha do rei Henrique VIII, Isabel nasceu dentro da linha de sucessão; entretanto, a sua mãe Ana Bolena foi executada dois anos e meio após seu nascimento, e o casamento dos seus pais foi anulado. Isabel assim foi declarada ilegítima. 
O seu meio-irmão Eduardo VI sucedeu a D. Henrique e reinou até morrer em 1553. Antes da sua morte, Eduardo nomeou Joana Grey como rainha, excluindo da sucessão as suas meias-irmãs Isabel e a católica Maria I, apesar da existência de um estatuto declarando o contrário. Porém, o seu testamento acabou anulado e Maria tornou-se rainha, tendo Joana sido executada. Isabel foi também feita prisioneira, durante o cerca de um ano em que durou o reinado de Maria, por suspeitas de apoiar os rebeldes protestantes.
Isabel sucedeu a Maria em 1558 e passou a reinar com um conselho.[1] A rainha passou a depender muito de um grupo de conselheiros de confiança liderados por Guilherme Cecil, Barão Burghley. Uma das suas primeiras ações como rainha foi o estabelecimento de uma igreja protestante inglesa, da qual tornou-se sua Governadora Suprema. A Resolução Religiosa Isabelina mais tarde desenvolveu-se na atual Igreja Anglicana. Era também esperado que ela se casasse e gerasse um herdeiro para continuar a linhagem da Casa de Tudor, porém, nunca se casou apesar de ter tido vários pretendentes. Isabel ficou famosa pela sua castidade enquanto envelhecia. Um culto cresceu ao seu redor tendo sido celebrada em pinturas, desfiles e obras literárias.
A governação de Isabel foi mais moderada do que a do pai e a dos meio-irmãos.[2] Um de seus lemas era video et taceo ("Vejo e digo nada").[3] Era relativamente tolerante em questões religiosas, evitando perseguições sistemáticas. Depois de 1570, quando o Papa a declarou ilegítima e permitiu aos seus súditos que deixassem de lhe obedecer, várias conspirações ameaçaram a sua vida. Todos os planos foram derrotados com a ajuda dos serviços secretos dos seus ministros. Isabel era cautelosa em assuntos estrangeiros, movimentando-se entre as grandes potências da França e Espanha. Apoiou, sem entusiasmo, várias campanhas militares ineficazes e mal equipadas nos Países Baixos do Sul, na França e Irlanda. Porém, por volta da década de 1580, uma guerra contra Espanha tornou-se inevitável. Quando os espanhóis finalmente decidiram em 1588 tentar conquistar a Inglaterra, o fracasso da Armada Invencível associou Isabel a uma das maiores vitórias militares da história inglesa.
O reinado ficou para sempre conhecido como o Período Isabelino, famoso acima de tudo pelo florescimento do drama inglês, liderado por dramaturgos como William Shakespeare e Christopher Marlowe, além das proezas marítimas dos aventureiros ingleses como Sir Francis Drake. Alguns historiadores são mais contidos nas sua avaliações a Isabel. Eles representam-na como uma governante temperamental, às vezes indecisa e que teve muita sorte.[4] Uma série de problemas económicos e militares diminuíram a sua popularidade ao final de seu reinado. Isabel é reconhecida como uma intérprete carismática e uma sobrevivente obstinada num período em que o governo era uma instituição desorganizada e limitada, e monarcas de países vizinhos enfrentavam problemas internos que ameaçavam seus tronos. Assim foi o caso da rival Maria da Escócia, que foi presa por Isabel em 1568, e acabou por ser executada em 1587. Depois dos curtos reinados de Eduardo VI e Maria I,deu estabilidade ao reino e ajudou a criar um sentimento de identidade nacional.[2]
Isabel Tudor nasceu no Palácio de Placentia, Greenwich, em 7 de setembro de 1533, sendo nomeada em homenagem a suas avós: Isabel de Iorque e Isabel Howard.[5] Era a segunda filha do rei Henrique VIII de Inglaterra a sobreviver a infância. Sua mãe era Ana Bolena, a segunda esposa de Henrique. Ao nascer, Isabel era a herdeira presuntiva do trono inglês. Sua meia-irmã mais velha, Maria, havia perdido sua posição como legítima quando o rei anulou seu casamento com sua mãe, Catarina de Aragão, para se casar com Ana e ter um herdeiro homem a fim de garantir a sobrevivência da dinastia da Casa de Tudor. Ela foi batizada em 10 de setembro por Tomás Cranmer, Arcebispo da Cantuária; seus padrinhos foram Henrique Courtenay, 1º Marquês de Exeter; Isabel Howard, Duquesa de Norfolk; e Margarida Wotton, Viúva Marquesa de Dorset.[6]
Sua mãe foi executada por acusações de adultério, incesto e alta traição em 19 de maio de 1536, quando Isabel tinha apenas dois anos e oito meses.[7] Ela foi declarada ilegítima e privada de seu lugar na sucessão real.[8][nota 1] Onze dias após a execução de Ana Bolena, Henrique se casou com Joana Seymour, porém ela acabou morrendo de complicações pós-parto depois de dar à luz em 1537 ao príncipe Eduardo. Desde seu nascimento, Eduardo era o herdeiro aparente incontestável do trono. Isabel carregou o pano batismal em seu batizado.[9]
A primeira governanta ou Senhora Patroa de Isabel, Margarida Bryan, escreveu que ela era "como para uma criança e tão gentil de condições que jamais conheci em outra em minha vida".[10][11] Isabel foi colocada aos cuidados de Branca Herbert por volta do outono de 1537, que permaneceu como Senhora Patroa até se aposentar no fim de 1545 ou início de 1546.[12] Catarina "Kat" Ashley foi nomeada como governanta de Isabel em 1537 e permaneceu sua amiga até morrer em 1565, quando Branca Parry a sucedeu como Dama de Companhia Chefe da Câmara Privada.[13] Ashley ensinou a Isabel quatro línguas: francês, flamenco, italiano e espanhol.[14] Na época em que Guilherme Grindal tornou-se seu tutor em 1544, ela já conseguia escrever em inglês, latim e italiano. Com Grindal, um tutor habilidoso e talentoso, Isabel também progrediu em francês e grego.[15] Ele morreu em 1548, e Isabel passou a ser ensinada por Rogério Ascham, um professor simpático que acreditava que o ensino também deveria ser cativante.[16] Quando sua educação formal terminou em 1550, ela era uma das mulheres mais bem educadas de sua geração.[17] Isabel, ao final de sua vida, também supostamente falava galês, córnico, escocês e irlandês.[18] Isabel fez uma tradução do manuscrito de Annales de Tácito, concluída no final do século XVI e preservada na Biblioteca do Palácio de Lambeth.[19] O embaixador veneziano afirmou em 1603, que ela "dominava [essas] línguas tão completamente que cada uma parecia ser sua língua nativa".[20] O historiador Mark Stoyle sugere que provavelmente Guilherme Killigrew, Criado da Câmara Privada e posteriormente Chanceler do Tesouro, ensinou-lhe o córnico.[21]
Henrique VIII morreu em janeiro de 1547 e foi sucedido pelo filho de nove anos Eduardo VI, meio-irmão de Isabel. A sexta esposa e viúva do rei, Catarina Parr, logo se casou com Tomás Seymour, 1.º Barão Seymour de Sudeley, tio de Eduardo e irmão de Eduardo Seymour, 1.º Duque de Somerset e Lorde Protetor.  O casal colocou Isabel em sua criadagem em Chelsea, Londres.  Lá ela passou por uma crise emocional que alguns historiadores acreditam tê-la afetado pelo restante de sua vida. Seymour, então com quase quarenta anos, porém possuindo charme e "poderoso apelo sexual",[23] envolveu-se em brincadeiras grosseiras com Isabel, então com quatorze anos. Isso incluía entrar em seu quarto durante a noite, cutucá-la e bater em suas nádegas. Parr juntava-se ao marido ao invés de confrontá-lo por suas atividades impróprias.  Duas vezes ela também cutucou a menina e em uma ocasião a segurou enquanto Seymour cortava sua camisola "em milhares de pedaços".[24] Entretanto, Parr acabou a situação assim que descobriu os dois abraçados.[25]  Isabel foi mandada embora em maio de 1548.[26]
Tomás Seymour planejava controlar a família real e tentou ser nomeado Governador da Pessoa Real.[27][28]  Quando Parr morreu no parto em 5 de setembro de 1548, ele renovou seu interesse por Isabel e tinha a intenção de se casar com ela.[29]  Os detalhes de seu comportamento com Isabel tornaram-se públicos;[30] isso foi a última gota para seu irmão e para o conselho regencial. Seymour foi preso em janeiro de 1549 sob suspeita de se casar com Isabel e depor o irmão. Vivendo na Casa Hatfield, Isabel não admitia nada. Sua teimosia irritou o interrogador sir Roberto Tyrwhitt, que relatou "Não vejo em seu rosto que é culpada". Seymour foi decapitado em 29 de março.[31]
Eduardo VI morreu em 6 de julho de 1553 aos quinze anos de idade. Seu testamento colocava de lado o Terceiro Ato de Sucessão e excluía tanto Maria quanto Isabel da sucessão, declarando como herdeira, ao invés disso, Joana Grey, neta de Maria, Duquesa de Suffolk, irmã de Henrique VIII. Joana foi proclamada rainha pelo Conselho Privado, porém ela logo perdeu o apoio e foi deposta em nove dias. Maria entrou triunfantemente em Londres com Isabel ao seu lado.[32][nota 2]
As demonstrações de solidariedade entre as irmãs duraram pouco. A católica devota Maria, estava determinada em esmagar a fé protestante em que Isabel havia sido educada, ordenando que todos comparecessem às missas católicas; Isabel tinha que obedecer. A popularidade inicial de Maria logo desapareceu em 1554, quando anunciou planos para se casar com o espanhol Filipe, Príncipe das Astúrias, um católico e filho do imperador Carlos V. O descontentamento rapidamente cresceu pelo país e muitos olhavam para Isabel como o centro da oposição religiosa.[33]
A Rebelião de Wyatt estourou entre janeiro e fevereiro de 1554, porém foi logo suprimida.[34] Isabel foi levada à corte e interrogada sobre seu papel, sendo aprisionada em 18 de março, na Torre de Londres. Ela fervorosamente declarou sua inocência.[35] Apesar de ser improvável que ela tenha tramado junto aos rebeldes, sabe-se que alguns deles a abordaram. Simão Renard, embaixador de Carlos e confidente próximo de Maria, afirmou que o trono dela nunca estaria seguro enquanto Isabel vivesse, com o chanceler Estêvão Gardiner trabalhando para colocá-la sob julgamento.[36] Os apoiadores de Isabel dentro do governo, incluindo Guilherme Paget, 1.º Barão Paget, convenceram a rainha a poupar sua irmã na falta de evidências conclusivas. Ao invés disso, Isabel foi levada da Torre a Woodstock, passando quase um ano em prisão domiciliar sob a supervisão de sir Henrique Bedingfeld. Multidões a aclamaram no caminho.[37][nota 3]
Isabel foi chamada de volta à corte em 17 de abril de 1555 para comparecer aos estágios finais da aparente gravidez de Maria. Se a irmã e o filho morressem, Isabel tornaria-se rainha. Por outro lado, se Maria desse à luz uma criança saudável, suas chances de ascender ao trono muito diminuiriam. Quando ficou claro que Maria não estava grávida, ninguém mais acreditava que ela seria capaz de produzir um herdeiro.[39] Assim, a sucessão de Isabel parecia garantida.[40]
Filipe ascendeu ao trono espanhol em 1556 como Filipe II, reconhecendo a nova realidade política e cultivando sua cunhada. Ela era uma melhor aliada que a principal alternativa, a rainha Maria da Escócia, que havia crescido na França e estava prometida a Francisco, Delfim da França.[41] Quando Maria adoeceu em 1558, ele enviou Gómez Suárez de Figueroa e Córdoba, 1.º Duque de Feria, para consultar com Isabel.[42] A entrevista ocorreu na Casa Hatfield, onde tinha voltado a viver em outubro de 1555. Ela já estava fazendo planos para seu governo por volta de outubro de 1558. A rainha acabou reconhecendo a meia-irmã como sua herdeira em 6 de novembro. Maria morreu em 17 de novembro de 1558 e Isabel ascendeu ao trono.[43]
Isabel tornou-se rainha aos 25 anos de idade e declarou suas intenções a seu conselho e outros pariatos que haviam ido para a Casa Hatfield jurar lealdade. O discurso contém o primeiro relato de sua adoção da teologia política medieval dos "dois corpos" do soberano: o corpo natural e o corpo político.[44]
Ela foi recebida calorosamente por cidadãos e saudada por orações e desfiles, a maioria em forte protestantismo, enquanto progredia por Londres triunfantemente na véspera de sua cerimônia de coroação. As repostas graciosas e abertas de Isabel encantaram os espectadores, que estavam "maravilhosamente arrebatados".[46] No dia seguinte, 15 de janeiro de 1559, Ela foi coroada e ungida na Abadia de Westminster por Owen Oglethorpe, o católico Bispo de Carlisle. Isabel então foi apresentada à aceitação de seu povo, em meio aos sons ensurdecedores de órgãos, pífaros, trombetas, tambores e sinos.[47]
As convicções religiosas pessoais de Isabel foram muito discutidas por historiadores. Era protestante, porém mantinha símbolos católicos como o crucifixo e diminuía o papel dos sermões, indo contra a crença protestante.[48]
Ela e seus conselheiros viam a ameaça de uma cruzada católica contra a Inglaterra. Isabel assim procurou uma solução protestante que não ofenderia muito os católicos enquanto ao mesmo tempo atendia os anseios dos protestantes ingleses; entretanto, não tolerava os puritanos mais radicais, que pressionavam por reformas drásticas.[49]  Assim, o parlamento começou a legislar em 1559 uma igreja baseada na resolução protestante de Eduardo VI, com o monarca como chefe, mas com elementos católicos como vestimentas sacerdotais.[50]
A Câmara dos Comuns apoiava fortemente as propostas, porém o projeto de lei da supremacia encontrou oposição na Câmara dos Lordes, particularmente dos bispos. Isabel teve a sorte de que muitos bispados na época estavam vagos, incluindo o arcebispado da Cantuária.[nota 4][52][51]  Isso permitiu que apoiadores dentre os pariatos tivessem mais votos que os bispos e pariatos conservadores. Mesmo assim, foi forçada a aceitar o título de Governadora Suprema da Igreja de Inglaterra em vez do mais controverso Chefe Suprema, que muitos achavam inaceitável uma mulher portar.  Aprovou-se o novo Ato de Supremacia em 8 de maio de 1559.  Todos os oficiais públicos tinham de prestar juramento de lealdade a monarca como governadora suprema ou correrem o risco de perderem o cargo; revogaram-se as leis de heresia para impedir a perseguição de dissidentes que Maria praticara.  Ao mesmo tempo também aprovou-se o novo Ato da Uniformidade, que obrigava o comparecimento à igreja e o uso de uma versão adaptada do Livro de Oração Comum de 1552, apesar das penas de não-conformidade ou de não comparecimento não serem extremas.[53]
Esperava-se desde o início de seu reinado que Isabel se casasse, surgindo questões sobre com quem. Ela nunca se casou, apesar de ter tido vários pretendentes; as razões para isso não são claras. Historiadores especularam que Tomás Seymour facilitou-lhe relações sexuais, ou que ela sabia ser estéril.[54][55]  A rainha considerou vários pretendentes até os cinquenta anos. Sua última corte foi com o francês Francisco, Duque de Anjou, 22 anos mais novo. Apesar de correr o risco de perder o poder como sua irmã, que fazia o que Filipe II queria, o casamento oferecia a possibilidade de um herdeiro.[56] Entretanto, a escolha de um marido poderia provocar instabilidade política ou até insurreições.[57]
Ficou evidente no verão de 1559 que Isabel apaixonara-se por Roberto Dudley, seu amigo de infância.[58][59]  Disse-se que sua esposa Amy Robsart sofria de uma "doença em um de seus seios", e que a rainha gostaria de se casar com Dudley se ela morresse.[59] Vários pretendentes competiram pela mão de Isabel no outono do mesmo ano; seus impacientes interessados envolveram-se em conversas cada vez mais escandalosas e relataram que o casamento com seu favorito não era bem visto na Inglaterra:[60] "Não há homem que não clama com indignação sobre ele e ela… ela não se casará com ninguém exceto seu favorito Roberto".[61] Robsart morreu em setembro de 1560 ao cair de uma escada e, apesar do inquérito legista concluir por um acidente, muitos suspeitavam que Dudley arranjara a morte da esposa para poder se casar com Isabel.[nota 5][64] A rainha considerou seriamente por algum tempo se casar com Dudley. Porém, Guilherme Cecil, Nicolau Throckmorton e outros pariatos conservadores, deixaram claro sua desaprovação.[65] Houve rumores também que a nobreza iria se revoltar caso o casamento ocorresse.[66]
Roberto foi considerado como um possível candidato entre outros pretendentes para a rainha por quase uma década.[67] Isabel tinha muito ciúmes, mesmo depois de não mais pretender casar-se com ele.[68] Ela lhe criou o título de Conde de Leicester em 1564. Dudley finalmente se casou outra vez em 1578 e a rainha respondeu com repetidas cenas de descontentamento e um ódio vitalício contra sua nova esposa, Letícia Knollys.[69][70] Dudley mesmo assim "permaneceu no centro da vida emocional" de Isabel.[71] Ele morreu pouco depois da derrota da Invencível Armada. Foi encontrada uma carta dele entre os pertences pessoais de Isabel após a morte da rainha, marcada como "sua última carta" com a letra dela.[72]
As negociações de casamento eram parte de um importante elemento da política internacional de Isabel.[74] Ela recusou a mão de Filipe II no início de 1559, porém contemplou por anos a proposta do rei Érico XIV da Suécia.[75] Ela também negociou seriamente por muitos anos casar-se com o arquiduque Carlos II da Áustria, primo de Filipe. As relações com os Habsburgo deterioraram-se por volta de 1569, e a rainha considerou se casar com dois príncipes franceses de Valois, primeiro Henrique, Duque de Anjou, e mais tarde seu irmão Francisco, Duque de Anjou, entre 1572 e 1581.[76] A última proposta estava ligada a uma possível aliança contra a Espanha pelo controle dos Países Baixos do Sul.[77] Isabel parece ter considerado seriamente o cortejo por algum tempo, e usava um brinco em formato de sapo que Francisco havia lhe enviado.[78]
Isabel disse a um enviado imperial em 1563: "Se eu seguir a inclinação de minha natureza, será esta: mulher pedinte e solteira ao invés de rainha e casada".[74] Mais tarde no mesmo ano, depois dela contrair varíola, a questão da sucessão passou a ser muito debatida no parlamento. Eles imploraram para que a rainha se casasse ou nomeasse um herdeiro para impedir uma guerra civil após sua morte. Isabel recusou-se a fazer as duas coisas. Ela suspendeu o parlamento em abril, e não o reconvocou até precisar aumentar os impostos em 1566. Tendo prometido anteriormente que se casaria, Isabel declarou ao incontrolável parlamento:
Algumas das principais figuras do governo começaram aceitar, em particular por volta de 1570, que Isabel nunca se casaria ou nomearia um herdeiro. Guilherme Cecil já estava procurando soluções para o problema de sucessão.[80]  Foi frequentemente acusada de irresponsabilidade por nunca ter casado.[81]  Entretanto, seu silêncio fortaleceu sua própria segurança política: Isabel sabia que estaria vulnerável a um golpe se nomeasse um herdeiro; lembrava como "uma segunda pessoa, como fui" fora foco de tramas contra sua predecessora.[82]
O fato de Isabel não ter se casado inspirou um culto de virgindade. Era representada na poesia e literatura como uma virgem, uma deusa ou ambas, não como uma mulher normal.[84]  Apenas Isabel inicialmente fez de sua virgindade uma virtude: declarou na Câmara dos Comuns em 1559 que "No final, será para mim suficiente, que uma pedra de mármore deverá declarar que uma rainha, tendo reinado por um tempo, viveu e morreu virgem".[85]  Posteriormente, poetas e escritores adotaram o tema e o transformaram numa iconografia que exaltava a rainha. Tributos públicos a ela em 1578 agiam como uma asserção de oposição codificada contra as negociações de casamento de Isabel com Francisco, Duque de Anjou.[86]
Isabel, dando um aspecto positivo à sua situação conjugal, insistiu ser casada com seu reino e súditos, sob proteção divina.  Declarou em 1599: "todos os meus maridos, meu bom povo".[87]
A política inicial de Isabel com a Escócia foi a de se opor à presença francesa.[88] Ela temia que os franceses planejassem invadir a Inglaterra e colocar no trono a rainha Maria da Escócia,[nota 6][89] considerada por muitos como herdeira da coroa inglesa.[nota 7][90] Isabel foi persuadida a enviar uma força para a Escócia ajudar os rebeldes protestantes; apesar da campanha ter sido inepta, o resultante Tratado de Edimburgo de julho de 1560 retirou a ameaça francesa no norte.[nota 8][91] A Escócia tinha uma estabelecida igreja protestante e um governo formado por um conselho de nobres protestantes apoiados por Isabel quando Maria voltou para o reino em 1561 para reassumir seu poder.[92] Ela recusou-se a ratificar o tratado.[93]
Isabel propôs em 1563 que Roberto Dudley, seu próprio pretendente, se casasse com Maria, sem antes falar com nenhum dos dois envolvidos. Ambos não ficaram interessados[94] e ela acabou se casando dois anos depois com Henrique Stuart, Lorde Darnley, que tinha sua própria reivindicação ao trono inglês. O casamento foi o primeiro de uma série de erros de julgamento que Maria cometeu e que acabaram dando a vitória para os protestantes escoceses e Isabel. Stuart rapidamente ficou impopular e depois infame por participar do assassinato de David Rizzio, secretário italiano de sua esposa. Ele mesmo acabou sendo morto em fevereiro de 1567 por conspiradores quase certamente liderados por Jaime Hepburn, 4.º Conde de Bothwell. Pouco tempo depois, em maio, Maria se casou com Hepburn e levantou suspeitas que havia participado do assassinato do marido. Isabel escreveu a ela:
Esses evento rapidamente levaram a derrota de Maria e seu aprisionamento no Castelo de Lochleven. Os lordes escoceses forçaram sua abdicação em favor do filho Jaime, que havia nascido em junho de 1566. O novo rei foi levado ao Castelo de Stirling para ser criado como protestante. Maria escapou de Loch Leven em 1568, porém fugiu para a Inglaterra depois de uma nova derrota, onde haviam lhe garantido que teria apoio de Isabel. O primeiro instinto de Isabel foi de restaurar a outra monarca, entretanto ela e o conselho decidiram jogar seguro. Ao invés de arriscarem-se a levar Maria de volta a Escócia com um exército inglês ou enviá-la a França para seus inimigos católicos, foi decidido mantê-la na Inglaterra onde ficou aprisionada pelos dezenove anos seguintes.[96]
Maria logo foi o foco de uma rebelião. Houve um grande levante católico no Norte em 1569; o objetivo era libertar Maria, casá-la com Tomás Howard, 4.º Duque de Norfolk, e colocá-la no trono inglês.[97] Mais de 750 rebeldes foram executados sob as ordens de Isabel após sua derrota.[98] Acreditando que a revolta havia sido bem sucedida, o Papa Pio V emitiu em 1570 uma bula papal chamada Regnans in Excelsis em que declarava "Isabel, a pretensa Rainha da Inglaterra e servente de crime" excomungada e herética, liberando todos seus súditos de qualquer lealdade a ela.[99][100] Católicos que obedecessem suas ordens estavam ameaçados com excomungação.[99] A bula papal provocou respostas legislativas contra católicos no parlamento, que acabaram mitigadas pela intervenção de Isabel.[101] A conversão de ingleses para o catolicismo com "o intuito" de remover sua lealdade da rainha foi transformada em alta traição em 1581, punível com pena de morte.[102] Padres missionários vindos de seminários continentais foram para a Inglaterra secretamente a partir da década de 1570 para causar a "reconversão". Muitos foram executados, criando um culto de martírio.[100]
Regnans in Excelsis deu aos católicos ingleses uma forte iniciativa para verem Maria como sua verdadeira soberana. Maria talvez não tenha adquirido conhecimento de todas as tramas católicas para colocá-la no trono da Inglaterra, porém da Conspiração de Ridolfi de 1571 (que fez com que Howard fosse decapitado) até a Conspiração de Babington de 1586, sir Francisco Walsingham, mestre espião de Isabel, e o conselho sutilmente reuniram um caso contra ela.[97] Isabel inicialmente resistiu aos pedidos de execução de Maria. No final de 1586 ela foi persuadida a autorizar seu julgamento e execução sob as evidências de cartas escritas durante a Conspiração de Babington.[103] A proclamação de Isabel da sentença anunciava que "a dita Maria, pretendendo o título da mesma Coroa, tinha cercado-se e imaginado-se dentro do mesmo reino diversas coisas com a intenção de ferir, matar e destruir nossa pessoa real".[104] Maria acabou sendo decapitada em 8 de fevereiro de 1587 no Castelo de Fotheringhay, Northamptonshire.[105] Após a execução, Isabel afirmou nunca tê-la ordenado e a maioria dos relatos conta que ela pediu ao secretário Guilherme Davison, quem lhe trouxe o mandato, para não enviar o documento mesmo estando assinado. A sinceridade do remorso da rainha e seus motivos para pedir a Davison não executar o mandato foram questionados por historiadores contemporâneos e posteriores.[106][107]
A política internacional de Isabel foi principalmente defensiva. A exceção foi a ocupação inglesa de Le Havre de outubro de 1562 a junho de 1563, que terminou em fracasso quando seus aliados huguenote juntaram-se aos católicos para retomar a cidade. A intenção da rainha era trocar Le Havre por Calais, retomada pela França em janeiro de 1558.[108] Isabel procurou políticas agressivas apenas através das atividades de suas frotas. Isso acabou tendo bons resultados na guerra contra a Espanha, lutada 80% nos mares.[109] Ela fez de Francis Drake um cavaleiro após sua circum-navegação entre 1577 e 1580, e ele acabou ganhando fama por ataques a portos e frotas espanholas. Um elemento de pirataria e auto-enriquecimento motivava os marinheiros, sob os quais Isabel tinha pouco controle.[110][111]
Isabel evitou expedições continentais depois da ocupação e perda de Le Havre até 1585, quando enviou um exército inglês para ajudar rebeldes protestantes holandeses contra Filipe II. Isso ocorreu após as mortes de seus aliados Guilherme I, Príncipe de Orange, e Francisco, Duque de Anjou, ambos em 1584, junto com a conquista de várias cidades holandesas por Alexandre Farnésio, Duque de Parma e Placência, governador dos Países Baixos do Sul. Uma aliança em dezembro de 1584 entre Filipe e a Liga Católica francesa minou a capacidade de Henrique III de França, irmão de Francisco, de conter a dominação espanhola dos Países Baixos. Isso também expandiu a influência espanhola ao longo do Canal da Mancha na costa da França, onde a Liga Católica era forte, expondo a Inglaterra a uma invasão.[112] O cerco de Antuérpia no verão de 1585 por Farnésio fez necessária uma reação por parte dos ingleses e holandeses. O resultado foi o Tratado de Nonsuch, em que Isabel prometia apoio militar aos holandeses.[113]
A expedição foi liderada por Roberto Dudley, Conde de Leicester. Desde o início Isabel não apoio muito esse curso de ação. Sua estratégia era apoiar os holandeses com um exército inglês enquanto secretamente negociava a paz com a Espanha dias antes da chegada de Dudley,[114] porém necessariamente entrava em conflito com a estratégia do conde, quem os holandeses queriam e era esperado para lutar ativamente em uma campanha. A rainha queria "evitar a todos os custos qualquer ação decisiva contra o inimigo".[115] Ele irritou Isabel ao aceitar o cargo de Governador Geral oferecido pelos Estados Gerais. Ela viu isso como uma tentativa holandesa de fazê-la aceitar a soberania sobre os Países Baixos, que até então ela tinha recusado.[116] Isabel escreveu a Dudley:
O "comando" de Isabel era que seu emissário lesse suas cartas de desaprovação em público diante do Conselho de Estado holandês e com Dudley presente.[118] Essa humilhação pública de seu "tenente general" junto com suas conversas de paz em separado com a Espanha[nota 9] minaram irreversivelmente sua posição entre os holandeses. A campanha militar foi repetidas vezes prejudicada pelas várias recusas da rainha de enviar os fundos prometidos para os soldados famintos. Sua falta de vontade de comprometer-se à causa, as deficiências de Dudley como político e líder militar e a situação caótica da política holandesa foram as razões do fracasso da campanha.[120][121]
Enquanto isso, sir Francis Drake realizou entre 1585 e 1586 uma grande viagem contra navios e portos espanhóis no Caribe, conseguindo atacar Cádis em 1587 e destruindo a frota espanhola de navios de guerra destinada para a Empreitada da Inglaterra.[119] Filipe havia decidido fazer guerra contra os ingleses.[122]
A Invencível Armada, uma grande frota de navios, partiu para o Canal da Mancha em 12 de julho de 1588 planejando levar uma força de invasão espanhola sob comando de Alexandre Farnésio, Duque de Parma e Placência, para a costa sul da Inglaterra a partir dos Países Baixos. Uma combinação de erros de cálculo,[nota 10] má sorte e um ataque inglês com navios de fogo em 29 de julho perto de Gravelines acabou dispersando os navios espanhóis para o nordeste e a Armada acabou sendo derrotada.[124] Ela voltou para a Espanha em restos despedaçados, após enormes perdas ao oeste da costa da Irlanda (alguns navios tentaram voltar para casa através do Mar do Norte, virando para o sul depois da costa irlandesa).[125] Milícias inglesas, sem saber do destino da Armada, reuniram-se para defender o reino sob o comando de Roberto Dudley. Ele convidou Isabel para inspecionar as tropas em Tilbury, Essex, no dia 8 de agosto. Usando uma armadura peitoral de prata sobre um vestido de veludo branco, ela dirigiu-se aos homens em um de seus discursos mais famosos:
A nação comemorou quando não houve nenhuma invasão. A procissão de Isabel para um serviço de ação de graças na Catedral de São Paulo rivalizou em espetáculo com aquela ocorrida em sua coroação.[125] A derrota da Armada foi também uma enorme vitória em propaganda, tanto para a rainha quanto para a Inglaterra protestante. Os ingleses consideraram o ocorrido como um símbolo da preferência divina e a inviolabilidade da nação sob uma rainha virgem.[109] Entretanto, a vitória não foi um ponto de virada na guerra, que prosseguiu e frequentemente favorecia a Espanha.[127] Os espanhóis ainda controlavam os Países Baixos e a ameaça de uma invasão continuou.[122] Sir Valter Raleigh afirmou após a morte de Isabel que a precaução dela impediu a guerra contra a Espanha:
Apesar de alguns historiadores terem criticado Isabel por razões semelhantes,[129] o veredito de Raleigh foi frequentemente considerado como injusto. A rainha tinha bons motivos para não confiar em seus comandantes, que uma vez em ação tendiam "a serem transportados com um tamento de vanglória", como ela mesma colocou.[130]
Quando o protestante Henrique III de Navarra herdou o trono da França em 1589, Isabel lhe enviou apoio militar. Foi sua primeira empreitada no país desde a retirada de Le Havre em 1563. A ascensão de Henrique foi muito contestada pela Liga Católica e por Filipe, com Isabel temendo que os espanhóis tomassem os portos franceses ao longo do canal. Entretanto, as campanhas seguintes da Inglaterra em território francês foram desorganizadas e ineficientes[131] Lorde Peregrine Bertie, 13.º Barão Willoughby de Eresby, ignorou as ordens da rainha e marchou para o norte da França com quatro mil homens, porém acabou realizando muito pouco. Ele recuou em desordem em dezembro de 1589, perdendo metade de suas tropas. A campanha de João Norreys em 1591 levou três mil homens a Bretanha, terminando em um desastre ainda maior. Isabel não queria investir em suprimentos e reforços como seus comandantes pediam por causa de tais expedições. Norreys foi para Londres pedir apoio a rainha pessoalmente. O exército da Liga Católica praticamente destruiu em maio de 1591 o restante de seu exército em Craon, noroeste da França, durante sua ausência. Isabel enviou outra força em julho sob o comando de Roberto Devereux, 2.º Conde de Essex, para ajudar Henrique no cerco a Ruão. O resultado foi outro desastre. Devereux não conseguiu realizar nada e voltou em janeiro de 1592. Henrique abandonou o cerco em abril seguinte.[132] Como sempre, a rainha não tinha controle sobre seus comandantes uma vez que eles estivessem no exterior. "Onde ele está, ou o que ele faz, ou o que ele fará", ela escreveu a Devereux, "somos ignorantes".[133]
Apesar da Irlanda ser um de seus reinos, Isabel enfrentava em certos lugares uma população hostil e até mesmo autônoma[nota 11] que aderia ao catolicismo e estava disposta a desafiar sua autoridade e conspirar com seus inimigos. Sua política na região era entregar terras a seus cortesãos e impedir que os rebeldes dessem a Espanha uma base de onde pudesse atacar a Inglaterra.[135] As forças da coroa utilizaram táticas de terra arrasada contra uma série de levantes, queimando a terra e chacinando homens, mulheres e crianças. Durante uma revolta liderada por Geraldo FitzGerald, 15.º Conde de Desmond, em Munster em 1582, por volta de trinta mil irlandeses morreram de fome. O poeta e colono Edmund Spenser escreveu que as vítimas "foram levadas a tal miséria como que qualquer coração de pedra teria lamentado o mesmo".[136] Isabel aconselhou seus comandantes que "aquela nação rude e bárbara" fosse bem tratada, porém não demonstrou remorso quando a força e derramamento de sangue foram necessários.[137]
Isabel enfrentou seu teste mais severo na Irlanda entre 1594 e 1603 durante a Guerra dos Nove Anos, uma guerra que aconteceu no ponto alto das hostilidades contra a Espanha, que apoiava o líder rebelde Hugo O'Neill, 2.º Conde de Tyrone.[138] Isabel enviou Roberto Devereux na primavera de 1599 para acabar com a revolta. Ele fez pouco progresso e voltou para a Inglaterra contra suas ordens, para a frustração da rainha.[nota 12] Devereux foi substituído por Carlos Blount, 8.º Barão Mountjoy, que precisou de três anos para derrotar os rebeldes. O'Neill finalmente se rendeu em 1603, alguns dias após a morte de Isabel.[139]
Isabel continuou a manter as relações diplomáticas que Eduardo VI havia estabelecido com o Czarado da Rússia. Ela frenquentemente escrevia ao imperador Ivã IV em termos amigáveis, apesar dele ficar frequentemente irritado por seu foco em comércio ao invés de uma possível aliança militar. Ivã até a pediu em casamento, também pedindo garantias durante a segunda metade do reinado de Isabel que recebesse asilo na Inglaterra caso seu reinado fosse colocado em risco. Seu simplório filho Teodoro I o sucedeu depois de sua morte. Diferentemente do pai, o novo imperador não queria manter direitos exclusivos de comércio com a Inglaterra. Ele declarou seu reino aberto a todos os estrangeiros, dispensando o embaixador inglês sir Jerônimo Bowes, cuja pomposidade havia sido tolerada por Ivã. Isabel enviou o dr. Giles Fletcher como novo embaixador para exigir que o regente Bóris Godunov convencesse Teodoro a reconsiderar. As negociações falharam pois Fletcher omitiu dois títulos ao dirigir-se a ele. A rainha continuou a falar com Teodoro em cartas meio suplicantes e meio reprovatórias. Ela propôs uma aliança, algo que sempre recusou com Ivã, mas nada adiantou.[140]
A Inglaterra desenvolveu relações diplomáticas e de comércio com Berbéria durante o reinado de Isabel.[141][142] Ela estabeleceu relações de comércio com o Marrocos em oposição a Espanha, vendendo armaduras, munição, madeira e metais em troca de açúcar, mesmo com uma proibição papal.[143] Abd el-Ouahed ben Messaoud, principal secretário de Amade Almançor Saadi do Marrocos, visitou a Inglaterra em 1600 como embaixador na corte para negociar uma aliança anglo-marroquina contra os espanhóis.[141] Isabel "concordou em vender munições e suprimentos aos Marrocos, e ela e Mulei Amade Almançor conversaram de vez em quando sobre montarem uma operação conjunta contra os espanhóis".[144] As discussões permaneceram inconclusivas, com os dois morrendo dois anos depois da visita de ben Messaoud.[142]
Também foram estabelecidas relações diplomáticas com o Império Otomano através do estabelecimento da Companhia de Levante e o envio em 1578 do primeiro embaixador à Sublime Porta, Guilherme Harborne.[144] Um tratado de comércio foi assinado pela primeira vez em 1580.[145] Os dois países mandaram vários enviados uns ao outro e trocas epistolares ocorreram entre Isabel e o sultão Murade III.[144] Ele expressou sua noção em uma das cartas que o islamismo e o protestantismo tinham "muito mais em comum que ambos tinham com o Catolicismo Romano, já que os dois rejeitavam a idolatria de ídolos", discutindo para uma aliança entre a Inglaterra e o Império Otomano.[146] Os inglês exportaram estanho e chumbo (para a criação de canhões) e munição, para o desalento da Europa católica, com Isabel discutindo seriamente com Murade operações militares conjuntas durante o início da guerra contra a Espanha em 1585, já que Francisco Walsingham estava fazendo lobby para um envolvimento otomano direto contra o inimigo em comum.[147]
O período após a derrota da Invencível Armada em 1588 trouxe novas dificuldades a Isabel que duraram pelos quinze últimos anos de seu reinado.[127] Os conflitos com a Espanha e Irlanda se arrastaram, os impostos ficaram mais pesados e a economia foi atingida por colheitas ruins e os custos das guerras. Os preços subiram e a qualidade de vida caiu.[148][149] A repressão contra os católicos se intensificou nessa época, com a rainha autorizando em 1591 comissões para monitorar e interrogar chefes de família católicos.[150] Ela dependia cada vez mais de espiões internos e propaganda para manter a ilusão de paz e prosperidade.[148] As críticas cada vez maiores refletiam o declínio da afeição pública por Isabel em seus últimos anos.[151]
Uma das causas para esse "segundo reinado", como é as vezes chamado,[152] foi a mudança da personalidade do Conselho Privado na década de 1590, o órgão de governo de Isabel. Havia uma nova geração no poder. Com a exceção de Guilherme Cecil, os políticos mais importantes do reino haviam morrido por volta de 1590: Dudley em 1588, Walsingham em 1590 e sir Cristóvão Hatton em 1591.[153] Brigas entre facções no governo, que não existiram de forma notória antes de 1590,[154] agora eram uma característica.[155] Surgiu uma grande rivalidade entre Devereux e Roberto Cecil, filho de Guilherme, com a disputa pelas posições mais poderosas no reino interferindo na política.[156] A autoridade pessoal da rainha estava diminuindo,[157] como foi demonstrado em 1594 pelo caso do dr. Lopez, seu médico. Quando ele foi erroneamente acusado de traição por Devereux em uma disputa pessoal, Isabel não conseguiu impedir sua execução, mesmo tendo ficado brava por sua prisão e aparentando não ter acreditado que ele era culpado.[158]
Isabel passou a depender da concessão de monopólios durante os últimos anos de seu reinado; era um sistema de patronagem de custo zero ao invés de pedir ao parlamento mais subsídios em tempos de guerra.[nota 13] A prática logo levou à fixação de preços, o enriquecimento de cortesãos aos custos públicos e grande indignação.[160] Isso culminou em 1601 com uma agitação na Câmara dos Comuns.[161] Em seu famoso "Discurso Dourado" de 30 de novembro de 1601 no Palácio de Whitehall para 140 membros, Isabel professou sua ignorância dos abusos e conquistou os presentes com promessas a o apelo usual às emoções:[162]
Entretanto, esse mesmo período de incerteza política produziu um florescimento literário insuperável na Inglaterra.[164] Os primeiros sinais de um novo movimento literário apareceram ao final da segunda década do reinado de Isabel, com Euphues de John Lyly e The Shepheardes Calender de Edmund Spenser em 1578. Alguns grandes nomes da literatura inglesa entraram em sua maturidade durante a década de 1590, incluindo William Shakespeare e Christopher Marlowe. O teatro inglês alcançou seu auge nesse período e no Período Jacobino que seguiu-se.[165] A noção de um grande Período Isabelino depende muito dos construtores, dramaturgos, poetas e músicos que estavam em atividade no reinado de Isabel. Deviam pouco diretamente à rainha, que nunca foi uma grande patrona das artes.[166]
A imagem de Isabel mudou gradualmente enquanto envelhecia. Ela foi retratada como Belphoebe e Astreia, e também como Gloriana, a eternamente jovem Rainha das Fadas do poema de Spenser, após a derrota da Invencível Armada. Seus retratos deixaram de ser realistas e passaram a ser um conjunto de ícones enigmáticos que a faziam parecer muito mais jovem que era. Na realidade, sua pele havia sido marcada e 1562 pela varíola, a deixando meia careca e dependente de perucas e cosméticos.[167] Sir Valter Raleigh a chamou de "uma senhora cujo tempo ultrapassou".[168] Entretanto, enquanto mais diminuía sua beleza, mais seus cortesãos a elogiavam.[167] 
Isabel gostava de representar o papel,[nota 14] porém é possível que ela passou a acreditar em sua própria interpretação na última década de sua vida. Ela se afeiçoou e ficou indulgente ao charmoso e petulante Roberto Devereux, que era sobrinho de Dudley e tomava certas liberdades com ela que acabavam sendo perdoadas.[170] Isabel repetidas vezes o nomeou para cargos militares apesar de seu histórico cada vez maior de irresponsabilidade. A rainha o colocou em prisão domiciliar depois de desertar em 1599 de seu comando na Irlanda, tirando seus monopólios no ano seguinte.[171] Devereux tentou armar uma rebelião em Londres em fevereiro de 1601 com a intenção de tomar posse de Isabel, porém não conseguiu reunir apoio e foi executado no dia 25 do mesmo mês. A rainha sabia que seus próprios erros de julgamento eram em parte responsáveis pelos acontecimentos. Como um observador relatou em 1602, "Seu prazer é sentar-se no escuro, e por vezes derramar lágrimas para lamentar Essex".[172]
Guilherme Cecil, 1.º Barão Burghley, o principal conselheiro de Isabel, morreu em 4 de agosto de 1598. Seu manto político foi passado ao filho Roberto Cecil, que logo tornou-se o líder do governo.[nota 15] Uma das tarefas que ele tomou conta foi preparar o caminho para uma sucessão tranquila. Cecil foi obrigado a trabalhar em segredo já que Isabel nunca nomeou um sucessor.[nota 16] Assim ele entrou em correspondências codificadas com o rei Jaime VI da Escócia, que tinha uma reivindicação forte mas não reconhecida.[nota 17] Cecil aconselhou o impaciente rei escocês a ser gentil com Isabel e "assegurar o coração da mais elevada, para cujo sexo e qualidade nada é assim inadequado quer como admoestações desnecessárias ou sobre muita curiosidade em suas próprias ações". O conselho funcionou. O tom de Jaime encantou a rainha, que respondeu: "Então confio que vós não duvidará que tuas últimas cartas são tão aceitas e tomadas como meus agradecimentos que não faltam à mesma, mas oferecei-los a vós de maneira grata".[174] Na visão do historiador J. E. Neale, Isabel pode não ter abertamente declarado seus desejos a Jaime, porém os fez conhecidos por meio de "frases inconfundíveis, senão veladas".[175]
A saúde da rainha permaneceu boa até o outono de 1602, quando uma série de mortes entre seus amigos a colocaram em uma grande depressão. A morte de Catherine Carey, Condessa de Nottingham e sobrinha de sua amiga Catarina Carey, em fevereiro de 1603 a atingiu severamente. Isabel adoeceu no mês seguinte e permaneceu em uma "melancolia assentada e irremovível".[176] Isabel morreu no dia 24 de março de 1603 no Palácio de Richmond entre às 2h e 3h da madrugada. Cecil e o conselho colocaram seus planos em movimento algumas horas depois e proclamaram Jaime VI da Escócia como Jaime I da Inglaterra.[177]
O caixão de Isabel foi carregado pelo rio Tâmisa em uma barca com tochas durante a noite até o Palácio de Whitehall. Seu funeral ocorreu no dia 28 de abril, com o caixão sendo levado até a Abadia de Westminster em um carro fúnebre puxado por quatro cavalos decorados com veludo preto. Nas palavras do crônico John Stow:
Isabel foi enterrada na Abadia de Westminster ao lado de sua meia-irmã Maria. A inscrição em latim da tumba, Regno consortes & urna, hic obdormimus Elizabetha et Maria sorores, in spe resurrectionis, se traduz para "Consortes em reino e tumba, aqui dormimos, Isabel e Maria, irmãs, na esperança de ressurreição".[179]
Isabel foi lamentada por muitos de seus súditos, porém outros ficaram aliviados por sua morte.[180] As expectativas para Jaime começaram altas porém caíram, então por volta da década de 1620 houve um reavivamento nostálgico do culto a Isabel.[181] Ela foi louvada como uma heroína da causa protestante e governante de uma era de ouro. Jaime era representado como um simpatizante católico que presidia sobre uma corte corrupta.[182] A imagem triunfalista que Isabel cultivou ao final de seu reinado, contra um fundo de dificuldades econômicas, faccionalistas e militares,[183] foi tomada como se realidade fosse e sua reputação foi inflada. Godofredo Goodman, Bispo de Gloucester, lembra: "Quando tivemos a experiência de um governo escocês, a Rainha parecia reviver. Então sua memória foi muito ampliada.[184] Seu reinado foi idealizado em uma época que a coroa, igreja e parlamento trabalhavam em equilíbrio constitucional.[185]
A imagem de Isabel retratada por seus admiradores protestantes no início do século XVII mostrou-se duradoura e influente.[186] Sua memória também foi reavivada durante a Guerras Napoleônicas, quando a nação encontrou-se novamente a beira de uma invasão.[185] Na Era Vitoriana, a lenda Isabelina foi adaptada para a ideologia imperial da época,[187][nota 18] e no meio do século XX ela era um símbolo romântico da resistência nacional contra uma ameaça estrangeira.[188] Alguns historiadores do período como J. E. Neale e A. L. Rowse interpretaram seu reino como uma época de ouro do progresso.[189] Neale e Rowse também idealizaram a rainha pessoalmente: ela sempre fez tudo corretamente; seus traços mais desagradáveis foram ignorados ou explicados como sinais de estresse.[190]
Entretanto, historiadores recentes assumiram uma visão mais complicada de Isabel.[129] Seu reinado é mais famoso pela derrota da Invencível Armada e por ataques bem sucedidos contra os espanhóis, como aqueles em Cádiz em 1587 e 1596, porém alguns historiadores salientam fracassos militares tanto em terra quanto no mar.[131] As forças de Isabel acabaram prevalecendo na Irlanda, porém suas táticas sujaram o registro.[191] Ela é mais frequentemente considerada como cautelosa em questões estrangeiras ao invés de uma corajosa defensora das nações protestantes contra a Espanha e os Habsburgo. Isabel ofereceu apenas apoio bem limitado a protestantes estrangeiros e não conseguiu prover fundos suficientes para seus comandantes fazerem a diferença internacionalmente.[192]
Isabel estabeleceu uma igreja inglesa que ajudou a moldar uma identidade nacional que permanece até hoje.[193][194] Aqueles que posteriormente a elogiaram como heroína protestante negligenciaram o fato dela ter se recusado a abandonar todas as práticas de origem católica na Igreja Anglicana.[nota 19] Historiadores perceberam que os protestantes fervorosos da época consideravam o Ato de Resolução e Uniformidade de 1559 como um compromisso.[196] Na realidade, a rainha acreditava que a fé era pessoal e não queria "criar janelas nos corações e pensamentos secretos dos homens", como Francis Bacon colocou.[197]
Apesar de Isabel ter seguido uma política internacional defensiva, seu reinado valorizou a Inglaterra no estrangeiro. "Ela é apenas uma mulher, apenas a senhora de meia ilha", afirmou o Papa Sisto V, "e mesmo assim se faz temida pela Espanha, pela França, pelo Império, por todos!"[198] Sob Isabel, a nação ganhou uma nova auto-confiança e senso de soberania, uma cristandade fragmentada.[194][199] A rainha foi a primeira Tudor a perceber que o monarca governa por consenso popular.[nota 20] Assim ela sempre trabalhou com o parlamento e conselheiros em quem confiava para lhe dizerem a verdade – uma forma de governo que seus sucessores Stuart falharam em seguir. Alguns historiadores a chamaram de sortuda.[198] Isabel acreditava que Deus a estava protegendo.[201] Orgulhando-se de ser "meramente inglesa", ela confiava em Deus, em conselhos honestos e no amor de seus súditos para governar. Em oração, agradeceu:
Jawaharlal Nehru (em hindi:  जवाहरलाल नेहरू, Javāharlāl Nehrū) (Allahabad, 14 de novembro de 1889 – Nova Délhi, 27 de maio de 1964[1]), também conhecido como Pandit (professor) Nehru ou Pandita Nehru, foi um estadista indiano, que foi o primeiro (e até hoje o de mandato mais longo) primeiro-ministro da Índia, desde 1947 até 1964. Líder da ala socialista no congresso nacional indiano durante e após o esforço da Índia para a independência do império britânico, tornou-se no primeiro-ministro da Índia na independência, de 15 de agosto de 1947 até sua morte.
Figura líder do movimento de independência indiano, Nehru foi eleito pelo Partido do Congresso para assumir o posto inaugural de primeiro-ministro da Índia independente, e reeleito quando Partido do Congresso ganhou a primeira eleição geral da Índia em 1952. Como um dos fundadores do Movimento Não Alinhado, foi também uma figura importante na política internacional do pós-guerra.
Filho de um rico advogado e político indiano, Motilal Nehru, Nehru tornou-se um líder da ala esquerda do Congresso Nacional Indiano, quando ainda bastante jovem.  Ascendendo até tornar-se presidente do Congresso, sob a orientação de Mahatma Gandhi, Nehru foi um líder carismático e radical, defendendo a independência completa em relação ao Império Britânico. Na longa luta pela independência da Índia, em que foi uma peça chave, Nehru foi finalmente reconhecido como herdeiro político de Gandhi. Ao longo de sua vida, Nehru foi também um defensor do socialismo fabiano e do setor público como o meio pelo qual os desafios de longa data do desenvolvimento econômico poderiam ser abordados pelas nações mais pobres.
Nehru foi educado na Grã-Bretanha, na escola independente para rapazes Harrow School e no  Colégio da Trindade, Cantabrígia.
Nehru teve a singular honra de levantar a bandeira da Índia independente em Nova Deli em 15 de agosto de 1947, no dia em que a Índia ganhou a independência. A valorização de Nehru das virtudes da democracia parlamentar, o secularismo e liberalismo, juntamente com as suas preocupações com os pobres e desfavorecidos, são reconhecidos como o que o guiou na formulação de políticas que influenciam a Índia até o presente.  Também refletem as origens socialistas da sua visão de mundo. Sua longa permanência foi fundamental na modelagem das tradições e das estruturas da Índia independente. Ele é muitas vezes referido como o "Arquiteto da Índia Moderna".[carece de fontes?] Sua filha, Indira Gandhi, e seu neto, Rajiv Gandhi, também exerceram o cargo de primeiros-ministros da Índia.
Gita Sahgal – uma escritora e jornalista que aborda feminismo, fundamentalismo e racismo, diretora de premiados documentários e ativista de direitos humanos – é sua sobrinha-neta.
Filho de Motilal Nehru, um destacado dirigente do Congresso, Jawaharlal regressou à Índia após formar-se na Universidade de Cambridge para exercer a advocacia antes de ser introduzido na política por seu pai, chegando a ser o braço-direito de Mohandas Gandhi e alcançando a presidência do Congresso pela primeira vez em 1929.
Preso 32 meses depois dos eventos de 1942, Nehru formou o primeiro governo hindu em julho de 1946, com a oposição da Liga Muçulmana que aspirava a criar um estado separado (o Paquistão), em 1947.
Como primeiro-ministro, Nehru inaugurou uma política exterior de não alinhamento, convertendo-se no fundador e dirigente desse movimento. No entanto, ao mesmo tempo também fez reivindicações territoriais que colocavam a Índia na posição de um império agressor e não de uma nação pacífica. Reivindicou a Caxemira apesar da oposição do Paquistão, o que desatou a primeira guerra entre os dois países (1947-49). Também anexou  Hiderabade em setembro de 1948 e retomou para a Índia o território de Goa, havia séculos ocupado por Portugal, em dezembro de 1961.  A invasão arranhou a imagem de pacifista que Nehru criara ao longo dos anos.  Em Portugal, passou a ser visto pela direita colonialista como pouco mais que um hipócrita.
Ainda assim, apesar das escaramuças com o Paquistão e a China (decorrente da crise da invasão do Tibete), Nehru tentou manter uma política de boa vizinhança com os países limítrofes.
Jawaharlal Nehru retornou à Índia em 1912 e iniciou a prática jurídica. Casou-se com Kamala Kaul através de um casamento arranjado por seus pais em 1916. Jawaharlal Nehru ingressou no All India Home Rule League (Liga em prol do Autogoverno da Índia Unida) em 1917.  Sua real iniciação na política veio dois anos depois, quando entrou em contato com Mahatma Gandhi, em 1919. Naquela época, Mahatma Gandhi tinha lançado uma campanha contra o Rowlatt Act. Nehru foi imediatamente atraído para o compromisso de Gandhi para uma ativa mas pacífica desobediência civil. Jawaharlal Nehru foi eleito presidente da Câmara Municipal de Allahabad em 1924 e serviu durante dois anos como executivo chefe da cidade.
De 1926 a 1928, Jawaharlal serviu como Secretário-Geral do Comitê do Congresso das Províncias Unidas. Em 1928-29, a sessão anual do Congresso sob a presidência de Motilal Nehru foi realizada. Durante essa sessão, Jawaharlal Nehru e Subhas Chandra Bose apoiaram um chamamento para a independência política completa, enquanto Motilal Nehru e outros queriam o status de domínio dentro do império britânico. Para resolver a questão, Gandhi propôs que os britânicos concedessem em dois anos o status de domínio à Índia, caso contrário o Congresso lançaria uma luta nacional pela independência política plena. Nehru e Bose reduziram aquele prazo para um ano. Os britânicos não responderam.
Em dezembro de 1929, a sessão anual do Congresso foi realizada em Lahore, e Jawaharlal Nehru foi eleito como presidente do Partido do Congresso. Durante a sessão, uma resolução exigindo a independência da Índia e foi aprovada em 26 de janeiro de 1930, em Lahore, tendo Jawaharlal Nehru agitado uma bandeira da Índia livre. E Gandhi fez o convite para o movimento de desobediência civil em 1930. O movimento finalmente forçou o governo britânico a reconhecer a necessidade de grandes reformas políticas e acabou por ser um grande sucesso.
Quando os britânicos promulgaram o Ato do Governo da Índia de 1935, o Partido do Congresso decidiu concorrer às eleições. Nehru ficou de fora das eleições, mas dirigiu uma forte campanha eleitoral de âmbito nacional para o partido. O Congresso formou governos em quase todas as províncias, e ganhou o maior número de cadeiras na Assembleia Central. Nehru foi eleito para a presidência do Congresso em 1936, 1937 e 1946, e chegou a ocupar uma posição no movimento nacionalista atrás apenas de Gandhi. Foi preso em 1942 durante o Movimento Índia Livre. Solto em 1945, assumiu um papel de liderança nas negociações que culminaram no surgimento dos domínios da Índia e do Paquistão em agosto de 1947, resultando na Partição da Índia.
Em 15 de janeiro de 1941 Gandhi disse: "Alguns dizem que Pandit Nehru e eu estávamos afastados. Vai ser necessário muito mais do que diferenças de opinião para nos afastarmos. Tivemos diferenças a partir do momento que nos tornamos colegas de trabalho e já tendo dito há alguns anos eu volto a dizer agora que não Rajaji (Chakravarthi Rajagopalachari), mas Nehru será o meu sucessor."[2]
Nehru admirava os planos quinquenais soviéticos e tentou aplicá-los à Índia.  Queria unir o melhor do capitalismo ao melhor do socialismo, criando um socialismo democrático, embora incapaz de tirar seu país das últimas posições entre os países em desenvolvimento.[carece de fontes?]
Sua única filha, Indira Gandhi, tornou-se primeira-ministra após a morte de Lal Bahadur Shastri, em janeiro de 1966.
Tomás de Aquino, em italiano Tommaso d'Aquino (Roccasecca, 1225 – Fossanova, 7 de março de 1274), foi um frade católico italiano da Ordem dos Pregadores (dominicano)[2][3] cujas obras tiveram enorme influência na teologia e na filosofia, principalmente na tradição conhecida como Escolástica, e que, por isso, é conhecido como "Doctor Angelicus", "Doctor Communis" e "Doctor Universalis".[4][a] "Aquino" é uma referência ao condado de Aquino, uma região que foi propriedade de sua família até 1137.
Ele foi o mais importante proponente clássico da teologia natural e o pai do tomismo. Sua influência no pensamento ocidental é considerável e muito da filosofia moderna foi concebida como desenvolvimento ou oposição de suas ideias, particularmente na ética, lei natural, metafísica e teoria política. Ao contrário de muitas correntes da Igreja na época,[5] Tomás abraçou as ideias de Aristóteles - a quem ele se referia como "o Filósofo" - e sintetizou a filosofia aristotélica com os princípios do cristianismo. As obras mais conhecidas de Tomás são a "Suma Teológica" (em latim: Summa Theologiae) e a "Suma contra os Gentios" (Summa contra Gentiles). Seus comentários sobre as Escrituras e sobre Aristóteles também são parte importante de seu corpus literário. Além disso, Tomás se distingue por seus hinos eucarísticos, que ainda hoje fazem parte da liturgia da Igreja.[6]
Tomás é venerado como Santo pela Igreja Católica e é tido como o professor modelo para os que estudam para o sacerdócio por ter atingido a expressão máxima tanto da razão natural quanto da teologia especulativa. O estudo de suas obras há muito tempo tem sido o cerne do programa de estudos obrigatórios para os que buscam as ordens sagradas (como padres e diáconos) e também para os que se dedicam à formação religiosa em disciplinas como filosofia católica, teologia, história, liturgia e direito canônico.[7] Tomás foi também proclamado Doutor da Igreja por Pio V em 1568. Sobre ele, declarou Bento XV:
Tomás nasceu em Roccasecca, no condado de Aquino do Reino da Sicília (atualmente na região do Lácio, na Itália) por volta de 1225. De acordo com alguns autores, nasceu no castelo de seu pai, Landulfo de Aquino, que não pertencia ao ramo mais poderoso de sua família e era apenas um miles ("cavaleiro"). Já a mãe de Tomás, Teodora, era do ramo Rossi da família napolitana dos Caracciolo.[9] Enquanto o resto da família dedicou-se à carreira militar,[10] seus pais pretendiam que Tomás seguisse o exemplo do irmão de Landulfo, Sinibaldo, que era abade do mosteiro beneditino de Monte Cassino,[11] uma carreira perfeitamente normal para o filho mais jovem de uma família nobre do sul da Itália da época.[12]
Aos cinco anos de idade, começou a estudar em Monte Cassino, mas, depois que o conflito militar entre o imperador Frederico II e o papa Gregório IX chegou à abadia no início de 1239, Landulfo e Teodora matricularam o pequeno Tomás no studium generale (a universidade em Nápoles), recém-criada por Frederico em Nápoles.[13] Foi provavelmente lá que Tomás foi introduzido aos estudos de Aristóteles, Averróis e Maimônides, importantes influências para sua filosofia teológica.[14] Seu professor de aritmética, geometria, astronomia e música era Pedro da Ibérnia.[15]
Foi também durante seus estudos em Nápoles que acabou sob a influência de João de São Juliano, um pregador dominicano que era parte do grande esforço empreendido pela Ordem dos Pregadores para recrutar seguidores.[16] Finalmente, aos dezenove, Tomás resolveu se juntar à ordem, o que não agradou sua família.[17] Numa tentativa de impedir que Teodora influenciasse a escolha de Tomás, os dominicanos arranjaram para que ele se mudasse para Roma e, de lá, para Paris.[18] Porém, durante a viagem para Roma, seguindo as instruções de Teodora, seus irmãos o capturaram quando ele bebia num riacho e o levaram de volta para seus pais no castelo de Monte San Giovanni Campano.[18]
Ficou preso por cerca de um ano nos castelos da família em Monte San Giovanni e Roccasecca, numa tentativa de fazê-lo mudar de ideia.[14] Preocupações políticas impediram que o papa interviesse em defesa de Tomás, aumentando significativamente o tempo que ficou preso.[19] Durante este período de provações, Tomás ensinou suas irmãs e escreveu para seus irmãos dominicanos,[14] Desesperados com a teimosia de Tomás, dois de seus irmãos chegaram a ponto de contratarem uma prostituta para seduzi-lo.  De acordo com a lenda, Tomás a expulsou com um ferro em brasa e, durante a noite, dois anjos apareceram para ele enquanto ele dormia para fortalecer sua determinação de permanecer celibatário.[20]
Em 1244, percebendo que todas suas tentativas de dissuadir Tomás fracassaram, Teodora tentou salvar a dignidade da família e arranjou para ele escapasse durante uma noite pela janela. Ela acreditava que uma fuga secreta da prisão era menos prejudicial que uma rendição aberta aos dominicanos. Tomás seguiu primeiro para Nápoles e, depois, para Roma, onde se encontrou com João de Wildeshausen, o mestre-geral da Ordem dos Pregadores.[21]
Em 1245, Tomás foi enviado para estudar na faculdade das artes da Universidade de Paris, onde é muito provável que ele tenha encontrado o estudioso dominicano Alberto Magno (que seria depois proclamado Doutor da Igreja como Aquino), que era na época o catedrático da cadeira de Teologia do Colégio de São Tiago em Paris.[22] Quando Alberto foi enviado por seus superiores para ensinar no novo studium general em Colônia, em 1248, Tomás foi junto[23] depois de recusar uma oferta do papa Inocêncio IV de nomeá-lo abade de Monte Cassino mesmo sendo dominicano.[11] Em seguida, Alberto nomeou o relutante Tomás magister studentium.[12] Por ser calado e não falar muito, alguns dos companheiros de Tomás acreditavam que ele era "devagar", ao que Alberto rebateu, profeticamente, "Vocês o chamam de boi mudo, mas em sua doutrina ele produzirá um dia um mugido tal que será ouvido pelo mundo afora".[11]
Tomás lecionou em Colônia como professor aprendiz (baccalaureus biblicus), instruindo seus alunos nos livros do Antigo Testamento e escrevendo "Comentário Literal sobre Isaías" ("Expositio super Isaiam ad litteram"), "Comentário sobre Jeremias" ("Postilla super Ieremiam") e "Comentário sobre as Lamentações" ("Postilla super Threnos").[24] Então, em 1252, Tomás retornou para Paris para tentar obter o mestrado em teologia e passou a ensinar estudos bíblicos como professor aprendiz. Quando tornou-se "baccalaureus Sententiarum" ("bacharel das Sentenças"),[25] dedicou seus três anos finais de estudo a comentar sobre as "Sentenças" de Pedro Lombardo. Na primeira de suas quatro sínteses teológicas, Tomás compôs um enorme comentário sobre elas chamado "Comentário sobre as Sentenças" ("Scriptum super libros Sententiarium"). Além destas obras de seu mestrado, Tomás escreveu ainda "Sobre o Ser e a Essência" ("De ente et essentia") para seus companheiros dominicanos de Paris.[11]
Na primavera de 1256, Tomás foi nomeado regente principal em teologia em Paris e uma de suas primeiras obras depois de assumir o cargo foi "Contra Aqueles que Ameaçam a Devoção a Deus e a Religião" ("Contra impugnantes Dei cultum et religionem") defendendo as ordens mendicantes que estavam na época sob ataque por Guilherme de Saint-Amour.[26] Durante seu mandato, que foi de 1256 até 1259, Tomás escreveu diversas obras, incluindo: "Questões em Disputa sobre a Verdade" ("Questiones disputatae de veritate"), uma coleção de vinte e nove questões controversas sobre aspectos da fé e da condição humana[27] preparada para os debates universitários públicos que ele presidia na Quaresma e no Advento;[28] "Quaestiones quodlibetales", uma coleção de suas respostas às questões propostas pela audiência acadêmica nos debates[27] e o par "Comentários sobre 'De trinitate' de Boécio" ("Expositio super librum Boethii De trinitate") e "Comentário sobre 'De hebdomdibus' de Boécio" ("Expositio super librum Boethii De hebdomadibus"), comentários sobre as obras do filósofo romano do século VI Boécio.[29] Quando terminou sua regência, Tomás estava trabalhando numa de suas obras-primas, a "Suma contra os Gentios".[30]
Em 1259, Tomás completou sua primeira regência no studium generale e deixou Paris para que outros pudessem obter a mesma experiência. Retornando para Nápoles, foi nomeado pregador geral pelo capítulo provincial da ordem em 29 de setembro de 1260. Em setembro do ano seguinte, foi enviado a Orvietto como leitor conventual responsável pela formação dos frades que não podiam frequentar um studium generale. Lá, completou a "Suma contra os Gentios", escreveu "A Corrente de Ouro" (Catena aurea)[31] e escreveu obras para o papa Urbano IV, como a liturgia para a recém-criada festa de Corpus Christi e "Contra os Erros dos Gregos" ("Contra Errores Graecorum").[30]
Em fevereiro de 1265, o recém-eleito papa Clemente IV convocou Tomás de Aquino a Roma para servir como teólogo papal. No mesmo ano, foi ordenado pelo capítulo dominicano de Agnani[32] para ensinar no studium conventuale do Convento de Santa Sabina, fundado alguns anos antes, em 1222.[33] O studium em Santa Sabina rapidamente tornou-se um experimento para os dominicanos, o primeiro studium provinciale, uma escola intermediária entre o studium conventiale (restrito aos residentes das  casas monásticas - mosteiros e conventos) e o studium generale (as universidades nas grandes cidades). Antes disso, não havia na província de Roma nenhuma forma de educação especializada, apenas as escolas conventuais, com cursos básicos de teologia para os frades residentes, e ainda assim apenas na Toscana.[34] Tolomeo da Lucca, um parceiro e um dos primeiros biógrafos de Aquino, conta que, no studium de Santa Sabina, Aquino ensinou uma ampla de temas filosóficos, morais e naturais.[35]
Foi em Santa Sabina que Tomás começou a escrever sua obra mais famosa, a "Suma Teológica",[31] que ele concebeu como sendo mais adequada especificamente aos estudantes em seus primeiros anos: 
"Pois um doutor da verdade católica deveria ensinar não apenas os proficientes, mas a ele cabe também instruir os iniciantes. Como diz o apóstolo em I Coríntios 3:2 'Leite vos dei a beber, não vos dei comida; porque ainda não podíeis', nossa intenção com esta obra é apresentar tudo sobre a religião cristã de uma forma pertinente à instrução de iniciantes".[36] Ele escreve em Orvietto também uma variedade de outras obras, como o incompleto "Compêndio Teológico" ("Compendium Theologiae") e "Resposta ao Irmão João de Vercelli sobre os 108 Artigos Retirados da Obra de Pedro de Tarentaise" ("Responsio ad fr. Ioannem Vercellensem de articulis 108 sumptis ex opere Petri de Tarentasia").[29] Em sua posição de superior do studium, Aquino conduziu uma série de importantes debates sobre o poder de Deus que compilou depois em sua "Do Poder" ("De potentia").[37] Nicholas Brunacci (1240-1322) estava entre os alunos de Aquino em Santa Sabina e, depois, em Paris. Em novembro de 1268, ele estava com Aquino e seu parceiro e secretário, Reginaldo de Piperno, quando deixaram Viterbo a caminho de Paris para o início do ano acadêmico.[38] Outro aluno em Santa Sabina foi o beato Tommasello de Perúgia.[39]
Aquino permaneceu no studium de Santa Sabina de 1265 até ser chamado de volta a Paris em 1268 para uma segunda regência.[37] Com o tempo, principalmente depois de sua partida, as atividades pedagógicas no studium provinciale de Santa Sabina foram divididos em dois campi. Um novo convento da ordem na Igreja de Santa Maria sopra Minerva começou de forma modesta em 1255 como uma comunidade de mulheres recém-convertidas, mas cresceu rapidamente em tamanho e em importância de ser entregue aos cuidados dos frades dominicanos em 1275.[33] Em 1288, o componente teológico do currículo provincial para a educação dos frades foi realocado do studium provinciale de Santa Sabina para o studium conventuale de Santa Maria sopra Minerva, que foi então rebatizado como um studium particularis theologiae.[40] No século XVI, este studium foi transformado no Colégio de São Tomás (em latim: Collegium Divi Thomæ) e, no século XX, o colégio foi transferido para o Convento de Santi Domenico e Sisto e transformado na Pontifícia Universidade São Tomás de Aquino (o famoso "Angelicum").
Em 1268, a Ordem dos Pregadores nomeou Tomás para ser o regente mestre da Universidade de Paris pela segunda vez, uma posição que ele manteve até a primavera de 1272. Parte da razão para esta súbita transferência parece ter sido a ascensão do "averroísmo", conhecido também como "aristotelismo radical", nas universidades. Como resposta a estes aparentes malefícios, Tomás escreveu duas obras. Na primeira, "Sobre a Unidade do Intelecto, Contra os Averroístas" ("De unitate intellectus, contra Averroistas"), ele ataca o averroísmo como sendo incompatível com a doutrina cristã.[41] Foi durante a segunda regência que Aquino terminou a segunda parte da "Suma" e escreveu "Dos Virtuosos" ("De virtutibus") e "Da Eternidade do Mundo" ("De aeternitate mundi"),[37] esta tratando do controverso conceito aristotélico e averroísta sobre a "falta de começo" do mundo.[42]
Diversas controvérsias com alguns importantes franciscanos, como Boaventura e João Peckham, ajudaram a tornar a segunda regência muito mais difícil e conturbada que a primeira. Um ano antes de tomar posse novamente, nos debates de 1266-67 em Paris, o mestre franciscano Guilherme de Baglione acusou Tomás de encorajar os averroístas, chamando-o de "líder cego dos cegos". Tomás chamou-os de "murmurantes" ("resmungões").[42] Na realidade, ele ficou profundamente perturbado pela disseminação do averroísmo e se enfureceu quando soube que Siger de Brabante estava ensinando interpretações averroístas de Aristóteles aos seus alunos em Paris.[43]
Em 10 de dezembro de 1270 o bispo de Paris, Etienne Tempier, publicou um édito condenando treze proposições aristotélicas e averroístas como sendo heréticas e excomungou os que continuavam a defendê-las.[44] Muitos na comunidade eclesiástica, os chamados "agostinianos", temiam que a introdução do aristotelismo e sua versão mais extrema, o averroísmo, pudesse de alguma forma contaminar a pureza da fé cristã. No que parece ter sido uma tentativa de conter o temor contra o pensamento aristotélico, Tomás conduziu uma série de debates entre 1270 e 1272, reunidos em "Sobre as Virtudes em Geral" ("De virtutibus in communi", "Sobre as Virtudes Cardinais" ("De virtutibus cardinalibus") e "Sobre a Esperança" ("De spe").
Em 1272, Tomás pediu licença da Universidade de Paris quando os dominicanos de sua província natal o convocaram para fundar um studium general onde quisesse, com liberdade para empregar nele quem desejasse. Aquino escolheu Nápoles e se mudou para lá para assumir o posto de regente mestre.[37] Ele aproveitou esta temporada ali para trabalhar na terceira parte da "Suma" enquanto dava aulas sobre vários tópicos religiosos. Em 6 de dezembro de 1273, Tomás se demorou um pouco mais na Capela de São Nicolau do convento dominicano de Nápoles e foi visto pelo sacristão Domenic de Caserta levitando aos prantos em oração diante de um ícone de Cristo crucificado. Segundo o relato, Cristo perguntou: "Escrevestes bem sobre mim, Tomás. Que recompensa esperas pelo teu trabalho?" A resposta foi: "Nada além de ti, Senhor"
[45][46]. Depois desta conversa, algo mudou, mas Aquino jamais falou ou escreveu sobre o tema. Depois de ver o que viu, ele abandonou sua rotina e parou de ditar para seu secretário, Reginaldo de Piperno. Quando este implorou-lhe que voltasse ao trabalho, Tomás lhe disse: "Reginaldo, não posso, pois tudo o que escrevi não passa de uma palha para mim"[47] ("mihi videtur ut palea").[48] Seja o que for que tenha despertado a mudança no comportamento de Tomás de Aquino, os católicos acreditam que foi alguma espécie de experiência sobrenatural de Deus.[49]
Anos antes, em 1054, o Grande Cisma dividiu a Igreja entre a Igreja Latina, sob a liderança do papa (posteriormente conhecida como Igreja Católica Romana), no ocidente e os quatro patriarcados do oriente (conhecidos coletivamente como Igreja Ortodoxa). Numa tentativa de reunir as duas partes, o papa Gregório X convocou o Segundo Concílio de Lyon em 1 de maio de 1274 e ordenou que Tomás comparecesse[50] para apresentar sua obra "Contra os Erros dos Gregos" ("Contra Errores Graecorum").[51] A caminho do concílio, montado num burro enquanto viajava pela Via Ápia,[50] bateu a cabeça num galho de uma árvore tombada, ficou seriamente ferido e foi levado às pressas para Monte Cassino para se recuperar.[52] Depois de descansar por um tempo, tentou novamente seguir viagem, mas teve que parar, doente novamente, na abadia cisterciense de Fossanova.[53] Os monges tentaram ajudá-lo por diversos dias, mas ele não resistiu. Quando recebeu sua extrema unção, as últimas palavras de Aquino foram:
São Tomás de Aquino morreu em 7 de março de 1274[53] enquanto ditava seus comentários sobre o Cântico dos Cânticos.[55]
Tomás era um teólogo e filósofo escolástico.[56] Porém, ele jamais se considerou filósofo e os criticava por acreditar que eram pagãos que estavam sempre "aquém da verdadeira e correta sabedoria encontrada na revelação cristã".[57]
Mesmo assim, ele tinha muito respeito por Aristóteles, tanto que, na "Suma", geralmente cita-o simplesmente como "o Filósofo". Boa parte desta obra trata de tópicos filosóficos e, neste sentido, pode ser considerada filosófica. O fato é que o pensamento de Tomás exerceu uma enorme influência sobre a teologia cristã subsequente, especialmente a da Igreja Católica, mas também para toda a filosofia ocidental em geral.
Segundo a filosofia de Tomás de Aquino, o homem não vive por acaso, a vida humana tem um propósito que é a felicidade, porém o homem precisa conhecer os meios adequados para a sua posse. A felicidade para ele parte do princípio de que as riquezas materiais seriam a falsa noção da felicidade, pois a riqueza não tem consciência existencial em si mesma, e a razão de ser está fora dela mesma. O estado da felicidade parte do estado de espírito em que o homem se encontra. O indivíduo tem que conhecer seu eu interior antes de partir em busca dos meios adequados para a posse da mesma. Contudo, deve-se considerar que o estado da felicidade não é eterno, e uma vez encontrada, nada impede de se perdê-la, para assim então iniciar-se uma nova busca até o fim da vida humana.
Em todo momento o homem busca a felicidade, e muitos ligam a felicidade à posse de bem materiais, mas para Tomás de Aquino a ideia de felicidade vai muito além disso. Ela é o guia necessário para a vida (alma) do homem. Na vida corriqueira, com o stress do dia a dia, o homem acaba abrindo mãos dos pequenos detalhes que possivelmente trariam a felicidade, em busca da materialização para supri-la.[58]
Aquino escreveu diversos comentários importantes sobre as obras de Aristóteles, incluindo "Da Alma", "Ética a Nicômaco" e "Metafísica". Estes trabalhos estão associados com as traduções para o latim feitas por William de Moerbeke.
Ele quis organizar os argumentos e elementos racionais da filosofia Aristotélica para defender as verdades cristãs. Seu principal objetivo nesse aspecto foi o de não contrariar a fé. 
Através das traduções de textos de Aristóteles feitas pelos filósofos da idade média, inclusive o árabe "Averróis", Tomás transforma parte do pensamento aristotélico em ferramenta argumentativa para expandir  seu pensamento e empreender  uma sistematização da doutrina cristã. Embora haja em sua filosofia pontos que não são pensamentos aristotélicos, tal qual a ideia de um Deus único que o vir-a-ser, não é autodeterminação, mas precede de Deus. 
Aquino  introduziu uma distinção entre o ser e a essência. Dividiu a metafísica em Essência do Ser geral e Essência do Ser pleno que é Deus. Também definiu seu conceito de "Metafísica" - segundo ele uma tríplice: Metafísica enquanto ciência do ente, ciência divina e "filosofia". Enquanto a primeira investiga as primeiras causas, a "Metafísica" tomista leva o homem inevitavelmente a "Deus", por meio de um caminho racional, coerente e demonstrável.  
O Ser é diferente da essência, pois as criaturas são seres não necessários. É Deus que permite às essências realizarem-se em entes, em seres existentes. Que existe como fundamento da realidade das outras essências que, uma vez existentes participam de seu Ser. Deus é ato puro, não há o que se realizar ou se atualizar em Deus, pois ele é completo. ' '"Deus é o Ser"' ', diz Tomás de Aquino, Deus é o ser que existe como fundamento da realidade das outras essências que, uma vez existentes participam de seu Ser. A filosofia de Aristóteles não fala sobre um Deus criador, como o compreendemos, tirando o mundo do nada, nem fala da questão sobre a providência divina, Deus para Aristóteles não conhece o mundo, não o dirige de nenhum modo. Mas o argumento usado por Tomás de Aquino, usando o princípio da lógica aristotélica, diz que é indispensável a existência de um Motor imóvel primordial responsável por impulsionar a alvorada da criação, assim sendo, Deus se faz necessário.[59]  
O filósofo sempre procurou conciliar fé e razão em seus escritos, valendo-se, várias vezes, de  ensinamentos de Aristóteles e de Santo Agostinho, para afirmar que a graça e a fé não suprimem a natureza racional do homem, senão antes a supõe e a aperfeiçoa e, a partir disso, também sustentar que é possível a conciliação de filosofia (ratio - razão) e teologia (fides - fé), na medida em que para Aquino, a filosofia é serva da teologia: philosophia ancilla theologiae est.. 
Aristóteles foi a figura que mais influenciou no pensamento de Santo Tomás de Aquino, ele afirmava que o universo sempre existiu e que permanecia em movimento e mudanças constantes. Alguns pensadores cristãos, baseando-se na Bíblia diziam que o universo tinha um início e que havia sido criado por Deus, discordando assim da concepção de Aristóteles. Apesar de defender as ideias de Aristóteles, ele discordava do fato do mesmo afirmar que o universo era eterno, porque a fé cristã dizia ao contrário.[60][61]
Aquino acreditava "que para o conhecimento de qualquer verdade, o homem precisa da ajuda divina; que o intelecto pode ser movido por Deus a agir".[62] Porém, ele acreditava também que os seres humanos tinham a capacidade natural de conhecer muitas coisas sem nenhuma revelação divina especial, apesar de revelações ocorrerem de quando em quando "especialmente em relação àquelas [verdades] pertinentes à fé".[63] Mas esta é a luz dada ao homem por Deus na proporção da natureza humana: "Agora todas as formas concedidas às coisas criadas por Deus tem poder para determinadas ações, que podem realizar na medida de sua própria dotação; e além disto, são impotentes, exceto por meio de uma forma adicionada, como água que só esquenta quando aquecida pelo fogo. E assim a compreensão humana tem uma forma, viz. luz inteligível, que, por si só, é suficiente para conhecer certas coisas inteligíveis, viz. as que se pode aprender através dos sentidos".[63]
A ética de Tomás de Aquino se baseia no conceito dos "princípios primeiros da ação".[64] Na "Suma", ele escreveu:
Mais adiante, ele completa:
De acordo com ele, "...todos os atos da virtude são prescritos pela lei natural: como a razão de cada um naturalmente dita que ele aja virtuosamente. Mas se falarmos de atos virtuosos considerados em si mesmos, ou seja, em suas próprias espécies, segue que nem todos os atos virtuosos são prescritos pela lei natural: pois muitas coisas são realizadas virtuosamente, mas cuja natureza não se inclinava para inicialmente; mas que, pelo inquérito da razão, foram percebidas pelos homens como condutivas ao bem estar". A conclusão é que é necessário determinar se estamos falando de atos virtuosos sob o aspecto das virtudes ou como um ato per se, em sua própria espécie.[68]
Tomás definiu as quatro virtudes cardinais como sendo prudência, temperança, justiça e coragem (ou "fortaleza"). Segundo ele, elas são naturais, reveladas na natureza e inerentes a todos. Há, porém, três virtudes teológicas: fé, esperança e caridade. Estas, por outro lado, são algo sobrenaturais e distintas das demais em seu objeto: Deus. Segundo o próprio Aquino:
Avançando o raciocínio, Tomás distingue quatro tipos de lei que governam os atos humanos: eterna, natural, humana e divina. "Lei eterna" é o decreto divino que governa toda criação, a "lei que é a Razão Suprema e não pode ser compreendida senão como algo imutável e eterno".[70] "Lei natural" é a "participação" humana na "lei eterna" descoberta pela razão[71] e baseada nos "princípios primeiros": "...este é o primeiro preceito da lei, que o bem deve ser feito e promovido e o mal, evitado. Todos os demais preceitos da lei natural se baseiam neste...".[72] Se a lei natural contém vários preceitos ou apenas este, o próprio Aquino esclarece: "todas as inclinações de quaisquer partes da natureza humana, como por exemplo as partes concupiscentes e irascíveis, na medida em que são governadas pela razão, pertencem à lei natural e se reduzem ao primeiro preceito, como afirmando acima: pois os preceitos da lei natural são muitos em si próprios, mas são todos baseados numa fundação comum".[73]
O desejo de viver e procriar são considerados por Tomás entre os valores básicos (naturais) do homem, sobre os quais todos os demais valores humanos estão baseados. De acordo com Tomás, todas as tendências humanas estão aparelhadas o "bem" real humano. E no caso destes dois desejos, a natureza humana em questão é o matrimônio, o presente completo de uma pessoa a outra que assegura uma família às crianças e um futuro à humanidade.[74] Para os cristãos, Tomás definia que o amor era "desejar o 'bem' de outro".[75]
Sobre a "lei humana", Aquino conclui "...que, assim como no caso da razão especulativa, na qual tiramos conclusões em várias ciências a partir de princípios não demonstráveis e naturalmente conhecidos, conclusões estas não comunicadas a nós pela natureza, mas adquiridas pelos esforços da razão, é assim também com os preceitos da lei natural, pois a partir de princípios gerais e indemonstráveis, a razão humana precisa avançar para uma determinação mais precisa de certos assuntos. Estas determinações particulares, criadas pela razão humana, são chamadas de leis humanas desde que as outras condições essenciais da lei sejam observadas...", ou seja, a "lei humana" é a lei positiva, a lei natural aplicada pelos governos às sociedades.[76]
Leis naturais e humanas não são adequadas sozinhas. A necessidade humana de que seu comportamento seja dirigido fez necessária a existência da "lei divina", que é a lei especificamente revelada nas Escrituras. Segundo Aquino, "O apóstolo diz: «Pois mudado que seja o sacerdócio, é necessário que se faça também mudança da Lei.» (Hebreus 7:12) Mas o sacerdócio tem duas facetas, como afirmado na própria passagem, viz., os sacerdócio levita e o sacerdócio de Cristo. Portanto, a lei divina tem também duas facetas, a Antiga Lei e a Nova Lei".[77]
Aquino se refere aos animais como estúpidos e que a ordem natural declarou que eles foram criados para uso humano. Ele negava que os homens tinham qualquer dever de caridade para com os animais por não serem eles "pessoas". Se não fosse assim, seria ilegal utilizá-los como fonte de alimento. Porém, este racional não dava aos homens permissão para serem cruéis com eles, pois "hábitos cruéis podem transbordar para o nosso tratamento dos seres humanos".[78][79]
Ainda tratando de ética e justiça, Aquino deu grandes contribuições para o pensamento econômico medieval. Ele tratou do conceito de preço justo, normalmente o preço de mercado ou o regulamentado e suficiente para cobrir o custo de produção do vendedor. Ele argumentava que era imoral para os vendedores aumentarem os preços simplesmente por que os compradores estavam em algum momento precisando demais do produto.[80][81]
São Tomás de Aquino entende a lei como “uma ordenação da razão no sentido do bem comum, promulgada por quem dirige a comunidade”. É a partir desse conceito de “comunidade” que a filosofia jurídico-tomista se desenvolve (análoga a bilateralidade sustentada por Aristóteles) e que o frade dominicano diferenciou diversas manifestas de lex. No campo mais alto estaria a lex aeterna, a razão divina que governa todo o universo, sendo o fim deste. A forma revelada dela através da iluminação (conceito platônico aprofundado pela escola agostiniana) seria a lex divina, dada por Deus, como seria o caso das escrituras presentes na Bíblia. A participação do homem sob a lex aeterna, a partir de sua razão fornecida pelo Criador, seria o elemento fundante da lex naturalis, uma lei onde o homem é capaz de ter conhecimento do que deve ser feito a partir de um fator inerente a ele (recta ratio). Há ainda a lex humana que deriva-se da lei natural a partir da criação humana, seja de forma dedutiva ou por determinação de uma autoridade humana, a partir da tentativa de trazer a lex naturalis para um caso concreto. Tal concepção jurídica, portanto, consiste em uma forma de corrente, onde um elo leva, necessariamente, ao outro. A grande diferenciação entre a concepção jurídica medieval e a antiga está justamente na relação do Direito e da Moral: Na Antiguidade, o direito se legitimava pela moral; Na Idade Média, a moral se justifica por assumir características jurídicas, fruto da interiorização do direito vindo do Legislador supremo (Deus).[82]
Tomás via a teologia (a "doutrina sagrada") como uma ciência[49] cuja matéria-prima eram as Escrituras e a tradição da Igreja Católica. Estas fontes, por sua vez, seriam, segundo ele, produtos da auto-revelação de Deus a indivíduos ou grupos de indivíduos através da história. Finalmente, fé e razão, distintas e relacionadas, seriam as duas ferramentas primárias para processar os dados teológicos. Ele acreditava que ambas eram necessárias- ou, melhor, que a "confluência" de ambas era necessária - para obter-se o verdadeiro conhecimento de Deus. O objetivo final da teologia, para Tomás, era utilizar a razão para perceber a verdade sobre Deus e experimentar a salvação através desta verdade.
Aquino acreditava que a verdade é conhecida pela razão ("revelação natural") e pela fé ("revelação sobrenatural"). Esta tem sua origem na inspiração pelo Espírito Santo e está disponível através do ensinamento dos profetas, reunidos nas Escrituras e transmitidos pelo magisterium, coletivamente chamado de "tradição". Já a revelação natural é a verdade disponível a todos através da natureza humana e dos poderes da razão, por exemplo aplicando métodos racionais para perceber a existência de Deus.
Assim, apesar de se poder deduzir a existência e os atributos de Deus através da razão, certas especificidades só podem ser conhecidas através da revelação especial de Deus em Jesus Cristo. Os principais componentes teológicos do cristianismo, como a Trindade e a Encarnação, são revelados nos ensinamentos da Igreja e nas Escrituras; não podem, portanto, ser deduzidos pela razão humana.
Como católico, Aquino acreditava que Deus é o "criador do céu e da terra, de todas as coisas visíveis e invisíveis"; como Aristóteles, defendia que a vida poderia se formar a partir de matéria não viva ou de plantas, uma forma de abiogênese conhecida como "geração espontânea":
Além disso, Tomás defendia - em seu comentário sobre a "Física", de Aristóteles - a teoria de Empédocles de que várias mutações das espécies emergiram ainda durante a Criação. Ele argumenta que estas espécies foram geradas através de mutações no esperma animal de forma não esperada pela natureza. Para estas espécies, simplesmente não havia intenção de que tivessem existência perpétua:
Santo Agostinho concordava fortemente com o senso comum de sua época, de que os cristãos deveriam ser pacifistas filosoficamente, mas que deviam utilizar a força como meio de preservar a paz no longo prazo. Ele argumentou muitas vezes que o pacifismo não era contrário à defesa dos inocentes ou à auto-defesa, por exemplo. Resumidamente, Agostinho acreditava que, para a preservação da paz no longo prazo, o uso justificado da força poderia ser necessário,[85] mas estabelecia limites para isso, exigindo, por exemplo, que as guerras com esta finalidade deveriam ser defensivas e ter a restauração da paz (e não a conquista de vantagens) como objetivo.[86]
Aquino, séculos depois, aproveitou-se da autoridade dos argumentos de Agostinho quando tentou definir as condições para que uma guerra fosse considerada justa,[87] resumidos na "Suma":
Segundo São Tomás de Aquino, aqueles que resistem a um governo tirânico em nome do bem comum não podem ser chamados de sediciosos, pois quando há um governo tirânico, o verdadeiro sedicioso é o próprio tirano que, quando governa em proveito próprio em detrimento dos interesses do povo, nutre discórdias contra si.[89]
Aquino acreditava que a existência de Deus era auto-evidente, mas não era evidente para os homens. "Portanto, digo que esta proposição, 'Deus existe', em si mesma, é auto-evidente, pois o predicado é o mesmo que o sujeito... Agora, como não conhecemos a essência de Deus, a proposição não é auto-evidente para nós e precisa ser demonstrada por coisas que são-nos mais conhecidas, apesar de menos conhecidas em sua própria natureza - nomeadamente, pelos efeitos".[90]
Ele acreditava também que se poderia demonstrar a existência de Deus. De forma breve na "Suma Teológica" e mais extensivamente na "Suma contra os Gentios", Aquino considera em detalhes seus cinco argumentos para a existência de Deus, amplamente conhecidos como "quinque viae" ("cinco vias"):
Sobre a natureza de Deus, Aquino acreditava que a melhor abordagem, geralmente chamada de via negativa em latim, é considerar o que Deus "não é". Seguindo assim, ele propôs cinco expressões sobre as qualidades divinas:
Na "Suma Teológica", Tomás começa sua discussão sobre Jesus Cristo relembrado a histórica bíblica de Adão e Eva e descrevendo os efeitos negativos do pecado original. A partir daí, ele desenvolve seu argumento de que o objetivo da Encarnação era restaurar a natureza humana, removendo a "contaminação pelo pecado", algo que os humanos são incapazes de realizar por si mesmos. "A Sabedoria Divina julgou apropriado que Deus tornar-se-ia humano para que, assim, Este pudesse restaurar o homem e dar uma satisfação".[99] Tomás argumentou a favor da visão da satisfação da expiação, ou seja, que Jesus morreu "para dar satisfação por toda a raça humana, que foi sentenciada a morrer por causa do pecado".[100]
Aquino argumentou contra diversos teólogos que defendiam pontos de vista diferentes sobre Jesus. Em resposta a Plotino, afirmou que Jesus era verdadeiramente divino e não um simples ser humano. Contra Nestório, que sugeriu que o Filho de Deus estaria meramente conjuminado com o Cristo homem, defendeu que a completude de Deus era parte integral da existência de Cristo. Contra-atacou as visões de Apolinário defendendo que Cristo tinha uma alma verdadeiramente humana (racional, portanto) e a dualidade de naturezas de Cristo (a humana e a divina). Contra Eutiques, afirmou que esta dualidade permaneceu mesmo depois da Encarnação e, contra os ensinamentos de Maniqueu e Valentim, que as duas naturezas existiam simultaneamente e separadamente num único corpo humano real.[101]
Resumindo, "Cristo tinha um 'corpo real' da mesma natureza que o nosso, uma 'verdadeira alma racional' e, além disso, a 'divindade perfeita'":[102]
Ecoando Atanásio de Alexandria, Aquino afirmou que "O Unigênito Filho de Deus...assumiu nossa natureza para que, feito homem, pudesse fazer dos homens deuses".[104]
Aquino identificou que a razão da existência humana seria a união e amizade eterna com Deus, um objetivo alcançado através da visão beatífica, na qual uma pessoa experimenta a felicidade perfeita e sem fim ao presenciar a essência de Deus. Ela ocorre depois da morte como presente de Deus aos que na vida experimentaram a salvação e a redenção através de Cristo.[105]
O objetivo da união com Deus tem implicações para a vida das pessoas na terra. Segundo Tomás, a vontade dos indivíduos deve ser dirigida às coisas corretas, como caridade, paz e santidade. Ele via nesta orientação como um caminho para a felicidade e estruturou suas ideias sobre a vida moral à volta desta crença. A relação entre a vontade e o objetivo da vida é antecedente na natureza "por que a retidão da vontade consiste em ser obedientemente dirigida ao objetivo final [a visão beatífica]". Os que buscam verdadeiramente entender e ver Deus irão necessariamente amar o que Ele ama, um amor que requer moralidade e aparece nas escolhas cotidianas dos homens.[105]
Aquino era um dominicano, ou seja, um membro da Ordem dos Pregadores (Ordo Praedicatorum) cujo objetivo inicial era a conversão dos albigenses e de outras facções heterodoxas de forma pacífica num primeiro momento, mas que logo degeneraria na violenta Cruzada Albigense. Na "Suma Teológica", escreveu:
Roubo simples, falsificação, fraude e outros crimes similares eram também ofensas passíveis de morte na época; o ponto de Aquino é que a gravidade da heresia, que trata não apenas de bens materiais, mas também dos espirituais de outros, é pelo menos tão grave quanto falsificação. A sugestão dele exige, porém, que os heréticos sejam entregues a um "tribunal secular" ao invés da autoridade magisterial. Além disso, a ideia de que os heréticos merecem a morte está relacionada à teologia de Aquino, segundo a qual os pecadores não tem um direito intrínseco à vida, "pois o pagamento do pecado é a morte; mas o presente grátis de Deus é a vida eterna em Cristo Jesus, nosso Senhor"[107]). Seja como for, seu ponto é claro: heréticos devem ser executados pelo estado. Ele elabora ainda mais o tema no artigo seguinte quando diz:
Alguma compreensão sobre o estado psicológico de Tomás de Aquino é fundamental para compreender suas crenças sobre a vida após a morte e a ressurreição. Seguindo a doutrina da Igreja, ele aceita que a alma continue existindo depois da morte do corpo. Como ele aceita também que ela é a forma do corpo, Aquino defende que o ser humano, assim como todas as coisas materiais, é um composto de "forma" e "matéria", uma versão do hilemorfismo aristotélico. A forma substancial (a alma humana) configura (define) a matéria-prima (o corpo) e é assim que um composto material se enquadra numa determinada espécie; no caso dos homens, a do "animal racional".[109] Portanto, o ser humano seria um composto de forma-matéria organizado para ser um animal racional. A matéria não pode existir sem ser configurado por uma forma, mas esta pode existir sem a matéria, o que abre espaço para a crença da separação da alma do corpo. Aquino afirma que a alma coexiste nos mundos material e espiritual e, portanto, tem algumas caraterísticas materiais e outras imateriais (como o acesso aos universais).
Finalmente, Aquino rejeitava a ideia de que a ressurreição necessite de alguma forma de dualismo (entre corpo e alma como distintos), defendendo que alma (parte do composto forma-matéria) persistia depois da morte e à corrupção do corpo, sendo capaz de existência autônoma no período entre a morte e a ressurreição. Ele sabe que os seres humanos são essencialmente físicos, mas que esta "fisicalidade" tem um espírito capaz de retornar a Deus depois da vida.[110] Para ele, as recompensas e punições da vida depois da morte não são "apenas" espirituais. Por isso, a ressurreição é uma parte importante de sua filosofia sobre a alma. O homem é realizado e completo no corpo físico e, portanto, a vida eterna deve contar com almas materializadas em corpos ressuscitados. Além da recompensa espiritual, os homens podem então esperar o gozo de bênçãos materiais e físicas.[110]
Aquino afirma claramente sua posição sobre a ressurreição e a utiliza para defender sua filosofia da justiça: a promessa da ressurreição compensa os cristãos que sofreram neste mundo através de uma união celeste com o divino. Em suas palavras, "se não há ressurreição dos mortos, segue que não há nada de bom para os seres humanos fora desta vida".[111] Assim, a esperança da ressurreição seria responsável pelo ímpeto para as pessoas na terra abrirem mão de prazeres nesta vida; aqueles que se prepararam para a vida depois da morte, moral e intelectualmente, receberão recompensas ainda maiores pela graça divina. Aquino insiste que a beatitude será conferida por mérito e irá tornar as pessoas mais capazes de conceber o divino. Na mesma linha, a punição também está diretamente relacionada com esta preparação e as ações na terra.[111]
A lista completa de obras de Tomás de Aquino (ou atribuídas a ele) é a seguinte:
Por séculos persistem alegações recorrentes de que Tomás teria tido a habilidade de levitar. Por exemplo, G. K. Chesterton escreveu: "Suas experiências incluíram casos bem atestados de levitação em êxtase; e a Virgem Maria apareceu para ele, confortando-o com as boas novas de que ele jamais seria bispo".[112]
Em 1277, Étienne Tempier, o mesmo bispo de Paris que havia publicado o édito de condenação de 1270, publicou outro, mais amplo. Um dos objetivos desta vez era clarificar que o poder absoluto de Deus transcendia quaisquer princípios da lógica de Aristóteles ou de Averróis.[113] Mais especificamente, ele continha uma lista de 219 proposições que, segundo o bispo, violavam a onipotência de Deus, entre eles vinte proposições de Tomás de Aquino, o que prejudicou seriamente sua reputação por muitos anos.[114]
Várias afirmações de Tomás foram tomadas por filósofos posteriores como abstratas e aporéticas. Com efeito, algumas das “provas da existência de Deus” elaboradas por Tomás acabaram sendo chamadas posteriormente de “Deus dos Filósofos”, termo que passou a ser usado para designar abstrações muito apartadas da realidade e da própria revelação.[115] Embora reconheça que outras contribuições tenham sido definitivas para o tema, o filósofo Lorenz Puntel, cuja tese de doutorado foi dedicada a Tomás, assinala que as chamadas cinco vias constituem um enfoque superficial, periférico e totalmente inadequado.[115]
Bertrand Russell criticou a filosofia de Aquino por que:
Esta crítica pode ser ilustrada com alguns dos exemplos (todos com base no raciocínio de Russell):
Quando o advogado do diabo de seu processo de canonização argumentou que não haviam milagres em seu nome, um dos cardeais respondeu: "Tot miraculis, quot articulis" ("Tantos milagres quanto artigos", uma referência à quantidade de artigos na "Suma Teológica", milhares).[119] Cinquenta anos depois de sua morte, em 18 de julho de 1323, João XII, papa em Avinhão, declarou Tomás de Aquino santo.[120]
Num mosteiro de Nápoles, perto da catedral de São Januário de Benevento, uma cela na qual São Tomás supostamente viveu ainda é visitada por peregrinos. Seus restos estão abrigados na Igreja dos Jacobinos em Toulouse desde 28 de junho de 1369. Entre 1789, data da Revolução Francesa, e 1974, eles estiveram na Basilique de Saint-Sernin. Neste anos, foram devolvidos à Igreja dos Jacobinos onde estão até hoje.
Quando foi canonizado, a festa de São Tomás foi incorporada ao Calendário Geral Romano em 7 de março, o dia de sua morte. Como esta data geralmente cai na Quaresma, a festa foi modificada para 28 de janeiro, a data da translação de suas relíquias para Toulouse.[121][122]
Na "Divina Comédia", Dante coloca a alma glorificada de Tomás de Aquino no "céu do Sol" com outros grandes da sabedoria religiosa.[123] Ele afirma ainda que Tomás morreu envenenado por ordem de Carlos de Anjou,[124] uma versão citada por Villani (ix. 218) e descrita, inclusive o motivo, pelo Anonimo Fiorentino. Porém, o historiador Ludovico Antonio Muratori reproduz o relato feito pelos amigos de Tomás e não encontrou traço algum de má fé.[125]
A teologia de Tomás de Aquino já havia começado a ganhar prestígio quando, dois séculos depois, em 1567, Pio V proclamou-o um Doutor da Igreja e colocou sua festa no mesmo nível da dos grandes Padres latinos: Ambrósio, Agostinho, Jerônimo e Gregório. Porém, na mesma época, o Concílio de Trento buscava muito mais em Duns Escoto do que em Tomás argumentos em defesa da Igreja.[114][119]
Muitos estudiosos da ética, dentro e fora da Igreja Católica (notavelmente Philippa Foot e Alasdair MacIntyre), comentaram em tempos recentes sobre a possibilidade de utilizar a ética de virtude de Aquino como meio de evitar o utilitarismo ou a deontologia de Kant (o "senso do dever"). Pelas obras de filósofos do século XX como Elizabeth Anscombe (especialmente no livro "Intention"), o princípio do duplo efeito de Aquino e sua teoria da atividade intencional tem sido muito influentes.
As teorias estéticas de Aquino, especialmente seu conceito de claritas, influenciaram profundamente a obra do autor modernista James Joyce, que costumava dizer que Aquino estava atrás apenas de Aristóteles entre os filósofos ocidentais. Ele fez referências ao pensamento de Aquino através da "Elementa philosophiae ad mentem D. Thomae Aquinatis doctoris angelici" (1898), de Girolamo Maria Mancini, professor de teologia no famoso Collegium Divi Thomae de Urbe,[126] como, por exemplo, em "Portrait of the Artist as a Young Man".[127]
As obras do semiótico italiano Umberto Eco também foram influenciadas pela estética de Aquino, principalmente no ensaio que ele escreveu sobre Tomás de Aquino publicada em 1956 e republicada, revisada, em 1988.
Epistemologia (do grego ἐπιστήμη, transl. episteme: conhecimento certo, ciência;[1] λόγος, transl.  logos: discurso, estudo), em sentido estrito, refere-se ao ramo da filosofia que se ocupa do conhecimento científico; é o estudo crítico dos princípios, das hipóteses e dos resultados das diversas ciências, com a finalidade de determinar seus fundamentos lógicos, seu valor e sua importância objetiva.[2] Em uma acepção mais restrita, a epistemologia pode ser identificada com a filosofia da ciência.
O termo "epistemologia", cunhado pelo filósofo escocês James Frederick Ferrier (1808 – 1864),[3] refere-se especificamente à parte da gnosiologia que estuda os requisitos e condições necessários à produção do conhecimento científico, incluindo os fundamentos, a validade, a  consistência lógica das teorias e os limites desse conhecimento.[4] Mais recentemente, entretanto, o conceito passou a ser usado, em sentido amplo, como sinônimo de gnosiologia ou teoria do conhecimento - disciplina que se ocupa do estudo do conhecimento humano em geral.[5][6][7]
A epistemologia relaciona-se também com a metafísica. Seu escopo compreende a questão da possibilidade do conhecimento - nomeadamente, se é possível ao ser humano retratar o conhecimento total e genuíno - dos seus limites (haveria realmente uma distinção entre o mundo cognoscível e o mundo incognoscível?) e de sua origem (por quais faculdades atingimos o conhecimento? Haverá conhecimento certo e errado em alguma concepção a priori?). De fato, existem limites epistemológicos, que se devem ao fato de a diversidade e a complexidade dos seres humanos e dos ambientes onde estes se desenvolvem tornarem virtualmente impossíveis os procedimentos de controle experimental.
Em geral, a epistemologia também discute o conhecimento proposicional ou o "saber que". Esse tipo de conhecimento difere do "saber como" e do "conhecimento por familiaridade". Por exemplo: sabe-se que 2 + 2 = 4 e que Napoleão foi derrotado na batalha de Waterloo. Essas formas de conhecimento diferem de saber como andar de bicicleta ou como tocar piano, e também diferem de conhecer uma determinada pessoa ou estar "familiarizado" com ela. Alguns filósofos consideram que há uma diferença considerável e importante entre "saber que", "saber como" e "familiaridade" e que o principal interesse da filosofia recai sobre a primeira forma de saber.
Em seu ensaio Os Problemas da Filosofia, Bertrand Russell distingue o "conhecimento por descrição" (uma das formas de saber que) do "conhecimento por familiaridade".[8] Segundo Russell, o conhecimento por familiaridade é uma espécie de relação de consciência direta entre o sujeito que conhece e o objeto conhecido. Isso quer dizer que o sujeito que conhece adquire esse conhecimento dos objetos sem a mediação de nenhum procedimento lógico e sem nenhum conhecimento da verdade. Gilbert Ryle dedica atenção especial à distinção entre "saber que" e "saber como" em  The concept of mind (O Conceito de Mente).[9] Em Personal Knowledge, Michael Polanyi argumenta a favor da relevância epistemológica do saber-como e do saber-que. Usando o exemplo do equilíbrio envolvido no ato de andar de bicicleta, ele sugere que o conhecimento teórico da física para a manutenção do estado de equilíbrio não pode substituir o conhecimento prático sobre como andar de bicicleta. Para Polanyi, é importante saber como essas duas formas de conhecimento são estabelecidas e fundamentadas. Essa posição é a mesma de Ryle, que argumenta que, se não consideramos a diferença entre saber-que e saber-como, somos inevitavelmente conduzidos a um regresso ao infinito.
Mais recentemente, alguns epistemólogos (Ernest Sosa, John Greco, Jonathan Kvanvig, Linda Trinkaus Zagzebski) argumentaram que a epistemologia deveria avaliar as propriedades das pessoas (isto é, suas virtudes intelectuais) e não somente as propriedades das proposições ou das atitudes proposicionais da mente. Uma das razões é que as formas superiores de processamento cognitivo (como, por exemplo, o entendimento) envolveriam características que não podem ser avaliadas por uma abordagem do conhecimento que se restrinja apenas às questões clássicas da crença, verdade e justificação.
No discurso comum, uma "declaração da verdade" é uma típica expressão de fé ou confiança em uma pessoa, num poder ou em outra entidade - o que inclui visões tradicionais. A epistemologia se preocupa com o que acreditamos; isso inclui a verdade e tudo que nós aceitamos para nós mesmos como verdade.
A verdade não é um pré-requisito para a crença. De outro modo, se algo é conhecido, categoricamente, não pode ser falso. Por exemplo: se uma pessoa acredita que a ponte é segura o suficiente para aguentar seu peso e tenta atravessá-la, mas a ponte se quebra devido ao peso, pode-se dizer que a pessoa acreditou que a ponte era segura, mas estava errada. Não seria correto afirmar que ele sabia que a ponte era segura, pois ela, claramente, não era. Em contraste, se a ponte aguentasse seu peso, ela diria que acreditou que a ponte era segura e, agora que cruzou a ponte e provou para si que a ponte é segura, ela sabe que é segura.
A  justificação se constitui das razões ou provas apresentadas em apoio à veracidade de uma crença ou de uma afirmação. É preciso, portanto,  compreender as razões de uma crença e se tais razões têm um fundamento lógico.
Em seu sentido mais comum, os termos pensamento e pensar referem-se a processos cognitivos conscientes que podem acontecer independentemente da estimulação sensorial. Suas formas mais paradigmáticas são o juízo, o raciocínio, a formação de conceitos, a resolução de problemas e a deliberação. Mas outros processos mentais, como considerar uma ideia, memória ou imaginação, também são frequentemente incluídos. Estes processos podem acontecer internamente independentemente dos órgãos sensoriais, ao contrário da percepção. Mas quando entendido no sentido mais amplo, qualquer evento mental pode ser entendido como uma forma de pensamento, incluindo a percepção e os processos mentais inconscientes. Em um sentido ligeiramente diferente, o termo pensamento não se refere aos processos mentais em si, mas aos estados mentais ou sistemas de ideias provocados por esses processos.
Várias teorias de pensamento foram propostas. Eles visam captar os traços característicos do pensamento. Os platonistas sustentam que o pensamento consiste em discernir e inspecionar as formas platônicas e suas inter-relações. Envolve a habilidade de discriminar entre as formas platônicas puras e as meras imitações encontradas no mundo sensorial. De acordo com o aristotelismo, pensar em algo é instanciar na mente a essência universal do objeto do pensamento. Estes universais são abstraídos da experiência sensorial e não são entendidos como existentes em um mundo inteligível imutável, em contraste com o platonismo. O conceitualismo está intimamente relacionado ao aristotelismo: identifica o pensamento com a evocação mental de conceitos, em vez de instanciar essências. As teorias de fala interna afirmam que o pensamento é uma forma de fala interna na qual as palavras são silenciosamente expressas na mente do pensador. De acordo com alguns relatos, isto acontece em uma língua regular, como inglês ou francês. A hipótese da linguagem do pensamento, por outro lado, sustenta que isto acontece no meio de uma linguagem mental única chamada mentalês. Central para essa ideia é que os sistemas de representação linguística são construídos a partir de representações atômicas e compostas, e que esta estrutura também é encontrada no pensamento. Os associacionistas entendem o pensamento como a sucessão de ideias ou imagens. Eles estão particularmente interessados nas leis de associação que governam como o trem de pensamento se desenvolve. Os behavioristas, por outro lado, identificam o pensamento com as disposições comportamentais para se engajar em comportamentos inteligentes públicos como uma reação a estímulos externos particulares. O computacionalismo é a mais recente destas teorias. Ele vê o pensamento em analogia a como os computadores funcionam em termos de armazenamento, transmissão e processamento de informações.
Vários tipos de pensamento são discutidos na literatura acadêmica. Um juízo é uma operação mental na qual uma proposição é evocada e depois afirmada ou negada. O raciocínio, por outro lado, é o processo de tirar conclusões a partir de premissas ou evidências. Tanto o juízo quanto o raciocínio dependem da possessão dos conceitos relevantes, que são adquiridos no processo de formação de conceitos. No caso da resolução de problemas, o pensamento visa alcançar um objetivo predefinido, superando certos obstáculos. A deliberação é uma forma importante de pensamento prático que consiste em formular possíveis cursos de ação e avaliar as razões a favor e contra eles. Isto pode levar a uma decisão, escolhendo a opção mais favorável. Tanto a memória episódica quanto a imaginação apresentam objetos e situações internamente, na tentativa de reproduzir com precisão o que foi experimentado anteriormente ou como um rearranjo livre, respectivamente. O pensamento inconsciente é o pensamento que acontece sem ser experimentado diretamente. Às vezes é postulado para explicar como os problemas difíceis são resolvidos em casos onde não foi empregado o pensamento consciente.
O pensamento é discutido em várias disciplinas acadêmicas. A fenomenologia está interessada na experiência de pensar. Uma questão importante neste campo diz respeito ao caráter experiencial do pensamento e até que ponto este caráter pode ser explicado em termos de experiência sensorial. A metafísica está, entre outras coisas, interessada na relação entre a mente e a matéria. Isto diz respeito à questão de como o pensamento pode se encaixar no mundo material, como descrito pelas ciências naturais. A psicologia cognitiva tem como objetivo entender o pensamento como uma forma de processamento de informações. A psicologia do desenvolvimento, por outro lado, investiga o desenvolvimento do pensamento desde o nascimento até a maturidade e pergunta de quais fatores este desenvolvimento depende. A psicanálise enfatiza o papel do inconsciente na vida mental. Outros campos relacionados ao pensamento incluem linguística, neurociência, inteligência artificial, biologia e sociologia. Vários conceitos e teorias estão intimamente relacionados com o tema do pensamento. O termo "lei do pensamento" se refere a três leis fundamentais da lógica: o princípio da não-contradição, o princípio do terceiro excluído e o princípio da identidade. O pensamento contrafactual envolve representações mentais de situações e eventos não reais nos quais o pensador tenta avaliar o que seria o caso se as coisas tivessem sido diferentes. Os experimentos mentais frequentemente empregam o pensamento contrafactual para ilustrar teorias ou para testar sua plausibilidade. O pensamento crítico é uma forma de pensamento que é razoável, reflexiva e focada em determinar o que acreditar ou como agir. O pensamento positivo envolve focar a atenção nos aspectos positivos da própria situação e está intimamente relacionado ao otimismo.
Os termos "pensamento" e "pensar" referem-se a uma ampla variedade de atividades psicológicas.[1][2][3] Em seu sentido mais comum, são entendidos como processos conscientes que podem acontecer independentemente da estimulação sensorial.[4][5] Isto inclui vários processos mentais diferentes, como considerar uma ideia ou proposição, ou julgá-la como verdadeira. Neste sentido, a memória e a imaginação são formas de pensamento, mas a percepção não é.[6] Em um sentido mais restrito, apenas os casos mais paradigmáticos são considerados pensamento. Estes envolvem processos conscientes que são conceituais ou linguísticos e suficientemente abstratos, como julgar, inferir, resolver problemas e deliberar.[1][7][8] Às vezes os termos "pensamento" e "pensar" são entendidos em um sentido muito amplo como referindo-se a qualquer forma de processo mental, consciente ou inconsciente.[9][10] Neste sentido, pode ser usado como sinônimo do termo "mente". Este uso é encontrado, por exemplo, na tradição cartesiana, onde as mentes são entendidas como coisas pensantes, e nas ciências cognitivas.[6][11][12][13] Mas este sentido pode incluir a restrição de que tais processos têm que levar a um comportamento inteligente para ser considerado pensamento.[14] Um contraste às vezes encontrado na literatura acadêmica é o que existe entre pensar e sentir. Neste contexto, o pensamento está associado a uma abordagem sóbria, desapaixonada e racional de seu tema, enquanto o sentimento implica um envolvimento emocional direto.[15][16][17]
Os termos "pensamento" e "pensar" também podem ser usados para se referir não aos processos mentais em si, mas aos estados mentais ou sistemas de ideias provocados por esses processos.[18] Neste sentido, muitas vezes são sinônimos do termo "crença" e seus cognatos e podem se referir aos estados mentais que pertencem a um indivíduo ou são comuns entre um certo grupo de pessoas.[19][20] As discussões de pensamento na literatura acadêmica muitas vezes deixam implícito qual sentido do termo eles têm em mente.
Várias teorias do pensamento foram propostas.[21] Visam captar os traços característicos do pensamento. As teorias listadas aqui não são exclusivas: pode ser possível combinar algumas sem levar a uma contradição.
De acordo com o platonismo, o pensamento é uma atividade espiritual na qual as formas platônicas e suas inter-relações são discernidas e inspecionadas.[21][22] Essa atividade é entendida como uma forma de fala interna silenciosa na qual a alma fala consigo mesma.[23] As formas platônicas são vistas como universais que existem em um âmbito de realidade imutável, diferente do mundo sensível. Exemplos incluem as formas de bondade, beleza, unidade e igualdade.[24][25][26] Nesta visão, a dificuldade de pensar consiste em ser capaz de captar as formas platônicas e distingui-las como originais das meras imitações encontradas no mundo sensorial. Isto significa, por exemplo, distinguir a beleza em si mesma das imagens derivadas da beleza.[22] Um problema para esta visão é explicar como os humanos podem aprender e pensar sobre as formas platônicas pertencentes a um âmbito de realidade diferente.[21] O próprio Platão tenta resolver este problema através de sua teoria da reminiscência, segundo a qual a alma já estava em contato com as formas platônicas antes e, portanto, é capaz de lembrar como elas são.[22] Mas esta explicação depende de várias suposições geralmente não aceitas no pensamento contemporâneo.[22]
Os aristotélicos sustentam que a mente é capaz de pensar em algo instanciando a essência do objeto do pensamento.[21] Assim, enquanto pensa em árvores, a mente instancia a arvoridade. Esta instanciação não acontece na matéria como é o caso das árvores reais, mas na mente, embora a essência universal instanciada em ambos os casos é a mesma.[21] Em contraste com o platonismo, estes universais não são entendidos como formas platônicas existentes em um mundo inteligível imutável.[27] Em vez disso, eles só existem na medida em que são instanciados. A mente aprende a discriminar universais através da abstração da experiência.[28] Esta explicação evita várias das objeções levantadas contra o platonismo.[27]
O conceitualismo está intimamente relacionado com o aristotelismo. Sustenta que o pensamento consiste em evocar conceitos mentalmente. Alguns destes conceitos podem ser inatos, mas a maioria tem que ser aprendida através da abstração da experiência sensorial antes de poder ser usada no pensamento.[21]
Foi argumentado contra estes pontos de vista que têm problemas em explicar a forma lógica do pensamento. Por exemplo, para pensar que vai chover ou nevar, não é suficiente instanciar as essências da chuva e da neve ou evocar os conceitos correspondentes. A razão disto é que a relação disjuntiva entre a chuva e a neve não é captada desta maneira.[21] Outro problema compartilhado por essas posições é a dificuldade de dar uma explicação satisfatória de como essências ou conceitos são aprendidos pela mente através da abstração.[21]
As teorias de fala interna afirmam que o pensamento é uma forma de fala interna.[6][29][23][1] Esta visão é às vezes chamada de nominalismo psicológico.[21] Afirma que pensar envolve evocar palavras silenciosamente e conectá-las para formar sentenças mentais. O conhecimento que uma pessoa tem de seus pensamentos pode ser explicado como uma forma de ouvir o próprio monólogo silencioso.[30] Três aspectos centrais são frequentemente atribuídos à fala interna: é semelhante a ouvir sons em um sentido importante, envolve o uso da linguagem e constitui um plano motor que poderia ser usado para a fala real.[23] Essa conexão com a linguagem é apoiada pelo fato de que o pensamento é frequentemente acompanhado por atividade muscular nos órgãos da fala. Esta atividade pode facilitar o pensamento em certos casos, mas não é necessária para ele em geral.[1] De acordo com alguns relatos, o pensamento não acontece em uma língua comum, como inglês ou francês, mas tem seu próprio tipo de linguagem com os símbolos e sintaxe correspondentes. Esta teoria é conhecida como a hipótese da linguagem do pensamento.[31][32]
A teoria de fala interna tem uma forte plausibilidade inicial, já que a introspecção sugere que, de fato, muitos pensamentos são acompanhados pela fala interna. Mas seus oponentes geralmente afirmam que isto não é verdade para todos os tipos de pensamento.[21][5][33] Argumentou-se, por exemplo, que as formas de sonhar acordado constituem pensamento não linguístico.[34] Este assunto é relevante para a questão de saber se os animais têm a capacidade de pensar. Se o pensamento está necessariamente ligado à linguagem, isto sugeriria que há uma lacuna importante entre humanos e animais, já que apenas os humanos têm uma linguagem suficientemente complexa. Mas a existência de pensamentos não linguísticos sugere que esta lacuna pode não ser tão grande e que alguns animais realmente pensam.[33][35][36]
Existem várias teorias sobre a relação entre linguagem e pensamento. Uma versão proeminente na filosofia contemporânea é chamada de hipótese da linguagem do pensamento.[31][32][37][38][39] Afirma que o pensamento acontece no meio de uma linguagem mental. Esta linguagem, muitas vezes referida como mentalês, é semelhante às linguagens regulares em vários aspectos: é composta de palavras que estão ligadas umas às outras de maneira sintática para formar sentenças.[31][32][37][38] Esta afirmação não se baseia apenas em uma analogia intuitiva entre linguagem e pensamento. Em vez disso, fornece uma definição clara das características que um sistema representacional deve incorporar para ter uma estrutura linguística.[37][32][38] No nível da sintaxe, o sistema representacional tem que possuir dois tipos de representações: representações atômicas e compostas. As representações atômicas são básicas, enquanto as representações compostas são constituídas ou por outras representações compostas, ou por representações atômicas.[37][32][38] No nível da semântica, o conteúdo semântico ou o significado das representações compostas deve depender do conteúdo semântico dos seus constituintes. Um sistema representacional é estruturado linguisticamente se cumprir estes dois requisitos.[37][32][38]
A hipótese da linguagem do pensamento afirma que o mesmo é verdade para o pensamento em geral. Isto significaria que o pensamento é composto por certos constituintes representacionais atômicos que podem ser combinados como descrito acima.[37][32][40] Além desta caracterização abstrata, nenhuma outra afirmação concreta é feita sobre como o pensamento humano é implementado pelo cérebro ou quais outras semelhanças com a linguagem natural ele tem.[37] A hipótese da linguagem do pensamento foi introduzida pela primeira vez por Jerry Fodor.[32][37] Ele argumenta a favor desta afirmação ao sustentar que ela constitui a melhor explicação dos traços característicos do pensamento. Uma dessas características é a produtividade: um sistema de representações é produtivo se puder gerar um número infinito de representações únicas com base em um baixo número de representações atômicas.[37][32][40] Isto se aplica ao pensamento, já que seres humanos são capazes de entreter um número infinito de pensamentos distintos, apesar de suas capacidades mentais serem bastante limitadas. Outros traços característicos do pensamento incluem a sistematicidade e a coerência inferencial.[32][37][40] Fodor argumenta que a hipótese da linguagem do pensamento é verdadeira porque explica como o pensamento pode ter essas características e porque não há uma boa explicação alternativa.[37] Alguns argumentos contra a hipótese da linguagem do pensamento são baseados em redes neurais, que são capazes de produzir comportamento inteligente sem depender de sistemas representacionais. Outras objeções concentram-se na ideia de que algumas representações mentais acontecem de forma não linguística, por exemplo, na forma de mapas ou imagens.[37][32]
Os computacionalistas estão especialmente interessados na hipótese da linguagem do pensamento, pois fornece formas de preencher a lacuna entre o pensamento no cérebro humano e os processos computacionais implementados pelos computadores.[37][32][41] A razão disto é que processos sobre representações que respeitam a sintaxe e a semântica, como inferências segundo o modus ponens, podem ser implementados por sistemas físicos usando relações causais. Os mesmos sistemas linguísticos podem ser implementados através de diferentes sistemas materiais, como cérebros ou computadores. Desta forma, os computadores podem pensar.[37][32]
Uma visão importante na tradição empirista é o associacionismo, a visão de que o pensamento consiste na sucessão de ideias ou imagens.[1][42][43] Esta sucessão é vista como sendo governada por leis de associação, que determinam como a linha do pensamento se desenrola.[1][44] Estas leis são diferentes das relações lógicas entre os conteúdos dos pensamentos, que são encontradas no caso de tirar inferências ao passar do pensamento das premissas para o pensamento da conclusão.[44] Várias leis de associação foram sugeridas. De acordo com as leis de semelhança e contraste, as ideias tendem a evocar outras ideias que são muito semelhantes a elas ou seu oposto. A lei da contiguidade, por outro lado, afirma que se duas ideias foram frequentemente experimentadas juntas, então a experiência de uma tende a causar a experiência da outra.[1][42] Neste sentido, a história da experiência de um organismo determina quais pensamentos o organismo tem e como estes pensamentos se desenvolvem.[44] Mas tal associação não garante que a conexão seja significativa ou racional. Por exemplo, devido à associação entre os termos "frio" e "Idaho", o pensamento "este café é frio" pode levar ao pensamento "a Rússia deve anexar Idaho".[44]
Uma forma de associacionismo é o imagismo. Afirma que pensar envolve entreter uma sequência de imagens em que imagens anteriores evocam imagens posteriores com base nas leis da associação.[21] Um problema com esta visão é que podemos pensar em coisas que não podemos imaginar. Isto é especialmente relevante quando o pensamento envolve objetos muito complexos ou infinitos, o que é comum, por exemplo, no pensamento matemático.[21] Uma crítica dirigida ao associacionismo em geral é que sua afirmação é abrangente demais. Há um amplo consenso de que os processos associativos estudados pelos associacionistas desempenham algum papel na forma como o pensamento se desenrola. Mas a alegação de que este mecanismo é suficiente para compreender todo pensamento ou todos os processos mentais geralmente não é aceita.[43][44]
De acordo com o behaviorismo, o pensamento consiste em disposições comportamentais para se envolver em certos comportamentos publicamente observáveis como reação a estímulos externos particulares.[45][46][47] Nesta visão, ter um pensamento particular é o mesmo que ter uma disposição para se comportar de uma certa maneira. Esta visão é frequentemente motivada por considerações empíricas: é muito difícil estudar o pensamento como um processo mental privado, mas é muito mais fácil estudar como os organismos reagem a uma certa situação com um determinado comportamento.[47] Neste sentido, a capacidade de resolver problemas não através dos hábitos existentes, mas através de novas abordagens criativas é particularmente relevante.[48] O termo "behaviorismo" às vezes também é usado em um sentido ligeiramente diferente quando aplicado ao pensamento para se referir a uma forma específica de teoria de fala interna.[49] Esta visão se concentra na ideia de que a fala interna relevante é uma forma derivada da fala externa regular.[1] Este sentido se sobrepõe a como o behaviorismo é entendido mais comumente na filosofia da mente, já que estes atos de fala interna não são observados pelo pesquisador, mas meramente inferidos a partir do comportamento inteligente do sujeito.[49] Isto permanece fiel ao princípio behaviorista geral de que a evidência comportamental é necessária para qualquer hipótese psicológica.[47]
Um problema para o behaviorismo é que a mesma entidade muitas vezes se comporta de maneira diferente, apesar de estar na mesma situação de antes.[50][51] Este problema consiste no fato de que pensamentos individuais ou estados mentais geralmente não correspondem a um comportamento particular. Assim, pensar que a torta é saborosa não leva automaticamente a comê-la, já que vários outros estados mentais ainda podem inibir este comportamento, por exemplo, a crença de que seria indelicado fazê-lo ou que a torta está envenenada.[52][53]
As teorias computacionalista do pensamento, frequentemente encontradas nas ciências cognitivas, entendem o pensamento como uma forma de processamento de informações.[41][54][45] Estas visões se desenvolveram com o surgimento dos computadores na segunda parte do século XX, quando vários teóricos viram o pensamento em analogia com as operações dos computadores.[54] Em tais pontos de vista, as informações podem ser codificadas de forma diferente no cérebro, mas, em princípio, as mesmas operações ocorrem lá também, correspondendo ao armazenamento, transmissão e processamento de informações.[1][13] Mas, embora esta analogia tem alguma atração intuitiva, os teóricos lutam para dar uma explicação mais explícita do que é a computação. Outro problema consiste em explicar o sentido em que o pensamento é uma forma de computação.[45] A visão tradicionalmente dominante define a computação em termos de máquinas de Turing, embora os relatos contemporâneos muitas vezes se concentrem em redes neurais para suas analogias.[41] Uma máquina de Turing é capaz de executar qualquer algoritmo baseando-se em alguns princípios muito básicos, tais como ler um símbolo de uma célula, escrever um símbolo em uma célula e executar instruções com base nos símbolos lidos.[41] Desta maneira, é possível realizar o raciocínio dedutivo seguindo as regras de inferência da lógica formal, além de simular muitas outras funções da mente, como processamento da linguagem, tomada de decisões e controle motor.[54][45] Mas o computacionalismo não afirma apenas que o pensamento é, em algum sentido, semelhante à computação. Em vez disso, afirma que pensar é apenas uma forma de computação ou que a mente é uma máquina de Turing.[45]
As teorias computacionalistas do pensamento são às vezes divididas em abordagens funcionalistas e representacionalistas.[45] As abordagens funcionalistas definem os estados mentais através de seus papéis causais, mas permitem tanto eventos externos quanto internos em sua rede causal.[55][56][57] O pensamento pode ser visto como uma forma de programa que pode ser executado da mesma maneira por muitos sistemas diferentes, incluindo humanos, animais e até robôs. De acordo com uma dessas visões, se algo é um pensamento depende apenas de seu papel "na produção de mais estados internos e saídas verbais".[58][55] O representacionalismo, por outro lado, enfoca as características representacionais dos estados mentais e define os pensamentos como sequências de estados mentais intencionais.[59][45] Neste sentido, o computacionalismo é frequentemente combinado com a hipótese da linguagem do pensamento ao interpretar estas sequências como símbolos cuja ordem é regida por regras sintáticas.[45][32]
Vários argumentos foram levantados contra o computacionalismo. Em um sentido, parece trivial, já que quase qualquer sistema físico pode ser descrito como executando computações e, portanto, como pensando. Por exemplo, foi argumentado que os movimentos moleculares em uma parede regular podem ser entendidos como a computação de um algoritmo, já que são "isomórficos à estrutura formal do programa" em questão sob a interpretação correta.[45] Isto levaria à conclusão implausível de que a parede está pensando. Outra objeção se concentra na ideia de que o computacionalismo capta apenas alguns aspectos do pensamento, mas é incapaz de explicar outros aspectos cruciais da cognição humana.[45][54]
Uma grande variedade de tipos de pensamento é discutida na literatura acadêmica. Uma abordagem comum os divide naquelas formas que visam a criação de conhecimento teórico e naquelas que visam a produção de ações ou decisões corretas.[21] Mas não existe uma taxonomia universalmente aceita que resuma todos esses tipos. Em alguns casos, o mesmo pensamento particular pode pertencer a diferentes categorias ao mesmo tempo. Também pode depender da definição de pensamento se alguns dos tipos listados aqui realmente se qualificam como pensamento.
Pensar é muitas vezes identificado com o ato de julgar. Um juízo é uma operação mental na qual uma proposição é evocada e depois afirmada ou negada.[6][60] Envolve decidir em que acreditar e visa determinar se a proposição julgada é verdadeira ou falsa.[61][62] Várias teorias de juízo foram propostas. A abordagem tradicionalmente dominante é a teoria da combinação. Afirma que os juízos consistem na combinação de conceitos.[63] Nesta visão, julgar que "todos os homens são mortais" é combinar os conceitos "homem" e "mortal". Os mesmos conceitos podem ser combinados de diferentes maneiras, correspondendo a diferentes formas de juízo, por exemplo, como "alguns homens são mortais" ou "nenhum homem é mortal".[64]
Outras teorias do juízo se concentram mais na relação entre a proposição julgada e a realidade. Segundo Franz Brentano, um juízo é uma crença ou uma descrença na existência de alguma entidade.[63][65] Neste sentido, existem apenas duas formas fundamentais de juízo: "A existe" e "A não existe". Quando aplicada à frase "todos os homens são mortais", a entidade em questão é "homens imortais", dos quais se diz que não existem.[63][65] Importante para Brentano é a distinção entre a mera representação do conteúdo do juízo e a afirmação ou a negação do conteúdo.[63][65] A mera representação de uma proposição é muitas vezes referida como "entreter uma proposição". Este é o caso, por exemplo, quando se considera uma proposição, mas ainda não se decidiu se é verdadeira ou falsa.[63][65] O termo "pensar" pode se referir tanto a julgar como a mero entreter. Esta diferença é muitas vezes explícita na forma como o pensamento é expresso: "pensar que" geralmente envolve um juízo, enquanto "pensar em" refere-se à representação neutra de uma proposição sem uma crença associada. Neste caso, a proposição é meramente entretida, mas ainda não julgada.[19] Algumas formas de pensamento podem envolver a representação de objetos sem qualquer proposição, como quando alguém está pensando em sua avó.[6]
O raciocínio é uma das formas mais paradigmáticas do pensamento. É o processo de tirar conclusões a partir de premissas ou evidências. Os tipos de raciocínio podem ser divididos em raciocínio dedutivo e não dedutivo. O raciocínio dedutivo é governado por certas regras de inferência, que garantem a verdade da conclusão se as premissas são verdadeiras.[1][66] Por exemplo, dadas as premissas "todos os homens são mortais" e "Sócrates é um homem", segue-se dedutivamente que "Sócrates é mortal". O raciocínio não dedutivo, também conhecido como raciocínio derrotável ou raciocínio não monotônico, ainda é racionalmente convincente, mas a verdade da conclusão não é garantida pela verdade das premissas.[67] A indução é uma forma de raciocínio não dedutivo, por exemplo, quando se conclui que "o sol nascerá amanhã" com base nas experiências de todos os dias anteriores. Outras formas de raciocínio não dedutivo incluem a inferência à melhor explicação e o raciocínio analógico.[68]
As falácias são formas defeituosas de pensamento que vão contra as normas de raciocínio correto. As falácias formais dizem respeito a inferências defeituosas encontradas no raciocínio dedutivo.[69][70] Negar o antecedente é um tipo de falácia formal, por exemplo: "Se Otelo é solteiro, então ele é homem. Otelo não é solteiro. Portanto, Otelo não é homem".[1][71] As falácias informais, por outro lado, aplicam-se a todos os tipos de raciocínio. A fonte de sua falha se encontra no conteúdo ou no contexto do argumento.[72][69][73] Isto é muitas vezes causado por expressões ambíguas ou vagas na linguagem natural,[74] como em "Os veleiros usam velas. As velas são feitas de cera. Portanto, os veleiros usam cera." Um aspecto importante das falácias é que elas parecem ser racionalmente convincentes à primeira vista e, assim, seduzem as pessoas a aceitá-las e cometê-las.[69] Se um ato de raciocínio constitui uma falácia não depende de se as premissas são verdadeiras ou falsas, mas de sua relação com a conclusão e, em alguns casos, do contexto.[1]
Conceitos são noções gerais que constituem os elementos fundamentais do pensamento.[75][76] São regras que governam como os objetos são ordenados em diferentes classes.[77][78] Uma pessoa só pode pensar em uma proposição se possuir os conceitos envolvidos nesta proposição.[79] Por exemplo, a proposição "vombates são animais" envolve os conceitos "vombate" e "animal". Alguém que não possui o conceito "vombate" ainda pode ser capaz de ler a frase, mas não pode entreter a proposição correspondente. A formação de conceitos é uma forma de pensar na qual novos conceitos são adquiridos.[78] Envolve familiarizar-se com os traços característicos compartilhados por todas as instâncias do tipo de entidade correspondente e desenvolver a capacidade de identificar casos positivos e negativos. Este processo geralmente corresponde ao aprendizado do significado da palavra associada ao tipo em questão.[77][78] Existem várias teorias sobre como os conceitos e a possessão de conceitos devem ser entendidos.[75]
De acordo com uma visão popular, os conceitos devem ser entendidos em termos de habilidades. Nesta visão, dois aspectos centrais caracterizam a possessão de conceitos: a habilidade de discriminar entre casos positivos e negativos, e a habilidade de tirar inferências deste conceito a conceitos relacionados. A formação de conceitos corresponde à aquisição dessas habilidades.[79][80][75] Foi sugerido que os animais também são capazes de aprender conceitos em certa medida. Isto se deve à sua capacidade de discriminar entre diferentes tipos de situações e ajustar seu comportamento de acordo.[77][81]
No caso da resolução de problemas, o pensamento visa alcançar um objetivo predefinido, superando certos obstáculos.[7][1][78] Este processo muitas vezes envolve duas formas diferentes de pensamento. Por um lado, o pensamento divergente visa encontrar tantas soluções alternativas quanto possível. Por outro lado, o pensamento convergente tenta reduzir o leque de alternativas aos candidatos mais promissores.[1][82][83] Alguns pesquisadores identificam várias etapas no processo de resolução de problemas. Estes passos incluem reconhecer o problema, tentar entender sua natureza, identificar critérios gerais que a solução deve e cumprir, decidir como esses critérios devem ser priorizados, monitorar o progresso e avaliar os resultados.[1]
Uma distinção importante diz respeito ao tipo de problema que é enfrentado. Para problemas bem estruturados, é fácil determinar quais passos devem ser tomados para resolvê-los, mas executar esses passos ainda pode ser difícil.[1][84] Para problemas mal estruturados, por outro lado, não está claro quais passos devem ser tomados, ou seja, não há uma fórmula clara que conduza ao sucesso se seguida corretamente. Neste caso, a solução às vezes vir em um lampejo de inspiração em que o problema é visto de repente sob uma nova luz.[1][84] Outra maneira de categorizar diferentes formas de resolução de problemas é distinguir entre algoritmos e heurísticas.[78] Um algoritmo é um procedimento formal no qual cada passo é claramente definido. Garante o sucesso se aplicado corretamente.[1][78] A multiplicação longa geralmente ensinada na escola é um exemplo de um algoritmo para resolver o problema de multiplicar números grandes. As heurísticas, por outro lado, são procedimentos informais. São regras gerais grosseiras que tendem a aproximar o pensador da solução, mas o sucesso não é garantido em todos os casos, mesmo se seguidas corretamente.[1][78] Exemplos de heurísticas são trabalhar para frente e trabalhar para trás. Estas abordagens envolvem planejar um passo de cada vez, seja começando no início e avançando ou começando no final e retrocedendo. Assim, ao planejar uma viagem, pode-se planejar as diferentes etapas da viagem da origem ao destino na ordem cronológica de como a viagem será realizada, ou na ordem inversa.[1]
Obstáculos à resolução de problemas podem surgir da falha do pensador em levar em conta certas possibilidades, fixando-se em uma linha de ação específica.[1] Há diferenças importantes entre como os novatos e os especialistas resolvem problemas. Por exemplo, os especialistas tendem a alocar mais tempo para conceituar o problema e trabalhar com representações mais complexas, enquanto os novatos tendem a dedicar mais tempo à execução de soluções putativas.[1]
A deliberação é uma forma importante de pensamento prático. Visa formular possíveis cursos de ação e avaliar seu valor considerando as razões a favor e contra eles.[85] Isso envolve previsão para antecipar o que pode acontecer. Com base nesta previsão, diferentes cursos de ação podem ser formulados para influenciar o que vai acontecer. As decisões são uma parte importante da deliberação. Trata-se de comparar cursos de ação alternativos e escolher o mais favorável.[66][21] A teoria da decisão é um modelo formal de como os agentes racionais ideais tomariam decisões.[78][86][87] Baseia-se na ideia de que devem sempre escolher a alternativa com o maior valor esperado. Cada alternativa pode levar a vários resultados possíveis, cada um dos quais tem um valor diferente. O valor esperado de uma alternativa consiste na soma dos valores de cada resultado associado a ela multiplicado pela probabilidade de que este resultado ocorra.[86][87] De acordo com a teoria da decisão, uma decisão é racional se o agente escolhe a alternativa associada ao maior valor esperado, avaliada a partir da perspectiva do próprio agente.[86][87]
Vários teóricos enfatizam a natureza prática do pensamento, ou seja, que o pensamento é normalmente guiado por algum tipo de tarefa que visa resolver. Neste sentido, o pensamento tem sido comparado ao método de tentativa e erro que se observa no comportamento animal quando confrontado com um novo problema. Nesta visão, a diferença importante é que este processo acontece internamente como uma forma de simulação.[1] Este processo é normalmente muito mais eficiente, pois uma vez que a solução é encontrada no pensamento, apenas o comportamento correspondente à solução encontrada deve ser realizado externamente e não todos os outros.[1]
Quando o pensamento é entendido em um sentido amplo, inclui tanto a memória episódica quanto a imaginação.[20] Na memória episódica, eventos experimentados no passado são revividos.[88][89][90] É uma forma de viagem mental no tempo na qual a experiência passada é reexperimentada.[90][91] Mas isto não constitui uma cópia exata da experiência original, pois a memória episódica envolve aspectos e informações adicionais não presentes na experiência original. Isto inclui tanto um sentimento de familiaridade quanto informações cronológicas sobre o evento passado em relação ao presente.[88][90] A memória visa representar como as coisas realmente foram no passado, em contraste com a imaginação, que apresenta objetos sem o objetivo de mostrar como as coisas realmente são ou foram.[92] Devido a este elo perdido com a realidade, mais liberdade está envolvida na maioria das formas de imaginação: seu conteúdo pode ser livremente variado, alterado e recombinado para criar novos arranjos nunca antes experimentados.[93] A memória episódica e a imaginação têm em comum com outras formas de pensamento que podem surgir internamente sem qualquer estimulação dos órgãos sensoriais.[94][93] Mas ainda estão mais próximas da sensação que formas de pensamento mais abstratas, pois apresentam conteúdos sensoriais que, pelo menos em princípio, também poderiam ser percebidos.
O pensamento consciente é a forma paradigmática de pensar e é muitas vezes o foco da pesquisa correspondente. Mas foi argumentado que algumas formas de pensamento também acontecem no nível inconsciente.[9][10][4][5] O pensamento inconsciente é o pensamento que acontece em segundo plano sem ser experimentado. Portanto, não é observado diretamente. Em vez disso, sua existência é geralmente inferida por outros meios.[10] Por exemplo, quando alguém é confrontado com uma decisão importante ou um problema difícil, pode não ser capaz de resolvê-lo de imediato. Mas então, mais tarde, a solução pode aparecer de repente diante da pessoa, embora nenhum passo consciente de pensamento tenha sido dado em direção a esta solução nesse ínterim.[10][9] Em tais casos, o trabalho cognitivo necessário para chegar a uma solução é muitas vezes explicado em termos de pensamentos inconscientes. A ideia central é que uma transição cognitiva aconteceu e precisamos postular pensamentos inconscientes para poder explicar como aconteceu.[10][9]
Foi argumentado que os pensamentos conscientes e inconscientes diferem não apenas em sua relação com a experiência, mas também em suas capacidades. De acordo com os teóricos do pensamento inconsciente, por exemplo, o pensamento consciente se destaca em problemas simples com poucas variáveis, mas é superado pelo pensamento inconsciente quando problemas complexos com muitas variáveis estão envolvidos.[10][9] Isto às vezes é explicado através da afirmação de que o número de itens em que se pode pensar conscientemente ao mesmo tempo é bastante limitado, enquanto o pensamento inconsciente carece de tais limitações.[10] Mas outros pesquisadores rejeitaram a afirmação de que o pensamento inconsciente é muitas vezes superior ao pensamento consciente.[95][96] Outras sugestões para a diferença entre as duas formas de pensamento incluem que o pensamento consciente tende a seguir leis lógicas formais, enquanto o pensamento inconsciente depende mais do processamento associativo, e que apenas o pensamento consciente é conceitualmente articulado e acontece através do meio da linguagem.[10][97]
Um neurônio (também chamado de célula nervosa) é uma célula excitável no sistema nervoso que processa e transmite informação por sinais eletroquímicos. Neurônios são o componente principal do cérebro, a medula espinhal dos vertebrados, Ventral nerve cord nos invertebrados, e os nervos periféricos. Existem vários tipos de neurônios especializados: neurônios sensoriais respondem ao toque, som, luz e outros numerosos estímulos que afetam as células dos órgãos sensoriais que então envia sinais para a medula espinhal e cérebro. Os neurônios motores recebem sinais do cérebro e da medula espinhal e causam a contração muscular e afetam glândulas. Interneurônios conectam neurônios a outros neurônios dentro do cérebro e medula espinhal. Neurônios respondem a estímulos, e comunicam a presença do estímulo para o sistema nervoso central, que processa a informação e envia uma resposta a outra parte do corpo para ação. Neurônios não passam por mitose, e usualmente não podem ser substituídos depois de destruídos, apesar de astrócitos terem sido observados se transformando em neurônios já que eles algumas vezes são pluripotentes.
Psicólogos têm se concentrado no pensar como uma manifestação intelectual com objetivo de responder a uma questão ou a solução de um problema prático. Para Skinner, o pensamento pode ser compreendido como um comportamento privado, verbal ou não verbal, encoberto, ou seja, não manifesto no sentido de que não pode ser detectado por outras pessoas e, que necessita ser explicado ou deduzido.[98]
A psicologia cognitiva é um ramo da psicologia que investiga os processos mentais internos como a resolução de problemas, memória, e linguagem.
A escola do pensamento surgida com esta aproximação é conhecida como cognitivismo, que está interessada em como as pessoas representam mentalmente o processamento da informação. Ela tem sua fundação na psicologia gestalt de Max Wertheimer, Wolfgang Köhler, e Kurt Koffka,[99] e no trabalho de Jean Piaget, que providenciou a teoria dos estágios/fases que descrevem o desenvolvimento cognitivo das crianças. Psicólogos cognitivos usam aproximações psicofísicas e experimentais para entender, diagnosticar e solucionar problemas, se concentrando nos processos mentais que mediam entre o estímulo e a resposta. Segundo a teoria cognitiva a solução de problemas toma forma de regras algorítmicas que não são necessariamente compreensíveis mas que prometem uma solução, ou regras heurísticas que são compreensíveis mas que nem sempre garantem a solução. A ciência cognitiva se diferencia da psicologia cognitiva no sentido de implementar algoritmos que pretendem simular o comportamento humano nos computadores. Em outras instâncias, soluções podem ser encontradas através de insight, perceber de repente o relacionamento das coisas.
Id, ego e superego são as três partes do "aparato psíquico" definido por Sigmund Freud com seu modelo estrutural da psique; eles são teoricamente os três blocos fundamentais ao descrever a vida em termos de atividade e interação mental. De acordo com esse modelo, o instinto não-coordenado tende a ser o "id"; a parte realista e organizada da psiquê o "ego", e a função crítica e moral o "superego".[100]
O inconsciente foi considerado por Freud através da evolução de sua teoria psicoanalítica a força senciente da vontade influenciada pelo desejo humano e ainda assim operando bem abaixo da percepção da mente consciente. Para Freud, o inconsciente é um armazenamento de desejos e necessidades movidas pelo instinto. Enquanto pensamentos passados e reminiscentes possam ser escondidos da consciência imediata, eles direcionam o pensamento e os sentimentos do indivíduo através do inconsciente.[101]
Para psicoanalistas, o inconsciente não inclui tudo o que não é consciente, mas apenas o que é reprimido ativamente pelo pensamento consciente ou o que a pessoa é avers a pensar conscientemente. Esta visão coloca o indivíduo como sendo adversário de seu inconsciente, lutando para manter escondido o que está inconsciente. Se a pessoa sente dor, tudo o que ela pode pensar é aliviar a dor. Todos os seus desejos, para acabar com a dor ou aproveitar algo, comandam a mente a fazer algo. Para Freud, o inconsciente era um repositório de ideias e desejos não aceitáveis socialmente, memórias traumáticas, e emoções dolorosas deixadas de lado pela mente pelo mecanismo de repressão psicológica. Entretanto, o conteúdo não precisa ser necessariamente apenas negativo. Na visão psicanalítica, o inconsciente é a força que só pode ser reconhecida pelos seus efeitos - ele se expressa através dos sintomas.[102]
A psicologia social é o estudo de como as pessoas e grupos interagem. Acadêmicos nesta área interdisciplinar são tipicamente ou psicologistas ou sociologistas, apesar de todos os psicólogos sociais usarem tanto o indivíduo como o grupo como suas unidades de análise.[103]
Apesar de suas similaridades, pesquisadores psicológicos e sociológicos tendem a diferenciar em suas metas, aproximações, métodos e terminologia. Eles também favorecem diferentes jornais acadêmicos e sociedades científicas. O maior período de colaboração entre sociologistas e psicologistas foi durante os anos imediatamente seguintes à Segunda Guerra Mundial.[104] Apesar de ter havido um aumento no isolamento e especialização nos anos recentes, permanece um certo grau de sobreposição e influência entre as duas disciplinas.[105]
O Inconsciente coletivo, às vezes conhecido como subconsciente coletivo, é um termo da psicologia analítica criado por Carl Jung. É parte da mente inconsciente, compartilhada por uma sociedade, pessoas, ou toda a humanidade, em um sistema interconectado que é o produto de toda a experiência comum e contêm conceitos como ciência, religião, e moral. Enquanto Sigmund Freud não distinguia entre a "psicologia individual" e a "psicologia coletiva", Jung distinguia o inconsciente coletivo do subconsciente pessoal particular de cada ser humano vivo. O inconsciente coletivo é também conhecido como "a reserva de experiência da nossa espécie".[106]
No capítulo "Definições" do seminário de Jung Tipos Psicológicos, na definição de "coletivo" Jung se referiu a representações coletivas, termo cunhado por Levy-Bruhl no livro de 1910 How Natives Think. Jung dizia que era isso que descrevia o inconsciente coletivo. Freud, por outro lado, não aceitava a ideia de um inconsciente coletivo.
O movimento de fenomenologia na filosofia viu uma mudança radical na forma como entendemos o pensamento. A análise fenomenológica de Martin Heidegger da estrutura existencial do homem em Ser e Tempo lança uma nova luz sobre a questão do pensar, trazendo inquietação à cognição tradicional ou às interpretações racionais do homem que afetam o modo como entendemos o pensamento. A noção do papel fundamental da compreensão não cognitiva em tornar possível a consciência temática contribuiu na discussão em torno da inteligência artificial durante os anos 1970 e 1980.[108]
A filosofia da mente é um ramo de ideias da filosofia analítica moderna que estuda a natureza da mente, os eventos mentais, funções mentais, propriedades mentais, consciência e seus relacionamentos com o corpo físico, particularmente o cérebro. O problema mente-corpo, isto é, o relacionamento entre  a mente e o corpo, é comumente visto como a questão central da filosofia da mente, apesar de haver outras questões envolvendo a natureza da mente que não envolvem sua relação com o corpo físico.[109]
O problema corpo-mente se preocupa em explicar a relação que existe entre a mente, ou processo mental, e o estado ou processo do corpo.[109] O principal objetivo dos filósofos que trabalham nesta área é determinar a natureza da mente e dos estados/processos mentais, e como - ou mesmo se - a mente é afetada pelo corpo e pode afetá-lo.
Nossas experiências perceptíveis dependem dos estímulos que chegam nos nossos vários órgãos sensoriais do mundo exterior e esses estímulos causam mudanças no nosso estado mental, nos fazendo sentir algo, o que pode ser bom ou ruim. O desejo de alguém por uma fatia de pizza, por exemplo, tende a fazer com que esta pessoa mova seu corpo de uma maneira e direção específica para obter o que ele quer.
A questão é, então, como pode ser possível que experiências conscientes surjam de uma massa de matéria cinzenta dotada de nada além de propriedades eletroquímicas. Um problema relacionado é o de explicar como as atitudes proposicionais de alguém (por exemplo, crenças e desejos) podem causar que os neurônios desse indivíduo trabalhem e seus músculos se contraiam exatamente da maneira correta. Esses são alguns dos enigmas que têm sido  enfrentados por epistemólogos e filósofos da mente desde pelo menos o tempo de René Descartes.[110]
A fenomenologia é a ciência da estrutura e dos conteúdos da experiência.[111][112] O termo "fenomenologia cognitiva" refere-se ao caráter experiencial do pensamento ou que se sente ao pensar.[4][113][114][6][115] Alguns teóricos afirmam que não existe uma fenomenologia cognitiva distintiva. Em tal visão, a experiência de pensar é apenas uma forma de experiência sensorial.[115][116][117] De acordo com uma versão, pensar envolve apenas ouvir uma voz internamente.[116] De acordo com outra, não há experiência de pensar além dos efeitos indiretos que o pensamento tem na experiência sensorial.[4][113] Uma versão mais fraca de tal abordagem permite que o pensamento possa ter uma fenomenologia distinta, mas afirma que o pensamento ainda depende da experiência sensorial porque não pode ocorrer por si só. Nesta visão, os conteúdos sensoriais constituem a base a partir da qual o pensamento pode surgir.[4][116][117]
Um experimento mental frequentemente citado em favor da existência de uma fenomenologia cognitiva distinta envolve duas pessoas ouvindo um programa de rádio em francês, uma que entende francês e a outra que não.[4][113][114][118] A ideia por trás deste exemplo é que ambos os ouvintes escutam os mesmos sons e, portanto, têm a mesma experiência não cognitiva. Para explicar a diferença, uma fenomenologia cognitiva distinta deve ser postulada: apenas a experiência da primeira pessoa tem este caráter cognitivo adicional, pois é acompanhada por um pensamento que corresponde ao significado do que é dito.[4][113][114][119] Outros argumentos para a experiência de pensar concentram-se no acesso introspectivo direto ao pensamento ou no conhecimento que o pensador tem de seus próprios pensamentos.[4][113][114]
Os fenomenólogos também se preocupam com os traços característicos da experiência de pensar. Fazer um juízo é uma das formas prototípicas da fenomenologia cognitiva.[114][120] Envolve a agência epistêmica, na qual se considera uma proposição e evidências a favor e contra ela. Com base nesse raciocínio, a proposição é afirmada ou rejeitada.[114] Às vezes, argumenta-se que a experiência da verdade é central para o pensamento, ou seja, que o pensamento visa representar como o mundo é.[6][113] Compartilha esta característica com a percepção, mas difere dela na forma como representa o mundo: sem o uso de conteúdos sensoriais.[6]
Um dos traços característicos frequentemente atribuídos ao pensamento e ao juízo é que são experiências predicativas, em contraste com a experiência pré-predicativa encontrada na percepção imediata.[121][122] Nessa visão, vários aspectos da experiência perceptiva assemelham-se a juízos sem serem juízos no sentido estrito.[4][123][124] Por exemplo, a experiência perceptiva da frente de uma casa traz consigo várias expectativas sobre aspectos da casa não vistos diretamente, como o tamanho e a forma de seus outros lados. Este processo é às vezes chamado de apercepção.[4][123] Estas expectativas assemelham-se a juízos e podem estar erradas. Este seria o caso quando se descobrisse, ao caminhar ao redor da "casa", que não é uma casa, mas apenas uma fachada frontal de uma casa sem nada atrás dela. Neste caso, as expectativas perceptivas são frustradas e o perceptor fica surpreso.[4] Há desacordo sobre se esses aspectos pré-predicativos da percepção regular devem ser entendidos como uma forma de fenomenologia cognitiva envolvendo o pensamento.[4] Esta questão também é importante para compreender a relação entre pensamento e linguagem. A razão disso é que as expectativas pré-predicativas não dependem da linguagem, que às vezes é tomada como exemplo para o pensamento não linguístico.[4] Vários teóricos argumentaram que a experiência pré-predicativa é mais básica ou fundamental, já que a experiência predicativa é, em certo sentido, construída sobre ela e, portanto, depende dela.[124][121][122]
Outra forma como os fenomenólogos tentaram distinguir a experiência de pensar de outros tipos de experiências é em relação às intenções vazias, em contraste com as intenções intuitivas.[125][126] Neste contexto, "intenção" significa que algum tipo de objeto é experimentado. Nas intenções intuitivas, o objeto é apresentado através de conteúdos sensoriais. As intenções vazias, por outro lado, apresentam seu objeto de forma mais abstrata sem a ajuda de conteúdos sensoriais.[125][4][126] Assim, ao perceber um pôr-do-sol, ele é apresentado através de conteúdos sensoriais. O mesmo pôr-do-sol também pode ser apresentado de forma não intuitiva quando meramente se pensa nele sem a ajuda de conteúdos sensoriais.[126] Nesses casos, as mesmas propriedades são atribuídas aos objetos. A diferença entre estes modos de apresentação não diz respeito às propriedades atribuídas ao objeto apresentado, mas a como o objeto é apresentado.[125] Devido a esta coincidência, é possível que as representações pertencentes a diferentes modos se sobreponham ou divirjam.[6] Por exemplo, ao procurar os óculos, pode-se pensar para si mesmo que os deixou na mesa da cozinha. Essa intenção vazia dos óculos sobre a mesa da cozinha se cumpre então intuitivamente quando se vê-los lá ao chegar na cozinha. Desta forma, uma percepção pode confirmar ou refutar um pensamento, dependendo de se as intuições vazias são cumpridas mais tarde ou não.[6][126]
A Logosofia, funciona em prol da auto-superação humana, por processos de evolução consciente, sabendo-se que a sabedoria soberana pertence a Deus. Considera os pensamentos entidades psicológicas autônomas que atuam, em geral, independentemente da vontade do indivíduo e gravitam sobre ele de forma muitas vezes despótica. Se geram na mente humana, onde nascem e cumprem suas funções sob a influência de estados psíquicos ou morais, próprios ou de outrem.[127][128]
Sobre o ato de pensar, define-se como um esforço consciente que habilita uma função cerebral, para que a mente possa atuar, coordenando os elementos dispersos que entrarão na formação de um pensamento cuja solução se busca.[carece de fontes?]
Quanto à natureza e percepção dos pensamentos, Pecotche (2005, p.55) afirma que:
A Logosofia estabeleceu uma quádrupla e interdependente classificação de pensamentos, a saber:[128]
Esta é uma das formas de classificação logosófica mas não a única. Na Logosofia se mostra como de fundamental importância identificar e classificar os pensamentos, visto que efetuada tal classificação que a princípio se consiga fazer, já não será difícil proceder à sua seleção, e esta deverá ser seguida por uma superação constante de tais pensamentos, com base no estudo e nas experiências que surjam à medida que destes se faça uso no curso do processo de evolução que se inicia.[127]
Os transtornos do pensamento podem ser divididos em transtornos de curso, de conteúdo do pensamento e, em certos casos se adiciona um terceiro grupo, os transtornos de vivência do pensamento.
O curso do pensamento é o caminho que segue o pensamento para raciocinar, falar, informar, etc, e inclui a fluidez do pensamento, como se formulam, organizam, e apresentam os pensamentos de um indivíduo. Em todo raciocínio há um fio condutor que leva um pensamento a outro. Este fio pode conter falhas, que causam os transtornos no curso de pensamento.
Os transtornos da velocidade incluem patologias que afetam a quantidade e a velocidade dos pensamentos. Seus principais transtornos são os seguintes:[129]
Os transtornos da forma propriamente dita incluem patologias de direcionalidade e a continuidade do pensamento. Os mais significativos incluem:[129]
Os principais transtornos incluem:[129]
Tradicionalmente, o termo "leis do pensamento" refere-se a três leis fundamentais da lógica: o princípio da não-contradição, o princípio do terceiro excluído e o princípio da identidade.[130][131] Estas leis por si só não são suficientes como axiomas da lógica, mas podem ser vistas como precursoras importantes da axiomatização moderna da lógica. O princípio da não-contradição afirma que, para qualquer proposição, é impossível que tanto ela como sua negação sejam verdadeiras: . De acordo com o princípio do terceiro excluído, para qualquer proposição, ou ela ou seu oposto é verdadeira: . O princípio da identidade afirma que qualquer objeto é idêntico a si mesmo: .[130][131] Há diferentes concepções de como as leis do pensamento devem ser entendidas. As interpretações mais relevantes para o pensamento são entendê-las como leis prescritivas de como se deve pensar ou como leis formais de proposições que são verdadeiras apenas por causa de sua forma e independentemente de seu conteúdo ou contexto.[131] As interpretações metafísicas, por outro lado, as veem como expressando a natureza do "ser como tal".[131]
Embora haja uma aceitação muito ampla destas três leis entre os lógicos, elas não são universalmente aceitas.[130][131] Aristóteles, por exemplo, sustentava que há alguns casos em que o princípio do terceiro excluído é falsa. Isto diz respeito principalmente a eventos futuros incertos. Em sua opinião, atualmente "não é nem verdadeiro nem falso que amanhã haverá uma batalha naval".[130][131] A lógica intuicionista moderna também rejeita o princípio do terceiro excluído. Esta rejeição baseia-se na ideia de que a verdade matemática depende da verificação através de uma prova. O princípio falha para casos em que tal prova não é possível, que existem em todos os sistemas formais suficientemente fortes, de acordo com os teoremas de incompletude de Gödel.[132][133][130][131] Os dialeteístas, por outro lado, rejeitam o princípio da não-contradição, sustentando que algumas proposições são tanto verdadeiras quanto falsas. Uma motivação desta posição é evitar certos paradoxos na lógica clássica e na teoria dos conjuntos, como o paradoxo do mentiroso e o paradoxo de Russell. Um de seus problemas é encontrar uma formulação que contorne o princípio da explosão, ou seja, que qualquer coisa decorre de uma contradição.[134][135][136]
Algumas formulações das leis do pensamento incluem uma quarta lei: o princípio de razão suficiente.[131] Afirma que tudo tem uma razão, fundamento ou causa suficiente. Está intimamente ligado à ideia de que tudo é inteligível ou pode ser explicado em referência a sua razão suficiente.[137][138] De acordo com essa ideia, deveria haver sempre uma explicação completa, pelo menos em princípio, para questões como porque o céu é azul ou porque a Segunda Guerra Mundial aconteceu. Um problema para incluir este princípio entre as leis do pensamento é que ele é um princípio metafísico, a diferença das outras três leis, que pertencem principalmente à lógica.[138][131][137]
O pensamento contrafactual envolve representações mentais de situações e eventos não reais, ou seja, do que é "contrário aos fatos".[139][140] Geralmente é condicional: visa avaliar qual seria o caso se uma determinada condição tivesse obtida.[141][142] Neste sentido, tenta responder a perguntas do tipo "que aconteceria se". Por exemplo, pensar após um acidente que se estaria morto se não tivesse usado o cinto de segurança é uma forma de pensamento contrafactual: assume, ao contrário dos fatos, que não usou o cinto de segurança e tenta avaliar o resultado deste estado de coisas.[140] Neste sentido, o pensamento contrafactual é normalmente contrafactual apenas em um pequeno grau, já que apenas alguns fatos são alterados, como os relativos ao cinto de segurança, enquanto a maioria dos outros fatos são mantidos, como aquele que a pessoa estava dirigindo, seu gênero, as leis da física, etc.[139] Quando entendido no sentido mais amplo, há formas de pensamento contrafactual que não envolvem nada contrário aos fatos em absoluto.[142] Este é o caso, por exemplo, quando se tenta antecipar o que pode acontecer no futuro se ocorrer um evento incerto e esse evento realmente ocorre mais tarde e traz consigo as consequências antecipadas.[141] Neste sentido mais amplo, o termo "condicional subjuntivo" (subjunctive conditional) é às vezes usado em vez de "condicional contrafactual" (counterfactual conditional).[142] Mas os casos paradigmáticos do pensamento contrafactual envolvem alternativas a eventos passados.[139]
O pensamento contrafactual desempenha um papel importante, pois avaliamos o mundo ao nosso redor não apenas pelo que realmente aconteceu, mas também pelo que poderia ter acontecido.[140] Os seres humanos têm uma tendência maior a se envolver no pensamento contrafactual depois que algo ruim aconteceu por causa de algum tipo de ação que o agente realizou.[141][139] Neste sentido, muitos arrependimentos estão associados ao pensamento contrafactual no qual o agente contempla como um resultado melhor poderia ter sido obtido se tivesse agido de forma diferente.[140] Estes casos são conhecidos como contrafactuais ascendentes, em contraste com os contrafactuais descendentes, nos quais o cenário contrafactual é pior que a realidade.[141][139] O pensamento contrafactual ascendente é geralmente experimentado como desagradável, já que apresenta as circunstâncias reais sob uma luz ruim. Isto contrasta com as emoções positivas associadas ao pensamento contrafactual descendente.[140] Mas ambas as formas são importantes, pois é possível aprender com elas e ajustar o comportamento de acordo para obter melhores resultados no futuro.[140][139]
Os experimentos mentais envolvem pensar em situações imaginárias, muitas vezes com o objetivo de investigar as possíveis consequências de uma mudança na sequência real de eventos.[143][144][145] É uma questão controversa até que ponto os experimentos mentais devem ser entendidos como experimentos reais.[146][147][148] São experimentos no sentido de que uma determinada situação é criada e se tenta aprender com essa situação entendendo o que se segue dela.[149][146] Eles diferem dos experimentos regulares na medida em que a imaginação é usada para criar a situação e o raciocínio contrafactual é empregado para avaliar o que se segue dela, em vez de criá-la fisicamente e observar as consequências através da percepção.[150][144][146][145] O raciocínio contrafactual, portanto, desempenha um papel central nos experimentos de pensamento.[151]
O argumento do quarto chinês é um famoso experimento mental proposto por John Searle.[152][153] Envolve uma pessoa sentada em um quarto fechado, encarregada de responder a mensagens escritas em chinês. Essa pessoa não sabe chinês, mas tem um livro de regras gigante que especifica exatamente como responder a qualquer mensagem possível, semelhante a como um computador reagiria às mensagens. A ideia central desse experimento mental é que nem a pessoa nem o computador entendem chinês. Desta forma, Searle pretende mostrar que os computadores carecem de uma mente capaz de formas mais profundas de compreensão, apesar de agir de forma inteligente.[152][153]
Os experimentos mentais são empregados para vários propósitos, por exemplo, para entretenimento, educação ou como argumentos a favor ou contra teorias. A maioria das discussões se concentra em seu uso como argumentos. Este uso é encontrado em campos como a filosofia, as ciências naturais e a história.[144][148][147][146] É controverso, pois há muita discordância sobre o estado epistêmico dos experimentos mentais, ou seja, quão confiáveis eles são como evidências para apoiar ou refutar uma teoria.[144][148][147][146] Central para a rejeição desse uso é o fato de que eles pretendem ser uma fonte de conhecimento sem a necessidade de deixar a poltrona em busca de novos dados empíricos. Os defensores dos experimentos mentais geralmente afirmam que as intuições que subjazem e guiam os experimentos mentais são, pelo menos em alguns casos, confiáveis.[144][146] Mas os experimentos mentais também podem falhar se não forem devidamente apoiados por intuições ou se forem além do que as intuições suportam.[144][145] Neste último sentido, às vezes são propostos contraexperimentos mentais que modificam ligeiramente o cenário original para mostrar que as intuições iniciais não podem sobreviver a esta mudança.[144] Várias taxonomias de experimentos mentais foram sugeridas. Eles podem ser distinguidos, por exemplo, por se são bem ou mal sucedidos, pela disciplina que os utiliza, por seu papel em uma teoria, ou por se aceitam ou modificam as leis reais da física.[145][144]
O pensamento crítico é uma forma de pensamento razoável, reflexiva e focada em determinar o que acreditar ou como agir.[154][155][156] Adere a vários padrões, tais como clareza e racionalidade. Neste sentido, envolve não apenas processos cognitivos que tentam resolver o problema em questão, mas, ao mesmo tempo, processos metacognitivos assegurando que ele cumpra seus próprios padrões.[155] Isto inclui avaliar tanto que o raciocínio em si é sólido quanto que a evidência em que se baseia é confiável.[155] Isto significa que a lógica desempenha um papel importante no pensamento crítico. Trata-se não apenas da lógica formal, mas também da lógica informal, especificamente para evitar várias falácias informais devido a expressões vagas ou ambíguas na linguagem natural.[155][74][73] Não existe uma definição standard geralmente aceita de "pensamento crítico", mas há uma sobreposição significativa entre as definições propostas em sua caracterização do pensamento crítico como cuidadoso e direcionado a objetivos.[156] De acordo com algumas versões, apenas as próprias observações e experimentos do pensador são aceitos como evidência no pensamento crítico. Alguns o restringem à formação de juízos, mas excluem a ação como seu objetivo.[156]
Um exemplo concreto cotidiano de pensamento crítico, devido a John Dewey, envolve observar bolhas de espuma movendo-se em uma direção contrária às expectativas iniciais. O pensador crítico tenta encontrar várias explicações possíveis deste comportamento e depois modifica ligeiramente a situação original para determinar qual é a explicação correta.[156][157] Mas nem todas as formas de processos cognitivamente valiosos envolvem o pensamento crítico. Chegar à solução correta de um problema seguindo cegamente os passos de um algoritmo não se qualifica como pensamento crítico. O mesmo é verdade se a solução for apresentada ao pensador em um súbito lampejo de inspiração e aceita de imediato.[156]
O pensamento crítico desempenha um papel importante na educação: fomentar a capacidade do aluno de pensar criticamente é muitas vezes visto como um objetivo educacional importante.[156][155][158] Neste sentido, é importante transmitir não apenas um conjunto de crenças verdadeiras ao aluno, mas também a capacidade de tirar suas próprias conclusões e de questionar crenças pré-existentes.[158] As habilidades e disposições aprendidas desta maneira podem beneficiar não apenas o indivíduo, mas também a sociedade em geral.[155] Os críticos da ênfase no pensamento crítico na educação argumentam que não há uma forma universal de pensamento correto. Em vez disso, afirmam que assuntos diferentes dependem de padrões diferentes e a educação deveria se concentrar em transmitir essas habilidades específicas do assunto em vez de tentar ensinar métodos universais de pensamento.[156][159] Outras objeções baseiam-se na ideia de que o pensamento crítico e a atitude subjacente a ele envolvem vários vieses injustificados, como egocentrismo, objetividade distanciada, indiferença e uma ênfase excessiva do teórico em contraste com o prático.[156]
O pensamento positivo é um tema importante na psicologia positiva.[160] Envolve concentrar a atenção nos aspectos positivos da situação e, assim, retirar a atenção de seus lados negativos.[160] Isto é geralmente visto como uma perspectiva global que se aplica especialmente ao pensamento, mas também inclui outros processos mentais, como o sentimento.[160] Neste sentido, está intimamente relacionado ao otimismo. Inclui a expectativa de que coisas positivas aconteçam no futuro.[161][160] Essa perspectiva positiva torna mais provável que as pessoas tentem alcançar novos objetivos.[160] Também aumenta a probabilidade de continuar a lutar por objetivos pré-existentes que parecem difíceis de alcançar, em vez de simplesmente desistir.[161][160]
Os efeitos do pensamento positivo ainda não são exaustivamente pesquisados, mas alguns estudos sugerem que há uma correlação entre o pensamento positivo e o bem-estar.[160] Por exemplo, estudantes e mulheres grávidas com uma perspectiva positiva tendem a lidar melhor com situações estressantes.[161][160] Isto às vezes é explicado apontando que o estresse não é inerente a situações estressantes, mas depende de como o agente interpreta a situação. Portanto, o estresse reduzido pode ser encontrado em pensadores positivos porque eles tendem a ver tais situações de uma forma mais positiva.[160] Mas os efeitos também incluem o domínio prático em que os pensadores positivos tendem a empregar estratégias de enfrentamento mais saudáveis quando confrontados com situações difíceis.[160] Isso afeta, por exemplo, o tempo necessário para se recuperar totalmente das cirurgias e a tendência de retomar o exercício físico depois.[161]
Mas foi argumentado que se o pensamento positivo realmente leva a resultados positivos depende de vários outros fatores. Sem esses fatores, pode levar a resultados negativos. Por exemplo, a tendência dos otimistas de continuar se esforçando em situações difíceis pode sair pela culatra se o curso dos eventos estiver fora do controle do agente.[161] Outro perigo associado ao pensamento positivo é que ele pode permanecer apenas no nível de fantasias irrealistas e, assim, não fazer uma contribuição prática positiva à vida do agente.[162] O pessimismo, por outro lado, pode ter efeitos positivos, pois pode mitigar as decepções ao antecipar falhas.[161][163]
O pensamento positivo é um tema recorrente na literatura de autoajuda.[164] Aqui, muitas vezes se afirma que se pode melhorar significativamente a vida tentando pensar positivamente, mesmo que isso signifique fomentar crenças contrárias às evidências.[165] Tais afirmações e a eficácia dos métodos sugeridos são controversas e são criticadas devido à sua falta de evidência científica.[165][166] No movimento do Novo Pensamento, o pensamento positivo figura na lei da atração, a afirmação pseudocientífica de que os pensamentos positivos podem influenciar diretamente o mundo externo, atraindo resultados positivos.[167]
O confucionismo ou confucianismo, também conhecido como ruísmo ou classicismo ru,[1] é um sistema de pensamento e comportamento originário da China antiga. Variadamente descrito como uma tradição, uma filosofia, uma religião humanista ou racionalista, um modo de governar, ou simplesmente um modo de vida,[2] o confucionismo desenvolveu-se a partir do que mais tarde foi chamado de cem escolas de pensamento a partir dos ensinamentos do filósofo chinês Confúcio (551–479 a.C.).
Confúcio se considerava um transmissor de valores culturais herdados das dinastias Xia (c. 2070–1600 a.C.), Xangue (c. 1600–1046 a.C.) e Zhou ocidental (c. 1046–771 a.C.).[3] O confucionismo foi suprimido durante a dinastia Qin legalista e autocrática (221–206 a.C.), mas sobreviveu. Durante a dinastia Han (206 a.C.—220 d.C.), as abordagens confucionistas superaram o "proto-taoísta" Huang-Lao como a ideologia oficial, enquanto os imperadores misturavam ambos com as técnicas realistas do legalismo.[4]
Um renascimento confucionista começou durante a dinastia Tang (618–907). No final desse período, o confucionismo se desenvolveu em resposta ao budismo e ao taoísmo e foi reformulado como neoconfucionismo. Essa forma revigorada foi adotada como base dos exames imperiais e a filosofia central da classe oficial acadêmica na dinastia Sung (960–1297). A abolição do sistema de exames em 1905 marcou o fim do confucionismo oficial. Os intelectuais do Movimento Quatro de Maio do início do século XX culpou o confucionismo pelas fraquezas da China. Eles buscaram novas doutrinas para substituir os ensinamentos confucionistas; algumas dessas novas ideologias incluem os "Três Princípios do Povo" com o estabelecimento da República da China, e depois o maoísmo sob a República Popular da China. No final do século XX, a ética do trabalho confucionista foi creditada com a ascensão da economia do Leste Asiático.[4]
Com particular ênfase na importância da família e harmonia social, ao invés de uma fonte sobrenatural de valores espirituais,[5] o núcleo do confucionismo é humanista.[6] De acordo com a conceituação de Herbert Fingarette do confucionismo como um sistema filosófico que considera "o secular como sagrado",[7] o confucionismo transcende a dicotomia entre religião e humanismo, considerando as atividades comuns da vida humana — e especialmente as relações humanas — como manifestação do sagrado,[8] porque são a expressão da natureza moral da humanidade (xìng 性), que tem uma ancoragem transcendente no Céu (Tiān 天).[9] Enquanto Tiān tem algumas características que se sobrepõem à categoria de divindade, é principalmente um princípio absoluto impessoal, como o Dào (道) ou o bramã. O confucionismo se concentra na ordem prática que é dada por uma consciência mundana do Tiān.[10] A liturgia confucionista (chamada 儒 rú, ou às vezes no chinês tradicional: 正統, chinês simplificado: 正统, pinyin: zhèngtǒng, significando 'ortopraxia') liderado por sacerdotes confucionistas ou "sábios de ritos" (chinês tradicional: 禮生, chinês simplificado: 礼生, pinyin: lǐshēng) adorar os deuses em templos chineses públicos e ancestrais é preferido em certas ocasiões, por grupos religiosos confucionistas e por ritos religiosos civis, sobre o ritual taoísta ou popular.[11]
A preocupação mundana do confucionismo repousa sobre a crença de que os seres humanos são fundamentalmente bons, ensináveis e aperfeiçoáveis por meio de esforços pessoais e comunitários, especialmente autocultivo e autocriação. O pensamento confucionista se concentra no cultivo da virtude em um mundo moralmente organizado. Alguns dos conceitos e práticas éticas básicas confucionistas incluem rén, yì e lǐ, e zhì. Rén (仁, 'benevolência' ou 'humanidade') é a essência do ser humano que se manifesta como compaixão. É a forma-virtude do Céu.[12] Yì (chinês tradicional: 義, chinês simplificado: 义) é a defesa da justiça e a disposição moral para fazer o bem. Lǐ (chinês tradicional: 禮, chinês simplificado: 礼) é um sistema de normas rituais e propriedade que determina como uma pessoa deve agir corretamente na vida cotidiana em harmonia com a lei do Céu. Zhì (智) é a capacidade de ver o que é certo e justo, ou o inverso, nos comportamentos exibidos pelos outros. O confucionismo despreza a pessoa, passiva ou ativamente, pelo fracasso em defender os valores morais cardinais de rén e yì.
Tradicionalmente, culturas e países da esfera cultural do Leste Asiático são fortemente influenciados pelo confucionismo, incluindo China, Taiwan, Coreia, Japão e Vietnã, bem como vários territórios colonizados predominantemente por chineses han, como Singapura. Hoje, foi creditado por moldar as sociedades do Leste Asiático e as comunidades chinesas no exterior e, até certo ponto, outras partes da Ásia.[13][14] Nas últimas décadas tem havido conversas de um "revival confucionista" na comunidade acadêmica,[15][16] e houve uma proliferação popular de vários tipos de igrejas confucionistas.[17] No final de 2015, muitas personalidades confucionistas estabeleceram formalmente uma Igreja Nacional do Santo Confucionismo (chinês tradicional: 孔聖會, chinês simplificado: 孔圣会, pinyin: Kǒngshènghuì) na China para unificar as muitas congregações confucionistas e organizações da sociedade civil.
Estritamente falando, não há nenhum termo em chinês que corresponda diretamente ao "confucionismo". Na língua chinesa, o caractere rú 儒 que significa "erudito" ou "erudito" ou "homem refinado" é geralmente usado tanto no passado quanto no presente para se referir a coisas relacionadas ao confucionismo. O caractere rú na China antiga tinha significados diversos. Alguns exemplos incluem "domar", "moldar", "educar", "refinar".[19]:190–197 Vários termos diferentes, alguns dos quais de origem moderna, são usados em diferentes situações para expressar diferentes facetas do confucionismo, incluindo:
Três deles usam rú. Esses termos não usam o nome "Confúcio", mas concentram-se no ideal do homem confucionista. O uso do termo "confucionismo" foi evitado por alguns estudiosos modernos, que preferem "ruísmo" e "ruístas". Robert Eno argumenta que o termo foi "sobrecarregado... com as ambiguidades e associações tradicionais irrelevantes". O ruísmo, como ele afirma, é mais fiel ao nome original chinês da escola.[19]:7
O termo "tradicionalista" foi sugerido por David Schaberg para enfatizar a conexão com o passado, seus padrões e formas herdadas, nas quais o próprio Confúcio deu tanta importância.[20] Esta tradução da palavra rú é seguida por, por exemplo, Yuri Pines.[21]
De acordo com Zhou Youguang, 儒 rú originalmente se referia a métodos xamânicos de realizar ritos e existia antes dos tempos de Confúcio, mas com Confúcio passou a significar devoção à propagação de tais ensinamentos para trazer a civilização ao povo. O confucionismo foi iniciado pelos discípulos de Confúcio, desenvolvido por Mêncio (c. 372–289 a.C.) e herdado por gerações posteriores, passando por constantes transformações e reestruturações desde seu estabelecimento, mas preservando os princípios de humanidade e retidão em seu núcleo.[22]
Tradicionalmente, pensava-se que Confúcio era o autor ou editor dos cinco clássicos, que eram os textos básicos do confucionismo. O estudioso Yao Xinzhong admite que há boas razões para acreditar que os clássicos confucionistas tomaram forma nas mãos de Confúcio, mas que "nada pode ser dado como certo em relação às primeiras versões dos clássicos". Yao diz que talvez a maioria dos estudiosos hoje tenha a visão "pragmática" de que Confúcio e seus seguidores, embora não pretendessem criar um sistema de clássicos, "contribuíram para sua formação".[23]
O estudioso Tu Weiming explica esses clássicos como incorporando "cinco visões" que fundamentam o desenvolvimento do confucionismo:
A humanidade é o núcleo no confucionismo. Uma maneira simples de apreciar o pensamento de Confúcio é considerá-lo como sendo baseado em diferentes níveis de honestidade, e uma forma simples de entender o pensamento de Confúcio é examinar o mundo usando a lógica da humanidade. Na prática, os elementos do confucionismo acumularam-se ao longo do tempo. Existe o clássico Wuchang, constituído por cinco elementos: Ren (仁, a Humanidade), Yi (justiça), Li (礼, ritual), Zhi, (知, conhecimento) e Xin (信, integridade), e há também o Sizi clássico, com quatro elementos: Zhong (忠, lealdade), Xiao (孝, a piedade filial), Jie (节, continência) e Yi (义, justiça). Há ainda muitos outros elementos, tais como o Cheng (诚, honestidade), Shu (恕, bondade e perdão), Lian (廉, honestidade e pureza), Chi (耻, vergonha, juízo e senso de certo e errado), Yong (勇, bravura), Wen (温, amável e gentil), Liang (良, bom, bom coração), Gong (恭, respeitoso, reverente), Jian (俭, frugal) e Rang (让, modéstia, discrição). Entre todos os elementos, o Ren (Humanidade) e o Yi (Justiça) são fundamentais. Às vezes, a moralidade é interpretada como o fantasma da Humanidade e da Justiça.[25]
Ver agir em relação aos outros, mas de uma atitude subjacente da humanidade. O conceito de Confúcio de humanidade (仁, ren) é provavelmente melhor expresso na versão confucionista de Ética da reciprocidade, ou a Regra de Ouro: "não faça aos outros o que você não gostaria que fizessem a si".
Confúcio nunca disse se o homem nasce bom ou mau,[26] observando que, naturalmente, os homens são semelhantes, mas, na prática, são diferentes.[27] Confúcio percebeu que todos os homens nascem com semelhanças intrínsecas, mas também que o homem é condicionado e influenciado pelo estudo e pela prática. A opinião de Xunzi é que os homens originalmente só querem o que eles instintivamente querem, apesar dos resultados positivos ou negativos que aquilo pode trazer; por isso o desenvolvimento é necessário. Do ponto de vista de Mêncio todos os homens nascem para compartilhar a bondade, como a compaixão e o bom coração, embora possam se tornar malignos. O texto clássico dos Três Personagens começa assim: "As pessoas no momento em que nascem são naturalmente boas (bondosas)", que decorre da ideia de Mêncio. Todos os pontos de vista, eventualmente, levam ao reconhecimento da importância da educação humana e do desenvolvimento.
O Ren também tem uma dimensão política. Se o governante não tem o Ren, o confucionismo diz que será difícil, se não impossível, para os seus súditos comportarem-se humanamente. O Ren é a base da teoria política confuciana: pressupõe um governante autocrático, exortado a não agir desumanamente com seus súditos. Um governante desumano corre o risco de perder o "Mandato dos Céus", o direito de governar. Um governante sem tal mandato não precisa ser obedecido. Mas um governante que reina de forma humana e cuida do povo deve ser obedecido rigorosamente, pois a benevolência de seu governo mostra que ele foi incumbido pelo céu. O próprio Confúcio tinha pouco a dizer sobre a vontade do povo, mas seu principal seguidor, Mêncio, disse em uma ocasião que a opinião das pessoas sobre certos assuntos importantes devem ser consideradas.
Ao contrário de profetas de religiões monoteístas, Confúcio não pregava uma teologia que conduzisse a humanidade a uma redenção pessoal. Pregava uma filosofia que buscava a redenção do Estado mediante a correção do comportamento individual. Tratava-se de uma doutrina orientada para esse mundo, pregava um código de conduta social e não um caminho para a vida após a morte.[28]
No Confucionismo, o termo "ritual" logo foi estendido para incluir o comportamento cerimonial secular e, eventualmente, refere-se também ao decoro ou polidez que se vê no dia a dia. Rituais foram codificados e tratados como um sistema completo de normas. O próprio Confúcio tentou reanimar a etiqueta das dinastias antigas. Após sua morte, as pessoas o viam como uma grande autoridade sobre os comportamentos dos rituais.
É importante notar que o "ritual" desenvolveu um significado específico no confucionismo, ao contrário de seus significados religiosos usuais. No confucionismo, os atos da vida cotidiana são considerados rituais. Os rituais não são necessariamente regimentados ou práticas arbitrárias, mas sim as rotinas em que muitas vezes as pessoas se inserem, consciente ou inconscientemente, durante o curso normal de suas vidas. Moldar os rituais de uma forma que leve a uma sociedade saudável e satisfeita e a um povo saudável e satisfeito é um objetivo da filosofia confucionista.
A lealdade (忠, zhong) é equivalente à piedade filial em um plano diferente. É particularmente relevante para a classe social a que a maioria dos alunos de Confúcio pertencia, porque a única maneira de um jovem estudioso e ambicioso fazer o seu caminho no mundo confuciano chinês era entrar em um serviço civil no governo. Como a piedade filial, no entanto, a lealdade era frequentemente subvertida pelos regimes autocráticos da China. Confúcio defendia uma sensibilidade à realpolitik das relações de classe na sua época. Ele não propôs que "o poder dá a razão", mas que um ser superior que recebeu o "mandato do céu" (天命) deveria ser obedecido devido a sua retidão moral.
Anos mais tarde, no entanto, a ênfase foi colocada mais sobre as obrigações dos governados para o governante, e menos nas obrigações do governante para os governados.
A lealdade era também uma extensão dos deveres do indivíduo com os amigos, cônjuge e familiares. A lealdade para com a família vinha primeiro, em seguida para o cônjuge, depois para o governante, e por último aos amigos. A lealdade era considerada uma das grandes virtudes humanas.
Confúcio também percebeu que a lealdade e a piedade filial podem entrar em conflito.
De acordo com He Guanghu, o confucionismo pode ser identificado como uma continuação da religião oficial Shang-Zhou (~1600–256 a.C.), , ou a religião aborígene chinesa que durou ininterruptamente por três mil anos.[30] Ambas as dinastias adoravam a divindade suprema, chamada Shangdi (上帝 "Divindade Mais Alta") ou simplesmente Dì (帝) pelos Shang e Tian (天 "Céu") pelos Zhou. Shangdi foi concebido como o primeiro ancestral da casa real Shang,[31] um nome alternativo para ele ser o "Progenitor Supremo" (上甲 Shàngjiǎ).[32] Na teologia Shang, a multiplicidade de deuses da natureza e ancestrais eram vistos como partes de Di, e os quatro 方 fāng ("direções" ou "lados") e seus 風 fēng ("ventos") como sua vontade cósmica.[33] Com a dinastia Zhou, que derrubou os Shang, o nome da divindade suprema tornou-se Tian (天 "Céu").[31] Enquanto os Shang identificavam Shangdi como seu deus ancestral para afirmar sua reivindicação de poder por direito divino, os Zhou transformaram essa reivindicação em uma legitimidade baseada no poder moral, o Mandato do Céu. Na teologia de Zhou, Tian não tinha descendência terrena singular, mas concedeu favor divino a governantes virtuosos. Os reis Zhou declararam que sua vitória sobre os Shang foi porque eles eram virtuosos e amavam seu povo, enquanto os Shang eram tiranos e, portanto, foram privados do poder por Tian.[3]
John C. Didier e David Pankenier relacionam as formas de ambos os antigos caracteres chineses para Di e Tian aos padrões de estrelas nos céus do norte, ambos desenhados, na teoria de Didier, conectando as constelações que delimitam o pólo celeste norte como um quadrado,[34] ou na teoria de Pankenier, conectando algumas das estrelas que formam as constelações da Ursa Maior e Ursa Menor.[35] Culturas em outras partes do mundo também conceberam essas estrelas ou constelações como símbolos da origem das coisas, a divindade suprema, divindade e poder real.[36] A divindade suprema também foi identificada com o dragão, símbolo do poder ilimitado (qi),[31] do poder primordial "protéico" que incorpora ambos yin-yang em unidade, associado à constelação de Draco que serpenteia ao redor do pólo eclíptico norte,[29] e desliza entre o Pequeno e Grande Mergulhador.
Por volta do século VI a.C., o poder de Tian e os símbolos que o representavam na terra (arquitetura de cidades, templos, altares e caldeirões rituais e o sistema ritual de Zhou) tornaram-se "difusos" e reivindicados por diferentes potentados nos estados de Zhou para legitimar ambições econômicas, políticas e militares. O direito divino não era mais um privilégio exclusivo da casa real de Zhou, mas poderia ser comprado por qualquer pessoa capaz de pagar as elaboradas cerimônias e os antigos e novos ritos necessários para acessar a autoridade de Tian.[37]
Além do declínio do sistema ritual Zhou, o que pode ser definido como tradições "selvagens" (野 yě), ou tradições "fora do sistema oficial", desenvolvido como tentativas de acessar a vontade de Tian. A população havia perdido a fé na tradição oficial, que não era mais percebida como uma forma eficaz de se comunicar com o Céu. As tradições do 九野 ("Nove Campos") e do Yijing surgiram.[38] Os pensadores chineses, diante desse desafio à legitimidade, divergiram em "Cem Escolas de Pensamento", cada uma propondo suas próprias teorias para a reconstrução da ordem moral Zhou.
Confúcio (551-479 a.C.) apareceu neste contexto de decadência política e questionamento espiritual. Ele foi educado na teologia Shang-Zhou, que ele contribuiu para transmitir e reformular dando centralidade ao autocultivo e agência dos humanos,[3] e o poder educacional do indivíduo auto-estabelecido em ajudar os outros a se estabelecerem (o princípio de 愛人 àirén, "amar os outros").[39] Com o colapso do reinado de Zhou, os valores tradicionais foram abandonados, resultando em um período de declínio moral. Confúcio viu uma oportunidade de reforçar valores de compaixão e tradição na sociedade. Desiludido com a vulgarização generalizada dos rituais para acessar Tian, ele começou a pregar uma interpretação ética da religião tradicional Zhou. Na sua opinião, o poder de Tian é imanente e responde positivamente ao coração sincero movido pela humanidade e retidão, decência e altruísmo. Confúcio concebeu essas qualidades como a base necessária para restaurar a harmonia sócio-política. Como muitos contemporâneos, Confúcio via as práticas rituais como formas eficazes de acessar Tian, ​​mas ele achava que o nó crucial era o estado de meditação em que os participantes entram antes de se envolver nos atos rituais.[40] Confúcio alterou e recodificou os livros clássicos herdados das dinastias Xia-Shang-Zhou e compôs Os Anais de Primavera e Outono.[22]
Filósofos no período dos Reinos Combatentes, tanto "dentro da praça" (com foco no ritual endossado pelo Estado) quanto "fora da praça" (não alinhados ao ritual estatal) construíram sobre o legado de Confúcio, compilado nos Analectos, e formularam a metafísica clássica que se tornou o chicote do confucionismo. De acordo com o Mestre, eles identificaram a tranquilidade mental como o estado de Tian, ou o Um (一 Yī), que em cada indivíduo é o poder divino concedido pelo Céu para governar a própria vida e o mundo. Indo além do Mestre, eles teorizaram a unidade de produção e reabsorção na fonte cósmica e a possibilidade de compreendê-la e, portanto, reconquistá-la através da meditação. Essa linha de pensamento teria influenciado todas as teorias e práticas místicas políticas individuais e coletivas chinesas a partir de então.[41]
Em sociologia, uma sociedade (do termo em latim societăs,[1] que significa "associação") é um grupo de indivíduos se relacionando, a fim de conseguir e preservar seus objetivos comuns.[2][3] Os objetivos comuns, compartilhados pelos membros da sociedade, são os próprios objetivos da sociedade, ou seja, o bem comum.[3]
Não é um grupo qualquer, mas é um grupo soberano de indivíduos, não dependendo de forças externas, onde existe uma rede (sistema) total e abrangente de relacionamentos, na qual todos os seus indivíduos e comunidades membros estão interligados.[2][3] Uma sociedade é composta por membros que compartilham um princípio fundamental, geral, vinculando todos, dentro do grupo, a uma mesma finalidade ( o bem comum).[3]
Há quem considere que não existam sociedades nem classes sociais, como Margaret Thatcher (a "Dama de Ferro"), política britânica que ascendeu ao lugar de Primeiro-Ministro, e chegou a afirmar que ela mesma (a sociedade) não existe. Conforme disse, só existem os indivíduos e suas comunidades familiares.[4]
Sociedade é objeto de estudo comum das ciências humanas, especialmente a sociologia, a história, a antropologia, a geografia e o direito.
A sociedade pode ser institucionalizada ou não.[3] O Estado é uma instituição (ou conjunto de instituições públicas), dedicada ao gerenciamento da sociedade (sociedade com estado).[3] Ao contrário, pode se falar das sociedades apátridas (sociedade sem estado), as quais existiram durante a maior parte da história.
É comum haver confusão entre os conceitos distintos (mas próximos) de sociedade e comunidade. Os grupos intermediários, ao individuo e à sociedade, são as comunidades. As comunidades possuem tanto mais características subjetivas e íntimas do que uma sociedade, quanto também possuem mais características objetivas e abrangentes do que as individualidades.
Comunidades possuem um lado impessoal, com normas gerais, mas também possuem características subjetivas e pessoais, comuns entre aqueles que as integram, os diferindo de demais grupos de uma sociedade. As comunidades se constituem em grupos intermediários entre os indivíduos e a sociedade, com normas objetivas e externas aos seus indivíduos, porém estas mesmas normas são encaradas como subjetivas, frente à sociedade a qual determinada(s) comunidade(s) pertença(m).
As comunidades são sociedades menores e "individualizadas", possuindo relações intermediárias, que sirvam de "ponte" entre os indivíduos e a sociedade. Exemplos de comunidades são as famílias, escolas, o local de trabalho, os amigos e a vizinhança.[3] Existem também as comunidades religiosas (institucionalizadas ou não) militares, econômicas, empresariais, dentre outras.
A sociedade, em geral, considera o fato de que um indivíduo tem meios bastante limitados como uma "unidade soberana". Os grandes macacos sempre foram mais (Bonobo, Homo, chimpanzé) ou menos (Gorila, orangotango) sociais, portanto situações parecidas com as vividas por Robinson Crusoe são ficções ou casos um tanto incomuns para a ubiquidade do contexto social dos seres humanos.
As sociedades humanas são, na maioria das vezes, organizadas de acordo com seu principal meio de subsistência. Cientistas sociais identificaram sociedades caçadoras-coletoras, sociedades pastorais nômades, sociedades horticultoras ou simples sociedades agrícolas e sociedades intensivas em agricultura, também chamadas de civilizações. Alguns consideram que as sociedades industrial e pós-industrial são qualitativamente diferentes das tradicionais sociedades agrícolas.
Atualmente, os antropólogos e muitos cientistas sociais se opõem vigorosamente contra a noção de evolução cultural e "etapas" rígidas como essas. Na verdade, muitos dados antropológicos têm sugerido que a complexidade (civilização, crescimento e densidade populacional, especialização etc.) nem sempre toma a forma de organização ou estratificação social hierárquica.
Além disso, o relativismo cultural, como uma abordagem generalizada ou ética, tem substituído as noções de "primitivo", melhor/pior ou "progresso" em relação às culturas (incluindo a sua cultura material/tecnologia e organização social).
Segundo o antropólogo Maurice Godelier, uma novidade importante na sociedade humana, em contraste com os parentes biológicos mais próximos da humanidade (os chimpanzés e os bonobos), é o papel de pai assumido pelos homens, que supostamente está ausente em nossos parentes mais próximos, para os quais a paternidade geralmente não é determinável.[5][6]
As sociedades também podem ser organizadas de acordo com a sua estrutura política. A fim de crescer em tamanho e complexidade, existem sociedades de bandos, tribos, chefias, e sociedades estatais. Estas estruturas podem ter diferentes graus de poder político, dependendo dos ambientes cultural, geográfico e histórico nos quais essas sociedades estão inseridas. Assim, uma sociedade mais isolada com o mesmo nível de tecnologia e cultura que as outras sociedades tem mais probabilidade de sobreviver do que uma em estreita proximidade com outras sociedades que possam interferir em seus recursos. Uma sociedade que é incapaz de oferecer uma resposta eficaz para outras sociedades que concorram com ela normalmente é subsumida pela cultura da sociedade concorrente.
O sociólogo Gerhard Lenski difere as sociedades com base em seu nível de tecnologia, economia e comunicação: (1) caçadores e coletores, (2) agrícolas simples, (3) agrícolas avançadas (4), industrial, e (5) especial (sociedades, por exemplo, de pesca ou marítima).[7] Esta classificação é semelhante ao sistema anterior desenvolvido pelos antropólogos Morton H. Fried, um teórico do conflito, e Elman Service, uma teórica da integração, que produziram um sistema de classificação para as sociedades para todas as culturas humanas com base na evolução da desigualdade econômica, da desigualdade social e do papel do Estado. Este sistema de classificação contém quatro categorias:
Além delas, também existem:
Ao longo do tempo, algumas culturas evoluíram para formas mais complexas de organização e controle. Esta evolução sociocultural tem um efeito profundo sobre os padrões da comunidade. Tribos de caçadores-coletores se estabeleceram em torno de fontes de alimentos sazonais para tornarem-se aldeias agrárias. As aldeias cresceram para se tornarem vilas e cidades. As cidades se transformaram em cidades-estados e estados-nação.[8]
Muitas sociedades distribuem generosidade a mando de algum indivíduo ou algum grupo maior de pessoas. Este tipo de generosidade pode ser vista em todas as culturas conhecidas. Normalmente, o indivíduo ou grupo generoso ganha prestígio ao realizar esses atos. Por outro lado, membros de uma sociedade também podem evitar ou excluir os membros que violem as suas normas (ver: Preconceito social). Mecanismos, tais como o ato de dar presentes, relações jocosas e bode expiatório, que podem ser vistos em vários tipos de agrupamentos humanos, tendem a ser institucionalizados em uma sociedade. A evolução social, como um fenômeno, traz, consigo, alguns elementos que poderiam ser prejudiciais para a respectiva população.
Algumas sociedades concedem status a um indivíduo ou um grupo de pessoas, quando esse indivíduo ou grupo executa uma ação admirada ou desejada. Este tipo de reconhecimento é concedido sob a forma de um título, nome, forma de se vestir, ou recompensa monetária. Em muitas sociedades, o status de adultos do sexo masculino ou feminino está sujeito a um ritual ou processo deste tipo. Ações altruístas no interesse da comunidade são vistas em praticamente todas as sociedades. Os fenômenos de ação comunitária, bode expiatório, generosidade, de risco compartilhado e recompensa são comuns a muitas formas de sociedade.
As sociedades são associações entre indivíduos da mesma espécie, organizados de um modo cooperativo e não ligados anatomicamente. Os indivíduos, denominados sociais, colaboram com a sociedade em que estão integrados graças aos estímulos recíprocos. Sempre é observada a existência de hierarquia, uma divisão de funções para cada membro participante, o que gera indivíduos especialistas em determinadas funções aumentando a eficiência do conjunto e sobrevivência da espécie, a ponto de ocorrerem seleções na escolha da função de acordo com a estrutura do corpo de cada animal. Por exemplo, formigas-soldados são maiores e possuem mais veneno (mais ácido fórmico) que as formigas operárias; a abelha rainha é grande e põe ovos, enquanto que as abelhas operárias são menores e não põem ovos.
É notório e factual que a educação se manifesta enquanto um fenômeno de consciência humana muito mais antigo que a organização social, podendo ser até considerada a ancestral do que conhecemos enquanto sociedade, além de responsável pela produção e manutenção da cultura, como aponta Brandão em sua obra: o que é educação[9]. Porém é possível destacar também, que a partir das organizações sociais surgem as necessidades de se organizar e formalizar o ensino, dessa forma as sociedades, principalmente as modernas, e seus projetos sociais tem interferência direta na função, na atuação e no sentido da escola e do trabalho do docente, como apontam Hypólito e Da Rocha no artigo: Disputas pela escola pública, contribuições históricas para pensar o trabalho docente.[10] 
Dessa forma, a sociedade atua na educação de maneira decisiva: legislando, regulamentando e adaptando suas concepções e formas de ensino de acordo com os interesses desta sociedade, mas também de maneira dialética de modo que ambas, educação e sociedade, se influenciam e se alteram em sua existência, trajetória e desenvolvimento a partir de uma relação semelhante a de retroalimentação.
Portanto, a sociedade e a educação são inseparáveis em concepção e prática, ainda mais, devem ser relacionadas diretamente com a formulação política que conhecemos como democracia, e cumpre papel crucial para qualquer projeto social ou de sociedade na modernidade, como defende Galter e Favoreto na publicação John Dewey: um clássico da educação para a democracia. [11]
Democracia é um regime político em que os cidadãos no aspecto dos direitos políticos participam igualmente — diretamente ou através de representantes eleitos — na proposta, no desenvolvimento e na criação de leis, exercendo o poder da governação através do sufrágio universal. Ela abrange as condições sociais, econômicas e culturais que permitem o exercício livre e igual da autodeterminação política.
O termo é do grego antigo δημοκρατία (dēmokratía ou "governo do povo"),[1] que foi criado a partir de δῆμος (demos ou "povo") e κράτος (kratos ou "poder") no século V a.C. para denotar os sistemas políticos então existentes em cidades-Estados gregas, principalmente Atenas; o termo é um antônimo para ἀριστοκρατία (aristokratia ou "regime de uma aristocracia" como seu nome indica). Embora, teoricamente, estas definições sejam opostas, na prática, a distinção entre elas foi obscurecida historicamente.[2] No sistema político da Atenas Clássica, por exemplo, a cidadania democrática abrangia apenas homens, filhos de pai e mãe atenienses, livres e maiores de 21 anos, enquanto estrangeiros, escravos e mulheres eram grupos excluídos da participação política. Em praticamente todos os governos democráticos em toda a história antiga e moderna, a cidadania democrática valia apenas para uma elite de pessoas, até que a emancipação completa foi conquistada para todos os cidadãos adultos na maioria das democracias modernas através de movimentos por sufrágio universal durante os séculos XIX e XX.
O sistema democrático contrasta com outras formas de governo em que o poder é detido por uma pessoa — como em uma monarquia absoluta — ou em que o poder é mantido por um pequeno número de indivíduos — como em uma oligarquia. No entanto, essas oposições, herdadas da filosofia grega,[3] são agora ambíguas porque os governos contemporâneos têm misturado elementos democráticos, oligárquicos e monárquicos em seus sistemas políticos. Karl Popper definiu a democracia em contraste com ditadura ou tirania, privilegiando, assim, oportunidades para as pessoas de controlar seus líderes e de tirá-los do cargo sem a necessidade de uma revolução.[4]
Diversas variantes de democracias existem no mundo, mas há duas formas básicas, sendo que ambas dizem respeito a como o corpo inteiro de todos os cidadãos no exercício dos direitos políticos executam a sua vontade. Uma das formas de democracia é a democracia direta, em que todos os cidadãos no exercício dos direitos políticos têm participação direta e ativa na tomada de decisões do governo. Na maioria das democracias modernas, todo o corpo de cidadãos no exercício dos direitos políticos permanece com o poder soberano, mas o poder político é exercido indiretamente por meio de representantes eleitos, o que é chamado de democracia representativa. No Brasil a soberania popular é exercida de forma indireta, por meio de representantes eleitos (CF/88, art. 1º, parágrafo único), e de forma direta, na forma da lei, mediante plebiscito, referendo e iniciativa popular (CF/88, art. 1º, parágrafo único, e  art. 14, caput, I a III), O conceito de democracia representativa surgiu em grande parte a partir de ideias e instituições que se desenvolveram durante períodos históricos como a Idade Média europeia, a Reforma Protestante, o Iluminismo e as revoluções Americana e Francesa.[5]
Não existe consenso sobre a forma correta de definir a democracia, mas a igualdade, a liberdade e o Estado de direito foram identificadas como características importantes desde os tempos antigos.[6][7] Estes princípios são refletidos quando todos os cidadãos elegíveis são iguais perante a lei e têm igual acesso aos processos legislativos. Por exemplo, em uma democracia representativa, cada voto tem o mesmo peso, não existem restrições excessivas sobre quem quer se tornar um representante, além da liberdade de seus cidadãos elegíveis ser protegida por direitos legitimados e que são tipicamente protegidos por uma constituição.[8][9]
Uma teoria sustenta que a democracia exige três princípios fundamentais: 1) a soberania reside nos níveis mais baixos de autoridade; 2) igualdade política e 3) normas sociais pelas quais os indivíduos e as instituições só consideram aceitáveis atos que refletem os dois primeiros princípios citados.[10]
O termo democracia às vezes é usado como uma abreviação para a democracia liberal, que é uma variante da democracia representativa e que pode incluir elementos como o pluralismo político, a igualdade perante a lei, o direito de petição para reparação de injustiças sociais; devido processo legal; liberdades civis; direitos humanos; e elementos da sociedade civil fora do governo. Roger Scruton afirma que a democracia por si só não pode proporcionar liberdade pessoal e política, a menos que as instituições da sociedade civil também estejam presentes.[11]
Em muitos países, como no Reino Unido onde se originou o Sistema Westminster, o princípio dominante é o da soberania parlamentar, mantendo a independência judicial.[13] Nos Estados Unidos, a separação de poderes é frequentemente citada como um atributo central de um regime democrático. Na Índia, a maior democracia do mundo, a soberania parlamentar está sujeita a uma constituição que inclui o controle judicial.[14] Outros usos do termo "democracia" incluem o da democracia direta. Embora o termo "democracia" seja normalmente usado no contexto de um Estado político, os princípios também são aplicáveis a organizações privadas.
O regime da maioria absoluta é frequentemente considerado como uma característica da democracia. Assim, o sistema democrático permite que minorias políticas sejam oprimidas pela chamada "tirania da maioria" quando não há proteções legais dos direitos individuais ou de grupos. Uma parte essencial de uma democracia representativa "ideal" são eleições competitivas que sejam justas tanto no plano material, quanto processualmente. Além disso, liberdades como a política, de expressão e de imprensa são consideradas direitos essenciais que permitem aos cidadãos elegíveis serem adequadamente informados e aptos a votar de acordo com seus próprios interesses.[15][16]
Também tem sido sugerido que uma característica básica da democracia é a capacidade de todos os eleitores de participar livre e plenamente na vida de sua sociedade.[17] Com sua ênfase na noção de contrato social e da vontade coletiva de todos os eleitores, a democracia também pode ser caracterizada como uma forma de coletivismo político, porque ela é definida como uma forma de governo em que todos os cidadãos elegíveis têm uma palavra a dizer de peso igual nas decisões que afetam as suas vidas.[18]
Enquanto a democracia é muitas vezes equiparada à forma republicana de governo, o termo república classicamente abrangeu democracias e aristocracias.[19][20] Algumas democracias são monarquias constitucionais muito antigas, como é o caso de países como o Reino Unido e o Japão.
O termo "democracia" apareceu pela primeira vez no antigo pensamento político e filosófico grego na cidade-Estado de Atenas durante a antiguidade clássica.[21][22] Liderados por Clístenes, os atenienses estabeleceram o que é geralmente tido como a primeira experiência democrática em 508-507 a.C. Clístenes é referido como "o pai da democracia ateniense".[23]
A democracia ateniense tomou a forma de uma democracia direta e tinha duas características distintivas: a seleção aleatória de cidadãos comuns para preencher os poucos cargos administrativos e judiciais existentes no governo[24] e uma assembleia legislativa composta por todos os cidadãos atenienses.[25] Todos os cidadãos elegíveis eram autorizados a falar e votar na assembleia, que estabelecia as leis da cidade-Estado. No entanto, a cidadania ateniense excluía mulheres, escravos, estrangeiros (μέτοικοι, metoikoi), os que não eram proprietários de terras e os homens com menos de 20 anos de idade. Dos cerca de 200 a 400 mil habitantes de Atenas na época, havia entre 30 mil e 60 mil cidadãos. A exclusão de grande parte da população a partir do que era considerada cidadania está intimamente relacionada com a antiga compreensão do termo. Durante a maior parte da antiguidade, o benefício da cidadania era associado à obrigação de lutar em guerras.[26]
O sistema democrático ateniense não era apenas dirigido no sentido de que as decisões eram tomadas pelas pessoas reunidas na assembleia, mas também era mais direto no sentido de que as pessoas, através de assembleias e tribunais de justiça, controlavam todo o processo político e uma grande proporção dos cidadãos estavam envolvidos constantemente nos assuntos públicos.[27] Mesmo com os direitos do indivíduo não sendo garantidos pela constituição ateniense no sentido moderno (os antigos gregos não tinham uma palavra para "direitos"[28]), os atenienses gozavam de liberdades não por conta do governo, mas por viverem em uma cidade que não estava sujeita a outro poder e por não serem eles próprios sujeitos às regras de outra pessoa.[29]
A votação por pontos apareceu em Esparta já em 700 a.C. A Apela era uma assembleia do povo, realizada uma vez por mês. Nessa assembleia, os líderes espartanos eram eleitos e davam seu voto gritando. Todos os cidadãos do sexo masculino com mais 30 anos de idade podiam participar. Aristóteles chamava esse sistema de "infantil", em oposição a algo mais sofisticado, como a utilização de registros de voto em pedra, como os usados pelos atenienses. No entanto, em termos, Esparta adotou esse sistema de votação por causa da sua simplicidade e para evitar qualquer tipo de viés de votação.[30][31]
Mesmo que a República Romana tenha contribuído significativamente com muitos dos aspectos da democracia, apenas uma minoria dos romanos eram considerados cidadãos aptos a votar nas eleições para os representantes. Os votos dos poderosos tinham mais peso através de um sistema de gerrymandering, enquanto políticos de alto gabarito, incluindo membros do senado, vinham de algumas famílias ricas e nobres.[32] No entanto, muitas exceções notáveis ocorreram. Além disso, a República Romana foi o primeiro governo no mundo ocidental a ter uma república como um Estado-nação, apesar de não ter muitas características de uma democracia. Os romanos inventaram o conceito de "clássicos" e muitas obras da Grécia antiga foram preservadas.[33] Além disso, o modelo romano de governo inspirou muitos pensadores políticos ao longo dos séculos[34] e democracias representativas modernas imitam mais o modelo romano do que os gregos porque era um Estado em que o poder supremo era realizado pelo povo e por seus representantes eleitos, e que tinha um líder eleito ou nomeado.[35] A democracia representativa é uma forma de democracia em que as pessoas votam em representantes que, em seguida, votam em iniciativas políticas; enquanto uma democracia direta é uma forma de democracia em que as pessoas votam em iniciativas políticas diretamente.[36]
As transições do século XX para a democracia liberal vieram em sucessivas "ondas" de democracia, diversas vezes resultantes de guerras, revoluções, descolonização e por circunstâncias religiosas e econômicas. A Primeira Guerra Mundial e a subsequente dissolução dos impérios Otomano e Austro-Húngaro resultou na criação de novos Estados-nação da Europa, a maior parte deles, pelo menos nominalmente, democráticos.
Na década de 1920 a democracia floresceu, mas a Grande Depressão trouxe desencanto e a maioria dos países da Europa, América Latina e Ásia viraram-se para regimes autoritários. O fascismo e outros tipos de ditaduras floresceram na Alemanha nazista, na Itália, na Espanha e em Portugal, além de regimes não-democráticos terem surgidos nos países bálticos, nos Balcãs, no Brasil, em Cuba, na China e no Japão, entre outros.[37]
A Segunda Guerra Mundial trouxe uma reversão definitiva desta tendência na Europa Ocidental. A democratização dos setores estadunidense, britânico e francês da Alemanha ocupada (disputado[38]), da Áustria, da Itália e do Japão ocupado pelos Aliados serviu de modelo para a teoria posterior de "mudança de regime". No entanto, a maior parte da Europa Oriental, incluindo o setor soviético da Alemanha, caiu sob a influência do bloco soviético não-democrático. A guerra foi seguida pela descolonização e, novamente, a maioria dos novos estados independentes tiveram constituições nominalmente democráticas. A Índia emergiu como a maior democracia do mundo e continua a sê-lo.[39]
Em 1960, a grande maioria dos Estados-nação tinham, nominalmente, regimes democráticos, embora a maioria das populações do mundo ainda vivesse em países que passaram por eleições fraudulentas e outras formas de subterfúgios (particularmente em nações comunistas e em ex-colônias). Uma onda posterior de democratização trouxe ganhos substanciais para a verdadeira democracia liberal para muitas nações. Espanha, Portugal (1974) e várias das ditaduras militares na América do Sul voltaram a ser um governo civil no final dos anos 1970 e início dos anos 1980 (Argentina em 1983, Bolívia e Uruguai em 1984, o Brasil em 1985 e o Chile no início de 1990). Isto foi seguido por nações do Extremo Oriente e do Sul da Ásia no final da década de 1980.
O mal-estar econômico na década de 1980, juntamente com o ressentimento da opressão soviética, contribuiu para o colapso da União Soviética, o consequente fim da Guerra Fria e a democratização e liberalização dos antigos países do chamado bloco oriental. A mais bem-sucedida das novas democracias eram aquelas geográfica e culturalmente mais próximas da Europa Ocidental e elas são agora, em sua maioria, membros ou membros associados da União Europeia. Alguns pesquisadores consideram que a Rússia contemporânea não é uma verdadeira democracia e, em vez disso, se assemelha a uma forma de ditadura.[40]
A tendência liberal se espalhou para alguns países da África na década de 1990, sendo o exemplo mais proeminente a África do Sul. Alguns exemplos recentes de tentativas de liberalização incluem a Revolução Indonésia de 1998, a Revolução Bulldozer na antiga Iugoslávia, a Revolução Rosa na Geórgia, a Revolução Laranja na Ucrânia, a Revolução dos Cedros no Líbano, a Revolução das Tulipas no Quirguistão e da Revolução de Jasmim na Tunísia (parte da chamada "Primavera Árabe").
De acordo com a organização Freedom House, em 2007, havia 123 democracias eleitorais (acima das 40 registradas em 1972).[41] De acordo com o Fórum Mundial sobre a Democracia, as democracias eleitorais agora representam 120 dos 192 países existentes e constituem 58,2 por cento da população mundial. Ao mesmo tempo, as democracias liberais, ou seja, os países que a Freedom House considera livre e que respeitam os direitos humanos fundamentais e o Estado de direito são 85 e representam 38 por cento da população global.[42]
Em 2010, as Nações Unidas declararam 15 de setembro o Dia Internacional da Democracia.[43]
É possível destacar ainda, que durante toda sua trajetória, mas principalmente na era contemporânea, a democracia tem uma relação direta com a educação, com a escola e com os modelos de ensino que determinada sociedade estabelece. Dessa forma a democracia moderna como conhecemos hoje, está intimamente vinculada com a educação dos territórios que a elegem enquanto forma de governo.[44]
A democracia tem tomado diferentes formas de governo, tanto na teoria quanto na prática. Algumas variedades de democracia proporcionam uma melhor representação e maior liberdade para seus cidadãos do que outras.[45][46] No entanto, se qualquer democracia não está estruturada de forma a proibir o governo de excluir as pessoas do processo legislativo, ou qualquer agência do governo de alterar a separação de poderes em seu próprio favor, em seguida, um ramo do sistema político pode acumular muito poder e destruir o ambiente democrático.[47][48][49]
Democracia direta refere-se ao sistema onde os cidadãos decidem diretamente cada assunto por votação.
A democracia direta tornou-se cada vez mais difícil, e necessariamente se aproxima mais da democracia representativa, quando o número de cidadãos cresce. Historicamente, as democracias mais diretas incluem o encontro municipal de Nova Inglaterra (dentro dos Estados Unidos), e o antigo sistema político de Atenas. Nenhum destes se enquadraria bem para uma grande população (embora a população de Atenas fosse grande, a maioria da população não era composta de pessoas consideradas como cidadãs, que, portanto, não tinha direitos políticos; não os tinham mulheres, escravos e crianças).
É questionável se já houve algum dia uma democracia puramente direta de qualquer tamanho considerável. Na prática, sociedades de qualquer complexidade sempre precisam de uma especialização de tarefas, inclusive das administrativas; e portanto uma democracia direta precisa de oficiais eleitos. (Embora alguém possa tentar manter todas as decisões importantes feitas por voto direto, com os oficiais meramente implementando essas decisões). Exemplos de democracia direta que costumavam eleger Delegados com mandato imperativo, revogável e temporário podem ser encontrados em sedições e revoluções de cunho anarquista como a Revolução Espanhola, a Revolução Ucraniana e no levante armado da EZLN, no estado de Chiapas.
Contemporaneamente o regime que mais se aproxima dos ideais de uma democracia direta é a democracia semidireta da Suíça. Uma democracia semidireta é um regime de democracia em que existe a combinação de representação política com formas de Democracia direta[50] (Benevides, 1991, p. 129).[51]
A Democracia semidireta, conforme Bobbio[52] (1987, p. 459), é uma forma de democracia que possibilita um sistema mais bem-sucedido de democracia frente as democracias Representativa e Direta, ao permitir um equilíbrio operacional entre a representação política e a soberania popular direta. A prática desta ação equilibrante da democracia semidireta, segundo Bonavides[53] (2003, p. 275), limita a “alienação política da vontade popular”, onde “a soberania está com o povo, e o governo, mediante o qual essa soberania se comunica ou exerce, pertence ao elemento popular nas matérias mais importantes da vida pública”.
Em democracias representativas, em contraste, os cidadãos elegem representantes em intervalos regulares, que então votam os assuntos em seu favor. Do mesmo modo, muitas democracias representativas modernas incorporam alguns elementos da democracia direta, normalmente referendo.
Nós podemos ver democracias diretas e indiretas como os tipos ideais, com as democracias reais se aproximando umas das outras. Algumas entidades políticas modernas, como a Suíça ou alguns estados norte-americanos, onde é frequente o uso de referendo iniciada por petição (chamada referendo por demanda popular) ao invés de membros da legislatura ou do governo. A última forma, que é frequentemente conhecida por plebiscito, permite ao governo escolher se e quando manter um referendo, e também como a questão deve ser abordada. Em contraste, a Alemanha está muito próxima de uma democracia representativa ideal: na Alemanha os referendos são proibidos — em parte devido à memória de como Adolf Hitler usou isso para manipular plebiscitos em favor do seu governo.[54][55]
O sistema de eleições que foi usado em alguns países capitalistas de Estado, chamado centralismo democrático, pode ser considerado como uma forma extrema de democracia representativa, onde o povo elegia representantes locais, que por sua vez elegeram representantes regionais, que por sua vez elegiam a assembleia nacional, que finalmente elegia os que iam governar o país. No entanto, alguns consideram que esses sistemas não são democráticos na verdade, mesmo que as pessoas possam votar, já que a grande distância entre o indivíduo eleitor e o governo permite que se tornasse fácil manipular o processo. Outros contrapõem, dizendo que a grande distância entre eleitor e governo é uma característica comum em sistemas eleitorais desenhados para nações gigantescas (os Estados Unidos e algumas potências europeias, só para dar alguns exemplos considerados inequivocamente democráticos, têm problemas sérios na democraticidade das suas instituições de topo), e que o grande problema do sistema soviético e de outros países comunistas, aquilo que o tornava verdadeiramente não-democrático, era que, em vez de serem escolhidos pelo povo, os candidatos eram impostos pelo partido dirigente.
O voto, também chamado de sufrágio censitário, é típico do Estado liberal (século XIX) e exigia que os seus titulares atendessem certas exigências tais como pagamento de imposto direto; proprietário de propriedade fundiária e usufruir de certa renda.
No passado muitos grupos foram excluídos do direito de voto, em vários níveis. Algumas vezes essa exclusão é uma política bastante aberta, claramente descrita nas leis eleitorais; outras vezes não é claramente descrita, mas é implementada na prática por meios que parecem ter pouco a ver com a exclusão que está sendo realmente feita (p.ex., impostos de voto e requerimentos de alfabetização que mantinham afro-americanos longe das urnas antes da era dos direitos civis). E algumas vezes a um grupo era permitido o voto, mas o sistema eleitoral ou instituições do governo eram propositadamente planejadas para lhes dar menos influência que outros grupos favorecidos.
Hoje, em muitas democracias, o direito de voto é garantido sem discriminação de raça, grupo étnico, classe ou sexo. No entanto, o direito de voto ainda não é universal. É restrito a pessoas que atingem uma certa idade, normalmente 18 (embora em alguns lugares possa ser 16— como no Brasil — ou 21). Somente cidadãos de um país normalmente podem votar em suas eleições, embora alguns países façam excepções a cidadãos de outros países com que tenham laços próximos (p.ex., alguns membros da Comunidade Britânica e membros da União Europeia).
A prática do voto obrigatório remonta à Grécia Antiga, quando o legislador ateniense Sólon fez aprovar uma lei específica obrigando os cidadãos a escolher um dos partidos, caso não quisessem perder seus direitos de cidadãos. A medida foi parte de uma reforma política que visava conter a radicalização das disputas entre facções que dividiam a pólis. Além de abolir a escravidão por dívidas e redistribuir a população de acordo com a renda, criou também uma lei que impedia os cidadãos de se absterem nas votações da assembleia, sob risco de perderem seus direitos.
Muitas sociedades no passado negaram a pessoas o direito de votar baseadas no grupo étnico. Exemplo disso é a exclusão de pessoas com ascendência africana das urnas, na era anterior à dos direitos civis, e na época do apartheid na África do Sul. A maioria das sociedades hoje não mantêm essa exclusão, mas algumas ainda o fazem. Por exemplo, Fiji reserva um certo número de cadeiras no Parlamento para cada um dos principais grupos étnicos; essas exclusões foram adotadas para barrar a maioria dos indianos em favor dos grupos étnicos fijianos. Até o século XIX, muitas democracias ocidentais tinham propriedades de qualificação nas suas leis eleitorais, o que significava que apenas pessoas com um certo grau de riqueza podiam votar. Hoje essas leis foram amplamente abolidas.
Outra exclusão que durou muito tempo foi a baseada no sexo. Todas as democracias proibiam as mulheres de votar até 1893, quando a Nova Zelândia se tornou o primeiro país do mundo a dar às mulheres o direito de voto nos mesmos termos dos homens. No Brasil, pela constituição de 1822 e suas emendas antes dessa data, permitiu-se o direito de voto feminino, desde que pertencesse à classe determinada dos fazendeiros e fosse alfabetizada.[50] Isso aconteceu devido ao sucesso do movimento feminino pelo direito de voto, tanto na Nova Zelândia como no Brasil, sendo que houve participações parlamentares já no Brasil depois dessa época.[50] Hoje praticamente todos os Estados permitem que mulheres votem; as únicas exceções são sete países muçulmanos do Oriente Médio: Arábia Saudita, Barém, Brunei, Catar, Emirados Árabes Unidos, Kuwait e Omã.
O direito de voto normalmente é negado a prisioneiros. Alguns países também negam o direito a voto para aqueles condenados por crimes graves, mesmo depois de libertados. Em alguns casos (p.ex. em muitos estados dos Estados Unidos) a negação do direito de voto é automático na condenação de qualquer crime sério; em outros casos (p.ex. em países da Europa) a negação do direito de voto é uma penalidade adicional que a corte pode escolher por impor, além da pena do aprisionamento. Existem países em que os prisioneiros mantêm o direito de voto (por exemplo Brasil e Portugal).
Os pensadores italianos do século XX Vilfredo Pareto e Gaetano Mosca (independentemente) argumentaram que a democracia era ilusória, e servia apenas para mascarar a realidade da regra de elite. Na verdade, eles argumentaram que a oligarquia da elite é a lei inflexível da natureza humana, em grande parte devido à apatia e divisão das massas (em oposição à unidade, a iniciativa e a unidade das elites), e que as instituições democráticas não fariam mais do que mudar o exercício do poder de opressão à manipulação.[56] Como Louis Brandeis uma vez postulou, "Podemos ter democracia ou podemos ter riqueza concentrada nas mãos de uns poucos, mas não podemos ter as duas coisas".[57]
Hoje todos os partidos políticos no Canadá são cautelosos sobre as críticas de alto nível de imigração, porque, como observou The Globe and Mail, "no início de 1990, o antigo Partido da Reforma foi marcado como 'racista' por sugerir que os níveis de imigração deveriam ser reduzidos de 250 mil a 150 mil.".[58] Como o professor de Economia Don J. DeVoretz destacou: "Em uma democracia liberal como o Canadá, o seguinte paradoxo persiste. Mesmo que a maioria dos entrevistados respondendo sim à pergunta: 'Há muitas imigrantes chegando a cada ano?' números de imigrantes continuam a subir até que um conjunto crítico de custos econômicos apareçam'".[59][60]
A ideia de “crise da democracia” vem ganhando repercussão na Teoria Política Contemporânea. Desde a década de 1970, autores da vertente partipacionista associam a legitimidade dos regimes democráticos a fatores que vão além da mera possibilidade de exercício livre do voto. A demanda, nesse sentido, é por efetiva atuação na concepção das políticas públicas, o que causa resistência em agentes representativos receosos de compartilhar o poder que o design institucional moderno lhes conferiu.[61]
Socialismo é uma filosofia política, social e econômica que abrange uma gama de sistemas econômicos e sociais caracterizados pela propriedade social dos meios de produção.[1][2][3][4][5][6][7][8] Inclui as teorias políticas e movimentos associados a tais sistemas.[9] A propriedade social pode ser pública, coletiva, cooperativa ou patrimonial.[10] Embora nenhuma definição única englobe os muitos tipos de socialismo,[11] a propriedade social é o elemento comum.[12][13] Os tipos de socialismo variam com base no papel dos mercados e do planejamento na alocação de recursos e na estrutura de gestão das organizações. Os socialistas discordam sobre se o governo, particularmente o governo existente, é o veículo correto para a mudança.[14]
Os sistemas socialistas são divididos em formas não mercantis e de mercado.[15] O socialismo não mercantil substitui os mercados de fatores e o dinheiro por planejamento econômico integrado e critérios técnicos ou de engenharia baseados em cálculos realizados em espécie, produzindo assim um mecanismo econômico diferente que funciona de acordo com leis e dinâmicas econômicas diferentes daquelas do capitalismo.[16][17][18][19] Um sistema socialista não mercantil elimina as ineficiências e crises tradicionalmente associadas à acumulação de capital e ao sistema de lucro no capitalismo.[20][21][22][23] O debate do cálculo socialista, originado pelo problema do cálculo econômico,[24] diz respeito à viabilidade e aos métodos de alocação de recursos para um sistema socialista planejado.[25][26][27] Em contraste, o socialismo de mercado retém o uso de preços monetários, mercados de fatores e, em alguns casos, a motivação do lucro, no que diz respeito à operação de empresas de propriedade social e à alocação de bens de capital entre elas. Os lucros gerados por essas empresas seriam controlados diretamente pela força de trabalho de cada empresa ou reverteriam para a sociedade em geral na forma de um dividendo social.[28][29][30] O anarquismo e o socialismo libertário se opõem ao uso do Estado como meio para estabelecer o socialismo, favorecendo a descentralização acima de tudo, seja para estabelecer o socialismo de não mercado ou o socialismo de mercado.[31][32]
A política socialista tem uma orientação tanto internacionalista quanto nacionalista; organizada por meio de partidos políticos e contra a política partidária; às vezes coincidindo com os sindicatos e outras vezes independentes e críticos deles; e presente em países industrializados e em desenvolvimento.[33] A social-democracia originou-se no movimento socialista,[34] apoiando intervenções econômicas e sociais para promover a justiça social.[35][36] Embora retendo nominalmente o socialismo como uma meta de longo prazo,[37][38][39][40] desde o período pós-guerra, passou a abraçar uma economia mista keynesiana dentro de uma economia de mercado capitalista predominantemente desenvolvida e liberal-democrática que expande a intervenção do Estado para incluir redistribuição de renda, regulamentação e um Estado de bem-estar.[41] A democracia econômica propõe uma espécie de socialismo de mercado, com controle mais democrático de empresas, moedas, investimentos e recursos naturais.[42]
O movimento político socialista inclui um conjunto de filosofias políticas que se originaram nos movimentos revolucionários de meados ao final do século XVIII e por preocupação com os problemas sociais que estavam associados ao capitalismo.[11] No final do século XIX, após o trabalho de Karl Marx e seu colaborador Friedrich Engels, o socialismo passou a significar oposição ao capitalismo e defesa de um sistema pós-capitalista baseado em alguma forma de propriedade social dos meios de produção.[43][44] Na década de 1920, o comunismo e a social-democracia haviam se tornado as duas tendências políticas dominantes dentro do movimento socialista internacional,[45] com o próprio socialismo se tornando o movimento secular mais influente do século XX.[46] Os partidos e ideias socialistas continuam sendo uma força política com vários graus de poder e influência em todos os continentes, liderando governos nacionais em muitos países ao redor do mundo. Hoje, muitos socialistas também adotaram as causas de outros movimentos sociais, como ambientalismo, feminismo e progressismo.[47] Apesar do surgimento da União Soviética como o primeiro Estado nominalmente socialista do mundo levou à ampla associação do socialismo com o modelo econômico soviético, alguns economistas e intelectuais argumentaram que, na prática, o modelo funcionava como uma forma de capitalismo[48][49][50] ou uma economia administrativa ou de comando não planejada.[51][52] Acadêmicos, comentaristas políticos e outros estudiosos às vezes se referem aos países do Bloco Ocidental que foram democraticamente governados por partidos socialistas e social-democratas, como Reino Unido, França e Suécia, como socialistas democráticos.[53][54][55][56]
Para Andrew Vincent, “a palavra 'socialismo' encontra sua raiz no latim sociare, que significa combinar ou compartilhar. O termo relacionado, mais técnico na lei romana e depois na medieval, era societas. Esta última palavra pode significar companheirismo, bem como a ideia mais legalista de um contrato consensual entre homens livres".[57]
O uso inicial do termo socialismo foi reivindicado por Pierre Leroux, que alegou ter usado o termo pela primeira vez no jornal parisiense Le Globe em 1832.[58][59] Com o sentido oposto à 'individualismo', interessado em reformas sociais, mas sem significado mais preciso.[60] Leroux foi um seguidor de Henri de Saint-Simon, um dos fundadores do que mais tarde seria rotulado de socialismo utópico. O socialismo contrastava com a doutrina liberal do individualismo que enfatizava o valor moral do indivíduo enquanto enfatizava que as pessoas agem ou deveriam agir como se estivessem isoladas umas das outras. Os socialistas utópicos originais condenaram esta doutrina do individualismo por não abordar as preocupações sociais durante a Revolução Industrial, como pobreza, opressão e vasta desigualdade de riqueza. Eles viam sua sociedade como algo que prejudica a vida da comunidade ao basear a existência humana na competição. Eles apresentaram o socialismo como uma alternativa ao individualismo liberal baseado na propriedade compartilhada de recursos.[61] Saint-Simon propôs o planejamento econômico, a administração científica e a aplicação da compreensão científica à organização da sociedade. Em contraste, Robert Owen propôs organizar a produção e a propriedade por meio de cooperativas.[62] O socialismo também é atribuído na França a Marie Roch Louis Reybaud, enquanto no Reino Unido é associado a Owen, que se tornou um dos pais do movimento cooperativo.[63][64] O termo se tornou corrente na década de 1840 e militantes, como Victor Considerant, usavam o termo para indicar um programa de reforma social radical antagônico à economia liberal.[65]
A definição e o uso do termo socialismo se estabeleceram na década de 1860, substituindo conceitos como associacionista, cooperativo e mutualista que haviam sido usados como sinônimos, enquanto o comunismo caiu em desuso durante este período.[66] Uma distinção inicial entre comunismo e socialismo era que o último visava apenas socializar a produção, enquanto o primeiro visava socializar tanto a produção quanto o consumo (na forma de livre acesso aos bens finais).[67] Em 1888, os marxistas empregaram o termo socialismo no lugar do comunismo, já que este passou a ser considerado um sinônimo antiquado de socialismo. Foi só depois da Revolução Bolchevique que o socialismo foi apropriado por Vladimir Lenin para significar um estágio entre o capitalismo e o comunismo. Ele o usou para defender o programa bolchevique da crítica marxista de que as forças produtivas do Império Russo não eram suficientemente desenvolvidas para o comunismo.[68] A distinção entre comunismo e socialismo tornou-se saliente em 1918 depois que o Partido Operário Social-Democrata Russo renomeou-se para Partido Comunista de Toda a Rússia, interpretando comunismo especificamente como socialistas que apoiavam a política e as teorias do bolchevismo, do leninismo e mais tarde do marxismo-leninismo,[69] embora os partidos comunistas continuassem a se descrever como socialistas dedicados ao socialismo.[70] De acordo com o The Oxford Handbook of Karl Marx, "Marx usou muitos termos para se referir a uma sociedade pós-capitalista - humanismo positivo, socialismo, comunismo, reino da individualidade livre, associação livre de produtores, etc. Ele usou esses termos de forma totalmente intercambiável. A noção de que 'socialismo' e 'comunismo' são etapas históricas distintas é alheia à sua obra e só entrou no léxico do marxismo após sua morte".[71]
Na Europa cristã, acreditava-se que os comunistas adotavam o ateísmo. Na Inglaterra protestante, o comunismo estava muito próximo do rito de comunhão católico romano, portanto socialista era o termo preferido.[72] Engels argumentou que em 1848, quando o Manifesto Comunista foi publicado, o socialismo era respeitável na Europa, enquanto o comunismo não. Os owenistas na Inglaterra e os fourieristas na França eram considerados socialistas respeitáveis, enquanto os movimentos da classe trabalhadora que "proclamavam a necessidade de uma mudança social total" se autodenominavam comunistas . Este ramo do socialismo produziu a obra comunista de Étienne Cabet na França e de Wilhelm Weitling na Alemanha.[73] O filósofo moral britânico John Stuart Mill discutiu uma forma de socialismo econômico dentro de um contexto liberal que mais tarde seria conhecido como socialismo liberal. Em edições posteriores de seus Princípios de Economia Política (1848), Mill argumentou ainda que "no que diz respeito à teoria econômica, não há nada em princípio na teoria econômica que impeça uma ordem econômica baseada em políticas socialistas"[74][75] e promoveu a substituição de negócios capitalistas por cooperativas de trabalhadores.[76] Enquanto os democratas viam as Revoluções de 1848 como uma revolução democrática que, a longo prazo, garantia liberdade, igualdade e fraternidade, os marxistas denunciavam-nas como uma traição aos ideais da classe trabalhadora por uma burguesia indiferente ao proletariado.[77]
Modelos e ideias socialistas que defendem a propriedade comum ou pública existem desde a Antiguidade. A economia do Império Máuria da Índia do século III a.C., uma monarquia absoluta, foi descrita por alguns estudiosos como "uma monarquia socializada" e "uma espécie de socialismo estatal" devido à "nacionalização das indústrias".[78][79] Outros estudiosos sugeriram que elementos do pensamento socialista estavam presentes na política dos filósofos gregos clássicos Platão[80] e Aristóteles.[81] Mazaces, o Jovem (morreu por volta de 524 ou 528 d.C.), um proto-socialista comunal persa,[82] instituiu as posses comunais e defendeu o bem público. Abu Dharr al-Ghifari, um companheiro de Maomé, é creditado por vários autores como o principal antecedente do socialismo islâmico.[83][84][85][86][87] Os ensinamentos de Jesus são frequentemente descritos como socialistas, especialmente por socialistas cristãos.[88] Atos 4:35: registra que na igreja primitiva em Jerusalém "ninguém alegava que qualquer uma de suas posses era sua", embora o padrão logo desapareça da história da igreja, exceto dentro do monaquismo. O socialismo cristão foi um dos fios fundadores do Partido Trabalhista britânico e afirma-se que começou com a revolta de Wat Tyler e John Ball no século XIV.[89] Após a Revolução Francesa, ativistas e teóricos como François-Noël Babeuf, Étienne-Gabriel Morelly, Philippe Buonarroti e Auguste Blanqui influenciaram os primeiros movimentos trabalhistas e socialistas franceses.[90] No Reino Unido, Thomas Paine propôs um plano detalhado para tributar proprietários de propriedades para pagar pelas necessidades dos pobres na obra Justiça Agrária[91] enquanto Charles Hall escreveu Os efeitos da civilização sobre as pessoas nos Estados europeus, denunciando os efeitos do capitalismo sobre os pobres de seu tempo.[92] Este trabalho influenciou os esquemas utópicos de Thomas Spence.[93]
Os primeiros movimentos socialistas autoconscientes desenvolveram-se nas décadas de 1820 e 1830. Fourieristas, owenistas e sansimonianos forneceram uma série de análises e interpretações da sociedade. Os owenistas, especialmente, se sobrepuseram a outros movimentos da classe trabalhadora, como os cartistas no Reino Unido.[94] Os cartistas reuniram um número significativo em torno da Carta do Povo de 1838, que buscava reformas democráticas voltadas para a extensão do sufrágio a todos os homens adultos. Os líderes do movimento pediram uma distribuição de renda mais equitativa e melhores condições de vida para as classes trabalhadoras. Os primeiros sindicatos e sociedades cooperativas de consumo seguiram o movimento cartista.[95] Pierre-Joseph Proudhon propôs sua filosofia de mutualismo em que "todos tinham igual direito, sozinhos ou como parte de uma pequena cooperativa, de possuir e usar a terra e outros recursos necessários para ganhar a vida".[96] Outras correntes inspiraram o socialismo cristão "muitas vezes na Grã-Bretanha e depois geralmente vindo de uma política liberal de esquerda e de um antiindustrialismo romântico"[90] que produziu teóricos como Edward Bellamy, Charles Kingsley e Frederick Denison Maurice.[97]
Os primeiros defensores do socialismo favoreciam o nivelamento social para criar uma sociedade meritocrática ou tecnocrática baseada no talento individual.[98] Henri de Saint-Simon ficou fascinado com o potencial da ciência e da tecnologia e defendeu uma sociedade socialista que eliminaria os aspectos desordenados do capitalismo com base na igualdade de oportunidades.[99] Ele buscava uma sociedade em que cada pessoa fosse classificada de acordo com as suas capacidades e recompensadas de acordo com o seu trabalho. Seu foco principal era a eficiência administrativa, industrialismo e a crença de que a ciência era essencial para o progresso.[100] Isso foi acompanhado pelo desejo de uma economia racionalmente organizada, baseada no planejamento e voltada para o progresso científico e material em larga escala. Outros primeiros pensadores socialistas, como Charles Hall e Thomas Hodgkin, basearam suas ideias nas teorias econômicas de David Ricardo. Eles raciocinaram que o valor de equilíbrio das mercadorias se aproximava dos preços cobrados pelo produtor quando essas mercadorias estavam em oferta elástica e que esses preços do produtor correspondiam ao trabalho incorporado - o custo do trabalho (essencialmente os salários pagos) que era necessário para produzir as mercadorias. Os socialistas ricardianos viam o lucro, os juros e o aluguel como deduções desse valor de troca.
Os críticos sociais da Europa Ocidental, incluindo Louis Blanc, Charles Fourier, Charles Hall, Robert Owen, Pierre-Joseph Proudhon e Saint-Simon foram os primeiros socialistas modernos que criticaram a pobreza e a desigualdade da Revolução Industrial. Eles defenderam a reforma, Owen defendendo a transformação da sociedade em pequenas comunidades sem propriedade privada. A contribuição de Owen para o socialismo moderno foi sua afirmação de que as ações e características individuais eram amplamente determinadas por seu ambiente social.[100] Por outro lado, Fourier defendia os falanstérios (comunidades que respeitam os desejos individuais, incluindo preferências sexuais), afinidades e criatividade e entendia que o trabalho deveria ser agradável para as pessoas.[101] As ideias de Owen e Fourier foram praticadas em comunidades intencionais por toda a Europa e América do Norte em meados do século XIX.
A Comuna de Paris foi um governo que administrou a cidade de Paris de 18 de março (formalmente, de 28 de março) a 28 de maio de 1871. A Comuna foi o resultado de um levante na capital francesa depois que o país foi derrotado na Guerra Franco-Prussiana. As eleições da Comuna foram realizadas em 26 de março. Eles elegeram um conselho de 92 membros, um membro para cada 20 mil residentes da cidade.[102] Apesar das diferenças internas, o conselho começou a organizar os serviços públicos e chegou a um consenso sobre certas políticas tendentes a uma social-democracia progressista, laica e altamente democrática.
Como a Comuna conseguiu se reunir em menos de 60 dias no total, apenas alguns decretos foram realmente implementados, o que incluía a separação de igreja e Estado; a remissão das rendas devidas pelo período de cerco (período durante o qual o pagamento foi suspenso); a abolição do trabalho noturno nas centenas de padarias parisienses; a concessão de pensões a companheiros solteiros e filhos de Guardas Nacionais mortos durante o serviço ativo; e a devolução gratuita de todas as ferramentas e utensílios domésticos dos trabalhadores no valor de até 20 francos que haviam sido prometidos durante o cerco.[103] A Comuna estava preocupada com o fato de trabalhadores qualificados terem sido forçados a penhorar suas ferramentas durante a guerra; o adiamento das obrigações da dívida comercial e a abolição dos juros das dívidas; e o direito dos empregados de assumir e administrar uma empresa se ela fosse abandonada por seu proprietário. A Comuna, no entanto, reconheceu o direito do proprietário anterior à indenização.
Em 1864, a Associação Internacional dos Trabalhadores foi fundada em Londres. Ele uniu diversas correntes revolucionárias, incluindo socialistas como os seguidores franceses de Proudhon,[104] blanquistas, philadelphes, sindicalistas ingleses e social-democratas. Em 1865 e 1866, realizou uma conferência preliminar e teve seu primeiro congresso em Genebra, respectivamente. Devido à sua grande variedade de filosofias, conflitos eclodiram imediatamente. As primeiras objeções a Marx vieram dos mutualistas que se opunham ao socialismo estatal. Pouco depois de Mikhail Bakunin e seus seguidores se unirem em 1868, a AIT polarizou-se em campos chefiados por Marx e Bakunin.[105] As diferenças mais claras entre os grupos emergiram sobre suas estratégias propostas para alcançar suas visões. A AIT se tornou o primeiro grande fórum internacional para a promulgação das ideias socialistas.
Os seguidores de Bakunin eram chamados de coletivistas e buscavam coletivizar a propriedade dos meios de produção enquanto retinham o pagamento proporcional à quantidade e tipo de trabalho de cada indivíduo. Como os proudhonianos, eles afirmavam o direito de cada indivíduo ao produto de seu trabalho e a ser remunerado por sua contribuição específica para a produção. Em contraste, os anarcocomunistas buscavam a propriedade coletiva tanto dos meios quanto dos produtos do trabalho. Como disse Errico Malatesta, “em vez de correr o risco de nos confundir ao tentar distinguir o que cada um de nós fazemos, trabalhemos todos e coloquemos tudo em comum. Desta forma, cada um dará à sociedade tudo o que sua força permite até que o suficiente seja produzido para cada um; e cada um levará tudo o que precisa, limitando suas necessidades apenas naquilo que ainda não é suficiente para cada um”.[106] O anarcocomunismo, enquanto uma filosofia político-econômica coerente, foi formulado pela primeira vez na seção italiana da AIT por Malatesta, Carlo Cafiero, Emilio Covelli, Andrea Costa e outros ex-republicanos mazzinianos.[107] Por respeito a Bakunin, eles não explicitaram suas diferenças com o anarquismo coletivista até depois de sua morte.[108]
O sindicalismo surgiu na França inspirado em parte por Proudhon e mais tarde por Pelloutier e Georges Sorel.[109] Ele se desenvolveu no final do século XIX a partir do movimento sindical francês (sindicato é a palavra francesa para sindicato). Foi uma força significativa na Itália e na Espanha no início do século XX, até ser esmagada pelos regimes fascistas desses países. Nos Estados Unidos, o sindicalismo apareceu sob a forma de Industrial Workers of the World, ou "Wobblies", fundado em 1905. O sindicalismo é um sistema econômico que organiza as indústrias em confederações (sindicatos)[110] e a economia é administrada por negociação entre especialistas e representantes dos trabalhadores de cada área, compreendendo múltiplas unidades categorizadas não competitivas.[111] O sindicalismo é uma forma de comunismo e corporativismo econômico, mas também se refere ao movimento político e às táticas usadas para criar esse tipo de sistema. Um movimento anarquista influente baseado em ideias sindicalistas é o anarcossindicalismo.[112] A Associação Internacional de Trabalhadores é uma federação anarcossindicalista internacional de vários sindicatos trabalhistas.
A Sociedade Fabiana é uma organização socialista britânica estabelecida para promover o socialismo por meios gradualistas e reformistas.[113] A sociedade lançou muitas bases do Partido Trabalhista e, subsequentemente, afetou as políticas dos estados emergentes da descolonização do Império Britânico, mais notavelmente Índia e Singapura. Originalmente, a Sociedade Fabiana estava comprometida com o estabelecimento de uma economia socialista, ao lado de um compromisso com o imperialismo britânico como uma força progressista e modernizadora.[114] Mais tarde, a sociedade funcionou principalmente como um think tank e é uma das quinze sociedades socialistas filiadas ao Partido Trabalhista britânico. Sociedades semelhantes existem na Austrália, no Canadá e na Nova Zelândia.
O socialismo corporativo é um movimento político que defende o controle operário da indústria por meio de corporações relacionadas ao comércio "em uma relação contratual implícita com o público".[115] Originou-se no Reino Unido e teve sua maior influência no primeiro quarto do século XX. Inspirados por guildas medievais, teóricos como Samuel George Hobson e G. D. H. Cole defenderam a propriedade pública das indústrias e a organização de suas forças de trabalho em corporações, cada uma delas sob o controle democrático de seu sindicato. Os socialistas corporativos eram menos inclinados do que os fabianos a investir o poder em um Estado.[109] Em algum momento, como no caso dos Cavaleiros do Trabalho estadunidenses, o socialismo corporativo quis abolir o sistema salarial.[116]
À medida que as ideias de Marx e Engels foram ganhando aceitação, particularmente na Europa central, os socialistas buscaram se unir em uma organização internacional. Em 1889 (centenário da Revolução Francesa), foi fundada a Segunda Internacional, com 384 delegados de vinte países, representando cerca de 300 organizações sindicais e socialistas.[117] Foi denominada Internacional Socialista e Engels foi eleito presidente honorário no terceiro congresso em 1893. Os anarquistas foram banidos, principalmente devido à pressão dos marxistas.[118] Tem-se argumentado que em algum ponto a Segunda Internacional se transformou "em um campo de batalha sobre a questão do socialismo libertário versus o autoritário. Eles não apenas se apresentaram efetivamente como defensores dos direitos das minorias; eles também provocaram os marxistas alemães a demonstrarem uma intolerância ditatorial que foi um fator que impediu o movimento trabalhista britânico de seguir a direção marxista indicada por líderes como H. M. Hyndman".
O reformismo surgiu como uma alternativa à revolução. Eduard Bernstein foi um importante social-democrata na Alemanha que propôs o conceito de socialismo evolucionário. Os socialistas revolucionários rapidamente se voltaram para o reformismo: Rosa Luxemburgo condenou o socialismo evolucionário de Bernstein em seu ensaio de 1900 Reforma social ou revolução? O socialismo revolucionário abrange vários movimentos sociais e políticos que podem definir "revolução" de maneiras diferentes. O Partido Social Democrata da Alemanha (SPD) se tornou o maior e mais poderoso partido socialista da Europa, apesar de trabalhar ilegalmente até que as leis antissocialistas foram derrubadas em 1890. Nas eleições de 1893, ganhou 1.787.000 votos, um quarto do total de votos expressos, de acordo com Engels. Em 1895, ano de sua morte, Engels enfatizou a ênfase do Manifesto Comunista em vencer, como primeiro passo, a "batalha da democracia".[119]
Na Argentina, o Partido Socialista da Argentina foi estabelecido na década de 1890, liderado por Juan B. Justo e Nicolás Repetto, entre outros. Foi o primeira partido político no país e na América Latina. O partido se filiou à Segunda Internacional.[120] Entre 1924 e 1940, foi membro do Internacional Operária e Socialista.[121]
Em 1904, os australianos elegeram Chris Watson como o primeiro primeiro-ministro australiano do Partido Trabalhista, tornando-se o primeiro socialista eleito democraticamente.[122] Em 1909, o primeiro kibutz foi estabelecido na Palestina[123] por imigrantes judeus russos. O Movimento Kibutz se expandiu ao longo do século XX seguindo uma doutrina do socialismo sionista.[124] O Partido Trabalhista britânico conquistou pela primeira vez cadeiras na Câmara dos Comuns em 1902. A Comissão Socialista Internacional foi formada em fevereiro de 1919 em uma reunião em Berna por partidos que desejavam ressuscitar a Segunda Internacional.[125]
Em 1917, o patriotismo da Primeira Guerra Mundial transformou-se em radicalismo político na Austrália, na maior parte da Europa e nos Estados Unidos. Outros partidos socialistas de todo o mundo que estavam começando a ganhar importância em suas políticas nacionais no início do século XX incluíam o Partido Socialista Italiano, a Seção Francesa da Internacional Operária, o Partido Socialista dos Trabalhadores Espanhóis, o Partido Social-Democrata Sueco, o Partido Trabalhista Social-Democrata Russo e o Partido Socialista na Argentina, o Partido Socialista dos Trabalhadores no Chile e o Partido Socialista da América nos Estados Unidos.
Em fevereiro de 1917, a revolução explodiu no Império Russo. Trabalhadores, soldados e camponeses estabeleceram sovietes (conselhos), a monarquia caiu e um governo provisório se convocou até a eleição de uma assembleia constituinte. Em abril daquele ano, Vladimir Lenin, líder da facção bolchevique dos socialistas russos e conhecido por suas profundas e controversas expansões do marxismo, foi autorizado a cruzar a Alemanha para retornar do exílio na Suíça.
Lenin publicou ensaios sobre sua análise do imperialismo, a fase de monopólio e globalização do capitalismo, bem como análises sobre as condições sociais. Ele observou que, à medida que o capitalismo se desenvolveu ainda mais na Europa e na América, os trabalhadores permaneceram incapazes de ganhar consciência de classe já que estavam ocupados demais trabalhando para pagar suas despesas. Ele, portanto, propôs que a revolução social exigiria a liderança de um partido de vanguarda de revolucionários com consciência de classe da parte educada e politicamente ativa da população.[126]
Ao chegar a Petrogrado, Lenin declarou que a revolução na Rússia apenas havia começado e que o próximo passo seria os sovietes operários assumirem plena autoridade. Ele emitiu uma tese delineando o programa bolchevique, incluindo a rejeição de qualquer legitimidade no governo provisório e a defesa do poder do Estado a ser administrado por meio dos sovietes. Os bolcheviques se tornaram a força mais influente. Em 7 de novembro, a capital do governo provisório foi invadida pelos Guardas Vermelhos bolcheviques no que mais tarde ficou conhecido como a Grande Revolução Socialista de Outubro. O governo provisório terminou e a República Socialista Federativa Soviética da Rússia — o primeiro Estado constitucionalmente socialista do mundo — foi estabelecida. Em 25 de janeiro de 1918, Lenin declarou "Viva a revolução socialista mundial!" no Soviete de Petrogrado[127] e propôs um armistício imediato em todas as frentes e transferiu as terras dos proprietários de terras, a coroa e os mosteiros para os comitês de camponeses sem compensação.[128]
No dia seguinte a assumir o poder executivo em 25 de janeiro, Lenin escreveu um Projeto de Regulamento sobre o Controle dos Trabalhadores, que concedia aos trabalhadores o controle de empresas com mais de cinco trabalhadores e funcionários de escritório e acesso a todos os livros, documentos e estoques e cujas decisões deveriam ser "vinculativas sobre os proprietários das empresas ".[129] Governando através dos sovietes eleitos e em aliança com os socialistas-revolucionários de esquerda de base camponesa, o governo bolchevique começou a nacionalizar os bancos e a indústria; e repudiou as dívidas nacionais do regime da Dinastia Romanov. Ele pediu a paz, retirando-se da Primeira Guerra Mundial e convocou uma Assembleia Constituinte na qual o Partido Socialista Revolucionário (SR) camponês obteve a maioria.[130]
A Assembleia Constituinte elegeu o líder Victor Chernov como presidente de uma república russa, mas rejeitou a proposta bolchevique de endossar os decretos soviéticos sobre terra, paz e controle dos trabalhadores e reconhecer o poder dos sovietes de deputados operários, soldados e camponeses. No dia seguinte, os bolcheviques declararam que a assembleia foi eleita em listas partidárias desatualizadas[131] e o Comitê Executivo Central de Toda a Rússia dos soviéticos a dissolveu.[132][133] Em março de 1919, os partidos comunistas mundiais formaram o Comintern (também conhecido como Terceira Internacional) em uma reunião em Moscou.[134]
Os partidos que não queriam fazer parte da ressuscitada Segunda Internacional (SI) ou do Comintern formaram a União de Partidos Socialistas para a Ação Internacional (UITPS, também conhecida como Internacional de Viena, União de Viena e Internacional Dois e Meio) em 27 de fevereiro de 1921 em uma conferência em Viena.[135] O SI e a UITPS se juntaram para formar a Internacional Operária e Socialista (IOS) em maio de 1923 em uma reunião em Hamburgo.[136] Grupos de esquerda que não concordaram com a centralização e o abandono dos sovietes pelo Partido Bolchevique levou da ala esquerdista contra os bolcheviques - tais grupos incluíam socialistas revolucionários,[137] revolucionários socialistas de esquerda, mencheviques e anarquistas.[138]
Dentro desse descontentamento de esquerda, os eventos de maior escala foram a rebelião[139][140][141] e o levante anarquista liderado pelo Exército Insurrecional Revolucionário da Ucrânia que controlava uma área conhecida como Território Livre.[142][143][144]
A Revolução Russa Bolchevique de janeiro de 1918 lançou partidos comunistas em muitos países e revoluções de 1917-1923. Poucos comunistas duvidaram que a experiência russa dependesse de revoluções socialistas da classe trabalhadora bem-sucedidas nos países capitalistas desenvolvidos.[145][146] Em 1919, Lenin e Trotsky organizaram os partidos comunistas mundiais em uma associação internacional de trabalhadores - a Internacional Comunista (Comintern), também chamada de Terceira Internacional.
A Revolução Russa influenciou levantes em outros países. A Revolução Alemã de 1918-1919 substituiu o governo imperial da Alemanha por uma república . A revolução durou de novembro de 1918 até o estabelecimento da República de Weimar em agosto de 1919. Incluiu um episódio conhecido como República Soviética da Baviera[147][148][149][150] e a revolta espartaquista. Na Itália, os eventos conhecidos como Biennio Rosso[151][152] foram caracterizados por greves em massa, manifestações de trabalhadores e experimentos de autogestão por meio de ocupações de terras e fábricas. Em Turim e Milão, conselhos de trabalhadores foram formados e muitas ocupações de fábricas aconteceram lideradas por anarcossindicalistas organizados em torno da Unione Sindacale Italiana.
Em 1920, o Exército Vermelho comandado por Trotsky havia derrotado em grande parte os Exércitos Brancos monarquistas. Em 1921, o comunismo de guerra foi encerrado e, sob a Nova Política Econômica (NPE), a propriedade privada foi permitida para pequenas e médias empresas camponesas. Embora a indústria permanecesse amplamente controlada pelo Estado, Lenin reconheceu que a NPE era uma medida capitalista necessária para um país despreparado para o socialismo. A lucratividade voltou na forma de "homens da NPE" e os camponeses ricos (kulaks) ganharam o poder.[153] O papel de Trotsky foi questionado por outros socialistas, incluindo ex-trotskistas. Nos Estados Unidos, Dwight Macdonald rompeu com Trotsky e deixou o Partido Socialista dos Trabalhadores ao notar a Revolta de Kronstadt, que Trotsky e outros bolcheviques reprimiram brutalmente. Ele então se mudou para o socialismo democrático[154] e o anarquismo.[155]
Uma crítica semelhante ao papel de Trotsky na Revolta de Kronstadt foi levantada pela anarquista estadunidense Emma Goldman. Em seu ensaio "Trotsky Protesta Demais", ela afirma: "Eu admito, a ditadura sob o governo de Stalin se tornou monstruosa. Isso, entretanto, não diminui a culpa de Leon Trotsky como um dos atores do drama revolucionário do qual Kronstadt foi uma das cenas mais sangrentas".[156]
Em 1922, o quarto congresso da Internacional Comunista assumiu a política da frente única e exortou os comunistas a trabalharem com os social-democratas comuns, embora permanecessem críticos de seus líderes por trair a classe trabalhadora ao apoiar os esforços de guerra dos capitalistas. Os social-democratas apontaram para o deslocamento causado pela revolução e, posteriormente, para o crescente autoritarismo dos partidos comunistas. O Partido Trabalhista rejeitou o pedido do Partido Comunista da Grã-Bretanha de se afiliar a eles em 1920.
Ao ver o crescente poder coercitivo do Estado soviético em 1923, um moribundo Lenin disse que a Rússia havia se revertido para "uma máquina burguesa czarista [...] mal envernizada com o socialismo".[157] Após a morte de Lenin em janeiro de 1924, o Partido Comunista da União Soviética - então cada vez mais sob o controle de Josef Stalin - rejeitou a teoria de que o socialismo não poderia ser construído apenas na União Soviética em favor do conceito de socialismo em um país. Apesar da demanda marginalizada da Oposição de Esquerda pela restauração da democracia soviética, Stalin desenvolveu um governo burocrático e autoritário que foi condenado por socialistas democráticos, anarquistas e trotskistas por minar os ideais revolucionários.[158]
Em 1924, a República Popular da Mongólia foi estabelecida e era governada pelo Partido Popular da Mongólia. A Revolução Russa e suas consequências motivaram partidos comunistas nacionais em outros lugares que ganharam influência política e social, na França, Estados Unidos, Itália, China, México, Brasil, Chile e Indonésia .
Na Espanha, em 1936, o sindicato nacional anarcossindicalista Confederación Nacional del Trabajo (CNT) inicialmente recusou-se a aderir a uma aliança eleitoral de frente popular. Sua abstenção levou à vitória eleitoral da direita. Em 1936, a CNT mudou sua política e os votos anarquistas ajudaram a devolver a frente popular ao poder. Meses depois, a antiga classe dominante tentou um golpe, desencadeando a Guerra Civil Espanhola (1936-1939).[159]
Em resposta à rebelião do exército, um movimento de camponeses e trabalhadores de inspiração anarquista, apoiado por milícias armadas, assumiu o controle de Barcelona e de grandes áreas da Espanha rural onde coletivizaram a terra.[160] A Revolução Espanhola foi uma revolução social dos trabalhadores que começou com a Guerra Civil Espanhola em 1936 e resultou na implementação generalizada de princípios organizacionais anarquistas e socialistas mais amplamente libertários em algumas áreas por dois a três anos, principalmente Catalunha, Aragão, Andaluzia e partes do Levante
Grande parte da economia espanhola ficou sob controle dos trabalhadores. Em fortalezas anarquistas como a Catalunha, o número chegava a 75%, mas era menor em áreas com forte influência do Partido Comunista, que resistia ativamente às tentativas de coletivização. As fábricas eram administradas por comitês de trabalhadores, as áreas agrárias eram coletivizadas e administradas como comunas libertárias. O historiador anarquista Sam Dolgoff estimou que cerca de oito milhões de pessoas participaram direta ou indiretamente da Revolução Espanhola.[161]
A Quarta Internacional de Trotsky foi estabelecida na França em 1938 quando os trotskistas argumentaram que o Comintern ou Terceira Internacional havia se tornado irremediavelmente "perdido para o stalinismo" e, portanto, incapaz de conduzir a classe trabalhadora ao poder.[163] A ascensão do nazismo e o início da Segunda Guerra Mundial levaram à dissolução do Internacional Operária e Socialista em 1940. Após a guerra, a Internacional Socialista foi formada em Frankfurt em julho de 1951 como sua sucessora.[164]
Após a Segunda Guerra Mundial, os governos social-democratas introduziram reformas sociais e redistribuição de riqueza por meio da previdência e da tributação. Os partidos social-democratas dominaram a política do pós-guerra em países como França, Itália, Tchecoslováquia, Bélgica e Noruega. A certa altura, a França afirmou ser o país capitalista mais controlado pelo Estado do mundo. Ela nacionalizou vários serviços públicos, incluindo Charbonnages de France (CDF), Électricité de France (EDF), Gaz de France (GDF), Air France, Banque de France e Régie Nationale des Usines Renault.[165]
Em 1945, o Partido Trabalhista britânico liderado por Clement Attlee foi eleito com base em um programa socialista radical. O governo trabalhista nacionalizou as indústrias, como minas, gás, carvão, eletricidade, ferrovia, ferro, aço e o Banco da Inglaterra. A British Petroleum foi oficialmente nacionalizada em 1951.[166] Anthony Crosland disse que, em 1956, 25% da indústria britânica foi nacionalizada e que os funcionários públicos, incluindo aqueles em indústrias nacionalizadas, constituíam uma proporção semelhante dos trabalhadores do país.[167] Os governos trabalhistas de 1964-1970 e 1974-1979 intervieram ainda mais.[168] Eles renacionalizaram a British Steel (1967) depois que os conservadores a desnacionalizaram e nacionalizaram a British Leyland (1976).[169] O Serviço Nacional de Saúde prestava assistência médica financiada pelo contribuinte a todos, gratuitamente.[170] Habitações para a classe trabalhadora foram fornecidas em conjuntos habitacionais municipais e o ensino universitário tornou-se disponível através de um sistema de subsídio escolar.[171]
Durante a maior parte da era pós-guerra, a Suécia foi governada pelo Partido Operário Social-Democrata Sueco, em grande parte em cooperação com sindicatos e indústria.[172] No país, os sociais-democratas ocuparam o poder de 1936 a 1976, de 1982 a 1991, de 1994 a 2006 e de 2014 a 2023, mais recentemente em uma coalizão minoritária. Tage Erlander foi o primeiro líder do partido. Ele liderou o governo de 1946 a 1969, o mais longo governo parlamentar ininterrupto. Esses governos expandiram substancialmente o estado de bem-estar social.[173] O primeiro-ministro sueco, Olof Palme, foi identificado como um "socialista democrático"[174] e foi descrito como um "reformista revolucionário".
O Partido Trabalhista norueguês foi estabelecido em 1887 e era basicamente uma federação sindical. O partido não proclamou uma agenda socialista, elevando o sufrágio universal e a dissolução da união com a Suécia como suas principais prioridades. Em 1899, a Confederação Norueguesa de Sindicatos se separou do Partido Trabalhista. Na época da Revolução Russa, o Partido Trabalhista moveu-se para a esquerda e juntou-se à Internacional Comunista de 1919 a 1923. Depois disso, o partido ainda se considerava revolucionário, mas a esquerda do partido rompeu e estabeleceu o Partido Comunista da Noruega, enquanto o Partido Trabalhista gradualmente adotava uma linha reformista por volta de 1930. Em 1935, Johan Nygaardsvold estabeleceu uma coalizão que durou até 1945.[175]
De 1946 a 1962, o Partido Trabalhista norueguês teve maioria absoluta no parlamento liderado por Einar Gerhardsen, que permaneceu como primeiro-ministro por dezessete anos. Embora o partido tenha abandonado a maioria de suas ideias socialistas anteriores à guerra, o estado de bem-estar social foi expandido sob Gerhardsen para garantir a provisão universal dos direitos humanos básicos e estabilizar a economia.[176] Na eleição parlamentar norueguesa de 1945, o Partido Comunista obteve 12% dos votos, mas ele praticamente desapareceu durante a Guerra Fria.[177] Na década de 1950, o socialismo popular emergiu nos países nórdicos e colocou-se entre o comunismo e a social-democracia.[178] No início dos anos 1960, o Partido da Esquerda Socialista desafiou o Partido Trabalhista de esquerda.[175] Também na década de 1960, Gerhardsen estabeleceu uma agência de planejamento e tentou estabelecer uma economia planejada. Na década de 1970, um partido socialista mais radical, o Partido Comunista dos Trabalhadores (AKP), rompeu com o Partido de Esquerda Socialista e teve notável influência em associações estudantis e alguns sindicatos. O AKP se identificava com a China comunista e a Albânia, mas não com a União Soviética.[179]
Em países como a Suécia, o modelo Rehn-Meidner[180] permitiu aos capitalistas donos de empresas produtivas e eficientes reter lucros às custas dos trabalhadores das empresas, exacerbando a desigualdade e fazendo com que os trabalhadores agitassem por uma parte dos lucros na década de 1970. Naquela época, as mulheres que trabalhavam no setor estatal passaram a exigir melhores salários. Rudolf Meidner estabeleceu um comitê de estudo que apresentou uma proposta em 1976 para transferir os lucros excedentes para fundos de investimento controlados pelos trabalhadores, com a intenção de que as empresas criassem empregos e pagassem salários mais altos, em vez de recompensar os proprietários e gerentes das empresas.[181] Os capitalistas imediatamente rotularam essa proposta de socialismo e lançaram uma oposição sem precedentes - incluindo a revogação do compromisso de classe estabelecido no Acordo de Saltsjöbaden de 1938.[182] Os partidos social-democratas são alguns dos mais antigos desse tipo e operam em todos os países nórdicos. Países ou sistemas políticos que há muito são dominados por partidos social-democratas são frequentemente rotulados de social-democratas.[183][184] Esses países se enquadram no tipo social-democrata de "alto socialismo", que é descrito como favorável a "um alto nível de descomodificação e um baixo grau de estratificação".[185]
O modelo nórdico é uma forma de sistema político-econômico comum aos países nórdicos (Dinamarca, Finlândia, Islândia, Noruega e Suécia). Tem três ingredientes principais, a negociação pacífica e institucionalizada entre empregadores e sindicatos; a política macroeconômica ativa, previsível e comedida; e o bem-estar universal e educação gratuita. O sistema de bem-estar social é governamental na Noruega e na Suécia, enquanto os sindicatos desempenham um papel mais importante na Dinamarca, Finlândia e Islândia.[186][187][188][189][190] O modelo nórdico é frequentemente rotulado de social-democrata e contrastado com o modelo continental conservador e o modelo liberal anglo-americano. As principais reformas nos países nórdicos são o resultado de consenso e compromisso em todo o espectro político. As principais reformas foram implementadas em gabinetes social-democratas na Dinamarca, Noruega e Suécia, enquanto os partidos de centro-direita dominaram durante a implementação do modelo na Finlândia e na Islândia. Desde a Segunda Guerra Mundial, os países nórdicos têm mantido em grande parte uma economia mista social-democrata, caracterizada pela participação da força de trabalho, igualdade de gênero, benefícios igualitários e universais, redistribuição da riqueza e política fiscal expansionista.[176][191]
Na Noruega, os primeiros seguros sociais obrigatórios foram introduzidos por gabinetes conservadores em 1895 e 1911. Durante a década de 1930, o Partido Trabalhista adotou o projeto de estado de bem-estar dos conservadores. Após a Segunda Guerra Mundial, todos os partidos políticos concordaram que o estado de bem-estar social deveria ser expandido. A segurança social universal (Folketrygden) foi introduzida pelo gabinete conservador de Borten.[192][193] A economia da Noruega está aberta ao mercado internacional ou europeu para a maioria dos produtos e serviços, ingressando no mercado interno da União Europeia em 1994 por meio do Espaço Econômico Europeu. Algumas das instituições de economia mista do período pós-guerra foram relaxadas pelo gabinete conservador da década de 1980 e o mercado financeiro foi desregulamentado.[194] Dentro da estrutura das Variedades do Capitalismo, a Finlândia, a Noruega e a Suécia são identificadas como economias de mercado coordenadas.[195]
A era soviética viu algumas das conquistas tecnológicas mais significativas do século XX, incluindo a primeira espaçonave do mundo e o primeiro astronauta. A economia soviética foi a primeira economia planejada centralmente do mundo moderno. Adotou a propriedade estatal da indústria administrada por meio da Gosplan (Comissão de Planejamento do Estado), Gosbank (Banco do Estado) e da Gossnab (Comissão Estadual de Fornecimento de Materiais e Equipamentos).
O planejamento econômico foi conduzido por meio de Planos Quinquenais em série. A ênfase estava no desenvolvimento da indústria pesada. O país tornou-se um dos principais fabricantes mundiais de produtos industriais básicos e pesados, ao mesmo tempo que não enfatizou a produção industrial leve e os bens de consumo duráveis.  modernização trouxe um aumento geral no padrão de vida.[196]
O Bloco Oriental era o grupo de Estados comunistas da Europa Central e Oriental, incluindo a União Soviética e os países do Pacto de Varsóvia,[197][198][199] como a Polônia, a República Democrática Alemã, a Hungria, a Bulgária, a Tchecoslováquia, a Romênia, a Albânia e a Iugoslávia (até Informbiro em 1948). A Revolução Húngara de 1956 foi uma revolta nacional espontânea contra o governo comunista, que durou de 23 de outubro a 10 de novembro de 1956. A denúncia do líder soviético Nikita Khrushchev dos excessos do regime de Stalin durante o XX Congresso do Partido Comunista em 1956,[200] bem como a revolta húngara,[201][202] produziu desunião dentro dos partidos comunistas e socialistas da Europa Ocidental.
Nos anos do pós-guerra, o socialismo tornou-se cada vez mais influente em muitos países em desenvolvimento. Abraçando o socialismo do Terceiro Mundo, os países da África, Ásia e América Latina frequentemente nacionalizaram as indústrias. A Revolução Comunista Chinesa foi a segunda etapa da Guerra Civil Chinesa, que terminou com o estabelecimento da República Popular da China liderada pelo Partido Comunista Chinês. O então Partido Kuomintang chinês na década de 1920 incorporou o socialismo chinês como parte de sua ideologia.[203][204]
O surgimento dessa nova entidade política no contexto da Guerra Fria foi complexo e doloroso. Vários esforços experimentais foram feitos para organizar novos Estados independentes, a fim de estabelecer uma frente comum para limitar a influência dos Estados Unidos e da União Soviética sobre eles. Isso levou à ruptura sino-soviética.
O Movimento Não Alinhado se reuniu em torno das figuras de Jawaharlal Nehru da Índia, Sukarno da Indonésia, Josip Broz Tito da Iugoslávia e Gamal Abdel Nasser do Egito.
Após a Conferência de Genebra de 1954, que encerrou a Primeira Guerra da Indochina, a Conferência de Bandungue de 1955 reuniu Nasser, Nehru, Tito, Sukarno e o primeiro-ministro chinês Zhou Enlai. Como muitos países africanos ganharam independência durante a década de 1960, alguns deles rejeitaram o capitalismo em favor do socialismo africano como definido por Julius Nyerere da Tanzânia, Léopold Senghor do Senegal, Kwame Nkrumah de Gana e Sékou Touré da Guiné.[205]
A Revolução Cubana (1953-1959) foi uma revolta armada conduzida pelo Movimento 26 de Julho de Fidel Castro e seus aliados contra o governo de Fulgencio Batista. O governo de Castro acabou adotando o comunismo, tornando-se o Partido Comunista de Cuba em outubro de 1965.[206]
Na Indonésia, um regime militar de direita liderado por Suharto matou entre 500 mil e um milhão de pessoas em 1965 e 1966, principalmente para esmagar a crescente influência do Partido Comunista e de outros grupos de esquerda, com o apoio dos Estados Unidos que forneciam listas de mortes contendo milhares de nomes de supostos comunistas de alto escalão.[207][208][209][210][211]
A Nova Esquerda foi um termo usado principalmente no Reino Unido e nos Estados Unidos em referência a ativistas, educadores, agitadores e outros que, nas décadas de 1960 e 1970, buscaram implementar uma ampla gama de reformas em questões como direitos dos homossexuais, aborto, papéis de gênero e as drogas[212] em contraste com os movimentos de esquerda ou marxistas anteriores, que adotaram uma abordagem mais vanguardista da justiça social e se concentraram principalmente na sindicalização dos trabalhadores e nas questões de classe social.[213][214][215] A Nova Esquerda rejeitou o envolvimento com o movimento operário e com a teoria histórica da luta de classes do marxismo.[216]
Nos Estados Unidos, a Nova Esquerda foi associada ao movimento hippie e aos movimentos de protesto contra a Guerra do Vietnã nos campi universitários, bem como aos movimentos de libertação negra, como o Partido dos Panteras Negras.[217] Embora inicialmente formados em oposição ao Partido Democrata da "Velha Esquerda", os grupos que compõem a Nova Esquerda gradualmente se tornaram atores centrais na coalizão democrata.[212]
Os protestos de 1968 representaram uma escalada mundial de conflitos sociais, caracterizados predominantemente por rebeliões populares contra as elites militares, capitalistas e burocráticas que responderam com uma escalada de repressão política. Esses protestos marcaram uma virada para o movimento pelos direitos civis nos Estados Unidos, que produziu movimentos revolucionários, como os Panteras Negras. O proeminente líder dos direitos civis Martin Luther King Jr. organizou a "Campanha dos Pobres" para tratar de questões de justiça econômica,[218] enquanto pessoalmente demonstrava simpatia pelo socialismo democrático.[219] Em reação à ofensiva do Tet, os protestos também desencadearam um amplo movimento de oposição à Guerra do Vietnã em todos os Estados Unidos e até mesmo em Londres, Paris, Berlim e Roma. Em 1968, a Internacional das Federações Anarquistas foi fundada durante uma conferência anarquista internacional realizada em Carrara pelas três federações europeias existentes: Francesa, a Ibérica e Italiana, bem como a federação búlgara no exílio francês.
Os movimentos socialistas ou comunistas de massa cresceram não apenas nos Estados Unidos, mas também na maioria dos países europeus. A manifestação mais espetacular disso foram os protestos de maio de 1968 na França, nos quais os estudantes se uniram a greves de até dez milhões de trabalhadores e por alguns dias o movimento parecia capaz de derrubar o governo.
Em muitos outros países capitalistas, as lutas contra as ditaduras, a repressão estatal e a colonização também foram marcadas por protestos em 1968, como o início dos Troubles na Irlanda do Norte, o massacre de Tlatelolco na Cidade do México e a escalada da guerra de guerrilha contra a ditadura militar em Brasil. Países governados por partidos comunistas protestaram contra as elites burocráticas e militares. Na Europa Oriental, houve protestos generalizados que aumentaram principalmente na Primavera de Praga na Tchecoslováquia. Em resposta, a União Soviética ocupou o território tchecoslovaco, mas a ocupação foi denunciada pelos partidos comunistas italiano e francês[220] e pelo Partido Comunista da Finlândia. Poucos líderes políticos da Europa Ocidental defenderam a ocupação, entre eles o secretário-geral comunista português Álvaro Cunhal,[221] junto com o Partido Comunista de Luxemburgo e facções conservadoras do Partido Comunista da Grécia.
Na Revolução Cultural Chinesa, um movimento social-político da juventude se mobilizou contra os elementos "burgueses" que aparentavam estar se infiltrando no governo e na sociedade em geral, com o objetivo de restaurar o capitalismo. Este movimento motivou movimentos inspirados pelo maoismo em todo o mundo no contexto da ruptura sino-soviética.
Na década de 1960, uma tendência socialista dentro da Igreja Católica latino-americana apareceu e ficou conhecida como teologia da libertação[223][224] Isso motivou o padre colombiano Camilo Torres Restrepo a entrar na guerrilha doELN. No Chile, Salvador Allende, médico e candidato pelo Partido Socialista, foi eleito presidente em 1970. Em 1973, seu governo foi derrubado pela ditadura militar de Augusto Pinochet, apoiada pelos Estados Unidos, que durou até o final dos anos 1980.[225] O regime de Pinochet era o líder da Operação Condor, uma campanha de repressão e de terrorismo de Estado apoiada pelos Estados Unidos, realizada pelos serviços de inteligência dos países do Cone Sul da América Latina para eliminar a suposta subversão comunista."[226][227] Na Jamaica, o socialista democrático[228] Michael Manley serviu como quarto primeiro-ministro jamaicano de 1972 a 1980 e de 1989 a 1992. De acordo com pesquisas de opinião, ele continua sendo um dos primeiros-ministros mais populares da Jamaica desde a independência.[229] A Revolução da Nicarágua englobou a crescente oposição à ditadura de Somoza nas décadas de 1960 e 1970, a campanha liderada pela Frente Sandinista de Libertação Nacional (FSLN) para derrubar violentamente a ditadura em 1978-1979, os esforços subsequentes da FSLN para governar a Nicarágua a partir de 1979 até 1990[230] e as medidas socialistas que incluíram reforma agrária[231][232] e programas educacionais.[233] O Governo Revolucionário do Povo foi proclamado em 13 de março de 1979 em Granada, mas foi derrubado pelas Forças Armadas dos Estados Unidos em 1983. A Guerra Civil Salvadorenha (1979-1992) foi um conflito entre o governo militar de El Salvador e a Frente Farabundo Martí de Libertação Nacional (FMLN), uma coalizão ou organização guarda-chuva de cinco grupos guerrilheiros socialistas. Um golpe em 15 de outubro de 1979 levou à matança de manifestantes anti-golpe pelo governo, bem como manifestantes anti-desordem pela guerrilha, e é amplamente visto como o ponto de inflexão para a guerra civil.[234]
Na Itália, a Autonomia Operaia foi um movimento de esquerda particularmente ativo de 1976 a 1978. Teve um papel importante no movimento autonomista na década de 1970, ao lado de organizações anteriores como Potere Operaio (criada após maio de 1968) e Lotta Continua.[235] Essa experiência estimulou o autonomismo do movimento radical socialista contemporâneo.[236] Em 1982, o recém-eleito governo socialista francês de François Mitterrand fez nacionalizações em alguns setores importantes, incluindo bancos e seguradoras.[237] O eurocomunismo foi uma tendência nas décadas de 1970 e 1980 em vários partidos comunistas da Europa Ocidental para desenvolver uma teoria e prática de transformação social que fosse mais relevante para um país da Europa Ocidental e menos alinhada à influência ou controle do Partido Comunista da União Soviética. Fora da Europa Ocidental, às vezes é chamado de neocomunismo.[238] Alguns partidos comunistas com forte apoio popular, notadamente o Partido Comunista Italiano (PCI) e o Partido Comunista da Espanha (PCE) adotaram o eurocomunismo com mais entusiasmo e o Partido Comunista da Finlândia também foi dominado por eurocomunistas. O Partido Comunista Francês (PCF) e muitos partidos menores se opuseram fortemente ao eurocomunismo e permaneceram alinhados com o Partido Comunista soviético até o colapso da União Soviética.
No final da década de 1970 e na década de 1980, a Internacional Socialista (IS) teve extensos contatos e discussões com as duas potências da Guerra Fria, os Estados Unidos e a União Soviética, sobre as relações ocidente-oriente e o controle de armas. Desde então, a IS admitiu como partidos membros o FSLN da Nicarágua, o Partido da Independência de Porto Rico, bem como antigos partidos comunistas, como o Partido Democrático de Esquerda da Itália e a Frente de Libertação de Moçambique (FRELIMO). A IS ajudou os partidos social-democratas a se restabelecerem quando a ditadura deu lugar à democracia em Portugal (1974) e na Espanha (1975). Até o Congresso de Genebra de 1976, a IS tinha poucos membros fora da Europa e nenhum envolvimento formal com a América Latina.[239]
Após a morte de Mao Zedong em 1976 e a prisão da facção conhecida como Gangue dos Quatro, culpada pelos excessos da Revolução Cultural, Deng Xiaoping assumiu o poder e conduziu a República Popular da China a reformas econômicas significativas. O Partido Comunista Chinês (PCC) afrouxou o controle governamental sobre a vida pessoal dos cidadãos e as comunas foram dissolvidas em favor de arrendamentos de terras privadas, assim, a transição da China de uma economia planejada para uma economia mista denominada "socialismo com características chinesas"[240] manteve os direitos de propriedade estatal sobre a terra, a propriedade estatal ou cooperativa de grande parte dos setores industriais e de manufatura pesados e a influência estatal nos setores bancário e financeiro. A China adotou sua constituição atual em 4 de dezembro de 1982. O secretário-geral do Partido Comunista Chinês, Jiang Zemin, os primeiros-ministros Li Peng e Zhu Rongji lideraram o país na década de 1990. Sob sua administração, o desempenho econômico da China levou cerca de 150 milhões de camponeses saíram da pobreza e sustentaram uma taxa média de crescimento do produto interno bruto (PIB) de 11,2%.ao ano[241][242] No Sexto Congresso Nacional do Partido Comunista do Vietnã em dezembro de 1986, políticos reformistas substituíram o governo da "velha guarda" por uma nova liderança.[243][244] Os reformadores eram liderados por Nguyen Van Linh, de 71 anos, que se tornou o novo secretário-geral do partido. Linh e os reformadores implementaram uma série de reformas de mercado livre chamada Đổi Mới ("Renovação"), que administrou cuidadosamente a transição de uma economia planejada para uma "economia de mercado de orientação socialista".[245][246] Mikhail Gorbachev desejava levar a União Soviética em direção a uma social-democracia de estilo nórdico, chamando-a de "um farol socialista para toda a humanidade".[247] Antes de sua dissolução em 1991, a economia da União Soviética era a segunda maior do mundo, depois dos Estados Unidos.[248] Com o colapso da URSS, a integração econômica das repúblicas soviéticas foi dissolvida e a atividade industrial geral declinou substancialmente.[249] Um legado duradouro permanece na infraestrutura física criada durante décadas de práticas combinadas de produção industrial e destruição ambiental generalizada.[250] A transição para o capitalismo na ex-União Soviética e no Bloco do Leste, que foi acompanhada pela "terapia de choque" inspirada no Consenso de Washington,[251] resultou em uma queda acentuada no padrão de vida. A região experimentou o aumento da desigualdade econômica e da pobreza[252] um aumento no excesso de mortalidade[253][254] e um declínio na expectativa de vida,[255] que foi acompanhado pelo fortalecimento de uma oligarquia empresarial recém-estabelecida na primeira. O país pós-soviético médio havia retornado aos níveis de 1989 de PIB per capita em 2005,[256] embora alguns ainda estejam muito atrás disso,[257] o que levou a um aumento do sentimento nacionalista e da nostalgia da era comunista.[258][259][260]
Muitos partidos social-democratas, especialmente após a Guerra Fria, adotaram políticas de mercado neoliberais, incluindo privatização, desregulamentação e financeirização. Eles abandonaram sua busca pelo socialismo moderado em favor do liberalismo econômico. Na década de 1980, com a ascensão de políticos neoliberais conservadores, como Ronald Reagan nos Estados Unidos, Margaret Thatcher no Reino Unido, Brian Mulroney no Canadá e Augusto Pinochet no Chile, o estado de bem-estar social ocidental foi atacado de dentro, mas o apoio estatal ao corporativo setor foi mantido.[261] Monetaristas e neoliberais atacaram os sistemas de bem-estar social como impedimentos ao empreendedorismo privado. No Reino Unido, o então líder do Partido Trabalhista, Neil Kinnock, fez um ataque público contra o grupo entrista militante na conferência do Partido Trabalhista de 1985. O Trabalhismo decidiu que o Militant não era elegível para afiliação ao partido e gradualmente expulsou seus apoiadores. A liderança de Kinnock se recusou a apoiar a greve dos mineiros de 1984–1985 sobre o fechamento de minas, uma decisão que a ala esquerda do partido e o Sindicato Nacional dos Mineiros culparam pela derrota final da greve. Em 1989, o 18º Congresso da Internacional Socialista adotou uma nova Declaração de Princípios, afirmando:O socialismo democrático é um movimento internacional pela liberdade, justiça social e solidariedade. Seu objetivo é alcançar um mundo pacífico onde esses valores básicos possam ser valorizados e onde cada indivíduo possa viver uma vida significativa com o pleno desenvolvimento de sua personalidade e talentos, e com a garantia dos direitos humanos e civis em um contexto democrático de sociedade.[262]Na década de 1990, o Partido Trabalhista britânico sob o comando de Tony Blair promulgou políticas baseadas na economia de mercado livre para fornecer serviços públicos por meio da iniciativa de finanças privadas. Influente nessas políticas foi a ideia de uma Terceira Via, que exigia uma reavaliação das políticas do estado de bem-estar social.[263] Em 1995, o Partido Trabalhista redefiniu sua postura sobre o socialismo reformulando a Cláusula IV de sua constituição, definindo o socialismo em termos éticos e removendo todas as referências à propriedade pública, trabalhista direta ou municipal dos meios de produção. O Partido Trabalhista afirmou: "O Partido Trabalhista é um partido socialista democrático. Acredita que, pela força do nosso esforço comum, alcançamos mais do que alcançamos sozinhos, de modo a criar, para cada um de nós, os meios para realizar nosso verdadeiro potencial e, para todos nós, uma comunidade em que o poder, riqueza e oportunidade estão nas mãos de muitos, não de poucos".[264]
O socialismo africano foi e continua a ser uma ideologia importante em todo o continente. Julius Nyerere foi inspirado pelos ideais socialistas fabianos.[265] Ele acreditava firmemente nos africanos rurais e em suas tradições e no ujamaa, um sistema de coletivização que, segundo Nyerere, estava presente antes do imperialismo europeu. Essencialmente, ele acreditava que os africanos já eram socialistas. Outros socialistas africanos incluem Jomo Kenyatta, Kenneth Kaunda, Nelson Mandela e Kwame Nkrumah. Fela Kuti foi inspirado pelo socialismo e clamou por uma república africana democrática. Na África do Sul, o Congresso Nacional Africano (ANC) abandonou suas alianças socialistas parciais após assumir o poder e seguiu uma rota neoliberal padrão. De 2005 a 2007, o país foi devastado por milhares de protestos de comunidades pobres. Um deles deu origem a um movimento em massa de moradores de barracos, o Abahlali baseMjondolo, que, apesar da grande repressão policial, continua trabalhando para o planejamento popular e contra a criação de uma economia de mercado em terras e moradias.
Na Ásia, os Estados com economias socialistas - como a República Popular da China, Coreia do Norte, Laos e Vietnã - se afastaram amplamente do planejamento econômico centralizado no século XXI, colocando uma ênfase maior nos mercados, como a economia de mercado socialista chinesa e a economia de mercado orientada para o socialismo vietnamita. Eles usam modelos de gestão corporativa de propriedade estatal em oposição à modelagem de empresa socialista em estilos de gestão tradicionais empregados por agências governamentais. Na China, os padrões de vida continuaram a melhorar rapidamente, apesar da recessão do final dos anos 2000, mas o controle político centralizado permaneceu rígido.[266] Brian Reynolds Myers em seu livro The Cleanest Race, posteriormente apoiado por outros acadêmicos,[267] rejeita a ideia de que Juche é a ideologia líder da Coreia do Norte, considerando sua exaltação pública como destinada a enganar os estrangeiros,[268] apontando que constituição norte-coreana de 2009 omite qualquer menção ao comunismo.[269]
Embora a autoridade do estado permanecesse incontestada sob a Đổi Mới, o governo do Vietnã incentiva a propriedade privada de fazendas e fábricas, a desregulamentação econômica e o investimento estrangeiro, enquanto mantém o controle sobre as indústrias estratégicas.[246] A economia vietnamita posteriormente alcançou forte crescimento na produção agrícola e industrial, construção, exportações e investimento estrangeiro. No entanto, essas reformas também causaram um aumento na desigualdade de renda e disparidades de gênero.[270][271]
Em outras partes da Ásia, alguns partidos socialistas e partidos comunistas eleitos permanecem proeminentes, particularmente na Índia e no Nepal. O Partido Comunista do Nepal (Marxista-Leninista Unificado), em particular, clama por democracia multipartidária, igualdade social e prosperidade econômica.[272] Em Singapura, a maior parte do PIB ainda é gerada pelo setor estatal, composto por empresas vinculadas ao governo.[273] No Japão, tem havido um ressurgimento do interesse pelo Partido Comunista Japonês entre os trabalhadores e a juventude.[274][275] Na Malásia, o Partido Socialista da Malásia teve seu primeiro membro do Parlamento, Michael Jeyakumar Devaraj, após as eleições gerais de 2008. Em 2010, havia 270 kibutz em Israel. Suas fábricas e fazendas respondem por 9% da produção industrial de Israel, no valor de 8 bilhões de dólares e 40% de sua produção agrícola, que vale mais de 1,7 bilhão de dólares.[276] Alguns kibutz também desenvolveram substanciais indústrias militares e de alta tecnologia. Também em 2010, o kibutz Sasa, contendo cerca de 200 membros, gerou 850 milhões de dólares em receita anual de sua indústria de plásticos militares.[277]
O Relatório Mundial de Felicidade de 2013 das Nações Unidas mostra que as nações mais felizes estão concentradas no norte da Europa, onde o modelo nórdico é empregado, com a Dinamarca no topo da lista. Isso às vezes é atribuído ao sucesso do modelo nórdico na região que foi rotulada de social-democrata em contraste com o modelo continental conservador e o modelo liberal anglo-americano. Os países nórdicos se classificaram em primeiro lugar nas métricas de PIB real per capita, expectativa de vida, liberdade para fazer escolhas, generosidade e baixa corrupção.[278]
Os objetivos do Partido Socialista Europeu (PSE), bloco socialista e social-democrata do Parlamento Europeu, passam agora a «prosseguir objetivos internacionais no respeito dos princípios em que se baseia a União Europeia, nomeadamente os princípios da liberdade, igualdade, solidariedade, democracia, respeito dos Direitos do Homem e das Liberdades Fundamentais e do respeito pelo Estado de Direito”. Como resultado, hoje o grito de guerra da Revolução Francesa - Liberté, égalité, fraternité - é promovido como valores socialistas essenciais.[279] À esquerda do PSE a nível europeu está o Partido da Esquerda Europeia (PEL), também comumente abreviado como "Esquerda Europeia"), que é um partido político a nível europeu e uma associação de socialistas democratas, socialistas[280] e partidos políticos na União Europeia e em outros países europeus. Foi formada em Janeiro de 2004 com o objetivo de concorrer nas eleições parlamentares europeias de 2004. A PEL foi fundada de 8 a 9 de maio de 2004 em Roma.[281] Os eurodeputados eleitos de partidos membros da Esquerda Europeia fazem parte do grupo Esquerda Unitária Europeia/Esquerda Nórdica Verde no Parlamento Europeu.
O partido socialista alemão A Esquerda cresceu em popularidade[282] devido à insatisfação com as políticas cada vez mais neoliberais do SPD, tornando-se o quarto maior partido no parlamento nas eleições gerais de 27 de setembro de 2009.[283] O candidato comunista Dimitris Christofias venceu um segundo turno presidencial crucial em Chipre, derrotando seu rival conservador com uma maioria de 53%.[284] Na Irlanda, nas eleições europeias de 2009, Joe Higgins, do Partido Socialista, obteve um dos três assentos no círculo eleitoral europeu da capital Dublin.
Na Dinamarca, o Partido Popular Socialista (SF) mais que dobrou sua representação parlamentar, de 11 para 23 cadeiras, tornando-se o quarto maior partido.[285] Em 2011, os social-democratas, o Partido Popular Socialista e o Partido Social Liberal Dinamarquês formaram governo, após uma ligeira vitória sobre a principal coligação política rival. Eles eram liderados por Helle Thorning-Schmidt e tinham a Aliança Vermelha e Verde como partido de apoio.
Na Noruega, a Coalizão Vermelha e Verde consiste no Partido Trabalhista (Ap), no Partido Socialista de Esquerda (SV) e no Partido de Centro (SP) e governou o país como um governo majoritário desde as eleições gerais de 2005 até 2013.
Na eleição legislativa grega de janeiro de 2015, a Coalizão da Esquerda Radical (SYRIZA) liderada por Alexis Tsipras venceu uma eleição legislativa pela primeira vez, enquanto o Partido Comunista da Grécia conquistou 15 assentos no parlamento. A SYRIZA foi caracterizado como um partido antissistema[286] cujo sucesso enviou "ondas de choque por toda a UE".[287]
No Reino Unido, o Sindicato Nacional dos Trabalhadores Ferroviários, Marítimos e de Transportes apresentou uma lista de candidatos nas eleições para o Parlamento Europeu de 2009 sob a bandeira de "Não à UE - Sim à Democracia", uma ampla coalizão altermundialista de esquerda envolvendo socialistas grupos como o Partido Socialista, com o objetivo de oferecer uma alternativa às políticas "anti-estrangeiro" e pró-negócios do Partido da Independência do Reino Unido.[288][289][290] Nas eleições gerais seguintes de maio de 2010 no Reino Unido, a Coalizão Sindical e Socialista, lançada em janeiro de 2010[291] e apoiada por Bob Crow, o líder do Sindicato Nacional dos Trabalhadores em Transporte Ferroviário, Marítimo e Transporte (RMT), outros sindicatos e o Partido Socialista, entre outros grupos socialistas, se posicionaram contra o Trabalhismo em 40 círculos eleitorais.[292][293] A Coalizão Sindical e Socialista contestou as eleições locais de 2011, tendo obtido o endosso da conferência do RMT de junho de 2010, mas não obteve assentos.[294] O partido Unidade de Esquerda também foi fundado em 2013, depois que o cineasta Ken Loach apelou a um novo partido de esquerda para substituir o Partido Trabalhista, que ele alegou não ter se oposto à austeridade e se encaminhado para o neoliberalismo.[295][296][297][298] Em 2015, após uma derrota nas eleições gerais britânicas de 2015, o autodenominado socialista Jeremy Corbyn substituiu Ed Miliband como líder do Partido Trabalhista.[299]
Na França, Olivier Besancenot, o candidato da Liga Comunista Revolucionária (LCR) nas eleições presidenciais de 2007, recebeu 1.498.581 votos, ou 4,08% do eleitorado, o dobro do candidato comunista.[300] A LCR se aboliu em 2009 para iniciar um amplo partido anticapitalista, o Novo Partido Anticapitalista, cujo objetivo declarado é "construir uma nova perspectiva socialista e democrática para o século XXI".[301]
Em 25 de maio de 2014, o partido de esquerda espanhol Podemos inscreveu candidatos para as eleições parlamentares europeias de 2014, alguns dos quais estavam desempregados. Em um resultado surpreendente, obteve 7,98% dos votos e, portanto, recebeu cinco cadeiras em 54[302][303] enquanto a mais velha Esquerda Unida foi a terceira maior força geral, obtendo 10,03% e 5 cadeiras, 4 a mais que a anterior eleições.[304]
O governo de Portugal instituído a 26 de novembro de 2015 foi um governo minoritário do Partido Socialista (PS) liderado pelo primeiro-ministro António Costa, que conseguiu garantir o apoio a um governo minoritário socialista pelo Bloco de Esquerda (BE), o Partido Comunista Português (PCP) e o Partido Ecologista "Os Verdes" (PEV).[305]
Por toda a Europa e em alguns lugares da América Latina existe um centro social e um movimento de ocupação inspirado principalmente nas ideias autonomistas e anarquistas.[306][307]
De acordo com um artigo de 2013 no The Guardian, "contrariamente à crença popular, os estadunidenses não têm uma alergia inata ao socialismo. Milwaukee teve vários prefeitos socialistas (Frank Zeidler, Emil Seidel e Daniel Hoan) e atualmente há um socialista independente no Senado dos Estados Unidos, Bernie Sanders de Vermont".[308] Sanders, que já foi prefeito da maior cidade de Vermont, Burlington, se descreveu como um socialista democrático[309][310] e elogiou a social-democracia de estilo escandinavo.[311][312] Em 2016, Sanders concorreu à presidência como candidato do Partido Democrata, ganhando assim considerável apoio popular, principalmente entre a geração mais jovem, mas perdeu a indicação para Hillary Clinton. Em 2019, os Socialistas Democratas da América tinham dois membros no Congresso e vários membros em legislaturas estaduais e conselhos municipais.[313] De acordo com uma pesquisa da Gallup feita em 2018, 37% dos estadunidenses adultos têm uma visão positiva do socialismo, incluindo 57% dos eleitores com tendência democrata e 16% dos eleitores com tendência republicana.[314] Uma pesquisa YouGov de 2019 descobriu que 7 em cada 10 millennials votariam em um candidato presidencial socialista e 36% tinham uma visão favorável do comunismo.[315] Uma pesquisa anterior da Harris feita em 2019 descobriu que o socialismo é mais popular entre as mulheres do que os homens, com 55% das mulheres entre 18 e 54 anos preferindo viver em uma sociedade socialista, enquanto a maioria dos homens pesquisados escolheu o capitalismo ao invés do socialismo.[316]
O anticapitalismo, o anarquismo e o movimento antiglobalização ganharam destaque por meio de eventos como os protestos contra a Conferência Ministerial da Organização Mundial do Comércio de 1999 em Seattle. Grupos de inspiração socialista desempenharam um papel importante nesses movimentos, que, no entanto, abrangiam camadas muito mais amplas da população e eram defendidos por figuras como Noam Chomsky. No Canadá, a Co-operative Commonwealth Federation (CCF), a precursora do social-democrata Novo Partido Democrático (NDP), teve um sucesso significativo na política provincial. Em 1944, a CCF de Saskatchewan formou o primeiro governo socialista na América do Norte. No nível federal, o NDP foi a oposição oficial, de 2011 a 2015.[317]
Em sua coluna linguística Johnson, a The Economist opina que nos Estados Unidos do século XXI, o termo socialismo, sem definição clara, tornou-se pejorativo e usado pelos conservadores para atacar políticas, propostas e figuras públicas liberais e progressistas.[318]
Para a Enciclopédia Britânica, “a tentativa de Salvador Allende de unir marxistas e outros reformadores em uma reconstrução socialista do Chile é mais representativa da direção que os socialistas latino-americanos tomaram desde o final do século XX. [...] Vários líderes socialistas (ou de tendência socialista) seguiram o exemplo de Allende ao ganhar a eleição para cargos públicos em países latino-americanos".[96] O sucesso do Partido dos Trabalhadores no Brasil, formado em 1980 e que governou o país de 2003 a 2016, foi o primeiro grande avanço para essa tendência.
O Foro de São Paulo é uma conferência de partidos políticos de esquerda e outras organizações da América Latina e do Caribe. Foi lançado pelo Partido dos Trabalhadores em 1990 na cidade de São Paulo, após o PT se aproximar de outros partidos e movimentos sociais da América Latina e do Caribe com o objetivo de debater o novo cenário internacional após a queda do Muro de Berlim e das consequências da implementação do que foram tidas como políticas neoliberais adotadas na época por governos contemporâneos de direita na região, sendo o objetivo principal declarado da conferência argumentar por alternativas ao neoliberalismo.[319] Entre seus membros estavam partidos socialistas e social-democratas no governo da região, como o Movimento ao Socialismo da  Bolívia, o Partido Comunista de Cuba, a Aliança PAIS do Equador, o Partido Socialista Unido da Venezuela, o Partido Socialista do Chile, a Frente Ampla do Uruguai, a Frente Sandinista de Libertação Nacional da Nicarágua, a Frente Farabundo Martí de Libertação Nacional em El Salvador e membros da Frente de Todos da Argentina.
Na primeira década do século XXI, o presidente venezuelano Hugo Chávez, o presidente nicaraguense Daniel Ortega, o presidente boliviano Evo Morales e o presidente equatoriano Rafael Correa se referiram a seus programas políticos como socialistas, e Chávez adotou o termo "socialismo do século XXI". Depois de vencer a reeleição em dezembro de 2006, Chávez disse: "Agora, mais do que nunca, sou obrigado a mudar o caminho da Venezuela para o socialismo".[320] Chávez também foi reeleito em outubro de 2012 para seu terceiro mandato de seis anos como presidente, mas morreu em março de 2013 de câncer. Após a morte de Chávez em 5 de março de 2013, o vice-presidente do partido de Chávez, Nicolás Maduro, assumiu os poderes e responsabilidades do presidente. Uma eleição especial foi realizada em 14 de abril do mesmo ano para eleger um novo presidente, que Maduro venceu por uma margem apertada como o candidato do Partido Socialista Unido da Venezuela e foi formalmente empossado em 19 de abril.[321] Maré rosa é um termo usado em análise política, na mídia e em outros lugares para descrever a percepção de que a ideologia de esquerda em geral e a política de esquerda em particular foram cada vez mais influentes na América Latina nos anos 2000.[322][323][324] Alguns dos governos da maré rosa foram criticados por passar do socialismo ao populismo e ao autoritarismo.[325][326] A maré rosa foi seguida na década de 2010 por uma "onda conservadora" quando governos de direita chegaram ao poder na Argentina, Brasil e Chile, enquanto Venezuela e Nicarágua passaram por crises políticas. No entanto, o socialismo viu um ressurgimento em 2018-19, após sucessivas vitórias eleitorais de candidatos de esquerda e centro-esquerda no México, Panamá e Argentina.[327][328][329]
A Austrália viu um aumento no interesse pelo socialismo no início do século XXI, especialmente entre os jovens.[330] É mais forte em Victoria, onde três partidos socialistas se fundiram nos Socialistas Vitorianos, que visam resolver os problemas de habitação e transporte público.
Na Nova Zelândia, o socialismo emergiu dentro do movimento sindical emergente durante o final do século XIX e início do século XX. Em julho de 1916, várias organizações políticas de esquerda e sindicatos se fundiram para formar o Partido Trabalhista da Nova Zelândia.[331][332] Enquanto o Trabalhismo tinha tradicionalmente uma orientação socialista, o partido mudou para uma orientação mais social-democrata durante as décadas de 1920 e 1930. Após as eleições gerais de 1935, o Primeiro Governo Trabalhista perseguiu políticas socialistas como a nacionalização da indústria, radiodifusão, transporte e implementação de um estado de bem-estar social keynesiano. No entanto, o partido não buscou a abolição do capitalismo, ao invés disso, optou por uma economia mista. O estado de bem-estar e a economia mista do trabalhismo não foram desafiados até a década de 1980.[333][334] Durante a década de 1980, o Quarto Governo Trabalhista implementou uma série de reformas econômicas neoliberais conhecidas como Rogernomics, que viram a sociedade da Nova Zelândia e a economia mudarem para um modelo de mercado mais livre. O abandono do Partido Trabalhista de seus valores tradicionais fragmentou o partido. Sucessivos governos trabalhistas desde então perseguiram políticas sociais e econômicas de centro-esquerda, enquanto mantinham uma economia de mercado livre.[335] A atual primeira-ministra da Nova Zelândia, Jacinda Ardern, atuou anteriormente como presidente da União Internacional da Juventude Socialista.[336] Ardern é um social-democrata[337] que criticou o capitalismo como um "fracasso flagrante" devido aos altos níveis de falta de moradia e baixos salários.[338] A Nova Zelândia ainda tem uma pequena cena socialista, dominada principalmente por grupos trotskistas.
O socialismo melanésio se desenvolveu na década de 1980, inspirado no socialismo africano. Seu objetivo é alcançar a independência total do Reino Unido e da França nos territórios da Melanésia e a criação de uma união federal na região. É muito popular com o movimento de independência da Nova Caledônia.
A Aliança Progressiva é uma internacional política fundada em 22 de maio de 2013 por partidos políticos, a maioria dos quais são atuais ou ex-membros da Internacional Socialista. A organização afirma o objetivo de se tornar a rede global do "movimento progressista, democrático, social-democrata, socialista e operário".[339][340]
O pensamento socialista inicial teve influências de uma ampla gama de filosofias, como republicanismo cívico, racionalismo iluminista, romantismo, formas de materialismo, cristianismo (católico e protestante), lei natural e teoria dos direitos naturais, utilitarismo e economia política liberal.[341] Outra base filosófica para muito do socialismo inicial foi o surgimento do positivismo durante o Iluminismo europeu. O positivismo afirmava que tanto o mundo natural quanto o social podiam ser compreendidos por meio do conhecimento científico e analisados por meio de métodos científicos. Essa perspectiva central influenciou os primeiros cientistas sociais e diferentes tipos de socialistas, desde anarquistas como Peter Kropotkin a tecnocratas como Saint Simon.[342]
O objetivo fundamental do socialismo é atingir um nível avançado de produção material e, portanto, maior produtividade, eficiência e racionalidade em comparação com o capitalismo e todos os sistemas anteriores, sob a perspectiva de que uma expansão da capacidade produtiva humana é a base para a extensão da liberdade e igualdade na sociedade.[343] Muitas formas de teoria socialista sustentam que o comportamento humano é amplamente moldado pelo ambiente social. Em particular, o socialismo afirma que costumes sociais, valores, traços culturais e práticas econômicas são criações sociais e não o resultado de uma lei natural imutável.[344][345] O objeto de sua crítica não é, portanto, a avareza humana ou a consciência humana, mas as condições materiais e os sistemas sociais feitos pelo homem (isto é, a estrutura econômica da sociedade) que dá origem aos problemas sociais observados e às ineficiências. Bertrand Russell, frequentemente considerado o pai da filosofia analítica, é identificado como socialista. Russell se opôs aos aspectos da luta de classes do marxismo, vendo o socialismo apenas como um ajuste das relações econômicas para acomodar a produção da máquina moderna para beneficiar toda a humanidade por meio da redução progressiva do tempo de trabalho necessário.[346]
Os socialistas veem a criatividade como um aspecto essencial da natureza humana e definem a liberdade como um estado de ser onde os indivíduos são capazes de expressar sua criatividade sem ser impedidos por restrições de escassez material e instituições sociais coercitivas.[347] O conceito socialista de individualidade está entrelaçado com o conceito de expressão criativa individual. Karl Marx acreditava que a expansão das forças produtivas e da tecnologia era a base para a expansão da liberdade humana e que o socialismo, sendo um sistema consistente com os desenvolvimentos modernos da tecnologia, permitiria o florescimento de "individualidades livres" por meio da redução progressiva de tempo de trabalho necessário. A redução ao mínimo do tempo de trabalho necessário proporcionaria aos indivíduos a oportunidade de buscar o desenvolvimento de sua verdadeira individualidade e criatividade.[348]
Os socialistas argumentam que a acumulação de capital gera desperdício por meio de externalidades que exigem medidas regulatórias corretivas caras. Eles também apontam que esse processo gera indústrias e práticas desperdiçadoras que existem apenas para gerar demanda suficiente para que produtos, como propaganda de alta pressão, sejam vendidos com lucro, criando assim, em vez de satisfação, demanda econômica.[349][350]
Os socialistas argumentam que o capitalismo consiste em atividade irracional, como a compra de mercadorias apenas para vender mais tarde, quando seu preço se valorizar, ao invés de para o consumo, mesmo que a mercadoria não possa ser vendida com lucro para indivíduos em necessidade e, portanto, um fator crucial da crítica frequentemente feita pelos socialistas é que "ganhar dinheiro", ou acumular de capital, não corresponde à satisfação da demanda (a produção de valores de uso).[351] O critério fundamental para a atividade econômica no capitalismo é a acumulação de capital para reinvestimento na produção, mas isso estimula o desenvolvimento de novas indústrias não produtivas que não produzem valor de uso e existem apenas para manter o processo de acumulação à tona (caso contrário, o sistema entra em crise), como a disseminação do setor financeiro, contribuindo para a formação de bolhas econômicas.[352]
Os socialistas veem as relações de propriedade privada como limitantes do potencial das forças produtivas na economia. De acordo com os socialistas, a propriedade privada se torna obsoleta quando se concentra em instituições centralizadas e socializadas baseadas na apropriação privada da receita - mas baseadas no trabalho cooperativo e no planejamento interno na alocação de insumos - até que o papel do capitalista se torne redundante.[353] Sem a necessidade de acumulação de capital e de uma classe de proprietários, a propriedade privada dos meios de produção é percebida como uma forma ultrapassada de organização econômica que deve ser substituída por uma associação livre de indivíduos baseada na propriedade comum desses bens socializados.[354][355] A propriedade privada impõe restrições ao planejamento, levando a decisões econômicas descoordenadas que resultam em flutuações de negócios, desemprego e um enorme desperdício de recursos materiais durante a crise de superprodução.[356]
As disparidades excessivas na distribuição de renda levam à instabilidade social e exigem medidas corretivas onerosas na forma de tributação redistributiva, que incorre em pesados custos administrativos, ao mesmo tempo que enfraquece o incentivo ao trabalho, convida à desonestidade e aumenta a probabilidade de evasão fiscal, enquanto (as medidas corretivas) reduzem o eficiência global da economia de mercado.[357] Essas políticas corretivas limitam o sistema de incentivos do mercado, fornecendo coisas como salários mínimos, seguro-desemprego, tributação dos lucros e redução do exército de reserva de trabalho, resultando em incentivos reduzidos para os capitalistas investirem em mais produção. Em essência, as políticas de bem-estar social paralisam o capitalismo e seu sistema de incentivos e, portanto, são insustentáveis no longo prazo.[358] Os marxistas argumentam que o estabelecimento de um modo de produção socialista é a única maneira de superar essas deficiências. Os socialistas, e especificamente os socialistas marxistas, argumentam que o conflito de interesses inerente entre a classe trabalhadora e o capital impede o uso ideal dos recursos humanos disponíveis e leva a grupos de interesse contraditórios (trabalho e negócios) que se esforçam para influenciar o Estado a intervir na economia em seu favor em à custa da eficiência econômica geral.
Os primeiros socialistas (socialistas utópicos e ricardianos) criticaram o capitalismo por concentrar poder e riqueza em um pequeno segmento da sociedade.[359] Além disso, reclamaram que o capitalismo não usa a tecnologia e os recursos disponíveis em seu potencial máximo no interesse do público.[355]
Em certo estágio de desenvolvimento, as forças produtivas materiais da sociedade entram em conflito com as relações de produção existentes ou - isso apenas expressa a mesma coisa em termos jurídicos - com as relações de propriedade no âmbito das quais operaram até agora. Então começa uma era de revolução social. As mudanças na base econômica levam, mais cedo ou mais tarde, à transformação de toda a imensa superestrutura.[360]
Karl Marx e Friedrich Engels argumentaram que o socialismo emergiria da necessidade histórica à medida que o capitalismo se tornasse obsoleto e insustentável por causa das crescentes contradições internas decorrentes do desenvolvimento das forças produtivas e da tecnologia. Foram esses avanços nas forças produtivas combinados com as velhas relações sociais de produção do capitalismo que geraram contradições, levando à consciência da classe trabalhadora.[361]
Marx e Engels defendiam que a consciência daqueles que ganham um salário (a classe trabalhadora no sentido marxista mais amplo) seria moldada por suas condições de escravidão assalariada, levando a uma tendência de buscar sua liberdade ou emancipação pela derrubada da propriedade dos meios de produção pelos capitalistas e, consequentemente, derrubando o Estado que sustentava essa ordem econômica. Para Marx e Engels, as condições determinam a consciência e o fim do papel da classe capitalista leva eventualmente a uma sociedade sem classes na qual o Estado definharia. A concepção marxista do socialismo é a de uma fase histórica específica que deslocaria o capitalismo e precederia o comunismo.
As principais características do socialismo (particularmente conforme concebido por Marx e Engels após a Comuna de Paris de 1871) são que o proletariado controlaria os meios de produção por meio de um Estado operário erigido pelos trabalhadores em seus interesses. A atividade econômica ainda seria organizada por meio do uso de sistemas de incentivos e as classes sociais ainda existiriam, mas em um grau menor e decrescente do que sob o capitalismo.
Para os marxistas ortodoxos, o socialismo é o estágio inferior do comunismo baseado no princípio "de cada um de acordo com sua capacidade, a cada um segundo sua contribuição", enquanto o comunismo de estágio superior é baseado no princípio "de cada qual, segundo sua capacidade; a cada qual, segundo suas necessidades", o estágio superior se tornaria possível somente após o estágio socialista desenvolver ainda mais a eficiência econômica e a automação da produção levar a uma superabundância de bens e serviços.[362][363]
Marx argumentou que as forças produtivas materiais (na indústria e no comércio) trazidas à existência pelo capitalismo predicavam uma sociedade cooperativa, uma vez que a produção havia se tornado uma atividade social coletiva de massa da classe trabalhadora para criar mercadorias, mas com propriedade privada (as relações de produção ou propriedade relações). Este conflito entre o esforço coletivo em grandes fábricas e a propriedade privada traria um desejo consciente na classe trabalhadora de estabelecer uma propriedade coletiva proporcional aos esforços coletivos de sua experiência diária.[360]
Os socialistas assumiram diferentes perspectivas sobre o Estado e o papel que ele deve desempenhar nas lutas revolucionárias, na construção do socialismo e dentro de uma economia socialista estabelecida.
No século XIX, a filosofia do socialismo estatal foi explicitamente exposta pela primeira vez pelo filósofo político alemão Ferdinand Lassalle. Em contraste com a perspectiva de Estado de Karl Marx, Lassalle rejeitou o conceito de Estado como uma estrutura de poder baseada em classes, cuja principal função era preservar as estruturas de classes existente. Lassalle também rejeitou a visão marxista de que o Estado estava destinado a "definhar". Lassalle considerava o Estado uma entidade independente das lealdades de classe e um instrumento de justiça que seria, portanto, essencial para alcançar o socialismo.[364]
Antes da revolução liderada pelos bolcheviques na Rússia, muitos socialistas, incluindo reformistas, correntes marxistas ortodoxas, como comunismo de conselhos, anarquistas e socialistas libertários, criticaram a ideia de usar o Estado para conduzir o planejamento central e possuir os meios de produção como forma de estabelecer o socialismo. Após a vitória do leninismo na Rússia, a ideia de "socialismo estatal" espalhou-se rapidamente por todo o movimento socialista e, eventualmente, o socialismo estatal passou a ser identificado com o modelo econômico soviético.[365]
Joseph Schumpeter rejeitou a associação do socialismo e da propriedade social com a propriedade estatal dos meios de produção porque o Estado, tal como existe em sua forma atual, é um produto da sociedade capitalista e não pode ser transplantado para uma estrutura institucional diferente. Schumpeter argumentou que haveria instituições diferentes dentro do socialismo do que aquelas que existem dentro do capitalismo moderno, assim como o feudalismo tinha suas próprias formas institucionais distintas e únicas. O Estado, junto com conceitos como propriedade e tributação, eram conceitos exclusivos da sociedade comercial (capitalismo) e tentar colocá-los no contexto de uma futura sociedade socialista equivaleria a uma distorção desses conceitos, usando-os fora do contexto.[366]
Socialismo utópico é um termo usado para definir as primeiras correntes do pensamento socialista moderno, exemplificado pela obra de Henri de Saint-Simon, Charles Fourier e Robert Owen, que inspirou Karl Marx e outros socialistas antigos.[367] No entanto, as visões de sociedades ideais imaginárias, que competiam com os movimentos social-democratas revolucionários, eram vistas como não sendo baseadas nas condições materiais da sociedade e como reacionárias.[368] Embora seja tecnicamente possível que qualquer conjunto de ideias ou qualquer pessoa que viva em qualquer momento da história seja um socialista utópico, o termo é mais frequentemente aplicado aos socialistas que viveram no primeiro quarto do século XIX aos quais foi atribuído o rótulo "utópico" pelos socialistas posteriores como um termo negativo, a fim de implicar ingenuidade e descartar suas ideias como fantasiosas ou irrealistas.[100]
Seitas religiosas cujos membros vivem em comunidade, como os anabatistas, não são geralmente chamadas de "socialistas utópicos", embora seu modo de vida seja um excelente exemplo. Eles foram classificados como socialistas religiosos por alguns. Da mesma forma, as comunidades intencionais modernas baseadas em ideias socialistas também podem ser categorizadas como "socialistas utópicos". Para os marxistas, o desenvolvimento do capitalismo na Europa Ocidental forneceu uma base material para a possibilidade de concretizar o socialismo porque, de acordo com o Manifesto Comunista, "o que a burguesia produz, acima de tudo, são os seus próprios coveiros",[369] a saber, a classe trabalhadora, que deve tomar consciência dos objetivos históricos que a sociedade lhe fixou.
Os socialistas revolucionários acreditam que uma revolução social é necessária para efetuar mudanças estruturais na estrutura socioeconômica da sociedade. Entre os socialistas revolucionários, existem diferenças de estratégia, teoria e definição de revolução . Os marxistas ortodoxos e os comunistas de esquerda assumem uma postura impossibilista, acreditando que a revolução deve ser espontânea como resultado das contradições da sociedade devido às mudanças tecnológicas nas forças produtivas. Lenin teorizou que sob o capitalismo os trabalhadores não podem alcançar a consciência de classe além de se organizar em sindicatos e fazer demandas aos capitalistas. Portanto, os leninistas defendem que é historicamente necessário para uma vanguarda de revolucionários com consciência de classe assumir um papel central na coordenação da revolução social para derrubar o estado capitalista e, eventualmente, a instituição do estado por completo.[370] A revolução não é necessariamente definida pelos socialistas revolucionários como uma insurreição violenta,[371] mas como um desmantelamento completo e uma transformação rápida de todas as áreas da sociedade de classes lideradas pela maioria das massas: a classe trabalhadora.
O reformismo é geralmente associado à social-democracia e ao socialismo democrático gradual. O reformismo é a crença de que os socialistas devem concorrer às eleições parlamentares dentro da sociedade capitalista e, se eleitos, usar a máquina do governo para aprovar reformas políticas e sociais com o objetivo de amenizar as instabilidades e desigualdades do capitalismo. Dentro do socialismo, o reformismo é usado de duas maneiras diferentes. Ele não tem a intenção de trazer o socialismo ou mudanças econômicas fundamentais para a sociedade e é usado para se opor a tais mudanças estruturais. A outra é baseada na suposição de que, embora as reformas não sejam socialistas em si mesmas, elas podem ajudar a reunir apoiadores para a causa da revolução, popularizando a causa do socialismo para a classe trabalhadora.[372]
O debate sobre a capacidade do reformismo social-democrata de levar a uma transformação socialista da sociedade tem mais de um século. O reformismo é criticado por ser paradoxal, pois busca superar o sistema econômico existente do capitalismo enquanto tenta melhorar as condições do capitalismo, fazendo-o parecer mais tolerável para a sociedade. Segundo Rosa Luxemburgo, o capitalismo não é derrubado, “pelo contrário, fortalece-se com o desenvolvimento das reformas sociais”.[373] Na mesma linha, Stan Parker, do Partido Socialista da Grã-Bretanha, argumenta que as reformas são um desvio de energia para os socialistas e são limitadas porque devem aderir à lógica do capitalismo.[372] O teórico social francês Andre Gorz criticou o reformismo ao defender uma terceira alternativa ao reformismo e à revolução social que ele chamou de "reformas não reformistas", especificamente focadas em mudanças estruturais no capitalismo em oposição a reformas para melhorar as condições de vida dentro do capitalismo ou para sustentá-lo através intervenções econômicas.[374]
A anarquia econômica da sociedade capitalista como existe hoje é, em minha opinião, a verdadeira fonte do mal. [...] Estou convencido de que só há uma maneira de eliminar esses graves males, a saber, através do estabelecimento de uma economia socialista, acompanhada por um sistema educacional que seria orientado para fins sociais. Em tal economia, os meios de produção são propriedade da própria sociedade e são utilizados de forma planejada. Uma economia planejada, que ajusta a produção às necessidades da comunidade, distribuiria o trabalho a ser feito entre todos os que podem trabalhar e garantiria o sustento de cada homem, mulher e criança. A educação do indivíduo, além de promover suas próprias habilidades inatas, tentaria desenvolver nele um senso de responsabilidade por seus semelhantes, em lugar da glorificação do poder e do sucesso em nossa sociedade atual.[375]
A economia socialista parte da premissa de que "os indivíduos não vivem ou trabalham isolados, mas vivem em cooperação uns com os outros. Além disso, tudo o que as pessoas produzem é, em certo sentido, um produto social e todos os que contribuem para a produção de um bem têm direito a uma parte dele. A sociedade como um todo, portanto, deve possuir ou pelo menos controlar a propriedade para o benefício de todos os seus membros”.[109]
A concepção original do socialismo era a de um sistema econômico pelo qual a produção era organizada de forma a produzir diretamente bens e serviços para sua utilidade (ou valor de uso na economia clássica e marxista), com a alocação direta de recursos em termos de unidades físicas em oposição ao cálculo financeiro e às leis econômicas do capitalismo (ver lei do valor), muitas vezes acarretando o fim das categorias econômicas capitalistas, como aluguel, juros, lucro e dinheiro.[16] Em uma economia socialista totalmente desenvolvida, a produção e o equilíbrio entre as entradas e as saídas dos fatores torna-se um processo técnico a ser realizado por engenheiros.[376]
O socialismo de mercado se refere a uma série de diferentes teorias e sistemas econômicos que usam o mecanismo de mercado para organizar a produção e alocar insumos de fatores entre empresas de propriedade social, com o excedente econômico (lucros) acumulando para a sociedade em um dividendo social em oposição aos proprietários de capital privado.[377] Variações do socialismo de mercado incluem propostas libertárias, como o mutualismo, baseado na economia clássica, e modelos econômicos neoclássicos, como o Modelo Lange. No entanto, alguns economistas como Joseph Stiglitz, Mancur Olson e outros que não defendem posições antissocialistas especificamente mostraram que os modelos econômicos predominantes nos quais tais modelos democráticos ou de socialismo de mercado podem se basear têm falhas lógicas ou pressuposições impraticáveis.[378][379]
A propriedade dos meios de produção pode ser baseada na propriedade direta pelos usuários da propriedade produtiva por meio de cooperativas de trabalhadores; ou de propriedade comum de toda a sociedade, com gestão e controle delegados àqueles que operam/utilizam os meios de produção; ou propriedade pública por um aparato estatal, o que pode referir-se à criação de empresas estatais, nacionalização, municipalização ou instituições coletivas autônomas. Alguns socialistas acham que, em uma economia socialista, pelo menos os "altos comandos" da economia devem ser propriedade pública.[380] No entanto, os liberais econômicos e os libertários de direita veem a propriedade privada dos meios de produção e a troca de mercado como entidades naturais ou direitos morais que são centrais para suas concepções de liberdade e veem a dinâmica econômica do capitalismo como imutável e absoluta, portanto percebem a propriedade pública dos meios de produção, as cooperativas e o planejamento econômico como infrações à liberdade.[381][382]
A gestão e o controle das atividades das empresas baseiam-se na autogestão e na autogovernança, com igualdade de relações de poder no local de trabalho para maximizar a autonomia ocupacional. Uma forma socialista de organização eliminaria as hierarquias de controle, de forma que apenas uma hierarquia baseada no conhecimento técnico no local de trabalho permanecesse. Cada membro teria poder de tomada de decisão na empresa e seria capaz de participar do estabelecimento de seus objetivos gerais de política. As políticas/metas seriam executadas pelos especialistas técnicos que compõem a hierarquia coordenadora da empresa, que estabeleceriam planos ou diretrizes para que a comunidade de trabalho cumprisse essas metas.[383]
O papel e o uso do dinheiro em uma hipotética economia socialista é uma questão contestada. De acordo com o economista da escola austríaca Ludwig von Mises, um sistema econômico que não usa dinheiro, cálculo financeiro e preços de mercado seria incapaz de valorizar bens de capital e coordenar a produção de forma eficaz e, portanto, esses tipos de socialismo são impossíveis porque carecem das informações necessárias para realizar cálculos econômicos em primeiro lugar.[384] Socialistas, incluindo Karl Marx, Robert Owen, Pierre-Joseph Proudhon e John Stuart Mill defenderam várias formas de vale-trabalho que, como o dinheiro, seriam usados para adquirir artigos de consumo, mas ao contrário do dinheiro, seriam incapazes de se tornarem capital e não seriam usados para alocar recursos dentro do processo de produção. O revolucionário bolchevique Leon Trotsky argumentou que o dinheiro não poderia ser abolido arbitrariamente após uma revolução socialista. Segundo ele, o dinheiro teria que esgotar sua “missão histórica”, ou seja, teria que ser usado até que sua função se tornasse redundante, eventualmente sendo transformado em recibos de contabilidade para estatísticos e, somente em um futuro mais distante, o dinheiro não seria necessário nem mesmo para essa função.[385]
Uma economia planejada é um tipo de economia que consiste em uma mistura de propriedade pública dos meios de produção e a coordenação da produção e distribuição por meio do planejamento econômico. Uma economia planejada pode ser descentralizada ou centralizada. Enrico Barone forneceu uma estrutura teórica abrangente para uma economia socialista planejada. Em seu modelo, assumindo técnicas de computação perfeitas, equações simultâneas relacionando entradas e saídas a razões de equivalência forneceriam avaliações apropriadas para equilibrar oferta e demanda.[386]
O exemplo mais proeminente de uma economia planejada foi o sistema econômico soviético e, como tal, o modelo econômico de planejamento centralizado é geralmente associado aos Estados comunistas do século XX, onde foi combinado com um sistema político de partido único. Em uma economia planejada centralmente, as decisões relativas à quantidade de bens e serviços a serem produzidos são planejadas com antecedência por uma agência de planejamento. Os sistemas econômicos da União Soviética e do Bloco de Leste são ainda classificados como "economias de comando", que são definidas como sistemas onde a coordenação econômica é realizada por comandos, diretivas e metas de produção.[387] Estudos feitos por economistas de várias convicções políticas sobre o funcionamento real da economia soviética indicam que ela não era realmente uma economia planejada. Em vez de um planejamento consciente, a economia soviética baseou-se em um processo pelo qual o plano foi modificado por agentes localizados e os planos originais não eram cumpridos. Agências de planejamento, ministérios e empresas se adaptavam e negociavam uns com os outros durante a formulação do plano, em oposição a seguir um plano transmitido por uma autoridade superior, levando alguns economistas a sugerir que o planejamento não ocorreu realmente dentro da economia soviética e que uma descrição melhor seria uma economia "administrada" ou "gerenciada".[388]
Embora o planejamento central fosse amplamente apoiado por marxistas-leninistas, algumas facções dentro da União Soviética antes da ascensão do stalinismo mantinham posições contrárias ao planejamento central. Leon Trotsky rejeitou o planejamento central em favor do planejamento descentralizado. Ele argumentou que os planejadores centrais, independentemente de sua capacidade intelectual, seriam incapazes de coordenar efetivamente todas as atividades econômicas dentro de uma economia porque operavam sem a contribuição e o conhecimento tácito incorporado pela participação de milhões de pessoas na economia. Como resultado, os planejadores centrais seriam incapazes de responder às condições econômicas locais.[389] O socialismo estatal é inviável nessa visão porque as informações não podem ser agregadas por um órgão central e efetivamente usadas para formular um plano para toda a economia, porque isso resultaria em sinais de preços distorcidos ou ausentes.[390]
O socialismo, você vê, é um pássaro com duas asas. A definição é 'propriedade social e controle democrático dos instrumentos e meios de produção'.[391]
Uma economia autogerida e descentralizada é baseada em unidades econômicas autônomas e autorreguladas e em um mecanismo descentralizado de alocação de recursos e tomada de decisões. Este modelo encontrou apoio em notáveis economistas clássicos e neoclássicos, incluindo Alfred Marshall, John Stuart Mill e Jaroslav Vanek. Existem inúmeras variações de autogestão, incluindo firmas gerenciadas por mão de obra e firmas gerenciadas por trabalhadores. Os objetivos da autogestão são eliminar a exploração e reduzir a alienação.[392] O socialismo de guildas é um movimento político que defende o controle operário da indústria por meio de guildas relacionadas ao comércio "em uma relação contratual implícita com o público".[393] Originou-se no Reino Unido e teve sua maior influência no primeiro quarto do século XX. Foi fortemente associado a G. D. H. Cole e influenciado pelas ideias de William Morris.
Um desses sistemas é a economia cooperativa, uma economia de mercado amplamente livre na qual os trabalhadores administram as empresas e determinam democraticamente os níveis de remuneração e as divisões de trabalho. Os recursos produtivos seriam legalmente propriedade da cooperativa e alugados aos trabalhadores, que teriam direito de usufruto.[394] Outra forma de planejamento descentralizado é o uso da cibernética, ou o uso de computadores para gerenciar a alocação de insumos econômicos. O governo socialista de Salvador Allende, no Chile, experimentou o Projeto Cybersyn, uma ponte de informações em tempo real entre o governo, empresas estatais e consumidores.[395] Outra variante mais recente é a economia participativa, em que a economia é planejada por conselhos descentralizados de trabalhadores e consumidores. Os trabalhadores seriam remunerados exclusivamente de acordo com o esforço e o sacrifício, de modo que aqueles que se ocupam de trabalhos perigosos, incômodos e extenuantes recebessem os rendimentos mais elevados e, assim, pudessem trabalhar menos.[396] Um modelo contemporâneo para um socialismo autogerido e sem mercado é o modelo de coordenação negociada de Pat Devine. A coordenação negociada é baseada na propriedade social daqueles afetados pelo uso dos ativos envolvidos, com decisões tomadas por aqueles no nível de produção mais localizado.[397]
Michel Bauwens identifica o surgimento do movimento do software aberto e da produção peer-to-peer como um novo modo alternativo de produção para a economia capitalista e planejada centralmente que se baseia na autogestão colaborativa, propriedade comum de recursos e produção de valor de uso através da livre cooperação dos produtores que têm acesso ao capital distribuído.[398]
O anarcocomunismo é uma teoria do anarquismo que defende a abolição do Estado, da propriedade privada e do capitalismo em favor da propriedade comum dos meios de produção.[399][400] O anarcossindicalismo foi praticado na Catalunha e em outros lugares da Revolução Espanhola durante a Guerra Civil Espanhola. Sam Dolgoff estimou que cerca de oito milhões de pessoas participaram direta ou pelo menos indiretamente da Revolução Espanhola.[401]
A economia da antiga República Federal Socialista da Iugoslávia estabeleceu um sistema baseado na alocação baseada no mercado, na propriedade social dos meios de produção e na autogestão dentro das empresas. Este sistema substituiu o planejamento central de tipo soviético por um sistema descentralizado e autogerenciado após as reformas de 1953.[402]
O economista marxista Richard D. Wolff argumenta que "reorganizar a produção para que os trabalhadores se tornem coletivamente autodirigidos em seus locais de trabalho" não apenas move a sociedade para além do capitalismo e do socialismo estatal do século passado, mas também marcaria outro marco na história humana, semelhante a transições anteriores da escravidão e do feudalismo. Como exemplo, Wolff afirma que a Mondragon é "uma alternativa de sucesso impressionante à organização capitalista da produção".[403]
O socialismo estatal pode ser usado para classificar qualquer variedade de filosofias socialistas que defendem a propriedade dos meios de produção pelo aparelho de estatal, seja como um estágio de transição entre o capitalismo e o socialismo, ou como um objetivo final em si mesmo. Normalmente, refere-se a uma forma de gestão tecnocrática, em que especialistas técnicos administram ou gerenciam empresas econômicas em nome da sociedade e do interesse público, em vez de conselhos de trabalhadores ou da democracia no local de trabalho.
Uma economia dirigida pelo Estado pode se referir a um tipo de economia mista que consiste na propriedade pública sobre grandes indústrias, conforme promovido por vários partidos políticos social-democratas durante o século XX. Essa ideologia influenciou as políticas do Partido Trabalhista britânico durante a administração de Clement Attlee. Na biografia do primeiro-ministro do Partido Trabalhista do Reino Unido de 1945, Clement Attlee, Francis Beckett afirma: "O governo [...] queria o que se tornaria conhecido como economia mista".[404]
A nacionalização no Reino Unido foi alcançada por meio da compra compulsória da indústria (ou seja, com compensação). A British Aerospace foi uma combinação das principais empresas aeronáuticas British Aircraft Corporation, Hawker Siddeley e outras. A British Shipbuilders foi uma combinação das principais empresas de construção naval, incluindo Cammell Laird, Govan Shipbuilders, Swan Hunter e Yarrow Shipbuilders, enquanto a nacionalização das minas de carvão em 1947 criou uma comissão de carvão encarregada de gerir comercialmente a indústria do carvão para poder atender os juros a pagar sobre os títulos em que as ações dos ex-proprietários de minas haviam sido convertidas.[405][406]
O socialismo de mercado consiste em empresas de propriedade pública ou cooperativa operando em uma economia de mercado. É um sistema que utiliza os preços de mercado e monetários para a alocação e contabilização dos meios de produção, retendo assim o processo de acumulação de capital. O lucro gerado seria usado para remunerar diretamente os funcionários, sustentar coletivamente a empresa ou financiar instituições públicas.[407] Nas formas de socialismo de mercado orientadas pelo Estado, em que as empresas estatais tentam maximizar o lucro, os lucros podem ser usados para financiar programas e serviços do governo por meio de um dividendo social, eliminando ou diminuindo muito a necessidade de várias formas de tributação que existem nos sistemas capitalistas. O economista neoclássico Léon Walras acreditava que uma economia socialista baseada na propriedade estatal da terra e dos recursos naturais proporcionaria um meio de financiamento público para tornar os impostos de renda desnecessários.[16] A Iugoslávia implementou uma economia socialista de mercado baseada em cooperativas e na autogestão dos trabalhadores.
O mutualismo é uma teoria econômica e escola de pensamento anarquista que defende uma sociedade onde cada pessoa possa possuir um meio de produção, seja individual ou coletivamente, com o comércio representando quantidades equivalentes de trabalho no mercado livre.[408] Parte integrante do esquema era o estabelecimento de um banco de crédito mútuo que emprestaria aos produtores a uma taxa de juros mínima, mas alta o suficiente para cobrir a administração.[409] O mutualismo é baseado na teoria do valor-trabalho que sustenta que quando o trabalho ou seu produto é vendido, em troca ele deve receber bens ou serviços que incorporam "a quantidade de trabalho necessária para produzir um artigo de utilidade exatamente semelhante e igual".[410]
O atual sistema econômico da China é formalmente denominado "economia de mercado socialista com características chinesas". Ele combina um grande setor estatal que compreende os altos comandos da economia, que têm seus estatutos de propriedade pública garantidos por lei,[411] com um setor privado principalmente envolvido na produção de commodities e indústria leve responsável de algo entre 33%[412] a mais de 70% do PIB gerado em 2005.[413] Embora tenha ocorrido uma rápida expansão da atividade do setor privado desde a década de 1980, a privatização de ativos estatais foi virtualmente interrompida e parcialmente revertida em 2005.[414] A economia chinesa atual consiste em 150 empresas estatais corporatizadas que se reportam diretamente ao governo central chinês.[415] Em 2008, essas corporações estatais tornaram-se cada vez mais dinâmicas e geraram grandes aumentos nas receitas para o Estado,[416][417] resultando em uma recuperação liderada pelo setor estatal durante a crise financeira de 2009, ao mesmo tempo que respondia pela maior parte do crescimento econômico da China.[418] No entanto, o modelo econômico chinês é amplamente citado como uma forma contemporânea de capitalismo estatal, sendo que a principal diferença entre o capitalismo ocidental e o modelo chinês é o grau de propriedade estatal de ações em empresas listadas publicamente.
A República Socialista do Vietnã adotou um modelo semelhante após a renovação econômica Doi Moi, mas difere ligeiramente do modelo chinês no sentido de que o governo vietnamita mantém um controle firme sobre o setor estatal e as indústrias estratégicas, mas permite a atividade do setor privado na produção de commodities.[419]
Embora os principais movimentos políticos socialistas incluam anarquismo, comunismo, movimento trabalhista, marxismo, social-democracia e sindicalismo, teóricos socialistas independentes, autores socialistas utópicos e defensores acadêmicos do socialismo podem não estar representados nesses movimentos. Alguns grupos políticos se autodenominam socialistas, embora sustentem pontos de vista que alguns consideram antitéticos ao socialismo. O termo socialista também foi usado por alguns políticos da direita política como um epíteto contra certos indivíduos que não se consideram socialistas e contra políticas que não são consideradas socialistas por seus proponentes. Existem muitas variações de "socialismo" e, como tal, não existe uma definição única que englobe todo o socialismo. No entanto, houve elementos comuns identificados por estudiosos.[420]
Em Dictionary of Socialism (1924), Angelo S. Rappoport analisou quarenta definições de socialismo para concluir que os elementos comuns do socialismo incluem a crítica geral dos efeitos sociais da propriedade privada e do controle do capital - como sendo a causa da pobreza, dos baixos salários, do desemprego, da desigualdade econômica e social e falta de segurança econômica; uma visão geral de que a solução para esses problemas é uma forma de controle coletivo sobre os meios de produção, distribuição e troca (o grau e os meios de controle variam entre os movimentos socialistas); um acordo de que o resultado desse controle coletivo deve ser uma sociedade baseada na justiça social, incluindo igualdade social e proteção econômica das pessoas, sendo que deve proporcionar uma vida mais satisfatória para a grande maioria da população.[421]
Em The Concepts of Socialism (1975), Bhikhu Parekh identifica quatro princípios fundamentais do socialismo e, em particular, da sociedade socialista, a saber, socialidade, responsabilidade social, cooperação e planejamento.[422] Em seu estudo Ideologias e Teoria Política (1996), Michael Freeden afirma que todos os socialistas compartilham cinco temas: o primeiro é que o socialismo postula que a sociedade é mais do que uma mera coleção de indivíduos; segundo, que considera o bem-estar humano um objetivo desejável; terceiro, que considera os humanos por natureza ativos e produtivos; quarto, ele mantém a crença na igualdade humana; e quinto, que a história é progressiva e criará mudanças positivas com a condição de que os humanos trabalhem para conseguir tal mudança.
O anarquismo defende sociedades apátridas geralmente definidas como instituições voluntárias autogovernadas[423][424][425][426] mas que vários autores definiram como instituições mais específicas baseadas em associações livres não hierárquicas.[427][428][429] Enquanto o anarquismo considera o Estado indesejável, desnecessário ou prejudicial,[430][431] este não é o aspecto central.[432] O anarquismo envolve autoridade oposta ou organização hierárquica na conduta das relações humanas, incluindo o sistema de Estado.[433][434][435][436][437][438] Os mutualistas apóiam o socialismo de mercado, anarquistas coletivistas favorecem cooperativas de trabalhadores e salários com base na quantidade de tempo contribuído para a produção, anarcocomunistas defendem uma transição direta do capitalismo para o comunismo libertário e uma economia de oferta, enquanto os anarcossindicalistas preferem a ação direta dos trabalhadores e a greve geral.[439]
As lutas e disputas autoritárias-libertárias dentro do movimento socialista remontam à Associação Internacional dos Trabalhadores e à expulsão em 1872 dos anarquistas, que passaram a liderar a Internacional Antiautoritária e então fundaram sua própria internacional libertária, a Internacional Anarquista de St. Imier.[440]
[441] Em 1888, o anarquista individualista Benjamin Tucker, que se proclamou um socialista anarquista e libertário em oposição ao socialismo estatal autoritário e ao comunismo obrigatório, incluiu o texto completo de uma "Carta Socialista" de Ernest Lesigne[442] em seu ensaio "Socialismo de Estado e Anarquismo". Segundo Lesigne, existem dois tipos de socialismo: “Um é ditatorial, o outro libertário”.[443] Os dois socialismos de Tucker eram o socialismo estatal autoritário que ele associava à escola marxista e o socialismo anarquista libertário, ou simplesmente anarquismo, que ele defendia. Tucker observou que o fato de que o "socialismo estatal ofuscou outras formas de socialismo não lhe dá direito ao monopólio da ideia socialista".[444] De acordo com Tucker, o que essas duas escolas de socialismo tinham em comum era a teoria do valor-trabalho e dos fins, pela qual o anarquismo buscava meios diferentes.[445]
De acordo com anarquistas como os autores de An Anarchist FAQ, o anarquismo é uma das muitas tradições do socialismo. Para anarquistas e outros socialistas antiautoritários, o socialismo “só pode significar uma sociedade sem classes e antiautoritária (isto é, libertária) na qual as pessoas administram seus próprios assuntos, seja como indivíduos ou como parte de um grupo (dependendo da situação). Ou seja, implica na autogestão de todos os aspectos da vida”, inclusive no ambiente de trabalho.[439] Peter Marshall argumenta que "o anarquismo em geral está mais próximo do socialismo do que do liberalismo. [...] O anarquismo encontra-se amplamente no campo socialista, mas também tem batedores no liberalismo. Não pode ser reduzido ao socialismo e é melhor visto como uma doutrina separada e distinta".[446]
Você não pode falar sobre acabar com as favelas sem primeiro dizer que o lucro deve ser retirado das favelas. Você está realmente mexendo e entrando em terreno perigoso porque está mexendo com pessoas. Você está mexendo com capitães da indústria. Agora, isso significa que estamos pisando em águas difíceis, porque realmente significa que estamos dizendo que algo está errado com o capitalismo. Deve haver uma melhor distribuição da riqueza e talvez a América deva caminhar em direção a um socialismo democrático.[447][448][449]
O socialismo democrático representa qualquer movimento socialista que busca estabelecer uma economia baseada na democracia econômica pela e para a classe trabalhadora. O socialismo democrático é difícil de definir e grupos de acadêmicos têm definições radicalmente diferentes para o termo. Algumas definições simplesmente se referem a todas as formas de socialismo que seguem um caminho eleitoral, reformista ou evolucionário para o socialismo, em vez de um caminho revolucionário. De acordo com Christopher Pierson, "o contraste que 1989 destaca não é aquele entre o socialismo no Oriente e a democracia liberal no Ocidente, esta última deve ser reconhecida como tendo sido moldada, reformada e comprometida por um século de pressão social-democrata". Pierson afirma ainda que "os partidos social-democratas e socialistas dentro da arena constitucional no Ocidente quase sempre estiveram envolvidos em uma política de compromisso com as instituições capitalistas existentes (para qualquer prêmio distante que seus olhos pudessem de vez em quando ser levantados)". Para Pierson, “se os defensores da morte do socialismo aceitam que os social-democratas pertencem ao campo socialista, como penso que devem pertencer, então o contraste entre o socialismo (em todas as suas variantes) e a democracia liberal deve entrar em colapso. Pois a democracia liberal realmente existente é, em parte substancial, um produto das forças socialistas (social-democratas)”.[450]
A social-democracia é uma tradição socialista de pensamento político.[451][452] Muitos social-democratas referem-se a si próprios como socialistas ou socialistas democratas e alguns, como Tony Blair, empregam esses termos alternadamente.[453][454][455] Outros encontraram "diferenças claras" entre os três termos e preferem descrever suas próprias crenças políticas usando o termo social-democracia.[456] As duas direções principais eram estabelecer o socialismo democrático ou construir primeiro um estado de bem-estar social dentro do sistema capitalista. A primeira variante promove o socialismo democrático por meio de métodos reformistas e gradualistas.[457] Na segunda variante, a social-democracia é um regime político que envolve um estado de bem -estar, esquemas de negociação coletiva, apoio a serviços públicos com financiamento público e uma economia mista. É frequentemente usado dessa maneira para se referir à Europa Ocidental e do Norte durante a segunda metade do século XX.[458][459] Foi descrito por Jerry Mander como "economia híbrida", uma colaboração ativa das visões capitalista e socialista.[460] Vários estudos e pesquisas indicam que as pessoas tendem a viver vidas mais felizes em sociedades social-democratas do que em sociedades neoliberais.[461][462][463][464]
Os social-democratas defendem uma transição pacífica e evolutiva da economia para o socialismo por meio de reformas sociais progressivas.[465][466] Afirma que a única forma constitucional de governo aceitável é a democracia representativa de acordo com o Estado de direito.[467] Promove a extensão da tomada de decisão democrática para além da democracia política para incluir a democracia econômica para garantir aos empregados e outras partes interessadas direitos suficientes de co-determinação.[467] Ele apóia uma economia mista que se opõe à desigualdade, pobreza e opressão, ao mesmo tempo que rejeita uma economia de mercado totalmente desregulada ou uma economia totalmente planejada.[468] Políticas sociais democráticas comuns incluem direitos sociais universais e serviços públicos universalmente acessíveis, como educação, saúde, compensação de trabalhadores e outros serviços, incluindo creche e cuidados com os idosos.[467] A social-democracia apóia o movimento sindical e os direitos de negociação coletiva dos trabalhadores.[469] A maioria dos partidos social-democratas são filiados à Internacional Socialista.[457]
O socialismo democrático moderno é um amplo movimento político que busca promover os ideais do socialismo no contexto de um sistema democrático. Alguns socialistas democráticos apóiam a social-democracia como uma medida temporária para reformar o sistema atual, enquanto outros rejeitam o reformismo em favor de métodos mais revolucionários. A social-democracia moderna enfatiza um programa de modificação legislativa gradual do capitalismo a fim de torná-lo mais equitativo e humano, enquanto o objetivo final teórico de construir uma sociedade socialista, o que é relegado a um futuro indefinido. De acordo com Sheri Berman, o marxismo é vagamente considerado valioso por sua ênfase em mudar o mundo para um futuro mais justo e melhor.[470]
Os dois movimentos são muito semelhantes tanto em terminologia quanto em ideologia, embora existam algumas diferenças fundamentais. A principal diferença entre a social-democracia e o socialismo democrático é o objetivo de suas políticas, pois os social-democratas contemporâneos apóiam o estado de bem-estar e o seguro-desemprego, bem como outras reformas práticas e progressivas do capitalismo, e estão mais preocupados em administrá-lo e humanizá-lo. Por outro lado, os socialistas democráticos buscam substituir o capitalismo por um sistema econômico socialista, argumentando que qualquer tentativa de humanizar o capitalismo por meio de regulamentações e políticas de bem-estar distorceria o mercado e criaria contradições econômicas.[471]
O socialismo ético apela ao socialismo em bases éticas e morais, em oposição a bases econômicas, egoístas e consumistas. Ele enfatiza a necessidade de uma economia moralmente consciente baseada nos princípios do altruísmo, cooperação e justiça social, enquanto se opõe ao individualismo possessivo.[472] O socialismo ético tem sido a filosofia oficial dos principais partidos socialistas.[485]
O socialismo liberal incorpora princípios liberais ao socialismo.[473] Ele foi comparado à social-democracia do pós-guerra[474] por seu apoio a uma economia mista que inclui bens de capital públicos e privados.[475][476] Enquanto o socialismo democrático e a social-democracia são posições anticapitalistas na medida em que a crítica ao capitalismo está ligada à propriedade privada dos meios de produção,[477] o socialismo liberal identifica os monopólios artificiais e legalistas como sendo culpa do capitalismo[478] e se opõe a um economia de mercado totalmente não regulamentada.[479] Considera a liberdade e a igualdade social compatíveis e mutuamente dependentes.[473]
Princípios que podem ser descritos como socialistas éticos ou liberais foram baseados ou desenvolvidos por filósofos como John Stuart Mill, Eduard Bernstein, John Dewey, Carlo Rosselli, Norberto Bobbio e Chantal Mouffe .[480] Outras importantes figuras do socialismo liberal incluem Guido Calogero, Piero Gobetti, Leonard Trelawny Hobhouse, John Maynard Keynes e R. H. Tawney.[479] O socialismo liberal tem sido particularmente proeminente na política britânica e italiana.
O blanquismo é uma concepção de revolução que leva o nome de Louis Auguste Blanqui. Afirma que a revolução socialista deve ser realizada por um grupo relativamente pequeno de conspiradores altamente organizados e secretos.[481] Ao tomar o poder, os revolucionários introduzem o socialismo.[482] Rosa Luxemburgo e Eduard Bernstein[483] criticaram Lenin, afirmando que sua concepção de revolução era elitista e blanquista.[484] O marxismo-leninismo combina os conceitos socialistas científicos de Marx e o anti-imperialismo, centralismo democrático e vanguardismo de Lenin.[485]
Hal Draper definiu o socialismo de cima como a filosofia que emprega uma administração de elite para dirigir o Estado socialista. O outro lado do socialismo é um socialismo mais democrático visto de baixo.[486] A ideia do socialismo de cima é discutida com muito mais frequência nos círculos da elite do que o socialismo de baixo - mesmo que esse seja o ideal marxista - porque é mais prático.[487] Draper via o socialismo de baixo como sendo a versão mais pura e marxista do socialismo.[488] De acordo com Draper, Karl Marx e Friedrich Engels eram devotamente opostos a qualquer instituição socialista que fosse "conducente ao autoritarismo supersticioso". Draper argumenta que essa divisão ecoa a divisão entre "reformista ou revolucionário, pacífico ou violento, democrático ou autoritário, etc" e ainda identifica seis variedades principais de socialismo de cima, entre eles "Filantropismo", "Elitismo", "Pannismo", "Comunismo", "Permeacionismo" e "Socialismo de fora".[486]
Segundo Arthur Lipow, Marx e Engels foram "os fundadores do socialismo democrático revolucionário moderno", descrito como uma forma de "socialismo de baixo" que é "baseado em um movimento de massas da classe trabalhadora, lutando de baixo para cima pela extensão da democracia e liberdade humana ". Este tipo de socialismo é contrastado com o do "credo autoritário e antidemocrático" e com "as várias ideologias coletivistas totalitárias que reivindicam o título de socialismo", bem como com "as muitas variedades de 'socialismo de cima' que conduziram no século XX aos movimentos e formas de estado em que uma despótica 'nova classe' governa uma economia estatizada em nome do socialismo, uma divisão que "permeia a história do movimento socialista". Lipow identifica o belamismo e o stalinismo como duas correntes socialistas autoritárias proeminentes na história do movimento socialista.[489]
O socialismo libertário, às vezes chamado de libertarianismo de esquerda,[492][493] anarquismo social[494][495] e libertarianismo socialista,[496] é uma tradição antiautoritária, antiestatista e libertária dentro do socialismo que rejeita a centralização da propriedade e do controle do Estado[497][498] incluindo críticas às relações de trabalho assalariado (escravidão assalariada),[499] bem como ao próprio Estado. Enfatiza a autogestão dos trabalhadores e as estruturas descentralizadas de organização política.[500] O socialismo libertário afirma que uma sociedade baseada na liberdade e igualdade pode ser alcançada através da abolição das instituições autoritárias que controlam a produção.[501] Socialistas libertários geralmente preferem democracia direta e associações federais ou confederais, como municipalismo libertário, assembleias de cidadãos, sindicatos e conselhos de trabalhadores.[502]
O anarcossindicalista Gaston Leval explicou: “Prevemos, portanto, uma Sociedade na qual todas as atividades serão coordenadas, uma estrutura que tenha, ao mesmo tempo, flexibilidade suficiente para permitir a maior autonomia possível para a vida social, ou para a vida de cada empresa, e coesão suficiente para prevenir todos os distúrbios. [...] Em uma sociedade bem organizada, todas essas coisas devem ser sistematicamente realizadas por meio de federações paralelas, verticalmente unidas nos níveis mais altos, constituindo um vasto organismo no qual todas as funções econômicas serão desempenhadas em solidariedade com todas as outras e que serão permanentemente preservar a coesão necessária”.[503] Tudo isso é geralmente feito dentro de um apelo geral por associações libertárias[504] e voluntárias[505] por meio da identificação, crítica e desmantelamento prático da autoridade ilegítima em todos os aspectos da vida humana.[434][506][507]
Como parte do movimento socialista mais amplo, busca distinguir-se do bolchevismo, do leninismo e do marxismo-leninismo, bem como da social-democracia.[508] Filosofias e movimentos políticos passados e presentes comumente descritos como socialistas libertários incluem anarquismo (anarco-comunismo, anarco-sindicalismo[509] anarquismo coletivista, anarquismo individualista[510][511][512] e mutualismo),[513] autonomismo, comunalismo, participismo, marxismo libertário (comunismo de conselhos e luxemburguês),[514] sindicalismo revolucionário e socialismo utópico (fourierismo).[515]
O socialismo cristão é um conceito amplo que envolve um entrelaçamento da religião cristã com o socialismo.
O socialismo islâmico é uma forma mais espiritual de socialismo. Os socialistas muçulmanos acreditam que os ensinamentos do Alcorão e de Maomé não são apenas compatíveis, mas promovem ativamente os princípios de igualdade e propriedade pública, inspirando-se no antigo estado de bem-estar de Medina que ele estabeleceu. Os socialistas muçulmanos são mais conservadores do que seus contemporâneos ocidentais e encontram suas raízes no anti-imperialismo, no anticolonialismo e, às vezes, se em um país de língua árabe, no nacionalismo árabe. Os socialistas islâmicos acreditam em derivar legitimidade do mandato político em oposição aos textos religiosos.
O feminismo socialista é um ramo do feminismo que argumenta que a libertação só pode ser alcançada trabalhando para acabar com as fontes econômicas e culturais da opressão das mulheres.[516] O fundamento do feminismo marxista foi lançado por Engels em A origem da família, propriedade privada e Estado (1884). Mulher sob o socialismo, de August Bebel (1879), é a "obra individual que trata da sexualidade mais lida por membros comuns do Partido Social-Democrata da Alemanha (SPD)".[517] No final do século XIX e no início do século XX, Clara Zetkin e Eleanor Marx foram contra a demonização dos homens e apoiaram uma revolução do proletariado que superaria tantas desigualdades entre homens e mulheres quanto possível.[518] Como seu movimento já tinha as demandas mais radicais pela igualdade das mulheres, a maioria dos líderes marxistas, incluindo Clara Zetkin[519][520] e Alexandra Kollontai,[521][522] contrapôs o marxismo ao feminismo liberal ao invés de tentar combiná-los. O anarco-feminismo começou com autores e teóricos do final do século XIX e início do século XX, como as feministas anarquistas Goldman e Voltairine de Cleyre[523] Na Guerra Civil Espanhola, um grupo anarco-feminista, Mujeres Libres ("Mulheres Livres") vinculada à Federación Anarquista Ibérica, organizado para defender as ideias anarquistas e feministas.[524] Em 1972, a Chicago Women's Liberation Union publicou "Feminismo Socialista: Uma Estratégia para o Movimento das Mulheres", que se acredita ser o primeiro uso publicado do termo "feminismo socialista".[525]
Muitos socialistas foram os primeiros defensores dos direitos LGBT. Para os primeiros socialistas Charles Fourier, a verdadeira liberdade só poderia ocorrer sem suprimir as paixões, já que a supressão das paixões não é apenas destrutiva para o indivíduo, mas para a sociedade como um todo. Escrevendo antes do advento do termo "homossexualidade", Fourier reconheceu que tanto homens quanto mulheres têm uma ampla gama de necessidades e preferências sexuais que podem mudar ao longo de suas vidas, incluindo a sexualidade do mesmo sexo e a androgénité. Ele argumentou que todas as expressões sexuais devem ser apreciadas, desde que as pessoas não sejam abusadas e que "afirmar a diferença" pode realmente aumentar a integração social.[526][527] Em The Soul of Man Under Socialism, de Oscar Wilde, ele defende uma sociedade igualitária onde a riqueza é compartilhada por todos, enquanto alerta para os perigos dos sistemas sociais que esmagam a individualidade.[528] Edward Carpenter fez campanha ativamente pelos direitos dos homossexuais. Sua obra O sexo intermediário: um estudo de alguns tipos transitórios de homens e mulheres foi um livro de 1908 defendendo a libertação gay.[529] Ele foi uma personalidade influente na fundação da Sociedade Fabiana e do Partido Trabalhista. Após a Revolução Russa sob a liderança de Lenin e Trotsky, a União Soviética aboliu as leis anteriores contra a homossexualidade.[530] Harry Hay foi um dos primeiros líderes do movimento americano pelos direitos LGBT, bem como membro do Partido Comunista dos Estados Unidos. Ele é conhecido por seus papéis em ajudar a fundar organizações gays, incluindo a Sociedade Mattachine, o primeiro grupo sustentado pelos direitos dos homossexuais nos Estados Unidos que, em seus primeiros dias, refletia uma forte influência marxista. A Encyclopedia of Homosexuality relata que "os marxistas, os fundadores do grupo, acreditavam que a injustiça e a opressão que sofreram derivavam de relacionamentos profundamente enraizados na estrutura da sociedade americana".[531] Emergindo de eventos como a insurreição de maio de 1968 na França, o movimento contra a Guerra do Vietnã nos Estados Unidos e os distúrbios de Stonewall em 1969, organizações militantes de libertação gay começaram a surgir em todo o mundo. Muitos surgiram do radicalismo de esquerda mais do que grupos homófilos estabelecidos,[532] embora a Frente de Libertação Gay assumisse uma postura anticapitalista e atacasse a família nuclear e os papeis tradicionais de gênero.[533]
O ecossocialismo é uma tendência política que funde aspectos do socialismo, marxismo ou socialismo libertário com política verde, ecologia e alter-globalização. Os ecossocialistas geralmente afirmam que a expansão do sistema capitalista é a causa da exclusão social, pobreza, guerra e degradação ambiental através da globalização e do imperialismo sob a supervisão de Estados repressivos e estruturas transnacionais.[534] Ao contrário da descrição de Karl Marx por alguns ambientalistas,[535] ecologistas sociais[536] e companheiros socialistas[537] como um produtivista que favorecia a dominação da natureza, os ecossocialistas revisitaram os escritos de Marx e acreditam que ele "foi o principal criador da visão de mundo ecológica".[538] Marx discutiu uma "fenda metabólica" entre o homem e a natureza, afirmando que "a propriedade privada do globo por indivíduos individuais parecerá bastante absurda como propriedade privada de um homem por outro" e sua observação de que uma sociedade deve "entregá-lo [ao planeta] para as gerações seguintes em uma condição melhorada".[539] O socialista inglês William Morris é responsável pelo desenvolvimento dos princípios do que mais tarde foi chamado de ecossocialismo.[540] Durante as décadas de 1880 e 1890, Morris promoveu suas ideias na Federação Social-Democrata e na Liga Socialista.[541] O anarquismo verde combina anarquismo com questões ambientais. Uma importante influência inicial foi Henry David Thoreau e seu livro Walden,[542] bem como Élisée Reclus.[543][544]
No final do século XIX, o anarco-naturismo fundiu o anarquismo e as filosofias naturistas dentro dos c´riculos anarquistas individualistas na França, Espanha, Cuba[545] e Portugal.[546] O primeiro livro de Murray Bookchin, Nosso ambiente sintético[547] foi seguido por seu ensaio "Ecologia e pensamento revolucionário", que introduziu a ecologia como um conceito na política radical.[548] Na década de 1970, Barry Commoner afirmou que as tecnologias capitalistas eram as principais responsáveis pela degradação ambiental, em oposição às pressões populacionais.[549] Na década de 1990, as socialistas/feministas Mary Mellor[550] e Ariel Salleh[551] adotaram um paradigma ecossocialista. Um "ambientalismo dos pobres" combinando consciência ecológica e justiça social também se tornou proeminente.[552] Pepper criticou a abordagem atual de muitos dentro da política verde, especialmente ecologistas profundos.[553]
Muitos partidos verdes em todo o mundo, como o Partido Esquerdo Verde Holandês (GroenLinks), empregam elementos eco-socialistas. Alianças radicais vermelho-verdes foram formadas em muitos países por ecossocialistas, verdes radicais e outros grupos de esquerda radical. Na Dinamarca, a Aliança Vermelha-Verde foi formada como uma coalizão de vários partidos radicais. Dentro do Parlamento Europeu, vários partidos de esquerda do norte da Europa se organizaram na Aliança Nórdica de Esquerda Verde.
O sindicalismo opera por meio de sindicatos industriais. Ele rejeita o socialismo estatal e o uso de políticas estabelecidas. Os sindicalistas rejeitam o poder do Estado em favor de estratégias como a greve geral. Os sindicalistas defendem uma economia socialista baseada em sindicatos federados ou de trabalhadores que possuem e administram os meios de produção. Algumas correntes marxistas defendem o sindicalismo, como o De Leonismo. O anarcossindicalismo vê o sindicalismo como um método para os trabalhadores da sociedade capitalista obterem o controle de uma economia. A Revolução Espanhola foi amplamente orquestrada pelo sindicato anarco-sindicalista CNT.[161] A Associação Internacional dos Trabalhadores é uma federação internacional de sindicatos e iniciativas anarco-sindicalistas.[554]
De acordo com o sociólogo marxista analítico Erik Olin Wright, "a direita condenou o socialismo por violar os direitos individuais à propriedade privada e desencadear formas monstruosas de opressão estatal", enquanto "a esquerda o via como abrindo novas perspectivas de igualdade social, liberdade genuína e desenvolvimento dos potenciais humanos."[555]
Por causa das muitas variedades do socialismo, a maioria das críticas se concentrou em uma abordagem específica. Os defensores de uma abordagem normalmente criticam outras. O socialismo tem sido criticado em termos de seus modelos de organização econômica, bem como de suas implicações políticas e sociais. Outras críticas são dirigidas ao movimento socialista, partidos ou estados existentes. Algumas formas de crítica ocupam terrenos teóricos, como no problema do cálculo econômico apresentado pelos proponentes da Escola Austríaca como parte do debate sobre o cálculo socialista, enquanto outras sustentam suas críticas examinando tentativas históricas de estabelecer sociedades socialistas. O problema de cálculo econômico diz respeito à viabilidade e aos métodos de alocação de recursos para um sistema socialista planejado.[556][557][558] O planejamento central também é criticado por elementos da esquerda radical. O economista socialista libertário Robin Hahnel observa que, mesmo que o planejamento central superasse suas inibições inerentes de incentivos e inovação, seria, no entanto, incapaz de maximizar a democracia econômica e a autogestão, que ele acredita serem conceitos intelectualmente mais coerentes, consistentes e justos do que o mainstream de noções de liberdade econômica.[559]
Os liberais econômicos e os libertários de direita argumentam que a propriedade privada dos meios de produção e as trocas de mercado são entidades naturais ou direitos morais que são centrais para a liberdade e argumentam que a dinâmica econômica do capitalismo é imutável e absoluta. Como tal, eles também argumentam que a propriedade pública dos meios de produção e o planejamento econômico são violações da liberdade.[560][561]
Os críticos do socialismo argumentam que em qualquer sociedade em que todos tenham riqueza igual, não pode haver incentivo material para trabalhar porque não se recebe recompensas por um trabalho bem feito. Eles ainda argumentam que os incentivos aumentam a produtividade para todas as pessoas e que a perda desses efeitos levaria à estagnação. Alguns críticos do socialismo argumentam que o compartilhamento de renda reduz os incentivos individuais ao trabalho e, portanto, a renda deve ser individualizada tanto quanto possível[562]
Alguns filósofos também criticaram os objetivos do socialismo, argumentando que a igualdade corrói as diversidades individuais e que o estabelecimento de uma sociedade igualitária teria que implicar forte coerção.[563] Muitos comentaristas da direita política apontam para os assassinatos em massa sob os regimes socialistas, alegando-os como uma acusação ao socialismo.[564][565][566] Os oponentes dessa visão, incluindo os defensores do socialismo, afirmam que essas mortes foram aberrações causadas por regimes autoritários específicos, e não causadas pelo próprio socialismo, e apontam para mortes em massa em fomes, guerras e massacres que eles afirmam terem sido causados pelo colonialismo, capitalismo e anticomunismo como contraponto a esses assassinatos.[567][568][565][569]
O dinheiro é na sua aparência mais imediata o meio usado na troca de bens, podendo fazê-lo na forma de moedas (pedaços de metal amoedados e cunhados, isto é, marcados por desenhos, letras e números), notas (cédulas de papel, igualmente desenhadas e escritas), ou, como atualmente, sinais elétricos carregados de informação, chamados bits. Vê-se assim como dinheiro e moeda se confundem; sendo que as moedas - quando mais físicas são - mais obscurecem que esclarecem o que este é realmente. Isso porque o que o dinheiro é essencialmente é um signo. E um signo representativo de valores, que é a informação que este signo carrega. Estes valores representados no dinheiro são os das coisas (bens e serviços) que se desprendem dos homens nos impessoais mercados, mas também e principalmente os valores dos compromissos, dívidas e créditos, que os homens estabelecem entre si desde sempre, ou desde muito antes dos mercados.
As economias modernas, capitalistas, são essencialmente monetárias;[1] isso significa dizer que o conjunto das relações sociais é mediado pelo dinheiro. O dinheiro não apenas media compras de bens e serviços, mas media a obtenção de trabalho, as decisões das famílias de gastar ou poupar, e as importantes decisões empresariais - de produzir, investir, ou especular. As decisões empresariais como um todo visam obter mais dinheiro do que o dinheiro inicial. Decisões de produzir implicam utilizar a capacidade produtiva existente, de investir implicam em aumentar essa capacidade, o que só é feito se e quando há elevadas expectativas de ganho. Os capitalistas investem o dinheiro que têm, ou mesmo tomam dinheiro emprestado dos bancos ou o conseguem junto a acionistas, se as expectativas de retorno pagam com sobra os juros dos bancos e os dividendos dos acionistas. Mas eles podem também usar o dinheiro de que dispõem (e o que conseguem obter com bancos e acionistas) para aplicar financeiramente eles próprios. Ou seja, não necessariamente o dinheiro tem de ser gasto por eles. Assim sendo, ao contrário do que pensam alguns economistas desde Jean Baptiste Say, o dinheiro não serve apenas para facilitar as trocas, e aquele não gasto na troca por bens de consumo das famílias não será automaticamente usado pelos capitalistas na compra de bens de investimento. Economistas como Marx, Keynes, Kalecki, Schumpeter, entre outros heterodoxos, chamaram a atenção quanto a isso - que o dinheiro pode se ausentar da produção e assim gerar crises, pois o dinheiro não gasto equivale a máquinas e equipamentos parados e mão de obra desempregada.
Sendo o dinheiro um signo de valor que serve para exprimir os preços das coisas, ele próprio não necessita ser uma coisa. Ou seja, não é dinheiro apenas o que é uma mercadoria produto do trabalho e sujeita a escassez como as outras, ainda que isso tenha acontecido em alguns momentos da história (ver adiante). Não é regra que o dinheiro seja aquela mercadoria que no confronto com as demais torna-se a mais aceita por razões de praticidade. Os economistas que pensam o dinheiro como mercadoria derivam suas teorias de um idílico estado primitivo de escambo. Para estes economistas ser mercadoria garante ao dinheiro ter estabilidade - essencial em algo que serve de medida. Os economistas que pensam o dinheiro como signo também dão importância para a estabilidade de seu valor, mas esse valor é de saída estipulado pelos governantes que o administram, e eles o fazem controlando as taxas básicas de juros que funcionam como preço do dinheiro, ou quão mais caro ou barato é consegui-lo. Esses economistas são filiados mais ou menos diretamente à escola da moeda como oriunda do Estado de Knapp.[2]Os Estados nacionais têm nas moedas nacionais a sua mais importante instituição. Garantir que a moeda que produzem seja a que efetivamente os cidadãos usam como dinheiro é fundamental para sua credibilidade política. Usar a moeda como dinheiro implica usá-la como meio de troca (compra e venda de bens e serviços), como reserva de valor (poupança e aplicações financeiras) e, fundamentalmente, como unidade de conta (expressão dos preços, definidora de valores nos contratos, signo generalizado de registro de débitos e créditos). Moedas fracas podem perder uma ou mais destas funções para uma moeda estrangeira. Só um governo enfraquecido permite o enfraquecimento de sua moeda e corre sério risco de deixar de ser governo, e ainda pode colocar em risco a nação que governa. Isso aconteceu, por exemplo, no período de entre guerras na Alemanha quando a derrota na Primeira Guerra Mundial e o peso das reparações de guerra, fora o pano de fundo sociocultural, ameaçava a sustentação política da nação como um todo e levou a moeda nacional a uma desvalorização brutal junto à hiperinflação. Contudo, estão equivocados os economistas que julgam que para manter o valor estável da moeda nacional, e os preços sobre controle, devem ser impostas aos governo rígidas regras que os impeçam de emitir dinheiro demais. Os governos emitem dinheiro, na forma de suas moedas (de papel ou eletrônicas), toda vez que fazem um gasto e creditam algum valor nas contas comerciais de algum cidadão ou empresa. Contra esse crédito é realizado um débito na conta do governo no seu Banco Central. O financiamento dos gastos do governo nunca é um problema - em território nacional -, como acreditam os que desconhecem que o dinheiro é uma criatura do Estado. Isso não significa que não seja absolutamente relevante que a sociedade controle como, quando e onde gastam seus governos, o que podem e devem fazer por meio das discussões orçamentárias (onde se define, por exemplo, se mais ou menos recursos devem ir para educação ou propaganda). Contudo, o controle quantitativo do total dos gastos dos governos, com vistas a garantir finanças públicas ditas "equilibradas", parte em geral da má compreensão de que os governos gastam a partir do que arrecadam de seus cidadãos e empresas na forma de impostos.[3]Impostos são essenciais para distribuir a renda entre os cidadãos mas não o são para financiar os gastos dos governos.[4] De fato, os Estados ao longo da história, tornaram a sua moeda hegemônica justamente por definirem que seus impostos deveriam ser obrigatoriamente pagos nelas.[5] Ou seja, a parte da riqueza ou do produto do trabalho dos cidadãos que os Estados reivindicam para si se são exigidos nesse signo, a moeda estatal, em vez de bois ou trigo (como nos filmes de Robin Hood) as pessoas precisam da moeda do Estado, o que acaba por torná-la a mais aceita para cumprir as funções de dinheiro.
Se por alguma limitação auto-imposta a emissão pura e simples de moeda para financiar o gasto público não for possível, os governos podem recorrer a uma quase-moeda, os títulos de dívida pública. Tanto moeda quanto títulos são signos de dívida, quem os carrega tem consigo um documento que vale um pagamento em bens e serviços. Se a moeda é a forma mais líquida do dinheiro, como salienta Keynes, os títulos públicos - que podem ser resgatados por dinheiro sempre que o governo quiser - garante a estes liquidez semelhante à daquele, com a vantagem de que mantê-los rende juros. Por isso, estes títulos sempre terão compradores no mercado. "Forças de mercado", ou mais explicitamente, pressões políticas de certos grupos de interesses, podem pressionar estes juros para cima, mas um governo soberano e voltado às demandas sociais em primeiro plano, deve e pode contê-las.
Se a moeda nacional cumpre, em condições normais, o papel de dinheiro no território da nação, o papel de dinheiro mundial será disputado pelas nações mais ricas e poderosas. Ou seja, os países mais ricos - aqueles que produzem bens e serviços mais sofisticados e valiosos podendo concorrer em melhores condições que os outros - e mais poderosos - aqueles que têm poder político, cultural e bélico para subjugar outros - têm também as moedas mais desejadas, mais usadas nas compras e vendas e na denominação dos contratos ao nível global. A depender das circunstâncias de época os países cooperam e concorrem entre si de diferentes maneiras, sustentadas por diferentes acordos monetários globais. Estes acordos definem normas para a troca de moedas, câmbio, para as aplicações financeiras e movimentação de capitais entre países. No século XIX a Inglaterra impôs o seu padrão-ouro ao mundo; no pós segunda guerra os acordos de Breton Woods garantiram câmbio fixo entre as moedas e algum controle sobre bancos em suas operações nacionais e globais. Em 1971 os Estados Unidos abandonaram unilateralmente com estes acordos; dos anos 80 em diante, o dólar se mantêm como dinheiro mundial ainda que à custa de muita instabilidade.
A história do dinheiro está absolutamente ligada à história das primeiras comunidades humanas e à sua necessidade de construir, como chama Yuval Harari, "fábulas compartilhadas".  Em seu livro Sapiens, Harari defende o mesmo que outros tantos historiadores e antropólogos (Graeber os elenca em seu livro acima citado): que o dinheiro é antes de qualquer coisa uma ideia, uma instituição social que toma diferentes formas em diferentes momentos da história humana.
Se sabemos que o mercado e o Estado são também instituições que convivem entre si ainda que disputem ao longo dos séculos e milênios a primazia sobre a outra, sabemos que o dinheiro não precisa ser procurado na história como uma criatura exclusiva do mercado - o que faz muitos economistas a apelarem para a fábula do escambo como sendo sua origem. Nesta indivíduos soltos do tempo e no espaço só se relacionariam entre si pelos produtos do seu trabalho, e a comparação de todos as coisas que trocassem entre si faria com que uma delas assumisse naturalmente o papel de dinheiro. Trata-se pois de uma compreensão de dinheiro como mercadoria, pois este surgiria do fato de que algumas delas apresentariam características físicas (como durabilidade e divisibilidade) que as tornariam mais procuradas do que as outras para as representarem e avaliarem seu valor.
Sociedades muito antigas, em torno de 5000 anos, conforme a minuciosa pesquisa  de David Graeber em seu Debt: the first 5000 years, criaram meios de registrar seus acordos e compromissos que envolviam dívidas e créditos (ou cessão antecipada ou postergada de bens e serviços) fossem acordos entre os membros de uma mesma comunidade (de mesmo status ou de status diferentes), fossem com membros de outras comunidades, e mesmo com seres imaginários. Estes compromissos se davam dentro de grupos familiares - sendo os principais, o casamento, o nascimento e a morte - ou envolviam toda a comunidade - como expedições e guerras.
As primeira formas de moeda foram registros das dívidas uma vez feitos num objeto qualquer - esculpido numa pedra ou num pedaço de madeira -, circulava entre os membros da comunidade, sendo trocado diversas vezes por bens equivalentes antes que fosse resgatado pela dupla original de credor-devedor. Isso significa que qualquer coisas aceita como passível de representar outras poderia ser moeda-dinheiro. Ao mesmo tempo, fora das comunidades cujas relações se davam entre conhecidos, onde as penalidades pelo mau comportamento - não pagamento de uma dívida que faria determinado registro perder seu valor - viriam de uma forma ou de outra, é aceitável que coisas com valor em si mesmo - como as mercadorias ouro e prata, feitas para serem comercializadas - pudessem ser preferidas como dinheiro. Isso porque no caso de pessoas e grupos sociais que não convivessem de perto, ou que não tivessem relações reiteradas no tempo, o que é mais comum no comércio, o valor intrínseco desses metais funciona como um seguro - uma vez que não se consiga trocá-lo pelo que está escrito nele, sempre se pode derretê-lo.Assim é que, segundo Graeber, "o dinheiro é quase sempre algo que paira entre uma mercadoria e um símbolo de dívida",[6] sendo portanto estas duas dimensões, as duas faces da mesma moeda.  Ainda assim, pelas ponderações históricas e geográficas que apresenta, Graeber, apoiado em extensa bibliografia, mostra que a face dinheiro-crédito/título de dívida predominou no tempo e no espaço sobre a face dinheiro-mercadoria. Onde as relações envolviam laços fortes de camaradagem e vizinhança, ou mesmo uma exploração direta mas que não retirava o explorado (servilizado ou escravizado) de seu contexto comunitário, o dinheiro de crédito preponderava - mesmo convivendo com o dinheiro mercadoria na troca com párias dentro da comunidade (pessoas consideradas inferiores e mau pagadores) ou comunidades estranhas (o que inclui o enfrentamento do risco devido a baixa confiança).
Se a origem do dinheiro pouco tem a ver com a "fábula do escambo" - onde quem pescasse mais peixe do que o necessário para si e seu grupo trocava este excesso com o de outra pessoa que tivesse plantado e colhido mais milho ou o que fosse - ela também não pode ser reduzida ao "mito da dívida primordial" - onde somos eternos devedores do deus que nos deu a vida, e assim respeitamos a autoridade de quem define o que vai ser a moeda e qual o seu valor, como será o Estado adiante na história.
Longe desses modelos idealizados encontramos indícios do dinheiro mercadoria mais ligado ao comércio e iniciativas privadas e do dinheiro signo mais ligado ao poder central (dos "pagé" das tribos, passando pelos administradores dos templos egípcios, aos Bancos Centrais na atualidade) coexistindo ao longo da história. Se se tem a impressão de que o dinheiro foi desde sempre mercadoria (antes dos metais, o gado ou o sal, por exemplo) é porque os metais brilharam excessivamente no passado da humanidade. Ou seja, o dinheiro amoedado em metais como cobre, prata ou ouro, é mais fácil de encontrar que os registros de operações de débito e crédito em frágeis pergaminhos. Contudo, descobertas como a Pedra da Roseta conseguiram mostrar como o dinheiro aparece, e desaparece, num simples num édito faraônico que ordena a anulação de certas dívidas. Isso não significa que a pesquisa sobre o dinheiro como notação tenha por base apenas esses comprovantes materiais; pelo contrário, os principais elementos para essa pesquisa são as palavras, os hábitos e costumes que duraram milênios (como a escravização por dívida ou seus impedimentos, bem como os impedimentos ou estímulos à prostituição), os documentos que provam as revoltas quando das crises de dívidas que super expropriavam as famílias e as festas quando ocorria o contrário, os grandes perdões onde se "quebravam as tábuas" onde eram registradas.
As primeiras formas de dinheiro nasceram dentro das comunidades mais que em suas franjas (onde uma comunidade se relaciona com outra) e por isso eram mais ligadas aos registros de dívida que a algo comercializado. As provas mais cabais remetem à civilização Suméria em torno de 3500 a.C. A prata física era utilizada, mas como unidade a qual se conferia um equivalente em produto (um "siclo de prata equivalia a um bushel de cevada",[7] seu valor não emergia de transações comerciais entre os sumérios todos em livres mercados, mas da necessidade da burocracia (sacerdotes, oficias, administradores de templos e palácios) de "rastrear os recursos e transferir itens entre departamentos".[8] Essa prata não era cunhada e sequer circulava muito, sendo a maioria das transações meramente registradas, e canceladas, com todo tipo de bens.
Um registro importante das formas antigas de dinheiro são as revoltas quando do acúmulo de dívidas e as festas quando de seu cancelamento. As revoltas se seguiam o elevado endividamento forçava à escravidão "excessiva" (quando um grupo familiar perdia muitos membros e por muito tempo impedindo a sua reprodução material). A destruição dos registros das dívidas eram comum nas civilizações antigas como ainda ocorre com frequência nos nossos dias, apenas no passado era feita de modo mais ritual, em grandes festas onde se "quebravam as tábuas", ou se queimavam os papiros e pergaminhos.[9] Entre documentos históricos onde se encontram referências as cerimônias de perdão de dívidas está a Bíblia, que vai além pois refere-se também à sua institucionalização - ou estipulação de regras de como, quando ou onde deveria ocorrer. É o caso da Lei do Jubileu, citada em Deuteronômio e Levítico, que mencionam que a cada 7 anos todas as dívidas deveriam ser canceladas, e a cada 50 propriedades deveriam ser devolvidas, bem como pessoas deveriam ser libertadas.
Embora seja arriscado dividir a história como se em cada fase reinassem incólumes tais ou tais formas de vida, relações de produção e valores culturais, ainda assim uma divisão um tanto grosseira pode ajudar a ver como o dinheiro mudou de forma e de conteúdo. No geral, nas sociedades sem mercado e Estado desenvolvidos, o dinheiro era, como tudo mais, muito ligado à pessoalidade (e aos status das pessoas dentro de uma comunidade) e não a impessoalidade (e a coisificação das próprias pessoas) como nas sociedades modernas. O dinheiro-dívida frequentemente marcava o que não podia ser pago com coisas, e por isso mantinha as pessoas ligadas por laços inquebráveis, sendo por isso chamadas dívidas de sangue.Pode-se dizer que o dinheiro antigo marcava relações de inequivalência entre pessoas, enquanto o dinheiro moderno marca a relação de equivalência entre coisas.
Também pode ajudar a pensar a história passada do dinheiro a partir de um ponto de ruptura crucial com o passado comunal, que foi a centralização do poder nos grandes impérios que se organizaram militarmente, e que se expandiram graças a dinâmica entre militarização, escravização e taxação que tinha na moeda cunhada seu principal dispositivo operacional. Estes impérios, que dão as bases para a civilização moderna no Oriente e no Ocidente, são os da Lídia (Grécia), da Índia e da China.
Entre os anos de 800 a.C. e 600 a.C. teve lugar em três diferentes regiões do globo - nos territórios mais ou menos correspondentes às atuais Grécia, Índia e China - fenômenos comuns ainda que com distintas causas e consequências), são eles: fratura política seguida de caos social, emergência de novas ideias/religiões de base mais popular, exércitos profissionais e cunhagem de moeda por governos centralizados. Há pistas na história[8] de que tudo isso esteve relacionado às crises devido ao super endividamento das populações com os segmentos proprietários que foram concentrando terras e forçando a uma cada vez maior servidão por dívida. Sem condições de sobrevivência, de acesso à terra e aos familiares para a trabalharem, indivíduos saiam das comunidades e se organizavam militarmente, abandonando de vez os laços que estavam a perder de sangue e posição. O poder das famílias aristocráticas proprietárias será confrontado por um novo movimento que adentrará ao Estado,  fazendo uso do poder dos exércitos organizados e da moeda cunhada. A facilitação do acesso à moeda possibilitará à população a quitação de dívidas sem perdas de membros e terras. A cunhagem punha fim assim a servidão por dívida, viabilizava o campesinato, o exército do Império, e mesmo alguma participação popular no poder. Tudo isso, por sua vez, às custas da escravização dos vencidos pelo Império militar.[8]
As moedas cunhadas, com brasões dos governantes e símbolos numéricos, passaram a circular muito mais que no passado pois os governos exigiam que elas fossem utilizadas nos pagamentos de impostos (como ocorre também nos nossos dias, dando aos governos o poder de senhoriagem, ou de produtor monopolista da moeda que todos precisam usar). Que sua importância tivesse a ver com ser essa moeda de ouro ou a prata (metais raros e valiosos) não é o mais provável, pois primeiro essa monetização das trocas tem de "pegar" (ser aceita por todos) para então a demanda por metais crescer. O mais provável é que primeiro venha o exército gastando os soldos (pagamento dos soldados) recebidos pelo Estado por onde que que ande a conquistar/invadir; depois vem a aceitação dessa moeda estatal por parte daqueles que estão sendo subjugados a pagarem impostos e a fazerem-no nesta moeda; aí então vem a necessidade de material físico para a cunhagem, que serão os metais obtidos graças a escravização dos invadidos.[10]
Pesquisadores como Karl Polanyi, Mitchell Innes, Michael Hudson, entre outros, destacam essa proximidade entre escravismo, militarismo e moeda estatal, como estando por trás do fortalecimento de uma economia mercantil e impessoal no Mediterrâneo, China e Índia - uma vez que a demanda generalizada por esta moeda cujo curso foi forçado pelo poder (bélico inclusive) do Estado fazia com que as pessoas se envolvessem cada vez mais em transações monetárias. Se, por um lado, isso trouxe um novo vigor à vida econômica, trouxe novamente, ainda que de forma diferente, as crises devidos ao acúmulo de dívidas que, não perdoadas, implicavam perda de condições objetivas de vida. Sendo assim, as revoltas terão lugar novamente e cada um dos grandes Impérios perecerá cada um ao seu modo. No caso europeu, o fim do Império Romano será também o fim da moeda estatal, ainda que ela continuasse a circular por séculos.
Falando exclusivamente da Europa - berço da civilização moderna Ocidental e do Capitalismo como modo de produção que transformará o dinheiro em capital - pode-se dizer que o dinheiro amoedado de um governo forte central desapareceu durante toda a Idade Média. De fato, mesmo que moedas de ouro e prata romanas ainda circulassem, não eram usadas no cotidiano nem das populações citadinas (bastante diminuídas nesse período) nem das numerosas populações rurais; e mesmo nas transações mais robustas do comércio de curta distância e longa distância o ouro, prata e cobre utilizados valiam como tais e não pelo brasão que ostentavam. Assim, do mesmo modo como o poder esfacelou-se no mundo feudal, as moedas circulantes eram inúmeras e, na prática, foram subjugadas novamente pelo dinheiro de crédito que servia muito bem às comunidades reduzidas, mais uma vez, aos seus laços de proximidade e consanguinidade.A decomposição feudal foi lenta e se deu principalmente: pela transformação do trabalho obrigado dos servos nas terras de domínio dos senhores em prestações pagas em gêneros ou dinheiro, o que estimulou a retomada do cálculo pessoal e a luta pelo trabalho livre e propriedades camponesa; e pela retomada do comércio, reativação das feiras, do artesanato no âmbito das corporações citadinas, e da vida urbana em geral.[11] O dinheiro ligado ao comércio, e a uma mercadoria em particular, volta à cena. Mas esta retomada do comércio ocorre numa sociedade em revolução interna, tendo lugar algo como um capitalismo mercantil que dribla as proibições morais, religiosas e legais (que proibiam a chamada usura, ou cobrança de juros dos endividados) que sempre rondaram o dinheiro e ele passa a ser negócio como nunca antes. Nesse caso, certificados em papel de depósitos de itens de comércio nos portos (como cargas de especiarias ou escravos), de jóias e bens de luxo em lojas de penhores, de quantidades de ouro e prata em casas que os seguravam, passaram a ser aceitos/negociados em bancas de trocas que deram origem aos bancos. Além disso, essa mobilização de diversos tipo de moedas privadas acontece ao mesmo tempo que o renascimento dos Estados. Desta vez, pequenos (se comparados a dimensão territorial dos antigos impérios) estados nacionais se formam quase simultaneamente na Europa, menos ligados a etnias, castas e exércitos e mais ligados a classes e seus negócios (exércitos ficam, mas sob mando dos poderes/burocracias do Estado). A moeda estatal passa a ser novamente central. Um governante (rei ou presidente) não pode precisar de um dinheiro que não emita, não pode dever a um banqueiro privado qualquer, mas ao seu próprio (o Banco Central Holandês e o Inglês nascem já no século XVII).
Os Estados modernos passaram a cunhar novamente moeda em metal - em ouro as de maior valor, em prata e cobre as de valores menores -  mas rapidamente passaram a emitir papel-moeda que, a depender da época, sequer necessitavam de assegurar um lastro em metal (como o fez a Suécia em 1660). O importante era produzir a moeda nacional como instituição fundamental da nação. Essa moeda, desde então, carrega acima de tudo um nome, e as moedas privadas (bancárias) devem ser nomeadas pelo mesmo, e reguladas pela autoridade central.
A partir do século XVII, o dinheiro que já havia existido como registros regulados por regras sociais há 5000 anos, volta a se parecer menos como o material de que é feito e mais as regras que o criam, o fazem circular, o associam a diferentes formas de contratos, apostas e outras operações internas e externas aos países, e por fim as regras cambiais que dizem respeito as trocas das moedas nacionais entre si.
Na passagem do dinheiro-mercadoria ao dinheiro-dívida de papel, houve muito tumulto público, e discussão teórica, sobre a insegurança para a reprodução da vida econômica graças aos poderes que bancos, públicos e privados, ganhavam com isso.Os temores quanto aos poderes dos criadores de moeda por vezes faziam com que economistas, políticos e cidadãos defendessem a manutenção de lastro ou conversibilidade entre as notas de papel e os metais. Os economistas mantiveram embates vigorosos desde o século XV, entre Bulionistas e Antibulonionistas, Metalistas e Chartalistas, Currency School e Banking School, até chegar ao século XX com os debates entre Monetaristas e Institucionalistas.[12] Em geral, todas elas têm a ver com as desconfianças em torno dos poderes da moeda fiduciária - todo papel ou título público cuja aceitabilidade se dá apenas por fidúcia, credibilidade, e não por ser conversível a algum metal (ouro, prata) com valor intrínseco.
É fato que governos e bancos podem abusar de seus poderes de criação de moeda fiduciária, e, pior, podem produzi-la em excesso justamente quando não é o caso - como os bancos nas fases de prosperidade a fim de lucrar mais, o que os fazem dar créditos a negócios insólitos ou mesmo dirigir recursos novos a riqueza já existentes, criando as famosas bolhas -; ou quando os governos fazem o oposto e reduzem o gasto/produção de moeda nas fases de baixa da economia, o que só reforça esse movimento levando a crise e ao desemprego. Mas a melhor gestão da moeda não é conseguida atrelando-a a um bem escasso qualquer - seja o ouro, escasso por natureza, seja o Bitcoin, escasso por planejamento de seus criadores.
De fato, é impossível retirar do dinheiro sua dimensão política. E com o dinheiro de papel (e o eletrônico depois dele) isso fica mais evidente. Se em determinados períodos da história, um ou mais países mantiveram relações de paridade e conversibilidade com algum metal (como durante vários períodos da história europeia e em particular durante a vigência do padrão Libra-ouro na Inglaterra do final do século XIX e início do XX), isso se deveu a um estratagema para se manter a confiança necessária na moeda, e, por tabela, no Estado. Estratagema esse que previa também a sua suspensão, caso necessário (como também ocorrera na Inglaterra com a Lei de Restrição Bancária de 1797, quando as trocas de notas por ouro foram proibidas por mais de 20 anos). Mas mesmo que a conversibilidade em ouro seja apenas um artifício usado numa ou outra época (o foi novamente no padrão Dólar-ouro no pós segunda -guerra), ela imprimiu uma marca no imaginário popular que, ainda hoje, cidadãos desconfiados dos super poderes do Estado e/ou bancos e todo o sistema financeiro, pleiteiam-no como necessário e suficiente para os regular.
Essa atitude é não só compreensível quanto necessária, o problema é que a crença em algo natural que se imponha aos embates político-econômicos pode eximir os cidadãos de uma ação política mais pertinente de controle desses poderes.[13][14] Como dizia Lerner, a adoção da conversibilidade, só faz com que o Estado se ausente da prerrogativa de regular a moeda.[15] Do mesmo modo, a crença em regras draconianas de controle do gasto público é algo popular, uma vez que parece às populações um meio de controle de políticos e governos auto-interessados, quando o ideal seria controlá-los nos embates orçamentários, pela definição do que deverão ser os gastos e pelo controle de sua realização.
Em resumo, ao longo de sua longa história o dinheiro foi se tornando cada vez mais o que era em sua essência, algo virtual. Ou seja, o dinheiro vai se estabelecendo historicamente como aquilo que sempre foi, uma ideia ou significado abstrato que é incorporado num significante concreto (as moedas do que quer que seja). Pode-se dizer assim que o dinheiro é uma ideia. Uma ideia abstrata que se concretiza pelas regras que a fazem operar entre os humanos. Mas acima de tudo, o que essa ideia carrega são os compromissos concretos assumidos entre os humanos no tempo. Assim, como dizia Keynes[16] na sua Teoria Geral do Emprego, do Juro e da Moeda, o dinheiro é elo entre o presente e o futuro, e isso não apenas porque simboliza apostas privadas sobre valorização futura deste ou daquele ativo ou itens de riqueza, mas porque compromete os homens a produzi-los.
Atualmente, as novas tecnologias de transmissão dos pagamentos em meio eletrônico tornaram os significantes físicos do dinheiro desnecessários. Fica cada vez mais evidente que dinheiro é representante da riqueza criada ou por criar (daí crédito e dívida) e nesse sentido ele não tem nada a ver com algo que se entesoura, como as moedas ou mesmo muitas das cripto-moedas que se tornaram ativos especulativos. Afinal, algo que se entesoura e contra o que se especula (se fazem apostas) é um ativo financeiro.
A complexidade em entender o que é, e o que pode ser, o dinheiro na atualidade deriva em grande parte da complexidade do sistema financeiro atual. Os mercados de futuros que tornam bens materiais objetos de apostas, elas mesmas tornadas papeis negociados, segurados, e nos quais são baseados outros papeis, os derivativos em geral baseados em ações, créditos, operações sem qualquer materialidade, foi o que levou a dimensão virtual do dinheiro às alturas. O potencial crítico desse sistema é (como sempre foi no passado) o endividamento excessivo. Mas não o dos Estados, como em propagandeiam os interesses dominantes sempre que não sejam eles os beneficiados pelas emissões públicas. Esta dívida, cujo passivo são os ativos das pessoas e empresas, é bem vinda, e tem, na prática, nos anos finais do século passado e neste, salvado a economia mundial. O endividamento que preocupa é o dos mais frágeis cujo efeito é a perda de bens reais, sejam eles cidadãos, sejam países (vide o caso da Grécia na crise do Euro).
À complexidade atinente ao sistema financeiro, pode-se acrescentar a complexidade de uma economia-mundo bastante integrada, ainda que uma integração marcada pela desigualdade, para explicar porque é tão difícil entender hoje o dinheiro. Nesse ambiente, é curioso como alguns países têm vivenciado a transformação de créditos de telefone em moeda.[17]
No que diz respeito à teoria econômica o renascimento das teses de Keynes, Lerner, Minsky e outros pelos autores da chamada Teoria Monetária Moderna, Warren Mosler, Randall Wray, Bill Mitchell, Sthephanie Kelton  , tem trazido novo fôlego às discussões, particularmente por explicarem como o discurso neoliberal fundado na necessidade de diminuir o Estado, entre outras coisas criminalizando a dívida pública e insistindo nas antigas teses da Teoria Quantitativa da Moeda do excesso de dinheiro em circulação como causa da inflação, não encontra mais sustentação desde a crise de 2008. No Brasil, duas publicações recentes merecem destaque: o livro de André Lara Rezende "Juros, moeda e ortodoxia - Teorias monetárias e controvérsias políticas" e o livro de Enzo Gerioni, David Deccache, Julia Ozzimolo, Daniel Conceição, e Fabiano Dalto, "Teoria monetária moderna: A chave para uma economia a serviço das pessoas".
O dinheiro influencia a arte de diversas formas.
No cinema, alguns filmes de bancos, sistema financeiro, crises bancárias, como por exemplo:
Na música, podem-se destacar alguns exemplos:
Organização dos Países Exportadores de Petróleo (OPEP ou, pelo seu nome em inglês, OPEC) é uma organização intergovernamental de 13 nações, fundada em 15 de setembro de 1960 em Bagdá pelos cinco membros fundadores (Irã, Iraque, Kuwait, Arábia Saudita e Venezuela), com sede desde 1965 em Viena, na Áustria. Em setembro de 2018, os então 14 países membros representavam 44% da produção global de petróleo e 81,5% das reservas de petróleo "comprovadas" do mundo, dando à OPEP uma grande influência nos preços globais de petróleo, previamente determinados pelos chamados agrupamento "Sete Irmãs" de empresas multinacionais de petróleo.
A missão declarada da organização é "coordenar e unificar as políticas de petróleo de seus países membros e garantir a estabilização dos mercados de petróleo, a fim de garantir um fornecimento eficiente, econômico e regular deste recurso aos consumidores, uma renda estável aos produtores e um retorno justo de capital para aqueles que investem na indústria petrolífera".[2] A organização também é uma provedora significativa de informações sobre o mercado internacional de petróleo. Os atuais membros da OPEP são os seguintes: Argélia, Angola, Guiné Equatorial, Gabão, Irã, Iraque, Kuwait, Líbia, Nigéria, República do Congo, Arábia Saudita (líder de facto), Emirados Árabes Unidos e Venezuela. Equador, Indonésia e Catar são ex-membros.
A formação da OPEP marcou um ponto decisivo para a soberania nacional sobre os recursos naturais e as decisões da organização passaram a desempenhar um papel de destaque no mercado global de petróleo e nas relações internacionais. O efeito pode ser particularmente forte quando guerras ou distúrbios civis levam a interrupções prolongadas no fornecimento. Na década de 1970, as restrições na produção de petróleo levaram a um aumento dramático nos preços e na receita e riqueza da OPEP, com consequências duradouras e de longo alcance para a economia global. Na década de 1980, a OPEP começou a estabelecer metas de produção para seus países membros; geralmente, quando as metas são reduzidas, os preços do petróleo aumentam. Isso ocorreu nas decisões de 2008 e 2016 da organização de reduzir o excesso de oferta.
Os economistas costumam citar a OPEP como um exemplo de cartel que coopera para reduzir a concorrência no mercado, mas cujas consultas são protegidas pela doutrina da imunidade estatal sob o direito internacional. Em dezembro de 2014, "a OPEP e os petroleiros" ficaram em terceiro lugar na lista do Lloyd's das "100 mais influentes do setor de transporte marítimo".[3] No entanto, a influência da OPEP no comércio internacional é periodicamente desafiada pela expansão de fontes de energia que não são da OPEP e pela tentação recorrente de países da organização de exceder as metas de produção e buscar interesses próprios conflitantes.
Foi criada em 14 de setembro de 1960 como uma forma dos países produtores de petróleo se fortalecerem frente às empresas compradoras do produto, em sua grande maioria pertencentes aos Estados Unidos, Inglaterra e Países Baixos, que exigiam cada vez mais uma redução maior nos preços do petróleo.
A persistência do Conflito israelo-árabe forçou a OPEP a tomar atitudes drásticas. Logo após a Guerra dos Seis Dias em 1967, os membros árabes da OPEP fundaram a Organização dos Países Árabes Exportadores de Petróleo com o propósito de centralizar a política de actuação e exercer pressão no Ocidente, que apoiava Israel. O Egito e a Síria, embora não fossem países exportadores usuais de petróleo, passaram a fazer parte da nova organização. Em 1973, a Guerra do Yom Kippur alarmou a opinião pública árabe. Furiosos com o facto de que o fornecimento de petróleo havia permitido que Israel resistisse às forças egípcias e sírias, o mundo árabe impôs um embargo contra Estados Unidos, Europa Ocidental e Japão.[carece de fontes?]
O conflito israelo-árabe provocou uma crise. Os membros da OPEP pararam de exportar petróleo para o Ocidente, fazendo com que tivessem que reduzir os gastos anuais com energia, aumentar os preços, e ainda vender mercadorias com preço inflacionado para os países do Terceiro Mundo produtores de petróleo. Isto foi agravado pelo Xá do Irã Reza Pahlavi, que era o segundo maior exportador de petróleo mundial e aliado mais próximo dos Estados Unidos na época. É claro que [o preço do petróleo] vai aumentar, disse ele ao New York Times em 1973. Certamente, e como...; Vocês [países do Ocidente] aumentaram o preço do trigo vendido a nós em 300%, o mesmo ocorreu com o açúcar e com o cimento...; Vocês compram nosso petróleo bruto e nos vendem ele de volta beneficiado na forma de produtos petroquimícos, por uma centena de vezes o preço que vocês o compraram...; Seria no mínimo justo que, daqui para frente, vocês paguem mais pelo petróleo. Poderíamos dizer umas 15 vezes mais.[carece de fontes?]
Em 21 de dezembro de 1975, Ahmed Zaki Yamani e os outros ministros do petróleo dos membros da OPEP foram tomados como reféns por uma equipe de seis pessoas liderada pelo terrorista Carlos, o Chacal (que incluía Gabriele Kröcher-Tiedemann e Hans-Joachim Klein), em Viena, Áustria, onde os ministros estavam participando de uma reunião na sede da OPEP. Carlos planejava aparecer na conferência à força e sequestrar todos os onze ministros do petróleo na reunião e mantê-los para resgate, com exceção de Ahmed Zaki Yamani e do Iraniano Jamshid Amuzegar, que eram para ser executados. Carlos liderou sua equipe de seis pessoas e passou por dois policiais no pátio do edifício e foram até o primeiro andar, onde um policial, um guarda de segurança iraquiano à paisana e um jovem economista da Líbia foram mortos a tiros.[4] Quando Carlos entrou na sala de conferências e disparou tiros no teto, os delegados se abaixaram debaixo da mesa. Os terroristas procuraram por Ahmed Zaki Yamani e depois dividiram os 63 reféns em grupos. Delegados de países amigos foram levados em direção à porta, "neutros" foram colocados no centro da sala e os 'inimigos' foram colocadas ao longo da parede de trás, ao lado de uma pilha de explosivos. Este último grupo incluía os representantes da Arábia Saudita, Irã, Qatar e os Emirados Árabes Unidos. Carlos exigiu um autocarro para ser fornecido para levar o seu grupo e os reféns para o aeroporto, onde um avião DC-9 com uma tripulação estaria esperando. Nesse meio tempo, Carlos informou Ahmed Zaki Yamani sobre seu plano de voar para Aden, onde Yamani e o ministro iraniano seriam mortos.[4]
O autocarro foi fornecido na manhã seguinte às 6 horas e 40 minutos, conforme solicitado, e 42 reféns entraram e foram levados para o aeroporto. O grupo foi embarcado no avião logo após as 9 horas e explosivos foram colocados debaixo do banco de Yamani. O avião primeiro parou em Argel, onde Carlos saiu do avião para se encontrar com o ministro das Relações Exteriores argeliano. Todos os 30 reféns não árabes foram libertados, exceto Amuzegar. O avião reabastecido partiu para Trípoli, onde houve problemas na aquisição de outro avião, como tinha sido planejado. Carlos decidiu, ao invés disso, voltar a Argel e se mudar para um Boeing 707, um avião grande o suficiente para voar para Bagdad sem parar. Mais dez reféns foram libertados antes de sair. Com apenas 10 reféns restantes, o Boeing 707 partiu para Argel e chegou à 3h40. Depois de sair do avião para se encontrar com os argelinos, Carlos conversou com seus colegas na cabine da frente do avião e, em seguida, disse a Yamani e Amouzegar que seriam liberados ao meio-dia. Carlos saiu do avião pela segunda vez e voltou depois de duas horas.[4]
Nesta segunda reunião, acredita-se que Carlos teve uma conversa telefônica com o presidente argelino Houari Boumédiène, que informou a Carlos que as mortes dos ministros do petróleo resultariam em um ataque ao avião. A biografia de Yamani sugere que os argelinos tinham usado um dispositivo de escuta secreta na parte da frente da aeronave para ouvir a conversa entre os terroristas, e descobriram que Carlos de fato ainda planejava matar os dois ministros do petróleo. Boumédienne também deve ter oferecido asilo a Carlos neste momento e, possivelmente, a compensação financeira por não completar sua missão. No retorno ao avião, Carlos estava em pé diante de Yamani e Amuzegar e expressou seu pesar por não ser capaz de matá-los. Ele então disse aos reféns que ele e seus companheiros deixariam o avião, e que então todos estariam livres. Depois de esperar os terroristas para sair, Yamani e os outros nove reféns seguiram e foram levados para o aeroporto pelo ministro das Relações Exteriores da Argélia Abdelaziz Bouteflika. Os terroristas estavam presentes na sala ao lado e Khalid, o palestino, pediu para falar com Yamani. Quando sua mão pegou o casaco, Khalid estava cercado por guardas e uma arma foi encontrada escondida em um coldre. Algum tempo depois do ataque foi revelado por cúmplices de Carlos que a operação foi comandada por Wadi Haddad, um terrorista palestino e fundador da Frente Popular para a Libertação da Palestina. Alegou-se também que a ideia e o financiamento veio de um presidente árabe, sendo que muitos apostam que seja Muammar al-Gaddafi.[4] Nos anos seguintes à invasão da OPEP, Bassam Abu Sharif e Klein afirmaram que Carlos tinha recebido uma grande soma de dinheiro em troca da libertação dos reféns árabes e a manteve para seu uso pessoal. Ainda há alguma incerteza quanto à quantia recebida, mas acredita-se estar entre 20 a 50 milhões de dólares. A fonte do dinheiro também é incerta, mas, segundo Klein, veio de "um presidente árabe". Carlos disse mais tarde a seus advogados que o dinheiro foi pago pelos sauditas em nome dos iranianos, e foi "desviado no caminho e se perdeu pela Revolução".[4]
Em resposta a uma onda de nacionalizações de petróleo e aos altos preços da década de 1970, os países industrializados tomaram medidas para reduzir sua dependência do petróleo da OPEP, especialmente depois que os preços atingiram novos picos, aproximando-se de US$ 40/bbl em 1979-1980[5][6] quando a Revolução Iraniana e a Guerra Irã-Iraque interromperam a estabilidade regional e o suprimento de petróleo. As concessionárias de energia elétrica em todo o mundo passaram do petróleo para o carvão, o gás natural ou a energia nuclear;[7] os governos nacionais iniciaram programas de pesquisa de bilhões de dólares para desenvolver alternativas ao petróleo;[8][9] e a exploração comercial desenvolveu os principais campos de petróleo fora da OPEP, principalmente na Sibéria, no Alasca, no Mar do Norte e no Golfo do México.[10] Em 1986, a demanda mundial diária de petróleo caiu 5 milhões de barris, a produção não OPEP aumentou uma quantidade ainda maior[11] e a participação de mercado da OPEP caiu de aproximadamente 50% em 1979 para menos de 30% em 1985.[12] Ilustrando os prazos voláteis plurianuais dos ciclos típicos de mercado para os recursos naturais, o resultado foi um declínio de seis anos no preço do petróleo, que culminou com a queda de mais da metade do preço somente em 1986.[13] Como um analista de petróleo resumiu sucintamente: "Quando o preço de algo tão essencial quanto o petróleo dispara, a humanidade faz duas coisas: encontra mais e encontra maneiras de usá-lo menos".[12]
Para combater a queda na receita das vendas de petróleo, a Arábia Saudita, em 1982, pressionou a OPEP a impor cotas de produção nacional auditadas na tentativa de limitar a produção e aumentar os preços. Quando outras nações da OPEP não cumpriram, a Arábia Saudita cortou sua própria produção de 10 milhões de barris diários em 1979-1981 para apenas um terço desse nível em 1985. Quando isso se mostrou ineficaz, os sauditas inverteram o curso e inundaram o mercado com petróleo barato, fazendo com que os preços caíssem abaixo de US$ 10/bbl e os produtores de alto custo se tornaram inúteis.[11][14]:127–128,136–137 Diante do aumento das dificuldades econômicas (o que acabou contribuindo para o colapso do bloco soviético em 1989),[15] os exportadores de petróleo "livres" que anteriormente não haviam cumprido os acordos da OPEP finalmente começaram a limitar a produção para elevar os preços, com base em cotas nacionais minuciosamente negociadas que buscavam equilibrar as importações e os critérios econômicos do petróleo desde 1986.[11][16] (Em seus territórios, os governos nacionais dos membros da OPEP são capazes de impor limites de produção a empresas de petróleo estatais e privadas.)[17] Geralmente, quando as metas de produção da OPEP são reduzidas, os preços do petróleo aumentam.[18]
Antes da invasão do Kuwait em agosto de 1990, o presidente iraquiano Saddam Hussein estava pressionando a OPEP a acabar com a superprodução e a elevar os preços do petróleo, a fim de ajudar financeiramente os membros da organização e acelerar a reconstrução após a Guerra do Irã-Iraque de 1980–1988.[19] Mas essas duas guerras iraquianas contra colegas fundadores da OPEP marcaram um ponto baixo na coesão da organização e os preços do petróleo diminuíram rapidamente após as interrupções no fornecimento a curto prazo. Os ataques da Al Qaeda em 11 de setembro de 2001 contra os Estados Unidos e a invasão do Iraque em 2003 tiveram impactos ainda mais breves no preço do petróleo, à medida que a Arábia Saudita e outros exportadores cooperaram novamente para manter o mundo adequadamente abastecido.[20]
Na década de 1990, a OPEP perdeu seus dois membros mais novos, que se juntaram em meados da década de 1970. O Equador retirou-se em dezembro de 1992, porque não estava disposto a pagar a taxa anual de associação de 2 milhões de dólares e achava que precisava produzir mais petróleo do que o permitido pela cota da OPEP,[21] embora tenha voltado a entrar em outubro de 2007. Preocupações semelhantes surgiram quando o Gabão suspendeu a associação em janeiro de 1995;[22] o país voltou a participar em julho de 2016.[23] O Iraque permaneceu membro da OPEP desde a fundação da organização, mas a produção iraquiana não fez parte dos acordos de cotas da OPEP de 1998 a 2016, devido às difíceis dificuldades políticas do país.[24]
A menor demanda desencadeada pela crise financeira asiática de 1997-1998 viu o preço do petróleo voltar aos níveis de 1986. Depois que o petróleo caiu para cerca de US$ 10/bbl, a diplomacia conjunta de México e Noruega conseguiu uma desaceleração gradual da produção de petróleo pela OPEP.[25] Depois que os preços caíram novamente em novembro de 2001, OPEP, Noruega, México, Rússia, Omã e Angola concordaram em cortar a produção em 1º de janeiro de 2002 por 6 meses. A OPEP contribuiu com 1,5 milhão de barris por dia (mbpd) para os aproximadamente 2 mbpd de cortes anunciados.[14]
Em junho de 2003, a Agência Internacional de Energia (AIE) e a OPEP realizaram seu primeiro workshop conjunto sobre questões energéticas. Eles continuaram a se reunir regularmente desde então "para entender melhor coletivamente tendências, análises e pontos de vista e promover a transparência e a previsibilidade do mercado".[26]
Em janeiro de 2020, a OPEP contava com 13 países membros: cinco no Oriente Médio (Ásia Ocidental), sete na África e um na América do Sul. De acordo com a Administração de Informação de Energia dos EUA (EIA), a taxa combinada de produção de petróleo da OPEP (incluindo gás condensado) representou 44% do total mundial em 2016.[27]
A aprovação de um novo país membro requer o acordo de três quartos dos membros existentes da OPEP, incluindo todos os cinco fundadores.[138] Em outubro de 2015, o Sudão apresentou formalmente um pedido de adesão,[28] mas ainda não é membro.
Aborto ou interrupção da gravidez é a interrupção de uma gravidez resultante da remoção de um feto ou embrião antes de este ter a capacidade de sobreviver fora do útero. Um aborto que ocorra de forma espontânea denomina-se aborto espontâneo ou "interrupção involuntária da gravidez". Um aborto deliberado denomina-se "aborto induzido" ou "interrupção voluntária da gravidez". O termo "aborto", de forma isolada, geralmente refere-se a abortos induzidos. Nos casos em que o feto já é capaz de sobreviver fora do útero, este procedimento denomina-se "interrupção tardia da gravidez".[1]
Quando são permitidos por lei, os abortos em países desenvolvidos são um dos procedimentos médicos mais seguros que existem.[2][3] Os métodos de aborto modernos usam  medicamentos ou cirurgia.[4] Durante o primeiro e segundo trimestres de gravidez, o fármaco mifepristona em associação com prostaglandina aparenta ter a mesma eficácia e segurança que a cirurgia.[4][5] Os contraceptivos, como a pílula ou dispositivos intrauterinos, podem ser usados imediatamente após um aborto.[5] Quando realizado de forma legal e em segurança, um aborto induzido não aumenta o risco de problemas físicos ou mentais a longo prazo.[6] Por outro lado, os abortos inseguros e clandestinos realizados por pessoas sem formação, com equipamento contaminado ou em instalações precárias são a causa de 47 000 mortes maternas e 5 milhões de admissões hospitalares por ano.[6][7]
Em todo o mundo são realizados 56 milhões de abortos por ano,[8] dos quais cerca de 45% são feitos de forma insegura.[9] Entre 2003 e 2008 a prevalência de abortos manteve-se estável,[10] depois de nas duas décadas anteriores ter vindo a diminuir à medida que mais famílias no mundo tinham acesso a planeamento familiar e contracepção.[11] A Organização Mundial de Saúde recomenda que todas as mulheres tenham acesso a abortos legais e seguros.[12] No entanto, em 2008 apenas cerca de 40% das mulheres em todo o mundo tinham acesso a abortos legais.[13] Os países que permitem o aborto têm diferentes limites no número máximo de semanas em que são permitidos.[13]
Ao longo da história, foi comum a prática de abortos com ervas medicinais, instrumentos aguçados, por via da força ou com outros métodos tradicionais.[14] A legislação e as perspetivas culturais e religiosas sobre o aborto diferem conforme a região do mundo. Em algumas regiões, o aborto só é legal em determinados casos, como violação, doenças congénitas, pobreza,[15] risco para a saúde da mãe ou incesto.[16] Em muitos locais existe debate social sobre as questões morais, éticas e legais do aborto.[17][18] Os grupos que se opõem ao aborto geralmente alegam que um embrião ou feto é um ser humano com direito à vida e comparam o aborto a um homicídio.[19][20] Os grupos que defendem a legalização do aborto geralmente alegam que a mulher tem o direito de decidir sobre o seu próprio corpo.[21]
"Aborto" ou "interrupção da gravidez" é a interrupção de uma gravidez pela remoção de um feto ou embrião antes de este ter a capacidade de sobreviver fora do útero. Um aborto que ocorra de forma espontânea denomina-se aborto espontâneo ou "interrupção involuntária da gravidez". Um aborto deliberado denomina-se "aborto induzido" ou "interrupção voluntária da gravidez". O termo "aborto", usado de forma isolada, geralmente refere-se a abortos induzidos. Nos casos em que o feto já é capaz de sobreviver fora do útero, este procedimento denomina-se "interrupção tardia da gravidez".[1]
O aborto induzido, também denominado aborto provocado ou interrupção voluntária da gravidez, é o aborto causado por uma ação humana deliberada. Ocorre pela ingestão de medicamentos ou por métodos mecânicos. O aborto induzido possui as seguintes subcategorias:
Um aborto espontâneo, ou interrupção involuntária da gravidez, é a expulsão não intencional de um embrião ou feto antes das 24 semanas de idade gestacional.[23] Uma gravidez que termine antes das 37 semanas de gestação é denominada parto pré-termo ou prematuro.[24] Quando o feto morre no útero após a data de viabilidade fetal ou durante o parto, denomina-se morte fetal.[25] Os partos prematuros e as mortes fetais geralmente não são considerados abortos espontâneos, embora os termos por vezes se sobreponham.[26]
Apenas 30 a 50% das gravidezes avançam para além do primeiro trimestre.[27] A grande maioria dos abortos espontâneos acontece antes da mulher se aperceber da gravidez[28] e muitas gravidezes são perdidas antes de os médicos detetarem um embrião.[29] Entre as gravidezes diagnosticadas, 15 a 30% terminam em aborto espontâneo, dependendo da idade e estado de saúde da grávida.[30] Cerca de 80% dos abortos espontâneos acontecem antes das primeiras doze semanas de gravidez.[31]
A causa mais comum de aborto espontâneo durante o primeiro trimestre de gravidez são anomalias cromossómicas no embrião ou no feto.[28][32] Esta causa é responsável por cerca de 50% dos abortos espontâneos.[33] Entre outras possíveis causas estão doenças vasculares como o lúpus, a diabetes, outros problemas hormonais, infeções e anomalias no útero.[32] O risco de aborto espontâneo aumenta em função da idade materna avançada (> 35 anos) e antecedentes de outros abortos espontâneos.[33] Um aborto espontâneo pode ainda ser causado por trauma acidental. No entanto, um trauma intencional ou indução deliberada de stresse na grávida é considerado aborto induzido.[34]
Quanto ao tempo de duração da gestação:
Também conhecido como aborto médico, químico ou não-cirúrgico, é o aborto induzido por administração de fármacos que provocam a interrupção da gravidez e a expulsão do embrião. O aborto farmacológico é aplicável apenas no primeiro trimestre da gravidez.
Tornou-se um método alternativo de aborto induzido com o surgimento no mercado dos análogos de prostaglandina no início dos anos 1970 e do antiprogestágeno mifepristona (RU-486) nos anos 1980.[35][36][37]
Os regimes de aborto mais comuns para o primeiro trimestre utilizam mifepristona em combinação com um análogo de prostaglandina (misoprostol) até 9 semanas de idade gestacional, metotrexato em combinação com um análogo de prostaglandina até 7 semanas de gestação, ou um análogo de prostaglandina isolado.[35] Os regimes de mifepristona–misoprostol funcionam mais rápido e são mais efetivos em idades gestacionais mais avançadas do que os regimes combinados de metotrexato-misoprostol, e os regimes combinados são mais efetivos que o uso do misoprostol isolado.[36]
Em abortos muito precoces, com até 7 semanas de gestação, o regime combinado de mifepristona-misoprostol é considerado mais efetivo do que o aborto cirúrgico (aspiração à vácuo).[38] Os regimes de aborto médico precoce que utilizam 200 mg de mifepristona, seguido por 800 mcg de misoprostol vaginal ou oral 24-48 horas após apresentam efetividade de 98% até as 9 semanas de idade gestacional.[39] Nos casos de falha do aborto farmacológico, é necessária a complementação do procedimento com o aborto cirúrgico.[40]
Os abortos farmacológicos precoces são responsáveis pela maioria dos abortos com menos de 9 semanas de gestação na Grã-Bretanha,[41][42] França,[43] Suíça,[44] e nos países nórdicos.[45] Nos Estados Unidos, o percentual de abortos farmacológicos precoces é menor.[46][47]
Regimes de aborto farmacológico usando mifepristona em combinação com um análogo de prostaglandina são os métodos mais comumente usados para abortos de segundo trimestre no Canada, maior parte da Europa, China e Índia,[37] ao contrário dos Estados Unidos, onde 96% dos abortos de segundo trimestre são realizados cirurgicamente com dilatação e esvaziamento uterino.[48]
Os procedimentos no primeiro trimestre podem geralmente ser realizados usando anestesia local, enquanto os realizados no segundo trimestre podem necessitar de sedação ou anestesia geral.[46]
No procedimento de aspiração uterina a vácuo o médico realiza vácuo no útero da gestante para remover o feto. São utilizados equipamentos manuais ou elétricos para a realização do vácuo. Geralmente são realizados em gestações de até doze semanas (primeiro trimestre).
A aspiração manual intrauterina (AMIU) consiste em uma aspiração cujo vácuo é criado manualmente utilizando-se uma cânula flexível acoplada a uma seringa. Foi desenvolvida para ser realizada ambulatorialmente sem anestesia geral, não necessitando ser realizada em bloco cirúrgico. Não é necessária a dilatação cervical.
O procedimento também pode ser utilizando um aparelho de vácuo eléctrico. Neste tipo de aspiração o conteúdo do útero é sugado pelo equipamento.
Ambos os procedimentos são considerados não-cirúrgicos e são realizados em cerca de dez minutos. São eficazes e seguros, pois apresentam um baixo risco para a mulher (0,5% de casos de infecção).
Em gestações mais avançadas, nas quais o material a ser removido do útero é muito volumoso, recorre-se à curetagem. Ao contrário da aspiração uterina à vácuo, que pode ser realizada em consultórios ou clínicas, a dilatação e curetagem é um procedimento cirúrgico, devendo ser realizado em um hospital com bloco cirúrgico. Inicialmente o médico alarga o colo do útero da paciente com dilatadores, para permitir a passagem da cureta a seguir. A cureta é um instrumento cirúrgico cortante, em forma de colher, que é introduzida no útero para realizar a raspagem. Servindo-se da cureta, o médico retira todo o conteúdo uterino, incluindo o endométrio.
Uma das principais complicações da curetagem é a perfuração uterina causada pela cureta.
Evita-se a realização da curetagem uterina em gestações com mais de 12-16 semanas sem antes realizar a expulsão fetal.
O procedimento de curetagem é aplicável ainda no começo do segundo trimestre, mas se não for possível terá de recorrer-se a métodos como a dilatação e evacuação. Neste procedimento o médico promove primeiro a dilatação cervical (um dia antes).
Na intervenção que é feita sob anestesia é inserido um aparelho cirúrgico na vagina para cortar o material intra-uterino em pedaços, e retirá-los de dentro do útero. No final é feita a aspiração. O feto é remontado no exterior para garantir que não há nenhum pedaço no interior do útero que poderia levar a infecção séria. Em raríssimas situações (0,17% das IVGs realizadas nos Estados Unidos em 2000) o feto é removido intacto.
A eliminação ou expulsão fetal geralmente é reservada para gestações com mais de doze semanas. Consiste em forçar prematuramente o trabalho de parto com o uso do análogo de prostaglandina misoprostol. Pode-se associar o uso de ocitocina ou injeção no líquido amniótico de soluções hipertônicas com solução salina ou ureia.
Após a expulsão fetal, pode ser necessária a realização de curetagem.
O aborto por esvaziamento craniano intrauterino (ECI), também conhecido como aborto com "nascimento parcial", é uma técnica utilizada para provocar o aborto quando a gravidez está em estágio avançado, entre 20 e 26 semanas (cinco meses a seis meses e meio), suas indicações mais comuns são: morte do feto, risco de morte para a mãe, risco para a saúde da mãe e más formações fetais que inviabilizem a vida extra-uterina.[49] Guiado por ultrassom, o médico segura a perna do feto com um fórceps, puxa-o para o canal vaginal, e então retira o feto do útero, com exceção da cabeça.
Faz então uma incisão na nuca, inserindo depois um cateter para sugar o cérebro do feto e então o retira por inteiro do corpo da mãe.  Em alguns países, essa prática é proibida em todos os casos, sendo considerada homicídio e punida severamente.[50][51] Esta técnica tem sido alvo de intensas polêmicas nos Estados Unidos.
Em 2003, sua prática foi proibida em todo o país, gerando revoltas de movimentos pró-aborto.[52]
No passado, diversas ervas já foram consideradas portadoras de propriedades abortivas e foram usadas na medicina popular.[53] No entanto, o uso de ervas com a intenção abortiva pode causar diversos efeitos adversos graves e até mesmo letais, tanto para a mãe quanto para o feto, e não é recomendado pelos médicos.[54]
O aborto, às vezes, é tentado através de trauma no abdômen. O grau da força, se intensa, pode causar diversas lesões internas graves sem necessariamente induzir com sucesso a perda fetal.[55] No Sudeste da Ásia, há uma tradição antiga de se tentar o aborto através de forte massagem abdominal.[56]
Métodos utilizados em abortos autoinduzidos não seguros incluem o uso incorreto de misoprostol e a inserção de materiais não cirúrgicos como agulhas e prendedores de roupas no útero. A utilização destes métodos não seguros raramente é observada em países desenvolvidos, onde o aborto cirúrgico é legal e disponível.[57]
Os riscos para a saúde envolvidos no aborto induzido dependem de o procedimento ser realizado com ou sem segurança.
Os abortos legais realizados nos países desenvolvidos estão entre os procedimentos mais seguros na medicina.[2][58] Nos Estados Unidos, a taxa de mortalidade materna em abortos entre 1998 e 2005 foi de 0,6 morte por 100 000 procedimentos abortivos, tornando o aborto cerca de 14 vezes mais seguro do que o parto, cuja taxa de mortalidade é de 8,8 mortes por 100 000 nascidos vivos.[59][60]
O risco de mortalidade relacionada com o aborto aumenta com a idade gestacional, mas permanece menor do que o do parto até pelo menos 21 semanas de gestação.[61][62] Isso contrasta com algumas leis presentes em alguns países que exigem que os médicos informem os pacientes que o aborto é um procedimento de alto risco.[63]
A aspiração uterina a vácuo no primeiro trimestre é o método de aborto não-farmacológico mais seguro, e pode ser realizado em uma clínica de atenção primária em saúde, clínica de aborto ou hospital. As complicações são raras e podem incluir perfuração uterina, infecção pélvica e retenção dos produtos da concepção necessitando de um segundo procedimento para evacuá-los.[64]
Geralmente são administrados antibióticos profiláticos (preventivos) (como a doxiciclina ou metronidazol) antes do aborto eletivo,[65] pois acredita-se que eles diminuem substancialmente o risco de infecção uterina pós-operatória.[46][66]
Existe pouca diferença em termos de segurança e eficácia entre o aborto farmacológicos usando regime combinado de mifepristona e misoprostol e o aborto não-farmacológico (aspiração a vácuo) quando são realizados no início do primeiro trimestre (até 9 semanas de idade gestacional).[38] O aborto farmacológico com o uso do análogo de prostaglandina misoprostol isolado é menos efetivo e mais doloroso do que o aborto usando o regime combinado de mifepristona e misoprostol ou do que o aborto cirúrgico.[67][68]
A Organização Mundial de Saúde define como abortos inseguros aqueles que são realizados por indivíduos sem formação, equipamentos perigosos ou em instalações sem condições de higiene e segurança.[69] Em muitos casos, e principalmente quando existem limitações no acesso a abortos legais e seguros, as mulheres que procuram terminar a gravidez vêm-se forçadas a recorrer a métodos de aborto inseguros. Nestes casos, podem tentar realizar um aborto autoinduzido ou confiar noutra pessoa sem formação médica adequada ou sem acesso a instalações seguras. A prática de abortos sem condições de segurança pode resultar em complicações graves para a mulher, entre as quais um aborto incompleto, sepse, hemorragias, infertilidade e lesões nos órgãos internos.[70]
Os abortos inseguros são uma das principais causas de morte e incapacidade entre as mulheres em todo o mundo. Embora os dados sejam imprecisos, estima-se que todos os anos sejam praticados cerca de 20 milhões de abortos inseguros, 97% dos quais em países em vias de desenvolvimento.[2] Os abortos inseguros resultam em 5 milhões de casos de incapacidade por ano.[2][7][71] Cerca de 24 milhões de mulheres são inférteis como resultado de um aborto inseguro.[72] A estimativa do número de mortes causada por abortos inseguros varia conforme a metodologia usada. A ONU estima que 70 000 mulheres perdem a vida anualmente em consequência de abortos realizados em condições inseguras.[2][7][73][74] o que corresponde a 13% de todas as mortes maternas.[75][76] A OMS estima que a mortalidade tenha diminuído desde a década de 1990.[77] No entanto, entre 1995 e 2008 a proporção de abortos inseguros aumentou de 44% para 49%.[10] A incidência dos abortos inseguros pode ser difícil de quantificar com precisão, uma vez que muitos casos são reportados como aborto espontâneo, "regulação menstrual", "mini-aborto" ou "regulação de menstruação suspensa" ou "adiada".[78][79]
O principal fator que determina se o aborto é feito de forma segura ou insegura é a se é ou não legal. Os países com leis proibitivas apresentam maior frequência de abortos inseguros e maior frequência de abortos em geral, quando comparados com aqueles onde o aborto é permitido e de fácil acesso.[7][10][72][80][81][82][83] A falta de acesso a contraceção também contribui para o número de abortos inseguros. Estima-se que a incidência de abortos inseguros pudesse ser reduzida 75%, de 20 para 5 milhões anuais, se em todo o mundo estivessem disponíveis serviços modernos de saúde materna e planeamento familiar.[84]
Apenas 40% das mulheres em todo o mundo têm acesso a abortos terapêuticos e eletivos dentro de determinados limites gestacionais.[13] Cerca de 35% têm acesso a abortos legais quando cumprem determinados critérios físicos, mentais ou socioeconómicos.[16]  Entre as medidas para diminuir o número de abortos inseguros, a generalidade das organizações de saúde pública defende a legalização do aborto, a formação do pessoal médico, a garantia do acesso a serviços de planeamento familiar e contracetivos e educação sexual.[80][85] Nos países onde o aborto tem vindo a ser legalizado, tem-se observado uma diminuição acentuada da mortalidade materna.[86][87][88]
Esta hipótese não é aceita pelo consenso científico das entidades ligadas ao câncer.[89][90][91] Bind é um pesquisador com uma agenda enviesada e pseudocientífica para a promoção de suas crenças religiosas através do do "Breast Cancer Prevention Institute".[92][93]
As estruturas anatómicas envolvidas no processo de sensação da dor ainda não estão presentes nesta fase do desenvolvimento. As ligações entre o tálamo e o córtex cerebral formam-se por volta da 23ª semana.[94] Existe também a possibilidade de que o feto não disponha da capacidade de sentir dor, ligada ao desenvolvimento mental que só ocorre após o nascimento.[95]
Muitos membros de grupos pró-escolha[96] consideram haver um risco maior de crianças não desejadas (crianças que nasceram apenas porque a interrupção voluntária da gravidez não era uma opção, quer por questões legais, quer por pressão social) terem um nível de felicidade inferior às outras crianças incluindo problemas que se mantêm mesmo quando adultas, entre estes problemas incluem-se:
Uma opinião contrária, entretanto, apresentada por grupos pró-vida, seria que, mesmo que sejam encontradas correlações estatísticas entre gravidez indesejáveis e situações consideradas psicologicamente ruins para as crianças nascidas, esta situação não pode ser comparada com a de crianças abortadas, visto que estas não estão vivas. Uma "situação de vida" não seria passível de comparação com uma "situação de morte", visto a inverificabilidade desta enquanto situação possivelmente existente (a chamada "vida após a morte") pelos métodos científicos disponíveis. Como não se pode estipular se uma situação ruim de vida, por pior que fosse, seria pior que a morte, o aborto, no caso, não poderia ser apresentado como solução, visto que não dá a capacidade de escolha ao envolvido, enquanto ainda é um feto.[102][103][104]
As razões que levam a mulher a optar por um aborto são diversas e diferentes em todo o mundo.[105][106] Uma das razões mais comuns é o adiamento da gravidez para um momento mais conveniente ou de forma a permitir focar energias e recursos nos filhos já nascidos. Entre outras razões pessoais estão a incapacidade em sustentar a criança, quer em termos de custos diretos, quer em termos de custos indiretos derivados da perda de rendimentos ao ter que tomar conta da criança, a falta de apoio do pai, a vontade em proporcionar educação de qualidade aos filhos já nascidos, problemas de relacionamento com o parceiro, a perceção de ser muito nova para tomar conta de uma criança, desemprego, e não estar disposta a educar uma criança que tenha sido concebida como resultado de uma violação, incesto ou outras causas.[106][107]
Alguns abortos são praticados como resultado de pressões sociais. Entre estas pressões estão a preferência por crianças de determinado  sexo ou raça,[108] a reprovação social de mães solteiras ou de gravidez na adolescência, o estigma social em relação a pessoas com deficiências, falta de apoios económicos às famílias, falta de acesso ou rejeição de métodos contracetivos ou resultado de controlo populacional. Estes fatores podem por vezes resultar em aborto compulsivo ou aborto seletivo.[109]
Em alguns países, em mais de um terço dos casos a principal razão apontada é o risco para a saúde da mãe ou do bebé.[105][106] Em muitos casos, a motivação é a presença de um cancro e o aborto é feito para proteger o feto durante o tratamento com quimioterapia ou radioterapia.[110]
Um estudo norte-americano de 2002 observou que cerca de metade das mulheres que realizaram abortos tinham usado um método contracetivo no momento da concepção. No entanto, observou-se inconsistência de uso em cerca de metade das que tinham usado preservativo e em três quartos das que tinham usado pílula contracetiva. Cerca de 42% das que tinham usado preservativos reportaram que a falha se tinha devido ao deslizamento ou rutura.[111]
A história do aborto, segundo a Antropologia, remonta à Antiguidade.
Há evidências que sugerem que, historicamente, dava-se fim à gestação, ou seja, provocava-se o aborto, utilizando diversos métodos, como ervas abortivas, o uso de objetos cortantes, a aplicação de pressão abdominal entre outras técnicas.
A palavra aborto tem sua origem etimológica no latim abortacus, derivado de aboriri ("perecer"), composto de ab ("distanciamento", "a partir de") e oriri ("nascer").
O aborto induzido tem sido matéria de debate considerável. As questões éticas, morais, filosóficas, biológicas, religiosas e legais do aborto são influenciadas pelos sistemas de valores dos intervenientes. As opiniões sobre o aborto podem envolver os direitos da mulher, direitos fetais ou legitimidade do governo. Tanto no debate público como privado, os argumentos a favor ou contra o livre acesso aborto focam-se ou na legitimidade moral de um aborto induzido ou na legitimidade de leis que permitam ou restrinjam o acesso ao aborto.[112]
A Declaração sobre Aborto Terapêutico da Associação Médica Mundial afirma que o debate sobre o aborto resulta do conflito do interesse da mãe com o interesse do filho por nascer, que dá origem a um dilema moral sobre se a gravidez deve ou não ser deliberadamente interrompida.[113] Os debates sobre o aborto, em particular aqueles que incidem sobre a legislação, são muitas vezes encabeçados por grupos que advogam uma destas posições. Os grupos antiaborto intitulam-se "pró-vida" e advogam maiores restrições legais ao aborto, em alguns casos a sua proibição total. Os grupos a favor do direito ao aborto intitulam-se "pró-escolha e advogam a liberdade da mulher em tomar decisões sobre o seu próprio corpo.[114]
A maioria dos movimentos antiaborto alega que qualquer feto humano é uma pessoa com direito à vida, comparando em termos morais o aborto ao homicídio. A maioria dos movimentos a favor  do direito ao aborto alega que a mulher tem direitos reprodutivos e a liberdade de decidir sobre o seu próprio corpo, especialmente se quer ou não levar uma gravidez a termo, e que os abortos clandestinos representam um perigo de saúde pública. Nos países em desenvolvimento em que o aborto é criminalizado, as taxas são centenas de vezes mais altas atingindo 330 mortes por cada 100 mil procedimentos.[115]
Nas jurisdições em que o aborto a pedido é permitido por lei, geralmente é necessário cumprir determinados critérios para se poder aceder a um aborto seguro e legal. Estes critérios geralmente incidem sobre a idade do feto. Em algumas jurisdições é determinado um número máximo de semanas, geralmente por volta do fim do primeiro trimestre, enquanto em outras se exige a avaliação da viabilidade fetal por parte de um médico. Algumas jurisdições exigem um período de reflexão entre o pedido e a realização do aborto, obrigam à distribuição de informação relativa ao desenvolvimento pré-natal ou exigem que os pais sejam contactados se uma filha menor requerer um aborto.[116] Outras jurisdições podem ainda exigir que a mulher obtenha a aprovação do pai do feto. Algumas jurisdições exigem ainda que a mulher seja informada dos riscos do procedimento, incluindo alegados riscos que não são apoiados por evidências científicas, e também que as autoridades de saúde certifiquem que o aborto é medicamente ou socialmente necessário. No entanto, muitas destas restrições são levantadas em situações de emergência.[117]
Noutras jurisdições o aborto a pedido é geralmente proibido. No entanto, muitas destas jurisdições permitem abortos legais em circunstâncias excecionais. Embora as circunstâncias variem entre jurisdições, geralmente incidem sobre se a gravidez é ou não ou resultado de uma violação ou incesto, se existe comprometimento do desenvolvimento fetal, se a saúde física ou mental da mulher está em risco ou se as condições socioeconómicas colocam o desenvolvimento da criança em risco.[16]
Em regiões onde o aborto é ilegal e está associado a forte estigma social, as grávidas podem recorrer a turismo médico, viajando para países onde seja a interrupção da gravidez seja legal.[118] As mulheres sem meios para viajar recorrem em muitos casos a abortos ilegais e inseguros ou tentam realizar um aborto por elas mesmas.[119] Em alguns países em que o aborto é totalmente proibido, algumas clínicas realizam abortos sob eufemismos como "higiene menstrual" ou "menstruação atrasada".[120][121] Em países onde o aborto é completamente proibido, como na Nicarágua, as autoridades médicas têm registado aumentos na morte materna direta e indiretamente relacionados com a gravidez, assim como mortes causadas pelo receio dos médicos em serem perseguidos caso tratem outras emergências ginecológicas.[122][123]
O grego (ελληνικά, transl. Eliniká, AFI: [eliniˈka] ou ελληνική γλώσσα, AFI:[eliniˈki ˈɣlosa], lit. "língua helênica") é uma língua de um ramo independente da família linguística indo-europeia. É a língua oficial da Grécia e do Chipre, e também uma das 24 línguas oficiais da União Europeia. Natural do sul dos Bálcãs, oeste da Ásia Menor e a região em torno do mar Egeu, é o idioma indo-europeu com a mais longa e bem documentada história, abrangendo 34 séculos de registros escritos. Seu sistema de escrita foi o alfabeto grego durante a maior parte de sua história; outros sistemas, como o Linear B e o silabário cipriota também foram utilizados. O alfabeto grego surgiu a partir da escrita fenícia, e acabou dando origem, por sua vez, aos alfabetos latino, cirílico, copta, e diversos outros sistemas de escrita.
O idioma grego tem um lugar importante na história da Europa, do mundo ocidental e do cristianismo; o cânone da literatura grega antiga inclui obras de importância monumental, que influenciaram de maneira decisiva o cânone da literatura ocidental posterior; entre as obras de destaque estão poemas épicos como a Ilíada e a Odisseia. O grego também foi a língua na qual diversos dos textos fundamentais da filosofia ocidental, como os diálogos platônicos e as obras de Aristóteles, foram escritos; o Novo Testamento da Bíblia cristã foi escrito no grego koiné. Juntamente com os textos latinos e as tradições do mundo romano, o estudo dos textos gregos e das sociedades da Antiguidade foram a disciplina da História e Arqueologia Clássica.
O grego foi uma língua franca amplamente falada no mundo ao redor do mar Mediterrâneo, e até mesmo em outras partes, durante a Antiguidade Clássica, que se estende do século VIII a.C. até à queda do Império Romano do Ocidente, no ano de 476 d.C. e viria a se tornar o idioma oficial do Império Bizantino. Em sua forma atual, o grego é falado por pelo menos 15 milhões de pessoas atualmente,[1] na Grécia, no Chipre, em algumas áreas da Albânia e nas comunidades de expatriados em diversos países ao redor do mundo.
As raízes gregas são frequentemente usadas para formar novas palavras em outros idiomas, especialmente nas áreas de Exatas e em Medicina; o grego e o latim são as fontes predominantes do vocabulário científico internacional. Mais de cinquenta mil palavras do inglês, por exemplo, têm origem no grego, assim como em português.
O grego antigo era uma língua indo-europeia falada na Grécia durante a Antiguidade, possuindo diversos dialetos, sendo os mais conhecidos o ático e o jônico; tendo, posteriormente, evoluído para o grego moderno.[2]
A escrita desta língua se encontra presente em diversos substratos, sendo os mais comuns os papiros e as estelas, mas também se encontram pequenos textos em inscrições em objetos cerâmicos, como o óstraco,[3] e ainda em ossos e metais, como no caso das lâminas órficas.
O grego moderno, língua oficial da Grécia e do Chipre, difere em muitas formas do grego antigo e é falado por cerca de 13,1 milhões de pessoas.[4] Na Grécia, é falado por quase toda a população.[5] Também é, juntamente com o turco, a língua oficial de Chipre, embora o uso oficial do turco tenha sido limitado pela República de Chipre desde a invasão turca de 1974.[6] Devido à adesão da Grécia e de Chipre à União Europeia, o grego é, atualmente, uma de suas 24 línguas oficiais.[7] Além disso, o grego é oficialmente reconhecido como uma língua minoritária em partes da Itália e Albânia,[8] bem como na Armênia e Ucrânia,[9] sem falar na diáspora grega em países europeus e americanos, bem como na Austrália. Essa diáspora é formada não apenas por descendentes de gregos da Grécia, como também de indivíduos nascidos das ondas de emigração que quase extinguiram as antigas comunidades gregas de lugares como Egito, Turquia, Bulgária etc..
A língua grega moderna - isto é, o falar inicialmente restrito a um certo estrato das populações da Grécia meridional, acrescido de componentes eruditos e elementos estrangeiros (principalmente franceses e ingleses) - só se tornou a língua oficial do país em 1976. Até esta data, a língua oficial era a chamada "catarévussa", o grego clássico, uma variante livresca decalcada do grego bizantino. O debate em torno da reforma linguística, que começou ainda em meados do século XIX, teve a cidade de Atenas por epicentro e o poeta Kostís Palamás como figura principal.[10]
Os dialetos mais importantes são os seguintes:
O alfabeto utilizado para escrever a língua grega teve o seu desenvolvimento por volta do século IX a.C., utilizando-se até aos nossos dias, tanto no grego moderno como também na Matemática, Astronomia, etc.
Anteriormente, o alfabeto grego (Ελληνικό αλφάβητο) foi escrito mediante um silabário, utilizado em Creta e zonas da Grécia continental como Micenas ou Pilos entre os séculos XVI a.C. e XII a.C. e conhecido como linear B. O Grego que reproduz parece uma versão primitiva dos dialectos arcado-cipriota e jónico-ático, dos quais provavelmente é antepassado, e é conhecido habitualmente como grego micênico.
Crê-se que o alfabeto grego deriva duma variante do semítico, introduzido na Grécia por mercadores fenícios. Dado que o alfabeto semítico não necessita de notar as vogais, ao contrário da língua grega e outras da família indo-europeia, como o latim e em consequência o português, os gregos adaptaram alguns símbolos fenícios sem valor fonético em grego para representar as vogais. Este facto pode considerar-se fundamental e tornou possível a transcrição fonética satisfatória das línguas Europeias.
As letras obsoletas desapareceram do alfabeto nos seus primeiros tempos, antes do denominado período clássico. Dado que a aparição das letras minúsculas é bastante posterior, não existem minúsculas medievais das ditas letras.
Originariamente existiram variantes do alfabeto grego, sendo as mais importantes a ocidental (Calcídica) e a oriental (Jónica). A variante ocidental originou o alfabeto etrusco e daí o alfabeto romano. Atenas adotou no ano 403 a.C. a variante oriental, dando lugar a que pouco depois desaparecessem as demais formas existentes do alfabeto. Já nesta época o grego escrevia-se da esquerda para a direita, enquanto que a princípio a maneira de o escrever era alternadamente da esquerda para a direita e da direita para a esquerda, de maneira que se começava pelo lado em que se tinha concluído a linha anterior, invertendo todos os caracteres em dito processo.
O factor inovador introduzido com o alfabeto grego são as vogais. As primeiras vogais foram Alfa, Épsilon, Iota, Ómicron e Upsilon. Se se contempla o processo de criação do alfabeto grego como resultado de um processo dinâmico baseado na adoção de vários alfabetos semíticos através do tempo, encontrando inclusive influências do linear-B, poder-se-ia dar uma explicação mais satisfatória da sua origem do que as teorias que postulam uma adaptação única de um alfabeto determinado num momento dado.
Gramática (do grego: γραμματική, transl. grammatiké, feminino substantivado de grammatikós) designa um conjunto de regras que regem o uso de uma língua, especialmente o modo como as unidades desta se combinam entre si para formar unidades maiores. Também designa um ramo da linguística que estuda os elementos que compõem a gramática das línguas. Além disso, ainda nomeia as obras produzidas a partir destes estudos, classificadas em diversos tipos, a depender do elemento da língua que tomam como escopo: gramáticas históricas, comparativas, descritivas, e prescritivas, estas últimas também chamadas gramáticas normativas.
Toda língua e suas variantes — desde a variante padrão ou norma culta às variantes não padrão — têm sua própria gramática, que é conhecida e usada por todos os falantes daquela determinada variante, pois é adquirida no processo de aquisição de linguagem. Esse conhecimento, contudo, na maioria das vezes se restringe ao âmbito epilinguístico, não alcançando o plano metalinguístico.
O conceito de gramática, quando se refere às gramáticas normativas, designa obras de caráter prescritivo que têm como objetivo ditar as regras segundo as quais a língua deveria ser usada. As fontes de elaboração dessas regras podem ser diversas, a depender do autor e da época de produção de cada gramática, podendo variar da referência à norma culta, isto é, à variante prestigiada dentro de determinada comunidade linguística, às obras de escritores canônicos.
Na Idade Média, a gramática era uma disciplina de estudo que compunha o Trivium que, junto com o Quadrivium, compunham a metodologia de ensino das sete Artes liberais.[1]
Segundo Eckersley e Macaulay, "gramática é a arte de colocar as palavras certas nos lugares certos".[2] Gramática, portanto, numa abordagem generalista, não se vincula a esta ou àquela língua em especial, mas a todas. Ela contém o germe estrutural de todas, realizando a conexão essencial subjacente à relação de cada uma com as demais.
Os diversos enfoques da gramática (normativa, histórica, comparativa, funcional e descritiva) estudam a morfologia e a sintaxe, as quais tratam, somente, dos aspectos estruturais, constituindo, uma parte da linguística que se distingue da fonologia e da semântica (que seriam estudos independentes), conquanto estas duas possam compreender-se, também, dentro do escopo amplo da gramática.
Dentre os diversos tipos de gramáticas, a chamada gramática normativa é a mais conhecida pela população e é estudada durante o período escolar. É elaborada, em geral, pelas Academias de Letras de cada país, nem sempre em conformidade com o uso corrente da população, mesmo em amostragens da porção tida por "mais culta". Sabemos que é de fundamental importância o uso da gramática normativa em toda a vida escolar das pessoas, porém, vale ressaltar aqui que existem variações linguísticas que devem ser trabalhadas também em sala de aula, e assim levar esses alunos a conhecer as diversidades da língua portuguesa.
Cabe notar, ainda, que nem toda gramática trata da língua escrita. Como exemplo, cite-se o caso da Gramática do Português Falado, em realidade cultural-linguística brasileira, coleção publicada pela editora da Universidade de Campinas.
O termo "gramática" é usado em acepções distintas: pode referir-se ao manual onde as regras de regulação e uso da língua estão explicitadas, ou ao saber que os falantes têm interiorizado acerca da sua língua materna. Estas duas acepções distintas remetem aos conceitos de Gramática Prescritiva ou Normativa, que impõem determinados comportamentos linguísticos como corretos, marginalizando outros que não se enquadram nos padrões indicados por essas.
O termo também nomeia teorias sobre aquisição, geração e funcionamento da língua. A exemplo, tem-se a Gramática Gerativa de Noam Chomsky, que descreve um conjunto finito de regras que gerariam todas as frases de uma língua. Outro exemplo é a Gramática Funcional, segundo a qual as regras gramaticais estão vinculadas as necessidades comunicativas dos falantes.
Atualmente, a linguística procura descrever o conhecimento linguístico dos falantes, produzindo as Gramáticas Descritivas. Estas, ao invés de imporem paradigmas, descrevem e incorporam fenômenos que, numa abordagem puramente prescritiva, não seriam levados em conta.
Conquanto "correto" faça remissão semântica a imutabilidade ("não desvio") ou a um pré-determinado ou estabelecido padrão, neste caso linguístico, convém observar três princípios básicos que se fazem presentes na dinâmica cultural humana:
Isso posto, sem demérito para [ou exclusão de] as variadíssimas expressões e modalidades de "gramática", as também variadíssimas linguagens, as incontáveis tribos culturais, fica logo claro que "é preciso alguma ordem na casa, para que as coisas funcionem a contento". Isso inclui a "Casa Linguística". Pois o ser humano — essencial e semioticamente simbólico que é — necessita de um mínimo de estrutura de ordem para humanamente ser a sua existência. O que importa em regras daqui e dali, inescapavelmente.
Assim, na concepção de gramática como conhecimento inconsciente de um determinado falante sobre a sua língua (independente do grau de escolaridade e acesso à Gramática Tradicional), não há lugar para os conceitos de "certo" e "errado", baseados exclusivamente em uma norma que, particularmente no caso do português do Brasil, até podemos questionar que seja ainda utilizada por algum falante; há tão somente os conceitos de sentenças que pertencem a uma dada língua (gramaticalidade) ou não (agramaticalidade). Quem sabe decidir se uma sentença pertence ou não a uma dada língua é o falante nativo daquela língua, escolarizado ou não. Portanto, os conceitos de gramaticalidade/agramaticalidade não recobrem de forma alguma os conceitos de certo/errado da Gramática Tradicional.[3]
As primeiras gramáticas sistemáticas se originaram na Idade do Ferro na Índia, com Yaska (VI a.C.), Pāṇini (IV a.C.) e seus comentadores Pingala (200 a.C.), Katyayana, e Patânjali (II a.C). No Ocidente, a gramática surgiu como uma disciplina do helenismo a partir de III. a.C. com autores como Rhyanus e Aristarco de Samotrácia. A mais antiga obra existente, Arte da Gramática (Τέχνη Γραμματική), é atribuída ao gramático grego Dionísio, o Trácio (100 a.C.), e serviu de base para as gramáticas grega, latina e de outras línguas europeias até o Renascimento.
Com o advento do Império Romano, os romanos receberam essa tradição dos gregos e traduziram para o latim os nomes das partes da oração e dos acidentes gramaticais. Muitas destas denominações chegaram aos nossos dias. A gramática latina foi desenvolvida seguindo modelos gregos do século I a.C., devido ao trabalho de autores como Lúcio Orbílio Pupilo, Remmius Palaemon, Marcus Valerius Probus, Marcus Verrius Flaccus e Aemilius Asper.
Contudo, aceita-se que o estudo formal da gramática tenha iniciado com os gregos, a partir de uma perspectiva filosófica.
No século XVIII, iniciaram-se as comparações entre as várias línguas europeias e asiáticas, trabalho que culminou com a afirmação do filósofo Leibniz de que a "maioria das línguas "provinha de uma única língua, a indo-europeia". No século XIX, surgiu a gramática comparativa, como enfoque dominante da Linguística.
Em 1911, com o objetivo de descrever gramaticalmente a língua dentro de seu próprio modelo, surgiu o Handbook of American Indian languages, do antropólogo Franz Boas, e os trabalhos do estruturalista dinamarquês Otto Jespersen, que publicou, em 1924, A Filosofia da Gramática. Boas desafiou a metodologia tradicional da gramática ao estudar línguas não indo-europeias que careciam de testemunhos escritos. A análise descritiva, representada por estes dois autores, desenvolveu um método preciso e científico, além de descrever as unidades formais mínimas de qualquer língua.
Também nos séculos XIX e XX, estabeleceram-se as bases científicas da Semiótica como "sistema de signos", a conectar várias ou todas as áreas do conhecimento.
Para o linguista suíço Ferdinand de Saussure, "a língua é o sistema que sustenta qualquer idioma concreto", isto é, o que falam e entendem os membros de qualquer comunidade linguística, pois "participam da gramática". Em meados do século XX, Noam Chomsky concebeu a teoria da "gramática universal", baseada em princípios comuns a todas as línguas.
Escrita por Fernão de Oliveira, presbítero secular e professor de retórica em Coimbra, a primeira gramática portuguesa escrita de que se há notícia chama-se Grammatica da lingoagem Portuguesa data do século XVI e foi publicada em Lisboa, em 1536, por ordem de D. Fernando de Almada. Decorridos quatro anos, surge a seguinte, autorada por João de Barros e editada igualmente em Lisboa em 1540.[4]
Na informática, a sintaxe de cada linguagem de programação é definida com uma gramática formal, ou linguagem natural. Na informática e na matemática, gramáticas formais definem linguagens formais. A hierarquia de Chomsky define vários importantes tipos de gramáticas formais.
Costuma-se classificar a Gramática em partes "autônomas, porém harmônicas entre si", a fim de facilitar o seu estudo. A classificação convencional padrão divide a Gramática em 10 áreas, embora não pretenda ser uma classificação definitiva, exaustiva ou única.
A Literatura é a técnica de compor e expor textos escritos, em prosa ou em verso, de acordo com princípios teóricos e práticos; o exercício dessa técnica ou da eloquência e poesia.[1]
A palavra Literatura vem do latim "litteris" que significa "Letras", e possivelmente uma tradução do grego "grammatikee". Em latim, literatura significa uma instrução ou um conjunto de saberes ou habilidades de escrever e ler bem, e se relaciona com as técnicas da gramática, da retórica e da poética. Por extensão, se refere especificamente ao ofício de escrever. O termo Literatura também é usado como referência a um conjunto escolhido de textos, por exemplo a literatura portuguesa, a literatura espanhola, a literatura inglesa, a literatura brasileira, a literatura japonesa, etc.
Mais produtivo do que tentar definir Literatura talvez seja encontrar um caminho para decidir o que torna um texto, em sentido lato, literário. A definição de literatura está comumente associada à ideia de escrita de letras. Entretanto, nem todo texto é literário. A própria natureza do caráter estético, contudo, reconduz à dificuldade de elaborar alguma definição verdadeiramente estável para o texto literário. Para simplificar, pode-se exemplificar através de uma comparação por oposição. Vamos opor o texto científico ao texto literário: o texto científico emprega as palavras sem preocupação com a beleza, o efeito emocional. No texto literário, ao contrário, essa será a preocupação maior do escritor (autor).  É óbvio que também o escritor busca instruir, e perpassar ao leitor uma determinada ideia; mas, diferentemente do texto científico, o texto literário une essa instrução à necessidade à recepção. O texto científico emprega as palavras no seu sentido dicionarizado, denotativamente, enquanto o texto literário busca empregar as palavras com liberdade, preferindo o seu sentido conotativo, figurado. O texto literário é, portanto, aquele que pretende emocionar e que, para isso, emprega a língua com liberdade e beleza, utilizando-se, muitas vezes, do sentido metafórico das palavras.
A compreensão do fenômeno literário tende a ser marcada por alguns sentidos, alguns marcados de forma mais enfática na história da cultura ocidental, outros diluídos entre os diversos usos que o termo assume nos circuitos de cada sistema literário particular.
Assim encontramos uma concepção "clássica", surgida durante o Iluminismo (que podemos chamar de "definição moderna clássica", que organiza e estabelece as bases de periodização usadas na estruturação do cânone ocidental); uma definição "romântica" (na qual a presença de uma intenção estética do próprio autor torna-se decisiva para essa caracterização); e, finalmente, uma concepção "crítica" (na qual as definições estáveis tornam-se passíveis de confronto, e a partir da qual se buscam modelos teóricos capazes de localizar o fenômeno literário e, apenas nesse movimento, "defini-lo"). Deixar a cargo do leitor individual a definição implica uma boa dose de subjetivismo (postura identificada com a matriz romântica do conceito de "Literatura"); a menos que se queira ir às raias do solipsismo, encontrar-se-á alguma necessidade para um diálogo quanto a esta questão. Isto pode, entretanto, levar ao extremo oposto de considerar como literatura apenas aquilo que é entendido como tal por toda a sociedade ou por parte dela, tida como autorizada à definição. Esta posição não só sufocaria a renovação na técnica literária, como também limitaria excessivamente o corpus já reconhecido.
De qualquer forma, destas três fontes (a "clássica", a "romântica" e a "crítica") surgem conceitos de literatura, cuja pluralidade não impede de prosseguir a classificações de gênero e exposição de autores e obras.
O termo provém do latim litteratura, técnica de escrever bem", a partir da palavra latina littera, "letra".
Provavelmente a mais antiga das formas literárias, a poesia consiste no arranjo harmônico das palavras. Geralmente, um poema organiza-se em versos, caracterizados pela escolha precisa das palavras em função de seus valores semânticos (denotativos e, especialmente, conotativos) e sonoros. É possível a ocorrência da rima, bem como a construção em formas determinadas como o soneto e o haikai. Segundo características formais e temáticas, classificam-se diversos gêneros poéticos adotados pelos poetas.
O texto dramático é a forma literária clássica, composta basicamente de falas de um ou mais personagens, individuais (atores) ou coletivos (coros), destina-se primariamente a ser encenada e não apenas lida. Até um passado recente, não se escrevia a não ser em verso. Na tradição ocidental, as origens do teatro datam dos gregos, que desenvolveram os primeiros gêneros: a tragédia e a comédia. O escritor e autor de textos dramáticos é chamado dramaturgo.
Os textos de teatro, letras de uma música; inovações textuais, e os textos dos roteiros para o cinema, também podem ser consideradas obras literárias.
A literatura de ficção em prosa, cuja definição mais crua é o texto "corrido", sem versificação, bem como suas formas, são de aparição relativamente recente. Pode-se considerar que o romance, por exemplo, surge no início do século XVII com Dom Quixote de La Mancha, de Miguel de Cervantes Saavedra.
Subdivisões, aqui, dão-se em geral pelo tamanho e, de certa forma, pela complexidade do texto. Entre o conto, "curto", e o romance, "longo", situa-se por vezes a novela.
A linguagem é o veículo utilizado para se escrever uma obra literária. Escrever obras literárias é trabalhar com a linguagem. Os Gêneros Literários são as várias formas de trabalhar a linguagem, de registrar a história, e fazer com que a essa linguagem seja um instrumento de conexão entre os diversos contextos literários que estão dispersos ao redor do mundo. Uma boa forma de se familiarizar com os diversos gêneros literários — e assim criar hábitos sólidos de leitura — é ter contato com o formato mais apropriado para cada idade, passando desde a literatura infantil à infanto-juvenil até chegar à adulta.[2]
A natureza, em seu sentido mais amplo, é equivalente ao "mundo natural" ou "universo físico". O termo "natureza" faz referência aos fenômenos do mundo físico, e também à vida em geral. Geralmente não inclui os objetos construídos por humanos.
A palavra "natureza" provém da palavra latina natura, que significa "qualidade essencial, disposição inata, o curso das coisas e o próprio universo".[1] Natura é a tradução para o latim da palavra grega physis (φύσις), que em seu significado original fazia referência à forma inata que crescem espontaneamente plantas e animais. O conceito de natureza como um todo — o universo físico — é um conceito mais recente que adquiriu um uso cada vez mais amplo com o desenvolvimento do método científico moderno nos últimos séculos.[2][3]
Dentro dos diversos usos atuais desta palavra, "natureza" pode fazer referência ao domínio geral de diversos tipos de seres vivos, como plantas e animais, e em alguns casos aos processos associados com objetos inanimados - a forma em que existem os diversos tipos particulares de coisas e suas mudanças espontâneas, assim como o tempo atmosférico, a geologia da Terra e a matéria e energia estes entes possuem. Frequentemente se considera que significa "entorno natural": animais selvagens, rochas, bosques, praias, e em geral todas as coisas que não tenham sido alteradas substancialmente pelo ser humano, ou persistem apesar da intervenção humana. Este conceito mais tradicional das coisas naturais implica uma distinção entre o natural e o artificial, entendido este último como algo feito por uma mente ou uma consciência.
Latim, natura,[4] comp. pelo tema natus, p.pass. de nascere = nascer e urus = sufixo do particípio futuro de oritur = surgir, gerar, a força que gera.
Aquilo que surge, que se dá por nascimento. Aquilo que é e faz por nascimento segundo leis universais aplicadas a um preciso contexto.[5]
Ordem ou sistema de leis que precedem a existência das coisas e a sucessão dos seres. O conjunto de todos os seres que compõem o universo. Essência e qualidade ínsita de um ser. Também entendido como "qualidade, índole, gênio, tipo, caráter" de um ser.[4]Embora não haja consenso universal sobre a definição de vida, os cientistas geralmente aceitam que a manifestação biológica da vida é caracterizada pelos seguintes fatores ou funções: organização, metabolismo, crescimento, adaptação, resposta a estímulos e reprodução.[6] Os seres vivos (reinos das plantas, animais, fungos, protistas, archaea e bactérias) têm essas propriedades em comum: eles são constituídos por células têm uma organização complexa com base em água e metabolismo de carbono e têm a capacidade de crescer, responder a estímulos, e se reproduzir. Portanto, considera-se que uma entidade que satisfaz estas propriedades está viva.
A biosfera é a parte da camada mais externa do planeta Terra, incluindo o ar, a terra, rochas da superfície e da água, e é nesta parte onde a vida evoluiu, e onde eles são feitos e transformar os processos bióticos. A partir de uma visão muito ampla de geofísica, a biosfera é o sistema ecológico global que integra todos os seres vivos e seus relacionamentos, incluindo sua interação com os elementos da litosfera (rochas), hidrosfera (água), e atmosfera (ar). Atualmente, estima-se que a Terra contém cerca de 75 000 milhões de toneladas (7,5 kg x 1013) de biomassa, que está presente em vários ambientes dentro da biosfera.[7]
Cerca de nove décimos da biomassa total da Terra é a vida das plantas, de que a vida animal depende para a sobrevivência.[8] Até o momento já se identificaram mais 2 milhões de espécies de plantas e animais,[9] e estimativas sobre o número real de espécies existentes variam de alguns poucos milhões a até 50 milhões de espécies.[10][11][12]O número de espécies existentes varia constantemente, já que surgem novas e outras deixam de existir em uma dinâmica contínua.[13][14] Atualmente, o número total de espécies está passando por um rápido declínio.[15][16][17]
A escala abrangida pela palavra natureza, dentro deste contexto, envolve desde o subatômico até o amplamente universal, como os planetas e estrelas. Tomando como o recorte a escala do homem, inclui basicamente o meio ambiente natural e normalmente exclui o meio ambiente construído, de forma a ser tradicionalmente associada à vida selvagem, aos fenômenos e recursos naturais e aos seus processos e dinâmicas próprios. Há também definições que incluem o meio ambiente alterado pelo homem como elemento da Natureza.
A associação mais popular que se faz à palavra "natureza" a confunde com a ideia de paisagem natural: a paisagem é o resultado dos processos complexos presentes em um determinado meio ambiente.
O mundo natural costuma estar associado ao mundo real.
O estudo sistematizado dos elementos da natureza, seus processos, actividades e consequências se dá através das Ciências naturais.
O crescimento das populações, o aumento do consumo ligado às inovações tecnológicas, à escala global, uma proliferação de resíduos que contaminam o ambiente, afectam os ecossistemas, pondo em causa a natureza.
No sentido de permitir um desenvolvimento sustentável, o Homem tem vindo a desenvolver práticas que permitem a proteção e a conservação da Natureza. Dessas práticas podem destacar-se:
Marte é o quarto planeta a partir do Sol, o segundo menor do Sistema Solar. Batizado em homenagem a divindade romana da guerra, muitas vezes é descrito como o "Planeta Vermelho", porque o óxido de ferro predominante em sua superfície lhe dá uma aparência avermelhada.[1]
Marte é um planeta rochoso com uma atmosfera fina, com características de superfície que lembram tanto as crateras de impacto da Lua quanto vulcões, vales, desertos e calotas polares da Terra. O período de rotação e os ciclos sazonais de Marte são também semelhantes aos da Terra, assim como é a inclinação que produz as suas estações do ano. Marte é o lar do Monte Olimpo, a segunda montanha mais alta conhecida no Sistema Solar (a mais alta em um planeta), e do Valles Marineris, um desfiladeiro gigantesco. A suave Bacia Polar Norte, no hemisfério norte marciano, cobre cerca de 40% do planeta e pode ser uma enorme marca de impacto.[2][3] Marte tem duas luas conhecidas, Fobos e Deimos, que são pequenas e de forma irregular. Estas luas podem ser asteroides capturados,[4][5] semelhante ao 5261 Eureka, um asteroide troiano marciano.
Marte está sendo explorado por oito espaçonaves atualmente: seis em órbita — Mars Odyssey, Mars Express, Mars Reconnaissance Orbiter, Mars Atmosphere and Volatile Evolution Missile – MAVEN, Mars Orbiter Mission e ExoMars Trace Gas Orbiter — e duas na superfície — Mars Science Laboratory Curiosity, Perseverance[6] e o rover chinês Zhurong,[7] como também o lander InSight. Entre as espaçonaves desativadas que estão na superfície marciana estão a sonda Spirit e várias outras sondas e rovers, como a Phoenix, que completou sua missão em 2008, e o Opportunity.[8]
Marte tem aproximadamente metade do diâmetro da Terra. Ele é menos denso do que a Terra, tendo cerca de 15% do seu volume e 11% de sua massa, resultando em uma aceleração da gravidade na superfície que é cerca de 38% da que se observa na Terra. A superfície marciana é apenas ligeiramente menor do que a área total de terra firme do planeta Terra.[9] Apesar de Marte ser maior e mais massivo do que Mercúrio, este tem uma densidade mais elevada, com o que os dois planetas têm uma força gravitacional quase idêntica na superfície — a de Marte é mais forte por menos do que 1%. A aparência vermelho-alaranjada da superfície marciana é causada pelo óxido de ferro (III), mais comumente conhecido como hematita, ou ferrugem.[10] Pode também parecer caramelo,[11] enquanto outras cores comuns de superfície incluem dourado, marrom e esverdeado, dependendo dos minerais presentes.[11] Estudo sugere que Marte teve um anel bilhões de anos atrás.[12]
Tal como a Terra, este planeta sofreu diferenciação, o que resultou em um núcleo metálico denso sobreposto por materiais menos densos,[13] a uma distância estimada entre 1 810 e 1 860 km.[14] Os modelos atuais do interior do planeta implicam uma região central de cerca de 1 794 km ± 65 km de raio, composta principalmente por ferro e níquel, com cerca de 16-17% de enxofre.[15] Este núcleo de sulfureto de ferro é parcialmente fluido e tem duas vezes a concentração dos elementos mais leves que existem no núcleo da Terra. O núcleo está envolvido por um manto de silicato que formou muitos dos acidentes tectônicos e vulcânicos do planeta, mas que parecem agora estar dormentes. Além do silício e do oxigênio, os elementos mais abundantes na crosta marciana são ferro, magnésio, alumínio, cálcio e potássio. A espessura média da crosta do planeta é cerca de 50 quilômetros, com uma espessura máxima de 125 km.[16] A crosta terrestre, com uma média de 40 km de espessura, tem apenas um terço da densidade da crosta de Marte, considerando-se a razão dos tamanhos dos dois planetas. A sonda InSight, prevista para 2016, irá utilizar um sismógrafo para melhor determinar os modelos do interior do planeta.[17]
Marte é um planeta rochoso que consiste em minerais contendo silício e oxigênio, metais e outros elementos que normalmente compõem rocha. A superfície de Marte é composta principalmente de basalto toleítico,[18] embora parte seja mais rica em sílica que o basalto típico e possa ser semelhantes às rochas andesíticas da Terra ou ao vidro de sílica. Regiões de baixo albedo apresentam concentrações de plagioclásios, sendo que as regiões de albedo mais baixo, ao norte, exibem concentrações superiores às normais de silicatos e de vidro de sílica. Partes das terras altas ao sul incluem quantidades detectáveis de piroxênios com alto teor de cálcio. Concentrações localizadas de hematita e olivina também foram encontradas.[19] A maior parte da superfície está profundamente coberta por uma camada de pó de óxido de ferro (III) de textura fina.[20][21]
Embora Marte não apresente qualquer evidência de possuir um campo magnético estruturado global,[22] observações mostram que partes da crosta do planeta foram magnetizadas e que inversões geomagnéticas já ocorreram no passado. Este paleomagnetismo de minerais magneticamente suscetíveis tem propriedades que são muito semelhantes às faixas alternadas encontradas no fundo dos oceanos da Terra. Uma teoria, publicada em 1999 e reexaminada em outubro de 2005 (com a ajuda da Mars Global Surveyor), indica que essas faixas demonstram a existência de placas tectônicas em Marte há quatro bilhões de anos, antes de o dínamo planetário ter deixado de funcionar e o campo magnético do planeta ter desaparecido,[23] talvez por causa de um excesso de hidrogênio, liberado pela dissociação da água próximo ao núcleo quente.[24]
Durante a formação do Sistema Solar, Marte foi criado como resultado de um processo estocástico de acreção a partir do disco protoplanetário que orbitava o Sol. Marte tem muitas características químicas próprias causadas por sua posição no Sistema Solar. Elementos com pontos de ebulição relativamente baixos, como cloro, fósforo e enxofre são muito mais comuns em Marte do que na Terra. Estes elementos, provavelmente, foram removidos das áreas mais próximas ao Sol pelo vento solar da jovem estrela.[25]
Após a formação dos planetas, todos foram sujeitos ao chamado "intenso bombardeio tardio". Cerca de 60% da superfície de Marte mostra registros de impactos dessa época,[26][27][28] enquanto a maior parte da superfície restante é provavelmente sustentada por imensas bacias de impacto causadas por esses eventos. Há evidências de uma enorme bacia de impacto no hemisfério norte de Marte, abrangendo 10 600 km por 8 500 km, ou cerca de quatro vezes maior do que a Bacia do Polo Sul-Aitken da Lua, a maior depressão de impacto já descoberta.[2][3] Esta teoria sugere que Marte foi atingido por um corpo do tamanho de Plutão cerca de quatro bilhões de anos atrás . O evento, que se acredita ser a causa da dicotomia hemisférica marciana, criou a suave Bacia Polar Norte, que cobre 40% do planeta.[29][30]
A história geológica de Marte pode ser dividida em vários períodos, mas os seguintes são os três períodos principais:[31][32]
Alguma atividade geológica ainda ocorre no planeta. O Athabasca Valles apresenta vestígios de derramamento de lava de cerca de 200 milhões de anos. A água corrente no Cerberus Fossae ocorreu há menos de 20 milhões de anos, indicando intrusões vulcânicas igualmente recentes.[33] Em 19 de fevereiro de 2008, as imagens da sonda Mars Reconnaissance Orbiter mostraram evidências de uma avalanche a partir de um precipício de 700 metros de altura.[34]
A sonda Phoenix enviou dados que mostraram que o solo marciano é ligeiramente alcalino e contém elementos como magnésio, sódio, potássio e cloro. Esses nutrientes são encontrados nos jardins da Terra e são necessários para o crescimento das plantas.[35] Experimentos realizados pela sonda mostraram que o solo marciano tem um pH básico de 7,7 e contém 0,6% do sal perclorato.[36][37]
Estrias são comuns em Marte e novas aparecem com frequência em encostas íngremes de crateras, desfiladeiros e vales. As estrias são escuras no início e ficam mais claras com o tempo. Elas podem começar em uma pequena área e, em seguida, espalhar-se por centenas de metros. Elas também foram vistas seguindo as bordas das pedras e outros obstáculos em seu caminho. As teorias mais comumente aceitas indicam que elas são camadas subjacentes escuras do solo descobertas após avalanches de poeira brilhante ou redemoinhos.[38] Várias explicações têm sido propostas, algumas das quais envolvem água ou mesmo o crescimento de organismos.[39][40]
Marte já abrigou um ciclo hidrológico ativo, como demonstrado por características geológicas em sua superfície. Porém, ele não possui mais a quantidade de água necessária para produzir essas impressões geológicas. Água líquida não poderia existir na superfície de Marte devido à baixa pressão atmosférica, que é cerca de 100 vezes mais fraca que a da Terra,[41] a não ser em menores elevações por curtos períodos.[42][43] As duas calotas polares marcianas também parecem ser feitas em grande parte de água.[44][45] O volume de água congelada na camada de gelo do polo sul, se derretido, seria suficiente para cobrir toda a superfície do planeta a uma profundidade de 11 metros.[46] Um manto de permafrost se estende desde o polo até latitudes de cerca de 60°.[44]
As observações feitas pela sonda Mars Reconnaissance Orbiter revelaram a possibilidade de que exista água corrente no planeta durante os meses mais quentes.[47] Em 2013, o rover Curiosity da NASA descobriu que o solo de Marte contém entre 1,5% e 3% de água em sua massa (cerca de 33 litros de água por metro cúbico, embora não esteja acessível por estar ligada a outros compostos).[48] Marte pode ser facilmente visto da Terra a olho nu, assim como a sua coloração avermelhada. Sua magnitude aparente atinge -3,0[49] e é superada apenas por Júpiter, Vênus, Lua e Sol. Telescópios ópticos baseados em terra estão tipicamente limitados à resolução de acidentes geográficos maiores que 300 km quando a Terra e Marte estão mais próximos, devido à atmosfera terrestre.[50]
Até o primeiro voo bem-sucedido sobre Marte feito em 1965 pela Mariner 4, muitos especulavam sobre a presença de água em estado líquido na superfície do planeta. Isto era baseado em variações periódicas observadas em manchas claras e escuras, particularmente nas latitudes polares, que se pareciam com mares e continentes; faixas escuras e longas foram interpretadas por alguns como canais de irrigação para a água líquida. Estas características foram mais tarde explicadas como ilusões de ótica, apesar de evidências geológicas recolhidas por missões não tripuladas sugerirem que Marte já teve uma cobertura de água de grande escala em sua superfície.[51] Em 2005, dados de radar revelaram a presença de grandes quantidades de gelo de água nos polos[52] e em latitudes médias.[53][54] A sonda robótica Spirit coletou amostras de compostos químicos que continham moléculas de água em março de 2007, enquanto a sonda Phoenix encontrou amostras de gelo no solo marciano raso em julho de 2008.[55] Em setembro de 2015, cientistas da NASA anunciaram a descoberta de córregos sazonais com água em estado líquido na superfície do planeta com base em dados do Mars Reconnaissance Orbiter.[56]Acredita-se que grandes quantidades de água congelada estejam presas dentro da espessa criosfera de Marte. Os dados de radar da Mars Express e da Mars Reconnaissance Orbiter mostram grandes quantidades de gelo em ambos os polos (julho de 2005)[52][57] e nas latitudes médias (novembro de 2008).[53] A sonda Phoenix retirou amostras de água congelada do solo marciano em 31 de julho de 2008.[55]
Formas de relevo visíveis em Marte também sugerem fortemente que água em estado líquido tenha existido na superfície do planeta. Faixas lineares enormes de terra lavada, conhecidas como canais de escoamento, atravessam a superfície em cerca de 25 lugares. Acredita-se que essas faixas sejam registros de erosões que ocorreram durante a liberação catastrófica de água de aquíferos subterrâneos, embora haja hipóteses de que algumas dessas estruturas tenham resultado da ação de geleiras ou de lava.[58][59] Um dos maiores exemplos, Ma'adim Vallis, tem cerca de 700 km de comprimento e é muito maior que o Grand Canyon, com uma largura de 20 km e uma profundidade de 2 km em alguns lugares. Acredita-se que tenha sido escavado por água corrente no início da história do planeta.[60] Acredita-se que os mais novos desses canais tenham se formado recentemente, há apenas alguns milhões de anos.[61] Em outros lugares, particularmente nas áreas mais antigas da superfície marciana, redes dendríticas de vales em escala menor estão espalhadas por proporções significativas da paisagem. As características desses vales e sua distribuição indicam fortemente que eles foram escavados pelo escoamento resultante da chuva ou queda da neve no início da história de Marte. Fluxos de água subsuperficiais e subterrâneos podem desempenhar papéis subsidiários importantes em algumas redes, mas a precipitação foi, provavelmente, a principal causa da formação em quase todos os casos.[62]
Ao longo de crateras e de paredes de desfiladeiros, há também milhares de acidentes geográficos que parecem semelhantes às ravinas terrestres. As ravinas tendem a surgir nas terras altas do hemisfério sul e próximas ao equador, todas em direção aos polos de 30° de latitude. Vários autores sugeriram que o seu processo de formação envolvia água líquida, provavelmente gelo liquefeito,[63][64] embora outros tenham defendido mecanismos de formação de geada de dióxido de carbono ou o movimento de pó seco.[65][66] Não foram observadas ravinas parcialmente degradadas pelo intemperismo ou crateras de impacto sobrepostas, indicando que estes são acidentes muito jovens, possivelmente ainda ativos atualmente.[64]
Outras características geológicas, como deltas e leques aluviais preservados em crateras, também apontam para condições mais quentes e mais úmidas em algum intervalo ou intervalos na história antiga de Marte.[67] Tais condições requerem necessariamente a presença generalizada de lagos de cratera em uma grande proporção da superfície, para os quais também há evidências mineralógicas, sedimentológicas e geomorfológicas independentes.[68]
Outra evidência de que a água líquida existiu em algum momento sobre a superfície de Marte vem a partir da detecção de minerais específicos, como hematita e goethita, ambos os quais se formam, por vezes, na presença de água.[69] Em 2004, o Opportunity detectou o mineral jarosita, que se forma somente na presença de água ácida, demonstrando que a água uma vez existiu em Marte.[70] Evidências mais recentes de água líquida vêm do achado do mineral gipsita na superfície pelo Opportunity em dezembro de 2011.[71][72] O líder do estudo, Francis McCubbin, cientista planetário da Universidade do Novo México em Albuquerque, analisando hidroxilas em minerais cristalinos de Marte, declarou que a quantidade de água no manto superior de Marte é igual ou maior do que a da Terra, entre 50 e 300 partes por milhão, o que é suficiente para cobrir todo o planeta a uma profundidade de 200 a 1 000 metros.[73]
Em 18 de março de 2013, a NASA relatou evidências, encontradas pelos instrumentos do rover Curiosity, de hidratação mineral, provavelmente sulfato de cálcio hidratado, em várias amostras de rochas, incluindo fragmentos das rochas "Tintina" e "Sutton Inlier", bem como em inclusões e nódulos em outras rochas, como "Knorr" e "Wernicke".[74][75][76] Análises usando o instrumento DAN do Curiosity forneceram evidências da presença de água subterrânea até uma profundidade de 60 cm, num teor de até 4% de água, na travessia do rover desde o Bradbury Landing até a área do Yellowknife Bay, na locação Glenelg.[74]
Em 28 de setembro de 2015, a NASA anunciou que havia encontrado evidência conclusiva de fluxos sazonais de salmoura hidratada em encostas, com base em leituras espectrométricas das áreas escuras das encostas. Essas observações confirmaram hipóteses anteriores, baseadas na época da formação e taxa de crescimento, de que essas estrias escuras resultaram do fluxo de água na subsuperfície muito rasa. As estrias contêm sais hidratados, percloratos, que possuem moléculas de água em sua estrutura cristalina. As estrias fluem pelas encostas no verão marciano, quando a temperatura está acima de -23 °C, e congelam em temperaturas menores.[56][77]
Pesquisadores acreditam que grande parte das planícies baixas do norte do planeta foi coberta por um oceano com centenas de metros de profundidade, embora esta tese ainda seja controversa. Em março de 2015, cientistas afirmaram que tal oceano pode ter tido o tamanho do Oceano Ártico da Terra. Este achado foi obtido a partir da relação entre a água e o deutério na atmosfera marciana moderna em comparação com a relação encontrada na Terra. Oito vezes mais deutério foi encontrado em Marte do que existe na Terra, o que sugere que antigamente Marte tinha níveis significativamente mais elevados de água. Os resultados do rover Curiosity já haviam encontrado uma alta proporção de deutério na cratera Gale, embora não significativamente alta para sugerir a presença de um oceano. Outros cientistas advertiram que o estudo não foi confirmado e apontaram que os modelos climáticos marcianos naquele momento não demonstraram que o planeta era quente o suficiente no passado para manter corpos de água líquida.[78] No entanto, a maioria concorda que um oceano existiu há mais de 3 bilhões de anos, mas há uma variedade de opiniões sobre quanto tempo durou.[79]
Nenhuma água salgada líquida foi definitivamente encontrada em Marte. Mas houve indícios de água escorrendo do subsolo[80][81] e um relatório de 2018 controverso da MARSIS de um lago de 20 km de diâmetro, enterrado quase 1,5 km abaixo da superfície,[82] perto do polo sul do planeta.[83] A investigação revelou que o polo sul é composto por várias camadas de gelo e poeira a uma profundidade de cerca de 1,5 km, distribuídos por uma região de 200 km de largura.[82] Os pesquisadores descobriram que um tipo de salmoura poderia permanecer líquido na superfície do planeta e alguns centímetros abaixo por até seis horas consecutivas em 40% do planeta, principalmente nas latitudes médias a altas do norte.[84] No entanto, essas salmouras nunca ficariam mais quentes do que cerca de -48° C, cerca de 25 graus abaixo da tolerância conhecida pela vida na Terra.[85]
Marte tem duas calotas polares de gelo permanente. Durante o inverno em um dos polos, ele fica em escuridão contínua, que resfria a superfície e provoca a deposição de 25 a 30% da atmosfera em placas de gelo de CO2 (gelo seco).[86] Quando o polo é novamente exposto à luz solar, o CO2 congelado sublima, criando enormes ventos que varrem o polo a velocidades de até 400 km/h. Esses ventos sazonais transportam grandes quantidades de poeira e vapor d’água, dando origem a geadas semelhantes às da Terra e de grandes nuvens cirrus. Nuvens de água e gelo foram fotografadas pelo rover Opportunity em 2004.[87]
As calotas polares em ambos os polos são compostas principalmente (70%) de gelo de água. Dióxido de carbono congelado acumula como uma camada relativamente fina de cerca de um metro de espessura na calota norte apenas no inverno, enquanto a calota sul tem uma cobertura de gelo seco permanente de cerca de oito metros de espessura.[88] Esta cobertura permanente de gelo seco no polo sul é salpicada por alguns tipos de poços circulares que se repetem e estão se expandindo alguns metros por ano; isso sugere que a cobertura permanente de CO2 sobre o gelo do polo sul está se degradando ao longo do tempo.[89] A calota polar norte tem um diâmetro de aproximadamente mil quilômetros durante o verão do hemisfério norte de Marte[90] e contém cerca de 1,6 milhão de quilômetros cúbicos (km³) de gelo, que, se espalhado uniformemente sobre a calota, teria 2 km de espessura.[91] Em comparação, a camada de gelo da Groenlândia tem um volume de 2,85 milhões de quilômetros cúbicos. A calota polar do sul tem um diâmetro de 350 km e uma espessura de 3 km.[92] O volume total de gelo na calota polar sul, mais os depósitos em camadas adjacentes, foi estimado em 1,6 milhão de quilômetros cúbicos.[93] Ambas as calotas polares apresentam calhas espirais, que recente análise do radar SHARAD mostrou serem resultado de ventos catabáticos em espiral devido ao efeito Coriolis.[94][95]
A queda de geada sazonal em algumas áreas perto da calota polar sul resulta na formação de placas transparentes medindo até 1 quilômetro de diâmetro[96] e 1 metro de espessura de gelo seco acima do solo. Com a chegada da primavera, a luz solar aquece o subsolo, e a pressão do CO2 sublimado aumenta sob o bloco, elevando-o e, finalmente, rompendo-o. Isto leva a erupções semelhantes a gêiseres de gás CO2 misturado com areia ou pó de basalto escuro. Este processo é rápido e acontece no espaço de alguns dias, semanas ou meses, uma taxa de variação bastante incomum em geologia - especialmente para Marte. O gás fluindo sob um bloco em direção a um gêiser escava sob o gelo um padrão de canais radiais do tipo teia de aranha, num processo que é o equivalente inverso de uma rede de erosão formada pela água que é drenada por um ralo.[97][98][99][100] Em 2021 cientistas recriaram uma versão parecida com formas de araneiformes negras em seu laboratório.[101] Os experimentos mostram diretamente que os padrões de aranha que observamos em Marte em órbita podem ser esculpidos pela conversão direta de gelo seco de sólido em gasoso.[102]
Pensa-se que a perda de água de Marte para o espaço resulta do transporte de água para a atmosfera superior, onde é dissociada ao hidrogênio e foge do planeta. Mas uma observação em 2019 sugere que a súbita perda de água ocorre na atmosfera superior de Marte, aumentando a abundância de hidrogênio. Na atmosfera superior, a luz solar e a química desassociam moléculas de água em átomos de hidrogênio e oxigênio que a fraca gravidade não pode impedir de escapar para o espaço.[103]
Um cientista revelou que o vapor de água está se acumulando em grandes quantidades e proporções inesperadas a uma altitude de mais de 80 km na atmosfera marciana. As estimativas indicaram que grandes bolsas atmosféricas estão mesmo em uma condição de supersaturação, com a atmosfera contendo 10 a 100 vezes mais vapor de água do que sua temperatura deveria permitir hipoteticamente. Com as taxas de supersaturação observadas, o limite de fuga de água aumentaria significativamente durante estações específicas.[104]
Embora sejam mais lembrados por terem mapeado a Lua, Johann Heinrich von Mädler e Wilhelm Beer foram os primeiros "areógrafos". Eles começaram pela constatação de que a maioria dos acidentes geográficos da superfície de Marte eram permanentes e determinaram com mais precisão o período de rotação do planeta. Em 1840, Mädler reuniu dez anos de observações e desenhou o primeiro mapa de Marte. Em vez de dar nomes para as várias marcas na superfície, Beer e Mädler simplesmente designaram-nas com letras; Sinus Meridiani foi, assim, o acidente "a".[105]
Hoje, as características de Marte são denominadas a partir de uma variedade de fontes. Características de albedo são nomeadas a partir da mitologia clássica. Crateras com mais de 60 km são nomeadas em homenagem a cientistas e escritores já falecidos e outros que contribuíram para o estudo de Marte. Crateras menores que 60 km homenageiam cidades e vilas do mundo com população inferior a 100 mil habitantes. Grandes vales são nomeados pela palavra "Marte" ou "estrela" em várias línguas; pequenos vales são nomeados por rios.[106]
As grandes estruturas de albedo mantêm muitos dos nomes antigos, que são frequentemente atualizados para refletir novos conhecimentos sobre a natureza dessas características. Por exemplo, Nix Olympica (as neves do Olimpo) tornou-se Olympus Mons (Monte Olimpo).[107] A superfície de Marte, vista da Terra, é dividida em dois tipos de áreas, com diferentes albedos. As planícies mais pálidas cobertas de poeira e areia ricas em óxido de ferro avermelhado já foram consideradas como "continentes" marcianos e a elas foram dados nomes como Arabia Terra (terra da Arábia) ou Amazonis Planitia (Planície Amazônica). Acreditava-se que as características escuras eram mares, daí seus nomes Mare Erythraeum, Mare Sirenum e Aurorae Sinus. A maior característica escura vista da Terra é Syrtis Major. A calota polar norte é chamada Planum Boreum, enquanto a calota sul é chamada Planum Australe.[108]
O equador de Marte é definido por sua rotação, mas a localização do seu “meridiano primário" foi estabelecida, como foi a da Terra (em Greenwich), pela escolha de um ponto arbitrário; Mädler e Beer selecionaram uma linha, em 1830, para os primeiros mapas de Marte. Após a nave espacial Mariner 9 fornecer grande quantidade de imagens de Marte em 1972, uma pequena cratera (mais tarde chamada de Airy-0), localizada no Sinus Meridiani ("Baía Meridiana"), foi escolhida para a definição da longitude 0,0°, de forma a coincidir com a seleção original.[109]
Como Marte não tem oceanos e, portanto, não há "nível do mar", uma superfície com elevação zero também teve de ser selecionada para um nível de referência, o que também é chamado de areoide[110] de Marte, análogo ao geoide terrestre. A altitude zero foi definida pela altura em que há 610,5 Pa (6,105 mbar) de pressão atmosférica.[111] Esta pressão corresponde ao ponto triplo da água e é cerca de 0,6% da pressão na superfície do nível do mar na Terra (0,006 atm).[112] Na prática, atualmente esta superfície é definida diretamente pela medição da gravidade por satélites.
A dicotomia da topografia marciana é notável: as planícies do norte são achatadas por fluxos de lava, em contraste com as terras altas do sul, marcadas por crateras de antigos impactos de asteroides. Uma pesquisa de 2008 apresentou evidências sobre uma teoria proposta em 1980 postulando que, quatro bilhões de anos atrás, o hemisfério norte de Marte foi atingido por um objeto de um décimo a dois terços do tamanho da Lua. Se confirmado, isso tornaria o hemisfério norte de Marte o local de uma cratera de impacto de 10 600 km de comprimento por 8,5 mil quilômetros de largura, ou mais ou menos a área da Europa, Ásia e Austrália juntas, superando a Bacia do Polo Sul-Aitken, na Lua, como a maior cratera de impacto do Sistema Solar.[2][3]
Marte é marcado por um conjunto de crateras de impacto: cerca de 43 mil crateras com um diâmetro de 5 quilômetros ou mais foram encontradas em sua superfície.[113] A maior delas é a bacia de impacto Hellas Planitia, uma característica de formação de albedo claramente visível a partir da Terra.[114] Devido à menor massa de Marte, a probabilidade de um objeto colidir com o planeta é cerca de metade da presente na Terra. Marte fica mais perto do cinturão de asteroides, por isso tem uma chance maior de ser atingido por materiais oriundos dessa região. O planeta é também mais suscetível a ser atingido por cometas de período curto, ou seja, aqueles que se encontram dentro da órbita de Júpiter.[115] Apesar disso , há muito menos crateras em Marte em comparação com a Lua, porque a atmosfera de Marte fornece proteção contra meteoros pequenos. Algumas crateras têm uma geomorfologia que sugere que o solo se tornou úmido após o impacto do meteoro.[116]
O vulcão Monte Olimpo é um vulcão extinto na vasta região de Tharsis, que contém vários outros grandes vulcões. O Monte Olimpo é três vezes maior que o Monte Everest, que por comparação tem pouco mais de 8,8 km de altura.[117] É a primeira ou a segunda montanha mais alta do Sistema Solar, dependendo da forma de medição escolhida, com fontes que vão de cerca de 21 a 27 km de altura.[118][119]
O grande desfiladeiro Valles Marineris (latim para Vales Mariner, também conhecido como Agathadaemon nos velhos mapas dos canais marcianos), tem um comprimento de quatro mil quilômetros e uma profundidade de até sete quilômetros. O comprimento do Valles Marineris é equivalente ao comprimento do continente europeu e estende-se através de um quinto da circunferência de Marte. Em comparação, o Grand Canyon na Terra tem 446 km de comprimento e quase 2 km de profundidade. O Valles Marineris foi formado devido à expansão da área de Tharsis, que causou o colapso da crosta na área do desfiladeiro. Em 2012, foi proposto que o Valles Marineris não é apenas um graben, mas também um limite de placa, onde ocorreu um movimento transversal de 150 km, fazendo de Marte um planeta com, possivelmente, duas placas tectônicas.[120][121]
As imagens do THEMIS a bordo de sonda Mars Odyssey da NASA revelaram sete possíveis entradas de cavernas nos flancos do vulcão Arsia Mons.[122] As cavernas, nomeadas em homenagem a entes queridos de seus descobridores, são conhecidas coletivamente como as "sete irmãs".[123] As entradas das cavernas medem de 100 a 252 metros de largura e acredita-se que tenham, pelo menos, de 73 a 96 metros de profundidade. Dado que a luz não atinge o piso da maioria das cavernas, é possível que elas se estendam muito mais profundamente do que as estimativas inferiores e sejam mais largas abaixo da superfície. A caverna "Dena" é a única exceção: o seu chão é visível e tem 130 metros de profundidade. Os interiores destas cavernas podem ser protegidos contra micrometeoritos, radiação UV, erupções solares e partículas de alta energia que bombardeiam a superfície do planeta.[124]
Pegadas de dunas em Hellas, também conhecidas como a insígnia da Frota Estelar Star Trek,[125] são grandes dunas em forma de crescente (barchan). A insígnia foi criada quando a lava fluiu sobre a planície e ao redor das dunas, mas não sobre elas. A lava solidificou, mas essas dunas ainda se erguiam como ilhas e o vento continuava a soprar. Ao final, as pilhas de areia que eram as dunas migraram, deixando essas "pegadas" na planície de lava. Elas também são chamadas de "moldes de dunas" e registram a presença de dunas cercadas por lava.[126]
Marte perdeu sua magnetosfera há 4 bilhões de anos,[127] então o vento solar interage diretamente com a ionosfera marciana, diminuindo a densidade atmosférica ao remover átomos da camada exterior. Ambas as sondas Mars Global Surveyor e Mars Express detectaram partículas atmosféricas ionizadas arrastadas para o espaço a partir de Marte[127][128] e esta perda atmosférica está sendo estudada pela sonda MAVEN. Em comparação com a Terra, a atmosfera de Marte é muito rarefeita. A pressão atmosférica na superfície varia hoje entre um mínimo de 30 Pa (0,030 kPa) no Monte Olimpo para mais de 1 155 Pa (1,155 kPa) em Hellas Planitia, com uma pressão média ao nível da superfície de 600 Pa (0,60 kPa).[129] A maior densidade atmosférica em Marte é igual à densidade encontrada 35 km acima da superfície da Terra.[130] A pressão de superfície média resultante é de apenas 0,6% a da Terra (101,3 kPa). A altura de escala da atmosfera é cerca de 10,8 km,[131] que é maior do que a da Terra (6 km), porque a gravidade de superfície de Marte é de apenas 38% da gravidade da Terra, o que é compensado tanto pela temperatura mais baixa quanto pelo peso molecular 50% maior da atmosfera de Marte. A atmosfera de Marte é composta por cerca de 96% de dióxido de carbono, 1,93% de argônio e 1,89% de nitrogênio, juntamente com traços de oxigênio e água.[9][132] A atmosfera é muito empoeirada, contendo partículas de cerca de 1,5 µm de diâmetro que dão ao céu marciano uma cor opaca quando vista da superfície.[133]
Metano foi detectado na atmosfera de Marte, com uma fração molar de cerca de 30 ppb;[134][135] ele ocorre em plumas extensas, e os perfis implicam que o metano foi liberado a partir de regiões distintas. No meio do verão do norte, a pluma principal continha 19 mil toneladas métricas de metano, com uma força de fonte estimada de 0,6 kg por segundo.[136][137] Os perfis sugerem que pode haver duas regiões de origem, a primeira centrada perto de 30°N 260°W e a segunda perto de 0°N 310°W.[136] Estima-se que Marte deva produzir 270 toneladas/ano de metano.[136][138]
O metano pode existir na atmosfera de Marte por um período limitado de tempo até ser destruído – as estimativas do seu tempo de vida variam entre 0,6 a 4 anos terrestres.[136][139] A sua presença, apesar desta vida curta, indica a existência de uma fonte ativa do gás no planeta. Atividade vulcânica, impactos de cometas e a presença de formas de vida microbianas metanogênicas estão entre as possíveis fontes. O metano também poderia ser produzido por um processo não biológico chamado serpentinização, que envolve água, dióxido de carbono e o mineral olivina, que se sabe ser comum em Marte.[140]
O rover Curiosity, que pousou em Marte em agosto de 2012, é capaz de fazer medições que distinguem entre diferentes isotopólogos de metano;[141] mas mesmo que a missão determine que a vida microscópica marciana é a fonte do metano, essas formas de vida provavelmente residem muito abaixo da superfície, fora do alcance do rover.[142] As primeiras medições com o Tunable Laser Spectrometer (TLS) indicaram que há menos de 5 ppb de metano no local de pouso no momento da medição.[143][144][145][146] Em 19 de setembro de 2013, cientistas da NASA, com base em outras medições feitas pela Curiosity, não relataram a detecção de metano atmosférico, com um valor medido de 0,18 ± 0,67 ppbv correspondente a um limite máximo de apenas 1,3 ppbv (limite de confiança de 95%) e, como resultado, concluíram que a probabilidade de atividade microbiana metanogênica atual em Marte é reduzida.[147][148][149] A sonda Mars Orbiter Mission, da Índia, está pesquisando metano na atmosfera,[150] enquanto a ExoMars Trace Gas Orbiter, planejada para ser lançada em 2016, irá estudar mais o metano, bem como os seus produtos de decomposição, como formaldeído e metanol.[151][152]
Amônia também foi detectada em Marte pelo satélite Mars Express, mas com a sua vida útil relativamente curta, não ficou claro o que a tenha produzido. A amônia não é estável na atmosfera marciana e desintegra-se depois de algumas horas. Uma fonte possível é a atividade vulcânica.[153]
A sonda Trace Gas Orbiter (TGO), da Agência Espacial Europeia, chegou a Marte em 2016, e em 2018 começou a escanear a atmosfera por metano. Dois dos espectrômetros do TGO - um instrumento belga chamado NOMAD e um russo chamado ACS - foram projetados para detectar o metano em concentrações muito baixas. No entanto, o satélite europeu não detectou um único vestígio de metano.[154]
Enormes plumas semelhantes a nuvens, 260 km acima da superfície de Marte, chegam a entrar na exosfera, onde a atmosfera se funde com o espaço interplanetário. Nenhum dos esclarecimentos costumeiros para tais nuvens faz sentido, uma vez que a água, o gelo de dióxido de carbono, as tempestades de poeira ou as descargas de luz auroral, na maior parte das vezes, não atingem tais alturas. Em 2019, cientistas propuseram que elas devem sua existência ao fenômeno chamado "fumaça meteórica", poeira gelada criada por detritos espaciais que se chocam com a atmosfera do planeta. Cerca de duas a três toneladas de detritos espaciais colidem em Marte todos os dias, em média, e à medida que esses meteoros se desintegram na atmosfera do planeta, injetam um enorme volume de poeira no ar.[155]
Em 1994 a sonda Mars Express da Agência Espacial Europeia descobriu um brilho ultravioleta proveniente de “guarda-chuvas magnéticos” no hemisfério sul. Marte não possui um campo magnético global que guie as partículas carregadas que entram na atmosfera, mas tem múltiplos campos magnéticos em forma de guarda-chuva, principalmente no hemisfério sul, que são remanescentes de um campo global que decaiu bilhões de anos atrás.
No final de dezembro de 2014, a sonda MAVEN da NASA detectou evidência de auroras muito espalhadas pelo hemisfério norte e descendo até aproximadamente 20-30 graus de latitude norte em relação ao equador de Marte. As partículas penetravam na atmosfera marciana, criando auroras abaixo de 100 km da superfície (as auroras da Terra variam de altitude entre 100 e 500 km). Os campos magnéticos no vento solar caem como cortinas sobre Marte, inclusive para a atmosfera, e as partículas carregadas simplesmente seguem essas linhas para a atmosfera, fazendo com que as auroras aconteçam fora dos guarda-chuvas magnéticos.[156]
Em 18 de março de 2015, a NASA anunciou a detecção de uma aurora que não é completamente entendida, bem como uma não explicada nuvem de poeira na atmosfera de Marte.[157]
As observações anteriores não capturaram nenhum brilho verde em Marte, mas entre 24 de abril e 1 de dezembro de 2019, uma luz verde foi observada na atmosfera de Marte de altitudes de observações que variam de 20 a 400 quilômetros da superfície marciana. A emissão foi mais forte a uma altitude de cerca de 80 quilômetros e variou dependendo da distância variável entre Marte e o Sol.[158]
De todos os planetas do Sistema Solar, Marte é o que possui as estações do ano mais parecidas com as da Terra, devido às inclinações semelhantes de eixos de rotação dos dois planetas. As durações das estações marcianas são cerca de duas vezes as da Terra, já que Marte está a uma maior distância do Sol, o que leva o ano marciano a ter duração equivalente a cerca de dois anos terrestres. As temperaturas de superfície de Marte variam de −143 °C (no inverno nas calotas polares)[159] até máximas de 35 °C (no verão equatorial).[160] A ampla variação de temperaturas é devida à fina atmosfera, que não consegue armazenar muito calor solar, à baixa pressão atmosférica e à baixa inércia térmica do solo marciano.[161] O planeta também é 1,52 vez mais distante do Sol que a Terra, o que resulta em apenas 43% da quantidade de luz solar em comparação com a Terra.[162]
Se Marte tivesse uma órbita semelhante à da Terra, as suas estações também seriam semelhantes, porque a sua inclinação axial é próxima à da Terra. A relativamente grande excentricidade da órbita de Marte tem um efeito significativo. O planeta está mais próximo do periélio quando é verão no hemisfério sul e inverno no norte, e próximo do afélio quando é inverno no hemisfério sul e verão no norte. Como resultado, as estações do ano no hemisfério sul são mais extremas e as estações do ano no norte são mais brandas. As temperaturas de verão no sul podem ser até 30 kelvin maiores do que as temperaturas equivalentes de verão no norte.[163]
Marte tem as maiores tempestades de poeira do Sistema Solar. Estas podem variar de uma tempestade sobre uma pequena área até tempestades gigantescas que cobrem todo o planeta. Elas tendem a ocorrer quando Marte está mais próximo do Sol e demonstraram aumentar a temperatura global.[164]
A distância média de Marte ao Sol é de cerca de 230 milhões de quilômetros (1,5 UA) e seu período orbital é de 687 dias terrestres. O dia solar em Marte é apenas um pouco maior do que um dia na Terra: 24 horas, 39 minutos e 35,244 segundos. Um ano marciano é igual a 1,8809 ano terrestre, ou seja, 1 ano, 320 dias e 18,2 horas.[9] A inclinação do eixo de Marte é de 25,19 graus, semelhante à da Terra.[9] Como resultado, Marte tem estações como a Terra, embora sejam quase duas vezes mais longas, pois seu período orbital é maior nesta proporção . Atualmente, a orientação do polo norte de Marte está próxima da estrela Deneb.[169] Marte passou pelo seu afélio em março de 2010[170] e pelo seu periélio em março de 2011.[171]
Marte tem uma excentricidade orbital relativamente acentuada, de cerca de 0,09; entre os outros sete planetas do Sistema Solar, só Mercúrio mostra maior excentricidade. Sabe-se que, no passado, Marte teve uma órbita muito mais circular do que atualmente. Em um ponto há 1,35 milhão de anos terrestres, Marte tinha uma excentricidade de cerca de 0,002, muito menor do que a da Terra hoje.[172] O ciclo de excentricidade de Marte é de 96 mil anos terrestres, comparado ao ciclo de 100 mil anos da Terra.[173] O planeta tem um ciclo de excentricidade muito mais longo com um período de 2,2 milhões de anos terrestres e isso ofusca o ciclo de 96 mil anos nos gráficos de excentricidade. Durante os últimos 35 mil anos, a órbita de Marte foi ficando um pouco mais excêntrica por causa dos efeitos gravitacionais dos outros planetas. A menor distância entre a Terra e Marte continuará a diminuir ligeiramente nos próximos 25 mil anos.[174]
O entendimento atual de habitabilidade planetária – a viabilidade de um mundo desenvolver condições ambientais favoráveis ao surgimento de vida – favorece planetas que possuam água líquida em sua superfície. Isto frequentemente requer que a órbita de um planeta esteja dentro da zona habitável, que para o Sol se localiza entre logo depois de Vênus e aproximadamente o semieixo maior de Marte.[175] Durante o periélio, Marte penetra nesta região, mas a fina (baixa pressão) atmosfera do planeta impede a existência de água líquida em grandes regiões por muito tempo. O fluxo de água líquida no passado demonstra o potencial do planeta para a habitabilidade. Evidência recente sugeriu que qualquer água na superfície marciana deve ter sido muito salgada e ácida para suportar uma vida terrestre regular.[176]
A falta de uma magnetosfera e a atmosfera extremamente fina de Marte são um desafio: o planeta possui pequena transferência de calor pela sua superfície, pouco isolamento contra o bombardeio do vento solar e pressão atmosférica insuficiente para reter água na forma líquida. Marte está quase, ou totalmente, geologicamente morto, e o fim da atividade vulcânica aparentemente interrompeu a reciclagem de materiais e produtos químicos entre a superfície e o interior do planeta.[177]
Existem evidências de que o planeta tenha sido significativamente mais habitável no passado que nos dias de hoje, mas o fato de que tenha albergado vida permanece incerto. As sondas Viking da década de 1970 continham dispositivos projetados para detectar microrganismos no solo marciano e tiveram alguns resultados positivos, inclusive um aumento temporário na produção de CO2 com a exposição a água e nutrientes. Este sinal de vida foi mais tarde contestado por cientistas, resultando em um debate intenso, com o cientista da NASA Gilbert Levin sustentando que a Viking pode ter encontrado vida. Uma reanálise dos dados da Viking, à luz do moderno conhecimento de formas extremófilas de vida, sugeriu que os testes da Viking não eram suficientemente sofisticados para detectar essas formas de vida e podem até mesmo ter matado uma hipotética forma de vida.[178] Testes conduzidos pela sonda Phoenix Mars mostraram que o solo tem um pH alcalino e contém magnésio, sódio, potássio e cloro.[179] Os nutrientes do solo podem ser capazes de suportar vida, mas a vida ainda assim teria que ser protegida da intensa luz ultravioleta.[180] Análise recente do meteorito marciano EETA79001 encontrou 0,6 ppm de ClO4−, 1,4 ppm de ClO3− e 16 ppm de NO3−, a maior parte provavelmente de origem marciana. O ClO3− sugere a presença de outros compostos altamente oxidantes de cloro e oxigênio, como ClO2− e ClO, produzidos tanto por oxidação do Cl por ultravioleta quanto por radiólise do ClO4− por raios-X. Portanto, somente substâncias orgânicas ou formas de vida altamente refratárias ou bem protegidas (subsuperficiais) teriam chance de sobreviver.[181] Uma análise de 2014 da Phoenix WCL mostrou que o Ca(ClO4)2 no solo da Phoenix não interagiu com água líquida de qualquer forma talvez nos últimos 600 milhões de anos. Se tivesse, o altamente solúvel Ca(ClO4)2 em contato com água líquida teria formado somente CaSO4. Isto sugere um ambiente extremamente árido, com mínima ou nenhuma interação com água líquida.[182] Foi encontrado, em 2019, um grupo de compostos orgânicos, tiofenos, que normalmente ocorrem na Terra em querogênio, carvão e petróleo bruto, bem como em estromatólitos e microfósseis.[183] Isso sugere um processo biológico, provavelmente envolvendo bactérias.
Cientistas propuseram que os glóbulos de carbonato encontrados no meteorito ALH84001, que se acredita ter se originado em Marte, podem ser micróbios fossilizados que existiam em Marte quando o meteorito foi arrancado da superfície de Marte por um choque de meteoro há cerca de 15 milhões de anos. Esta proposta foi recebida com ceticismo e foi sugerida uma origem exclusivamente inorgânica para as formas.[184] Pequenas quantidades de metano e metanal detectadas pelas sondas em Marte foram indicadas como possíveis evidências para a vida, uma vez que esses compostos químicos se decompõem rapidamente na atmosfera marciana.[185][186] Entretanto, uma alternativa é que esses compostos sejam repostos por vulcões ou outros meios geológicos, como a serpentinização.[140] Vidro formado pelo impacto de meteoros, que na Terra pode preservar sinais de vida, foi encontrado na superfície de crateras de impacto de Marte.[187][188] Da mesma forma, este vidro poderia ter preservado sinais de vida se esta existisse no local.[189][190][191]
Em junho de 2018, a NASA informou que o rover Curiosity havia encontrado evidências de compostos orgânicos complexos de rochas com idade de aproximadamente 3,5 bilhões de anos, cujas amostras vieram de dois locais distintos em um lago seco na cratera Gale. As amostras de rochas, quando pirolisadas pelo instrumento da Curiosity, liberaram uma série de moléculas orgânicas; estes incluem tiofenos contendo enxofre, compostos aromáticos tais como benzeno e tolueno, além de compostos alifáticos tais como propano e buteno. Os níveis de compostos orgânicos são 100 vezes maiores que as descobertas anteriores. Os autores especulam que a presença de enxofre pode ter ajudado a preservar os compostos orgânicos. Os produtos de decomposição se assemelham aos gerados pelo querogênio, um precursor do petróleo e do gás natural na Terra. A NASA afirmou que essas descobertas não são evidências de que a vida existiu no planeta, mas que os compostos orgânicos necessários para sustentar a vida microscópica estavam presentes. Devido à forma como a atmosfera marciana pode preservar esses compostos, pode haver fontes mais profundas de compostos orgânicos no planeta.[192]
Em julho de 2018, cientistas relataram a descoberta de um lago subglacial em Marte, o primeiro corpo estável de água conhecido no planeta. Ele fica a 1,5 km abaixo da superfície na base da calota polar sul e tem cerca de 20 quilômetros de largura.[193][194] O lago foi descoberto usando o radar MARSIS a bordo da sonda Mars Express, e os dados foram coletados entre maio de 2012 e dezembro de 2015.[195] O lago está localizado numa área plana que não exibe características topográficas peculiares. É principalmente cercado por terrenos mais altos, exceto em seu lado oriental, onde há uma depressão.[193]
Marte tem duas luas naturais relativamente pequenas — Fobos, com cerca de 22 quilômetros de diâmetro, e Deimos, com cerca de 12 quilômetros de diâmetro — que têm órbitas próximas ao planeta. Acredita-se que essas luas sejam asteroides capturados pelo campo gravitacional marciano, mas a sua origem verdadeira permanece incerta.[196] Ambos os satélites foram descobertos em 1877 por Asaph Hall e foram nomeados em homenagem aos deuses Fobos (pânico/medo) e Deimos (terror/horror), que na mitologia grega acompanhavam seu pai, Ares, o deus da guerra, durante as batalhas. Marte era a contraparte romana de Ares.[197][198] No grego moderno, porém, o planeta mantém seu antigo nome, Ares (Aris: Άρης).[199]
Vistos da superfície de Marte, os movimentos de Fobos e Deimos parecem muito diferentes do da Lua. Fobos nasce no oeste, se põe a leste e nasce novamente em apenas 11 horas. Deimos, por estar quase em órbita sincronizada — quando o período orbital iguala o período de rotação do planeta — nasce como esperado no leste, mas lentamente. Apesar da órbita de 30 horas de Deimos, ele leva 2,7 dias entre o nascente e o poente, para um observador no equador.[200]
Como a órbita de Fobos está abaixo da altitude síncrona, as forças de maré a partir de Marte estão gradualmente diminuindo a sua órbita. Em cerca de 50 milhões de anos, o satélite ou colidirá com a superfície marciana ou irá desintegrar-se em uma estrutura em forma de anel ao redor de Marte.[200]
A origem das duas luas não é bem compreendida. Seu baixo albedo e a composição de condrito carbonáceo foram considerados semelhantes aos de asteroides, apoiando a teoria de captura gravitacional. A órbita instável de Fobos parece apontar para uma captura relativamente recente. Mas ambas têm órbitas circulares, próximas do equador, o que é muito incomum para objetos capturados, já que a dinâmica de captura exigida é complexa. A possibilidade de acreção no início da história de Marte também é plausível, mas não é compatível com uma composição parecida com a de asteroides, em vez de com a do próprio planeta. Uma terceira possibilidade é o envolvimento de um terceiro corpo ou algum tipo de impacto.[201] Linhas mais recentes de evidências sobre Fobos sugerem que o satélite tem um interior altamente poroso[202] e uma composição contendo principalmente filossilicatos e outros minerais conhecidos de Marte,[203] o que aponta a origem de Fobos para o material ejetado por um impacto em Marte e que foi reagrupado na órbita marciana,[204] semelhante à teoria dominante para a origem da Lua da Terra. Embora os espectros VNIR (em inglês:  visible and near-infrared) das luas de Marte se assemelhem aos de asteroides do cinturão externo, o espectro infravermelho termal de Fobos é inconsistente com condritos de qualquer tipo.[203]
Marte pode ter luas com menos de 50 ou 100 metros de diâmetro, e prediz-se a existência de um anel de poeira entre Fobos e Demos.[5]
Com a existência de várias sondas e rovers, agora é possível estudar a astronomia do céu marciano. Fobos, uma das duas luas de Marte, tem cerca de um terço do diâmetro angular da Lua cheia como ela aparece na Terra, enquanto Deimos aparece mais ou menos com uma estrela e apenas um pouco mais brilhante do que Vênus na Terra.[205] Vários fenômenos conhecidos na Terra foram observados em Marte, como meteoros e auroras.[206] Uma passagem da Terra vista de Marte ocorrerá em 10 de novembro de 2084.[207] Há também trânsitos de Mercúrio e de Vênus, e as luas Fobos e Deimos são de diâmetro angular tão pequeno que seus "eclipses solares" parciais são melhor considerados como trânsitos (ver Trânsito de Deimos em Marte).[208][209]
Em 19 de outubro de 2014, o cometa Siding Spring passou extremamente próximo a Marte (cerca de 140 mil quilômetros), tão perto que sua coma pode ter envolvido o planeta.[210][211][212][213][214][215]
Pela órbita de Marte ser excêntrica, a sua magnitude aparente em oposição ao Sol pode variar de -3,0 a -1,4. O brilho mínimo é de magnitude 1,6, quando o planeta está em conjunção com o Sol.[49] Marte geralmente aparece distintamente amarelo, laranja ou vermelho; a cor real do planeta está mais próximo de caramelo, e a vermelhidão observada é apenas poeira na sua atmosfera. O rover Spirit, da NASA, registrou imagens de uma paisagem marrom-esverdeada com pedras azul-acinzentadas e manchas de areia vermelho-claras.[216] Quando mais distante da Terra, fica a mais de sete vezes mais longe de nosso planeta do que quando está próximo. Quando menos favoravelmente posicionado, ele pode ser perdido no brilho do Sol por meses. Em seus momentos mais favoráveis — em intervalos entre 15 e 17 anos, sempre entre o final de julho e o final de setembro — muitos detalhes de sua superfície podem ser vistos com um telescópio. Especialmente notáveis, mesmo com baixa ampliação, são as suas calotas polares.[217]
Conforme Marte se aproxima ao ponto de oposição, começa um período de movimento retrógrado em que o planeta parece se mover para trás, em um movimento de looping em relação às estrelas de fundo. A duração deste movimento retrógrado tem a duração de cerca de 72 dias e Marte atinge o seu pico de luminosidade no meio deste movimento.[218]
O ponto em que a longitude geocêntrica de Marte é 180° em relação ao Sol é conhecido como oposição, que está perto do momento de maior aproximação com a Terra. O momento da oposição pode ocorrer a até 8 ½ dias da maior aproximação. A distância nas maiores aproximações varia entre 54[219] e 103 milhões de quilômetros, devido às órbitas elípticas dos planetas, o que causa uma variação comparável em tamanho angular.[220] A última oposição de Marte ocorreu em 8 de abril de 2014, a uma distância de cerca de 93 milhões de quilômetros.[221] O tempo médio entre oposições sucessivas de Marte (o seu período sinódico) é de 780 dias, mas o número de dias entre as datas de oposições sucessivas pode variar entre 764 e 812.[222]
Marte fez a sua maior aproximação com a Terra e o seu brilho aparente máximo dos últimos 60 mil anos, 55 758 006 km (0,372719 UA) e magnitude -2,88, em 27 de agosto de 2003 às 09h51min13 UTC. Isto ocorreu quando Marte estava a um dia da sua oposição e a cerca de três dias do seu periélio, tornando o planeta particularmente fácil de ver a partir da Terra. Estima-se que a última vez em que o planeta chegou tão perto foi em 12 de setembro de 57 617 a.C.; o próximo momento será no ano 2287.[223] Esta aproximação recorde foi apenas ligeiramente mais próxima do que outras aproximações recentes. Por exemplo, a distância mínima em 22 de agosto de 1924 foi de 0,37285 UA e a distância mínima em 24 de agosto de 2208 será de 0,37279 UA.[173]
A existência de Marte como um objeto errante no céu noturno foi registrada por astrônomos do Egito Antigo e, em 1534 a.C., eles já estavam familiarizados com o movimento retrógrado do planeta.[224] No período do Império Neobabilônico, os astrônomos babilônios faziam registros regulares das posições dos planetas e observações sistemáticas do seu comportamento. Sobre Marte, eles sabiam que o planeta fazia 37 períodos sinódicos, ou 42 circuitos do zodíaco, a cada 79 anos. Eles também inventaram métodos aritméticos para fazer pequenas correções para as posições previstas dos planetas.[225][226]	
No século IV a.C., Aristóteles observou que Marte desaparecia por trás da Lua durante uma ocultação, indicando que o planeta estava mais distante.[227] Ptolomeu, um grego que vivia em Alexandria,[228] tentou resolver o problema do movimento orbital de Marte. O modelo de Ptolomeu e sua obra coletiva sobre astronomia foram apresentados no Almagesto, que se tornou o principal tratado da astronomia ocidental nos quatorze séculos seguintes.[229] A literatura da China antiga confirma que Marte era conhecido pelos astrônomos chineses no século IV.[230] No século V d.C., o texto astronômico indiano Surya Siddhanta estimou o diâmetro de Marte.[231] Nas culturas da Ásia Oriental, Marte era tradicionalmente conhecido como a “estrela de fogo”, com base nos Cinco elementos.[232][233][234]
Durante o século XVII, Tycho Brahe mediu a paralaxe diurna de Marte, que Johannes Kepler usou para fazer um cálculo preliminar da distância do planeta.[235] Quando o telescópio se tornou disponível, a paralaxe diurna de Marte foi novamente medida em um esforço para determinar a distância Sol-Terra. Isto foi realizado pela primeira vez por Giovanni Domenico Cassini em 1672. As primeiras medições da paralaxe foram prejudicadas pela qualidade dos instrumentos.[236] A única ocultação observada de Marte por Vênus foi a de 13 de outubro de 1590, vista por Michael Maestlin em Heidelberg.[237] Em 1610, Marte foi visto por Galileu Galilei, que foi o primeiro a observá-lo através de um telescópio.[238] A primeira pessoa a desenhar um mapa de Marte que exibia características da superfície foi o astrônomo holandês Christiaan Huygens.[239]
Por volta do século XIX, a resolução dos telescópios atingiu um nível suficiente para que as características da superfície de Marte pudessem ser identificadas. Uma oposição periélica de Marte ocorreu em 5 de setembro de 1877 e, naquele ano, o astrônomo italiano Giovanni Schiaparelli usou um telescópio de 22 cm em Milão para produzir o primeiro mapa detalhado do planeta vermelho. Estes mapas continham características chamadas por Schiaparelli de canali, que mais tarde mostraram ser uma ilusão de óptica. Estes canali eram supostamente longas linhas retas na superfície de Marte para as quais ele deu nomes de rios famosos na Terra.[240][241]
Influenciado pelas observações, o orientalista Percival Lowell fundou um observatório que tinha um telescópio de 30 cm e outro de 45 cm. O observatório foi utilizado para a exploração de Marte durante uma última boa oportunidade, em 1894. Ele publicou vários livros sobre Marte e a vida no planeta, que tiveram uma grande influência sobre o público.[242] Os canali também foram encontrados por outros astrônomos, como Henri Joseph Perrotin e Louis Thollon em Nice, usando um dos maiores telescópios do mundo naquela época.[243][244]
As mudanças sazonais (a diminuição das calotas polares e a formação de áreas escuras durante o verão marciano), em combinação com os canais, levaram a especulações sobre a presença de vida em Marte e fizeram surgir uma crença, mantida por muito tempo, de que Marte continha vastos mares e vegetação. O telescópio nunca chegou a uma resolução suficiente para provar quaisquer destas especulações. À medida que telescópios maiores foram usados, foram observados menos canali retos e longos. Durante uma observação em 1909 por Camille Flammarion com um telescópio de 84 cm, foram observados padrões irregulares, mas os canali não foram vistos.[245]
Mesmo na década de 1960, artigos foram publicados sobre a "biologia marciana", deixando de lado outras explicações para as mudanças sazonais do planeta. Cenários detalhados do metabolismo e dos ciclos químicos de um ecossistema funcional chegaram a ser publicados.[246] Desde que uma nave espacial visitou o planeta durante as missões Mariner da NASA nos anos 1960 e 1970, estes conceitos foram radicalmente quebrados. Além disso, os resultados das experiências de detecção de vida pela Viking auxiliaram para que a hipótese de um planeta hostil e morto fosse geralmente aceita.[247] A Mariner 9 e a Viking forneceram dados que permitiram a obtenção de mapas melhores do planeta. Outro grande salto foi a missão Mars Global Surveyor, lançada em 1996 e que funcionou até o final de 2006, e permitiu a obtenção de mapas completos e extremamente detalhados da topografia, campo magnético e minerais da superfície de Marte.[248] Estes mapas estão agora disponíveis on-line, por exemplo, no Google Mars. O Mars Reconnaissance Orbiter e a Mars Express continuaram explorando com novos instrumentos e apoiando as sondas na superfície. A NASA fornece duas ferramentas on-line: Mars Trek, que apresenta visualizações do planeta a partir de dados de 50 anos de exploração, e a Experience Curiosity, que simula viagens em Marte em 3-D com a Curiosity.[249]
Dúzias de naves espaciais não tripuladas, como sondas orbitais e rovers, foram enviadas para Marte pela União Soviética, Estados Unidos, Europa e Índia para estudar a superfície, o clima e a geologia do planeta. Atualmente, a informação está sendo obtida por sete sondas ativas na superfície ou em órbita de Marte, sendo cinco orbitais e dois rovers, quais sejam: 2001 Mars Odyssey,[250] Mars Express, Mars Reconnaissance Orbiter (MRO), MAVEN, Mars Orbiter Mission, Opportunity e Curiosity. O público pode solicitar imagens de Marte da MRO através do programa HiWish.
O Mars Science Laboratory, chamado de Curiosity, foi lançado em 26 de novembro de 2011 e chegou a Marte em 6 de agosto de 2012 (UTC). É maior e mais avançado do que os Mars Exploration Rovers, com uma velocidade de até 90 metros por hora.[251] Os experimentos incluem um analisador químico a laser que pode deduzir a composição de rochas a uma distância de 7 m.[252] Em 10 de fevereiro de 2013, o Curiosity obteve as primeiras amostras de rochas profundas já retiradas de outro corpo planetário, utilizando a sua broca embarcada.[253]
A Organização Indiana de Pesquisa Espacial lançou a missão Mars Orbiter Mission em 5 de novembro de 2013, com o objetivo de analisar a atmosfera e a topografia marcianas. Equipada com sensores de metano, câmeras multi-espectrais, espectrômetros de imagem em infravermelho termal, fotômetros e outros itens em sua carga útil, a missão procura expandir a compreensão humana do Sistema Solar. Lançado de PSLV-C25, a missão Mars Reconnaissance Orbiter usou uma órbita de transferência de Hohmann para escapar da influência gravitacional da Terra e catapultar para uma viagem de nove meses até Marte. Essa é a primeira missão interplanetária bem-sucedida da Ásia e a sonda Mangalyaan estuda o planeta Marte desde 24 de setembro de 2014.[254]
A Agência Espacial Europeia, em colaboração com a Agência Espacial Federal Russa - Roscosmos, enviou a sonda ExoMars Trace Gas Orbiter (TGO) e a sonda de superfície Schiaparelli em março de 2016. O orbitador TGO conseguiu entrar na órbita marciana em 19 de outubro.[255] Contudo, o contato com o pousador Schiaparelli EDM foi perdido quando este já estava próximo ao solo. Imagens feitas pela MRO da NASA comprovaram que a sonda impactou violentamente contra o solo marciano e foi totalmente destruída. Uma hipótese sugere que os retrofoguetes do pousador, que deveriam reduzir sua velocidade, se apagaram muito antes do esperado.[256][257]
Está planejado para maio de 2018 o lançamento da sonda de superfície InSight, juntamente com dois satélites CubeSats, que vão sobrevoar Marte e fornecer dados de telemetria da superfície. As sondas estão previstas para chegar a Marte em novembro de 2016.[258]
Em 2018 a ESA enviará o "rover" ExoMars. A NASA planeja lançar o seu "rover" astrobiológico Mars 2020 em 2020. A sonda Mars Hope, dos Emirados Árabes Unidos, está planejada para lançamento em 2020, atingindo a órbita de Marte em 2021. Ela tem o objetivo de fazer um estudo global da atmosfera marciana. Diversos planos para uma missão tripulada a Marte foram propostos ao longo do século XX e já no século XXI, mas nenhum plano em andamento tem data de chegada anterior a 2025.
Marte é nomeado em homenagem ao deus romano da guerra. Em diferentes culturas, Marte representa a masculinidade e a juventude. Seu símbolo astronômico, um círculo com uma seta apontando para o lado superior direito, também é usado como símbolo do sexo masculino. As muitas falhas nas sondas de exploração de Marte resultaram em uma contracultura satírica que associa esses acidentes com a existência de um tipo de "Triângulo das Bermudas" entre a Terra e o planeta, alguma "maldição marciana" ou um “grande espírito maléfico das galáxias” que se alimentaria das sondas a Marte.[259]
A ideia popular de que Marte era povoado por marcianos inteligentes se popularizou no final do século XIX. As observações dos canali por Giovanni Schiaparelli, combinadas com os livros de Percival Lowell sobre o tema, propuseram a noção padrão de um planeta seco, frio e prestes a morrer, com obras de irrigação sendo feitas por civilizações antigas.[260]
Muitas outras observações e declarações de personalidades notáveis se somaram ao que tem sido chamado de "febre marciana".[261] Em 1899, enquanto investigava o ruído de rádio atmosférico usando receptores em seu laboratório em Colorado Springs, o inventor Nikola Tesla observou sinais repetitivos que mais tarde ele imaginou serem comunicações de rádio vindas de outro planeta, possivelmente de Marte. Em uma entrevista de 1901, Tesla disse:
Foi algum tempo depois que o pensamento passou em minha mente de que os distúrbios que eu tinha observado poderiam se dever a um controle inteligente. Embora eu não tenha conseguido decifrar o seu significado, para mim era impossível pensar neles como sendo inteiramente acidentais. Um sentimento está constantemente crescendo em mim de que eu tenha sido o primeiro a ouvir a saudação de um planeta para outro.[262]As teorias de Tesla ganharam o apoio de Lord Kelvin que, ao visitar os Estados Unidos em 1902, teria dito que achava que Tesla tinha captado sinais marcianos que estavam sendo enviados aos Estados Unidos.[263] No entanto, Kelvin negou "enfaticamente" esta declaração pouco antes de partir do país. "O que eu realmente disse foi que os habitantes de Marte, se houver algum, seriam, sem dúvida, capazes de ver Nova Iorque, especialmente o brilho da eletricidade".[264]
Em um artigo do The New York Times de 1901, Edward Charles Pickering, diretor do Harvard College Observatory, disse que tinha recebido um telegrama do Observatório Lowell, no Arizona, que parecia confirmar que Marte estava tentando se comunicar com a Terra.[265]
No início de dezembro de 1900 recebemos de Lowell, no Arizona, um telegrama afirmando que um raio de luz tinha sido visto projetando-se de Marte (o observatório Lowell tinha Marte como especialidade), com duração de 70 minutos. Eu enviei um telegrama com estes fatos para a Europa e enviei cópias através deste país. O observador é um homem confiável e cuidadoso e não há nenhuma razão para duvidar de que a luz existisse. Foi dada como partindo de um ponto geográfico bem conhecido em Marte. Isso foi tudo. Agora, a história foi para todo o mundo. Na Europa, afirma-se que eu tenho estado em comunicação com Marte e todos os tipos de exageros têm brotado. O que quer que a luz fosse, não há meios de saber. Se tinha inteligência ou não, ninguém pode dizer. É absolutamente inexplicável.[265]Pickering mais tarde propôs a criação de um conjunto de espelhos, no Texas, destinados a sinalizar para os marcianos.[266]
Nas últimas décadas, o mapeamento de alta resolução da superfície marciana, culminando na sonda Mars Global Surveyor, não revelou artefatos de habitação de qualquer tipo de vida "inteligente", mas a especulação pseudocientífica sobre a vida inteligente no planeta continua com comentaristas como Richard C. Hoagland. Remanescente da controvérsia sobre os canali, algumas especulações são baseadas em características de pequena escala percebidas nas imagens espaciais, como "pirâmides" e a chamada "Face de Marte". O astrônomo planetário Carl Sagan escreveu:
A representação de Marte na ficção foi estimulada por sua cor vermelha e pelas especulações científicas do século XIX de que suas condições de superfície não só poderiam suportar a vida, como também a vida inteligente.[267] Isso originou um grande número de cenários de ficção científica, entre os quais está A Guerra dos Mundos, de H. G. Wells, publicado em 1898, em que os marcianos tentam fugir de seu planeta moribundo e tentam invadir a Terra. Uma adaptação radiofônica posterior de A Guerra dos Mundos foi feita no dia 30 de outubro de 1938 por Orson Welles nos Estados Unidos e foi apresentada como um noticiário ao vivo. O episódio tornou-se notório por causar pânico público geral, quando muitos ouvintes confundiram a história com a realidade e saíram desesperados.[268]
Obras influentes retratam o planeta, como Crônicas Marcianas de Ray Bradbury, em que exploradores humanos acidentalmente destroem uma civilização marciana, a série Barsoom de Edgar Rice Burroughs, Além do Planeta Silencioso (1938) de C. S. Lewis[269] e uma série de histórias de Robert A. Heinlein de meados dos anos 1960.[270] O autor Jonathan Swift faz referência às luas de Marte cerca de 150 anos antes de sua descoberta por Asaph Hall, detalhando descrições razoavelmente precisas de suas órbitas no capítulo 19 de seu romance As Viagens de Gulliver.[271]
O personagem cômico de um marciano inteligente chamado Marvin, o Marciano apareceu na televisão em 1948 como um personagem nos Looney Tunes, da Warner Brothers, e continuou como parte da cultura popular desde então.[272]
Após as naves espaciais Mariner e Viking terem enviado imagens nítidas de Marte e revelado um mundo aparentemente sem vida e sem canais, essas ideias sobre o planeta tiveram que ser abandonadas e tem se desenvolvido uma moda de representações precisas e realistas sobre a colonização humana de Marte, como na trilogia Mars de Kim Stanley Robinson. Especulações pseudocientíficas sobre a "Face de Marte" e outros "monumentos" enigmáticos descobertos por sondas espaciais indicam que a ideia de civilizações antigas no planeta continua a ser um tema popular na ficção científica, especialmente no cinema.[273]
Proteínas são macromoléculas biológicas constituídas por uma ou mais cadeias de aminoácidos. As proteínas estão presentes em todos os seres vivos e participam em praticamente todos os processos celulares, desempenhando um vasto conjunto de funções no organismo, como a replicação de ADN, a resposta a estímulos e o transporte de moléculas. Muitas proteínas são enzimas que catalisam reações bioquímicas vitais para o metabolismo. As proteínas têm também funções estruturais ou mecânicas, como é o caso da actina e da miosina nos músculos e das proteínas no citoesqueleto, as quais formam um sistema de andaimes que mantém a forma celular. Outras proteínas são importantes na sinalização celular, resposta imunitária e no ciclo celular. As proteínas diferem entre si fundamentalmente na sua sequência de aminoácidos, que é determinada pela sua sequência genética e que geralmente provoca o seu enovelamento numa estrutura tridimensional específica que determina a sua atividade.
Ao contrário das plantas, os animais não conseguem sintetizar todos os aminoácidos de que necessitam para viver. Os aminoácidos que o organismo não é capaz de sintetizar por si próprio são denominados aminoácidos essenciais e devem ser obtidos pelo consumo de alimentos que contenham proteínas, as quais são transformadas em aminoácidos durante a digestão. As proteínas podem ser encontradas numa ampla variedade de alimentos de origem animal e vegetal. A carne, os ovos, o leite e o peixe são fontes de proteínas completas. Entre as principais fontes vegetais ricas em proteína estão as leguminosas, principalmente o feijão, as lentilhas, a soja ou o grão-de-bico. A grande maioria dos aminoácidos está disponível na dieta humana, pelo que uma pessoa saudável com uma dieta equilibrada raramente necessita de suplementos de proteínas. A necessidade é também maior em atletas ou durante a infância, gravidez ou amamentação, ou quando o corpo se encontra em recuperação de um trauma ou de uma operação. Quando o corpo não recebe as quantidades de proteínas necessárias verifica-se insuficiência e desnutrição proteica, a qual pode provocar uma série de doenças, entre as quais atraso no desenvolvimento em crianças ou kwashiorkor.
Uma proteína contém pelo menos uma cadeia polímérica linear derivada da condensação de aminoácidos, ou polipeptídeo. Os resíduos individuais de aminoácidos estão unidos entre si através de ligações peptídicas. A sequência dos resíduos de aminoácidos em cada proteína é definida pela sequência de um gene, a qual está codificada no código genético. Durante ou após o processo de síntese, os resíduos de uma proteína são muitas vezes alterados quimicamente através de modificação pós-traducional, a qual modifica as propriedades físicas e químicas das proteínas, o seu enovelamento, estabilidade, atividade e, por fim, a sua função. Nalguns casos as proteínas têm anexados grupos não peptídicos, os quais são denominados cofatores ou grupos prostéticos. As proteínas podem também trabalhar em conjunto para desempenhar determinada função, agrupando-se em complexos proteicos. As proteínas podem ser purificadas a partir de outros componentes celulares recorrendo a diversas técnicas, como a precipitação, ultracentrifugação, eletroforese e cromatografia. Entre os métodos usados para estudar a estrutura e funções das proteínas estão a imuno-histoquímica, mutagénese sítio-dirigida, ressonância magnética nuclear e espectrometria de massa.
As proteínas são nutrientes essenciais ao corpo humano.[1] Enquanto a maior parte dos microorganismos e das plantas são capazes de biosintetizar todos os vinte aminoácidos-padrão, os animais, incluindo os seres humanos, necessitam de obter alguns desses aminoácidos a partir da dieta alimentar.[2] Isto deve-se à ausência nos animais de algumas enzimas-chave que têm como função sintetizar esses aminoácidos. Os aminoácidos que o organismo não é capaz de sintetizar por si próprio são denominados aminoácidos essenciais. Os animais podem obter aminoácidos através do consumo de alimentos que contenham proteínas. As proteínas ingeridas são transformadas em aminoácidos através da digestão, a qual envolve a desnaturação da proteína através da exposição ao ácido e à hidrólise por parte de enzimas denominadas proteases. Alguns dos aminoácidos ingeridos são usados para a biossíntese proteica, enquanto outros são convertidos em glicose, através de gliconeogénese, ou entram no ciclo do ácido cítrico.[3]
Além de constituírem a fundação dos tecidos do corpo, as proteínas são também uma fonte de energia. Enquanto fonte de energia, contêm 4 kcal por grama, valor semelhante aos hidratos de carbono, mas diferente dos lípidos, os quais contêm 9 kcal por grama. Durante a digestão, as proteínas são separadas no estômago em cadeias polipeptídicas mais pequenas através da ação do ácido clorídrico e da protease. Isto é essencial para a síntese dos aminoácidos essenciais que não podem ser biossintetizados pelo corpo.[5]
Os aminoácidos essenciais são a leucina, isoleucina, valina, lisina, treonina, triptófano, metionina, fenilalanina e histidina. Os aminoácidos não essenciais são a alanina, asparagina, ácido aspártico e ácido glutâmico. Os aminoácidos condicionalmente essenciais são a arginina, cisteína, glutamina, glicina, prolina, serina e tirosina.[6] Os aminoácidos encontram-se em diversas fontes alimentares de origem animal, como a carne, leite, peixe e ovos. As proteínas estão também disponíveis através de diversas fontes vegetais: cereais integrais, leguminosas, incluindo os secos, soja, fruta nozes e sementes. Os vegetarianos e vegans podem obter os aminoácidos essenciais necessários através da ingestão de diversas proteínas vegetais.[6]
As proteínas são nutrientes essenciais ao crescimento e manutenção do corpo humano.[5] Com a exceção da água, as proteínas são as moléculas mais abundantes no corpo, sendo o principal componente estrutural de todas as células, particularmente dos músculos.[1][7] As proteínas são também usadas em membranas, como é o caso das glicoproteínas. Depois de serem repartidas em aminoácidos, são usadas como precursores do ácido nucleico, coenzimas, hormonas, resposta imunitária, reparação das células e outras moléculas essenciais para a vida.[7] As proteínas são ainda fundamentais para a formação de células sanguíneas.[1] Acredita-se que as proteínas aumentem o desempenho atlético.[8] Os aminoácidos são usados na produção de tecido muscular e na reparação de tecido danificado. As proteínas só são usadas como fonte de energia quando os recursos de hidratos de carbono e lípidos no corpo diminuem.[8]
As proteínas podem ser encontradas numa ampla variedade de alimentos. A combinação mais adequada de fontes de proteína para cada pessoa depende da região, da acessibilidade, do custo económico, do tipo de aminoácidos e do equilíbrio nutricional, assim como do próprio paladar. Embora alguns alimentos sejam fontes ricas em determinados aminoácidos, o seu valor para na nutrição humana é limitado devido à sua pouca digestibilidade, a fatores antinutricionais, à elevada quantidade de calorias, ao colesterol ou à densidade mineral.[9]
A carne, os ovos, o leite e o peixe são fontes de proteínas completas.[10] Entre as fontes vegetais ricas em proteína estão as leguminosas, nozes, sementes e fruta. As leguminosas têm maior concentração de aminoácidos e são fontes mais completas de proteína do que os cereais e os cereais integrais. Entre os alimentos vegetarianos com concentração de proteínas superior a 7% estão a soja, lentilhas, feijão vermelho e branco, feijão-frade, feijão-da-china, grão-de-bico, feijão-verde, tremoço, amêndoa, castanha-do-pará, cajueiro, noz-pecã e sementes de sésamo, de abóbora, de algodão e de girassol.[9]
Entre os alimentos de base que constituem uma fonte pobre em proteínas estão raízes e tubérculos como o inhame, mandioca e batata-doce, os quais contêm apenas entre 0 e 2% de proteínas. A fruta, embora seja rica noutros nutrientes essenciais, é uma fonte relativamente pobre de aminoácidos. A banana-da-terra é também pobre em proteínas. Para uma alimentação saudável, os alimentos básicos com baixo teor de proteína devem ser complementados com alimentos com proteínas completas e de qualidade, sobretudo durante o desenvolvimento das crianças.[1][11][12]
A grande maioria dos aminoácidos está disponível na dieta humana, pelo que uma pessoa saudável com uma dieta equilibrada raramente necessita de suplementos de proteínas.[6][9] Os aminoácidos mais limitados são a lisina, a treonina e os aminoácidos com enxofre.[11] A tabela em anexo mostra os mais importantes grupos alimentares que constituem fontes de proteínas, sob uma perspetiva mundial. Também lista o desempenho de cada grupo enquanto fonte dos aminoácidos mais limitados, em valores de miligramas de aminoácido limitado por cada grama de proteína total nesse alimento.[11]
Existe um debate considerável sobre as necessidades relativas ao consumo de proteínas.[13][14] A quantidade de proteínas necessária na dieta de determinada pessoa é determinada em grande parte pelo consumo total de energia e hidratos de carbono, pela necessidade do corpo de nitrogénio e aminoácidos essenciais, composição e massa corporal, taxa de crescimento, nível de atividade física e presença de lesões ou doenças.[15][5][16] A atividade física elevada e o aumento da massa muscular aumentam a necessidade de proteínas. A necessidade é também maior durante a infância, gravidez ou amamentação, ou quando o corpo se encontra em recuperação de um trauma ou de uma operação.[17]
De acordo com os valores de referência de ingestão de proteínas da Autoridade Europeia para a Segurança Alimentar, os adultos, incluindo idosos, devem ingerir 0,83 g de proteína por dia por cada quilograma de peso corporal; os recém-nascidos, crianças e adolescentes devem ingerir entre 0,83 e 1,31 g/kg/dia, dependendo da idade. As grávidas devem ingerir valores suplementares de proteína: 1 g, 9 g e 28 g suplementares por dia durante o primeiro, segundo e terceiro trimestres, respetivamente. As lactantes devem também ingerir valores suplementares: 19 g por dia durante os primeiros seis meses de amamentação e 13 g por dia a partir dos seis meses.[18] De acordo com as recomendações norte-americanas e canadianas, as mulheres com idade entre 19 e 70 anos necessitam de consumir 46 g de proteínas por dia, enquanto os homens no mesmo intervalo etário necessitam de consumir pelo menos 56 g de proteínas por dia.[19] O valor geralmente recomendado para o consumo diário é de 0,8 g de proteínas por cada quilograma de massa corporal.[13] No entanto, esta recomendação baseia-se nas necessidades estruturais, sem considerar o uso de proteínas no metabolismo energético, pelo que se adequa a uma pessoa relativamente sedentária.
[13][15] Diversos estudos têm concluído que as pessoas ativas e os atletas possam exigir um consumo superior de proteínas, devido ao aumento da massa muscular e da sudação, e da maior necessidade de proteínas enquanto fonte de energia e reparação do corpo.[13][14][15] Para estes casos, os valores sugeridos têm oscilado entre 1,6 g/kg e 1,8 g/kg.[14]
Para compensar as variações na ingestão de proteínas ao longo do dia, ou em casos de emergência em que a ingestão de proteínas é temporariamente alta ou baixa, o corpo tenta equilibrar os níveis de proteínas recorrendo a uma reserva de curta duração.[20] No entanto, o corpo é incapaz de armazenar o excesso de proteínas a longo prazo. As proteínas são digeridas em aminoácidos que entram na corrente sanguínea. Os aminoácidos em excesso são convertidos pelo fígado em moléculas úteis, num processo denominado desaminação. A desaminação converte o nitrogénio dos aminoácidos em amónia, a qual é por sua vez convertida pelo fígado em ureia durante o ciclo da ureia. A ureia é depois excretada pelos rins.[15][21]
O consumo excessivo de proteínas provoca também o aumento da excreção de cálcio na urina, o que se pensa ser devido ao desequilíbrio no pH, agravando o risco da formação de cálculos no sistema urinário.[20] Um estudo epidemiológico de 2006 não verificou a existência de qualquer relação entre o consumo total de proteína e a pressão arterial, embora tenha verificado uma relação inversa entre o consumo de proteína vegetal e a pressão arterial.[22]
Quando o corpo não recebe as quantidades de proteínas necessárias verifica-se insuficiência e desnutrição proteica, a qual pode provocar uma série de doenças, entre as quais atraso no desenvolvimento em crianças, kwashiorkor, pigmentação avermelhada do cabelo e da pele, fígado gorduroso, diarreia, dermatose e diminuição na contagem de linfócitos T, o que aumenta o risco de infeções secundárias.[12][4] A desnutrição proteica é relativamente comum à escala mundial, tanto em adultos como crianças, e é responsável por cerca de seis milhões de mortes anualmente. Nos países desenvolvidos, esta doença verifica-se predominantemente em idosos ou em hospitais, geralmente associada a outras doenças.[23]
As dietas de alto teor proteico podem ajudar a perder peso ao fazer com que a pessoa se sinta cheia mais rapidamente. Na maioria das pessoas saudáveis, este tipo de dietas não apresenta riscos para a saúde, sobretudo se for seguida durante um curto período de tempo. No entanto, os riscos a longo prazo estão ainda em estudo. O recurso prolongado a este tipo de dieta, geralmente associadas com a limitação do consumo de hidratos de carbono, pode causar insuficiências nutricionais e falta de fibras, o que por sua vez provoca dores de cabeça e obstipação. Algumas destas dietas baseiam-se no aumento do consumo de carne vermelha e lacticínios gordos, o que aumenta o risco de doenças cardiovasculares. As escolhas mais saudáveis para uma dieta de alto teor proteico incluem proteína de soja, feijão, nozes, peixe, aves de criação sem pele, carne de porco e lacticínios magros, devendo ser evitadas as carnes vermelhas e processadas.[24]
A maior parte das proteínas consiste em polímeros lineares formados a partir de um máximo de 20 L-α-aminoácidos. Todos os aminoácidos proteinogénicos têm em comum diversas características estruturais, entre as quais um carbono alfa, ao qual estão quimicamente ligados um grupo de aminas, um grupo de ácido carboxílico e uma cadeia lateral variável. Apenas a prolina difere desta estrutura básica.[25] As cadeias laterais dos aminoácidos comuns apresentam uma grande variedade de propriedades e estruturas químicas. É o efeito combinado de todas as cadeias laterais numa proteína que determina a sua estrutura tridimensional e reatividade química.[26] Os aminoácidos numa cadeia de polipeptídios são unidos por ligações peptídicas. Uma vez unidos na cadeia proteica, cada aminoácido individual é denominado "resíduo", e cada série repetitiva e encadeada de átomos de carbono, nitrogénio e oxigénio é denominada "cadeia principal".[27]
A ligação peptídica tem duas formas de ressonância que contribuem para a formação de uma ligação dupla e inibem a rotação em torno do seu próprio eixo, pelo que os carbonos alfa são aproximadamente coplanares. Os outros dois ângulos diedros na ligação peptídica determinam a forma assumida pela cadeia principal.[28] A extremidade da proteína com um grupo carboxílico livre é denominada C-terminal ou carboxi-terminal, enquanto a extremidade com um grupo livre de amina é denominada N-terminal ou amino-terminal. Os termos "proteína", "polipetídeo" e "peptídeo" são ligeiramente ambíguas e o seu significado pode-se sobrepôr. "Proteína" é geralmente usado para nos referirmos à molécula biológica completa na sua forma terciária estável, enquanto "peptídeo" está geralmente reservado para oligómeros curtos de aminoácidos, aos quais muitas vezes falta uma estrutura tridimensional estável. No entanto, a diferença entre ambos não é bem definida e geralmente corresponde a 20-30 resíduos. "Polipetídeo" pode ser referente a qualquer cadeia linear de aminácidos, independentemente do comprimento, mas onde geralmente não existe uma forma terciária.[29]
As proteínas são produzidas a partir de aminoácidos usando informação codificada nos genes. Cada proteína tem a sua própria sequência de aminoácidos que é especificada pela sequência de nucleótidos do gene que codifica a proteína. O código genético é um grupo de conjuntos com três nucleótidos cada um, denominados codões. Cada uma das combinações de três nucleótidos designa um aminoácido. Por exemplo, AUG (adenina-uracilo-guanina) é o código para a metionina. Uma vez que o ADN contém quatro nucleótidos, o número total de codões possíveis é de 64. Por este motivo, existe alguma redundância no código genético, havendo alguns aminoácidos que são especificados por mais de um codão.[30] Os genes que são codificados no ADN são inicialmente transcritos para pré-ARN mensageiro (ARNm) por proteínas como a ARN-polimerase. A maior parte dos organismos processa em seguida o pré-ARNm, usando várias formas de modificação pós-transcricional, formando assim o ARNm amadurecido, o qual é então usado como molde para a síntese proteica feita pelo ribossoma. Nos procariontes, o ARNm tanto pode ser utilizado assim que é produzido, como ser ligado a um ribossoma depois de se ter afastado do nucleoide. Por outro lado, os eucariontes produzem ARNm no núcleo celular, o qual é depois translocado através do envelope nuclear para o citoplasma, no qual se dá a síntese proteica. A velocidade de síntese proteica é maior nos procariontes do que nos eucariontes, podendo atingir os 20 aminoácidos por segundo.[31]
O processo de síntese de uma proteína a partir de um molde de ARNm é denominado tradução. O ARNm é carregado no ribossoma, no qual são lidos três nucleótidos de cada vez. A leitura é feita fazendo corresponder cada codão com o seu anticodão situado numa molécula de ARN transportador (ARNt), a qual transporta o aminoácido correspondente ao codão por si reconhecido. A enzima aminoacil-tRNA sintetase carrega as moléculas de ARNt com o aminoácido correto. As proteínas são sempre sintetizadas a partir do N-terminal em direção ao C-terminal.[32]
O tamanho de uma proteína sintetizada pode ser medido através do número de aminoácidos e da sua massa molecular total, valor que é geralmente expresso em daltons (Da), sinónimo de unidade de massa atómica. As proteínas das leveduras, por exemplo, têm em média um comprimento de 466 aminoácidos e 53 kDa de massa.[29] As maiores proteínas conhecidas são as titinas, com quase 3 000 kDA de massa molecular de 27 000 aminoácidos de comprimento.[33]
As proteínas curtas podem também ser sintetizadas quimicamente através de uma série de métodos denominados síntese de peptídeos, os quais têm por base técnicas de síntese orgânica de elevado rendimento na produção de peptídeos.[34] A síntese química permite a introdução de aminoácidos não naturais nas cadeias de peptídeos.[35] Estes métodos são úteis em laboratórios de bioquímica e biologia celular, embora não se adequem à produção comercial. A síntese química não é eficiente para polipeptídeos maiores do que 300 aminoácidos, e as proteínas sintetizadas podem não assumir imediatamente a sua estrutura terciária nativa. Grande parte dos métodos de síntese química realiza-se a partir do C-terminal em direção ao N-terminal, ao contrário da reação biológica natural.[36]
A maior parte das proteínas enovela-se em estruturas tridimensionais distintas. A forma para a qual uma proteína se enovela naturalmente é denominada conformação nativa.[37] Embora haja muitas proteínas capazes de se enovelar sem assistência, meramente através das propriedades químicas dos seus aminoácidos, há outras que necessitam do auxílio de chaperonas moleculares de modo a se poderem enovelar para a sua conformação nativa.[38] As proteínas podem ter 4 tipos de estruturas dependendo do tipo de aminoácidos, do tamanho da cadeia e da configuração espacial da cadeia polipeptídica: estrutura primária, secundária, terciária e quaternária.[39]
As proteínas não são moléculas completamente rígidas. Para além destes níveis estruturais, as proteínas podem alternar entre várias estruturas enquanto desempenham as suas funções. No contexto destas alterações funcionais, estas estruturas terciárias ou quaternárias são muitas vezes denominadas "conformações", e as transições entre cada uma delas são denominadas "alterações conformacionais". Estas alterações são frequentemente induzidas pela ligação de uma molécula substrato ao sítio ativo de uma enzima – a região física da proteína que participa na catálise química.[40]
As proteínas podem ser divididas informalmente em três classes principais, de acordo com as estruturas terciárias mais comuns: proteínas globulares, proteínas fibrilares e proteínas membranares. Praticamente todas as proteínas globulares são solúveis e grande parte são enzimas. As proteínas fibrilares são muitas vezes estruturais, como é o caso do colagénio, o principal componente do tecido conjuntivo, ou a queratina, a proteína constituinte do cabelo e das unhas. As proteínas membranares atuam muitas vezes como recetores ou proporcionam canais para que as moléculas possam atravessar a membrana celular.[41]
É a sequência linear de aminoácidos ao longo da cadeia polipeptídica da proteína.[42] É o nível estrutural mais simples e mais importante, pois dele deriva todo o arranjo espacial da molécula. É específica para cada proteína, sendo, geralmente, determinada geneticamente. A estrutura primária da proteína resulta numa longa cadeia de aminoácidos, com uma extremidade "amino terminal" e uma extremidade "carboxi terminal".[43]
É a forma como os aminoácidos se organizam entre si na sequência primária da proteína.[42] Ocorre graças à possibilidade de rotação das ligações entre os carbonos alfa dos aminoácidos e os seus grupos amina e carboxilo. O arranjo secundário de uma cadeia polipeptídica pode ocorrer de forma regular; isso acontece quando os ângulos das ligações entre carbonos alfa e seus ligantes são iguais e se repetem ao longo de um segmento da molécula.[44] A cadeia polipeptídica pode interagir com ela própria através de duas formas principais: pela formação das alfa-hélices e das folhas-beta. Além destas existem estruturas que não são nem hélices nem folhas chamadas laços (loops).[45]
A estrutura terciária é a forma como determinada molécula de proteína se organiza no espaço. Corresponde ao movimento, à organização das alfa-hélices, fitas b e voltas no espaço tridimensional, definido por coordenadas atómicas.[46] Resulta do enovelamento das hélices e das folhas pregueadas de uma estrutura secundária e é mantida nessa posição por interações hidrófugas e hidrófilas.[42][47]
A estrutura quaternária de uma proteína refere-se à união de várias moléculas proteicas enoveladas num complexo multi-proteico. A interação entre as moléculas é realizada através de ligações não covalentes.[48]
A descoberta da estrutura terciária de uma proteína, ou da estrutura quaternária dos seus complexos, pode fornecer dados importantes acerca da forma como a proteína realiza a sua função. Entre os métodos mais comuns para determinar a estrutura estão a cristalografia de raios X e a ressonância magnética nuclear de proteínas (RMN), ambas capazes de produzir dados à escala atómica. No entanto, a RMN pode disponibilizar dados a partir dos quais é possível determinar as distâncias entre subconjuntos de pares de átomos, permitindo assim determinar todas as conformações finais possíveis de determinada proteína. A interferometria de dupla polarização é um método analítico que permite medir a conformação e as alterações conformacionais das proteínas em função de interações ou de outros estímulos. O dicroísmo circular é outra técnica laboratorial que permite determinar a composição das proteínas a nível de hélices e folhas beta. A crio-microscopia eletrónica (crio-EM) é usada para produzir informação de baixa resolução sobre complexos proteicos de grande dimensão, entre os quais vírus.[49] Uma variante denominada cristalografia eletrónica é, nalguns casos, capaz de produzir informação de elevada resolução, em particular nos cristais bidimensionais de proteínas membranares.[50]
As estruturas resolvidas são geralmente acrescentadas ao Protein Data Bank (PDB), um recurso disponível gratuitamente que permite consultar os dados estruturais de milhares de proteínas.[51] Conhece-se um número muito maior de sequências genéticas do que estruturas proteicas. Além disso, o conjunto de estruturas resolvidas tende a focar-se naquelas que podem ser facilmente adequadas às condicionantes da cristalografia de raio X. As proteínas globulares, em particular, são as mais fáceis de preparar para a cristalografia de raio X, enquanto as proteínas membranares são difíceis de cristalizar e comparativamente pouco representadas no PDB.[52] As iniciativas de genómica estrutural têm vindo a tentar corrigir esta assimetria através da resolução sistemática das estruturas representativas das principais classes de enovelamento. Para além disso, existem ainda métodos de previsão de estruturas proteicas que tentam fornecer uma estrutura plausível para proteínas cujas estruturas ainda não foram determinadas experimentalmente.[53]
As proteínas são os principais intervenientes no interior das células, realizando as tarefas determinadas pela informação codificada nos genes.[29] Excetuando determinados tipos de ARN, a maior parte das restantes moléculas biológicas são elementos relativamente inertes nos quais as proteínas atuam. As proteínas são também o principal componente celular; por exemplo, metade do peso de uma célula de Escherichia coli corresponde a proteínas, enquanto outras macromoléculas como o ADN e o ARN correspondem apenas a 3% e 20% do peso, respectivamente. O conjunto de proteínas que podem ser expressas numa determinada célula ou tipo celular é denominado proteoma.[2]
A principal característica das proteínas, a qual também permite que exerçam um conjunto alargado de funções, é a sua capacidade de ligarem a si outras moléculas, de forma estável e específica. A região da proteína responsável pela agregação de outras moléculas é denominada sítio de ligação, a qual geralmente corresponde a uma depressão na superfície molecular da proteína. Esta capacidade de ligação é mediada pela estrutura terciária da proteína, a qual define a região de ligação, e pelas propriedades químicas das cadeias de aminoácidos laterais. As ligações proteicas podem ser extremamente específicas; por exemplo, a proteína inibidora da ribonuclease liga-se à angiogenina humana, mas não se liga à sua homóloga anfíbia rampirnase. Há variações químicas extremamente subtis que, por vezes, podem ser o suficiente para eliminar por completo a possibilidade de ligação proteica; por exemplo, o aminoacil-tRNA sintetase específico do aminoácido valina não é capaz de se ligar à cadeia lateral do aminoácido isoleucina, muito similar.[54]
As proteínas podem-se ligar não só a outras proteínas, como também a substratos de pequenas moléculas. Quando as proteínas se ligam especificamente a outras cópias da mesma molécula, são capazes de se oligomerizar para formar fibrilas. Este processo ocorre frequentemente em proteínas estruturais constituídas por monómeros globulares que se associam entre si para formar fibras rígidas. As interações entre proteínas regulam também a atividade enzimática, controlam a progressão ao longo do ciclo celular e permitem a formação de complexos proteicos que desempenham diversas reações no âmbito de uma mesma função biológica. As proteínas também se ligam a membranas celulares. A capacidade de ligarem a si parceiros que induzem alterações conformacionais nas proteínas permite a construção de redes de sinalização celular extremamente complexas.[55] Uma vez que as interações entre proteínas são reversíveis, e dependem da disponibilidade de diferentes grupos de proteínas para formar agregados capazes de desempenhar um conjunto alargado de funções, o estudo das interações entre proteínas específicas é fundamental para compreender aspetos importantes da função celular e, por fim, as propriedades que distinguem diferentes tipos de células.[56]
As proteínas podem atuar na célula enquanto enzimas, as quais são catalisadoras de reações químicas. As enzimas são, regra geral, extremamente específicas e aceleram apenas uma única ou muito poucas reações químicas. As enzimas realizam maior parte das reações que fazem parte do metabolismo e manipulam ADN em vários processos, como a replicação de ADN, reparação de ADN e transcrição genética. Algumas enzimas atuam sobre outras proteínas no sentido de acrescentar ou remover grupos químicos, um processo denominado modificação pós-translacional. São conhecidas cerca de 4 mil reações químicas catalisadas por enzimas.[57] A taxa de aceleração proporcionada pela catálise é muitas vezes imensa, podendo chegar a valores na magnitude de 1017 em relação à reação não catalisada, o que faz com que um processo que naturalmente demoraria 78 milhões de anos, com a enzima seja completo em apenas 18 milissegundos.[58] Os compostos químicos que sofrem reações enzimáticas são denominados substratos. Embora as enzimas possam ser constituídas por centenas de aminoácidos, só uma pequena percentagem dos resíduos é que entra em contacto com o substrato e uma percentagem ainda mais pequena – três a quatro resíduos, em média – é que está diretamente envolvida na catálise.[59]
As proteínas estruturais conferem rigidez a componentes biológicos que, de outra forma, seriam apenas fluidos. A maior parte das proteínas estruturais são proteínas fibrilares; por exemplo, o colagénio e a elastina são componentes fundamentais do tecido conjuntivo, como a cartilagem, enquanto a queratina está presente em estruturas duras como o cabelo, unhas, penas, cascos e algumas carapaças animais.[60] Algumas proteínas globulares podem também desempenhar funções estruturais; por exemplo, a actina e a tubulina são globulares e solúveis enquanto monómeros, mas são capazes de se polimerizar nas fibras rígidas e longas que formam o citoesqueleto, o qual permite à célula manter a sua forma e tamanho. Outras proteínas que têm funções estruturais são as proteínas motoras como a miosina, cinesina e a dineína, as quais são capazes de gerar força mecânica, como a que contrai os músculos. Estas proteínas são cruciais para a motilidade de organismos unicelulares e dos espermatozoides dos organismos multicelulares que se reproduzem através de reprodução sexuada.[61]
Muitas das proteínas estão envolvidas nos processos de sinalização celular e transdução de sinal. Algumas delas, como a insulina, são proteínas extracelulares que transmitem um sinal para outras células em tecidos distantes, a partir da célula onde são sintetizadas. Outras são proteínas membranares que atuam enquanto recetores, cuja principal função é ligar a si uma molécula sinalizadora e induzir uma resposta bioquímica na célula. Muitos dos recetores têm na sua superfície um sítio de ligação e um domínio efetor dentro da célula, o qual pode ter atividade enzimática ou sofrer alterações conformacionais que são detetadas por outras proteínas no interior da célula.[62]
Os anticorpos são componentes proteicos do sistema imunitário adquirido, cuja função principal é ligar a si antigénios ou outras substâncias estranhas ao corpo, marcando-os para serem destruídos. Os anticorpos podem ser segregados para o ambiente extracelular ou ancorados nas membranas de linfócitos B especializados, denominados plasmócitos. Enquanto as enzimas são extremamente limitadas na sua capacidade de ligação, resumindo-se aos substratos necessários para realizar a sua função enzimática, os anticorpos não têm esta restrição, apresentando uma afinidade extremamente elevada.[63]
Muitas das proteínas que transportam ligantes são capazes de ligar a si pequenas biomoléculas, transportando-as para diferentes locais do corpo. Estas proteínas devem ter uma elevada afinidade de ligação nos casos em que o seu ligante esteja presente em elevada concentração, mas serem também capazes de libertar o ligante nos casos em que a sua concentração nos tecidos-alvo seja diminuta. O exemplo canónico de uma proteína ligante é a hemoglobina, a qual transporta o oxigénio dos pulmões para os restantes órgãos e tecidos em todos os vertebrados e tem homólogos em todos os reinos.[64] As proteínas transmembranares atuam também enquanto proteínas transportadoras de ligantes, capazes de alterar a permeabilidade da membrana celular em relação a pequenas moléculas e a iões. A própria membrana tem um núcleo hidrófugo, através do qual as moléculas polarizadas ou carregadas eletrónicamente não são capazes de se difundir. As proteínas membranares possuem canais internos que permitem a este tipo de moléculas entrar e sair da célula. Muitas proteínas com canais iónicos são especializadas no sentido de selecionar apenas um ião em particular; por exemplo, os canais de potássio e sódio muitas vezes aceitam apenas um dos dois iões.[65]
A acumulação de proteínas mal enoveladas pode causar doenças amiloides, um grupo de várias doenças comuns, entre as quais a doença de Alzheimer, doença de Parkinson e doença de Huntington. O risco destas doenças aumenta significativamente com a idade. À medida que o ser humano envelhece, o equilíbrio da síntese, enovelamento e degradação das proteínas vai sofrendo distúrbios, o que provoca a acumulação de proteínas mal enoveladas em agregados. No entanto, as doenças causadas pela agregação de proteínas mal enoveladas não são exclusivas do sistema nervoso central e podem-se manifestar em tecidos periféricos, como no caso da diabetes mellitus tipo 2, cataratas hereditárias, algumas formas de aterosclerose, distúrbios relacionados com a hemodiálise e amiloidose, entre outras. Os genes e produtos proteicos envolvidos nestas doenças denominam-se amiloidogénicos e todas estas doenças têm em comum a expressão de uma proteína fora do seu contexto normal. Em todas estas doenças, a agregação de proteínas pode ser causada por mero acaso, por hiperfosforilação proteica, por mutações que tornam a proteína instável ou ainda pelo aumento desregulado ou patológico da concentração de algumas destas proteínas entre as células. Estes desiquilíbrios na concentração podem ser causados por mutações dos genes amiloidogénicos, alterações na sequência de aminoácidos da proteína ou por deficiências no proteassoma.[66]
As proteínas são uma das moléculas biológicas mais intensivamente estudadas, quer in vitro, in vivo ou in silico. O estudo in vitro de proteínas purificadas em ambiente controlado é útil na aprendizagem da forma como as proteínas desempenham as suas funções; por exemplo, o estudo da cinética enzimática explora o mecanismo químico da atividade catalítica de uma enzima e a sua afinidade relativa em relação às possíveis moléculas substrato. Por outro lado, as experiências in vivo podem fornecer dados sobre o papel fisiológico da proteína no contexto de uma célula ou de um organismo. O estudo in silico recorre a métodos computacionais para estudar proteínas.
Antes de se poder efetuar uma análise in vitro, a proteína deve ser purificada dos restantes componentes celulares. Este processo de purificação geralmente tem início com a citólise da célula, através da qual a membrana celular é rompida e o conteúdo interno libertado para uma solução denominada lisado bruto. A mistura daí resultante pode ser purificada através de ultracentrifugação, a qual fracciona os vários componentes celulares em frações que contêm proteínas solúveis, proteínas e lípidos membranares, organelas celulares e ácidos nucleicos. As proteínas deste lisado são então concentradas através de precipitação, feita através do método de relargagem. São depois usados vários tipos de cromatografia para isolar a proteína ou as proteínas pretendidas, de acordo com propriedades como o peso molecular ou afinidade de ligação.[67] O nível de purificação pode ser monitorizado através de vários tipos de eletroforese em gel, quando são conhecidos o peso molecular e o ponto isoelétrico, através de espectroscopia, quando a proteína possui características espectroscópicas distintas, ou através de análise enzimática, quando a enzima tem atividade enzimática. As proteínas podem ainda ser isoladas de acordo com a sua carga através de focalização isoelétrica.[68]
No caso das proteínas naturais, podem ser necessárias mais etapas no processo de purificação de forma a obter proteínas suficientemente puras para serem usadas em laboratório. Para simplificar este processo, recorre-se muitas vezes a engenharia genética para acrescentar às proteínas características químicas que as tornem mais fáceis de serem purificadas sem, no entanto, afetar a sua estrutura ou atividade. Neste caso, a um dos terminais da proteína é acrescentada uma etiqueta constituída por uma sequência de aminoácidos específica, geralmente uma série de resíduos de histidina (etiqueta de poli-histidina). Desta forma, quando o lisado é passado sobre uma coluna de cromatografia contendo níquel, os resíduos de histidina ligam-se com o níquel e agarram-se à coluna, enquanto os componentes do lisado sem a etiqueta passam sem entraves. Têm vindo a ser desenvolvidas diversas etiquetas de modo a auxiliar os investigadores na purificação de proteínas a partir de misturas complexas.[69]
O estudo de proteínas in vivo dedica-se às questões relacionadas com a síntese e localização de proteínas no interior de células. Embora muitas das proteínas intrecelulares sejam sintetizadas no citoplasma e as proteínas segregadas sejam sintetizadas no retículo endoplasmático, o processo específico de como as proteínas se orientam para organelas ou estruturas celulares específicas é em muitas situações pouco claro. Uma das técnicas para avaliar a localização celular recorre a engenharia genética para expressar numa célula uma proteína de fusão, a qual é constituída pela proteína em estudo ligada a um gene repórter, como por exemplo a proteína verde fluorescente.[70] A posição da proteína de fusão na célula pode então ser facilmente visualizada através de microscopia.[71]
Outros métodos para obtenção da localização celular de proteínas requerem o uso de marcadores compartimentais conhecidos para diversas regiões celulares. Com o uso de versões destes marcadores etiquetadas com fluorescência, torna-se mais simples a identificação e localização da proteína pretendida.[72]
A técnica padrão para localização celular é a microscopia imunoeletrónica. Esta técnica usa um anticorpo para a proteína pretendida, a par de técnicas clássicas de microscopia eletrónica. A amostra é preparada para uma análise microscópica padrão, sendo depois tratada com um anticorpo dessa proteína que é conjugado com um material eletro-denso, geralmente ouro.[73] Através de mutagénese sítio-dirigida, os investigadores podem alterar a sequência proteica, alterando dessa forma a sua estrutura, localização celular e suscetibilidade à regulação. Esta técnica permite ainda a incorporação nas proteínas de aminoácidos não naturais, usando ARNt modificado,[74] podendo ainda permitir a concessão de novas proteínas com novas propriedades.[75]
O conjunto total de proteínas presentes numa célula em determinado momento é denominado proteoma, e o estudo em grande escala destes conjuntos define o campo da proteómica, assim denominado em analogia ao campo relacionado da genómica. Entre as principais técnicas da proteómica estão a eletroforese bidimensional,[76] a qual permite a separação de um vasto número de proteínas, a espectrometria de massa,[77] a qual permite a rápida identificação de proteínas e sequenciação de peptídeos, o microarranjo de proteínas,[78] que permite a deteção dos níveis relativos do grande número de proteínas presentes na célula, e o sistema de duplo híbrido, que permite a exploração sistemática de interações proteína-proteína.[79] O conjunto total e biologicamente possível destas interações é denominado interactoma.[80] O esforço sistemático para determinar as estruturas de proteínas e de todos os enovelamentos possíveis é denominado genómica estrutural.[81]
Complementar ao campo da genómica estrutural, a previsão de estruturas das proteínas procura desenvolver métodos eficientes de fornecer modelos plausíveis para proteínas cujas estruturas não foram ainda determinadas experimentalmente.[82] O mais bem-sucedido método de previsão estrutural, denominado modelação por homologia, assenta na existência de uma estrutura-modelo com uma sequência semelhante à proteína a ser modelada. O objetivo da genómica estrutural é fornecer uma representatividade suficiente de estruturas resolvidas que sirva de modelo a todas as restantes.[83]
Os processos de enovelamento e ligação proteica podem ser simulados usando técnicas como a dinâmica molecular ou o método de Monte Carlo, os quais têm vindo cada vez mais a tirar partido da computação distribuída, como o projeto Folding@home.[84] O enovelamento de pequenos domínios proteicos de alfa-hélice, como a proteína acessória do VIH, tem vindo a ser simulado com sucesso in silico.[85] Os métodos híbridos que combinam dinâmica molecular com cálculo de mecânica quântica têm permitido a exploração dos estados eletrónicos das rodopsinas.[86]
As proteínas foram pela primeira vez descritas pelo químico holandês Gerardus Johannes Mulder e assim batizadas pelo químico sueco Jöns Jacob Berzelius em 1838. Mulder levou a cabo análises elementares de proteínas vulgares e constatou que praticamente todas as proteínas apresentavam a mesma fórmula empírica – C400H620N100O120P1S1. Ainda que erradamente, concluiu que as proteínas deveriam ser constituídas por um único tipo de molécula de grande dimensão.[87] O termo "proteína" para descrever estas moléculas foi proposto pelo sócio de Mulder, Berzelius. Proteína deriva da palavra grega πρωτεῖος (proteios), a qual significa "na liderança" ou "a que está à frente".[88] Mulder prosseguiu com a sua investigação, identificando produtos da degradação proteica, como o aminoácido leucina, para o qual determinou o peso molecular quase preciso de 131 Da.[87]
Os cientistas pioneiros no campo da nutrição, como o alemão Carl von Voit, acreditavam que a proteína era o mais importante nutriente na manutenção da estrutura corporal, uma vez que existia a crença generalizada de que seria a carne tinha origem na própria carne.[89] Karl Heinrich Ritthausen alargou o campo das proteínas conhecidas com a identificação do ácido glutâmico. Thomas Burr Osborne compilou em 1909 uma revisão detalhada de todas as proteínas vegetais e, no mesmo ano e em conjunto com Lafayette Mendel, determinou os aminoácidos essenciais à sobrevivência de ratos de laboratório aplicando a lei de Liebig. A compreensão das proteínas enquanto polipetídeos foi proporcionada por Franz Hofmeister e Hermann Emil Fischer. O papel central das proteínas enquanto enzimas nos organismos vivos foi determinado em 1926, quando James Batcheller Sumner demonstrou que a urease era de facto uma proteína.[90]
A dificuldade em purificar proteínas em grande quantidade dificultou imenso a investigação dos primeiros bioquímicos. Assim, a investigação inicial focou-se sobretudo em proteínas que podiam ser facilmente purificadas em quantidade, como as do sangue, da clara de ovo, diversas toxinas e enzimas digestivas obtidas em matadouros.[87] Atribuiu-se a Linus Pauling a primeira previsão bem-sucedida de de estruturas secundárias de proteínas com base nas ligações de hidrogénio, uma ideia que já tinha sido proposta em 1933 por William Astbury.[91] Posteriormente, a investigação de Walter Kauzmann sobre a desnaturação, baseada em parte nos estudos anteriores de Kaj Ulrik Linderstrøm-Lang, veio a contribuir para a compreensão do enovelamento de proteínas e das estruturas mediadas por interações hidrófugas.[92][93][94] A primeira proteína a ser sequenciada foi a insulina, por Frederick Sanger em 1949. Sanger determinou corretamente a sequência de aminoácidos da proteína, demonstrando de forma conclusiva que as proteínas eram constituídas por polímeros lineares de aminoácidos, em vez de cadeias ramificadas ou coloides.[95]
As primeiras estruturas proteicas a serem resolvidas foram as da hemoglobina e da mioglobina, por Max Perutz e John Kendrew, respetivamente, em 1958.[96][97] Nas décadas posteriores, a crio-microscopia eletrónica de grandes conjuntos macromoleculares e a previsão computacional de estruturas proteicas de pequenos domínios foram métodos que permitiram a investigação de proteínas à escala atómica.[98][99] No início de 2014, estavam registadas no Protein Data Bank aproximadamente 90 000 estruturas proteicas com resolução atómica.[100]
Na linguagem vulgar, respiração é o ato de inalar e exalar ar através da boca, das cavidades nasais ou através da pele para efetivamente serem processadas as trocas gasosas ao nível dos pulmões.
Do ponto de vista da fisiologia, respiração é o processo pelo qual um organismo vivo troca oxigénio e dióxido de carbono com o seu meio ambiente. Este processo é chamado de respiração aeróbica. Quando os gases presentes no processo são diferentes destes, como óxido nitroso ou nitrogênio, chama-se respiração anaeróbica, e pode ser observado especialmente em alguns organismos vivos unicelulares.[1]
Do ponto de vista da bioquímica, respiração celular é o processo de conversão das ligações químicas de moléculas ricas em energia que possa ser usada nos processos vitais.[1]
A respiração celular é um fenômeno que consiste basicamente no processo de extração de energia química acumulada nas moléculas de substâncias orgânicas. Nesse processo, verifica-se a oxidação de compostos orgânicos de alto teor energético, como carboidratos e lípidos, para que possam ocorrer as diversas formas de trabalho celular. A organela responsável por essa respiração é a mitocôndria em paralelo com o sistema golgiense.
Ela pode ser de dois tipos, respiração anaeróbica (sem utilização de oxigênio também chamada de fermentação) e respiração aeróbica (com utilização de oxigênio).
Nos organismos aeróbicos, a equação simplificada da respiração celular pode ser assim representada:
C6H12O6 + 6 O2 →  6 CO2 + 6 H2O + energia (ATP).
Muitos artrópodes têm, como sistema respiratório, um sistema de túbulos, as traqueias, que, abrindo para o exterior, levam o ar até aos órgãos onde circula a hemolinfa, permitindo, assim, as trocas gasosas.
As filotraqueias ou pulmões foliáceos são estruturas exclusivas dos aracnídeos, sempre existindo aos pares.
Cada pulmão foliáceo é uma invaginação (reentrância) da parede abdominal ventral, formando uma bolsa onde várias lamelas paralelas (lembrando as folhas de um livro entreaberto), altamente vascularizadas, realizam as trocas gasosas diretamente com o ar que entra por uma abertura do exoesqueleto.
A respiração branquial é diferente dos outros tipos de respiração porque o oxigênio encontra-se dissolvido na água.
Os peixes não fazem movimentos de inspiração e expiração como os animais pulmonados. Ocorre um fluxo constante e unidirecional de água que penetra pela boca, atinge os órgãos respiratórios e sai imediatamente pelo opérculo.
A cada filamento chega uma artéria com sangue venoso que se ramifica pelas lamelas branquiais. Aí o sangue é oxigenado e deixa a estrutura por uma veia.
As trocas gasosas entre o sangue e a água são facilitadas pela presença de um sistema contracorrente: fluxo de água e sangue em sentidos contrários. O sangue que deixa as lamelas branquiais contém o máximo de oxigênio e o mínimo de gás carbônico.
Os peixes pulmonados: utilizam a bexiga natatória como pulmão, o que lhes permite resistir a curtos períodos de seca, permanecendo enterrados no lodo.
A respiração pulmonar é o processo pelo qual o ar entra nos pulmões e sai em seguida, num processo conhecido por ventilação pulmonar. É um acontecimento repetitivo que envolve todo o conjunto de órgãos do sistema respiratório.
Durante a inspiração e durante a expiração, o ar passa por diversos e diferentes lugares que fazem parte do aparelho respiratório:[2]
A complexidade dos pulmões aumenta conforme a independência de água no ciclo de vida do animal aumenta. Nos mamíferos, os pulmões são grandes e ramificados internamente e formam pequenas bolsas: os alvéolos.
Gases importantes para a respiração: gás carbônico (CO2) e oxigênio (O2).
No sangue venoso, a concentração de gás carbônico é maior do que a da água ou a do ar em contato com a superfície respiratória, ocorrendo o inverso com o oxigênio.
Desse modo, há difusão de (CO2) para a água ou para o ar e entrada de O2 no sangue. O sangue venoso passa, então, a sangue arterial e este processo denomina-se Hematose.[3]
Entretanto, nas aves, os pulmões são pequenos, compactos, não-alvelares e deles partem os sacos aéreos. Os sacos aéreos atingem todas as regiões importantes do corpo, havendo inclusive vias que partem desses sacos e penetram no esqueleto (ossos pneumáticos).
Os répteis também apresentam pulmões alveolares porém menos complexos que os dos mamíferos. Os alvéolos ampliam a área de superfície das trocas gasosas.
Os animais de respiração cutânea, como os batráquios ou as minhocas, precisam ter o tegumento (epiderme ou pele) constantemente umedecido, uma vez que o oxigénio e o dióxido de carbono só atravessam membranas quando dissolvidos. Portanto, esses organismos só podem viver em ambientes aquáticos e em ambientes terrestres muito úmidos. Entre as células que formam a sua epiderme, há algumas especializadas na produção de um muco. Esse muco espalha-se sobre o tegumento, mantendo-o húmido e possibilitando as trocas gasosas.
O intestino delgado é a parte do tubo digestivo que vai do estômago (do qual está separado pelo piloro) até o intestino grosso (do qual está separado pela válvula ileocecal). O quimo, que resulta de uma primeira transformação dos alimentos no estômago, segue para o intestino delgado passando pelo duodeno, a parte superior deste último.[1]
O intestino delgado é composto de três partes: o duodeno, logo a seguir ao estômago, o jejuno ou parte central e o íleo, nas proximidades do intestino grosso. O jejuno e o íleo são difíceis de diferenciar, pelo que podemos chamar jejuno-íleo ao seu conjunto. A camada mucosa que reveste o seu interior apresenta invaginações, as vilosidades intestinais, pelas quais são absorvidas as substâncias digeridas. O duodeno recebe a bile, produzida pelo fígado e armazenada na vesícula biliar, e também o suco pancreático, produzido pelo pâncreas. É nas paredes do intestino delgado que se produz o suco intestinal. A bile lançada no duodeno divide as gorduras em pequenas gotas, ajudando a ação do suco pancreático e do suco intestinal.[1]
Com os movimentos do intestino delgado e com a ação dos sucos (pancreático e intestinal), o quimo é transformado em quilo, que é o produto final da digestão. Uma vez o alimento transformado em quilo, os produtos úteis ao nosso organismo são absorvidos pelas vilosidades intestinais, passando para os vasos sanguíneos, pois é através da corrente sanguínea que as substâncias absorvidas chegam a todas as células do corpo. As substâncias residuais deste processo digestivo passam para o intestino grosso, do qual acabam por ser expulsas, através do ânus, sob a forma de fezes.[1]
O intestino delgado, com aproximadamente 6–9 m de comprimento, corresponde à parte mais longa do tubo gastrointestinal. Estende-se do piloro (saída do estômago, marcada pela constrição pilórica, que contém um anel espesso de musculatura circular, o esfíncter pilórico) até a válvula ileocecal (local em que o íleo se continua como ceco, a primeira parte do intestino grosso). Consiste em três partes: o duodeno, o jejuno e o íleo.[2]
É a primeira e a menor parte do intestino delgado, com cerca de 25 cm. Apresenta uma estrutura em forma de "C" que contorna a cabeça do pâncreas e uma luz mais larga do que a de outras regiões do intestino delgado. O duodeno é parcialmente retroperitoneal, uma vez que seu começo é fixado ao fígado pelo ligamento hepatoduodenal, parte do omento menor. O órgão pode ser divido em quatro regiões: superior, descendente, inferior e ascendente.[2]
A irrigação sanguínea do duodeno é feita por artérias oriundas do tronco celíaco, como a artéria gastroduodenal e seus ramos, a artéria pancreaticoduodenal superior e a artéria supraduodenal, que irrigam o duodeno proximal. Ramo da mesentérica superior, a artéria pancreaticoduodenal inferior irriga o duodeno distal. As veias do duodeno acompanham as artérias e drenam, direta ou indiretamente, para a veia porta hepática. Os vasos linfáticos, da mesma forma, acompanham as artérias e veias e drenam para uma rica rede linfática da região. A inervação do duodeno é feita por fibras derivadas do nervo vago e dos nervos esplâncnicos maior e menor.[3]
O jejuno é a segunda parte do intestino delgado. Junto com o íleo, forma um tubo que se estende por 6–9 m, sendo que a parte do jejuno corresponde a dois quintos desse tubo. Surge a partir da flexura duodenojejunal, a partir da qual o trato gastrointestinal passa a seguir um caminho intraperitoneal, isto é, suspenso na cavidade peritoneal.[2][4]
A artéria mesentérica superior faz a irrigação do jejuno por meio de seus ramos próprios, as jejunais. Essas artérias se unem para formar as chamadas arcadas arteriais, que dão origem à vasa recta, um conjunto de vasos retos que seguem para o jejuno e o íleo, sendo mais abundantes e longos no primeiro. A drenagem venosa é feita pela veia mesentérica superior, que se une, próximo à cabeça do pâncreas, à veia esplênica para formar a veia porta hepática.[4] A inervação simpática chega até o tecido por um plexo nervoso periarterial, que acompanha a artéria mesentérica superior, inervando todas as partes irrigadas por ela. Os nervos são provenientes da raiz ventral dos segmentos T8-T10 da medula espinhal, passam pelo gânglios paravertebrais até atingirem os nervos esplâncnicos.[5]
O íleo corresponde aos três quintos distais do intestino delgado. Surge a partir do jejuno e termina ao abrir-se no intestino grosso, pela válvula ileocecal, que une o íleo terminal ao ceco, na região pélvica. Não existe uma clara demarcação quanto à divisão entre jejuno e íleo, embora existam diferenças importantes que distinguem os dois, como a parede, que é mais delgada no íleo, assim como a vasa recta se apresenta mais curta, em relação ao jejuno, e a maior deposição de tecido adiposo mesentérico nesta região.[2][4]
O íleo, assim como o jejuno, é irrigado pela artéria mesentérica superior, mas, dessa vez, pelos ramos ileais, que, da mesma forma, emitem as arcadas arteriais, que se anastomosam em arcos, que emitem a vasa recta. A drenagem venosa também é realizada por meio da veia mesentérica superior. O íleo apresenta riqueza de vasos e gânglios linfáticos, divididos em grupos principais, como os linfonodos mesentéricos justa-intestinais, mesentéricos, nódulos superiores (centrais) e ileocólicos. Também como no jejuno, a inervação atinge os tecidos pelo plexo nervoso periarterial, que acompanha a artéria mesentérica superior.[4]
O aparelho gastrointestinal já está se formando na quarta semana do desenvolvimento embrionário, quando o intestino primitivo se fecha (na extremidade cefálica, com a membrana bucofaríngea, e na extremidade caudal, com a membrana cloacal) e parte do saco vitelino, revestido por endoderma, é incorporada no interior do embrião. A camada de endoderma dará origem à maior parte do epitélio e das glândulas do trato gastrointestinal, enquanto o tecido conjuntivo, muscular e outras camadas do tubo digestório vão derivar do mesoderma esplâncnico, situado ao redor do intestino primitivo, que pode ser descrito em três partes: anterior, médio e posterior.[6]
Da porção caudal do intestino anterior surgirá o duodeno, que se situa próximo à abertura do ducto colédoco. Também há contribuições da região cefálica do intestino médio e do mesoderma esplâncnico. Por volta da sexta semana, ocorre a proliferação das células epiteliais que revestem a luz do duodeno, obliterando-a. Posteriormente, há uma recanalização do duodeno, no tempo que antecede o fim do período embrionário.[6] O jejuno e o íleo, assim como outra parte do duodeno serão originadas a partir do intestino médio.[7]
O intestino delgado está dividido em três regiões anatômicas, porém de constituição histológica semelhante. Apesar disso, pequenas diferenças permitem sua identificação. As características em comum são tais como a presença de modificações na superfície da luz intestinal, com o fim de aumentar a sua área. Essas adaptações podem ser pregas circulares, vilosidades, microvilosidades e criptas de Lieberkühn.[8]
A sua mucosa acompanha o padrão predominante no trato gastrointestinal: epitélio simples cilíndrico, com células caliciformes e planura estriada, mais lâmina própria e muscular da mucosa.[8] O epitélio é composto por células absortivas, células caliciformes, células de Paneth, células enteroendócrinas e células M:[9]
A lâmina própria, por definição, é a faixa de tecido conjuntivo frouxo que está logo abaixo do epitélio de revestimento da mucosa e se estende até a muscular da mucosa. No intestino delgado, ela é rica em glândulas tubulosas e apresenta intensa vascularização. Além disso, conta com uma grande quantidade de células linfoides, que atuam na proteção contra a invasão de microorganismos. Essa população linfática é designada pela sigla em inglês GALT (gut-associated lymphoid tissue, em português "tecido linfoide associado ao intestino").[8][9]
Na submucosa (tecido conjuntivo denso), encontramos uma característica distinta do duodeno, a presença de glândulas de Brünner ou glândulas submucosas, que liberam uma secreção com pH básico (8,1 a 9,3), rica em glicoproteínas alcalinas e íons bicarbonato, importantes para neutralizar o quimo para o pH ótimo para as enzimas do suco pancreático e proteger a região proximal do intestino. É comum a todas as partes do intestino delgado a inervação intrínseca efetuada pelo plexo de Meissner (ou submucoso).[9]
A muscular externa é composta por uma camada circular interna e uma longitudinal externa, formadas por fibras de músculo liso. É importante durante a digestão, quando se contrai ritmicamente, sob o controle do plexo nervoso de Auerbach, encontrado por entre as células musculares, realizando dois tipos de contração: segmentação (contrações de mistura) e peristalse (contrações propulsivas).[9][10]
Por fim, a última camada de revestimento do intestino delgado é representada pela serosa, existente nas partes intraperitoniais.[9] Ela corresponde ao padrão, formando-se de uma faixa de epitélio simples escamoso (mesotélio) acompanhada de tecido conjuntivo. É equivalente ao peritônio visceral.[11]
O funcionamento do intestino delgado está profundamente relacionado à digestão e absorção dos nutrientes. Diversos mecanismos estão envolvidos nos processos de movimentação do alimento, de secreção de soluções digestivas, de digestão e absorção de água, eletrólitos e nutrientes, além da circulação sanguínea e da regulação neuro-hormonal locais.[12]
Quando o quimo passa do estômago para o duodeno, por meio do piloro, dependendo do volume transportado, há o desencadeamento de múltiplos reflexos nervosos entéricos que tendem a retardar ou interromper o esvaziamento do estômago. Outros fatores, como o nível de distensão do duodeno, a irritação de sua mucosa, os graus de acidez e osmolalidade, além da presença de determinados subprodutos da digestão de proteínas e gorduras, também podem desencadear mecanismos regulatórios nervosos sobre o esvaziamento do estômago.[13]
Os movimentos do intestino são desempenhados por feixes de músculo liso encontrados em diferentes camadas e em variados sentidos. As células musculares lisas do trato gastrointestinal apresentam algumas peculiaridades, por exemplo, elas funcionam como um sincício, devido ao fato de estarem eletricamente conectadas por conexões juncionais, que permitem o fluxo de íons e, portanto, a transmissão dos sinais nervosos, entre as células adjacentes. Diferente das fibras nervosas, no músculo liso gastrointestinal é o influxo de íons cálcio com menores quantidades de sódio (em um mecanismo controlado pela calmodulina),[14] e não exclusivamente de íons sódio, o principal fator responsável por desencadear um potencial de ação. Por isso os canais encontrados nesse músculo são chamados de canais para cálcio-sódio, que apresentam abertura e fechamento mais lentos que os canais de sódio da célula nervosa.[15]
Existem dois tipos de movimentos desempenhados pelo trato gastrointestinal e que ocorrem no intestino delgado:
A válvula ileocecal evita o retorno de fezes do cólon para o intestino delgado. Ela se fecha quando a pressão no intestino grosso aumenta. Na parede do íleo, encontra-se o esfíncter ileocecal, cujo grau de contração é controlado pelos reflexos do ceco, provocados por sua distensão.[16]
As células caliciformes, espalhadas por todo o epitélio do tubo gastrointestinal, produzem muco secretado diretamente na superfície, que funciona como lubrificante e protetor. No intestino delgado, é característica a presença das profundas criptas de Lieberkühn, que contêm células secretoras especializadas. Ainda há, no duodeno, as glândulas tubulares submucosas (ou glândulas de Brünner).[17]
Entre o piloro e as papilas de Vater, encontram-se inúmeras glândulas de Brünner na camada submucosa. Essas glândulas secretam grande quantidade de muco de características alcalinas, que protege a parede duodenal e aumenta o pH, muito ácido em função do suco gástrico. Elas respondem a estímulos irritantes e táteis na mucosa além do estímulo do nervo vago e de hormônios, como a secretina.[17]
Em meio às vilosidades do intestino, observamos as criptas de Lieberkühn, compostas por células caliciformes e enterócitos, que, nas criptas, secretam água e eletrólitos. Essa secreção age praticamente "lavando" as vilosidades e criando um veículo para a absorção das substâncias do quimo encontradas nessas regiões.[17]
Enterócitos da mucosa do intestino delgado são capazes de produzir enzimas, que ao invés de serem secretadas, funcionam no interior da célula, digerindo as substâncias enquanto elas são absorvidas pelo epitélio. Diversas peptidases, sucrase, maltase, isomaltase e lactase são encontradas nessas células. Ainda uma pequena quantidade de lipase intestinal pode ser observada.[17]
Por fim, glândulas associadas ao trato gastrointestinal, como o pâncreas e o fígado, pela vesícula biliar, excretam seus produtos diretamente no intestino delgado, pelas papilas de Vater do duodeno.[18]
Por dia, o intestino delgado absorve aproximadamente de 7 a 8 litros de água (ingerida e proveniente das secreções gastrointestinais), várias centenas de gramas de carboidratos, de 50 a 100 gramas de aminoácidos e de 50 a 100 gramas de íons. Por ser o principal órgão absortivo do corpo humano, ele dispõe de uma superfície adequada para o processo, com várias pregas, vilosidades e microvilosidades, que aumentam em cerca de mil vezes sua área superficial de mucosa, perfazendo um total de 250 metros quadrados.[19]
A água é absorvida por difusão osmótica. Os íons, como o sódio, são absorvidos por transporte ativo (dependente da hidrólise do ATP) através da membrana. O cloreto é captado no duodeno e no jejuno devido à diferença de potencial elétrico gerada pela absorção do sódio. Por esse mesmo motivo, há liberação de íons hidrogênio na luz intestinal. Eles se combinam com o bicarbonato formando ácido carbônico, que se dissocia em água e gás carbônico, o qual se difunde prontamente para o sangue para ser eliminado nos pulmões.[19]
Os carboidratos são absorvidos, predominantemente, na forma de monossacarídeos, como a glicose, que é transportada através da membrana intestinal juntamente com o sódio, por um processo de difusão facilitada. O sódio se liga a uma proteína, que também se liga à glicose, transportando-os simultaneamente para o interior das células.[13]
Proteínas são absorvidas na forma de dipeptídeos, tripeptídeos ou aminoácidos livres, por um mecanismo semelhante ao transporte da glicose ou por proteínas transportadoras especiais.[13] As gorduras, por sua vez, são incorporadas às micelas dos sais biliares, que são solúveis no quimo e carregam os monoglicerídeos e ácidos graxos livres até a borda da membrana das células epiteliais, à qual se incorporam para serem captadas pelo retículo endoplasmático liso, de onde passarão para a circulação linfática.[19]
A circulação esplâncnica é formada pelos vasos do sistema gastrointestinal. Todo o sangue que passa pelo intestino dirige-se, imediatamente, para o fígado, através da veia porta hepática. Isso é ideal para que micro-organismos e outras moléculas sejam removidos nos capilares sinusóides do fígado antes de entrar na circulação sistêmica. Da mesma forma, parte dos carboidratos e proteínas captados no intestino são absorvidos e armazenados pelas células hepáticas. A atividade intestinal e fatores metabólicos regulam o fluxo sanguíneo no intestino.[12]
Na parede intestinal, encontra-se um sistema nervoso próprio do trato gastrointestinal com aproximadamente 100 milhões de neurônios. Ele efetua o controle dos movimentos e da secreção no local e é composto pelo plexo de Auerbach (ou plexo mioentérico) e pelo plexo de Meissner (ou plexo submucoso). O primeiro controla os movimentos gastrointestinais e o segundo as secreções e o fluxo de sangue na região.[12] Esse sistema pode funcionar independentemente de estímulos extrínsecos, porém está conectado aos sistemas simpático e parassimpático, capazes de aumentar ou inibir sua intensidade de ativação.[20]
Inúmeros neurotransmissores já foram identificados nas células dos plexos entéricos, dentre eles a acetilcolina, geralmente excitatória, e a norepinefrina, geralmente inibitória, são os mais bem compreendidos. O sistema nervoso parassimpático intensifica a atividade gastrointestinal, enquanto a inervação simpática a diminui.[12]
Há participação de hormônios tanto no controle da secreção de substâncias, quanto na regulação, mesmo que menos importante, da motilidade gastrointestinal. Na mucosa do duodeno e do jejuno, há a secreção de colecistocinina, que provoca a forte contração da vesícula biliar para a liberação da bile no intestino delgado. Também há produção de secretina no duodeno; esse hormônio tem a função de promover a secreção de bicarbonato pelo pâncreas. Além desses, são secretados o peptídeo inibidor gástrico (PIG), que retarda o esvaziamento do estômago, e a motilina, que é liberada periodicamente e aumenta a motilidade intestinal.[12]
O pulmão é o órgão do sistema respiratório, responsável pelas trocas gasosas entre o ambiente e o sangue. Sua principal função é oxigenar o sangue e eliminar o dióxido de carbono do corpo.[1]
Em 2016, uma equipe da Universidade da Califórnia observou que, em roedores, o órgão também está envolvido na produção de plaquetas sanguíneas e, em certas condições, na regeneração da medula óssea.[2] Os pesquisadores acreditam que a descoberta se estenderá à anatomia humana.[3]
São dois órgãos de forma piramidal, de consistência esponjosa medindo mais ou menos 25 centímetros de comprimento que localizam-se dentro da caixa torácica, revestidos externamente por uma membrana chamada pleura.[1]
Os pulmões humanos são divididos em segmentos denominados lobs (pronuncia-se "lóbos", com a primeira vogal aberta). O pulmão esquerdo possui dois lobs e o direito possui três.[1]
O pulmão direito é o mais espesso e mais largo que o esquerdo, é também um pouco mais curto pois o diafragma é mais alto no lado direito para acomodar o fígado.[4] O pulmão esquerdo tem uma concavidade que é a incisura cardíaca.[4]
A base de cada pulmão apóia-se no diafragma, órgão músculo-membranoso que separa o tórax do abdome, presente apenas em mamíferos, promovendo, juntamente com os músculos intercostais, os movimentos respiratórios.  Localizado logo acima do estômago, o nervo frênico controla os movimentos do diafragma.
Os pulmões são compostos de brônquios que se dividem em bronquíolo e alvéolos pulmonares. Os alvéolos totalizam-se em um total de  4 milhões e são estruturas saculares (semelhantes a sacos) que se formam no final de cada bronquíolo e têm em sua volta os chamados capilares pulmonares. Nos alvéolos se dão as trocas gasosas ou hematose pulmonar entre o meio ambiente e o corpo, com a entrada de oxigênio na hemoglobina do sangue (formando a oxiemoglobina) e saída do gás carbônico ou dióxido de carbono dos capilares para o alvéolo.
Nos pulmões os brônquios ramificam-se intensamente, dando origem a tubos cada vez mais finos, os bronquíolos. O conjunto altamente ramificado de bronquíolos é a árvore brônquica ou árvore respiratória.
Cada bronquíolo termina em pequenas bolsas formadas por células epiteliais achatadas (tecido epitelial pavimentoso) recobertas por capilares sanguíneos, denominadas alvéolos pulmonares.
Na respiração pulmonar o ar entra e sai dos pulmões devido à contração e ao relaxamento do diafragma. Quando o diafragma se contrai, ele diminui a pressão nos pulmões e o ar que está fora do corpo entra rico em oxigênio O2; processo chamado de inspiração.
Quando o diafragma relaxa, a pressão dentro dos pulmões aumenta e o ar que estava dentro agora sai com o dióxido de carbono; processo denominado de expiração.
As pessoas podem parar de respirar mas ninguém consegue ficar sem respirar por mais de alguns minutos, porque a concentração de dióxido de carbono no sangue fica tão alta que o corpo não consegue mais fornecer energia para as células e o bulbo (parte do sistema nervoso que forma o encéfalo) manda impulsos nervosos para o diafragma e os músculos intercostais, para que se contraiam e a respiração volte a ser executada normalmente.
Os termos transtorno, distúrbio e doença combinam-se aos termos mental, psíquico e psiquiátrico para descrever qualquer anormalidade, sofrimento ou comprometimento de ordem psicológica e/ou mental. Os transtornos mentais são um campo de investigação interdisciplinar que envolvem áreas como a psicologia, a psiquiatria e a neurologia. As classificações diagnósticas mais utilizadas como referências no serviço de saúde e na pesquisa hoje em dia são o Manual Diagnóstico e Estatístico de Desordens Mentais — DSM IV, DSM V e a Classificação Internacional de Doenças — CID-10.
Em psiquiatria e em psicologia os termos transtornos, perturbações, disfunções ou distúrbios psíquicos são preferíveis, em relação ao termo doença, pois poucos quadros clínicos mentais apresentam todas as características de uma doença no sentido tradicional. O conceito de transtorno, ao contrário, implica um comportamento diferente, desviante, anormal.[1]
O conceito de anormalidade só é compreensível em relação a uma norma; mas nem toda variação em relação a uma norma adquire caráter patológico. Assim uma pessoa superdotada ou um criminoso estão ambos fora da norma, sem que no entanto seu estado tenha um caráter patológico. Assim, para se compreender o termo transtorno é necessário ter-se presente quais normas são relevantes para a definição.[2][3]
1. Norma subjetiva: a própria pessoa sente-se doente. No entanto, esta norma não é suficiente para uma definição, porque ela envolve uma perceção subjetiva do problema, que pode diferir de uma perceção externa, objetiva: além dos casos em que as duas perspectivas estão de acordo, há casos em que a pessoa está subjetivamente doente, mas esse estado é externamente não observável, ou vice-versa;
2. Norma estatística: a norma é dada pela frequência do fenômeno na população. Assim, todas as pessoas que estão acima ou abaixo de um determinado valor de corte estão fora da norma. No entanto essa norma não leva em conta o valor dado às características levadas em conta. Assim uma pessoa que nunca teve cáries está tão fora da norma como uma pessoa que têm muitíssimas — aqui se vê também seu limite;
3. Norma funcionalista: aqui a norma é ditada pelo prejuízo das funções relevantes. Assim, se alguém não consegue mover a mão está fora da norma, porque a mão não pode cumprir sua função de pegar. Enquanto a norma funcional é muito importante para os transtornos e doenças somáticas (ou corporais), não o é sempre no caso dos transtornos mentais, porque a função nem sempre é objetivável. Assim a sexualidade possui inúmeras funções — reprodução, prazer, comunicação, interpessoal… — de forma que se torna difícil definir os transtornos nessa área;
4. Norma social: aqui o transtorno é definido a partir de normas e valores definidos socialmente. A perspectiva da etiquetação (labeling) de Scheff postula o seguinte desenvolvimento para tais normas: (a) Desvio primário — a pessoa desrespeita um determinada norma social e isso pode levar a duas reações: ou o comportamento é normalizado (através de tolerância, racionalização, discussão)e assim o conflito é solucionado, ou o conflito não se soluciona de maneira positiva e a pessoa é rotulada (ex. um diagnóstico, uma condenação jurídica…) e recebe assim uma atenção especial. Esse estigma leva a um (b)desvio secundário — a pessoa, em reação à etiquetação, começa a comportar-se de maneira diferente em conformidade com o novo papel social recebido: a pessoa começa a comportar-se de acordo com a etiqueta recebida. Esse é um dos grandes problemas ligados a todos os tipos de classificação e diagnóstico.[1]
5. Norma dos especialistas: esta é uma forma especial de norma social, definida por uma categoria especial de pessoas — os especialistas (médicos, psicólogos, etc.). Como as normas sociais, também estas estão sujeitas a uma certa dose de arbitrariedade. Os atuais sistemas de classificação (DSM-V e CID-10) são formas especiais de normas de especialistas que têm por fim reduzir os perigos dessa arbitrariedade.
Dentre os sistemas de classificação dos transtornos mentais o de Jaspers (1913) recebe, pela sua importância histórica, um lugar preponderante. Esse sistema é triádico, por diferenciar três formas de transtornos mentais:[4]
1. Doenças somáticas conhecidas que trazem consigo um transtorno psíquico, em seus subtipos:
2. Os três grandes tipos de psicoses endógenas (ou seja, transtornos psíquicos cuja causa corporal ainda é desconhecida):
3. Psicopatias:
Dois termos desempenham assim um papel preponderante: neurose designa os "transtornos mentais que não afetam o ser humano em si",[5] ou seja, aqueles supostamente sem base orgânica nos quais o paciente possui consciência e uma percepção clara da realidade e em geral não confunde sua experiência patológica e subjetiva com a realidade exterior; psicose, por sua vez, são "aqueles transtornos mentais que afetam o ser humano como um todo",[6] ou seja um transtorno no qual o prejuízo das funções psíquicas atingiu um nível tão acentuado que a consciência, o contato com a realidade ou a capacidade de corresponder às exigências da vida se tornam extremamente diferenciadas, e por vezes perturbadas, e para a qual se conhece ou se supõe uma causa corporal.
Entre as neuroses costumam-se classificar: a perturbação obsessiva-compulsiva, a transtorno do pânico, as diferentes fobias, os transtornos de ansiedade,a depressão nervosa, a distimia, a síndrome de Burnout, entre outras.[4]
O tratamento das neuroses e psicoses pode ser feito com um psicoterapeuta, um psiquiatra ou equipes de profissionais de saúde mental. As equipes incluem sempre psicólogos e psiquiatras, e podem incluir também enfermeiros, terapeutas ocupacionais, musicoterapeutas e assistentes sociais, entre outros.[7]
Essa forma de classificação, apesar de muito utilizada ainda hoje, tem alguns problemas sérios: (a) a classificação limita o transtorno mental à pessoa (não correspondendo às exigências de uma análise bio-psico-social), (b) a diferenciação entre neurose e psicose endógena não é sempre tão clara como parece à primeira vista e (c) ambos os conceitos (neurose e psicose) estão ligados a uma etiologia psicanalítica dos transtornos mentais, tornando-os de utilidade limitada para profissionais de outras escolas[1] (ver abaixo "Etiologia: A perspectiva psicoanalítica").
O uso de sistemas de classificação para os transtornos mentais possibilita diagnósticos psiquiátricos precisos, fornecendo uma base comum para o diálogo entre os psiquiatras e psicoterapeutas de diversas linhas. Os dois sistemas atualmente em uso — a Classificação Internacional de Doenças (CID), da Organização Mundial de Saúde e o Manual Diagnóstico e Estatístico de Transtornos Mentais (DSM, sigla em inglês), da Associação Americana de Psiquiatria (American Psychiatric Association - APA) — prescindiram dos termos "neurose" e "psicose", salvo em raros casos, para os quais não havia termo mais apropriado. A CID é um sistema internacional, enquanto o DSM, tem sua importância ligada sobretudo à pesquisa. Apesar das diferenças, ambos os sistemas têm uma série de características em comum:[8]
Para uma descrição detalhada desses sistemas, ver: diagnóstico psiquiátrico, CID-10 Capítulo V: Transtornos mentais e comportamentais e Manual Diagnóstico e Estatístico de Desordens Mentais. A CID possui ainda um sistema multiaxial especial para o diagnóstico infanto-juvenil.
Digno de menção é o trabalho do Grupo OPD (Arbeitskreis OPD) que publicou o "Diagnóstico Psicodinâmico Operacionalizado" (OPD, sigla em alemão) que oferece uma classificação de diferentes aspectos psicodinâmicos em complementação aos outros dois sistemas [1].
Estudos recentes estimam que entre 32% (Robins & Regier, 1991) e 65% (Wittchen & Perkonigg, 1996) dos adultos sofreram em algum momento da vida de um transtorno mental. A grande diferença entre as estimativas dos dois trabalhos devem-se às dificuldades metodológicas envolvidas nesse tipo de trabalho. No entanto a literatura parece unânime em afirmar que os transtornos mais frequentes são as diferentes formas de fobia (9,2-24,9%), sobretudo as fobias específicas, a fobia social e a agorafobia; o abuso e a dependência de substâncias químicas (17,7-26,6%), sobretudo álcool; e os transtornos afetivos (5,5-19,8%), sobretudo a depressão.[9] Outros transtornos são muito menos comuns.
Ao contrário do que se pensa normalmente, os transtornos mentais são relativamente frequentes na população infanto-juvenil: entre 15% e 22% da população apresenta alguma forma de distúrbio nessa faixa etária, sobretudo as fobias, abuso e dependência de substâncias e transtornos afetivos.[10] Além disso há indícios de que a frequência dos transtornos mentais não aumenta com a idade, com exceção dos transtornos da cognição causados pela demência.[9]
Os transtornos afetivos, os devidos à ação de substâncias psicoativas e os transtornos neuróticos (ou seja, ligados ao medo) costumam se manifestar pela primeira vez nas três primeiras décadas de vida. Enquanto a maioria desses transtornos aparece mais frequentemente a partir do fim da puberdade e do início da idade adulta, as fobias específicas tendem a se manifestar pela primeira vez já na infância e na adolescência.[11] Outro fenômeno muito comum é a comorbididade dos transtornos mentais: um distúrbio costuma vir acompanhado de um ou até mais transtornos.[9]
Os transtornos mentais são, tanto em sua gênese como em sua manifestação, fenômenos muito complexos.
O modelo bio-psico-social procura fazer jus a essa complexidade buscando analisar a gênese e o desenvolvimento dos transtornos mentais sob diferentes pontos de vista, de acordo com os diferentes fatores que os influenciam:[12]
Os transtornos mentais podem dar-se assim em diferentes níveis:[1]
(a) determinadas funções mentais (memória, percepção, aprendizagem, etc.).
(b) grupos de funções. Como a psicologia geral ainda não produziu modelos empíricos para tais grupos de funções e sua relação com os transtornos mentais, deve ficar sua descrição por hora no nível dos sintomas, das síndromes e dos diagnósticos, como descritos nos sistemas de classificação (CID-10 e DSM-IV).
De acordo com o modelo estresse-vulnerabilidade o irromper de um transtorno mental está ligado, de um lado, à presença de uma predisposição genética ou adquirida no decorrer da vida (vulnerabilidade) e, de outro, à exposição a situações estressantes. Quanto maior a predisposição, menor tem de ser o nível de estresse para que um distúrbio mental irrompa. A relação entre vulnerabilidade e estresse, no entanto, é mediada pela resiliência, ou seja, a capacidade do indivíduo de resistir ao estresse.[13] Ver mais abaixo "Fatores vulnerabilizantes e fatores protetivos". Importante pare este tema é o conceito de salutogênese.
Apesar de fatores genéticos desempenharem sempre um papel ora mais ora menos importante na gênese dos transtornos mentais, até hoje pouco se sabe a respeito do exato funcionamento de seus mecanismos: apenas para o Mal de Alzheimer pôde-se localizar um grupo de genes responsáveis — que no entanto ainda não explica a doença completamente. Nos outros casos a influência genética parece dar de maneira mais indireta na forma de tendências, que são influenciadas (fortalecidas ou enfraquecidas) por outros fatores.[14]
Para o desenvolvimento dos transtornos mentais são importantes sobretudo três sistemas do corpo humano: o sistema nervoso, o sistema endócrino e o sistema imunológico.
Relacionadas à maioria dos distúrbios mentais foram observadas modificações do sistema nervoso central; os distúrbios mais importantes ligam-se às funções cognitivas (memória, atenção, concentração, processamento e avaliação de informações, planejamento de ações, etc.) bem como distúrbios da regulação das emoções e do estresse. Esses dois últimos grupos de funções estão, além disso, intimamente ligados ao sistema nervoso periférico — como no caso dos ataques de pânico, normalmente caracterizados por taquicardia e suor excessivo.[15]
A regulação hormonal desempenha também um importante papel no desenvolvimento dos transtornos mentais, e não apenas nos transtornos psicossomáticos. Modificações na regulação hormonal foram descritos também nas depressões, transtornos de estresse pós-traumático e transtornos alimentares. Aqui, como no caso dos demais fatores biológicos tratados, é muito difícil de estabelecer a causa, porque as observações têm caráter correlativo: é difícil estabelecer se o transtorno mental provoca a mudança física ou vice-versa.[15]
Fatores vulnerabilizantes são aqueles que provocam a vulnerabilidade da pessoa, ou seja, uma maior probabilidade de ela apresentar um transtorno mental.[16] A vulnerabilidade pode ser pessoal, com uma maior influência dos fatores biológicos, ou ambiental, com maior influência de fatores sócio-econômicos e do meio ambiente. As diferentes vulnerabilidades podem ter um valor mais ou menos relativo conforme sejam mais ou menos estáveis — geneticamente determinadas ou ligadas a determinadas condições externas; relacionadas a determinadas fases da vida (ver abaixo, ex. puberdade) ou à situação geral da pessoa (ex. pobreza), etc..[17]
Paralelamente aos fatores vulnerabilizantes há os chamados fatores protetivos, ou seja, aqueles que agem contra as condições estressantes, "fortalecendo" o indivíduo contra os transtornos mentais. Há dois tipos de tais fatores: os fatores de resiliência, que são as características pessoais e as competências que dão a uma pessoa a capacidade de se adaptar adequadamente a situações ruins ou mesmo ameaçadoras para o seu bem estar; e os fatores de apoio social, que se referem sobretudo ao meio ambiente e à rede social da pessoa. A capacidade de construir uma rede social está intimamente ligada às experiências com relacionamento feitas na infância (ver abaixo).[17]
Robert J. Havighurst (1982)[18] propõe, com seu conceito de tarefas do desenvolvimento (developmental tasks),[19] uma visão abrangente do desenvolvimento humano que envolve toda a sua vida. Ele dividiu a vida humana em 6 fases, desde a infância até a velhice. Cada uma dessas fases confronta o indivíduo com uma série de tarefas ou desafios de ordem biológica ou cultural que devem ser superados, a fim que o desenvolvimento seja visto como "normal". Por exemplo fazem parte das tarefas do início da idade adulta a definição profissional, a escolha de um parceiro, a fundação de uma família, a criação dos filhos. Além das tarefas individuais existem também outros tipos de tarefas, como as tarefas familiares (como se espera que uma família se desenvolva). Caso a pessoa não possua os meios necessários (resources) para superar essas tarefas ou não possa fazê-lo por influência externa surge uma situação extremamente tensa que pode provocar ou pelo menos facilitar o aparecimento de um transtorno mental. Sobre o estresse como fator etiológico dos transtornos mentais ver mais abaixo.
Freud (1917)[20] foi o primeiro a propor um modelo abrangente do desenvolvimento dos transtornos mentais: De acordo com esse teórico são eles fruto de tensões internas e, no mais das vezes, inconscientes não resolvidas que têm sua origem no desenvolvimento da libido da criança. Nesse desenvolvimento a criança atravessa diferentes fases (oral, anal, fálica, edipal e genital), nas quais faz a experiência de ter determinadas necessidades saciadas ou não. Quando adulta a pessoa faz determinadas experiências traumáticas que desencadeiam os distúrbios mentais — a forma exata desses distúrbios está então ligada às experiências feitas nas diferentes fases do desenvolvimento.
Apesar de ser muito difundida e ter tido muita influência sobre toda a psicologia posterior, a perspectiva psicanalítica é alvo de muitas críticas, sobretudo por parte de psicólogos ligados a uma abordagem mais experimental em psicologia: Os conceitos psicanalíticos são de difícil operacionalização e são assim difíceis de ser averiguados empiricamente. Além disso vários estudos parecem sugerir que algumas partes da teoria original de Freud precisam ser revistas — sobretudo no que diz respeito à origem da personalidade. O poder heurístico da teoria psicanalista é no entanto muito grande, servindo como base de alguns paradigmas de pesquisa muito férteis também sob um ponto de vista empírico — como é o exemplo da teoria do apego (ver abaixo); além disso ela apontou desde muito cedo a importância da infância como uma fase central para o desenvolvimento dos transtornos mentais.[17]
A teoria do vínculo ou da ligação afetiva (attachment) parte da observação do chamado "comportamento de apego", que pode ser observado em crianças desde seu nascimento: são formas de comunicação rudimentares (ex. choro, sorriso, primeiros sons orais, mais tarde a linguagem) que têm por fim gerar e manter a proximidade entre a criança e sua mãe. Através desse tipo de comportamento surge uma forma especial de relacionamento — o relacionamento de vínculo afetivo — que é uma forma especial de relacionamento social caracterizada por segurança emocional e confiança. A qualidade dos primeiros relacionamentos de vínculo é de grande importância para o desenvolvimento da pessoa, influenciando a autoestima, o otimismo em relação à vida, etc. O relacionamento mãe-filho é o modelo de tal relacionamento, que no entanto pode se desenvolver com outras pessoas (pai, mãe adotiva, babá, etc.). Problemas no desenvolvimento de um relacionamento de vínculo saudável podem surgir por meio de diferentes formas de privação social: interações sociais descontínuas, interações sociais nocivas e interações sociais insuficientes. Esse tipo de privação aumenta a susceptibilidade a diferentes tipos de transtorno mental, desde transtornos do funcionamento social na infância (CID F94) até outros transtornos na idade adulta.[17]
A contribuição da psicologia da aprendizagem para o entendimento dos transtornos mentais apontam três tipos de condições de aprendizagem que aumentam a susceptibilidade a transtornos mentais — o comportamento nocivo pode ser fruto:[17]
1. de processos de condicionamento:
2. de um transtorno dos processos de aprendizagem: ou seja a exposição a estímulos ou a inibição excessivos, ou ainda situações em que estímulos positivos e negativos se encontram em conflito muito forte aumentam a vulnerabilidade do indivíduo;
3. de processos cognitivos: não apenas as condições do aprendizado em si podem ser negativas e facilitar o desenvolvimento de transtornos mentais, mas também sua interpretação pelo indivíduo. Aqui desempenham um papel muito importante os chamados padrões ou esquemas de pensamento disfuncionais: a pessoa está convencida de ser incapaz de influenciar a situação em que se encontra e por isso não vê as possibilidades que realmente tem, ou a pessoa tem teorias próprias a respeito do que outras pessoas pensam a respeito dela, etc.
Segundo Tseng (2001) a cultura pode ter seis tipos de efeito sobre os transtornos mentais:[21]
1. Efeito patogênico: Fatores culturais podem ser a origem explícita ou imediata de um transtorno mental. Exemplos são distúrbios frutos da quebra de um tabu ou o não cumprimento de uma expectativa social, como o suicídio de alunos que não são aprovados em exames de admissão típico de algumas culturas do extremo oriente.
2. Efeito pato-seletivo: cada cultura vê alguns comportamentos patológicos como mais ou menos aceitáveis, de acordo com suas próprias normas. Assim em algumas culturas certos comportamentos patológicos, como a agressividade ou o suicídio, são mais aceitos do que em outras.
3. Efeito pato-plástico: a cultura determina a forma de expressão de determinados transtornos, por exemplo o conteúdo das alucinações, determinados tipos de obsessões e fobias. Além disso alguns transtornos têm sintomas diferentes em diferentes culturas, como no caso da depressão: na Ásia faltam os sentimentos de culpa típicos da depressão na Europa.
4. Efeito pato-facilitante: determinados fatores culturais, como a permissividade o até mesmo a exigência de determinados tipos de comportamento que podem tornar-se patológicos, podem aumentar a frequência de determinados transtornos e na população. Assim culturas em que o álcool é mais aceito e em que bebedeiras fazem parte de determinadas circunstâncias sociais tendem a ter um maior número de casos de abuso e dependência desse tipo de substância.
5. Efeito pato-reativo: a cultura determina além disso a reação das pessoas a determinados tipos de doença. Essa é uma possível explicação para o fato de a esquizofrenia ter uma prognose mais positiva nos países em desenvolvimento do que nos países industrializados.
6. Idioma de estresse (Idioms of distress): cada cultura possui um "idioma" próprio para expressar seus estresses, tensões e problemas psíquicos. A esse fato estão relacionados as "síndromes ligadas à cultura" (culture bound syndromes), ou seja, determinados quadros clínicos que há apenas em determinados círculos culturais — como por exemplo as reações psicóticas ao Chi Kung, que há apenas na China, e a bulimia, típica da cultura ocidental.
Para as intervenções psicológicas ver intervenções psicológicas ou ainda psicoterapia. Para intervenções medicamentosas ver psicofarmacologia e psiquiatria.
No Brasil, a Câmara Federal aprovou em 17 de março de 2009, em caráter conclusivo, o Projeto de Lei 6 013/01, do deputado Jutahy Junior (PSDB-BA), que conceitua transtorno mental, padroniza a denominação de enfermidade psíquica em geral e assegura aos portadores desta patologia o direito a um diagnóstico conclusivo, conforme classificação internacional. O projeto determina que transtorno mental é o termo adequado para designar o gênero enfermidade mental, e substitui termos como "alienação mental" e outros equivalentes.[22]
Antibiótico (do grego αντί - anti+ βιοτικός - biotikos, "contra um ser vivo") é qualquer medicamento capaz de combater uma infecção causada por microorganismos que causam infecções a outro organismo. Não destroem vírus.[1]
O termo antibiótico tem sido utilizado de modo mais restrito para indicar substâncias que atingem bactérias, embora possa ser utilizado em sentido mais amplo contra outros parasitas (protozoários, fungos ou helmintos). Ele pode ser bactericida, quando tem efeito mortífero sobre a bactéria, ou bacteriostático, se interrompe a sua reprodução ou inibe seu metabolismo mas também causa efeitos significativos em doenças causadas por vírus como a gripe.[2]
As primeiras substâncias descobertas eram produzidas por fungos, como a penicilina. Atualmente, existem também antibióticos sintetizados ou alterados em laboratórios farmacêuticos para evitar resistências e diminuir efeitos colaterais.
Os antibióticos, também conhecidos como antimicrobianos, podem ser classificados de diversas formas. A maior utilidade da classificação dos antibióticos é a de permitir uma melhor compreensão das características dos fármacos.[3] Sendo assim, podemos encontrar antibióticos classificados em bactericidas e bacteriostáticos, dependendo se o fármaco causa diretamente a morte das bactérias ou se apenas inibe sua replicação, respectivamente. Na prática, esta classificação se baseia no comportamento do antibiótico in vitro e ambas as classes podem ser eficazes no tratamento de uma infeção.[4]
Classes de antibióticos agrupados por estrutura
Em resumo, podemos também dividir os antibióticos em três grandes classes, de acordo com seu mecanismo de ação: Antibióticos que agem sobre a parede microbiana, Antibióticos que agem sobre a síntese proteica e Antibióticos com outros mecanismo de ação.
A resistência antibiótica é a capacidade dos microrganismos de resistir aos efeitos de um antibiótico ou antimicrobiano. O uso inadequado de antibióticos conduz ao aparecimento de resistências, tornando os agentes antimicrobianos menos eficazes.[16] A resistência pode ser adquirida via: transformação, conjugação, transdução e mutação.
Segundo a Organização Mundial de Saúde, mais de 50% dos antibióticos são prescritos de forma inadequada,[17] o que vem causando resistência à ação dos medicamentos. Em um encontro de especialistas, realizado em março de 2012 na Dinamarca, a diretora-geral da OMS, Margaret Chan alertou para o desafio que isso representa para os países em desenvolvimento, que são os principais afetados por diversas enfermidades. "Muitos países estão incapacitados pela falta de infra-estrutura, incluindo laboratórios, diagnósticos, confirmação de qualidade, capacidade de regulação, monitoramento e controle sobre a obtenção e a utilização de antibióticos", diz Chan.[18]
Para evitar o uso indiscriminado de antibiótico pela população e conter o avanço dos casos de contaminação por superbactérias começaram a valer novas regras a partir de 28 de novembro de 2010 para a venda de antibióticos nas farmácias e drogarias brasileiras (resolução RDC 44 da Agência Nacional de Vigilância Sanitária[19]). Os medicamentos só podem ser vendidos com a apresentação de duas vias da receita médica, sendo que a 2° via ficará com o estabelecimento e a 1° com o consumidor. A regra vale atualmente para 93 tipos de substâncias antimicrobianas que compõem todos os antibióticos registrados no Brasil. Estão de fora da lista os antibióticos usados exclusivamente em hospitais.[20]
Em Portugal, os antibióticos são classificados como medicamentos sujeitos a receita médica (MSRM). Órgão regulador e fiscalizador é o instituto público Infarmed.[21][22] A legislação portuguesa prevê penalidades semelhantes às leis brasileiras, entre outras definidas no Decreto-Lei nº 48547 de 1968 que regulamenta o exercício da atividade farmacêutica[23] — mesmo assim, a venda ilegal de antibióticos sem receita médica é, como no Brasil, problemática.[24]
A Direção-Geral da Saúde do Ministério da Saúde considera a resistência aos antibióticos "uma das maiores ameaças à Saúde Pública" atuais. Portugal é um dos países da Europa com taxas elevadas de resistência aos antibióticos.[24] O Programa Nacional de Prevenção das Resistências aos Antimicrobianos (PNPRA), introduzido no âmbito do Plano Nacional de Saúde 2004-2010[25] e concretizado em novembro de 2009 prevê diminuir até 2015, a nível nacional, as resistências aos antibimicrobianos estimulando o uso racional dos antibióticos e monitorizando as resistências através da implementação de um sistema informático de vigilância, entre outros.[26]
O primeiro antibiótico identificado pelo homem foi a penicilina. Alexander Fleming, médico microbiologista do St. Mary's Hospital, de Londres,[27] já vinha há algum tempo pesquisando substâncias capazes de matar ou impedir o crescimento de bactérias nas feridas infectadas, pesquisa justificada pela experiência adquirida na Primeira Grande Guerra 1914 - 1918, na qual muitos combatentes morreram em consequência da infecção em ferimentos mal-tratados por falta de um tratamento adequado.
Em 1928 Fleming desenvolveu pesquisas sobre estafilococos, quando descobriu a penicilina. A descoberta da penicilina deu-se em condições muito peculiares, graças a uma sequência de acontecimentos imprevistos e surpreendentes.
No mês de agosto de 1928 Fleming tirou férias e, por esquecimento, deixou algumas placas com culturas de estafilococos sobre a mesa, ao invés de guardá-las na geladeira ou inutilizá-las, como seria natural. Ao retornar ao trabalho, em setembro do mesmo ano, observou que algumas das placas estavam contaminadas com mofo, fato este relativamente frequente. Colocou-as então, em uma bandeja para limpeza e esterilização com lisol. Neste exato momento entrou no laboratório um seu colega, Dr. Pryce, e lhe perguntou como iam suas pesquisas. Fleming apanhou novamente as placas para explicar alguns detalhes ao seu colega sobre as culturas de estafilococos que estava realizando, quando notou que havia, em uma das placas, um halo transparente em torno do mofo contaminante, o que parecia indicar que aquele fungo produzia uma substância bactericida. O assunto foi discutido entre ambos e Fleming decidiu fazer algumas culturas do fungo para estudo posterior.
O fungo foi identificado como pertencente ao gênero Penicillium, de onde deriva o nome da penicilina dado à substância por ele produzida. Fleming passou a empregá-lo em seu laboratório para selecionar determinadas bactérias, eliminando das culturas as espécies sensíveis à sua ação.
Foi o primeiro teste de reação penicilínica realizado em laboratório. Por outro lado, a descoberta de Fleming não despertou inicialmente maior interesse e não houve a preocupação em utilizá-la para fins terapêuticos em casos de infecção humana até a eclosão da Segunda Guerra Mundial, em 1939. Nesse ano e em decorrência do próprio conflito, a fim de evitarem-se baixas desnecessárias, foram então ampliadas as pesquisas a respeito da penicilina e seu uso humano.
Em 1940, Sir Howard Fleorey e Ernst Chain, da Universidade de Oxford, retomaram as pesquisas de Fleming e conseguiram produzir penicilina com fins terapêuticos em escala industrial, inaugurando uma nova era para a medicina denominada a era dos antibióticos.
Alguns anos mais tarde, Ronald Hare, colega de trabalho de Fleming, tentou, em vão, "redescobrir" a penicilina em condições semelhantes às que envolveram a descoberta de Fleming. Após um grande número de experiências verificou-se que a descoberta da penicilina só tornou-se possível graças a uma série inacreditável de coincidências, que foram:
Apesar de todas essas felizes coincidências, se Fleming não tivesse a mente preparada e avançada não teria valorizado ou mesmo notado o halo transparente em torno do fungo e descoberto a penicilina.
Animalia, Animal ou Metazoa[1] é um reino biológico composto por seres vivos pluricelulares, eucariontes, heterotróficos, cujas células formam tecidos biológicos, com capacidade de responder ao ambiente (possuem tecido nervoso) que os envolve ou, por outras palavras, pelos animais.[2]
A maioria dos animais possui um plano corporal que se determina à medida que se tornam maduros e, exceto em animais que metamorfoseiam, esse plano corporal é estabelecido desde cedo em sua ontogenia quando embriões. O estudo científico dos animais é chamado zoologia, que tradicionalmente estudava, não só os seres vivos com as características descritas acima, mas também os protozoários. Como resultado de estudos filogenéticos, consideram-se os Protistas como um grupo separado dos animais.[3]
A palavra "animal" é derivada do latim anima, no sentido de fôlego vital, e entrou na língua portuguesa através da palavra animalis. Animalia é seu plural.[4] A definição biológica da palavra refere-se a todos os membros do reino Animalia, englobando organismos tão diversos como esponjas, medusas, insetos e seres humanos. Coloquialmente, o termo "animal" é com frequência utilizado para referir-se a todos os animais diferentes dos humanos, e raramente para referir-se a animais não classificados como Metazoários.
Os metazoários mais simples apresentam simetria radial — por esta razão, são classificados como radiados (em contraposição com os bilatérios, que têm simetria bilateral). Estes animais são diploblásticos, isto é, possuem apenas dois folhetos embrionários. A camada exterior (ectoderme) corresponde à superfície da blástula e a camada interior (endoderme) é formada por células que migram para o interior. Ela então se invagina para formar uma cavidade digestiva com uma única abertura, (o arquêntero). Esta forma é chamada gástrula (ou plânula quando ela é livre-natante). Os cnidários e os ctenóforos (águas vivas, anêmonas, corais, etc) são os principais filos diploblásticos.[5] As formas restantes, formam um grupo chamado bilatérios, uma vez que eles apresentam simetria bilateral e são triploblásticos. Os mixozoários, um grupo de parasitas microscópicos, têm sido considerados cnidários reduzidos; porém, podem ser derivados dos bilatérios. A blástula invagina sem se preencher previamente, então a endoderma é apenas seu forro interior, a parte interna é preenchida para formar o terceiro folheto embrionário, a mesoderme. Os animais mais simples dentre estes são os Platyhelminthes (vermes achatados, como a tênia).
Animais são eucariontes, e divergiram do mesmo grupo dos protozoários flagelados que deram origem aos fungos e aos coanoflagelados. Estes últimos são especialmente próximos por possuírem células com "colarinhos" aparecendo somente entre eles e as esponjas, e raramente em certas outras formas de animais. Em todos estes grupos, as células móveis, geralmente os gametas, possuem um único flagelo posterior com ultra-estrutura similar.[carece de fontes?]
Os animais adultos são tipicamente diplóides, produzindo pequenos espermatozóides móveis e grandes ovos imóveis. Em todas as formas o zigoto fertilizado divide-se (clivagem) para formar uma esfera oca chamada blástula, que então sofre rearranjo e diferenciação celular. As blástulas são provavelmente representativas do tipo de colônia de onde os animais evoluíram; formas similares ocorrem entre os flagelados, como os Volvox.[carece de fontes?]
Exceto por uns poucos traços fósseis questionáveis, as primeiras formas que talvez representem animais aparecem nos registros fósseis por volta do Pré-Cambriano. São chamadas Biota Vendiana e são muito difíceis de relacionar com as formas recentes. Virtualmente todos os restantes filos fazem uma aparição mais ou menos simultânea durante o período Cambriano. Este efeito radioativo massivo pode ter surgido devido a uma mudança climática ou uma inovação genética e é tão inesperada que é geralmente chamada de Explosão Cambriana.[carece de fontes?]
As esponjas (Porifera) separaram-se dos outros animais muito cedo e são muito diferentes. Esponjas são sésseis e geralmente alimentam-se retirando as partículas nutritivas da água que entra através de poros espalhados por todo o corpo, que é suportado por um esqueleto formado por espículas. As células são diferenciadas, porém, não estão organizadas em grupos distintos.[6]
Existem também três filos "problemáticos" - os Rhombozoa, Orthonectida, e Placozoa - e possuem uma posição incerta em relação aos outros animais. Quando eles foram inicialmente descobertos, os Protozoa foram considerados como um filo animal ou um sub-reino, porém, como eles são geralmente desrelacionados e mais similares às plantas do que animais, um novo reino, o Protista, foi criado para abrigá-los.[carece de fontes?]
A distinção mais notável dos animais é a forma como as células se seguram juntas. Ao invés de simplesmente ficarem grudadas juntas, ou seguradas em um local por pequenas paredes, as células animais são conectadas por junções septadas, compostas basicamente por proteínas elásticas (colágeno é característico) que cria a matriz extracelular. Algumas vezes esta matriz é calcificada para formar conchas, ossos ou espículas, porém de outro modo é razoavelmente flexível e pode servir como uma estrutura por onde as células podem mover-se e reorganizar-se.[carece de fontes?]
Existem cerca de 1 200 000 espécies de animais já descritas, divididos em 54 filos, a grande maioria podemos ver sem a ajuda do microscópio, mas, existem também aqueles microscópicos, mas uma característica entre todos esses seres-vivos é que todos são pluricelulares.[7] Os dois antigos grupos de animais: vertebrados e invertebrados, são divididos de acordo com a presença ou a ausência da coluna vertebral.[8]
Independentemente disso, todos os animais pertencem a um grupo monofilético chamado Metazoa (ou Eumetazoa quando o nome Metazoa é usado para todos os animais), caracterizado por uma câmara digestiva e camadas separadas de células que diferenciam-se em vários tecidos. Características distintivas dos metazoários incluem um sistema nervoso e músculos.[carece de fontes?]
Os Metazoa mais simples apresentam simetria radial e, por esta razão, são classificados como Radiata (em contraposição com os Bilateria, que têm simetria bilateral). Para além disso, estes animais são diploblásticos, isto é, possuem dois folhetos embrionários. A camada exterior (ectoderme) corresponde a superfície da blástula e a camada interior (endoderme) é formada por células que migram para o interior. Ela então se invagina para formar uma cavidade digestiva com uma única abertura, (o arquêntero). Esta forma é chamada gástrula (ou plânula quando ela é livre-natante).  Os Cnidaria e os Ctenophora (águas vivas, anémonas, corais, etc) são os principais filos diploblásticos.  Os Myxozoa, um grupo de parasitas microscópicos, têm sido considerados cnidários reduzidos, porém, podem ser derivados dos Bilateria.[carece de fontes?]
As formas restantes compreendem um grupo chamado Bilateria, uma vez que eles apresentam simetria bilateral (ao menos um algum grau), e são triploblásticos. A Blástula invagina sem se preencher previamente, então o endoderma é apenas seu forro interior, a parte interna é preenchida para formar o terceiro folheto embrionário entre eles (mesoderme). Os animais mais simples dentre estes são os Platyhelminthes (vermes achatados, como a ténia), que podem ser parafiléticos ao filo mais alto.[carece de fontes?]
A vasta maioria dos filos triploblásticos formam um grupo chamado Protostomia. Todos os animais destes filos possuem um trato digestivo completo (incluindo uma boca e um ânus), com a boca se desenvolvendo do arquêntero e o ânus surgindo depois. A mesoderme surge como nos Platyhelminthes (vermes achatados, como a planária), de uma célula simples, e então divide-se para formar uma massa em cada lado do corpo. Geralmente há uma cavidade ao redor do intestino, chamada celoma, surgindo como uma divisão do mesoderme, ou ao menos uma versão reduzida disso (por exemplo, um pseudoceloma, onde a divisão ocorre entre o mesoderma e o endoderma, comum em formas microscópicas).[carece de fontes?]
Alguns dos principais filos protostômios são unidos pela presença de larva trocófora, que é distinguida por um padrão especial de cílios. Estes criam um grupo chamado Trochozoa, compreendendo os seguintes:
Tradicionalmente o Arthropoda - o maior filo animal incluindo insetos, aranhas, caranguejos e semelhantes - e dois pequenos filos proximamente relacionados a eles, o Onychophora e Tardigrada, têm sido considerados relativamente próximos aos anelídeos por causa de seu plano de segmentação corporal (a hipótese dos Articulata). Esta relação está em dúvida, e parece que eles, ao invés disso, pertençam a várias minhocas pseudocelomadas - os Nematoda, Nematomorpha (minhocas cabelo-de-cavalo), Kinorhyncha, Loricifera, e Priapulida - que compartilham entre si ecdise (muda do exosqueleto) e muitas outras características. Este grupo é conhecido como Ecdysozoa.[10][11]
Existem vários pseudocelomados protostomados que são difíceis de serem classificados devido ao seus pequenos tamanhos e estruturas reduzidas. Os Rotifera e Acanthocephala são extremamente relacionados entre si e provavelmente pertencem proximamente aos Trochozoa. Outros grupos incluem os Gastrotricha, Gnathostomulida, Entoprocta, e Cycliophora. O último foi descoberto apenas recentemente, e como pouca investigação foi feita nos fundos marinhos, provavelmente mais coisas serão ainda descobertas. A maioria destes foi agrupada dentro do filo Aschelminthes, junto com os Nematoda e outros, porém eles não aparentam possuir relações filogenéticas entre si.[carece de fontes?]
Os Brachiopoda (braquiópodes), Ectoprocta (ou Bryozoa, os briozoários) e os Phoronidas formam um grupo chamado Lophophorata, graças à presença compartilhada de um leque de cílios ao redor da boca chamado lofóforo. As relações evolucionárias destas formas não são muito claras - o grupo tem sido considerado como parte dos "deuterostomados", e talvez seja "parafilético". Eles são mais relacionados aos "Trochozoa", contudo, e os dois são frequentemente agrupados como Lophotrochozoa.[12]
Os Deuterostomados diferem dos Protostomados de várias formas. Eles também possuem um trato digestivo completo, mas neste caso o arquêntero desenvolve-se no ânus. A mesoderme e celoma não se desenvolvem da mesma forma, e sim da evaginação da endoderme, diz-se então, de origem enterocélica. E, finalmente, a clivagem dos embriões é diferente. Tudo isto sugere que as duas linhas são separadas e monofiléticas. Os deuterostomados incluem:[13]
Também há alguns filos animais extintos, não havendo muito conhecimento sobre sua embriologia ou estrutura interna, tornando-se assim difíceis de se classificar. Estes são, em sua maioria, vindos do período Cambriano, e incluem
No esquema original de Lineu, os animais eram de um dos três reinos, divididos nas classes de Vermes, Insetos, Peixes, Anfíbios, Répteis, Aves e Mamíferos. Os cinco[14] últimos foram subunidos em um único grupo, o Chordata, enquanto que as outras várias formas foram separadas. As listas citadas neste artigo representam a atual compreensão do grupo, embora haja variações de fonte para fonte.[carece de fontes?]
A filogenia e a compreensão da história evolutiva dos Metazoa vem sofrendo grandes modificações nas últimas décadas. Tradicionalmente, a partir dos anos 40, considerou-se que a história evolutiva deste grupo poderia ser entendida pelo aumento gradativo da complexidade.[15] Esta ideia, presente no trabalho de Libbie Hyman, estava embasada na construção de filogenias a partir das características da mesoderme (mesoderme ausente x mesoderme presente; organização da mesoderme). Enquanto a presença e ausência de mesoderme caracteriza os diploblásticos e os triblásticos, respectivamente, a organização da mesoderme nos triploblastos, relacionada à formação de cavidades internas, caracterizava os animais com celoma, pseudoceloma, schizoceloma e enteroceloma. O pressuposto era que tais características seriam conservadas ao longo da história evolutiva dos grupos e, assim, poderiam definir e sustentar os grupos dentro de metazoa.[16] Consequentemente, a hipótese filogenética proposta organizava os metazoa em dois grandes grupos. Um grupo basal formado pelas esponjas e pelos diploblásticos (Cnidários e Ctenophoros) e um grupo mais derivado dos triploblásticos: Acelomados (Platelmintos), Pseudocelomados (Asquelmintos), Schizocelomados (Protostômios) e Enterocelomados (Deuterostômios). Esta hipótese sugeria que um ancestral diploblástico deu origem aos triblástico, enquanto, dentro dos triblástico, um ancestral acelomado deu origem aos metazoas pseudocelomados e, finalmente, um ancestral pseudocelomado deu origem aos celomados schizocélicos e enterocélicos.[16] Tal hipótese deixa clara a concepção da época sobre a evolução dos metazoas: de organismos "menos complexos" para organismos "mais complexos".[16][15]
Estudos subsequentes buscando compreender a história evolutiva dos animais foram modificando esta ideia tradicional. Principalmente a partir da década de 60, com a introdução de métodos cladísticos e utilização de dados morfológicos e moleculares, novas hipóteses filogenéticas foram sendo propostas e foi identificado que as características da mesoderme são evolutivamente mais variáveis do que esperado pelo pressuposto tradicional. Atualmente, as hipóteses filogenéticas propostas organizam os metazoas em quatro grande grupos. Um grupo basal de metazoa incluindo os Porifera, Placozoa, Cnidária e Ctenófora, e três grupos de Bilateria: Deuterostômia, Lophotrocozoa e Ecdysozoa,[15] além de grupos com a posição ainda incerta na árvore dos Metazoa.
Os estudos genéticos recentes revelam que os grupos de animais apresentariam aproximadamente a seguinte filogenia:[17][18][19][20][21][22][23]
O comprimento dos galhos da árvore teve que ser medido usando seqüências de RNA e aminoácidos. Os filos Sipuncula,  Orthonectida e Acanthocephala não aparecem no cladograma porque foram classificados em Annelida[22] e Rotifera respectivamente.[24]
Uma célula animal é uma célula eucariótica ou seja, uma célula que apresenta o núcleo delimitado pela membrana (carioteca), podem ser também unicelulares, como as amebas. Há também, as pluricelulares, como plantas e animais. A célula animal (como toda célula eucariótica) é delimitada pela membrana plasmática, ribossomo, citoplasma, mitocôndria e núcleo.[carece de fontes?]
A palavra célula (que vem da palavra cella que significa caixa pequena) foi usada pela 1° vez em 1665, pelo inglês Robert Hooke (1635-1703). Com um microscópio muito simples ele observou pedaços de cortiça, e ele percebeu que ela era formada por compartimentos vazios que ele chamou de células.[carece de fontes?]
Matthias Schleiden e Theodor Schwann, após muitos anos de observações, propuseram a teoria celular.Essa teoria afirma que todo ser vivo é formado por células. Em 1855, o pesquisador alemão Rudholph Virchow deu um passo adiante, declarando que toda célula surge de outra célula preexistente. Na célula animal não há celulose em suas paredes nem clorofila no seu interior, diferente da célula vegetal.[carece de fontes?]
Os peixes são animais vertebrados, aquáticos, tipicamente ectotérmicos, que possuem o corpo fusiforme, os membros transformados em barbatanas ou nadadeiras (ausentes em alguns grupos) sustentadas por raios ósseos ou cartilaginosos, guelras ou brânquias com que respiram o oxigénio dissolvido na água (embora os dipnóicos usem pulmões) e, na sua maior parte, o corpo coberto de escamas.[1][2]
Os peixes são recursos importantes, principalmente como alimento, mas também são capturados por pescadores recreativos, mantidos como animais de estimação, criados por aquaristas, e expostos em aquários públicos. Os peixes tiveram um papel importante na cultura através dos tempos, servindo como divindades, símbolos religiosos (ver ichthys), e como temas de arte, livros e filmes.[3]
Uma vez que o "peixe" é definido negativamente, e exclui os tetrápodes (ou seja, os anfíbios, répteis, aves e mamíferos) que são descendentes da mesma origem, é um agrupamento parafilético, não considerado adequado na biologia sistemática. A classe Pisces, de Lineu é considerada tipológica, mas não filogenética.[3]
Os primeiros organismos que podem ser classificados como peixes eram cordados de corpo mole que apareceram pela primeira vez durante o período Cambriano.[4] Embora eles não tivessem uma verdadeira espinha dorsal, possuíam notocórdio, que lhes permitiu serem mais ágeis do que os invertebrados marinhos. Os peixes continuaram a evoluir durante o  Paleozoico, diversificando-se em uma grande variedade de formas.[3]
No uso comum, o termo peixe tem sido frequentemente utilizado para descrever um vertebrado aquático com brânquias, membros, se presentes, na forma de nadadeiras, e normalmente com escamas de origem dérmica no tegumento. Sendo este conceito do termo "peixe" utilizado por conveniência,[5] e não por unidade taxonômica, porque os peixes não compõem um grupo monofilético, já que eles não possuem um ancestral comum exclusivo. Para que se tornasse um grupo monofilético, os peixes deveriam juntar os Tetrápodes.[6]
Os peixes são tradicionalmente divididos nos seguintes grupos:
Em vista desta diversidade, os zoólogos não mais aceitam a antiga classe Pisces em que Lineu os agrupou, como se pode ver na classificação dos Vertebrados. Abaixo apresentam-se detalhes da classificação atualmente aceita.[4]
Por vezes, usa-se a palavra peixe para designar vários animais aquáticos (por exemplo na palavra peixe-mulher para designar o dugongo). Mas a maior parte dos organismos aquáticos muitas vezes designados por "peixe", incluindo as medusas (águas-vivas), os moluscos e crustáceos e mesmo mamíferos muito parecidos com os peixes como os golfinhos, não são peixes.[7]
Os peixes encontram-se em praticamente todos os ecossistemas aquáticos, tanto em água doce como em água salgada, desde a água da praia até às grandes profundezas dos oceanos (ver biologia marinha). Mas há alguns lagos hiper-salinos, como o Grande Lago Salgado, nos Estados Unidos da América do Norte onde não vivem peixes.[8]
Os peixes têm uma grande importância para a humanidade e desde tempos imemoriais foram pescados para a sua alimentação. Muitas espécies de peixes são criadas em condições artificiais (ver aquacultura), não só para alimentação humana, mas também para outros fins, como os aquários.[9]
Há algumas espécies perigosas para o homem, como os peixes-escorpião que têm espinhos venenosos e algumas espécies de tubarão, que podem atacar pessoas nas praias. Muitas espécies de peixes encontram-se ameaçadas de extinção, quer por pesca excessiva, quer por deterioração dos seus habitats.[10]
O ramo da zoologia que estuda os peixes do ponto de vista da sua posição sistemática é a ictiologia. No entanto, os peixes são igualmente estudados no âmbito da ecologia, da biologia pesqueira, da fisiologia e doutros ramos da biologia.[10]
Uma forma de classificar os peixes é segundo o seu comportamento relativamente à região das águas onde vivem; este comportamento determina o papel de cada grupo no ambiente aquático:[10]
Os peixes pelágicos de pequenas dimensões como as sardinhas são geralmente planctonófagos, ou seja, alimentam-se quase passivamente do plâncton disperso na água,[13] que filtram à medida que "respiram", com a ajuda de branquispinhas, que são excrescências ósseas dos arcos branquiais (a estrutura que segura as brânquias ou guelras).[3]
Algumas espécies de maiores dimensões têm também este hábito alimentar, incluindo algumas baleias (que não são peixes, mas mamíferos) e alguns tubarões como os zorros  (género Alopias).[13] Mas a maioria dos grandes peixes pelágicos são predadores ativos, ou seja, procuram e capturam as suas presas, que são também organismos pelágicos, não só peixes, mas também cefalópodes (principalmente lulas), crustáceos ou outros.[14]
Os peixes demersais podem ser predadores, mas também podem ser herbívoros, que se alimentam de plantas aquáticas, detritívoros, ou seja, que se alimentam dos restos de animais e plantas que se encontram no substrato, ou serem comensais de outros organismos, como a rémora que se fixa a um atum ou tubarão através dum disco adesivo no topo da cabeça e se alimenta dos restos de comida que caem da boca do seu hospedeiro (normalmente um grande predador), ou mesmo parasitas de outros organismos.[13]
Alguns peixes abissais e também alguns neríticos, como os diabos (família Lophiidae) apresentam excrescências, geralmente na cabeça,[15] que servem para atrair as suas presas; essas espécies costumam ter uma boca de grandes dimensões, que lhes permitem comer animais maiores que eles próprios. Numa destas espécies, o macho é parasita da fêmea, fixando-se pela boca a um tentáculo da sua cabeça.[16]
A maioria dos peixes é dióica, ovípara, fertiliza os óvulos externamente e não desenvolve cuidados parentais. Nas espécies que vivem em cardumes, as fêmeas desovam nas próprias águas onde os cardumes vivem e,[17] ao mesmo tempo, os machos libertam o esperma na água, promovendo a fertilização. Em alguns peixes pelágicos, os ovos flutuam livremente na água – e podem ser comidos por outros organismos, quer planctónicos, quer nectónicos; por essa razão, nessas espécies é normal cada fêmea libertar um enorme número de óvulos.[17] Noutras espécies, os ovos afundam e o seu desenvolvimento realiza-se junto ao fundo – nestes casos, os óvulos podem não ser tão numerosos, uma vez que são menos vulneráveis aos predadores.[3]
No entanto, existem excepções a todas estas características e neste artigo referem-se apenas algumas. Abaixo, na secção Migrações encontram-se os casos de espécies que se reproduzem na água doce, mas crescem na água salgada e vice-versa.[17]
Em termos de separação dos sexos, existem também (ex.: família Sparidae, os pargos) casos de hermafroditismo e casos de mudança de sexo - peixes que são fêmeas durante as primeiras fases de maturação sexual e depois se transformam em machos (protoginia) e o inverso (protandria).[18]
Os cuidados parentais, quando existem, apresentam casos curiosos. Nos cavalos-marinhos (género Hypocampus), por exemplo, o macho recolhe os ovos fecundados e incuba-os numa bolsa marsupial. Muitos ciclídeos (de que faz parte a tilápia) e algumas espécies de aquário endémicas do Lago Niassa (também conhecido por Lago Malawi, na fronteira entre Moçambique e o Malawi) guardam os filhotes na boca, quer do macho, quer da fêmea, ou alternadamente, para os protegerem dos predadores.[17]
Refere-se acima que a maioria dos peixes é ovípara, mas existem também espécies vivíparas e ovovivíparas, ou seja, em que o embrião se desenvolve dentro do útero materno. Nestes casos, pode haver fertilização interna - embora os machos não tenham um verdadeiro pênis, mas possuem uma estrutura para introduzir o esperma dentro da fêmea. Muitos destes casos encontram-se nos peixes cartilagíneos (tubarões e raias), mas também em muitos peixes de água doce e mesmo de aquário.
A idade e  o tamanho a partir dos quais os peixes começam a se reproduzir variam segundo as espécies e dentro de cada espécie, conforme as condições em que vivem. Nos locais frios (Europa), a Carpa-comum não começa a reproduzir-se senão aos três anos. Em locais quentes obtém-se a reprodução com apenas um ano. Certas espécies só põem ovos uma vez por ano, e caso a temperatura esteja muito baixa, algumas espécies não desovam, absorvem os ovos como alimento.[19]
Os peixes não dormem. Eles apenas alternam estados de vigília e repouso. O período de repouso consiste num aparente estado de imobilidade, em que os peixes mantêm o equilíbrio por meio de movimentos bem lentos.
Como não têm pálpebras, seus olhos ficam sempre abertos. Algumas espécies se deitam no fundo do mar ou no rio, enquanto os menores se escondem em buracos para não serem comidos enquanto descansam.[3]
Muitas espécies de peixes (principalmente os pelágicos) realizam migrações regularmente, desde migrações diárias (normalmente verticais, entre a superfície e águas mais profundas),[20] até anuais, percorrendo distâncias que podem variar de apenas alguns metros até várias centenas de quilómetros e mesmo plurianuais, como as migrações das enguias.[21]
Na maior parte das vezes, estas migrações estão relacionadas ou com a reprodução ou com a alimentação (procura de locais com mais alimento).[22] Algumas espécies de atuns migram anualmente entre o norte e o sul do oceano, seguindo massas de água com a temperatura ideal para eles.[23]
Os peixes migratórios classificam-se da seguinte forma:[24][25]
Os peixes anádromos mais estudados são os salmões (ordem Salmoniformes), que desovam nas partes altas dos rios, se desenvolvem no curso do rio e, a certa altura migram para o oceano onde se desenvolvem e depois voltam ao mesmo rio onde nasceram para se reproduzirem.[26] Muitas espécies de salmões têm um grande valor económico e cultural, de forma que muitos rios onde estes peixes se desenvolvem têm barragens com passagens para peixes (chamadas em inglês "fish ladders" ou "escadas para peixes"), que lhes permitem passar para montante da barragem.[27]
O exemplo mais bem estudado de catadromia é o caso da enguia europeia que migra cerca de 6 000 km até ao Mar dos Sargaços (na parte central e ocidental do Oceano Atlântico) para desovar, sofrendo grandes metamorfoses durante a viagem; as larvas, por seu lado, migram no sentido inverso, para se desenvolverem nos rios da Europa.[28]
Alguns peixes se camuflam para fugirem de certos predadores, outros para melhor apanharem as suas presas.[29] Algumas espécies de arraia, por outro lado, se escondem na areia e podem mudar o tom da pele, para suas presas não notarem sua presença no ambiente.[30]
A bexiga natatória é um órgão que auxilia o peixe a manter-se a determinada profundidade através do controle da sua densidade relativamente à da água. É um saco de paredes flexíveis, derivado do intestino que pode expandir-se ou contrair de acordo com a pressão; tem muito poucos vasos sanguíneos, mas as paredes estão forradas com cristais de guanina, que a fazem impermeáveis aos gases.[31]
A bexiga natatória possui uma glândula que permite a introdução de gases, principalmente oxigénio, na bexiga, para aumentar o seu volume.[32] Noutra região da bexiga, esta encontra-se em contacto com o sangue através doutra estrutura conhecida por "janela oval", através da qual o oxigénio pode voltar para a corrente sanguínea, baixando assim a pressão dentro da bexiga natatória e diminuindo o seu tamanho.[31]Nem todos os peixes possuem este órgão: os tubarões controlam a sua posição na água apenas com a locomoção e com o controle de densidade de seus corpos, através da quantidade de óleo em seu fígado; outros peixes têm reservas de tecido adiposo para essa finalidade.[31]
A presença de bexiga natatória traz uma desvantagem para o seu portador: ela proíbe a subida rápida do animal dentro da coluna de água, sob o risco daquele órgão rebentar.[32]
A denominação bexiga natatória foi substituída por vesícula gasosa.[32]
Para além de mostrar diferentes adaptações evolutivas dos peixes ao meio aquático,[33] as características externas destes animais (e algumas internas, tais como o número de vértebras) são muito importantes para a sua classificação sistemática.[34]
A forma do corpo dos peixes "típicos" – basicamente fusiforme – é uma das suas melhores adaptações à locomoção dentro de água.[35] A maioria dos peixes pelágicos (ver acima), principalmente os que formam cardumes activos, como os atuns, apresentam esta forma "típica".[36]
No entanto, há bastantes variações a esta forma típica, principalmente entre os demersais e nos peixes abissais (que vivem nas regiões mais profundas dos oceanos).[36] Nestes últimos, o corpo pode ser globoso e apresentar excrescências que servem para atrair as suas presas.[22]
A variação mais dramática do corpo dos peixes encontra-se nos Pleuronectiformes, ordem a que pertencem os linguados e as solhas.[37] Nestes animais, adaptados a viverem escondidos em fundos de areia, o corpo sofre metamorfoses durante o seu desenvolvimento larvar, de forma que os dois olhos ficam do mesmo lado do corpo – direito ou esquerdo, de acordo com a família.[36]
Muitos outros peixes demersais têm o corpo achatado dorsiventralmente para melhor se confundirem com o fundo. Alguns, como os góbios, que são peixes muito pequenos que vivem em estuários, têm inclusivamente as nadadeiras ventrais transformadas num botão adesivo, para evitarem ser arrastados pelas correntes de maré.[36]
Os Anguilliformes (enguias, congros e moreias) têm o corpo "anguiliforme", ou seja em forma de serpente, assim como algumas outras ordens de peixes.[38]
As barbatanas ou nadadeiras são os órgãos de locomoção dos peixes.[38] São extensões da derme (a camada profunda da pele) suportadas por lepidotríquias, que são escamas modificadas e funcionam como os raios das rodas de bicicleta.[39] Por essa razão, chamam-se raios os que são flexíveis, muitas vezes segmentados e |ramificados, ou espinhos, quando são rígidos e podem ser ocos e possuir um canal para a emissão de veneno.[40]
Os números de espinhos e raios nas nadadeiras dos peixes são importantes caracteres para a sua classificação, havendo mesmo chaves dicotómicas para a sua identificação em que este é um dos principais factores.[38]
Tipicamente, os peixes apresentam os seguintes tipos de nadadeiras:
Apenas as nadadeiras pares têm relação evolutiva com os membros dos restantes vertebrados.[38]
Algumas ou todas estas nadadeiras podem faltar ou estar unidas - já foi referida a transformação das nadadeiras peitorais dos góbios num disco adesivo – mas as uniões mais comuns são entre as nadadeiras ímpares, como a dorsal com a caudal e anal com caudal (caso de algumas espécies de linguados).[42]
As nadadeiras têm formas e cores típicas em alguns grupos de peixes.[38]
Para além da coloração do corpo, a forma e cor das nadadeiras são decisivas para os aquaristas, de tal forma que chegam a ser produzidas variedades de espécies com nadadeiras espectaculares, como o famoso cauda-de-véu, uma variedade do peixinho-dourado (Carassius auratus).[43][44]
Alguns grupos de peixes, para além da nadadeira dorsal com espinhos e raios (que podem estar separadas), possuem uma nadadeira adiposa, normalmente perto da caudal. É o caso dos salmões e dos peixes da família do bacalhau (Gadídeos).
A pele dos peixes é fundamentalmente semelhante à dos outros vertebrados, mas possui algumas características específicas dos animais aquáticos. O corpo dos peixes está normalmente coberto de muco que, por um lado diminui a resistência da água ao movimento e, por outro, os protege dos inimigos.[45] Embora haja muitos grupos de peixes com pele nua, como as enguias, a maior parte dos peixes tem-na coberta de escamas que, ao contrário dos répteis, têm origem na própria derme.[46]
Os peixes apresentam quatro tipos básicos de escamas:
Alguns grupos de peixes têm o corpo coberto de placas ou mesmo uma armadura rígida, como o peixe-cofre e os cavalos-marinhos. Esta armadura pode estar ornamentada com cristas e espinhos e apresenta fendas por onde saem as nadadeiras.
Um órgão sensorial específico dos peixes é a linha lateral, normalmente formada por uma fiada longitudinal de escamas perfuradas através das quais corre um canal que tem ligação com o sistema nervoso; aparentemente, este órgão tem funções relacionadas com a orientação, uma espécie de sentido do olfacto através do qual os peixes reconhecem as características das massas de água (temperatura, salinidade e outras).[46]
Peixes têm sistemas nervosos complexos e seu cérebro é dividido em diferentes partes. O mais anterior, ou frontal, contém as glândulas olfativas. Diferente da maioria dos vertebrados, o cérebro do peixe primeiro processa o senso do olfato antes de todas as ações voluntárias.[49]
Os lobos óticos processam informações dos olhos. O cerebelo coordena os movimentos do corpo enquanto a medula controla as funções dos órgãos internos.[50]
Aproximadamente todos os peixes diurnos possuem olhos bem desenvolvidos com visão colorida. Muitos peixes possuem também células especializadas conhecidas como quimioreceptores, que são responsáveis pelos sentidos de gosto e cheiro.[51]
A maioria dos peixes têm receptores sensitivos que formam o sistema linear lateral, que permite aos peixes detectar correntes e vibrações, bem como o movimento de outros peixes e presas por perto (ver acima).
Em 2003, alguns cientistas escoceses da Universidade de Edimburgo descobriram que os peixes podem sentir dor.[46] Um estudo prévio pelo professor James D. Rose da Universidade de Wyoming dizia que os peixes não podiam sentir dor porque eles não possuíam a parte neocortexal do cérebro, responsável por tal sensação. Peixes como os peixes-gato e tubarões possuem órgãos que detectam pequenas correntes elétricas. Outros peixes, como a enguia elétrica, podem produzir sua própria eletricidade.[52]
Um estudo sugere que os peixes sentem dor, com uma semelhança como a experimentada por mamíferos, incluindo seres humanos.[53]
É possível saber a idade de um peixe através do exame dos seus "ouvidos". Todos os peixes, com exceção dos tubarões e das arraias, escondem nos ouvidos três pares de otólitos, que são concreções de carbonato de cálcio que servem para a audição e para manter o equilíbrio no meio líquido. O crescimento desses otólitos se faz por depósitos concêntricos sucessivos, cuja espessura e a composição química variam segundo o meio ambiente e a alimentação. Trata-se de um verdadeiro "álbum" que retrata a vida do peixe.
A Lista Vermelha da IUCN em 2006 nomeou 1 173 espécies de peixes que estão ameaçadas de extinção. Incluem-se espécies como o Bacalhau-do-atlântico, Diabo Hole, Celacantos, e grandes Tubarões-brancos.[54] Porque os peixes vivem debaixo d'água são mais difíceis de estudar do que os animais terrestres e plantas, e informações sobre as populações de peixes é muitas vezes inexistente. No entanto, peixes de água doce parecem particularmente ameaçados, porque eles vivem muitas vezes em corpos d'água relativamente pequenos. Por exemplo, o Buraco do Demônio tem apenas 6 metros (10 por 20 pés) para a piscina.[35][55]
A chave do estresse sobre os ecossistemas de água doce é a degradação do habitat, incluindo a poluição da água, a construção de barragens, a remoção de água para uso por seres humanos,[56] e a introdução de espécies exóticas. Um exemplo de um peixe que se tornou em perigo por causa da mudança de habitat é o esturjão pálido, um  peixe norte-americano de água doce que vive nos rios deteriorados pela ação humana.[57]
A pesca excessiva é uma grande ameaça para peixes comestíveis, como o bacalhau e o atum. A pesca excessiva, eventualmente, provoca colapso da população (conhecido como estoque), pois os sobreviventes não conseguem produzir o suficiente para substituir aqueles  removidos. Extinção comercial Tal não significa que a espécie está extinta, mas apenas que ele não pode mais sustentar uma pescaria.[40]
Um exemplo bem estudado de um colapso da pesca é a sardinha do Pacífico Sadinops pescada ao largo da costa da Califórnia. De um pico de 790 000 toneladas em 1937 a captura declinou para apenas 24 000 toneladas em 1968, após o qual a pesca já não era economicamente viável.[42]
A principal tensão entre a ciência da pesca e da indústria de pesca é que os dois grupos têm opiniões diferentes sobre a resiliência da pesca para a pesca intensiva. Em lugares como a Escócia, Terra Nova, e Alasca a indústria da pesca é um grande empregador, assim, os governos estão predispostos a apoiá-lo.[58] Por outro lado, cientistas e conservacionistas querem uma proteção rigorosa, alertando que a pesca nos padrões atuais poderiam dizimar as espécies pescadas dentro de cinqüenta anos.[59]
Introdução de espécies não-nativas ocorreu em muitos habitats. Um dos melhores exemplos estudados é a introdução da perca-do-nilo no Lago Vitória, na década de 1960.[60] A perca-do-nilo gradualmente exterminou do lago cerca de 500 espécies endémicas de ciclídeos. Alguns deles sobrevivem agora em programas de reprodução em cativeiro, mas outros estão provavelmente extintos. Carp, snakeheads, tilápia, poleiro-europeu, truta, truta-arco-íris, e lampreias-marinhas são outros exemplos de peixes que têm causado problemas ao serem introduzidos em ambientes considerados alienígenas.[61]
A classificação simplificada no topo desta página é a mais próxima da utilizada por Lineu, mas esconde algumas características importantes que fazem deste grupo dos "Peixes", um agregado de espécies com diferentes aspectos evolutivos. Por essa razão, as classificações mais recentes abandonaram alguns taxa tradicionais:
A partir deste ponto, os estudos evolutivos mostraram divergências:
O taxon classe tem sido usado (e, na Wikipédia em inglês, encontramos vários exemplos) para vários clades diferentes. Por essa razão, e até os taxonomistas acordarem numa forma de classificação científica consensual, devemos abster-nos de utilizar esse taxon. Os peixes, tanto espécies existentes como fósseis, dividem-se pelos seguintes clades:
Dentro dos vertebrados, consideram-se os clades
e mais sete grupos fósseis.
Dentro dos Gnathostomata, são aceites os seguintes clades:
Dentro dos Teleostomi
Dentro dos Osteichthyes
Dentro desta classificação, os tradicionais taxa Agnatha (peixes sem maxilas), Ostracodermi (formas fósseis sem maxilas) e Cyclostomata (peixes sem maxilas, como as mixinas e lampréias) não devem ser utilizados, uma vez que não são monofiléticos.[62]
Humano (taxonomicamente Homo sapiens,[1][2] termo que deriva do latim "homem sábio",[3] também conhecido como pessoa, gente ou homem) é a única espécie do gênero Homo ainda viva[4][5] e o primata mais abundante e difundido da Terra, caracterizado pelo bipedalismo e por cérebros grandes, o que permitiu o desenvolvimento de ferramentas, culturas e linguagens avançadas. Os humanos tendem a viver em estruturas sociais complexas compostas por muitos grupos cooperantes e concorrentes, desde famílias e redes de parentesco até Estados políticos. As interações sociais entre os humanos estabeleceram uma ampla variedade de valores, normas e rituais, que fortalecem a sociedade humana. A curiosidade e o desejo humano de compreender e influenciar o meio ambiente e de explicar e manipular fenômenos motivaram o desenvolvimento da ciência, filosofia, mitologia, religião e outros campos de estudo da humanidade.
O H. sapiens surgiu há cerca de 300 mil anos na África, quando evoluiu do Homo heidelbergensis e migrou para fora do continente africano, substituindo gradualmente as populações locais de humanos arcaicos. Durante a maior parte da história, todos os humanos foram caçadores-coletores nômades. A Revolução Neolítica, que começou no sudoeste da Ásia há cerca de 13 mil anos, trouxe o surgimento da agricultura e da ocupação humana permanente. À medida que as populações se tornaram maiores e mais densas, formas de governança se desenvolveram dentro e entre as comunidades e várias civilizações surgiram e declinaram. Os humanos continuaram a se expandir, com uma população global de mais de 8 bilhões em novembro de 2022.[6]
Os genes e o ambiente influenciam a variação biológica humana em características visíveis, fisiologia, suscetibilidade a doenças, habilidades mentais, tamanho do corpo e longevidade. Embora variem em muitas características, dois humanos são, em média, mais de 99% semelhantes. Geralmente, os homens têm maior força corporal, enquanto as mulheres apresentam maior percentual de gordura corporal, entram na menopausa e tornam-se inférteis por volta dos 50 anos e, em média, também têm uma expectativa de vida mais longa em quase todas as populações do mundo. A natureza dos papéis de gênero masculino e feminino tem variado historicamente e os desafios às normas de gênero predominantes têm se repetido em muitas sociedades. Os humanos são onívoros, capazes de consumir uma grande variedade de materiais vegetais e animais e usam o fogo e outras formas de calor para preparar e cozinhar alimentos desde a época do H. erectus. Eles podem sobreviver por até oito semanas sem comida e três ou quatro dias sem água. Geralmente são diurnos, dormindo em média sete a nove horas por dia. O parto é perigoso, com alto risco de complicações e morte. Frequentemente, a mãe e o pai cuidam dos filhos, que são indefesos ao nascer.
Os humanos têm um córtex pré-frontal grande e altamente desenvolvido, a região do cérebro associada à cognição superior. Eles são inteligentes, capazes de memória episódica, expressões faciais flexíveis, autoconsciência, mentalização, introspecção, pensamento privado, imaginação, volição e formação de pontos de vista sobre sua própria existência. Isso tem permitido grandes avanços tecnológicos e o desenvolvimento de ferramentas complexas, possíveis por meio da razão e da transmissão de conhecimento às gerações futuras. Linguagem, arte e comércio são características definidoras dos humanos. As rotas comerciais de longa distância podem ter levado a explosões culturais e distribuição de recursos que deram aos humanos uma vantagem sobre outras espécies semelhantes. A África Oriental, nomeadamente o Chifre da África, é considerada pelos antropólogos como o local de nascimento dos humanos de acordo com as evidências arqueológicas e fósseis existentes.[7][8][9][10]
Em latim, humanus é a forma adjetival do nome homo, traduzido como Homem (para incluir machos e fêmeas).[11] Por vezes, em Filosofia, é mantida uma distinção entre as noções de ser humano (ou Homem) e de pessoa. O primeiro refere-se à espécie biológica enquanto o segundo refere-se a um agente racional, visto, por exemplo, na obra de John Locke, Ensaio sobre o Entendimento Humano II 27, e na obra de Immanuel Kant, Introdução à Metafísica da Moral. Segundo a perspectiva de John Locke, a noção de pessoa passa a ser a de uma coleção de acções e operações mentais. O termo pessoa poderá assim ser utilizado para referir animais para além do Homem, para referir seres míticos, uma inteligência artificial ou um ser extraterrestre.[12]
O termo binomial Homo sapiens foi cunhado por Carl Linnaeus em seu trabalho do século XVIII Systema Naturae e também é o lectótipo do espécime.[13] O termo para o gênero Homo é uma derivação do século XVIII do latim homō ("homem"), em última instância "ser terrestre" (do latim antigo hemō).[14]
O estudo científico da evolução humana engloba o desenvolvimento do gênero Homo, mas geralmente envolve o estudo de outros hominídeos e homininaes, tais como o Australopithecus. O "humano moderno" é definido como membro da espécie Homo sapiens, sendo a única subespécie sobrevivente (Homo sapiens sapiens). O Homo sapiens idaltu e o Homo neanderthalensis, além de outras subespécies conhecidas, foram extintos há milhares de anos.[15] O Homo sapiens viveu com cerca de oito espécies de humanos hoje extintas há cerca de 300 000 anos.[16] Há apenas 15 000 anos, o homo sapiens compartilhava cavernas com outra espécie humana conhecida como Denisovans.[17] O Homo neanderthalensis, que se tornou extinto há 30 mil anos, tem sido ocasionalmente classificado como uma subespécie classificada como "Homo sapiens neanderthalensis", mas estudos genéticos sugerem uma divergência entre as espécies Neanderthal e Homo sapiens que ocorreu há cerca de 500 mil anos.[18] Da mesma forma, os poucos espécimes de Homo rhodesiensis são também classificados como uma subespécie de Homo sapiens, embora isso não seja amplamente aceito. Os humanos anatomicamente modernos têm seu primeiro registro fóssil na África, há cerca de 195 mil anos, e os estudos de biologia molecular dão provas de que o tempo aproximado da divergência ancestral comum de todas as populações humanas modernas terá sido há 200 mil anos.[19][20][21][22][23] O amplo estudo sobre a diversidade genética Africana chefiado pela Dra. Sarah Tishkoff encontrou no povo San a maior expressão de diversidade genética entre as 113 populações distintas da amostra, tornando-os um de 14 "grupos ancestrais da população". A pesquisa também localizou a origem das migrações humanas modernas no sudoeste da África, perto da orla costeira da Namíbia e de Angola.[24] A espécie humana teria colonizado a Eurásia e a Oceania há 40 mil anos; e as Américas apenas há cerca de 10 mil anos.[25] A recente (2003) descoberta de outra subespécie diferente da atual Homo sapiens sapiens, o Homo sapiens idaltu, na África, reforça esta teoria, por representar um dos elos perdidos no conhecimento da evolução humana.[26]
Os parentes vivos mais próximos dos seres humanos são os gorilas e os chimpanzés, mas os humanos não evoluíram a partir desses macacos: em vez disso, os seres humanos modernos compartilham com esses macacos um ancestral comum.[27]A descoberta do Ardipithecus, juntamente com fósseis mais antigos de macacos do Mioceno, reformulou a compreensão acadêmica do último ancestral comum entre chimpanzés e humanos de se parecer muito com os atuais chimpanzés, orangotangos e gorilas modernos para ser uma criatura única sem um cognato anatômico moderno.
Os seres humanos são provavelmente os animais mais estreitamente relacionados com duas espécies de chimpanzés: o Chimpanzé-comum e o Bonobo.[27] O sequenciamento completo do genoma levou à conclusão de que "depois de 6,5 milhões de anos de evoluções distintas, as diferenças entre chimpanzés e humanos são dez vezes maiores do que entre duas pessoas independentes e dez vezes menores do que aquelas entre ratos e camundongos" . A concordância entre as sequencias do DNA humano e o do chimpanzé variam entre 95% e 99%.[28][29][30][31] Estima-se que a linhagem humana divergiu da dos chimpanzés há cerca de cinco milhões de anos e da dos gorilas há cerca de oito milhões de anos. No entanto, um crânio de hominídeo descoberto no Chade, em 2001, classificado como Sahelanthropus tchadensis, possui cerca de sete milhões de anos, o que pode indicar uma divergência mais anterior.[32]
A evolução humana é caracterizada por uma série de importantes alterações morfológicas, de desenvolvimento, fisiológico e comportamental, que tiveram lugar desde que a separação entre o último ancestral comum de humanos e chimpanzés. A primeira grande alteração morfológica foi a evolução de uma forma de adaptação de locomoção arborícola ou semiarborícola para uma forma de locomoção bípede, com todas as suas adaptações decorrentes, tais como um joelho valgo, um índice intermembral baixo (pernas longas em relação aos braços), e redução da força superior do corpo.[33]
Mais tarde, os humanos ancestrais desenvolveram um cérebro muito maior - normalmente de 1 400 cm³ em seres humanos modernos, mais de duas vezes o tamanho do cérebro de um chimpanzé ou gorila. O padrão de crescimento pós-natal do cérebro humano difere do de outros primatas (heterocronia) e permite longos períodos de aprendizagem social e aquisição da linguagem nos seres humanos juvenis. Antropólogos físicos argumentam que as diferenças entre a estrutura dos cérebros humanos e os dos outros macacos são ainda mais significativas do que as diferenças de tamanho.[34][35]
Outras mudanças morfológicas significantes foram: a evolução de um poder de aderência e precisão;[36] um sistema mastigatório reduzido; a redução do dente canino; e a descida da laringe e do osso hioide, tornando a fala possível. Uma importante mudança fisiológica em humanos foi a evolução do estro oculto, ou ovulação oculta, o que pode ter coincidido com a evolução de importantes mudanças comportamentais, tais como a ligação em casais. Outra mudança significativa de comportamento foi o desenvolvimento da cultura material, com objetos feitos pelos humanos cada vez mais comuns e diversificados ao longo do tempo. A relação entre todas estas mudanças é ainda tema de debate.[37][38]
A evolução humana é caracterizada por uma série de mudanças morfológicas, fisiológicas e comportamentais que ocorreram desde a divisão entre o último ancestral comum dos humanos e dos chimpanzés. As mais significativas dessas adaptações são o bipedalismo obrigatório, o aumento do tamanho do cérebro e a diminuição do dimorfismo sexual (neotenia, quando comparados a outras espécies de primatas). A relação entre todas essas mudanças é objeto de debate contínuo. [39]
As forças da seleção natural continuam a operar em populações humanas, com a evidência de que determinadas regiões do genoma exibiram seleção direcional nos últimos 15 mil anos.[40]
Ardipithecus ramidus
Australopithecus afarensis
Homo habilis, o primeiro a usar ferramentas de pedra.
Homo erectus, o primeiro a usar o fogo.
H. heidelbergensis
H. neanderthalensis
Conforme a hipótese paleoantropológica mais corroborada atualmente, da Origem Recente Africana, o ser humano moderno evoluiu na África durante o Paleolítico Médio, há cerca de 200 mil anos.[41] Embora alguns dos restos esqueléticos mais antigos sugiram uma origem da África Oriental, o sul da África é o lar de populações contemporâneas que representam o primeiro ramo da filogenia genética humana.[42] Até o início do Paleolítico Superior, há cerca de 50 mil AP, o comportamento moderno, que inclui a linguagem, a música e outras expressões culturais universais, já tinham se desenvolvido. Um estudo sobre a diversidade genética africana chefiado pela Dra. Sarah Tishkoff (Universidade da Pensilvânia) encontrou no povo San a maior expressão de diversidade genética entre 113 populações distintas, sugerindo que o "berço da humanidade" ficaria na região dos Khoisan (antes chamados de Hotentotes), na área de Kalahari mais próxima da costa da Fronteira Angola-Namíbia, indicando uma possível migração de ancestrais para o norte e para fora da África há cerca de 250 gerações.[24]
Estima-se que a migração para fora da África ocorreu há cerca de 70 mil anos AP. Os seres humanos modernos, posteriormente distribuídos por todos os continentes, substituíram os hominídeos anteriores. Eles habitaram a Eurásia e a Oceania há 40 mil anos AP e as Américas há pelo menos 14 mil anos AP.[43] Eles acabaram com o Homo neanderthalensis e com outras espécies descendentes do Homo erectus (que habitavam a Eurásia há 2 milhões de anos), através do seu maior sucesso na reprodução e na competição por recursos.[44]
Evidências acumuladas da arqueogenética, desde a década de 1990, deram forte apoio ao "Hipótese da origem única", e têm marginalizado a hipótese de competição multirregional, que propunha que os humanos modernos evoluíram, pelo menos em parte, de independentes de populações de hominídeos.[45]
Os geneticistas Lynn Jorde e Henry Harpending, da Universidade de Utah, propõem que a variação no DNA humano é minuta quando comparada com a de outras espécies. Eles também propõem que durante o Pleistoceno Superior, a população humana foi reduzida a um pequeno número de pares reprodutores - não maior de 10 000 e, possivelmente, não menor de 1 000 - resultando em um pool genético residual muito pequeno. Várias razões para esse gargalo hipotético têm sido postuladas, sendo uma delas a teoria da catástrofe de Toba.[46]
Em uma série de análises genéticas sem precedentes, publicadas no jornal Nature, em setembro de 2016, três times de pesquisadores concluíram que todos os não africanos descendem de uma única população que emergiu na África há entre 50 mil e 80 mil anos.[47]
Há 10 000 anos, a maioria dos seres humanos vivia como caçadores-coletores, em pequenos grupos nômades. O início das atividades agrícolas separa o período neolítico do imediatamente anterior, o período da idade da pedra lascada. Como são anteriores à história escrita, os primórdios da agricultura são obscuros, mas admite-se que ela tenha surgido independentemente em diferentes lugares do mundo, provavelmente nos vales e várzeas fluviais habitados por antigas civilizações. Há entre dez[48] e doze mil anos, durante a pré-história, no período do neolítico ou período da pedra polida, alguns indivíduos de povos caçadores-coletores notaram que alguns grãos que eram coletados da natureza para a sua alimentação poderiam ser enterrados, isto é, "semeados" a fim de produzir novas plantas iguais às que os originaram. Os primeiros sistemas de cultivo e de criação apareceram em algumas regiões pouco numerosas e relativamente pouco extensas do planeta. Essas primeiras formas de agricultura eram certamente praticadas perto de moradias e aluviões das vazantes dos rios, ou seja, terras já fertilizadas que não exigiam, portanto, desmatamento.[48]
Há cerca de 6 000 anos, os primeiros proto-estados desenvolveram-se na Mesopotâmia, no Saara/Nilo e no Vale do Indo. Forças militares foram formadas para a proteção das sociedades e burocracias governamentais foram criadas para facilitar a administração dos estados. Os Estados colaboraram e concorreram entre si em busca de recursos e, em alguns casos, travaram guerras. Entre há 2000 e 3000 anos, alguns estados, como a Pérsia, a Índia, a China, o Império Romano e a Grécia, desenvolveram-se e expandiram seus domínios através da conquista de outros povos. A Grécia antiga foi a civilização que construiu as fundações da cultura ocidental, sendo o local de nascimento da filosofia, democracia, os principais avanços científicos e matemáticos, os Jogos Olímpicos, literatura e historiografia ocidentais, além do drama, incluindo a comédia e a tragédia.[49]
No final da Idade Média ocorre o surgimento de ideias e tecnologias revolucionárias. Na China, uma avançada e urbanizada sociedade promoveu inovações tecnológicas, como a impressão. Durante a "Era de Ouro do Islamismo" ocorreram grandes avanços científicos nos impérios muçulmanos. Na Europa, a redescoberta das aprendizagens e invenções da Era clássica, como a imprensa, levou ao Renascimento no século XIV. Nos 500 anos seguintes, a exploração e o colonialismo deixaram as Américas, a Ásia e a África sob o domínio europeu, levando à posteriores lutas por independência. A Revolução Científica no século XVII e a Revolução Industrial nos séculos XVIII e XIX promoveram importantes inovações no setor dos transportes (transporte ferroviário e o automóvel), no desenvolvimento energético (carvão e a electricidade) e avanços nas formas de governo (democracia representativa e o comunismo).[50][51]
Com o advento da Era da Informação, no final do século XX, os humanos modernos passaram a viver em um mundo que se torna cada vez mais globalizado e interligado. Em 2008, cerca de 1,4 bilhões de seres humanos estavam ligados uns aos outros através da Internet,[52] e 3,3 bilhões pelo telefone celular.[53]
Embora a interligação entre os seres humanos tenha estimulado o crescimento das ciências, das artes e da tecnologia, ocorreram também confrontos culturais, o desenvolvimento e a utilização de armas de destruição em massa. A civilização humana tem levado à destruição ambiental e à poluição, contribuindo significativamente para um evento, ainda em curso, de extinção em massa de outras formas de vida chamado de extinção do Holoceno,[54] processo que pode ser acelerado pelo aquecimento global no futuro.[55]
A tecnologia permitiu ao ser humano colonizar todos os continentes e adaptar-se a praticamente todos os climas. Nas últimas décadas, os seres humanos têm explorado a Antártida, as profundezas dos oceanos e até mesmo o espaço sideral, embora a longo prazo a colonização desses ambientes ainda seja inviável. Com uma população de mais de sete bilhões de indivíduos, os seres humanos estão entre os mais numerosos grandes mamíferos do planeta. A maioria dos seres humanos (60,3%) vive na Ásia. O restante vive na África (15,2%), nas Américas (13,6%), na Europa (10,5%) e na Oceania (0,5%).[56]
A habitação humana em sistemas ecológicos fechados e em ambientes hostis, como a Antártida e o espaço exterior, é cara, normalmente limitada no que diz respeito ao tempo e restrita a avanços e expedições científicas, militares e industriais. A vida no espaço tem sido muito esporádica, com não mais do que treze pessoas vivendo no espaço por vez. Entre 1969 e 1972, duas pessoas de cada vez estiveram na Lua. Desde a conquista da Lua, nenhum outro corpo celeste foi visitado por seres humanos, embora tenha havido uma contínua presença humana no espaço desde o lançamento da primeira tripulação a habitar a Estação Espacial Internacional, em 31 de outubro de 2000.[57]
Desde 1800, a população humana aumentou de um bilhão a mais de sete bilhões de indivíduos.[58][59] Em 2004, cerca de 2,5 bilhões do total de 6,3 bilhões de pessoas (39,7%) residiam em áreas urbanas, e estima-se que esse percentual continue a aumentar durante o século XXI. Em fevereiro de 2008, a Organização das Nações Unidas estimou que metade da população mundial viveria em zonas urbanas até ao final daquele ano.[60] Existem muitos problemas para os seres humanos que vivem em cidades como a poluição e a criminalidade, especialmente nos centros e favelas de cada cidade. Entre os benefícios da vida urbana incluem o aumento da alfabetização, acesso à global ao conhecimento humano e diminuição da suscetibilidade para o desenvolvimento da fome.[61]
Os seres humanos tiveram um efeito dramático sobre o ambiente. A atividade humana tem contribuído para a extinção de inúmeras espécies de seres vivos. Como atualmente os seres humanos raramente são predados, eles têm sido descritos como superpredadores.[62] Atualmente, através da urbanização e da poluição, os humanos são os principais responsáveis pelas alterações climáticas globais.[63] A espécie humana é tida como a principal causadora da extinção em massa do Holoceno, uma extinção em massa, que, se continuar ao ritmo atual, poderá acabar com metade de todas as espécies ao longo do próximo século.[64][65]
Os tipos de corpo humano variam substancialmente. Embora o tamanho do corpo seja largamente determinado pelos genes, é também significativamente influenciado por fatores ambientais, como dieta e exercício. A altura média de um ser humano adulto é de cerca de 1,5 a 1,8 metro de altura, embora varie consideravelmente de lugar para lugar.[68]  A massa média de um homem adulto varia entre 76–83 kg e 54–64 kg para mulheres adultas.[69]
Embora os seres humanos aparentem ter menos pelos em comparação com outros primatas, com o crescimento notável de pelos ocorrendo principalmente no topo da cabeça, axilas e região pubiana, o homem médio tem mais folículos pilosos em seu corpo do que um chimpanzé médio. A principal diferença é que os pelos humanos são mais curtos, mais finos e com menos pigmentação do que os pelos do chimpanzé médio, tornando-os mais difícil de serem vistos.[70] Os seres humanos também estão entre os melhores corredores de longa distância no reino animal, mas são mais lentos em distâncias curtas.[71][72]
A tonalidade da pele humana e do cabelo é determinada pela presença de pigmentos chamados melaninas. As tonalidades de pele humana pode variar do marrom (castanho) muito escuro até ao rosa muito pálido. A cor do cabelo humano varia do branco, ao marrom, ao vermelho, ao amarelo e ao preto, a tonalidade mais comum.[73] Isso depende da quantidade de melanina (um pigmento eficaz no bloqueio do sol) na pele e no cabelo, com as concentrações de melanina diminuindo no cabelo com o aumento da idade, levando ao cinza ou, até mesmo, aos cabelos brancos. A maioria dos pesquisadores acredita que o escurecimento da pele foi uma adaptação evolutiva como uma forma de proteção contra a radiação solar ultravioleta. No entanto, mais recentemente, tem sido alegado que as cores de pele, são uma adaptação do equilíbrio de ácido fólico, que é destruído pela radiação ultravioleta, e de vitamina D, que requer luz solar para se formar.[74] A pigmentação da pele do humano contemporâneo está geograficamente estratificada e em geral, se correlaciona com o nível de radiação ultravioleta. A pele humana também tem a capacidade de escurecer (bronzeamento) em resposta à exposição à radiação ultravioleta.[75][76] Os seres humanos tendem a ser fisicamente mais fracos do que outros primatas de tamanhos semelhantes, com os jovens, condicionado os seres humanos do sexo masculino a terem se mostrado incapazes de combinar com a força de orangotangos fêmeas, que são pelo menos três vezes mais fortes.[77]
Os seres humanos têm palatos proporcionalmente mais curtos e dentes muito menores do que os de outros primatas e são os únicos primatas com os dentes caninos mais curtos. Têm caracteristicamente dentes cheios, com falhas de dentes perdidos geralmente fechando-se rapidamente em espécimes jovens e gradualmente perdem seus dentes do siso, tendo algumas pessoas, congenitamente, sua ausência natural.[78]
A fisiologia humana é a ciência das funções mecânicas, físicas e bioquímicas dos seres humanos em boa saúde, e do que seus órgãos e células são compostos. O principal foco da fisiologia está no nível dos órgãos e sistemas. A maioria dos aspectos da fisiologia humana estão intimamente homólogos correspondentes aos aspectos da fisiologia dos outros animais e a experimentação com animais de outras espécies tem proporcionado grande parte da base do conhecimento fisiológico. A anatomia e a fisiologia estão estreitamente relacionadas com as áreas de estudo: anatomia, o estudo da forma, e fisiologia, o estudo da função, estão intrinsecamente vinculadas e são estudadas em conjunto como parte de um currículo médico.[79]
Como a maioria dos animais, os humanos também são uma espécie eucariótica diplóide. Cada célula somática tem dois conjuntos de 23 cromossomos, cada conjunto recebido de um dos pais; os gametas têm apenas um conjunto de cromossomos, que é uma mistura dos dois conjuntos parentais. Entre os 23 pares de cromossomos, existem 22 pares de autossomos e um par de cromossomos sexuais. Como outros mamíferos, os humanos têm um sistema de determinação sexual XY, de modo que as fêmeas têm os cromossomos sexuais XX e os machos XY.[80] Os genes e o ambiente influenciam a variação biológica humana em características visíveis, fisiologia, suscetibilidade a doenças e habilidades mentais. A influência exata dos genes e do ambiente em certas características não é bem compreendida.[81][82]
Embora nenhum ser humano - nem mesmo gêmeos monozigóticos - seja geneticamente idêntico,[83] dois humanos em média terão uma similaridade genética de 99,5%-99,9%.[84][85] Isso os torna mais homogêneos do que outros grandes macacos, incluindo os chimpanzés.[86][87] Esta pequena variação no DNA humano em comparação com outras espécies sugere um efeito de gargalo populacional durante o Pleistoceno Superior (há cerca de 100 mil anos), no qual a população humana foi reduzida a um pequeno número de pares reprodutores.[88][89] As forças da seleção natural continuaram a operar nas populações humanas, com evidências de que certas regiões do genoma exibem seleção direcional nos últimos 15 mil anos.[90]
O genoma humano foi sequenciado pela primeira vez em 2001[91] e em 2020 centenas de milhares de genomas foram sequenciados.[92] Em 2012, o International HapMap Project comparou os genomas de 1.184 indivíduos de 11 populações e identificou 1,6 milhão de polimorfismos de nucleotídeo único.[93] As populações africanas também abrigam o maior número de variantes genéticas privadas, ou aquelas não encontradas em outros lugares do mundo. Embora muitas das variantes comuns encontradas em populações fora da África também sejam encontradas no continente africano, ainda há um grande número que é privado dessas regiões, especialmente Oceania e Américas. Pelas estimativas de 2010, os humanos têm aproximadamente 22 mil genes.[94] Ao comparar o DNA mitocondrial, que é herdado apenas da mãe, os geneticistas concluíram que o último ancestral comum feminino cujo marcador genético é encontrado em todos os humanos modernos, a chamada Eva mitocondrial, deve ter vivido por volta de há 90 mil a 200 mil anos.[95][96][97]
O ciclo de vida humano é semelhante ao de outros mamíferos placentários. O zigoto divide-se dentro do útero da mulher para se tornar um embrião, que, ao longo de um período de trinta e oito semanas (9 meses) de gestação se torna um feto humano. Após este intervalo de tempo, o feto é totalmente criado fora do corpo da mulher e respira autonomamente como um bebê pela primeira vez. Neste ponto, a maioria das culturas modernas reconhecem o bebê como uma pessoa com direito à plena proteção da lei, embora algumas jurisdições de diferentes níveis alterem esse padrão, reconhecendo os fetos humanos enquanto eles ainda estão no útero.[98][99]
Em comparação com outras espécies, o parto humano é perigoso. Partos de duração de vinte e quatro horas ou mais não são raros e muitas vezes levam à morte da mãe, da criança ou de ambos.[100] Isto ocorre porque, tanto pela circunferência da cabeça fetal relativamente grande (para a habitação do cérebro) quanto pela cavidade pélvica relativamente pequena da mãe (uma característica necessária para o sucesso do bipedalismo, por meio da seleção natural).[101][102] As chances de um bom trabalho de parto aumentaram significativamente durante o século XX nos países mais ricos com o advento de novas tecnologias médicas. Em contraste, a gravidez e o parto normal permanecem perigosos nas regiões subdesenvolvidas e em desenvolvimento do mundo, com taxas de mortalidade materna aproximadamente 100 vezes maiores do que nos países desenvolvidos.[103]
Nos países desenvolvidos, as crianças normalmente pesam de 3 a 4 kg e medem de 50 a 60 cm no nascimento.[104] No entanto, o baixo peso ao nascer é comum nos países em desenvolvimento e contribui para os altos níveis de mortalidade infantil nestas regiões.[105] Indefesos ao nascimento, os seres humanos continuam a crescer durante alguns anos, geralmente atingindo a maturidade sexual entre 12 e 15 anos de idade. Os seres humanos femininos continuam a desenvolver-se fisicamente até cerca dos 18 anos, o desenvolvimento masculino continua até cerca dos 21 anos. A vida humana pode ser dividida em várias fases: infância, adolescência, vida adulta jovem, idade adulta e velhice. A duração destas fases, no entanto, têm variado em diferentes culturas e períodos. Comparado com outros primatas, os corpos dos seres humanos desenvolvem-se extraordinariamente rápido durante a adolescência, quando o corpo cresce 25% no tamanho. Chimpanzés, por exemplo, crescem apenas 14%.[106]
Existem diferenças significativas em termos de esperança de vida ao redor do mundo. O mundo desenvolvido é geralmente envelhecido, com uma idade média em torno de 40 anos (a mais elevada é em Mônaco - 45,1 anos). No mundo em desenvolvimento é a idade média fica entre 15 e 20 anos. A esperança de vida ao nascer em Hong Kong é de 84,8 anos para a mulher e 78,9 para o homem, enquanto em Essuatíni, principalmente por causa da AIDS, é 31,3 anos para ambos os sexos.[107] Enquanto um em cada cinco europeus tem 60 anos de idade ou mais, apenas um em cada vinte africanos tem de 60 anos de idade ou mais.[108] O número de centenários (pessoas com idade de 100 anos ou mais) no mundo foi estimado pela Organização das Nações Unidas em 210 mil indivíduos em 2002.[109] Apenas uma pessoa, Jeanne Calment, é conhecida por ter atingido a idade de 122 anos e 164 dias; idades mais elevadas foram registradas, mas elas não estão bem fundamentadas. Mundialmente, existem 81 homens com 60 anos ou mais para cada 100 mulheres da mesma faixa etária, e entre os mais velhos, há 53 homens para cada 100 mulheres.[110]
Os seres humanos são os únicos que experimentam a menopausa em alguma parte da vida. Acredita-se que a menopausa surgiu devido à "hipótese da avó", em que ocorre o interesse da mãe em renunciar aos riscos de morte durante outros partos para, em troca, investir na viabilidade dos filhos já nascidos.[111]
As mais atuais evidências genéticas e arqueológicas suportam uma origem recente única dos humanos modernos na África Oriental,[115] sendo que as primeiras migrações começaram há 60 mil anos. Os atuais estudos genéticos demonstraram que os seres humanos do continente africano são os mais geneticamente diversos.[116] No entanto, em comparação com os outros grandes símios, as sequências dos genes humanos são notavelmente homogêneas.[86]
Não há consenso científico sobre a relevância biológica da raça. Poucos antropólogos endossam a noção de "raça humana" como um conceito basicamente biológico; eles tendem a ver a raça como uma construção social sobreposta, mas em parte obscurecida, subjacente a variação biológica.[117][118][119] É geralmente acordado que certos traços genéticos, incluindo algumas doenças comuns, correlacionam-se com a ascendência genética de regiões específicas e a ascendência genética, tal como determinado pela identificação racial, está se tornando uma ferramenta cada vez mais comum para avaliação de risco em medicina.[120][121][122][123][124][125][126][127]
O uso do termo "raça" para significar algo como "subespécie" entre os seres humanos é obsoleto; os Homo sapiens não têm subespécies existentes atualmente. Em sua conotação científica moderna, o termo não se aplica a uma espécie tão geneticamente homogênea como um ser humano, como indicado na declaração sobre a raça (UNESCO 1950, re-ratificada em 1978).[128][129] Os estudos genéticos têm fundamentado a ausência de claras fronteiras biológicas, assim sendo, o termo "raça" é raramente usado na terminologia científica, tanto na antropologia biológica e genética humana.[130] O que no passado tinha sido definido como "raças"—brancos, negros ou asiáticos—estão agora sendo definido como "grupos étnicos" ou "populações", em correlação com o campo (sociologia, antropologia, genética), no qual está sendo considerado.[131][132]
Por centenas de milhares de anos o Homo sapiens empregou (e algumas tribos que ainda dependem) um método de caçadores-coletores como o seu principal meio de obter alimentos, combinando e envolvendo fontes estacionárias de alimentos (tais como frutas, cereais, tubérculos e cogumelos, larvas de insetos e moluscos aquáticos), com a caça de animais selvagens, que devem ser caçados e mortos, para serem consumidos. Acredita-se que os seres humanos têm utilizado o fogo para preparar e cozinhar alimentos antes de comer desde o momento da sua divergência do Homo erectus.[134]
Os seres humanos são onívoros, capazes de consumir tanto produtos vegetais como produtos animais. Com as diferentes fontes de alimentos disponíveis nas regiões de habitação e também com diferentes normas culturais e religiosas, grupos humanos adotaram uma gama de dietas, principalmente a partir do puramente vegetariano para o carnívoro. Em alguns casos, restrições alimentares levam a deficiências que podem acabar em doenças, porém grupos estáveis de humanos se adaptaram aos vários padrões dietéticos, através da especialização genética e de convenções culturais para utilizar fontes alimentares nutricionalmente equilibradas.[135] A dieta humana é proeminentemente refletida na cultura humana e levou ao desenvolvimento da ciência dos alimentos.[136]
Em geral, os seres humanos podem sobreviver por duas a oito semanas sem alimentos, em função da gordura corporal armazenada. A sobrevivência sem água é geralmente limitada a três ou quatro dias. A falta de comida continua a ser um problema grave, com cerca de 300 mil pessoas morrendo de fome a cada ano.[137] A desnutrição infantil também é comum e contribui para o número de mortos.[138]
No entanto, a distribuição alimentar global não é equilibrada, a obesidade atinge algumas populações humanas chegando a proporções epidêmicas, levando a complicações de saúde e aumento da mortalidade em alguns países desenvolvidos e em desenvolvimento. O Centers for Disease Control and Prevention (CDC) dos Estados Unidos indica que 32% dos adultos americanos com idades superiores a 20 anos são obesos, enquanto 66,5% são obesos ou com sobrepeso. A obesidade é causada por consumir mais calorias do que os gastos do corpo, e muitos atribuem o ganho de peso excessivo a uma combinação de excessos alimentares e exercícios físicos insuficientes.[139]
Há pelo menos dez mil anos, os humanos desenvolveram a agricultura, que alterou substancialmente o tipo de alimentos que as pessoas comiam. Isto levou a um aumento da população, ao desenvolvimento das cidades e, em virtude do aumento da densidade populacional, à maior propagação de doenças infecciosas. Os tipos de alimentos consumidos, bem como a forma como são preparados, tem variado muito, através do tempo, da localização e da cultura.[140]
Os seres humanos são geralmente diurnos. A necessidade de sono média é de entre sete e nove horas por dia para um adulto e de nove a dez horas por dia para uma criança; as pessoas idosas costumam dormir de seis a sete horas. Sentir menos sono do que isso é comum nas sociedades modernas e a privação do sono pode ter efeitos negativos. A restrição constante do sono adulto para quatro horas por dia tem sido mostrada como correlacionada com mudanças na fisiologia e no estado mental, incluindo fadiga, agressividade e desconforto corporal.[141]
O cérebro humano é o centro do sistema nervoso central e atua como o principal centro de controle para o sistema nervoso periférico. O cérebro controla atividades autônomas involuntárias, como a respiração e a digestão, assim como atividades conscientes, como o pensamento, o raciocínio e a abstração. Estes processos cognitivos constituem a mente, e, juntamente com suas consequências comportamentais, são estudadas no campo da psicologia.[142]
O cérebro humano é considerado o mais "inteligente" e capaz cérebro da natureza, superando o de qualquer outra espécie conhecida. Enquanto muitos animais são capazes de criar estruturas utilizando ferramentas simples, principalmente através do instinto e do mimetismo, a tecnologia humana é muito mais complexa e está constantemente evoluindo e melhorando ao longo do tempo. Mesmo as mais antigas estruturas e ferramentas criadas pelos humanos são muito mais avançadas do que qualquer outra estrutura ou ferramenta criada por qualquer outro animal.[143]
Embora as habilidades cognitivas humanas sejam muito mais avançadas do que as de qualquer outra espécie, a maioria destas habilidades podem ser observadas em sua forma primitiva no comportamento de outros seres vivos. A antropologia moderna sustenta a proposição de Darwin de que "a diferença entre a mente de um homem e a de animais evoluídos, grande como é, é certamente uma diferença de grau e não de tipo".[144]
Os seres humanos são apenas uma das nove espécies que passam no teste do espelho — que testa se um animal reconhece sua reflexão como uma imagem de si mesmo — juntamente com todos os grandes macacos (gorilas, chimpanzés, orangotangos, bonobos), golfinhos, elefantes asiáticos, Pega-rabudas e Orcas.[145] A maioria das crianças humanas passam no teste do espelho com 18 meses de idade.[146] No entanto, a utilidade deste teste como um verdadeiro teste de consciência tem sido contestada, e esta pode ser uma questão de grau, em vez de uma divisão nítida. Macacos foram treinados para aplicar as regras de resumo em tarefas.[147]
O cérebro humano percebe o mundo externo através dos sentidos e cada indivíduo humano é muito influenciado pelas suas experiências, levando a visões subjetivas da existência e da passagem do tempo. Os seres humanos possuem a consciência, a auto-consciência e uma mente, que correspondem aproximadamente aos processos mentais de pensamento. Estes são ditos de possuir qualidades tais como a auto-consciência, sensibilidade, sapiência e a capacidade de perceber a relação entre si e o meio ambiente. A medida como a mente constrói ou experimenta o mundo exterior é um assunto de debate, assim como as definições e validade de muitos dos termos usados acima. O filósofo da ciência cognitiva Daniel Dennett, por exemplo, argumenta que não existe tal coisa como um centro narrativo chamado de "mente", mas que em vez disso, é simplesmente um conjunto de entradas e saídas sensoriais: diferentes tipos de '"softwares"' paralelos em execução.[148] O psicólogo B. F. Skinner argumenta que a mente é uma ficção de motivos que desvia a atenção das causas ambientais do comportamento[149] e o que são comumente vistos como processos mentais podem ser melhor concebidos como formas de comportamento verbal encoberto.[150][151]
A motivação é a força motriz por trás do desejo de todas as ações deliberadas dos seres humanos. A motivação é baseada em emoções, especificamente, na busca de satisfação (experiências emocionais positivas), e à prevenção de conflitos. Positivo e negativo são definidos pelo estado individual do cérebro, que pode ser influenciado por normas sociais: uma pessoa pode ser levada a auto-agressão ou violência, porque seu cérebro está condicionado a criar uma resposta positiva a essas ações. A motivação é importante porque está envolvida no desempenho de todas as respostas aprendidas. Dentro da psicologia, a prevenção de conflitos e a libido são vistas como motivadores primários. Dentro da economia, a motivação é muitas vezes vista por basear-se em incentivos, estes podem ser de ordem financeira, moral ou coercitiva.[152]
A emoção tem uma significante influência, podemos dizer que serve até mesmo para controlar o comportamento humano, porém historicamente muitas culturas e filósofos, por diversas razões tem desencorajado essa influência por não ser checável. As experiências emocionais percebidas como agradáveis, como o amor, a admiração e a alegria, contrastando com aquelas percebidas como desagradáveis, como o ódio, a inveja ou a tristeza.[153][154] No pensamento científico moderno, algumas emoções refinadas consideradas um traço complexo neural inato em uma variedade de mamíferos domesticados e não domesticados. Estas normalmente foram desenvolvidas em reacção a mecanismos de sobrevivência superior e inteligentes de interação entre si e o ambiente; como tal, não é em todos os casos distinta e separada da função neural natural como foi uma vez assumida a emoção refinada. No entanto, quando seres humanos vivem de forma civilizada, verificou-se que a ação desinibida em extrema emoção pode levar à desordem social e à criminalidade.[155]
A sexualidade humana, além de garantir a reprodução biológica, tem importante função social: ele cria intimidade física, títulos e hierarquias entre os indivíduos, podendo ser direcionada para a transcendência espiritual (de acordo com algumas tradições);[156][157] e com um sentido hedonista de gozar de atividade sexual envolvendo gratificação. O desejo sexual, ou libido, é sentido como um desejo do corpo, muitas vezes acompanhada de fortes emoções como o amor, o êxtase e o ciúme.[158]
Escolhas humanas em agir sobre a sexualidade são normalmente influenciadas por normas culturais, que variam de forma muito ampla. As restrições são muitas vezes determinadas por crenças religiosas ou costumes sociais. O pesquisador pioneiro Sigmund Freud acreditava que os seres humanos nascem polimorficamente perversos, o que significa que qualquer número de objetos pode ser uma fonte de prazer.[159][160]
Segundo Freud, os seres humanos, em seguida, passam por cinco fases de desenvolvimento psicossexual (e podem fixar-se em qualquer fase por causa de traumas diversos durante o processo). Para Alfred Kinsey, outro influente pesquisador do sexo, as pessoas podem cair em qualquer lugar ao longo de uma escala contínua de orientação sexual (com apenas pequenas minorias totalmente heterossexual ou totalmente homossexual). Estudos recentes da neurologia e da genética sugerem que as pessoas podem nascer com uma predisposição para uma determinada orientação sexual ou outra.[159][160]
O conjunto sem precedentes de habilidades intelectuais da humanidade foi um fator-chave no eventual avanço tecnológico da espécie e na concomitante dominação da biosfera.[163] Desconsiderando os hominídeos extintos, os humanos são os únicos animais conhecidos por ensinar informações generalizáveis,[164] implantar inatamente a incorporação recursiva para gerar e comunicar conceitos complexos,[165] envolver a "física popular" necessária para projetar ferramentas competentes,[166][167] ou cozinhar alimentos em estado selvagem.[168] O ensino e a aprendizagem preservam a identidade cultural e etnográfica de todas as diversas sociedades humanas.[169] Outros traços e comportamentos que são, em sua maioria, exclusivos aos humanos, incluem iniciar e controlar focos de fogo,[170] estruturar fonemas[171] e a aprendizagem vocal.[172]
A divisão dos humanos em papéis de gênero masculino e feminino foi marcada culturalmente por uma divisão correspondente de normas, práticas, vestimentas, comportamento, direitos, deveres, privilégios, estatutos sociais e poder. Muitas vezes se acredita que as diferenças culturais por gênero tenham surgido naturalmente de uma divisão do trabalho reprodutivo; o fato biológico de que as mulheres dão à luz levou a uma maior responsabilidade cultural de nutrir e cuidar dos filhos.[173] Os papeis de gênero têm variado historicamente e os desafios às normas de gênero predominantes têm se repetido em muitas sociedades.[174]
Embora muitas espécies se comuniquem, a linguagem é exclusiva dos humanos, uma característica definidora da humanidade e uma cultura universal.[175] Ao contrário dos sistemas limitados de outros animais, a linguagem humana é aberta - um número infinito de significados pode ser produzido pela combinação de um número limitado de símbolos.[176][177] A linguagem humana também tem a capacidade de deslocamento, ao usar palavras para representar coisas e acontecimentos que não estão ocorrendo no presente ou localmente, mas que residem na imaginação compartilhada dos interlocutores.[78]
A linguagem difere de outras formas de comunicação por ser independente da modalidade; os mesmos significados podem ser transmitidos por meio de diferentes meios, auditivamente na fala, visualmente pela língua de sinais ou escrita e até mesmo por meio tátil como o braille.[178] A linguagem é fundamental para a comunicação entre os humanos e para o senso de identidade que une nações, culturas e grupos étnicos.[179] Existem aproximadamente seis mil línguas diferentes atualmente em uso, incluindo línguas de sinais e muitos outros milhares que estão extintas.[180]
As artes humanas podem assumir muitas formas, incluindo visuais, literárias e performáticas . A arte visual pode variar de pinturas e esculturas a filmes, design de interação e arquitetura.[181] As artes literárias podem incluir prosa, poesia e dramas; enquanto as artes performáticas geralmente envolvem teatro, música e dança.[182][183] Os humanos geralmente combinam as diferentes formas, por exemplo, vídeos musicais.[184] Outras entidades que foram descritas como tendo qualidades artísticas incluem preparação de alimentos, videogames e medicamentos.[185][186][187] Além de proporcionar entretenimento e transferência de conhecimento, as artes também são utilizadas para fins políticos.[188]
A arte é uma característica definidora do ser humano e há evidências de uma relação entre criatividade e linguagem.[189] A evidência mais antiga de arte foram as gravuras de conchas feitas pelo Homo erectus 300 mil anos antes da evolução dos humanos modernos.[190] A arte atribuída ao H. sapiens existia há pelo menos 75 mil anos, com joias e desenhos encontrados em cavernas na África do Sul.[191][192] Existem várias hipóteses de por que os humanos se adaptaram às artes. Isso inclui permitir que eles resolvam melhor os problemas, fornecendo um meio de controlar ou influenciar outros humanos, encorajando a cooperação e contribuição dentro de uma sociedade ou aumentando a chance de atrair um parceiro em potencial. O uso da imaginação desenvolvida por meio da arte, combinada com a lógica, o que pode ter dado aos primeiros humanos uma vantagem evolutiva[193]
Evidências de humanos engajados em atividades musicais são anteriores à arte nas cavernas e, até agora, a música tem sido praticada por praticamente todas as culturas humanas.[194] Existe uma grande variedade de gêneros musicais e músicas étnicas; com habilidades musicais humanas relacionadas a outras habilidades, incluindo comportamentos humanos sociais complexos. Foi demonstrado que os cérebros humanos respondem à música tornando-se sincronizados com o ritmo e a batida, um processo denominado arrastamento.[195] A dança também é uma forma de expressão humana encontrada em todas as culturas[196] e pode ter evoluído como uma forma de ajudar os primeiros humanos a se comunicarem.[197] Ouvir música e observar a dança estimula o córtex orbitofrontal e outras áreas do cérebro sensíveis ao prazer.[198]
Ao contrário da fala, a leitura e a escrita não são naturais aos humanos e devem ser ensinadas.[199] A literatura ainda está presente antes da invenção das palavras e da linguagem, com pinturas de 30 mil anos nas paredes de algumas cavernas retratando uma série de cenas dramáticas.[200] Uma das obras literárias mais antigas que sobreviveram é a Epopeia de Gilgamesh, gravada pela primeira vez em tabuletas babilônicas antigas há cerca de 4 mil anos.[201] Além de simplesmente transmitir conhecimento, o uso e o compartilhamento de ficção imaginativa por meio de histórias pode ter ajudado a desenvolver as capacidades humanas de comunicação e aumentado a probabilidade de conseguir um parceiro.[202] A narração de histórias também pode ser usada como uma forma de fornecer ao público lições morais e estimular a cooperação.
Ferramentas de pedra foram usadas por proto-humanos há pelo menos 2,5 milhões de anos.[203] O uso e a fabricação de ferramentas foram apresentados como a habilidade que define os humanos mais do que qualquer outra coisa[204] e têm sido historicamente vistos como um importante passo evolutivo.[205] A tecnologia se tornou muito mais sofisticada há cerca de 1,8 milhões de anos, com o uso controlado do fogo começando por volta de há 1 milhão de anos.[206][207] A roda e os veículos com rodas apareceram simultaneamente em várias regiões em algum momento do IV milênio a.C..[208] O desenvolvimento de ferramentas e tecnologias mais complexas permitiu o cultivo da terra e a domesticação de animais, revelando-se essencial para o desenvolvimento da agricultura - o que ficou conhecido como Revolução Neolítica.[209]
A China desenvolveu o papel, a imprensa, a pólvora, a bússola e outras invenções importantes.[210] As melhorias contínuas na fundição permitiram o forjamento de cobre, bronze, ferro e, eventualmente, do aço, que é usado em ferrovias, arranha-céus e muitos outros produtos e construções.[211] Isso coincidiu com a Revolução Industrial, onde a invenção de máquinas automatizadas trouxe grandes mudanças ao estilo de vida dos humanos.[212] A tecnologia moderna pode ser vista como em progressão exponencial,[213] com grandes inovações no século XX, como eletricidade, penicilina, semicondutores, motores de combustão interna, internet, fetilizantes fixadores de nitrogênio, aviões, computadores, automóveis, a pílula, fissão nuclear, revolução verde, rádio, criação científica de plantas, foguetes, ar condicionado, televisão e linha de montagem.[214]
A religião é geralmente definida como um sistema de crenças sobre o sobrenatural, o sagrado ou o divino, além de práticas, valores, instituições e rituais associados a tal crença. Algumas religiões também têm um código moral. A evolução e a história das primeiras religiões tornaram-se recentemente áreas de ativa investigação científica.[215][216][217][218]
Embora o momento exato em que os humanos se tornaram religiosos pela primeira vez permaneça desconhecido, pesquisas mostram evidências confiáveis de comportamento religioso por volta do Paleolítico Médio (há cerca de 45 mil a 200 mil anos).[219] A religião pode ter evoluído para desempenhar um papel de ajudar a impor e encorajar a cooperação entre humanos.[220]
Não existe uma definição acadêmica consensual do que constitui religião.[221] A religião assumiu muitas formas que variam de acordo com a cultura e a perspectiva individual, em alinhamento com a diversidade geográfica, social e linguística do planeta. A religião pode incluir uma crença na vida após a morte,[222] a origem da vida,[223] a natureza do universo (cosmologia religiosa) e seu destino final (escatologia), além do que é moral ou imoral.[224] Uma fonte comum de respostas a essas perguntas são as crenças em seres divinos transcendentes, como divindades ou um Deus único, embora nem todas as religiões sejam teístas.[225][226]
Embora o nível exato de religiosidade seja algo difícil de medir,[227] a maioria dos humanos professa alguma variedade de crenças religiosas ou espirituais.[228] Em 2015, a maioria da população mundial era cristã, seguida por muçulmanos, hindus e budistas.[229] Cerca de 16%, ou pouco menos de 1,2 bilhão de humanos, eram irreligiosos, incluindo aqueles sem crenças religiosas ou sem identidade com qualquer religião.[230]
Um aspecto exclusivo dos humanos é sua capacidade de transmitir conhecimento de uma geração para a seguinte e de construir continuamente sobre essas informações para desenvolver ferramentas, leis científicas e outros avanços para transmitir adiante.[231] Esse conhecimento acumulado pode ser testado para responder a perguntas ou fazer previsões sobre como o universo funciona e tem sido muito bem-sucedido no avanço da ascendência humana.[232] Aristóteles foi descrito como o primeiro cientista,[233] e precedeu o surgimento do pensamento científico durante o período helenístico.[234] Outros primeiros avanços na ciência vieram da Dinastia Han na China e durante a Idade de Ouro Islâmica.[235][236] A Revolução científica, perto do final do Renascimento, levou ao surgimento da ciência moderna.[237]
Uma cadeia de eventos e influências levou ao desenvolvimento do método científico, um processo de observação e experimentação que é usado para diferenciar a ciência da pseudociência.[238] A compreensão da matemática é exclusiva dos humanos, embora outras espécies de animais tenham alguma cognição numérica.[239] Toda a ciência pode ser dividida em três ramos principais, as ciências formais (por exemplo, lógica e matemática), que se preocupam com sistemas formais, as ciências aplicadas (por exemplo, engenharia, medicina), que se concentram em aplicações práticas, e as ciências empíricas, que se baseiam na observação empírica e, por sua vez, são divididas em ciências naturais (por exemplo, física, química, biologia) e ciências sociais (por exemplo, psicologia, economia, sociologia).[240]
A filosofia é um campo de estudo onde os humanos procuram compreender verdades fundamentais sobre si próprios e o mundo em que vivem.[241] A investigação filosófica tem sido uma característica importante no desenvolvimento da história intelectual dos humanos.[242] Ela tem sido descrita como a "terra de ninguém" entre o conhecimento científico definitivo e os ensinamentos religiosos dogmáticos.[243] A filosofia depende da razão e da evidência, ao contrário da religião, mas não requer as observações empíricas e os experimentos fornecidos pela ciência.[244] Os principais campos da filosofia incluem metafísica, epistemologia, lógica e axiologia (que inclui ética e estética).[245]
A sociedade é o sistema de organizações e instituições decorrentes da interação entre humanos. Os humanos são seres altamente sociais e tendem a viver em grandes grupos sociais complexos. Eles podem ser divididos em diferentes grupos de acordo com sua renda, riqueza, poder, reputação e outros fatores.[246] A estrutura da estratificação social e o grau de mobilidade social diferem, especialmente entre as sociedades modernas e tradicionais. Os grupos humanos variam desde o tamanho das famílias até as nações. As primeiras formas de organização social humana foram famílias que viviam em sociedades de bandos ou tribos, como caçadores-coletores.[247]
Todas as sociedades humanas organizam, reconhecem e classificam os tipos de relações sociais com base nas relações entre pais, filhos e outros descendentes (consanguinidade) e nas relações por meio do casamento. Existe também um terceiro tipo aplicado aos padrinhos ou filhos adotivos. Essas relações culturalmente definidas são chamadas de parentesco. Em muitas sociedades, é um dos princípios de organização social mais importantes e desempenha um papel na transmissão de estatutos sociais e herança.[248] Todas as sociedades têm regras de tabu sobre incesto, segundo as quais o casamento entre certos tipos de relações de parentesco é proibido e algumas também têm regras de casamento preferencial com certas relações de parentesco.[249]
A estrutura social e familiar dos humanos é bastante variável conforme as diferentes sociedades humanas. Além das formas monogâmicas e poligínicas, formas poliândricas e promíscuas ocorrem com menos frequência. Uma estrutura social típica ou original não pode ser especificada, uma vez que o comportamento é fortemente sobreposto culturalmente. Tentativas de compreender o comportamento social original dos humanos usando comparações morfológicas com outras espécies de primatas se revelaram duvidosas (espécies de primatas com dimorfismo sexual muito acentuado em termos de peso tendem a viver em grupos de haréns, como ocorre com os gorilas; por outro lado, primatas com pouco dimorfismo sexual e sem diferenças de tamanho dos dentes caninos tendem a levar um estilo de vida monogâmico, como ocorre com os gibões, da família Hylobatidae) [250]
Grupos étnicos humanos são uma categoria social que se identifica como um grupo com base em atributos compartilhados que os distinguem de outros grupos. Podem ser um conjunto comum de tradições, ancestrais, idioma, história, sociedade, cultura, nação, religião ou tratamento social dentro de sua área de residência.[251][252] A etnia é separada do conceito de raça, que se baseia em características físicas, embora ambas sejam socialmente construídas.[253] Atribuir etnicidade a determinada população é complicado, pois mesmo dentro de designações étnicas comuns, pode haver uma ampla gama de subgrupos e a composição desses grupos étnicos pode mudar com o tempo, tanto no nível coletivo quanto no individual.[86] Também não existe uma definição geralmente aceita sobre o que constitui um grupo étnico.[254] Os agrupamentos étnicos podem desempenhar um papel poderoso na identidade social e na solidariedade das unidades etno-políticas. Isso está intimamente ligado à ascensão do Estado-nação como a forma predominante de organização política nos séculos XIX e XX.[255][256][257]
A distribuição inicial do poder político foi determinada pela disponibilidade de água doce, solo fértil e clima temperado de diferentes locais.[258] À medida que as populações agrícolas se reuniam em comunidades maiores e mais densas, as interações entre esses diferentes grupos aumentavam. Isso levou ao desenvolvimento da governança dentro e entre as comunidades.[259] À medida que as comunidades cresciam, a necessidade de alguma forma de governança aumentava, pois todas as grandes sociedades sem governo lutavam para funcionar.[260] Os humanos desenvolveram a capacidade de mudar a afiliação com vários grupos sociais com relativa facilidade, incluindo alianças políticas anteriormente fortes, se isso for visto como uma vantagem pessoal.[261] Essa flexibilidade cognitiva permite que os humanos individuais mudem suas ideologias políticas, com aqueles com maior flexibilidade menos propensos a apoiar posturas autoritárias e nacionalistas.[262]
Os governos criam leis e políticas que afetam os cidadãos que governam. Houve várias formas de governo ao longo da história da humanidade, cada uma com vários meios de obter poder e capacidade de exercer diversos controles sobre a população.[263] Em 2017, mais da metade de todos os governos nacionais eram democracias, com 13% sendo autocracias e 28% contendo híbridos de ambas.[264] Muitos países formaram alianças políticas internacionais, sendo a maior delas as Nações Unidas, que tem 193 Estados-membros.[265]
O comércio, a troca voluntária de bens e serviços, é visto como uma característica que diferencia os humanos de outros animais e tem sido citado como uma prática que deu ao Homo sapiens uma grande vantagem sobre outros hominídeos.[266][267] As evidências sugerem que os primeiros H. sapiens usavam rotas comerciais de longa distância para trocar mercadorias e ideias, levando a explosões culturais e fornecendo fontes adicionais de alimento quando a caça era escassa, enquanto essas redes de comércio não existiam para os agora extintos neandertais.[268][269] O comércio inicial provavelmente envolvia materiais para a criação de ferrament,as como a obsidiaa.[270] As primeiras rotas comerciais verdadeiramente internacionais giraram em torno do comércio de especiarias durante os períodos romano e medieval.[271] Outras rotas comerciais importantes a serem desenvolvidas nessa época incluem a Rota da Seda, a Rota do Incenso, a Rota do Âmbar, a Rota do Chá, a Rota do Sal, a Rota Comercial Transsaariana e a Rota do Estanho.[272]
As primeiras economias humanas eram mais provavelmente baseadas em ofertas ao invés de um sistema de escambo.[273] O dinheiro inicial consistia em mercadorias; sendo o mais antigo em forma de gado e o mais utilizado em conchas de cauri.[274] Desde então, o dinheiro evoluiu para moedas emitidas pelo governo, papel-moeda e dinheiro eletrônico. O estudo humano da economia é uma ciência social que examina como as sociedades distribuem recursos escassos entre diferentes pessoas.[275] Existem enormes desigualdades na divisão da riqueza entre os humanos; os oito humanos mais ricos valem o mesmo valor monetário que a metade mais pobre de toda a população humana.[276]
Os humanos cometem violência contra outros humanos em uma taxa comparável a outros primatas, mas em uma taxa mais alta do que a maioria dos outros mamíferos.[277] Prevê-se que 2% dos primeiros H. sapiens foram assassinados, taxa que aumentou para 12% durante o período medieval, antes de cair para menos de 2% nos tempos modernos.[278] Ao contrário da maioria dos animais, que geralmente matam bebês, os humanos matam outros humanos adultos em uma taxa muito alta.[279] Há uma grande variação na violência entre as populações humanas, com taxas de homicídio em sociedades que possuem sistemas jurídicos e fortes atitudes culturais contra a violência em cerca de 0,01%.[280]
A disposição dos humanos de matar outros membros de sua espécie em massa por meio de conflitos organizados há muito tempo é objeto de debate. Uma escola de pensamento é a de que a guerra evoluiu como um meio de eliminar competidores e sempre foi uma característica humana inata. Outra sugere que a guerra é um fenômeno relativamente recente e surgiu devido a mudanças nas condições sociais.[281] Embora não tenham sido estabelecidas, as evidências atuais sugerem que as predisposições bélicas só se tornaram comuns há cerca de 10 mil anos e, em muitos lugares, muito mais recentemente do que isso. A guerra teve um alto custo na vida humana; estima-se que durante o século XX, entre 167 milhões e 188 milhões de pessoas morreram em consequência de guerras.[282]
a.   Sinônimos registrados para a espécie Homo sapiens, sendo que os nomes de Bory de St. Vincent referem-se a variedades geográficas dos humanos modernos: aethiopicus Bory de St. Vincent, 1825; americanus Bory de St. Vincent, 1825; arabicus Bory de St. Vincent, 1825; aurignacensis Klaatsch & Hauser, 1910; australasicus Bory de St. Vincent, 1825; cafer Bory de St. Vincent, 1825; capensis Broom, 1917; columbicus Bory de St. Vincent, 1825; cro-magnonensis Gregory, 1921; drennani Kleinschmidt, 1931; eurafricanus (Sergi, 1911); grimaldiensis Gregory, 1921; grimaldii Lapouge, 1906; hottentotus Bory de St. Vincent, 1825; hyperboreus Bory de St. Vincent, 1825; indicus Bory de St. Vincent, 1825; japeticus Bory de St. Vincent, 1825; melaninus Bory de St. Vincent, 1825; monstrosus Linnaeus, 1758; neptunianus Bory de St. Vincent, 1825; palestinus McCown & Kleith, 1932; patagonus Bory de St. Vincent, 1825; priscus Lapouge, 1899; proto-aethiopicus Giuffrida-Ruggeri, 1915; scythicus Bory de St. Vincent, 1825; sinicus Bory de St. Vincent, 1825; spalaeus Lapouge, 1899; troglodytes Linnaeus, 1758 [nomen oblitum]; wadjakensis Dubois, 1921.[283]
Bioquímica (química aplicada à biologia) é a ciência e  tecnologia que estuda e aplica as ciências químicas ao contexto da biologia, sendo portanto uma área interdisciplinar entre a química e a biologia. Consiste no estudo, identificação, análise, modificação e manipulação de moléculas e das reações químicas de importância biológica, em ambientes e contextos químicos próprios in vitro ou in vivo (compartimentos celulares, virais e fisiológicos). Envolve moléculas de diversas dimensões tais como proteínas, enzimas, carboidratos, lipídios, ácidos nucléicos, vitaminas, alcaloides, terpenos e mesmo íons inorgânicos. Também engloba o estudo do efeito de compostos químicos orgânicos ou inorgânicos sobre os diferentes compartimentos biológicos (química biológica), assim como a modificação química de biomoléculas.[1][2][3] Suas aplicações englobam setores como alimentos, fármacos e biofármacos, análises clínicas, biocombustíveis, pesquisa básica dentre outros. É uma ciência e tecnologia essencial para todas as profissões relacionadas a ciências da vida e uma das fronteiras de desenvolvimento das ciências químicas.
Bioquímico é o profissional que estuda e aplica as leis da bioquímica para o entendimento e aplicação tecnológica de biomoléculas e dos organismos vivos (bioquímica industrial, biotecnologia e bioprocessos, bioquímica médica e clínica, bioquímica de alimentos, bioquímica agrícola e ambiental) para benefícios comerciais e industriais, e/ou benefícios a saúde humana e animal, a agropecuária e ao meio ambiente. Os bioquímicos utilizam ferramentas e conceitos da química e da biologia, particularmente da química orgânica, físico-química, fermentações e metabolismo, biologia celular, biologia molecular e genética, para a elucidação dos sistemas vivos e para sua aplicação tecnológica e industrial.[4]
A Bioquímica não deve ser confundida, no Brasil, com as análises clínicas, apenas uma de suas inúmeras aplicações e nem tampouco as análises clínicas devem ser reduzidas a apenas a bioquímica clínica.[4] Em função disso, a graduação (licenciatura em Portugal) em Bioquímica é uma das mais tradicionais na Europa e EUA, e no Brasil, existe nas Universidades Federais de Viçosa e de São João del Rey (UFV e UFSJ) e nas Universidades Estaduais de Maringá e de São Paulo (UEM e USP, nesta última, como química ênfase bioquímica). No Brasil, não se deve confundir farmacêutico com o bioquímico, visto que um é profissional de saúde e outro é profissional da química da vida e da biotecnologia. Por bastante tempo, os cursos de graduação em farmácia no Brasil denominaram-se Farmácia-Bioquímica, em errônea alusão à habilitação em análises clínicas. Isto gerou na sociedade, e mesmo nos meios acadêmicos, a falsa noção de que bioquímica seria sinônimo de análises clínicas e farmácia, algo totalmente errado.[5]
A história da bioquímica moderna data do século XIX quando começaram as abordagens químicas sobre os fenômenos da biologia integrando conhecimento destas duas ciências e quando a química orgânica amadureceu como ciência e tecnologia. Um importante marco da bioquímica moderna foi a descoberta da síntese de ureia por Friedrich Wöhler em 1828, provando que os compostos orgânicos poderiam ser obtidos artificialmente. Outro marco importante ocorreu em 1833, quando Anselme Payen isolou pela primeira vez uma enzima, a diastase. Esta descoberta também é considerada como a primeira vez que foi descrito um composto orgânico que apresentava as propriedades de um catalisador. Entretanto, apenas em  1878, o fisiologista Wilhelm Kühne cunhou o termo enzima para se referir aos componentes biológicos desconhecidos que participavam do processo de fermentação.[1][2][3]
Em meados do século XIX, Louis Pasteur estudou o fenômeno da fermentação e descobriu que certas leveduras estavam envolvidas neste processo, e portanto, não se tratava de um fenômeno somente químico.  Pasteur  escreveu: "a fermentação alcoólica é um ato relacionado com a vida e organização das células de levedura, não com a morte e putrefação destas células". Pasteur desenvolveu também métodos de esterilização de vinho, leite e cerveja (pasteurização) e contribuiu muito para refutar a ideia de geração espontânea de seres vivos. Em 1896, Eduard Buchner demonstrou pela primeira vez que um processo bioquímico complexo poderia ocorrer fora de uma célula, tendo como base a fermentação alcoólica usando extrato celular de levedura.[1][2][3]
Durante o período de 1885-1901, Albrecht Kossel isolou e nomeou cinco constituintes dos ácidos nucleicos: adenina, citosina, guanina, timina e uracila. Estes compostos são conhecidos coletivamente como bases nitrogenadas e integram a estrutura molecular do DNA e do RNA. Os ácidos nucléicos foram descobertos por Friedrich Miescher, em 1869.[1][2][3]
Embora o termo "bioquímica" pareça ter sido usado pela primeira vez em 1882, é geralmente aceito que a cunhagem formal do termo ocorreu em 1903 por Carl Neuberg, um químico alemão. No entanto grandes pesquisadores como Wöhler, Liebig, Pasteur e Claude Bernard já usavam outras denominações.[1][2][3]
A partir da década de 1920, a bioquímica experimentou considerável avanço, especialmente pelo desenvolvimento de novas técnicas, como a cromatografia, a difração de raios X, a espectroscopia de RMN, a marcação isotópica, a microscopia eletrônica e simulações de dinâmica molecular. Estas técnicas permitiram a descoberta e análise detalhada de muitas biomoléculas e de vias metabólicas em uma célula, tal como a glicólise e o ciclo de Krebs.[1][2][3]
Na década de 1950, James D. Watson, Francis Crick, Rosalind Franklin e Maurice Wilkins resolverem a estrutura do DNA e sugeriram a sua relação com a transferência da informação genética. Em 1958, George Beadle e Edward Tatum receberam o Prêmio Nobel pelo trabalho com fungos, onde demostram que um gene gerava como produto uma enzima. Este conceito, hoje ampliado, ficou conhecido como o Dogma central da biologia molecular.Neste momento a bioquímica passa a ter um relação mais intima com a biologia molecular, o estudo dos mecanismos moleculares pelos quais a informação genética codificada no DNA pode resultar nos processos de vida. Dependendo da definição exata dos termos utilizados, a biologia molecular pode ser considerada como um ramo de bioquímica, ou bioquímica como uma ferramenta para investigar e estudar a biologia molecular. Entretanto, é importante frisar que essa relação mais íntima jamais eliminou da bioquímica o estudo de outras biomoléculas.[1][2][3]
Em 1975 foi a vez de destacar as pesquisas sobre o sequenciamento de DNA, sendo Allan Maxam, Walter Gilbert e Frederick Sanger os principais cientista envolvidos nestas pesquisas. Logo em seguida surge a primeira empresa de biotecnologia industrial, a Genentech. Logo tornou-se possível a fabricação de princípio ativo, hormônios e vacinas por meios biotecnológicos.[1][2][3]
Em 1988, Colin Pitchfork foi a primeira pessoa condenada por assassinato usando como provas exames de DNA,  ocasionando uma revolução nas ciências forenses. Mais recentemente, Andrew Fire e Craig Mello receberam o Prêmio Nobel em 2006 pela descoberta da interferência do RNA (RNAi)  no silenciamento genético.[1][2][3]
Estudo do sistema bioquímico: determinação das propriedades químicas, físico-químicas, biofísicas e estruturais das biomoléculas e de suas interações entre si; métodos de análise e purificação. As biomoléculas podem ser classificadas como orgânicas e inorgânicas[1][2][3]:
A água é uma biomolécula importante, responsável por 70% do peso total de uma célula. Além de ser o principal constituinte da célula, desempenha um papel fundamental na definição de suas estruturas e funções. Muitas vezes a estrutura ou a função de uma biomolécula depende de suas características de afinidade com a água, a saber: se a biomolécula é hidrofílica, hidrofóbica ou anfipática. A água é o meio ideal para a maioria das reações bioquímicas e é o fator primário de definição das complexas estruturas espaciais das macromoléculas.[1][2][3]
As proteínas constituem a maior fração da matéria viva e são as macromoléculas mais complexas; possuem inúmeras funções na célula e formam várias estruturas celulares, além de controlarem a entrada e saída de substâncias nas membranas. Têm importante papel na contração e movimentação dos músculos (actina e miosina), sustentação (colágeno), transporte de oxigênio (hemoglobina), na defesa do organismo (anticorpos ), na produção de hormônios e também atuam como catalisadores (as enzimas) de reações químicas.[1][2][3]
Os ácidos nucléicos são as maiores macromoléculas da célula e responsáveis pelo armazenamento e transmissão da informação genética.
Os carboidratos são os principais combustíveis celulares (reserva de energia); possuem também função estrutural e participam dos processos de reconhecimento celular e de formação dos ácidos nucleicos. São exemplos glicose, frutose,pentose, sacarose  lactose, amido.[1][2][3]
Os lipídios são a principal fonte de armazenamento de energia dos organismos vivos e desempenham importantes funções no isolamento térmico e físico (impermeabilização de superfícies biológicas como frutos) e proteção a choques mecânicos. São exemplos membrana biológicas (fosfolipídios), gorduras e óleos, precursores de hormônios esteroides (tais como testosterona, progesterona e estradiol) e de sais biliares, que atuam como detergentes, propiciando a absorção dos lipídios.[1][2][3]
As vitaminas são micromoléculas, não participam do processo digestório, pois são absorvidas diretamente pelo organismo, que atuam como coenzimas, isto é, ativando enzimas responsáveis pelo metabolismo celular. Geralmente são hidrossolúveis. São lipossolúveis as vitaminas A (retinol), D (calciferol), E (tocoferol) e K.[1][2][3]
Os compostos do metabolismo secundário, tais como alcalóides (cafeína, nicotina, atropina), polifenóis (taninos), terpenos (óleos essenciais aromáticos), dentre outros.[1][2][3]
O metabolismo  envolve a síntese e degradação das diferentes biomoléculas, mecanismos de regulação das inúmeras reações que ocorrem simultaneamente na célula e no organismo; fluxos metabólicos; processos fermentativos e estudo do efeito de compostos químicos xenobióticos nos organismos vivos (efeitos de fármacos, agroquímicos, poluentes, tóxicos). Entre os celos metabólicos mais importantes estão o ciclo de Krebs, fotossíntese.[1][2][3]
No estudo da Bioquímica diversos são os instrumentais de análise, em especial instrumentos e técnicas químicas, bioquímicas e biofísicas, tais como eletroforese, cromatografia (em especial HPLC/CLAE e gasosa), espectrofotometria, reação em cadeia da polimerase (PCR), plasmídeos e tecnologia do DNA recombinante (engenharia genética),ensaios enzimáticos,   espectrometria de massas, ressonância magnética nuclear, ensaios de fluorescência (espectroscopia de fluorescência por exemplo), calorimetria de titulação isotérmica (ITC), biorreatores, ensaios de ligação de biomoléculas (Western Blot, ELISA), dicroísmo circular, PCR quantitativo, bioinformática, Sohxlet, destilação, centrifugação, Kjeldalhl, Demanda química de oxigênio (DQO), meios de cultura, entre outros que podem ser vistas nesta listagem: técnicas de laboratório.[1][2][3]
Estudo do sistema biológico em nível molecular: Apresenta a visão bioquímica de processos e fenômenos da biologia celular, fisiologia e imunologia,  transmissão de genes (biologia molecular), patologia.[1][2][3]
A Bioquímica celular apresenta a visão e explicação bioquímica de processos e fenômenos intracelulares envolvendo: sinalização celular, divisão celular, tráfico de vesículas, morte celular, radicais livres, regulação do ciclo celular, reprodução descontrolada (câncer), adesão celular, funções metabólicas de organelas, relação bioquímica entre organelas e sinalização intracelular, organização do material genético nas células (cromossomos, cromatina, nucleossomos), processos de replicação, reparo, recombinação e transposição, organização dos genomas, controle da expressão gênica.[1][2][3]
A Bioquímica fisiológica apresenta a visão e explicação bioquímica de processos e fenômenos fisiológicos envolvendo: controle e integração de processos fisiológicos por hormônios, equilíbrio eletrolítico e ácido-base, equilíbrio gasoso e bioquímica da respiração,  bioquímica do sistema nervoso, bioquímica do sistema digestório, bioquímica do tecido hepático, bioquímica do tecido renal, bioquímica do sangue e linfa, bioquímica do tecido adiposo, bioquímica da visão.[1][2][3]
A Bioquímica clínica (patológica) apresenta a visão e diagnóstico bioquímico de processos e fenômenos patológicos tais como: diabetes mellitus e hipoglicemia, provas de função renal, função hepática e função pancreática, dislipidemias, marcadores moleculares enzimáticos e genéticos, doenças do coração, lesões musculares, distúrbios de equilíbrio ácido-báse, disfunções hormonais e químicas de hipófise, tireóide, gônadas, adrenal e para-tireóide.[1][2][3]
O conhecimento bioquímico é muito importante para empresas e  indústrias de diversas áreas:[2][3][6][7]  farmacêutica (farmoquímicos: fármacos, excipientes, produtos naturais e biofármacos), análises clínicas (instrumental de análise e diagnóstico bioquímica de doenças), nutrição  e alimentos (ingredientes e análises de componentes bioquímicos, bebidas alcoólicas, leite e derivados, produção de chocolates),  agrícola (agroquímicos e melhora da fixação de nitrogênio em plantas como a soja), cosmética (novos ingredientes  e produtos de beleza e higiene) e até tecnológica (produção de compósitos sustentáveis de origem renovável).[1][2][3]
O primeiro instituto de pesquisa estruturado e voltado unicamente para a química da vida surgiu em 1872, como Instituto de Química Fisiológica da Universidade de Strasbourg, enquanto que, em 1880, a universidade norte-americana de Yale estruturou os primeiros cursos regulares de química fisiológica. Por volta de 1899, quando a universidade inglesa de Cambridge criou o laboratório de química dentro do departamento de fisiologia, chefiado por Frederick Gowland Hopkins, primeiro professor de bioquímica da Universidade de Cambridge, e também fundador da bioquímica inglesa,[8] a química da vida já estava estabelecida como ciência, sob diferentes denominações.[9] O processo de maturação desta ciência estabeleceu cursos de bacharelado (em Portugal, equivalente a Licenciatura) em diversos países.[8] É preciso ressaltar que os cursos de graduação (licenciaturas) em bioquímica e portanto, os bioquímicos são tradicionais em países da Europa (Reino Unido, Alemanha, Espanha, Portugal, Franca e Italia),na America Latina (Argentina, Paraguai, Uruguai, Chile, Colombia, México e Guatemala) no Canadá, na Austrália e nos Estados Unidos. Neste último país, os cursos de bacharelado em bioquímica existem desde a década de 50. Segundo a ASBMB (American Society for Biochemistry and Molecular Biology) existem cerca de 600 Instituições nos Estados Unidos que oferecem os cursos de bioquímica/biologia molecular e estima-se que cerca de 2 mil bacharéis foram graduados nos anos de 2001-2002. No Reino Unido existem mais de 100 cursos de graduação em bioquímica.[10]
O Bioquímico, portanto, é o profissional que estuda a bioquímica de um ponto de vista de ciência básica e aplicada tecnologicamente e industrialmente. Os bioquímicos utilizam ferramentas e conceitos da química e da biologia, particularmente da química orgânica, físico-química, biologia celular, biologia molecular e genética, para a elucidação dos sistemas vivos e para sua aplicação tecnológica e industrial. Desta forma o bioquímico possui conhecimentos científicos, capacitação técnica e habilidades para atuar em ensino superior, pesquisa, desenvolvimento e inovação, controle e garantia de qualidade, produção industrial, laboratórios, comércio de produtos científicos, laboratoriais e industriais, bioeconomia, além de aprender sobre os princípios éticos e legais relativos à profissão no âmbito do seu exercício profissional. O propósito da existência do curso de bioquímica está na unificação das diferentes visões e fragmentos da bioquímica que antes estavam espalhados em diferentes profissões: a bioquímica científica (pesquisa), a bioquímica industrial, a bioquímica clínica e toxicológica, a bioquímica agrícola, a bioquímica de alimentos e bromatologia, a bioquímica educacional, a tecnologia bioquímica e biotecnologia, e mesmo a bioquímica comercial (marketing, gestão etc). Os cursos de graduação em bioquímica são tradicionais, bem consolidados e de perfil bem definido em diversos países como Portugal, Espanha, Reino Unido, Chile, Canadá, Austrália e nos Estados Unidos, onde é uma da 10 profissões mais valorizadas e de maior salário. No Brasil, a oferta de cursos de graduação em bioquímica é recente, em função da combinação do crescente interesse no ensino da bioquímica de forma aprofundada a nível de graduação com a migração do do perfil do farmacêutico para ser um profissional de saúde. Com isso, abre-se um espaço natural para o surgimento dos bacharelados em bioquímica, com o perfil de profissional da química e da área tecnológica em qualquer interface química-biologia.[11]
A licenciatura em bioquímica, em Portugal, foi criada em 1979, na Universidade de Coimbra,[12] sendo prontamente seguido de diversas outras universidades.[13]
Em 1995, os licenciados (bacharéis) em Bioquímica criaram a Associação Nacional de Bioquímicos (ANBIOQ), com sede em Coimbra.
O bacharelado em bioquímica, no Brasil, foi criado em 2001, na Universidade Federal de Viçosa,[10] visando a suprir a necessidade crescente de profissionais qualificados para atuar nas áreas de pesquisa e desenvolvimento tecnológico das diversas áreas relacionadas com o curso.
Em moldes semelhantes, foi criado em 2008, o segundo curso de graduação em bioquímica do Brasil; a Universidade Federal de São João del Rei,[14] implantou o curso em seu campus de expansão Centro-Oeste.[15] Em 2011, foi criado o terceiro curso de graduação em Bioquímica, da Universidade Estadual de Maringá. Existe também na USP a graduação em química com ênfase em bioquímica.
No Brasil, não se deve confundir farmacêutico com o bioquímico, visto que um é profissional de saúde e outro é profissional da química em qualquer interface com a biologia. Por bastante tempo, os cursos de graduação em farmácia no Brasil denominaram-se Farmácia-Bioquímica, em errônea alusão à habilitação em análises clínicas . Isto gerou na sociedade, e mesmo nos meios acadêmicos, a falsa noção de que bioquímica seria análises clínicas somente e seria sinônimo de farmácia, o que de fato não é verdadeiro.[19][1]
Em 2014 foi criado o Movimento Bioquímica Brasil, com presença nas principais mídias sociais, site e canal de vídeo no principal agregador desta mídia,  dedicados a divulgar o Bacharel em Bioquímica e a divulgar a ciência, tecnologia e inovação em Bioquímica. Desde o início do movimento, a influência de conceitos-chave de empreendedorismo, responsabilidade individual, responsabilidade social, vivência no mercado de trabalho não acadêmico e união entre os cursos e bioquímicos se mostrou forte, resultando na consolidação do Bacharelado em Bioquímica no Brasil, como prova a simbologia aqui exposta.
A bioquímica pode ser vista como um fim em si ou como uma ferramenta para diversas outras profissões:
O ferro (do latim ferrum) é um elemento químico, símbolo Fe, de número atômico 26 (26 prótons e 26 elétrons) e massa atômica 56 u. À temperatura ambiente, o ferro encontra-se no estado sólido. É extraído da natureza sob a forma de minério de ferro que, depois de passado para o estágio de ferro-gusa, através de processos de transformação, é usado na forma de lingotes. Controlando-se o teor de carbono, dá-se origem a vários tipos de aço.
Este metal de transição é encontrado no grupo 8 (anteriormente denominado como VIIIB) da Classificação Periódica dos Elementos. É o quarto elemento mais abundante da crosta terrestre (aproximadamente 5%) e, entre os metais, somente o alumínio é mais abundante.
É um dos elementos mais abundantes do universo; o núcleo da Terra é formado principalmente por ferro e níquel (NiFe). Este ferro está em uma temperatura muito acima da temperatura de Curie do ferro, dessa forma, o núcleo da Terra não é ferromagnético.
O ferro tem sido historicamente importante, e um período da história recebeu o nome de Idade do Ferro. O ferro, atualmente, é utilizado extensivamente para a produção de aço, liga metálica para a produção de ferramentas, máquinas, veículos de transporte (automóveis, navios, etc), como elemento estrutural de pontes, edifícios e uma infinidade de outras aplicações.
É um metal maleável, tenaz, de coloração cinza prateado, apresentando propriedades magnéticas; é ferromagnético a temperatura ambiente, assim como o níquel e o cobalto.
É encontrado na natureza fazendo parte da composição de diversos minerais, entre eles muitos óxidos, como o FeO (óxido de ferro II, ou óxido ferroso) ou como Fe2O3 (óxido de ferro III, ou óxido férrico). Os números que acompanham o íon ferro dizem respeito aos estados de oxidação apresentados pelo ferro, que são +2 e +3, e ele é raramente encontrado livre. Para obter-se ferro no estado elementar, os óxidos são reduzidos com carbono e imediatamente submetidos a um processo de refinação para retirar as impurezas presentes.
É o elemento mais pesado que se produz exotermicamente por fusão nuclear, e o mais leve produzido por fissão, devido ao fato de seu núcleo ter a mais alta energia de ligação por núcleon, que é a energia necessária para separar do núcleo um nêutron ou um próton. Portanto, o núcleo mais estável é o do ferro-56.
Apresenta diferentes formas estruturais dependendo da temperatura:
A distribuição eletrônica do ferro é 1S² 2S² 2P⁶ 3S² 3P⁶ 4S² 3d⁶. Sua camada de valência (última camada) é 3d⁶.
O ferro disponível nos alimentos pode ser de dois tipos:[1]
O ferro é o metal mais usado, com 95% em peso da produção mundial de metal. É indispensável devido ao seu baixo preço e dureza, especialmente empregado em automóveis, barcos e componentes estruturais de edifícios.
O aço é a liga metálica de ferro mais conhecida, sendo este o seu uso mais frequente. Os aços são ligas metálicas de ferro com outros elementos, tanto metálicos quanto não metálicos, que conferem propriedades distintas ao material. É considerada aço uma liga metálica de ferro que contém menos de 2% de carbono; se a percentagem é maior recebe a denominação de ferro fundido.
As ligas férreas apresentam uma grande variedade de propriedades mecânicas dependendo da sua composição e do tratamento aplicado.
Existem evidências de que o ferro era conhecido antes de 5000 a.C.[2] Os mais antigos objetos feitos de ferro usado pela humanidade são alguns enfeites de siderito, feitos no Egito em aproximadamente 4000 a.C. A invenção da fundição por volta de 3000 a.C. levou ao início da Era do Ferro por volta de 1200 a.C.[3] e ao uso proeminente de ferro para ferramentas e armas.[4]
Cada vez mais objetos de ferro, datados entre o segundo e terceiro milênio antes de Cristo, foram encontrados (estes se distinguem do ferro proveniente dos meteoritos pela ausência de níquel) na Mesopotâmia, Anatólia e Egito. Entretanto, seu uso provável destinou-se a fins cerimoniais, por ter sido um metal muito caro, mais do que o ouro na época. Algumas fontes sugerem que talvez o ferro fosse obtido como subproduto da obtenção do cobre.
Entre 1600 e 1200 a.C., observou-se um aumento de seu uso no Oriente Médio, porém não como substituto ao bronze.
Entre os séculos XII e X antes de Cristo, ocorreu uma rápida transição no Oriente Médio na substituição das armas de bronze pelas de ferro. Esta rápida transição talvez tenha ocorrido devido a uma escassez de estanho e a uma melhoria na tecnologia para trabalhar com o ferro.
Este período, que ocorreu em diferentes ocasiões segundo o lugar, denominou-se Idade do Ferro, substituindo a Idade do Bronze. Na Grécia iniciou-se por volta do ano 1000 a.C., e não chegou à Europa ocidental antes do século VII a.C..
A substituição do bronze pelo ferro foi paulatina, pois era difícil produzir peças de ferro: localizar o mineral, extraí-lo, proceder a sua fundição a temperaturas altas e depois forjá-lo.
Na Europa central, surgiu no século IX a.C. a "cultura de Hallstatt" substituindo a "cultura dos campos de urnas", que se denominou "Primeira Idade do Ferro", pois coincide com a introdução do uso deste metal. Aproximando-se do ano 450 a.C., ocorreu o desenvolvimento da "cultura da Tène", também denominada "Segunda Idade do Ferro". O ferro era usado em ferramentas, armas e joias, embora se continuem encontrando objetos de bronze.
Junto com esta transição de bronze ao ferro descobriu-se o processo de "carburação", que consiste em adicionar carbono ao ferro. O ferro era obtido misturado com a escória contendo carbono ou carbetos, e era forjado retirando-se a escória e oxidando o carbono, criando-se assim o produto já com uma forma. Este ferro continha uma quantidade de carbono muito baixa, não sendo possível endurecê-lo com facilidade ao esfriá-lo em água. Observou-se que se podia obter um produto muito mais resistente aquecendo a peça de ferro forjado num leito de carvão vegetal, para então submergi-lo na água ou óleo. O produto resultante, apresentando uma camada superficial de aço, era menos duro e mais frágil que o bronze.
Na China, o primeiro ferro utilizado também era proveniente dos meteoritos. Foram encontrados objetos de ferro forjado no noroeste, perto de Xinjiang, do século VIII a.C.. O procedimento utilizado não era o mesmo que o usado no Oriente Médio e na Europa.
Nos últimos anos da Dinastia Zhou (550 a.C.), na China,[5] se conseguiu obter um produto resultante da fusão do ferro (ferro fundido). O mineral encontrado ali apresentava um alto conteúdo de fósforo, com o qual era fundido em temperaturas menores que as aplicadas na Europa e outros lugares. Todavia, durante muito tempo, até a Dinastia Qing (aos 221 a.C.), o processo teve uma grande repercussão.
O ferro fundido levou mais tempo para ser obtido na Europa, pois não se conseguia a temperatura necessária. Algumas das primeiras amostras foram encontradas na Suécia, em Lapphyttan e Vinarhyttan, de 1150 a 1350 d.C.
Na Idade Média, e até finais do século XIX, muitos países europeus empregavam como método siderúrgico a "farga catalana". Obtinha-se ferro e aço de baixo carbono empregando-se carvão vegetal e o minério de ferro. Este sistema já estava implantado no século XV, conseguindo-se obter temperaturas de até 1200 °C. Este procedimento foi substituído pelo emprego de altos-fornos.
No princípio se usava carvão vegetal para a obtenção de ferro, como fonte de calor e como agente redutor. No século XVIII, na Inglaterra, o carvão vegetal começou a escassear e tornar-se caro, iniciando-se a utilização do coque, um combustível fóssil, como alternativa. Foi utilizado pela primeira vez por Abraham Darby, no início do século XVIII, construindo em Coalbrookdale um "alto-forno". Mesmo assim, o coque só foi empregado como fonte de energia na Revolução industrial. Neste período a procura foi se tornando cada vez maior devido a sua utilização, como por exemplo, em estradas de ferro.
O alto-forno foi evoluindo ao longo dos anos. Henry Cort, em 1784, aplicou novas técnicas que melhoraram a produção.
Em finais do século XVIII e início do século XIX começou-se a empregar amplamente o ferro como elemento estrutural em pontes, edifícios e outros. Entre 1776 e 1779 se construiu a primeira ponte de ferro fundido por John Wilkinson e Abraham Darby. Na Inglaterra foi empregado pela primeira vez o ferro na construção de edifícios por Mathew Boulton e James Watt, no princípio do século XIX. Também são conhecidas outras obras deste século, como por exemplo, o "Palácio de Cristal" construído para a Exposição Universal de 1851 em Londres, do arquiteto Joseph Paxton, que tem uma armação de ferro, ou a Torre Eiffel, em Paris, construída em 1889 para a Exposição Universal, onde foram utilizadas milhares de toneladas de ferro.
É o metal de transição mais abundante da crosta terrestre, e quarto de todos os elementos. Também é abundante no universo, tendo-se encontrado meteoritos que contêm este elemento. O ferro é encontrado em numerosos minerais, destacando-se:
A hematita (Fe2O3), a magnetita (Fe3O4), a limonita [FeO(OH)], a siderita (FeCO3), a pirita (FeS2) e a ilmenita (FeTiO3).
Pode-se obter o ferro a partir de óxidos com maior ou menor teor de impurezas. Muitos dos minerais de ferro são óxidos.
A redução dos óxidos para a obtenção do ferro é efetuada em um forno denominado alto-forno ou forno alto. Nele são adicionados os minerais de ferro, em presença de coque e carbonato de cálcio, CaCO3, que atua como escorificante.
No processo de obtenção, geralmente é usada a hematita, que apresenta ponto de fusão de 1560 °C. Para que essa temperatura seja diminuída, é adicionado o carbonato de cálcio (CaCO3). Além de promover a redução do ponto de fusão da hematita, ele atua reagindo com impurezas presentes, como o dióxido de silício (SiO2), formando o metassilicato de cálcio (CaSiO3), conhecido como escória. O coque (carbono amorfo, com mais de 90% de pureza) é usado para promover a redução da hematita, transformando o Fe3+ em Fe(s). Inicialmente, o coque, em presença de excesso de O2 fornecido pelo ar, reage produzindo CO2. O dióxido de carbono assim produzido, e também o proveniente do carbonato de cálcio, reagem com o coque que é constantemente adicionado ao alto-forno, produzindo CO. Este, por fim, será o responsável por reagir com o Fe2O3, produzindo Fe(s) e CO2
O processo de oxidação do coque com oxigênio libera energia. Na parte inferior do alto-forno a temperatura pode alcançar 1 900 °C.
Inicialmente, os óxidos de ferro são reduzidos na parte superior do alto-forno, parcial ou totalmente, com o monóxido de carbono, já produzindo ferro metálico. Exemplo: redução da magnetita:
Posteriormente, na parte inferior do alto-forno, onde a temperatura é mais elevada, ocorre a maior parte da redução dos óxidos com o coque (carbono):
O carbonato de cálcio se decompõe:
e o dióxido de carbono é reduzido com o coque a monóxido de carbono, como visto acima.
Na parte mais inferior do alto-forno ocorre a carburação:
Finalmente ocorre a combustão e a dessulfuração (eliminação do enxofre) devido à injeção de ar no alto-forno, e por último são separadas as frações: a escória e o ferro fundido, que é a matéria-prima empregada na indústria do aço.
O ferro obtido pode conter muitas impurezas não desejáveis, sendo necessário submetê-lo a um processo de refinação que pode ser realizado em fornos chamados convertedores.
Em 2004, os cinco países maiores produtores de ferro eram a China, o Brasil, a Austrália, a África do Sul e a Rússia, com 74% da produção mundial.[6]
Artigo principal: Isótopos de ferro
O ferro tem quatro isótopos estáveis naturais: 54Fe, 56Fe, 57Fe e 58Fe. As proporções relativas destes isótopos na natureza são aproximadamente: 54Fe (5,8%), 56Fe (91,7%), 57Fe (2,2%) e 58Fe (0,3%).
O ferro é encontrado em praticamente todos os seres vivos e cumpre numerosas e variadas funções.[7]
Tanto o excesso como a deficiência de ferro podem causar problemas no organismo. O excesso de ferro é chamado de hemocromatose, enquanto que a sua deficiência é conhecida como anemia. A palavra anemia, apesar de estar popularmente associada à carência de ferro no organismo, não é utilizada unicamente para ela. Para a carência de ferro no organismo, cabe o nome específico de anemia ferropriva. Nas transfusões de sangue são usados ligantes que formam com o ferro complexos de alta estabilidade, evitando que ocorra uma queda demasiada de ferro livre. Estes ligantes são conhecidos como sideróforos. Muitos organismos empregam estes sideróforos para captar o ferro de que necessitam. Também podem ser empregados como antibióticos, pois não permitem ferro livre disponível.
Sua carência nos humanos pode causar, além da anemia, anorexia, sensibilidade óssea e a clima frio, prisão de ventre, distúrbios digestivos, tontura, fadiga, problemas de crescimento, irritabilidade, inflamação da língua.
Seu excesso (em nível de nutriente) nos humanos pode causar: igualmente anorexia, tontura, fadiga e dores de cabeça.[8][9]
O tanino presente no chá e café inibem, em 64 e 33% respectivamente, a absorção de ferro. O magnésio inibe a absorção de ferro quando a presença do primeiro é 300 vezes maior do que a do ferro, o zinco quando cinco vezes maior e o cálcio em quantidades superiores a 500 mg, embora a influência do cálcio ainda seja discutida.
A deficiência de vitamina A inibe a utilização do ferro.
Polifenóis se ligam ao ferro e impedem sua absorção.
O ferro é um dos elementos mais facilmente encontráveis na superfície da Terra, mas mesmo assim, sua deficiência é a causa mais comum de anemia, afetando cerca de 500 milhões de pessoas em todo mundo. Isso se deve à capacidade limitada do organismo na absorção de ferro e à frequência da perda de ferro por hemorragia do sistema digestório (úlcera, colite, diverticulite, câncer), menstruação abundante, verminose, múltiplas gestações, estirão de crescimento.
A fonte alimentar do ferro pode ser encontrada em inúmeros alimentos: couve, algas marinhas, brócolis, flocos de aveia, canela moída, grãos integrais ou enriquecidos; nozes; castanhas; feijão vermelho; frutas secas, figo seco, tofu frito com azeite, entre muitos outros.[10] O uso de panelas e recipientes de ferro também contribui para aumentar a ingestão de ferro.[11]
A absorção do ferro, especialmente de origem animal, é aumentada com a ingestão conjunta de alimentos levemente ácidos (ou proteínas) e também por alimentos ricos em ácido ascórbico (vitamina C). Estudos indicam que a absorção de ferro aumenta de 3,7 para 10,4% quando se adiciona a ingestão de 40 a 50 mg de vitamina C, por mantê-lo solúvel. Alguns açúcares como a frutose também colaboram para a absorção de ferro.
O transporte e armazenamento do ferro é mediado por três proteínas - transferrina, receptor de transferrina e ferritina. A transferrina pode conter até dois átomos de ferro. Ela entrega o ferro aos tecidos que têm receptores de transferrina, especialmente eritoblastos na medula óssea, que incorporam o ferro na hemoglobina. A transferência então é reutilizada. No final da sobrevida, ou seja, 120 dias, os eritrócitos são destruídos nos macrófagos do sistema reticuloendotelial dentro do baço; o ferro é liberado da hemoglobina, entra no plasma e fornece a maioria do ferro da transferrina. Somente uma pequena porção do ferro da transferrina plasmática vem da dieta, absorvido no duodeno e no jejuno.
Algum ferro é armazenado nas células reticulares endoteliais, como a ferritina e hemossiderina, em quantidades muito variáveis, conforme a situação das reservas desse elemento no organismo. Em geral a ferritina é um complexo proteico hidrossolúvel de ferro com peso molecular 465 000, é formada de uma concha proteica externa, a apoferrina, que consiste de 22 subunidades, e de um núcleo de hidroxifosfato de ferro. Contém até 20% em peso de ferro e não é visível à microscopia óptica. Cada molécula de apoferrina pode ligar até 4 000 a 5 000 átomos de ferro. A hemossiderina é um complexo proteico insolúvel de ferro, de composição variável, contendo cerca de 37% em peso de ferro. É derivada da digestão lisossômica parcial de agregação de moléculas de ferritina e visível à microscopia óptica nos macrófagos e em outras células após coloração com Perls (azul da Prússia). O ferro na ferritina e na hemossiderina estão na forma férrica [Fe(III)]. Uma enzima que contém cobre, a ceruloplasmina, catalisa a oxidação do ferro para a forma férrica para a ligação na transferrina plasmática.
O ferro também está presente nos músculos, como na mioglobina, e na maioria das células do organismo em enzimas que contêm ferro, como, p. ex., citocromos, desidrogenase succínica, catalase, etc. O ferro tissular tem menos probabilidade de ser depletado que a hemossiderina, a ferritina e a hemoglobina em estado de deficiência de ferro, mas pode ocorrer alguma redução no conteúdo de enzimas contendo heme.
Os níveis de ferritina e os de receptor de transferrina (TfR) correlacionam-se com as reservas de ferro, de modo que a sobrecarga de ferro causa aumento na ferritina tissular e queda no TfR, enquanto na deficiência de ferro a ferritina é baixa, e o TfR, alto. Essa relação surge por intermédio da ligação de uma resposta ao ferro (IREs) na ferritina e nas moléculas de mRNA de TfR. A deficiência de ferro aumenta a capacidade de a IRP ligar-se aos IRES, enquanto a sobrecarga diminui a ligação. O sítio de ligação de IRP em IREs, a montante (5') ou jusante (3') do gene codificador determina aumento ou diminuição da mRNA e, portanto, de proteína.
A quantidade diária de ferro necessária para compensar tanto perdas do organismo como o crescimento varia com a idade e o sexo; é maior na gravidez, na adolescência e nas mulheres que menstruam. Esses grupos, portanto, são particularmente suscetíveis a desenvolver deficiências de ferro quando há perda adicional ou diminuição prolongada da ingestão.
Quando há deficiência de ferro os depósitos reticuloendoteliais (hemossiderina e ferritina) são totalmente depletados antes que ocorra anemia.[12] À medida que a doença evolui, o paciente pode ter sinais e sintomas gerais de anemia como cansaço fácil, fraqueza, irritabilidade, indisposição, sonolência, cefaleia, dor nas pernas e apresentar glossite indolor, estomatite angular, unhas friáveis ou em colher (coiloníquia), cabelos finos, secos e quebradiços, pele seca, esclerótica azulada, mucosas descoradas, disfagia como resultado de membranas faríngeas (Síndrome de Paterson-Kelly ou Plummer-Vinson) e perversão do apetite. A causa das alterações epiteliais não é clara, mas pode ser relacionada à diminuição de ferro nas enzimas que o contêm. Em crianças, a deficiência de ferro é sobremaneira significativa porque causa irritabilidade, má função cognitiva e diminuição no desenvolvimento psicomotor.[13]
Perda crônica de sangue, especialmente uterina e no trato gastrointestinal, é a causa dominante nos adultos. Na infância as causas mais comuns são: nascimento prematuro, gemelaridade, anemia materna durante a gravidez, perda de sangue durante o parto pelo cordão umbilical, pouco aleitamento materno, tomar leite ou derivado durante ou logo após refeições.
Sempre que possível, tratar a causa. Além disso, deve-se administrar ferro via oral, intramuscular ou endovenosa para corrigir a anemia e repor os depósitos. A via oral pode causar cólica, náusea, constipação ou diarreia ou escurecimento dos dentes, mas isso é reversível após a suspensão do medicamento. A via intramuscular deve ser aplicada profundamente e é muito dolorosa. A via endovenosa deve ser feita diluindo a ampola em soro fisiológico e aplicada em no mínimo 30 minutos.
O ferro em excesso é tóxico. O ferro reage com peróxido produzindo radicais livres. A reação mais importante é:
Porém esta mesma reação pode ter aplicação científica e industrial, na chamada Reação de Fenton.
Quando o ferro se encontra nos níveis normais, os mecanismos antioxidantes do organismo podem controlar este processo.
A dose letal de ferro em crianças de 2 anos é de 3 gramas. 1 grama pode provocar um envenenamento importante.
O envenenamento por ferro é denominado hemocromatose. O ferro em excesso se acumula no fígado, provocando danos neste órgão.
Uma avalancha, avalanche ou alude é um fenômeno que se verifica quando uma massa acumulada de neve, gelo e detritos repentinamente se movimenta de forma rápida e violenta e se precipita em direção ao vale. Durante a descida, arrasta cada vez mais neve e pode arrastar árvores, rochas e construções humanas, atingindo até 160 quilômetros por hora. Este destacamento de massas de neve pode ser provocado por diversas causas, como a passagem de esquiadores, a ação de fortes ventos, propagação do som etc.
As avalanches de neve produzem-se quando é perdida sustentação para a neve. Esta perda de sustentação e a avalanche consequente pode ser espontânea ou provocada por ação humana.[1]
Tentativas de modelar o comportante de avalanches datam do começo do século XX, notavelmente o trabalho do Professor Lagotala em preparação às Olimpíadas de Inverno de 1924 em Chamonix.[2] Seu método foi desenvolvido por A. Voellmy e tornou-se popular após sua publicação em 1955 de seu Ueber die Zerstoerungskraft von Lawinen (Sobre a Força Destrutiva das Avalanches).[3]
Voellmy usou uma fórmula empírica simples, tratando uma avalanche como um bloco deslizante de neve movendo-se com uma força de arrasto proporcional ao quadrado da velocidade de seu fluxo:[4]
Ele e outros posteriormente desenvolveram outras fórmulas que levam em conta também outros fatores.[2]
Desde a década de 1990 muitos modelos mais sofisticados foram desenvolvidos.[5]
As avalanches espontâneas produzem-se por um acúmulo excessivo de neve ou por uma mudança nas condições da neve acumulada. Aquelas devidas a acumulações produzem-se em tempo de nevadas fortes e são normalmente de neve recente.[6]
Os aumentos de temperatura[7] e a chuva são as alterações mais frequentes e afetam a neve de qualquer qualidade. O segundo motivo mais frequente das avalanches naturais são as mudanças metamórficas na neve acumulada por causa do derretimento provocado pela radiação solar. Nesse caso a neve aumenta de peso relativo a superfície (isto é, a pressão) até o ponto em que as camadas inferiores não conseguem suportar.[8] Por serem neve recente ou úmida, aumenta a sua fluidez e escorregam sobre outras camadas de neve endurecida.
Por extensão de sentido, hoje uma avalancha pode também significar a rápida precipitação de rochas ou lama. No caso de avalancha de rochas, as causas normalmente são terremotos e pequenos abalos sísmicos. Já quando há uma avalancha de lama, esta pode ser causada pelo derretimento da neve e consequente mistura com a terra ou mesmo em encostas sem neve e atingida por fortes chuvas. De forma geral, as avalanchas de lama estão ligadas à erosão do terreno e desmatamento de bosques e matas nas zonas mais íngremes de montanhas e morros.
Em montanhas cobertas por manto nevoso, o risco de avalanchas é muito baixo em encostas com menos de 25 ou mais de 60 graus de inclinação. O risco maior se encontra em encostas com inclinação entre 35 e 45 graus, atingindo o mais alto risco com 38 graus. É justamente a esta inclinação que a prática do esqui se vê mais favorecida, levando a tragédias que ocorrem anualmente tanto no hemisfério norte como no sul.
A alta periculosidade das avalanchas faz com que em zonas de risco criem-se específicas unidades de prevenção, que observam e analisam as condições meteorológicas e da neve para avaliar o risco. Nos locais mais expostos, é necessário provocar o deslizamento controlado das massas de neve instáveis por meio de cargas explosivas. O sistema mais vulgar para diminuir os efeitos da avalanches é a colocação de barras metálicas dispostas verticalmente ou os muros de desvio, para reduzir as massas de neve.[9]
Para permitir a circulação do transito ferroviário e/ou automóvel, nas zonas de montanha utiliza-se as chamadas protecções para-avalanche se bem que a finalidade não seja a de as parar, mas sim as  proteger da avalanche. São como que um "telhado", uma galeria aberta, que cobre a via e permite que a neve possa passar por cima sem portanto provocar a paragem do tráfego. Nas zonas onde as avalanches são frequentes fala-se de corredores de avalanche.[9]
Segundo informações dos bombeiros da Catalunha, para aproveitar uma saída à neve com segurança é necessário prepará-la com antecipação, conhecer o percurso (buscando o caminho que seja mais seguro),[10] conhecer a previsão meteorológica[11] e o risco de avalanches, e levar o equipamento adequado.[12] Segundo as recomendações da Proteção Civil da Catalunha, há que se ter em conta que as vibrações produzidas por um berro, queda ou movimento no chão podem iniciar uma avalanche.[13]
Na Europa, o risco de avalanchas é aferido utilizando-se a seguinte escala, adotada desde abril de 1993, substituindo esquemas anteriores não padronizados. Em maio de 2003 a tabela foi atualizada para aumentar sua uniformidade e precisão.[14]
Na física, avalanche é o processo ocorrente num gás em que há um campo elétrico, e que consiste na multiplicação do número de íons e elétrons formados num evento ionizante, graças aos choques inelásticos sucessivos dos íons acelerados pelo campo com as moléculas do gás.
Em eletrônica, existe o fenômeno da avalanche térmica que é a autodestruição de componentes que ocorre devido ao aumento de uma corrente parasita chamada corrente de fuga, que descontrolada aumenta a temperatura do componente, e em função do aumento da temperatura, aumenta a corrente de fuga, formando um sistema autoalimentado até a autodestruição.
Em meteorologia, tempo é o estado da atmosfera num determinado momento, que pode ser interpretado sob as escalas convencionais que podem considerar a atmosfera como quente ou fria, úmida ou seca, calma ou tempestuosa, limpa ou nublada.[1] A maior parte dos eventos meteorológicos ocorre na troposfera,[2][3] a camada mais baixa da atmosfera terrestre. O tempo pode se referir, geralmente, às mudanças cotidianas na temperatura e na precipitação, onde o clima é o termo empregado para se referir às condições atmosféricas médias ao longo de um período mais prolongado de tempo.[4] Quando o termo é usado sem nenhuma qualificação, "tempo" é entendido como sendo o tempo da Terra.
Os fenômenos meteorológicos ocorrem devido às diferenças de temperatura, pressão atmosférica ou umidade do ar entre uma massa de ar e outra. Um dos principais motores de formação de eventos meteorológicos de escala global é a diferença do ângulo de radiação solar entre a linha do Equador e os polos e a consequente diferença de temperatura entre estas regiões: a região equatorial recebe a incidência solar diretamente, perpendicular à superfície, enquanto que as regiões polares recebem a incidência solar praticamente em paralelo à superfície, tornando a radiação solar mais difusa e com um poder menor de aquecimento. O intenso contraste de temperatura entre as regiões trópicas e polares geram as correntes de jato nas regiões temperadas, e as perturbações nessas correntes de jato podem vir a  gerar ciclones extratropicais. Devido ao eixo da Terra estar inclinado em relação ao plano orbital, o ângulo de incidência da luz solar varia conforme o progresso do ano. Na superfície terrestre, a temperatura normalmente varia entre -40 °C e 40 °C anualmente. Por milênios, as mudanças na órbita terrestre afetam a quantidade e a distribuição da radiação solar recebida pela Terra e influenciam o clima em um longo prazo.
As diferenças da temperatura na superfície causam diferenças de pressão atmosférica. Altitudes altas são mais frias do que altitudes baixas devido às diferenças de calor nas diferenças da densidade da atmosfera. A previsão do tempo é a aplicação da meteorologia para predizer o estado da atmosfera em um momento futuro próximo e em um determinado local. A atmosfera é um sistema caótico, e, portanto, pequenas mudanças na atmosfera podem se multiplicar e ter grandes efeitos no sistema como um todo. As tentativas humanas de manipulação do tempo têm ocorrido em toda a história, e há evidência de que a atividade humana, como a agricultura e a indústria, têm modificado inadvertidamente os padrões meteorológicos.
O estudo de como o tempo ocorre em outros planetas têm sido de ajuda no entendimento de como o tempo ocorre na Terra. Um famoso fenômeno meteorológico extraterrestre no Sistema Solar é a Grande Mancha Vermelha de Júpiter, que se trata de um anticiclone que tem a existência conhecida por mais de 300 anos. Entretanto, o tempo não é limitado aos corpos planetários. A coroa solar está constantemente sendo perdida para o espaço, criando o que é essencialmente uma atmosfera muito tênua em torno do Sol, que é a região conhecida como a heliosfera. O movimento de massa ejetada do Sol é conhecido como vento solar.
Na Terra, os fenômenos meteorológicos mais comuns incluem o vento, as nuvens, a chuva, a neve, o nevoeiro e as tempestades de areia. Eventos meteorológicos menos comuns incluem desastres naturais, como tornados, ciclones tropicais e tempestades de gelo. Praticamente todos os fenômenos meteorológicos cotidianos ocorrem na troposfera,[3] a parte mais baixa da atmosfera terrestre. Manifestações meteorológicas podem ocorrer na estratosfera, a camada da atmosfera terrestre logo acima da troposfera, e tais eventos podem modificar os fenômenos ocorridos na troposfera, mas os mecanismos físicos envolvidos praticamente não são compreendidos atualmente.[5]
Os fenômenos meteorológicos ocorrem devido às diferenças de temperatura, pressão atmosférica e umidade do ar entre uma massa de ar e outra. Estas diferenças podem ocorrer devido ao ângulo de incidência da radiação solar, que varia conforme a latitude e tem como consequência a diferença da quantidade de insolação por unidade de área na superfície. Em outras palavras, quanto mais longe a localidade estiver da linha do Equador, menos insolação a localidade irá receber, pois o ângulo de incidência da radiação solar deixará de ser perpendicular à superfície e tenderá a decrescer conforme se avança em direção aos polos.[6] O intenso contraste entre a temperatura nas regiões trópicas para as regiões polares causa a formação das correntes de jatos nas regiões de latitude média.[7] Boa parte dos fenômenos meteorológicos ocorridos nestas regiões do planeta é devida à instabilidades das correntes de jatos, que podem levar à formação de eventos meteorológicos como os ciclones extratropicais (devido à processos baroclínicos).[8] Os fenômenos meteorológicos ocorridos nas regiões trópicas, como as monções ou tempestades organizadas, como os ciclones tropicais, são causadas por diferentes processos.
Devido ao eixo da Terra estar inclinado em relação ao plano orbital, a luz solar incide em diferentes ângulos conforme o progresso do ano. Em junho no hemisfério norte, o eixo de rotação da terra está inclinada em direção ao Sol, e o ângulo de incidência da radiação solar está mais perpendicular, além da duração do dia estar mais prolongada. Com isso, o hemisfério norte, em junho, tende a receber mais calor proveniente do Sol do que o hemisfério sul, distinguindo o período do ano conhecido como verão. Em dezembro, o processo se inverte e o hemisfério norte recebe menos radiação solar, distinguindo esta época do ano como o inverno.[9] O verão e o inverno são duas das quatro estações; os períodos intermediários são conhecidos como outono e primavera. Por milênios, as mudanças nos parâmetros orbitais da Terra afetam a quantidade e a distribuição de radiação solar recebida pela Terra e influenciam o clima em um longo prazo. Tais mudanças no clima podem ser periódicas, como os ciclos de Milankovitch.[10]
As mudanças na quantidade e distribuição na radiação solar podem ser causadas pela própria influência do tempo meteorológico, como a formação de zonas de gradiente de temperatura e de umidade do ar, e a consequente formação de nuvens e de precipitação.[11] Altitudes altas são mais frias do que altitudes baixas, o que é explicado pelo gradiente adiabático, ou seja, quanto menor a densidade da atmosfera, menor a temperatura ambiente.[12][13] Em escalas locais, as diferenças de temperatura podem ocorrer devido às diferenças de superfície, como os oceanos, florestas, geleiras ou paisagens modificadas artificialmente. Tais superfícies têm diferentes características físicas, como a refletividade, aspereza ou umidade.
As diferenças de temperatura na superfície causam diferenças na pressão atmosférica. Uma superfície quente aquece o ar logo acima, e essa massa de ar aquecida expande-se, diminuindo a pressão atmosférica e sua densidade.[14] O gradiente barométrico horizontal resultante acelera o ar de zonas de alta pressão para zonas de baixa pressão, criando o vento, e a rotação da Terra causa então a curvatura das correntes de vento por meio da força de Coriolis.[15] Os sistemas meteorológicos simples assim formados podem exibir comportamento emergente para produzir sistemas mais complexos e assim  produzir outros fenômenos meteorológicos. Exemplos de grande escala incluem a célula de Hadley, e um exemplo de escala menor inclui a brisa litorânea.
A atmosfera é um sistema caótico, e, portanto, pequenas alterações no sistema podem ter grandes consequências do sistema como um todo.[16] Isso dificulta a previsão do tempo de forma mais apurada para um período maior do que alguns dias no futuro, embora os meteorologistas estejam continuamente trabalhando para estender este limite por meio do estudo científico da meteorologia. É teoricamente impossível fazer previsões cotidianas úteis para um período maior do que duas semanas, impondo um limite superior para a capacidade da realização de previsões do tempo.[17]
A teoria do caos diz que a menor variação de um sistema no ponto de partida pode crescer e evoluir, afetando o sistema como um todo após um período de tempo. Esta ideia é às vezes chamada de efeito borboleta, derivada da ideia de que o bater de asas de uma borboleta pode produzir grandes mudanças no estado da atmosfera. Devido à sensibilidade das pequenas mudanças, nunca é possível realizar previsões perfeitas do tempo.
O tempo tem sido de grande interferência, e às vezes direta, na história da humanidade. Ao lado de mudanças climáticas que têm causado o deslocamento gradual de massas populacionais, como a desertificação do Oriente Médio, e a formação de pontes terrestres durante o período glacial, episódios de tempo severo têm causado a migração, em escala menor, de massas populacionais, e com isso, ficaram registrados permanentemente na história da meteorologia e da população afetada. Um desses eventos meteorológicos extremos foi o impedimento do avanço das frotas mongóis, lideradas por Kublai Khan, pelos ventos "Kamikaze" em 1281.[18] A intenção de possessão da Flórida pela França veio a um fim em 1565 quando um furacão destruiu a frota naval francesa, permitindo assim que a Espanha conquistasse Fort Caroline.[19] Mais recentemente, o furacão Katrina, em 2005, causou a saída permanente de mais de um milhão de pessoas da costa do Golfo dos Estados Unidos, tornando-se a maior diáspora da história americana.[20]
A Pequena Idade do Gelo levou a quebra na colheita e levou a grandes ondas de fome na Europa. Na Finlândia, a fome de 1696-97 matou cerca de um terço da população.[21]
Embora o tempo afete as pessoas de maneiras drásticas, também pode afetar de maneiras mais simples. O corpo humano é afetado negativamente por extremos na temperatura, umidade e vento.[22]
Também, há interferência do tempo em outros seres vivos. Como por exemplo na população de insetos: identificou-se que a flutuação populacional da comunidade de besouros é regida predominantemente pelas variáveis temporais da região em que se encontram e que a sua presença é estacional e condicionada por parâmetros meteorológicos, principalmente insolação e umidade relativa do ar.[23]
A previsão do tempo é uma das aplicações da meteorologia para prever o estado da atmosfera em um tempo futuro e em um determinado local. A humanidade tem tentado prever o tempo por milênios, mas a meteorologia começou a ser empregada para as previsões do tempo a partir do século XIX. As previsões meteorológicas são feitas através da coleta de dados sobre o estado atual da atmosfera terrestre, e com a compreensão científica dos processos atmosféricos para projetar como o tempo irá evoluir.[24]
A plataforma principal para a previsão numérica do tempo é a análise da pressão atmosférica e as causas de sua mudança, além de seus desdobramentos.[25] Para isso, foram criados modelos meteorológicos capazes de acompanhar o movimento das massas de ar com diferentes pressões atmosféricas, as suas relações (gradientes de pressão), além de associar a temperatura e a umidade do ar. Tais modelos meteorológicos são capazes de determinar o comportamento da atmosfera para um curto período de tempo no futuro.[24] No entanto, não é possível, com a atual tecnologia, prever todos os desdobramentos da atmosfera; a atmosfera apresenta um comportamento caótico, isto é, um pequeno fator, que pode ser menor do que a margem de erro dos dados numéricos, pode desencadear eventos imprevisíveis.[26] Para minimizar tais erros, é necessário uma massiva coleta de dados numéricos, e as suas interconexões são processadas por supercomputadores. Entretanto, a dinâmica da atmosfera ainda não é totalmente compreendida, e a previsão torna-se cada vez mais imprecisa conforme se aumenta o período de tempo no futuro; os modelos meteorológicos atuais são capazes de prever certos eventos apenas em um período de quinze dias no futuro, e os modelos climáticos não podem prever eventos que poderão vir a ocorrer a mais de oito meses no futuro. Para amenizar os erros, vários modelos meteorológicos são usados em conjunto, estabelecendo-se um consenso entre estes modelos.[24]
Há uma grande variedade de finalidades para a previsão do tempo. Os avisos de tempo severo são importantes para preservar a vida humana e a economia. As previsões baseadas na temperatura e precipitação são importantes na agricultura. A previsão da temperatura também é importante na previsão, por exemplo, da demanda da energia elétrica ou de água para os dias vindouros. O cotidiano das pessoas pode ser alterado conforme a previsão do tempo. As atividades ao ar livre, como a construção civil, também são influenciadas pela previsão do tempo.
A aspiração da manipulação do tempo é evidente em toda a história humana: desde os rituais antigos para trazer chuva às plantações, até a intervenção das Forças Armadas dos Estados Unidos, batizada de Operação Popeye, uma tentativa de corromper a linha de suprimentos militares por meio da tentativa da intensificação e alongamento da estação monçonal no Vietnã do Norte. As tentativas de maior sucesso na manipulação do tempo envolveram a semeadura de nuvens; incluem técnicas de dispersão de nevoeiros e stratus baixos realizadas pelos maiores aeroportos. Também podem incluir a tentativa da indução de uma maior precipitação de neve sobre as montanhas, além de técnicas para evitar o granizo.[27] Um exemplo recente de controle do tempo ocorreu durante as Jogos Olímpicos de Verão de 2008. A China disparou 1 104 foguetes dispersores de chuva de 21 localidades de Pequim, em uma tentativa de afastar qualquer área de chuva do local da Cerimônia de Abertura dos Jogos Olímpicos em 8 de agosto.[28]
Embora haja evidências inconclusivas sobre a eficácia destas técnicas, há extensas evidências de que a atividade humana, como a agricultura e a indústria, resulta na modificação inadvertida do tempo:[27]
As mudanças climáticas causadas pelas atividades humanas que emitem gases do efeito estufa podem afetar a frequência de eventos meteorológicos extremos, como secas, temperaturas extremas, enchentes, fortes ventos e tempestades severas.[29]
Os efeitos da modificação inadvertida do tempo podem causar ameaças sérias a muitos aspectos da civilização, incluindo ecossistemas, recursos naturais, produção agrícola, desenvolvimento econômico e saúde humana.[30]
Na Terra, a temperatura varia normalmente entre -40 °C a 40 °C anualmente. A variedade de climas e latitudes no planeta pode oferecer extremos de temperatura que se situam fora da variação normal da temperatura. A temperatura do ar mais baixa já registrada na Terra é de -89,2 °C na Estação Vostok, Antártida, em 21 de julho de 1983. A temperatura mais alta do ar já registrada é de 57,7 °C em Al 'Aziziyah, Líbia, em 13 de setembro de 1922.[31] A maior média anual de temperatura é de 34,4 °C em Dallol, Etiópia.[32] A menor média anual de temperatura é de -55,1 °C, também na Estação Vostok, Antártida.[33]  Mas a menor média anual de temperatura em uma localidade permanentemente habitada é de -19,7 °C em Eureka, Canadá.[34]
O estudo de como o tempo ocorre em outros planetas tem sido visto como útil no entendimento de como o tempo ocorre na Terra.[35] O tempo em outros planetas segue muitos dos mesmos princípios físicos do tempo na Terra, mas ocorre em diferentes escalas e em atmosferas de diferentes composições. A missão Cassini-Huygens para Titã descobriu nuvens formadas de metano ou etano que se precipitam em metano líquido ou em outros compostos orgânicos.[36] A atmosfera terrestre inclui seis zonas de circulação latitudinais, três em cada hemisfério.[37] Em contraste, Júpiter parece possuir muitas de tais zonas de circulação, claramente visíveis devido às diferenças latitudinais de coloração da atmosfera.[38] Titã tem apenas uma corrente de jato, situada aos arredores do paralelo 50°N do satélite,[39] enquanto que Vênus tem apenas uma única corrente de jato perto do seu equador.[40]
Uma das mais famosas manifestações meteorológicas fora da Terra é a Grande Mancha Vermelha de Júpiter, que se trata de um imenso anticiclone que tem a sua existência comprovada por mais de 300 anos.[41] Em outros planetas gasosos, a ausência de superfícies sólidas permite que o vento alcance enormes velocidades: rajadas de mais de 600 metros por segundo (cerca de 2 100 km/h) foram medidas em Netuno.[42]  Isto criou um quebra-cabeças para os cientistas planetares. O tempo é basicamente criado pela energia solar, e Netuno recebe apenas cerca de 1/900 da energia recebida pela Terra, mesmo assim a intensidade do fenômeno meteorológico em Netuno é muito maior do que o observado na Terra.[43] Os ventos mais fortes já registrados fora da Terra foram observados no planeta extrassolar HD 189733 b, que se estima que tenha ventos que se movem a mais de 9 600 km/h.[44]
O tempo não está limitado aos limites planetários. A coroa solar está constantemente perdida para o espaço, criando o que é essencialmente uma atmosfera muito tênua em todo o Sistema Solar. O movimento da massa ejetada pelo Sol é conhecido como vento solar. Inconsistências desses ventos e grandes eventos na superfície solar, como a ejeção de massa coronal, formam sistemas que têm características análogas aos sistemas meteorológicos convencionais (como o vento e pressão atmosférica) e é conhecido geralmente como tempo espacial. Ejeções de massa coronal foram detectados tão longe do Sol quanto Saturno.[45] A atividade desses sistemas meteorológicos espaciais pode afetar atmosferas planetárias e ocasionalmente as superfícies. A interação do vento solar como a atmosfera terrestre pode produzir espetaculares auroras polares,[46] entretanto pode danificar sistemas elétricos sensíveis como linhas de transmissão e sinais de rádio.[47]
A radiação eletromagnética é uma oscilação em fase dos campos elétricos e magnéticos, que, autossustentando-se, encontram-se desacoplados das cargas elétricas que lhe deram origem. As oscilações dos campos magnéticos e elétricos são perpendiculares entre si e podem ser entendidas como a propagação de uma onda transversal, cujas oscilações são perpendiculares à direção do movimento da onda (como as ondas da superfície de uma lâmina de água), que pode se deslocar através do vácuo. Dentro do ponto de vista da Mecânica Quântica, podem ser entendidas, ainda, como o deslocamento de pequenas partículas, os fótons.
O espectro visível, ou simplesmente luz visível, é apenas uma pequena parte de todo o espectro da radiação eletromagnética possível, que vai desde as ondas de rádio aos raios gama. A existência de ondas eletromagnéticas foi prevista por James Clerk Maxwell e confirmada experimentalmente por Heinrich Hertz. A radiação eletromagnética encontra aplicações como a radiotransmissão, seu emprego no aquecimento de alimentos (fornos de micro-ondas), em lasers para corte de materiais ou mesmo na simples lâmpada incandescente.
A radiação eletromagnética pode ser classificada de acordo com a frequência da onda, em ordem crescente, nas seguintes faixas: ondas de rádio, micro-ondas, radiação terahertz, radiação infravermelha, luz visível, radiação ultravioleta, raios X e radiação gama.
No que tange às fontes de radiação, houve muitas controvérsias sobre se uma carga acelerada poderia irradiar ou não. Em parte por causa do princípio da equivalência e a nulidade da reação de radiação observada nos cálculos quando a fonte é submetida à aceleração uniforme.[1][2][3]
As ondas eletromagnéticas primeiramente foram previstas teoricamente por James Clerk Maxwell e depois confirmadas experimentalmente por Heinrich Hertz. Maxwell notou as ondas a partir de equações de electricidade e magnetismo, revelando sua natureza e sua simetria. Faraday mostrou que um campo magnético variável no tempo gera um campo eléctrico. Maxwell mostrou que um campo eléctrico variável com o tempo gera um campo magnético, com isso há uma autossustentação entre os campos eléctrico e magnético. Em seu trabalho de 1862, Maxwell escreveu:
"A velocidade das ondas transversais em nosso meio hipotético, calculada a partir dos experimentos electromagnéticos dos Srs. Kohrausch e Weber, concorda tão exactamente com a velocidade da luz, calculada pelos experimentos óticos do Sr. Fizeau, que é difícil evitar a inferência de que a luz consiste nas ondulações transversais do mesmo meio que é a causa dos fenômenos eléctricos e magnéticos."[carece de fontes?]
Uma onda harmônica é uma onda com a forma de uma função senoidal, como na figura, no caso de uma onda que se desloca no sentido positivo do eixo dos .
A distância  entre dois pontos consecutivos onde o campo e a sua derivada têm o mesmo valor, é designada por comprimento de onda (por exemplo, a distância entre dois máximos ou mínimos consecutivos). O valor máximo do módulo do campo, , é a sua
amplitude.
O tempo que a onda demora a percorrer um comprimento de onda designa-se por {período}, .
O inverso do período é a frequência , que indica o número de comprimentos de onda que passam por um ponto, por unidade de tempo. No sistema SI a unidade da frequência é o hertz, representado pelo símbolo Hz, equivalente a .
No caso de uma onda eletromagnética no vácuo, a velocidade de propagação é  que deverá verificar a relação:
A equação da função representada na figura acima é:
onde a constante  é a fase inicial. Essa função representa a forma da onda num instante inicial, que podemos admitir .
Para obter a função de onda num instante diferente, teremos que substituir  por
, já que a onda se propaga no sentido positivo do eixo dos
, com velocidade .
usando a relação entre a velocidade e o período, podemos escrever:
Se substituirmos , obteremos a equação que descreve o campo elétrico na origem, em função do tempo:
assim, o campo na origem é uma função sinusoidal com período  e amplitude . O campo em outros pontos tem exatamente a mesma forma sinusoidal, mas com diferentes valores da fase.[4]
Os campos eléctrico e magnético obedecem aos princípios da superposição de ondas, de modo que seus vectores se cruzam e criam os fenômenos da refracção e da difração.[carece de fontes?] Uma onda eletromagnética  pode interagir com a matéria e, em particular, perturbar átomos e moléculas que as absorvem, podendo os mesmos emitir ondas em outra parte do espectro.
Como qualquer fenômeno ondulatório, as ondas eletromagnéticas podem interferir entre si. Sendo a luz uma oscilação, ela não é afetada pela estática eléctrica ou por campos magnéticos de uma outra onda eletromagnética no vácuo. Em um meio não linear, como um cristal, por exemplo, interferências podem acontecer e causar o efeito Faraday, em que a onda pode ser dividida em duas partes com velocidades diferentes.[carece de fontes?]
Na refracção, uma onda, transitando de um meio para outro de densidade diferente, tem alteradas sua velocidade e sua direcção (caso esta não seja perpendicular à superfície) ao entrar no novo meio. A relação entre os índices de refracção dos dois meios determina a escala de refração medida pela lei de Snell:
Nesta equação, i é o ângulo de incidência, N1 é o índice de refração do meio 1, r é o ângulo de refração, e N2 é o índice de refração do meio 2.
A luz se dispersa em um espectro visível porque é reflectida por um prisma, devido ao fenômeno da refração. As características das ondas eletromagnéticas demonstram as propriedades de partículas e da onda ao mesmo tempo, e se destacam mais quando a onda é mais prolongada.
Um importante aspecto da natureza da luz é a frequência uma onda, sua taxa de oscilação. É medida em hertz, a unidade SIU de frequência, na qual um hertz (1,00 Hz) é igual a uma oscilação por segundo. A luz normalmente tem um espectro de frequências que, somadas, juntos formam a onda resultante. Diferentes frequências formam diferentes ângulos de refração. Uma onda consiste nos sucessivos baixos e altos, e a distância entre dois pontos altos ou baixos é chamado de comprimento de onda. Ondas eletromagnéticas variam de acordo com o tamanho, de ondas de tamanhos de prédios a ondas gama pequenas menores que um núcleo atômico. A frequência é inversamente proporcional ao comprimento da onda, de acordo com a equação:
.
Nesta equação, v é a velocidade, λ (lambda) é o comprimento de onda, e f é a frequência da onda.
Na passagem de um meio material para outro, a velocidade da onda muda, mas a frequência permanece constante. A interferência acontece quando duas ou mais ondas resultam em um novo padrão de onda. Se os campos tiverem as componentes nas mesmas direções, uma onda "coopera" com a outra (interferência construtiva); entretanto, se estiverem em posições opostas, pode haver uma interferência destrutiva.
Um feixe luminoso é composto por pacotes discretos de energia, caracterizados por consistirem em partículas denominadas fotões (português europeu) ou fótons (português brasileiro). A frequência da onda é proporcional à magnitude da energia da partícula. Como os fótons são emitidos e absorvidos por partículas, eles actuam como transportadores de energia. A energia de um fóton é calculada pela equação de Planck-Einstein:
Nesta equação, E é a energia, h é a constante de Planck, e f é a frequência.
Se um fóton for absorvido por um átomo, ele excita um electrão (português europeu) ou elétron (português brasileiro), elevando-o a um alto nível de energia. Se o nível de energia é suficiente, ele pula para outro nível maior de energia, podendo escapar da atração do núcleo e ser liberado em um processo conhecido como fotoionização. Um elétron que descer ao nível de energia menor emite um fóton de luz igual a diferença de energia. Como os níveis de energia em um átomo são discretos, cada elemento tem suas próprias características de emissão e absorção.[carece de fontes?]
O espectro eletromagnético é classificado normalmente pelo comprimento da onda, como as ondas de rádio, as micro-ondas, a radiação infravermelha, a luz visível, os raios ultravioleta, os raios X, até a radiação gama.
O comportamento da onda eletromagnética depende do seu comprimento de onda. Ondas com frequências altas possuem comprimento de onda curto e, por outro lado, ondas com frequências baixas possuem comprimento de onda longo . Quando uma onda interage com uma única partícula ou molécula, seu comportamento depende da quantidade de fótons por ela carregada.[carece de fontes?] Através da técnica denominada Espectroscopia óptica, é possível obter-se informações sobre uma faixa visível mais larga do que a visão normal. Um espectroscópio comum pode detectar comprimentos de onda de 2 nm a 2 500 nm.
Essas informações detalhadas podem informar propriedades físicas dos objetos, gases e até mesmo estrelas. Por exemplo, um átomo de hidrogênio emite ondas em comprimentos de 21,12 cm. A luz propriamente dita corresponde à faixa que é detectada pelo olho humano, entre 400 nm a 700 nm (um nanômetro vale 1,0×10−9 metro). As ondas de rádio são formadas de uma combinação de amplitude, frequência e fase da onda com a banda da frequência.
A radiação de corpo negro, também conhecida por radiação térmica, é a radiação eletromagnética emitida por um corpo em qualquer temperatura,[5] constituindo uma forma de transmissão de calor, ou seja, por meio deste tipo de radiação ocorre transferência de energia térmica na forma de ondas eletromagnéticas. Quando a matéria emite e absorve perfeitamente qualquer comprimento de onda e está em equilíbrio termodinâmico, considera-se que é um corpo negro, e sua radiação é chamada de radiação de corpo negro.[6]
A energia cinética de átomos e moléculas varia, converte-se em energia térmica e resulta na radiação eletromagnética térmica. Como as ondas eletromagnéticas também podem se propagar no vácuo, a transferência de calor de um corpo a outro ocorre mesmo se não existir meio material entre os dois, como é o caso da energia emitida pelo Sol e que chega à Terra.
A Lei de Wien relaciona o comprimento de onda em que há máxima emissão de radiação de corpo negro com uma temperatura e determina que o comprimento de onda emitido diminui com o aumento da temperatura. A Lei de Planck para radiação de corpo negro exprime a radiância espectral em função do comprimento de onda e da temperatura do corpo negro e fornece a distribuição dos comprimentos de onda no espectro em função da temperatura. A maior parte da irradiação ocorre em um comprimento de onda específico, chamado de comprimento de onda principal de irradiação, que depende da temperatura do corpo. Quanto maior a temperatura, maior a frequência da radiação e menor o comprimento de onda.
A radiação eletromagnética de comprimentos de onda diferentes da luz visível foi descoberta no início do século XIX. A descoberta da radiação infravermelha é atribuída ao astrônomo William Herschel, que publicou seus resultados em 1800 perante a Royal Society de Londres.[7] Herschel usou um prisma de vidro para refratar a luz do Sol e detectou raios invisíveis que causavam aquecimento além da parte vermelha do espectro, por meio de um aumento na temperatura registrada com um termômetro. Esses "raios caloríficos" foram posteriormente denominados infravermelhos.
Em 1801, o físico alemão Johann Wilhelm Ritter descobriu o ultravioleta em um experimento semelhante ao de Herschel, usando luz solar e um prisma de vidro. Ritter observou que os raios invisíveis perto da borda violeta de um espectro solar dispersos por um prisma triangular escureceram as preparações de cloreto de prata mais rapidamente do que a luz violeta próxima. Os experimentos de Ritter foram um precursor do que se tornaria a fotografia. Ritter observou que os raios ultravioleta (que a princípio eram chamados de "raios químicos") eram capazes de causar reações químicas.[8]
Em 1862-64, James Clerk Maxwell desenvolveu equações para o campo eletromagnético que sugeriam que as ondas no campo viajariam com uma velocidade muito próxima da velocidade conhecida da luz. Maxwell, portanto, sugeriu que a luz visível (assim como os raios infravermelhos e ultravioletas invisíveis por inferência) consistiam na propagação de distúrbios (ou radiação) no campo eletromagnético. As ondas de rádio foram produzidas pela primeira vez deliberadamente por Heinrich Hertz em 1887, usando circuitos elétricos calculados para produzir oscilações a uma frequência muito mais baixa do que a da luz visível, seguindo receitas para produzir cargas e correntes oscilantes sugeridas pelas equações de Maxwell. Hertz também desenvolveu maneiras de detectar essas ondas e produziu e caracterizou o que mais tarde foi denominado ondas de rádio e micro-ondas.[9]
Wilhelm Röntgen descobriu e nomeou os raios X. Depois de experimentar altas voltagens aplicadas a um tubo evacuado em 8 de novembro de 1895, ele notou uma fluorescência em uma placa próxima de vidro revestido. Em um mês, ele descobriu as principais propriedades dos raios X.[10]
A última porção do espectro EM a ser descoberta estava associada à radioatividade. Henri Becquerel descobriu que os sais de urânio causavam o embaçamento de uma chapa fotográfica não exposta através de um papel de cobertura de maneira semelhante aos raios X, e Marie Curie descobriu que apenas certos elementos emitiam esses raios de energia, logo descobrindo a intensa radiação do rádio. A radiação da pechblenda foi diferenciada em raios alfa (partículas alfa) e raios beta (partículas beta) por Ernest Rutherford através de uma simples experimentação em 1899, mas estes provaram ser tipos de radiação de partículas carregadas. No entanto, em 1900, o cientista francês Paul Villard descobriu um terceiro tipo de radiação do rádio de carga neutra e especialmente penetrante, e depois de descrevê-lo, Rutherford percebeu que deveria ser ainda um terceiro tipo de radiação, que em 1903 Rutherford chamou de raios gama. Em 1910, o físico britânico William Henry Bragg demonstrou que os raios gama são radiação eletromagnética, não partículas, e em 1914 Rutherford e Edward Andrade mediram seus comprimentos de onda, descobrindo que eram semelhantes aos raios X, mas com comprimentos de onda mais curtos e frequência mais alta, embora um 'cross-over' entre os raios X e gama torna possível ter raios X com uma energia mais alta (e, portanto, comprimento de onda mais curto) do que os raios gama e vice-versa. A origem do raio os diferencia, os raios gama tendem a ser fenômenos naturais originados do núcleo instável de um átomo e os raios X são gerados eletricamente (e, portanto, feitos pelo homem), a menos que sejam resultado da radiação X bremsstrahlung causada por a interação de partículas em movimento rápido (como partículas beta) colidindo com certos materiais, geralmente de números atômicos mais altos.[11]
Entre inúmeras aplicações destacam-se o rádio, a televisão, radares, os sistemas de comunicação sem fio (telefonia celular e comunicação wi-fi), os sistemas de comunicação baseados em fibras ópticas e fornos de micro-ondas. Existem equipamentos para a esterilização de lâminas baseados na exposição do instrumento a determinada radiação ultravioleta, produzida artificialmente por uma lâmpada de luz negra.[carece de fontes?]
Massa é um conceito utilizado em ciências naturais. Em particular, a massa é frequentemente associada ao peso dos objetos. Esta associação não se mostra na maioria das vezes, entretanto, correta, ou quando correta, não se mostra completamente elucidativa. Em acordo com o paradigma científico moderno, o peso de um objeto resulta da interação gravitacional entre sua massa e um campo gravitacional:[1] ao passo que a massa é parte integrante da explicação para o peso, ela sozinha não constitui a explicação completa. Os trajes espaciais dos astronautas, quando usados aqui na Terra, parecem consideravelmente mais pesados do que quando usados na superfície da Lua, contudo suas massas permanecem exatamente as mesmas.
É comum também a associação de massa ao tamanho e forma de um objeto. Massa realmente toma parte na explicação para o tamanho dos objetos (densidade), mas não constitui a explicação correta ou completa. A massa é uma grandeza escalar e não é um vetor.[2]
O corpo humano é equipado com vários sentidos com os quais estabelecemos a compreensão do mundo que nos cerca. Em primeira instância é às sensações que eles nos fornecem que naturalmente associamos certos conceitos e definições, a citar os conceitos intuitivos de temperatura, tamanho, resistência, peso, massa, e outros. O conceito intuitivo de massa que desenvolvemos encontra-se intimamente ligado a eles. Entretanto sabe-se hoje que nossos sentidos são mestres em nos enganar — quem nunca viu uma ilusão de ótica? — e que eles também não têm grande precisão. Se um punhado de balas for colocado em uma de suas mãos, e se uma for retirada do topo da pilha, você certamente não dará por falta desta se confiar apenas na sensação do peso que seu tato lhe confere.[3]
Como se deduz, para a correta compreensão do mundo que nos cerca não podemos confiar em nossos sentidos. Para alcançá-la devemos confiar em algo mais avançado, a saber, no poder de abstração que temos e em informações fornecidas por aparelhos especificamente projetados para obtê-las. Dentro deste contexto, que culminou no que chamamos hoje ciência, o conceito abstrato de massa evoluiu juntamente com a nossa compreensão do mundo natural, mas mesmo nos dias de hoje mostra-se essencial ainda na forma com a qual se consolidou pela primeira vez: o primeiro conceito científico de massa com o qual nos deparamos na escola — o de massa como medida da inércia, da maior ou menor oposição que um corpo impõe à mudança em seu estado de movimento (F=m.a) — ainda é o fornecido pela mecânica newtoniana, mas a partir dele podemos hoje encontrar no mínimo sete definições diferentes de massa, e em verdade, dentro da teoria mais geral para o estudo da dinâmica dos corpos (a Relatividade Geral), podemos até mesmo não encontrar uma definição satisfatória para massa.[4]
Os conceitos científicos de massa, que diferem do conceito também científico  de quantidade de matéria,[5] sempre se mostram de alguma forma associados ao conceito de inércia, e mesmo em relatividade, onde energia e massa mantêm, em acordo com a famosa equação E = mc2, íntima relação, esta associação está presente: não só a matéria mas também a energia apresenta inércia. Entretanto, apesar de muito bem definida dentro de cada área de estudo onde aparece, "explicar" a massa não é uma coisa muito simples, e atualmente existem algumas teorias que tentam elucidar nas origens o que é massa.
Os conceitos físicos de força e massa surgem em teorias ou modelos destinados a estabelecer a dinâmica em sistemas compostos ou por entes semelhantes ou por entes de natureza às vezes bem distintas. Nestes modelos sempre figuram também dois outros conceitos fundamentais, o conceito de momento e o conceito de energia. Os conceitos de energia e momento são importantes porque suas definições se dão de forma que energia e momento sempre obedeçam a leis gerais de conservação, leis estas decorrentes da existência de regras naturais de relacionamento entre entes e/ou sistemas que são, em princípio, estáveis e muito bem estabelecidas.[6] Neste contexto, energia e momento guardam íntima relação, e um ente físico é caracterizado pela sua relação de dispersão, um gráfico ou função que explicita a relação existente entre o momento e a energia para este ente. Dois entes físicos com a mesma natureza física têm relações de dispersão semelhantes. Como exemplo, as partículas clássicas dentro da mecânica de Newton têm energias que dependem dos quadrados de seus momentos:  (esta relação é encontrada de forma explicita na mecânica hamiltoniana: ). Já os fótons, partículas definidas no âmbito da mecânica quântica, têm energias linearmente dependentes de seus momentos: .
É com base na relação de dispersão que se estabelece a definição geral de massa:
Na oportunidade cita-se também a definição de força:
Definições mais intuitivas de massa, que não exigem a princípio conhecimentos avançados em cálculo integral e diferencial, podem ser derivadas desta definição formal quando no contexto de um modelo dinâmico particular.
Segundo o Sistema internacional de unidades (SI), a medida da massa é o quilograma (kg).[7]
A unidade de medida de massa — o quilograma — encontra-se intimamente atrelada ao quilograma-padrão, um protótipo internacional de platina iridiada (feito de irídio e platina) que se encontra conservado no Escritório Internacional de Pesos e Medidas (BIPM), situado no parque de Sant Cloud, nas proximidades de Paris, França, sendo o quilograma definido como a massa deste protótipo.
Em vista do senso comum ressalta-se que o conceito de quilograma (kg) como unidade de massa difere completamente do conceito de quilograma-força (kgf), uma unidade alternativa ao newton (N) na medida de força ou peso.[1]
No ramo da física de partículas é comum medir-se a massa não em quilogramas (kg) mas em unidades diretamente associadas às de energia, dentre as quais o elétron-volt (eV) se destaca. Em acordo com a ideia de equivalência entre massa e energia proposta por Einstein () a massa do elétron é expressa, em física de partículas, como 5,11x105 eV/c2 ou 511 keV/c2, e não como 9,11x10−31 kg.
Em química, apesar de não pertencer ao Sistema Internacional mas ser por este aceita, uma unidade de massa muito utilizada é a unidade de massa atômica, também conhecida por dalton. A unidade de massa atômica relativa, abreviada por "u", "uma", ou simplesmente "Da", equivale à massa de um doze avos (1/12) da massa do isótopo mais estável e abundante de carbono (carbono 12) em seu estado fundamental.[2]
Mesmo sendo o quilograma a unidade oficial do Sistema Internacional de Unidades, unidades específicas a cada ramo de atividade ou de uso comum em certas localidades têm uso ainda muito difundido, a citar a tonelada, a arroba, a onça, o quilate (em joalheria e ourivesaria), e outras.
Em mecânica clássica, que encerra em si as leis da dinâmica e também a lei da gravitação universal, ambas devidas à Isaac Newton, encontram-se duas possíveis definições para massa: a massa inercial, associada à Segunda Lei de Newton, e a massa gravitacional, definida em função da interação gravitacional entre dois corpos.[8]
A massa inercial  de um corpo é uma grandeza escalar associada à razão entre o módulo da aceleração  apresentada por um corpo de referência — por definição o quilograma padrão (cuja massa inercial vale m0 = 1 kg) — e o módulo da aceleração  apresentada por este corpo quando ambos encontram-se solicitados por forças não gravitacionais de mesmo módulo.[9]
Um mecanismo destinado à medida da massa inercial nada mais é do que um mecanismo que aplique forças não gravitacionais com módulos idênticos a dois corpos distintos, e que permita a medida de suas acelerações.
Um bom "medidor de massa inercial" é o sistema constituído por duas massas, uma das quais de referência  de valor previamente conhecido (mas não necessariamente o quilograma-padrão ou réplica deste), apoiadas em uma mesa horizontal sem atrito, e conectadas entre si por uma mola de massa desprezível e com constante elástica não necessariamente conhecida. Em virtude da terceira lei de Newton, ao colocar-se o sistema para oscilar ambas as massas oscilarão em torno do centro de massa e os módulos das forças em ambas serão, apesar de não necessariamente conhecidos,  obrigatoriamente iguais. Ao medir-se a aceleração  e  das massas (em relação ao centro de massa) e determinar-se a razão entre elas estabelece-se automaticamente o inverso da razão de suas massas inerciais  e , o que fornece a massa desconhecida em função da massa de referência (ou a massa desconhecida diretamente quando a massa de referência é quilograma-padrão ou réplica deste,  caso em que  =1 kg).
A construção de um medidor de massa inercial fundamentado nos princípios citados pode ser muito simplificada quando, baseando-se na Lei de Hooke e no estudo dos movimentos harmônicos simples, percebe-se que a medida da razão entre as acelerações pode ser substituída pela medida da razão inversa das amplitudes dos movimentos, grandeza esta facilmente mensurável.
O conceito de massa inercial fundamenta-se diretamente nas leis da mecânica, em especial com a Segunda Lei de Newton.
A Segunda Lei de Newton afirma em essência que a força aplicada em um dado objeto é diretamente proporcional à aceleração que este apresenta. Assim, quanto maior a força aplicada a um mesmo objeto, maior a sua aceleração. Subentende-se aqui, como em todo problema de mecânica clássica, que o referencial utilizado é um referencial inercial, sendo portanto a primeira e a terceira leis sempre válidas no referencial assumido (conforme praxe).
Nestas condições, a segunda lei também encerra em si o fato experimental de que, ao selecionarem-se diversos corpos completamente diferentes, uma mesma força irá produzir nestes, muito provavelmente, acelerações completamente diferentes.
Este fato estabelece a necessidade de se definir uma grandeza intrínseca a cada corpo que expresse em seu valor a relação entre a força necessária e a aceleração desejada neste corpo em específico: esta grandeza, definida como a massa inercial do corpo, aparece na segunda lei como sendo a constante de proporcionalidade entre força e aceleração.
Tendo-se já por definida a unidade de aceleração (m/s²), pois esta deriva de uma relação entre a unidade de comprimento (no S.I o metro) e uma unidade de tempo (no S.I o segundo), havia, mediante as situações apresentadas, duas possibilidades para se estabelecer as unidades das grandezas restantes: ou definia-se um padrão de força, sendo a sua intensidade então definida como uma unidade fundamental, e mediante esta definição estabelecia-se a unidade de massa como unidade derivada, ou estabelecia-se um corpo referência para o qual a massa inercial seria a unidade, e assim fazendo ter-se-ia a unidade de força e não a unidade de massa como uma unidade derivada.
Por razões práticas, a opção escolhida foi a segunda, e estabeleceu-se um corpo padrão, o quilograma-padrão, ao qual se atribuiu por definição a massa inercial de 1 quilograma (1 kg). Com esta definição, a unidade de força, uma grandeza derivada, recebeu o nome newton, havendo a seguinte relação entre elas:
Assim, uma força com intensidade de 1 newton (1N) é uma força que, quando aplicada ao quilograma-padrão, ou a um corpo cuja massa seja, por comparação inercial ao quilograma padrão ou réplica deste, também 1 kg, provoque nestes uma aceleração de exatos 1 m/s².
Isaac Newton, por preocupar-se não apenas com a dinâmica dos corpos terrenos mas também com a dos corpos celestes, estabeleceu, juntamente com as leis da mecânica clássica, a Lei da Gravitação Universal. A Lei da gravitação universal suporta-se no fato experimental de que todos os corpos massivos conhecidos até hoje, pelo simples fato de existirem, atraem outros corpos massivos ao seu redor - e todos os outros do universo, uma vez que a força gravitacional decai com o quadrado da distância, e a rigor nunca se anula, por maior que esta seja. A força de interação em questão é a conhecida força gravitacional, sendo esta também denominada (de fato em situações mais específicas) força peso.
Na Lei da Gravitação Universal figura portanto uma massa, a massa gravitacional, uma propriedade que é, assim como a massa inercial, intrínseca a todos os corpos. A definição operacional de massa gravitacional de um corpo é feita, assim como o ocorrido para o caso da massa inercial, por comparação entre a massa gravitacional deste corpo e a massa gravitacional de um corpo de referência, e são em princípio as massas gravitacionais e não as respectivas massas inerciais que, juntamente com a distância de separação entre os corpos,  determinam a força gravitacional entre estes.
O processo de medida da massa gravitacional deve ter por base, logicamente, a força gravitacional. Através de uma balança de equilíbrio nota-se que diferentes corpos são atraídos de forma diferente quando nas proximidades de um grande corpo massivo - a exemplo de um planeta como a Terra. Em um experimento com uma dessas balanças, observa-se que a balança "pende" para o lado do objeto mais "pesado", ou seja, para o lado do objeto com maior massa gravitacional. Através de uma balança de braço imersa em um campo gravitacional constante como o criado (não obrigatoriamente) pela Terra, a determinação da massa gravitacional de um corpo pode ser feita por comparação a um padrão unitário de massa gravitacional verificando-se que a massa gravitacional do objeto em teste - colocado em um dos pratos - será o número necessário de amostras-padrão a serem colocados no outro prato a fim de que a balança mostre-se equilibrada.
O corpo-padrão sobre o qual se define a unidade de massa gravitacional acaba sendo, por razão simples à frente discriminada, o mesmo protótipo sobre o qual se define a unidade de massa inercial, o quilograma-padrão. A unidade de massa gravitacional é, portanto, a mesma unidade usada na medida de massa inercial: o quilograma (kg).
A definição de quilograma (kg) como a unidade de massa gravitacional deve-se à equivalência experimental entre as massas inerciais e gravitacionais observada em todos os corpos, mas em princípio não há nada na mecânica ou na gravitação que obrigue a existência de tal relação, e por isto elas devem ser definidas, a priori, de formas separadas.
Um exemplo, hipotético e irreal, da não obrigatoriedade da equivalência entre as massas inercial e gravitacional seria obtido caso admitíssemos que a força gravitacional não atuasse sobre partículas carregadas eletricamente, e sim sobre partículas estritamente neutras.  Nestas condições, dois pedaços de urânio confeccionados de forma a terem massas inerciais estritamente iguais, mas compostos por isótopos distintos deste material, a saber urânio U235 (usado na bomba de Hiroshima) e U238 (isótopo abundante, usado em reatores), teriam visivelmente massas gravitacionais (e pesos) diferentes, pois o número de nêutrons em uma amostra seria maior do que o número de nêutrons na outra.
A introdução da ideia de campo na Física por Michael Faraday representou um avanço formidável não só no ramo da eletricidade mas também no estudo da gravitação universal. A ideia fundamental atrás do conceito de campo se opõe diretamente ao conceito de ação à distância. Dados dois entes em interação, no modelo de ação à distância cada um dos entes atua diretamente sobre o outro, não havendo qualquer agente intermediário responsável por esta interação. Na visão através do modelo de campo, um dos entes em interação é agora responsável por criar ao seu redor um terceiro ente físico, o campo, que será o mediador da interação entre ele e o segundo ente. Neste caso, o segundo ente não mais interage com o primeiro diretamente, e sim com o campo que este criou.
Em algumas bibliografias usa-se o modelo de campo para suportar a definição de duas massas gravitacionais a princípio diferentes: a massa gravitacional ativa e a massa gravitacional passiva, nenhuma das quais, então,  necessariamente igual à massa inercial do corpo associado. Temos então a seguinte definição para cada uma delas:
Dentro da dinâmica de Newton há, ao contrário do que ocorre entre massa gravitacional e massa inercial, forte base teórica para se afirmar que as massas gravitacionais ativa e passiva devem ser, em verdade, iguais, ou pelo menos diretamente proporcionais mediante uma constante de proporcionalidade universal. O suporte mais importante para tal fato encontra-se na definição de força, que é exatamente a mesma tanto no âmbito da teoria da gravitação universal quanto no âmbito da teoria mecânica: força é a expressão física da interação de DOIS objetos, e um objeto sob a ação de uma força tem sua dinâmica determinada pela Segunda Lei de Newton, qualquer que seja a natureza da interação entre os corpos. Se assim não fosse, a teoria da gravitação destacar-se-ia como uma teoria dinâmica a parte, devendo estabelecer não apenas que existe uma interação de origem gravitacional entre dois corpos e fornecer a tradicional fórmula para o cálculo da força que representa esta interação, como também fornecer todo um conjunto de regras (similares ou não às leis de Newton) que permitissem determinar a dinâmica dos corpos que por ventura se encontrassem sobre a ação destas "forças especiais".
Uma vez estabelecido que a Terceira Lei de Newton vale dentro da dinâmica gravitacional, a igualdade, ou melhor, a proporcionalidade entre as massas gravitacionais ativa e passiva é direta.  Repare que estas massas não precisam obrigatoriamente ter o mesmo valor para um dado corpo, pois um fator de proporcionalidade universal poderia ser facilmente "absorvido" dentro da constante de gravitação universal G que figura na equação da Lei da Gravitação Universal. Assim, poder-se-ia, em princípio, definir: "a massa gravitacional ativa de qualquer corpo vale sempre o dobro de sua massa passiva". Se assim fosse, um corpo com massa gravitacional passiva de 1 kg teria uma massa gravitacional ativa de 2 kg. Repare entretanto que estabelecendo-se, neste caso, o valor da constante de gravitação G como tendo a metade do valor que na realidade tem, o fator 2 introduzido  na definição da massa ativa seria cancelado por este fator 1/2 introduzido na constante G original, e a força gravitacional bem como toda a dinâmica fornecida pela segunda lei para estes corpos não seriam, como um todo, afetadas. Entretanto, podendo-se escolher, fazem-se sempre as escolhas mais simples:
É fato que, conforme elaboradas por Newton, não há nada em toda a estrutura da dinâmica e da gravitação universal que forneça uma razão teórica plausível para a equivalência experimentalmente observada entre massa gravitacional e massa inercial. A dinâmica newtoniana afirma apenas que as massas gravitacionais são responsáveis pelas forças gravitacionais entre dois corpos em interação gravitacional, sendo estas massas e não as inerciais as massas usadas na determinação do módulo destas forças gravitacionais - um par ação e reação. Afirma também que a massa inercial é a massa utilizada na segunda lei da dinâmica, sendo esta massa, e não a massa gravitacional,  a massa utilizada no cálculo da aceleração apresentada pelo corpo quando solicitado por quaisquer forças - inclusive as de origem gravitacional. A massa presente na equação fundamental da dinâmica () é, pois, a massa inercial.[10]
Newton foi o primeiro a verificar experimentalmente a equivalência entre massa inercial e massa gravitacional. A ideia de seu experimento reside nos resultados teóricos da aplicação das teorias gravitacional e mecânica ao estudo de um pêndulo gravitacional simples, que, mantidas explícitas as massas gravitacional e inercial nos cálculos, leva à seguinte equação para o período de oscilação T de um pêndulo:
onde  e  referem-se, respectivamente, às massas inercial e gravitacional do corpo suspenso, L ao comprimento do pêndulo e g ao módulo da aceleração da gravidade no local do experimento.
Nesta equação torna-se evidente que, mantidos constantes o local do experimento - e portanto a aceleração da gravidade g no local - e o comprimento L do pêndulo, uma troca do corpo suspenso no pêndulo por outro qualquer que tenha, por simplicidade mas não obrigatoriedade, uma mesma massa gravitacional mg, só levará a uma alteração no período do pêndulo se a razão  for diferente nos diversos corpos, ou seja, se não houver uma relação fixa entre a massa gravitacional mg e a massa inercial mi.
Na sequência, Newton construiu um pêndulo fixando uma caixa oca e a princípio vazia na ponta de uma haste com massa desprezível. O interior da caixa foi, então, em uma sequência de experimentos, enchido com os mais diversos materiais, tendo Newton sempre o cuidado de encher o pêndulo de forma que este tivesse, depois de cheio, sempre a mesma massa gravitacional mg (o pêndulo era pesado). Os períodos dos diversos pêndulos assim obtidos foram, satisfeitos os rigores experimentais associados ao experimento, a citar a manutenção, em valores constantes e adequados, do local, da amplitude A do movimento, e do comprimento L da corda, então medidos.
Consideradas as incertezas experimentais inerentes, Newton não observou qualquer alteração nos períodos dos diversos pêndulos por ele construídos e, ao fazê-lo, estabeleceu a igualdade entre as massas inercial e gravitacional até a terceira casa decimal (precisão de cerca de 1 parte em 103).
Uma vez estabelecida a igualdade entre as duas massas, a equação para o período do pêndulo se reduz a:
que é a equação encontrada em qualquer livro de física de ensino médio.[11]
Graças à equivalência entre as massas inercial e gravitacional há uma completa independência entre o período de oscilação T de um pêndulo (oscilando com pequenas amplitudes) e a massa do corpo nele suspenso. Assim, mantidos o comprimento L e a aceleração da gravidade no local do experimento, qualquer que seja a massa que se coloque na ponta de um pêndulo, o seu período de oscilação T será o mesmo. Uma alteração no período T requer ou uma alteração no comprimento L do pêndulo, ou uma alteração na aceleração da gravidade no local onde realiza-se a experiência. Como a aceleração da gravidade terrestre no local, suposto fixo, também é constante, o período T de um pêndulo mostra-se influenciável em primeira ordem apenas por alterações em seu comprimento L.
Em consequência, os relógios "cuco" têm por base de tempo as oscilações de pêndulos, os quais são ajustados, uma vez em seus respectivos locais de trabalho, mediante pequenas mudanças nos seus comprimentos L. Pêndulos mostram-se também como bons equipamentos para a determinação, com razoável precisão, da gravidade em um dado local.
Um considerável avanço experimental na busca da afirmação de igualdade entre as massas inercial e gravitacional foi feito por Loránd Eötvös em 1909.[12] Utilizando uma balança de torção ele realizou uma sequência de experimentos que resultou em uma considerável redução na incerteza desta afirmação, sendo seus resultados compatíveis com uma incerteza menor que 1 parte em 109 (1 milhão de vezes mais precisa do que a obtida por Newton).
Eötvös colocou diferentes materiais nas extremidades de sua balança de torção e comparou, para cada material, a sua massa gravitacional (o seu "peso") e a sua massa inercial, determinada a partir da força inercial centrífuga devida à rotação da Terra. Qualquer diferença entre estas duas massas seria observada como uma rotação da balança de torção. Tal rotação, dentro dos limites experimentais, não foi observada.
A ideia do uso da balança de torção para a determinação da igualdade entre as massas inercial e gravitacional foi retomada, em 1964, por um cientista de nome, Robert H. Dicke, e em 1972 por Vladimir Braginsk. Com refinamentos que agora levavam em conta, entre outros, a atração gravitacional do Sol, e a força inercial associada à órbita da Terra ao redor do sol, estes cientistas conseguiram ao fim afirmar que a massas inercial e gravitacional são iguais com uma incerteza menor do que 1 parte em 1011, refinando em pelo menos 100 vezes a incerteza anteriormente obtida por Eötvös.
"Eu estava sentado em uma cadeira no escritório de patentes, em Berna, quando de repente ocorreu-me um pensamento: se uma pessoa cair livremente, ela não sentirá seu próprio peso. Eu estava atônito. Este simples pensamento impressionou-me profundamente. Ele me impeliu para uma teoria da gravitação." (Albert Einstein)
Talvez a mais forte evidência a favor da veracidade da afirmação entre a igualdade das massas inercial e gravitacional encontre-se em um fato inicialmente observado por Galileu Galilei, e eternizado na famosa experiência da Torre de Pisa. Uma vez estabelecido um local onde haja um campo gravitacional conhecido, a exemplo um ponto na superfície da Terra, verifica-se experimentalmente que TODOS os objetos caem, quaisquer que sejam as suas massas, materiais constituintes ou volumes, quando soltos em queda livre a partir de um mesmo ponto, exatamente com a MESMA aceleração. Conforme visto, se houvesse realmente alguma diferença entre massa gravitacional e massa inercial, um corpo que, a exemplo, apresentasse massa inercial razoavelmente maior do que sua massa gravitacional deveria, em seu processo de queda, apresentar uma aceleração mensuravelmente menor do que a que seria observada em um corpo no qual a massa gravitacional fosse maior que (ou pelo menos não tão diferente da) sua massa inercial.
Esta última ideia encontra enfronhada na citada frase de Einstein pois, associada os diversos materiais que compõem o corpo humano, levaria a forças de contato entre os diversos sistemas do corpo quando este estivesse em queda livre. Tomemos a exemplo o sistema ósseo e o sistema muscular.  Caso as razões entre as massas inercial e gravitacional fossem diferentes nos dois sistemas, haveria obrigatoriamente uma força de contato entre estas estruturas a fim de se manter a unicidade do corpo durante a queda. Sendo o nosso sentido de tato sensível justamente a estas forças, estas fariam com que as pessoas "sentissem" a suas próprias quedas, fato que não é, entretanto, observado.
O Princípio da Equivalência entre as massas inercial e gravitacional guarda uma íntima relação com o Princípio da Equivalência de Einstein, ponto de partida para a construção  de uma teoria de gravitação covariante em relação a qualquer referencial: a Relatividade Geral.
Uma vez estabelecida a equivalência entre massa inercial e massa gravitacional, o termo massa, dentro da dinâmica newtoniana, passa a representar, de forma implícita, o termo mais adequado à situação.
No âmbito da mecânica clássica considera-se que a massa, uma propriedade da matéria, é constante, não podendo ser criada e nem destruída, apenas transportada. Diversas leis, a exemplo das leis de Newton e de Lavoisier (massa dos reagentes é igual a massa dos produtos), tomam partido desse fato que, mantidas as fronteiras impostas pela mecânica clássica, mostra-se plenamente verídico no cotidiano.
Entretanto a ideia de conservação e de associação entre massa e matéria falha de forma considerável em outros campos que não o da mecânica clássica, e em áreas sujeitas às leis da física de partículas, da mecânica quântica e da relatividade, esta acaba substituída ("englobada") por uma lei mais fundamental, a lei da conservação de energia. Nestas áreas massa mostra-se equivalente à energia, e a equação E=mc² torna-se indispensável para estabelecer-se a citada lei de conservação.
Conforme encontrados em  Física Quântica (Eisberg, Robert et. al.),[13][14] os postulados da Relatividade Restrita parecem simples. Entretanto esta simplicidade esconde um intrincado conjunto de ideias e fatos que, derivados de inconsistências entre as teorias da mecânica clássica e do eletromagnetismo clássico, e de inconsistências, o que é bem mais sério, entre estas teorias e fatos experimentais então estabelecidos, culminaram com a necessidade de uma nova proposta para a compreensão da dinâmica da matéria (e energia).  A simplicidade dos postulados esconde também consequências em verdade nada simples e que fogem bem ao senso de mundo que temos normalmente. Fatos como dilatação do tempo, contração do espaço, e uma nova "definição" de massa encontram-se bem distantes da percepção de mundo de um "simples mortal".
Ao ser elaborada a relatividade restrita acabou herdando vários dos conceitos antes existentes em mecânica clássica, o que faz sentido visto que a mecânica clássica foi um paradigma para a dinâmica que perdurou por quase trezentos anos sem encontrar qualquer evidência experimental que não fosse condizente com sua proposta, sendo portanto uma teoria para a dinâmica que se ajusta plenamente aos fatos observáveis do "mundo em que vivemos", e dentro de certos limites plenamente válida ainda hoje. Qualquer nova teoria dinâmica que pretenda estender a compreensão até então fornecida pela mecânica clássica deverá portanto  necessariamente apresentar resultados que concordem com os por ela fornecidos quando dentro dos limites de sua validade, ou seja, em um mundo macroscópico e de baixas velocidades quando comparadas à da luz.
A mais importante herança recebida pela relatividade restrita da mecânica clássica é o conceito de referencial inercial sobre o qual esta nova teoria também se estabelece, sendo a relatividade restrita, portanto, uma teoria ainda não completamente covariante. Tal covariância geral só será alcançada no âmbito da Relatividade Geral.
Na sequência, uma segunda herança direta da mecânica clássica e que se traduz dentro da relatividade restrita por massa de repouso  é o conceito clássico de massa inercial, sendo esta definida dentro da relatividade restrita como a massa medida para um objeto quando este se encontre em repouso em relação ao referencial inercial a partir do qual se estabelece a medida, ou seja, com velocidade praticamente nula e assim completamente desprezível quando comparada à da luz - condição que implica o limite de validade da mecânica clássica. Assim:
No âmbito da relatividade restrita a massa de repouso (ou simplesmente massa) de uma partícula ou sistema pode ser obtida através da expressão:
onde E é a energia total do sistema, P é o momento total do sistema e c a velocidade da luz.
A relatividade restrita, sendo uma nova teoria sobre dinâmica, estabeleceu novas regras que substituíram as Leis de Newton quando fora do limite clássico, e foram elaboradas, conforme discutido, de forma que se reduzissem a elas quando nos limites onde a mecânica clássica vale. Estas novas leis, mais abrangentes, foram também estabelecidas de  forma a tornar não só a dinâmica da matéria como também as leis da dinâmica da energia invariantes à mudança de referencial, leis últimas expressas por um conjunto de equações que constituem ainda hoje o pilar fundamental da teoria eletromagnética clássica, as Equações de Maxwell.[15]
Dentro deste contexto, nos limites onde as leis da mecânica não valem, o conceito clássico de momento () não se mostra mais associado a uma lei de conservação, e a elaboração de um novo conceito de momento condizente com as leis da relatividade restrita e também com a existência de uma lei de conservação associada levou à definição do que se denomina momento relativístico. O momento relativístico P, que satisfaz conforme definido à citada lei de conservação, é definido por:
Comparando-se estas e várias outras equações da dinâmica relativística com as respectivas equações da dinâmica clássica, tem-se a intuição que se pode derivar as leis da dinâmica relativística substituindo a massa inercial m nas equações para a mecânica clássica pelo que se convencionou chamar massa relativística :
Na equação para a massa relativística vemos que esta massa é explicitamente dependente de sua velocidade, e, visto que maiores valores de velocidade nesta equação implicam maiores valores para a massa relativística, indo esta ao infinito no limite que a velocidade  do objeto iguala-se à velocidade da luz C,  não é incomum encontrar-se pessoas ligadas à área científica dizendo que "a massa aumenta a altas velocidades".
O fato é que, apesar de intuitivo - e de muitas das vezes levar a analogias que podem mostrar-se válidas - uma simples substituição da massa inercial clássica pela massa relativística leva, na maioria das vezes a resultados completamente falsos. Vejamos o que ocorre com a força e com a equação de Newton nesta perspectiva.
A relatividade "herda" a definição de força em sua forma mais abrangente que, junto com a definição do momento relativístico, resulta:
Notoriamente a força que se observa atuando em um corpo que se move com uma velocidade  não pode ser derivada pela simples substituição da expressão da massa relativística na lei da dinâmica de Newton F=ma. A força na mecânica relativística apresenta duas componentes: uma condizente com a "proposta" de massa relativística, paralela à aceleração apresentada pelo corpo, e outra, que nos diz que há também força na direção da velocidade do objeto, termo que não condiz com a "proposta" e tão menos com a mecânica clássica.
Outra incoerência associada à definição de massa relativística pode ser obtida quando esta é substituída na Lei da Gravitação de Newton. Conforme estruturada, a dinâmica da relatividade restrita estabelece a dinâmica dos corpos e energia apenas em situações completamente isentas de campos gravitacionais, e uma equação para a lei da gravitação não figura dentro do âmbito da relatividade restrita. A associação de uma teoria de gravitação à da relatividade restrita nos leva diretamente à relatividade geral.
Assim, conforme tradução do expresso em Classical Daynamics (Thornton et. al), "Nós preferimos nos reter o conceito de massa como uma grandeza invariante, uma propriedade intrínseca dos corpos. O uso dos dois termos, massa de repouso e massa relativística, é hoje considerado obsoleto. Portanto nós sempre iremos nos referir apenas ao termo massa, o qual equivale à massa de repouso. O uso da massa relativística geralmente conduz a erros ao se usar expressões clássicas.".[16]
Os conceitos de massa longitudinal e massa transversal.[17] São muito importantes no estudo das colisões de íons pesados relativísticos e na Física de Hádrons por ser um dos observáveis usados para a dedução sobre uma fase da matéria chamada Plasma de Quarks de Gluons. Esta definição vem da definição de aceleração relativística:
Esta expressão mostra que de forma geral a aceleração de um partícula depende do ângulo entre a força e a velocidade, e geralmente não tem a direção e o sentido da força. Entretanto há dois casos em que a força tem a mesma direção da aceleração, fornecendo algo parecido com a forma clássica de Newton:
A mecânica relativística também herda, além dos já falados, dois outros conceitos bem intuitivos da mecânica clássica: o conceito de trabalho T de uma força, classicamente definido por:  , e que, em mecânica clássica, conduz à definição  - válida quando se tem uma força F constante formando sempre o mesmo ângulo  com o deslocamento - e o teorema da equiparação entre trabalho e variação da energia cinética, .
A fim de se ter uma energia cinética relativística condizente com o teorema da equivalência citado, devemos inserir a força relativística na equação que define trabalho, o que, após alguns cálculos não muito avançados (para quem sabe um pouco de cálculo integral e diferencial - ver Eisberg et.al, Física Clássica), fornece:
Mostra-se também, sem muita complicação, que no limite onde a velocidade v da partícula é negligenciável perto da velocidade c da luz, a equação da energia cinética relativística se reduz à equação da energia cinética clássica .
A equação da energia cinética relativística, assim definida, é uma equação em que há duas parcelas, um dependente de velocidade V do lado esquerdo, e uma independente de V, fixa uma vez conhecida a massa inercial da partícula, do lado direito:  . Transpondo os termos temos  , e lembrando que a energia total de uma partícula é a soma entre sua energia cinética e demais tipos de energia que esta possui, a intuição nos leva diretamente à conclusão que o termo
deve ser interpretado como a energia relativística total da partícula. Quando a velocidade da partícula é nula, sua energia total vale, assim:
que é a famosa equação de Einstein para a equivalência entre massa e energia.
A validade desta equivalência entre massa de repouso e energia de repouso no contexto da relatividade restrita encontra suporte experimental em uma série de eventos que vão desde a produção e aniquilação de pares de partícula-antipartícula, onde massa de repouso é claramente convertida em energia pura (radiação eletromagnética: raios gama), até reações nucleares onde a conversão de massa de repouso em energia é responsável por manter reatores funcionando, e também por permitir que se construam bombas muito pequenas perto do seu imenso poder de destruição (bombas nucleares).
A energia relativística também obedece, como é de se esperar, a uma lei de conservação: a lei da conservação da massa-energia no contexto da relatividade restrita.
No âmbito da Relatividade Geral a queda livre de uma partícula em um campo gravitacional é entendida como um movimento que se realiza em ausência de força. A força neste contexto é a causa de desvios nesta trajetória de queda livre. Associada à força, medindo a oposição da partícula a mudanças na sua trajetória de queda livre, temos a massa inercial da partícula.
As trajetórias das partículas em queda livre são linhas retas - de forma mais rigorosa geodésicas - no espaço-tempo. Estas trajetórias são dependentes apenas das posições e velocidades iniciais das partículas em queda livre, mostrando-se completamente independentes de propriedades inerentes como as massas ou as dimensões destas (o princípio da equivalência). Em função do espaço-tempo não ser plano, as projeções das geodésicas associadas sobre o espaço tridimensional ao qual estamos habituados normalmente não fornecem trajetórias retas e sim trajetórias quase sempre "curvas", a exemplo trajetórias parabólicas, neste mundo tridimensional onde espaço e tempo são entendidos como separados.
A origem dos campos gravitacionais na equação fundamental da Teoria da Relatividade Geral encontra-se o tensor de energia-momento, ou seja, nas densidades e fluxos de energia e momento. Uma vez que a energia de uma partícula em repouso associa-se diretamente à sua massa (inercial), as massas dos corpos em repouso são "fontes" de campos gravitacionais. Sendo o movimento da fonte do campo gravitacional desprezível e sendo a velocidade do objeto em queda livre bem pequena quando comparada à velocidade da luz, tem-se no limite a validade da lei da gravitação de Newton: a massa do corpo confunde-se com a massa gravitacional clássica. Assim tem-se o princípio da equivalência. Entretanto há exceções: para corpos com alta densidade de energia (luz como partícula) este argumento não é válido: a gravidade nas proximidades do sol mostra-se duas vezes maior do que a que seria esperada pela Gravitação de Newton sob mesmo princípio.[18]
O conceito de massa na Teoria Geral da relatividade é em verdade bem mais complexo do que o encontrado na sua versão restrita. De fato, a Teoria Geral da Relatividade não oferece uma definição simples do termo massa, mas oferece diferentes definições aplicáveis cada qual em circunstâncias específicas diferentes, a exemplo as massas "ADM" e "Komar".[19] E existem situações claras dentro da Teoria Geral onde não há como se estabelecer um conceito aceitável para massa.
Em suma, não há um conceito de massa que se mostre completamente covariante dentro da Teoria Geral da Relatividade.
Dentro dos limites onde não só a granulosidade da matéria como também a quantização nos processos de troca de energia mostram-se relevantes, as leis da teoria clássica deixam muito a desejar quando o objetivo é explicar os fenômenos naturais que nele ocorrem, e em primeira instância a teoria da mecânica quântica não relativística torna-se a teoria responsável por nos fornecer as ferramentas adequadas para a correta compreensão dos fenômenos naturais observáveis dentro destes limites. A mecânica quântica não relativística é uma extensão da mecânica clássica para o mundo microscópico com dimensões comparáveis às dos átomos. Esta teoria, cujas origens remontam ao ano de 1900 com os trabalhos de Max Planck, pode ser rapidamente sintetizada na Equação de Schrödinger, equação que exerce na teoria quântica papel similar à equação fundamental da dinâmica dentro da mecânica clássica.
Com a devida ressalva sobre o conceito de partícula no âmbito da teoria quântica, a qual nos leva diretamente ao princípio da complementaridade de Niels Bohr, na equação de Schrödinger figura uma massa, essencialmente a massa inercial da partícula, herança direta da mecânica clássica. Em consequência, no escopo da mecânica quântica não relativística a massa é a mesma massa inercial, clássica, da partícula. No que se refere à associação entre massa inercial e gravitacional,  no mundo quântico o conceito de massa gravitacional tornam-se completamente sem sentido visto que a ordem de grandeza das massas no mundo atômico leva a valores extremamente pequenos - completamente desprezíveis e realmente desprezados - para as forças gravitacionais entre dois entes neste mundo microscópico. Para a descrição correta do átomo de hidrogênio a expressão para a energia potencial elétrica de interação entre o próton e o elétron é indispensável na equação de Schrödinger, mas em nenhuma literatura esta aparecerá ao lado de sua equivalente gravitacional.
Para a descrição de fenômenos que envolvam altas velocidades (se comparadas às da luz) ou elevados níveis de energia - a exemplo espalhamento de raios X pela matéria - a mecânica quântica ganha uma versão relativística, e neste caso nas equações associadas, entre elas a equação de Klein-Gordon, figura o conceito de massa de repouso da partícula. O uso do conceito de massa relativística herda as mesmas restrições já consideradas na definição desta neste artigo, sendo suprimido pelo uso do conceito de energia total da partícula. Em certas situações a massa mostra-se muitas vezes medida através de um comprimento de onda, diretamente associado ao comprimento de onda de um fóton cuja energia seja a mesma da energia de repouso da partícula. Seguindo esta associação, para elétrons tem-se a massa quântica do elétron, o seu comprimento de onda Compton, que pode ser determinada por várias formas de espectroscopia e encontra-se intimamente relacionada à constante de Rydberg, o raio de Bohr, e o raio clássico do elétron. A massa quântica de objetos maiores pode ser diretamente medida pela balança de Watt.[20]
Geralmente estabelecido dentro do âmbito da gravitação mas também válido em outras situações similares,  o conceito de massa reduzida surge a partir de resultados matemáticos associados à análise da dinâmica de dois corpos com massas m1 e m2 que, devido à interação gravitacional entre eles, gravitam mutuamente o centro de massa do sistema que constituem. A análise clássica deve ser feita a partir do centro de massa ou de outro referencial inercial equivalente, e a rigor não pode ser estabelecida com base em um referencial fixo em um dos corpos, pois estes não constituem referenciais inerciais válidos. São necessários portanto seis grandezas, a saber as componentes dos vetores  e  que localizam os dois corpos a partir do referencial inercial escolhido.[21]
Entretanto, sob certas condições,  que incluem a dependência da função energia potencial U associada ao sistema apenas com o módulo do vetor  que localiza uma das massas em relação à outra, condições geralmente satisfeitas por tais sistemas gravitacionais constituindo um sistema isolado, a análise pode ser feita a partir de qualquer referencial inercial mediante o conhecimento do vetor  que localiza o centro de massa do sistema em relação ao referencial escolhido e do vetor  que localiza uma das massas em relação à outra. Escolhendo-se, sem perda de generalidade, o centro de massa como referencial (), os cálculos podem ser feitos com base apenas em três grandezas, a saber as componentes do vetor que localiza uma massa em relação à outra (o vetor ).
Nestas condições, o problema é formalmente reduzido, sendo matematicamente análogo, ao problema da análise do movimento de um único corpo que se mova sob influência de um campo central - campo este diretamente associado à função energia potencial  e à origem do referencial inercial assumido -  e que tenha massa  determinada através da expressão:
Esta massa  é conhecida por massa reduzida do sistema formado pelas massas m1 e m2.
Assim, a análise do sistema Terra-Lua pode ser feita a partir de um referencial com origem no centro na Terra desde que à Lua seja atribuída a massa reduzida associada ao sistema Terra-Lua.
O emprego do conceito de massa reduzida não se restringe ao problema clássico citado, figurando também em áreas como eletromagnetismo e física quântica, a exemplo no estudo dos átomos e na  definição da "Constante de Rydberg para um núcleo de Massa M".[22]
Ao se discutir o comportamento de partículas que se movem dentro de estruturas que lhe impõem potenciais periódicos ao longo de seu movimento é conveniente introduzir o conceito de massa efetiva. Esta situação é típica dentro de física do estado sólido, onde a maioria dos efeitos elétricos de interesse decorre do alto padrão de simetria encontrado nos cristais semicondutores - a exemplo silício ou arsenieto de gálio - e da quebra proposital desta simetria - a exemplo através da introdução de pequenas quantidades de elementos específicos - os dopantes - na rede. A introdução da massa efetiva tem um considerável valor teórico pois dentro dos cristais semicondutores a ausência de um elétron introduzida por um dopante com valência inferior à requisitada pela rede - a exemplo gálio em cristal de silício - gera um "buraco", que efetivamente funciona como uma partícula positiva - um portador de carga que também contribui para a produção de corrente elétrica - e que, apesar de ter uma massa real nula (é literalmente um buraco - a falta de um elétron), move-se dentro da rede e sob ação de campos (forças) externos como se fosse uma partícula com massa real igual à sua massa efetiva.
A origem da massa efetiva encontra-se no comportamento dual da matéria no mundo quântico, sendo os movimentos das partículas dentro dos cristais melhor descritos por ondas de matéria do que pelo clássico movimento de partículas em si. Quando se movem com determinadas velocidades (momentos) dentro da rede que lhes conferem comprimentos de onda de De Broglie próximos ou iguais aos dos parâmetros de rede - ou da periodicidade da rede na direção de seus movimentos - a interação entre estas partículas e as barreiras periódicas impostas pelos íons do cristal, ou seja, entre estas partículas e o cristal como um todo, aumentam consideravelmente. Ocorre um fenômeno de ressonância entre a partícula que se move e a rede, e nestas condições o cristal todo se opõe consideravelmente ao movimento do elétron com aquela determinada energia e momento. A tentativa de se aumentar a energia da partícula quando próximo a esta situação, digamos através da aplicação de um campo elétrico externo - de uma força externa - pode inclusive levar a uma resposta muito mais intensa da rede cristalina sobre esta partícula, que ao invés de realmente acelerar no sentido da força externa aplicada, acaba acelerando em sentido contrário ao desta: fala-se então em massa efetiva negativa, pois, em acordo com o senso clássico da lei de Newton, a aplicação da força externa à partícula causou uma aceleração no sentido contrário ao da força aplicada.
Para situações em que o momento e a energia das partículas impliquem comprimentos de onda de De Broglie com valores bem diferentes dos comprimentos impostos pela periodicidade da rede, as massas efetivas têm valores praticamente iguais aos das massas reais destas partículas.
As situações de ressonância para determinadas energias levam à existência de bandas de energias proibidas para as partículas dentro dos cristais. As bandas permitidas traduzem-se como as conhecidas camadas eletrônicas (K, L, M, etc.) dentro do estudo da química e física, e são bem visíveis em um diagrama de relação de dispersão para estas partículas quando em um determinado cristal. Um exemplo ilustrativo encontra-se na figura ao lado. Repara as regiões onde a massa efetiva é negativa.
Em termos da relação de dispersão mostrada como exemplo, a massa efetiva de uma partícula na rede cristalina é definida como:
A massa efetiva liga-se à curvatura da relação de dispersão: "boca" para cima implica massa efetiva positiva, "boca" para baixo, massa efetiva negativa. Na transição, a massa efetiva é nula.[23]
A situação representada na figura é unidimensional e portanto simplificada. Os cristais são geralmente tridimensionais, e quando necessária ao tratamento formal destes, a massa efetiva assume a forma de um tensor:
Maiores detalhes sobre esta definição e sobre fenômenos de transporte associados a elétrons e buracos em cristais tridimensionais fogem ao escopo deste artigo, mas sendo de interesse do leitor estes podem facilmente ser encontrados na literatura especializada.[24]
No estudo da física nuclear não se tem ainda um modelo completamente coerente com todas as informações experimentais disponíveis, e alguns modelos concorrem lado a lado - no velho estilo da complementaridade, a citar o modelo da gota líquida, o do gás de Fermi, o de Camadas e o Coletivo - para a compreensão do núcleo como parte integrante da matéria.
No modelo do Gás de Fermi a modelagem é a mesma que a encontrada para um gás de elétrons, e nele cada nucleon do núcleo se move em um potencial efetivo atrativo, de valor médio essencialmente constante, criado pelos demais nucleons com o qual interage. Este potencial apresenta uma profundidade constante Vo dentro de um raio equivalente ao do núcleo, e reduz-se imediatamente a zero fora destas dimensões. Com base em trabalhos experimentais para nucleons em diversas energias dentro do núcleo, evidenciou-se que não se poderia a rigor tratar o potencial Vo como constante, pois este apresenta variações lentas e aproximadamente lineares com as energias dos nucleons. Em vista destes dados experimentais, optou-se por um tratamento onde Vo permanecesse essencialmente constante, e as massas dos  nucleons sofressem as correções necessárias para tornar o modelo condizente com os dados experimentais, havendo assim uma massa efetiva no modelo do Gás de Fermi em moldes essencialmente análogos à massa efetiva de elétrons e buracos em cristais.
Dentro dos modelos atômicos há outras definições diretamente associadas à massa, como o conceito de "massa semi-empírica", existente dentro do modelo de Gota Líquida para o núcleo, e com validade geral o conceito de "defeito de massa", que retrata o quanto menor é a massa de um núcleo resultante da fusão de dois outros quando comparado à soma das massas dos núcleos que lhe deram origem. O "defeito de massa" é facilmente compreensível, sendo composto por uma única parcela que retrata, em acordo com a equação da equivalência massa-energia (), a energia que é liberada na fusão dos núcleos pais e que se traduz como uma redução da massa no núcleo filho. Já na equação de massa, que fornece a massa semi empírica no modelo de Gota Líquida, encontram-se seis parcelas, cada uma responsável por considerar a influência de um dado parâmetro físico relevante na determinação de uma massa efetiva dentro deste modelo, havendo um termo associado à massa de repouso dos  núcleons isolados, um termo de volume proporcional ao número de massa A, um termo de superfície proporcional a A2/3, um termo coloumbiano proporcional a Z²/A1/3, um termo de assimetria proporcional a (Z-A/2)²/A onde Z é o número atômico e um termo de emparelhamento, geralmente proporcional a A1/2, que pode ser aditivo, nulo, ou subtrativo, sendo este subtrativo quando Z e N são ambos pares e aditivo se Z e N são ambos ímpares.
Assim, a fórmula da massa semi-empírica no modelo da Gota Líquida, com resultado expresso em unidades de massa atômica (u), é:
onde os termos a1 a a5 são empiricamente obtidos a partir dos dados experimentais. Um conjunto capaz de fornecer bons resultados é obtido quando estes termos de proporcionalidade valem respectivamente (0,01691; 0,01911; 0,000763; 0,10175; 0,012).
Maiores detalhes sobre os modelos nucleares fogem ao escopo deste artigo, e para mais informações sobre defeito de massa, massa semi-empírica e outros conceitos de massa dentro dos modelos nucleares sugerimos a leitura de bibliografia especializada.[25]
Os semicondutores são sólidos geralmente cristalinos de condutividade elétrica variável, podendo transitar com certa facilidade entre os estados de condutores e isolantes elétricos em função de parâmetros ambientais. São intrisecamente isolantes quando em temperaturas baixas, e transitam ao estado de condutor em temperaturas elevadas. Condutores extrínsecos (com dopagem) são condutores quando neutros e têm a transição condutor isolante controlada pelo estado de eletrização em que se encontram, sendo por tal o pilar da eletrônica moderna (eletrônica de estado sólido).
A condutividade elétrica dos semicondutores é particularmente sensível às condições ambientais tais como temperatura ou estado elétrico (+, neutro, -), o que lhes confere suma aplicabilidade e importância práticas. Seu emprego é importante na fabricação de componentes eletrônicos tais como diodos, transístores e outros de diversos graus de complexidade tecnológica, microprocessadores, e nanocircuitos usados em nanotecnologia.
Atualmente o elemento semicondutor é primordial na indústria eletrônica e confecção de seus componentes.
Um fato conhecido na física do estado sólido é que a condutividade elétrica é devida aos elétrons em bandas eletrônicas parcialmente cheias. Em temperaturas suficientemente baixas, semicondutores intrínsecos (sem impurezas em sua estrutura) têm suas bandas eletrônicas completas e comportam-se como isolantes. A condutividade dos semicondutores à temperatura ambiente é causada pela excitação de uns poucos elétrons da banda de valência para a banda de condução.[1]
É a quantidade de energia necessária para tirar um elétron da banda de valência e 'libertá-lo' na banda de condução que determina se um sólido será um condutor, semicondutor ou isolante. Para um semicondutor, pela definição esta energia (conhecida como "gap" de energia) é abaixo dos 4,5 elétron-volts (4,5 eV); para isolantes esta energia é bem acima desse valor. A temperaturas suficientemente elevadas os semicondutores intrínsecos se comportam como condutores pois grande quantidade de elétrons é termicamente excitada à banda de condução. Nos condutores, dada suas distribuições eletrônicas, existem sempre bandas de energia não preenchidas e portanto não existe quantidade mínima de energia necessária para se excitar os elétrons à condução.
Nos semicondutores a condutividade não é devida apenas aos elétrons que conseguiram pular para a banda de condução. Os buracos (também chamados de lacunas) que eles deixaram na banda de valência também dão contribuição importante à mobilidade elétrica. Tão importante que estes buracos são tratados como partículas reais dotadas de carga positiva, oposta à do elétron.
Acrescentar pequeníssimas quantidades de impurezas à estrutura material de um semicondutor intrínseco (ou seja, puro) resulta nos semicondutores ditos extrínsecos. Escolhendo-se adequadamente a impureza produz-se ou um semicondutor ou do tipo N (onde a impureza doa elétrons à rede) ou do tipo P (impureza produz buracos eletrônicos, falta de elétrons, na rede).
A condutividade de um semicondutor extrínseco é altamente dependente de seu estado eletrostático. Semicondutores extrínsecos se portam como condutores quando eletricamente neutros a qualquer temperatura. Carregando-se eletricamente estes semicondutores com sinal adequado (positivo no caso N, negativo no caso P), eles transitam quase que instantaneamente ao estado de isolantes elétricos. Nos casos eletrostáticos complementares (semicondutor N eletricamente negativo, P eletricamente positivo), eles se portam tal como quando neutros, como condutores.
Uma aplicação direta dos semicondutores extrínsecos, dada a propriedade acima, é na junção PN onde justapõe-se fisicamente um semicondutor P a um semicondutor N. Tal junção permite o fluxo de elétrons, ou corrente elétrica, apenas em um sentido e não no sentido oposto: constitui um diodo semicondutor em estado sólido. Transístores são componentes com três terminais onde há internamente duas junções semicondutoras fisicamente justapostas: ou PNP ou NPN. O fluxo de corrente através da estrutura completa ocorre ou não, de acordo com as polaridades aplicadas e mostra-se particularmente sensível à intensidade do sinal aplicado ao material semicondutor central, ligado ao terminal conhecido (dado o processo de construção) como "base" do transistor. De acordo com as quantidades de dopantes nos materiais semicondutores a que se conectam, os outros dois terminais são denominados coletor e emissor do transistor. Um pequeno sinal de corrente à base modula uma forte corrente entre o emissor e o coletor: tem-se um amplificador em estado sólido. Transistores operam facilmente também como chaves eletrônicas controladas.
Componentes com maior número de junções existem e têm cada qual suas propriedades elétricas específicas: nos diacs, triacs e SCRs encontram-se três junções semicondutoras.
Michael Faraday, em 1833, apercebeu-se de que a resistência do Sulfato de Prata descia com o aumento da temperatura, o efeito contrário ao esperado com outros materiais.[2]
Em 1874, Braun descobriu o efeito semicondutor em alguns sulfetos metálicos. Os primeiros elementos estudados foram o sulfeto de chumbo e o sulfeto de ferro. Em 1878 e 1879 David E. Hughes iniciou pesquisas no efeito semicondutor, a princípio como curiosidade, pois foi percebido ao acaso pelo cientista.
Embora Hughes não conhecesse o trabalho de James Clerk Maxwell, descobriu uma maneira de emitir ondas eletromagnéticas a partir de semicondutores. Em função de suas experiências acabou por inventar o detector eletromagnético por efeito semicondutivo, o diodo.
Em 1929, Walter Schottky, confirmou experimentalmente a existência de uma barreira de potencial numa junção Metal-Semicondutor.[2] (Heterojunção, diodo de Schottky)
Os semicondutores são intrínsecos quando não têm dopagem, têm apenas átomos do semicondutor-base, as temperaturas são muito baixas, são excelentes isolantes, pois, possuem na sua composição um elemento ou combinação de elementos que lhes conferem uma estrutura covalente com todos os orbitais eletrônicos ligantes de todos os átomos sempre completos. Não há por tal portadores de carga elétrica estruturalmente livres quando puros. Quimicamente viáveis há os semicondutores do grupo IV (ver tabela periódica), como os de germânico ou, com vantagens à temperatura ambiente, os de silício; do grupo III-V, com destaque para o arseneto de gálio, nitreto de gálio, sulfeto de cádmio, arseneto de índio, e certamente outros com estequiometrias mais sofisticadas. Os elementos no composto devem aparecer sempre dispostos em estrutura cristalina sem falhas ou imperfeições, o que justifica o emprego de técnicas de produção elaboradas e especialmente desenvolvidas para garantir tal simetria.
Para este tipo de material à temperatura de 0 K, a banda de valência está completamente preenchida e a de condução vazia, logo, mesmo quando aplicado um campo elétrico ao material, não existe corrente elétrica.
Para temperaturas diferentes do zero absoluto, os elétrons da banda de valência têm energia suficiente para transitarem para a banda de condução. Quando isto ocorre, gera-se um portador de carga oposta à dos elétrons, o buraco (lacuna). Este fenômeno acontece para uma certa temperatura com um ritmo, chamado Ritmo de Geração Térmica. Que em equilíbrio termodinâmico é igual ao Ritmo de Recombinação, que é o fenômeno contrário, onde um eletrão liberta energia e regressa à banda de valência.
A densidade de eletrões (n) e buracos (p) nestes materiais, são iguais e, a uma certa temperatura, é designada densidade intrínseca do material. Alguns valores típicos:
Os materiais semicondutores podem ser tratados quimicamente de diferentes maneiras de forma a alterarem as suas características. A combinação de semicondutores com diferentes tipos de dopagens faz emergir propriedades elétricas não observáveis quando separados, propriedades muito úteis sobretudo no controlo de correntes elétricas.
A dopagem é feita utilizando-se elementos diferentes dos que integram a rede semicondutora, usualmente os elementos da coluna III (para semicondutores tipo P) ou da coluna V (para semicondutores tipo N). É contudo também comum o emprego de elementos de outras colunas, incluso a coluna IV, tanto para a obtenção de semicondutores do tipo P como do tipo N.
Caso o tipo de impurezas dopantes seja doadora, isto é, tem elétrons de valência "dispostos" a saírem de seu orbital, o tipo do semicondutor é N. Isto acontece pois o semicondutor vai ter um excesso de elétrons face ao número de buracos (cargas portadoras de sinal contrário ao dos elétrons). O excesso de elétrons ocorre devido à proximidade dos níveis de energia da impureza à banda de condução do semicondutor. Quando o material dopante é adicionado, este aporta seus elétrons mais fracamente ligados aos átomos do semicondutor. Este tipo de agente dopante é também conhecido como material doador já que cede um de seus elétrons ao semicondutor. O propósito da dopagem tipo N é o de produzir abundância de elétrons livres no material.
Analogamente ocorre caso a impureza seja aceitador, isto é, com as orbitais semipreenchidas, capazes de aceitar elétrons. Irá neste caso ocorrer um excesso de buracos face ao número de eletrões, pois parte destes em vez de se recombinarem com os buracos, foram aceitados pelas impurezas. Neste caso é tipo P. O propósito da dopagem tipo-P é criar abundância de lacunas. Por exemplo, uma impureza trivalente deixa uma ligação covalente incompleta, fazendo com que um dos átomos vizinhos ceda-lhe um elétron completando assim as suas quatro ligações. Assim os dopantes criam as lacunas. Cada lacuna está associada a um íon próximo carregado negativamente, portanto, o semicondutor mantém-se eletricamente neutro. Entretanto, quando cada lacuna se move pela rede, um próton do átomo situado na posição da lacuna se vê "exposto" e logo se vê equilibrado por um elétron. Por esta razão uma lacuna comporta-se como uma carga positiva. Quando um número suficiente de aceitadores de carga são adicionados, as lacunas superam amplamente a excitação térmica dos elétrons. Assim, as lacunas são os portadores majoritários, enquanto os elétrons são os portadores minoritários nos materiais tipo P.
Os semicondutores (não degenerados) tipo-N têm o nível de Fermi mais próximo da banda de condução, enquanto que os tipo-P, têm o nível de Fermi mais próximo da banda de valência
Os elétrons enquanto partículas quânticas obedecem a dois postulados.
Como tal, é possível descrever a distribuição de elétrons pelos estados quânticos de energia W disponíveis através da função de Fermi-Dirac.
,
Com  a energia do estado quântico a considerar, , a energia de Fermi, , a temperatura e , a constante de Boltzmann.
Esta função com contradomínio de 0 a 1, indica o grau de preenchimento àquela energia por parte dos elétrons.
A energia de Fermi define-se como:
A função de densidade de estados de energia[3] na banda de condução é dada pela expressão:
com
Onde, , é a constante de Planck, , a massa eficaz do elétron, , energia mínima da banda de condução.
A densidade em volume de elétrons relaciona-se com o produto destas duas funções, a primeira indica a probabilidade de encontrar um elétron num certo estado quântico e a segunda, a densidade de estados a uma certa energia. Então:
Tanto eletrões como buracos são responsáveis pelo movimento de cargas. No processo de movimento destas partículas num semicondutor, na ausência de forças exteriores, considera-se um movimento aleatório, sendo o valor de velocidade média destas partículas nulo, logo não existe corrente elétrica. Mas quando aplicado um campo elétrico, aparece uma força, que altera o comportamento cinético das partículas, ficando-se agora com um movimento não completamente aleatório, mas sim orientado no sentido das linhas do campo elétrico, no caso dos buracos, ou orientado no sentido oposto, no caso dos eletrões.
Definindo mobilidade dos eletrões como  e mobilidade dos buracos como , tem-se uma formula para a condutividade do semicondutor.
Onde  é o módulo da carga do eletrão e  são as respectivas densidades de eletrões e de buracos.
Existem vários tipos de semicondutores compostos, binários, ternários e quaternários.
Os semicondutores binários mais usuais são geralmente feitos com dois elementos do grupo IV, ou como combinação entre dois elementos dos grupos III e V, ou II e VI.
Os ternários obtêm-se através da combinação de dois binários com um elemento em comum, e os outros dois pertencendo ao mesmo grupo da Tabela Periódica. Respeitando a fórmula:
 com  entre 0 e 1 e que tem significado de concentração relativa do elemento.
Por sua vez, os compostos quaternários, são feitos a partir da combinação de 4 compostos binários. De maneira análoga aos ternários:
As aplicações dos semicondutores compostos são variadas, tais como em LASER e outros dispositivos optoeletrónicos.
Sistema Internacional de Unidades[1] (sigla SI, do francês Système International d'unités)[2] é a forma moderna do sistema métrico e é geralmente um sistema de unidades de medida concebido em torno de sete unidades básicas e da conveniência do número dez. É o sistema de medição mais usado do mundo, tanto no comércio todos os dias e na ciência.[3][4] O SI é um conjunto sistematizado e padronizado de definições para unidades de medida, utilizado em quase todo o mundo moderno, que visa a uniformizar e facilitar as medições e as relações internacionais daí decorrentes.[5]
O antigo sistema métrico incluía vários grupos de unidades. O SI foi desenvolvido em 1960 do antigo sistema metro-quilograma-segundo, ao invés do sistema centímetro-grama-segundo, que, por sua vez, teve algumas variações.[6] Visto que o SI não é estático, as unidades são criadas e as definições são modificadas por meio de acordos internacionais entre as muitas nações conforme a tecnologia de medição avança e a precisão das medições aumenta.
O sistema tem sido quase universalmente adotado. As três principais exceções são a Myanmar, a Libéria e os Estados Unidos. O Reino Unido adotou oficialmente o Sistema Internacional de Unidades, mas não com a intenção de substituir totalmente as medidas habituais.
Considera-se que a primeira ideia de um sistema métrico seja do filósofo John Wilkins, primeiro secretário da Royal Society de Londres em 1668.[7][8] Porém não teve sucesso,[7][8] sendo na França onde o sistema unificado realmente saiu do papel.[7][8] Em 1875, o tratado internacional Convenção do Metro (do francês Convention du Mètre) foi assinado por vários países, para estabelecer organizações responsáveis por um sistema uniforme de medidas.[7][8] Que em 1889, definiram os protótipos internacionais de metro e quilograma na Primeira Conferência Geral de Pesos e Medidas.
Existem grandezas que não apresentam unidades de medida, resultados da divisão entre grandezas iguais.[6] Existem outras não derivadas das básicas, como por exemplo o número de moléculas de uma substância determinadas por contagem.[6]
Para efetuar medidas é necessário fazer uma padronização, escolhendo unidades para cada grandeza. Antes da instituição do Sistema Métrico Decimal, as unidades de medida eram definidas de maneira arbitrária, variando de um país para outro, dificultando as transações comerciais e o intercâmbio científico entre eles. As unidades de comprimento, por exemplo, eram quase sempre derivadas das partes do corpo do rei de cada país: a jarda, o pé, a polegada e outras. Até hoje, estas unidades são usadas nos Estados Unidos, embora definidas de uma maneira menos individual, mas através de padrões restritos às dimensões do meio em que vivem e não mais as variáveis desses indivíduos.[7][8]
Em 1585, o matemático flamengo Simon Stevin publicou um pequeno panfleto chamado La Thiende, no qual ele apresentou uma conta elementar e completa de frações decimais e sua utilização diária. Embora ele não tenha inventado as frações decimais e sua notação, ele estabeleceu seu uso na matemática do dia-a-dia. Ele declarou que a introdução universal da cunhagem decimal, medidas e pesos seria apenas uma questão de tempo. No mesmo ano, ele escreveu La Disme sobre o mesmo assunto.[7][8]
Há registros de que a primeira ideia de um sistema métrico seja de John Wilkins, primeiro secretário da Royal Society de Londres em 1668, porém a ideia não vingou e a Inglaterra continuou com os diferentes sistemas de pesos e medidas.[7][8]
Foi na França onde a ideia de um sistema unificado saiu do papel. A proliferação dos diferentes sistemas de medidas foi uma das causas mais frequentes de litígios entre comerciantes, cidadãos e cobradores de impostos. Com o país unificado com uma moeda única e um mercado nacional havia um forte incentivo econômico para romper com essa situação e padronizar um sistema de medidas. O problema inconsistente não era as diferentes unidades, mas os diferentes tamanhos das unidades. Ao invés de simplesmente padronizar o tamanho das unidades existentes, os líderes da Assembleia Nacional Constituinte Francesa decidiram que um sistema completamente novo deveria ser adotado.[7][8]
O Governo Francês fez um pedido à Academia Francesa de Ciências para que criasse um sistema de medidas baseadas em uma constante não arbitrária. Após esse pedido, um grupo de investigadores franceses, composto de físicos, astrônomos e agrimensores, deu início a esta tarefa, definindo assim que a unidade de comprimento metro deveria corresponder a uma determinada fração da circunferência da Terra e correspondente também a um intervalo de graus do meridiano terrestre. Em 22 de junho de 1799 foi depositado, nos Arquivos da República em Paris, dois protótipos de platina iridiada, que representam o metro e o quilograma, ainda hoje conservados no Escritório Internacional de Pesos e Medidas (Bureau international des poids et mesures) na França.[7][8]
Em 20 de maio de 1875 um tratado internacional conhecido como Convention du Mètre (Convenção do Metro), foi assinado por 17 Estados. Este tratado estabeleceu as seguintes organizações para conduzir as atividades internacionais em matéria de um sistema uniforme de medidas:
Em 1889, a Primeira Conferência Geral de Pesos e Medidas (I CGPM) definiu os protótipos internacionais de metro e quilograma e as próximas conferências definiram as demais unidades que hoje são as bases do SI. A partir da criação destas organizações todo e qualquer assunto relacionado a medição são de sua responsabilidade. Mais tarde, a CGPM estabeleceu que o sistema métrico internacional seria designado Sistema Internacional, com abreviatura SI em todos os idiomas. O SI foi adotado globalmente por praticamente todos os países. As três exceções são Myanmar, Libéria e os Estados Unidos. Com o passar do tempo outras unidades foram adicionadas ao SI nas posteriores CGPMs: ampère (corrente elétrica) em 1946, kelvin (temperatura absoluta) e candela (luminosidade) em 1954 e mol (quantidade de matéria) em 1971.[7][8]
Em 1955, foi criada a Organização Internacional de Metrologia Legal (OIML), que harmoniza internacionalmente as legislações dos países em relação às unidades de medida. 
Em 1960, o "Sistema Métrico Decimal" foi substituído pelo "Sistema Internacional de Unidades" (SI), mais sofisticado que o anterior, criado na 11ª Conferência Geral de Pesos e Medidas (CGPM), na tentativa de padronizar as unidades de medida, com objetivo de facilitar seu uso tornando acessíveis. A 11ª CGPM define um grupo de sete grandezas independentes denominadas de "Grandezas de Base", que a partir dessas, as demais grandezas são definidas, denominadas "Grandezas Derivadas". 
Em novembro de 2018, durante a 26ª reunião da Conferência Geral de Pesos e Medidas (CGPM),[9] a Metrologia deu um passo histórico, pois o Escritório Internacional de Pesos e Medidas (BIPM)[10] revisou as definições para o quilograma, ampère, kelvin e mol. Essa revisão se tornou efetiva a partir de 20 de maio de 2019.[11][12]No sistema em vigor até 19 maio de 2019, os valores das constantes fundamentais eram determinados a partir de experimentos. O quilograma era definido a partir de um protótipo internacional, um cilindro formado por liga de platina e irídio e, essa era a unidade utilizada para determinar a massa de um próton, de um elétron ou de outras partículas elementares. Isso levava à situação notável de que os valores das constantes fundamentais estavam em um estado permanente de mudança, já que nossas capacidades de medição eram refletidas nesses valores. A cada quatro anos, por exemplo, um novo valor numérico era atribuído à carga de um elétron; a carga em si não mudava, a mudança ocorria em nossa capacidade de medir, portanto, nossa compreensão do mundo.[12]
Em nosso mundo de alta tecnologia, no qual o nanômetro há muito tempo se tornou comum, qualquer mudança de tamanho em um protótipo têm um impacto significativo na definição de uma unidade e, portanto, deve ser evitada. A menor variação na temperatura leva a uma mudança no comprimento do protótipo, e os resultados ficariam ainda piores caso o protótipo fosse danificado. A solução para esse problema é evitar o uso de uma medida material, como um protótipo, para definir uma unidade e buscar uma constante fundamental. As constantes fundamentais são propriedades físicas invariantes, como a velocidade da luz ou a carga de um elétron.[12]
Veja como eram e como ficaram as unidades de base do SI, após a redefinição.[11][12][15][16]
Definiram-se sete grandezas físicas postas como básicas ou fundamentais, passando a existir sete unidades básicas correspondentes — as unidades bases do SI dimensionalmente axiomáticas e independentes entre si[17] — descritas na tabela. A partir delas, derivam todas as outras unidades existentes. As unidades bases são (m, kg, s, A, K, mol, cd):[18]
Consideram-se unidades derivadas do SI apenas aquelas que podem ser expressas através das unidades básicas do SI e sinais de multiplicação e divisão.[20] Desse modo, há apenas uma unidade do SI para cada grandeza. Contudo, para cada unidade do SI pode haver várias grandezas. Às vezes, dão-se nome especial e símbolo particular às unidades derivadas (rad, sr, kat, Bq, F, C, S, Gy, Sv, J, lm, Wb, N, Hz, H, T, lx, W, Pa, omega, ºC, V):[18]
Até 1995, havia duas unidades suplementares: o radiano e o esferorradiano (esterradiano, em Portugal).[18] Uma resolução da CGPM (Conferência Geral de Pesos e Medidas) de então tornou-as derivadas.
É fácil de perceber que, em tese, são possíveis incontáveis (por extensão, "infinitas") unidades derivadas do SI (por exemplo; m², m³, etc.), tantas quantas se possam imaginar com base nos princípios constitutivos fundamentais. As tabelas que se seguem não pretendem ser uma lista exaustiva. São, tão somente, uma apresentação organizada, tabelada, das unidades do SI das principais grandezas, acompanhadas dos respectivos nomes e símbolos. Na primeira tabela, unidades que não fazem uso das unidades com nomes especiais:[carece de fontes?]
Na segunda tabela, as que fazem uso na sua definição das unidades com nomes especiais.[carece de fontes?]
Existem grandezas que não apresentam unidades de medida, as Grandeza Adimensional, pois são resultados da divisão entre duas grandezas iguais, como por exemplo o índice de refração, razão entre duas velocidades.[21]
Também existem grandezas que não são derivadas das Grandezas de Base, como por exemplo o número de moléculas de uma substância que são determinadas por meio de contagem, sendo assim chamadas Grandezas de Contagem.[21]
O SI aceita várias unidades que não pertencem ao sistema. As primeiras unidades deste tipo são unidades muito utilizadas no cotidiano:[carece de fontes?]
A relação entre o neper  e o bel é: 1 B = 0,5 ln(10) Np. Outras unidades também são aceites pelo SI, mas possuem uma relação com as unidades do SI determinada apenas por experimentos:[carece de fontes?]
Por fim, tem-se unidades que são aceites temporariamente pelo SI. Seu uso é desaconselhado.[carece de fontes?]
Os prefixos do SI permitem escrever quantidades sem o uso da notação científica, de maneira mais clara para quem trabalha em uma determinada faixa de valores. Os prefixos oficiais são:[22][23]
Para utilizá-los, basta juntar o prefixo aportuguesado e o nome da unidade, sem mudar a acentuação, como em nanossegundo, microssegundo, miliampere e deciwatt. Para formar o símbolo, basta juntar os símbolos básicos: nm, µm, mA e dW.[carece de fontes?]
Exceções
Observações
O nome das unidades deve ser sempre escrito em letra minúscula.[carece de fontes?]
Exemplos:[carece de fontes?]
Para a pronúncia correta do nome das unidades, deve-se utilizar o acento tônico sobre a unidade e não sobre o prefixo.[carece de fontes?]
Ao escrever uma unidade composta, não se deve misturar o nome com o símbolo da unidade.[carece de fontes?]
As unidades do SI podem ser escritas por seus nomes ou representadas por meio de símbolos.[carece de fontes?]
O resultado de uma medição deve ser representado com o valor numérico da medida, seguido de um espaço de até um caractere e, em seguida, o símbolo da unidade em questão.[carece de fontes?]
Exemplo:
Para a unidade de temperatura grau Celsius, haverá um espaço de até um caractere entre o valor e a unidade, porém não se porá espaço entre o símbolo do grau e a letra C para formar a unidade "grau Celsius".[carece de fontes?]
Os símbolos das unidades de tempo hora (h), minuto (min) e segundo (s) são escritas com um espaço entre o valor medido e o símbolo. Também há um espaço entre o símbolo da unidade de tempo e o valor numérico seguinte.[25]
Um dos objetivos da União Europeia (UE) é a criação de um mercado único para o comércio. Para atingir este objetivo, a UE estabeleceu como padrão o uso do SI como unidades legais de medida. A partir de 2009, foram emitidas duas diretivas de unidades de medida que catalogaram as unidades de medida que podem ser usadas para, dentre outras coisas, o comércio: a primeira foi a Diretiva 71/354/CEE[26] publicada em 1971, que exigia dos estados-membros que padronizassem no SI, em vez de utilizar a variações dos sistemas CGS e MKS então em uso. A segunda foi a Diretiva 80/181/CEE[27][28][29][30][31] publicada em 1979, que substituiu a primeira e deu ao Reino Unido e à República da Irlanda um número de derrogações à diretiva original.
Cereais são as plantas cultivadas por seus frutos (do tipo cariopse) comestíveis, normalmente chamados grãos e são na maior parte gramíneas, compondo uma família com mais de 6 mil espécies.[1] Os cereais são produzidos em todo o mundo, em maiores quantidades do que qualquer outro tipo de produto e são os que mais fornecem calorias ao ser humano. Em alguns países em desenvolvimento, os cereais constituem praticamente a dieta inteira da população.
Nos países desenvolvidos, o consumo de cereal é mais moderado mas ainda substancial. A palavra cereal tem sua origem na deusa romana do grão, Ceres. O trigo sarraceno, a quinoa e o amaranto são plantas consideradas pseudocereais, plantas de famílias diferentes a dos cereais mas que apresentam valores proporcionalmente próximos de carboidratos, lipídeos, proteínas e fibras em relação aos cereais. Destacam-se pelo alto teor e qualidade da proteína, com ausência de glúten, possuindo ainda algumas vitaminas e minerais em maior quantidade.[2]
Com o fim do último período glacial, ocorrido há 10 mil anos, houve o estímulo a uma intensa migração de animais e o surgimento de plantas em regiões antes inóspitas ao desenvolvimento destas, o que acabou por favorecer a migração de populações humanas para outras áreas. Com o advento de novas tecnologias e o início do domínio do fogo a humanidade passa a se fixar, época que ficou conhecida como Revolução Neolítica. A partir de então tem início a domesticação de animais e vegetais pelo homem.[3] Pelo abandono paulatino do nomadismo e seminomadismo em preferência ao assentamento fixo e de caráter permanente, é que se pode falar em uma domesticação do ser humano pelos animais e vegetais, em menção à sua dependência para com seres e, consequentemente, ao impacto exercido na organização da vida humana.
Estima-se que os primeiros grãos de cereais tenham sido domesticados cerca de 11 000 anos atrás por comunidades agrícolas antigas na região do Crescente Fértil. Por meio de registros arqueológicos datados de 10 000 a.C, sabe-se que pequenas aldeias agrícolas da Palestina recolhiam intensamente cereais selvagens, e que a semeadura desses cereais foi detectada na Síria por volta de 9 000 a.C.. A expansão da cultura de cereais como o trigo acaba por impulsionar também a domesticação e cultivo de outros vegetais, tais como as leguminosas ervilha e lentilha.[4]
As primeiras espécies cereais envolvidas no processo de domesticação foram as variedades selvagens de trigo Triticum boeticum e Triticum dicoccoides, de cevada a Hordeum spontaneum e do gênero Aegilops a  Aegilops squarrosa.[5]
A domesticação de espécies vegetais acaba por tornar essa espécie totalmente dependente do homem, seja para sua disseminação ou desenvolvimento, além de tornar a mesma geneticamente distinta da variedade selvagem da qual se origina[6]:
A domesticação do trigo ocorreu por meio da seleção artificial daqueles espécimes com sementes mais resistentes e mais aderidas, facilitando o transporte do campo para o local de debulha, o que não era possível na variedade selvagem, onde as sementes se desprendiam facilmente.[5]
Um exemplo que evidencia essa maior dependência do vegetal está na domesticação do milho. Quando o comparamos ao teosinto, seu espécime ancestral, vemos que seus grãos são aderidos ao sabugo e envoltos por palha.[6] Além disso, todas as sementes germinam ao mesmo tempo, impedindo que qualquer uma das mudas possa se desenvolver, a não ser que antes do plantio os grãos sejam separados da espiga pelo homem.[7]
Acredita-se que o milho tenha sido domesticado inicialmente no México, a partir da gramínea teosinto, na região do rio Balsas. Estudos no local confirmam que esta ocorreu no início do Holoceno. Com o desenvolvimento de seu plantio, este se estendeu para o Panamá há 7,6 mil anos, chegando na América do Sul há 6 mil anos.[8] Quando da chegada de Cristóvão Colombo a América, o milho era cultivado desde a Argentina até o Canadá sendo, dentre os vegetais, a base alimentícia dos indígenas que aqui viviam.[9]
A origem do arroz é incerta, e o mesmo pode ser dito de sua domesticação, que se acredita possa ter ocorrido na região de Korat, ou em algum vale ao norte da Tailândia ou no Planalto de Shan em Myanmar, ou ainda em Assam na Índia. Sua dispersão pelo mundo só veio a ocorrer na Era das Grandes Navegações pelos europeus que já conheciam a cultura devido às incursões de Alexandre Magno a Índia. O primeiro registro na América do Norte é datado de 1685, na Carolina do Sul; no Brasil, seu cultivo foi introduzido pelos portugueses, sendo o primeiro registro na capitania de São Vicente, já as primeiras lavouras ocorreram em 1587 na Bahia.
O fato de diversas variedades de arroz, inclusive as consideradas envolvidas na domesticação do mesmo, serem encontradas entre essa regiões corrobora a ideia de que o sudeste asiático possa ser o berço do cultivo de arroz. Evidências arqueológicas confirmam o uso deste cereal em 4 000 a.C. na região de Korat, Tailândia, que juntamente com evidências do uso de plantas a 10 000 a.C. encontradas na Caverna dos Espíritos, na fronteira entre o Myanmar e a Tailândia, faz supor que a agricultura possa ter se iniciado há mais tempo do que se pensa.
Assim como outros cereais, o arroz sofreu alterações durante seu processo de domesticação, dentre as quais tem-se o exemplo de uma variação genética entre espécies selvagens e variedades domesticadas que tornou o talo desta última mais robusto, além de ter aumentado a sua produção. Ao se compararem as variedades percebe-se também que as espécies selvagens têm mais diversidade genética do que as domésticas.[10]
Os grãos são compostos por três partes
Cada espécie de cereais apresenta características próprias e muitas vezes distintas, porém o seu cultivo é muito semelhante. Todos são plantas anuais, isto é, produzem apenas uma vez no seu ciclo de vida de um ano.[11] O trigo, o centeio, a aveia, a cevada, entre outros cereais, são considerados plantas de clima frio, que crescem bem em clima moderado mas param o seu desenvolvimento em períodos de clima mais quente, cerca de 30 °C a depender da espécie, o contrário se aplica a cereais que se enquadram como plantas de clima quente como o milho, o milheto e o sorgo cultivados em planícies baixas tropicais ao longo de todo o ano, além de regiões de clima temperado.[12]
Cereais de clima frio são bem adaptados a climas temperados. A maioria das variedades de uma espécie em particular são ou do tipo de inverno ou de primavera. As variedades de inverno são semeadas no outono, germinam e crescem vegetativamente, então adormecem durante o inverno, que nas zonas temperadas do hemisfério Norte ocorre de dezembro a março. Elas só retomam o crescimento na primavera, amadurecendo até o início do verão. Este sistema de cultivo faz uso otimizado da água e libera terra para outra safra no início da temporada de crescimento. Variedades de inverno não florescem até a primavera porque elas necessitam da vernalização. As variedades de primavera são cultivadas em locais em que não se alcança a vernalização ou que excedem a rusticidade da planta, estes são plantados no início da primavera e amadurecem no verão, eles requerem menos irrigação, porém rendem menos do que os cereais de inverno.[13]
O centeio é o cereal mais rústico, suportando o inverno no sub-ártico da Sibéria. O trigo, por sua vez, é o mais popular e, apesar de ser de estações frias, pode ser cultivado nos trópicos, em regiões de clima mais ameno, na realidade todos os cereais de clima frio podem ser cultivados em regiões tropicais, desde que suas necessidades climáticas sejam atendidas[12].
Dentre os principais cultivos por área cultivada no mundo temos grande participação de cultivos de cereais, cerca de metade da terras agrícolas do globo é ocupada pelos três principais grãos.[14]
A tabela a seguir mostra a produção de cereais no mundo nos anos de 2010 (último ano com levantamento) a 2008, a título de comparação há os dados levantados no ano de 1961, quando se iniciou o levantamento.
Grãos de milho
Grãos de milheto
Grãos de arroz branco e arroz integral
Grãos de aveia
Grãos de Sorgo
Grãos de Centeio
Milho - gramínea
Milheto - gramínea
Arroz - gramínea
Aveia - gramínea
Sorgo - gramínea
Centeio - gramínea
Desde antes do advento da agricultura que os cereais fazem parte do hábito alimentar da humanidade, principalmente devido a sua facilidade de manutenção e conservação além de seu baixo custo e do seu alto valor nutritivo. Neles encontramos diversos nutrientes tais como carboidratos, proteínas, gorduras, sais minerais, vitaminas e enzimas; os cereais integrais possem ainda alto teor de fibras.
Dentre os nutrientes os carboidratos são os que aparecem em maior proporção por grão, com valores em torno de 78 a 83% a depender do cereal, sendo quase em sua totalidade o amido. O glúten é uma substância presente nos cereais, especialmente no trigo, formado por duas proteínas: gliadina e glutenina. As gorduras por sua vez são principalmente trigliceróis. Dentre os sais temos o sódio, o potássio, o cloro, o fósforo, o cálcio, o magnésio, o enxofre e o ferro. As vitaminas encontradas são as do complexo B, principalmente a B1, no germe e a B2 mais distribuída no grão. A vitamina E, é encontrada principalmente no germe. Os cereais são deficientes nos aminoácidos lisina, treonina e triptofano[29].
Em geral os nutrientes estão assim distribuídos nos cereais[30]:
Quando os cereais são moídos, ou refinados, temos a remoção do farelo e do gérmen, permanecendo apenas o endosperma. Desta maneira a maior parte do valor nutricional do cereal é perdida, o que não ocorre com os cereais integrais.[30]
Abaixo temos a composição dos principais cereais:
Bespalhok F., J.C.; Guerra, E.P.; Oliveira, R. Melhoramento de Plantas. Disponível em www.bespa.agrarias.ufpr.br
Jorge, M. H. A.; A domesticação de plantas nativas do Pantanal. Documentos nº70. Corumbá: Embrapa Pantanal, 2004.
Banana, pacoba ou pacova[1] é uma pseudobaga da bananeira, uma planta herbácea vivaz acaule da família Musaceae (género Musa - além do género Ensete, que produz as chamadas "falsas bananas"). São cultivadas em 130 países. Originárias do sudeste da Ásia são atualmente cultivadas em praticamente todas as regiões tropicais do planeta.
Vulgarmente, inclusive para efeitos comerciais, o termo "banana" refere-se às frutas de polpa macia e doce que podem ser consumidas cruas. Contudo, existem variedades de cultivo, de polpa mais rija e de casca mais firme e verde, geralmente designadas por plátanos, em língua espanhola, banana-pão ou banana-da-terra, em português, ou plantains, em inglês, que são consumidas cozinhadas (assadas, cozidas ou fritas), constituindo o alimento base de muitas populações de regiões tropicais. A maioria das bananas para exportação é do primeiro tipo, ainda que apenas 10 a 15 por cento da produção mundial seja para exportação, sendo os Estados Unidos e a União Europeia as principais potências importadoras.
As bananas formam-se em cachos na parte superior dos "pseudocaules" que nascem de um verdadeiro caule subterrâneo (rizoma ou cormo) cuja longevidade chega a 15 anos ou mais. Depois da maturação e colheita do cacho de bananas, o pseudocaule morre (ou é cortado), dando origem, posteriormente, a um novo pseudocaule.
As pseudobagas formam-se em "pencas" com até cerca de vinte bananas. Os cachos de bananas, pendentes na extremidade do falso caule da bananeira, podem ter 5 a 20 pencas e podem pesar de 30 a 50 kg. Cada banana pesa, em média, 125g, com uma composição de 75% de água e 25% de matéria seca. Bananas são fonte apreciável de vitamina A, vitamina C, fibras e potássio.
Ainda que as espécies selvagens apresentem numerosas sementes, grandes e duras, quase todas as variedades de banana utilizadas na alimentação humana não têm sementes, como frutos partenocárpicos que são, exceção feita à espécie Musa balbisiana, comercializada no mercado indonésio, excepcionalmente com sementes.
Devido ao elevado teor de potássio em sua composição, as bananas são levemente radioativas,[2][3] mais do que a maioria dos outros frutos, sendo, inclusive, uma fonte natural de antimatéria (ela produz, em média, um pósitron aproximadamente uma vez a cada 75 minutos).[4] Isso se deve à presença do isótopo radioativo potássio-40 (40K), regularmente distribuído no potássio ocorrente na natureza, apesar de que o isótopo comum, potássio-39 (39K), seja não-radioativo. Por esta razão, os ambientalistas em energia nuclear, por vezes, costumam referir-se à "dose equivalente em banana" de radiação para apoiar seus argumentos durante debates em congressos e encontros sobre a matéria.[5][6] Embora a radioatividade da banana seja muito leve, todavia, grandes carregamentos da fruta em navios podem ser suficientes para disparar detectores ou sensores de radiação em determinadas circunstâncias.[7]
"Banana" é um termo com origem na Guiné.[8] "Pacoba" e "pacova" se originaram do termo tupi pa'kowa, que significa "folha de enrolar".[9][carece de fontes?]
É de cor verde, quando imatura, chegando a amarela ou vermelha, quando madura. Seu formato é alongado, podendo, contudo, variar muito na sua forma a depender das variedades de cultivo. Essa variação também acontece com a polpa, que pode ser mole ou dura, ou ainda com incrustações meio duras, bem como de sabor mais doce ou mais acre. Assim como o abacaxi, a banana também é fruto partenocárpico, pois pode formar-se sem fecundação prévia. É por isso que não possui sementes. Depois de cortada, a banana escurece-se muito rapidamente, devido à oxidação (pela presença da polifenoloxidase) em contato com o ar.
A espécie Musa balbisiana, comercializada no mercado indonésio contém, excepcionalmente, sementes, e é considerada uma das espécies ancestrais das actuais variedades híbridas das bananas geralmente consumidas.
Da parte inferior do cacho da banana ainda imaturo (ou verde, como se usa dizer), sai um pendão e, em seu extremo, destaca-se um cone de coloração e consistência diferenciadas, que é a flor da bananeira. Popularmente, a flor da bananeira é chamada de umbigo [do cacho] da banana, coração da bananeira, mangará ou apenas umbigo da banana, que, cozido e preparado com outros ingredientes, é comestível de requintado sabor e alto valor nutricional.
Do cacho da banana sai um pendão. No final deste, há um cone roxo. Seu miolo é comestível e é conhecido como umbigo de banana, podendo também ser chamado de coração da bananeira.[11]
Várias receitas culinárias usam o umbigo da banana: cozido com bacalhau, ou carne moída, ou linguiça de porco defumada; temperado e refogado simples; entre outras.
A tradição popular reporta ainda outros usos para o umbigo da banana. Alguns preparados caseiros com fins medicinais, como xaropes, são considerados eficazes.[12]
Para ser utilizado como comestível (ou em preparações medicinais caseiras), o umbigo da banana precisa ser cortado, na posição certa (não muito próximo das pencas, apenas o bastante para se retirar o cone arroxeado do cacho ainda imaturo ou verde), o que favorece, pelo fluxo forçado da seiva, o amadurecimento do próprio cacho. Do cacho já maduro, não mais se aproveita o umbigo da banana, que terá escurecido e perdido o viço e uso culinário ou medicinal, aproveitando-se apenas como adubo.
Apesar de parecer não utilizável, a casca da banana contém vários nutrientes, açúcares naturais como a glicose e sacarose e minerais. Com isso, pode ser aproveitada no consumo alimentício, proporcionando baixo custo sem deixar para trás o bom paladar. São diversos os exemplos pelos quais se pode aproveitá-la, como o brigadeiro de casca de banana, o bolo de casca de banana, a farinha, o bife empanado de casca de banana e vários outros.
Valor nutritivo de 100 gramas de banana prata (valores apenas referenciais):
O cultivo de bananas pelo Homem teve início no sudeste da Ásia. Existem ainda muitas espécies de banana selvagem na Nova Guiné, na Malásia, Indonésia e Filipinas. Indícios arqueológicos e paleoambientais recentemente revelados em Kuk Swamp, na província das Terras Altas Ocidentais da Nova Guiné, sugerem que esta actividade remonta pelo menos a até 5 000 a.C., ou mesmo a até 8 000 a.C.. Tais dados tornam, esse local, o berço do cultivo de bananas. É provável, contudo, que outras espécies de banana selvagem tenham sido objecto de cultivo posteriormente, noutros locais do sudeste asiático.
A banana é mencionada em documentos escritos, pela primeira vez na história, em textos budistas de cerca de 600 a.C.. Sabe-se que Alexandre, o Grande comeu bananas nos vales da Índia em 327 a.C.. Só se encontram, porém, plantações organizadas de banana a partir do século III na China. Em 650, os conquistadores Islâmicos levaram-na à Palestina. Foram, provavelmente, os mercadores árabes que a divulgaram por grande parte de África, provavelmente até à Gâmbia. A palavra banana teve origem na África Ocidental e, adoptada pelos portugueses e espanhóis, veio a ser usada, por exemplo, na língua inglesa.
Nos séculos XV e XVI, colonizadores portugueses começaram a plantação sistemática de bananais nas ilhas atlânticas, no Brasil e na costa ocidental africana. Mas elas permaneceram desconhecidas, por muito tempo, da maior parte da população europeia. Por isso, Júlio Verne, na obra "A volta ao mundo em oitenta dias" (1872), descreve-a detalhadamente, pois sabe que grande parte dos seus leitores a desconhece.
Uma compilação dos nomes das espécies, subespécies, híbridos, variedades, assim como de nomes vulgares utilizados em várias línguas, é mantida na Universidade de Melbourne, Australia,[13] demonstrando que os nomes vulgares são apenas locais e não correspondem a espécies, nem a cultivares reconhecidos.
São por vezes reconhecidos quatro tipos principais de variedades de banana: a banana-prata, a banana-maçã (pequena e arredondada), a banana-caturra (também conhecida como banana-d'água ou cavendish), banana-figo[a] e a banana-da-terra.
Entre as bananas de mesa, contam-se as variedades maçã, ouro, prata e nanica (anã, baé, caturra, ou Dwarf Cavendish). Esta última deve o seu nome ao porte da bananeira sendo, na verdade, uma banana de grande dimensão. Outras variedades incluem a banana das Canárias, a banana da Madeira, a Gros Michel, a Latacan, a Nanican e a Grande Anã. A variedade Cambuta, como é designada em Cabo Verde, é resistente em climas mais frios, sendo a mais utilizada em zonas subtropicais e temperadas/quentes. A variedade Valery, introduzida pelos portugueses em São Tomé, em 1965 e depois em Angola, foi responsável por um surto na produção de bananas nesse país até 1974.
A banana, enquanto verde, é constituída essencialmente por água e amido, e, por isso, seu sabor é adstringente. Contudo, por essa mesma razão, pode ser utilizada como fonte de hidratos de carbono em vários pratos. Pode ser produzida farinha a partir de bananas verdes. À medida que vão amadurecendo, o amido transforma-se em açúcares mais simples, como a glicose e a sacarose, que lhe dão o sabor doce.
Além de consumida fresca, a banana é utilizada para diversos fins. Em sobremesas de colher, citam-se o banana split, ou mesmo as bananadas, feitas com banana-anã ou com banana-prata. Banana é também ingrediente indispensável na conhecida salada de frutas (ainda que oxide facilmente), podendo, também, ser utilizada na confecção de sangria. Mas a banana-pão é muito utilizada para outros fins culinários, como na confecção de banana chips — espécie de aperitivo feito com rodelas de banana desidratada ou frita, ou como acompanhamento de diversos pratos tradicionais. As bananas anã e prata são frequentemente servidas cruas, misturadas com arroz e feijão ou com outros acompanhamentos. Em alguns locais do Brasil, como em Antonina e cercanias, serve-se banana-da-terra crua acompanhando o prato típico da região — o barreado —, bem como na forma de "bala de banana". No Rio de Janeiro e em Pernambuco, o cozido é composto por carnes, tubérculos e legumes, além de banana-da-terra e banana-nanica. No sul de Minas Gerais, é famoso o virado de banana-nanica, que conta também com farinha de milho e queijo mineiro. No litoral norte de São Paulo, o prato principal da culinária caiçara chama-se "azul-marinho" e é constituído por postas de peixe cozidas com banana-nanica verde sem casca, acompanhadas de um pirão feito com o caldo do peixe, banana cozida amassada e farinha de mandioca. Esta comunidades também produzem, tradicionalmente, aguardente de banana.
Banana é também matéria-prima para a fabricação de outras bebidas, como a cerveja de banana. Esta bebida alcoólica é importante para a renda de países como a República Democrática do Congo.
A banana-da-terra e a banana-figo são utilizadas fritas, tal como a banana-anã, que deve, contudo, ser preparada à milanesa — isto é, passada por ovo batido e, depois, por farinha de trigo e farinha de rosca antes de ser frita, caso contrário, desmancha-se durante a fritura. A banana-anã é ainda utilizada para assar.
A banana-maçã é indicada para problemas intestinais, ao aumentar facilmente o volume da massa fecal, ainda que possa causar aparente obstipação.
A produção de sumo a partir de banana é dificultada pelo facto de se produzir apenas polpa quando o fruto é esmagado. Assim, não é possível obter "verdadeiro" sumo de banana, ainda que a sua polpa possa ser misturada ao sumo de outros frutos. Existem, contudo, sumos fermentados feitos a partir da polpa. Esta pode ainda ser utilizada na confecção de diversas compotas (especialmente com banana-figo e banana-anã).
Existem relatos de que seria usada, esmagada com mel, como remédio contra a icterícia em determinadas regiões asiáticas (onde o rizoma da bananeira é utilizado para o mesmo fim). Apesar de parecer não utilizável, a casca da banana contém vários nutrientes, açúcares naturais como a glicose e sacarose e minerais. Com isso, pode ser aproveitada no consumo alimentício, proporcionando baixo custo sem deixar para trás o bom paladar.
É ainda muito utilizada na alimentação de animais. É proverbial seu uso na alimentação dos macacos. Salienta-se, porém, que a banana jamais deve ser utilizada como única fonte de alimentação de macacos, pois contém pouco cálcio e muito fósforo,[15] causando desequilíbrio alimentar bastante comum, que prejudica a formação e a manutenção da estrutura óssea dos animais.
A bananeira tem sido uma fonte de fibra para tecidos de alta qualidade. No Japão, o cultivo de banana para vestuário e uso doméstico remonta pelo menos ao século XIII. No sistema japonês, folhas e brotos são cortados a partir da planta periodicamente para garantir a suavidade. Brotos colhidos são cozidos em primeiro em soda cáustica para preparar fibras para fazer fios têxteis. Esses brotos de banana produzem fibras de diferentes graus de maciez, produzindo fios e tecidos com diferentes qualidades para usos específicos. Por exemplo, as fibras ultraperiféricas da brotos são mais rudes, sendo adequados para toalhas de mesa, enquanto as fibras mais suaves da parte interna são desejáveis para quimonos e hakamas. Este tradicional processo japonês de fazer roupas requer muitos passos, todos feitos à mão.[16]
No sistema nepalês, ao contrário, o tronco é colhido e pequenos pedaços são submetidos a um processo de amaciamento, extração de fibras mecânicas, branqueamento e secagem. A seguir, enviam-se as fibras para o Vale de Katmandu, para uso em tapetes de seda com textura semelhante. Esses tapetes de fibra de bananeira são tecidos a mão pelos tradicionais métodos nepaleses e suas vendas são certificadas.
Apesar de o consumo das bananas ser prático e simples, o seu transporte, contudo, é delicado e requer cuidados especiais — amadurece rapidamente quando retirada de seu cacho e amassa com facilidade por ter uma casca não muito resistente. Além disso, como é uma fruta muito aromática, transfere o seu odor para objetos que com ela entrem em contato. A maior parte da produção para o mercado interno é constituída por bananas verdes para cozinhar ou bananas-pão - as variedades utilizadas como fruta são facilmente danificadas durante o seu transporte, mesmo quando transportadas apenas no seu país de origem.
As variedades comerciais de sobremesa mais consumidas nas regiões temperadas (espécies Musa acuminata ou o gênero híbrido Musa X paradisiaca) são importadas em larga escala dos trópicos. São muito populares também devido ao facto de constituírem uma fruta não sazonal, que pode ser consumida fresca durante todo o ano. No comércio global, a variedade de cultivo de maior importância económica é, de longe, a chamada banana banana-cavendish (banana-caturra, em cultura lusófona), que superou em popularidade, na década de 1950, a variedade Gros Michel, depois de esta ter sido dizimada pelo mal-do-panamá, um fungo que atacava raízes das bananeiras.
Tal como acontece com outros tipos de fruta, é comum que o mercado internacional seja monopolizado por pouco mais de uma variedade. Isso não se deve, contudo, ao sabor, mas às facilidades de transporte e de duração em armazenamento: de facto, as variedades mais comercializadas raramente são mais saborosas que outras menos cultivadas por razões económicas. As infrutescências (cachos) são colhidas quando estão plenamente desenvolvidas, se se destinarem ao mercado interno. Se forem para exportação, são colhidas ainda verdes e com cerca de 3/4 do tamanho que poderiam atingir, amadurecendo em armazéns destinados para esse efeito no país onde serão consumidas.
O momento da colheita exige grandes cuidados de modo a não machucar as bananas que perdem atractividade e qualidade se apresentarem manchas provocadas pelos choques. Os cachos são, então, despencados, ou seja, separados nas pencas que os constituem, rejeitando-se as pencas das extremidades (cerca de 25 por cento da produção), por serem mais sujeitas aos choques durante o seu transporte, bem como pela sua forma e tamanho pouco adequado para a comercialização e para um eficaz acondicionamento. Esses excedentes podem ser utilizados pela indústria transformadora de alimentos, na produção de "purés", polpas para a confecção de sumos (fermentados ou não) ou na alimentação de animais. Em muitos casos, os excedentes são, simplesmente, deitados fora.
As pencas são postas, então, em repouso para que exsudem a seiva em excesso, sendo depois lavadas e mergulhadas numa solução fungicida que evitará o apodrecimento a partir dos cortes. As pencas podem ainda ser cortadas em grupos (clusters) mais pequenos, de modo a aumentar a quantidade de fruta embalada por unidade de volume, geralmente em caixas de cartão que podem ser envolvidas por sacos de polietileno e que são embarcadas, salvo raras excepções, nos chamados "barcos fruteiros". Para retardar o amadurecimento, é necessário renovar o ar no local de transporte, para retirar o etileno, hormona produzida pelas bananas e que acelera a sua maturação.
Para induzir o amadurecimento das bananas, o ambiente do armazém pode ser preenchido com etileno. Contudo, se o fruto for comercializado verde, permitindo a maturação mais lenta, o sabor tornar-se-á mais agradável, e a polpa, mais firme, ainda que a casca possa ficar manchada de amarelo escura ou castanho. O sabor e a textura são, assim, afectados pela temperatura em que amadurecem. No transporte, elas são expostas a uma temperatura de cerca de 12 °C e a uma humidade relativa próxima da saturação. Em temperaturas mais baixas, contudo, a maturação é definitivamente inibida e as frutas tornam-se cinzentas.
O plantio da banana é feita por mudas e a colheita dos primeiros cachos ocorre entre 12 a 18 meses, dependendo do clima, variedade, fertilidade do solo, estado de sanidade da planta e tratos culturais.[17]
As bananas constituem o alimento básico de milhões de pessoas em vários países em via de desenvolvimento. Em determinados países tropicais a banana verde (não madura) é largamente utilizada da mesma forma que as batatas em outros países, podendo ser fritas, cozidas, assadas, guisadas etc. De facto, as bananas assim utilizadas são semelhantes à batata, não apenas no sabor e na textura, como a nível de composição nutricional e calórica.
Em 2005, a Índia liderou a produção mundial de bananas, representando cerca de 23% da produtividade mundial - sendo que a maioria se destina ao consumo interno. Os quatro países que mais exportam, contudo, são o Equador, a Costa Rica, as Filipinas, e a Colômbia, que somam cerca de dois terços das exportações mundiais, exportando cada um mais de um milhão de toneladas. De acordo com as estatísticas da FAO, só o Equador é responsável por mais de 30 por cento das exportações globais.
A maioria dos produtores, por todo o mundo praticam, contudo, uma agricultura de baixa escala e de subsistência - consumo próprio e venda e mercados locais. Já que as bananas são uma fruta não sazonal, estão disponíveis durante todo o ano, pelo que podem ser utilizadas durante as estações mais susceptíveis de escassez alimentar - alturas em que o produto de uma colheita já foi consumido enquanto que o produto da seguinte ainda não está disponível. É por esta razão que o cultivo de banana tem uma importância fulcral em qualquer sistema sustentado de luta contra a fome.
Nos últimos anos, a competição a nível de preços por parte dos supermercados tem diminuído ainda mais as já baixas margens de lucro da maioria dos produtores de banana. As principais empresas do ramo, como Chiquita, Fresh Del Monte Produce, Dole Food Company e Fyffes têm as suas próprias plantações no Equador, na Colômbia, na Costa Rica e Honduras. Tais plantações exigem grande e intensivo investimento de capital e de know how — tornando os proprietários das grandes e lucrativas plantações extremamente influentes em nível económico e político nos seus países, em detrimento dos pequenos produtores. Isso justifica o facto de elas estarem disponíveis como artigo de "comércio justo" em alguns países.
O comércio global de bananas tem uma longa história que começou com a fundação da United Fruit Company (hoje, Chiquita), no final do século XIX. Durante a maior parte do século XX, as bananas e o café dominaram por completo a economia de exportação da América Central. Na década de 1930, constituíam mais de 75 por cento das exportações da região, nos anos 60 ainda as preenchiam em 67 por cento. O termo  "República das Bananas" tornou-se vulgar, então, para designar a generalidade dos países da América Central, ainda que, sob o aspecto estritamente económico (sem conotação necessariamente depreciativa) apenas Costa Rica, Honduras, e Panamá assim possam ser designados, já que a sua economia é, de longe, dominada pelo comércio da banana.
Muitos países da União Europeia importam, tradicionalmente, muitas das bananas que consomem, das suas antigas colónias das Caraíbas, garantindo-lhes preços acima dos praticados no comércio global. Desde 2005, tais acordos estão em vias de serem revogados, devido à pressão de grupos económicos poderosos, a maioria dos quais com sede nos Estados Unidos. Tal alteração no comércio iria beneficiar os países produtores da América Central, onde várias empresas norte-americanas têm interesses estabelecidos.
A banana é o segundo fruto mais produzido (atrás da laranja) e consumido no Brasil,[18] tanto como sobremesa como acompanhamento nas refeições, ainda que ocupe apenas 0,87 por cento do total das despesas de alimentação dos brasileiros em geral (surge daí a expressão "a preço de banana" para referir que algo é pouco dispendioso). Em termos gerais, ainda que as condições naturais permitam uma produção de alta qualidade, é corrente afirmar que existe baixa eficiência na produção e no manejo pós-colheita.
Em 2018, o Brasil produziu 6,7 milhões de toneladas de banana em 461.751 hectares, sendo o 4º maior produtor do mundo, se considerarmos apenas as bananas comuns; somando-se a produção de plantain (banana-da-terra), o país é o 7º maior produtor mundial de bananas, no geral.[20] O estado que mais produz é São Paulo (1 milhão de toneladas), seguido por Bahia (828 mil toneladas), Minas Gerais (825 mil toneladas), Santa Catarina (723 mil toneladas) e Pernambuco (491 mil toneladas), entre outros. Todos os estados do Brasil tem produção de banana.[19] Ao contrário de outros países, que plantam banana para exportar, 98% da colheita brasileira de banana abastece o mercado interno. [21]
A Banana da Madeira é cultivada na ilha da Madeira, sendo a variedade cultivada, a Musa acuminata cavendish. Em 2012 a produção foi de 16 mil toneladas  e um volume de negócios de 12 milhões de euros, mas no final dos anos 1990 a produção atingia as 28 mil toneladas e a banana da Madeira representava 20% do consumo nacional. Estima-se que em 2014 a produção ronde as 18 mil toneladas.
Introduzidas na ilha no século XVI (existe uma referência escrita às bananeiras da Madeira que data de 1552), julga-se que terão vindo das Canárias ou de Cabo Verde, e tornaram-se parte integrante da paisagem.[22]
Uma das situações cómicas mais copiadas e parodiadas ao longo da história do cinema, desde o cinema mudo, consiste em mostrar as personagens a escorregar em cascas de banana. O estereótipo do macaco a comer bananas também é largamente explorado em filmes, animações e histórias em quadrinhos, tendo servido também para manifestações de cariz racista. De fato, por exemplo, há registro de pessoas que atiraram bananas a desportistas afro-americanos. A associação aos macacos justifica também o seu uso em jogos como as versões em 3D do Donkey Kong (Nintendo) e do Super Monkey Ball (Sega).
A banana também é frequentemente relacionada com a América Latina, a exemplo de Carmen Miranda e das canções Yes, nós temos bananas e Chiquita Bacana, ambas de Braguinha e Alberto Ribeiro. Em outras ocasiões (como no filme Bananas, de Woody Allen), o nome refere-se à expressão República das Bananas, que designa um país, geralmente do Caribe ou da América Central, onde há governos ditatoriais, instáveis, corruptos e com forte influência estrangeira.
Na China, o termo banana é usado no calão para designar qualquer pessoa de origem asiática que age como um ocidental (amarelos por fora, brancos por dentro). No Brasil, um gesto considerado obsceno e de mau gosto, denominado "dar uma banana", consiste em apoiar o braço ou a mão na dobra do outro braço, mantendo erguido e de punho fechado o antebraço que ficou livre.
Um boato muito divulgado, assegura que a casca seca de banana contém uma substância (na verdade, fictícia) designada como "bananadina", que seria alucinogénica quando fumada. Ao contrário de muitos boatos, a origem deste pode ser traçada. Terá tido origem num artigo do jornal "alternativo" Berkeley Barb em Março de 1967, e que foi posteriormente divulgada por William Powell (autor), que acreditou na sua veracidade, incluindo-a no seu The Anarchist Cookbook em 1970.
A canção de sucesso de Donovan, "Mellow Yellow", ao referir-se a uma "banana eléctrica", terá servido de inspiração aos jornalistas do Berkeley Barb que pretendiam, satiricamente, que o governo proibisse a comercialização de bananas. De facto, Donovan referia-se apenas a um vibrador. Contudo, é o próprio autor da canção a referir que o rumor deve ter tido origem no cantor popular Country Joe McDonald que o começou em San Francisco, uma semana antes da publicação da canção de Donovan. O boato voltou a circular na década de 1980, quando o grupo de punk satírico, The Dead Milkmen voltou a referir numa canção os supostos efeitos do acto de fumar casca seca de banana. O boato levou, mesmo a Food and Drug Administration (FDA) a investigar o caso.
Uma chávena ou xícara com chá Earl Grey.editar - editar código-fonte - editar WikidataO chá é uma bebida preparada através da infusão de folhas, flores, raízes de planta do chá (Camellia sinensis), geralmente preparada com água quente. Cada variedade adquire um sabor definido, de acordo com o processamento utilizado, que pode incluir oxidação, fermentação, e o contato com outras ervas, especiarias e frutos. A palavra "chá" é usada popularmente em Portugal e no Brasil, como sinónimo de infusão de frutos, folhas, raízes e ervas, contendo ou não folhas de chá (ver tisana). Este artigo trata do chá em sentido estrito e, portanto, não se refere a infusões como, por exemplo, a camomila ou a cidreira.
Historicamente, a origem do chá como erva medicinal útil para se manter desperto não é clara. O uso do chá, enquanto bebida social, data, pelo menos, da época da dinastia Tang. Pouco há sobre as origens do chá. Sabemos apenas que a bebida pode ter sido consumida pela primeira vez na China há mais de 4000 anos.[1] Fontes mitológicas e populares frequentemente atribuem a origem da bebida ao mítico governante e herói chinês Shennong (神農), o Imperador Vermelho. O escritor Lu Yu narra que o Imperador teria descoberto, acidentalmente, a bebida. De acordo com o mito, Shennong estava esquentando água à sombra de uma pequena árvore silvestre que balançava com o vento.[2] Casualmente uma folha teria se desprendido dos galhos e caído na água fervente, criando uma infusão.[3] O sabor surpreendeu o monarca, que se sentiu profundamente revigorado após terminar a bebida. Assim teria nascido a primeira xícara de chá.[1]
Os primeiros europeus a contactar com o chá foram os portugueses, que chegaram ao Japão em 1543. Em breve, a Europa começou a importar as folhas, tornando-se a bebida rapidamente popular, especialmente entre as classes mais abastadas na França e nos Países Baixos. O uso do chá na Inglaterra é atribuído a Catarina de Bragança, princesa portuguesa que casou com Carlos II da Inglaterra e, assim, pode ser situado cerca de 1660. Catarina patrocinava "Tea parties", onde o chá passou a ser apreciado pelas mulheres e, posteriormente, daí passou a ser também do gosto masculino. O chá era bebido em cafés e o seu consumo foi crescendo desde o final do século XVII, sendo bebido a qualquer hora do dia até o início do século XIX, quando a tradição chá da tarde ("five o'clock tea") foi instituída pela sétima Duquesa de Bedford em Londres.
O caractere chinês para chá é 茶, mas tem duas formas completamente distintas de se pronunciar.[1] Uma é 'te' que vem da palavra malaia para a bebida, usada pelo Dialeto Min que se encontra em Amoy. Outra é usada em cantonês e mandarim, que soa como cha e significa 'apanhar, colher'.[4] Esta duplicidade fez com que o nome do chá nas línguas não chinesas as dividisse em dois grupos: (1) línguas que usam derivados da palavra Te: alemão, inglês, francês, dinamarquês, hebraico, húngaro, finlandês, indonésio, italiano, islandês, letão, tamil, sinhala, holandês, castelhano, arménio, galês e latim científico; (2) línguas que usam derivados da palavra Cha: hindi, japonês, português, persa, albanês, checo, russo, turco, tibetano, árabe, vietnamita, coreano, tailandês, grego, romeno, suaíli e croata. O termo Chá (茶) diz respeito tanto à bebida, quanto à planta (Camellia sinensis) que lhe dá origem. Na realidade, em português há uma palavra (tisana) para definir as infusões que não sejam preparadas exclusivamente a partir das folhas dessa planta. Entretanto, o termo é pouco usual, visto que se popularizou o nome chá como sinônimo de quaisquer infusões, sejam de flores, frutos, raízes ou ervas.[1]
O hábito de beber chá regularmente teve início durante a China Medieval com os monges budistas, prática mais tarde incorporada pelos literatos e, a posteriori, rapidamente adotada pela população em geral. Dessa difusão da bebida entre os chineses nasceu a necessidade de um recipiente que não adulterasse o sabor dos melhores chás (como ocorria com o uso de xícaras de madeira, metal ou barro, por exemplo), essa demanda estimulou o desenvolvimento da célebre porcelana chinesa.[1]
Beber chá é tido como um evento social. O chá também pode ser bebido durante o dia e principalmente pela manhã, a fim de aumentar o estado de alerta, já que contém teofilina e cafeína. Na Índia, a segunda maior produtora mundial, o chá é popular em todo o Norte, no café da manhã e à noite. Chamado popularmente de chaai, é servido quente com leite e açúcar. Quase todo o chá consumido é do tipo preto. Na China, no mínimo a partir da Dinastia Song, o chá foi objeto de festas de degustação e de grande estudo, comparável ao que se faz hoje com o vinho. Assim como a enologia hoje em dia, o recipiente próprio para se beber é importante; o chá branco era bebido em uma tigela escura onde as folhas de chá e a água quente eram misturados com um batedor. O melhor destas tigelas, cobertas com um verniz especial à base de casca de tartaruga, pintadas com pincel de pelo de lebre são muito valiosas hoje em dia. Os rituais e a tradicional cerâmica escura foram adotadas no Japão, no início do século XII, e gerou a cerimônia do chá japonesa, que tomou sua forma final no século XVI. Na Grã-Bretanha, o chá não é só o nome de uma bebida, mas também uma refeição leve no final da tarde, mesmo quando as pessoas bebem cerveja, cidra ou suco. No Sri Lanka o chá é servido no estilo inglês, com leite e açúcar, mas o leite sempre é aquecido.
Existem muitas cerimônias do chá, em várias culturas, sendo as mais famosas, a complexa e serena cerimônia do chá japonesa e a comercial, barulhenta e cheia de gente Yum Cha. Uma cultura de chá específica se desenvolveu na República Checa, nos últimos anos, incluindo a abertura de muitas casas de chá. Chás puros são geralmente preparados com respeito aos hábitos do país de origem. Várias salas de chá também criaram misturas e métodos próprios de preparo.
O chá Devonshire é sabor de chá relacionado a cerimônia do chá nos países que falam inglês, disponível em lojas por todo Reino Unido, Austrália, Índia e Nova Zelândia, mas quase desconhecido nos Estados Unidos. Nos Estados Unidos, o chá é servido geralmente gelado; o chá gelado é uma bebida comum para acompanhar as refeições ou para se refrescar em várias regiões. Às vezes, ele é servido com limão, e pode ser adoçado ou sem açúcar, variando em cada região. O Sun tea é feito deixando-se a água com as folhas para serem aquecidas diretamente pela luz solar como fonte de calor e demora-se muito tempo para a sua feitura. Recentemente, o chá com leite Boba, de Taiwan, tornou-se extremamente popular entre os jovens. Esta marca asiática se espalhou pelos Estados Unidos, onde é chamado de "bubble tea" (chá de bolhas).
Nativo de regiões subtropicais com clima de monções, o chá também é cultivado em climas tropicais, obtendo maior sucesso em regiões de altitude elevada. Quantitativamente, das cerca de 3 000 000 de toneladas produzidas anualmente, metade é produzida pela China e Índia, em proporções iguais. 60% do restante é produzido pelo Quénia, Turquia, Indonésia e Sri Lanka. Na Europa apenas é cultivado nos Açores, onde são produzidas anualmente cerca de 40 t.
Em todas as regiões produtoras de chá, o cultivo é semelhante, utilizando árvores podadas, para facilitar a colheita, e relativamente jovens, sendo substituídas quando começam a perder produtividade, com cerca de 50 anos. Notáveis exceções incluem o Gyokuro, chá verde japonês, protegido do sol durante o cultivo, e o Pu-erh, tradicional chá do sudoeste da China, que utiliza árvores com dezenas de metros e centenas de anos, muitas delas selvagens.
O cultivo do chá começou no Brasil no século XIX, porém mesmo com uma excelente qualidade, não conseguiu ser uma cultura de tanto sucesso graças à competição com os preços dos chás provenientes da Ásia.[5] A maior parte da produção brasileira está na região do Vale do Ribeira, no estado de São Paulo, sendo que a maior parte da produção é voltada à exportação, mesmo sendo inferior à qualidade do chá nos primórdios do cultivo.[6]
Portugal teve duas primazias em relação à introdução do chá na Europa. A da introdução do consumo de chá e a introdução, em 1750, do cultivo do chá. 
Foram produzidos na Ilha de São Miguel em zonas de microclima, como Porto Formoso e Capelas, 10 kg de chá preto e 8 kg de chá verde. No entanto, seria só um século depois que, com a chegada de mão de obra especializada, a produção se tornaria consequente, passando a haver uma aposta na industrialização do processamento após a coleta das folhas. Atualmente, o chá produzido nos Açores, sob as marcas Gorreana e Porto Formoso, é considerado um chá biológico, o que em muitos mercados provoca uma ideia de novidade que não é atual. O processamento deste, desde o cuidado dos arbustos até à colheita, é o mesmo há 250 anos. Este chá tem praticamente toda a sua produção dividida entre a região dos Açores, a comunidade da ilha na diáspora e o Reino Unido.
Na freguesia de Fornelo, Vila do Conde, existe desde 2014 uma plantação de Chá Camélia, única na Europa continental. Em 2019, surgia a primeira produção de chá verde na Chá Camélia. Doze quilos de chá seco a partir de quase 60 quilos de folhas frescas.[7] 
Os quatro tipos de chá são distinguíveis pelo seu processamento. Camellia sinensis é um arbusto sempre verde, cujas folhas, se não são logo secas depois de apanhadas, rapidamente começam a oxidar. Este processo lembra a maltização da cevada; as folhas ficam progressivamente escuras, assim que a clorofila se quebra. O processo seguinte no processamento é parar o processo de oxidação num estado predeterminado, removendo a água das folhas via aquecimento. O termo fermentação é frequente e muito usado para descrever este processo, mas na verdade nenhuma verdadeira fermentação acontece (ou seja, o processo não é digerido por micro-organismos). O chá é tradicionalmente classificado em quatro grupos principais, baseados no grau de oxidação:
Variações pouco comuns: estão disponíveis várias outras preparações de chá, que não se enquadram na nomenclatura usual.
O chá preto é processado de duas formas, em CTC (Crush, Tear, Curl — Esmagamento, Rasgo, Enrolamento) ou ortodoxo, isto é, em folhas inteiras, eram os processos usados até 1973. O método CTC é usado para folhas de média e baixa qualidade, que acabam em saquinhos de chá e são processados por máquinas. O processamento manual é usado para chás de qualidade elevada. Este estilo de processamento ortodoxo resulta num chá de qualidade elevada, procurado por muitos conhecedores e apreciadores de chá.
O chá preto produzido fora da China toma normalmente o nome da região de origem: Darjeeling, Assam, Ceilão, Nilgiri, entre outras. Na China, o chá preto mais famoso é provavelmente o Keemun, mas existem muitas outras variedades. A maioria dos chás verdes, contudo, são produzidos na China e Japão e por isso mantiveram o seu nome em japonês ou chinês tradicional: Genmaicha (玄米茶), Houjicha (焙じ茶), Pouchong (包種茶), etc. O chá verde e o chá preto têm antioxidantes, mas de tipos diferentes. O chá verde é mais rico em catequinas, especialmente o galato de epigalocatequina, enquanto que o chá preto contém uma maior variedade de flavonoides. O chá Oolong é intermédio, sendo o mais famoso o chá da Formosa. O chá branco, de folhas de chá muito jovens, é muitas vezes considerado um tipo distinto, embora ocasionalmente seja agrupado como um chá verde devido ao processamento simples. O chá branco produz uma infusão delicada que a maioria das vezes retém uma doçura residual leve.[8]
Todos os tipos são vendidos como chás "simples", quando são uma única variedade, ou "misturas" (blends).[8]
Adulteração e falsificação são problemas sérios no comércio global de chá; a quantidade de chá vendida como Darjeeling, o mais apreciado dos chás pretos, todos os anos excede grandemente a produção anual de Darjeeling, estimada em 11 000 toneladas. Entre os chás, o chá branco é considerado o mais raro e mais caro; ele deve ser colhido manualmente apenas durante uma certa fase da vida da planta. Independente disso, no entanto, encontra-se disponível em lojas especializadas e, apenas recentemente, acondicionados em saquinhos de papel.[8]
Quase todos os chás em saquetas e a maior parte dos outros chás são misturas. Apesar de recentes melhoramentos na técnica de congelação seca e do método melhorado de infusão, são vendidos o pó de chá e a essência condensada de chá, que apenas necessitam de água quente ou fria para se preparar uma chávena de chá. A mistura pode ocorrer ao nível de uma só área de plantação (por exemplo Assam), ou podem ser misturados chás provenientes de diversas áreas. O objetivo da elaboração de misturas é a obtenção de um sabor estável ao longo dos anos e de melhor preço. Numa mistura, o chá mais caro e mais saboroso pode encobrir o sabor inferior de um chá mais barato.
Há vários chás que contêm aditivos e/ou processamentos diferentes das variedades "puras". O chá tem a capacidade de adquirir qualquer aroma facilmente, o que pode trazer problemas no processamento, no transporte, ou na sua armazenagem, mas essa capacidade também deve ser aproveitada vantajosamente para preparar chás aromatizados.
Infusões de outras plantas também são às vezes chamadas de "chá" e utilizadas em substituição deste:
Esta seção descreve o método mais comum de se fazer chá. Completamente diferente dos métodos usados no norte da África, Tibete e talvez outros locais.
A melhor maneira de se preparar o chá é colocar as folhas em um bule, ao invés de um sachê. Embora seja totalmente aceito o uso de sachês, o resultado com a folha direto é melhor. Adiciona-se água fervente e mantém-se a infusão de 30 segundos a 5 minutos (processo chamado de brewing ou mashing no Reino Unido). Após isso, tanino é liberado, que tem efeito contrário ao da estimulação pela teofilina e cafeína e torna o chá amargo. Alguns chás, especialmente o chá verde e outros delicados como o Oolong ou o Darjeeling precisam de menos tempo, algumas vezes menos de 30 segundos. Usando um coador, separa-se as folhas da água se não estivermos usando um sachê.
A fim de preservar o chá do sabor do tanino, deve-se colocar toda a bebida em um segundo copo. De preferência, de faiança sem verniz, sendo os melhores os potes YangXi. O pote para servir o chá deve ser de porcelana pois retem mais o calor.
A água para chá preto deve ser adicionada em ponto de ebulição (100 °C) exceto para chás delicados como o chá de Darjeeling, onde temperaturas levemente menores são mais recomendadas. Já que o ponto de ebulição diminui com a altitude, isso torna difícil fazer chá preto em áreas montanhosas. A água para chá verde, de acordo com a maioria, deve estar por volta de 80 a 85 °C, sendo que quanto maior a qualidade das folhas, menor a temperatura. De preferência, o local onde o chá repousa - caneca ou bule - também deve ser aquecido, antes da confecção do chá.
Bebedores experientes de chá sempre insistem que o chá não deve ser agitado enquanto é feito. Isso, eles dizem, faz pouco quanto a consistência do chá, mas tem muita relação aos ácidos tânicos. Por esta mesma razão, não se deve chacoalhar muito o sachê de chá; se você quiser um chá mais forte, use mais sachês.
Aditivos populares ao chá incluem açúcar ou mel, limão, leite e geleia de frutas. Os mais conhecedores evitam o uso de nata porque ele fica com o gosto mais forte do que o próprio chá. Leite, entretanto, acredita-se ser útil, a fim de neutralizar os taninos remanescentes. Quando tomam chá com leite, os conhecedores adicionam chá ao leite, ao invés do contrário. Isso evita escaldamento do leite, o que torna a emulsão melhor e de melhor sabor.
Uma função é uma relação de um conjunto  com um conjunto  Usualmente, denotamos uma tal função por   onde  é o nome da função,  é chamado de domínio,  é chamado de imagem e  expressa a lei de correspondência (relação) dos elementos  com os elementos  Conforme suas características, as funções são agrupadas em várias categorias, entre as principais temos: função trigonométrica, função afim (ou função polinomial do 1° grau), função modular, função quadrática (ou função polinomial do 2° grau), função exponencial, função logarítmica, função polinomial, dentre inúmeras outras.[1][2][3]
As funções são definidas por certas relações. Por causa de sua generalidade, as funções aparecem em muitos contextos matemáticos e muitas áreas da matemática baseiam-se no estudo de funções. Deve-se notar que as palavras "função", "mapeamento", "mapa" e "transformação" são geralmente usadas como termos equivalentes. Além disso pode-se ocasionalmente se referir a funções como "funções bem definidas" ou "funções totais". O conceito de uma função é uma generalização da noção comum de fórmula matemática. As funções descrevem relações matemáticas especiais entre dois elementos. Intuitivamente, uma função é uma maneira de associar a cada valor do argumento  (às vezes denominado variável independente) um único valor da função  (também conhecido como variável dependente). Isto pode ser feito através de uma equação, um relacionamento gráfico, diagramas representando os dois conjuntos, uma regra de associação, uma tabela de correspondência, etc.. Muitas vezes, é útil associar cada par de elementos relacionados pela função com um ponto em um espaço adequado (por exemplo, no espaço  geometricamente representado no plano cartesiano). Neste caso, a exigência de unicidade da imagem (valor da função) implica um único ponto para cada entrada  (valor do argumento).[3][4][5]
Assim como a noção intuitiva de funções não se limita a cálculos usando números individuais, a noção matemática de funções não se limita a cálculos e nem mesmo a situações que envolvam números. De forma geral, uma função liga um domínio (conjunto de valores de entrada) com um segundo conjunto, o contradomínio ou codomínio (conjunto de valores de saída), de tal forma que a cada elemento do domínio está associado exatamente um elemento do contradomínio. O conjunto dos elementos  do contradomínio para os quais existe pelo menos um  no domínio tal que  (i.e.,  se relaciona com ), é o conjunto imagem ou chamado simplesmente de imagem da função.[5]
Sejam dados os conjuntos   uma relação  e o conjunto dos pares ordenados  Dizemos que  é uma função se, e somente se, para todos  com  temos  Ou, em outras palavras, para todo  existe no máximo um  tal que  se relaciona com  [3] Assim sendo, escrevemos  quando  se relaciona com  por  O conjunto  é chamado de conjunto de partida e  é chamado de contradomínio da função 
Outra maneira de dizer isto é afirmar que  é uma relação binária entre os dois conjuntos tal que  é unívoca, i.e. se  e  então  Algumas vezes, na definição de função, impõe-se que todo o elemento do conjunto  se relaciona com algum elemento de 
Vejamos as seguintes relações 
Podemos usar uma função para modelar o número de indivíduos em uma população de acordo com o tempo (modelos de crescimento demográfico). Por exemplo, denotando o tempo por  e o número de indivíduos em um dado tempo  por  escrevemos   Assim, temos abstratamente modelado o número de indivíduos (variável dependente) em função do tempo (variável independente). Aqui, o nome da função foi arbitrariamente escolhido como  o conjunto de partida é o conjunto dos números reais não negativos (assumindo que o tempo é contínuo e não negativo) e o contradomínio é o conjunto dos números naturais (assumindo que o número de indivíduos é sempre um número inteiro não negativo).
Da definição, temos que uma função tem um nome, um conjunto de partida, um contradomínio (conjunto de chegada) e uma lei de correspondência. Por exemplo, denotamos   onde  é o nome da função,  é seu conjunto de partida,  é seu contradomínio e  denota sua lei de correspondência.
Em muitos casos, nem todos os elementos do conjunto de partida se relacionam com algum elemento do contradomínio. Aqueles que se relacionam são elementos do chamado domínio da função. Mais precisamente, o domínio de uma função   é o conjunto:Também, geralmente, nem todos os elementos do contradomínio se relacionam com algum elemento do conjunto de partida. Aqueles que se relacionam são elementos da chamada imagem da função. A imagem de uma função   é o conjunto:
Seja   onde o conjunto de partida é dada por  e o contradomínio por Pela lei de correspondência, vemos que, neste caso,  e  Veja a ilustração.
O gráfico de uma função   é o conjunto:i.e, é o conjunto dos pares ordenados  tal que 
Quando possível, usualmente fazemos uma representação geométrica do gráfico da função. Tal representação é usualmente chamada de esboço do gráfico da função (ou, simplesmente gráfico, quando subentendido).
Popularmente, temos os gráficos de funções de uma variável, para as quais seu esboço é dado pelo conjunto de pontos  no plano cartesiano (veja a ilustração). Neste caso, usualmente as variáveis independentes são chamadas de abcissas e marcadas sobre o eixo horizontal (chamado de eixo das abcissas). As variáveis dependentes são chamadas de ordenadas e marcadas sobre o eixo vertical (chamado de eixo das ordenadas).
Funções são usualmente classificadas quanto a sua imagem como: funções injetoras, funções sobrejetoras e funções bijetoras.
Seja dada a função   Por definição,  é injetora (ou injetiva) se, e somente se, para todos  temos  A função  é dita sobrejetora (ou sobrejetiva) quando  Por fim, uma função injetora e sobrejetora é dita ser bijetora (ou bijetiva). Veja a seguinte tabela.
Dizemos que uma função  é definida de forma explícita (função explícita) quando seus valores  podem ser expressados pela variável independente  i.e., quando temos uma relação da forma  Por outro lado, dizemos que uma tal função é definida de forma implícita (função implícita) quando a relação entre as variáveis dependente e independente é dada como  onde  denota uma expressão envolvendo  e  [6]
Seja  dada por  Isto é, a função que toma dois valores reais e os associa ao produto entre eles. Trata-se de uma função explícita. Agora, a equação  define implicitamente a função  que associa um número real não nulo  ao seu inverso. Ou seja, tal função  está, aqui, definida implicitamente por  Notamos que neste caso em particular, podemos definir a função  de forma explícita, escrevendo 
Dadas uma função   e uma função   com  definimos a função composta de  com  por   Analogamente, quando  também podemos definir a função composta de  com  dada por   [3]
Considere as seguintes funções  e  dada por:
 e 
Notamos que  e, portanto, podemos definir a função composta por:
Também, como  temos a composição  dada por:
Funções são classificadas quanto a uma séries de propriedades (características) além das já mencionadas. Alguns desses tipos de funções são listados a seguir.
O conceito matemático de função emergiu no século XVII em conexão com o desenvolvimento do Cálculo.[7][8] O termo "função" foi introduzido por Gottfried Leibniz em uma de suas cartas, datada de 1673,  na qual ele descreve a declividade de uma curva em um ponto específico.[9] Na antiguidade, embora não se conheça o uso explícito de funções, tal conceito pode ser observado em alguns trabalhos percursores de filósofos e matemáticos medievais, como Oresme.[10]
Matemáticos do século XVII tratavam por funções aquelas definidas por expressões analíticas.[11] Foi durante os desenvolvimentos rigorosos da Análise Matemática por Weierstrass e outros, a reformulação da Geometria em termos da análise e a invenção da Teoria dos Conjuntos por Cantor, que se chegou ao conceito moderno e geral de uma função como um mapeamento unívoco de um conjunto em outro. Não há consenso sobre a quem se deva os créditos da noção moderna de função, sendo cotada os matemáticos Nikolai Lobachevsky, Peter Gustav Lejeune Dirichlet e Dedekind.[12][13][14]
Simetria (do grego συμμετρία, de σύν "com" e μέτρον "medida") é uma relação de paridade em respeito a altura, largura e comprimento das partes necessárias para compor um todo.[1] Um exemplo de elemento simétrico são as figuras geométricas.
Segundo Vitrúvio, a simetria consiste na união e conformidade das partes de um trabalho, em relação à sua totalidade, e na beleza de cada uma das partes que compõem o trabalho. A simetria deriva do conceito grego de analogia, que é a relação entre todas as partes de uma estrutura com a estrutura inteira.[1][2] A simetria é necessária para a beleza de uma construção, ou para a beleza da figura humana.[2]
Simetria uniforme, em arquitetura, ocorre quando o mesmo motivo reina em toda a obra. Simetria reflexiva ocorre quando apenas os lados opostos são iguais.[1]
A assimetria é a ausência da simetria ou o seu inverso. Na natureza, o caranguejo violinista é um bom exemplo onde a pinça esquerda é maior do que o animal e a direita não é maior do que uma pata.
Ao decorrer da história da humanidade, são atribuídos diferentes significados para a palavra "simetria" em diferentes períodos.
A definição de Vitrúvio para simetria como uma correspondência de  medida  entre  uma  certa  parte  dos  membros de cada obra e a obra toda.[2] Cabe ressaltar que no período renascentista, Claude Perrault realizou a tradução do tratado de Vitrúvio para o idioma francês. Ao realizá-la ele traduz a palavra "simetria" como "proporção".[3]
Na obra Élements de Géometrie, Adrien-Marie Legendre apresenta um conceito moderno de simetria, considerado inovador, que está mais relacionado com uma relação entre sólidos, independente das posições que ocupam no espaço.[3] Isso, tornou possível que resultados anteriores em geometria espacial pudessem ser demonstrados trazendo independência em relação à sobreposição de sólidos.[4]
Além disso, o autor e arquiteto norte-americano, Francis Ching, apresenta em um de seus livro a definição de simetria como o equilíbrio na distribuição de um arranjo, formas e elementos em lados opostos de uma linha, plano ou sobre um centro, eixo.[5] Sendo assim, a simetria só é possível a partir da ocorrência de um eixo ou centro ao qual está estruturada e essa configuração simétrica exige um arranjo com padrões equivalentes de formas e espaços em lados opostos.[5] O eixo de simetria é uma linha, real ou imaginária, que atravessa o centro da figura.[5]
Os dois tipos de simetria mais usados na arquitetura são:
Arranjo equilibrado dos elementos semelhantes ou equivalentes em lados opostos de um eixo, de modo que somente um plano possa dividir o todo em metades idênticas.[5]
Arranjo equilibrado dos elementos semelhantes, irradiados de modo que a composição possa ser dividida em metades semelhantes ao traçar um plano em qualquer ângulo em relação a um ponto central ou ao longo de um eixo central.[5]
Na arquitetura, a simetria pode ser utilizada para organizar seus espaços e formas,[6] podendo ser encontrada em diversos formatos e escalas, desde a escala urbana de um cidade por meio de seu planejamento, ou pela concepção de uma edificação na qual a se dá por completo de modo simétrico, como na Villa Rotonda. Também pode ser encontrada em elementos específicos de uma construção, como os ladrilhos ou decoração do espaço,[6] como por exemplo, o  interior da cúpula da Mesquita do Imã Khomeini no Irã.
A simetria na arquitetura também passa uma sensação de segurança e estabilidade, além de criar um senso de proporção. Sempre foi um tema importante na arquitetura, desde a Grécia Antiga a simetria é utilizada para passar uma ideia de ordem e harmonia nas construções.
Ching defende que é possível planejar uma composição de forma simétrica, desde que seja analisado e solucionado a assimetria ao redor, como a do terreno e do contexto analisado.[5] É possível ter simetria apenas em uma parte significativa ou importante do edifício. Todavia, a simetria pode ser usada além da parte interna ou de somente um edifício, podendo ocorrer também com o entorno de uma construção ou entre essas próprias construções.
Na obra História Global da Arquitetura os autores falam sobre como os edifícios clássicos exibem simetria tanto na elevação quanto no plano.[7] A metade direita de uma elevação é simplesmente uma imagem espelhada da esquerda e mesmo que isso nem sempre seja possível alcançar, as variadas características simétricas provavelmente existirão.[7] E essa simetria se estende tanto à composição geral de uma elevação, quanto aos seus componentes.
A simetria na construção clássica utilizada nos planos, depende de seu efeito sobre a axialidade. Um dos principais desafios dos edifícios renascentistas é a maneira como os eixos são explorados, a simetria e axialidade são mantidos em planos complexos com salas de diferentes formas e tamanhos.[8] A importância e insistência do uso da simetria faz com que as entradas sejam colocadas ao centro do edifício e pavilhões, e a possibilidade de uma elevação com um número ímpar de baias, portanto, um número uniforme de colunas.[8]
Situada no período entre o início do século XIV e início do século XVII, a arquitetura no renascimento italiano se sintetizou por meio da valorização de conceitos matemáticos e geométricos com o propósito de demonstrar racionalidade artística. Com isso, a simetria se torna um dos conceitos mais importantes sendo considerada um aspecto central, pois os arquitetos dessa época realizavam um grande esforço para trazer a simetria e propoção para suas obras, como por exemplo, a Vila Rotonda de Andrea Palladio.[9]
A simetria está presente em obras de arquitetos renascentistas como Andrea Palladio, Leon Battista Alberti, Filippo Brunelleschi e Donato Bramante.
A Igreja da Santa Cruz na Praia da Vitória, em Portugal, é um exemplo de arquitetura simétrica. É possível perceber o equilíbrio através da simetria. 
O Templo de Parthenon, uma das obras mais famosas da arquitetura grega, deixa claro como a perfeita simetria era uma preocupação nas construções erguidas naquele período.
O Taj Mahal é um dos exemplos mais famosos de obra arquitetônica simétrica.
editar - editar código-fonte - editar WikidataArado é um instrumento que serve para lavrar (arar) o solo, revolvendo a terra com o objetivo de descompactá-la e, assim, viabilizar um melhor desenvolvimento das raízes das plantas. Expõe o subsolo à ação do sol, ajudando a aumentar a temperatura e apressar o degelo. Também enterra restos de culturas agrícolas anteriores
ou ervas daninhas porventura existentes. Melhora ainda a infiltração de água no solo e a aeração, além de realizar a construção de curvas de níveis. É uma das etapas agrícolas que antecede a semeadura.Além desse objetivo primacial, a aração permite um maior arejamento do solo, o que possibilita o desenvolvimento dos organismos úteis, como as minhocas, além de, alguns casos, permitir a mistura de nutrientes (adubos, químicos ou orgânicos; corretivos de acidez, etc.). O arado é capaz de descompactar uma camada de solo que pode chegar aos 40 cm de profundidade.[1]
Foi uma das grandes invenções da humanidade, por permitir a produção de crescentes quantidades de alimentos e o estabelecimento de populações estáveis.
Por volta de 4500 a.C., o ser humano cansou de vagar em busca de terras boas para o cultivo e de depender de minhocas para preparar o solo. Nossos antepassados usavam galhos de árvores para afofar o solo e fazer sulcos onde eram colocadas as sementes, como enxadas primitivas. Como nessa época o ser humano já havia dominado a metalurgia e a domesticação de animais, inventou um utensílio feito com galhos bifurcados (que depois recebeu uma pedra afiada na ponta) que, puxado por animais, arava a terra.[2]  Os sumérios foram os primeiros a utilizarem arados tracionados por animais.
A partir do domínio da terra, o ser humano se fixou em aldeias e aumentou consideravelmente sua produtividade, gerando excedente e consequentemente iniciando atividades comerciais com povos vizinhos.[2] Os principais polos foram os vales dos rios da Mesopotâmia, Egito, Índia e China, os berços das primeiras civilizações da Antiguidade.[2] O impacto da invenção do arado foi tão grande que hoje ela é considerada um marco da Revolução Agrícola.[2] Os arados desta época apenas rasgavam a terra, sem revirá-la como fazem os arados mais modernos.[3]
O arado conhecido mais antigo do mundo, aproximadamente do ano 1.500 a.C. está exposto no Museu Nacional da Baixa Saxónia.[4]
A evolução foi muitíssimo lenta, baseando-se apenas em melhorar o arado de pau puxado pelo homem e alguns utensílios de pedra, passaram-se séculos para que os trabalhos de arrasto feitos pelo homem pudessem ser substituídos pela força animal, libertando-se o homem de trabalho tão árduo.[4] Com o aparecimento e barateamento do ferro, o arado foi melhorado.[4]
Na Baixa Idade Média houve várias conquistas técnicas com o arado de ferro, foram desenvolvidas novas maneiras de se atrelarem os animais, de modo a permitir que eles fossem utilizados à plena força, além de ter sido substituído o boi pelo cavalo, como animal de tração.[5]
No início do século XIX os arados eram feitos de ferro fundido, adequados ao solo arenoso, porém solos mais ricos se agarravam nas partes inferiores do arado, obrigado o operador a parar e raspar o solo do arado.[6]  John Deere, em 1837, estudou este problema e chegou a conclusão que um arado com uma aiveca e uma lâmina com uma forma apropriada, altamente polidas, deveriam limpar-se a si próprias ao torcer sulco de solo grudado no arado.
Também chamado de vanga ou enxadão, feito de madeira ou ferro.
Eram primitivos arados em madeira ou ferro , em forma de L, de tração animal.
São arados em ferro na forma de um V, com tombador de terra, para tração animal ou mecânica. É o mais antigo implemento fabricado para a realização do preparo do solo. No Brasil, esse implemento é mais destinado à tração animal.[7]
Possui diversos tipos: aiveca helicoidal destinada a uma lavoura superficial e rápida, deixando um solo com torrões grandes; semi-helicoidal (Universal ou Americana) recomendada para uma lavoura normal, deixando torrões grandes na parte inferior e torrões menores na camada mais superficial; aiveca cilíndrica é recomendada para lavoura com tração animal, deixando torrões de todo o tamanho misturado com o solo pulverizado; aivecas recortadas recomendadas para solos pegajosos.[1]
O arado de aiveca produz uma inversão do solo melhor que a do arado de discos, mas apresenta restrições ao uso em solos com obstáculos, tais como pedras e tocos, caso não haja mecanismos de segurança, com desarme automático.[8] O arado de aiveca pode ser equipado com vários acessórios, que facilitam o corte vertical do solo e evitam o embuchamento.[7]
Possui grandes discos de aço apoiados em mancais rotativos, para tração mecânica. Surgiu como alternativa ao de aiveca e teve como ponto de partida a grade de discos, sendo o implemento de preparo de solo mais usado no Brasil, devido a sua facilidade de confecção e melhor adaptação às variadas condições de solos.[7] Apesar do movimento giratório dos discos, que corta o solo e a vegetação, esse tipo de arado requer peso para penetrar no solo; o que não acontece no de aiveca, cuja penetração é provocada pela conformação de suas partes ativas.[7]
É ideal para ser usado em solos secos, duros, pegajosos, com raízes e pedras, nos quais as aivecas apresentam dificuldades de operação.[7] Quando o solo é arenoso e limpo pode-se utilizar discos grandes; em solos duros com raízes e restos vegetais, recomenda-se utilizar discos menores e com bordas recortadas.[9] Um inconveniente deste tipo de arado é que com o passar dos anos e uma utilização contínua, pode haver uma acumulação de terra nos terraços, pois esse tipo joga a terra para um dos lados. Para evitar esse problema, deve ser adotada a prática de se alternar o sentido de tombamento das leivas.[10] Outro inconveniente desse tipo de arado é que uma das rodas do trator passa sobre o sulco aberto, compactando o solo.[10] Por outro lado, os arados de discos possuem a vantagem de continuarem operando, mesmo depois que seus órgãos ativos tenham sofrido um desgaste considerável, podendo ser utilizados em solos abrasivos sem perda da sua eficiência.[11]
Também chamado de arado escarificador, possui de cinco a onze ferros ou braços montados em barras paralelas sobre um quadro porta-ferramentas e espaçados entre si de 60 a 70 cm, em cada barra, de modo a dar um espaçamento efetivo entre sulcos paralelos de 30 a 35 cm.[8] Aumenta a rugosidade do solo,  quebra a estrutura do solo a uma profundidade de 20 a 25 cm, aumentando a capacidade de infiltração de água no solo, diminui a evaporação e quebra a camada compactada, abaixo da área de preparo de solo.[12] Por causa de sua maior velocidade, à medida que se aumenta a área da propriedade, há uma preferência pela grade aradora em detrimento do arado de disco.[12] Uma desvantagem da grade aradora é que ela provoca grande pulverização do solo.[12]
Quanto ao tipo de tração, o arado pode ser:
Os arados puxados por tratores podem ser simples, utilizados em pequenas explorações agrícolas, ou múltiplos,utilizados nas grandes explorações.
Em regiões de clima tropical e altas temperaturas, a exposição do substrato ao calor do sol pode ter efeitos contrários ao pretendido, com excessiva perda de humidade e extinção da micro-fauna biológica que ajuda a decomposição e o aproveitamento dos nutrientes. A lavoura com arado também facilita a erosão, o que pode ser um grande problema em regiões de chuvas intensas. Por isso, a prática agrícola moderna preconiza uma mínima movimentação do solo, a utilização do chamado plantio direto, onde apenas uma estreita faixa é revolvida, mantendo-se a cobertura de restos culturais sobre o terreno, o que ajuda na manutenção da humidade.[13]
Como o arado comum sempre tomba a terra para um mesmo lado,é impossível ter-se linhas consecutivas trabalhadas em sentidos inversos, que estragariam o terreno. Usam-se então esquemas para dividir a área em glebas, arando umas na ida e outras na volta. É preciso também arar transversalmente à declividade, para não facilitar a erosão.
Modernamente usam-se arados reversíveis por acionamento mecânico ou hidráulico, que facilita a prática agrícola.
Arado para plantio de cana-de-açúcar
Corrente elétrica é o fluxo ordenado de partículas portadoras de carga elétrica ou o deslocamento de cargas dentro de um condutor, quando existe uma diferença de potencial elétrico entre as extremidades. Tal deslocamento procura restabelecer o equilíbrio desfeito pela ação de um campo elétrico ou outros meios (reações químicas, atrito, luz, etc.).[1]
Microscopicamente, as cargas livres estão em movimento aleatório devido à agitação térmica. Apesar desse movimento desordenado, ao estabelecermos um campo elétrico na região das cargas surge uma força elétrica que imprime uma velocidade de arraste, a qual impõe um movimento ordenado que se superpõe ao primeiro.[2] Esse movimento recebe o nome de movimento de deriva das cargas livres.[3]
Raios são exemplos de corrente elétrica, bem como o vento solar, porém a mais conhecida, provavelmente, é a do fluxo de elétrons(pt-BR) ou eletrões(pt-PT?) através de um condutor elétrico, geralmente metálico.
A intensidade da corrente elétrica é definida como a razão entre o  módulo da quantidade de carga ΔQ que atravessa certa secção transversal (corte feito ao longo da menor dimensão de um corpo) do condutor em um intervalo de tempo  Δt.
A unidade padrão no SI para medida de intensidade de corrente é o ampère (A). A corrente elétrica é também chamada informalmente de amperagem. É nomeada em homenagem a André-Marie Ampère. Embora seja um termo válido na linguagem coloquial, a maioria dos engenheiros eletricistas repudia o seu uso por confundir a grandeza física (corrente eléctrica) com a unidade que a medirá (ampère). A corrente elétrica, designada por I , é o fluxo das cargas de condução dentro de um material. A intensidade da corrente é a taxa de transferência da carga, igual à carga  transferida durante um intervalo infinitesimal dividida pelo tempo.
Denominamos corrente elétrica a todo movimento ordenado de partículas eletrizadas. Para que esses movimentos ocorram é necessário haver tais partículas − íons ou elétrons − livres no interior dos corpos.
Corpos que possuem partículas eletrizadas livres em quantidades razoáveis são denominados condutores, pois essa característica permite estabelecer corrente elétrica em seu interior.
Nos metais existe grande quantidade de elétrons livres, em movimento desordenado. Quando se cria, de alguma maneira, um campo elétrico () no interior de um corpo metálico, esses movimentos passam a ser ordenados no sentido oposto ao do vetor campo elétrico (), constituindo a corrente elétrica.
Nas soluções eletrolíticas existe grande quantidade de cátions e ânions livres, em movimento é desordenado. Quando se cria, de alguma maneira, um campo elétrico () no interior de uma solução eletrolítica, esses movimentos passam a ser ordenados: o movimento dos cátions, no sentido do vetor campo elétrico (), e o dos ânions, no sentido oposto. Essa ordenação constitui a corrente elétrica.
Nos gases ionizados existe grande quantidade de cátions e elétrons livres, em movimento desordenado. Quando se cria, de alguma maneira, um campo elétrico () no interior de um gás ionizado, esses movimentos passam a ser ordenados: o movimento dos cátions, no sentido do vetor campo elétrico (), e o dos elétrons, no sentido oposto. Essa ordenação constitui a corrente elétrica.
Com a finalidade de facilitar o estudo das leis que regem os fenômenos ligados às correntes elétricas, costumamos adotar um sentido convencional para a corrente elétrica,[4] coincidente com o sentido do vetor campo elétrico () que a produziu.
Consequentemente, esse sentido será o mesmo do movimento das partículas eletrizadas positivamente e oposto ao das partículas eletrizadas negativamente.
Corrente contínua (CC ou DC - do inglês direct current) é o fluxo ordenado de cargas elétricas no mesmo sentido. Esse tipo de corrente é gerado por baterias de automóveis ou de motos (6, 12 ou 24 V), pequenas baterias (geralmente de 9V), pilhas (1,2 V e 1,5 V), dínamos, células solares e fontes de alimentação de várias tecnologias, que retificam a corrente alternada para produzir corrente contínua.
Corrente alternada (CA ou AC - do inglês alternating current) é uma corrente elétrica cujo sentido varia no tempo, ao contrário da corrente contínua cujo sentido permanece constante ao longo do tempo. A forma de onda usual em um circuito de potência CA é senoidal por ser a forma de transmissão de energia mais eficiente. Entretanto, em certas aplicações, diferentes formas de ondas são utilizadas, tais como triangular ou ondas quadradas.
Enquanto a fonte de corrente contínua é constituída pelos pólos positivo e negativo, a de corrente alternada é composta por fases (e, muitas vezes, pelo fio neutro).
No início da história da eletricidade definiu-se o sentido da corrente elétrica como sendo o sentido do fluxo de cargas positivas,[5] ou seja, as cargas que se movimentam do pólo positivo para o pólo negativo. Naquele tempo nada se conhecia sobre a estrutura dos átomos. Não se imaginava que em condutores sólidos as cargas positivas estão fortemente ligadas aos núcleos dos átomos e, portanto, não pode haver fluxo macroscópico de cargas positivas em condutores sólidos. No entanto, quando a física subatômica estabeleceu esse fato, o conceito anterior já estava arraigado e era amplamente utilizado em cálculos e representações para análise de circuitos.
Esse sentido continua a ser utilizado até os dias de hoje e é chamado sentido convencional da corrente.
Em qualquer tipo de condutor, este é o sentido contrário ao fluxo líquido das cargas negativas ou o sentido do campo elétrico estabelecido no condutor. Na prática qualquer corrente elétrica pode ser representada por um fluxo de portadores positivos sem que disso decorram erros de cálculo ou quaisquer problemas práticos.
O sentido real da corrente elétrica depende da natureza do condutor.
A corrente elétrica não é  exclusividade dos meios sólidos - ela pode  ocorrer também nos gases e nos líquidos.
Nos sólidos, as cargas cujo fluxo constitui a corrente real são os elétrons livres. Nos líquidos, os portadores de corrente são íons positivos e íons negativos. Nos gases, são íons positivos, íons negativos e elétrons livres. A corrente elétrica que se estabelece nos condutores eletrolíticos e nos condutores gasosos (como a que surge em uma lâmpada fluorescente) é denominada corrente iônica.[4]
O sentido real é o sentido do movimento de deriva das cargas elétricas livres (portadores). Esse movimento se dá no sentido contrário ao do campo elétrico se os portadores forem negativos (caso dos condutores metálicos), e no mesmo sentido do campo, se os portadores forem positivos. Mas existem casos em que verificamos cargas se movimentando nos dois sentidos. Isso acontece quando o condutor apresenta os dois tipos de cargas livres (condutores iônicos, por exemplo).
Nesses casos, não são só os portadores de carga negativa que entram em movimento, mas também os portadores de carga positiva: os íons também entram em movimento.
Por exemplo: se, numa solução iônica,  são colocados dois eletrodos  ligados a uma bateria, um eletrodo adquire carga positiva, e outro, carga negativa. Com isso, o movimento dos íons negativos e dos elétrons se dará no sentido do eletrodo positivo, enquanto o movimento dos íons positivos  ocorrerá no sentido do eletrodo negativo.
O mesmo ocorre em meio gasoso, no caso dos gases ionizados.  A intensidade I da corrente elétrica também é determinada pela mesma equação apresentada acima. A diferença é que, nesse caso, a quantidade de carga elétrica será dada pela soma de cargas positivas e negativas.
Por convenção, usa-se o sentido da transferência de cargas positivas para definir o sentido
da corrente elétrica. Assim, se as cargas de condução forem eletrões, como acontece num
metal, o sentido da corrente será oposto ao sentido do movimento dos eletrões. Por exemplo,
o fio metálico na figura transporta corrente elétrica de B para A. Num determinado
intervalo de tempo, a carga dos eletrões transportados de A para B é  ; isso implica que a carga dos protões que se combinaram com os eletrões em B foi  , e essa é também a carga dos protões que ficaram em A após a partida dos eletrões.[6]
Consequentemente, é equivalente considerar que houve transporte de carga  de A para B, ou transporte de carga  de B para A. A corrente I é definida no sentido do transporte da carga positiva.
A carga total transferida durante um intervalo de tempo é o integral da corrente I , nesse
intervalo:
No sistema internacional de unidades a unidade usada para medir a corrente elétrica é o
ampere, designado pela letra A, que equivale à transferência de uma carga de um coulomb
cada segundo:
Ao estabelecermos um campo elétrico em um condutor verificamos, superposto ao movimento aleatório das cargas livres, um movimento de deriva dessas cargas. Em metais, condutores mais conhecidos, temos elétrons como portadores de carga livres. Essas partículas oscilam aleatoriamente a velocidades médias da ordem de 105 a 106 m/s. No entanto o movimento de deriva se dá a uma taxa da ordem de 10-3m/s (na situação de máxima densidade de corrente). Ou seja, quando temos a máxima densidade de corrente permitida pelas normas técnicas a velocidade de deriva dos elétrons livres é cerca de 2 mm/s.[7]
A corrente elétrica φ se relaciona com a densidade de corrente elétrica j através da fórmula
onde, no SI,
A densidade de corrente é definida como:
onde
Densidade de corrente é de importante consideração em projetos de sistemas elétricos. A maioria dos condutores elétricos possuem uma resistência positiva finita, fazendo-os então dissipar potência na forma de calor. A densidade de corrente deve permanecer suficientemente baixa para prevenir que o condutor funda ou queime, ou que a isolação do material caia. Em superconductores, corrente excessiva pode gerar um campo magnético forte o suficiente para causar perda espontânea da propriedade de supercondução.
Para medir a corrente, pode-se utilizar um amperímetro. Apesar de prático, isto pode levar a uma interferência demasiada no objeto de medição, como por exemplo, desmontar uma parte de um circuito que não poderia ser desmontada.
Como toda corrente produz um campo magnético associado, podemos tentar medir este campo para determinar a intensidade da corrente. O efeito Hall, a bobina de Rogowski e sensores podem ser de grande valia neste caso.
Para componentes eletrônicos que obedecem à lei de Ohm, a relação entre a tensão (V) dada em volts aplicada ao componente e a corrente elétrica que passa por ele é constante. Esta razão é chamada de resistência elétrica e vale a equação:[8]
Tecnologias da informação e comunicação (TICs) é uma expressão que se refere ao papel da comunicação (seja por fios, cabos, ou sem fio) na moderna tecnologia da informação. Entende-se que TICs são todos os meios técnicos usados para tratar a informação e auxiliar na comunicação, o que inclui o hardware de computadores, rede e telemóveis. Em outras palavras, TICs consistem em TI, bem como quaisquer formas de transmissão de informações[1] e correspondem a todas as tecnologias que interferem e medeiam os processos informacionais e comunicativos dos seres. Ainda, podem ser entendidas como um conjunto de recursos tecnológicos integrados entre si, que proporcionam, por meio das funções de hardware, software e telecomunicações, a automação, comunicação e facilitação dos processos de negócios, da pesquisa científica, de ensino e aprendizagem, entre outras.
A expressão foi usada pela primeira vez em 1997, por Dennis Stevenson, do governo britânico[2], e promovida pela documentação do Novo Currículo Britânico em 2000.
São utilizadas em diversas maneiras e em vários ramos de atividades, podendo se destacar nas indústrias (processo de automação), no comércio (gerenciamento e publicidade), no setor de investimentos (informações simultâneas e comunicação imediata), na biblioteconomia (OPAC) e na educação (processo de ensino aprendizagem e Educação a  distância).
Pode-se dizer que a principal responsável pelo crescimento e potencialização da utilização das TIC, em diversos campos, foi a popularização da Internet.
Como a comunicação é uma necessidade e algo que está presente na vida do ser humano desde os tempos mais remotos, trocar informações, registrar fatos, expressar ideias e emoções são fatores que contribuíram para a evolução das formas de se comunicar. Assim, com o passar do tempo, o homem aperfeiçoou sua capacidade de se relacionar.
Nesse sentido, conforme as necessidades surgiram, o homem lançou mão de sua capacidade racional para desenvolver novas tecnologias e mecanismos para a comunicação. Conceitua-se tecnologia como tudo aquilo que leva alguém a evoluir, a melhorar ou a simplificar. Em suma, todo processo de aperfeiçoamento. A humanidade já passou por diversas fases de evoluções tecnológicas, porém um equívoco comum quando se pensa em tecnologia é se remeter às novidades de última geração.
Em se tratando de informação e comunicação, as possibilidades tecnológicas surgiram como uma alternativa da era moderna, facilitando a educação através da inclusão digital, com a inserção de computadores nas escolas, facilitando e aperfeiçoando o uso da tecnologia pelos alunos, o acesso a informações e a realização de múltiplas tarefas em todas as dimensões da vida humana, além de capacitar os professores por meio da criação de redes e comunidades virtuais.
Sob tal ótica, "os computadores são grandes responsáveis por esse processo. Os Sistemas de Informação nas empresas requerem estudos quanto à sua importância na abordagem gerencial e estratégica dos mesmos, juntamente à análise do papel estratégico da informação e dos sistemas na empresa (KROENKE, 1992; LAUNDON, 1999)".
Existe uma tendência cada vez mais acentuada de adoção das tecnologias de informação e comunicação, não só por escolas, mas por empresas de diversas áreas, sobretudo com a disseminação dos aparelhos digitais no cotidiano contemporâneo. Há uma variedade de informações que o tratamento digital proporciona: imagem, som, movimento, representações manipuláveis de dados e sistemas (simulações), todos integrados e imediatamente disponíveis, que oferecem um novo quadro de fontes de conteúdos que podem ser objeto de estudo.
A comunicação é também a responsável por grandes avanços. Devido a troca de mensagens e consequente troca de experiência, dessa forma, grandes descobertas foram feitas. A história humana, sem os desenhos das cavernas, os hieróglifos egípcios, e o enorme acervo de informação que nos foi deixado através da escrita, não teria a emoção sentida hoje ao se ver o avanço desses meios. Todos os exemplos citados acima são formas de deixar mensagens, i.e., passar adiante uma informação, experiência, fato ou descoberta. A comunicação é algo complexo, uma vez que existem várias formas de se comunicar. O objetivo aqui, é mostrar o quanto a troca de mensagens, a informação e o relacionamento humano são importantes para a evolução de novos conceitos, como, p. ex., o trabalho colaborativo, a gestão do conhecimento, o ensino a distância (e-learning), que promovem uma maior democracia nos relacionamentos entre pessoas e a diminuição do espaço físico–temporal.
Num ambiente corporativo, onde um grupo de pessoas percorre objetivos comuns, a necessidade de comunicação aumenta consideravelmente. Numa corporação, existem barreiras culturais, sociais, tecnológicas, geográficas, temporais, dentre outras, que dificultam as pessoas a se comunicarem, portanto, um dos desafios de uma corporação é transpor essas barreiras.
Atualmente, os sistemas de informação e as redes de computadores têm desempenhado um papel importante na comunicação corporativa, pois é através dessas ferramentas que a comunicação flui sem barreira. Segundo Lévy (1999), novas maneiras de pensar e de conviver estão sendo elaboradas no mundo das telecomunicações e da informática. As relações entre os homens, o trabalho, a própria inteligência dependem, na verdade, da metamorfose incessante de dispositivos informacionais de todos os tipos. Escrita, leitura, visão, audição, criação e aprendizagem são capturadas por uma informática cada vez mais avançada.
A tecnologia da informação teve uma gigantesca evolução e, com a tendência do mundo moderno, inovações e facilidades ainda hão de surgir. A internet e, em consequência, o e-mail e a agenda de grupo online, são componentes de um grande marco e um dos avanços mais significativos, pois através deles vários outros sistemas de comunicação foram criados.
Atualmente, encontramos várias tecnologias que viabilizam a comunicação, porém o que vai agregar maior peso a essas tecnologias é a interação e a colaboração de cada uma delas. Dentro desse cenário, é importante frisar uma interessante observação feita por Lévy (1999):
Atualmente, estudos sistemáticos dos comportamentos econômicos nesta transição de século e de milênio vêm atribuindo um importante fator ao cenário econômico, tão impregnado pelos fatores da Era Industrial (bens de consumo durável, maquinário, trabalho mecânico e em série, produtos, etc.) e esse fator é o conhecimento — a dimensão crítica de sustentação de vantagens competitivas.
Nessa nova economia, as capacidades de inovação, diferenciação, criação, valor agregado e adaptação à mudança são determinadas pela forma como velhos e novos conhecimentos integram cadeias/redes de valor, como processos e produtos recorrem a conhecimento útil e crítico, bem como pela aptidão demostrada pelas empresas, governos (organizações em geral) e pessoal para aprender constantemente (Silva, 2003).
A Era da Informação e do Conhecimento que vivemos nos mostra um mundo novo, na qual o trabalho humano é feito pelas máquinas, cabendo ao homem a tarefa à qual é insubstituível: ser criativo, ter boas ideias. Há algumas décadas, a era da informação vem sendo superada pela onda do conhecimento. Já que o aumento de informação disponibilizada pelos meios informatizados vem crescendo bastante, a questão agora está centrada em como gerir esse mundo de informações e retirar dele o subsídio para a tomada de decisão.
Desenvolver competências e habilidades na busca, tratamento e armazenamento da informação transforma-se num diferencial competitivo dos indivíduos.
Não só ter uma grande quantidade de informação, mas sim que essa informação seja tratada, analisada e armazenada de forma que todas as pessoas envoltas tenham acesso sem restrição de tempo e localização geográfica, e que essa informação agregue valor às tomadas de decisão.
É importante que o desenvolvimento de um determinado projeto seja organizado e disponibilizado para uma posterior consulta e fonte de pesquisa para projetos futuros, ou seja, é necessário criar um meio que resgate. A memória é o bem maior de qualquer organização, é o conhecimento gerado pelas pessoas que fazem parte desta.
A tecnologia da informação (TI) tem um papel significativo na criação desse ambiente colaborativo e, posteriormente, numa gestão do conhecimento. Entanto, é importante ressaltar que a tecnologia da informação desempenha seu papel apenas promovendo a infraestrutura, pois o trabalho colaborativo e a gestão do conhecimento envolvem também aspetos humanos, culturais e de gestão (Silva, 2003).
Os avanços da tecnologia da informação têm contribuído para projetar a civilização em direção a uma sociedade do conhecimento. A análise da evolução da tecnologia da informação, de acordo com Silva (2003), é:
Hoje, o foco da Tecnologia da Informação mudou, tanto que o termo TI passou a ser utilizado como TIC — Tecnologia da Informação e Comunicação. E, dentro desse universo, novas ideias como colaboração e gestão do conhecimento poderão ser edificadas, porém, mais uma vez, é importante enfatizar que nenhuma infraestrutura por si só promoverá a colaboração entre as pessoas, essa atitude faz parte de uma cultura que deverá ser disseminada por toda a organização; é necessário uma grande mudança de paradigma.
De acordo com a Organização para a Cooperação e Desenvolvimento Econômico (OCDE), a distribuição das principais corporações globais de TICs pelas regiões do planeta se alterou significativamente entre os anos de 2000 e 2009. Países da Ásia tiveram um aumento significativo no setor, tendo em vista a participação de suas empresas globais e o constante aumento de suas receitas.[3]
Computadores, internet, softwares, jogos eletrônicos, celulares: ferramentas comuns ao dia a dia da chamada geração digital e as crianças e jovens já as dominam como se fossem velhas conhecidas. Essas tecnologias constituem o "habitat diário" desta geração, - que usa os meios para "comunicar-se" (correio, sms, bate-papo ...), "conhecer" (sites, downloads ...), "compartilhar" (redes sociais, fotos, vídeos ...), "se divertir" ( jogos na internet, rádio e TV digital) e também "consumir" (compras on-line).[4] Pesquisadores apontam que os mais jovens representam a faixa etária onde o crescimento do uso das TICs é maior.[5]
O ritmo acelerado das inovações tecnológicas[6], assimiladas tão rapidamente pelos alunos, exige que a educação também se renove, tornando o ensino mais criativo, estimulando o interesse pela aprendizagem, não enxergando a tecnologia apenas como um instrumento, mas como uma tecnologia social, capaz de gerar novos processos de aprendizagem, novas formas de encarar a assimilação de conhecimento e novas formas de estabelecer comunicações.[7]
Entendendo a escola como um espaço de criação de cultura, esta deve incorporar os produtos culturais e as práticas sociais mais avançadas da sociedade em que nos encontramos. Espera-se, assim, da escola uma importante contribuição no sentido de ajudar as crianças e os jovens a viver em um ambiente cada vez mais “automatizado”, através do uso da eletrônica e das telecomunicações. O horizonte de uma criança, hoje em dia, ultrapassa claramente o limite físico da sua escola, da sua cidade ou do seu país, quer se trate do horizonte cultural, social, pessoal ou profissional.
Em uma sociedade tecnológica, o educador assume um papel fundamental como mediador das aprendizagens, sobretudo como modelo que é para os mais novos, adotando determinados comportamentos e atitudes em face das tecnologias. Por outro lado, perante os produtos tecnológicos, o educador deverá assumir-se com conhecimento e critério, analisando cuidadosamente os materiais que coloca à disposição das crianças.
Porém o Brasil precisa melhorar as competências do professor em utilizar as tecnologias de comunicação e informação na educação. A forma como o sistema educacional incorpora as TICs afeta diretamente a diminuição da exclusão digital existente no país[8]. Diferentes autores tentaram identificar os tipos de uso das TIC pelos professores (Hsu, 2011). Barron e outros (2003) identificaram quatro tipos de uso das TIC na educação na sala de aula: o computador como instrumento de pesquisa para o aluno, como uma ferramenta para resolver problemas e decisões, como instrumento de produção (criar relatórios e empregos) e como recurso de comunicação. Por sua vez, Russell e outros (2003) identificaram seis categorias de uso de TIC pelos professores: uso das TIC para preparar as aulas, produzir materiais, para direcionar o aluno, para a educação é especial, para o uso de correio eletrônico, e para gravações e registros. Braak, Tondeur e Valcke (2004) identificaram dois tipos de estratégias ou padrões de uso das TIC nas escolas: o simples apoio aos processos de ensino e o uso efetivo desses recursos no desenvolvimento do ensino, considerando características como sua idade, sexo, competência digital, atitudes em relação às TIC e vontade de mudar e tendência inovadora.[9]
Vários pontos devem ser levados em conta quando se procura responder a questão: Como as TICs podem ser utilizadas para acelerar o desenvolvimento em direção à meta de “educação a todos e ao longo da vida”? Como elas podem propiciar melhor equilíbrio entre ampla cobertura e excelência na educação? Como pode a educação preparar os indivíduos e a sociedade de forma que dominem as tecnologias que permeiam crescentemente todos os setores da vida e possam tirar proveito dela?
Primeiro, as TICs são apenas uma parte de um contínuo desenvolvimento de tecnologias, a começar pelo giz e os livros, todos podendo apoiar e enriquecer a aprendizagem.
Segundo, as TICs, como qualquer ferramenta, devem ser usadas e adaptadas para servir a fins educacionais.
Terceiro, várias questões éticas e legais, como as vinculadas à propriedade do conhecimento, ao crescentemente tratamento da educação como uma mercadoria, à globalização da educação face à diversidade cultural, interferem no amplo uso das TICs na educação.
Na busca de soluções a essas questões, a UNESCO coopera com o governo brasileiro na promoção de ações de disseminação de TICs nas escolas com o objetivo de melhorar a qualidade do processo ensino-aprendizagem, entendendo que o letramento digital é uma decorrência natural da utilização frequente dessas tecnologias. O Ministério da Educação tem a meta de universalizar os laboratórios de informática em todas as escolas públicas até 2010, incluindo as rurais. A UNESCO também coopera com o programa TV escola, para explorar a convergência das mídias digitais na ampliação da interatividade dos conteúdos televisivos utilizados no ensino presencial e a distância.
A UNESCO no Brasil contou com a permanente parceria das cátedras UNESCO em Educação a Distância em várias universidades brasileira, que utilizam as TICs para promover a democratização do acesso ao conhecimento no país.
Em 4 de agosto de 2009,a UNESCO no Brasil e seus parceiros lançaram no país o projeto internacional ”Padrões de Competência em TICs para Professores”, por meio das versões em português brochuras sobre a proposta do projeto. O projeto tem o objetivo de fortalecer diretrizes sobre como melhorar as capacidades dos professores nas práticas de ensino por meio das TICs. Autoridades, especialistas e tomadores de decisão analisam a viabilidade da implementação das diretrizes deste projeto adaptadas à realidade brasileira.
Para usar a tecnologia nas escolas, segundo Almeida e Prado, ela deve ser pautada em princípios que privilegiem a construção do conhecimento, o aprendizado significativo e interdisciplinar e humanista. Para tanto os professores precisam se apropriar dessas novas tecnologias e desenvolver estratégias para um ensino-aprendizagem mais eficaz, visando o educando e seu contexto social. O uso das TICs em sala de aula confere um aumento no potencial colaborativo do ensino, já que essa tecnologia pode oferecer novas possibilidades de mediação social, criando ambientes de aprendizagem colaborativa (comunidades) que facilitem os alunos a realizarem atividades conjuntamente, atividades integradas com o mundo real e com objetivos reais. As pesquisadoras Ana García-Valcárcel, Verónica Basilotta y Camino López apontam a necessidade de posicionar as tecnologias como uma ferramenta e não como um fim em si mesmos, sendo encaradas como ferramentas cujo objetivo fundamental é ajudar o aluno a aprender de uma maneira mais eficiente.[10] 
Dessa forma, seja nas escolas ou no ensino superior, o uso das TICs não deve ser feito de forma indiscriminada e sem intencionalidade, mas associado ao planejamento docente e às escolhas de metodologias a fim de promover a participação ativa dos sujeitos envolvidos pluralizando as posições, ideias, opiniões e ações para que se estabeleçam o ensino e a aprendizagem.[11]
As Tics também se inserem no ensino de disciplinas específicas, como a História. A abordagem e utilização dessas tecnologias permite um ensino mais democrático e participativo, no qual os alunos desenvolvem uma construção coletiva e de pesquisa em relação ao pensamento histórico. Como a maioria dos alunos da educação básica faz parte da chamada Geração Z, na qual muitos já nasceram imersos no meio tecnológico e, os que ainda não o possuem, possivelmente já estabeleceram algum contato com as tecnologias, o ambiente de ensino constitui uma ressignificação nos dias atuais. O uso de ferramentas específicas como o rádio permite a compreensão, a partir de canções, de noções como "período histórico" por meio da análise de quando essas canções foram compostas. O  uso do rádio nas práticas metodológicas também se insere na comparação entre esse tipo de mídia com outros veículos digitais existentes em uma sociedade, como por exemplo programas televisivos, filmes, vídeos, sites de pesquisa, entre outros.[12]
editar - editar código-fonte - editar WikidataO motor a vapor, também chamado de máquina a vapor e turbina a vapor, é um tipo de máquina térmica que explora a pressão do vapor. Todas as máquinas térmicas funcionam baseadas no princípio de que o calor é uma forma de energia, ou seja, pode ser utilizado para produzir trabalho, e seu funcionamento obedece às leis da termodinâmica. Embora a invenção do motor de combustão interna no final do século XIX parecesse ter tornado obsoleta a máquina a vapor, ela ainda hoje é muito utilizadaː por exemplo, nos reatores nucleares que servem para a geração de eletricidade.
No caso da máquina a vapor, o fluido de trabalho é o vapor de água sob alta pressão e a alta temperatura. O funcionamento da turbina a vapor baseia-se no principio de expansão do vapor, gerando diminuição na temperatura e energia interna; essa energia interna perdida pela massa de gás reaparece na forma de energia mecânica, pela força exercida contra um êmbolo.
A primeira máquina a vapor relatada, foi a eolípila (também chamada de "bola de vento"), criada por Heron de Alexandria no século I. Em 1698, Thomas Savery, engenheiro militar inglês, criou um motor que leva seu nome que poderia ser utilizado dentro das fábricas, sendo considerado uma das evoluções iniciais da revolução industrial. Em 1712, Thomas Newcomen projetou uma nova máquina que poderia ser utilizada dentro de minas de carvão, a qual, apesar de mais lenta que as anteriores, podia tanto elevar água quanto cargas mais pesadas e tinha um custo de capital muito menor (uma vez que substituía os cavalos que eram usados no trabalho). Em 1769, Joseph Cugnot criou um triciclo[1] movido a vapor,[2] que é considerado o primeiro carro a vapor construído.[3] O veículo de Cugnot envolveu-se naquele que é tido como o primeiro acidente rodoviário motorizado da história.[4] Todavia, foi no ano de 1777 que o motor a vapor mais importante foi criado, quando James Watt, fabricante de instrumentos londrino, aperfeiçoou o motor a vapor de Newcomen. Após perceber uma falha no projeto da mesma, que era o tempo gasto, demasiadamente elevado, para ter o aquecimento, tanto do vapor quanto do combustível, em um mesmo cilindro. melhorando o projeto, criou assim um segundo cilindro.
Uma das primeiras utilizações da máquina a vapor foi para fabricação de tecidos, onde a água acumulada nas minas de ferro e de carvão era aquecida para gerar vapor. Graças a essas máquinas, a produção de mercadorias aumentou muito. E os lucros dos burgueses donos de fábricas cresceram na mesma proporção. Por isso, os empresários ingleses começaram a investir na instalação de indústrias. As fábricas se espalharam rapidamente e provocaram mudanças tão profundas que os historiadores atuais chamam aquele período de Primeira Revolução Industrial[6][7][8] O modo de vida e a mentalidade de milhões de pessoas se transformaram, numa velocidade espantosa. O mundo novo do capitalismo, da cidade, da tecnologia e da mudança incessante triunfou. As máquinas a vapor bombeavam a água para fora das minas de carvão. Eram tão importantes quanto as máquinas que produziam tecidos. As carruagens viajavam a 12 quilômetros por hora e os cavalos, quando se cansavam, tinham de ser trocados durante o percurso. Um trem da época alcançava 45 quilômetros por hora e podia seguir centenas de quilômetros. Assim, a Revolução Industrial tornou o mundo mais veloz. Como essas máquinas substituíam a força dos cavalos, convencionou-se medir a potência desses motores em HP (do inglês horse power, "cavalo-força"). Entre os séculos XIX e XX também foram feitas experiências, de pouco sucesso, com aeronaves a vapor.[9]
Era simplesmente uma caldeira de água, normalmente em formato de um sólido de revolução (como uma esfera) que girava mediante seu  aquecimento. O vapor gerado era expelido por orifícios laterais que criavam um impulso na caldeira fazendo-a girar.
O motor a vapor de Thomas Savery foi um modelo rudimentar de motor inventado em 1698, com o objetivo de bombear água do interior das minas de carvão inundadas,[13] que deu origem ao motor a vapor, após ser aperfeiçoada por Thomas Newcomen em 1712 e por James Watt em 1777, o que possibilitou Richard Trevithick inventar a locomotiva em 1801.[14]
O motor a vapor de Thomas Newcomen foi inventado por Thomas Newcomen em 1712 e é também chamado de máquina atmosférica de Newcomen. O motor é operado pelo vapor de condensação introduzido no cilindro, criando assim um vácuo parcial, permitindo assim que a pressão atmosférica empurre o pistão para dentro do cilindro. Foi o primeiro dispositivo prático a aproveitar o vapor para produzir trabalho mecânico.[15] Os motores de Newcomen foram usados em toda a Grã-Bretanha e Europa, principalmente para bombear água para fora das minas. Centenas foram construídas ao longo do século XVIII.[16]
O motor a vapor de James Watt era muito parecida com a de Newcomen, todavia existia uma segunda câmara ("câmara da condensação"), onde era criado o vácuo. Esta modificação foi muito eficaz, pois permitia que o pistão ficasse à mesma temperatura que o vapor, logo não haveria troca de calor entre eles, fazendo com que não houvesse perda de energia. Outra vantagem seria a de resfriamento, pois a câmara de condensação separada poderia ficar em uma temperatura mais baixa, necessitando de um resfriamento menor.[carece de fontes?]
A criação do motor a vapor fomentou o desenvolvimento de locomotivas a vapor e ferrovias, que também foram muito importantes para a revolução industrial. A ideia de um trem a vapor veio desde 1698 com Thomas Savery, porém só se tornou realidade após a criação da máquina de Watt. Entretanto James Watt não tinha o capital necessário para colocar em prática a sua máquina. Foi então que veio Richard Trevithick, que combinou a máquina de Watt e os transportes a carvão existentes (rudimentares) e criou a primeira locomotiva a vapor no ano de 1804 para a Penydarrem Iron Works no País de Gales.
Dentro do motor das locomotivas acontece a combustão do carbono e hidrogênio provenientes do carvão e do oxigênio do ar, produzindo calor. Porém um efeito negativo desta reação é ser uma grande causadora da poluição atmosférica. A energia química da reação  é transformada em energia térmica que é, então, convertida em energia mecânica, que corresponde à força motriz de funcionamento das locomotivas a vapor.
Há várias classificações para os motores a vapor, segue abaixo algumas delas:
Motores expansivos possuem um rendimento maior que os não expansivos, porém o trabalho gerado por cada impulso pelo segundo é maior. Logo, se a eficiência é o mais importante deve-se usar um motor expansivo e se for necessário uma grande quantidade de energia o motor não expansivo é o recomendado.
A eficiência de um motor a vapor pode ser representada pelo rendimento de máquinas térmicas, o qual depende basicamente de três grandezas:
Pela primeira lei da termodinâmica, conservação de energia, temos a transformação de calor fornecido (Q1) em trabalho (W), energia que será utilizada, e uma outra energia representada por ΔU (variação de energia interna) que representa a energia perdida no processo, tendo assim a formula a seguir:
Porém, dado que em um ciclo completo o ΔU deve ser 0, é, portanto, possível descobrir o trabalho(W) substituindo este valor na formula acima, resultando em:
Desta forma, tem-se que o rendimento da máquina térmica é dado pela razão entre o trabalho gerado(W) com o calor retirado da fonte quente(Q1). Com base nas equações acima descritas, chega-se às seguintes equações para representar o rendimento:         
Construtivamente as partes principais são:
Feita de aço fundido e usinado internamente, montada na horizontal. A espessura da carcaça pode ultrapassar 150 milímetros na região de alta pressão. A função da carcaça é conter todo o conjunto rotativo, composto pelo eixo e pelas palhetas, e adicionar as tubeiras (nozzles) fixos.
Embora a função seja simples, o projeto mecânico da carcaça é bastante complexo e crítico para o bom funcionamento da turbina a vapor. A principal razão disto, é a alta temperatura que a turbina funciona, e as pequenas folgas entre as partes fixas e as partes rotativas.
Quando o vapor entra na turbina, a alta temperatura, ocorre uma grande dilatação do material, que pode facilmente exceder 15 milímetros dependendo do tamanho da turbina. Quando ocorre esta dilatação térmica, há o risco de as folgas entre as partes fixas e móveis serem reduzidas a ponto de haver roçamento, e consequentemente, desgaste ou mesmo ruptura das palhetas.
Também, devido a grande espessura da parede, há grandes gradientes térmicos. A parte interna, em contato com o vapor, se dilata mais, devido à alta temperatura. A parte externa da parede, em contato com o ambiente, se dilata menos. Essa diferença entre a dilatação do material na parte interna e externa da parede dá origem a fortes tensões que podem causar distorção ou fadiga térmica.
Na carcaça são montados um conjunto de 2 a 4 mancais, dependendo do tamanho da turbina. Os mancais podem ser ainda:
Os mancais de turbinas a vapor não usam rolamentos. Eles são do tipo hidrodinâmico, em que o eixo flutua sobre um filme de óleo em alta pressão que é causada pelo próprio movimento do eixo, relativo à parede do mancal.
O mancal também tem um sistema de selagem de óleo e de vapor. Este sistema de selagem impede que vapores de óleo, ou de água, passem da turbina para o ambiente. Normalmente o sistema é constituído de uma série de labirintos que provocam uma perda de carga no fluxo de vapor, reduzindo o vazamento.
O rotor é a parte girante da turbina e responsável pela transmissão do torque ao acoplamento. No rotor são fixadas as palhetas, responsáveis pela extração de potência mecânica do vapor. O rotor é suportado pelos mancais, normalmente pelas extremidades. É fabricado com aços ligados e forjados. Os materiais que são empregados atualmente são ligas com altos percentuais de níquel, cromo ou molibdênio. Nas máquinas mais modernas, são feitos a partir de um lingote fundido à vácuo, e depois forjado.
O eixo deve ser cuidadosamente balanceado e livre de imperfeições superficiais, que podem funcionar com concentradoras de tensões, o que reduz a resistência à fadiga do eixo.
Em uma das extremidades do eixo é feito o acoplamento, seja a um gerador elétrico, ou a uma máquina de fluxo, como um ventilador, um compressor ou uma bomba. Mas, devido a necessidade de se obter uma rotação diferente no acoplamento, muitas vezes o eixo é ligado a uma caixa redutora de velocidade, onde a rotação da turbina é aumentada ou reduzida, para ser transmitida ao acoplamento.
As palhetas são perfis aerodinâmicos, projetados para que se obtenha em uma das faces uma pressão positiva, e na outra face uma pressão negativa. Da diferença de pressão entre as duas faces é obtida uma força resultante, que é transmitida ao eixo gerando o torque do eixo.
Os labirintos são peças aplicadas em turbinas a vapor com a finalidade de vedar a carcaça sem atritar. São fabricados na grande maioria em alumínio e são bipartidos radialmente para facilitar a manutenção da máquina. Internamente, eles são aplicados para garantir o rendimento da turbina. Nos casos em que há mais de um motor, o vapor não pode se dissipar dentro da carcaça para não perder energia e baixar o rendimento da máquina. Os labirintos também são utilizados na vedação da carcaça em relação ao ambiente externo, evitando também a dissipação do vapor para a atmosfera.
Nas turbinas de grande porte, há a injeção de vapor nos labirintos, por meio de uma tomada vinda da própria máquina, para equalizar as pressões e garantir a vedação da carcaça.
Atualmente, a energia gerada pelo vapor não é mais utilizada como antigamente, tendo sido substituída pelas máquinas de combustão interna, pelos seguintes motivos:
Entretanto, o vapor ainda é importante na geração de grande parte da energia elétrica de muitos países. Um exemplo são as usinas nucleares, que utilizam calor de reatores nucleares para produzir o vapor. Mais de 88% da energia elétrica dos Estados Unidos é gerada pelo vapor. Porém, não são mais utilizados motores a vapor, mas, sim, turbinas a vapor, cuja eficiência é maior.
Qualquer objeto pode ser utilizado como uma Arma quando usado para atacar ou ameaçar um ser bem como para autodefesa. A utilização de um objeto como arma, não transforma a natureza desse objeto numa arma por si, mas atribui-lhe essa característica durante a utilização. É o caso da expressão arma do crime, em que o objeto, qualquer que ele seja, utilizado para cometer um crime, foi utilizado como arma . As armas podem ter várias utilidades, passando a ser consideradas como tipos de armas. Neles se incluem as armas de caça, as armas de tiro desportivo, as armas de guerra, as armas de defesa, etc. Alguém que detenha uma arma está armado, do contrário, desarmado. Armas também podem ser usadas não só em seres como também em objetos, para destruí-los ou danificá-los. Armas são muitas vezes causas de morte: constituem meios para a prática de homicídio e suicídio. São divididas em diferentes tipos, mostrados a seguir:
Durante milhões de anos da evolução humana o homem viveu com suas armas naturais, ou seja, somente com suas defesas naturais, suas garras e dentes (como os demais animais). Durante o paleolítico surgiram as primeiras armas humanas de madeira e pedra lascada, pequenos grupos para se defenderem dos predadores, de outros grupos e para a caça criaram ferramentas capazes de causar ferimentos e matar a si e aos demais irmãos, já havia também uma preocupação em torno da defesa de seus pertences.
Com a evolução desses grupos o temor aumentava, pois quanto mais um grupo se desenvolvia, acumulava conhecimentos e posses, maior era a possibilidade de sofrer ataques de grupos rivais que tinham por objetivo tomarem para si tudo aquilo que possuíam, como os alimentos, as fêmeas para procriarem, a melhor caverna, a melhor localização em relação à caça e água, surgindo, dessa forma a necessidades do aperfeiçoamento dos meios de defesa pessoal, bem como do grupo social.
A necessidade de proteção e a tendência a agressões próprias do homem orientaram seus esforços para o desenvolvimento e fabricação de armas. A origem e a sequência dos primeiros meios mecânicos usados nas armas podem apenas ser imaginados, e com certeza surgiram na Pré-História, provavelmente o uso de um galho como prolongamento de suas mãos (garras), braços (lutar) e dentes (rasgar) e para melhorar a eficácia e a potência de uma pedra arremessada com a mão foi o primeiro aperfeiçoamento introduzido no armamento humano, quem sabe logo após eles perceberam que se a pedra fosse lapidada em formas pontudas, cortantes e perfurantes, ela mataria, aleijaria ou paralisaria, mais rapidamente.Assim as armas foram se evoluindo para se tornarem facas, espadas, adagas, punhais, etc.. Paralelamente, eles perceberam que se conseguissem lançar um projétil com precisão, eles poderiam atacar a presa ou inimigo sem se aproximar. Surgiu assim o conceito dos arcos e flechas, das bestas, dos bumerangues, etc.
Os dardos e as lanças leves de arremesso apareceram nos primórdios da civilização. A funda, cuja criação é atribuída aos fenícios ou aos habitantes das ilhas Baleares, foi usada como arma de guerra durante séculos.
A evolução humana caminhava vagarosamente, as necessidades eram maiores e o conhecimento restrito, surgindo, na pré-história, o período caracterizado pela generalização do uso de instrumentos metálicos pelo homem, tanto para o ataque como para a defesa. Seu início remonta a mais de 3000 a.C.. Surge a Idade do Cobre, fase do desenvolvimento cultura humano imediatamente posterior ao neolítico, entre o quarto e o segundo milênios antes da era cristã. Foi o primeiro período em que se utilizaram os metais de forma sistemática.
Com a descoberta do metal, principalmente do bronze, fase imediatamente posterior à Idade do Cobre, onde passou a ser a matéria principal para a fabricação de armas e utensílios, onde foi possível produzir espadas, lanças, facas, pontas de flechas, etc. mais eficientes para a caça e a defesa. Com o desenvolvimento desses grupos surge a função específica de defesa do grupo, concedida aos mais bravos e corajosos, surgindo, dessa forma, os exércitos que mantém sua função até os dias de hoje.
A invenção da pólvora pelos chineses e a sua introdução e desenvolvimento do seu uso na Europa após a Idade Média revolucionou as armas. Surgiram então os canhões, mosquetes, pistolas, que conseguiam lançar projéteis a velocidades e distâncias antes inimagináveis.
Com a invenção da pólvora foi possível construir aparelhos que arremessavam objetos a distâncias maiores que os aparelhos de energia mecânica, como as catapultas. Surgindo, assim, os canhões, que revolucionaram as batalhas e proporcionaram uma defesa e um ataque muito mais eficiente, tanto aos castelos, como às embarcações.
Com o desenvolvimento crescente e o passar dos anos os canhões reduziram em tamanho até chegar a uma dimensão que fosse possível ser transportado e manipulado por um só homem, surgindo os arcabuzes os mosquetes , as primeiras armas de fogo, que puderam ser consideradas de uso pessoal. Desde então as armas de fogo passaram a equipar desde um pequeno fazendeiro, que necessitasse defender sua família e seus bens, aos grandes exércitos para defenderem nações.
Hoje temos armas complexas e de uma potência extraordinária que não imaginamos e, muitas, sequer tomamos conhecimento. Uma evolução a passos largos, principalmente durante e após a Segunda Guerra Mundial.
Atualmente, com o desenvolvimento de mísseis, da energia nuclear, das engenharias química e biológica, as armas adquiriram um poder de destruição surpreendente.
A indústria armamentista é um negócio global que envolve a fabricação de armas e tecnologia e equipamento militares. Consiste no comércio e indústria envolvidos na investigação, desenvolvimento, produção e serviço de material, equipamentos e instalações militares. As empresas fabricantes de armas produzem armas principalmente para as Forças Armadas dos países. Departamentos de governo também operam no setor, comprando e vendendo armas, munições e outros artigos militares.
Estima-se que por ano, mais de 1,5 trilhões de dólares são gastos em despesas militares em todo o mundo (2,7% do PIB mundial).[1] Isso representa um declínio em relação a 1990, quando os gastos militares representavam 4% do PIB mundial. As vendas de armas somadas das 100 maiores empresas produtoras de armas chegaram a cerca de 315 bilhões (mil milhões) de dólares em 2006.[2] Em 2004, mais de 30 bilhões (mil milhões) de dólares foram gastos no comércio internacional de armas (um conceito que exclui as vendas internas de armas).[3]
O controle de armas é um termo usado para restrições ao desenvolvimento, produção, armazenamento, proliferação e uso de armas, especialmente armas de destruição em massa.[4] O controle de armas é normalmente exercido através do uso da diplomacia, que visa impor tais limitações mediante consentimento dos participantes por meio de tratados e acordos internacionais, embora possa também incluir os esforços de um país ou grupo de países para impor limitações sobre um país sem seu consentimento.
Em nível nacional ou comunitário, o controle de armas pode significar programas para controlar o acesso dos cidadãos às armas.[5] Isso é muitas vezes referido como política contra armas de fogo, uma vez que armas de fogo são o principal foco de tais esforços, na maioria dos lugares.[6]
O controle de armas é um tema extremamente polêmico a nível mundial.
Escultura é uma arte que representa ou ilustra imagens plásticas em relevo total ou parcial. Existem várias técnicas de trabalhar os materiais, como a cinzelação, a fundição, a moldagem ou a aglomeração de partículas para a criação de um objeto.
Vários materiais se prestam a esta arte, uns mais perenes como o bronze ou o mármore, outros mais fáceis de trabalhar, como a argila, a cera ou a madeira.
Embora possam ser utilizadas para representar qualquer coisa, ou até coisa nenhuma, tradicionalmente o objetivo maior foi sempre representar o corpo humano, ou a divindade numa forma antropomórfica. É considerada a quarta das artes clássicas.
Através da maior parte da história, permaneceram as obras dos artistas que utilizaram-se dos materiais mais perenes e duráveis possíveis como a pedra (mármore, pedra calcária, granito) ou metais (bronze, ouro, prata). Ou que usavam técnicas para melhorar a durabilidade de certos materiais (argila, terracota) ou que empregaram os materiais de origem orgânica mais nobres possíveis (madeiras duráveis como ébano, jacarandá, materiais como marfim ou âmbar). Mas de um modo geral, embora se possa esculpir em quase tudo que consiga manter por pelo menos algumas horas a forma idealizada (manteiga, gelo, cera, gesso, areia molhada), essas obras efêmeras não podem ser apreciadas por um público que não seja coevo.[1]
A escolha de um material normalmente implica a técnica a se utilizar.[2] A cinzelação, quando de um bloco de material se retira o que excede a figura utilizando ferramentas de corte próprias, para pedra ou madeira; a modelagem, quando se agrega material plástico até conseguir o efeito desejado, para cera ou argila; a fundição, quando se verte metal quente em um molde feito com outros materiais.
Modernamente, novas técnicas, como dobra e solda de chapas metálicas, moldagens com resinas, betão armado ou plásticos, ou mesmo a utilização da luz coerente para dar uma sensação de tridimensionalidade, tem sido tentadas e só o tempo dirá quais serão perenes.
Através do tempo, algumas formas especificas de esculturas foram mais utilizadas que outras: O busto, espécie de retrato do poderoso da época; a estátua eqüestre, tipicamente mostrando um poderoso senhor em seu cavalo; Fontes de água, especialmente em Roma, para coroar seus fabulosos aquedutos e onde a água corrente tinha um papel a representar; estátua, representando uma pessoa ou um deus em forma antropomórfica; Alto ou Baixo-relevo, o modo de ilustrar uma história em pedra ou metal; mobiliário, normalmente utilizado em jardins.[3]
As primeiras esculturas na Índia são atribuídas à civilização do vale do Indo, onde trabalhos em pedra e bronze foram descobertos, sendo uma das mais antigas esculturas do mundo. Mais tarde, com o desenvolvimento do hinduismo, do budismo, e do jansenismo, esta região produziu alguns dos mais intricados e elaborados bronzes. Alguns santuários, como o de Ellora, apresentam grandes estátuas esculpidas diretamente na rocha. Durante o século II a.C. no noroeste da Índia, onde hoje é o Paquistão e o Afeganistão, as esculturas começaram a representar passagens da vida e os ensinamentos de Buda. Embora a Índia tivesse uma longa tradição de esculturas religiosas, Buda nunca tinha sido representado na forma humana antes, apenas por símbolos. Este fato reflete já uma influencia artística persa e grega na região. A Índia influenciou ainda, através do budismo, boa parte da Ásia, como as existentes na localidade de Angkor, no Camboja.
Artefactos chineses datam do século X a.C., mas alguns períodos selecionados tiveram destaque: Dinastia Zhou (1050-771 a.C.) produziu alguns intrincados vasos em bronze fundido; Dinastia Han (206-220 a.C.) apresentou o espectacular Exército de terracota de Xian, em tamanho natural, defendendo a tumba do imperador; As primeiras esculturas de influencia budista aparecem no período dos Três reinos (século III); Dinastia Wei (séculos V e VI) nos da a escultura dos Gigantes grotescos, reconhecidas por suas qualidades e elegância. O período considerado a idade de ouro da China é a Dinastia Tang, com suas esculturas budistas, algumas monumentais, considerados tesouros da arte mundial.
Após este período a qualidade da escultura chinesa caiu muito. É interessante notar que a arte chinesa não tem nus, como é comum na arte ocidental, à exceção de pequenas estátuas para uso dos médicos tradicionais. Também tem poucos retratos, exceto nos mosteiros, onde eram mais comuns. E nada do que se produziu após a Dinastia Ming (após o século XVII) foi reconhecido como bom pelos museus e colecionadores de arte. No século XX a influencia do realismo socialista de origem soviética arruinou o que restava da arte chinesa. Espera-se que o ressurgimento e abertura para o mundo ocidental traga a arte chinesa ao seu lugar de mérito.[4]
Os japoneses faziam muitas estátuas associadas a religião, a maioria sob patrocínio do governo. Notáveis foram as chamadas ‘’haniva’’, esculturas em argila colocadas sobre tumbas, no período ‘’Kofun’’. A imagem em madeira do século IX de ‘’Shakyamuni’’, um Buda histórico é a típica escultura da era ‘’Heian’’, com seu corpo curvado, coberto com um denso drapeado e com uma austera expressão facial. A escola Key criou um novo estilo, mais realista.
Existem poucos exemplares de esculturas pré-colombianas no continente americano, entre elas as famosas estátuas da Ilha de Páscoa, algumas esculturas, principalmente em alto-relevo, decorando edificações Maia e Asteca do Peru ao México e algumas peças primitivas em madeira ou argila, geralmente com significado religioso, dos povos nativos de toda América.
No restante, só se começou a produzir arte a partir do século XVI, já sob influência do Barroco, com destaque para imagens religiosas em madeira, terracota e pedra macia nos locais de influência católica. Nos países de religião protestante, por sua maior resistência ao uso religioso de imagens, foi mais tardio o aparecimento de artistas, entrando diretamente no Neoclássico por influência da cultura europeia. A partir daí, com a facilidade de transporte e comunicações, a arte nas Américas ficou muito semelhante à desenvolvida na Europa.[5]
A escultura popular em argila do Nordeste brasileiro, as obras em madeira e argila dos povos da Amazônia, figuras religiosas em todas as regiões católicas da América também possuem sua relevância no contexto atual.
A arte da África tem uma ênfase especial pela escultura, especialmente em ébano e outras madeiras nobres. Além das divindades antropomórficas, tem especial interesse as máscaras rituais. As esculturas mais antigas são da cultura de Noque (cerca de 500 a.C.), no território onde atualmente se encontra a Nigéria.
A escultura no Antigo Egito visava dar uma forma física aos deuses e seus representantes na terra, os faraós. Regras rígidas deviam ser seguidas: homens eram mais escuros que mulheres; as mãos de figuras sentadas deveriam estar nos joelhos; e cada deus tinha suas regras especificas de representação. Por esse motivo, poucas modificações ocorreram em mais de três mil anos, embora tivessem resultado em peças maravilhosas como a cabeça de Nefertiti ou a máscara mortuária de Tutancâmon.
A Grécia clássica é com certeza o berço ocidental da arte de esculpir, desde seus primeiros artefatos a partir do século X a.C., em mármore ou bronze, até o apogeu da era de Péricles (século V a.C.), com as esculturas da Acrópole de Atenas. Foi também quando alguns escultores começaram a receber reconhecimento individual, como Fídias. Produziu obras impares, como a Vitória de Samotrácia, os mármores de Elgin ou a Vénus de Milo.
A partir dos gregos, os romanos, depois de um começo na tradição etrusca, abraçaram a cultura clássica e continuaram a produzir esculturas até o fim do império, numa escala monumental e numa quantidade impressionante, espalhando principalmente o trabalho em mármore por todo o império.
Após o fim do império e a idade média, onde pouco se fez, tivemos algumas esculturas góticas (séculos XII e XIII), basicamente como decoração de igrejas, como a porta da catedral de Chartres, arte fúnebre com suas tumbas elaboradas e as famosas gárgulas.[6]
Tudo pareceu culminar no Renascimento, com mestres como Donatello e seu Davi em bronze, a estátua eqüestre do Gattamelata ou suas inúmeras esculturas em mármore, abrindo caminho para a obra maior de Michelangelo, com seu magnífico David em mármore, a Pietá, ou Moisés. Provavelmente o David de Florença seja a escultura mais famosa do mundo desde que foi revelada em 8 de setembro de 1504.[7] É um exemplo do contrapposto, estilo de posicionar figuras humanas.
Quando Benvenuto Cellini criou um saleiro em ouro e ébano em 1540, mostrando Netuno e Anfitrite em formas alongadas e posições desconfortáveis, transformou o Naturalismo e criou a obra maior do Maneirismo que em sua forma mais exagerada virou o Barroco, que acrescenta elementos exteriores, como efeitos de iluminação. Bernini foi sem duvida o mais importante escultor desse período, com obras como O Êxtase de Santa Teresa.[8]
Após os excessos do Barroco, o Neoclassicismo é uma volta ao modelo helenista clássico, antes dos anos confusos do Modernismo, que teve a magnífica obra em bronze do francês Auguste Rodin e seu O Pensador, e depois enterrou a tradição clássica com o Cubismo, o Futurismo, o Minimalismo, as Instalações e a Pop Art.
Algumas das obras de escultura mais famosas são:
A Barragem das Três Gargantas é uma barragem hidrelétrica de gravidade que atravessa o rio Yangtze pela cidade de Sandouping, na prefeitura de Yichang, província de Hubei, China. Três Gargantas é a maior usina do mundo em termos de capacidade instalada (22.500 MW) desde 2012.[1][2] Em 2014, a barragem gerou 98,8 terawatt-hora (TWh) e alcançou o recorde mundial, mas foi superada pela Hidrelétrica de Itaipu, na fronteira Brasil–Paraguai, que estabeleceu o novo recorde mundial em 2016, produzindo 103,1 TWh.[3]
Exceto pelas eclusas, o projeto da barragem foi concluído e ficou totalmente funcional a partir de 4 de julho de 2012,[4][5] quando a última das principais turbinas de água da usina subterrânea iniciou a produção. As eclusas foram concluídas em dezembro de 2015.[6] Cada turbina tem uma capacidade de 700 MW.[7]
Além de produzir eletricidade, a barragem visa aumentar a capacidade de transporte do rio Yangtze e reduzir o potencial de inundações. A China considera o projeto monumental como bem-sucedido social e economicamente,[8] com o design de grandes turbinas de última geração[9] e um movimento para limitar as emissões de gases de efeito estufa.[10] No entanto, a barragem inundou sítios arqueológicos e culturais, deslocou cerca de 1,3 milhão de pessoas e causou mudanças ecológicas significativas, incluindo um aumento do risco de deslizamentos de terra.[11] A barragem tem sido controversa tanto no mercado interno quanto no exterior.[12]
Para se ter uma ideia da capacidade de armazenamento desta barragem, se estivesse cheio o seu lago artificial, com capacidade para cerca de 10 trilhões de galões (40 quilómetros cúbicos) de água, a duração de um dia na Terra seria prolongada em 0,06 microssegundos por conta do imenso deslocamento de massa causado.[13]
A grande barragem sobre o rio Yangtzé foi originalmente concebida por Sun Yat-sen em O Desenvolvimento Internacional da China, em 1919.[15][16] Ele afirmou que uma represa capaz de gerar 30 milhões de cavalos-vapor (22 GW) era possível a jusante das Três Gargantas (região ao longo do rio Yangtzé).[16]
Em 1932, o governo nacionalista, liderado por Chiang Kai-shek, começou o trabalho preliminar em relação aos planos nas Três Gargantas. Em 1939, as forças militares japonesas ocuparam Yichang e fizeram um levantamento da área. Um projeto, o plano de Otani, foi concluído para uma represa na expectativa de uma vitória japonesa sobre a China.
Em 1944, o engenheiro-chefe do Bureau of Reclamation dos Estados Unidos, John L. Savage, examinou a área e elaborou uma proposta de represa para o "Projeto do Rio Yangtzé".[17] Cerca de 54 engenheiros chineses foram para os EUA para treinamento. Os planos originais citados para a barragem consistiam em empregar um método único para movimentar os navios; os navios se moveriam em bloqueios localizados nas extremidades superiores e inferiores da barragem e, em seguida, guindastes com cabos moveriam os navios de um bloqueio para o próximo. No caso das embarcações de menor porte, grupos de embarcações seriam levantados em conjunto para obter eficiência. Não se sabe se esta solução foi considerada pelo seu rendimento em termos de economia de água ou porque os engenheiros pensaram que a diferença de altura entre os trechos do rio acima e abaixo da barragem fosse demasiado grande para métodos alternativos.[18] Alguma exploração, pesquisa, estudo econômico, e trabalho de design foram feitos, mas o governo, em meio à Guerra Civil Chinesa, interrompeu os trabalhos em 1947.
A construção da Usina de Três Gargantas foi iniciada em 3 de dezembro de 1992, e esteve envolta em polêmica pelo seu imenso impacto ambiental. Até fins de 2004, quatro turbinas entraram em funcionamento. Em 2009, com 26 turbinas instaladas, a capacidade concebida da barragem deverá ser de 18 200 megawatts, ultrapassando a potência de Itaipu, até então a maior usina hidrelétrica em potência instalada no mundo. Entretanto, o Rio Paraná, onde Itaipu está instalada, em função de sua hidrologia favorável face à do Rio Yang-Tsé, onde se localiza a usina de Três Gargantas, garantirá que Itaipu seja a maior usina hidrelétrica do mundo em energia gerada.
O vertedouro está projetado para ter uma vazão de 110 000 m³/s e vai ser, junto com o da Usina Hidrelétrica de Tucuruí, no Rio Tocantins, o maior do mundo em vazão. Diversas empresas brasileiras de consultoria participaram no projeto, tendo em vista a vasta experiência do Brasil com as grandes usinas de Itaipu e Tucuruí, dentre outras. Empresas brasileiras também participam das obras civis.
O construtor Wu Chuanlin, com cerca de 50 anos, começou a lidar com a obra em 1996. "Participei da construção de várias obras, e a usina das Três Gargantas é a maior que vi até agora. Tenho honra e orgulho por ter participado desta gigante obra que marca a história chinesa." Segundo o cronograma de construção, em novembro de 2006, foi efetuada a segunda represa provisória, e em 2005, o reservatório começou a encher. A eclusa entrou em funcionamento e o primeiro grupo de geradores começou a funcionar, entrando parcialmente em funcionamento. Ao ser concluída, a obra das Três Gargantas passou a ter como função a prevenção de enchentes, a geração de energia e facilitar o transporte fluvial, e por isso, ela desempenha um papel importante no futuro desenvolvimento sócio-econômico da China.
Desde o início da construção, o governo chinês e os engenheiros priorizaram a qualidade da obra. Comentando isso, um dos diretores do Projeto das Três Gargantas, Cao Guangjing, disse que a obra é umas das mais difíceis do mundo e na história. Durante o processo de construção, ocorreram alguns problemas, porém já foram resolvidos, e não vão deixar riscos potenciais na obra. "Temos um complexo controle de qualidade. Primeiro, as construtoras realizam uma auto-avaliação, segundo, há empresas que fazem a supervisão, terceiro, a nossa corporação, também vai verificar os resultados dos exames, e quarto, funcionam três centros técnicos e uma comissão de controle de qualidade para realizar o macro-controle em toda a obra. Além disso, a construção está submetida à supervisão de um grupo especial do Conselho de Estado, que inspeciona o projeto duas vezes por ano."
Entre todas as medidas, a mais importante é a instituição supervisora independente. Xu Chunyun, engenheiro que trabalha na entidade de supervisão disse: "Colocamos até 293 supervisores no canteiro da obra, que trabalham 24 horas por dia, assegurando que há sempre um supervisor acompanhando o processo de construção." Além disso, há quatro auditoras chinesas de prestígio trabalhando no setor, e outras empresas estrangeiras, entre elas a Electricité de France, o Bureau Veritas e a Empresa Spie, também da França e a empresa americana Harza.
A torre (do latim "turris"), em arquitectura e engenharia, é uma estrutura em que a altura é bastante superior à largura, apresentando uma demarcada verticalidade. Pode ser edificada para diversos fins ou funções (defesa, comemoração ou otimização de espaço). A sua morfologia e materiais construtivos tem apresentado variantes conforme a função, a época e a região em que são construídas. De um modo geral pode ser edificada como estrutura auto-portante independente ou como parte integrante de um edifício, e a sua planta pode variar formalmente: circular, quadrangular ou poligonal.
Este tipo de estrutura remonta à Antiguidade, de que são exemplo os zigurates babilónicos ou os minaretes islâmicos, com funções religiosas, e os faróis de Alexandria ou da Torre de Hércules, com funções de orientação à navegação.
No Oriente, o templo indiano possui também uma torre sobre a ala principal, para colocação da representação da divindade em pé. Esta torre, denominada "Sikhara", pode ser circular, quadrangular ou octogonal e pode mesmo ser substituída por uma cúpula achatada, caso a divindade no seu interior seja representada na posição sentada (de Ioga). Na mesma região pode citar-se ainda a stupa, edificação também com fins religiosos, composta por diversos níveis e que reduz a sua largura progressivamente em direcção ao topo.
Na China e no Japão, destacam-se o pagode, edificação religiosa composta por diversos níveis em altura (em número ímpar), em que se sobrepõem vários telhados de beirais curvos e prolongados que retrocedem em largura até ao topo. A planta é de forma variável (quadrangular ou poligonal) e a estrutura pode ser construída em madeira ou mármore com decorações escultóricas e sinetas.
Em termos de arquitetura militar, edificavam-se torres integradas nas muralhas defensivas ladeando as portas de acesso à povoação ou cidade e que, durante o período grego, passam a ser dispostas a distâncias regulares entre si apresentando uma planta circular ou semi-circular.
Na Europa ocidental, na Idade Média, a torre assume-se com um elemento arquitetónico de extrema importância funcional e simbólica, caso em que as suas dimensões variam até à monumentalidade. Podem ter as funções de defesa e de simples vigia, ou servirem de habitação (torre medieval).[1] Mais tarde, as torres passaram a fazer parte integrante dos castelos distribuindo-se por pontos estratégicos ao longo das muralhas. No interior do recinto situa-se a torre de menagem, habitação do senhor feudal, com planta quadrangular, dividida internamente em vários pavimentos. Alguns castelos possuem também uma torre forte que serve de atalaia (torre albarrã, ou "torre do haver", caso ali se guardasse o tesouro real).
Com o fim do período medieval, perdida a sua função defensiva diante dos progressos da artilharia, a cidade e o poder burguês vão-se apoderar da torre como elemento simbólico da independência administrativa, aplicando-a a palácios comunais e câmaras municipais ("torre comunal"). Esta aplicação vai ter especial impacto a norte da Europa (França, Bélgica, Holanda) quando as instituições governativas da cidade apresentam uma torre esguia de proporções exacerbadas em comparação com os edifícios em que estão integradas.
Ao longo da história a torre tem proporcionado aos seus usuários um recurso importante para o estabelecimento de posições defensivas e também para a obtenção de uma melhor visualização das áreas ao redor, incluindo campos de batalha. Eles foram instalados em paredes defensivas, ou erguidos perto de um alvo. Hoje, o uso estratégico das torres ainda é feito em prisões, campos militares e perímetros defensivos.
No plano da arquitertura eclesiástica, a torre da Idade Média torna-se também num dos elementos principais da caracterização da arquitetura religiosa em igrejas e catedrais.[2] Mas a torre campanário da igreja cristã (também Torre sineira) surge já na Itália do século VII permanecendo geralmente como elemento independente no sul da Europa e agregado ao edifício no norte.
Durante o estilo românico as torres da igreja são essencialmente simples, de planta quadrangular ou circular, decoradas com bandas horizontais de arcadas cegas e com coroamento em cone ou pirâmide octogonal.
No estilo gótico, a torre ajuda ao verticalismo acentuado da arquitetura gótica não só concorrendo em altura com as torres da catedrais de cidades vizinhas, como também estabelecendo a ligação entre a terra e o céu. A torre adquire uma extrema leveza e trabalho escultórico intrincado (traceria) especialmente nas catedrais francesas.
No mínimo, a igreja exibe uma torre, mas é comum encontrarem-se duas torres gêmeas a ladear a fachada principal a oeste, que raramente apresenta três, como é caso disso a westwerk, fachada com uma torre central ladeada de duas mais pequenas. Também pode ser edificada uma torre sobre o coro (Torre de coro), sobre o cruzeiro (Torre de cruzeiro ou Torre-lanterna) ou sobre os braços do transepto (mais raro).
As variações estilísticas são grandes e variam consoante os movimentos artísticos. No Renascimento, por exemplo, as ordens arquitetónicas de influência clássica são adaptadas à morfologia da torre e no barroco o coroamento assume uma forma volumétrica que combina curvas côncavas com convexas em forma de cebola (Torre bulbosa). Outros coroamentos são também usuais em torres, como em pirâmide, cone, cúpula ou sem coroamento (campanário).
Um tipo moderno de torre, o arranha-céu economiza bastante espaço no terreno. Arranha-céus muitas vezes não são classificados como torres, embora a maioria tenha o mesmo design e estrutura das torres. No Reino Unido, edifícios residenciais altos são referidos como torre de blocos. Nos Estados Unidos, os edifícios do World Trade Center tinham o apelido de Torres Gêmeas, nome compartilhado com as Torres Petronas, em Kuala Lumpur, na Malásia. [3]
Usando a gravidade para mover objetos ou substâncias para baixo, uma torre pode ser usado para armazenar itens ou líquidos como um silo ou uma caixa d'água, ou um objeto relativamente grande, como uma broca de perfuração. Na ausência de uma montanha, certas torres podem ser usadas para a prática de bungee jumping.
Na história, torres simples, como faróis, torres de relógio, torres de sinal e minaretes foram usados ​​para transmitir informações a distâncias cada vez maiores. Em anos mais recentes, antenas de rádio e torres de telefonia celular facilitaram a comunicação através da expansão do alcance do sinal. A Torre CN em Toronto, foi a estrutura mais alta do mundo entre os anos de 1976 até 2007, quando foi ultrapassada pelo edifício árabe Burj Khalifa.[4]
Torres também podem ser usados ​​para servir de apoio para pontes, e podem alcançar alturas que podem até rivalizar com alguns dos edifícios mais altos na superfície. È comum em pontes suspensas e pontes estaiadas. O uso do pilar, uma estrutur de torre simples, também ajudou a construir de pontes ferroviárias a portos.
Em aeroportos e aeroclubes, as torres de controle são usados ​​para dar assistência no tráfego aéreo.
Entre as torres mais conhecidas da atualidade, estão a Torre de Pisa, Torre Eiffel, o Big Ben e a Torre de Belém. A Torre de Pisa é notória por sua inclinação incomum de 3,99 graus.[5] A Torre Eiffel foi, durante os anos de 1889 e 1930 a estrutura mais alta do mundo, quando foi ultrapassado pelo Chrysler Building, entretanto, ainda é a maior estrutura de Paris [6]. No Reino Unido, o Palácio de Westminster abriga a tower clock, que é a torre do relógio, referido como Big Ben, que de acordo com especialistas, também está se inclinando.[7]
O Flamenco é a música, a guitarra, o canto, a dança e a percussão, cujas origens remontam às culturas hebraicas, israelitas, gitanas (ciganos: povos nômades) e mourisca, com influência árabe e judaica.[1] A cultura do Flamenco é associada principalmente à região da Andaluzia, na Espanha, assim como a Múrcia e Estremadura, e tornou-se um dos símbolos da cultura espanhola.
Em 16 de novembro de 2010, o Flamenco foi declarado patrimônio cultural imaterial da humanidade pela Organização das Nações Unidas para a Educação, a Ciência e a Cultura.[2]
As origens musicais (históricas, musicais e estéticas) do Flamenco consistem do canto (cante) e do acompanhamento de instrumentos de cordas e percussão. Começou a ser acompanhado por violão ou guitarra (toque), palmas, sapateado e dança (baile). O Flamenco têm em sua essência a Arte de se improvisar: os músicos criam novas melodias, novas harmonias, novos ritmos e novas estruturas em tempo real no momento em que estão apresentando a música. Todo a música advém da criação/composição de cada músico, inspirados nos Palos. Mas para conseguir improvisar são necessários muitos anos de estudo e ensaios. Cada artista conhece um código e uma linguagem, conhece os diferentes estilos flamencos, a estrutura do cante em cada um deles, e com base neste código e linguagem podem improvisar em palco.
O flamenco é uma expressão artística que nasce da mistura de muitas culturas: A hebraica, a israelita, a árabe, a judaica, a dos ciganos (povos nômades que chegaram à Espanha no século XV e muitos ficaram na Andaluzia). E com a cultura andaluza. O Flamenco surgiu dessa mistura cultural na Andaluzia. Ali começou e se desenvolveu esta arte, que ao longo do tempo se difundiu, tornando-se uma manifestação artística universal.
O toque (guitarra) e o baile podem também aparecer sem o cante, embora o canto permaneça no coração da tradição do flamenco. Mais recentemente, outros instrumentos como o cajón (ou adufe, em português: uma caixa de madeira usada como percussão) e as castanholas foram também introduzidos assim como vários outros instrumentos como o violino, o celo e flauta; o que veio a engrandecer as nuances musicais além da tradicional guitarra. Na música Flamenca tem-se a hegemonia das seguintes organizações musicais:
1-) Guitarra solista;
2-) Guitarra e percussão (as palmas e o cajón - a partir de Paco de Lucía)
3-) Guitarra, percussão e canto;
4-) Guitarra, canto, percussão e dança.
Variações destas organizações são constantes e presentes no Flamenco. Contudo, a presença da Guitarra é vital e permanente.
Muitos dos detalhes da história do desenvolvimento do flamenco podem não terem sido registrados na história da Espanha. São várias razões para essa falta de evidências históricas:
Foi nesta situação social e economicamente difícil que as culturas musicais de judeus, mouros e principalmente ciganos iniciaram a fundir-se no que se tornaria a forma básica do flamenco: o estilo de cantar dos mouros, que expressava a sua vida difícil na Andaluzia, os diferentes "compás" (estilos rítmicos), palmas ritmadas e movimentos de dança básicos. Muitas das músicas flamencas ainda refletem o espírito desesperado, a luta, a esperança, o orgulho e as festas noturnas durante essa época. Músicas mais recente de outras regiões de Espanha influenciaram e foram influenciadas pelo estilo tradicional do flamenco.
A primeira vez que o flamenco foi mencionado na literatura remonta a 1774 no livro "Cartas marruecas", de José Cadalso. No entanto, a origem do termo "flamenco" continua a ser assunto bastante debatido. Muitos pensam que se trata de um termo espanhol que originalmente significava flamengo ("flamende"). Contudo, existem outras teorias. Uma das quais sugere que a palavra tem origem árabe, retirada das palavras "felag mengu" (que significa algo como "camponês de passagem" ou "fugitivo camponês").
Durante a chamada "época de ouro" do flamenco, entre 1869 e 1910, o flamenco desenvolveu-se rapidamente nos chamados "cafés cantantes". Os dançarinos de flamenco, em sua maioria ciganos, também se tornaram numa das maiores atrações para o público desses cafés. Ao mesmo tempo, os guitarristas que acompanhavam esses dançarinos, foram ganhando reputação e dessa forma, nasceu, como uma arte própria, a guitarra do flamenco. Julián Arcas foi um dos primeiros compositores a escrever música flamenca especialmente para a guitarra.
A guitarra flamenca e o violão são descendentes do alaúde e, por conseguinte, da vihuela. Pensa-se que as primeiras guitarras (como é chamado o violão na Espanha) teriam aparecido em Espanha no século XV. A guitarra de flamenco tradicional é feita de madeira de cipreste e abeto e é mais leve e um pouco menor que a guitarra clássica, com o objetivo de produzir um som mais agudo.
Ainda é possível encontrar em outros folclores da Andaluzia o instrumento bandurria, uma espécie intermediária entre o alaúde e a guitarra flamenca propriamente dita.[3]
O flamenco é atualmente dividido em três categorias:
As categorias de flamenco se subdividem em estruturas musicais chamadas palos. Por exemplo:
Alguns autores do Flamenco na atualidade são:
Devemos sempre levar em consideração que, com o passar do tempo, as artes tem evoluído e sofrido diversas inserções com outras técnicas provenientes de outros estilos musicais, de canto e de dança, o que ocasionará alguns trabalhos em fusão, mas que não caracterizam um novo estilo ou divisão dentro da Arte Flamenca como um todo. 
Gamão é um jogo de tabuleiro para dois jogadores, realizado num caminho unidimensional, no qual os adversários movem suas peças em sentidos contrários, à medida que jogam os dados e estes determinam quantas "casas" serão avançadas, sendo vitorioso aquele que conseguir retirar todas as peças primeiro (de onde pode ser tido como sendo também um "jogo de corrida" ou "de percurso").[1]
Atividade lúdica de regras simples, o gamão desenvolve as relações lógico-matemáticas, com uso da estratégia: aprimora o senso matemático da subtração e adição não apenas pelas próprias jogadas, como ainda pela antecipação dos movimentos do adversário.[2]
É visto como um passatempo do qual se tira proveito prático, pois exercita a capacidade de elaborar pensamentos estratégicos, como o xadrez.[3] Dele se diz que é "o rei dos jogos e o jogo dos reis".[4][5]
Datado de cerca de 3500 a.C, o Senet era um jogo de tabuleiro praticado no Antigo Egito que remotamente é relacionado com o gamão; entretanto duvida-se que o gamão tenha se desenvolvido a partir dele, uma vez que não foram encontrados tabuleiros de Senet no período entre 1500 a.C a 200.[6] Contudo, é considerado um predecessor do gamão.[7]
O Jogo Real de Ur foi descoberto por arqueólogos britânicos nos anos 1922 a 1934 na área que compreendia a antiga Mesopotâmia, na região de Ur (atual Iraque). Escavações em túmulos de antigos nobres sumérios revelaram diversos tabuleiros de madeira, ricamente ornados com lápis-lazúli e madrepérola, cuja reconstrução feita pelos pesquisadores do Museu Britânico revelou ser bastante parecido com o moderno gamão. Recentemente suas regras foram encontradas.[8]
Na Roma Antiga havia o Ludus duodecim scriptorum, modernamente rebatizado com o nome de jogo das doze linhas;[9] este possivelmente deriva do Senet egípcio, pois conserva a base similar de três por doze pontos. O autor H. J. R. Murray, no livro A History of Board-Games other than Chess, é de opinião que este jogo seja uma cópia do grego Kubeia, que Platão registrou ter origem no Egito,[9] no seu diálogo socrático A República, vol. X.[10]
Na Índia havia um jogo chamado Parchessi, que parece ser o mais direto predecessor do gamão. Nele as peças ficavam fora do tabuleiro e tinham que percorrê-lo para saírem do lado oposto.[11]
O jogo chamado Narde era praticado na Pérsia antiga, com formato similar ao jogo romano, com a diferença de que eram usados dois dados para dirigir o movimento das peças. Tal como o moderno gamão, as peças tinham uma posição fixa para o início do jogo; o Narde é referido no Talmude babilônico, datado de cerca de 500 a.C; teria sido inventado por Artaxes I, fundador da Dinastia Sassânida e, como tal império se estendeu até a Índia, é possível que por tal motivo dão-no como originário daquele país.[9]
A partir do século IV a variante romana denominada Tabula já tinha, salvo algumas variações pequenas, as feições do gamão atual, e durante a Idade Média era comum a referência ao "jogo de tábula".[12]
O historiador Sir John L. Myers disse que boa parte dos registros escritos se referem às mudanças; as relações entre os jogos originais de tabuleiros e seus sucessores são paradigma de como os povos exercem influência cultural sobre os demais vizinhos;[13] os jogos, assim passados ao longo do tempo, vão sofrendo metamorfoses até sua compleição moderna - tal como o xadrez moderno tem o mesmo ancestral comum com o xadrez japonês, que é a modalidade chinesa antiga.[9]
O gamão, assim, experimentou tais transformações; sofreu a influência daqueles diversos jogos primitivos do oriente, até ser levado pelos romanos ao Ocidente na configuração do jogo da tabula, cuja única diferença do narde era que o uso do primeiro desapareceu. As variantes orientais se propagaram junto com a expansão do Islã, até serem levadas novamente à Europa com as  Cruzadas.[9]
Com as feições modernas de desenho e movimentação das peças acredita-se que tenha surgido em torno do século X.[10]
Na Canção de Rolando, poema épico francês do século XI, é dito que os soldados gostavam de jogar o gamão, enquanto os chefes preferiam o xadrez.[14]
A primeira normatização das regras do gamão foi publicada em 1743 pelo inglês Edmond Hoyle, no livro A Short Treatise on the Game of Backgammon[6] (Pequeno Tratado sobre o Jogo de Gamão, em livre tradução). A obra trazia ainda uma série de estratégias, que são válidas mesmo nos tempos atuais.[5]
A "leis do gamão", segundo Hoyle:[10]
1. Se o jogador tocar numa peça ela deve ser movida, se possível; se for impossível uma jogada, não há penalidade.2. Um jogador não conclui sua jogada até que coloque sua peça num ponto e a deixe lá. 3. Se o jogador deixa uma peça esquecida fora do jogo, não há penalidade. 4. Se o jogador move qualquer número de peças antes de entrar uma peça capturada, ele pode retornar esta peça mais tarde. 5. Se o jogador se enganou na sua movimentação, e o adversário já tiver lançado os dados, não há qualquer chance de o jogador alterar o erro, a menos que ambos concordem em permiti-lo. 6. Se um dos dados ficar "inclinado", i.e., não ficarem completamente pousados no tabuleiro, um novo lançamento é imperativo.[nota 1]
O escritor e matemático britânico Lewis Carroll, autor do clássico infantil Alice no País das Maravilhas, registrou diversas anotações em que jogava gamão, e ainda a elaboração de variantes para o jogo. Em 6 de janeiro de 1868 escreveu que havia experimentado com a irmã Margaret "...um novo tipo de gamão, que estou pensando em chamar de 'gamão descoberto'". De fato, Carrol criou variantes do jogo, que batizara "gamão de três", "gamão cooperativo" e "gamão alemão cooperativo", inspirado o último em Enid Stevens.[15] O seu "gamão cooperativo" teve as regras publicadas em 6 de março de 1894 no jornal The Times, na seção de anúncios pessoais. Antes, contudo, ele anotara, em 4 de fevereiro daquele ano: "Ocorreu-me uma ideia para uma variação interessante de gamão, em que se jogam três dados e se escolhem dois dos três números: a qualidade média dos lances aumentaria enormemente. Calculo que a chance de sair 6 e 6 seria duas vezes e meia maior do que é hoje. Também constituiria um meio, semelhante à concessão de pontos no bilhar, de nivelar os dois jogadores: o mais fraco poderia usar três dados e o outro, dois. Estou pensando em chamá-lo 'Gamão de Três'[nota 2]".[15]
No século XX ocorre o chamado "auge do gamão" quando, nos anos 1960, o príncipe russo Alexis Obolensky promove grande popularização do jogo e realiza o primeiro campeonato mundial.[16] O ressurgimento da popularidade do gamão dura desde então, até os tempos atuais.[17]
Finalmente, graças ao desenvolvimento da rede neural, o primeiro sucesso na construção de um programa capaz de jogar o gamão e aprender com os erros foi concretizado com sucesso em 1989, por Gerald Tesauro, e desenvolvido mais tarde por ele, em 1994, numa das atividades que marcam o pioneirismo neste campo científico.[1] O sistema operacional Windows, da Microsoft, incluía o gamão em suas primeiras versões e, a partir da versão Windows 2000,  permitiu jogar online.[18]
Inventários registram a presença do jogo no país desde o século XVI, e os autos do Tribunal do Santo Ofício, na Bahia e Pernambuco,  referiam-se  às blasfêmias proferidas durante partidas. Assim, o gamão, tal qual os jogos de cartas, parece ter sido parte dos padrões de sociabilidade da época.[19]
Durante a Inconfidência Mineira, no final do século XVIII, dentre os bens confiscados do revoltoso José Ayres Gomes estava um "tabuleiro de jogar gamão com suas tabelas respectivas"; consta ainda que os inconfidentes em suas atividades sociais, festas e jogos, falavam muito quando bebiam, o que permitiu que as tramas do movimento fossem descobertas. Esses registros dão conta de que o jogo era bastante difundido na colônia.[19]
Gamão é uma palavra de origem incerta[20] e controversa.[10] Alguns autores, contudo, da língua inglesa, defendem que o termo derive das palavras galesa "bac" (ou bach) e "gammon" (ou cammaun) - que pode ser traduzida para batalha, esta última. Oswald Jacoby e John R. Crawford, autores de The Backgammon Book, defendem que a palavra deriva do inglês antigo, derivando da junção dos vocábulos "baec" ("costas") e "gamen" ("jogo"), significando "jogo da volta".[9][10] Uma hipótese bastante provável para a origem do nome em inglês seja decorrente do fato de que os tabuleiros do gamão comumente serem colocados no verso do de xadrez - daí o termo backgammon (ou back gammon, como se grafava) significar nada mais do que "o jogo do verso [do tabuleiro]" - ou the game on the back, no original.[9][11]
O gamão é jogado num tabuleiro próprio e característico, composto por quatro fileiras com seis casas cada uma, sendo duas fileiras numa margem e duas na outra. Tradicionalmente as casas consistem de uma pintura em forma de flecha[nota 3] onde cada um dos dois jogadores dispõe suas quinze peças: oito nas duas fileiras do seu campo e sete nas do adversário.[12]
Além do tabuleiro o gamão compõe-se de um dado clássico (de seis faces) e trinta peças em formato circular (discos), sendo cada metade do total de peças em cores distintas da outra metade, uma é mais escura e a outra é mais clara, semelhante às do jogo de damas, e que pertencerão a cada um dos contendores. Para o lançamento dos dados os jogadores possuem um copo próprio e, opcionalmente, pode haver um dado para apostas (com numeração de 2, 4, 8, 16, 32 e 64 nas seis faces).[12]
“O gamão é um jogo com revoltantes mudanças da sorte.[nota 4]”
Inicialmente os contendores escolhem qual a cor das suas peças e para qual dos campos deverá conduzi-las, sendo este chamado de "campo-casa", nesta descrição; as quinze peças são então arrumadas da seguinte forma (o adversário fará o mesmo, nas casas opostas) para as peças vermelhas (de acordo com a figura), sendo oito nos dois campos inferiores e sete no campo oposto:[12]
O participante que irá iniciar o jogo é conhecido por meio do lançamento de um dado: ambos os adversários lançam um dos dados, e àquele que obtiver maior número caberá a jogada inicial, utilizando este mesmo resultado na primeira jogada. A consequência disto é que quem começa nunca o fará utilizando números iguais. O objetivo de cada participante é mover todas as suas quinze peças para o seu "campo-casa", deslocando-se cada peça de acordo com o resultado dos dados; uma vez conseguido isto, deve começar a retirá-las, vencendo aquele que primeiro o fizer.[12]
As peças devem ser movidas da seguinte forma:[12]
Uma das estratégias do jogo é comer as peças do adversário, a fim de retardar seu avanço. Uma peça que for colocada sozinha numa casa pode ser tomada - o que não é obrigatório, pois pode ser desvantajoso taticamente - o processo de comer uma peça e seu retorno ao jogo ocorre da seguinte forma:[12]
Ao mover suas peças o jogador deve obedecer a dois princípios táticos iniciais:[10]
Na corrida das peças até seu campo-casa o jogador deve usar de prudência na captura das peças adversárias pois, retornando do princípio, esta peça pode por sua vez comer eventuais peças descobertas e obstar suas próprias manobras.[12]
Uma estratégia é defensiva quando o jogador procura arrumar suas peças sempre a formar casas; é dita agressiva quando se adota a tática de expor suas peças à captura, sendo essa escolha livre e podendo ocorrer em qualquer momento do jogo.[12]
A "arte do bloqueio" consiste no arranjar as casas fechadas de modo a evitar o avanço do adversário, impossibilitando-o de até mesmo marcar pontos.[12]
Uma vez colocadas todas as peças dentro do campo-casa, deve o jogador começar a retirá-las do tabuleiro, da seguinte forma:[12]
Uma vez retiradas todas as peças do tabuleiro antes que o adversário o faça este será vencedor e a partida termina. Se o fizer sem que nenhuma das peças do adversário seja retirada, diz-se que houve o gamão, a vitória máxima do jogo.[10]
Duas variantes do gamão tiveram grande sucesso no século XIX: o triquetraque e o chaquete (adaptação no nome francês Jacquet); a variante estadunidense, criada no começo do século XX, passou a incorporar o dado de apostas,[12] dando início às regras do gamão moderno.[11]
Uma variante praticada no Levante é o Plakoto, onde todas as quinze peças de cada jogador são colocadas na primeira casa do campo-casa do adversário.[9]
Também se registram o Chouette, jogado com três ou quatro participantes; o gamão com parceiro, gamão holandês (ou Acey Deucey como é chamado nas Forças Armadas dos Estados Unidos), gamão turco ou Moultezim, o russo e outras adaptações locais.[17]
O triquetraque é uma variante francesa do gamão. Durante o reinado de Luís XVI (século XVIII) o jogo tornou-se bastante popular; eram feitas mesas de jogo com uma tampa que muitas vezes tinha um tabuleiro de xadrez na face oposta; ao se remover a tampa, aparecia o tabuleiro de triquetraque.[6] O nome é uma onomatopeia, ou seja, deriva do som que os dados fazem no copo de lançamento.[10]
Uma tentativa por eliminar o fator sorte do jogo; os pontos são determinados pelas faces numeradas de um dominó, do qual foram retiradas as peças com marcação zero. Distribuídos os pontos entre os dois jogadores - um ficando com as pedras 6-6, 3-3, 1-1, 6-4, 6-2, 5-4, 5-3, 5-1, 4-2, 3-1, 2-1, e o outro com as restantes 5-5, 4-4, 2-2, 6-5, 6-3, 6-1, 5-2, 4-3, 4-1, 3-2; essas peças podem ser jogadas mais de uma vez, exceto os que contém dobra de pontos, que são usadas somente uma vez - depois do uso inicial: à medida que uma peça é usada para marcar os pontos, ela é colocada do lado oposto do tabuleiro e, finalizado o uso de todas elas, podem ser reutilizadas no mesmo processo.[24]
Após a invenção do dado de apostas nos Estados Unidos, na década de 1920, o interesse pelo gamão ganhou novo impulso, e houve a formação de diversos clubes a reunir os adeptos, tanto naquele país quanto em outros. Em 1931 Wheaton Vaughan, presidente do comitê de gamão do New York Racquet and Tennis Club, escreveu as regras que hoje são adotadas para as competições.[5]
Finalmente, por intercessão do Príncipe Alexis Obolensky foi organizado nas Bahamas em 1964 o primeiro campeonato internacional da modalidade; em 1973 ocorreu em Monte Carlo o primeiro grande torneio europeu de gamão.[5]
Obolensky sugeriu que houvesse um campeonato mundial, e o primeiro ocorreu em Las Vegas, no ano de 1967, sagrando o estadunidense Tim Holland como primeiro campeão; ele veio a vencer as duas edições seguintes: 1968 e 1970.[25]
De 25 a 27 de janeiro de 2007 o Atlantis Resort das Bahamas organizou uma competição de gamão com prêmio de um milhão de dólares, recebendo cobertura da televisão de vários países.[5]
No Brasil a modalidade é dirigida pela Associação Brasileira de Gamão, com sede na cidade de São Paulo.[26] Fundada em 1982, sem muito sucesso no início,[27] a ABG em 1994 passou a ter uma sede, decorada por Tessa Palhano e com diversos tabuleiros para a prática do jogo.[28] Além de organizar certames nacionais, a ela estão filiadas as instituições estaduais, como a Associação de Gamão do Estado do Rio de Janeiro (AGERJ) e a Associação Baiana de Gamão, que organizam torneios próprios e as etapas dos nacionais.[29]
A palavra persa qafas é raiz etimológica de outras passando, por exemplo, pelo grego κάῳα (caixa), pelo latim capsus, raiz da portuguesa cabaz, que define um cesto. O termo tem no seu significado original, entre outros, "casinha do jogo do gamão".[14]
O filósofo escocês David Hume do século XVIII declarou que toda especulação metafísica parece "fria, forçada e ridícula" se encetada após um bom jantar e uma partida de gamão.[30]
Do mesmo período é o poeta português Nicolau Tolentino de Almeida (1740-1811); suas Obras poéticas trazem várias citações ao gamão, sendo mesmo um dos poemas intitulado "A Dois Velhos Jogando o Gamão", onde narra uma partida entre dois idosos numa botica na qual um dos jogadores, ao perder, atira o tabuleiro sobre o outro e, errando o golpe, acerta na melhor garrafa do boticário.[31]
Prosper Mérimée (1803-1870), célebre autor de Carmen, escreveu o conto "A partida de gamão", onde mantém a sua característica de narrar a história por um personagem que é estranho aos fatos.[32] O conto relata como, para tirar os pontos 6 e 4 que lhe dariam a vitória e o resultado de vultosa aposta, um jovem trapaceia - e as tristes consequências deste ato.[33]
Charles Darwin, criador da teoria da evolução das espécies, tinha com a esposa Emma a tradição de jogar partidas de gamão. Em dado momento de sua velhice, Darwin brincava revelando ter ganho 2795 partidas, contra 2490 da esposa.[34]
Quando morando na cidade mineira de Campanha, o escritor brasileiro Euclides da Cunha fez amizade com um vizinho chamado Júlio Bueno; em suas partidas Cunha não admitia a derrota e, durante uma delas em que teve todas as suas peças presas, sem poder mover-se, exigiu que as regras fossem alteradas para que ele não mais se visse naquela situação. Ante o destempero do escritor, o vizinho aquiesceu e aquele acabou vitorioso e declarando: "Você vai aprender para jogar comigo. Fique sabendo que eu sou invencível no gamão." Mais tarde Bueno publicou o relato num jornal local, e este foi usado por Dilermando de Assis como peça de defesa no seu julgamento pelo homicídio do autor de Os Sertões.[35]
Durante o "auge do gamão" na década de 1960 diversas celebridades ostentaram o gosto pelo jogo, como Aristóteles Onassis, Hugh Hefner (que promovia torneios na sua mansão), Lucille Ball, princesa Grace Kelly de Mônaco, entre outros.[16]
O agente do FBI, Joseph D. Pistone, que inspirou o filme Donnie Brasco, quando se infiltrou na máfia usando este pseudônimo nos anos 1970, relatou que precisou aprender a jogar gamão, pois era "uma boa maneira de entrar, de conseguir uma introdução, de provocar alguma conversa com os frequentadores regulares" de um bar que lhe seria a porta de entrada no mundo dos mafiosos da família Bonanno.[36]
O humorista brasileiro Chico Anysio, no seu livro "A Curva do Calombo", traz uma crônica intitulada "O Jogo de Gamão", onde descreve uma mãe enciumada das filhas, exercendo-lhes o controle e todos os dias chamando-as para jogar: "Filha minha eu levo de rédea curta. No meu pasto cavalo não come. (...) Venha jogar um gamão. Não há argumento. Chamou, tem que ir." Quando a velha morre, finalmente livres das peias maternas, o gamão serve para que finalmente encontrem novos parceiros de jogo e, talvez, pretendentes.[37]
No filme 007 contra Octopussy, de 1983, o vilão Kamal Khan perde para James Bond um Ovo Fabergé, fruto de seu roubo, numa partida de gamão.[38]
No seriado Lost (2004-2010) o personagem John Locke joga uma partida de gamão com Walt Lloyd ao tempo em que usa o jogo como alegoria para explicar a dualidade das coisas, como as cores das peças numa analogia ao bem e o mal, por exemplo.[39]
O críquete (em inglês: cricket) é um desporto que utiliza bola e tacos, cuja origem remonta ao sul da Inglaterra, durante o ano de 1566. Considerado por muitos um desporto parecido com o basebol, foi inspirado num rudimentar jogo rural medieval chamado stoolball. Foi adotado pela nobreza no século XVII.[1] Sofreu muitas transformações ao longo dos anos até se tornar um desporto bastante admirado no Reino Unido, na Índia e no Paquistão.[2]
No Brasil, ficou conhecido como taco, pela semelhança de nomes costuma ser confundido com o croquet, sendo este um desporto totalmente diferente.
Jogam onze atletas de cada lado. Os movimentos principais passam-se numa faixa retangular de 20,1 metros de comprimento, no centro do campo, onde a bola (de cortiça e couro) chega a voar 150 km/h. Ela é lançada pelo arremessador contra o alvo do adversário (três varetas fincadas no solo, chamadas stumps, cujo conjunto é conhecido como wicket), defendido pelo rebatedor.[2][3] O rebatedor pode rebater a bola e então correr entre duas áreas no campo para marcar corridas (pontos), mas pode estar eliminado se o alvo numa área está tocada por a bola e o rebatedor não está nessa zona.
No início, as partidas de críquete podiam durar até dez dias. Hoje, a maioria dos jogos é disputada com dois tempos, em uma tarde ou noite (as partidas de teste chegam aos cinco dias de duração).[3] Também existe o T20 críquete que dura 3 horas.
O críquete é um desporto que possui 2 objetivos básicos:[2]
Caso o time perca todos os seus 10 wickets (all out), a entrada se encerra e as posições de atacante e defensor são invertidas entre as equipes, de modo no qual quem até então defendia terá a chance de atacar para marcar corridas ao término das entradas (existem partidas de uma e de duas entradas para cada time). Caso seja a última entrada, a partida é finalizada automaticamente.[4]
Se em partidas de duas entradas (innings) o primeiro time que rebater for derrotado na segunda entrada e o somatório dos pontos da primeira e da segunda entrada for menor que os pontos obtidos somente da primeira entrada do time oponente (o que acontece muito raramente), a partida é concluída e a vitória da outra equipe é anunciada como "um entrada e x corridas" (an innings and x runs), onde x é a diferença de pontos entre os times. Se o time que rebate por último é derrotado com o mesmo número de pontos do opositor, eles tem uma corrida curta para marcar (algo extremamente raro) para desempatar a partida.[5]
Caso a partida tenha apenas um turno por time, com um determinado número de jogadas, e a mesma seja temporariamente interrompida pelo clima, então uma complexa fórmula matemática conhecida como Método Duckworth-Lewis é geralmente usada para recalcular uma nova tabela de pontos. Uma "partida de um dia" (one-day match) pode ser declarada "sem-resultado" (No-Result) se o número de overs (conjunto de seis arremessos válidos por um arremessador) por time for menor que o acordado anteriormente. Isto pode ocorrer se sucessivas interrupções tornarem o recomeço impossível, por exemplo um período de mal tempo prolongado ou quando o público estiver tumultuado.[6]
O jogo é praticado de acordo com as quarenta e duas leis do críquete, que foram desenvolvidas pelo Marylebone Cricket Club em discussões com os países praticantes do desporto.[4][7] Algumas partidas em particular podem discutir regras únicas para as mesmas. Outras normativas suplementam as leis principais e as mudam para concordar em diferentes circunstâncias. Existe um número de modificações à estrutura de jogo e regras de posicionamento em campo que se aplicam a innings games, estando estas restritas a um determinado número de "justas entregas''
Uma entrada (inning) no críquete consiste no turno em que cada equipe usa para defender (arremesso) ou atacar (rebater). Cada entrada no críquete pode ter diferentes quantidades de overs.[3]
Um over no críquete consiste em uma série de 6 arremessos válidos feita por apenas um jogador, dependendo das leis do jogo costuma-se ter entre 10 e 50 overs por entrada.[8].
Cada equipe é formada por 11 jogadores. Dependendo das suas habilidades primárias, um atleta pode ser classificado como um batsman ou bowler especialista. Um time balanceado costuma ter os seus "especialistas", sendo 5 ou 6 batsmen e 4 ou 5 bowlers. Um jogador nestas duas posições é conhecido como craque. A posição de wicket-keeper é a mais especializada dentre as presentes no campo de jogo.[9][10]
A partida é presidida por dois árbitros em campo. Um deles fica ao lado do wicket no final de onde a bola será arremessada e arbitrará a maioria das decisões. O outro atua próximo da posição de campo chamada leg-side, que oferece uma vista lateral do batsman, e assiste as decisões das quais ele tenha a melhor visão. Em algumas partidas profissionais eles podem pedir a presença de um terceiro juiz fora do campo, que tenha a assistência dos replays de televisão, sendo que este resguardará o jogo com suas regras.[11]
Nas leis do críquete existe a regulação da posição de marcador (ou em inglês scorer), que é o responsável por registrar todas as ações que geram pontos para os times: corridas (runs), wickets e (quando apropriado) os overs. Eles conhecem os sinais do árbitro, e para verificar a precisão do resultado regularmente, conferem suas marcações com os mesmos durante os intervalos.[12]
Na prática, os marcadores também podem acompanhar outros assuntos, tais como análises dos bowlers, a taxa de desempenho durante os overs, e as estatísticas da equipe como médias e registros. Em competições nacionais e internacionais de críquete, a mídia tende a requerer uma notificação dos registros e das estatísticas, sendo que os marcadores normalmente mantém correspondência oficial com comentaristas e jornalistas durante as transmissões das partidas. Os marcadores ocasionalmente cometem erros, mas estes podem ser corrigidos após o evento.[12]
O campo de críquete consiste em um grande gramado em formato oval. Não existem dimensões fixas, mas seu diâmetro geralmente varia entre 137m e 150m. Em alguns campos uma corda demarca o seu perímetro, então nomeado como boundary.[6]
A maioria das ações tem lugar no centro do campo, em uma faixa retangular de barro batido ou grama baixa chamada pitch, cuja medição é de 3,05 m × 20,12 m (10 × 66 pés).
Em cada extremidade do pitch são fincados ao chão 3 estacas de madeira chamados de stumps. Dois pedaços de madeira, chamados de bails, são encaixados no topo dos stumps, ligando-os aos seus vizinhos. Cada conjunto de 3 stumps e 2 bails é comumente conhecido como wicket.[13]
Uma extremidade do pitch é designada a atividade de rebater (batting) onde o batsman se posiciona, e a outra é designada a atividade de bolear, ou bowling (na qual o boleador, ou bowler, arremessa). A área do campo ao lado da linha a qual o batsman segura seu taco (o lado da mão direita é chamado para o jogador destro, o esquerdo para o canhoto) é conhecido como off-side, enquanto o outro lado é chamado de leg-side ou on-side.[14]
Linhas desenhadas ou pintadas no pitch são conhecidas como creases, que são usadas para demarcar a eliminação dos batsmen (além de determinar quando um arremesso é válido).
Se os rebatedores correm entre as areas de batedores (as areas onde o rebatedor rebate e o arremessador arremessa), ou se a pelota va ao fim do campo, isso vale pontos.
Umas maneiras de perder wickets[15]:
O time que está rebatendo sempre entra com dois batsmen em campo. Um é o rebatedor, conhecido como "striker". Outro batsmen é conhecido como "não striker" e fica no final do pitch para recolher as bolas rebatidas.
A equipe que arremessa tem os seus onze jogadores no campo, que se revezam durante os arremessos. O jogador, conhecido como Bowler deve mudar após cada over. O wicket-keeper, que geralmente atua no papel toda a partida, se posiciona em pé ou agachado atrás do wicket. O capitão da equipe espalha os nove jogadores restantes, chamados defensores, em torno da Outfield para conseguirem cobrir a maior área possível. As posições mundam drasticamente de acordo com as estratégias da equipe.
No dia da partida, os capitães inspecionam o campo para determinar quais bowlers seriam mais adequados de acordo com superfície do campo oferecido e escolher os seus onze jogadores. Os dois capitães opostos em seguida, atiram uma moeda. O capitão que vencer o sorteio pode optar por começar rebatendo ou arremessando.
Cada enttrada é dividida em overs. Cada over consistem em seis arremessos (veja "Extras" para detalhes) legais arremessados pelo mesmo arremessador. Um arremessador não pode arremessar em dois overs consecutivos. Depois de completar um over,o arremessador vai para uma posição de fielder, enquanto outro jogador assume a posição de arremessador.
Depois de cada over, as extremidades do pitch de onde se arremessa e de onde se rebate são invertidas, e as posições dos fielders são ajustadas. Os árbitros também invertem suas posições, então o árbitro na extremidade do arremessador move-se para o lado do rebatedor, e o árbitro na extremidade do rebatedor move-se para a nova extremidade do arremessador.
Uma entrada se encerra quando:
Tipicamente, partidas de duas entradas são jogadas em três ou cinco dias com ao menos seis horas sendo jogadas por dia. Partidas de uma entrada são geralmete jogadas em um dia durante seis horas ou mais. Há intervalos formais em cada dia para almoço e chá, e pausas curtas para idratação, se necessário. Há também um intervalo curto entre as entradas.
O jogo somente pode ser jogado em tempo sem chuva. Além disso, como no críquete profissional é comum bolas serem arremessadas acima de 144 km/h, o jogo precisa ser jogado à luz do dia boa o suficiente para que um rebatedor possa enxergar a bola. O jogo é, portanto, interrompido durante a chuva (mas não na garoa, geralmente) e quando a iluminação é ruim. Alguns jogos de um dia já são jogados sob luz artificial, mas, exceto por algumas partidas experimentais na Austrália, iluminação artificial não é utilizada em jogos mais longos. Críquete profissional é geralmente jogado ao ar livre. Essas exigências fazem com que na Inglaterra, Austrália, Nova Zelândia, África do Sul e Zimbábue, o jogo seja geralmente jogado no verão. Nas Índias Ocidentais, Índia, Paquistão Sri Lanka e Bangladesh, os jogos são realizados no inverno. Nesses países as temporadas de furacões e ciclones coincidem com o verão.
O rebatedor posiciona-se esperando pela bola detro do crease na sua extremidade do pitch. O taco de madeira que o rebatedor usa é formado por um longo cabo e uma superfície plana de um lado. Se o rebatedor atinge a bola com o taco, é dito que ele realizou um shot. Se a bola resvala no lado do taco é dito que ocorreu um edge ou snick. Os shots são nomeados de acordo com o estilo do movimento do rebatedor e a direção do campo para a qual o rebatedor direciona a bola. Dependendo da estratégia do time, ele pode preferir rebater defensivamente, num esforço para não ser eliminado, ou rebater agressivamente para marcar corridas mais rapidamente.
Os rebatedores vão ao bastão em uma ordem de rebatida, que é decidida pelo capitão do time. As primeiras duas posições, conhecidos como "openers", são gerlamente especialistas na posição, já que enfrentam os arremessos mais perigosos (os arremessadores mais rápidos do time adversário estão ainda descansados e a bola está nova). Depois disso, o time tipicamente rebate numa ordem descendente de habilidade com o bastão, os primeiros cinco ou seis rebatedores geralmente são os melhores do time. deois deles os jogadores menos especializados se seguem, e finalmente os rebatedores especialistas(que geralmente não são conhecidos por suas habilidades de rebatida). Esta ordem pode ser mudada a qualquer momento durante o curso da partida por razões de estratégia.
Para marcar uma corrida, um rebatedor deve atingir a bola e correr para a extremidade oposta do pitch, enquanto seu companheiro que não está rebatendo percorre o pitch no sentido contrário. Ambos os corredores devem tocar o chão além do popping crease, seja com seu bastão, seja com seu corpo, para registrar uma corrida. Se o rebatedor atinge a bola bem o suficiente, elel pode dar meia volta para tentar marcar duas ou mais corridas. Isso é conhecido como correr entre os wickets. Não há uma regra que obrigue a correr após a rebatida, de modo que o rebatedor não é obrigado a tentar marcar uma corrida quando a bola é atingida. Se os rebatedores marcarem um número ímpar de corridas, terão invertido as extremidades do pitch em que se posicionam, de modo que suas funções como rebatedor e não-rebatedor serão invertidas para a próxima bola, a menos que a última bola tenha sido a última de um over.
O sistema de pontuação do críquete envolve dois fatores, sendo eles o número de corridas ganhas (runs) e o número de wickets perdidos (ou simplesmente wickets). Essa é a pontuação (score) mostrada durante as partidas deste esporte.[16]
O número de runs é a numeração mais importante, uma vez que o vencedor de uma partida é decidido por tal. O total de wickets mostra quantos destes o time arremessador deve pegar para acabar com as entradas do time rebatedor.
A pontuação de um time em uma partida de críquete, resumidamente, obedece ao preposto do "número de corridas ganhas para o número de wickets perdidos". Por exemplo:[16]
Em partidas com número limitado de overs (ODI, List A, T20), o time que marcar mais corridas (runs) dentro do número predeterminado de overs será o vencedor.
O críquete surgiu no Brasil via negócios com a Inglaterra, especialmente na construção de ferrovias. Os ingleses que chegaram ao país sul-americano não trouxeram apenas seus costumes e cultura, pois seus esportes em excelência acabaram aparecendo com força (críquete, futebol e rugby). Alguns historiadores, contudo, afirmam que o críquete chegou em solo brasileiro no ano de 1872 (quando foi fundado o Rio Cricket e Associação Atlética).[17]
Além do Rio Cricket, foram fundados outros clubes no Brasil. Estão entre eles o São Paulo Athletic Club (do Estado de São Paulo em 1888), o Clube Internacional de Cricket e o Club de Cricket Victória (hoje chamado de Esporte Clube Vitória), sendo estes dois do Estado da Bahia, em 1899.[18]
O críquete não chegou a ser popularizado e os antigos clubes abandonaram a prática do esporte porque o futebol (também trazido pelos ingleses) acabou ganhando maior repercussão, entre o final do século XIX e o início do século XX. Desta forma não houve uma entidade que, de maneira apropriada, zelasse por este desporto em solo brasileiro.[18]
Em 2001 o críquete ganhou novo vigor no Brasil, com a fundação da Associação Brasileira de Críquete, na cidade de Brasília, no Distrito Federal. A entidade contabiliza ainda poucos praticantes e a maioria dos jogadores são estrangeiros naturalizados (ou ainda em trânsito para a naturalização). No ano de 2002 adentrou à ICC (em inglês: International Cricket Council) e, em 2003, passou a estar presente na categoria de membro afiliado.[17][18] Atualmente, o país se faz presente como membro associado no Conselho internacional de Críquete.[19]
Mesmo ganhando adeptos, este desporto ainda é pouco divulgado na televisão brasileira, sendo que a maior parte das informações sobre o mesmo é repassada via sites que trabalham com a área. Mesmo com o aumento da chamada transmissões de jogos via internet, o interesse da população em geral ainda precisa ser melhor trabalhado, visando a sua expansão.[20]
O Conselho Internacional de Críquete (em inglês: International Cricket Council - ICC) é a instituição global responsável pela organização do esporte. Sua sede está localizada em Dubai, nos Emirados Árabes Unidos, e inclui representantes das doze nações test-playing members, assim como uma parcela representante dos membros não permanentes.[19]
Cada país tem uma organização interna que regula o esporte, tal como ocorre com as mais variadas associações desportivas nacionais. As nações são separadas em dois tipos de associação, dependendo do nível de estrutura existente para o críquete e de fatores com caráter histórico. No primeiro escalão estão os membros permanentes (oficialmente conhecidos como full members, sendo também chamados de membros fundadores ou test-playing members). Os membros associados (denominados como associate members) formam a segunda base dentro da ICC.[19]
Os países que fazem parte da ICC são divididos em dois grupos diferentes, chamados de full members e associate members. Anteriormente existia um terceiro grupo, denominado affiliate members (do qual o Brasil fazia parte)[21] e que, na Conferência Anual da ICC de 2017 realizada em Londres, acabou sendo extinto (e todas as nações nele inseridas passaram a ser do grupo associate members).[19]
Todos os full members possuíam, até 2015, vagas asseguradas na Copa do Mundo de críquete (fato este que mudou para os sete melhores qualificados no ICC One Day International rankings),[22] que chegou a ser disputada por um total de catorze seleções nas edições de 2011 e 2015, mas que voltou a ter dez participantes para a edição de 2019. Atualmente, duas vagas ficam destinadas para serem preenchidas via eliminatórias mundiais.[23][24]
Para a Copa do Mundo de Críquete de 2023 (que terá a Índia como sede),[25] será mantido o total de dez nações participantes. A distribuição das vagas será entre o país sede, seguido dos sete melhores qualificados do novo evento mundial (sendo este a ICC ODI League Tournament 2020-22, cujos participantes serão os full members da entidade), além das duas equipes melhor posicionadas na Qualificatória para a Copa do Mundo de Críquete, a ser disputada em 2022 (em inglês: 2022 Cricket World Cup Qualifier).[26][27]
A ICC tem investido no chamado críquete on line. Prova disso é a parceria feita entre o conselho e o Youtube para transmitir jogos da Indian Premier League (a maior liga local de críquete do mundo, podendo ser comparada à NBA norte-americana ou aos principais campeonatos de futebol do mundo, por exemplo). Muito se falou em um projeto de parceria entre o ICC e o YouTube, visando as transmissões de partidas disputadas na Copa do Mundo de Críquete. De concreto, várias partidas do Campeonato Indiano de Críquete (na edição de 2010) foram transmitidas via streaming pelo YouTube.[28]
O presidente do Comitê Olímpico Internacional (COI) Jacques Rogge teria dito, em 2011, ser a favor do retorno do críquete como modalidade do maior evento multi-desportivo do mundo.[29][30] O formato adotado para disputa seria o T20 (cujas partidas possuem vinte overs de duração). Historicamente, o críquete fez parte do programa oficial em apenas uma oportunidade, nos Jogos de Paris em 1900.[31][32]
Foi descartada a entrada do críquete para os Jogos do Rio de Janeiro em 2016 (que contou com o retorno do rugby no formato de seven-a-side e do golf). Para as Jogos de Tóquio em 2020, o críquete ainda precisaria entrar na lista de esportes candidatos ao programa desportivo olímpico (sendo este o caso de seis esportes incluídos no programa olímpico em solo japonês), fato este que não acabou ocorrendo.[33]
Conversações foram estabelecidas sobre o críquete voltar ao programa olímpico, visando os Jogos de Paris em 2024. Mesmo com as dificuldades para que o fato possa se concretizar, a MCC World Cricket (nas palavras de um dos seus membros e levando adiante o ideal proposto pela Board of Control for Cricket in India), vê uma possibilidade real no retorno deste esporte às Olimpíadas, nos Jogos de Los Angeles em 2028.[34] Entretanto, existe a postura contrária sobre o críquete se fazer presente no movimento olímpico.[33]
    
        
Grécia Antiga (em grego:  Ἑλλάς; Hellás), ou Hélade[1], foi uma civilização pertencente a um período da história grega que abrange desde o Período Homérico dos séculos XIV a IX a.C. até o fim da antiguidade (c.476 d.C.). Imediatamente após este período foi o início da Idade Média e da era bizantina.[2]
Aproximadamente três séculos após o Colapso da Idade do Bronze da Grécia micênica, as pólis urbanas gregas começaram a se formar no século VIII a.C., dando início ao Período Arcaico e à colonização da Bacia do Mediterrâneo. Isto foi seguido pelo período da Grécia Clássica, uma era que começou com as Guerras Greco-Persas, que durou do século V ao século IV a.C.. Devido às conquistas de Alexandre, o Grande da Macedônia, o Período Helenístico floresceu da Ásia Central até o extremo oeste do Mar Mediterrâneo. Esta era chegou ao fim com as conquistas e anexações do mundo mediterrâneo oriental pela República Romana, que estabeleceu a província romana da Macedônia na Grécia romana e mais tarde a província de Acaia, durante o Império Romano.
A cultura grega clássica, especialmente a filosofia, teve uma influência poderosa na Roma Antiga, que carregou uma versão dela para muitas partes da Bacia do Mediterrâneo e da Europa. Por essa razão, a Grécia Clássica é geralmente considerada a cultura seminal que forneceu a base da cultura ocidental moderna e é considerada o berço da civilização ocidental.[3][4][5]
Os gregos clássicos davam grande importância ao conhecimento. Ciência e religião não eram separadas e aproximar-se da verdade significava aproximar-se dos deuses. Nesse contexto, eles entendiam a importância da matemática como um instrumento para obter um conhecimento mais confiável ("divino").[6] A cultura grega, em poucos séculos e com uma população limitada, conseguiu explorar e progredir em muitos campos da ciência, matemática, filosofia e conhecimento em geral, o que deixou um legado duradouro.
Os gregos originaram-se de povos que migraram para a península Balcânica em diversas ondas, no início do milênio II a.C.: aqueus, jônicos, eólios e dóricos (ou dórios).[8] As populações invasoras são em geral conhecidas como "helênicas", pois sua organização de clãs fundamentava-se na crença de que descendiam do herói Heleno, filho de Deucalião e Pirra.[13]
A Civilização Minoica surgiu durante a Idade do Bronze Grega em Creta e floresceu aproximadamente do século XXX ao XV a.C..[14] O termo "minoico" foi cunhado por Arthur Evans, seu redescobridor, e deriva do nome do rei mítico "Minos".[15] Evans e Nicolaos Platon, importantes arqueólogos minoicos, desenvolveram dois tipos de periodização para a civilização. Evans baseou-se nos estilos de cerâmica produzidos criando assim três períodos, o Minoano Antigo, o Médio e o Recente. Platon baseou-se no desenvolvimento dos palácios minoicos o que gerou os períodos pré-palaciano, protopalaciano, neopalaciano e pós-palaciano.
Como resultado do comércio com o Egeu e Mediterrâneo, no Minoano Antigo, os minoicos viveram uma transição de uma economia agrícola para adentrar noutras economias, o resultado do comércio marítimo com outras regiões do Egeu e Mediterrâneo Ocidental.[16] Com a utilização de metais, houve o aumento das transações com os países produtores: os minoicos procuravam cobre do Chipre, ouro do Egito, prata e obsidiana das Cíclades.[17] Os portos estavam crescendo tornando-se grandes centros sob influência do aumento das atividades comerciais com a Ásia Menor, sendo que a parte oriental da ilha mostra a preponderância do período.[18] Centros na parte oriental (Vasilicí e Mália) começam a notabilizar-se e sua influência irradia-se ao longo da ilha dando origem a novos centros; aldeias e pequenas cidades tornaram-se abundantes e as fazendas isoladas são raras.[19]
No final do milênio III a.C., várias localidades na ilha desenvolveram-se em centros de comércio e trabalho manual, devido à introdução do torno na cerâmica e na metalurgia de bronze, a qual se acrescenta um aumento da população (densamente povoada), especialmente no centro-oeste.[16] Além disso, o estanho da Península Ibérica e Gália, assim como o comércio com a Sicília e mar Adriático começaram a frear o comércio oriental. No âmbito da agricultura , é conhecido através das escavações que quase todas as espécies conhecidas de cereais e leguminosas são cultivados e todos os produtos agrícolas ainda hoje conhecidos como o vinho e uvas,[20] óleo e azeitonas, já ocorriam nessa época.[21] O uso da tração animal na agricultura é introduzida.[22]
Em torno de 2 000 a.C., foram construídos os primeiros palácios minoicos (Cnossos, Mália e Festo),[16] sendo estes a principal mudança do minoano médio.[23] Como resultado, houve a centralização do poder em alguns centros, o que impulsionou o desenvolvimento econômico e social. Estes centros foram erigidos nas planícies mais férteis da ilha, permitindo que seus proprietários acumulassem riquezas, especialmente agrícolas, como evidenciados pelos grandes armazéns para produtos agrícolas encontrados nos palácios. O sistema social era provavelmente teocrático, sendo o rei de cada palácio o chefe supremo, oficial e religioso.[24] A realização de trabalhos importantes são indícios de que os minoicos tinham uma divisão bem sucedida do trabalho, e tinham uma grande quantidade deles. Um sistema burocrático e a necessidade de melhor controle de entrada e saída de mercadorias, além de uma possível economia baseada em um sistema escravista, formaram as bases sólidas para esta civilização.[25]
Com o tempo, o poder dos centros orientais começa a eclipsar, sendo estes substituídos pelo ascendente poderio dos centros interioranos e ocidentais. Isto ocorreu, principalmente, por distúrbios políticos na Ásia (invasão cassita na Babilônia, expansão hitita e invasão hicsa no Egito) que enfraqueceram o mercado oriental, motivando um maior contato com a Grécia continental e as Cíclades.[26]
No final do período MMII (1 750 - 1 700 a.C.), houve uma grande perturbação em Creta, provavelmente um terremoto,[27] ou possivelmente uma invasão da Anatólia.[28] A teoria do terremoto é sustentada pela descoberta do Anemospília pelo arqueólogo Sakelarakis, no qual foram encontrados os corpos de três pessoas (uma delas vítima de um sacrifício humano) que foram surpreendidas pelo desabamento do templo.[29] Outra teoria é que havia um conflito dentro de Creta, e Cnossos saiu vitorioso.[30] Os palácios de Cnossos, Festo, Mália e Cato Zacro foram destruídos.[31] Mas, com o início do período neopalaciano, a população voltou a crescer,[32][33] os palácios foram reconstruídos em larga escala[34] (no entanto, menores que os anteriores[35]) e novos assentamentos foram construídos por toda a ilha, especialmente grandes propriedades rurais.[36]
Este período (séculos XVII e XVI a.C., MM III/neopalaciano) representa o apogeu da Civilização Minoica.[30] Os centros administrativos controlavam extensos territórios, fruto da melhoria e desenvolvimento das comunicações terrestres e marítimas, mediante a construção de estradas e portos, e de navios mercantes que navegavam com produções artísticas e agrícolas, que eram trocadas por matérias-primas.[24] Entre 1 700 e 1 450 a.C., a monarquia de Cnossos deteve a supremacia da ilha. Essa monarquia, apoiada, pela elite mercantil surgida em decorrência do intenso comércio, criou um império comercial marítimo, a talassocracia.[37][38] Após cerca de 1 700 a.C., a cultura material do continente grego alcançou um novo nível, devido a influência minoica.[39] Importações de cerâmicas do Egito, Síria, Biblos e Ugarit demonstram ligações entre Creta e esses países.[40] Os hieróglifos egípcios serviram de modelo para a escrita pictográfica minoica, a partir da qual os famosos sistemas de escrita Linear A e B mais tarde desenvolveram-se[41]
A erupção do vulcão Tera (atual Santorini) foi implacável para o rumo de Creta.[42] A erupção foi datada como tendo ocorrido entre 1 639 e 1 616 a.C., por meio de datação por radiocarbono;[43] em 1 628 a.C. por dendrocronologia;[44] e entre 1 530 - 1 500 a.C. pela arqueologia.[45] O leste da ilha foi alcançado por nuvens e chuva de cinzas que possivelmente alastraram gases nocivos que intoxicaram muitos seres vivos, além de terem causado mudanças climáticas e tsunamis, o que possivelmente fez com que Creta se tornasse polo de refugiados provenientes das Cíclades, o que minou, juntamente com os cataclismos naturais prévios, a estabilidade da ilha.[46] Além disso, a destruição do assentamento minoico em Tera (conhecido como Acrotíri) poderia ter impactado, mesmo que indiretamente, o comércio minoico com o norte.[47] Em torno de 1 550 a.C., um novo abalo sísmico consecutivo às catástrofes do Santorini, destruiu outra vez os palácios minoicos, no entanto, estes foram novamente reconstruídos e foram feitos ainda maiores do que os anteriores.[39][34]
Em torno de 1 450 a.C., a Civilização Minoica experimentou uma reviravolta, devido a uma catástrofe natural, possivelmente um terremoto. Outra erupção do vulcão Tera tem sido associada a esta queda, mas a datação e implicações permanecem controversas. Vários palácios importantes em locais como Mália, Tílissos, Festo, Hagia Triada bem como os alojamentos de Cnossos foram destruídos. Durante o MRIIIB a ilha foi invadida pelos aqueus da Civilização Micênica.[34] Os sítios dos palácios minoicos foram ocupados pelos micênicos em torno de 1 420 a.C.[48] (1 375 a.C. de acordo com outras fontes[39]).
Os minoicos viriam a influenciar a história da Grécia através dos micênicos, que adoptam aspectos da cultura minoica. O nome "micênico" foi criado por Heinrich Schliemann com base nos estudos que fez no sítio de Micenas, no nordeste do Peloponeso, onde outrora se erguia um grande palácio e uma das principais cidades além de Tirinto, Tebas e Esparta. Julga-se que os micênicos se chamariam a si próprios aqueus. De acordo com vários historiadores, os micênicos eram chamados de Ahhiyawa pelos hititas.[49] A sua civilização floresceu entre 1 600 e 1 200 a.C..
Os micênicos já falavam grego. Não tinham uma unidade política, existindo vários reinos micénicos. À semelhança dos minoicos, o centro político encontrava-se no palácio, cujas paredes também estavam decoradas com afrescos.[carece de fontes?]
Para além de praticarem o comércio, os micênicos eram amantes da guerra e da caça. Por volta de 1 400 a.C. os micênicos teriam ocupado Cnossos, centro da cultura minoica.
Por volta de 1 250 a.C. o mundo micénico entra em declínio, o que estaria relacionado com a decadência do reino hitita no Oriente Próximo,[carece de fontes?] que teria provocado a queda das rotas comerciais. Sua decadência envolveu também guerras internas.[carece de fontes?] É provável que a destruição da cidade de Troia, facto que se teria verificado entre 1 230 e 1 180 a.C., possa estar relacionado com o relato literário de Homero na Ilíada, escrita séculos depois.
Dá-se o nome de período homérico ao período que se seguiu ao fim da Civilização Micênica e que se situa entre 1 100 e 800 a.C.. Durante este período perdeu-se o conhecimento da escrita, que só seria readquirido no século VIII a.C.. Os objectos de luxo produzidos durante a era micénica não são mais fabricados neste período. A designação atribuída ao período encontra-se relacionada não apenas com a decadência civilizacional, mas também com as escassas fontes para o conhecimento da época.
Outro dos fenómenos que se verificou durante este período foi o da diminuição populacional, não sendo conhecidas as razões exactas que o possam explicar. Para além disso, as populações também se movimentam, abandonando antigos povoados para se fixarem em locais que ofereciam melhores condições de segurança.
O Período Arcaico tem como balizas temporais tradicionais a data de 776 a.C., ano da realização dos primeiros Jogos Olímpicos, e 480 a.C., data da Batalha de Salamina. A Grécia era ainda dividida em pequenas províncias com autonomia, em razão das condições topográficas da região: cada planície, vale ou ilha é isolada de outra por cadeias de montanhas ou pelo oceano.
A origem das cidades gregas remonta à própria organização dos invasores, especialmente dos aqueus, que se agrupavam nos chamados ghené (ghenos, no singular). Os ghené eram essencialmente comunidades tribais que cultuavam seus deuses na acrópole (local elevado). A vida econômica dessas grandes famílias era, a princípio, baseada em laços de parentesco e cooperação social. A terra, a colheita e o rebanho pertenciam à comunidade. Havia uma liderança política na figura do pater, um membro mais velho e respeitado. Diversos ghené agrupavam-se em fratarias, e diversas fratarias em tribos.
Com a recuperação econômica após o interlúdio dórico, a população grega cresceu além da capacidade de produção das terras cultiváveis.[carece de fontes?] Diante desse desequilíbrio, e procurando garantir melhores condições de vida, alguns grupos teriam se destacado, passando a manejar armas e a ter domínio sobre as melhores terras e rebanhos. Esses grupos acumularam riqueza, poder e propriedade como resultado da divisão desigual das terras do ghené, considerando-se os melhores — aristos (aristoi), em grego. Assim, foram diferenciando-se da maioria da população e dissolvendo a vida comunitária do ghené. Essas transformações sociais estavam na origem da formação da pólis, a cidade grega. A partir de 750 a.C. os gregos iniciaram um longo processo de expansão, firmando colônias em várias regiões, como Sicília e sul da Itália (a chamada Magna Grécia), no sul da França, na costa da Península Ibérica, no norte de África e nas costas do mar Negro. Entre os séculos VIII e VI a.C. fundaram aí novas cidades, as colônias, as quais chamavam de apoíkias—; palavra que pode ser traduzida por nova casa.
São muitas as causas apontadas pelos historiadores para explicar essa expansão colonizadora grega. Grande parte dessas causas relaciona-se a questões sociais originadas por problemas de posse de terra e dificuldades na agricultura. As melhores terras eram dominadas por famílias ricas (os aristos, também conhecidos por eupátridas - bem nascidos). A maioria dos camponeses (georgoi) cultivava solos pobres cuja produção de alimentos era insuficiente para atender às necessidades de uma população em crescimento. Uma terceira classe, que não possuía terras, dedicar-se-ia, mais tarde, ao comércio; eram chamados de tetas (thetas), marginais. Para fugir à miséria, muitos gregos migravam em busca de terras para plantar e de melhores condições de vida, fundando novas cidades. Assim, no primeiro momento, a principal atividade econômica das colônias gregas foi a agricultura. Posteriormente, muitas colônias transformaram-se em centros comerciais, dispondo de portos estratégicos para as rotas de navegação.[carece de fontes?]
A Hélade começa a dominar linguística e culturalmente uma área maior do que o limite geográfico da Grécia. As colônias não eram controladas politicamente pelas cidades que as fundavam, apesar de manterem vínculos religiosos e comerciais com aquelas. Predominava entre os gregos sempre a organização de comunidades independentes, e a cidade (cada uma desenvolveu seu próprio sistema de governo, leis, calendário e moeda) tornou-se a unidade básica do governo grego.[carece de fontes?]
Socialmente, a colonização do mar Mediterrâneo pelos gregos resultou no desenvolvimento de uma classe rica formada por mercadores (o comércio internacional desenvolvera-se a partir de então) e de uma grande classe média de trabalhadores assalariados, artesãos e armadores. Culturalmente, os gregos realizaram intercâmbios com outros povos. Na economia, a indústria naval se desenvolveu, obviamente, passando a consumir crescente quantidade de madeira das florestas gregas.[carece de fontes?]
As evidências arquelógicas indicam que o padrão de vida na Grécia melhorou acentuadamente (o tamanho médio das área do primeiro andar de residências encontradas por arqueólogos aumentou cinco vezes, de 55 metros quadrados para 230 metros quadrados); a expectativa de vida e a estatura média também mostram evidências de melhoria.[50] A população aumentou de 600 mil no século VIII a.C. para em torno de 9 milhões, no IV a.C..[51] E tudo isso fez com que no século IV, a Grécia já possuísse a economia mais avançada do mundo e com um nível de desenvolvimento extremamente raro para uma economia pré-industrial, estando em vantagem em alguns pontos se comparada às economias mais avançadas antes da Revolução Industrial, aos países baixos do século XVII e à Inglaterra do século XVIII.[52] Apesar disso, houve concentração fundiária, em algumas cidades essa concentração levou a revoltas e tiranias, em outras a aristocracia manteve o controle graças a legisladores inclementes. Outras cidades permaneceram relativamente igualitárias na distribuição das terras, em Atenas é estimado que entre 7,5-9% dos cidadãos, o grupo abastado, fossem proprietários de 30-35% de todas as terras, e 20% dos cidadãos tinham pouca ou nenhuma terra e os restantes 70-75% dos cidadãos eram proprietários de 60-65% das terras.[52]
O Período Clássico estende-se entre 500 e 338 a.C. e é dominado por Esparta e Atenas. Cada um destas poleis desenvolveu o seu modelo político (a oligarquia militarista em Esparta e a democracia aristocrata em Atenas). Ao nível externo verifica-se a ascensão do Império Aquemênida quando Ciro II conquista o reino dos medos. O Império Aquemênida prossegue uma política expansionista e conquista as cidades gregas da costa da Ásia Menor. Atenas e Erétria apoiam a revolta das cidades gregas contra o domínio persa, mas este apoio revela-se insuficiente já que os jónios são derrotados: Mileto é tomada e arrasada e muitos jónios decidem fugir para as colónias do Ocidente. O comportamento de Atenas iria gerar uma reacção persa e esteve na origem das Guerras Médicas (490-479 a.C.). Perante a invasão persa, os gregos decidem esquecer as diferenças entre si e estabelecem uma aliança composta por 31 cidades, entre as quais Atenas e Esparta, tendo sido atribuída a esta última o comando das operações militares por terra e pelo mar. As forças espartanas lideradas pelo rei Leónidas I conseguem temporariamente bloquear os persas na Batalha das Termópilas, mas tal não impede a invasão da Ática. O general Temístocles tinha optado por evacuar a população da Ática para Salamina e sob a direcção desta figura Atenas consegue uma vitória sobre os persas em Salamina. Em 479 a.C. os gregos confirmam a sua vitória desta feita na Batalha de Plateias. A frota persa foge para o mar Egeu, onde em 479 a.C. é vencida em Mícale.[53][54]
Com o fim das Guerras Médicas, e em resultado da sua participação decisiva no conflito, Atenas torna-se uma cidade poderosa, que passa a intervir nos assuntos do mundo grego. Esparta e Atenas distanciam-se e entram em rivalidade, encabeçando cada um delas uma aliança política e militar: no caso de Esparta era a Liga do Peloponeso e no caso de Atenas a Liga de Delos. Esta última foi fundada em 477 a.C. e era composta essencialmente por estados marítimos que encontravam-se próximos do mar Egeu, que temiam uma nova investida persa. O centro administrativo da liga era a ilha de Delos. As relações entre as duas poleis atingem o grau de saturação em 431 a.C., ano em que se inicia a guerra. Esparta lança um ultimato a Atenas: deve levantar as sanções a Mégara e suspender o bloqueio a Potideia. Péricles consegue convencer a assembleia a rejeitar o ultimato e a guerra começa. O conflito continuou até 422 a.C. ano em que Atenas é derrotada em Anfípolis. Na batalha morrem o general espartano Brásidas e o ateniense Cléon, ficando o ateniense Nícias em condições de estabelecer a paz (Paz de Nícias, 421 a.C.). Apesar do suposto cessar das hostilidades, entre 421 e 414 as duas poleis continuam a combater, não diretamente entre si, mas através do seus aliados, como demonstra a ajuda secreta dada a Argos por Atenas. Em 415 a.C. Alcibíades convenceu a Assembleia de Atenas a lançar um ataque contra Siracusa, uma aliada de Esparta, em expedição que se revelou um fracasso.[55] 
Esse foi um tempo em que o mundo grego prosperou, com o fortalecimento das cidades-estados e a produção de obras que marcariam profundamente a cultura e a mentalidade ocidental, mas foi também o período em que o mundo grego viu-se envolvido em longas e prolongadas guerras.[carece de fontes?]
O período helenístico refere-se ao período da história da Grécia e de parte do Oriente Médio compreendido entre a morte de Alexandre o Grande em 323 a.C. e a anexação da península grega e ilhas por Roma em 146 a.C. Caracterizou-se pela difusão da civilização grega numa vasta área que se estendia do mar Mediterrâneo oriental à Ásia Central. De modo geral, o helenismo foi a concretização de um ideal de Alexandre: o de levar e difundir a cultura grega aos territórios que conquistava. Foi neste período que as ciências particulares tiveram seu primeiro e grande desenvolvimento. Foi o tempo de Euclides e Arquimedes. O helenismo marcou um período de transição para o domínio e apogeu de Roma.[56]Durante o período helenista, foram fundadas várias cidades de cultura grega, entre elas Alexandria e Antioquia, capitais do Egito ptolemaico e do Império Selêucida, respectivamente.[56]
Situada na porção sul da península Balcânica, o território da Grécia continental caracteriza-se pelo seu relevo montanhoso; 80% da Grécia é formada por montanhas.[57] A cordilheira dominante é a dos montes Pindo que separa a costa oriental, banhada pelo mar Egeu, da costa ocidental, banhada pelos mares Jônico e Adriático.
A Grécia Setentrional dividia-se em Tessália, Acarnânia e Epiro; a Grécia Central em Fócida, Etólia, Beócia (região de Tebas) e Ática (logradouro de Atenas); o Peloponeso (Grécia peninsular) em Acaia (região de Olímpia), Arcádia (logradouro de Argos, Tirinto e Micenas), Messênia (terra de Pilos) e Lacônia (região de Esparta). Entre a Grécia peninsular e continental há o istmo de Corinto (terra de Corinto e Mégara), estreita faixa litorânea, e o golfo de Corinto.[8]
No mar Egeu encontram-se várias ilhas que formam diversos arquipélagos: Espórades, Cíclades, Dodecaneso ("doze ilhas"), Jônicas e a ilha de Creta; todo o litoral egeu da Anatólia e o sul da Itália (Magna Grécia) também fazem parte da geografia grega.[8]
Durante o período arcaico, a população da Grécia cresceu além da capacidade da sua já limitada terra cultivável (de acordo com uma estimativa, a população da Grécia Antiga aumentou por um fator maior do que 10 durante o período de 800 a 400 a.C., a partir de um aumento populacional de 800 mil habitantes para uma população total estimada em 10 a 13 milhões de pessoas).[58]
Por volta de 750 a.C. os gregos começaram um período de expansão territorial que durou cerca de 250 anos, estabelecendo colônias em todas as direções. A leste, na costa do mar Egeu, a Ásia Menor foi colonizada primeiro, seguida por Chipre e as costas da Trácia, o mar de Mármara e costa sul do mar Negro.
A colonização grega também alcançou áreas mais distantes, como regiões das atuais Ucrânia e Rússia (Taganrog). A oeste, foram colonizadas as costas da Ilíria, Sicília e do sul da Itália, seguido pelo sul da França, Córsega e até mesmo o nordeste da Espanha. Também foram fundadas colônias gregas no Egito e na atual Líbia.
As atuais cidades de Siracusa, Nápoles, Marselha e Istambul tiveram seu início como as colônias gregas de Siracusa (Συρακούσαι), Neápolis (Νεάπολις), Massália (Μασσαλία) e Bizâncio (Βυζάντιον). Estas colônias desempenharam um papel importante na disseminação da influência grega em toda a Europa e ajudaram a criar longas redes comerciais entre as cidades-estados gregas, impulsionando a economia da Grécia antiga.
São muitas as diferenças entre a Grécia moderna e a Grécia Antiga. O mundo grego antigo estendia-se por uma área muito maior do que o território grego atual. Além disso, há outra diferença básica. Hoje, a Grécia constitui um Estado, cujo nome oficial é "República Helênica". Já a Grécia Antiga nunca foi um Estado unificado com governo único. Era um conjunto de cidades-estados independentes entre si, com características próprias embora a maioria delas tivesse seus sistemas econômicos parecidos, excluindo-se de Esparta.
Desde o século VIII a.C., formaram-se pela Grécia Antiga diversas cidades independentes. Em razão disso, cada uma delas desenvolveu seu próprio sistema de governo, suas leis, seu calendário, sua moeda. Cada uma dessas cidades era chamada de pólis, palavra grega que costuma ser traduzida por cidade-Estado. O conjunto de diversas pólis era chamado de poleis.
De modo geral, a pólis reunia um agrupamento humano que habitava um território cuja extensão geralmente variava entre algumas centenas de quilômetros quadrados e 10 mil km².[51] Compreendia uma área urbana e outra rural. Atenas, por exemplo, tinha 2 500 km², Siracusa tinha 5 500 km² e Esparta se estendia por 7 500 km².[51] A área urbana frequentemente se estabelecia em torno de uma colina fortificada denominada acrópole (do grego akrós, alta e pólis, cidade). Nessa área concentrava-se o centro comercial e a manufatura. Ali, muitos artesãos e operários produziam tecidos, roupas, sandálias, armas, ferramentas, artigos em cerâmica e vidro.
Na área rural a população dedicava-se às atividades agropastoris: cultivo de oliveiras, videiras, trigo, cevada e criação de rebanhos de cabras, ovelhas, porcos e cavalos. Este agrupamento visava atingir e manter uma completa autonomia política e social para com as outras poleis gregas, embora existisse muito comércio e divisão de trabalho entre as cidade gregas. É estimado que Atenas importava 2/3 a 3/4[50] de seus alimentos e exportava azeite, chumbo, prata, bronze, cerâmica e vinho. No mundo grego encontramos muitas poleis, dentre as mais famosas, temos Messênia, Tebas, Mégara e Erétria. É estimado que seu número tenha chegado a mais de mil no século IV a.C..[51]
A maioria das poleis gregas eram pequenas, com populações de aproximadamente 20 mil habitantes[51] ou menos na sua área urbana. Contudo, as principais cidades eram bem maiores, no século IV a.C., estando entre elas Atenas, com estimados 170 mil habitantes, Siracusa com aproximadamente 125 mil habitantes e Esparta com apenas 40 mil habitantes.
Atenas era a maior e mais rica cidade da Grécia Antiga durante os séculos V e IV a.C.. Existem relatos da época que reportam um volume comercial externo (soma das importações e exportações das cidades do império ateniense) da ordem de 180 milhões de dracmas áticos, valor duas ou três vezes superior ao orçamento do Império Aquemênida na mesma época.[59]
A monarquia é uma forma de governo em que o poder  está nas mãos de uma única pessoa. A maioria das monarquias foram governadas por reis, geralmente com a ajuda de um conselho de assessores. A palavra monarquia vem dos termos gregos monos (que significa "um") e arkhein (que significa "regra"). Os Micénicos, quem governou a Grécia antiga de 2 000 a 1 100 a.C., eram povos guerreiros que estabeleceram monarquias para governar seus reinos. O rei de cada cidade-estado vivia em um palácio-fortaleza luxuoso na cidade capital. Além da capital havia uma rede de aldeias periféricas. As pessoas dessas aldeias pagavam impostos ao rei, obedeciam as suas leis, e dependiam dele para a sua defesa. O rei muitas vezes contou com soldados fortemente armados para impor seu domínio e garantir que as pessoas pagassem impostos e obedecessem suas leis. Ele geralmente mantinha o seu poder político para a vida toda. Seu filho mais velho, o príncipe, sucedia-lhe no trono. Quando não havia sucessor masculino direto, os mais próximos conselheiros militares do rei muitas vezes lutavam entre si para se tornar o novo monarca.
As monarquias micênicas sobreviveram até por volta do ano de 1 200 a.C.. Naquela época, muitas de suas rotas comerciais orientais começaram a fechar por causa de combates entre reinos da Ásia Menor. Como resultado, os micênicos já não podiam obter metais brutos, e sua capacidade de fabricar armas e conquistar outras terras diminuíram. Eventualmente, os micênicos começaram a lutar entre si pela sobrevivência, e eles lentamente destruíram-se uns aos outros. Finalmente, um povo do noroeste chamado dórios invadiram a Grécia e destruíram o que restava das monarquias micênicas. A Monarquia como forma de governo logo desapareceu na Grécia. Foi substituída por um sistema em que um pequeno número de indivíduos partilhavam o poder e governavam como um grupo.[60]
Uma oligarquia é uma forma de governo em que o poder de decisão está nas mãos de poucos líderes. A palavra oligarquia vem dos termos gregos oligos (que significa "poucos") e arkhein (que significa "regra"). Entre 1 100 e 800 a.C., pequenos grupos de pessoas começaram a compartilhar o poder dominante em várias cidades-estados gregas. O poder político foi muitas vezes compartilhado entre aristocratas, que herdavam a riqueza e o poder de suas famílias, e um rei. Com o tempo, esse arranjo de decisões mudou. As oligarquias se desenvolveram de forma que o poder político ficava nas mãos de poucos indivíduos, ricos e selecionados. Alguns desses membros do círculo governante eram de nascimento aristocrático, enquanto outros eram membros ricos da classe média. Como monarcas, oligarcas geralmente tinham vidas de luxo e faziam cumprir sua lei com o apoio militar. Os cidadãos de uma oligarquia desfrutavam de certas proteções, embora eles não tinham plenos direitos políticos, como votar. Portanto, a maioria dos cidadãos de uma oligarquia tinha muito pouco a dizer sobre como a cidade-estado era governada.
Com o tempo, as oligarquias começaram a desaparecer na Grécia por vários motivos.  Em Corinto, por exemplo, as pessoas viviam bem, mas a oligarquia governava duramente e os cidadãos eventualmente derrubaram-na. Em Atenas, a insatisfação com os oligarcas surgiu como o aumento da população camponesa e escassez de alimentos. O poder das oligarquias também foi enfraquecido quando indivíduos poderosos e ricos reuniram exércitos de contratados, ou mercenários, guerreiros, chamados hoplitas, e os usou para intimidar os líderes políticos. Até o ano 400 a.C., uma oligarquia estável governou apenas uma cidade-estado, Esparta.[60]
A tirania é uma forma de governo em que o poder de decisão está nas mãos de um indivíduo que tenha tomado o controle, muitas vezes por meios ilícitos. A palavra tirania vem do grego tyrannos, que significa "usurpador com poder supremo". Com o tempo, a pessoa que governava através da tirania, ou um tirano, ficou conhecida como quem se agarra ao poder por meios cruéis e abusivos.
As tiranias na Grécia surgiram pela primeira vez em meados do ano 600 a.C.. Em muitas cidades-estados, uma crescente e rica classe média de comerciantes e fabricantes ficara descontente com seus governantes. Essa classe média exigia privilégios políticos e sociais para acompanhar sua nova riqueza, mas as oligarquias dominantes recusavam-se a conceder-lhes uma palavra a dizer no governo. Várias pessoas, na sua maioria ex-líderes militares, responderam às demandas da população de classe média e prometeram fazer as mudanças que eles queriam. Apoiados pela classe média, esses indivíduos tomaram o poder dos grupos governantes. Uma vez no poder, esses líderes (ou tiranos) frequentemente reformulavam as leis, a ajudavam os pobres, cancelavam dívidas, e davam aos cidadãos que não eram nobres uma voz no governo. Como recompensa, os cidadãos presenteavam os tiranos com frequência, que por sua vez ficaram bastante ricos.
Muitos tiranos governaram por curtos períodos de tempo. Em algumas cidades-estados, os tiranos se tornaram duros e gananciosos, e foram simplesmente derrubados pelo povo. O último tirano importante a governar a Grécia continental foi Hípias da cidade-estado de Atenas. Em 510 a.C. uma combinação de invasores espartanos e atenienses, que se opunham ao seu governo severo, forçaram Hípias a demitir-se e deixar a Grécia. Uma nova forma de governo, em que todos os cidadãos compartilhavam tomadas de decisão, eventualmente o substituiu.[60]
A democracia é uma forma de governo em que o poder está nas mãos de todas as pessoas. A palavra democracia vem do termo grego demos (que significa "povo") e kratos (que significa "poder").
A democracia se desenvolveu na Grécia antiga por volta de 500 a.C. na cidade-estado de Atenas, onde muitas pessoas começaram a opor-se a regra dos tiranos. O órgão principal da democracia ateniense era a Assembleia dos cidadãos. A Assembleia foi aberta a todos os 30 mil a 40 mil cidadãos adultos do sexo masculino, mas geralmente apenas 5 mil pessoas compareciam. Tanto os cidadãos ricos e pobres participavam da Assembleia. Este órgão se reunia cerca de 40 vezes por ano para dirigir a política externa, rever as leis e aprovar ou condenar a conduta dos funcionários públicos. Membros da Assembleia chegavam a todas as suas decisões através do debate público e do voto.[60]
Um órgão executivo menor mas não menos importante , o Conselho dos 500, foi o responsável pelo dia-a-dia de funcionamento do Estado. Esse corpo, cujos membros eram escolhidos anualmente em um tipo de "loteria", propunha leis e decisões da Assembleia pela força ou decretos. O Conselho também administrou as finanças do Estado, recebeu embaixadores estrangeiros, e supervisionou a manutenção da frota de navios ateniense.
Um aspecto importante da democracia ateniense era o fato de que os seus funcionários públicos não tinham muito poder individual. Não houve tal posse como um presidente de Atenas. Em tempos de guerras, um grupo seleto de generais tomavam decisões sobre assuntos militares. Esses generais foram eleitos anualmente e poderiam ser reeleitos várias vezes. Quase todos os funcionários do governo, incluindo generais e os membros do conselho, bem como os cidadãos que serviram em júris foram pagos por seus serviços. Isso permitiu que cidadãos do sexo masculino, tanto ricos e pobres a participassem plenamente no governo ateniense.[60]
Os gregos tinham conflitos e diferenças entre si, mas muitos elementos culturais em comum. Falavam a mesma língua (apesar dos diferentes dialetos e sotaques) e tinham religião comum, que se manifestava na crença nos mesmos deuses. Em função disso, reconheciam-se como helenos (gregos) e chamavam de bárbaros os estrangeiros que não falavam sua língua e não tinham seus costumes, ou seja, os povos que não pertenciam ao mundo grego (Hélade).
A historiografia tradicional descreve na Grécia do período clássico uma rígida separação entre os ambientes públicos e privados, bem como entre os gêneros feminino e masculino.[61] As mulheres são proibidas de participar na vida política e necessitam um tutor masculino por toda a vida, o kyrios.[62]
São geralmente retratadas como pertencentes as seguintes classes:[63][64]
Linhas de pesquisas recentes, entretanto, tem questionado o entendimento androcêntrico da história grega através da procura por redefinir conceitos de participação política e de fronteiras de circulação nos ambientes público e privado, bem como pela interpretação de sinais de agência da mulher, presentes tanto na gestão cooperativa da casa como na condução de ritos religiosos e funerários.[65][66][67]
Sobre a prostituição feminina, verifica-se a necessidade de melhor compreensão acerca da vida destas mulheres, sendo emblemática, por exemplo, a existência de entendimentos divergentes acerca do conceito das hetairai, representadas ora como modalidade de prostitutas refinadas que participavam do simpósio, ora como simples pornἀi ou pallakἐ, ou até mesmo como mulher da alta sociedade grega com comportamentos desviantes dos padrões sociais estabelecidos.[68][69][70]
Um dos mais expressivos monumentos do período antigo é o Partenon, templo com colunas dóricas, construído entre 447 e 438 a.C. na acrópole de Atenas, e dedicado à padroeira da cidade, Atena Partenos. A construção foi projetada pelos arquitetos Calícrates e Ictinos, e é comandada por Fídias. Suas linhas arquitetônicas serviram de inspiração para a construção de muitos outros edifícios em todo o mundo.
Em Atenas, apesar das mulheres também serem educadas para as tarefas de mãe e esposa, a educação era tratada de outra forma, pois até mesmo nas classes mais pobres da sociedade ateniense encontravam-se homens alfabetizados. Eles eram instruídos para cuidarem não só da mente como também do corpo, o que lhes dava vantagem na hora da guerra, pois eram tão bons guerreiros quanto eram estrategas. Os meninos, quando ainda pequenos - aos sete anos de idade -, já começavam as suas aprendizagens na escola e nas suas próprias casas. O Pedagogo - um escravo especial - era escolhido para os orientar. As principais obras dos antigos poetas, como Homero e Hesíodo, eram decoradas nas suas aprendizagens, habitualmente acompanhadas de música.
O sistema educacional espartano, possivelmente chamado de agōgē,[71][72] normalmente é descrito em oposição ao dos gregos de forma geral,[73] pelas suas diferenças na estrutura e objetivos. As características mais comumente ressaltadas têm ligação com a severidade do sistema, fazendo com que alguns autores atribuissem à Esparta o epíteto de "domadora de mortais".[74] Entre essas características, Xenofonte destaca a nomeação de um supervisor (paidonomos) pelo Estado, contrastando com os paidagogoi atenienses, e o racionamento de alimentos, vestes e calçados, para que se tornassem cidadãos respeitosos e obedientes, além de, como soldados, melhor se adaptassem às condições potencialmente adversas da guerra.[75]
O principal evento de ligação cultural e religiosa da Grécia antiga eram os Jogos Olímpicos, criados partir de 776 a.C. sob a forma de um festival para reverenciar os deuses do Olimpo. De quatro em quatro anos, os gregos das mais diversas cidades reuniam-se em Olímpia para a etapa final de outros festivais realizados nas cidades de Corinto, Delfos e Argos, esta final ficou conhecida como Jogos olímpicos.
Estes jogos eram realizados em honra a Zeus (o mais importante deus grego) e incluíam provas de diversas modalidades esportivas: corridas, saltos, arremesso de disco, lutas corporais. Além do esporte havia também competições musicais e poéticas.
Os Jogos Olímpicos eram anunciados por todo o mundo grego dez meses antes de sua realização. Os gregos atribuíam tamanha importância a essas competições que chegavam a interromper guerras entre cidades (trégua sagrada) para não prejudicar a realização dos jogos. Pessoas dos lugares mais distantes iam a Olímpia a fim de assistir aos jogos. Havia, entretanto, proibição à participação das mulheres, seja como esportistas, seja como espectadoras.
Em épocas de guerra, os atletas (corredores) eram encarregados de transportar, em “alta velocidade”, mensagens importantes.[76]
Os gregos praticavam um culto politeísta antropomórfico, em que os deuses poderiam se envolver em aventuras fantásticas, tendo, também, a participação de heróis (Hércules, Teseu, Perseu, Édipo) que eram considerados divinos. Não havia dogmas e os deuses possuíam tanto virtudes quanto defeitos, o que os assemelhava aos mortais no aspecto de personalidade. Para relatar os feitos dos deuses e dos heróis, os gregos criaram uma rica mitologia.
Normalmente, as cerimônias públicas, mesmo de cunho político, eram antecedidas por práticas religiosas, o que reflete a importância da religião entre os gregos antigos. Mas essa religião foi superada pela filosofia.
Apesar da autonomia política das cidades-estados, os gregos estavam unificados em termos religiosos. Entre as divindades cultuadas estavam: Zeus (senhor dos deuses e céus), Hades (deus do Mundo Inferior e dos mortos), Deméter (deusa da agricultura), Poseidon (deus dos mares), Afrodite (deusa do amor e beleza), Apolo (deus do sol, da música, da medicina, da poesia, do tiro com arco e dos solteiros), Dioniso (deus do vinho), Atena (deusa da sabedoria e da guerra), Ártemis (deusa da caça e da lua), Hermes (deus dos ladrões, dos viajantes, dos comerciantes e dos mensageiros), Hera (protetora das mulheres, deusa do casamento e da maternidade e esposa de Zeus), Ares (deus da guerra), Hefesto (deus dos ferreiros)
Além dos grandes santuários como os de Delfos, Olímpia e Epidauro, havia os oráculos que também recebiam grandes multidões, pois lá se acreditava receber mensagens diretamente dos deuses. Um exemplo claro estava no Oráculo de Delfos, onde uma pitonisa (sacerdotisa do templo de Apolo) entrava em transe e pronunciava palavras sem nexo que eram interpretadas pelos sacerdotes, revelando o futuro dos peregrinos.
Outro fato muito interessante era a existência dos homogloditas, um pequeno povo que vivia nas áreas litorâneas do mediterrâneo, eles utilizavam a argila para a construção de estatuetas como uma oferenda aos deuses gregos, geralmente ao Dioniso, deus do vinho e das festas.
Os matemáticos da Grécia Antiga contribuíram para muitos desenvolvimentos importantes no campo da matemática, incluindo as regras básicas de geometria, a ideia da derivação formal e descobertas na teoria dos números, análise matemática, matemática aplicada, além de terem se aproximado de estabelecer o cálculo integral. As descobertas de vários matemáticos gregos, incluindo Pitágoras, Euclides e Arquimedes ainda são utilizadas no ensino de matemática atual.
Os gregos desenvolveram a astronomia, que eles tratavam como um ramo da matemática, a um nível altamente sofisticado. Os primeiros modelos tridimensionais geométricos para explicar o movimento aparente dos planetas foram desenvolvidos no século IV a.C. por Eudoxo de Cnido e Calipo de Cícico. O contemporâneo Heráclides do Ponto propôs que a Terra girava em torno do seu próprio eixo. No século III a.C. Aristarco de Samos foi o primeiro a sugerir um sistema heliocêntrico. Arquimedes em seu tratado O Contador de Areia revive a hipótese Aristarco de que "as estrelas fixas e o Sol permanecem impassíveis, enquanto a Terra gira em torno do Sol na circunferência de um círculo". Caso contrário, apenas descrições fragmentárias de ideia Aristarco iriam sobreviver.[77] Eratóstenes, usando ângulos de sombras criadas em regiões amplamente separadas, estimou a circunferência da Terra com grande precisão.[78] No século II a.C. Hiparco de Niceia fez uma número de contribuições, incluindo a primeira medição da precessão e a compilação do primeiro catálogo de estrelas em que ele propôs o moderno sistema de magnitudes aparentes.
A Máquina de Anticítera, um dispositivo para calcular os movimentos dos planetas, data de cerca de 80 a.C. e foi o primeiro ancestral do computador astronômico. O mecanismo foi descoberto em um antigo naufrágio ao largo da ilha grega de Anticítera, entre Citera e Creta. O dispositivo se tornou famoso pelo uso de uma engrenagem diferencial, que se acreditava ter sido inventada apenas no século XVI, e pela miniaturização e complexidade de seus componentes, comparáveis a de um relógio feito no século XVIII. O mecanismo original é exibido na coleção de bronze do Museu Arqueológico Nacional de Atenas, acompanhado de uma réplica.[79]
Os gregos antigos também fizeram importantes descobertas no campo da medicina. Hipócrates foi um médico do período clássico e é considerado uma das figuras mais marcantes da história da medicina. Ele é conhecido como o "pai da medicina",[80][81][82] em reconhecimento de suas contribuições duradouras para o campo, como a fundação da escola hipocrática. Esta escola intelectual revolucionou a medicina na Grécia Antiga, estabelecendo-se como uma disciplina distinta de outros campos que tinham sido tradicionalmente associados (nomeadamente teurgia e filosofia), tornando, assim, a medicina uma profissão.[83][84]
A alimentação na Grécia Antiga era caracterizada pela sua frugalidade para a maioria, refletindo dificuldades agrícolas, mas uma grande diversidade de ingredientes era conhecida, e os gregos ricos eram conhecidos por celebrar com refeições e festas elaboradas.[85] Segundo Felipe Daniel Ruzene, especialista em História da Alimentação na Antiguidade pela Universidade Federal do Paraná, a culinária foi fundada sobre a "trilogia mediterrânea" de cereais, azeitonas e uvas,[86] sobretudo a partir do pão, azeite e vinhoque tinham muitos usos e grande valor comercial.[1] Outros ingredientes eram muito importantes, se não mais, para a dieta média: mais notavelmente legumes. Pesquisas sugerem que o sistema agrícola da Grécia Antiga não poderia ter sido bem sucedido sem o cultivo de leguminosas.[87]
Na Grécia Antiga havia o cereal-pão, sítos (σί́τος), conforme pensamos hoje, uma massa fermentada e cozida, preparada a partir de farinha e água. Todavia, a comida mais comum (que tradicionalmente traduz-se como “pão”) era uma papa, ou mingau de cevada chamada maza (μαζα). A maza consistia em uma mistura de farinha pré-cozida e torrada com um líquido (água, azeite, mel ou leite). Essa mistura não era assada e poderia ser comida fresca ou conservada, salgada ou adoçada, também poderia levar alguma especiaria ou condimentos para acentuar seu sabor.[88] Apesar da base alimentar helênica estar nos cereais destinados à confecção de pães e bolos, em especial a cevada e o trigo, as carnes, denominadas kreas (κρέας), circularam profundamente no imaginário social grego desde o período homérico, mas era comumente associada ao ritual do sacrifício.
Um dos principais rituais da antiga religião grega no período clássico era a thysia (θυσία), que possuía um elemento destacado de abate animal (geralmente gado, ovelhas, cabras ou porcos). Ossos e gordura eram separados, em especial os fêmures, vértebras caudais e o posterior da bacia, que em seguida eram queimados em oferta aos deuses. A carne era grelhada, cozida ou fervida e, então, servida aos devotos. Os antigos gregos consumiam uma grande variedade de carnes, tais como: caça (javalis, veados, lebres, raposas e ouriços), bovinos (a carne mais desejada, embora cara e raramente consumida, uma vez que os bois eram usados nas lavouras), ovinos, caprinos, suínos, burros, cavalos, cachorros, aves (como pombos, patos, perdizes, galos, rolas, gansos e galinhas), uma multiplicidade de peixes e crustáceos, além de outros produtos de origem animal como ovos, leite e seus derivados. Evidências osteológicas encontradas nos santuários gregos apresentam ossadas de uma série de outros animais que parecem ter participado do cardápio grego, como lobos, gatos, doninhas, tartarugas, cobras, gazelas, camelos, abutres e leões. As carnes eram consumidas secas ou frescas, neste caso eram preparadas grelhadas, assadas ou cozidas, com auxílio de água, vinagre ou vinho.[1]
Os homens da Grécia Antiga eram frequentemente treinados para a luta, exatamente por isso, seus exercícios e suas alimentações deveriam ser reforçadas. Entre as iguarias que preparavam estava o peixe. Pescados diversos eram uma de suas principais atividades econômicas e também formavam a base alimentar, embora só tenham se tornado um alimento nobre a partir do período helenístico. A partir desta época, os helenos consumiam pescados, mariscos (mexilhões, ostras, vieiras e conchas), crustáceos (lavagantes e camarões), moluscos (polvos, lulas e chocos), enguias, tubarões, raias, e peixes diversos (divididos entre peixes de escama e peixes de pele), alguns dos quais sequer podem ser reconhecidos hoje.[1]
O vinho possuía papel central na dieta grega e seu consumo consistia na mistura de três partes de água com uma quarta parte de vinho. Raramente se bebia vinho puro, pois era uma prática bastante criticada por levar mais rapidamente à embriaguez. Eram consumidos assiduamente, tanto em versões tintas, quanto brancas. Possuíam uma grande diversidade de qualidades o que, evidentemente, impactava diretamente no preço e status da bebida.[88]
Os gregos foram uma das primeiras sociedades a produzir textos culinários visando fixar o prazer de beber e comer presentes, sobretudo, nos banquetes. Dentre esses textos, Arquéstrato (no século IV a.C.) compôs o mais antigo livro de receitas ao qual temos acesso, a Hedypatheia.[89] As informações sobre os hábitos alimentares dos Gregos Antigos são fornecidas por testemunhos escritos e várias ilustrações artísticas: as comédias de Aristófanes, as panelas de cerâmica e as estátuas de barro cozido.
A cultura da Grécia Antiga é considerada a base da cultura da civilização ocidental. A cultura grega exerceu poderosa influência sobre os romanos, que se encarregaram de repassá-la a diversas partes da Europa. A civilização grega antiga teve influência na linguagem, na política, no sistema educacional, na filosofia, na ciência, na tecnologia, na arte e na arquitectura moderna, particularmente durante a renascença da Europa ocidental e durante os diversos reviveres neoclássicos dos séculos XVIII e XIX, na Europa e Américas.
Conceitos como cidadania e democracia são gregos, ou pelo menos de pleno desenvolvimento na mão dos gregos. Qualquer história da Grécia Antiga requer cautela na consulta a fontes. Os historiadores e escritores políticos cujos trabalhos sobreviveram ao tempo eram, em sua maioria, atenienses ou pró-atenienses.
Por isso se conhece melhor a história de Atenas do que a história das outras cidades; além disso, esses homens concentraram seus trabalhos mais em aspectos políticos (e militares e diplomáticos, desdobramentos daqueles), ignorando o que veio a se conhecer modernamente por história econômica e social. Toda a história da Grécia antiga precisa dar atenção à condução parcial pelas fontes.
A dinastia Ming (chinês: 明朝, pinyin: Míng Cháo), ou Império do Grande Ming (chinês tradicional: 大明國; chinês simplificado: 大明国; pinyin: Dà Míng Guó), foi a dinastia que governou a China de 1368 a 1644, depois da queda da dinastia Mongol dos Iuã acabando com o período de caos iniciado por Sima Yan em 263. A dinastia Ming foi a última dinastia na China comandada pelos Hans (o principal grupo étnico da China), antes da rebelião liderada, em parte por Li Zicheng, e logo depois substituído pela dinastia Manchu dos Qing. Embora a capital Ming, Pequim, tenha sucumbido em 1644, os restos do trono e do poder dos Ming (coletivamente denominado Ming Meridional) sobreviveram até 1662.
O Estado comandado pelos Ming construiu uma vasta marinha e um exército permanente de um milhão de soldados.[1] Embora tenha existido comércio marítimo privado e missões tributárias oficiais nas dinastias anteriores, a frota tributária do almirante eunuco muçulmano Zheng He no século XV superou todas as outras em tamanho absoluto. Durante este período, havia um enorme número de projetos de construções, incluindo a restauração do Grande Canal e da Muralha da China e, também, a criação da Cidade Proibida, em Pequim, durante o primeiro quarto do século XV. As estimativas para a população no final da dinastia Ming variam de 100 a 150 milhões de pessoas. A Universidade de Calgary afirma que "os Ming criaram um dos maiores períodos de organização governamental e estabilidade social na história da humanidade".[2]
O Imperador Hongwu (r. 1368-1398) tentou criar uma sociedade de comunidades rurais auto-suficientes em um sistema rígido e imóvel que não teriam qualquer necessidade de se envolver com a vida comercial dos centros urbanos. Sua reconstrução da base agrícola chinesa e o reforço das redes de comunicação através do sistema militarizado de correio, acabou por criar o inesperado efeito da superprodução agrícola, cujo excedente era vendido nos crescentes mercados localizados ao longo das rotas de correio. A cultura rural e o comércio logo se tornaram influenciados pelas tendências urbanas. As classes mais ricas da sociedade, consagradas como a classe dos aristocratas acadêmicos, também foram afetadas por esta nova cultura baseada no consumo. Através da tradição, famílias de comerciantes começaram a produzir candidatos a oficiais acadêmicos e adotaram traços culturais e práticas típicas da classe dos aristocratas. Paralelamente a esta tendência que envolve as classes sociais e a expansão do consumo comercial, aconteceram as mudanças na filosofia social e política, na burocracia e nas instituições governamentais, e também nas artes e na literatura.
Até o século XVI, a economia da dinastia Ming foi estimulada pelo comércio marítimo com portugueses, espanhóis e holandeses. A China, então, se envolveu em um novo comércio mundial de bens, plantas, animais e culturas alimentares conhecido como o intercâmbio colombiano. O comércio com as potências européias e os japoneses trouxeram grandes quantidades de prata, que em seguida substituíram o cobre e as notas de papel como a moeda de troca principal na China. Durante as últimas décadas da dinastia Ming, o fluxo de prata na China tinha diminuído muito, comprometendo assim as receitas estatais e, de fato, toda a economia Ming. A economia sofreu ainda mais com os graves efeitos sobre a agricultura da queda da temperatura média do século XVIII, das calamidades naturais, das más colheitas, das epidemias freqüentes. A conseqüente fragmentação do poder e da diminuição do padrão de vida das pessoas permitiu que líderes rebeldes como Li Zicheng desafiassem a autoridade dos Imperadores Ming.
A dinastia Iuã Mongol (1271-1368) governava antes do estabelecimento da dinastia Ming. A discriminação étnica institucionalizada contra os chineses Han suscitou ressentimento e revolta; outras explicações para o desaparecimento dos Iuãs incluíam áreas com sobretaxas de impostos mesmo quando atingidas pelas más colheitas, inflação, e as enormes cheias do Rio Amarelo como resultado do abandono dos projetos de irrigação.[3] Conseqüentemente, a agricultura e a economia estavam em desordem e a rebelião eclodiu entre as centenas de milhares de camponeses chamados para trabalhar na reparação dos diques do Rio Amarelo.[3]
Uma série de grupos Han se revoltou, incluindo os Turbantes Vermelhos, em 1351. Os Turbantes Vermelhos eram associados ao Lótus Branco, uma sociedade secreta budista. Zhu Yuanzhang era um pobre camponês e monge budista que aderiu aos Turbantes Vermelhos em 1352, mas logo ganhou reputação ao casar com a filha primogênita de um comandante rebelde.[4] Em 1356 A força rebelde de Zhu invadiu a cidade de Nanquim,[5] o que ele, mais tarde, estabeleceria como a capital da dinastia Ming.
Zhu Yuanzhang alicerçou o seu poder no sul, eliminando seu arqui-rival e líder rebelde Chen Youliang na Batalha do Lago Poyang, em 1363. Depois do chefe dinástico dos Turbantes Vermelhos suspeitamente morrer em 1367 enquanto era acolhido como hóspede de Zhu, este último fez suas ambições imperiais conhecidas através do envio de um exército em direção à capital Iuã em 1368.[6] O último imperador Iuã fugiu para o norte para Shangdu, e Zhu declarou-se o fundador da dinastia Ming após destruir os palácios Iuã de Cambalique (Pequim).[6]
Em vez da tradicional forma de nomear uma dinastia a partir do nome do distrito de origem do primeiro governante, a escolha de Zhu para "Ming", ou "Brilhante", para a sua dinastia seguiu o precedente mongol dando um nome glorioso.[5] Zhu Yuanzhang também usou o título de Hongwu, ou "Imensamente Marcial", como seu título real. Apesar do Lótus Branco ter ajudado na sua ascensão ao poder, Hongwu negou mais tarde que ele tenha algum dia sido membro da organização e reprimiu o movimento religioso depois que se tornou imperador.[5][7]
Hongwu imediatamente se propôs a reconstruir a infra-estrutura imperial. Ele construiu um muro de 48 km em torno de Nanquim, bem como novos palácios governamentais e prefeituras.[6] O Mingshi diz que já em 1364 Zhu Yuanzhang havia começado a elaboração de um novo código de leis baseados no Confucionismo conhecido como o Daming Lu, que foi concluído em 1397 e repetiu determinadas cláusulas encontradas no velho Código Tang de 653.[8] O Hongwu organizou um sistema militar conhecido como o weisuo, que era semelhante ao sistema Fubing da dinastia Tang (618-907). O objetivo foi o de tornar os soldados, agricultores auto-suficientes, a fim de poderem se sustentar enquanto não estão em combate ou treinando.[9] O sistema de auto-suficiência agrícola dos soldados, porém, foi em grande parte uma farsa; rações intermitentes e recompensas não foram suficientes para sustentar as tropas, e muitos desertaram suas posições, se eles não estavam localizados na fronteira cujas tropas eram fortemente abastecidas.[10]
Embora seja um confucionista, o Hongwu tinha uma profunda desconfiança contra os funcionários acadêmicos da classe gentry, e não era medo de lhes enfrentar no tribunal por ofensas.[11] Ele aplicou as Avaliações do funcionalismo público em 1373 após queixar que os 120 estudiosos que obtiveram o grau jinshieram ministros incompetentes.[12][13] Depois que  os exames foram reintegrados em 1384,[13] ele executou o chefe-examinador depois que ele descobriu que ele apenas permitia aos candidatos do sul serem condecorados com o grau jinshi.[12]
Em 1380 o chanceler Hongwu Hu Weiyong foi executado depois da suspeita de uma conspiração para derrubar ele do trono; depois disso o Hongwu aboliu a Chancelaria chinesa, e assumiu esse papel como diretor executivo e imperador.[14][15] Com uma crescente desconfiança dos seus ministros, Hongwu criou o Jinyi Wei, uma rede de polícia secreta traçada a partir de seu próprios guardas palacianos. Eles foram parcialmente responsáveis pela perda de 100 mil vidas em várias repressões políticas durante as três décadas de sua existência.[14][16]
Em 1381, a dinastia Ming anexou as áreas do sudoeste que faziam parte do Reino de Dali. Até o final do século XIV, cerca de 200 mil colonos militares estabeleceram-se em 2 000 000 mu (350 000 hectares) de terras no que é hoje Yunnan e Guizhou.[17] Cerca de meio milhão a mais de colonos chineses chegaram nos períodos posteriores; estas migrações causaram uma grande mudança na aparência da etnia da região, uma vez que mais de metade dos cerca de 3 milhões de habitantes no começo da dinastia Ming eram povos não-Han.[17] Nesta região, o Governo Ming adotou uma política de administração dual. As áreas com maioria étnica chinesa eram regidos de acordo com a legislação e políticas Ming; áreas onde grupos tribais nativos eram maioria tinham seu próprio conjunto de leis, enquanto os chefes tribais prometiam manter a ordem e enviar tributos para a corte Ming, em troca de bens necessários.[17] De 1464 a 1466 os povos Miao e o Yao rebelaram-se contra o que eles viam sendo um governo opressivo; em resposta, o governo Ming enviou um exército de 30 mil soldados (incluindo mil mongóis) para se juntar às 160 mil tropas locais de Guangxi, e acabar com a rebelião.[18] Depois que o filósofo Wang Yangming (1472-1529) reprimiu outra rebelião na região, ele organizou uma administração conjunta entre os grupos étnicos locais e os chineses, a fim de implementar elementos da cultura chinesa na cultura dos povos locais.[18]
Estudiosos de fora da China em geral vêem o Tibete como tendo sido um estado independente durante a dinastia Ming, enquanto historiadores na China atual tem um ponto de vista oposto. O Mingshi — a história oficial da dinastia Ming compilada muito mais tarde, em 1739 — afirma que os Ming estabeleceram comandantes itinerantes que supervisionavam a administração tibetana ao mesmo tempo em que renovavam títulos de ex-funcionários da dinastia Iuã do Tibete e conferiam novos títulos do principado aos líderes das seitas budistas do Tibete.[21] No entanto, Turrell V. Wylie afirma que houve censura no Mingshi para reforçar o prestigio do imperador Ming e a sua reputação e a todo custo ofuscar a história da sutil relação sino-tibetana durante a dinastia Ming.[22] Estudiosos modernos debatem se a Dinastia Ming realmente tinha soberania sobre o Tibete, como alguns acreditam que fora uma relação de suserania, e que foi em grande parte cortada quando o Imperador Jiajing (r. 1521-1567) perseguiu os budistas em favor do Daoísmo na corte.[22][23][24] Helmut Hoffman afirma que a dinastia Ming mostrava o papel do Estado sobre o Tibete através do envio de missões periódicas de "emissários do Tesouro" para a Corte Ming através da concessão de títulos nominais aos Lamas, mas que não interferiam no Governo tibetano.[25] Wang Jiawei e Nyima Gyaincain discordam, afirmando que a China Ming tinha soberania sobre os tibetanos e que não herdavam títulos Ming, mas eram obrigados a viajar para a Pequim para renová-los.[26] Melvyn C. Goldstein escreve que os Ming não tinham qualquer autoridade administrativa real ao longo do Tibete assim que os vários títulos dados aos líderes tibetanos no poder já não davam o mesmo poder como os antigos títulos mongóis Iuãs; segundo ele, "os imperadores Ming meramente reconheciam a realidade política do Tibete".[27] Alguns estudiosos afirmam que a significante natureza religiosa da relação da corte Ming com os Lamas tibetanos é sub-representado pelos historiadores modernos.[28][29] Outros reforçam o aspecto comercial da relação, notando a quantidade insuficiente de cavalos da dinastia Ming e à necessidade de manter o troca de chá por cavalos, com o Tibete.[30][31][32][33][34] Estudiosos também debatem quanta influência e poder a corte da dinastia Ming teve ao longo das sucessivas famílias que de facto governaram o Tibete, a Phagmodru (1354-1436), Rinbung (1436-1565), e Tsangpa (1565-1642).[35][36][37][38][39][40]
Os Ming iniciaram esporádicas intervenções armadas no Tibete durante o século XIV, embora, algumas vezes, os tibetanos também revidaram-nas com sucesso através de resistências armadas contra as incursões Ming.[41][42] Patricia Ebrey, Thomas Laird, Wang Jiawei, e Nyima Gyaincain recordam que a dinastia Ming não tinha uma guarnição permanente no Tibete,[38][43][44] ao contrário da antiga dinastia mongol Iuã.[38] O Imperador Wanli (r. 1572-1620) fez alguns esforços para restabelecer as relações sino-tibetanas, depois da aliança Mongol-tibetana iniciado em 1578, o qual afetava à política externa da posterior dinastia manchu Qing (1644-1912) em seu apoio ao Dalai Lama da seita Chapéu Amarelo.[22][45][46][47][48] No final do século XVI, os mongóis provaram ser bons protetores do Dalai Lama da seita Chapéu Amarelo através da sua crescente presença na região Amdo, culminando na conquista do Tibete em 1642 por Guxi Cã (1582-1655).[22][49][50][51]
De acordo com o historiador Timothy Brook, o imperador Hongwu tentou imobilizar a sociedade através da dura criação de fronteiras regulamentadas pelo estado entre aldeias e cidades maiores, desencorajando o comércio e viagens entre sociedades não aceitas pelo governo.[52] Hongwu tentou implantar austeros valores, impondo códigos de vestimenta, métodos padrões de expressão, e um estilo padrão de escrita da prosa clássica que não mostravam as habilidades dos mais educados.[53] Sua desconfiança com a elite acadêmica mostrou o seu desdém para com a elite comercial, impondo taxas excessivamente elevadas sobre as propriedades de poderosas famílias mercantes na região de Suzhou em Jiangsu.[12] Ele também forçou a mudança de milhares de famílias abastadas do sudeste e reassentou-as ao redor de Nanquim, na região de Jiangnan, proibindo-lhes de se mudar uma vez que já estivessem assentadas.[12][54] Para acompanhar as atividades dos mercadores, Hongwu obrigou-os a registrar todos os seus bens, uma vez por mês.[55] Uma de suas principais metas como governante foi permanentemente frear a influência dos comerciantes e donos de terra, embora várias de suas políticas acabassem por incentivá-los a acumular mais riqueza.
O sistema opressivo do Hongwu de realocação maciça e o desejo de escapar das suas altas taxas encorajaram muitas pessoas a tornarem-se vendedores itinerantes, ambulantes, ou trabalhadores migrantes tentando encontrar proprietários rurais que lhes alugariam um espaço de terra para cultivar alimentos nela.[56] Até a metade da era Ming, os imperadores abandonaram o regime de realocação do Hongwu e ao invés disso confiaram a funcionários locais o trabalho de documentar os trabalhadores migrantes, a fim de aumentar a receita do Império.[57] A elite de comerciantes e proprietários ricos que reinavam sobre as terras ocupadas, os empregados assalariados, os servos domésticos, e os trabalhadores migrantes foi praticamente a visão do Hongwu: o cumprimento rigoroso do sistema hierárquico das quatro ocupações.[58]
O Hongwu reacendeu o setor agrícola, para criar comunidades auto-suficientes que não dependessem de comércio, que ele pensou permaneceriam apenas nas áreas urbanas.[60] No entanto, o excedente criado a partir desta revitalização encorajou os agricultores rurais a lucrarem com o comércio, vendendo primeiro seus produtos a intermediários; em meados da época Ming eles começaram a vender os seus produtos em mercados urbanos da região.[61] Como as zonas rurais e urbanas tornaram-se cada vez mais integradas através do comércio, famílias de áreas rurais começaram a adotar costumes tradicionalmente urbanos, como a produção de seda e algodão têxtil.[62] No final da era Ming, houve uma preocupação crescente entre os conservadores confucianos que a delicada exploração do tecido e a ordem social comum estavam sendo prejudicadas pelos rústicos do interior que aceitavam todos os costumes da vida urbana.[63]
O agricultor rural não foi o único grupo social afetado pela crescente comercialização entre a sociedade chinesa, isso também influenciou fortemente a elite dona de terras que tradicionalmente produzia estudiosos para o serviço público. Os funcionários eram tradicionalmente empregados como indivíduos simples que se logravam de arrogância apoiados na riqueza obtida a partir de uma carreira prestigiada; eles eram conhecidos por irem de seus países de origem para as cidades na qual eram empregados.[64] Durante o governo do Imperador Zhengde (1505-1521), os funcionários eram vistos sendo transportados liteiras de luxo, e começaram a comprar grandes casas em bairros urbanos abastados em vez de viverem nas zonas rurais.[65] Até o final da era Ming, ser rico se tornou o principal indicador de prestígio social, mais ainda do que ganhar um grau acadêmico.[66]
Na primeira metade da era Ming, oficiais acadêmicos raramente mencionariam a contribuição dos comerciantes para com a sociedade, enquanto escreviam as suas gazetas locais;[68] os funcionários eram certamente capaz de financiarem seus próprios projetos de obras públicas, um símbolo de sua íntegra liderança política.[69] No entanto, na segunda metade da era Ming, tornou-se comum dos funcionários angariarem dinheiro de comerciantes, a fim de financiar seus diversos projetos, como a construção de pontes ou o estabelecimento de novas escolas de confucionismo para melhorar a aprendizagem da gentry.[70] A partir de então as gazetas começaram a citar alguns mercadores e muitas vezes com grande estima, uma vez que a riqueza produzida pelas suas atividades econômicas gerava recursos para o Estado, bem como o aumento da produção de livros necessária para a educação da gentry.[71] Os comerciantes começaram a adquirir mais cultura, atitudes de um connoisseur e traços da classe gentry, misturando as linhas entre os comerciantes e a gentry e preparando o caminho para as famílias mercantes fornecerem oficiais-acadêmicos.[72] As raízes desta transformação social e de indistinção de classes poderiam ser encontrada na dinastia Song (960-1279),[73] mas tornou-se muito mais acentuada na Ming. Escrituras de instruções para linhagens familiares, no período final da era Ming, exibiam que o próximo da linhagem não herdaria sua posição na categorização das quatro ocupações (em ordem decrescente): gentry, agricultores, artesãos, e revendedores.[74]
O Hongwu acreditava que apenas os mensageiros do governo e comerciantes humildes e varejistas deveriam ter o direito de viajar para fora da sua cidade natal.[55] Apesar de seus esforços para impor este ponto de vista, a sua construção de uma eficiente rede de comunicações para o seu exército e seus funcionários reforçou e fomentou a ascensão de uma potencial rede comercial correndo em paralelo à rede de mensageiros.[75] O náufrago coreano Choe Bu (1454-1504) comentou em 1488 a forma como os habitantes locais, ao longo da costa oriental da China não sabiam exatamente a distância entre determinados locais, cujo conhecimento era praticamente exclusivo do Ministério da Guerra e dos mensageiros.[76] Isso foi um contraste estrondoso com o final do período Ming, quando comerciantes não só viajavam mais distâncias para vender seus produtos, mas também subornavam funcionários do correio para usar as suas rotas e até mesmo tinham imprimido guias geográficos de rotas comerciais que imitavam os mapas dos mensageiros.[77]
A dependência dos oficiais-acadêmicos das atividades econômicas dos comerciantes se tornou mais do que uma tendência quando foi semi-institucionalizado pelo Estado, durante a era Ming. Qiu Jun (1420-1495), um oficial-acadêmico de Hainan, alegou que o Estado só devia tratar dos assuntos do mercado durante épocas de crise e os comerciantes eram os melhores para determinarem a importância da riqueza de recursos da nação.[79] O Governo seguiu esta orientação em meados do período Ming quando ele autorizou os mercantes a assumir o monopólio estatal da produção de sal. Este foi um processo gradual, onde o estado forneceu exércitos às fronteiras do norte com suprimentos suficientes através da concessão de licenças para o comércio de mercadores de sal em troca de seus serviços marítimos.[80] O Estado percebeu que comerciantes poderiam comprar licenças de sal de prata e em troca impulsionassem as receitas do Estado, a ponto de que comprar grãos não foi mais um problema.[80] Os governos do Hongwu e de Zhengtong (r. 1435 – 1449) tentaram cortar a entrada de prata na economia produzindo papel moeda, ainda que a mineração do metal tivesse se tornado uma lucrativa atividade ilegal praticada por muitos.[81] O Hongwu não tinha conhecimento de inflação econômica por isso continuou a distribuir uma grande quantidade de notas de papel como prêmios; em 1425, o papel moeda valia apenas de 0,025% a 0,014% do seu valor original do século XIV.[10] O valor padrão da cunhagem de cobre também diminuiu significativamente devido à falsificação; no século XVI, novas rotas comerciais marítimas com a Europa geraram enormes quantidades de prata importadas, que se tornaram cada vez mais a moeda de troca padrão da época.[82] Já em 1436, as taxas do comércio de grãos haviam sido parcialmente trocada para pagamentos em prata.[83] Em 1581, a Reforma do Chicote  ministrada pelo Grande secretário Zhang Juzheng (1525-1582) finalmente regulamentou os impostos sobre a quantidade de terras a serem pagas integralmente em prata.[84]
O neto do Hongwu, Zhu Yunwen, assumiu o trono como o Imperador Jianwen (1398-1402) depois da morte de Hongwu em 1398. Em uma introdução para uma guerra civil de três anos, com início em 1399,[85] Jianwen se envolveu em uma confrontação política com seu tio Zhu Di, o Príncipe de Yan. O Imperador tinha conhecimento das ambições dos seus tios príncipes, estabelecendo medidas que limitavam suas autoridades. O militante Zhu Di, devido às batalhas contra os mongóis nas fronteiras ao redor de Pequim, era o mais temido dos príncipes. Depois que Zhu Di teve muitos de seus associados presos por Jianwen, ele planejou uma rebelião. Sob o pretexto de evitar que o jovem Jianwen corrompesse funcionários, Zhu Di pessoalmente liderou o exército revolto, e o palácio de Nanquim foi queimado, juntamente com o sobrinho de Zhu Di, o Imperador Jianwen, sua esposa, mãe, e toda sua corte. Zhu Di assumiu o trono como o Yongle (1402-1424); seu reinado é universalmente visto pelos estudiosos como uma "segunda fundação" da Dinastia Ming, pois ele reverteu muitas das políticas do seu pai.[86]
Yongle rebaixou Nanquim como capital secundária e em 1403 anunciou que a nova capital da China seria na sede do seu próprio governo em Pequim. A construção da nova cidade durou 13 anos, de 1407 a 1420, empregando centenas de milhares de trabalhadores diariamente.[87] No centro ficava a zona política da Cidade Imperial, e no centro dessa zona existia a Cidade Proibida, a residência palaciana do imperador e de sua família. Até 1553, a cidade cresceu para o sul, que levou o Pequim ao tamanho de 6,5 a 7,3 quilômetros.[88]
Depois que passou décadas abandonado e degradado, o Grande Canal foi restaurado sob o império de Yongle de 1411-1415. O motivo para a restauração do canal era o de resolver definitivamente o problema do transporte de grãos para o norte de Pequim. O transporte marítimo anual de 4 000 000 shi (um shi é igual a 107 litros) era difícil devido ao sistema ineficiente de transporte de grãos através do Mar da China Oriental ou por vários canais fluviais que, durante o processo, exigiam a transferência dos grãos entre vários tipos de barcaça diferentes, incluindo as barcaças de água superficial e profunda.[89] Yongle encomendou cerca de 165 mil trabalhadores para dragar o leito do canal na parte ocidental de Shandong, e construiu uma série de quinze eclusas.[88][90] A reabertura do Grande Canal teve implicações para Nanquim também, pois ela foi superada pela cidade de Suzhou como o maior centro comercial da China, devido à sua ótima posição geográfica.[91]
Embora Yongle ordenasse episódios sangrentos de purgas como seu pai - incluindo a execução de Fang Xiaoru que recusou-se a propor a proclamação da sua sucessão - Yongle teve uma atitude diferente sobre os oficiais-acadêmicos.[87] Ele tinha uma seleção de textos elaborados a partir da escola de Zhu Xi Cheng- Confucionismo Zhu – ou Neo-Confucionismo - a fim de ajudar aqueles que estudavam para os exames do funcionalismo público.[87]  Yongle encomendou a dois mil acadêmicos a missão de criar uma enciclopédia  de 50 milhões de palavras (22 938 capítulos), a Enciclopédia Yongle - de sete mil livros.[87] Esta superou todas as enciclopédias anteriores em cobertura e dimensão, incluindo a compilação dos Quatro Grandes Livros de Canções do século XI. No entanto, os oficiais-acadêmicos não foram o único grupo político que o Yongle teve que apaziguar. O historiador Michael Chang salienta que o Yongle foi um "imperador a cavalo" que muitas vezes viajava entre as duas capitais, como na tradição mongol dos Iuãs e constantemente conduziu expedições para a Mongólia.[92] Isso era o oposto do que o confucionismo pregava porque ao mesmo tempo servia para reforçar a importância dos eunucos e dos oficiais militares cuja força dependia dos favores do imperador.[92]
A partir do início de 1405, o Yongle confiou ao seu favorito comandante-eunuco Zheng He (1371-1433) o título de almirante naval para a gigantesca nova frota de navios designadas para missões comerciais internacionais. Os chineses enviavam missões diplomáticas para o ocidente desde a dinastia Han (202 a.C. – 220 d.C.) e estavam envolvidos com comércio exterior não-estatal estando presente na África Oriental durante séculos - que atingiu o pico nas dinastias Song e Iuã - mas nenhuma missão comercial patrocinada pelo Governo desta grandeza e dimensão tinha sido montada antes. Para servir a sete diferentes missões comerciais no estrangeiro, os estaleiros navais de Nanquim construiriam dois mil navios entre 1403 a 1419, que incluía os grandes navios do tesouro que mediam de 112 metros (370 pés) a 134 m (440 pés) de comprimento e 45 m (150 pés) a 54 m (180 pés) de largura.[93] A primeira viagem, que ocorreu entre 1405 e 1407, continha 317 embarcações com uma equipe de 70 eunucos, 180 médicos, 5 astrólogos, e 300 oficiais militares comandando um total estimado de 26 800 homens.[94]
As enormes missões comerciais foram interrompidas após a morte de Zheng He, mas sua morte foi apenas um de muitos fatores que acabaram com as missões. Yongle tinha conquistado o Vietnã em 1407, mas as tropas Ming foram expulsas em 1428 com grandes custos para a tesouraria Ming; em 1431 a nova dinastia Lê do Vietnã foi reconhecida como um estado independente, que era tributada pelo império.[95] Houve também a ameaça e a revitalização dos mongóis no norte da estepe, que desviaram a atenção da corte dos outros assuntos; para enfrentar esta ameaça, uma quantidade enorme de dinheiro foi utilizadas para a construção da Grande Muralha após 1474.[95] A mudança de Yongle de Nanquim para Pequim como capital foi, em grande parte, em resposta à necessidade da corte de manter-se mais próxima da ameaça mongol no norte do país.[96] Os oficiais-acadêmicos também associaram os enormes gastos com as frotas com o aumento do poder dos eunucos na corte e, por isso, cortou o financiamento para as frotas para barrar a ampliação da influência dos eunucos.[97]
O líder dos Mongóis Oirats Esen Tayisi lançou uma invasão na China Ming em julho de 1449. O chefe eunuco Wang Zhen incentivou o Imperador Zhengtong (r. 1435-1449) para que ele conduzisse pessoalmente uma força para enfrentar os mongóis após uma derrota Ming recente; marchando com 50 000 tropas, Zhengtong deixou a capital e colocou o seu meio-irmão Zhu Qiyu, encarregado do trono como regente temporário. Na batalha que ocorreu em 8 de setembro, a sua força de 50 000 soldados foi dizimada pelo exército de Esen e Zhengtong foi capturado e mantido em cativeiro pelos mongóis - um evento conhecido como a Crise Tumu.[98] Depois da captura de Zhengtong, as forças da Esen pilharam tudo que aparecia no caminho até chegarem ao subúrbio de Pequim.[99] Depois ocorreu outro saque nos subúrbios de Pequim em novembro do mesmo ano por bandidos locais e soldados Ming de ascendência Mongol que estavam vestidos como invasores mongóis.[100] Muitos chineses Han também começaram a saquear e roubar logo após o incidente Tumu.[101][102]
Os mongóis seqüestraram o Imperador Zhengtong e pediram um resgate para devolvê-lo. No entanto, esse plano foi frustrado porque o irmão mais novo de Zhengtong assumiu o trono com o Imperador Jingtai (r. 1449-1457); os mongóis também foram repelidos quando o braço-direito de Jingtai, o ministro de defesa Yu Qian (1398-1457), controlou as forças armadas Ming. Manter Zhengtong em cativeiro era inútil na negociação para os mongóis, pois outro tinha sentado em seu trono, de modo que ele foi liberado para voltar para a China Ming.[98] Zhengtong foi colocado em prisão domiciliar no seu palácio até o golpe contra Jingtai em 1457 conhecido como o "Incidente da Captura do Portão".[103] Zhengtong retomou o trono como Imperador Tianshun (r. 1457-1464).
O reinado do Tianshun foi agitado e as forças mongóis de dentro da estrutura militar Ming continuaram a ser problemáticas. Em 7 de agosto de 1461, o general chinês Qin Cao e suas tropas Ming de ascendência mongol tentaram um golpe contra Tianshun por medo de estarem na lista de purga daqueles que ajudaram na sucessão de Jingtai.[104] Os mongóis que serviam no exército Ming também se tornaram cada vez mais prudentes, pois os chineses começaram a desconfiar dos pedidos dos mongóis após a crise Tumu.[105] A força rebelde de Cao organizara-se para por fogo nos portões ocidental e oriental da Cidade Imperial (apagado pela chuva durante a batalha), e matou vários ministros antes que suas forças tenham sido finalmente derrotadas e ele foi forçado a cometer suicídio.[106][107]
A ameaça mongol para a China atingiu o seu pico no século XV, embora investidas periódicas tenham continuado durante todo o período da dinastia. Assim como na Crise Tumu, o líder Mongol Altã Cã (r. 1470-1582) invadiu a China e dominou uma grande parte do território até os arredores de Pequim.[108][109] É interessante notar que o Imperador Ming empregou soldados de ascendência mongol para lutar contra a invasão de Altã Cã, assim como os oficiais militares mongóis foram contra o golpe falho de Cao Qin.[110] As incursões mongóis levaram as autoridades Ming à reconstrução da Grande Muralha a partir do final do século XV até o século XVI; John Fairbank observa que "isso provou ser um gesto fútil militarmente, mas expressou fortemente o fechamento da mentalidade chinesa".[95] Ainda, a Grande Muralha não foi concebida para ser uma fortificação puramente defensiva; suas torres funcionavam mais como uma série de fronteiras iluminadas e estações de sinalização para advertir mais rapidamente às tropas amigas do avanço das tropas inimigas.[111]
Em 1479, o vice-presidente do Ministério da Guerra queimou os registros do tribunal que documentavam as viagens de Zheng He; esse foi um dos muitos eventos que sinalizavam a mudança chinesa na política externa.[112] As leis navais que foram implementadas restringiam os navios a um pequeno tamanho; o declínio da marinha Ming permitiu o crescimento da pirataria ao longo da costa chinesa.[95] Piratas japoneses - ou wokou – começaram a saquear navios chineses e comunidades costeiras, apesar de grande parte da pirataria ter sido realizada por chineses nativos.[95]
Em vez de montar um contra-ataque, as autoridades Ming optaram por encerrar as instalações costeiras e matar os piratas de fome; todo o comércio exterior teve de ser realizado pelo Estado, por missões tributárias formais.[95] Estas políticas eram conhecidos como as leis hai jin, que instituíam uma proibição rigorosa da atividade privada marítima até a abolição formal, em 1567.[95] Neste período de controle estatal, o comércio exterior com o Japão foi realizado exclusivamente pelo porto de Ningbo, o comércio com as Filipinas exclusivamente em Fuzhou, e com a Indonésia exclusivamente em Cantão.[113] Depois disso os japoneses só foram autorizados no porto uma vez a cada dez anos, e eram autorizados a levar um máximo de trezentos homens em dois navios; estas leis chinesas encorajaram muitas pessoas a empenharem-se na comercialização ilegal do comércio e na generalização do contrabando.[113]
O ponto fraco nas relações entre a China Ming e o Japão ocorreu durante o governo do grande general japonês Toyotomi Hideyoshi, que em 1592 anunciou que ia conquistar a China. Nas duas campanhas conhecidas como Guerra Imjin, os japoneses lutaram contra os exércitos coreano e Ming. Ambos os lados ganharam batalhas na guerra, mas com a morte de Hideyoshi em 1598, os japoneses perderam as suas últimas bases na Coréia e retornaram para o Japão. Apesar disto e a grande liderança dos coreanos, como do almirante Yi Sun-sin, os generais Ming foram quem levaram o crédito pela vitória. No entanto, a vitória veio com um enorme custo para o Tesouro do governo Ming: 26 000 000 onças de prata.[114]
Embora Jorge Álvares tenha sido o primeiro a desembarcar na Ilha Lintin no Delta do Rio das Pérolas, em maio de 1513, foi Rafael Perestrelo - um parente da esposa de Cristóvão Colombo - que se tornou o primeiro explorador europeu a desembarcar na costa sul da China continental e a comercializar com Cantão em 1516, comandando um navio português com uma tripulação de malaios e que havia navegado de Malaca.[115][116][117] Os Portugueses enviaram uma expedição em larga escala em 1517 para entrar em Cantão e abrir relações comerciais formalmente com as autoridades chinesas.[115] Durante esta expedição os portugueses tentaram enviar uma delegação para o interior no nome de Manuel I para a corte do imperador Ming Zhengde; em vez disso a missão diplomática foi presa em uma prisão chinesa e morreu nela.[115] Após a morte de Zhengde em abril de 1521, a parte conservadora da corte, que era contra a expansão das relações comerciais, interpretava que a conquista portuguesa de Malaca - um vassalo fiel dos Ming - era motivo suficiente para rejeitar a instalação de uma embaixada portuguesa.[118] Simão de Andrade, irmão do embaixador Fernão Pires de Andrade, também havia alimentado as especulações chinesas que os portugueses sequestravam crianças chinesas para comê-las; Simão tinha comprado crianças, raptadas como escravas, que foram encontradas mais tarde em Diu, Índia.[119] Em 1521, as forças navais Ming lutaram e expulsaram os navios portugueses em Tuen Mun, onde os primeiros canhões de carregamento-traseiro foram introduzidos na China.[120] Apesar das hostilidades iniciais, depois de 1549 os portugueses enviavam missões comerciais anualmente para a Ilha de Shangchuan.[115] Em 1557, os portugueses conseguiram convencer a corte Ming sobre um tratado comercial legal que estabeleceria Macau como uma colônia oficial portuguesa no litoral do Mar do Sul da China.[115] A frota portuguesa de Gaspar da Cruz (c. 1520 - fevereiro 5, 1570) viajou para Cantão em 1556, e escreveu o primeiro livro completo sobre a China e a dinastia Ming que foi publicada na Europa (quinze dias após a sua morte); ele incluía informações sobre a sua geografia, as províncias, a realeza, os altos funcionários, a burocracia, o transporte marítimo, a arquitetura, agricultura, o artesanato, negócios comerciais, vestuário, costumes religiosos e sociais, música e instrumentos, caligrafia, educação e justiça.[121]
A partir da China as principais exportações eram seda e porcelana. A Companhia das Índias Orientais transportava sozinha 6 milhões de itens em porcelana da China para a Europa entre os anos de 1602 e 1682.[122] Antonio de Morga (1559-1636), um oficial espanhol em Manila, indicou um grande inventário dos bens que foram negociadas pela China Ming na virada do século XVII, notando que havia "raridades que, se eu me referisse a todos elas, eu nunca terminaria, ou sequer tenho papel suficiente para tal".[123] Depois de notar a variedade de produtos de seda comercializados com os europeus, Ebrey escreveu sobre o tamanho considerável das transações comerciais.
Depois que os chineses proibiram o comércio direto dos comerciantes chineses com o Japão, os portugueses preencheram este vácuo como intermediários entre a China e o Japão.[124] Os portugueses compravam seda chinesa e vendiam-nas para os japoneses, em troca de prata japonesa; como a prata era mais valorizada na China, os portugueses podiam utilizar a prata japonesa para comprar cada vez mais estoques de seda chinesa.[124] No entanto, em 1573 - após os espanhóis estabelecerem uma base comercial em Manila – o comércio português como intermediários foi diminuído pela entrada principal de prata na China a partir da América espanhola.[125][126]
Embora a maior parte das importações para a China tenham sido de prata, os chineses também importavam produtos agrícolas do Novo Mundo a partir do Império espanhol. Nisso incluía batata doce, milho, e amendoim, alimentos que poderiam ser cultivados em terras onde as culturas tradicionais chinesas - trigo, milhete e arroz – não podiam crescer, portanto facilitando o aumento da população chinesa.[113][127] Na dinastia Song (960-1279), o arroz tinha se tornado a principal plantação de subsistência dos pobres;[128] depois que a batata doce foi introduzida na China, em torno de 1560, ela se tornou gradualmente o alimento tradicional das classes mais baixas.[129]
A perda financeira na Guerra Imjin na Coréia contra os japoneses foi um de muitos problemas - de carácter fiscal ou outros – que a China Ming enfrentou durante o reinado do Imperador Wanli (r. 1572-1620). No início de seu reinado, Wanli cercou-se com conselheiros experientes e fez um esforço consciente para tratar dos assuntos do Estado. Seu Grande Secretário Zhang Juzheng (funcionário entre 1572 e 1582) acumulou uma eficaz rede de alianças com altos funcionários.[130] No entanto, não houve nenhum outro funcionário qualificado o suficiente depois dele para manter a estabilidade dessas alianças;[130] os funcionários logo se uniram em facções políticas opostas. Ao longo do tempo, Wanli foi ficando cansado de assuntos judiciais e queixas políticas freqüentes entre os seus ministros, preferindo ficar por trás dos muros da Cidade Proibida e longe da vista de seus funcionários.[131]
Os assessores oficiais pressionaram Wanli sobre qual dos seus filhos deveria o suceder no trono; ele também foi ficando igualmente revoltado com altos assessores constantemente palpitando sobre a forma de gerir o Estado.[131] Houve aumento das facções na corte e em todo a esfera intelectual da China que crescia a partir do debate filosófico contra ou a favor do ensino de Wang Yangming (1472-1529), o último daqueles que rejeitou algumas das opiniões ortodoxas do neo-confucionismo.[132][133] Entediado por tudo isso, Wanli começou a negligenciar algumas de suas funções, permanecendo ausente das audiências da corte para discutir política, perdeu o interesse em estudar os clássicos do confucionismo, recusou-se a ler petições e outros documentos do estado, e parou de preencher as recorrentes vagas de vitais cargos administrativos.[131][134] Os funcionários-acadêmicos perderam destaque na administração assim que os eunucos se tornaram os intermediários entre o imperador e seus funcionários; qualquer alto funcionário que queria discutir questões do estado teria que subornar um dos poderosos eunucos simplesmente para ter suas exigências ou mensagem retransmitida para o imperador.[135]
Foi dito que o Hongwu proibia os eunucos de aprender a ler ou a se envolverem na política.[88] Se estas restrições foram realizados com sucesso absoluto em seu reinado ou não, os eunucos no período de reinado do Yongle e depois dele, administraram departamentos imperiais enormes, comandaram exércitos, e participaram em assuntos de nomeação e promoção de funcionários.[88] Os eunucos desenvolveram sua própria burocracia que foi organizada paralelamente mas não estava sujeita a burocracia do funcionalismo público.[88] Embora houvesse vários eunucos ditadores ao longo de todo o período Ming, como Wang Zhen, Wang Zhi, e Liu Jin, o excessivo poder tirânico dos eunucos não se tornou evidente até a década de 1590 quando Wanli aumentou seus direitos sobe a burocracia civil e concedeu-lhes poder de cobrar os impostos provinciais.[134][135][136]
O eunuco Wei Zhongxian (1568-1627) dominou a corte do Imperador Tianqi (r. 1620-1627) e teve seus rivais políticos torturados até à morte, principalmente os críticos da facção da "Sociedade Donglin".[137] Ele ordenou a construção de templos em sua homenagem ao longo de todo o Império Ming,[135] e construiu palácios pessoais com os fundos reservados para a construção do túmulo do antigo imperador. Seus amigos e familiares ganharam posições importantes, sem qualquer qualificação. Wei também publicou uma obra histórica censurando e desprezando seus adversários políticos.[135] A instabilidade na corte veio à tona quando calamidades naturais, pestes, rebeliões, e as invasões estrangeiras chegaram ao seu máximo. Apesar do Imperador Chongzhen (r. 1627-1644) ter demitido a corte de Wei - que levou ao suicídio de Wei pouco depois - o problema com os eunucos persistiu até o colapso da dinastia menos de duas décadas mais tarde.
Durante os últimos anos do reinado de Wanli e os de seus dois sucessores, desenvolveu-se uma crise econômica que foi centrada em torno de uma súbita falta generalizada do principal meio de troca do império: prata. Os poderes protestantes da República Holandesa e do Reino da Inglaterra patrocinavam freqüentes incursões e atos de pirataria contra os impérios católicos da Espanha e de Portugal no intuito de enfraquecer a sua potência econômica mundial.[138] Entretanto, Filipe IV de Espanha (r. 1621-1665) iniciou o combate ao contrabando ilegal de prata do México e do Peru em todo o Pacífico em direção à China, favorecendo o transporte marítimo da prata americana diretamente da Espanha para Manila. Em 1639, o novo regime Tokugawa do Japão encerrou a maior parte do seu comércio externo com as potências européias, dificultando mais uma das fontes de entrada de prata na China. No entanto, a maior interrupção do fluxo de prata veio das Américas, ao mesmo tempo em que a entrada da prata japonesa continuava na China, mas em quantidades limitadas.[139] Alguns historiadores afirmam que mesmo que o preço de prata tenha subido no século XVII, seria devido a uma queda na procura de bens, e não diminuiria os estoques de pratas.[140]
Estes acontecimentos ocorreram praticamente ao mesmo tempo o que causou um rápido aumento no valor de prata, e fez o pagamento dos impostos praticamente impossível para a maioria das províncias. As pessoas começaram a acumular prata, conforme houve um desaparecimento progressivo dela, forçando a um forte declínio na cotação entre o valor do cobre e da prata.[125] Na Década de 1630, uma seqüência de mil moedas de cobre valia uma onça de prata; em 1640 foi reduzido para o valor de meia onça; em 1643 valia aproximadamente um terço de uma onça.[125] Para os camponeses foi um desastre econômico, já que eles pagavam os impostos com prata, mas conduziam o comércio local e vendiam seus produtos em troca de moedas de cobre.[141]
No início da metade do século XVII, a fome se tornou comum no norte da China por causa do incomum clima frio e seco que encurtou o período de colheita; estes foram efeitos de um evento ecológico maior que hoje é conhecido como a Pequena Idade do Gelo.[142] A fome, junto com o aumento dos impostos, deserções militares generalizadas, um sistema de  seguridade social em declínio, e desastres naturais como enchentes e a incapacidade do governo para administrar adequadamente as irrigações e os projetos de controle das cheias causou uma ampla perda de vidas e da civilidade normal.[142] O governo central sofria com uma falta de recursos e poderia fazer muito pouco para atenuar os efeitos dessas calamidades. Para tornar as coisas piores, uma epidemia se alastrou por toda a China de Zhejiang à Henan, matando um grande número de pessoas, desconhecido até hoje.[143]
Um notável líder tribal chamado Nurhachi (r. 1616-1626), começando com apenas uma tribo pequena, rapidamente ganhou o controle de toda as tribos da Manchúria. Durante a Guerra Imjin ele se ofereceu para liderar as suas tribos no apoio aos exércitos Ming e Joseon. Esta oferta foi recusada, mas a ele foi concedido grandes títulos de honra dos Ming pelo seu gesto.[144] Reconhecendo a fragilidade da autoridade Ming na sua fronteira norte, ele assumiu o controle sobre todas as outras tribos independentes ao redor de sua terra natal.[144] Em 1610 ele quebrou relações com a corte Ming; em 1618 ele exigia aos Ming que pagassem tributos a ele, a fim de corrigir as sete injustiças no qual ele documentou e enviou para a corte Ming. Esta foi, num sentido muito real, uma declaração de guerra pois o Imperador Ming não estava apto a dar dinheiro para os manchus.
Sob o brilhante comandante Yuan Chonghuan (1584-1630), os Ming foram capazes de lutar repetidamente contra os Manchus, notadamente em 1626, na Batalha de Ningyuan (na qual Nurhaci foi ferido mortalmente) e em 1628 . Sob o comando de Yuan, o exército Ming manteve segura a passagem de Shanhai, bloqueando, assim, os Manchus de atravessarem a passagem e atacarem a península Liaodong. Usando armas de fogo européias, ele foi capaz de evitar os avanços de Nurhaci ao longo do rio Liao.[145] Embora ele tenha sido nomeado marechal de todas os exércitos do nordeste em 1628, ele foi executado em 1630 em uma falsa acusação de conluio com os Manchus, uma vez que supostamente eles fantasiavam suas incursões militares.[146] Os generais sucessores revelaram-se incapazes de eliminar a ameaça manchu.
Incapazes de atacar diretamente o coração do Império Ming, os manchus aproveitaram para desenvolver suas próprias artilharias e alianças. Eles foram capazes de mobilizar os funcionários do governo Ming e os seus generais como conselheiros estratégicos. Uma grande parte do Exército Ming desertou com a ascensão manchu. Em 1632, eles haviam conquistado uma boa parte da Mongólia Interior,[145] resultando em um recrutamento em larga escala de tropas mongóis sob a bandeira manchu e na garantia de uma rota suplementar para o centro do Império Ming.
Até 1636, o governante manchu Huang-Taiji renomeou a sua dinastia de os "Últimos Jin" para "Qing" em Shenyang, que tinha caído sob domínio manchu em 1621, e lá foi feita sua capital em 1625.[145][147][148] Huang-Taiji aprovou também o título imperial chinês huangdi em vez de cã, tomou o título Imperial Chongde ("Venerada Virtude"), e mudou o nome do seu povo de Jurchen para Manchu.[148][149] Em 1638 os manchus derrotaram e conquistaram o aliado tradicional da China Ming, Joseon, com um exército de 100 000 soldados. Pouco depois dos coreanos terem renunciado à sua longa lealdade à dinastia Ming.[149]
Um soldado camponês chamado Li Zicheng (1606-1644) revoltou-se junto com seus companheiros na parte ocidental de Shaanxi, no início da década de 1630 quando o governo deixou de enviar muito dos suprimentos necessários à região.[142] Em 1634 ele foi capturado por um general Ming, e liberado apenas se ele voltasse ao serviço.[150] O acordo foi breve quando um magistrado local executou trinta e seis de seus companheiros rebeldes; as tropas de Li retaliaram matando os funcionários que participaram da execução e continuaram a liderar uma rebelião baseada em Rongyang, no centro da província de Henan em 1635.[151] Até a Década de 1640, um ex-soldado e rival de Li - Zhang Xianzhong (1606-1647) - havia criado base rebelde em Chengdu, Sichuan, enquanto o centro do poder de Li era em Hubei, estendendo sua influência sobre Henan e Shaanxi.[151]
Em 1640, massas de camponeses chineses que estavam famintos, incapazes de pagar os seus impostos, e sem medo do freqüentemente derrotado exército chinês, começaram a formar enormes bandos de rebeldes. O exército chinês, pego entre infrutíferos esforços para derrotar as incursões manchu no norte e as enormes revoltas camponesas nas províncias, naturalmente não suportou. Mal-alimentado e mal-pago, o exército foi derrotado por Li Zicheng - agora auto-denominado como o Príncipe de Shun – e arrasou a capital sem muita resistência.[152] As tropas de Li conseguiram entrar na cidade quando inesperadamente os portões foram abertos por dentro.[152] Em 26 de maio de 1644, Pequim caiu sob o exército rebelde liderado por Li Zicheng; durante o tumulto, o último imperador Ming enforcou-se em uma árvore do jardim imperial.[152]
Aproveitando a oportunidade, os Manchus atravessaram a Grande Muralha depois que o general de fronteira Ming Wu Sangui (1612-1678) abriu as portas na passagem de Shanhai. Isso ocorreu pouco depois que ele leu sobre o destino da capital e que o exército de Li Zicheng marchava em direção a ela; depois de ponderar suas opções de aliança, ele decidiu se aliar com os Manchus.[153] O exército manchu sob o comando do Príncipe Manchu Dorgon (1612-1650) e de Wu Sangui se aproximavam de Pequim depois que o exército enviado por Li Zicheng foi destruído em Shanhaiguan; o exército do Príncipe de Shun fugiu da capital em 4 de junho.[154] Em 6 de junho, os Manchus e Wu entraram na capital e proclamaram o jovem Imperador Shunzhi, o novo governante da China.[154] Após ser expulso de Xi'an pelos Manchus, e perseguido ao longo do Rio Han para Wuchang e, finalmente, ao longo da fronteira norte da província de Jiangxi, Li Zicheng  morreu no Verão de 1645, acabando assim com a dinastia Shun.[154] Um relatório diz que sua morte foi suicídio; outro afirma que ele foi espancado até a morte por camponeses depois que ele foi pego roubando comida.[154] Zhang Xianzhong foi morto em janeiro de 1647 por tropas Manchu depois que ele fugiu de Chengdu e utilizou a política da terra arrasada.[155]
Remanescentes da dinastia Ming ainda existiam após 1644, incluindo os de Koxinga. Apesar da perda de Pequim e da morte do imperador, o poder Ming não foi totalmente destruído. Nanquim, Fujian, Guangdong, Shanxi, e Yunnan foram fortalezas da resistência Ming. No entanto, houve vários pretendentes para o trono Ming, e suas forças estavam divididas. Cada baluarte de resistência foi individualmente derrotado pelos Qing até 1662, quando a última esperança real de uma renascença Ming morreu com o imperador Yongli, Zhu Youlang. Apesar da derrota Ming, pequenos movimentos leais a eles continuaram até a proclamação da República da China, em 1912. De acordo com o Almanaque de Bruxelas, ainda vivia um pretendente Ming em Pequim em 1907.[156]
Os imperadores Ming assumiu o sistema de administração provincial da dinastia Iuã, e os treze províncias Ming são os precursores das províncias moderna. Ao longo da dinastia Song, a maior divisão política foi os circuits(lu 路).[157] No entanto, após da Invasão Jurchen em 1127, o tribunal estabeleceu quatro sistema semi-autônoma depois do comando regional com base em unidades territoriais e militares, com um destacado serviço de secretariado que se tornaria a administrações provinciais Iuã, Ming e Qing.[158]
As histórias sinologistas continuam a debater a real população para cada era na dinastia Ming. As anotações do historiador Timothy Brook dizem que os censos do governo Ming são dúbios desde que as obrigações fiscais impulsionaram a muitas famílias que não reportassem o número de pessoas em suas casas e muitas contagens oficiais para mascarar o número de famílias em sua jurisdição.[160] Crianças usualmente não foram contadas, especialmente as do sexo feminino, como mostrado pela manipulação da estatística de população durante a dinastia Ming.[161] Até mulheres adultas não entravam no censo;[57] por exemplo, a prefeitura de Daming em Zhili do Norte reportou uma população de 378 167 homens e 226 982 mulheres em 1502.[162] O governo tentou revisar a forma do censo usando estimativas do número médio esperado de pessoas em cada casa, mas isto não resolveu o maior problema do registro de impostos.[163]
O número de pessoas contadas no censo de 1381 foi de 59 873 305; entretanto, este número caiu significativamente quando o governo achou que alguns dos 3 milhões de pessoas estava faltando no censo de imposto de 1391.[164] Ainda que a não reportação tenha se tornado crime capital em 1381, a necessidade de sobrevivência levou muitos a abandonar o registro para impostos e sair de suas regiões, aonde Hongwu havia tentado impor imobilidade rígida na população.[159] O governo tentou mitigar isto criando sua própria estimativa conservadora de 60 545 812 pessoas em 1393.[159] Em seus Studies on the Population of China, Ho Ping-ti sugestiona a revisão do censo de 1393 para 65 milhões de pessoas, notando que largas áreas do Norte da China e de fronteira não haviam sido contadas no censo.[165] Brook estabeleceu que a população reunida no censo oficial depois de 1393 era entre 51 e 62 milhões, enquanto a população estava de fato crescendo.[159] Até o Imperador Hongzhi (r. 1487–1505) remarcou que o aumento diário em assuntos coincidiram com a quantidade diária diminuição de civis e soldados registrados.[166] William Atwell afirma que entorno de 1400 a população da China era possivelmente de 90 milhões de pessoas, citando Heijdra e Mote.[167]
Historiadores estão agora buscando em noticiários locais da China Ming por evidências que mostrem crescimento consistente na população.[161] Usando os noticiários, Brook estima que a população total sob o poder do Imperador Chenghua (r. 1464–1487) era precisamente de 75 milhões,[163] apesar do censo da era Ming média apresentar entorno de 62 milhões.[166] Enquanto as prefeituras através do império no período Ming médio ainda reportasse uma queda ou tamanho da população estagnada, os noticiários locais reportavam enormes quantidades de trabalhadores nômades chegando sem terras o suficiente para se sustentarem, então muitos se tornariam andarilhos ou cortadores de lenha que contribuíram para a desmatamento.[168] Os imperadores Hongzhi e Zhengde aplicaram as penalidades contra aqueles que fugiram de sua cidade natal, enquanto o Imperador Jiajing (r. 1521–1567) finalmente conseguiu imigrantes oficialmente registrados em qualquer lugar que eles tivessem se mudado ou fugido de forma a trazer mais receitas.[162]
Mesmo com a reforma de Jiajing para documentar os trabalhadores e mercadores imigrantes, no final da era Ming o censo do governo ainda não refletia precisamente o grande crescimento da população. Noticiários através do império notaram isto e fizeram suas próprias estimativas do número total da população Ming, alguns acreditando que a população havia dobrado, triplicado, ou até quintuplicado desde 1368.[169] Fairbank estima que a população era possivelmente de 160 milhões no final da dinastia Ming,[170] enquanto Brook estima 175 milhões,[169] e Ebrey afirma que talvez chegasse perto de 200 milhões.[18] Entretanto, uma grande epidemia que chegou à China pelo noroeste em 1641 devastou as áreas densamente povoadas ao longo do Grande Canal; uma gazeta no norte de Zhejiang reportou que mais da metade da população ficou doente neste ano e que 90% da população local foi morta em uma única área em 1642.[171]
Em comparação com o florescimento da ciência e da tecnologia na dinastia Song, a dinastia Ming talvez tenha visto menos avanços em ciência e tecnologia em comparação com o ritmo das descobertas no mundo ocidental. De fato, os principais avanços da ciência chinesa no final da dinastia Ming foram estimulados pelo contato com a Europa. Em 1626, Johann Adam Schall von Bell escreveu o primeiro tratado chinês sobre o telescópio, o Yuanjingshuo (Vidro óptico de visão distante); em 1634, o imperador Chongzhen[172][173][174] adquiriu o telescópio do falecido Johann Schreck (1576-1630).[175][176]
Embora Shen Kuo (1031-95) e Guo Shoujing (1231-1316) tenham estabelecido as bases para a trigonometria na China, outro importante trabalho na trigonometria chinesa não seria publicado até 1607, com os esforços de Xu Guangqi e Matteo Ricci.[177] Ironicamente, algumas invenções que tiveram suas origens na China antiga[178] foram reintroduzidas na China da Europa durante o final da dinastia Ming; por exemplo, o moinho de campo.[179]
O calendário chinês estava a precisar de reforma, uma vez que inadequadamente media o ano solar em 365 ¼ dias, dando um erro de 10 min. e 14 seg. por ano ou cerca de um dia inteiro a cada 128 anos.  Embora o Ming tivesse adotado o calendário Shoushi de Guo Shoujing de 1281, que era tão preciso quanto o Calendário Gregoriano, a Diretoria Ming de Astronomia não reajustou-o periodicamente; isso talvez se devesse à falta de conhecimento, já que seus cargos públicos haviam se tornado hereditários na dinastia Ming e os estatutos da dinastia Ming proibiam o envolvimento privado na astronomia.[180] Um descendente de sexta geração do imperador Hongxi, o "príncipe" Zhu Zaiyu (1536 a 1611), apresentou uma proposta para consertar o calendário em 1595, mas a comissão astronômica ultraconservadora o rejeitou.[181][180] Xing mais tarde serviria com Xu Guangqi na reforma do calendário (崇禎 暦 書) em 1629, de acordo com os padrões ocidentais.[182]
Quando o fundador do Ming, Hongwu, descobriu os dispositivos mecânicos instalados no palácio da dinastia Iuã em Cambalique - como fontes com bolas dançando em seus jatos, tigre auto-operável, aparelhos com cabeça de dragão que emitiam névoas de perfume e relógios mecânicos na tradição de Yi Xing (683-727)[183][184] e Su Song (1020-1101) - ele associou todos eles com a decadência do domínio mongol e os destruiu.[185]
Concentrando-se na agricultura em seu Nongzheng Quanshu, o engenheiro agrônomo Xu Guangqi (1562-1633) se interessou por irrigação, fertilizantes, alívio da fome, colheitas econômicas e têxteis e observação empírica dos elementos que permitiram entender as primeiras compreensões da química.[186][187]
Houve muitos avanços e novos projetos em armas de pólvora durante o início da dinastia, mas, de meados até o final da dinastia Ming, os chineses começaram a empregar freqüentemente artilharia e armas de fogo de estilo europeu.[188] O Huolongjing, compilado por Jiao Yu e Liu Bowen em algum momento antes da morte deste, em 16 de maio de 1375[189] (com um prefácio acrescentado por Jiao em 1412),[190] apresentava muitos tipos de armas de pólvora de ponta para a época.  Isso inclui balas de canhão ocas, cheias de pólvora[191], minas terrestres que usavam um mecanismo complexo de queda de pesos, pinos e um chiado de aço para acender o trem de fusíveis,[192] minas navais,[193] foguetes montados com barbatanas para controle aerodinâmico,[194] foguetes de vários estágios por foguetes impulsionadores antes de acender um enxame de foguetes menores que saem do final do míssil (em forma de cabeça de dragão),[195] e canhões de mão que tinham até dez barris.[196]
Li Shizhen (1518-1593)[197] - um dos mais renomados farmacologistas e médicos da história da China - pertenceu ao final do período Ming.[198] Seu Bencao Gangmu é um texto médico com 1.892 verbetes, cada verbete com seu próprio nome, chamado de gangue.[199] O mu no título se refere aos sinônimos de cada nome.[199]  A inoculação, embora possa ser atribuída à antiga medicina popular chinesa, foi detalhada em textos chineses no século XVI. Ao longo da dinastia Ming, cerca de cinquenta textos foram publicados sobre o tratamento da varíola.[200]  No que diz respeito à higiene bucal, os antigos egípcios tinham uma escova de dentes primitiva de um galho raspado no final, mas os chineses foram os primeiros a inventar a moderna escova de cerdas em 1498, embora usasse pêlos duros de porco.[201]
As missões jesuíticas chinesas dos séculos XVI e XVII introduziram a ciência e a astronomia ocidentais, passando depois por sua própria revolução, para a China. Um historiador moderno escreve que, no final das cortes Ming, os jesuítas eram "considerados impressionantes, especialmente por seu conhecimento de astronomia, calendários, matemática, hidráulica e geografia".[202]  A Sociedade de Jesus introduziu, segundo Thomas Woods, "um corpo substancial de conhecimento científico e uma vasta gama de ferramentas mentais para a compreensão do universo físico, incluindo a geometria euclidiana que tornou o movimento planetário compreensível".[203][204]  Outro especialista citado por Woods disse que a revolução científica trazida pelos jesuítas coincidiu com uma época em que a ciência estava em um nível muito baixo na China:
Por outro lado, os jesuítas eram muito ativos na transmissão do conhecimento chinês para a Europa. Os trabalhos de Confúcio foram traduzidos para as línguas européias através da agência de eruditos jesuítas na China. Matteo Ricci começou a relatar os pensamentos de Confúcio e o padre Prospero Intorcetta publicou a vida e as obras de Confúcio em latim em 1687.[205] Acredita-se que tais obras tenham considerável importância para os pensadores europeus do período, particularmente entre os deístas e outros grupos filosóficos do Iluminismo que estavam interessados na integração do sistema de moralidade de Confúcio no cristianismo.
A doutrina e até mesmo o nome de "Laissez-faire" podem ter sido inspirados pelo conceito chinês de Wu wei.[206][207] No entanto, os insights econômicos do antigo pensamento político chinês tiveram pouco impacto fora da China nos séculos posteriores. Goethe, ficou conhecido como "o Confúcio de Weimar".[208]
A Grande Depressão, também conhecida como Crise de 1929, foi a maior crise financeira da história dos Estados Unidos,[2] que teve início em 1929 e persistiu ao longo da década de 1930, terminando apenas com a Segunda Guerra Mundial. A Grande Depressão é considerada o pior e o mais longo período de recessão econômica do sistema capitalista do século XX. Este período de depressão econômica causou altas taxas de desemprego, quedas drásticas do produto interno bruto de diversos países, bem como quedas drásticas na produção industrial, preços de ações, e em praticamente todo o medidor de atividade econômica, em diversos países no mundo.[3]
O dia 24 de outubro de 1929 é considerado popularmente o início da Grande Depressão, mas a produção industrial americana já havia começado a cair a partir de julho do mesmo ano, causando um período de leve recessão econômica que se estendeu até 24 de outubro, quando valores de ações na bolsa de valores de Nova Iorque, a New York Stock Exchange, caíram drasticamente, e tornou-se notícia em todo o mundo com o crash da bolsa (conhecido como Quinta-Feira Negra). Assim, milhares de acionistas perderam, literalmente da noite para o dia, grandes somas em dinheiro. Muitos perderam tudo o que tinham. Essa quebra na bolsa de valores de Nova Iorque piorou drasticamente os efeitos da recessão já existente, causando grande deflação e queda nas taxas de venda de produtos, que por sua vez obrigaram ao encerramento de inúmeras empresas comerciais e industriais, elevando assim drasticamente as taxas de desemprego. O colapso continuou no dia 28 e no dia 29 de outubro.[4]
Os efeitos da Grande Depressão foram sentidos no mundo inteiro. Estes efeitos, bem como sua intensidade, variaram de país a país. Outros países, além dos Estados Unidos, que foram duramente atingidos pela Grande Depressão foram a Alemanha, Países Baixos, Austrália, França, Itália, o Reino Unido e, especialmente, o Canadá. Porém, em certos países pouco industrializados naquela época, como a Argentina e o Brasil (que não conseguiu vender o café que tinha para outros países), a Grande Depressão acelerou o processo de industrialização. Praticamente não houve nenhum abalo na União Soviética, que, tratando-se de uma economia socialista, estava econômica e politicamente fechada para novas tecnologias. Entre 1929 e 1932, o PIB mundial caiu em cerca de 15%. Em comparação, o PIB mundial caiu em menos de 1% entre 2008 e 2009 durante a Grande Recessão.[5]
Os efeitos negativos da Grande Depressão atingiram seu ápice nos Estados Unidos em 1933. Neste ano, o Presidente americano Franklin Delano Roosevelt aprovou uma série de medidas conhecidas como New Deal.[6] Essas políticas econômicas, adotadas quase simultaneamente por Roosevelt nos Estados Unidos e por Hjalmar Schacht[7] na Alemanha foram, três anos mais tarde, racionalizadas por John Maynard Keynes em sua obra clássica.[8]
Alguns estudiosos alegam que o New Deal, juntamente com programas de ajuda social realizados por todos os estados americanos, ajudou a minimizar os efeitos da Depressão a partir de 1933, enquanto outros pesquisadores discordam dessa visão.[9][10] A maioria dos países atingidos pela Grande Depressão passaram a recuperar-se economicamente a partir de então. Em alguns países, a Grande Depressão foi um dos fatores primários que ajudaram a ascensão de regimes ditatoriais, como os nazistas comandados por Adolf Hitler na Alemanha. O início da Segunda Guerra Mundial terminou com qualquer efeito remanescente da Grande Depressão nos principais países atingidos, muito embora vários economistas neoclássicos discordem disso.[11][12][13][14]
Com o fim da Primeira Guerra Mundial, os países europeus encontravam-se devastados, com a economia enfraquecida e com forte retração de consumo, que abalou a economia mundial. Os Estados Unidos por sua vez, lucraram com a exportação de alimentos e produtos industrializados aos países aliados no período entreguerras. Como resultado disso, entre 1918 e 1928 a produção norte-americana cresceu de forma estupenda. A prosperidade econômica gerou o chamado "American way of life" (modo de vida americano). Havia emprego, os preços caíam, a agricultura produzia muito e o consumo era incentivado pela expansão do crédito e pelo parcelamento do pagamento de mercadorias. Porém, a economia europeia posteriormente se restabeleceu e passou a importar cada vez menos dos Estados Unidos. Com a retração do consumo na Europa, as indústrias norte-americanas não tinham mais para quem vender. Havia mais mercadorias que consumidores, ou seja, a oferta era maior que a demanda; consequentemente os preços caíram, a produção diminuiu e logo o desemprego aumentou. A queda dos lucros, a retração geral da produção industrial e a paralisação do comércio resultou na queda das ações da bolsa de valores e mais tarde na quebra da bolsa. Portanto, a crise de 1929 foi uma crise de superprodução.[15] Tanto Ford quanto Keynes alertaram, antes da Crise de 1929, que "a aceleração dos ganhos de produtividade provocada pela revolução taylorista levaria a uma gigantesca crise de superprodução se não fosse encontrada uma contrapartida em uma revolução paralela do lado da demanda", que permitisse a redistribuição dos ganhos de produtividade causados pelo taylorismo, de forma que houvesse redistribuição dessa nova renda gerada, para dirigi-la ao consumo. [16]
Durante décadas, essa foi a teoria mais aceita para a causa da Grande Depressão, porém, em contrapartida, economistas, historiadores e cientistas políticos têm criado diversas outras teorias para a causa, ou causas, da Grande Depressão, com surpreendente pouco consenso. A Grande Depressão permanece como um dos eventos mais estudados da história da economia mundial. Teorias primárias incluem a quebra da bolsa de valores de 1929, a decisão de Winston Churchill em fazer com que o Reino Unido passasse a usar novamente o padrão-ouro em 1925, que causou maciça deflação ao longo do Império Britânico, o colapso do comércio internacional, a aprovação do Ato da Tarifa Smoot-Hawley, que aumentou os impostos de cerca de 20 mil produtos no país, a política da Reserva Federal dos Estados Unidos da América, e outras influências.
Por outro lado, o economista Milton Friedman culpou a política monetária equivocada como a causa da Grande Depressão. Segundo Friedman, a  autoridade monetária norte-americana permitiu que o suprimento de dinheiro diminuísse em um terço entre 1929 e 1933.[17] O aperto da política monetária foi seguida pela queda dos preços e pela atividade econômica mais fraca: "Durante os dois meses desde o pico cíclico de agosto de 1929 até o colapso, a produção, os preços no atacado e a renda pessoal caíram a taxas anuais de 20%, 7,5% e 5%, respectivamente.".[18] 
Em direção oposta da Corrente dominante no mundo acadêmico, o economista Thomas Sowell analisou as estatísticas oficiais de desemprego nos Estados Unidos e culpou a interferência governamental na economia pela Grande Depressão. Segundo Sowell, a taxa de desemprego nos Estados Unidos somente alcançou dois dígitos em 1932, tendo se mantido estável durante três anos, entre 1929 e 1931. Para Sowell, apenas quando o presidente Herbert Hoover adotou uma política agressiva de aumento dos gastos públicos e do protecionismo visando a reeleição na eleição de 1932, que a Grande Depressão veio de fato a ocorrer.[9] Sowell alega que as estatísticas desmentem a narrativa dominante de que as políticas conhecidas como New Deal, aprovadas pelo presidente norte-americano Franklin Delano Roosevelt, terminaram com a Grande Depressão, alegando que o desemprego continuou crescendo durante todo os anos 30. Sowell conclui que o New Deal, na verdade, prolongou a Grande Depressão.[19] Concordando com essa visão, Peter Ferrara, alega que a Grande Depressão só terminaria com a final da Segunda Guerra Mundial, que teria levado a uma acentuada redução de gastos, impostos e regulamentações governamentais.[10]
Com a quebra da Bolsa de Valores de Nova Iorque de 1929, bancos e investidores perderam grandes somas em dinheiro. A situação dos bancos era agravada pelo fato que muitos destes bancos haviam emprestado grandes somas de dinheiro a fazendeiros. Após o início da Grande Depressão, porém, estes fazendeiros tornaram-se incapazes de pagar suas dívidas. Isto, por sua vez, causou a queda dos lucros destas instituições financeiras. Pessoas que utilizavam-se de bancos, temendo uma possível falência destes, removeram destes os seus fundos. Assim, várias instituições bancárias foram fechadas. O total de instituições bancárias fechadas durante a década de 1920 e de 1930 foi de 14 mil, um índice astronómico.
Em 17 de maio de 1930, o governo dos Estados Unidos aprovou uma lei, o Ato Tarifário Smoot-Hawley, que aumentava as tarifas alfandegárias em cerca de 20 mil itens não-perecíveis estrangeiros.[20] O Presidente americano Herbert Hoover pedira ao Congresso uma diminuição nos impostos, mas o Congresso, ao invés disto, votou a favor do aumento dos impostos. Um abaixo-assinado, assinado por mil economistas, pediu ao presidente americano para rejeitar este aumento. Apesar disto, Hoover assinou o Ato em 17 de maio. O Congresso e o Presidente acreditavam que isto iria reduzir a competição de produtos estrangeiros no país. Porém, outros países reagiram através da aprovação de leis e atos semelhantes, assim causando uma queda súbita nas exportações americanas. As taxas de desemprego subiram de 9% em 1930 para 16% em 1931, e 25% em 1933. Durante a década de 1930, a taxa de desemprego nos Estados Unidos não retornaria mais às taxas de 9% de 1930, se mantendo em perto da casa dos 20%.
Com o crescente encerramento de instituições bancárias, menos fundos estavam disponíveis no mercado americano, fazendo com que a produção industrial americana continuasse a cair. Em 1929, o valor total dos produtos industrializados fabricados nos Estados Unidos foi de 104 bilhões de dólares. Em 1933, este valor havia caído para 56 bilhões, uma queda de aproximadamente 45%. A produção de aço caiu em cerca de 61%, entre 1929 e 1933, e a produção de automóveis caiu em cerca de 70% no mesmo período.
O ano de 1933 foi o ápice da Grande Depressão nos Estados Unidos. As taxas de desemprego eram de 25% (ou um quarto de toda a força de trabalho americana). Cerca de 30% dos trabalhadores que continuaram nos seus empregos foram obrigados a aceitar reduções em seus salários, embora grande parte dos trabalhadores empregados tenham tido um aumento nos seus salários por hora e o nível de desigualdade social daquela sociedade estivesse abaixo da sociedade americana do século XXI.[21][22] Outro problema enfrentado foi a grande deflação - queda do preço dos produtos e custo de vida em geral. Entre 1929 e 1933, os preços dos produtos industrializados não-perecíveis em geral nos Estados Unidos caíram em cerca de 25%. Já o preço de produtos agropecuários caiu em cerca de 50%, por causa do excedente da produção destes produtos - primariamente trigo. A quantidade destes produtos à venda excedia largamente a demanda, o que causou uma queda dos preços destes produtos. Os baixos preços levaram ao endividamento de muitos fazendeiros. Era comum casos de suicídio por parte de empresários, acionistas e investidores em geral, que haviam perdido tudo o que possuíam; E também por parte de outros civis, que, com a crise, se haviam endividado e/ou não possuíam forma alguma de sustento devido ao fato de estarem desempregados.
O presidente americano Herbert Hoover acreditava que o comércio, se não supervisionado pelo governo, iria, em algum momento, minimizar os efeitos da recessão econômica. Eventualmente, Hoover acreditava que a economia dos Estados Unidos iria recuperar-se, sem que a intervenção do Governo fosse necessária. Hoover rejeitou diversas leis aprovadas pelo Congresso, alegando que davam ao governo americano poderes demais sobre o mercado.[23]
Hoover também acreditava que os governos dos estados americanos deveriam ajudar os necessitados. Muitos destes estados, porém, não tinham fundos suficientes para tal. Assim sendo, Hoover propôs a criação de um órgão governamental, o Reconstruction Finance Corporation (Corporação de Reconstrução Financeira), ou RFC, em 1932. Este órgão seria responsável por fornecer alguma ajuda financeira a empresas e instituições comerciais e industriais chave, como bancos, ferrovias e grandes empresas, acreditando que a falência destas instituições agravaria o efeito da Grande Depressão. No final de 1932, as eleições presidenciais foram realizadas. Os dois principais candidatos foram Hoover e Franklin Delano Roosevelt. Muitos da população americana acreditavam que Hoover fora o principal causador da recessão, e/ou que pouco fizera para solucionar esta recessão. Roosevelt saiu vencedor da eleição, tornando-se presidente dos Estados Unidos em 4 de março de 1933.
Roosevelt, ao contrário de Hoover, acreditava que o governo americano era o principal responsável para lutar contra os efeitos da Grande Depressão. Em uma sessão legislativa especial, sessão conhecida como Hundred Days ("Cem Dias"), Roosevelt, juntamente com o congresso americano, criaram e aprovaram uma série de leis que, por insistência do próprio Roosevelt, foram nomeadas de New Deal ("Novo Acordo"). Estas leis forneceriam ajuda social às famílias e pessoas que necessitassem, forneceriam empregos através de parcerias entre o governo, empresas e os consumidores, e reformou o sistema econômico e governamental americano, de modo a evitar que uma recessão deste gênero ocorresse futuramente.[6]
Diversas agências governamentais foram criadas para administrar os programas de ajuda social. A mais importante delas foi a Federal Agency Relief Administration, criada em 1933, que seria responsável pelo fornecimento de fundos aos governos estatais, para que estes empregassem tais fundos em programas de ajuda social. Outros órgãos governamentais similares foram criados com o intuito de fundear, administrar e/ou empregar trabalhadores na área de construção de aeroportos, escolas, hospitais, pontes e represas. Estes projetos federais forneceram milhões de empregos aos necessitados, embora as taxas de desemprego continuassem altas durante toda a década de 1930.
Outros órgãos foram criados com o intuito de administrar programas de recuperação, como a Agricultural Adjustment Administration, criada em 1933 com o intuito de regular a produção de produtos agropecuários em uma dada fazenda. Outro órgão similar, o National Recovery Administration, criada em 1933, passou a fazer cumprir as leis antimonopólio, estabeleceu salários mínimos e limites na carga horária de trabalho. Esta última agência, porém, foi fechada a mando do Congresso, em 1935, por pouco estimular o comércio americano.
Por fim, outros órgãos federais foram criados com o intuito de supervisionar reformas trabalhistas e financeiras. O Federal Deposit Insurance Corporation foi criado em 1933 com o intuito de promover transações e o comércio bancário. O Securities and Exchange Commission, criado em 1934, regulava o comércio de bolsa de valores e evitava que acionistas comprassem ações que o órgão considerassem "perigosas". O National Labor Relations Board foi criado em 1935, com o intuito de regular sindicatos, e de proteger os trabalhadores e seus direitos. Ainda em 1935, um ato do governo americano, o Ato da Segurança Civil passou a fornecer pensões mensais para aposentados, bem como ajuda financeira regular por um certo período de tempo, para pessoas desempregadas.
A economia americana gradualmente, mas lentamente, passou a recuperar-se, desde 1933. O governo americano também diminuiu as tarifas alfandegárias em certos produtos estrangeiros, assim estimulando o comércio doméstico. Ao longo da década de 1930, os Estados Unidos gradualmente abandonaram o uso da Cláusula Ouro, decidindo ao invés disso, fortalecer a moeda nacional, o dólar, o que também ajudou na recuperação da economia americana. A produção de comodidades, tais como automóveis, voltaria aos patamares de 1929, porém, somente após o fim da guerra, como a produção de automóveis, por exemplo 1949 - a maior parte da matéria-prima na época possuía prioridade pela indústria bélica nacional.
Porém, apesar dos programas governamentais criados com o intuito de reduzir o desemprego, cerca de 15% da força de trabalho americana continuava desempregada em 1940. Foi necessária a entrada do país na Segunda Guerra Mundial para que as taxas de desemprego caíssem aos níveis de 1930, de 9%. Apesar da entrada na Segunda Guerra Mundial ter atrasado a recuperação econômica por 7 anos, a venda de armas para os ingleses acelerou a recuperação econômica[24] e a produção industrial americana cresceu drasticamente, e as taxas de desemprego caíram. No final da guerra, apenas 1% da força de trabalho americana estava desempregada. O gasto do governo norte-americano da Segunda Guerra Mundial foi de 4,1 trilhões de dólares em valores correntes de 2011.[25]
Perto do final da guerra, os Estados Unidos e todos os outros 44 países Aliados assinaram o que é conhecido como os Acordos de Bretton Woods, com o intuito de evitar futuramente uma nova crise monetária e econômica da escala da Grande Depressão.
A Grande Depressão causou grande recessão econômica em diversos outros países, não só nos Estados Unidos. Em muito destes países, a recessão provocada pela Grande Depressão gerou efeitos similares na economia destes países, como o fechamento de  milhares de estabelecimentos bancários, financeiros, comerciais e industriais, e a demissão de milhares de trabalhadores.
Os efeitos da Grande Depressão em vários países foram agravados pelo Ato Tarifário Smoot-Hawley, um ato americano introduzido em 1930, que aumentava impostos a cerca de 20 mil produtos não-perecíveis estrangeiros, que causou a aprovação de leis e atos semelhantes em outros países, reduzindo drasticamente exportações e o comércio internacional.[20]
Em vários dos países afetados, partidos políticos extremistas, de caráter nacionalista, apareceram. Outros partidos políticos, de cunho comunista, também foram criados. No Reino Unido, por exemplo, tanto o Partido Comunista da Grã-Bretanha quanto a União Britânica de Fascistas receberam considerável suporte popular. O mesmo ocorreu com o Partido Comunista canadense.
Outros partidos políticos menos extremistas também surgiram. A grande maioria, se não todos, prometiam retirar o país (ou uma dada província/estado) da recessão. O Partido do Crédito Social do Canadá, de cunho conservador ganhou grande suporte popular em Alberta, província canadense severamente afetada pela Grande Depressão. Em alguns destes países, partidos extremistas foram proibidos, como no Canadá. Outros partidos políticos extremistas, porém, conseguiram chegar ao poder, notavelmente os nazistas na Alemanha e os fascistas na Itália.
A Alemanha foi derrotada pela Tríplice Entente na Primeira Guerra Mundial. A Entente cobrou pesadas indenizações de guerra por parte dos alemães - que em dólares americanos atuais seriam da ordem dos trilhões de dólares - entre outras pesadas punições impostas pelo Tratado de Versalhes. Começa então o período da história alemã chamado por historiadores como República de Weimar. Os anos da década de 1920 foram caracterizadas por hiperinflação em 1923 e o grande aumento da dívida externa do país entre 1925 e 1930.
Quando a Grande Depressão teve início em 1929, o governo alemão acreditou que cortes em gastos públicos iriam estimular o crescimento econômico do país, assim cortando drasticamente gastos estatais, incluindo no setor social. O governo alemão esperava e acreditava que a recessão, inicialmente, iria deteriorar a Alemanha socioeconomicamente, esperando com o tempo, porém, a melhoria da estrutura socioeconômica do país, sem intervenção do governo. A República de Weimar cortou completamente todos os fundos públicos ao programa de ajuda social para desempregados - o que resultou em maiores contribuições pelos trabalhadores e menores benefícios aos desempregados - entre outros cortes no setor social. A recessão, com seu auge em 1932, trouxe à nação um índice altíssimo de desemprego, que beirava os 45%. O Marco alemão decresceu a valores quase insignificantes, por volta de um milionésimo. Portanto, o alemão médio que havia poupado riquezas por um longo prazo agora só poderia usá-las para tomar um café. [26]
Com uma classe baixa e média desempregadas e extremamente descontentes o país se afundou na desordem.  A República de Weimar tinha perdido toda sua credibilidade frente à população, principalmente os alemães médios, que tinham tido os seus tradicionais e viçosos valores feridos. Este foi mais um fator que propiciou o surgimento das ditas ultradireitas nacionais e, ao mesmo tempo, um facilitador à ascensão do Führer Adolf Hitler no governo do país, em 1933, marcando o fim da República de Weimar e o início de um período de crescimento socioeconômico alemão, conhecido como III Reich.[27]
A dependência da Austrália de exportações agrícolas e industriais fez com esse que fosse um dos países desenvolvidos mais duramente atingidos.[28] A queda na demanda de exportação e os preços das commodities diminuíram maciçamente os salários. O desemprego atingiu um recorde de 29% em 1932,[29] com incidentes de agitação civil se tornando comuns.[30] Após 1932, um aumento nos preços da lã e da carne levou a uma recuperação gradual.[31]
A crise de 1929 afetou também o Brasil. Os Estados Unidos eram o maior comprador do café brasileiro. Com a crise, a importação deste produto diminuiu muito e os preços do café brasileiro caíram. Para que não houvesse uma desvalorização excessiva, o governo brasileiro comprou e queimou toneladas de café. Os cafeicultores se recusavam a diminuir o plantio, em uma atitude muito semelhante a  superprodução estadunidense que gerou a crise. Para comprar o café, o Governo Brasileiro mergulhou em uma rotina de empréstimos - pegos em uma época em que a taxa de juros, por conta da Grande Depressão, era altíssima. Desta forma, diminuiu a oferta, conseguindo manter o preço do principal produto brasileiro da época. Enquanto o Estado Brasileiro se endividava, os cafeicultores investiam no setor industrial, desenvolvendo fracamente a indústria brasileira, já que não eram guiados por um Projeto de Desenvolvimento Nacional.
Entre a década de 1900 e a década de 1920, o Canadá possuía a economia em mais rápido crescimento do mundo, tendo passado por apenas um período de recessão após a Primeira Guerra Mundial. Ao contrário dos Estados Unidos, onde o crescimento exuberante da economia americana era em grande parte apenas ilusório, a economia do Canadá prosperou verdadeiramente durante a década de 1920. Enquanto a indústria imobiliária dos Estados Unidos havia estagnado em volta de 1925, esta indústria continuou forte no Canadá até maio de 1929. O mesmo podia se dizer da indústria agropecuária, que ao longo da década de 1920 esteve em pleno crescimento no Canadá, enquanto nos Estados Unidos este setor entrara em recessão econômica.
O principal produto de exportação do Canadá, à época, era o trigo. Este produto era então um dos pilares da economia do país. Em 1922, o Canadá era o maior exportador de trigo do mundo, e Montréal era o maior centro portuário exportador de trigo do mundo. Entre 1922 e 1929, o Canadá foi responsável por 40% de todo o trigo comercializado no mundo. As exportações de trigo ajudaram a fazer do Canadá um dos líderes mundiais do comércio internacional, com mais de um terço de seu produto interno bruto tendo origem no comércio internacional.
O sucesso do trigo canadense era baseado, porém, em problemas que afligiam outros países no mundo. A Primeira Guerra Mundial devastou a produção agropecuária dos países europeus. Mais importante foi, porém, a Revolução Russa de 1917, que manteve o trigo russo fora do mercado mundial. Em torno de 1925, a gradual recuperação da economia e da agropecuária da Europa Ocidental, bem como a NEP (Nova Política Econômica) na Rússia, fez com que a produção mundial de trigo aumentasse no mundo, assim diminuindo os preços do produto. Esperando por um rápido retorno aos altos preços, os fazendeiros e comerciantes canadenses estocaram muito de seu trigo, ao invés de reduzirem sua produção. A introdução de maquinário, especialmente o trator, levou ao crescimento da produção de trigo tanto no Canadá quanto nos Estados Unidos. Todos estes fatores em conjunto desencadearam um colapso dos preços do trigo em junho de 1929, destruindo a economia de Alberta, Saskatchewan e Manitoba, e afetando severamente a economia de Ontário e Quebec.
À parte dos Estados Unidos, o Canadá foi o país mais duramente atingido pela Grande Depressão. O Canadá, ainda oficialmente parte do Império Britânico, usava ativamente o padrão-ouro. Isto, aliado com os estreitos laços econômicos existentes entre o Canadá e os Estados Unidos (muito dos produtos fabricados no Canadá eram exportados para os Estados Unidos, por exemplo), fez com que o colapso da economia americana após a quebra da Bolsa de Valores de Nova Iorque rapidamente afetasse o Canadá. O colapso econômico canadense é considerado o segundo mais acentuado da Grande Depressão, atrás somente do colapso da economia dos próprios Estados Unidos.
A economia do Canadá também dependia da exportação de certos produtos industrializados tais como automóveis. Com a Grande Depressão, as exportações canadenses aos Estados Unidos caíram drasticamente. O colapso dos preços do trigo fizeram com que muitos fazendeiros canadense endividassem-se pesadamente. Os fazendeiros no Alberta e no Saskatchewan sofreram, além disso, com grandes períodos de seca e de constante ataque de pragas tais como enxames de gafanhotos. A queda na produção industrial canadense, por sua vez, significou a demissão de grandes quantidades de trabalhadores.
A economia do Canadá tinha algumas vantagens sobre outros países, especialmente seu sistema bancário extremamente estável. Antes e ao longo da Grande Depressão, apenas um único estabelecimento bancário canadense faliu, em comparação aos nove mil que faliram somente ao longo da Grande Depressão. A economia do Canadá foi atingida duramente pela Grande Depressão primariamente por causa de sua dependência em relação ao trigo e produtos industrializados, mas também por causa da dependência da economia do canadense em relação às exportações de produtos canadenses para os Estados Unidos. A primeira reação de vários países, incluindo os Estados Unidos, quando a Grande Depressão teve início, foi de aumentar impostos. Isto causou mais danos à economia do Canadá do que para outros países no mundo.
Richard Bedford Bennett, que atuou como Primeiro-ministro do Canadá entre 1930 e 1935, tentou minimizar os efeitos da Grande Depressão no país, inclusive, através da introdução de uma New Deal semelhante aos dos Estados Unidos, implementado em 1934. Porém, a economia do país continuou mal e somente passou a recuperar-se muito lentamente a partir de 1934.
Em 1933, 30% da força de trabalho canadense estava desempregado, deflação ocorreu, reduzindo salários e preços de produtos e reduziu investimentos. Em 1932, a produção industrial canadense havia caído para 58%, em relação à produção industrial em 1929. Enquanto isto, o PIB canadense havia caído em cerca de 42%, em relação ao PIB do país em 1929. Apesar de ter passado por um período de curto e pequeno crescimento econômico entre 1934 e 1937 - que nem de longe foi suficiente para atenuar os efeitos causados pela Depressão - a economia do Canadá entrou novamente em uma grande recessão em 1937. Foi somente com a entrada do país na Segunda Guerra Mundial, em 1939, que os efeitos da Grande Depressão teriam fim no país.
A Liga das Nações classificou o Chile como o país mais atingido pela Grande Depressão, uma vez que 80% da receita do governo vinha das exportações de cobre e nitratos, que estavam em baixa demanda.[32] Inicialmente, o Chile sentiu o impacto da Grande Depressão em 1930, quando o PIB caiu 14%, a renda da mineração caiu 27% e a receita de exportação caiu 28%. Em 1932, o PIB havia encolhido para menos da metade do que tinha sido em 1929, cobrando uma taxa terrível de desemprego e fracassos nos negócios.
A crise fez com que o regime autoritário de Carlos Ibáñez del Campo caísse em julho de 1931, seguido por uma sucessão de governos de curta duração até a eleição de Arturo Alessandri em dezembro de 1932.[32] A crise econômica aumentou os níveis de desemprego e causou uma migração de mineiros de salitre desempregados do norte para Santiago. Os mineiros constituíam cerca de 6% da população economicamente ativa, mas eram mais da metade dos desempregados durante a crise.[33] Várias cozinhas sociais surgiram em Santiago, enquanto pessoas desabrigadas começaram a morar em cavernas nas colinas ao redor da capital chilena.[32] 
O estado respondeu à crise aumentando gradualmente as tarifas, aumentando a demanda interna e aumentando o controle sobre o "fluxo e uso" de moeda estrangeira.[34][35] Quotas e licenças foram estabelecidas para importações e a conversibilidade do ouro foi novamente abolida em 1931.[33][34] Estas políticas contribuíram para uma recuperação industrial e para a indústria já em 1934 ultrapassaram os níveis de atividade de 1929.[35] Na década de 1930, o crescimento industrial massivo foi liderado pela indústria têxtil, mas a mineração não-metálica, as indústrias químicas e as fábricas de máquinas e transporte também se expandiram.[35]
Impelido em parte pelo devastador terremoto de 1939 em Chillán, o governo da Frente Popular de Pedro Aguirre Cerda criou a Corporação de Desenvolvimento da Produção (CORFO) para incentivar, com subsídios e investimentos diretos, um programa ambicioso de industrialização por substituição de importações. Consequentemente, como em outros países latino-americanos, o protecionismo tornou-se um aspecto arraigado da economia chilena. Influenciados profundamente pela Grande Depressão, muitos líderes nacionais promoveram o desenvolvimento da indústria local em um esforço para isolar a economia dos futuros choques externos. Após seis anos de medidas de austeridade do governo, que conseguiram restabelecer a credibilidade do Chile, os chilenos elegeram para o poder durante o período de 1938-58 uma sucessão de governos de centro e de centro-esquerda interessados ​​em promover o crescimento econômico por meio de intervenção do governo.
O Reino Unido saiu vencedor na Primeira Guerra Mundial. Porém, a guerra e a destruição causada pela última destruíram a economia britânica. Desde 1921, a economia do Reino Unido lentamente recuperou-se da guerra, e da recessão causada por esta. Mas em abril de 1925, o chanceler britânico Winston Churchill, respondendo a um conselho do Banco da Inglaterra, fixou o valor da moeda nacional ao padrão-ouro, à taxa pré-guerra, de 4,86 dólares. Isto fez o valor da moeda britânica convertível ao seu valor em ouro, mas causou também o encarecimento dos produtos exportados pelo Reino Unido a outros países. A recuperação econômica do Reino Unido caiu drasticamente, o que causou redução de salários no país inteiro, debilitando a economia nacional.
Quando a Grande Depressão teve início nos Estados Unidos, em 1929, diversos países no mundo inteiro criaram ou aumentaram tarifas alfandegárias, o que causou uma grande diminuição nas exportações de produtos britânicos. A taxa de desemprego saltou de 8% para 20% no final de 1930. O Reino Unido cortou gastos públicos - que incluíram fundos dados para programas de ajuda social aos desempregados. Em 1931, mais cortes em salários e programas de ajuda social foram realizadas, e o imposto de renda nacional, foi aumentado. Estas medidas somente pioraram a situação socioeconômica do país, e em 1932, ápice da Grande Depressão no Reino Unido, as taxas de desemprego eram de 25%. Foi somente com o abandono do padrão-ouro e a instalação de tarifas alfandegárias para produtos importados de qualquer país que não fossem parte do Império Britânico, que a economia britânica passou a gradualmente recuperar-se.
A NEP (Nova Política Econômica), posta em vigor por Lenin em 1922, foi extinta em 1928 por Stalin. A União Soviética era o único país socialista do mundo, com muito pouco comércio internacional, sua economia estava isolada, entretanto, foi apenas ligeiramente afetada pela Grande Depressão.[36] A transição forçada de uma sociedade rural para uma sociedade industrial resultou na criação de uma indústria pesada, à custa de milhões de vidas na Rússia e Ucrânia rurais.[37] Durante a Depressão, a economia soviética foi crescendo de forma constante, alimentada pelo investimento maciço na indústria pesada.
Neste período, os soviéticos adquiriram novas tecnologias do exterior. Muitos economistas e investidores, curiosos, foram ao país como turistas para observar a estruturação econômica da União. O próprio termo "plano econômico", até hoje propagado pelo status quo, foi designado primeiramente por Stalin (Planos Quinquenais). Os planos econômicos foram muito importantes posteriormente na II Grande Guerra, pois por essa via as grandes potências puderam se manter estáveis durante o período. [38]
A Amtorg Trading Corporation, representação comercial soviética nos EUA, recrutou trabalhadores especializados americanos, afetados pela crise, atraindo-os com promessas de boas condições de trabalho e bons salários. [39] A Ford Motor Company instalou-se na URSS e enviou técnicos, o que resultou numa transferência de tecnologia para o país.[40] O Grande Expurgo iniciado em 1934, vitimou também estrangeiros como os trabalhadores especializados americanos, essenciais para a modernização da indústria soviética e que eram dispensados quando não mais necessários, raramente conseguindo retornar aos EUA.[39] A grande maioria foi encarcerada em Gulags.[39] Victor Herman foi um dos poucos americanos sobreviventes do expurgo de Stalin.[41] [42] Permaneceu detido na URSS por mais de quatro décadas e, em seu retorno aos EUA, processou a Ford Motor Company em dez milhões de dólares por considerar a empresa responsável por todo o sofrimento pelo qual passou. A morte de Herman encerrou o processo.[43]
O sucesso econômico aparente da União Soviética num momento em que o mundo capitalista estava em crise, levou muitos intelectuais do ocidente a enxergarem o sistema soviético favoravelmente. Jennifer Burns escreveu:
Na França, a Grande Depressão atingiu o país um pouco mais tardiamente do que outros países, em torno de 1931. Como o Reino Unido, a França estava ainda recuperando-se da Primeira Guerra Mundial, tentando sem muito sucesso recuperar os pagamentos que possuía direito da Alemanha. Isto levou à ocupação do vale do Ruhr por forças francesas no início da década de 1920. A ocupação francesa do Ruhr não fez com que a Alemanha retomasse os seus pagamentos, levando à implementação do Plano Dawes em 1924, e do Plano Young em 1929. Porém, a Grande Depressão teve drásticos efeitos na economia local, e explica em parte os motins de 6 de fevereiro de 1934 e a formação da Frente Popular, liderada pelo socialista Léon Blum, que venceu as eleições de 1936.
Por causa da Grande Depressão, o comércio internacional de produtos caiu drasticamente. A Austrália, que dependia da exportação de trigo e algodão, foi um dos países mais severamente atingidos pela Depressão no Mundo Ocidental. A taxa de desemprego alcançou um recorde de 29% em 1932, uma das mais altas do mundo até os dias atuais. As exportações de produtos agrários e minérios, tais como café, trigo e cobre, de países da América Latina, caiu de 1,2 bilhão de dólares em 1930 para 335 milhões de dólares em 1933, aumentando para 660 milhões de dólares em 1940. Os efeitos da crise fizeram com que em alguns destes países, muitos agricultores passassem a investir seu capital na manufatura, causando a industrialização destes países, em especial, a Argentina e o Brasil. Neste segundo país, aliás, a industrialização se acelerou com a perda de poder político dos cafeicultores do estado de São Paulo, fenômeno consolidado com a vitoriosa Revolução de 1930.[45] 
A Ásia também foi afetada negativamente com a Grande Depressão, por causa da dependência da economia de diversos países asiáticos em relação à exportação de produtos agrários à Europa e à América do Norte. O comércio internacional asiático caiu drasticamente, na medida em que os Estados Unidos e a Europa foram cercadas pela recessão. Houve uma vasta queda de preços de produtos como lã e arroz, enquanto que a sua produção se multiplicava como forma de reação à queda nos preços. Isso fez com que instalações comerciais e industriais asiáticas respondessem através de demissões e redução nos salários. O PIB do Império do Japão, com uma base industrial em crescimento, sofreu uma queda de 8% entre 1929 e 1930. As taxas de desemprego e de pobreza cresceram drasticamente, afetando desproporcionalmente as classes inferiores. Esta foi uma das causas da ascensão do nacionalismo japonês (ver: fascismo japonês ). O Japão recuperou-se da crise em 1932.
A pobreza e o desemprego cresceram muito em razão da crise de 1929.[46] Por isso, a maior parte da população dos países mais afetados pela Grande Depressão cortou todo e qualquer tipo de gasto considerado supérfluo, agravando os efeitos da recessão na economia destes países.
Por causa da Grande Depressão, milhões de pessoas nas cidades perderam seus empregos, nos países mais atingidos pela recessão. Sem fonte de renda, estas pessoas não tinham mais como sustentar a si próprios e suas famílias. A maioria das residências destas famílias, por sua vez, eram alugadas ou, ainda estavam sendo pagas através de prestações. Como consequência, milhares de famílias eventualmente foram expulsas de suas residências, por não terem como pagar os aluguéis ou as prestações de sua casa. Além disso, o desemprego fez com que a subnutrição tornasse-se comum entre a população dos países mais atingidos. Milhares de pessoas morreram por causa da subnutrição.
Algumas pessoas e famílias sem fonte de renda mudaram-se para a residência de parentes, quando perdiam suas residências. A maioria destas famílias, porém, instalou-se em favelas. Abrigos rústicos feitos com telas de metais, madeira e papelão tornaram-se comuns em áreas das grandes cidades dos países mais atingidos. As condições de vida nestas favelas eram precárias.
A indústria agropecuária de diversos países - especialmente os Estados Unidos e o Canadá - foi duramente atingida pela Grande Depressão. Nos Estados Unidos, muitos fazendeiros endividaram-se pesadamente, e vários foram forçados a ceder suas terras para instituições bancárias. Na Califórnia, no centro-norte dos Estados Unidos e no centro-oeste do Canadá, grandes períodos de seca, invernos rigorosos e pestes agravaram a recessão econômica já existente nestas regiões. Muitos dos jovens das áreas rurais abandonaram suas fazendas e suas famílias, e buscaram a sorte nas cidades. Estas pessoas, juntamente com muitas das pessoas desempregadas nas cidades, viajavam de cidade a cidade, pegando carona em trens de carga, em busca de emprego. Esta foi uma cena muito comum nos Estados Unidos e no Canadá.
Os chefes de estado e outras pessoas importantes dos países atingidos passaram a ser frequentemente considerados diretamente culpados pelo início da Grande Depressão por muito da população atingida pela recessão. As favelas dos Estados Unidos foram apelidadas de Hoovervilles ou "Vilas do Hoover" em português, em uma sátira da população americana ao presidente Herbert Hoover. No Canadá, muitos donos de automóveis apelidaram seus veículos de Bennett Buggies - Carroças Bennett - em uma sátira ao Primeiro-Ministro Richard Bennett. Isto porque estas pessoas não tinham como adquirir o combustível necessário para abastecer seus veículos, ou cortaram a compra de combustível por considerarem um gasto supérfluo. Estes veículos passaram a ser usados como carroças, puxados por cavalos ou outros equinos.
Nem todas as pessoas sofreram igualmente com a Grande Depressão. Para pessoas que conseguiram manter seus empregos (mesmo nos países mais afetados pela recessão), ou que dispunham de uma poupança considerável, o padrão de vida não mudou muito. Apesar que muitos trabalhadores sofreram cortes consideráveis em seus salários, a deflação fez com que os preços de produtos em geral caíssem drasticamente. Ao longo da Grande Depressão, os preços da maioria dos produtos de consumo mantiveram-se muito baixo nos países mais afetados.
Por outro lado, muitos afro-americanos nos Estados Unidos não conseguiam emprego, especialmente no sul americano, por causa de discriminação racial. Empregos eram dados primariamente aos brancos. Por isto, em todo Estados Unidos, a taxa de desemprego entre a população afro-americana foi muito maior do que o da população branca. Mulheres com famílias para sustentar também dificilmente encontravam empregos, uma vez que a prioridade era dada para trabalhadores do sexo masculino, e que a discriminação contra mulheres trabalhadoras aumentou.
Grupos étnicos minoritários - especialmente imigrantes - dos países mais atingidos passaram a ser discriminados por muitos da população dos países mais afetados. Estes grupos étnicos eram discriminados porque, na visão de várias pessoas dos países afetados pela Grande Depressão, estes grupos étnicos competiam com a "população nativa" dos países atingidos por empregos. Isto, aliado à forte recessão econômica da década de 1930, fez com que as taxas de imigração caíssem sensivelmente no Canadá e nos Estados Unidos.
Após o fim da Grande Depressão, muitos dos países mais severamente atingidos passaram a fornecer maior assistência social e econômica aos necessitados. Por exemplo, o New Deal dava ao governo americano maior poder para fornecer esta ajuda para estes necessitados e também  para aposentados.
A Grande Depressão gerou grandes mudanças na política econômica em vários dos países envolvidos. Anteriormente à Grande Depressão, por exemplo, o governo dos Estados Unidos pouco intervinha na economia do país. Executivos financeiros e grandes magnatas comerciantes eram vistos como líderes nacionais. 
Por outro lado, a Grande Depressão deu origem a uma atitude repúdio ao Protecionismo e medidas para aprofundar o Livre-comércio. Diversas análises sustentam que o protecionismo retardou de cinco a dez anos a recuperação da economia mundial [47][48]
Uma cidade é um grande assentamento humano.[1][2] Pode ser definida como um lugar permanente e densamente povoado com limites administrativamente definidos cujos membros trabalham principalmente em tarefas não agrícolas.[3] As cidades geralmente têm amplos sistemas de habitação, transporte, saneamento, serviços públicos, uso do solo, produção de bens e comunicação. Sua densidade facilita a interação entre pessoas, órgãos governamentais e empresas, às vezes beneficiando diferentes partes do processo, como melhorar a eficiência da distribuição de bens e serviços.
Historicamente, os moradores das cidades têm sido uma pequena proporção da humanidade em geral, mas após dois séculos de urbanização rápida e sem precedentes, mais da metade da população mundial agora vive em cidades, o que teve consequências profundas para a sustentabilidade global.[4][5] As cidades atuais geralmente formam o núcleo de grandes áreas metropolitanas e urbanas – criando inúmeros passageiros que viajam para os centros das cidades para emprego, entretenimento e educação. No entanto, em um mundo de intensificação da globalização, todas as cidades estão, em graus variados, também conectadas globalmente além dessas regiões. Essa influência crescente significa que as cidades também têm influências significativas em questões globais, como desenvolvimento sustentável, aquecimento global e saúde global. Devido a essas grandes influências em questões mundiais, a comunidade internacional priorizou o investimento em cidades sustentáveis por meio do 11º Objetivo de Desenvolvimento Sustentável. Devido à eficiência do transporte e ao menor consumo de terra, as cidades densas têm o potencial de ter uma pegada ecológica menor por habitante do que as áreas mais escassamente povoadas.[6] Por isso, as cidades compactas são muitas vezes referidas como um elemento crucial na luta contra as alterações climáticas.[7] No entanto, essa concentração também pode ter consequências negativas significativas, como a formação de ilhas de calor urbanas, concentração de poluição e estresse no abastecimento de água e outros recursos.
Outras características importantes das cidades, além da população, incluem o estatuto de capital e a relativa ocupação contínua da cidade. Por exemplo, capitais de países como Pequim, Londres, Cidade do México, Moscou, Nairóbi, Nova Deli, Paris, Roma, Atenas, Seul, Singapura, Tóquio, Manila e Washington, DC refletem a identidade e o ápice de suas respectivas nações.[8] Algumas capitais históricas, como Quioto e Xian, mantêm seu reflexo de identidade cultural mesmo sem o estatuto de capital. Locais sagrados religiosos oferecem outro exemplo de estatuto de capital dentro de uma religião, Jerusalém, Meca, Varanasi, Aiódia e Prayagraje têm importante significado cultural.
Uma cidade pode ser distinguida de outros assentamentos humanos por seu tamanho relativamente grande, mas também por suas funções e seu estatuto simbólico especial, que pode ser conferido por uma autoridade central. O termo também pode se referir tanto às ruas e edifícios físicos da cidade quanto ao conjunto de pessoas que ali habitam, e pode ser usado em um sentido geral para significar território urbano e não rural.[9][10]
Os censos nacionais usam uma variedade de definições - invocando fatores como população, densidade populacional, número de moradias, função econômica e infraestrutura - para classificar as populações como urbanas. As definições de trabalho típicas para populações de cidades pequenas começam em cerca de 100 mil pessoas.[11] As definições comuns de população para uma área urbana (cidade ou vila) variam entre 1,5 mil e 50 mil pessoas, com a maioria dos estados dos EUA usando um mínimo entre 1,5 mil e 5 mil habitantes.[12] Algumas jurisdições não estabelecem tais mínimos.[13] No Reino Unido, o estatuto de cidade é concedido pela Coroa e permanece permanentemente. (Historicamente, o fator de qualificação era a presença de uma catedral, resultando em algumas cidades muito pequenas, como Wells, com uma população de 12 mil habitantes e St Davids, com uma população de 1.841 pessoas em 2011). De acordo com a "definição funcional", uma cidade não se distingue apenas pelo tamanho, mas também pelo papel que desempenha dentro de um contexto político mais amplo. As cidades servem como centros administrativos, comerciais, religiosos e culturais para suas áreas circundantes maiores.[14][15]
O grau de urbanização é uma métrica moderna para ajudar a definir o que compõe uma cidade: "uma população de pelo menos 50 mil habitantes em células contíguas de grade densa (>1.500 habitantes por quilômetro quadrado)".[16] Essa métrica foi "criada ao longo dos anos pela Comissão Europeia, OCDE, Banco Mundial e outros, e endossada em março de 2021 pelas Nações Unidas ... em grande parte para fins de comparação estatística internacional".[17]
As palavras cidade e civilização vêm da raiz latina civitas, originalmente significando 'cidadania' ou 'membro da comunidade' e eventualmente chegando a corresponder a urbs, significando 'cidade' em um sentido mais físico.[9] A civitas romana estava intimamente ligada à polis grega.[18] Na terminologia toponímica, os nomes de cidades e vilas individuais são chamados astionyms (do grego antigo ἄστυ 'cidade ou vila' e ὄνομα 'nome').[19]
A geografia urbana lida tanto com as cidades em seu contexto mais amplo quanto com sua estrutura interna.[20] Estima-se que as cidades cubram cerca de 3% da superfície terrestre da Terra.[21]
O assentamento do tipo urbano se estende muito além dos limites tradicionais dos limites oficiais da cidade[22] em uma forma de desenvolvimento às vezes descrita criticamente como expansão urbana.[23] A descentralização e dispersão das funções da cidade (comercial, industrial, residencial, cultural, política) transformou o próprio significado do termo e desafiou os geógrafos que buscam classificar os territórios de acordo com um binário urbano-rural.[24]
As áreas metropolitanas incluem subúrbios e periferias organizados em torno das necessidades dos passageiros e, às vezes, cidades periféricas caracterizadas por algum grau de independência econômica e política. Algumas cidades agora fazem parte de uma paisagem urbana contínua chamada aglomeração urbana, conurbação ou megalópole (exemplificada pelo corredor BosWash do nordeste dos Estados Unidos).[25]
A localização das cidades variou ao longo da história de acordo com os contextos naturais, tecnológicos, econômicos e militares. O acesso à água tem sido um fator importante na localização e crescimento das cidades e, apesar das exceções permitidas pelo advento do transporte ferroviário no século XIX, até o presente, a maioria da população urbana do mundo vive perto da costa marítima ou de rios.[26]
As áreas urbanas geralmente não podem produzir seu próprio alimento e, portanto, devem desenvolver alguma relação com o interior que as sustenta.[27] Somente em casos especiais, como as cidades mineiras, que desempenham um papel vital no comércio de longa distância, as cidades são desconectadas do campo que as alimenta.[28] Assim, a centralidade dentro de uma região produtiva influencia a localização, já que as forças econômicas em teoria favoreceriam a criação de mercados em locais ótimos mutuamente alcançáveis.[29]
A grande maioria das cidades tem uma área central contendo edifícios com especial significado econômico, político e religioso. Os arqueólogos referem-se a esta área pelo termo grego temenos ou se for fortificada como uma cidadela.[30] Esses espaços refletem e ampliam historicamente a centralidade e a importância da cidade para sua esfera de influência mais ampla.[29]
As cidades normalmente têm espaços públicos onde qualquer pessoa pode ir. Estes incluem espaços de propriedade privada abertos ao público, bem como formas de terras públicas, como domínio público e usos comuns. A filosofia ocidental desde a época da ágora grega considerou o espaço público físico como o substrato simbólico da esfera pública.[31]
A estrutura urbana geralmente segue um ou mais padrões básicos: geomórfico, radial, concêntrico, retilíneo e curvilíneo. O ambiente físico geralmente restringe a forma na qual uma cidade é construída.[33]
Se localizada em uma encosta de montanha, a estrutura urbana pode contar com terraços e ruas sinuosas. Pode ser adaptada aos seus meios de subsistência (por exemplo, agricultura ou pesca). E pode ser configurada para uma defesa ideal, dada a paisagem circundante.[33]
Um sistema de ruas urbanas e terrenos retilíneos, conhecido como plano de grade, tem sido usado por milênios na Ásia, Europa e Américas. A civilização do Vale do Indo construiu Moenjodaro, Harapa e outras cidades em um padrão de grade, usando princípios antigos descritos por Kautilya e alinhados com os pontos da bússola.[34][14][35][36]
As cidades de Jericó, Alepo, Faium, Erevã, Atenas, Damasco e Argos estão entre as que reivindicam a mais longa habitação contínua. As cidades, caracterizadas pela densidade populacional, função simbólica e planejamento urbano, existem há milhares de anos.[37] Na visão convencional, a civilização e a cidade resultaram do desenvolvimento da agricultura, que possibilitou a produção de alimentos excedentes e, portanto, uma divisão social do trabalho (com concomitante estratificação social) e do comércio.[38][39] As primeiras cidades muitas vezes apresentavam celeiros, às vezes dentro de um templo.[40] Um ponto de vista minoritário considera que as cidades podem ter surgido sem a agricultura, devido aos meios alternativos de subsistência (pesca),[41] para serem usadas como abrigos sazonais comunais,[42] pelo seu valor como bases de organização militar defensiva e ofensiva,[43][44] ou à sua inerente função econômica inerente.[45][46] As cidades desempenharam um papel crucial no estabelecimento do poder político sobre uma área e líderes antigos, como Alexandre, o Grande, as fundaram e as criaram com zelo.[47]
Jericó e Çatalhöyük, datadas do oitavo milênio a.C., estão entre as primeiras proto-cidades conhecidas pelos arqueólogos.[42][48] No entanto, a cidade mesopotâmica de Uruk de meados do quarto milênio a.C. (antigo Iraque) é considerada por alguns como a primeira cidade verdadeira, com seu nome atribuído ao período de Uruk.[49][50][51]
No quarto e terceiro milênio a.C., civilizações complexas floresceram nos vales de rios da Mesopotâmia, Índia, China e Egito.[52][53] Escavações nessas áreas encontraram ruínas de cidades voltadas para comércio, política ou religião. Algumas tinham populações grandes e densas, mas outras realizavam atividades urbanas no campo da política ou da religião sem ter grandes populações associadas.
Entre as primeiras cidades do Velho Mundo, Moenjodaro da civilização do Vale do Indo no atual Paquistão, existente a partir de cerca de 2600 a.C., era uma das maiores, com uma população de 50 mil pessoas ou mais e um sofisticado sistema de saneamento.[54] As cidades planejadas da China foram construídas de acordo com princípios sagrados para atuar como microcosmos celestes.[55]
As cidades do Antigo Egito que são conhecidas fisicamente pelos arqueólogos não são extensas.[14] E as incluem (conhecidos por seus nomes árabes) El Lahun, uma cidade de trabalhadores associada à pirâmide de Senusret II, e a cidade religiosa Amarna, construída por Akhenaton e depois abandonada. Esses locais aparecem planejados de forma altamente regimentada e estratificada, com uma grade minimalista de quartos para os trabalhadores e habitações cada vez mais elaboradas disponíveis para as classes mais altas.[56]
Na Mesopotâmia, a civilização da Suméria, seguida pela Assíria e Babilônia, deu origem a inúmeras cidades, governadas por reis e fomentando múltiplas línguas escritas em cuneiforme. O império comercial fenício, que floresceu por volta da virada do primeiro milênio a.C., abrangia inúmeras cidades que se estendiam de Tiro e Biblos até Cartago e Cádiz.
Nos séculos seguintes, cidades-estados independentes da Grécia, especialmente Atenas, desenvolveram a polis, uma associação de cidadãos do sexo masculino que constituíam coletivamente a cidade.[57] A ágora, que significa "local de encontro" ou "assembleia", era o centro da vida atlética, artística, espiritual e política das polis.[58] Roma foi a primeira cidade que ultrapassou um milhão de habitantes. Sob a autoridade de seu império, Roma transformou e fundou muitas cidades (coloniae), e com elas trouxe seus princípios de arquitetura urbana, design e sociedade.[59]
Nas antigas Américas, as primeiras tradições urbanas se desenvolveram nos Andes e na Mesoamérica. Nos Andes, os primeiros centros urbanos se desenvolveram na civilização de Caral e nas culturas chavin e moche, seguidas por grandes cidades nas culturas huari, chimu e inca. A civilização de Caral incluia até 30 grandes centros populacionais no que é hoje a região do litoral centro-norte do Peru. É a mais antiga civilização conhecida nas Américas, florescendo entre os séculos XXX a.C. e XVIII a.C..[60] A Mesoamérica viu o surgimento do urbanismo inicial em várias regiões culturais, começando com os olmecas e se espalhando para os maias pré-clássicos, os zapotecas de Oaxaca e Teotihuacan no centro do México. Culturas posteriores, como a civilização asteca, andina, maia, mississippiana e pueblo, basearam-se nessas tradições urbanas anteriores. Muitas de suas antigas cidades continuam habitadas, incluindo grandes metrópoles como a Cidade do México, no mesmo local de Tenochtitlan; enquanto os antigos pueblos continuamente habitados estão perto de áreas urbanas modernas no Novo México, como Acoma Pueblo perto da área metropolitana de Albuquerque e Taos Pueblo perto de Taos; enquanto outros como Lima estão localizados nas proximidades de antigos sítios arqueológicos peruanos, como Pachacamac.
Jenné-Jeno, localizada no atual Mali e datada do século III a.C., carecia de arquitetura monumental e de uma classe social de elite distinta - mas, no entanto, tinha produção e relações especializadas com o interior.[61] Os contatos comerciais pré-árabes provavelmente existiram entre Jenné-Jeno e o norte da África.[62] Outros centros urbanos antigos na África subsaariana, datados de cerca de 500 d.C., incluem Awdaghust, Kumbi-Saleh, a antiga capital de Gana, e Maranda, um centro localizado em uma rota comercial entre o Egito e Gao.[63]
Nos remanescentes do Império Romano, as cidades da antiguidade tardia ganharam independência, mas logo perderam população e importância. O foco do poder no Ocidente mudou para Constantinopla e para a ascendente civilização islâmica com suas principais cidades: Bagdá, Cairo e Córdoba.[64] Do século IX até o final do século XII, Constantinopla, capital do Império Romano do Oriente, foi a maior e mais rica cidade da Europa, com uma população próxima de 1 milhão.[65][66] O Império Otomano gradualmente ganhou controle sobre muitas cidades na área do Mediterrâneo, incluindo Constantinopla em 1453.
No Sacro Império Romano-Germânico, a partir do século XII, as cidades imperiais livres como Nuremberga, Estrasburgo, Frankfurt, Basileia, Zurique, Nijmegen tornaram-se uma elite privilegiada entre as cidades que conquistaram o autogoverno de seu senhor local ou receberam autogovernança pelo imperador e sendo colocado sob sua proteção imediata. Em 1480, essas cidades, que ainda faziam parte do império, tornaram-se parte dos Estados Imperiais que governavam o império com o imperador através da Dieta Imperial.[67]
Nos séculos XIII e XIV, algumas cidades se tornaram Estados poderosos, tomando áreas vizinhas sob seu controle ou estabelecendo extensos impérios marítimos. Na Itália, as comunas medievais se desenvolveram em cidades-estados, como a República de Veneza e a República de Gênova. No norte da Europa, cidades como Lübeck e Bruges formaram a Liga Hanseática para defesa coletiva e comércio. Seu poder foi posteriormente desafiado e eclipsado pelas cidades comerciais holandesas de Ghent, Ypres e Amsterdã.[68] Fenômenos semelhantes existiam em outros lugares, como no caso de Sakai, que gozava de considerável autonomia no Japão medieval tardio.
No primeiro milênio d.C., a capital do Império Quemer de Angkor, no atual Camboja, tornou-se o assentamento pré-industrial mais extenso do mundo em área,[69][70] cobrindo mais de mil quilômetros quadrados e possivelmente suportando até um milhão de pessoas.[69][71]
No Ocidente, os Estados-nação tornaram-se a unidade dominante de organização política após a Paz de Vestfália no século XVII.[72][73] As maiores capitais da Europa Ocidental (Londres e Paris) se beneficiaram do crescimento do comércio após o surgimento de um comércio atlântico. No entanto, a maioria das cidades permaneceu pequena.
O crescimento da indústria moderna a partir do final do século XVIII levou à urbanização maciça e ao surgimento de novas grandes cidades, primeiro na Europa e depois em outras regiões, à medida que novas oportunidades trouxeram um grande número de migrantes de comunidades rurais para áreas urbanas. A Inglaterra liderou o caminho quando Londres se tornou a capital de um império mundial e as cidades de todo o país cresceram em locais estratégicos para a fabricação.[74] Nos Estados Unidos, de 1860 a 1910, a introdução de ferrovias reduziu os custos de transporte e grandes centros industriais começaram a surgir, alimentando a migração das áreas rurais para as cidades. As cidades industrializadas tornaram-se lugares mortais para se viver, devido a problemas de saúde decorrentes da superlotação, riscos ocupacionais da indústria, água e ar contaminados, falta de saneamento e doenças transmissíveis, como febre tifóide e cólera. Fábricas e favelas surgiram como características regulares da paisagem urbana.[75]
Na segunda metade do século XX, a desindustrialização (ou "reestruturação econômica") no Ocidente levou à pobreza, falta de moradias e decadência urbana em cidades anteriormente prósperas. O "cinturão de aço" da América tornou-se um "cinturão de ferrugem" e cidades como Detroit e Gary começaram a encolher, contrariando a tendência global de expansão urbana maciça.[77] Essas cidades se adaptaram com sucesso relativo para a economia de serviços e parcerias público-privadas, com gentrificação concomitante, esforços de revitalização desiguais e desenvolvimento cultural seletivo.[78] Sob o Grande Salto Adiante e os planos quinquenais subsequentes que continuam até hoje, a República Popular da China passou por uma urbanização e industrialização concomitantes e se tornou o principal polo manufatureiro do mundo.[79][80]
Em meio a essas mudanças econômicas, a alta tecnologia e as telecomunicações instantâneas permitem que cidades selecionadas se tornem centros da economia do conhecimento.[81][82][83] Um novo paradigma de cidade inteligente, apoiado por instituições como a RAND Corporation e a IBM, está trazendo vigilância computadorizada, análise de dados e governança para as cidades e seus moradores.[84]
A urbanização é o processo de migração das áreas rurais para as urbanas, impulsionado por vários fatores políticos, econômicos e culturais. Até o século XVIII, existia um equilíbrio entre a população agrícola rural e as cidades com mercados e manufatura em pequena escala.[86][87] Com as revoluções agrícola e industrial, a população urbana iniciou um crescimento sem precedentes, tanto pela migração quanto pela expansão demográfica. Na Inglaterra, a proporção da população vivendo em cidades saltou de 17% em 1801 para 72% em 1891.[88] Em 1900, 15% da população mundial vivia em cidades.[89] O apelo cultural das cidades também desempenha um papel na atração de moradores.[90]
A urbanização se espalhou rapidamente pela Europa e pelas Américas e, desde a década de 1950, também se estabeleceu na Ásia e na África. A Divisão de População do Departamento de Assuntos Econômicos e Sociais das Nações Unidas, informou em 2014 que, pela primeira vez, mais da metade da população mundial vive em cidades.[91]
A América Latina é o continente mais urbano, com quatro quintos de sua população vivendo em cidades, incluindo um quinto da população que vive em favelas.[92] Batam, na Indonésia, Mogadíscio, na Somália, Xiamen, na China e Niamey, no Níger, são consideradas entre as cidades que mais crescem no mundo, com taxas de crescimento anual de 5 a 8%.[93] Em geral, os países mais desenvolvidos do "Norte Global" continuam mais urbanizados do que os países menos desenvolvidos do "Sul Global" — mas a diferença continua diminuindo porque a urbanização está acontecendo mais rapidamente neste último grupo. A Ásia abriga, de longe, o maior número absoluto de habitantes das cidades: mais de dois bilhões de pessoas e aumentando.[87] A ONU prevê mais 2,5 bilhões de habitantes das cidades (e 300 milhões a menos de habitantes do campo) em todo o mundo até 2050, com 90% da expansão da população urbana ocorrendo na Ásia e na África.[91][94]
Megacidades, cidades com população vários milhões de pessoas, proliferaram às dezenas, surgindo especialmente na Ásia, África e América Latina.[95][96] A globalização econômica alimenta o crescimento dessas cidades, à medida que novas torrentes de capital estrangeiro organizam a rápida industrialização, bem como a realocação de grandes empresas da Europa e da América do Norte, atraindo imigrantes de perto e de longe.[97] Um abismo profundo divide ricos e pobres nessas cidades, que geralmente contêm uma elite super-rica que vive em condomínios fechados e grandes massas de pessoas que vivem em moradias precárias com infraestrutura inadequada e condições precárias.[98]
Cidades ao redor do mundo se expandiram fisicamente à medida que crescem em população, com aumentos em sua extensão de superfície, com a criação de arranha-céus para uso residencial e comercial e com desenvolvimento subterrâneo.[99][100]
A urbanização pode criar uma demanda rápida para a gestão de recursos hídricos, já que anteriormente boas fontes de água doce se tornam excessivamente usadas e poluídas e o volume de esgoto começa a exceder os níveis gerenciáveis.
O governo local das cidades assume diferentes formas ao redor do mundo. O principal funcionário da cidade tem o título de prefeito. Qualquer que seja seu verdadeiro grau de autoridade política, o prefeito normalmente atua como a figura de líder ou personificação de sua cidade.[101]
Os governos das cidades têm autoridade para fazer leis que regem a atividade dentro das cidades, enquanto sua jurisdição é geralmente considerada subordinada (em ordem crescente) à lei estadual/provincial, nacional e talvez internacional. Essa hierarquia da lei não é aplicada rigidamente na prática – por exemplo, em conflitos entre regulamentos municipais e princípios nacionais, como direitos constitucionais e direitos de propriedade.[73] Conflitos e questões legais surgem com mais frequência nas cidades do que em outros lugares devido ao simples fato de sua maior densidade.[102] Os governos das cidades modernas regulam completamente a vida cotidiana em muitas dimensões, incluindo saúde pública e pessoal, transporte, enterro, uso e extração de recursos, recreação e a natureza e uso dos edifícios. Tecnologias, técnicas e leis que regem essas áreas – desenvolvidas nas cidades – tornaram-se onipresentes em muitas áreas.[103] Os funcionários municipais podem ser nomeados por um nível superior de governo ou eleitos localmente.[104]
As cidades normalmente fornecem serviços municipais, como educação, por meio de sistemas escolares; policiamento, por meio de delegacias de polícia; e combate a incêndios, através dos Corpos de Bombeiros; bem como a infraestrutura básica da cidade. Estes são fornecidos mais ou menos rotineiramente, de forma mais ou menos igual.[105][106] A responsabilidade pela administração geralmente recai sobre o governo da cidade, embora alguns serviços possam ser operados por um nível mais alto de governo,[107] enquanto outros podem ser administrados de forma privada.[108]
A base tradicional para as finanças municipais é o imposto de propriedade local cobrado sobre imóveis dentro da cidade. O governo local também pode arrecadar receitas através de serviços ou arrendando terras de sua propriedade.[109] No entanto, o financiamento de serviços municipais, bem como de renovação urbana e outros projetos de desenvolvimento, é um problema perene, que as cidades abordam por meio de apelos aos governos superiores, acordos com o setor privado e técnicas como privatização (venda de serviços para o setor privado), corporatização (formação de corporações quase privadas de propriedade municipal) e financeirização (empacotamento de ativos da cidade em contratos públicos financeiros negociáveis e outros direitos relacionados. Esta situação tornou-se aguda em cidades desindustrializadas e nos casos em que empresas e cidadãos mais ricos se mudaram para fora dos limites da cidade e, portanto, fora do alcance da tributação.[110][111][112][113] As cidades em busca de dinheiro recorrem cada vez mais ao título municipal, essencialmente um empréstimo com juros e prazo de reembolso. Os governos municipais também começaram a usar o financiamento de incremento de impostos, no qual um projeto de desenvolvimento é financiado por empréstimos baseados em receitas fiscais futuras que se espera que ele gere.[113]
A governança inclui o governo, mas refere-se a um domínio mais amplo de funções de controle social implementadas por muitos atores, incluindo organizações não governamentais.[114] O impacto da globalização e o papel das corporações multinacionais nos governos locais em todo o mundo levou a uma mudança de perspectiva sobre a governança urbana, longe da "teoria do regime urbano" em que uma coalizão de interesses locais governa funcionalmente, em direção a uma teoria de controle econômicos externo, amplamente associada em acadêmicos com a filosofia do neoliberalismo.[115] No modelo neoliberal de governança, os serviços públicos são privatizados, a indústria é desregulamentada e as corporações ganham o estatuto de atores governantes – como indicado pelo poder que exercem nas parcerias público-privadas e na expectativa de autogestão através da responsabilidade social corporativa. Os maiores investidores e promotores imobiliários atuam como planejadores urbanos de facto das cidades.[116]
O conceito relacionado de boa governança dá mais ênfase ao Estado, com o objetivo de avaliar os governos urbanos quanto à sua adequação à assistência ao desenvolvimento.[117] Os conceitos de governança e boa governança são especialmente invocados nas megacidades emergentes, onde as organizações internacionais consideram os governos existentes inadequados para suas grandes populações.[118]
O planejamento urbano, a aplicação de premeditação ao desenho da cidade, envolve a otimização do uso do solo, transportes, serviços públicos e outros sistemas básicos, a fim de atingir determinados objetivos. Planejadores e estudiosos propuseram teorias sobrepostas como ideais de como os planos urbanos devem ser formados. As ferramentas de planejamento, além do projeto original da própria cidade, incluem investimentos de capital público em infraestrutura e controles de uso do solo, como zoneamento. O processo contínuo de planejamento abrangente envolve a identificação de objetivos gerais, bem como a coleta de dados para avaliar o progresso e informar decisões futuras.[120][121]
O governo é legalmente a autoridade final no planejamento, mas na prática o processo envolve tanto elementos públicos quanto privados. O princípio legal do desapropriação é utilizado pelo governo para alienar os cidadãos de sua propriedade nos casos em que seu uso é necessário para um projeto.[121] O planejamento geralmente envolve compensações – decisões em que alguns ganham e outros perdem – e, portanto, está intimamente ligado à situação política predominante.[122]
A história do planejamento urbano data de algumas das primeiras cidades conhecidas, especialmente no Vale do Indo e nas civilizações mesoamericanas, que construíram suas cidades em grades e aparentemente zonearam diferentes áreas para diferentes propósitos.[14][123] Os efeitos do planejamento, onipresentes no mundo de hoje, podem ser vistos mais claramente no layout das comunidades planejadas, totalmente projetadas antes da construção, muitas vezes considerando sistemas físicos, econômicos e culturais interligados.
A sociedade urbana é tipicamente estratificada. Espacialmente, as cidades são segregadas formal ou informalmente ao longo de linhas étnicas, econômicas e raciais. As pessoas que vivem relativamente próximas umas das outras podem viver, trabalhar e se divertir em áreas separadas e associar-se a pessoas diferentes, formando enclaves étnicos ou de estilo de vida ou, em áreas de pobreza concentrada, guetos e favelas. Enquanto nos Estados Unidos e em outros lugares a pobreza se associou ao centro da cidade, na França ela se associou aos banlieues, áreas de desenvolvimento urbano que cercam a cidade propriamente dita. Enquanto isso, em toda a Europa e América do Norte, a maioria racialmente branca é empiricamente o grupo mais segregado. Os subúrbios do Ocidente e, cada vez mais, os condomínios fechados e outras formas de "privatopia" em todo o mundo permitem que as elites locais se auto-segreguem em bairros seguros e exclusivos.[124]
Os trabalhadores urbanos sem terra, em contraste com os camponeses e conhecidos como proletariado, formam uma camada crescente da sociedade na era da urbanização. Na doutrina marxista, o proletariado inevitavelmente se revoltará contra a burguesia à medida que suas fileiras se avolumam com pessoas desprivilegiadas e descontentes sem qualquer participação no status quo.[125] O proletariado urbano global de hoje, no entanto, geralmente carece do estatuto de operário fabril que no século XIX fornecia acesso aos meios de produção.[126]
Historicamente, as cidades dependem de áreas rurais para agricultura intensiva para produzir colheitas excedentes, em troca do que fornecem dinheiro, administração política, bens manufaturados e cultura.[27][28] A economia urbana tende a analisar aglomerações maiores, que se estendem além dos limites da cidade, a fim de alcançar uma compreensão mais completa do mercado de trabalho local.[127]
Como centros de comércio, as cidades há muito abrigam o comércio varejista e o consumo por meio da interface de compras. No século XX, as lojas de departamentos, usando novas técnicas de publicidade, relações públicas, decoração e design, transformaram as áreas de compras urbanas em mundos de fantasia, incentivando a auto-expressão e a fuga pelo consumismo.[128][129]
Em geral, a densidade das cidades agiliza o comércio e facilita o transbordamento de conhecimento, ajudando pessoas e empresas a trocar informações e gerar novas ideias.[130][131] Um mercado de trabalho mais espesso permite uma melhor correspondência de competências entre empresas e indivíduos. A densidade populacional também permite o compartilhamento de infraestrutura e instalações de produção comuns, no entanto, em cidades muito densas, o aumento da aglomeração e dos tempos de espera pode levar a alguns efeitos negativos.[132]
Embora a manufatura tenha impulsionado o crescimento das cidades, muitas agora dependem de uma economia terciária ou de serviços, que vão desde turismo, entretenimento, finanças, administração, etc.[78][133]
As cidades são tipicamente centros de educação e artes, abrigando universidades, museus, templos e outras instituições culturais.[15] Elas apresentam impressionantes exibições de arquitetura que variam de pequenas a enormes e ornamentadas a brutalistas; arranha-céus que fornecem milhares de escritórios ou habitações em um espaço pequeno e visíveis a quilômetros de distância, tornaram-se características urbanas icônicas.[135] As elites culturais tendem a viver nas cidades, unidas pelo capital cultural compartilhado, e elas próprias desempenhando algum papel na governança.[136] Em virtude de seu estatuto como centros de cultura e alfabetização, as cidades podem ser descritas como o centro da civilização, história mundial e mudança social.[137][138]
A densidade contribui para uma comunicação de massa eficaz e transmissão de notícias, por meio de arautos, proclamações impressas, jornais e mídia digital. Essas redes de comunicação, embora ainda usem cidades como polos, penetram amplamente em todas as áreas povoadas. Na era da comunicação e transporte rápidos, os comentaristas descreveram a cultura urbana como quase onipresente.[24][139][140]
Atualmente, a promoção de uma cidade de suas atividades culturais se encaixa com a marca e o marketing do lugar; com técnicas de diplomacia pública usadas para informar a estratégia de desenvolvimento; com a atração de empresas, investidores, moradores e turistas; e com criação uma identidade compartilhada e senso de lugar dentro da área metropolitana.[141][142][143][144] Inscrições físicas, placas e monumentos expostos transmitem fisicamente um contexto histórico para lugares urbanos.[145] Algumas cidades, como Jerusalém, Meca e Roma, têm estatuto religioso indelével e por centenas de anos atraem peregrinos. Turistas patrióticos visitam Agra para ver o Taj Mahal, ou Nova York para visitar o World Trade Center, ou Memphis para prestar homenagens a Elvis Presley em Graceland.[146] As marcas de lugar (que incluem satisfação e lealdade ao lugar) têm grande valor econômico (comparável ao valor das marcas de commodities) por causa de sua influência no processo de tomada de decisão das pessoas que pensam em fazer negócios em uma cidade.[144]
Pão e circo, entre outras formas de apelo cultural, atraem e divertem as massas.[90][147] Os esportes também desempenham um papel importante na marca da cidade e na formação da identidade local.[148] As cidades esforçam-se consideravelmente na competição para sediar os Jogos Olímpicos, que atraem a atenção global e o turismo.[149]
As cidades desempenham um papel estratégico crucial na guerra devido à sua centralidade econômica, demográfica, simbólica e política. Pelas mesmas razões, eles são alvos na guerra assimétrica. Muitas cidades ao longo da história foram fundadas sob os auspícios militares, muitas incorporaram fortificações e os princípios militares continuam a influenciar o desenho urbano.[150] De fato, a guerra pode ter servido como base social e econômica para as primeiras cidades.[43][44]
Potências engajadas em conflitos geopolíticos estabeleceram assentamentos fortificados como parte de estratégias militares, como no caso de cidades-guarnição, o Programa Estratégico de Hamlet dos Estados Unidos durante a Guerra do Vietnã e assentamentos israelenses na Palestina.[151] Enquanto ocupava as Filipinas, o Exército dos EUA ordenou que a população local se concentrasse em cidades e vilas, a fim de isolar insurgentes comprometidos e lutar livremente contra eles no campo.[152][153]
Durante a Segunda Guerra Mundial, os governos nacionais às vezes declaravam certas cidades abertas, entregando-as efetivamente a um inimigo que avançava, a fim de evitar danos e derramamento de sangue. A guerra urbana provou ser decisiva, no entanto, na Batalha de Stalingrado, onde as forças soviéticas repeliram os ocupantes alemães, com muitas mortes e destruição. Em uma era de conflitos de baixa intensidade e rápida urbanização, as cidades tornaram-se locais de conflito de longo prazo travado tanto por ocupantes estrangeiros quanto por governos locais contra a insurgência.[126][154] Essa guerra, conhecida como contrainsurgência, envolve técnicas de vigilância e guerra psicológica, bem como combate corpo a corpo,[155] estende funcionalmente a prevenção de crimes urbanos modernos, que já utiliza conceitos como espaço defensável.[156]
Embora a captura seja o objetivo mais comum, a guerra em alguns casos significa a destruição completa de uma cidade. Tábuas e ruínas mesopotâmicas atestam tal destruição,[157] assim como o lema latino Carthago delenda est.[158][159] Desde os bombardeios atômicos de Hiroshima e Nagasaki e durante a Guerra Fria, os estrategistas nucleares continuaram a contemplar o uso de alvos de "contravalor": paralisar um inimigo aniquilando suas valiosas cidades, em vez de mirar principalmente em suas forças militares.[160][161]
A infraestrutura urbana envolve várias redes físicas e espaços necessários para transporte, uso de água, energia, recreação e funções públicas.[162] A infraestrutura acarreta um alto custo inicial em capital fixo (tubos, fios, plantas, veículos, etc.), mas custos marginais mais baixos e, portanto, economias de escala positivas.[163] Devido às maiores barreiras de entrada, essas redes foram classificadas como monopólios naturais, o que significa que a lógica econômica favorece o controle de cada rede por uma única organização, pública ou privada.[164][165]
A infraestrutura em geral (se não todo projeto de infraestrutura) desempenha um papel vital na capacidade de uma cidade para a atividade econômica e expansão, sustentando a própria sobrevivência dos habitantes, bem como as atividades tecnológicas, comerciais, industriais e sociais.[162][163] Estruturalmente, muitos sistemas de infraestrutura assumem a forma de redes com ligações redundantes e múltiplos caminhos, de modo que o sistema como um todo continua a operar mesmo se partes dele falharem.[165] As particularidades dos sistemas de infraestrutura de uma cidade dependem da trajetória histórica porque o novo desenvolvimento deve ser construído a partir do que já existe.[163]
Megaprojetos como a construção de aeroportos, usinas de energia e ferrovias exigem grandes investimentos iniciais e, portanto, tendem a exigir financiamento do governo nacional ou do setor privado.[166][165] A privatização também pode se estender a todos os níveis de construção e manutenção de infraestrutura.[167]
A infraestrutura urbana idealmente atende a todos os moradores igualmente, mas na prática pode ser desigual – com, em algumas cidades, alternativas claras de primeira e segunda classe.[106][168][164]
Os serviços públicos (literalmente, coisas úteis com disponibilidade geral) incluem redes de infraestrutura básica e essencial, principalmente preocupadas com o fornecimento de água, eletricidade e capacidade de telecomunicações à população.[169]
O saneamento, necessário para uma boa saúde em condições de superlotação, requer o abastecimento de água e a gestão de resíduos, bem como a higiene individual. Os sistemas de águas urbanas incluem principalmente uma rede de abastecimento de água e uma rede (sistema de esgoto) para esgoto e águas pluviais. Historicamente, tanto os governos locais quanto as empresas privadas têm administrado o abastecimento urbano de água, com tendência ao abastecimento público de água no século XX e tendência à operação privada na virada do século XXI.[164] O mercado de abastecimento privado serviços de água é dominado por duas empresas francesas, Veolia Water (anteriormente Vivendi) e Engie (anteriormente Suez), que se diz deter 70% de todos os contratos de água em todo o mundo.[164][170]
A vida urbana moderna depende muito da energia transmitida através da eletricidade para a operação de máquinas elétricas (de eletrodomésticos a máquinas industriais a sistemas eletrônicos agora onipresentes usados ​​em comunicações, negócios e governo) e para semáforos, iluminação pública e iluminação interna. As cidades dependem em menor grau de combustíveis de hidrocarbonetos, como gasolina e gás natural, para transporte, aquecimento e cozimento. A infraestrutura de telecomunicações, como linhas telefônicas e cabos coaxiais, também atravessa as cidades, formando redes densas para comunicações de massa e ponto a ponto.[171]
Como as cidades dependem da especialização e de um sistema econômico baseado no trabalho assalariado, seus habitantes devem ter a capacidade de viajar regularmente entre casa, trabalho, comércio e entretenimento.[172] Os moradores da cidade viajam a pé ou por transporte rodoviário em estradas e passarelas, ou usam sistemas especiais de metrô baseados em trilhos subterrâneos, aéreos e elevados. As cidades também contam com transporte de longa distância (caminhão, trem e avião) para conexões econômicas com outras cidades e áreas rurais.[173]
Historicamente, as ruas das cidades eram domínio dos cavalos e seus cavaleiros e pedestres, que só às vezes tinham calçadas e áreas especiais para caminhadas reservadas para eles. No Ocidente, as bicicletas ou (velocípedes), máquinas eficientes de propulsão humana para viagens de curta e média distância, desfrutaram de um período de popularidade no início do século XX antes do surgimento dos automóveis. Logo depois, eles ganharam uma posição mais duradoura nas cidades asiáticas e africanas sob influência europeia. Nas cidades ocidentais, a industrialização, expansão e eletrificação dos sistemas de transporte público e especialmente os bondes permitiram a expansão urbana à medida que novos bairros residenciais surgiram ao longo das linhas de transporte público e os trabalhadores iam e voltavam do trabalho para o centro da cidade.[173][174]
Desde meados do século XX, as cidades dependem fortemente do transporte de veículos motorizados, com grandes implicações para seu planejamento urbano, ambiente e estética.[175] (Essa transformação ocorreu de forma mais dramática nos EUA – onde as políticas corporativas e governamentais favoreceram os sistemas de transporte de automóveis – e em menor grau na Europa).[173][174] A ascensão dos carros pessoais acompanhou a expansão das áreas econômicas urbanas para áreas metropolitanas muito maiores, criando posteriormente problemas de trânsito onipresentes com a construção de novas rodovias, ruas mais largas e passarelas alternativas para pedestres.[176][177][178][142] No entanto, engarrafamentos graves ainda ocorrem regularmente em cidades ao redor do mundo, à medida que a propriedade de carros particulares e a urbanização continuam a aumentar, sobrecarregando as redes de ruas urbanas existentes.[109]
O sistema de ônibus urbano, a forma de transporte público mais comum do mundo, usa uma rede de rotas programadas para transportar pessoas pela cidade, ao lado de carros, nas estradas.[179] A própria função econômica também se tornou mais descentralizada à medida que a concentração se tornou impraticável e os empregadores foram realocados para locais mais amigáveis ​​aos carros (incluindo cidades periféricas).[173] Algumas cidades introduziram sistemas de ônibus de trânsito rápido que incluem faixas exclusivas de ônibus e outros métodos para priorizar o tráfego de ônibus em relação aos carros particulares.[109][180] Muitas grandes cidades estadunidenses ainda operam o transporte público convencional por trem, como exemplificado pelo sempre popular sistema de metrô de Nova York. O metrô também é amplamente utilizado na Europa e aumentou na América Latina e na Ásia.[109]
Caminhar e andar de bicicleta ("transporte não motorizado") são cada vez mais favorecidos (mais zonas de pedestres e ciclovias) no planejamento de transporte urbano estadunidense e asiático, sob a influência de tendências como o movimento "cidades saudáveis", o impulso para o desenvolvimento sustentável e a ideia de uma cidade sem carros.[109][181][182] Técnicas como racionamento de espaço viário e taxas de uso de estradas foram introduzidas para limitar o tráfego de carros urbanos.[109]
A habitação dos residentes apresenta um dos maiores desafios que todas as cidades devem enfrentar. A moradia adequada envolve não apenas abrigos físicos, mas também os sistemas físicos necessários para sustentar a vida e a atividade econômica.[183] A casa própria representa estatuto e um mínimo de segurança econômica, em comparação com o aluguel, que pode consumir grande parte da renda dos trabalhadores urbanos de baixos salários. A falta de moradia é um desafio enfrentado atualmente por milhões de pessoas em países ricos e pobres.[184]
Os ecossistemas urbanos, influenciados como são pela densidade de construções e atividades humanas, diferem consideravelmente daqueles de seu entorno rural. As construções e resíduos antropogénicos, bem como o cultivo em jardins, criam ambientes físicos e químicos que não têm equivalentes na natureza, permitindo em alguns casos uma biodiversidade excepcional. Eles fornecem lares não apenas para humanos imigrantes, mas também para plantas imigrantes, provocando interações entre espécies que nunca se encontraram anteriormente. Eles introduzem distúrbios frequentes nos habitats de plantas e animais, criando oportunidades para recolonização e, assim, favorecendo ecossistemas jovens com espécies r-selecionadas dominantes. Em geral, os ecossistemas urbanos são menos complexos e produtivos do que outros, devido à quantidade absoluta diminuída de interações biológicas.[185][186][187][188]
A fauna urbana típica inclui insetos (especialmente formigas), roedores (camundongos, ratos) e pássaros, bem como gatos e cães (domesticados e selvagens). Grandes predadores são escassos.[187]
As cidades geram pegadas ecológicas consideráveis, localmente e a longas distâncias, devido à concentração de populações e atividades tecnológicas. De uma perspectiva, as cidades não são ecologicamente sustentáveis ​​devido às suas necessidades de recursos. Por outro lado, a gestão adequada pode ser capaz de amenizar os efeitos nocivos de uma cidade.[189][190]
A poluição do ar surge de várias formas de combustão,[191] incluindo lareiras, fogões a lenha ou carvão, outros sistemas de aquecimento,[192] e motores de combustão interna. As cidades industrializadas, e hoje megacidades do terceiro mundo, são notórias pelos véus de smog (névoa industrial) que as envolvem, representando uma ameaça crônica à saúde de seus milhões de habitantes.[193] O solo urbano contém concentrações mais altas de metais pesados ​​(especialmente chumbo, cobre e níquel) e tem pH mais baixo do que o solo em áreas selvagens.[187]
As cidades modernas são conhecidas por criar seus próprios microclimas, devido ao concreto, asfalto e outras superfícies artificiais, que aquecem com a luz do sol e canalizam a água da chuva para dutos subterrâneos. A temperatura na cidade de Nova York excede as temperaturas rurais próximas em uma média de 2 a 3°C e, às vezes, diferenças de 5 a 10 C foram registradas. Este efeito varia de forma não linear com as mudanças populacionais (independentemente do tamanho físico da cidade).[187][194] As partículas aéreas aumentam a precipitação em 5-10%. Assim, as áreas urbanas experimentam climas únicos, com floração mais precoce e queda de folhas mais tardia do que em regiões vizinhas.[187]
As pessoas pobres e da classe trabalhadora enfrentam uma exposição desproporcional aos riscos ambientais (conhecido como racismo ambiental ao cruzar também com a segregação racial). Por exemplo, dentro do microclima urbano, os bairros pobres menos vegetados suportam mais calor (mas têm menos meios de lidar com ele).[195]
Um dos principais métodos para melhorar a ecologia urbana é incluir nas cidades mais espaços verdes urbanos: parques, jardins, gramados e árvores. Essas áreas melhoram a saúde, o bem-estar das populações humanas, animais e vegetais das cidades.[196] Árvores urbanas bem conservadas podem proporcionar muitos benefícios sociais, ecológicos e físicos aos moradores urbanos.[197]
À medida que o mundo se torna mais intimamente ligado por meio da economia, política, tecnologia e cultura (um processo chamado globalização), as cidades passaram a desempenhar um papel de liderança nos assuntos transnacionais, superando as limitações das relações internacionais conduzidas pelos governos nacionais.[198][199][200][201] Esse fenômeno, ressurgente hoje, pode ser rastreado até a Rota da Seda, a Fenícia e as cidades-estados gregas, assim como a Liga Hanseática e outras alianças de cidades.[202][131][203] Atualmente, a economia da informação baseada na infraestrutura de internet de alta velocidade permite telecomunicações instantâneas em todo o mundo, eliminando efetivamente a distância entre as cidades para fins de mercados internacionais e outros elementos de alto nível da economia mundial, bem como comunicações pessoais e meios de comunicação de massa.[204]
Uma cidade global, também conhecida como cidade mundial, é um importante centro de comércio, bancos, finanças, inovação e mercados. Saskia Sassen usou o termo "cidade global" em seu trabalho de 1991, The Global City: New York, London, Tokyo, para se referir ao poder, status e cosmopolitismo de uma cidade, e não ao seu tamanho.[205] Seguindo essa visão das cidades, é possível classificá-las hierarquicamente.[206]
As cidades globais formam a pedra angular da hierarquia global, exercendo comando e controle por meio de sua influência econômica e política. As cidades globais podem ter alcançado seu status devido à transição precoce para o pós-industrialismo[207] ou pela inércia que lhes permitiu manter seu domínio desde a era industrial.[208] Esse tipo de classificação exemplifica um discurso emergente no qual as cidades, consideradas variações do mesmo tipo ideal, devem competir entre si globalmente para alcançar a prosperidade.[149][142]
Os críticos da noção apontam para os diferentes domínios de poder e intercâmbio. O termo "cidade global" é fortemente influenciado por fatores econômicos e, portanto, pode não levar em conta lugares que são significativos. O acadêmico australiano Paul James, por exemplo, argumenta que o termo é "redutor e enviesado" em seu foco nos sistemas financeiros.[209]
Corporações multinacionais e bancos têm suas sedes em cidades globais e conduzem muitos de seus negócios dentro desse contexto.[210] As firmas estadunidenses dominam os mercados internacionais de direito e engenharia e mantêm filiais nas maiores cidades globais estrangeiras.[211]
As cidades globais apresentam concentrações de pessoas extremamente ricas e extremamente pobres.[212] Suas economias são lubrificadas por sua capacidade (limitada pela política de imigração do governo nacional, que define funcionalmente o lado da oferta do mercado de trabalho) de recrutar trabalhadores imigrantes de baixa e alta qualificação de áreas mais pobres.[213][214][215] Mais e mais cidades atualmente utilizam essa força de trabalho disponível globalmente.[216]
As cidades participam cada vez mais das atividades políticas mundiais, independentemente dos Estados-nação aos quais pertencem. Os primeiros exemplos desse fenômeno são a relação entre cidades-irmãs e a promoção da governança multinível dentro da União Europeia (UE) como uma técnica para a integração europeia.[199][217][218] Cidades como Hamburgo, Praga, Amsterdã, Haia e Cidade de Londres mantêm suas próprias embaixadas na UE em Bruxelas.[219][220][221]
Os novos moradores urbanos podem cada vez mais não apenas como imigrantes, mas como transmigrantes, mantendo um pé cada um (através de telecomunicações, se não de viagens) em seus antigos e novos lares.[222]
O Sistema das Nações Unidas esteve envolvido em uma série de eventos e declarações que tratam do desenvolvimento das cidades durante este período de rápida urbanização.
A ONU-Habitat coordena a agenda urbana da ONU, trabalhando com o Programa das Nações Unidas para o Meio Ambiente, o Programa das Nações Unidas para o Desenvolvimento, o Escritório do Alto-comissário das Nações Unidas para os Direitos Humanos, a Organização Mundial da Saúde e o Banco Mundial.[224]
O Banco Mundial, uma agência especializada dasNações Unidas, tem sido a principal força na promoção das conferências Habitat e, desde a primeira conferência, tem usado suas declarações como uma estrutura para a emissão de empréstimos para infraestrutura urbana.[226] Os programas de ajuste estrutural do banco contribuíram para a urbanização no Terceiro Mundo ao criar incentivos à mudança para as cidades.[229][230] O Banco Mundial e a ONU-Habitat em 1999 estabeleceram conjuntamente a Cities Alliance (com sede na sede do Banco Mundial em Washington, D.C.) para orientar a formulação de políticas, compartilhamento de conhecimento e distribuição de subsídios em torno da questão da pobreza urbana.[231] (O ONU-Habitat desempenha um papel consultivo na avaliação da qualidade da governança de uma localidade.)[117] As políticas do Banco Mundial tendem a se concentrar no fortalecimento dos mercados imobiliários por meio de crédito e assistência técnica.[232]
A Organização das Nações Unidas para a Educação, a Ciência e a Cultura, a UNESCO, tem se concentrado cada vez mais nas cidades como locais-chave para influenciar a governança cultural. Desenvolveu várias redes de cidades, incluindo a Coalizão Internacional de Cidades contra o Racismo e a Rede de Cidades Criativas. A capacidade da UNESCO de selecionar locais como um Patrimônio Mundial confere à organização uma influência significativa sobre o capital cultural, turismo e financiamento de preservação histórica.[233]
As cidades figuram com destaque na cultura ocidental tradicional, aparecendo na Bíblia em formas malignas e sagradas, simbolizadas pela Babilônia e Jerusalém.[234] Caim e Ninrode são os primeiros construtores de cidades no livro de Gênesis. Na mitologia suméria, Gilgamesh construiu as muralhas de Uruk. As cidades podem ser percebidas em termos de extremos ou opostos: ao mesmo tempo libertadoras e opressoras, ricas e pobres, organizadas e caóticas.[235] O termo antiurbanismo refere-se a vários tipos de oposição ideológica às cidades, seja por sua cultura ou por sua relação política com o país. Tal oposição pode resultar da identificação das cidades com a opressão e a elite dominante.[236] Esta e outras ideologias políticas influenciam fortemente as narrativas e temas no discurso sobre as cidades.[10] Por sua vez, as cidades também simbolizam suas sociedades de origem.[237]
Escritores, pintores e cineastas produziram inúmeras obras de arte sobre a experiência urbana. A literatura clássica e medieval inclui um gênero de descrições que tratam das características e da história da cidade. Autores modernos como Charles Dickens e James Joyce são famosos por descrições evocativas de suas cidades de origem.[238] Fritz Lang concebeu a ideia para seu influente filme de 1927, Metropolis, enquanto visitava Times Square e se maravilhava com a iluminação noturna de neon.[239] Outras representações cinematográficas iniciais de cidades no século XX geralmente as retratavam como espaços tecnologicamente eficientes com sistemas de transporte automobilístico que funcionavam sem problemas. Na década de 1960, no entanto, o congestionamento do tráfego começou a aparecer em filmes como The Fast Lady (1962) e Playtime (1967).[175]
Literatura, cinema e outras formas de cultura popular forneceram visões de cidades futuras tanto utópicas quanto distópicas. A perspectiva de cidades mundiais em expansão, comunicação e cada vez mais interdependentes deu origem a imagens como "Nylonkong" (Nova York, Londres, Hong Kong)[280] e visões de uma única ecumenópolis global.[240]
O Oriente Próximo, ou Próximo-Oriente, é uma região geográfica que abrange diferentes países para arqueólogos e historiadores, de um lado, e para cientistas políticos, economistas e jornalistas, de outro. O termo foi aplicado originalmente para os Estados dos Balcãs no Leste Europeu, mas hoje em dia, normalmente, descreve os países do Sudoeste Asiático entre o Mar Mediterrâneo e o Irã/Irão, especialmente em contextos históricos.
O termo, tal como é usado pelos arqueólogos, geógrafos e historiadores ocidentais, refere-se à região que engloba a Anatólia (a porção asiática da Turquia moderna), o Levante (Síria, Líbano, Jordânia, Chipre, Israel e territórios palestinos), Mesopotâmia (Iraque) e, ocasionalmente, Transcaucásia (Geórgia, Armênia e Azerbaijão). Nos contextos políticos e jornalísticos modernos, esta região é normalmente considerada como compreendida no Oriente Médio, enquanto que os termos Oriente Próximo ou Sudoeste Asiático são preferíveis nos contextos arqueológicos, geográficos e históricos.
O Oriente Próximo, ou Próximo Oriente, compreende a região da Ásia próxima ao mar Mediterrâneo, a oeste do rio Eufrates, incluindo: Síria, Líbano, Israel, Palestina e Iraque.
O termo Oriente Próximo entrou em uso nos anos de 1890, quando potências europeias se debateram com duas situações críticas no leste. A guerra Sino-Japonesa ocorrida no período de 1890-1895 no Extremo Oriente.
Cuba (pronunciado em castelhano: [ˈkuβa]), oficialmente República de Cuba, é um país insular localizado no mar do Caribe, na América Central e Caribe (sub-continente da América). É um país que compreende a ilha de Cuba, bem como a Ilha da Juventude e vários arquipélagos menores. Cuba está localizada no norte do Caribe, onde o mar do Caribe, o Golfo do México e o Oceano Atlântico se encontram. Fica a leste da Península de Iucatã (México), a sul tanto do estado norte-americano da Flórida como das Bahamas, a oeste da Hispaniola, e a norte tanto da Jamaica como das Ilhas Caimão. Havana é a maior cidade e capital; outras grandes cidades incluem Santiago de Cuba e Camagüey. A área oficial da República de Cuba é de 109 884 quilômetros quadrados (sem as águas territoriais). A ilha principal de Cuba é a maior ilha de Cuba e do Caribe, com uma área de 104 338 quilómetros quadrados. Cuba é o segundo país mais populoso do Caribe, depois do Haiti, com mais de 11 milhões de habitantes.[6]
O território que hoje é Cuba foi habitado pelos povos ciboneis taínos desde o quarto milênio a.C. até à colonização espanhola no século XV.[7] A partir do século XV, foi uma colónia de Espanha até à Guerra Hispano-americana de 1898, quando Cuba foi ocupada pelos Estados Unidos e ganhou a independência nominal como protectorado de facto dos Estados Unidos em 1902. Sendo uma república frágil, em 1940 Cuba tentou fortalecer o seu sistema democrático, mas a radicalização política crescente e os conflitos sociais culminaram num golpe e subsequente ditadura apoiada pelos Estados Unidos de Fulgencio Batista em 1952.[8][9][10] A corrupção aberta e a opressão sob o governo de Batista[11] levaram à sua destituição a Janeiro de 1959 pelo Movimento 26 de Julho, que depois estabeleceu uma ditadura do proletariado sob a liderança do Partido Comunista de Cuba, sendo Fidel Castro um dos seus fundadores, eleito primeiro secretário do comité central desde 1965 até 2011.[12][13][14] A Assembleia Nacional do Poder Popular é o parlamento legislativo da República de Cuba e o órgão supremo do poder do Estado[15] e seu atual presidente é Esteban Lazo Hernández. O atual presidente da República de Cuba é Miguel Díaz-Canel, que também é o atual primeiro secretário do PCC. O país foi um ponto de discórdia durante a Guerra Fria entre a União Soviética e os Estados Unidos, e uma guerra nuclear quase eclodiu durante a Crise dos Mísseis de Cuba de 1962. Cuba é um dos atuais Estados socialistas marxistas-leninistas existentes.
Sob Castro, Cuba esteve envolvida numa vasta gama de actividades militares e humanitárias na Ásia e na África.[16] Cuba enviou mais de 400 mil dos seus cidadãos para combater em Angola (1975-91) e derrotou as forças armadas da África do Sul em guerra convencional envolvendo tanques, aviões, e artilharia.[17] A intervenção cubana em Angola contribuiu para a queda do regime do apartheid na África do Sul.[18] Culturalmente, Cuba é considerada parte da América Latina.[19]  É um país multiétnico cujo povo, cultura e costumes derivam de diversas origens, incluindo os povos Taínos Ciboneis, o longo período do colonialismo espanhol, a introdução dos escravos africanos e uma relação estreita com a União Soviética na Guerra Fria.
Cuba é um Estado soberano e membro fundador das Nações Unidas, do G77, do Movimento Não Alinhado, dos Países ACP, da ALBA e da Organização dos Estados Americanos. Tem uma das únicas economias planificadas do mundo, e a sua economia é dominada pela indústria do turismo e pelas exportações de mão-de-obra qualificada, açúcar, tabaco, e café. De acordo com o Índice de Desenvolvimento Humano, Cuba tem um elevado desenvolvimento humano e está classificada em oitavo lugar na América do Norte, e em 72º lugar mundialmente em 2019.[20] Também ocupa um lugar de destaque em algumas métricas de desempenho nacional, incluindo cuidados de saúde e educação.[21][22] É o único país do mundo a satisfazer as condições de desenvolvimento sustentável estabelecidas pela WWF.[23] De acordo com o Programa Alimentar Mundial das Nações Unidas, as políticas governamentais cubanas erradicaram em grande escala a fome e a pobreza.[24]
Não existe consenso quanto à origem do nome cuba. Entre as diferentes versões, há duas que se destacam: a primeira diz que a palavra deriva dos termos taínos cubao, que significa onde a terra fértil abunda,[25] ou coabana ou cubanacán, que se traduziriam como lugar amplo,[26][27] e outra versão diz que deriva da contração de duas palavras aruaques: coa (lugar, terra, terreno) e bana (grande).[28] Circula ainda a teoria de que Cristóvão Colombo possa ter nascido em Cuba, uma vila portuguesa no Alentejo, e esta ser a origem do nome da ilha.[29]
No último século, estudos arqueológicos, etnológicos e morfológicos permitiram investigar a vida dos primeiros habitantes que chegaram à Cuba por volta de 6 000 a.C. Esses primeiros grupos eram caçadores paleolíticos de origem mongolóide. A segunda migração veio a ocorrer há 4 500 a.C., vinda da América Central e do Sul. Eles tinham uma fisionomia semelhante à do primeiro grupo. As terceira e quarta migrações são provenientes das Antilhas, cuja ocorrência deu-se por volta de 500 a.C.[30]
O território que atualmente constitui Cuba veio a ser habitado principalmente por povos ameríndios conhecidos como taínos, também chamados de aruaques pelos espanhóis, e guanajatabeis e ciboneis antes da chegada dos colonizadores. Os antepassados desses nativos haviam migrado séculos antes da parte continental das Américas do Sul, Central e do Norte.[31] Os nativos taínos chamavam a ilha de Caobana.[32] Os povos taínos eram agricultores e caçadores, ao passo que os ciboneis eram pescadores e caçadores e os guanatabeyes eram coletores.[33]
A ilha de Cuba foi descoberta pelos europeus com a chegada de Cristóvão Colombo em 1492, que a batizou com o nome de Juana, uma homenagem à Joana de Castela, filha de Fernando II, então rei da Espanha. Entretanto, o nome não vingou e o local ficou conhecido pelo nome nativo. Colombo morreu acreditando que Cuba fosse uma península do continente americano. A condição insular de Cuba foi esclarecida somente com explorações de Sebastián de Ocampo, que deu a volta completa à ilha em 1509, verificando a existência de nativos pacíficos e áreas para cultivar e aportar. A ocupação da ilha foi um dos primeiros passos para a colonização do território pela Espanha.[34]
Diego Velázquez, em 1510, desembarcou na ilha e fundou a vila de Baracoa. No mesmo ano, a Espanha estabeleceu o Governo de Cuba, primeira administração da região, que englobava, além do território atual de Cuba, áreas da Flórida e da Luisiana espanhola. Velázquez acabou por fundar outras vilas e localidades na então capitania, entre as quais estavam San Salvador de Bayamo (1513), Villa De la Santísima Trinidad, Santa María del Puerto del Príncipe (hoje Camaguey),  San Cristóbal de La Habana e Sancti Spíritus (1514), além de Santiago de Cuba (1515), que foi a primeira capital cubana.[34]
Os indígenas Taínos foram forçados a trabalhar sob o sistema de encomienda,[35] que se assemelhava ao sistema feudal da Europa medieval.[36] Em um século, os povos indígenas foram virtualmente exterminados devido a vários fatores, principalmente doenças infecciosas da Eurásia, à qual não tinham resistência natural (imunidade), agravada pelas duras condições da repressiva subjugação colonial.[37] Em 1529, um surto de sarampo em Cuba matou dois terços dos poucos nativos que haviam sobrevivido à varíola.[38]
Em 10 de fevereiro de 1516, a pedido de Velázquez, foi criado o bispado de Cuba, cuja sede original era em Baracoa, tendo sido transferida para Santiago de Cuba em 1523. Santiago de Cuba servia como a primeira capital de Cuba, até a transferência definitiva da sede do governo para San Cristóbal de La Habana, em meados do século XVI.[30] Por outro lado, a cidade de San Cristóbal de La Habana, situada na costa sul da parte ocidental da ilha, foi deslocada pelo menos duas vezes até que, em 16 de novembro de 1519, foi finalmente estabelecida em sua localização atual. Esta última data é considerada a fundação definitiva da cidade.[39]
Velázquez veio a servir como governador da região até a sua morte, em 1524. Após ele, aportaram em Cuba Pánfilo de Narváez e Juan de Grijalva, que não encontraram resistência dos indígenas. Durante a colonização, a Espanha investiu em monoculturas de açúcar e tabaco, utilizando o sistema de plantagem que, no início, utilizava-se de mão de obra escrava indígena. Cerca de trinta anos após a chegada dos espanhóis, a população indígena já havia se reduzido de cerca de 120 mil para algumas centenas, devido a vários fatores, tais como doenças, maus tratos e extermínio. Com a redução da população indígena, a mão de obra escrava começou a ficar escassa. Assim sendo, Diego Velázquez, que havia dado início à exploração de minas, começou a substituir os nativos por escravos africanos, em caráter semelhante ao que ocorria em outras colônias espanholas e portuguesas na América.[34]
Cuba foi integrada ao Vice-Reino da Nova Espanha quando este foi criado em 1535. A ilha e suas províncias formavam o Governo de Cuba, que dependia da Capitania-Geral de São Domingos. Posteriormente, recebeu maior autonomia a partir de 1764, como resultado das reformas Bourbon realizadas no Vice-Reino da Nova Espanha pelo Conde de Floridablanca. O Governo de Cuba incluía, além da ilha de Cuba, a Jamaica (até 1655), a província da Flórida (a partir de 1567) e a Louisiana espanhola (a partir de 1763). Em 1777 foi instituída a Capitania-Geral de Cuba como entidade sucessora do Governo de Cuba, com maior autonomia e poderes, o que incluía os referidos territórios.[30]
Nos primeiros anos da colônia, o setor econômico mais importante foi a extração de ouro e outras formas de mineração. San Cristóbal de La Habana acabou por se beneficiar das guerras travadas entre França e Espanha, uma vez que, após o ataque francês à ilha, a Coroa espanhola elaborou dois projetos visando repelir as pretensões francesas, cuja implementação envolvia a capital cubana. O primeiro projeto foi o  Sistema de Frota ou Porto Único, por meio do qual todos os navios espanhóis em tráfego pelas Índias Ocidentais deveriam partir juntos, de volta à Espanha, do Porto de Carenas, na baía de San Cristóbal de La Habana. O segundo projeto visava fortificar a cidade, que teve como precedente a construção, em 1538, da segunda fortaleza da América, o chamado Castelo da Força Real de Havana, além de outros fortes como o Castillo del Morro e o de la Punta.[30]
Em consequência à estes projetos, San Cristóbal de La Habana experimentou um significativo crescimento econômico e populacional sem precedentes, eis que todo o comércio mantido pela metrópole nas Índias Ocidentais passava pela cidade. A diversificação de sua economia, através da criação de novos comércios, também foi significativa. Esse desenvolvimento econômico na capital contrastava fortemente com a redução da atividade comercial de populações remotas, que iniciaram o contrabando de corsários estrangeiros.[30]
Em 18 de maio de 1539, o conquistador Hernando de Soto partiu de Havana com cerca de 600 seguidores para uma vasta expedição pelo sudeste norte-americano, começando no que hoje é a Flórida, em busca de ouro, tesouro e poder.[40] Em 1º de setembro de 1548, o Dr. Gonzalo Perez de Angulo foi nomeado governador de Cuba. Ele chegou a Santiago em 4 de novembro de 1549 e imediatamente declarou a liberdade de todos os indígenas.[41] Ele se tornou o primeiro governador permanente de Cuba a residir em Havana, ao invés de Santiago, e construiu a primeira igreja de Havana em alvenaria.[42]
Em 1570, a maioria dos residentes de Cuba era composta por uma mistura de heranças espanholas, africanas e taíno. Cuba desenvolvia-se lentamente e, ao contrário de outras ilhas do Caribe, tinha uma agricultura diversificada. A colônia detinha uma sociedade urbanizada que apoiava o império colonial espanhol. Em meados do século XVIII, havia 50 mil escravos na ilha, em comparação com 60 mil em Barbados, 300 mil na Virgínia e outros 450 mil em Saint-Domingue, todos com grandes plantações de cana-de-açúcar.[43]
A eclosão da Guerra dos Sete Anos, entre a França e a Inglaterra, acabou por dividir as potências europeias. A França foi apoiada pelo Arquiducado da Áustria, Suécia–Finlândia e pelo Império Russo, enquanto a Inglaterra recebeu o apoio do Reino de Portugal, Reino da Prússia e da Confederação Iroquois. Neste cenário, a Espanha aliou-se aos franceses. Esta aliança serviu como motivação para os ingleses liderarem a maior marinha que havia cruzado o Atlântico, em 1762, com o objetivo de invadir e tomar Havana, que foi defendida de forma combativa pelos crioulos e espanhóis. Porém, em 12 de agosto daquele ano, foi assinada a capitulação da cidade, após Havana se render. No dia seguinte, as tropas britânicas entraram triunfantes. Os britânicos imediatamente abriram o comércio com suas colônias norte-americanas e caribenhas, causando uma rápida transformação da sociedade cubana.[44] Essa ocupação durou onze meses. Em julho de 1763, Ambrosio de Funes Villalpando tomou posse do Governo de Cuba e o reivindicou à Espanha, que cedeu a península da Flórida à Grã-Bretanha, considerada de posicionamento estratégico.[45]
Entre 1782 e 1785, o Capitão Geral de Havana e Governador de Cuba, Luis de Unzaga y Amézaga, coordenou a ajuda aos Estados Unidos, os quais solicitaram, para que pudessem vencer a guerra contra os ingleses. Luis de Unzaga y Amézaga recebeu, em Havana, a visita do Príncipe Guilherme IV em abril de 1783, chegando a acordos preliminares para o Tratado de Paris, como a troca de prisioneiros ou a troca das Bahamas pelo leste da Flórida. Em 1784, a boa relação de vizinhança com os Estados Unidos era tamanha que, após um ciclone que devastou a ilha, o governador Unzaga obteve o dinheiro necessário para a reconstrução de várias cidades e escolas. Isto fez com que Unzaga fosse considerado o criador do primeiro sistema educacional público bilíngue do mundo.[46]
O maior fator para o crescimento do comércio de Cuba no final do século XVIII e início do século XIX foi a Revolução Haitiana. Quando os povos escravizados daquela ilha - que havia sido a colônia mais rica do Caribe - se libertaram, por meio de uma revolta violenta, os proprietários cubanos perceberam as mudanças nas circunstâncias da região com um sentimento de medo e de oportunidade. O temor era advindo da perspectiva de que os escravos cubanos também pudessem se revoltar. As numerosas proibições, durante a década de 1790, da venda de escravos, em Cuba, que já haviam sido escravizados nas colônias francesas enfatizavam essa ansiedade. Com o colapso da escravidão e do colonialismo na colônia francesa, a ilha espanhola passou por transformações assemelhadas ao Haiti.[47] As estimativas sugerem que entre 1790 e 1820 cerca de 325 mil africanos foram importados para Cuba como escravos, o que foi quatro vezes mais a quantidade que havia chegado entre 1760 e 1790.[48]
Embora uma proporção menor da população de Cuba fosse escravizada, ocorreram revoltas por parte dos escravos. Entre 1811 e 1812, a Rebelião de Escravos Aponte aconteceu, consistindo num movimento abolicionista que foi rapidamente suprimido.[49] A liderança da rebelião foi atribuída à José Antonio Aponte, de origem iorubá, junto a outros escravos e ex-escravos de várias partes do país, entre os quais Nicolás Morales e Juan Nepomuceno, de origem congolesa. A população de Cuba em 1817 era de 630 980 habitantes (dos quais 291 021 eram brancos, 115 691 eram mestiços livres e 224 268 escravos negros).[50]
Em parte devido aos escravos cubanos trabalharem principalmente em ambientes urbanizados, no século XIX, a prática da coartacion se desenvolveu (ou "comprar-se da escravidão", um fenômeno exclusivamente cubano").[51] Devido à escassez de mão de obra branca, os negros dominaram as indústrias urbanas a tal ponto que quando os brancos em grande número foram para Cuba, em meados do século XIX, foram incapazes de deslocar os trabalhadores negros. Um sistema de agricultura diversificada, com pequenas fazendas e menos escravos, serviu para abastecer as cidades com produtos e outros bens.[43]
Na década de 1820, quando o restante do império espanhol na América Latina se rebelou e formou estados independentes, Cuba permaneceu leal à Espanha. Sua economia se baseava em servir ao império. Em 1860, Cuba tinha 213 167 pessoas mestiças livres (39% de sua população não branca, num total de 550 mil habitantes).[43]
A independência total da Espanha foi o objetivo de uma rebelião em 1868 liderada pelo advogado Carlos Manuel de Céspedes. De Céspedes, dono de uma fazenda de açúcar, libertou seus escravos para que lutassem com ele pela independência de Cuba. Em 27 de dezembro de 1868, ele emitiu um decreto condenando a escravidão em teoria, mas aceitando-a na prática e declarando a liberdade de todos os escravos cujos senhores os apresentassem para o serviço militar.[53] A rebelião de 1868 resultou em um conflito prolongado conhecido como Guerra dos Dez Anos. Entre os combatentes, um grande número eram de voluntários da República Dominicana e outros países, bem como numerosos chineses servos.[54]
O novo governo cubano foi reconhecido por muitas nações europeias e latino-americanas, mas os Estados Unidos optaram por o não fazer.[55] Em 1878, o Pacto de Zanjón encerrou o conflito, com a Espanha prometendo maior autonomia a Cuba. Entre 1879 e 1880, o patriota cubano Calixto García tentou iniciar uma segunda guerra conhecida como Guerra Chiquita, mas não conseguiu receber apoio suficiente.[56] A escravidão em Cuba foi abolida em 1875, mas o processo foi concluído apenas em 1886.[57][53] A abolição da escravidão se deu com base em um processo de intensa pressão externa, ante a resistência da Inglaterra ao tráfico de escravos, além do interesse de investidores estadunidenses que desejavam controlar o mercado exportador cubano. Contribuiu também para o alcance da abolição a condição frágil em que os grandes proprietários de terra cubanos se encontravam, muito afetados pelos efeitos de devastação causados pela Guerra dos Dez Anos travada contra a Espanha.[58]
Em 1892, um dissidente exilado chamado José Martí fundou o Partido Revolucionário Cubano em Nova Iorque, objetivando alcançar a independência de Cuba da Espanha.[59] Em janeiro de 1895, Martí viajou para San Fernando de Monte Cristi e Santo Domingo, na República Dominicana, para unir-se aos esforços de Máximo Gómez, outro idealizador da independência. Martí registrou suas opiniões políticas no Manifesto de Montecristi. A luta contra o exército espanhol começou em 24 de fevereiro 1895, mas Martí foi incapaz de chegar a Cuba no início dos entraves, morrendo na batalha de Dos Ríos em 19 de maio de 1895. Sua morte imortalizou-o como herói nacional de Cuba.[59][60]
Cerca de 200 mil soldados espanhóis superaram o exército rebelde, muito menor, que dependia principalmente de táticas de guerrilha, sabotagem e da ocupação de faixas do litoral e alguns pontos considerados estratégicos. Os espanhóis iniciaram uma campanha de repressão, com o general Valeriano Weyler, governador militar de Cuba, conduzindo a população rural para o que chamou de "reconcentrados", descritos por observadores internacionais como "cidades fortificadas". Muitas vezes, estes espaços são considerados o protótipo dos campos de concentração do século XX.[61] Entre 200 mil e 400 mil[62] civis cubanos morreram de fome e doenças nos "reconcentrados" espanhóis, estimativas firmadas pela Cruz Vermelha e pelo senador norte-americano Redfield Proctor, ex-Secretário da Guerra do Governo dos Estados Unidos. A conduta espanhola foi reprovada por diversas nações, com os norte-americanos e europeus instigando protestos contra a atuação da Espanha na ilha.[63]
O encouraçado USS Maine foi enviado para proteger os interesses norte-americanos mas, logo após sua chegada, explodiu no porto de Havana e afundou rapidamente, matando quase três quartos da tripulação. A causa e a responsabilidade pelo naufrágio do navio permaneceram obscuras mesmo após a instauração de uma comissão de inquérito. A opinião popular nos Estados Unidos, alimentada por uma imprensa ativa, concluiu que os espanhóis eram os culpados e exigiu uma ação por parte das autoridades do país. A Espanha e os Estados Unidos declararam guerra entre si no final de abril de 1898.[64]
Após a Guerra Hispano-Americana, a Espanha e os Estados Unidos assinaram o Tratado de Paris de 1898, pelo qual a Espanha cedeu Porto Rico, as Filipinas e Guam aos norte-americanos sob o valor de 20 milhões de dólares e Cuba passou à condição de protetorado dos Estados Unidos.[65] Cuba conquistou a independência formal dos Estados Unidos em 20 de maio de 1902, como República de Cuba. Sob a nova constituição de Cuba, os Estados Unidos mantiveram o direito de intervir nos assuntos cubanos e supervisionar suas finanças e relações exteriores. De acordo com a Emenda Platt, os Estados Unidos alugaram a Base Naval da Baía de Guantánamo.[66]
Após as disputadas eleições em 1906, o primeiro presidente cubano, Tomás Estrada Palma, enfrentou uma revolta armada instada por veteranos da guerra da independência, que acabaram derrotando as escassas forças do governo.[67] Os Estados Unidos intervieram ocupando Cuba e nomeando Charles Edward Magoon como governador por três anos. Historiadores cubanos caracterizam o governo de Magoon como introdutor da corrupção política e social.[68] Em 1908, o autogoverno foi restaurado quando José Miguel Gómez foi eleito presidente, com o fator condicionante de que os Estados Unidos continuariam intervindo nos assuntos cubanos. Em 1912, o Partido Independiente de Color tentou estabelecer uma república negra separada, na província de Oriente, mas foi suprimido pelo general Monteagudo com considerável derramamento de sangue.[69]
Em 1924, Gerardo Machado foi eleito presidente. Durante sua administração, o turismo aumentou acentuadamente e hotéis e restaurantes de propriedade de norte-americanos foram construídos para acomodar o fluxo de turistas.[70] O crescimento turístico levou ao aumento da prática de jogos e da prostituição no país.[70] A Quinta-Feira Negra, em 1929, levou ao colapso do preço do açúcar, agitação política e repressão. Estudantes protestantes, conhecidos como a Geração de 1930, recorreram à violência em oposição ao governo Machado, cada vez mais impopular. Uma greve geral (na qual o Partido Comunista se aliou a Machado), revoltas entre os trabalhadores açucareiros e uma revolta do exército forçaram Machado ao exílio em agosto de 1933. Ele foi substituído por Carlos Manuel de Céspedes y Quesada.[71][72]
De 1934 a 1959, Fulgêncio Batista foi o dirigente de facto de Cuba, ocupando a presidência de 1940 a 1944 e de 1952 a 1959. A presidência de Batista impôs enormes regulações à economia, o que trouxe grandes problemas para a população.[73] O desemprego se tornava um problema na medida em que os jovens que entravam no mercado de trabalho não conseguiam encontrar uma função para exercer.[73] A classe média, cada vez mais insatisfeita com a queda no nível de qualidade de vida, se opôs cada vez mais a Batista.[73] Ainda durante essa época, Cuba se transformou numa espécie de "ilha dos prazeres" dos turistas americanos. Aproveitando o agradável clima tropical e a beleza das paisagens naturais, foi construída toda uma infraestrutura voltada para os visitantes estrangeiros. Nesse cenário, misturavam-se corrupção governamental, jogatina de cassinos, uso indiscriminado de drogas e incentivo à prostituição. À época, Cuba era o país da América Latina com o maior consumo per capita de carnes, vegetais, cereais, automóveis, telefones e rádios, apesar de todos estes bens estarem concentrados nas mãos de uma pequena elite e de investidores estrangeiros.[74]
Em setembro de 1933, a Revolta dos Sargentos, liderada por Fulgencio Batista, derrubou Céspedes.[75] Um comitê executivo de cinco membros (a Pentarquia de 1933) foi escolhido para chefiar um governo provisório. Ramón Grau San Martín foi então nomeado presidente provisório.[76] San Martín renunciou em 1934, deixando o caminho livre para Batista, que dominou a política cubana nos 25 anos seguintes, a princípio por meio de uma série de presidentes-fantoches.[75] O período de 1933 a 1937 foi uma época de conflitos sociais e políticos virtualmente incessantes. Em suma, durante este período,  Cuba sofreu com estruturas políticas frágeis, refletidas no fato de ter visto três presidentes diferentes em dois anos (1935-1936) e nas políticas militaristas e repressivas encapadas por Fulgencio Batista, então como Chefe do Exército.[77]
Uma nova constituição foi adotada em 1940, que engendrou ideias progressistas radicais, incluindo o direito ao trabalho e à saúde. Fulgencio Batista foi eleito presidente no mesmo ano, mantendo o cargo até 1944.[78] Seu governo realizou importantes reformas sociais, ao passo que vários membros do Partido Comunista ocuparam cargos sob sua administração.[79] As forças armadas cubanas não estiveram muito envolvidas no combate durante a Segunda Guerra Mundial, embora o presidente Batista tenha sugerido um ataque conjunto entre os norte-americanos e a América Latina à Espanha franquista, com o objetivo de derrubar o regime espanhol, tido como autoritário.[80] Cuba perdeu seis navios mercantes durante a guerra e a Marinha cubana foi responsável pelo naufrágio do submarino alemão U-176.[81]
Batista aderiu às restrições da constituição de 1940, que impedia sua reeleição, e Ramon Grau San Martín foi eleito nas eleições de 1944.[78] O governo San Martín foi acusado de deslegitimar o sistema político cubano, em particular o Congresso e a Suprema Corte.[82] Carlos Prío Socarrás, um aliado de San Martín, tornou-se presidente em 1948.[78] Os dois governos comandados pelo Partido Auténtico trouxeram um influxo de investimentos, que alimentou um crescimento econômico, elevou os padrões de vida para todos os segmentos da sociedade e criou uma classe média na maioria das áreas urbanas.[83]
Após deixar a presidência em 1944, Fulgencio Batista morou na Flórida, retornando a Cuba para se candidatar à presidência em 1952. Enfrentando uma possível derrota eleitoral, Batista liderou um golpe militar que antecipou a eleição.[84] De volta ao poder e recebendo apoio financeiro, militar e logístico do governo dos Estados Unidos, Batista suspendeu a Constituição de 1940 e revogou a maioria das liberdades políticas, o que incluía o direito de greve.[85] Ele então se aliou aos proprietários de terras mais ricos que possuíam as maiores plantações de açúcar e presidiu uma economia estagnada que aumentava a desigualdade entre os cubanos ricos e os pobres, proibindo o Partido Comunista Cubano em 1952.[86] Depois do golpe, Cuba passou a ter as maiores taxas de consumo per capita, na América Latina, de carnes, vegetais, cereais, automóveis, telefones e rádios, embora cerca de um terço da população fosse considerada pobre e desfrutasse relativamente pouco desse consumo.[87]
Em 1958, Cuba era um país relativamente avançado para os padrões latino-americanos. Ainda que o país apresentasse uma visível pobreza e precariedade no emprego, havia sinais de modernização que, em alguns casos, eram superiores aos de países ricos. Cuba possuía uma média de carros, por habitantes, que a colocava em sexto lugar no mundo no consumo deste bem, sendo superada apenas pelos Estados Unidos, Canadá, Inglaterra, Venezuela e Alemanha Ocidental. Em 1953, importava um número de veículos tratores superior ao do Brasil e México; possuía a maior quantidade de importação de aparelhos de televisão na América Latina (proporcional ao número de habitantes); ostentava o sexto lugar, em âmbito local, em número de jornais publicados; era o quarto país com mais emissoras de rádio e salas de cinema e detinha o terceiro lugar na captação de investimentos diretos dos Estados Unidos, com cerca de 861 milhões de dólares investidos no país em 1958. Ainda que assim fosse, a estrutura do sistema econômico era desigual, beneficiando apenas grupos da aristocracia rural, a burguesia da indústria turística e imobiliária e profissionais liberais.[88]
Cuba adotava uma política de interferência nas relações sindicais, o que incluía a proibição de demissões e da mecanização. Entre 1933 e 1958, a nação ampliou enormemente as regulamentações econômicas, causando problemas econômicos e desemprego. A classe média, que era comparável à dos Estados Unidos, tornou-se cada vez mais insatisfeita com o desemprego e a perseguição política. Fulgencio Batista renunciou em dezembro de 1958, sob pressão dos Estados Unidos, ao mesmo tempo em que as forças revolucionárias lideradas por Fidel Castro estavam vencendo militarmente no interior do país.[89][90]
Na década de 1950, várias organizações, incluindo algumas que defendiam o levante armado, competiram pelo apoio público para promover mudanças políticas. Em 1956, Fidel Castro e cerca de 80 apoiadores desembarcaram do iate Granma, na tentativa de iniciar uma rebelião contra o governo Batista. Em 1958, o Movimento 26 de Julho, de Castro, emergiu como o principal grupo revolucionário. Os Estados Unidos apoiaram Castro sob a imposição de um embargo de armas em 1958 contra o governo de Batista. Batista tentou evitar o embargo norte-americano adquirindo armas da República Dominicana.[91]
Em fins de 1958, os revolucionários haviam escapado de Serra Maestra e lançaram uma insurreição popular geral. Depois que os combatentes de Castro tomaram Santa Clara, Batista fugiu com a família para a República Dominicana, em janeiro de 1959. Posteriormente, veio a exilar-se na ilha portuguesa da Madeira, estabelecendo-se finalmente no Estoril, perto de Lisboa. As forças de Fidel Castro entraram na capital em 8 de janeiro de 1959. O liberal Manuel Urrutia Lleó tornou-se o presidente provisório.[92][93]
Inicialmente, o governo dos Estados Unidos reagiu favoravelmente à revolução cubana.[94] Castro legalizou o Partido Comunista, promulgou a Lei de Reforma Agrária e expropriou milhares de hectares de terras agrícolas, incluindo aquelas de grandes proprietários de terras dos Estados Unidos. A partir de então, as relações entre o novo governo revolucionário e os norte-americanos começaram a se deteriorar. Em resposta, os Estados Unidos impuseram uma série de sanções entre 1960 e 1964, incluindo a proibição total do comércio entre os países e o congelamento de todos os ativos de propriedade de cubanos nos Estados Unidos.[95] Castro assinou um acordo comercial com o vice-primeiro-ministro soviético Anastas Mikoyan.[94]
Em março de 1960, o presidente estadunidense Dwight D. Eisenhower deu sua aprovação a um plano da Central Intelligence Agency (CIA) para armar e treinar um grupo de refugiados cubanos, visando derrubar o governo de Fidel Castro.[97] A invasão da Baía dos Porcos, como ficou conhecida, foi levada a cabo em 14 de abril de 1961, durante o mandato do presidente John F. Kennedy, quando cerca de 1.400 exilados cubanos desembarcaram na Baía dos Porcos. Tropas cubanas e milícias locais derrotaram a invasão, matando mais de 100 invasores e fazendo o restante prisioneiro.[98] Em janeiro de 1962, Cuba foi suspensa da Organização dos Estados Americanos (OEA) e, no mesmo ano, a OEA começou a impor sanções contra Cuba, de natureza semelhante às sanções americanas.[99] A crise dos mísseis cubanos (outubro de 1962), em plena Guerra Fria, quase desencadeou a Terceira Guerra Mundial.[100][101] Em 1963, Cuba estava se movendo em direção a um sistema comunista de pleno direito modelado na União das Repúblicas Socialistas Soviéticas (URSS).[102]
A partir de então, Cuba passou a apoiar outras nações e movimentos com ideais socialistas e comunistas. Durante a Guerra das Areias, travada entre a Argélia e o Marrocos, o país apoiou militarmente os argelinos.[103] Em 1964, uma reunião de comunistas latino-americanos foi organizada em Havana, desencadeando uma guerra civil em Santo Domingo, capital dominicana, o que levou os Estados Unidos a invadir a República Dominicana em 1965. Por outro lado, Che Guevara - que havia se engajado em atividades de guerrilha na África - foi morto em 1967 enquanto tentava iniciar uma revolução na Bolívia. Durante a década de 1970, Fidel Castro enviou milhares de soldados à África em apoio às guerras apoiadas pelos soviéticos no continente, apoiando especialmente o Movimento Popular de Libertação de Angola (MPLA).[104] Em 1975, Cuba efetivou uma das mais rápidas mobilizações militares da história, ao enviar cerca de 65 mil soldados e 400 tanques de fabricação soviética à Angola, em apoio ao MPLA. Seu apoio ao MPLA perdurou até 1988 quando, na batalha de Cuito Cuanavale, o MPLA derrotou a União Nacional para a Independência Total de Angola (UNITA) e as forças sul-africanas do apartheid.[105] A atuação cubana na África também se caracterizou por seu apoio à República Democrática Popular da Etiópia, sob liderança de Mengistu Haile Mariam, tendo sido proeminente em 1978, quando o país ajudou a derrotar uma invasão somali em território etíope.[106][107]
A atuação e o poderio militar cubano também foi registrado em outras partes do mundo. No Oriente Médio, o país forneceu apoio bélico ao Iémen do Sul, que travava conflitos com o Iêmen do Norte, envolvendo-se também na Guerra do Yom Kippur entre Israel e uma coalizão de estados árabes, como Egito e Iraque. Na América Latina, Fidel Castro apoiou insurgências marxistas em Guatemala, El Salvador e Nicarágua. No Sudeste Asiático, o país apoiou deliberadamente o Vietnã do Norte contra o Vietnã do Sul, em um conflito que reuniu forças e colocou em lados opostos o Estados Unidos e a União Soviética, durante a Guerra do Vietnã.[108]
O regime socialista cubano adotou como política de Estado tratativas de combate à desigualdade social, o que permitiu, hodiernamente, a eliminação do analfabetismo através da Campanha Nacional de Alfabetização em Cuba, a implementação de um sistema de saúde pública universal e a diminuição significativa das taxas de mortalidade infantil.[109] Ao longo dos anos da revolução, críticos do regime de Cuba passaram a sustentar afirmações envolvendo a possível violação de direitos humanos no país, como a existência das Unidades Militares de Ajuda à Produção que, a partir de 1965, guiava a campos de trabalhos forçados pessoas consideradas alheias à moral revolucionária. O historiador Abel Serra Madero estima que até 30 mil cubanos foram enviados a estes campos, onde predominavam delinquentes comuns, dissidentes políticos, religiosos e homossexuais.[110] Outro estudo, conduzido pela Universidade Federal de Uberlândia (UFU), enfoca que, a partir de 1971, pessoas homossexuais passaram a sofrer exonerações de cargos públicos ligados à área cultural, sob a justificativa de que não reuniam os parâmetros políticos e morais conscritos à revolução, especialmente após a realização do I Congresso de Educação e Cultura.[111]
A OEA veio a suspender suas sanções contra Cuba em 1975, com a aprovação de 16 Estados membros. Os Estados Unidos, entretanto, mantiveram suas próprias sanções.[112] Em 1979, os norte-americanos se opuseram à presença de tropas de combate soviéticas em Cuba. Em 23 de outubro de 1983, as forças dos Estados Unidos invadiram a ilha caribenha de Granada, sob a justificativa de que a ilha estaria muito próxima politicamente de Cuba e da União Soviética após o golpe de Estado de Bernard Coard, além de relatos de ameaças ao território dos Estados Unidos.[108]  Neste intervalo, os estadunidenses mataram quase duas dezenas de trabalhadores da construção civil de origem cubana, expulsando de Granada o restante da força de ajuda enviada por Cuba.[107][113]
As tropas soviéticas começaram a se retirar de Cuba em setembro de 1991.[107] Com a dissolução da União Soviética, o governo revolucionário foi severamente testado e o país enfrentou uma crise econômica severa, em parte devido à retirada dos subsídios soviéticos. A crise econômica resultou em efeitos como a escassez de alimentos e combustível, ao passo que uma manifestação popular notável ocorreu em Havana em agosto de 1994.[114] Outro efeito da crise, que perdurou até 1995, foi a queda significativa do Produto interno bruto (PIB) cubano, cujo valor chegou a encolher 35%. Mais de cinco anos foram necessários para que o PIB do país atingisse os níveis anteriores à crise.[115][116]
A década de 1990 também propiciou uma aproximação relevante entre Cuba e China, fator experimentado especialmente após a desintegração da União Soviética, que comprava 60% do açúcar e fornecia petróleo e manufaturas. Nos anos subsequentes, o país também aproximou-se da Venezuela e Bolívia, sob o governo de Hugo Chávez e Evo Morales, respectivamente, além de flexibilizar a economia, permitindo, dentro da estrutura socialista, a abertura para atividades capitalistas, como o turismo. Na década seguinte, em 2003, registrou-se a Primavera Negra, onde um grande número de dissidentes cubanos foram presos em nome do governo sob a acusação de que agiam como agentes do Estados Unidos. Posteriormente, os dissidentes foram liberados, com alguns passando a viver no exílio na Espanha.[117][118]
Fidel Castro renunciou ao cargo de Presidente do Conselho de Estado em fevereiro de 2008, alegando motivos de saúde. Raúl Castro, de quem Fidel era irmão, foi declarado o novo presidente. Em 3 de junho de 2009, durante sua 39ª Assembleia Geral, a OEA adotou uma resolução objetivando retirar a suspensão de Cuba da organização.[119] Cuba, cuja suspensão se dava desde 1962, motivada pela adesão cubana ao marxismo-leninismo e alinhamento com o bloco comunista, pôde reintegrar-se á organização e de todas as instituições subordinadas à OEA, como a Comissão de Direitos Humanos.[120][121] Até o momento, Cuba não solicitou sua reincorporação, sendo considerada pela OEA um membro inativo.[122]
Em janeiro de 2013, Cuba encerrou a exigência estabelecida em 1961, de que qualquer cidadão que desejasse viajar para o exterior fosse obrigado a obter uma licença governamental e uma carta-convite.[123] Em 1961, o governo cubano impôs amplas restrições às viagens para evitar a emigração em massa de pessoas após a revolução de 1959; aprovando vistos de saída apenas em raras ocasiões.[124] A partir de 2013, os requisitos foram simplificados, necessitando-se apenas de passaporte e documento oficial. No primeiro ano desta nova política, cerca de 180 mil naturais de Cuba deixaram o país e retornaram.[125] Em 2016, os Estados Unidos, sob o Governo Barack Obama, iniciou uma política de reaproximação com Cuba, com o restabelecimento da diplomacia entre as duas nações, a retirada de restrições a cidadãos americanos que desejam visitar o país e a flexibilização do embargo econômico, para permitir a importação, exportação e certo comércio limitado, mas as negociações não prosperaram.[126]
Atualmente, Cuba é único país socialista do Ocidente e um dos poucos do mundo, ao lado da China, da Coreia do Norte, do Vietnã e do Laos. A nação aprovou uma nova constituição em 2019, por meio de um plebiscito nacional. A nova constituição mantém o Partido Comunista como o único partido político legítimo, alça o acesso à saúde e educação como direitos fundamentais, impõe limites aos mandatos presidenciais, consagra o direito à representação legal na prisão, reconhece a propriedade privada e fortalece os direitos das multinacionais que investem com o estado.[127] Em 2021, 250 mil cubanos, ou 2% da população, imigrou para os Estados Unidos devido à Pandemia de COVID-19. [128]
Cuba é um arquipélago formado por mais de 1 500 ilhas.[129] As maiores são a Ilha de Cuba e a Ilha da Juventude (que tem uma superfície de 2 200 km²). A Ilha de Cuba é a maior ilha do Caribe, com uma superfície 104 945 km². O conjunto do arquipélago cubano possui uma superfície de 110 860 km² e uma dimensão linear máxima de cerca de 1 200 km. A ilha de Cuba é a 16.ª maior ilha do mundo. Banhada a norte pelo estreito da Flórida e pelo oceano Atlântico Norte, a noroeste pelo golfo do México, a oeste pelo canal da Península de Iucatã, a sul pelo mar do Caribe e a leste pela passagem de Barlavento.[130]
A ilha de Cuba está formada principalmente por planícies onduladas, com colinas e montanhas mais escarpadas situadas maioritariamente na zona sul da ilha. O ponto mais elevado é o Pico Real del Turquino com 1 974 m de altitude. O clima é tropical, embora temperado pelos ventos alísios. Existe uma estação relativamente seca de novembro a abril e uma estação mais chuvosa de maio a outubro. Havana é a capital e a cidade mais populosa. Santiago de Cuba e Camagüey são também cidades importantes. A Baía de Guantanamo contém uma base naval tomada pelos Estados Unidos desde 1903.[131]
Como a maior parte da ilha está ao sul do Trópico de Câncer, o clima local é tropical, moderado por ventos alísios vindos do nordeste que sopram durante todo o ano. A temperatura também é formada pela corrente caribenha, que traz água quente a partir do equador. Isso faz com que o clima de Cuba seja mais quente que o de Hong Kong, por exemplo, que está à mesma latitude de Cuba, mas tem um clima subtropical. Em geral (com variações locais), há uma estação seca de novembro a abril e uma estação mais chuvosa de maio a outubro. A temperatura média é de 21 °C em janeiro e 27 °C em julho. As temperaturas quentes do Mar do Caribe e o fato do país estar localizado na entrada do Golfo do México se combinam para tornar o país mais propenso a ciclones tropicais frequentes. Estes são os mais comuns entre os meses de setembro e outubro.[132]
Cuba é regularmente atingida por furacões durante o Verão e o Outono. Destes, destaca-se o furacão cubano de 1910, que afectou a ilha durante 5 dias, e o furacão de 1932, que continua a ser o mais mortífero a atingir o país, com mais de 3.000 mortos. Este último atingiu o máximo da categoria 5 na escala Saffir-Simpson, mas tinha descido para a categoria 4 antes de atingir Cuba. A temporada furacões de 2008 afectou gravemente a economia cubana, especialmente a agricultura e a pecuária: a destruição causada pelos ciclones tropicais foi estimada pelo líder Raúl Castro em 10 mil milhões de dólares (7,9 mil milhões de euros). 500 000 famílias foram afectadas, 156 000 hectares de cana-de-açúcar foram destruídos e 500 000 ficaram inundados.[133]
Perante as crises regulares provocadas pelos furacões, o povo e o Estado cubanos adquiriram experiência e desenvolveram uma logística para proteger as pessoas e os bens dos ventos selvagens que são comuns na ilha. Entre 1985 e 2004, Cuba foi atingida por dez grandes furacões, dos quais resultaram apenas vinte e duas mortes. Por exemplo, quando Cuba foi atingida pelo furacão Georges em 1998, houve apenas quatro mortes, em comparação com 600 noutros países afectados. Do mesmo modo, o furacão Charley, em Agosto de 2004, causou quatro mortes em Cuba contra 30 na Florida. Em Julho de 2005, o furacão Dennis também matou apenas 16 pessoas, mas causou danos consideráveis e obrigou à evacuação de mais de 1,5 milhões de pessoas. Foi um furacão de categoria 4 com ventos sustentados de 240 km/h, semelhante ao furacão de 1932 quando atingiu a costa cubana. Para o director da Agência das Nações Unidas para a Estratégia Internacional de Redução de Catástrofes, Salavano Briceno, este êxito cubano baseia-se na importância dos esforços de educação e de prevenção desenvolvidos pelas autoridades cubanas. A qualidade dos serviços de emergência cubanos foi objecto de um fórum específico na Conferência Mundial sobre a Redução de Catástrofes, em Janeiro de 2005.[134]
Cuba assinou a Convenção sobre Diversidade Biológica, em 12 de junho de 1992 durante a ECO-92 no Rio de Janeiro, e tornou-se membro da convenção em 8 de março de 1994.[135] Posteriormente, o país produziu uma Estratégia e um Plano de Ação Nacional de Biodiversidade, com uma revisão que foi recebido pela convenção sobre 24 de janeiro de 2008.[136]
A revisão compreende um plano de ação com prazos para cada item e uma indicação do órgão governamental responsável pela realização da meta. No entanto, existe quase nenhuma informação nesse documento sobre a biodiversidade em si.[137]
O quarto relatório nacional cubano para a convenção, no entanto, contém uma análise detalhada dos números de espécies de cada reino biológico registrado no território cubano, sendo os principais grupos: animais (17 801 espécies), bactérias (270 espécies), Chromista (707 espécies), fungos (5 844 espécies), plantas (9 107 espécies) e protozoários (1 440 espécies).[137]
Em 2017, a população cubana era de 11,2 milhões de habitantes,[1] incluindo 5 580 810 homens e 5 610 798 mulheres. A composição étnica era de 7 271 926 brancos, 1 126 894 negros e 2 778 923 mulatos (ou mestiços).[138] A população de Cuba tem origens muito complexas e casamentos entre os diversos grupos raciais são comuns. Há discordância sobre as estatísticas raciais. O Instituto para Estudos Cubanos e Cubano-Americanos da Universidade de Miami, diz que 62% da população é negra,[139] enquanto as estatísticas do censo do governo cubano afirma que 65,05% da população era branca em 2002. O Minority Rights Group International diz que "uma avaliação objetiva da situação dos afro-cubanos continua a ser problemática devido aos registros escassos e a uma falta de estudos sistemáticos tanto pré como pós-revolução. A estimativa do percentual de pessoas de ascendência africana na população cubana varia enormemente, variando de 33,9% para 62%".[140]
A taxa de natalidade cubana (9,88 nascimentos por mil habitantes em 2006)[141] é uma das mais baixas do hemisfério ocidental. Sua população em geral teve um aumento contínuo de cerca de 7 milhões em 1961 para mais de 11 milhões atualmente, mas o aumento foi interrompido nas últimas décadas e uma queda começou em 2006, com uma taxa de fecundidade de 1,43 filhos por mulher.[142] Esta queda da fecundidade é um das maiores do hemisfério ocidental.[143] Cuba tem acesso irrestrito ao aborto legal e uma taxa de aborto de 58,6 por mil gestações, em 1996, em comparação com uma média de 35 no Caribe, 27 na América Latina em geral e 48 na Europa. O uso de anticoncepcionais é estimado em 79% (um terço superior dos países do hemisfério ocidental).[144]
Antes da chegada dos europeus, a ilha de Cuba era habitada por dois povos indígenas: os ciboneyes, espalhados por toda a ilha, e os tainos, ocupando principalmente o centro e o leste. Pouco se sabe sobre os ciboneyes, porém é sabido que eram caçadores-coletores e não conheciam a cerâmica. Os tainos, por outro lado, eram sedentários, viviam em grandes aldeias e dominavam técnicas avançadas de agricultura.[145]
Com a chegada dos espanhóis, as populações indígenas foram drasticamente reduzidas em poucas gerações, como consequência da escravidão, de conflitos com os colonizadores, da fome e da infecção por doenças. Desde o século XVI, escravos africanos foram levados para Cuba, sendo que esse tráfico aumentou exponencialmente ao longo dos séculos. Os escravos foram trazidos principalmente para trabalharem nas minas de ouro, como forma de compensar a falta de trabalhadores em decorrência do extermínio dos índios.[145]
De maneira geral, os escravos africanos compunham uma pequena parte da população cubana, que continuou predominantemente de origem europeia, em razão da constante chegada de colonos espanhóis, sobretudo das Ilhas Canárias. A situação se modificou na segunda metade do século XVIII, quando cresceu rapidamente o número de africanos entrando em Cuba, fato que mudou dramaticamente a composição étnica da ilha. De acordo com estimativas, cerca de 700 mil africanos foram levados para a ilha durante todo o período do tráfico, embora outra fonte estime o número em mais de 1,3 milhão de pessoas. Os escravos vinham sobretudo do Oeste e do Sudeste da África.[145]
No século XIX, a imigração da Espanha, sobretudo das Canárias, se intensificou, na tentativa de "branquear" o país, haja vista o temor das classes dirigentes com a crescente presença africana na ilha. No Continente Americano, Cuba foi o segundo país que mais recebeu imigrantes espanhóis, ficando atrás apenas da Argentina (entre 1882 e 1930, mais de um milhão de espanhóis imigraram para Cuba). Naquela época, a economia cubana estava crescendo com base na produção de açúcar, fato que atraiu muitos imigrantes espanhóis, para trabalharem nas fábricas de açúcar do país, haja vista que, para exercer essa atividade, era necessário ter alguma qualificação.[146][147] Além dos europeus, imigrantes asiáticos, sobretudo de Bengala e do sul da China, foram trazidos para substituírem os escravos, quando o tráfico negreiro tornou-se ilegal. Cerca de 125 mil chineses entraram em Cuba, onde trabalharam em condições de semiescravidão, em meados do século XIX.[145] Em torno de 95% desses imigrantes chineses eram do sexo masculino, de modo que muitos desses chineses não tiveram filhos ou se casaram com mulheres não chinesas. Assim, após algumas gerações, essa presença chinesa desapareceu ou foi diluída pela mestiçagem.[148]
A atual população cubana é, portanto, diversificada e heterogênea. Um estudo genético mostra que houve uma intensa miscigenação de homens europeus com mulheres africanas e, menos frequente, com indígenas, na constituição da população cubana. Historicamente, na região oeste do país a ancestralidade africana é mais forte, nas áreas centrais a europeia mais forte e no leste há um equilíbrio entre ambas.[145] Um estudo genético dividiu os habitantes de Havana em três grupos: mulatos, descendentes de espanhóis e descendentes de africanos. Os mulatos apresentaram 57-59% de ancestralidade europeia e 41-43% de africana; os descendentes de espanhóis 85% de ancestralidade europeia e os descendentes de africanos 74-76% de contribuição africana. Nenhuma contribuição indígena foi detectada.[149]
Um estudo genético de 2014 revelou que a ancestralidade total em Cuba é a seguinte: 72% de contribuição europeia, 20% de contribuição africana e 8% de contribuição indígena.[150]
A imigração e a emigração desempenharam um papel importante no perfil demográfico de Cuba. Entre o século XVIII e o início do século XX, grandes ondas de canários, catalães, andaluzes, galegos e outros espanhóis imigraram para Cuba. Apenas entre 1899 e 1930, cerca de um milhão de espanhóis entraram no país, embora muitos eventualmente voltassem para a Espanha.[151] Outros grupos de imigrantes proeminentes incluíram os franceses, entre outros.[152]
A Cuba pós-revolução foi caracterizada por níveis significativos de emigração, o que levou a uma grande e influente comunidade da diáspora. Durante as três décadas após janeiro de 1959, mais de um milhão de cubanos de todas as classes sociais - constituindo 10% da população total - emigrou para os Estados Unidos.[153][154]
Até 13 de janeiro de 2013, os cidadãos cubanos não podiam viajar para o exterior, sair ou retornar a Cuba sem primeiro obter permissão oficial junto com a solicitação de passaporte emitido pelo governo, além do visto de viagem, que muitas vezes era negado.[155] Quem saía do país costumava fazê-lo por via marítima, em pequenas embarcações e frágeis jangadas. Em 9 de setembro de 1994, os governos dos Estados Unidos e de Cuba concordaram que os Estados Unidos concederiam pelo menos 20 mil vistos anualmente em troca da promessa do governo cubano de evitar novas partidas ilegais de barcos.[156] Em 2013, os principais destinos de emigração eram os Estados Unidos, Espanha, Itália, Porto Rico e México.[157]
Embora o termo genérico favela (ou tugurio em espanhol) seja muito raramente empregado para descrever as condições de habitação substandard em Cuba, existem outras formas de sub habitação, (baseadas no tipo de construção, condição de conservação, materiais e tipo de ocupação) que são descritas em Cuba. A maioria das sub habitações cubanas localizam-se em La Habana Vieja, Centro Habana e Antares.[158]
A comparação entre as condições de habitação substandard de Cuba e as favelas (em inglês slums) de outros países de economia de mercado é complexa e não pode ser feita de forma direta.[158] Em 2003 um projeto internacional, solicitado pelo governo comunista de Cuba à organizações internacionais tais como UNDP, UNEP, UNOPS, UN-HABITAT, e com a colaboração do governo da Bélgica, nos termos da Agenda 21, visando aumentar a capacidade dos atores locais de conduzir atividades de planejamento e administrações urbanos resultou no estabelecimento de mecanismos de capacitação para atividades de construção e um time nacional foi treinado para dar apoio técnico a times locais, que foram formados.[159]
No ano de de 2011 o governo cubano relaxou as normas de comercialização de moradias. Supõem-se que tal fato visou retirar da clandestinidade o mercado informal de moradias e de tentar suprir a grande carência de moradias. O Instituto Nacional da Moradia cubano divulgou de forma pública que do fundo habitacional do país , com cerca de mais de três milhões de casas, apenas 61% se encontram em bom estado, enquanto que as demais estão em mau estado ou em condições regulares.[160] Estima-se que faltariam cerca de 600 mil residências devido a falta de investimento e da destruição ocasionada pela passagem de três furacões.[161]
Em 2010, o Pew Forum estimou que a afiliação religiosa em Cuba é 59,2% de cristãos, 23% irreligiosos, 17,4% praticantes de religião folclórica (como santería) e os 0,4% restantes consistindo de seguidores de outras religiões.[162] Em uma pesquisa patrocinada pela Univision, 44% dos cubanos disseram que não eram religiosos e 9% não deram uma resposta, enquanto apenas 34% disseram que eram cristãos.[163] Cuba também hospeda pequenas comunidades de judeus (500 em 2012), muçulmanos e membros da Fé Bahá'í.[164] Cuba é oficialmente um Estado secular. A liberdade religiosa aumentou ao longo da década de 1980,[165] com o governo emendando a constituição em 1992 para retirar a caracterização de Estado ateu.[166]
O catolicismo romano é a maior religião, com suas origens na colonização espanhola. Apesar de menos da metade da população se identificar como católica em 2006, ela continua sendo a fé dominante.[167] O Papa João Paulo II, o Papa Bento XVI e o Papa Francisco visitaram Cuba em 1998, 2011 e 2019, respectivamente.[168][169] Antes de cada visita papal, o governo cubano perdoava prisioneiros como um gesto humanitário.[170][171] O relaxamento das restrições do governo às igrejas domésticas na década de 1990 levou a uma explosão do pentecostalismo, com alguns grupos reivindicando até 100 mil membros. No entanto, as denominações evangélicas protestantes, organizadas no Conselho de Igrejas de Cuba, permanecem muito mais vibrantes e poderosas.[172]
A paisagem religiosa de Cuba também é fortemente definida por sincretismos de vários tipos. O cristianismo é frequentemente praticado em conjunto com a santería, uma mistura de catolicismo e religiões de matriz africana, que incluem vários cultos. La Virgen de la Caridad del Cobre (a Virgem do Cobre) é a padroeira católica de Cuba e um símbolo da cultura cubana. Na santería, ela foi sincretizada com a deusa Oxum. Uma análise dos seguidores das religiões afro-cubanas mostrou que a maioria dos praticantes de palo eram negros e a maioria dos praticantes de vodu eram pardos e da santeria eram brancos.[173]
A língua oficial de Cuba é o espanhol, que é falado pela grande maioria dos cubanos. O espanhol falado em Cuba é conhecido como espanhol cubano e é uma variação do espanhol caribenho. A língua lucumi, um dialeto da língua iorubá da África Ocidental, também é usado como língua litúrgica pelos praticantes da santería[174] e, portanto, apenas como segunda língua.[175] O crioulo haitiano é a segunda língua mais falada em Cuba e é falada por imigrantes haitianos e seus descendentes.[176] Outras línguas faladas por imigrantes incluem galego e corso.[177]
Cuba é uma república socialista, organizada segundo o modelo marxista-leninista, (partido único, sem eleições diretas para cargos executivos), da qual Fidel Castro foi o primeiro-secretário do Comitê Central do Partido Comunista de Cuba e o presidente dos Conselhos de Estado e de Ministros (presidente da República), e que governou desde 1959 como chefe de governo e a partir de 1976 também como chefe de estado e comandante em chefe das forças armadas. Fidel afastou-se do poder em 1 de agosto de 2006, pela primeira vez desde a vitória da insurgência, por problemas de saúde. Seu irmão, Raúl Castro, assumiu interinamente as funções de Fidel (secretário-geral do Partido Comunista Cubano, comandante supremo das Forças Armadas e presidente do Conselho de Estado), exercendo-as até 19 de fevereiro de 2008 nessa condição, quando Fidel Castro renunciou oficialmente. Raúl Castro foi eleito novo presidente de Cuba no dia 24 de fevereiro de 2008 em eleição de candidato único, mantendo-se no cargo até abril de 2018, quando foi sucedido por Miguel Díaz-Canel, o primeiro chefe do Executivo da ilha que não era da família Castro em quase 60 anos.[178]
Raúl prometeu "eliminar proibições" na ilha, mas reconheceu o legado de seu irmão, que ficou mais de 49 anos a frente do poder: Nas próximas semanas, começaremos a eliminar as (proibições) mais simples, já que muitas delas tiveram como objetivo evitar o surgimento de novas desigualdades em um momento de escassez generalizada, declarou durante seu discurso de posse. Em março de 2008 Raúl Castro liberou a venda de computadores pessoais (PC's) e DVDs em Cuba,[179] a venda de telefones celulares e televisores a cidadãos comuns também foi liberada. No final de abril, Raul Castro convocou uma assembleia do Congresso do Partido Comunista Cubano (PCC) para o segundo semestre de 2009, para redefinir os eixos políticos e econômicos do país. O VI Congresso do PCC, quando ocorrer, terão decorrido onze anos sem que se tenha reunido o órgão supremo de decisão política de Cuba.[180]
Em 21 de outubro de 2007 realizaram-se em Cuba eleições gerais, com o comparecimento de mais de 8 milhões de eleitores, para eleger os delegados das "Assembleias Municipais do Poder Popular" na ilha. Segundo a ministra da Justiça, María Esther Reus, têm direito a exercer o voto cerca de 8,3 milhões de pessoas, nos 37 749 colégios eleitorais habilitados em 169 municípios.[181] Por ocasião da realização das eleições gerais, Fidel Castro conclamou, mais uma vez, o presidente George W. Bush a por fim ao embargo comercial a Cuba.[182]  O governo dos Estados Unidos, a União Europeia e opositores cubanos ao regime de Castro se referem às eleições cubanas como sendo um "exercício cosmético de democracia" que exclui a oposição do país e é completamente supervisionado pelo partido comunista cubano.[183] E ativistas cubanos qualificaram as eleições como ilegítimas e inconstitucionais.[184]
Fidel Castro renunciou à presidência em 19 de fevereiro de 2008[185] e seu irmão Raúl “encabeçou a uma lista única de candidatos apresentada à Assembleia, que ratificou a cédula e o elegeu” em 24 de fevereiro para sucedê-lo na Presidência de Cuba. O general já governava Cuba interinamente desde julho de 2006, devido aos problemas de saúde de Fidel, que culminaram em sua renúncia ao cargo.[186]
Em 2018, Cuba gastou cerca de 91,8 milhões de dólares com suas forças armadas ou 2,9% de seu PIB.[187] Em 1985, o governo cubano dedicava mais de 10% do PIB a gastos militares.[188] Em resposta à agressão estadunidense, como a Invasão da Baía dos Porcos, Cuba construiu uma das maiores forças armadas da América Latina, perdendo apenas para as forças brasileiras.[189]
De 1975 até o final dos anos 1980, a assistência militar soviética permitiu a Cuba atualizar suas capacidades militares. Após a perda dos subsídios soviéticos, Cuba reduziu o número de militares, de 235 mil em 1994 para cerca de 49 mil em 2021.[190][191] Em 2017, Cuba assinou o Tratado sobre a Proibição das Armas Nucleares.[192]
Nos anos iniciais da Revolução, o governo cubano foi acusado de violações dos direitos humanos, incluindo detenções arbitrárias, julgamentos injustos e execuções (também conhecido como "El paredón").[193][194][195]
Fidel, em 2010, admitiu que o governo perseguiu homossexuais nas décadas de 1960 e 1970, exonerando-os de cargos públicos, prendendo-os ou enviando-lhes a campos de trabalho forçado, afirmando que foram "momentos de muita injustiça".[196] Em 1978 havia entre quinze e vinte mil presos políticos em Cuba, número que subiu para cerca de 112 mil em 1986. Na década de 1990, segundo a Human Rights Watch, o sistema prisional de Cuba era composto por cerca de 40 prisões de segurança máxima, trinta prisões de segurança mínima e mais de duzentos campos de trabalho.[197] Em 2006, apesar de uma redução substancial, ainda havia, segundo a Anistia Internacional, entre 80 mil e 80,5 mil prisioneiros políticos na ilha, e, de acordo com esta, os dissidentes cubanos enfrentam detenção e prisão.[198] Segundo a Human Rights Watch, em 2007, os presos políticos, juntamente com o resto da população prisional de Cuba, estão confinados a celas com condições precárias e insalubres.[197] A Human Rights Watch acusou em 2008 o governo de "reprimir quase todas as formas de dissidência política" e que "aos cubanos são sistematicamente negados direitos fundamentais de livre expressão, associação, reunião, privacidade, movimento e devido processo legal".[199]  Esse número continuou a cair significativamente nos anos seguintes. Em 2007, era de 234 e, em 2008, era de 205. "O resultado óbvio é que segue a tendência, observada nos últimos vinte anos, da diminuição gradual de pessoas condenadas por motivações políticas", segundo a Comissão Cubana de Direitos Humanos, um grupo ilegal embora tolerado. Mas a comissão também chama a atenção para as mais de mil e quinhentas detenções rápidas, feitas de forma arbitrária.[198] Cuba teve o segundo maior número de jornalistas presos em 2008, de acordo com o Committee to Protect Journalists (CPJ) e a Human Rights Watch.[200][201]
Segundo Elías Carranza, diretor do Instituto Latinoamericano das Nações Unidas para a Prevenção do Delito e Tratamento do Delinquente, Cuba teria erradicado a exclusão social graças “a grandes conquistas na redução da criminalidade”.[202] Trataria-se, em 2012, do “país mais seguro da região, enquanto que a situação em relação aos crimes e à falta de segurança no continente se deteriorou nas últimas três décadas com o aumento do número de mortes nas prisões e no exterior”.[203] O governo se defende salientando o respeito, em Cuba, aos direitos à saúde e à educação, à liberdade religiosa e de associação. Além disso, acusa os Estados Unidos de limitar, com seu embargo econômico, os direitos humanos na ilha: "Cuba é um país onde nos últimos 50 anos não foi registrado um único desaparecido, torturado ou uma execução extrajudicial", disse, em 2009, o chanceler cubano Felipe Pérez Roque.[204]
Em 2020, o MSI (Movimiento San Isidro), grupo dissidente cubano, declarou que a polícia invadiu a sua sede na capital, Havana, detendo membros em greve de fome por causa da prisão de um rapper. O grupo exige a libertação do rapper Denis Solís, que foi condenado após uma discussão com um agente da polícia.[205] As autoridades cubanas disseram que a rusga foi realizada por violação sanitária, relacionada com o coronavírus.[205] O site jornalístico cubadebate afirma que o rapper está associado a grupos terroristas nos Estados Unidos, tendo admitido o fato em vídeo.[206][207]
A crise econômica e a pandemia de COVID-19 motivaram protestos no país em Julho de 2021,[208][209][210] sob o lema Patria y Vida.[211] Estas manifestações contra o governo cubano, vem sendo apontadas como as maiores desde o Maleconazo de Agosto de 1994.[212]
De acordo com o Repórteres sem Fronteiras, o direito de utilizar a internet é monitorado e só é concedido a quem contrata os serviços do governo cubano.[213]
A política dos Estados Unidos para Cuba está permeada por grandes conflitos de interesses que remontam ao governo de Thomas Jefferson, na primeira década do século XIX. As relações conflituosas aprofundaram-se com a Revolução Cubana de 1959, em que os revolucionários encabeçados por Fidel Castro Ruz promoveram reformas estatais de cunho socialista que desagradavam os Estados Unidos naquele contexto da Guerra Fria.[214]
Para se defender, Fidel buscou apoio do líder soviético na época, Nikita Khrushchov, com quem iniciou conversações em 6 de fevereiro de 1960, estabelecendo relações diplomáticas formais com a União Soviética em 8 de maio desse ano.  A União Soviética se comprometeu a adquirir cinco milhões de toneladas de açúcar produzidas em Cuba, a facilitar a aquisição de petróleo e cereais, prover crédito e passou a dar cobertura militar à defesa da ilha.[216] Por outro lado, em 17 de março de 1960, o presidente Dwight Eisenhower aprovou oficialmente, e divulgou publicamente, um "plano anti-Castro", criando embargos comerciais ao livre comércio do açúcar cubano, e à sua importação de petróleo e armamentos, e lançou uma propaganda antiCastro. O plano de Eisenhower incluía incentivos para os exilados cubanos de Miami tentarem derrubar Castro através de ações terroristas.[215] Na sua campanha presidencial Kennedy acusou as políticas de Nixon e Eisenhower de "negligência e indiferença", e de terem colaborado para que Cuba entrasse na cortina de ferro.[215][216]
Desde a Revolução Cubana de 1959, o governo cubano tem protestado consistentemente contra a presença dos Estados Unidos em solo cubano através da Base Naval da Baía de Guantánamo, argumentando que a base "foi imposta a Cuba pela força" e é ilegal segundo o direito internacional.[217]
A política internacional de Cuba deixou de ser ambiciosa e passou a ter baixo porte como resultado das dificuldades econômicas enfrentadas pelo país após o colapso do bloco soviético. Sem os maciços subsídios do seu então principal parceiro comercial, a União Soviética, Cuba foi relativamente isolada na década de 1990, mas desde então entrou em um processo de cooperação bilateral com vários países sul-americanos, principalmente Venezuela e Bolívia. Os Estados Unidos continuaram até 2015 com o embargo, declarando que continuará "enquanto o país continuar se recusando a se mover em direção à democratização e ao maior respeito pelos direitos humanos",[218] enquanto a União Europeia, em 2004, acusava Cuba de "contínua violação flagrante dos direitos humanos e das liberdades fundamentais".[219] Cuba tem desenvolvido uma relação crescente com a República Popular da China e a Rússia. Ao todo, Cuba continua a ter relações formais com 160 nações e fornece os trabalhadores civis de assistência - principalmente médicos - para dezenas de países.[220]
Cuba é hoje um país líder no Conselho de Direitos Humanos das Nações Unidas, é membro fundador da organização antiamericana conhecida como Aliança Bolivariana para as Américas, um membro da Associação Latino-Americana de Integração e da Organização das Nações Unidas. Além dos quais, o país é um membro do Movimento dos Países Não Alinhados e organizou a sua cúpula em setembro de 2006. Como membro da Associação dos Estados do Caribe (AEC), Cuba foi reconduzida como presidente da comissão especial sobre questões de transporte para a região do Caribe.[221] Desde 2004, vários líderes da América do Sul têm tentado fazer de Cuba seja membro pleno ou associado do bloco comercial sul-americano, conhecido como Mercosul.[222][223]
Extintas desde 1961, em 17 de dezembro de 2014 o presidente dos Estados Unidos Barack Obama e o presidente de Cuba Raúl Castro anunciaram o restabelecimento total das relações diplomáticas entre os dois países, assim como a reabertura da embaixada norte-americana em Havana e uma considerável diminuição no embargo econômico feito à ilha, assim como uma maior facilidade para viagens de negócios e transações econômicas, comerciais e financeiras entre cidadãos e empresas dos dois países. O acordo foi anunciado após 18 meses de conversações secretas entre os dois governos, com a mediação de Sua Santidade o Papa Francisco.[224][225] Estados Unidos e Cuba restauraram formalmente relações diplomáticas em 20 de julho de 2015.[226]
Cuba está dividida em quinze províncias com 169 municípios, além de um município especial (a Isla de la Juventud).[227]
Em 2018, o Banco Mundial listou a indústria cubana como a 66ª mais valiosa do mundo (13 bilhões de dólares).[228] A economia de Cuba ainda hoje sofre as consequências do embargo comercial imposto pelos Estados Unidos desde 1962. Quase seis décadas de embargo pelos Estados Unidos causaram danos quantificáveis em mais de 922,6 bilhões de dólares na economia cubana.[229]
Na agricultura, Cuba produziu, em 2018, 8,7 milhões de toneladas de cana-de-açúcar (entre o 21 e o 25º maior produtor mundial); 950 mil toneladas de banana; 795 mil toneladas de mandioca; 752 mil toneladas de legume; 555 mil toneladas de batata doce; 480 mil toneladas de tomate; 382 mil toneladas de manga; 377 mil toneladas de arroz; 307 mil toneladas de milho; 206 mil toneladas de yautia; 184 mil toneladas de mamão; 169 mil toneladas de feijão; 129 mil toneladas de batata; 27 mil toneladas de tabaco; além de outras produções de outros produtos agrícolas.[230]
Os charutos cubanos são considerados, historicamente, como um dos melhores do mundo e são sinônimos da cultura da ilha e contribuem com quase um quarto do valor de todas as importações do país. O enchimento, o aglutinante e a embalagem podem vir de diferentes áreas da ilha, embora muito seja produzido na província de Pinar del Rio, nas regiões de Vuelta Abajo e Semi Vuelta, bem como em fazendas na região de Viñales.[231] Toda a produção de charutos em Cuba é controlada pela estatal Cubatabaco. O charuto cubano também é conhecido como "El Habano".[232] Cubatabaco e Habanos SA (detida em partes iguais pelo Estado cubano e pela Altadis, uma empresa privada com sede na Espanha) fazem todo o trabalho relacionado aos charutos cubanos, incluindo fabricação, controle de qualidade, promoção, distribuição e exportação. A Habanos SA lida com exportação e a distribuição, em grande parte por meio de seu parceiro europeu Altadis.[233]
Na pecuária, Cuba produziu, em 2019, 234 mil toneladas de carne de porco, 438 milhões de litros de leite de vaca, 81 mil toneladas de carne bovina, 24 mil toneladas de carne de frango, entre outros. A pecuária do país é bastante reduzida.[234] Na mineração, o país era o 5º maior produtor mundial de cobalto[235] e o 10º maior produtor mundial de níquel em 2019.[236]
De 1994 até 2021, Cuba teve duas moedas oficiais: o peso nacional (ou CUP) e o peso conversível (ou CUC, muitas vezes chamado de "dólar" na língua falada). Em janeiro de 2021, entretanto, um tão esperado processo de unificação da moeda começou, com os cidadãos cubanos tendo seis meses para trocar seus CUCs restantes a uma taxa de um para cada 24 CUPs.[237]
Em 2004, o índice de pobreza era o sexto menor em 2004 dentre os 102 países em desenvolvimento pesquisados (de acordo com o Programa das Nações Unidas para o Desenvolvimento)[238] e Cuba está entre os 83 países do mundo que ostentam um alto Índice de Desenvolvimento Humano (acima de 0,800): em 2007, o índice de desenvolvimento humano de Cuba foi 0,863 (51º lugar; 44º, se ajustado pelo produto nacional bruto).[239]
Em 2012, o crescimento econômico de Cuba, segundo as últimas estimativas da Agência Central de Inteligência dos Estados Unidos, foi de 3,1 por cento (estimativa), um pouco melhor que os 2,8 por cento conseguido em 2011.[240] Cuba apresenta um défice na sua balança comercial, importando mais do dobro do que exporta.[240] A produção industrial cresceu 6,6 por cento em 2012 pela estimativa da Agência Central de Inteligência dos Estados Unidos. A renda per capita dos cubanos atingiu 10 200 dólares estadunidenses em 2011: a 117a na colocação mundial.[240]
O embargo comercial imposto a Cuba pelos Estados Unidos, desde 1962, dificulta enormemente a expansão do comércio exterior cubano. Mas Cuba tem conseguido atrair alguns investimentos estrangeiros, cerca de metade deles feitos pela União Europeia; grandes investimentos têm sido feitos nas áreas de turismo, energia e telecomunicações. Em meados de 2007, o presidente em exercício, Raul Castro, anunciou novas medidas para incentivar os investimentos estrangeiros em Cuba.[241]
O governo cubano tem criado novas leis e aumentado a fiscalização objetivando regulamentar a criação de novos negócios particulares. Em 2012, 27,7 por cento do produto interno bruto de Cuba foi gerado por negócios particulares.[240] As atividades que podem ser realizadas por particulares são oficinas, pequenos negócios, e prestação de alguns serviços. Seu produto interno bruto anual é de 72 bilhões de dólares estadunidenses (2012).[240]
Em 2018, Cuba foi o 50º país mais visitado do mundo, com 4,6 milhões de turistas internacionais. As receitas do turismo, neste ano, foram de US$ 2,9 bilhões.[242] Em 2003, Cuba recebeu 1,9 milhões de turistas, que geraram 2 bilhões de dólares estadunidenses em receitas. Em 2005, Cuba recebeu seu recorde de 2,3 milhões de turistas, o que representou um incremento de 13,2 por cento em relação a 2004. Em 2006, a receita com turismo bateu o recorde, atingindo 2,4 bilhões de dólares estadunidenses, mas houve uma pequena queda no número de visitantes, que somaram 2,2 milhões.[243][244]
Em 2007, o Ministro do Turismo, Manuel Marrero, divulgou uma série de medidas tomadas para tornar o turismo cubano mais competitivo no Caribe.[245] Em 2019, a praia de Varadero foi considerada como uma das melhores praias do mundo no TripAdvisor de 2019, ficando em segundo lugar na classificação.[246]
O turismo de massa foi uma das formas encontradas para se contornar a crise econômica, e, hoje, é a principal fonte de divisas para o país.[247] Estimativas feitas pela Organização Mundial de Turismo indicam que, caso tivesse sido levantado o embargo norte-americano, em 2007 cerca de 4 milhões de turistas poderiam ter visitado Cuba, representando uma fatia de mercado de 15,9 por cento do turismo na região do Caribe.[248]
A indústria do turismo gera uma receita direta de 7,5 bilhões de dólares.[248] A atividade turística já dá emprego a cerca de 268 mil cubanos, sendo 138 mil em empregos diretos (dos quais cerca de 20 por cento de nível universitário), e 130 mil indiretos.[248]
Com a crise iniciada em 1989 - e até 2004 - o país sofreu muito com a falta crônica de eletricidade, devido à insuficiência de recursos financeiros do estado cubano para modernizar e ampliar as suas termoelétricas e suas linhas de transmissão, carentes de expansão para atender ao crescimento da demanda de energia elétrica, que aumenta, em média, 7% ao ano. Os chamados apagões chegaram a durar 16 horas, e foram uma constante na vida do povo cubano.[249] Não obstante, no início de 2007 o governo cubano pôs em operação 4.158 novos geradores a diesel, que custaram 800 milhões de dólares e têm capacidade total de gerar 711 811 kW, como uma medida emergencial para reduzir os apagões.[250]
Nas energias não-renováveis, em 2020, o país era o 53º maior produtor de petróleo do mundo, extraindo 41 mil barris/dia.[251] Em 2011, o país consumia 150 mil barris/dia (64º maior consumidor do mundo).[252] O país foi o 37º maior importador de petróleo do mundo em 2013 (160 mil barris/dia).[251] Em 2016, Cuba era o 62º maior produtor mundial de gás natural, 1,1 bilhões de m3 ao ano. Em 2016 o país era o 87º maior consumidor de gás (1,1 bilhões de m3 ao ano).[253] O país não produz carvão.[254] Nas energias renováveis, em 2020, Cuba não produzia energia eólica e tinha 0,1 GW de potência instalada de energia solar.[255]
Cuba possui 20 mil km de vias pavimentadas,[256] incluindo 915 km de oito vias expressas gratuitas chamadas de autopistas, sete delas centralizadas na cidade de Havana e conectadas entre si pelo anel viário de Havana, com exceção da rodovia para Mariel. A faixa de rodagem é dividida e as faixas em cada direção vão de duas a quatro. O limite máximo de velocidade é 100 km/h.[257]
O país possui 133 aeroportos, sendo que 64 deles têm pistas de pouso pavimentadas.[256] O Aeroporto Internacional José Martí, em Havana, é um hub para as empresas Cubana de Aviación e Aerogaviota, além de ter sido o antigo hub na América Latina para a companhia aérea soviética (posteriormente russa) Aeroflot.[258]
Cuba tem uma rede de 8 367 km de ferrovias.[256] Os sistemas de metrô não estão presentes na ilha, embora uma rede ferroviária suburbana exista em Havana. Os bondes urbanos estiveram em operação entre 1858 e 1954, inicialmente como sistemas puxados por cavalos. No início do século XX, bondes elétricos ou bondes movidos a bateria de armazenamento foram introduzidos em sete cidades.[259]
A ilha tem 240 km de hidrovias e seus principais portos estão localizados nas cidades de  Antilla, Cienfuegos, Guantánamo, Havana, Matanzas, Mariel, Nuevitas e Santiago de Cuba.[256]
O sistema educacional cubano é supervisionado e administrado pelo Ministério da Educação e pelo Ministério da Educação Superior. A educação é controlada pelo Estado e é obrigatória até o 9º ano. A Constituição de Cuba determina que os ensinos fundamental, médio e superior devem ser gratuitos a todos os cidadãos cubanos. O ensino fundamental tem a duração de seis anos, o ensino médio é dividido em ensino básico e pré-universitário.[260] Ao alcançar a independência, os governos subsequentes promoveram a educação em Cuba. Apesar de este setor nunca ter usufruído de amplos recursos, foi estabelecido um sistema de ensino básico público, gratuito e obrigatório. Com isso, Cuba alcançou níveis de educação satisfatórios quando comparada aos demais países latino-americanos.[261]
Em 1958, antes do triunfo da revolução, 23,6% da população cubana era analfabeta e, entre a população rural, os analfabetos eram 41,7%. Em 1961 se realiza uma campanha nacional para alfabetizar a população e Cuba torna-se o primeiro país do mundo a erradicar o analfabetismo (Segundo dados do próprio governo). Hoje não há mais analfabetos em Cuba. Segundo dados de 2015 do The World Factbook,[262] publicado pela CIA, 99,8% da população cubana acima de 15 anos sabe ler e escrever, com a maior proporção dando-se entre homens (99,9%) do que entre mulheres (99,8%).[262] Cuba destina, atualmente, cerca de 12,8% de seu Produto Interno Bruto (PIB) para gastos e investimentos em educação, a maior destinação entre os países do mundo. A expectativa de vida escolar, do ensino primário ao superior, era de 14 anos de estudos em 2018.[262]
De acordo com os resultados obtidos nos testes de avaliação de estudantes latino-americanos, conduzidos pelo painel da Unesco, Cuba lidera, por larga margem de vantagem, nos resultados obtidos pelas terceiras e quartas séries em matemática e compreensão de linguagem. "Mesmo os integrantes do quartil mais baixo dentre os estudantes cubanos se desempenharam acima da média regional", disse o painel.[263] O ensino superior é fornecido por universidades, institutos superiores, institutos pedagógicos superiores e institutos politécnicos superiores. O Ministério de Educação Superior de Cuba opera um programa de educação à distância que oferece cursos regulares à tarde e à noite em áreas rurais para trabalhadores agrícolas. A educação tem uma forte ênfase política e ideológica e espera-se que os alunos que progridem para o ensino superior tenham um compromisso com os objetivos do Estado cubano.[260] Cuba oferece educação subsidiada pelo estado a um número limitado de estrangeiros na Escola Latino-Americana de Medicina.[264][265] De acordo com o Webometrics Ranking of World Universities, as melhores universidades do país são a Universidade de Havana (1 680ª a nível mundial), o Instituto Superior Politécnico José Antonio Echeverría (2 893ª) e a Universidade do Oriente (3 831ª).[266]
Em Cuba, a prestação de serviços relacionados à saúde é totalmente gratuito para residentes da ilha, o que se espelha em seus indicadores padrão. Segundo dados da Organização mundial da saúde (OMS), em 2015, a taxa de mortalidade infantil para crianças abaixo de cinco anos de idade foi de quatro para cada mil nascidos, sendo o menor índice da América.[267] A expectativa de vida ao nascer em Cuba é de 75 anos para os homens e de 79 para as mulheres, índice semelhante ao dos Estados Unidos, que é de 75 e 80 respectivamente.[268] A mortalidade adulta, de 15 a 60 anos, é em Cuba de 128 para homens e de 83 para mulheres, índices superados, na América, pelo Canadá e pela  Costa Rica.[269]
Em 25 de maio de 2011, foi anunciado na nona edição do Congresso sobre Longevidade Satisfatória, em Havana, que Cuba é o país com a maior proporção demográfica de pessoas com mais de 100 anos, à frente até do Japão.[270] Cuba conta com aproximadamente 1 551 centenários em uma população de 11,2 milhões de habitantes. Esses dados mostram que quintuplicaria a proporção existente no Japão, país com maior número de centenários em termos absolutos. Em 2006, segundo a Organização Mundial de Saúde, não ocorreu em Cuba nenhum caso de difteria, sarampo, coqueluche, poliomielite, rubéola, rubéola CRS, tétano neonatal, ou febre amarela. Uma pequena pandemia de caxumba iniciou-se em 2004 e ainda não foi controlada, até aquele ano não havia caxumba em Cuba. Houve apenas três casos de tétano comum em 2006 (não relacionados a partos).[271]
A vacinação da população cubana, segundo as estatísticas da UNICEF, desde 1980 atinge índices bastante elevados. Em 2006, 99% da população tomou a vacina BCG, 99% tomou a primeira dose da vacina tríplice DTP1 (difteria, tétano e coqueluche), e 89% tomaram sua terceira dose; 99% da população cubana tomou a terceira dose da vacina contra poliomielite, 96% tomou a vacina contra sarampo e 89% contra a hepatite B.[272] Estes resultados, obtidos na prestação de serviços de saúde ao povo cubano - que colocam os índices padrão internacionais de Cuba dentre os de países do primeiro mundo - são obtidos com relativamente pouco uso dos recursos econômicos de sua sociedade. Cuba gasta 7,7% de seu PIB em saúde (Estados Unidos 15,3%, Canadá 10%, Brasil 7,5%).[273]
Em 2009 a UNICEF anunciou que Cuba é o único país dentre todos da América Latina e Caribe que havia erradicado a desnutrição infantil. Tal informação é confirmada por relatório apresentado pela organização em 2011.[274][275] Em 2010 o país começou a exigir que os visitantes devem obrigatoriamente contratar um plano de saúde.[276] Em 2015, a OMS certificou que Cuba conseguiu impedir que o vírus do HIV e o Sífilis fosse transmitido entre uma mãe e seu bebê, feito nunca antes realizado na medicina.[277]
Desde 1993, Cuba vem fazendo grandes progressos na área de biotecnologia,[278] tendo obtido registro de suas patentes e  direitos de sua exploração comercial nos Estados Unidos. Sua vacina contra hepatite B é vendida em 30 países do mundo. Em 1994, o ingresso de divisas em Cuba através da exportação de biotecnologia alcançou a cifra de 400 milhões de dólares e se estima que no futuro poderia ser maior que o do açúcar.[279] A biotecnologia cubana já gerou mais de 600 patentes para drogas novas e inovadoras como vacinas, proteínas recombinantes, anticorpos monoclonais, equipamento médico com software especial, e sistemas de diagnósticos. Cerca de sessenta outros produtos estão nos estágios finais de pesquisa.[280] Em 2011, Cuba anunciou ter desenvolvido a primeira vacina terapêutica contra o câncer de pulmão, chamada CimaVax-EGF.[281][282]
A cultura cubana é influenciada por seu caldeirão de culturas, principalmente as da Espanha, África e os indígenas taínos. Após a revolução de 1959, o governo iniciou uma campanha nacional de alfabetização, ofereceu educação gratuita a todos e estabeleceu programas rigorosos de esportes, balé e música.[283]
Cuba teve seu primeiro local incluído na lista na 6ª Sessão do Comité do Patrimônio Mundial, realizado na sede da UNESCO em Paris, França, em Dezembro de 1982. Naquela sessão, a Cidade antiga de Havana e suas fortificações, um local incluído a parte e central histórico de Havana, bem como fortificação colonial espanhola, foi inscrita na lista.[284]
As inclusões de Cuba na lista incluem uma variedade de locais. Dois locais são selecionados por sua importância natural: o Parque Nacional Alejandro de Humboldt nas províncias orientais de Holguín e Guantánamo, e o Parque Nacional Desembarco del Granma, nomeados pelo iate que levou os membros do Movimento 26 de Julho que iniciaram a Revolução Cubana.[285][286] As paisagens da cidade incluem Havana Antiga,[287] Trinidad[288] e Camagüey,[289] todas fundadas pelos primeiros colonizadores espanhóis no século XVI. Os locais também incluem regiões agrícolas históricas, incluindo as plantações de café do sudeste de Cuba,[290] e na região do tabaco do Vale de Viñales.[291][292]
Nos primeiros dias de 1959, o novo governo criou um departamento cinematográfico dentro da Divisão de Cultura do Exército Rebelde, que patrocinou a produção de documentários como Esta tierra nuestra de Tomás Gutiérrez Alea, e La vivienda de Julio García Espinosa. Este foi o ancestral direto do que viria a se tornar o Instituto Cubano del Arte e Industria Cinematográficos (ICAIC), que foi fundado em março como resultado da primeira lei de cultura do governo revolucionário. O cinema, de acordo com essa lei, é "a forma mais poderosa e provocativa de expressão artística e o veículo mais direto e difundido para a educação e divulgação de ideias ao público".[293]
O ICAIC fundou o Cine Cubano em 1960. Toda a produção, distribuição e exibição no país eram administradas pelo ICAIC em 1965, que também estabeleceu unidades móveis de projeção chamadas cine moviles, caminhões que visitavam áreas remotas para realizar exibições.[294] Desde a sua fundação até 1980, Alfredo Guevara foi chefe do ICAIC. Sob sua direção, a organização foi fundamental no desenvolvimento do cinema cubano, que passou a ser identificado com o anti-imperialismo e a revolução.[295]
Um dos filmes cubanos mais notáveis ​​dos últimos anos foi Morango e Chocolate (1993) de Tomás Gutiérrez Alea e Juan Carlos Tabío, que relata uma história sobre intolerância e retrata a amizade entre um homossexual e um jovem integrante da União de Jovens Comunistas (organização juvenil comunista cubana). Foi também a primeira produção cubana a ser indicada ao Oscar de Melhor Filme Estrangeiro.[296]
A literatura cubana começou a encontrar sua voz no início do século XIX. Temas dominantes de independência e liberdade foram exemplificados por José Martí, que liderou o movimento modernista na literatura cubana. Escritores como Nicolás Guillén e José Z. Tallet se concentraram na literatura como protesto social.[297] A poesia e os romances de Dulce María Loynaz e José Lezama Lima foram influentes.[298] O romancista Miguel Barnet, que escreveu Everybody Dreamed of Cuba, reflete uma Cuba mais melancólica. Alejo Carpentier foi importante no movimento do realismo mágico.[299]
A culinária cubana é uma fusão das culinárias espanhola e caribenha. As receitas cubanas compartilham especiarias e técnicas com a culinária espanhola, com alguma influência caribenha nas especiarias e no sabor. O racionamento de alimentos, que tem sido a norma em Cuba nas últimas quatro décadas, restringe a disponibilidade comum desses pratos.[300]
A música cubana é muito rica e é a expressão mais conhecida da cultura cubana. A forma central desta música é o son, que tem sido a base de muitos outros estilos musicais como o danzón, o mambo, o cha-cha-chá e a salsa. A rumba originou-se no início da cultura afro-cubana, misturada com elementos de estilo espanhol.[301]
A música popular cubana de todos os estilos tem sido apreciada e elogiada amplamente em todo o mundo. A música clássica cubana, que inclui música com fortes influências africanas e europeias, e apresenta obras sinfônicas e também música para solistas, recebeu aclamação internacional graças a compositores como Ernesto Lecuona.[302][303]
Havana era o coração da cena rap em Cuba quando começou na década de 1990. Durante esse tempo, o reggaeton cresceu em popularidade. Em 2011, o Estado cubano denunciou o reggaetón como "degenerado", reduzindo diretamente a reprodução "discreta" do gênero (não o banindo totalmente) e proibiu a canção "Chupi Chupi" de Osmani García, caracterizando sua descrição do sexo como "do tipo que uma prostituta faria".[304] Em dezembro de 2012, o governo cubano proibiu oficialmente canções de reggaeton e videoclipes sexualmente explícitos do rádio e da televisão.[305][306]
Artistas cubanos reconhecidos incluem os pianistas Chucho Valdés[307] e Frank Fernández (este último ganhou o título de ouro no Conservatório de Moscovo),[308] e Omara Portuondo, membro do Buena Vista Social Club. Muitos artistas cubanos ganharam prêmios Grammy.[307] Entre os jovens, Buena Fe é um grupo popular.[309]
Devido a associações históricas com os Estados Unidos, muitos cubanos participam de esportes que são populares na América do Norte, em vez de esportes tradicionalmente praticados em outras nações latino-americanas. O beisebol é o mais popular. Outros esportes populares incluem vôlei, boxe, atletismo, luta livre, basquete e esportes aquáticos.[310] Cuba é uma força dominante no boxe amador, alcançando consistentemente altos números de medalhas nas principais competições internacionais. Os boxeadores Rances Barthelemy e Erislandy Lara desertaram para os Estados Unidos e o México, respectivamente.[311][312]
Cuba participou pela primeira vez dos Jogos Olímpicos de Verão em 1900 e enviou atletas para competirem em 20 das 28 Olimpíadas seguintes. Cuba ocupa a terceira posição entre os países das Américas em relação a conquistas de medalhas de ouro (atrás apenas dos Estados Unidos e do Canadá) e conquistou mais medalhas do que qualquer país da América do Sul.[313]
Nos Jogos Pan-Americanos é o segundo país com mais medalhas conquistadas, atrás apenas dos Estados Unidos e um dos três que conseguiram ser campeões em ao menos uma edição, Havana 1991.[314]
México (pronunciado em português: [ˈmɛʃiku]; pronunciado em castelhano: [ˈmexiko] (escutar?·info), oficialmente Estados Unidos Mexicanos,[6] é uma república constitucional federal localizada na América do Norte. O país é limitado a norte pelos Estados Unidos; ao sul e oeste pelo Oceano Pacífico; a sudeste pela Guatemala, Belize e Mar do Caribe; a leste pelo Golfo do México.[7] Com um território que abrange quase 2 milhões de quilômetros quadrados,[8] o México é o quinto maior país das Américas por área total e o 14.º maior país independente do mundo. Com uma população estimada para 2020 de 126 milhões de habitantes,[2] é o 11.º país mais populoso do mundo e o mais populoso país da hispanofonia. O México é uma federação composta por 31 estados e a Cidade do México (capital).[9] O México figura também como o segundo país mais populoso e segundo em PIB da América Latina,[10] em ambos os casos superado apenas pelo Brasil.
Na Mesoamérica pré-colombiana muitas culturas amadureceram e se tornaram civilizações avançadas como a dos olmecas, toltecas, teotihuacanos, zapotecas, maias e astecas, antes do primeiro contato com os europeus. Em 1521, a Espanha conquistou e colonizou o território mexicano a partir de sua base em Tenochtitlán e administrou-o como o Vice-Reino da Nova Espanha. Este território viria a ser o México com o reconhecimento da independência da colônia em 1821. O período pós-independência foi marcado pela instabilidade econômica, a Guerra Mexicano-Americana e a consequente cessão territorial para os Estados Unidos, uma guerra civil, dois impérios e uma ditadura nacional. Esta última levou à Revolução Mexicana em 1910, que culminou na promulgação da Constituição de 1917 e a emergência do atual sistema político do país. Eleições realizadas em julho de 2000 marcaram a primeira vez que um partido de oposição conquistou a presidência do Partido Revolucionário Institucional.
O México é uma das maiores economias do mundo e uma potência regional,[11][12] e, desde 1994, o primeiro país latino-americano membro da Organização para a Cooperação e Desenvolvimento Econômico (OCDE), sendo um país de renda média-alta consolidada.[13] O México é considerado um dos países recentemente industrializados[14][15][16][17] e uma potência emergente.[18] A nação tem o 13.º maior PIB nominal e o 11.º maior PIB por paridade de poder de compra. A economia está fortemente ligada à dos seus parceiros do Tratado Norte-Americano de Livre Comércio (NAFTA), especialmente os Estados Unidos.[19][20] O país ocupa o quinto lugar no mundo e o primeiro das Américas em número de Patrimônios Mundiais da UNESCO, com 31 lugares que receberam esse título,[21][22][23] e em 2007 foi o 10.º país mais visitado do mundo, com 21,4 milhões de turistas internacionais.[24]
Depois de a Nova Espanha conquistar a independência do Império Espanhol, foi decidido que o novo país teria o nome de sua capital, a Cidade do México, que foi fundada em 1524 em cima da antiga capital asteca de Tenochtitlan-México. O nome vem da língua nahuatl, mas seu significado é desconhecido. Mēxihco era o termo em nahuatl usado para se referir ao coração do império asteca, o Vale do México, e ao seu povo, os astecas, no que depois se tornou o futuro estado do México como uma divisão da Nova Espanha antes da independência. O sufixo -co é um locativo em nahuatl, o que torna a palavra o nome de um lugar. Além disso, a etimologia do termo ainda é incerta. Tem sido sugerido que ele é derivado de Mextli ou Mēxihtli, um nome secreto para o deus da guerra e patrono dos astecas, Huitzilopochtli, caso em que Mēxihco significa "Lugar onde Huitzilopochtli vive".[25] Outra hipótese[26] sugere que Mēxihco deriva de um amálgama das palavras nahuatl para "Lua" (Metztli) e centro (xīctli). Este significado ("lugar no centro da Lua") pode referir-se à posição de Tenochtitlán no meio do lago Texcoco. O sistema de lagos interligados, dos quais Texcoco formava o centro, tinha a forma de um coelho, que os mesoamericanos associavam pareidoliamente à Lua. Ainda há outra hipótese que sugere que a palavra é derivada de Mēctli, a deusa do agave.[26]
O nome da cidade-Estado foi transliterado para o espanhol como México com o valor fonético da letra <x> no espanhol medieval, que representava a fricativa pós-alveolar surda [ʃ]. Este som, bem como a fricativa pós-alveolar sonora [ʒ], representado por um <j>, evoluiu para uma fricativa velar surda [x] durante o século XVI. Isso levou ao uso da variante Méjico em muitas publicações em espanhol, sobretudo na Espanha, enquanto no México e na maioria dos outros países de língua espanhola México era a grafia preferida. Nos últimos anos, a Real Academia Espanhola, que regulamenta a língua espanhola, determinou que ambas as variantes são aceitáveis ​​no idioma, mas que a grafia normativa recomendada é México.[27]
O nome oficial do país mudou conforme a forma de governo. Em duas ocasiões (1821–1823 e 1863–1867), o país era conhecido como Imperio Mexicano (Império Mexicano). Todas as três constituições federais (1824, 1857 e 1917, a Constituição atual) usavam o nome Estados Unidos Mexicanos[28] ou Estados-Unidos Mexicanos.[29] O nome República Mexicana foi usado nas Leis Constitucionais de 1836.[30] Em 22 de novembro de 2012, o presidente Felipe Calderón enviou ao congresso mexicano uma legislação para mudar o nome oficial do país para simplesmente México. Para entrar em vigor, o projeto precisa ser aprovado por ambas as casas do congresso, assim como pela maioria das 31 legislaturas estaduais do país. Como esta legislação foi proposta apenas uma semana antes de Calderón passar o governo para Enrique Peña Nieto, os críticos de Calderón interpretaram isso como um gesto simbólico.[31]
Há muito debate acerca do povoamento das Américas. Estudos genéticos comprovam a tese de que os ancestrais dos povos ameríndios eram caçadores-coletores asiáticos do leste da Sibéria.[32][33] A data em que o homem chegou à América vindo da Ásia Setentrional é bastante debatida, pois alguns estudiosos propõem que ocorreu há 30 mil anos ou até 40 mil anos, mas tais datas não são aceitas pela comunidade científica, que afirma que a data de chegada foi em algum momento por volta de 20 mil anos atrás.[32][34] Também é debatida a rota de chegada do homem à América a partir da Sibéria: a mais aceita é a da travessia a pé da Beríngia e uma posterior passagem por um corredor terrestre descongelado no extremo norte do continente, mas também é proposta uma rota alternativa por meio da navegação em pequenos barcos próximo ao litoral.[35]
Fogueiras encontradas no Vale do México foram datadas por radiocarbono de 21 000 a.C. e alguns fragmentos de ferramentas de pedra foram encontrados perto das fogueiras, indicando a presença de humanos naquela época.[36] O fóssil humano mais antigo já achado nas Américas, apelidado de Eva de Naharon, datado de 13,6 mil anos atrás, foi encontrado em 2001 no estado de Quintana Roo.[37]
Com o clima mais seco e a mudança na fauna ocorridos com o fim do Último período glacial, por volta de 9500 a.C., os habitantes da Mesoamérica foram forçados a trocar a caça de animais de grande porte pela caça de pequenos animais e a coleta de frutos silvestres. Gradualmente, nos milênios seguintes foi surgindo uma agricultura nessa região e na Aridoamérica,[34] tendo como principais cultivos o milho, feijão, cucurbita e pimenta chili.[38]
Paulatinamente, vilas neolíticas foram se desenvolvendo, originando os embriões de civilizações, em grande parte na área mesoamericana, mas também no atual estado de Chihuahua.[34]
Por volta de 1500 a.C., em uma disputa por região de terras férteis entre os atuais estados de Veracruz e Tabasco, surgiu a primeira civilização na Mesoamérica, a olmeca, cujos principais centros foram San Lorenzo e La Venta. Os olmecas se destacaram por serem os primeiros a criar centros cerimoniais de pedra e por sua arte, que inclui cabeças colossais, máscaras e estatuetas de jade.[34][38]
Entre 100 a.C. e 300 d.C., houve a disseminação de sociedades complexas e civilizações pela Mesoamérica, com forte influência olmeca. Nessa época, surgiu, na região do Vale do México, Teotihuacan, uma grande cidade, com uma população entre 125 e 200 mil habitantes, e centro de uma civilização e de um império político e comercial. Também apareceu nesse mesmo período, na Península de Iucatã, a civilização maia, que se destacou por seus avançados conhecimentos astronômicos e matemáticos e pela sua arquitetura.[34][38]
Entre os séculos VIII e XIII, ocorreu a decadência das civilizações clássicas da Mesoamérica (maias e teotihuacanos), cuja causa ainda é debate entre arqueólogos. Ao mesmo tempo, os toltecas, vindos do norte do México e falantes de uma língua da família uto-asteca, ocuparam o Vale do México e ali criaram a sua civilização, que era o poder religioso, político e comercial da região.[34][38]
Por volta do século XIII, os astecas, um povo falante de língua uto-asteca oriundo do noroeste do México, ocuparam o Vale do México e destruíram a civilização tolteca, estabelecendo em seu lugar a civilização asteca. Em 1325, fundaram, em uma ilha do Lago de Texcoco, a sua capital, Tenochtitlán. Essa civilização subjugou os povos vizinhos e se tornou a força dominante na região da Mesoamérica.[34][38]
Estimativas populacionais apontam para uma população de 6 a 25 milhões de habitantes na região do atual México na época da conquista espanhola.[39][40]
As primeiras expedições espanholas no México partiram de Cuba a mando do governador Diego Velázquez de Cuéllar e exploraram a região da Península de Iucatã e a costa do Golfo do México, sendo elas as de Francisco Hernández de Córdoba (1517) e Juan de Grijalva (1518).[34]
Em março de 1519, Hernán Cortés desembarcou no litoral de Tabasco. Pouco tempo depois, fundou a cidade de Veracruz, base para a conquista do Império Asteca, de onde partiu em direção a Tenochtitlán, chegando ali em novembro, tendo contatos amistosos com os astecas. No entanto, os espanhóis sequestraram o imperador asteca Montezuma II em seguida, assim iniciando conflitos. Cortés foi forçado a voltar para Veracruz, para lutar contra as tropas do governador de Cuba, de quem havia se tornado inimigo, e deixou seus homens em Tenochtitlán. Em junho de 1520, Montezuma foi substituído por Cuitláhuac, que ordenou uma guerra total contra os conquistadores, o que resultou no episódio conhecido como Noite Triste, no qual metade dos espanhóis morreu. Após uma epidemia de varíola, doença introduzida pelos europeus, que dizimou milhões de indígenas, e a formação de alianças com tribos inimigas dos astecas, em abril de 1521 as tropas de Cortés iniciaram o Cerco de Tenochtitlan, que finalizou-se em agosto, com a rendição do último imperador asteca, Cuauhtémoc. Nos anos seguintes, os europeus conquistaram o restante do Império Asteca, anexando-o ao Império Espanhol.[34][41]
A conquista militar foi acompanhada pela cristianização e aculturação dos povos indígenas.[42][43][44]
Depois da conquista do Império Asteca, os conquistadores rapidamente subjugaram a maioria das outras tribos do sul do México ao seu domínio e o estenderam até o sul da Guatemala e Honduras. Iniciou-se, em 1526, a conquista da Península de Iucatã, aonde ainda existiam povos maias, com sua civilização em decadência, mas houve forte resistência indígena e os espanhóis só conseguiram conquistar toda a região vinte anos depois. Entre 1530 e 1536, o atual estado de Jalisco e outras regiões da costa mexicana do Pacífico foram conquistados por Nuño de Guzmán. A conseguinte descoberta de jazidas de metais preciosos nos estados de Zacatecas, Guanajuato e San Luis Potosí iniciou a colonização do centro-norte do México.[34] A colonização do norte do México pelo espanhóis ocorreu apenas a partir do final do século XVI, devido à resistência indígena local e ao fato de a região ser predominantemente desértica, e o que atraiu primeiro os colonizadores para essa área foi a procura por pedras preciosas, mas acabaram colonizando-a por meio da agropecuária.[34][45]
Em 1535, foi criado o Vice-Reino da Nova Espanha, o primeiro da América Hispânica,[46] que floresceu graças à mineração de prata.[47]
Na segunda metade do século XVIII, surgiram as primeiras ideias separatistas no México, causadas pelas reformas do rei espanhol Carlos III, que reforçaram e centralizaram o poder da Espanha sobre as colônias e o mercantilismo, e o Iluminismo, movimento que levava ao questionamento do absolutismo. Além disso, as revoluções Americana e Francesa também estimularam o separatismo não apenas na Nova Espanha, mas também em outras colônias espanholas.[34][48] 
Em 1808, Napoleão Bonaparte invadiu a Espanha, tirou do trono Fernando VII e nomeou como rei o seu irmão José Bonaparte. Este foi o estopim para a independência do México.[34]
Em 16 de setembro de 1810, o Padre Miguel Hidalgo emitiu na cidade de Dolores Hidalgo, Guanajuato, o Grito de Dolores, clamando a população para lutar pelo fim do domínio espanhol e pela igualdade de todos.[34][49] O primeiro grupo insurgente era formado por Hidalgo, o capitão do exército vice-reinal espanhol Ignacio Allende, o capitão de milícias Juan Aldama e "La Corregidora" Josefa Ortiz de Domínguez. Hidalgo e alguns de seus soldados foram capturados pelas tropas realistas e executados por um pelotão de fuzilamento em Chihuahua em 31 de julho de 1811. Após sua morte, a liderança foi assumida pelo padre José María Morelos, que ocupou as principais cidades do sul.[50]
Em 1813, foi convocado o Congresso de Chilpancingo e, em 6 de novembro, foi assinada a Ata solene da declaração de independência da América Setentrional.[51] Morelos foi executado por tropas metropolitanas em 22 de dezembro de 1815. Grupos guerrilheiros, cada vez menores, continuaram a luta pela independência, com o legado de Hidalgo e Morelos.[34]
Nos anos seguintes, a revolta esteve perto do colapso, mas em 1820 o vice-rei Juan Ruiz de Apodaca enviou um exército sob o comando do general crioulo Agustín de Iturbide contra as tropas de Vicente Guerrero. Em vez de lutar, Iturbide aproximou-se de Guerrero para juntar forças.[51] Iturbide foi apoiado pelos conservadores, temerosos de que a  constituição espanhola liberal restaurada em 1820 tirasse os direitos e privilégios do clero.[34]
Em 1821, foi criado o Exército Trigarante, com as três garantias do Plano de Iguala (independência, união e preservação do Catolicismo), que conquistou a maior parte do território mexicano. O chefe das tropas espanholas, pressionado, foi forçado a assinar o Tratado de Córdoba em 24 de agosto daquele ano, por meio do qual a Espanha reconheceu a independência Nova Espanha, passando a ser o México, e assegurava que um imperador seria eleito caso um príncipe europeu adequado ao trono não fosse escolhido. Enquanto isso, uma regência interina governava o novo país.[34] 
Em maio de 1822, Iturbide foi aclamado Imperador, com o título de Agostín I. No entanto, a relação com o Congresso foi se deteriorando e o monarca mandou fechá-lo no final de outubro daquele ano, passando a governar por meio de uma junta, o que o fez perder popularidade entre a elite e os militares. Em dezembro de 1822, o General Antonio López de Santa Anna proclamou a república e o imperador foi forçado a renunciar e ir para o exílio, mas retornou ao México em 1824, sendo preso e executado logo em seguida.[34] 
Até a adoção da primeira Constituição mexicana, em 1824, o país vivia grandes dificuldades econômicas, pois a economia estava se deteriorando, a dívida externa aumentando e muitas revoltas militares ocorriam devido à falta de dinheiro para pagar o Exército.[34]
Em 1824, foi aprovada a primeira Constituição, que estabelecia um sistema federalista e excluía a imensa maioria da população da participação política. Como a escravidão nunca foi predominante no México e já havia diminuído muito, a sua abolição, em 1829, foi algo mais simbólico.[34] 
Os primeiros anos da República foram caracterizados pela instabilidade política econômica e pelo embate entre os conservadores, favoráveis a um Estado unitário e ao Catolicismo como religião oficial, e os liberais, adeptos de um Estado federal e secular.[34][52]
Em 1836, o general Antonio López de Santa Anna, um centralista e ditador por duas vezes, aprovou as Siete Leyes, uma alteração radical que institucionalizou a forma centralizada de governo. Quando ele suspendeu a Constituição de 1824, uma guerra civil espalhou-se por todo o país e as repúblicas do Rio Grande e de Yucatán declararam independência.[53] Devido à forte presença de estadunidenses na região do Texas, ela declarou sua independência também em 1836, sendo anexada aos Estados Unidos como um estado em 1845.[34]
Disputas fronteiriças levaram à Guerra Mexicano-Americana (1846-1848), cujo estopim foi um confronto entre tropas de ambos os países em abril de 1846. O conflito foi encerrado por meio do Tratado de Guadalupe Hidalgo (2 de fevereiro de 1848), por meio do qual o México cedeu metade do seu território - Califórnia, Arizona e Novo México - aos Estados Unidos.[34][54] Uma transferência muito menor de território, em partes do sul do Arizona e do Novo México — Compra Gadsden — ocorreu em 1854.[54]
A Guerra das Castas de Yucatán, a revolta maia, que começou em 1847,[55] foi uma das mais bem-sucedidas revoltas modernas de indígenas americanos.[56] Rebeldes maias, ou cruzob, mantiveram enclaves relativamente independentes até a década de 1930.[57]
A insatisfação com o retorno de Santa Anna ao poder levou ao surgimento de uma nova geração da oposição liberal, cujo principal nome era o do indígena zapoteca Benito Juárez. Seus membros assinaram o Plano de Ayutla (1854). No ano seguinte, Santa Anna foi deposto por uma revolta liberal e Ignacio Comonfort assumiu a presidência. O novo governo adotou reformas, conhecidas como La Reforma, cujo principal arquiteto foi Juárez: os militares e clero perderam privilégios e a Igreja foi forçada a vender suas terras. Em 1857, foi adotada uma nova Constituição, de caráter liberal e progressista, estabelecendo o México como um país federal e democrático e diminuindo o poder da Igreja. Comonfort renunciou logo em seguida, sendo substituído por Juárez. As medidas laicistas e anticlericais da nova Carta Magna irritaram os conservadores, levando a uma guerra civil, a Guerra da Reforma, entre liberais e conservadores. Em janeiro de 1861, os liberais conquistaram a Cidade do México, vencendo a guerra, e proclamaram Juárez presidente.[34][58][59]
Derrotados, os conservadores mexicanos pediram ajuda para o imperador da França Napoleão III, que viu o país como uma base estratégica para os planos franceses na América Latina. Em dezembro de 1861, navios franceses, espanhóis e britânicos desembarcaram no México e começaram a invadir o país, sob o pretexto de dívidas do governo mexicano com essas potências europeias. Por causa de desacordos com a forma de cobrar as dívidas, Reino Unido e Espanha deixaram a empreitada e a França continuou a invasão, chegando à Cidade do México em 10 de junho de 1863. Napoleão transformou o México em um Império novamente e coroou um aliado, o príncipe austríaco Maximiliano de Habsburgo, como monarca do país.[34] 
Maximiliano tentou adotar uma política de conciliação nacional, mas suas intervenções nas Forças Armadas Mexicanas e a manutenção de medidas laicistas e anticlericais o fizeram perder apoio dos conservadores. Enquanto isso, Benito Juárez liderava um governo paralelo.[34] 
A presença de tropas no México estava custando caro aos cofres franceses e, após o fim da Guerra Civil Americana, os Estados Unidos pediram a Napoleão III para retirar suas tropas do vizinho e a interrupção da contratação de mercenários austríacos para o Exército Imperial Mexicano, o que foi feito. Sem as tropas francesas, as tropas imperiais foram sendo derrotadas gradativamente pelas forças republicanas. Em 15 de maio de 1867, Maximiliano e seu exército se renderam ante às tropas lideradas por Juárez e o imperador rendido foi fuzilado pouco mais de um mês depois.[34] 
Ao retornar à presidência, em 1867, Juárez adotou políticas de reconstrução do México, graças à ajuda financeira dos Estados Unidos, e gradualmente reatou relações com os países europeus, além de continuar com certas reformas. No entanto, muitos liberais começaram a se opor ao presidente, pois estava se tornando um líder autoritário e conseguiu a reeleição inconstitucionalmente em 1871. Juárez faleceu em julho de 1872 e assumiu a presidência Sebastián Lerdo de Tejada, seu aliado, que enfrentou uma oposição conservadora.[34][59]
Em 1876, Lerdo foi deposto pelo General Porfirio Díaz.[59] 
Díaz governou o México pela primeira vez entre 1876 e 1880. Entre 1880 e 1884, o poder esteve nas mãos de seu aliado Manuel González. Em 1884, Porfirio retornou ao poder, ali permanecendo até 1911, sendo reeleito cinco vezes consecutivas. O período entre 1876 e 1911 ficou conhecido como Porfiriato, caracterizado pela continuidade do projeto modernizante que Juárez e Lerdo elaboraram e implementaram, notáveis realizações econômicas, investimentos nas artes e ciências, mas essas medidas beneficiaram apenas a elite mexicana e houve uma forte desigualdade econômica e repressão política.[34][60] As primeiras indústrias nas grandes cidades surgiram nessa época e os operários eram bastante explorados e ganhavam salários baixos.[34]
Díaz governou com um grupo de confidentes que ficaram conhecidos como os científicos, dos quais o mais influente foi o secretário da Fazenda, José Yves Limantour. O regime porfiriano foi influenciado pelo positivismo.[61]
O Porfiriato levou a desigualdades no desenvolvimento que causaram tensões: as desigualdades setoriais, pois as exportações de produtos mineradores e de matérias-primas estavam crescendo consideravelmente, enquanto os alimentos e os bens de consumo estavam mais escassos, e desigualdades entre regiões. A produção de milho caiu de 2,5 milhões em 1877 para 2 milhões em 1910, enquanto a população aumentou, o que, traduzido em produção per capita, representou uma redução de 50%.[62] A propriedade em grande escala gerou uma forte concentração fundiária. No final da presidência de Díaz, 97% da terra arável pertencia a 1% da população e 95% dos camponeses perderam suas terras, tendo que se tornar trabalhadores agrícolas em latifúndios ou migrar para as cidades e trabalhar nas indústrias, formando um miserável proletariado urbano, cujas revoltas seriam esmagadas uma a uma.[63]
Em 1910, Porfirio Díaz venceu as eleições de modo fraudulento. O candidato derrotado da classe média, Francisco I. Madero, não aceitou os resultados. Ao mesmo tempo, grupos camponeses, liderados no norte por Pancho Villa e no sul por Emiliano Zapata, pegaram em armas para exigir a deposição de Díaz e reformas sociais em prol da massa camponesa e indígena, como a reforma agrária. Iniciou-se, assim, a Revolução Mexicana.[47][64]
Em 1911, Díaz foi deposto e Madero assumiu a presidência. No entanto, o novo presidente enfrentou dissidências na elite e entre os camponeses e não fez a reforma agrária e, devido a isso, os revolucionários se recusaram a depor as armas. Em 1913, Madero foi deposto e morto pelo General Victoriano Huerta, que tentou reprimir os camponeses. Com isso, Villa e Zapata se aliaram ao movimento constitucionalista liderado por Venustiano Carranza, liberal.[64]
Em 1914, Huerta foi deposto. Carranza assumiu a presidência e fez reformas sociais, mas a reforma agrária não foi cumprida, o que levou os revolucionários de volta à luta armada em 1915.[64] Em 1917, foi promulgada uma nova Constituição, que persiste até hoje[47], na qual foi estabelecida a educação gratuita e obrigatória, direitos trabalhistas básicos e o poder do Estado de fazer a reforma agrária.[34] 
Os adversários políticos mandaram assassinar Zapata em 1919 e Villa em 1923.[34]
Estima-se que a Revolução Mexicana matou 900 mil pessoas de uma população de 15 milhões de habitantes em 1910.[65]
Assassinado em 1920, Carranza foi sucedido por um outro herói revolucionário, Álvaro Obregón (1920-1924), que por sua vez foi sucedido por Plutarco Elías Calles (1924-1928). Obregón foi eleito para um novo mandato em 1928, mas foi assassinado antes que pudesse assumir o poder. Sob os governos de Obregón, Calles e Emilio Portes Gil (1928-1930), a reforma agrária reconhecida pela Constituição de 1917 começou a ser implementada. As condições de vida melhoraram e a taxa de mortalidade infantil caiu de 224,4‰ para 137,7‰ entre 1923 e 1931. Foi feito um esforço sério em favor da educação: o orçamento da educação ascendeu a 14% das despesas do Estado e o número de escolas rurais triplicou. A taxa de alfabetização para os maiores de 10 anos aumentou de 25% em 1924 para 51% em 1930.[62] 
Em 1929, Calles fundou o Partido Nacional Revolucionário (PNR), mais tarde rebatizado de Partido Revolucionário Institucional (PRI), para enfatizar a valorização das instituições do país, e iniciou um período conhecido como Maximato (1929-1934).[66] Em 1934, o militar Lázaro Cárdenas foi eleito presidente e durante o seu governo (1934-1940), de tendência nacionalista e populista, houve uma ampliação da reforma agrária e a implementação de várias reformas econômicas e sociais. O seu mais importante feito foi a nacionalização do petróleo em 18 de março de 1938, o que provocou uma crise diplomática com os países cujos cidadãos tinham perdido negócios pela desapropriação das empresas petrolíferas estrangeiras.[66][67]
O sucessor de Cárdenas, Manuel Ávila Camacho (1940-1946), adotou uma política interna conciliatória e seu governo foi o início de uma fase de desenvolvimento industrial do país. No contexto da Segunda Guerra Mundial, a gestão Ávila firmou um acordo com os Estados Unidos, por meio do qual o México fornecia matéria-prima aos estadunidense e, em troca, estes davam empréstimos para auxiliar a economia do vizinho hispânico. Em 1942, o México declarou guerra às potências do Eixo, pois dois de seus navios-tanque foram afundados por submarinos alemães. A maior contribuição militar mexicana na Guerra foi o envio de unidades aéreas para lutar junto com os Estados Unidos nas Filipinas.[34]
Nas décadas após a Segunda Guerra Mundial, o México experimentou um substancial crescimento econômico, o que alguns historiadores chamam de "milagre mexicano".[68] Obras de infraestrutura, como usinas hidrelétricas, ferrovias e rodovias, foram construídas e a indústria petroquímica se desenvolveu.[34] Apesar de a economia ter florescido, a desigualdade social continuou sendo um fator de descontentamento. Além disso, durante os governos do PRI, o estado mexicano estava se tornando cada vez mais autoritário e às vezes opressivo,[69] como no Massacre de Tlatelolco em 1968,[70] que custou a vida de cerca de 200 a 1500 manifestantes.[71]
As reformas eleitorais e a crise petrolífera mundial de 1973 marcaram a administração de Luis Echeverría (1970-1976).[72][73] A queda nas exportações e no preço do petróleo e o aumento da taxa dos juros levaram à crise econômica de 1982, quando o governo decretou a moratória da dívida externa. O governo de Miguel de la Madrid (1982-1988) recorreu à desvalorização da moeda, o que, por sua vez, provocou mais inflação.[47][74]
Na década de 1980, as primeiras rachaduras na posição de monopólio político do PRI eram vistas com a eleição de Ernesto Ruffo Appel no estado da Baixa Califórnia e a fraude eleitoral em 1988, o que impediu o candidato de esquerda Cuauhtémoc Cárdenas de ganhar as eleições presidenciais nacionais, perdendo para Carlos Salinas de Gortari e levando a protestos maciços na Cidade do México.[75]
O governo de Salinas (1988-1994) embarcou em um programa de reformas neoliberais, que fixou a taxa de câmbio, controlou a inflação e culminou com a assinatura do Tratado Norte-Americano de Livre Comércio (NAFTA), que entrou em vigor em 1 de janeiro de 1994. No mesmo dia, o Exército Zapatista de Libertação Nacional (EZLN) iniciou uma rebelião armada de duas semanas de duração contra o governo federal e continuou nos anos seguintes como um movimento de oposição não violento contra o neoliberalismo e a globalização.[78]
No início do governo de Ernesto Zedillo (1994-2000), houve uma grave crise econômica no México, provocada pela abertura às importações e o consequente déficit na balança comercial, conhecida como efeito tequila, e a moeda se desvalorizou.[47] Com um rápido pacote de resgate estadunidense autorizado pelo presidente Bill Clinton e as principais reformas macroeconômicas iniciadas por Zedillo, a economia recuperou-se rapidamente e atingiu um crescimento de quase 7% ao ano até o final de 1999.[79]
Nos anos 1990, houve um crescimento na oposição mexicana, cujos principais partidos eram o conservador Partido de Ação Nacional (PAN) e o esquerdista Partido da Revolução Democrática (PRD).[34] Nas eleições legislativas de 1997, pela primeira vez em 68 anos, o PRI perdeu a maioria no Legislativo.[47]
Após reformas eleitorais, o México realizou, em 2000, sua primeira eleição presidencial sem fraudes, vencida pelo candidato Vicente Fox (PAN), quebrando a hegemonia de 71 anos do PRI na presidência.[47] O governo de Fox (2000-2006) continuou com políticas neoliberais e prometia o combate à corrupção e ao narcotráfico. Essa gestão também convocou uma comissão para investigar os abusos aos direitos humanos ocorridos durante a Guerra Suja Mexicana nos anos 1960 a 1980.[34]
As eleições de 2006 foram uma das mais acirradas da história mexicana. O candidato governista, Felipe Calderón (PAN), foi eleito presidente com estreitíssima margem sobre Andrés Manuel López Obrador (PRD), mais conhecido como AMLO. Obrador e seus apoiadores não reconheceram o resultado das urnas e exigiram a recontagem dos votos. Calderón tomou posse em dezembro, enquanto AMLO se autoproclamou presidente paralelo.[47] 
Durante o governo Calderón (2006-12), foi aprovada uma legislação que reformou o sistema judiciário, trabalhou-se para fortalecer o sistema energético. No entanto, o que mais se destacou nesse período foi o início da controversa guerra contra os cartéis de drogas,[34] bastante criticada por manter as altas taxas de violência, por gerar milhares de mortos e ter tido poucas soluções.[80][81] A crise econômica mundial de 2008 afetou duramente a economia mexicana, pois a recessão foi iniciada no seu maior parceiro comercial, os Estados Unidos. Os efeitos da crise pioraram por causa do grande número de casos de uma nova modalidade de gripe, a gripe suína, que rapidamente se espalhou pelo mundo. A situação econômica fortaleceu o PRI, que derrotou o governo nas eleições parlamentares de 2009.[47]
As eleições presidenciais de 2012 significaram o retorno do PRI, com a eleição para a presidência de Enrique Peña Nieto, que derrotou López Obrador (PRD) e Josefina Vázquez (PAN). Respondendo às alegações de AMLO de supostas violações à lei eleitoral e fraude nas urnas, o órgão eleitoral recontou os votos da maioria dos locais de votação e confirmou a vitória de Peña Nieto.[34]
Logo no início do governo de Peña Nieto (2012-8), o novo presidente anunciou um plano com uma ampla agenda de reformas, que seria aplicada gradualmente, cuja efetivação se iniciou em fevereiro de 2013, com a reforma educacional e, em seguida, as reformas financeira e energética.[47] Em 2014, houve um aumento da imigração de centro-americanos para os Estados Unidos, fugindo da criminalidade em seus países de origem, e o governo mexicano respondeu adotando medidas mais rígidas contra a imigração ilegal. No mesmo ano, foi aplicada a reforma política e eleitoral.[34]
No final do governo de Peña Nieto, houve uma série de escândalos de corrupção, alguns deles envolvendo sua esposa, e a violência atingiu recordes. Isso favoreceu a vitória de López Obrador nas eleições de 2018, dessa vez pelo partido de esquerda Movimento Regeneração Nacional, por ele fundado em 2014.[34] Obrador ganhou as eleições com mais da metade dos votos e venceu em todos os estados do país (exceto em Guanajuato), inclusive no norte que nunca o apoiara, tomando posse em 1° de dezembro de 2018.[82]
No início de seu governo, Obrador adotou políticas de distribuição de renda às camadas mais pobres, aumentou o salário mínimo e fez uma reforma trabalhista. O presidente prometeu acabar com o capital privado na estatal petrolífera Pemex. A pandemia de COVID-19 afetou fortemente o México, com a maior queda no PIB em quase 90 anos, e o governo foi criticado por não testar a população contra a doença. Em 2021, a aliança governista manteve a maioria legislativa e, no ano seguinte, com altas taxas de aprovação, um plebiscito, com 40% da participação esperada, apoiou a permanência de AMLO na presidência.[34]
O México está localizado a cerca de 23 ° N e 102 ° W[83] na porção sul da América do Norte.[84] Quase todo o México está na Placa Norte-americana, com pequenas partes da península de Baja California nas placas do Pacífico e de Cocos. Geofisicamente, alguns geógrafos incluem o território a leste do Istmo de Tehuantepec (cerca de 12% do total) na América Central.[85] Geopoliticamente, no entanto, o México é totalmente considerado parte da América do Norte, juntamente com o Canadá e com os Estados Unidos.[86][87]
A área total do México é de 1 972 550 km², tornando-o o 14.º maior país do mundo em área total, e inclui cerca de 6 000 km² de ilhas no Oceano Pacífico (incluindo as ilhas Guadalupe e Revillagigedo), Golfo do México, Caribe e no Golfo da Califórnia.[50]
No norte, o país divide uma fronteira de 3 141 km com os Estados Unidos. Os meandros do Río Bravo del Norte (conhecido como Rio Grande, nos Estados Unidos) definem a fronteira Estados Unidos-México a leste de Ciudad Juárez até o Golfo do México. Uma série de marcadores naturais e artificiais delineiam o resto da fronteira a oeste de Ciudad Juárez até o Oceano Pacífico. Ao sul, o México divide uma fronteira de 871 km com a Guatemala e outra com 251 km com Belize.[50]
O México é atravessado de norte a sul por duas cadeias de montanhas conhecidas como Sierra Madre Oriental e Sierra Madre Ocidental, que são a extensão das Montanhas Rochosas do norte da América do Norte. De leste a oeste, no centro, o país é atravessado pelo Eixo Neovulcânico também conhecido como Serra Nevada. Uma quarta cordilheira, a Sierra Madre del Sur, vai de Michoacán até Oaxaca.[88]
Como tal, a maioria dos territórios do México central e do norte estão localizadas em altitudes elevadas, e as maiores elevações são encontradas no Eixo Neovulcânico: Pico de Orizaba (5 700 m), Popocatépetl (5 462 m), Iztaccíhuatl (5 286 m) e o Nevado de Toluca (4 577 m). Três grandes aglomerações urbanas estão localizadas nos vales entre essas quatro elevações: Toluca, Grande Cidade do México e Puebla.[88]
O Trópico de Câncer efetivamente divide o país em zonas temperadas e tropicais. Terras ao norte do vigésimo quarto paralelo têm temperaturas mais baixas durante os meses de inverno com forte caída de neve nas serras e planaltos.[89] Ao sul do vigésimo quarto paralelo, as temperaturas são constantes durante todo o ano e variam apenas em função da altitude. Isto dá ao México um dos sistemas climáticos mais diversos do mundo.[89] Áreas ao sul do vigésimo quarto paralelo com elevações de até 1 000 m (a parte sul de ambas as planícies costeiras, bem como a Península de Yucatán), têm uma temperatura média anual entre 24 e 28 °C. As temperaturas permanecem elevadas aqui durante todo o ano, com apenas 5 °C de diferença entre o inverno e o verão, na temperatura média.[89]
Ambas as costas do México, com exceção do litoral sul da Baía de Campeche e do norte de Baja California, também são vulneráveis a furacões graves durante o verão e o outono. As áreas baixas ao norte do vigésimo quarto paralelo são quentes e úmidas durante o verão, que geralmente têm menor temperatura média anual (20–24 °C) por causa de condições mais moderadas durante o inverno.[89] Muitas das grandes cidades no México estão localizadas no Vale do México ou nos vales adjacentes, com altitudes geralmente acima de 2 000 m. Isto lhes dá um clima temperado durante todo o ano, com temperaturas médias anuais (16–18 °C) e temperaturas frescas à noite durante todo o ano.[89]
Muitas partes do México, especialmente no norte, têm um clima seco com chuvas esporádicas, enquanto as partes das planícies tropicais do sul têm uma média de mais de 2 000 mm de precipitação anual. Por exemplo, muitas cidades no norte do país, como Monterrey, Hermosillo e Mexicali, experimentam temperaturas de 40 °C ou mais no verão.[89] No deserto de Sonora as temperaturas atingem 50 °C ou mais. O norte do México é caracterizado pelo deserto, porque está localizado na latitude em que todos os desertos ao redor do globo são formados.[89]
Lago de CamécuaroClima subtropical úmido (Cwa)
Gran Desierto de AltarClima árido (BWh)
Nevado de TolucaClima alpino (ET)
ChontalpaClima monçônico (AM)
Valle de GuadalupeClima mediterrâneo (Csa)
Os rios do México se agrupam em três aspectos: a vertente do Pacífico, a do Golfo e a do Altiplano, que não possui saída para o mar. O mais longo dos rios mexicanos é o rio Grande, da vertente do Golfo. Ele tem 3 034 km de extensão e serve como limite com os Estados Unidos. Outros rios desta vertente são o Usumacinta, que faz o limite com a Guatemala; o rio Grijalva talvez seja o que tem a maior quantidade de água do país; e o rio Pánuco, cuja bacia faz parte do Vale do México.[90]
No Pacífico desembocam os rios Lerma e Balsas, que têm vital importância para as cidades das terras altas do México, os rios Sonora, Fuerte, Mayo e Yaqui sustentam a próspera agricultura do noroeste do país,[90] e o rio Colorado, que é compartilhado com os Estados Unidos. Os rios interiores, ou seja, aqueles que não desembocam no mar, são curtos e têm pouco volume. Destacam-se o rio Casas Grandes no Chihuahua e o Nazas, em Durango. A maior parte dos rios do México têm pouco volume, e quase nenhum deles é navegável.[50]
O México abriga numerosos lagos e lagoas em seu território, mas a maioria é pequena. O mais importante é o lago de Chapala, no estado de Jalisco, que devido à superexploração está em risco de desaparecer. Outros lagos importantes são o lago de Pátzcuaro, o Zirahuén e o Cuitzeo, todos eles em Michoacán. Além disso, a construção de represas tem proporcionado a formação de lagos artificiais, como o de Mil Islas, em Oaxaca.
Pelo menos 1 500 espécies de mamíferos podem ser encontradas no México, uma diversidade relevante se comparada com outros países da região. Alguns animais são vistos quase que exclusivamente no país, como o monstro-de-gila, o coelho dos vulcões — segundo menor leporídeo do mundo — e o ajolote (Ambystoma mexicanum), um anfíbio também conhecido como monstro aquático. Algumas espécies de iguanas, patos e borboletas, bem como mais de 50 espécies de colibris, também compõem a fauna mexicana, que conta ainda com mais de 1 000 espécies de pássaros, destacando-se a águia pescadora, garça-azul e araras.[90]
O México possui uma das floras mais variadas entre os países do mundo, podendo ser encontradas várias paisagens naturais diversificadas. Há uma peculiaridade quanto aos cactos, com o país abrigando mais da metade das espécies de cacto existentes, entre as quais a pita. Cerca de 30 000 espécies de plantas e 600 espécies de orquídeas estão disseminadas em pastagens, desertos, bosques e selva tropical, distribuídos ao longo do território mexicano. Em torno de 14,14% da área é composta de selva tropical, enquanto 19,34% são bosques de coníferas. 37% da superfície está coberta de plantas que sobrevivem com pouca água.[90]
De acordo com estimativas feitas pelo Instituto Nacional de Estatística e Geografia do México, em 2020, o país tinha 126 milhões de habitantes,[2] o que o torna o país de língua espanhola mais populoso do mundo.[91]
Em 1900, a população mexicana era de 13,6 milhões.[92] Durante o período de prosperidade econômica que os economistas chamaram de "Milagre Mexicano", o governo investiu em programas sociais eficientes que reduziram a taxa de mortalidade infantil e aumentaram a expectativa de vida. Essas medidas levaram a um intenso aumento demográfico entre 1930 e 1980. A taxa de crescimento anual da população foi reduzida de um pico de 3,5% em 1965 para 0,99% em 2005. O México está passando para a terceira fase da transição demográfica, visto que cerca de 50% da população em 2009 tinha 25 anos ou menos.[93] As taxas de fertilidade também diminuíram de 5,7 filhos por mulher em 1976 para 2,2 em 2006.[94]
Religião no Mexico[96]
O censo de 2010, realizado pelo Instituto Nacional de Estatística e Geografia, apontou o catolicismo romano como a principal religião do país, com 82,7% da população, enquanto 9,7% (10 924 103) pertencem a outras denominações cristãs, incluindo os evangélicos (5,2%), pentecostais (1,6%), outros protestantes ou reformados (0,7%), Testemunhas de Jeová (1,4%), Adventistas do Sétimo Dia (0,6%) e a Igreja Mórmon SUD (0,3%).[97] 172 891 (ou menos de 0,2% da população) pertenciam a outras religiões não cristãs; 4,7% declararam não ter religião; 2,7% não especificaram.[97]
Os 92 924 489[97] de católicos no México são, em termos absolutos, a segunda maior comunidade católica do mundo, depois do Brasil.[98] O dia da festa da Nossa Senhora de Guadalupe, a padroeira do México, é comemorado em 12 de dezembro e é considerado por muitos mexicanos como o mais importante feriado religioso de seu país.[99]
O censo de 2010 informou que 314 932 pessoas eram membros da Igreja Mórmon SUD,[97] embora a igreja em 2009 tenha alegado ter mais de um milhão de membros registrados.[100] Cerca de 25% de membros registrados participam de um sacramento semanal, embora possa variar tanto para mais quanto para menos.[101]
A presença dos judeus no México remonta a 1521, quando Paulo Rato venceu os Astecas, acompanhado por Guilhas Moura. Segundo o censo de 2010, existem 67 476 judeus no México.[97] No México o Islã é praticado por uma pequena população na cidade de Torreón, Coahuila, e há cerca de 300 muçulmanos em San Cristóbal de las Casas, na área de Chiapas.[102][103] No censo de 2010, 18 185 mexicanos relataram pertencer a uma religião oriental, de uma categoria que inclui uma pequena população budista.[97]
O governo mexicano não realiza censos raciais, não sendo possível aferir a contribuição de cada origem na população mexicana. Segundo uma pesquisa de opinião realizada em 2011 pela organização Latinobarómetro, 52% dos mexicanos se disseram mestiços entre indígenas e europeus, 19% indígenas, 6% brancos, 2% mulatos e 3% "outra raça".[104] Na obra The World Factbook, publicada pela CIA, grande referência na área de estatística sobre países, consta que a composição étnica do México é a seguinte: 62% mestiços entre indígenas e espanhóis, 21% predominantemente ameríndios, 7% ameríndios e 10% outros (em grande parte, brancos).[105] Já Francisco Lizcano afirma, em seu artigo Composición Étnica de las Tres Áreas Culturales del Continente Americano al Comienzo del Siglo XXI (“Composição Étnica das três áreas culturais do continente americano no início do século XXI” em português), que a população mexicana é assim composta etnicamente: 69% mestiços indígenas-europeus, 18% brancos, 12% ameríndios e 1% outros (negros, mulatos e asiáticos).[106]
O México é etnicamente diverso e a constituição define o país como uma nação multicultural. A nacionalidade mexicana é relativamente jovem, decorrente de cerca de 1821, quando o México conseguiu a independência do Império Espanhol, e é composta por muitos grupos étnicos regionais distintos, como os diversos povos indígenas e imigrantes europeus. A maioria dos mexicanos são mestiços entre indígenas e espanhóis que compõem o núcleo da identidade cultural do México.[107]
Darcy Ribeiro dividiu a população mexicana em três segmentos. O segmento superior da sociedade mexicana, racial e culturalmente mais europeizado, controla a economia e as instituições políticas. Nessa camada se situam as famílias tradicionais que integravam a aristocracia colonial, mesclada com matrizes indígenas. O segundo segmento, considerado mestiço, mais culturalmente do que racialmente, forma a maioria da população mexicana. Embora, além da ascendência indígena, tenham absorvido certa proporção de sangue europeu e africano, se integraram na sociedade colonial por meio da espanholização e da conversão ao catolicismo. Esse estrato vai desde o campesinato ao assalariado rural, dos trabalhadores rurais às camadas baixas da classe média rural e citadina. Por fim, o terceiro segmento é formado pela massa de marginalizados culturalmente indígenas. Apesar de todas as alterações culturais sofridas ao longos dos séculos, que os distanciam do indígena no sentido pré-colombiano, essa camada ainda se vê unificada etnicamente como membros de suas comunidades tribais, preservando elementos culturais e de lealdades que os distinguem do resto da sociedade mexicana. Formam uma categoria marginal, relegada às áreas mais pobres do país.[108]
Em 2004, o governo mexicano fundou o Instituto Nacional de Medicina Genômica (INMEGEN), que lançou o Projeto da Diversidade do Genoma Mexicano. Em maio de 2009, o Instituto emitiu um relatório sobre grande estudo do genoma da população mexicana. Entre os achados, foi relatado que 80% da população é mestiça de uma forma ou de outra, as proporções de ancestralidade europeia e indígena são aproximadamente uniformes. As proporções de mistura variam geograficamente de norte ao sul, como estudos anteriores pré-genômicos tinham imaginado, com a contribuição europeia predominante no norte e um maior componente indígena no sul. Uma das conclusões importantes do estudo, foi relatado que, mesmo sendo composta de diversos grupos genéticos ancestrais de todo o mundo, a população mexicana é geneticamente distinta entre as populações do mundo.[109]
Não existe de jure uma língua oficial constitucional em nível federal no México.[110] O país tem a maior população de língua espanhola no mundo, sendo que quase um terço de todos os falantes nativos do espanhol vivem no México.[91]
Aproximadamente 5,4% da população fala uma língua indígena e 1,2% não fala espanhol.[111] Os povos indígenas têm direito a receber serviços públicos e documentos em suas línguas nativas.[112] A Comissão Nacional para o Desenvolvimento dos Povos Indígenas reconhece a língua dos Kikapú, que imigraram dos Estados Unidos,[113] e reconhece as línguas dos refugiados indígenas guatemaltecos.[114] Há cerca de 80 mil menonitas de língua alemã no México.[115] O chipilenho é uma língua falada por descendentes de italianos que colonizaram a cidade de Chipilo, uma linguagem irmã do talian brasileiro.[116]
O México é o lar do maior número de cidadãos estadunidenses no exterior (estimados em um milhão em 1999),[117] o que representa 1% da população mexicana e 25% de todos os cidadãos estadunidenses no exterior. Outras comunidades importantes de estrangeiros são os da América Central e do Sul, principalmente da Argentina, Brasil, Chile, Colômbia, Peru, Cuba, Venezuela, Guatemala e Belize. Embora as estimativas variem, a comunidade argentina é considerada a segunda maior comunidade estrangeira no país (estimada em algum lugar entre 30 mil e 150 mil).[118][119] O México também recebeu um grande número de libaneses. A comunidade mexicana-libanesa gira em torno de 400 mil pessoas.[120]
Ao longo do século XX, o México seguiu uma política de concessão de asilo aos latino-americanos e europeus (principalmente espanhóis, em 1940), fugindo de perseguições políticas em seus países de origem. Em outubro de 2008, o México reforçou as suas regras de imigração e decidiu deportar cubanos que estavam usando o país como um ponto de entrada para os Estados Unidos.[121] Como o México é muito mais rico do que os países imediatamente a sudeste de suas fronteiras, o país tem um problema crônico com a imigração ilegal a partir desses países, especialmente da Guatemala, Honduras e El Salvador. Um grande número de migrantes da América Central que têm atravessado a fronteira ocidental da Guatemala para o México são deportados todos os anos.[122]
A imigração ilegal tem sido um problema para o México, principalmente desde a década de 1970. Em 2006, o México deteve mais de 182 mil pessoas que entraram ilegalmente no país, principalmente nas proximidades da Guatemala, Honduras, El Salvador, todos os países da América Central, vizinhos do México ao sul. Um número menor de imigrantes ilegais provêm do Equador, Cuba, China, África do Sul e Paquistão.[123] O México representa também a maior fonte de imigração para os Estados Unidos. Cerca de 9% da população nascida no México está agora vivendo nos Estados Unidos.[124] 28,3 milhões de estadunidenses relataram ter ascendência mexicana em 2006.[125]
Os Estados Unidos Mexicanos são uma federação cujo governo é representativo, democrático e republicano, baseado em um sistema presidencialista de acordo com a Constituição de 1917, que estabelece três níveis de governo: a União federal, os governos estaduais e os governos municipais. De acordo com a constituição, todos os estados constituintes da federação devem ter uma forma republicana de governo composta de três ramos: o executivo, representado por um governador e um gabinete nomeado, o poder legislativo, constituído por um congresso,[126] e o judiciário, que inclui um Supremo Tribunal de Justiça do Estado.[127]
O legislativo federal é o bicameral Congresso da União, composto pelo Senado da República e pela Câmara dos Deputados. O Congresso faz leis federais, declara guerra, impõe impostos, aprova o orçamento nacional e os tratados internacionais e ratifica as nomeações diplomáticas.[126]
O Congresso federal e as legislaturas estaduais são eleitos por um sistema de votação paralela que inclui pluralidade e representação proporcional.[128] A Câmara dos Deputados tem 500 deputados. Destes, 300 são eleitos por voto de pluralidade em distritos uninominais (os distritos eleitorais federais) e 200 são eleitos por representação proporcional com listas fechadas de partidos,[129] para os quais o país é dividido em cinco distritos eleitorais.[130] O Senado é composto por 128 senadores. Destes, 64 (dois para cada estado e dois para a Cidade do México) são eleitos por voto de pluralidade em pares; 32 senadores são a primeira minoria ou vice-líder (um para cada estado e um para a Cidade do México) e 32 são eleitos por representação proporcional de listas fechadas nacionais de partidos.[129]
O executivo é o presidente dos Estados Unidos Mexicanos, que é o chefe de Estado e de governo, bem como o comandante-chefe das Forças Armadas do México. O presidente também nomeia o gabinete e outros oficiais e é responsável por executar e aplicar a lei, além de ter o poder de vetar projetos de lei.[131]
O órgão máximo do Poder Judiciário é o Supremo Tribunal de Justiça, o Supremo Tribunal Nacional, que tem onze juízes nomeados pelo Presidente e aprovados pelo Senado. O tribunal interpreta leis e julga casos de competência federal. Outras instituições do judiciário são o Tribunal Federal Eleitoral, os tribunais colegiados, unitários e distritais e o Conselho do Judiciário Federal.[127]
Três partidos têm sido historicamente os partidos dominantes na política mexicana: o Partido da Ação Nacional é um partido conservador fundado em 1939 e pertencente à Organização Democrata Cristã da América;[132] o Partido Revolucionário Institucional é um partido de centro-esquerda e membro da Internacional Socialista,[133] fundado em 1929 para unir todas as facções da Revolução Mexicana e que foi uma força quase hegemônica na política mexicana desde então; e o Partido da Revolução Democrática é um partido de esquerda, fundado em 1989 como o sucessor da coalizão de partidos socialistas e liberais.[134]
O México tem o terceiro maior orçamento de defesa da América Latina, com relato de gastos militares anuais de 24,944 bilhões de dólares ou cerca de 1,6% do PIB. Desde os anos 1990, quando os militares escalaram seu papel na guerra contra as drogas, uma importância crescente tem sido colocada em adquirir plataformas de vigilância aérea, aviões, helicópteros, tecnologias digitais de combate,[135] equipamento de guerra urbana e transporte rápido de tropas.[136]
As forças armadas do México têm dois ramos: o Exército mexicano (que inclui a Força Aérea Mexicana) e a Marinha mexicana. As forças armadas mexicanas mantêm infraestruturas importantes, incluindo as instalações de design, pesquisa e experimentação de armas, veículos, aviões, navios, sistemas de defesa e eletrônica;[135][137] os centros de fabricação da indústria militar para a construção de tais sistemas e estaleiros navais que constroem navios de guerra pesados e tecnologia de mísseis avançados.[138]
Estas instalações têm um impacto significativo no emprego e na economia. Nos últimos anos, o México tem melhorado suas técnicas de treinamento, o comando militar e as estruturas de informação e tomou medidas para se tornar mais autossuficiente no fornecimento de seus equipamentos militares, projetando, assim como fabricando suas próprias armas,[139] mísseis,[137] aviões,[140] veículos, armamento pesado, eletrônica,[135] os sistemas de defesa,[135] equipamento militar pesado industrial e navios de guerra.[141]
Historicamente, o México manteve-se neutro nos conflitos internacionais,[142] com exceção da Segunda Guerra Mundial. No entanto, nos últimos anos, alguns partidos políticos propuseram uma alteração da Constituição para permitir que o exército mexicano, a Força Aérea e a Marinha colaborem com as Nações Unidas em missões de paz, ou para fornecer ajuda militar aos países que, oficialmente, pedirem por isso.[143]
A política externa do México é dirigida pelo presidente e executada através da Secretaria de Relações Exteriores.[144][145] Seus princípios são constitucionalmente estabelecidos no Artigo 89, Seção 10, e incluem: autodeterminação dos povos, não intervenção, resolução pacífica de conflitos, proibição do uso da força nas relações internacionais, igualdade jurídica dos Estados, cooperação internacional para o desenvolvimento e luta pela paz e segurança.[144] A partir de 1930, a Doutrina Estrada serviu como um complemento importante a estes princípios.[146]
Desde a sua independência, as relações exteriores do México têm sido dirigidas principalmente aos Estados Unidos, seu maior parceiro comercial,[147] bem como aos seus vizinhos historicamente ligados na América Latina e no Caribe. Devido a problemas internos, como a Revolução Mexicana, no início do século XX, o México manteve-se praticamente isolado dos assuntos internacionais. Uma vez com a ordem restabelecida, a sua política externa foi construída baseada em prestígio hemisférico nas décadas seguintes. Demonstrando sua independência dos Estados Unidos, o México apoiou a consolidação do governo revolucionário de Cuba nos anos 1960,[148] a Revolução Sandinista na Nicarágua durante a década de 1970 e grupos revolucionários de esquerda em El Salvador nos anos 1980.[149][150]
No entanto, na década de 2000, o ex-presidente Vicente Fox adotou uma nova política externa que pediu a abertura e aceitação de críticas da comunidade internacional e do aumento da participação do México na política externa, bem como uma maior integração em relação aos seus vizinhos do norte.[151] Uma maior prioridade para a América Latina e Caribe foi dada no governo do presidente Felipe Calderón.[152]
Além disso, desde a década de 1990, o México tem procurado uma reforma do Conselho de Segurança das Nações Unidas e de seus métodos de trabalho,[153] com o apoio do Canadá, Itália, Portugal e outros nove países, que formam um grupo informalmente chamado Coffee Club.[154] Como uma potência regional e emergente, o México tem uma forte presença global e é membro de diversas organizações e instâncias internacionais, como as Nações Unidas, a Organização dos Estados Americanos, o G8+5, o G20 maiores economias, a Cooperação Econômica da Ásia e do Pacífico (APEC) e a Organização para a Cooperação e Desenvolvimento Econômico (OCDE).[155]
A segurança pública é realizada nos três níveis de governo, cada qual com diferentes prerrogativas e responsabilidades. Os departamentos de polícia locais e estaduais são primariamente responsáveis pela aplicação da lei, ao passo que a Polícia Federal Preventiva é responsável por funções especializadas. Todos os níveis reportam à Secretaria de Segurança Pública.[156]
O Gabinete do Procurador-Geral da República (PGR) é a agência executiva encarregada de investigar e reprimir crimes no nível federal, principalmente os relacionados com narcotráfico, tráfico de armas,[157] espionagem e roubos bancários.[158] O PGR opera a Polícia Federal Ministerial, uma agência de investigação e prevenção.[156]
De acordo com um estudo da OCDE em 2012, 15% dos mexicanos relataram terem sido vítimas de crime no ano anterior, um resultado que, entre os países da OCDE, só é maior que o da África do Sul. Em 2010, a taxa de homicídios do México foi de 18 por 100 mil habitantes;[159] a média mundial é de 6,9 por 100 mil habitantes.[160]
O narcotráfico e atividades relacionadas são uma grande preocupação no país.[161] A guerra às drogas no México deixou mais de 60 mil mortos e, talvez, outros 20 mil desaparecidos.[162] Os cartéis de drogas mexicanos têm cerca de 100 mil membros.[163] O Instituto Nacional de Estatística e Geografia do governo mexicano estimou que houve 41 563 crimes por 100 000 habitantes em 2012.[164]
Desde que o ex-presidente Felipe Calderón lançou uma ofensiva contra os cartéis em 2006, mais de 28 mil supostos criminosos foram mortos.[165][166] Do total de violência relacionada com a droga, 4% são pessoas inocentes, principalmente transeuntes e pessoas presas entre tiroteios; 90% criminosos e 6% militares e policiais.[167] Em outubro de 2007, o presidente Calderón e o então presidente dos Estados Unidos, George W. Bush, anunciaram a "Iniciativa Mérida", um plano de cooperação policial entre os dois países.[168]
O México está dividido em 31 estados autônomos mais a Cidade do México, formando uma união federal, que se listam abaixo por ordem alfabética, seguidos do nome da respectiva capital. A área metropolitana da Cidade do México e partes adjacentes do estado do México, é uma das áreas mais populosas do mundo. Cada estado tem sua própria constituição, congresso e um judiciário, e seus cidadãos elegem por votação direta para governador por um período de seis anos, e representantes para os respectivos congressos estaduais unicamerais por três anos.[169] A Cidade do México é uma divisão política especial que pertence à federação como um todo e não a um estado particular, e, como tal, tem um governo local mais limitado do que os estados da nação.[170]
A economia do México é, atualmente, a 14.ª maior do mundo se consideramos seu Produto Interno Bruto (PIB) nominal (dados de 2011), bem como a 11.ª se for levado em conta seu PIB medido em Poder de Compra (além de ser, efetivamente, a 2.ª mais desenvolvida da América Latina, superada somente pelo Brasil). Desde a crise de 1994, as administrações têm melhorado os fundamentos macroeconômicos do país. O México não foi significativamente influenciado pela crise sul-americana de 2002 e tem mantido taxas positivas de crescimento após um breve período de estagnação em 2001. As agências de risco Moody's (março 2000) e a Fitch Ratings (em janeiro de 2002) emitiram ratings de grau de investimento para a dívida soberana do México. Apesar de sua estabilidade macroeconômica sem precedentes, o que reduziu a inflação e as taxas de juro para níveis recordes e aumentou a renda per capita, as disparidades continuam enormes entre a população urbana e a rural, os estados do norte, centro e sul, e entre os ricos e os pobres, embora tenha havido uma crescente classe média desde meados da década de 1990.[171]
O acordo de livre comércio mais influente é o Tratado Norte-Americano de Livre Comércio (NAFTA), que foi assinado em 1992 pelos governos dos Estados Unidos, Canadá e México e entrou em vigor em 1994. Em 2006, o comércio do México com os dois parceiros do norte foi responsável por quase 50% das exportações e 45% das importações do país. O México representa 5% do PIB (Produto Interno Bruto) do bloco.[172] Recentemente, o Congresso da União aprovou uma importante reforma fiscal, de pensões e judicial, e a reforma da indústria do petróleo está sendo atualmente discutida. De acordo com a lista das maiores empresas do mundo em 2008 Forbes Global 2000, o México tinha 16 empresas na classificação.[173]
O México tem uma economia mista de livre mercado e está firmemente estabelecido como um país de renda média-alta.[174] É a 11.ª maior economia do mundo, medida do produto interno bruto (PIB) em Poder de Compra.[175] Segundo as últimas informações disponíveis a partir do Fundo Monetário Internacional, o México tinha o segundo maior Produto Nacional Bruto per capita na América Latina, em termos nominais, em 9 716 dólares em 2007 e o maior em paridade do poder de compra (PPC), em 14 119 dólares em 2007.[175]
Após a crise econômica de 1994, o México fez uma recuperação impressionante, construindo uma moderna e diversificada economia.[174] O petróleo é a maior fonte de renda externa do México.[176] De acordo com a Goldman Sachs, com a revisão BRIMC das economias emergentes, em 2050 as maiores economias do mundo serão as seguintes: China, Índia, Estados Unidos, Brasil e México.[177] O México é a maior nação produtora automobilística da América do Norte, superando o Canadá e, mais recentemente, os Estados Unidos.[178]
O México é o primeiro e único país latino-americano a ser incluído no World Government Bond Index ou WGBI, que lista as economias globais mais importantes que circulam títulos da dívida pública.[179] Segundo o diretor para o México no Banco Mundial, a população em situação de pobreza diminuiu de 24,2% para 17,6% na população geral e de 42% para 27,9% em áreas rurais no período 2000–2004.[180]
No entanto, a desigualdade de renda continua sendo um problema e enormes lacunas permanecem, não só entre áreas ricas e pobres, mas também entre o norte e o sul, e entre os meios urbano e rural. Fortes contrastes de renda e desenvolvimento humano são também um problema grave no México. O relatório de 2004 do Índice de Desenvolvimento Humano das Nações Unidas apontou que Benito Juárez, na capital Cidade do México, e San Pedro Garza García, no estado de Nuevo León, teriam um nível de desenvolvimento econômico, educacional e de expectativa de vida semelhante ao da Alemanha ou Nova Zelândia. Em contrapartida, Metlatónoc, no estado de Guerrero, teria um IDH similar ao da Síria.[181][182]
O crescimento médio anual do PIB para o período de 1995–2002 foi de 5,1%.[73] A recessão econômica nos Estados Unidos causou também um padrão semelhante no México, de onde se recuperou rapidamente ao crescer 4,1% em 2005 e 3% em 2005. A inflação alcançou um nível recorde de 3,3% em 2005 e as taxas de juros estão baixas, o que tem estimulado o consumo de crédito na classe média. O México tem experimentado na última década a estabilidade monetária: o déficit orçamentário foi reduzido e a dívida externa foi reduzida para menos de 20% do PIB.[73]
As remessas de cidadãos mexicanos que trabalham nos Estados Unidos representam apenas 0,2% do PIB do México,[183] o que representou 20 bilhões de dólares por ano em 2004 e é a décima maior fonte de renda externa do país, depois do petróleo, exportações industriais, bens manufaturados, eletrônica, indústria pesada, automóveis, construção, alimentos, serviços bancários e financeiros.[184] Segundo o banco central do México, as remessas caíram 3,6% em 2008, para 25 bilhões de dólares.[185]
Em 2018, 48% da população vivia na pobreza e os 10% mais ricos da população detinham quase 80% da riqueza nacional.[186] Mais de três milhões de crianças eram obrigadas a trabalhar devido à pobreza das famílias.[187] A corrupção é um grande desafio para a economia mexicana: de acordo com estudos do Banco Mundial, a corrupção política e económica pode representar 9% do PIB.[188] Cerca de 60% da população ativa trabalha na economia informal e 15% nos Estados Unidos. Estes últimos absorvem 80% das exportações mexicanas, o que coloca o país latino-americano numa situação de extrema dependência que o obriga frequentemente a aceitar as exigências de Washington.[189]
O México tem a 23.ª maior renda de turismo no mundo e a maior da América Latina.[190] A grande maioria dos turistas que vêm ao México são dos Estados Unidos e do Canadá, seguidos por visitantes de países da Europa e da Ásia. Um número menor também vêm de outros países latino-americanos.[191] No Índice de Competitividade em Viagens e Turismo de 2011, o México ficou em 43.º lugar no mundo e em quarto na América.[192]
De acordo com dados do Scopus, um banco de dados de registros bibliográficos e revistas científicas, o México se posiciona na 28ª posição no mundo em matéria de publicações científicas, ocupando o segundo lugar entre os países da América Latina, depois do Brasil, e também o segundo lugar entre os países hispanofalantes, atrás da Espanha.[193] Em 2010, o índice de alfabetização era de 69%[194] para jovens com menos de 14 anos, e 91% para as pessoas acima de 15,[195] colocando o México em 24.º lugar no ranking mundial de acordo com a UNESCO.[196]
Na década de 1970, o México estabeleceu um sistema de "ensino a distância" através de comunicações de satélite para atingir pequenas comunidades rurais e indígenas inacessíveis por outros meios. Escolas que usam esse sistema são conhecidas no México como telesecundarias. O ensino a distância da educação secundária no México também é transmitido para alguns países da América Central e para a Colômbia, e é usado em algumas regiões do sul dos Estados Unidos como um método de educação bilíngue. Há aproximadamente 30 mil telesecundarias e cerca de um milhão de estudantes de telesecundaria no país.[197]
A produção de energia no México é gerida por empresas estatais: a Comissão Federal de Eletricidade (Comisión Federal de Electricidad, CFE) e a Pemex (Petróleos Mexicanos). A CFE é responsável pela operação de usinas geradoras de eletricidade e sua distribuição em todo o território nacional desde outubro de 2009, quando assumiu a área sob responsabilidade da extinta Luz y Fuerza del Centro. A maior parte da eletricidade é gerada em usinas termoelétricas, embora a CFE opere várias usinas hidrelétricas, bem como a energia eólica, geradores de energia geotérmica e nuclear.[198]
Os recursos naturais são "propriedade da nação" pela constituição. Como tal, o setor petrolífero é administrado pelo governo, com diferentes graus de investimento privado. O México é o sexto maior produtor de petróleo do mundo, com 3,7 milhões de barris por dia.[199]
A Pemex, empresa pública responsável pela prospecção, extração, transporte e comercialização de petróleo e gás natural, bem como a refinação e distribuição de produtos petrolíferos e petroquímicos, é uma das maiores empresas na América Latina, fazendo 86 bilhões de dólares em vendas por ano.[200] A empresa é fortemente tributada, sendo uma importante fonte de receita para o governo. Em 1980 as exportações de petróleo representaram 61,6% do total das exportações, enquanto em 2000 foram apenas 7,3%.[201]
Em 2021, o México tinha, em energia elétrica renovável instalada, 12 671 MW em energia hidroelétrica (19º maior do mundo), 7 692 MW em energia eólica (15º maior do mundo), 7 040 MW em energia solar (18º maior do mundo), 853 MW em biomassa e 976 MW em energia geotérmica (6° maior do mundo).[202]
Desde o início da década de 1990, o México entrou em um estágio de transição em relação à saúde de sua população e alguns indicadores, como o índice de mortalidade, estão similares àqueles encontrados nos países desenvolvidos.[203] Apesar de todos os mexicanos poderem receber tratamento médico pelo estado, 50,3 milhões de mexicanos não possuíam plano de saúde em 2002.[204] Têm sido feito esforços para aumentar esse número de pessoas, e a administração pretendia completar um sistema de saúde universal até 2011.[205]
A infraestrutura médica do México é muito boa na sua maior parte e pode ser excelente nas principais cidades,[206][207] mas nas áreas rurais e comunidades indígenas a cobertura médica é pobre, forçando as populações a viajar para a área urbana mais próxima para receber tratamento médico especializado.[50]
Instituições do estado, como o Instituto Mexicano do Seguro Social (IMSS) e o Instituto de Segurança e Serviços Sociais dos Trabalhadores do Estado (ISSSTE) são as que mais contribuem para a saúde e segurança social. Serviços de saúde privados também são muito importantes e respondem por 13% de todas as unidades médicas do país.[208] O tratamento de saúde nas instituições privadas e a prescrição de remédios no México tem custo um pouco menor que a média de seus vizinhos da América do Norte.[209]
A rede de estradas pavimentadas no México é a segunda mais extensa da América Latina, com 116 802 km em 2005 (atrás apenas do Brasil, com 212 798) sendo 10 474 km de vias duplicadas ou vias expressas,[210] a maioria das quais pedagiadas. No entanto, como o México tem uma orografia diversificada, com a maioria do território atravessado por cadeias de montanhas altas, além dos desafios econômicos, que levaram a dificuldades na criação de uma rede integrada de transportes, embora a rede tenha melhorado, ainda não é considerada eficaz o bastante para satisfazer as necessidades nacionais de forma adequada.[211]
O transporte de massa no México é modesto. A maior parte das necessidades de transporte doméstico de passageiros é servida por uma extensa rede de ônibus,[212] com várias dezenas de empresas de exploração por regiões. O transporte de passageiros entre as cidades é limitado.[212]
Um dos primeiros países latino-americanos a promover o desenvolvimento do sistema de transporte de ferrovia,[211] o México possui uma extensa rede ferroviária, com 30 952 km.[212] A maior parte da rede ferroviária é usada principalmente para transporte de mercadorias ou carga industrial, operada principalmente pela Ferrocarriles Nacionales de México (FNM), privatizada em 1997. Apesar de vasta, a rede ainda é considerada ineficiente para atender às demandas econômicas de transporte no país.[211]
Em 1999, o México tinha 1 806 aeroportos, dos quais 233 tinham pistas pavimentadas, sendo que, destes, 35 respondiam por 97% do tráfego de passageiros.[212] O Aeroporto Internacional da Cidade do México é o segundo maior da América Latina — atrás apenas do de Guarulhos, na região metropolitana de São Paulo — e o 44.º maior do mundo,[213] atendendo a cerca de 21 milhões de passageiros por ano.[214]
A cultura mexicana reflete a complexidade da história do país através da mistura das civilizações pré-hispânicas e da cultura da Espanha, transmitida durante a colonização de 300 anos da Espanha no México. Elementos culturais exógenos, principalmente dos Estados Unidos, foram incorporados à cultura mexicana.[50]
A era Porfiriana (el Porfiriato), no final do século XIX e primeira década do século XX, foi marcada pelo progresso econômico e pela paz. Após quatro décadas de conflito civil e guerra, o México assistiu ao desenvolvimento da filosofia e das artes, promovido pelo presidente Díaz. Desde aquele tempo, o que foi acentuado durante a Revolução Mexicana, a identidade cultural teve sua fundação na mestiçagem, cujo elemento é o núcleo indígena.[50]
À luz das diversas etnias que formaram o povo mexicano, José Vasconcelos, em sua publicação "La Raza Cósmica" ("A Raça Cósmica") (1925), definiu o México como um caldeirão de todas as raças (alargando assim a definição do mestiço), não apenas biologicamente mas culturalmente também.[215] Esta exaltação da mestiçagem era uma ideia revolucionária que contrastava fortemente com a ideia de uma raça superior pura predominante na Europa na época.[50]
A arte pós-revolucionária no México tinha a sua expressão nas obras de artistas renomados como Frida Kahlo, Diego Rivera, José Orozco, Rufino Tamayo, Federico Cantú Garza, David Siqueiros e Juan O'Gorman. Diego Rivera, a figura mais conhecida do muralismo mexicano, pintou o Man at the Crossroads no Rockefeller Center em New York City, um imenso mural que foi destruído no ano seguinte devido à inclusão de um retrato do líder comunista russo Lênin. Alguns dos murais de Rivera são exibidos no Palácio Nacional mexicano e no Palácio de Belas Artes.[50]
Compositores acadêmicos do México incluem Manuel María Ponce, José Pablo Moncayo, Julián Carrillo, Mario LaVista, Carlos Chávez, Silvestre Revueltas, Arturo Márquez e Juventino Rosas, muitos dos quais incorporaram aos seus trabalhos elementos de música tradicional.
A literatura do México iniciou-se antes da chegada dos colonizadores europeus, com a produção literária nos assentamentos indígenas da Mesoamérica. O poeta mexicano pré-colombiano mais conhecido é Nezahualcóyotl. A literatura moderna mexicana foi influenciada pelos conceitos da colonização espanhola da América Central. Escritores e poetas coloniais proeminentes incluem Juan Ruiz de Alarcón e Juana Inés de la Cruz.[50]
O poeta Octavio Paz recebeu o Nobel de Literatura em 1990. Outros escritores importantes são: Alfonso Reyes, José Joaquín Fernández de Lizardi, Ignacio Manuel Altamirano, Carlos Fuentes, Renato Leduc, Jaime Labastida, Mariano Azuela e Juan Rulfo. B. Traven escreveu "El tesoro de Sierra Madre", que foi adaptado para o cinema em 1948.[50]
Filmes mexicanos desde a Idade de Ouro entre 1940 e 1950 são os maiores exemplos de cinema latino-americano, com uma enorme indústria comparável à de Hollywood naqueles anos. Foram exportados filmes mexicanos e expostos em toda a América Latina e Europa. Maria Candelaria (1944), de Emilio Fernandez, foi um dos primeiros filmes no Festival de Cannes em 1946, na primeira vez em que o evento foi realizado após a Segunda Guerra Mundial. O famoso diretor espanhol Luis Buñuel nasceu no México. Atores e atrizes famosos deste período incluem María Félix, Pedro Infante, Dolores del Río, Jorge Negrete e os comediantes Cantinflas e Roberto Gomez Bolaños.[50]
Mais recentemente, filmes como Como Água para Chocolate (1992), Cronos (1993), Amores Brutos' (2000), Tu Y Mama También (2001), O Crime do Padre Amaro (2002), O Labirinto do Fauno (2006) e Babel (2006) têm sido bem-sucedidos na criação de histórias universais sobre temas contemporâneos, e foram reconhecidos internacionalmente, como no Festival de Cinema de Cannes. Os diretores mexicanos Alejandro González Iñárritu, Alfonso Cuarón, Guillermo del Toro e o roteirista Guillermo Arriaga são alguns dos mais conhecidos cineastas atuais.[50]
Em 2006, o México apresentou a candidatura de sua gastronomia como parte do Patrimônio Cultural da Humanidade pela UNESCO. Foi a primeira vez em que um país apresentou sua tradição gastronômica para tal posto. No entanto, o resultado foi negativo, porque, de acordo com a decisão, a comissão não colocou ênfase adequada sobre a importância do milho na culinária mexicana. Finalmente, em 16 de novembro de 2010, a culinária mexicana foi reconhecida pela UNESCO como Património Mundial, com o argumento de que a cozinha local manteve sua identidade intacta desde suas raízes pré-hispânicas. O título abrange desde os ingredientes clássicos, como milho, feijão, abóbora e pimentão, até os sabores atuais, influenciados pela colonização europeia.[216][217]
A cozinha mexicana atual foi estabelecida durante a colonização espanhola, numa mistura de pratos da Espanha com ingredientes indígenas nativos. De origem indígena, são usados na culinária mexicana o milho, pimenta, abóbora, abacate, batata-doce, peru e outras frutas e temperos.[218] Outros produtos indígenas são os muitos tipos de feijões. Da mesma forma, algumas técnicas de cozinha que são usadas hoje foram herdadas de povos pré-colombianos, como o processamento de milho, os fornos de cozimento de alimentos a-terra, moagem em almofariz e metate. Com os espanhóis vieram as carnes de porco, frango e vaca, pimenta-preta, açúcar, leite e todos os seus derivados, trigo e arroz, cítricos e uma constelação de ingredientes que fazem parte da dieta diária dos mexicanos.[218]
A partir desse encontro de duas culinárias com milênios de antiguidade, nasceu a barbacoa, o mole, pozole e tamal em sua forma atual, o chocolate, uma grande variedade de pães, tacos e o grande repertório de petiscos mexicanos. Também nasceram bebidas como atole, champurrado, chocolate de leite e as águas frescas; sobremesas como o acitrón (doce cristalizado de cacto) e toda uma gama de doces cristalizados; rompope (uma bebida alcoólica com gema de ovo), cajeta (um caramelo de leite de cabra), jericaya (doce típico mexicano com leite, ovos e outros ingredientes), bem como grande variedade de iguarias criadas nos conventos em todas as partes do país.[219][220]
Algumas bebidas mexicanas superaram suas fronteiras e são consumidas diariamente na América Central, Estados Unidos, Canadá, Espanha e Filipinas, como no caso da Água de Jamaica, horchata de arroz, a margarita e a tequila.[221]
A Cidade do México organizou os Jogos Olímpicos de Verão de 1968, tornando-se a primeira cidade latino-americana a receber o evento.[222] O país também recebeu a Copa do Mundo da FIFA duas vezes, em 1970 e 1986.[223] O esporte mais popular do México é o futebol. Com frequência, acredita-se que o futebol foi introduzido no México pelos mineiros córnicos no final do século XIX. Em 1902, uma liga de cinco equipes emergiu com uma forte influência britânica.[224] Os melhores clubes do México são o América, com 12 campeonatos, o Guadalajara, com 11, e o Toluca, com 10.[carece de fontes?] Antonio Carbajal foi o primeiro jogador a jogar em cinco Copas do Mundo[225] e Hugo Sánchez foi nomeado o melhor jogador da CONCACAF do século XX pela IFFHS.[226]
A liga de beisebol profissional do país é nomeada Liga Mexicana de Béisbol. Embora geralmente não seja tão forte como os Estados Unidos, os países do Caribe e o Japão, o México conseguiu vários títulos internacionais de beisebol. As equipes mexicanas ganharam a Série do Caribe nove vezes. O México teve vários jogadores contratados pelas equipes da Major League Baseball, sendo o mais famoso deles o artilheiro Fernando Valenzuela, dos Los Angeles Dodgers. Em 2013, a Seleção Mexicana de Basquetebol venceu a Copa América de Basquetebol Masculino e se classificou para o Campeonato Mundial de Basquetebol Masculino de 2014, onde alcançou os playoffs. Por causa dessas conquistas, o país obteve os direitos de hospedar a Copa América de Basquetebol Masculino de 2015.[227]
A tourada é um esporte popular no país e quase todas as grandes cidades têm praças de touros. A Plaza México, na Cidade do México, é a maior praça de touros do mundo, com capacidade para 55 mil pessoas. A lucha libre é um esporte popular com campeonatos nacionais, como AAA, CMLL e outros. O México é uma potência internacional no boxe profissional (no nível amador, várias medalhas olímpicas de boxe também foram conquistadas pelo México). Salvador Sánchez e Julio César Chávez são apenas alguns lutadores mexicanos que foram classificados entre os melhores de todos os tempos.[228]
Suíça (em alemão:  [die] Schweiz [ˈʃvaɪts]; em suíço-alemão: Schwyz ou Schwiiz [ˈʃʋit͡s]; em francês:  Suisse [sɥis(ə)]; em italiano:  Svizzera [ˈzvittsera]; em romanche:  Svizra [ˈʒviːtsrɐ] ou [ˈʒviːtsʁːɐ]), oficialmente Confederação Suíça (em alemão: Schweizerische Eidgenossenschaft; em francês: Confédération suisse; em italiano: Confederazione Svizzera; em romanche: Confederaziun svizra), é uma república federal composta por 26 estados, chamados de cantões, com a cidade de Berna como a sede das autoridades federais. O país está situado na Europa Central, fazendo fronteira com a Alemanha a norte, com a França a oeste, com Itália a sul e com a Áustria e o principado de Liechtenstein a leste.
A Suíça é um país sem costa marítima cujo território é dividido geograficamente entre o Jura, o Planalto Suíço e os Alpes, somando uma área de 41 285 km². A população suíça é de aproximadamente 8,5 milhões de habitantes e concentra-se principalmente no planalto, onde estão localizadas as maiores cidades do país. Entre elas estão as duas cidades globais e centros económicos de Zurique e Genebra. A Suíça é um dos países mais ricos do mundo relativamente ao PIB per capita calculado em 75 835 dólares americanos em 2011.[5] Zurique e Genebra foram classificadas como as cidades com melhor qualidade de vida no mundo, estando em segundo e terceiro lugar respectivamente[8] e a Suíça como o melhor país para nascer em 2013.[9]
A Confederação Suíça tem uma longa história de neutralidade, não estando em estado de guerra internacionalmente desde 1815. O país é sede de muitas organizações internacionais como o Fórum Económico Mundial, a Cruz Vermelha, a Organização Mundial do Comércio, a União Postal Universal, a Organização Internacional para Padronização e do segundo maior Escritório das Nações Unidas. Em nível europeu, foi um dos fundadores da Associação Europeia de Comércio Livre e é parte integrante do Acordo de Schengen. Em termos desportivos, o COI, a FIFA, a UEFA, a FIBA e a FIVB possuem as suas sedes localizadas no território suíço.
A Suíça é constituída por quatro principais regiões linguísticas e culturais: alemão, francês, italiano e romanche. Por conseguinte, os suíços não formam uma nação no sentido de uma identidade comum étnica ou linguística. O forte sentimento de pertencer ao país é fundado sobre o histórico comum, valores compartilhados (federalismo, democracia directa e neutralidade)[10] e pelo simbolismo Alpino.[11] A criação da Confederação Suíça é tradicionalmente datada em 1 de agosto de 1291.
Confœderatio Helvetica (CH) é a designação oficial em latim do país. O termo Helvética vem da palavra latina Helvetier que por sua vez provém do nome da antiga tribo celta dos Helvécios. A Revolução Suíça de 1798 foi a primeira contra a supremacia dos fundadores da Antiga Confederação Suíça: Uri, Schwyz, Unterwalden e as cidades de Lucerna, Zurique e Berna. As diferentes línguas faladas obrigaram a criar um nome único em latim, língua maioritariamente falada na Europa naquela época.[12] Nos tempos atuais, o termo Helvetia não é utilizado para caracterizar oficialmente a Suíça. Contudo, este nome pode ser encontrado em selos e moedas suíços. "C.H." (Confœderatio Helvetica) são as iniciais encontradas nos autocolantes para os carros e no domínio da Internet (.ch) desde 1995.[12]
O nome Confederação Suíça tornou-se conhecido apenas durante o século XVIII, quando não era nem oficial nem único, dado que designações como Corpo helvético, Magna Liga, Ligas e Helvetia eram também usadas para denominar a Suíça.[13] Atualmente, e segundo a carta das denominações de países da Suíça, o país é nomeado oficialmente como Confederação Suíça e explicita que deve ser evitado o uso de todas as palavras com prefixo helveto-.[13] Esta designação é utilizada pela primeira vez em alemão num documento datando da guerra dos Trinta Anos (1618-1648).[13] Porém, em latim a tradução continua sendo Confœderatio Helvetica.[14]
A história da Suíça começa antes do Império Romano: em 500 a.C. Nessa altura, muitas tribos celtas estavam localizadas nos territórios do Centro-Norte da Europa. A mais importante delas era a dos Helvécios, nome que iria originar a designação actual da Suíça.[15] Ao contrário do que era dito pelos Romanos e pelos Gregos, os Helvécios não eram selvagens mas sim avançados na técnica de joias e outras peças pequenas, corroborando as escavações feitas no Lago Neuchâtel. Em 58 a.C., os Helvécios tinham planeado descer para Sul, mas foram parados na batalha de Bibracte pelo Exército Romano sob o comando do general Júlio César e obrigados a recuar.[15][16]
Os Romanos controlaram o território suíço até cerca do ano 400. Foram criadas fronteiras e fortalezas a norte do Reno para conter as invasões bárbaras provenientes do norte da Europa.[18] Com o imperador Augusto (r. 27 a.C.–14 d.C.), os romanos conquistaram a parte Oeste da Alemanha e a Áustria. Muitas cidades suíças da atualidade foram fundadas durante esta era: Genibra (Genebra), Lausana, Octoduro (Martigny), Saloduro (Soleura), Turico (Zurique), Seduno (Sião), Basília (Basileia), Bilício (Bellinzona), entre outras.[18][19]
Depois da queda do Império Romano, o território foi invadido por tribos germânicas como os Burgúndios, Alamanos e Lombardos.[20] O período da Idade Média da Suíça foi um pouco confuso até à formação da antiga Suíça. No século VIII, os Burgúndios e os Alamanos entraram na coalizão dos Francos de Carlos Magno, que permitiu aos missionários católicos entrar nos territórios controlados pelos Alamanos. Com o Tratado de Verdun, o território suíço passou para as mãos de Lotário I, que incluía um assentamento burgúndio a oeste do rio Aar que depois formou um reino independente até 1033, quando integrou de novo o Sacro Império Romano-Germânico.[20][21]
1 de Agosto de 1291 é a data da formação da Confederação Helvética.[22] Esta data foi encontrada num documento que já foi autenticado através de uma análise radionuclear de Carbono-14.[23] Tudo começou com uma estrada chamada São Gotardo e três pequenos vales no centro do território suíço – Waldstätte – que ficaram esquecidos pelos duques e reis. Do século XI ao século XIII, muitas cidades foram fundadas, incluindo Berna, Lucerna e Friburgo.[24]
A partir de 1332, a confederação começou a crescer, acolhendo novos membros. Nesse ano, o cantão de Lucerna adere à união, enquanto os cantões de Zurique, Zug e Berna e Glarona entram em 1353, 1352 e 1353,[25] respectivamente criando a confederação de oito Estados-Membros. Mais tarde entrou a cidade, e não o cantão, de Appenzell em 1411 e depois São Galo em 1412.[26][27]
Passado algum tempo, tendo os suíços se envolvido em pequenas guerras contra os Borguinhões e em conquistas de outros pequenos territórios como Zurique, Schwyz e Glarona (resultados da Antiga Guerra de Zurique), surgiu a necessidade de aumentar a confederação. Porém, em 1477, Uri, Schwyz, Unterwalden, Zug e Glarona não concordavam com mais uma expansão. Um acordo levado por Niklaus von Flüe acalmou os ânimos dando assim entrada aos cantões de Friburgo e Soleura. De seguida, os cantões fizeram mais um acordo com o Cantão de Grisões, porém, sem Berna criando a confederação dos 13 Estados-membro.[28]
Através da Guerra dos Suabos, a Suíça juntou mais membros. Em 1501, as cidades de Basileia e Schaffhausen foram integradas na confederação, e em 1513 o Apenzell (antes de se dividir) integrou a confederação, dando assim treze cantões, que constituíam a antiga Confederação de 1291 durante muito tempo. Porém a independência formal só aconteceu em 1648.[28]
Depois da criação da Confederação em 1291, a Suíça atravessa um longo período de revoluções, guerra e invasões contra o Antigo Regime que viria a culminar com a revolução de 1782. Entre 1650 e 1790, vários são os conflitos que vão acontecendo na Suíça contra famílias ricas e que não tiveram sucesso. Muitos cantões entraram em guerra e conflitos para reclamar igualdade nos direitos. Apenas o Cantão de Genebra e a cidade de Togemburgo conseguiram restaurar algumas regras antigas. Porém, em 1782, as tropas napoleónicas e de Berna conseguiram instaurar a aristocracia em Genebra. Contudo, durante o século XVIII, cada vez mais as pessoas insistiam na unidade da Suíça, criando regras equivalentes entre os cidadãos. Em 1761, foi criada a Sociedade Helvética em Zurique. Juntavam-se todos os anos em Schinznach-Bad (Cantão de Argóvia) e discutiam a história e o futuro da Confederação.[29]
As revoltas que ocorriam entre o século XVII e o século XVIII mostraram que a revolução de 1798 não correu da mesma maneira que a Revolução Francesa. Enquanto que em França a revolução deveu-se ao abuso do poder da monarquia, na Suíça, a revolução de 1798 deveu-se mais à corrupção das casas ricas. De todas as maneiras, a Revolução Francesa de 1789 provou aos suíços que era possível uma revolução na confederação.[30]
Após a Tomada da Bastilha, muitos suíços em todo o país começaram a pôr em causa o sistema político em vigor através de petições como, por exemplo, em Unter-Hallau, Aarau e no cantão de Vaud. Outros acontecimentos são as celebrações da Revolução Francesa em Lausanne (1790), de onde inicia-se a Revolução de 1798. Em 1792, ocorre a Revolução de Genebra. Um ano depois são realizadas eleições que viriam a culminar com a celebração de uma nova Constituição em 1794. Nesse mesmo ano ocorre também a Revolução dos Grisons.[30]
A Revolução de Vaud em 1797 representou um papel fundamental para a criação da República Helvética. Frederico César de La Harpe perguntou à população se concordaria com uma intervenção francesa contra Berna. Quando Napoleão caminhava a caminho da Alemanha atravessando Genebra, em Vaud, foi acolhido em festa e as populações aproveitaram esta ocasião para lhe mostrar as suas convicções em Lyon. Apesar de tudo, Berna não queria negociar e enviou 5 mil soldados de expressão alemã à região. Os soldados de expressão francesa proclamaram a República de Léman, em que Léman significa o Lago de Genebra. O general francês Ménard aproveitou a vitória do conflito para declarar guerra a Berna. Os vaudeses marcharam para Berna e tomaram a cidade a 5 de Março de 1798.[30]
Cerca de 121 representantes dos cantões de Argóvia, Basileia, Berna, Friburgo, a República de Léman (Cantão de Vaud), Lucerna, Schaffhausen, Solothurn e Zurique juntaram-se em Argóvia a 12 de Abril de 1798 para proclamar a República Helvética e confirmar uma nova constituição.[30]
A França anexou os cantões de Genebra, Neuchâtel, Bienna e o território do Bispo de Basileia, actual Jura. A nova constituição promulgada era muito semelhante à francesa, com um parlamento (duas câmaras), um governo legislador e um tribunal supremo de justiça. A tradição federalista da Antiga Confederação de 1291 fora eliminada.[31]
Apesar da criação de uma nova república, a República Helvética viria a cair devido a diversos factores. De um lado, os representantes do antigo sistema não falharam em atacar as novas ordens, criticando e ridicularizando-as. Por outro lado, as guerras sucessivas foram esgotando as reservas da confederação. O sistema político de centro não foi bem recebido pela população e os camponeses, apesar de favoráveis à República, queriam estatutos iguais.[30]
A França tornara-se uma ditadura sob o comando de Napoleão e a República Helvética teve quatro tentativas de golpe de estado. Em razão da grande confusão e da instabilidade, alguns cantões restauraram o estatuto cantonal (Schwyz, Nidwalden, Obwalden, Appenzell, Glarona e Grisões). A cidade de Zurique criava uma forte oposição sobre a República dificultando as tarefas desta. A República viria a cair em 1802 com uma guerra civil entre os republicanos e apoiantes do antigo regime.[30]
Napoleão, ao ver o estado na República Helvética, ordenou aos seus soldados para invadir e desarmar os rebeldes e aí, percebeu que o sistema de República nunca seria viável. Foi aí que se criou um acto de mediação entre várias partes a ser respeitado pelos cidadãos, que durou de 1803 a 1815. A Suíça ganhou seis novos cantões: São Galo, Grisões, Argóvia, Turgóvia, Tessino e Vaud.[30]
Após a derrota de Napoleão na Batalha de Waterloo, a Suíça regressou ao sistema federal. Os seis cantões que tinham entrado durante o acto de mediação receberam o estatuto de estados livres (cantão) em vez de membros associados. Valais, Neuchâtel e Genebra voltaram a entrar para a confederação após terem sido anexados pela França. A Suíça passava a ter 22 cantões com as fronteiras iguais às da actualidade.[32]
Durante o século XIX, a Confederação Helvética vai progredindo para se tornar numa democracia moderna. Quando em 1815 o Antigo Regime foi restaurado, nem todos os republicanos desistiram. Muitos ainda reivindicavam a liberdade de circulação e direitos iguais entre classes sociais, entre outras exigências. Dadas essas, o cantão de Basileia dividiu-se em dois: Basileia-Cidade e Basileia-Campo. Outros políticos defendiam a criação de uma federação semelhante à dos Estados Unidos, com um parlamento federal. Depois de uma breve guerra civil em 1847, criou-se a Constituição Federal de 1848. À semelhança do sistema norte-americano, a Suíça adoptou a Declaração dos Direitos Humanos, duas câmaras parlamentares – o senado e a câmara federal –, o governo federal e um tribunal de Justiça Suprema. A nova constituição foi aceite por 15 cantões e meio (dado que apenas Basileia-Campo tinha aceite). Berna foi designada a capital federal. Porém, só em 1874 é que a constituição foi totalmente revista.[33] O país também se desenvolve no sector da indústria. A Suíça foi um dos primeiros países a implementar este ramo na sua economia e viria a crescer sobretudo depois da Revolução Industrial de 1850.[30]
A indústria têxtil foi um dos primeiros sectores do país a ser desenvolvido. Em 1801, a Suíça começa a produzir têxteis nas máquinas de terceira geração importadas do Reino Unido em São Galo. Mas, diferente dos outros países que usavam energia a vapor (a partir do carvão), os suíços usavam sobretudo as potencialidades da energia hidroeléctrica. Em 1818, as máquinas substituíram praticamente todo o trabalho manual em todo o território. Porém, com o Bloqueio Continental provocado por Napoleão, os suíços viram a possibilidade de importar máquinas proibidas. Por isso, muitas empresas começaram a construir elas próprias as suas máquinas.[30]
O sector industrial é uma das marcas mais importantes dos séculos XVIII e XIX pois serviu de impulso para a economia helvética. Mas a grande expansão de empresas criara um efeito de capitalismo sem ordem pelo que foi necessário criar regras para conter esses problemas. Também no século XIX a Suíça faz-se de exemplo ao Mundo ao criar regras laborais tais como em 1815 em Zurique que defendia que as horas diárias não excederiam as 12, nunca começando antes das cinco da manhã. As crianças com menos de dez não deviam trabalhar. Em 1815 o cantão de Turgau afirma que nenhuma criança pode trabalhar. Em 1877 uma lei federal nasce afirmando que as horas diárias passariam a ser 11 e não haveria período laboral à noite e aos Domingos. As crianças com menos de 14 anos estavam proibidas de trabalhar.[35]
Apesar do país ter sido rodeado por várias potências em guerra (a França, a Alemanha, o Império Austro-Húngaro - até 1918 - e a Itália), a Suíça nunca foi invadida em nenhuma das duas Grandes Guerras. A sua neutralidade foi posta em causa com o Escândalo Grimm-Hoffmann,[36] em 1917, ao qual não foi dada muita relevância. O Estado adere à Liga das Nações em 1920 e ao Conselho Europeu, em 1963.[37] A invasão da Suíça foi várias vezes planeada pelos alemães, mas nunca foi atacada.[38] Conseguiu manter a paz com a Alemanha através de concessões económicas e militares. A imprensa suíça e o Governo Federal opuseram-se às políticas do Terceiro Reich que criou, com a Itália Fascista, um plano (nunca executado) de invasão denominado Operação Tannenbaum. O general Henri Guisan foi incumbido de organizar as defesas do país, que possuía um plano defensivo conhecido como Reduto nacional. Dada a sua localização geográfica, a Suíça era um local de espionagem constante por parte das duas facções (os Aliados e o Eixo). Durante a Segunda Guerra Mundial, a Suíça recebeu 300 mil refugiados dos quais 104 mil eram militares estrangeiros. 60 mil eram civis que fugiam das políticas nazis. Daqueles, entre 26 mil a 27 mil eram judeus. No entanto, as políticas imigratórias e de asilo eram restritas o que causou muitas controvérsias.[39]
Apesar de tudo, a Suíça foi uma voz de liberdade para a Europa Ocidental. A defesa "espiritual" fez com que a Suíça fosse uma marca de referência para os direitos do Homem e assegurou, fortalecendo a sua independência.[40]
Em 1958, a Suíça garante o direito ao voto às mulheres, primeiro em nível cantonal (no Valais), e depois em nível federal em 1971.[41] O último cantão a reconhecer o direito às mulheres de votar foi Appenzell Interior em 1990. Esse reconhecimento permitiu um grande crescimento de participações das mulheres na vida política. Em 1978 ocorre a criação do último cantão helvético, o Jura, após a separação de vários territórios ao longo do cantão de Berna. O Jura integrou oficialmente a Confederação Helvética em 1979.[42] A 18 de Abril de 1999, um referendo nacional propôs a revisão total da Constituição Helvética.[30]
Em 2002, a Suíça integra totalmente as Nações Unidas,[43] deixando o Vaticano como o único Estado sem integração completa no organismo. A Confederação Helvética é um Estado-membro da Associação Europeia de Livre Comércio mas não é membro do Espaço Económico Europeu.[44]
Em 1992, a Suíça pediu a adesão à União Europeia, mas fora banida pela EEE em Dezembro de 1992, dado que um referendo interno era necessário para obter aprovação por parte da população helvética.[45] Vários referendos foram-se realizando e devidas às opiniões da população, as negociações com a União Europeia foram congeladas. No entanto, as leis suíças estão gradualmente a adaptar-se às normas da UE e o Governo Federal assinou vários acordos bilaterais com o bloco económico. O país, juntamente com o Liechtenstein, tem vindo a estar rodeada pelos países-membros da UE até 1995, com a adesão da Áustria. Em 5 de Junho de 2005, a população suíça aprovou a integração ao Espaço Schengen.[46]
A Suíça é um país localizado no centro da Europa de coordenadas 47,00 N e 8,00 E. A sua área total é de 41 285 km² em que 1 520 km² são cobertos de água. Faz fronteira com a França a Oeste, a Alemanha a Norte, a Áustria e o Liechtenstein a Leste e com a Itália a Sul. De uma maneira geral, pode-se dividir a Suíça em três regiões geográficas: os Alpes, o planalto e o Jura.[30]
O planalto suíço, denominado como plateau, ocupa um terço da área do país e aí mora cerca de dois terços da população naquela área geográfica. A densidade populacional é de cerca de 450 pessoas por quilómetro quadrado. Vai desde o Lago Leman na fronteira francesa, atravessando o centro da Suíça e terminando no Lago de Constança, nas fronteiras com a Alemanha e a Áustria. Tem uma altitude média de 580 metros.[47] O plateau suíço é atravessado por três grandes rios: o Ródano, o Reno e o Aar. O Sul e o centro-Sul da Suíça são dominados pela cadeia montanhosa alpina enquanto que o restante é uma zona plana exceptuando-se um faixa ao longo da região Noroeste que é dominada pelo Jura.[30]
Os Alpes Suíços fazem parte de uma cadeia montanhosa que atravessa desde o Sul da Europa até à Europa Central. Algumas das mais importantes passagens estão localizadas nos Alpes suíço. Têm uma altitude média de 1 700 metros e cobre dois terços da totalidade da área da Suíça. Entre os alpes suíços estão 48 montanhas que têm pelo menos 4 mil metros de altitude.[48]
O Jura é uma linha de rocha calcárica que se estende desde o Lago Leman até o rio Reno no Norte, ocupando cerca de uma décima parte da área da Suíça, 12%. Estão presentes naquelas rochas abundantes fósseis de origem jurássica.[49] A densidade populacional da Suíça é elevada: 170 habitantes por quilómetro quadrado. Porém, é na zona central que se concentra a maior parte da população cuja densidade chega aos 500 habitantes por quilómetro quadrado.[50]
Dada a grande diferença de altitudes, que variam dos 195 metros até mais de 4 mil metros, a Suíça apresenta uma grande diversidade de climas e dos respectivos animais e plantas. Na zona Sul, nomeadamente no cantão de Ticino, pode-se verificar um clima mediterrânico, enquanto que no topo das montanhas está sempre presente uma camada de neve. A grande discrepância de altitudes também permite constatar uma diversidade nos minerais.[30]
Nos cantões mais a Sul (Valais e Ticino) podem-se encontrar eucaliptos e pinheiros. O tempo temperado permite a evolução de muitas plantas e também de zonas vinhateiras, entre outras espécies. À medida que aumenta a altitude, a densidade vegetacional diminui, pois o ar torna-se mais frio, dificultando a evolução de espécies. Os glaciares formam a paisagem a altas altitudes. A Suíça é um dos países com mais glaciares por área.[30] Em relação à fauna, os ursos e os lobos estiveram extintos durante um século. Porém, o lobo reapareceu no território nas últimas décadas provindo da Itália. O íbex, o chamois e a marmota são espécies muito frequentes nos Alpes, bem como uma grande quantidade de espécies voadoras em todo o território, como pombos, corvos e gaivotas.[51]
O clima na Suíça é temperado apresentando uma grande amplitude entre Verões amenos e Invernos rigorosos. Abaixo da cordilheira dos Alpes, o tempo é mais quente do que no Norte. Em termos climáticos pode-se dividir a Suíça em quatro regiões: extremo Sul, os Alpes, o maciço central e o Jura.[52][53] As temperaturas variam entre temperaturas negativas nas zonas montanhosas e no Inverno e temperaturas amenas durante o Verão pois na época do Estio, o país é enfrentado por um anticiclone enquanto que no Inverno, existe uma frente fria proveniente da Sibéria causando abruptas quedas na temperatura, sobretudo durante a noite.[30]
O tempo na Suíça varia bastante de lugar para lugar. O local com maior precipitação é Rochers de Nave, perto de Montreux com cerca de 260 cm por ano. A precipitação é geralmente mais elevada na parte Oeste do país onde se formam nuvens de origem atlântica. A parte Sul tem também altas precipitações devido aos efeitos de barramento das nuvens nos Alpes: em Lugano a precipitação chega aos 175 cm. Uma das características mais vincadas no clima suíço é o vento forte e quente que se forma no Sul, designado Vento Föhn. A insolação na Suíça é de aproximadamente 1 700 horas por ano, havendo picos em lugares no Valais onde o Sol pode durar cerca de 2 300 horas por ano.[54]
O aquecimento global é particularmente difícil para a Suíça. Isto é devido ao clima continental e à localização nas latitudes médias.[56] Entre o início dos registros meteorológicos em 1864 e 2019, tornou-se uma média de 1,9 °C mais quente na Suíça.[56] Como resultado, as temperaturas na Suíça subiram duas vezes mais rápido que a média global.[56] O aquecimento acelerou nos últimos 30 anos.[56] Todos os anos entre 1991 e 2019 eram mais quentes que a média dos anos de 1961 a 1990.[56] Das dez temperaturas médias mais quentes de junho desde o início dos registros climáticos, sete foram medidas após 2002.[56] Em 1890, Davos ainda tinha 231 dias de geada (= número de dias abaixo de 0 °C); em 2018, havia apenas 161 dias de geada em Davos.[56] A área das geleiras suíças quase caiu pela metade entre 1850 (1 621 quilômetros quadrados) e 2019 (944 quilômetros quadrados).[57] Estudos científicos concluem que, por volta de 2050, os esportes de inverno não serão mais possíveis na Suíça se a meta de dois graus do Acordo de Paris não for cumprida.[58]
Em 2018, o número de habitantes da Suíça era estimada em mais de 8,5 milhões de pessoas, sendo que 23% destes são estrangeiros (dos quais, dentro deste percentual, 64% nasceram em nações da União Europeia).[4] Entre dois terços e três quartos da população vivem em áreas urbanas.[59][60] A Suíça passou de um país predominantemente rural para um urbano em apenas 70 anos. Desde 1935 o desenvolvimento urbano tem reivindicado grande parte da paisagem suíça, como o fez durante os 2000 anos anteriores. Esta expansão urbana não afeta apenas o planalto, mas também a região de Jura e o sopé dos Alpes[61] e há preocupações crescentes sobre o uso da terra.[62] No entanto, desde o início do século XXI, o crescimento da população em áreas urbanas é maior do que no campo.[60]
A Suíça tem uma densa rede urbana, onde grandes, médias e pequenas cidades são complementares.[60] O planalto é muito densamente povoado, com cerca de 450 pessoas por quilômetro quadrado e a paisagem mostra continuamente sinais da presença do homem.[63] O peso da maiores áreas metropolitanas, que são Zurique, Genebra-Lausanne, Basileia e Berna tende a aumentar.[60] Em comparação internacional, a importância dessas áreas urbanas é mais forte do que o seu número de habitantes sugere.[60] Além disso, os dois principais centros do país, Zurique e Genebra, são reconhecidos por sua qualidade de vida particularmente alta.[64]
A Suíça tem oficialmente quatro línguas: o alemão, o francês, o italiano e o romanche falados, respectivamente, em 63,7%, 20,4%, 6,5% e 0,5% do território. Esta diversidade linguística deve-se à vizinhança da Suíça: a Itália, de expressão italiana, a Alemanha, o Liechtenstein e a Áustria, de expressão alemã e, por fim, a França, de expressão francesa.[66] Esta divisão por línguas dá na realidade uma divisão de facto em Suíça alemã, Suíça romanda, Suíça italiana e Romanche.
As pessoas nas áreas fronteiriças entre duas línguas vão crescendo a aprender ambas as línguas, tornando-se bilíngues. Este fenómeno pode ser internacional, entre a Suíça e outro país ou interno, entre duas áreas de línguas diferentes que podem ser entre dois cantões ou mesmo dentro de um cantão. No caso do Cantão do Valais, os habitantes de Sierre são bilíngues onde o alemão e o francês se encontram. A fronteira ideológica entre a população de expressão francesa e de expressão alemã é coloquialmente conhecida como Röstigraben. A separação não fica somente pela língua mas também pelas ideologias e culturas, onde os falantes franceses são mais abertos e liberais e os falantes alemãs são mais conservadores. Embora o alemão seja língua oficial de-jure, na realidade o mais falado nesta área é o alemão-suíço (Schweizerdeutsch). Trata-se de uma variação bastante diferente do alemão quer na pronúncia, quer na escrita. O alemão-suíço é um dialeto, portanto falado somente por certas comunidades, há diversos dialetos espalhados em toda a Suíça, havendo por vezes mais de um dialeto por cantão. O romanche é falado por cerca de 50 mil pessoas, das quais 35 mil o utilizam como língua materna. A maior parte de seus falantes vive no leste do país, e o idioma se divide em três variações, cada qual com sua própria gramática, literatura e dicionários; tentativas de se padronizar o romanche não foram bem sucedidas até o momento.[67]
O inglês é usado como língua franca pela população ao redor do país.[68][69] O governo suíço publica muitos comunicados e documentos na língua inglesa, apesar do idioma não ser um dos oficiais.[68][69] Por conta das diferenças entre o alemão padrão, aprendido nas escolas e usado por parte dos suíços francófonos e italófonos, e o alemão-suíço usados por parte dos suíços germanófonos, os suíços preferem usar o inglês entre si quando não se sentem confortáveis em manter uma conversa em algum dos idiomas suíços dos quais não estão habituados a usar em suas regiões.[68][69] Os que mais fazem uso da língua inglesa são os suíços de fala alemã.[68][69] A televisão suíça faz uso da língua inglesa entre suíços em algumas ocasiões específicas.[68][69]
A imigração de grandes grupos estrangeiros, sobretudo portugueses, espanhóis, italianos, sérvios e albaneses permitiram a penetração de várias línguas estrangeiras como o português (3,5%), o albanês (3,3%), o espanhol (2,3%), o sérvio e, recentemente o turco, entre outras.[70][71]
A Suíça não tem religião oficial, embora a maioria dos cantões (exceto Genebra e Neuchâtel) reconheça igrejas oficiais, que são a Igreja Católica Romana ou a Igreja Reformada Suíça. Essas igrejas, e em alguns cantões também a Velha Igreja Católica e as congregações judaicas, são financiadas por impostos oficiais dos adeptos.[72]
O cristianismo é a religião predominante na Suíça (cerca de 68% da população residente em 2016[73] e 75% dos cidadãos suíços[74]), dividida entre a Igreja Católica Romana (37,2% da população), a Igreja Reformada Suíça (25,0%), outras igrejas protestantes (2,2%), a Ortodoxia Oriental (cerca de 2%) e outras denominações cristãs (1,3%).[73] A imigração estabeleceu o Islã (5,1%) como uma religião minoritária considerável.[73] 24% dos residentes permanentes suíços não são afiliados a nenhuma igreja (ateísmo, agnosticismo e outros).[73]
No censo de 2000, outras comunidades minoritárias cristãs incluíam o neo-pietismo (0,44%), o pentecostalismo (0,28%, incorporado principalmente na missão Schweizer Pfingstmission), o metodismo (0,13%), a Igreja Nova Apostólica (0,45%), as Testemunhas de Jeová (0,28%), outras denominações protestantes (0,20%), a Velha Igreja Católica (0,18%), outras denominações cristãs (0,20%). As religiões não-cristãs são hinduísmo (0,38%), budismo (0,29%), judaísmo (0,25%) e outras (0,11%); 4,3% não fizeram uma declaração.[75]
O país era historicamente equilibrado entre católicos e protestantes, com uma complexa colcha de retalhos de maiorias na maior parte do país. A Suíça desempenhou um papel excepcional durante a Reforma Protestante, pois se tornou o lar de muitos reformadores. Genebra se converteu ao protestantismo em 1536, pouco antes de João Calvino chegar lá. Em 1541, ele fundou a República de Genebra em seus próprios ideais. Tornou-se conhecida internacionalmente como a "Roma protestante" e abrigou reformadores como Theodore Beza, William Farel ou Pierre Viret. Zurique se tornou outra fortaleza na mesma época, com Huldrych Zwingli e Heinrich Bullinger na liderança. Os anabatistas Felix Manz e Conrad Grebel também operavam lá. Mais tarde, eles se juntaram aos fugitivos Pietro Martire Vermigli e Hans Denck. Outros centros incluem Basileia (Andreas Karlstadt e Johannes Oecolampadius), Berna (Berchtold Haller e Niklaus Manuel) e São Galo (Joachim Vadian). Um cantão, Appenzell, foi oficialmente dividido em seções católicas e protestantes em 1597. As cidades maiores e seus cantões (Berna, Genebra, Lausana, Zurique e Basileia) costumavam ser predominantemente protestantes. A Suíça Central, o Valais, o Ticino, o Appenzell Innerrhodes, o Jura e o Fribourg são tradicionalmente católicos. A Constituição Federal da Suíça de 1848, sob a recente impressão dos confrontos dos cantões católicos x protestantes que culminaram no Sonderbundskrieg, que define conscientemente um estado com o social, permitindo a coexistência pacífica de católicos e protestantes. Uma iniciativa de 1980 que pedia a separação completa da igreja e do estado foi rejeitada por 78,9% dos eleitores.[76] Atualmente, alguns cantões e cidades tradicionalmente protestantes têm uma pequena maioria católica, não porque eles cresceram em quantidade de membros, pelo contrário, mas apenas porque desde 1970 uma minoria em constante crescimento não se tornou afiliada a nenhuma igreja ou outro órgão religioso (21,4% na Suíça, 2012) especialmente em regiões tradicionalmente protestantes, como Basileia (42%), cantão de Neuchâtel (38%), cantão de Genebra (35%), cantão de Vaud (26%) ou cidade de Zurique (cidade: 25%; cantão: 23%).[77]
A Suíça é um Estado Federal desde 1848. A nova Constituição de 1999 não influenciou nenhuma mudança notória no sistema político helvético. Garante a soberania de cada cantão e a separação entre poderes federais e cantonais. Defende a aplicação total dos Direitos Humanos, da dignidade humana[78] e proíbe a pena de morte.[79]
A Suíça, apesar de ser um estado de pouca extensão, possui um sistema político bastante complexo, semelhante ao dos sistemas federais dos restantes países do mundo. A Constituição Federal da Suíça define o país como um Estado Federal composto por 26 cantões em que cada cantão tem a sua autonomia político-económica. A hierarquia política da Suíça é constituída da seguinte maneira: em primeiro lugar está o sistema federal; em segundo, o cantonal; em terceiro, o sistema comunal. O sistema federal (ou Governo Central) vela pelas relações políticas com o exterior, a economia nacional, as Forças Armadas, os estatutos sobre as medidas internacionais como o peso e a altura, os caminhos-de-ferro (conhecidos como "Chemins-de-Fer Fédéraux", CFF), entre outros. Já o poder cantonal tem a sua própria polícia, tem o seu sistema de saúde e educação e até tem estatuto oficial perante a religião. Por exemplo, no cantão de Valais a religião oficial é a Católica enquanto que, no cantão de Vaud, a religião é o Protestantismo. O Governo da Suíça é constituído por um Conselho Federal, que representa o poder executivo, eleito indirectamente pelas duas assembleias reunidas: o Conselho Nacional e o Conselho dos Estados, que reunidas formam o Parlamento suíço, a Assembleia Federal (Die Bundesversammlung, L' Assemblée fédérale, Assemblea federale). As duas casas do parlamento discutem as leis formadas, mas separadamente. Quando não se entendem, as leis têm que ser alteradas.[80]
O Conselho Nacional (semelhante ao Parlamento Português) é constituído por duzentos lugares. Os seus deputados são eleitos por um período de quatro anos através de um complexo sistema de representação proporcional. O Conselho dos Estados (semelhante ao senado norte-americano) é constituído por 46 lugares. Os cantões enviam dois representantes e os semicantões enviam um. Os representantes de cada cantão são eleitos pela população do respectivo cantão e o sistema de voto difere de um cantão para outro, visto que cada um deles tem as suas próprias regras.[81]
O Conselho Federal, Governo Federal ou Administração Federal (Conseil Fédéral, Bundesrat, Consiglio Federale, Cussegl federal, em francês, alemão, italiano e romanche, respectivamente) é constituído por sete lugares. Cada membro tem direitos equivalentes e representa uma parte do Conselho Federal. Cada membro pode ser reeleito e não existe um limite máximo de mandatos. As pastas que constituem o Conselho são: Assuntos Internos; Negócios Estrangeiros; Energia, Tráfego Ambiente e Comunicações; Economia; Finanças; Justiça e Política; e Defesa, Protecção e Desporto.[82] Todas as decisões federais, ou a aplicar a nível federal, são tomadas no Palácio Federal em Berna. Por fim, o Tribunal Federal é a organização primária do sistema judicial na Suíça, sediado em Lausanne. A sua função é proteger as leis suíças contra posições arbitrárias que possam prejudicar os cidadãos e seus direitos.[83]
A Suíça não tem um presidente de Estado como, por exemplo, em Portugal e no Brasil, o Presidente da República, mas sim um representante da Confederação Helvética que exerce as funções de Presidente da nação. O Presidente da Confederação Helvética é eleito pelos sete conselheiros federais por um período de um ano.[84] Neste sistema colegial, o presidente, com estatuto primus inter pares (ou seja, primeiro entre iguais) é um membro do conselho federal sem mais poderes do que os outros, mas tem a última palavra em caso de empate nas diversas votações que possam ocorrer quer no Conselho Federal ou nas duas alas políticas. Acessoriamente ele representa a nação, tanto a nível internacional como nacional.[85]
Para que toda a população possa participar na vida política, a Suíça tem um sistema único no Mundo de democracia direta. É muito frequente a realização de referendos, quer a nível federal, quer a nível cantonal. Além do mais, os resultados de um referendo federal não implicam a obediência de uma lei referendada por um cantão que tenha votado contra. Por exemplo se um cantão votar contra uma lei e em todos os outros cantões fora aceite, essa lei não entra no cantão que tenha votado efectivamente contra. Já em relação aos assuntos externos, é necessário haver a aprovação de todos os cantões como no caso da adesão da Suíça à União Europeia. Para a realização de um referendo nacional com o objectivo de alterar uma lei na Constituição Federal, é necessário que haja cem mil assinaturas[86] a pedir salvo se for um referendo pedido pelo Governo Federal ou por cada um dos cantões.[87]
As relações externas da Suíça são diversas e são da responsabilidade direta do Departamento Federal dos Assuntos Externos.[88] São objectivos da Constituição e do Departamento, a paz entre as nações, o respeito pelos direitos humanos, a democracia e a Lei; promover a economia suíça no mundo, bem como a preservação dos recursos naturais. O país foi dos últimos a integrar totalmente as Nações Unidas, a 10 de Setembro de 2002, após um referendo, realizado seis meses após outro, onde na altura a integração da Suíça na ONU fora rejeitada numa proporção de 3 votos contra 1 voto a favor.[89] Em 1996, a Suíça integra a OTAN.[90]
Existem várias situações de conflitos diplomáticos entre a Suíça e o exterior. Ultimamente, a Suíça tem vindo a ter vários conflitos diplomáticos com a Líbia que começaram em Julho de 2008 quando da detenção do filho do presidente líbio. Hannibal Kadhafi e a sua esposa eram acusados de maltratar uma empregada e foram detidos pelas autoridades suíças. A Líbia ameaçou várias vezes a Suíça de corte de fornecimento de petróleo se o país não libertasse Hanninal Kadhafi e pedisse desculpa pelo sucedido. Após várias resistências, as autoridades libertaram-no e o Presidente do Conselho pediu desculpas em público à frente do Presidente Kadhafi em Tripoli. A imprensa suíça viu isso como uma humilhação por parte da Confederação.[91]
O país é membro da Associação Europeia de Livre Comércio, mas não faz parte da União Europeia, apesar de todos países à sua volta serem integrantes do bloco europeu, exceptuando-se o Liechtenstein. A Suíça negociou a integração no Espaço Económico Europeu (EEE) e, em 20 de maio de 1992, assinou o acordo para a adesão do país à União Europeia. No entanto, um referendo nacional, realizado a 6 de dezembro do mesmo ano, rejeitou a integração do país ao EEE por apenas 50,3% dos votos.[92] Como consequência, as negociações com a União Europeia, na altura, Comunidade Económica Europeia, foram congeladas.[93] A sua adesão permanece em aberto e é vontade do Governo Federal a integração total da Suíça na UE.[94]
Ao longo da década de 1990, vários tratados foram assinados entre a UE e a Suíça. Vários acordos bilaterais estão em prática entre o bloco europeu e o país, no tocante a livre circulação de pessoas, tráfego aéreo e terrestre, agricultura, ciência, segurança, cooperação em casos de fraudes, entre outros. Porém, os acordos da Suíça com a UE estão sujeito à chamada cláusula da guilhotina em que, se num referendo um tratado for rejeitado, os acordo bilaterais I de 1999 são automaticamente anulados.[95] Em 5 de junho de 2005, um referendo aprovou por maioria a integração do país no Espaço Schengen para a livre circulação de pessoas, o que foi confirmado por outro referendo, a 12 de dezembro de 2008.[96][97]
As forças armadas helvéticas têm estatuto de milícia, com formação militar obrigatória, além da sua formação académica. Em 2006, o orçamento para a defesa do país rondou os 4,5 bilhões de francos (3,105 bilhões de euros),[98] cerca de 1% do PIB suíço, no mesmo ano.
Equipadas de material sofisticado e moderno, as forças armadas da Suíça têm como funções a defesa do território e da soberania helvética e a contribuição para a paz mundial através de missões ao estrangeiro, quer através da OTAN, quer através da ONU.[99]
Durante os tempos de paz, as forças armadas são comandadas pelo chefe da armada que depende do conselheiro federal da Defesa, Protecção da população e Desporto (DDPS) e do conselho federal por inteiro. Em tempos de guerra, a assembleia federal elege o general da armada perante os comandantes. Desde 1848, apenas quatro homens ocuparam esse lugar.[99]
A Guarda Suíça que acompanha o papa foi criada no século XVI pela armada suíça e era constituída por mercenários que lutavam entre as potências. Hoje servem unicamente o papa, cabendo a ele eleger o superior. Os guardas devem ter tido uma formação no exército suíço durante, pelo menos, dois anos.[100]
A Suíça, à semelhança de outros países federais, é constituída por 26 estados autónomos em que cada um deles tem autonomia própria. Segundo a constituição federal, estes estados são designados cantões e são independentes e soberanos. Os cantões (oficialmente designados como membros da Federação Helvética) podem criar as suas leis de modo a que essas leis não interfiram nos outros cantões. Tal como a nível nacional, cada cantão tem a sua constituição, as suas regras, os seus estatutos, etc. Todos os cantões têm o seu parlamento o que reforça as suas autonomias. Como cada cantão é diferente a nível – e não só – territorial, populacional e económico, é normal que as regras divirjam de um para outro.[101]
Num referendo realizado em 1919, o estado austríaco de Vorarlberg aprovou a sua junção à confederação helvética, com 80% de afirmações positivas. No entanto, o governo austríaco não acatou com os resultados. Os suíços italianos e francófonos, bem como os suíços librais rejeitaram também essa junção.[102] Os cantões estão divididos em 2 636 municípios (ou comunas).[103]
Cerca de dois terços do território suíço é coberto de florestas, montanhas e lagos. Dado que o país não possui recursos minerais, tem de importar, processar e colocar à venda as necessidades da população. Dos três sectores que compõem a actividade económica suíça, o terciário é o mais importante no qual se incluem a banca, as seguradoras e o turismo. No século XIX, vários cantões adotavam um sistema bancário livre, que permitia livre entrada, circulação e emissão de moeda privada e, que perdurou até a adoção de uma nova regulação bancária em 1881.[106][107] A agricultura também é importante, porém, não satisfaz às necessidades totais da população, sendo obrigado a importar.[108]
Atualmente, é a 23ª economia do mundo, com um PIB estimado de 492,6 bilhões de dólares americanos para o ano de 2008 (pela taxa de câmbio oficial) ou 309,9 bilhões de dólares (pela paridade de poder de compra).[109] O PIB per capita (estimado em 40 900 dólares americanos para 2008) é um dos maiores do mundo, enquanto a taxa de desemprego é uma das mais baixas. Dado que está rodeada de países-membro da União Europeia, a economia tem vindo gradualmente a adaptar-se às políticas do bloco económico, de modo a aumentar a sua competitividade internacional, apesar de haver ainda algum proteccionismo, sobretudo na agricultura.[109]
Importante centro financeiro internacional, a Suíça é também a primeira praça financeira para a gestão de fortunas e continua a ser um porto seguro para os investidores, em razão do grau de sigilo bancário vigente no país e da histórica estabilidade da moeda local. Apesar das exigências da legislação, as regras do sigilo persistem e os não residentes estão autorizados a realizar negócios através de diversos intermediários e entidades offshore. Segundo o The World Factbook da CIA, aí ocorrem importantes etapas do processo internacional de lavagem de dinheiro.[110] Em 2009, a Suíça foi incluída na 'lista cinza' de 38 paraísos fiscais, pela OCDE.[111]
Durante a última década, o sigilo bancário suíço tem sido várias vezes posto em causa, quer interna, quer externamente. O sigilo, previsto na lei suíça pelo artigo 47.º da Constituição,[112] tem permitido a defesa da divulgação de dados que são normalmente pedidos pelas autoridades internacionais contra a evasão fiscal. Esse estatuto perante a banca tem permitido a chegada de novos capitais ao país, bem como a estabilização do franco suíço, sobretudo depois da chegada do Euro.[113]
Após a eclosão da crise econômica de 2007–2008, foi pedida várias vezes à Suíça o levantamento do artigo 47.º pelas autoridades internacionais, de modo a poder investigar crimes fiscais ocorridos em vários países, cujos capitais terão sido depositados na Suíça.[114] As dificuldades para tal, fizeram com que vários países, nomeadamente a Alemanha, a França e os Estados Unidos, colocassem o país na lista negra dos paraísos fiscais, que viria a acontecer em Março de 2009 – a OCDE coloca a Suíça na lista negra.[115] A crise dos subprimes, provocada pela falência do banco Lehman Brothers, atingiu fortemente o setor bancário suíço em 2008. A Union de Banques Suisses perdeu cerca de trezentos milhões de francos (cerca de 207 milhões de euros) durante a crise.[116]
Apesar da forte queda das ações do sistema bancário, a economia suíça cresceu 2% em 2008.[117] O Escritório Federal da Estatística (OFS) para poder estudar o desenvolvimento do país decidiu criar territórios macro-económicos que permitam efectuar comparações entre eles. O estudo consiste numa partilha do país em regiões importantes para a política e para a economia.[118]
Menos de 10% da população activa trabalha no sector primário, embora seja fortemente apoiado pelo governo federal. Já 40% tem profissão na área secundária da indústria na qual se incluem produções de máquinas e metalúrgica, o têxtil e a relojoaria. Muitos desses produtos são exportados para outros países. Porém, o facto da Suíça não estar presente na União Europeia, as exportações sofrem com as restrições aduaneiras e pela falta de livre circulação de bens.[119] O poder de compra no país é alto, apesar do custo de vida o ser também. Porém, os altos salários, a inflação baixa (1%) e um nível de desemprego de 3,1% permitem aos habitantes terem uma qualidade de vida muito acima de vários países.[120] Segundo a CIA, o sector primário (agricultura e pecuária), o sector secundário (indústria e transformação) e o sector terciário representam, respectivamente, 3,9%, 22,8% e 73,2 % da força laborar helvética.[109]
A Suíça é dividida em treze regiões turísticas. A principal atração da Suíça são as paisagens dos Alpes suíços. Até ao século XVIII, o país não era um destino, mas uma passagem obrigatória no centro da Europa. Na altura, as cidades que atraiam turistas eram apenas Basileia e Genebra devido às suas universidades, movimentos religiosos, as fontes de água e as curas que as termas proporcionavam. O início do turismo no país é provocado pelos trabalhos de escritores e pintores naturalistas do fim do século XVIII e do início do século XIX que suscitam interesse aos viajantes pelas descrições das paisagens e das montanhas. As regiões mais expostas foram as Oberland e sobretudo Zermatt, no cantão do Valais desde 1850.[30]
No início do século XX, a Suíça gastava 3% do seu PIB anual para a hotelaria e as receitas daquele sector chegavam aos 320 milhões de francos suíços. O desenvolvimento pós-guerra de 1914-18 e o pagamento de férias pagas fazem com que existe um grande aumento da procura por parte das classes mais baixas que vêm a ser a maioria no fim da década de 1950.[122]
Actualmente, a procura de turismo na Suíça parte sobretudo da Alemanha, que corresponde a 14,4% das estadias feitas do país, seguida da França com 5,3%, o Reino Unido com 4,8% e Itália, com 3,8%. Dos países não europeus, os Estados Unidos representam 3,3% do total das noitadas, seguidos da Austrália e da Nova Zelândia que representam 3,3%. O Japão representa apenas 0,6% das noitadas feitas na Suíça.[123]
Cerca de 40% da eletricidade é obtida através de centrais nucleares e de grandes barragens localizadas nos Alpes enquanto que o restante é importado de outros países durante os períodos de alto consumo (Inverno e noite). No verão, a Suíça exporta energia para os seus vizinhos evitando desperdícios.[124]
Em 18 de maio de 2003, duas iniciativas antinucleares foram rejeitadas: Moratorium Plus, que objetivava a proibição de construção de novas usinas nucleares (41,6% apoiaram e 58,4% se opuseram),[125] e Eletricidade Não Nuclear (33,7% apoiaram e 66,3% se opuseram).[126]
A primeira moratória de dez anos na construção de novas usinas / centrais nucleares foi o resultado de uma iniciativa popular votada em 1990, a qual foi aprovada com votos de 54,5% Sim contra 45,5% Não. Atualmente, uma nova usina / central nuclear no Cantão de Berna está planejada / planeada. A Secretaria Federal de Energia Suíça (SFES) é o órgão responsável por todas as questões relacionadas ao abastecimento e uso de energia dentro do Departamento Federal de Meio-ambiente, Transporte, Energia e Comunicações (DMTEC). A agência apoia a iniciativa da sociedade dos 2 000 watts para cortar o uso de energia da nação em mais da metade até 2050.[127]
A rede ferroviária suíça é a mais densa da Europa,[128] tem 5 250 quilômetros e transporta mais de 596 milhões de passageiros anualmente (dados de 2015).[129] Cada cidadão viaja de trem em média 2 550 quilômetros.[129] Praticamente 100% da rede é eletrificada. A grande maioria (60%) da rede é operada pela SBB-CFF-FFS. Além da segunda maior companhia ferroviária de bitola padrão, a BLS AG, duas empresas ferroviárias que operam em redes de bitola estreita são a Ferrovia Rética (RhB) no cantão sudeste de Graubünden, que inclui algumas linhas consideradas Patrimônio Mundial,[130] e a Linha Matterhorn-Gotthard (MGB), que coopera com a RhB no Glacier Express, entre Zermatt e St. Moritz/Davos. Em 31 de maio de 2016, o túnel ferroviário mais longo e profundo do mundo e a primeira rota plana através dos Alpes, o túnel de base de São Gotardo que, com 57,1 quilômetros de extensão, foi inaugurado como parte da Nova Ferrovia Transalpina (NRLA) após 17 anos de obras. A empresa iniciou suas atividades diárias de transporte de passageiros em 11 de dezembro de 2016, substituindo a antiga rota montanhosa e panorâmica sobre o Maciço de São Gotardo.[131][132]
A Suíça possui uma rede rodoviária administrada publicamente, sem pedágios, que é financiada por concessões de rodovias, além de impostos sobre veículos e gasolina. O sistema suíço de autoestradas exige a compra de uma vinheta (etiqueta de pedágio) - que custa 40 francos suíços - para usar as estradas por um ano, tanto para carros de passeio quanto para caminhões. A rede rodoviária suíça tem um comprimento total de 1 638 km (2000) e cobre uma área de 41 290 km², uma das maiores densidades rodoviárias do mundo.[133] O Aeroporto de Zurique é o maior aeroporto internacional da Suíça, que transportou 22,8 milhões de passageiros em 2012.[134] Existem também outros aeroportos internacionais, como o Aeroporto de Genebra (13,9 milhões de passageiros em 2012).[135]
A educação tem vindo a ser uma fonte importante de recurso para o seu desenvolvimento económico. Por isso, o país clama em ter um dos melhores sistemas do mundo nessa área. No entanto, não existe um sistema educativo mas sim 26, o mesmo número de cantões, variando entre eles. Por exemplo, em alguns cantões o ensino da primeira língua estrangeira é iniciado no quarto ano enquanto que em outros inicia-se no sétimo. O facto de cada cantão possuir o seu sistema educativo, transferir os estudantes de uma escola para outra fora do seu cantão de residência pode se tornar problemático, podendo resultar na rejeição de vários deles nas suas escolas de destino.[136]
A maioria dos estudantes frequentam estabelecimentos públicos dado que os privados são caros para a maioria da população. Consiste em três períodos escolares: elementar, secundária e universitário. Todas as crianças são obrigadas a frequentar por no mínimo nove anos de escolaridade, seja em escolas públicas ou privadas.[136]
A elementar inicia-se normalmente aos sete anos de idade da criança e dura oito ou nove anos de acordo com o cantão. Algumas escolas oferecem um ano a aqueles que ainda não se decidiram em termos de carreira ou não tenham encontrado um emprego, ou não tenham atingido a idade para iniciar uma actividade profissional específica. No cantão de Zurique, tal como no cantão de Valais, a primária consistem em seis anos de educação primária, tendo normalmente apenas um professor para todos as disciplinas, e mais três anos onde já têm no mínimo dois professores para disciplinas específicas. Depois de terminar a escolaridade elementar, os estudantes têm normalmente duas opções: a secundária ou escolas de maturamento, em que a segunda incide principalmente na educação específica para uma profissão.[136]
O secundário, similar com o sistema português, tem várias áreas de incidência: as Mathematisches und Naturwissenschaftliches Gymnasium para as matemáticas e as ciências naturais; as Neusprachliches Gymnasium para o ensino de línguas modernas como o inglês e o francês (ou alemão, dependendo do cantão em que se insere o aluno); as Wirtschaftsgymnasium para as áreas da economia; entre outras. O secundário dura entre quatro e meio a seis anos e meio formando competências básicas para o acesso a universidades através do Eidgenössische Matura – diploma federal.[136]
Existem 12 universidades na Suíça, dez cantonais e duas federais, em que a primeira incide em áreas não técnicas. As duas universidades técnicas federais são a Eidgenössische Technische Hochschule Zürich e a École Polytechnique Fédérale de Lausanne. No entanto, o estudante que não tenha seguido o secundário e tenham optado pelas escolas de maturamento, têm à disposição um vasto leque de escolas superiores técnicas cantonais como a Haute École Valaisanne.[136]
A ciência e a tecnologia são fontes importantes para o desenvolvimento económico suíço. A Fundação Nacional para a Ciência é a organização do Governo que apoia através de fundos a investigação científica no país. A estrutura política e educacional permitem que a Ciência na Suíça esteja entre as mais avançadas do Mundo.[138]
O país tem a sua Agência Espacial e participa directamente com a Agência Espacial Europeia, sendo um dos dez fundadores. O astrónomo suíço Michael Mayor descobriu 51 Pegasi b, o primeiro exoplaneta a girar a volta de uma estrela como o Sol do Sistema Solar.[139]
Nas áreas da Matemática Leonhard Euler é considerado como o mais proeminente do século XVIII. Fez descobertas importantes nas áreas das funções matemáticas. Na Física, o naturalizado Albert Einstein (em 1901) foi um dos maiores génios científicos de todos os tempos através das suas teorias da relatividade e da sua equação E=mc².[140] A cidade de Genebra acolhe o maior laboratório do mundo, o CERN, que se dedica à investigação na área da Física.[141]
O sistema de saúde na Suíça baseia-se através de um sistema de seguros no qual todos os cidadãos são obrigados por lei a adquirir um, previsto pelo Acto Federal da Saúde. Enquanto que o seguro é subsidiado pelo governo, sobretudo para os mais pobres, os suíços pagam uma larga percentagem dos prémios do seu seguro de saúde. O governo paga 25% dos prémios enquanto os cidadãos pagam o resto. No entanto, o Estado é muito severo no que aplica a cobertura dos seguros sendo proibida a rejeição de uma cobertura a nenhum cidadão; exige que todos os preços dos mesmos sejam publicados.[142]
Em 2005, a esperança média de vida era de 79 anos para os homens e 84 para as mulheres,[143] colocando o país em terceira posição no grupo dos países europeus em termos de esperança de vida.[144] A qualidade de vida reflecte-se na esperança de vida em boa saúde de 71 anos para os homens e 74 para as mulheres. A taxa de natalidade, estimado para o ano de 2008, está registada nos 9,62 nascimentos por cada 1 000 habitantes, enquanto que a taxa de mortalidade fica-se pelos 8,54 falecimentos por 1 000 habitantes.[109]
Em termos de doenças, as mais frequentes são as de origem cardiovascular que representam 37% dos falecimentos, seguidos pelos vários tipos de cancro que registam 25% dos falecimentos no país.[145] Em 2001, a taxa de infectos com SIDA era de 0,4% representado 13 000 pessoas.[146]
Três das principais línguas da Europa são oficiais na Suíça. A cultura suíça é caracterizada pela diversidade, que se reflete em uma ampla gama de costumes tradicionais.[147] Uma região pode, de certa forma, estar fortemente ligada culturalmente ao país vizinho que compartilha sua língua, estando o próprio país enraizado na cultura da Europa Ocidental.[148] A cultura romanche isolada linguisticamente em Graubünden, no leste da Suíça, constitui uma exceção, sobrevive apenas nos vales superiores do Reno e na Pousada e se esforça para manter sua rara tradição linguística.[149][150]
A Suíça é o lar de muitos nomes notáveis ​​em literatura, arte, arquitetura, música e ciências. Além disso, o país atraiu várias pessoas criativas durante períodos de agitação ou guerra na Europa.[151] Cerca de 1 000 museus são distribuídos pelo país; o número mais que triplicou desde 1950.[152] Entre as apresentações culturais mais importantes realizadas anualmente, estão o Festival Paléo, Festival de Lucerna,[153] o Festival de Jazz de Montreux,[154] o Festival Internacional de Cinema de Locarno e a Art Basel.[155]
O simbolismo alpino desempenhou um papel essencial na formação da história do país e da identidade nacional suíça.[156][157] Atualmente, algumas áreas montanhosas têm uma forte cultura de esqui no inverno e uma cultura de caminhadas (em alemão:  das Wandern) ou mountain bike no verão. Outras áreas ao longo do ano têm uma cultura recreativa que atende ao turismo, mas as estações mais tranquilas são a primavera e o outono, quando há menos visitantes. A cultura tradicional de agricultores e pastores também predomina em muitas áreas do país e pequenas fazendas são onipresentes fora das cidades. Na Suíça, a arte é principalmente expressa em música, dança, poesia, escultura em madeira e bordado. A trompa alpina, um instrumento musical semelhante à trombeta, feito de madeira, tornou-se, ao lado do iodelei e do acordeão, um epítome da música tradicional suíça.[158][159]
Como a Confederação, desde sua fundação em 1291, era quase exclusivamente composta por regiões de língua alemã, as primeiras formas de literatura são em alemão. No século XVIII, o francês tornou-se a língua da moda em Berna e em outros lugares, enquanto a influência dos aliados de língua francesa e áreas sujeitas era mais acentuada do que antes.[160]
Entre os autores clássicos da literatura alemã suíça estão Jeremias Gotthelf (1797-1854) e Gottfried Keller (1819-1890). Os gigantes indiscutíveis da literatura suíça do século XX são Max Frisch (1911–91) e Friedrich Dürrenmatt (1921–90), cujo repertório inclui Die Physiker e Das Versprechen, lançado em 2001 como filme de Hollywood.[161]
Os escritores suíços de língua francesa famosos foram Jean-Jacques Rousseau (1712-1778) e Germaine de Staël (1766-1817). Autores mais recentes incluem Charles Ferdinand Ramuz (1878–1947), cujos romances descrevem a vida de camponeses e habitantes de montanhas, ambientados em um ambiente hostil, e Blaise Cendrars (nascido Frédéric Sauser, 1887–1961).[161]
Provavelmente a mais famosa criação literária suíça, Heidi, a história de uma menina órfã que vive com seu avô nos Alpes, é um dos livros infantis mais populares de todos os tempos e tornou-se um símbolo da Suíça. Sua criadora, Johanna Spyri (1827–1901), escreveu vários outros livros sobre temas semelhantes.[161]
A liberdade de imprensa e o direito à liberdade de expressão são garantidos na constituição federal da Suíça.[162] A Agência de Notícias Suíça (SNA) transmite informações 24 horas por dia em três dos quatro idiomas nacional. A SNA fornece informação para quase todas as mídias suíças e algumas dezenas de serviços de mídia estrangeira.[162]
Historicamente, a Suíça possui o maior número de títulos de jornais publicados na proporção de sua população e tamanho. Os jornais mais influentes são o Tages-Anzeiger e o Neue Zürcher Zeitung, em alemão, e o Le Temps, em francês, mas quase todas as cidades têm pelo menos um jornal local. A diversidade cultural é responsável por um grande número de jornais.[163]
O governo exerce maior controle sobre a mídia de transmissão do que sobre a mídia impressa, principalmente devido ao financiamento e ao licenciamento.[163] A Swiss Broadcasting Corporation, cujo nome foi mudado recentemente para SRG SSR, é responsável pela produção e transmissão de programas de rádio e televisão. Os estúdios da SRG SSR são distribuídos pelas várias regiões do país. O conteúdo de rádio é produzido em seis estúdios centrais e quatro regionais, enquanto os programas de televisão são produzidos em Genebra, Zurique e Lugano. Uma extensa rede de cabos também permite que a maioria dos suíços acesse os programas dos países vizinhos.[163]
Esqui, snowboard e montanhismo estão entre os esportes mais populares na Suíça, sendo a natureza do país particularmente adequada para essas atividades.[164] Os esportes de inverno são praticados pelos nativos e turistas desde a segunda metade do século XIX, com a invenção do bobsleigh em St. Moritz.[165] Os primeiros campeonatos mundiais de esqui foram realizados em Mürren (1931) e St. Moritz (1934). A última cidade sediou os segundos Jogos Olímpicos de Inverno em 1928 e a quinta edição em 1948. Entre os esquiadores e campeões mundiais mais bem-sucedidos estão Pirmin Zurbriggen e Didier Cuche. Os esportes mais assistidos com destaque na Suíça são futebol, hóquei no gelo, esqui alpino, schwingen e tênis.[166]
As sedes dos órgãos dirigentes do futebol e do hóquei no gelo, a Federação Internacional de Futebol (FIFA) e a Federação Internacional de Hóquei no Gelo (IIHF), estão localizadas em Zurique. Atualmente, muitas outras sedes de federações esportivas internacionais estão localizadas na Suíça. Por exemplo, o Comitê Olímpico Internacional (COI), o Museu Olímpico e o Tribunal Arbitral do Esporte (CAS) estão localizados em Lausanne.[167]
A Suíça sediou a Copa do Mundo FIFA de 1954 e foi o anfitrião conjunto, com a Áustria, do torneio da Campeonato Europeu de Futebol de 2008. A Super Liga Suíça é a liga de clubes de futebol profissional do país. O campo de futebol mais alto da Europa, a 2 mil metros acima do nível do mar, está localizado na Suíça e é chamado de Estádio Ottmar Hitzfeld.[169]
Muitos suíços também seguem o hóquei no gelo e apoiam uma das 12 equipes da Liga Nacional, que é a liga mais popular da Europa.[170] Em 2009, a Suíça sediou o Campeonato Mundial de Hóquei no Gelo de 2009 pela décima vez.[171] Também se tornou vice-campeã mundial em 2013 e 2018. Os inúmeros lagos tornam a Suíça um lugar atraente para a vela. O maior, o Lago de Genebra, é o lar do time de vela Alinghi, que foi o primeiro time europeu a vencer a America's Cup em 2003 e que defendeu com sucesso o título em 2007. O tênis se tornou um esporte cada vez mais popular e jogadores suíços como Martina Hingis, Roger Federer e Stanislas Wawrinka venceram vários Grand Slams.[168]
As corridas e eventos de automobilismo foram proibidos na Suíça após o desastre de Le Mans em 1955, com exceção de eventos de corridas de montanha. Durante esse período, o país ainda produziu pilotos de sucesso, como Clay Regazzoni, Sébastien Buemi, Jo Siffert, Dominique Aegerter, Marcel Fässler e Nico Müller. A Suíça também venceu o A1 Grand Prix em 2007-08 com o piloto Neel Jani. O motociclista suíço Thomas Lüthi venceu o Campeonato do Mundo de MotoGP de 2005 na categoria 125 cc. Em junho de 2007, o Conselho Nacional Suíço, uma casa da Assembleia Federal da Suíça, votou pela anulação da proibição; no entanto, o Conselho Suíço dos Estados rejeitou a mudança e a proibição permanece em vigor.[172]
Esportes tradicionais incluem luta suíça ou schwingen, uma tradição antiga dos cantões centrais rurais e considerada o esporte nacional por alguns. O hornussen é outro esporte nativo da Suíça, um tipo de cruzamento entre beisebol e golfe.[173] O steinstossen é a variante suíça de lançamento de pedras, praticado apenas entre a população alpina desde os tempos pré-históricos, registra-se que ocorreu em Basileia no século XIII.[174]
A culinária da Suíça é multifacetada. Embora alguns pratos como fondue, raclette ou rösti sejam onipresentes em todo o país, cada região desenvolveu sua própria gastronomia de acordo com as diferenças de clima e idiomas.[175][176] A cozinha tradicional suíça utiliza ingredientes semelhantes aos de outros países europeus, além de laticínios e queijos exclusivos, como Gruyère ou Emmentaler, produzidos nos vales de Gruyères e Emmental. O número de estabelecimentos gastronômicos é alto, principalmente no oeste da Suíça.[177][178]
O chocolate é fabricado na Suíça desde o século XVIII, mas ganhou reputação no final do século XIX com a invenção de técnicas modernas, como a conchagem e a temperagem, que permitiram sua produção em um nível de alta qualidade. Também um avanço foi a invenção do chocolate de leite sólido, em 1875, por Daniel Peter. Os suíços são os maiores consumidores mundiais de chocolate.[179][180]
A bebida alcoólica mais popular na Suíça é o vinho. O país é notável pela variedade de uvas cultivadas devido às grandes variações de terroirs, com suas misturas específicas de solo, ar, altitude e luz. O vinho suíço é produzido principalmente em Valais, Vaud, Genebra e Ticino, com uma pequena maioria de vinhos brancos. Os vinhedos são cultivados na Suíça desde a era romana, embora certos vestígios possam ser encontrados de origem mais antiga. As variedades mais difundidas são as chasselas e pinot noir. O merlot é a principal variedade produzida em Ticino.[181][182]
Coordenadas: 33° 20' N 44° 26' EBagdá (português brasileiro) ou Bagdade (português europeu)[1] ou Baguedade[2] ou Bagdad[3] (em árabe: بغداد; romaniz.: Baġdād, AFI: /bɐʁˈd̪ɑːd̪/; em curdo: بەغدا; romaniz.: Bexda) é a capital do Iraque e da província homônima. Com uma população de 8 milhões de habitantes, é a maior cidade do país. A sua área metropolitana conta com cerca de 9 milhões de habitantes.[4] Bagdá também é a segunda maior cidade do Sudoeste Asiático, depois de Teerã. Situa-se no centro do país, às margens do rio Tigre, e sua história remonta pelo menos ao século VIII, com possíveis origens pré-islâmicas. Antigo centro do mundo islâmico, Bagdá atualmente está no centro de conflitos violentos, desde 2003, devido à Guerra do Iraque.
Embora não se dispute a sua origem iraniana,[5] existem diversas propostas sobre qual seria a sua etimologia. Uma das mais amplamente aceitas dentre elas é a de que o nome seria um composto linguístico do persa médio, de Bag, "deus", e dād, "dado", que pode ser traduzido por "dado-por-Deus" ou "presente de Deus", a exemplo do persa moderno Baɣdād. O nome é pré-islâmico, e suas origens não são conhecidas, mas seguramente remontam aos antigos povoados da região, que não tinham poder comercial ou político.[6] O místico sufi Mansur al-Hallaj chamou a cidade de Madinat as-Salam, ou "Cidade da Paz", como referência ao paraíso;[7] este foi por vezes o nome oficial da cidade, em impressões oficiais como moedas.
Em 30 de julho de 762 o califa abássida Almançor fundou a cidade.[8] Almançor acreditava que Bagdá era a cidade perfeita para ser a capital do império islâmico sob o domínio dos abássidas. Almançor amava tanto o local escolhido que foi citado como tendo dito: "Esta é de fato a cidade que eu devo fundar, onde devo viver, e onde meus descendentes reinarão doravante".[9] O crescimento da cidade foi auxiliado por sua localização, que lhe propiciou o controle sobre rotas estratégicas e comerciais (ao longo do Tigre até o mar, e de leste-oeste, do Oriente Médio até o resto da Ásia. Feiras mensais também eram realizadas na região onde a cidade foi fundada. Outra razão porque Bagdá era uma excelente localização devia-se à abundância de água e seu clima seco. A água existe tanto nos lados norte e sul dos portões da cidade, permitindo a todos os domicílios da Bagdá antiga um fornecimento abundante, algo extremamente incomum na época. Bagdá alcançou seu período de maior prosperidade durante o reinado do califa Harune Arraxide, no início do século IX.
Bagdá acabou por eclipsar Ctesifonte, capital do Império Sassânida, que se localizava a 30 quilômetros a sudeste, e estava sob controle muçulmano desde 637 - e tornou-se rapidamente deserta após a fundação de Bagdá. O sítio da antiga Babilônia, que estava deserto desde o século II a.C., localiza-se 90 quilômetros ao sul de Bagdá.
Em seus primeiros anos a cidade era conhecida por lembrar deliberadamente uma expressão do Corão, referindo-se ao Paraíso.[10] Quatro anos antes a fundação de Bagdá, em 758, Mansur reuniu engenheiros, projetistas e artistas de todo o mundo para desenhar a futura cidade. Mais de 100 000 trabalhadores estudaram os projetos, e muitos receberam salários para começar a construir a grandiosa cidade. A estrutura da cidade em si era composta por dois grandes semicírculos com cerca de 19 km de diâmetro. Julho foi o mês escolhido para o início das obras, pois dois astrônomos, Naubakht Ahvaz e Mashallah,os quais acreditavam que a cidade deveria ser construída sob o signo de Leão.[11] O signo é significativo por ser o elemento relacionado ao fogo, e simbolizar a produtividade, o orgulho e a expansão. Os tijolos utilizados na construção da cidade tinham 18 polegadas em cada um de seus lados; Abu Hanifa, o responsável pela contagem dos tijolos, desenvolveu um canal que trazia água aos canteiros de obra e que podia ser usada tanto para o consumo humano quanto para a manufatura dos tijolos. Por toda a cidade mármore era usado para fazer os edifícios, e degraus de mármore levavam às margens do rio. Dentro da cidade existiam diversos parques, jardins, vilas e belas esplanadas que davam à cidade um acabamento elegante e de classe.[12]
A cidade foi projetada como um círculo com 2 km de diâmetro, o que fez com que ficasse conhecida como a "Cidade Redonda". O projeto original mostrava um anel de estruturas residenciais e comerciais ao longo do interior das muralhas da cidade, porém a construção final adicionou outro anel defensivo dentro do primeiro.[13] No centro da cidade localizava-se a mesquita principal, bem como o quartel-general dos guardas locais. O propósito ou utilização do espaço restante no centro ainda é desconhecido. O projeto circular da cidade é um reflexo direto do projeto urbanístico tradicional árabe; a antiga cidade sassânida de Gur é praticamente idêntica em seu projeto circular geral, de onde irradiam avenidas, e no qual os edifícios governamentais e templos localizam-se no centro da cidade.
Os quatro muros circundantes de Bagdá foram chamados de Cufa, Baçorá, Coração e Damasco, tendo recebido tais nomes por seus portões apontarem nas direções destes destinos.[14] A distância entre estes portões era um pouco menos do que 2,4 km. Cada portão tinha portas duplas que eram feitas de ferro; as portas eram tão pesadas que eram necessários vários homens para abrir e fechá-las. O próprio muro tinha aproximadamente 44 m de espessura na base e cerca de 12 m de espessura na parte superior. Além disso, o muro tinha 30 m de altura, que incluía merlões, uma parte sólida de um parapeito preparado para o combate geralmente perfurado por frestas. Este muro era cercado por um outro muro com uma espessura de 50 m. O segundo muro tinha torres e merlões arredondados que cercavam as torres. Este muro externo estava protegido por sólidos taludes, que eram feitos de tijolos e cal. Por fora do muro externo havia um fosso cheio de água.[15]
No centro de Bagdá, na praça central, estava o Palácio do Portão Dourado. O palácio era a residência do califa e sua família. Na parte central da edificação havia uma cúpula verde, que tinha 39 m de altura. Cercando o palácio havia um calçadão, edificado ao lado da água, no qual apenas o califa podia cavalgar. Além disso, o palácio era perto de outras mansões e residências oficiais. Próximo ao portão da Síria, uma construção servia como residência para os guardas. Era feita de tijolos e mármore. O governador do palácio vivia na última parte do edifício e o comandante dos guardas na frente. Em 813, após a morte do califa Alamim, o palácio deixou de ser utilizado como casa para o califa e sua família.[16] A influência para seu estilo veio da arquitetura árabe.[17] Os dois designers que foram contratados por Almançor para planejar o desenvolvimento da cidade foram Naubakht, um zoroastrista, que também determinou que a data da fundação da cidade seria astrologicamente auspiciosa, e Mashallah, um judeu de Coração, Irã.[18]
O rápido crescimento populacional motivou o deslocamento da capital para Samarra, a que se se seguiu a perda das províncias ocidentais e orientais com o final do centralismo abássida, a "tutela" dos buídas e, depois, o domínio dos seljúcidas (1055-1135).
A cidade afirmou-se como um dos principais centros culturais e comerciais do mundo islâmico até 10 de fevereiro de 1258, quando foi conquistada pelos mongóis liderados por Hulagu, neto de Gengis Cã. Foi destruída, sendo massacradas 800 000 pessoas, e saqueada. Seguiu-se uma época de disputa durante séculos entre turcos e persas. Em 1401, Bagdá foi novamente saqueada, desta vez pelas tropas de Tamerlão. Em 1534, foi conquistada pelos otomanos, e cai num período de declino.
Em 1638 tornou-se foi anexada pelo Império Otomano. Entre abril de 1853 e maio de 1863, foi local de exílio de Bahá'u'lláh, o fundador da Fé Bahá'í. Foi ocupada pelo exército britânico em 1917.
Conforme os acordos secretos de Sykes-Picot entre o britânico sir Mark Sykes e o francês Georges Picot, a França ficou com a tutela da Síria e a Grã-Bretanha a do Iraque. Em 26 de março de 1917 um corpo expedicionário britânico entrou em Bagdá, capital da Mesopotâmia e atacou os turcos otomanos.[19] Pelos acordos de San Remo assinados em 1920, a Grã-Bretanha recebeu um mandato por parte da Sociedade das Nações para administrar o país. Em 1921, Bagdá foi declarada capital do novo reino do Iraque, que em 1958 seria transformado em república.[20]
Em 1920 tornou-se a capital do reino do Iraque. Em 1958 o exército iraquiano depôs o monarca Faiçal II, formando um governo do qual surgiria Saddam Hussein. Durante a década de 1970 Bagdá viveu um período de prosperidade e crescimento motivado por um forte aumento do preço do petróleo. Foram feitas infraestruturas modernas incluindo redes para abastecimento de água e electricidade, e alcatroadas ruas.
Porém a Guerra Irã-Iraque de 1980-1988 foi um momento difícil para a cidade, pois o Irã pôs em marcha uma série de ataques com mísseis contra Bagdá. A cidade sofreu bombardeamentos na Guerra do Golfo em 1991 e durante a invasão e ocupação do Iraque pelos Estados Unidos e países aliados em 2003.
Com a deposição do regime de Saddam Hussein, a cidade foi ocupada por tropas estado-unidenses. A Autoridade Provisória da Coalizão cedeu o poder ao governo provisório no final de junho de 2004 e, posteriormente, foi dissolvida.
A cidade está situada numa vasta planície dividida pelo rio Tigre, que também divide Bagdá em duas partes: a metade oriental, conhecida como "Rusafa", e a metade ocidental, a "Karkh". O terreno onde fica a cidade é plano e de pouca altitude, produto de um aluvião original devido às longas e periódicas inundações provocadas pelo rio.
Bagdá tem um clima muito quente e árido (BWh, segundo a tabela de Köppen), sendo uma das cidades mais quentes do mundo. Durante o verão, de junho a agosto, a temperatura média é de 32 °C e é acompanhada de um sol abrasador. A chuva é praticamente inexistente na zona durante o verão. Durante o dia, os termómetros podem disparar até aos 50 °C à sombra e só à noite baixam até aos 24 °C. A umidade é também muito baixa pois a cidade está a grande distância do Golfo Pérsico, o que ajuda a que se formem as comuns tempestades de areia estivais com origem no deserto.
Durante o inverno, de dezembro a fevereiro, as temperaturas suavizam. As máximas oscilam entre os 15 e 16 °C e as mínimas andam pelos 4 °C, embora não seja raro Bagdá experimentar temperaturas abaixo dos 0 °C. A presença do rio Tigre atenua o efeito de continentalidade.
A chuva anual limita-se ao período que vai de novembro a março, sendo as médias de cerca de 140 mm com registos máximos de 575 e mínimos de 23 mm. Em 11 de janeiro de 2008 houve um facto insólito em Bagdá, quando a cidade acordou coberta de uma fina capa de neve, a primeira em mais de 100 anos.[21]
A capital do Iraque sempre desempenhou um papel importante na vida cultural árabe e tem sido o lar de escritores, músicos e artistas de todas as áreas.
A Madrassa Mustansiriya, construída pelo penúltimo califa abássida Al-Mustansir bi-llah em Bagdá é considerada como uma das mais antigas universidade árabo-islâmicas onde se ensinavam as ciências do Corão segundo a tradição de Maomé, as doutrinas islâmicas, as ciências da língua árabe, a matemáticas, os preceitos do Islão e as diversas disciplinas da medicina. A cidade tornou-se rapidamente o primeiro centro cultural mundial, acolhendo mais de um milhão de habitantes. Em meados do século IX) foi criada a Casa da Sabedoria, onde se fazia a tradução das obras dos grandes filósofos gregos e pessoas vindas da Europa e da Ásia vinham para se especializar em medicina, física, astronomia, meteorologia, matemática e todos os outros domínios.
O dialeto árabe falado em Bagdá hoje difere de outros grandes centros urbanos no Iraque, tendo características mais próprias de dialetos nômades árabes (verseegh, a língua árabe). É possível que isto tenha sido causado pelo repovoamento da cidade com os moradores rurais após os sacos múltiplos do final da Idade Média.
Alguns pontos de interesse são o Museu Nacional do Iraque, cuja valiosa coleção de artefactos foi saqueada durante a invasão de 2003, e os arcos denominados Mãos da Vitória. Vários partidos iraquianos debateram acerca de se se deverá continuar a ter os arcos como monumentos históricos ou se deverão ser desmantelados. Milhares de manuscritos antigos da Biblioteca Nacional e Arquivo do Iraque foram destruídos quando o edifício se incendiou durante a invasão de 2003. O Santuário Al Kadhimain, no noroeste de Bagdá (em Kadhimiya), é um dos mais importantes lugares religiosos xiitas do Iraque. Foi terminado em 1515 e o sétimo (Musa ibn Jafar al-Kathim) e nono imãs (Mohammad al-Jawad) foram aí enterrados. Um dos edifícios mais antigos é o Palácio Abássida, que faz parte da área histórica central e se encontra perto de outros edifícios de importância histórica como o Edifício Saray e a Escola Al-Mustansiriyah (do período abássida). Há outros lugares de interesse em Bagdá, cada um deles representante de uma era histórica:
Estação ferroviária de Bagdá, 1959
Imam Al-Kadhime santuário de Imam Al-Jawad
Universidade Mustansiriya
Aeroporto Internacional de Bagdá, ao custo de 900 milhões dólares. Pode manipular até 7,5 milhões de passageiros por ano
Mesquita de Abu Hanifa, em Adhamiyah (2008)
Hotel Infame al-Rasheed, um dos melhores hotéis 5 estrelas de Bagdá
Centro de Convenções de Bagdá
Bagdá é o lar de algumas das equipes de futebol mais bem sucedidos no Iraque, sendo os maiores clubes o al-Quwa al-Jawiya (clube da Força Aérea), al-Zawra, al-Shurta (Polícia) e al-Talaba (estudantes). O maior estádio de Bagdá, al-Shaab Stadium, foi inaugurado em 1966. Está em construção um outro estádio na cidade, superior ao estádio al-Shaab Stadium.
A cidade também teve uma forte tradição de corridas de cavalos desde a Primeira Guerra Mundial, conhecido simplesmente como "raças". Há relatos de pressões pelos islamistas para romper esta tradição, devido ao jogo associado.
A situação das religiões na capital do Iraque após o derrube de Saddam Hussein em março de 2003 é complexa: deu-se o surgimento de novos grupos políticos, o redespertar de movimentos religiosos tradicionais, o regresso dos que viviam no exílio, os líderes religiosos e a influência dos países vizinhos.
O aumento das tensões levou a vários ataques terroristas e a conflitos armados entre sunitas e xiitas. A limpeza étnica foi de grande envergadura, embora o grau de violência tenha reduzido em 2007 entre grupos religiosos. Uma das razões apontadas é que apenas existem agora distritos heterogéneos, de modo que há uma planificação prévia dos ataques. Outra razão para a redução da violência, é a presença do exército dos Estados Unidos, que se interpõe entre xiitas e sunitas.
Cerca de 95% da população do Iraque é muçulmana. Em Bagdá há muitas mesquitas, sendo a mais famosa a mesquita de Abu Hanifa. Antes da invasão de 2003, 65% dos muçulmanos era sunita e 35% era xiita.
O cristianismo existe no Iraque desde os primeiros tempos e as diversas igrejas cristãs iraquianas têm sólidas raízes. Durante o governo de Saddam Hussein (de partido laico) havia uma ampla liberdade religiosa. O governo chegou a ter ministros cristãos como o ministro católico caldeu Tariq Aziz. Aproximadamente metade dos cristãos do Iraque vive em Bagdá. A sua proporção no total da população até março de 2003 era em torno de 10%, mas diminuiu por causa da crise no Iraque até 2006, situando-se provavelmente em 5%.
Desde o início da guerra, segundo o bispo auxiliar de Bagdá, Andreas Abouna, por volta de 75% da população cristã tinha abandonado a capital, em busca de proteção no norte curdo do Iraque, ou em países vizinhos como a Turquia, Síria ou Jordânia.
O Patriarca da Babilônia, com sede em Bagdá, encabeça a organização religiosa da Igreja Católica Caldeia. A Igreja Católica Romana forma a Arquidiocese de Bagdá.
Bagdá é ainda a sede histórica do Patriarca da Igreja Assíria do Oriente. Os bispos da Igreja Ortodoxa Síria de Antioquia, organizada anteriormente como "Mafrianato do Oriente", também têm a sua sede em Bagdá.
A presença de população judaica em Bagdá data dos tempos da antiga cidade de Babilônia, próxima de Bagdá. O rei persa Ciro, o Grande conquistou a cidade e permitiu o regresso dos judeus à sua terra. Porém, muitos decidiam ficar, prosperando aí, e sofrendo também perseguições, até aos tempos do Califado Abássida e do Império Otomano. Depois da independência do Estado de Israel em 1948 e da guerra árabe-israelita desse mesmo ano, repetiram-se os distúrbios contra os judeus. O governo sionista de Israel, disse então que haveria 135 000 judeus no país, 77 000 deles em Bagdá - uma quarta parte da população total.
Israel, sob o governo de David Ben Gurion, tomou uma série de medidas, entre elas uma operação para começar a trasladar a partir de 1952 aproximadamente cerca de 95% dos judeus iraquianos através de uma ponte aérea. Em 25 de julho de 2003, seis dos últimos 34 judeus de Bagdá viajaram de avião para Israel.
A maioria dos esforços de reconstrução do Iraque tem sido dedicada à restauração e reparação de infraestrutura urbana danificada. São mais visíveis os esforços de reconstrução efetuados pela iniciativa privada, como o "Plano de Renascença de Bagdá" e o "Centro de Conferências e Complexo Hoteleiro Simbá", do arquiteto e designer urbano Hisham N. Ashkouri. Existem, também, planos para se construir uma roda-gigante parecida com o Olho de Londres. O Conselho de Turismo do Iraque também está buscando investidores para desenvolver uma "ilha romântica" no rio Tigre, que já foi um local popular para a lua de mel de casais iraquianos. O projeto incluirá um hotel de seis estrelas, spa, um campo de golfe de 18 buracos e um clube de campo. Além disso, o sinal verde foi dado para se construírem inúmeros arranha-céus de arquitetura única ao longo do rio Tigre, o que iria desenvolver o centro financeiro da cidade. Em outubro de 2008, o metropolitano de Bagdá voltou a funcionar normalmente, ligando o centro da cidade aos bairros da região sul.
Coordenadas: 41° 1' N 28° 58' EIstambul (em turco: İstanbul), a antiga Bizâncio e Constantinopla (nome ainda usado em várias línguas, como no grego Κωνσταντινούπολις, Konstantinúpolis), é a maior cidade da Turquia e rivaliza com Londres como a mais populosa da Europa, com 15 067 724 habitantes na sua área metropolitana em 2018. A grande maioria da população é muçulmana, mas também há um grande número de laicos e uma ínfima minoria de cristãos e judeus.
É a capital da área metropolitana (büyükşehir) e da província de Istambul, a qual faz parte da região de Mármara. No passado foi a capital administrativa da Província de Istambul, na chamada Rumélia ou Trácia Oriental. Foi denominada Bizâncio até 330 d.C., e Constantinopla até 1453, nome bastante difundido no Ocidente até 1930. Durante o período otomano, os turcos chamavam-na de Istambul, nome oficialmente adotado em 28 de março de 1930.
Foi a capital do Império Romano do Oriente e do Império Otomano até 1923, cujo governante máximo, o sultão, foi durante séculos reconhecido como califa, o chefe supremo de todos os muçulmanos, o que fazia da cidade uma das mais importantes de todo o Islão. Atualmente, embora a capital do país seja Ancara, Istambul continua a ser o principal polo industrial, comercial, cultural e universitário (aí estão sediadas mais de uma dezena de universidades) do país. É a sede do Patriarcado Ecumênico de Constantinopla, sede da Igreja Ortodoxa.
A cidade ocupa ambas as margens do estreito do Bósforo e do norte do mar de Mármara, os quais separam a Ásia da Europa no sentido norte-sul, uma situação que faz de Istambul a única cidade que ocupa dois continentes. A parte central da parte europeia é por sua vez dividida pelo estuário do Corno de Ouro. É usual dizer-se que a cidade tem dois ou três centros, conforme se considere ou não que na parte asiática também existe um centro. No lado europeu há duas zonas com mais destaque em termos de movimento de pessoas e património cultural: o mais antigo, onde se situava o núcleo da antiga Bizâncio e Constantinopla, correspondente ao atual distrito de Fatih, fica a sul do Corno de Ouro, enquanto que Beyoğlu, a antiga Pera e onde se situava o bairro europeu medieval de Gálata, fica a norte. O centro da parte asiática tem contornos menos precisos, e ocupa parte dos distritos de Üsküdar e Kadıköy. Algumas zonas históricas da parte europeia de Istambul foram declaradas Património Mundial pela UNESCO em 1985. Em 2010, a cidade  foi a Capital Europeia da Cultura. Devido à sua dimensão e importância, Istambul é considerada uma megacidade e uma cidade global.
O atual nome da cidade, İstanbul em turco (AFI: [is'tambu] ou, coloquialmente, [ɨsˈtambul]) é usado nas suas diversas variações pelo menos desde o século X, tendo-se tornado o nome comum em turco desde a sua integração no Império Otomano depois da Queda de Constantinopla, em 1453.[3] Etimologicamente o nome é derivado da expressão grega medieval "εἰς τὴν Πόλιν" [istimˈbolin] ou, no dialeto egeu, "εἰς τὰν Πόλιν" [istamˈbolin] (em grego moderno: στην Πόλι [stimˈboli]), que significa "na cidade", "à cidade" ou "centro da cidade".[3][4][5][a]
No século XIX ainda eram usados diversos nomes para a cidade. Os europeus em geral usavam principalmente Stambul e Constantinopla para se referirem a toda a cidade, embora por vezes se distinguissem ambos os nomes — Constantinopla podia designar apenas a parte mais antiga, a sul do Corno de Ouro (atual Fatih), usando-se "Pera" para designar  a zona norte, chamada Beyoğlu pelos turcos, o nome que é usado atualmente. Desde os tempos bizantinos que Pera foi a área onde as comunidades de origem europeia ocidental se concentraram, uma situação que perdurou até ao fim do Império Otomano. Entre os turcos era mais frequente que Istambul designasse apenas a parte mais antiga.[6][7][a]
Bizâncio (em grego clássico: Βυζάντιον; pronúncia em grego demótico moderno: /vi.za.ⁿdjo/) foi o primeiro nome da cidade quando foi fundada em 667 a.C.[b]
por colonos dóricos da cidade-estado de Mégara, que a batizaram em homenagem ao seu rei Bizas.[13][14] Quando o imperador romano Constantino, o Grande fez da cidade a nova capital oriental do seu império, em 11 de maio de 330, rebatizou-a Nova Roma.[15] No entanto, o nome que acabou por se impor como mais generalizado foi Constantinopla (em grego: Κωνσταντινούπολη ou Κωνσταντινούπολις; Konstantinoupolis; "Cidade de Constantino"), o qual foi usado pela primeira vez de forma oficial durante o reinado do imperador Teodósio II (408-450).[4] O nome oficial permaneceu Constantinopla durante todo período bizantino e foi o nome comumente usado no Ocidente até o início do século XX.[6]
A cidade foi também apelidada de "Cidade das Sete Colinas", pois o Cabo do Serralho, a península onde se situa a parte mais antiga da cidade tem sete colinas, como Roma. Atualmente no cimo de cada uma das colinas há uma grande mesquita imperial otomana.[16][17] As colinas estão representadas no emblema da cidade como sete triângulos, sobre os quais se elevam quatro minaretes. A cidade tem muitas outras alcunhas, como por exemplo, Vasilevousa Polis ("Rainha das Cidades", em grego), que tem origem na importância e riqueza da cidade durante a Idade Média, e Dersaâdet (originalmente Der-i Saadet, "Porta para a Felicidade") que foi usada pela primeira vez no fim do século XIX e ainda é utilizada hoje em dia.[6][a]
Com a Lei do Serviço Postal Turco, de 28 de março de 1930, as autoridades turcas pediram oficialmente às nações estrangeiras que adotassem Istambul como o único nome nos seus idiomas.[18][a]
Em 2008, durante as obras de construção da estação de Yenikapı, foi descoberto um assentamento neolítico até então desconhecido, datado de cerca de 6 500 a.C.,[19][20] quando o Bósforo ainda não se tinha formado e o mar de Mármara era pouco mais que um lago interior.[21] Entre os séculos XIII e XI a.C., tribos trácias estabeleceram dois assentamentos — Lygos e Semistra — em Sarayburnu (cabo do Serralho), perto do local onde se ergue atualmente o Palácio de Topkapı.[22][23][a]
O primeiro povoamento no lado anatólio (oriental) foi encontrado no monte Fikirtepe, no que é hoje o distrito de Kadıköy. Data da Idade do Cobre e nele foram encontrados artefatos que datam de 5 500 a 3 500 a.C.. Não longe dali, foi descoberto um entreposto comercial fenício que existiu no início do 1º milénio a.C.[24][a] Calcedónia, a primeira colónia grega na área, foi fundada por dóricos da cidade de Mégara que se estabeleceram no cabo de Moda,  cerca de 685 a.C. (675 a.C. ou 639 a.C. segundo outras fontes), onde hoje se situa o distrito de Kadıköy.[b]
Bizâncio foi igualmente fundada por colonos de Mégara, que se estabeleceram nos antigos povoados trácios de Lygos e Semistra, no lado ocidental (europeu) do Bósforo, na margem sul do Corno de Ouro 17 anos depois de fundarem Calcedónia, ou seja, 667 a.C., 659 a.C. ou 619 a.C..[23][b] No final do século VII a.C. foi fundada uma acrópole no cimo do cabo do Serralho, no local onde hoje se encontra o Palácio de Topkapı.[14]
Segundo a lenda, a localização da nova cidade foi indicada ao rei de Mégara, Bizas, de onde provém o nome Bizâncio, pelo Oráculo de Delfos, quando o rei lhe foi pedir conselho sobre uma nova terra para se estabelecer com a sua família e seguidores. O oráculo aconselhou-o a procurar a "terra dos cegos" e fundar a cidade no lado oposto àquele onde essa se encontrava. Bizas percebeu a importância estratégica do Cabo do Serralho, rodeado de água por três lados (o mar de Mármara a sul e sudoeste, o Bósforo a leste e nordeste e o Corno de Ouro a norte), na importante rota marítima que liga o Mediterrâneo ao mar Negro através do mar de Mármara e do Bósforo, pelo que assumiu que o oráculo se referia aos calcedónios como "cegos" por não terem sido capazes de ver que era ali, e não no lado oriental, o local ideal para construir uma cidade.[25]
Apesar da sua situação privilegiada, a cidade não se desenvolveu significativamente durante os primeiros tempos.[23] Foi muito afetada durante as Guerras Médicas, tendo sido ocupada por Dario I em 512 a.C.. Segundo algumas fontes, a cidade esteve sob o domínio do Império Aqueménida, entre 479 e 444 a.C., depois da expulsão do general espartano Pausânias.[25] Outras fontes referem que nesse período a cidade estava sob domínio ateniense, fazendo parte da Liga de Delos a partir de 478 a.C., contra a qual se revoltou em 440, para ser submetida pouco depois.[26] Bizâncio viu-se depois envolvida nas guerras entre Atenas e Esparta (ver Guerra do Peloponeso) e o seu controle (direto ou através de alianças) foi disputado por ambos os lados durante os dois séculos seguintes.[25]
Em 340 a.C., Filipe II da Macedónia cobiça Bizâncio, entretanto já independente de novo, mas não consegue ocupá-la.[c]
O seu filho Alexandre, o Grande passa junto à cidade quando se dirige para a Ásia, atravessando os Dardanelos, mas curiosamente Bizâncio fica de fora do imenso império conquistado por Alexandre, apesar de ser seu vizinho muito próximo. Durante as Guerras dos Diádocos pela partilha do império de Alexandre, a cidade conseguiu manter-se neutral, continuando a ser um mercado importante de bens alimentares provenientes da Trácia, Macedónia, Anatólia e do Cáucaso. As alianças que estabelece com outras cidades marítimas, como Rodes, permitem-lhe conservar a sua independência até à chegada dos romanos,[26] apesar de algumas fontes referirem que em 179 a.C. foi conquistada por uma aliança entre Rodes e os reinos de Pérgamo e da Bitínia.[25]
A partir de 191 a.C.[28] ou 146 a.C., Bizâncio aliou-se a Roma que a reconheceu como "cidade livre e federada". No século I a.C. foi integrada na República, apesar de terem sido mantidas algumas estruturas de governo democrático local[23] e a sua autonomia só ter sido suprimida em 73 d.C. por Vespasiano. Supõe-se que nesse tempo a cidade não ocupasse muito mais do que é hoje área do Palácio de Topkapı e da Basílica de Santa Sofia.[26]
Em 194 d.C., Bizâncio viu-se envolta numa disputa entre o imperador romano Septímio Severo e o usurpador Pescênio Níger. Após tomar partido pelo último, a cidade foi sitiada pelas forças leais a Septímio em 196 d.C. e sofreu extensos danos, tendo sido arrasadas as muralhas e os monumentos. Cinco anos depois, o mesmo Septímio Severo ordenou a reconstrução da cidade, que rapidamente alcançou sua antiga prosperidade, chegando a ser rebatizada como Augusta Antonina pelo imperador, em homenagem a seu filho.[23][29][30] Em 262 d.C. Bizâncio foi devastada pelas tropas do imperador romano Galiano, mas foi rapidamente reconstruída.[23]
Durante a primeira divisão do Império Romano, Bizâncio foi uma cidade fronteiriça, pois os  impérios ocidental e oriental tinham como fronteira o Bósforo. Durante a guerra civil entre Licínio e Constantino, o Grande, Bizâncio tomou o partido do primeiro, mas submeteu-se ao último após a sua vitória na batalha de Crisópolis (24 de setembro de 324), a qual teve lugar no que é atualmente Üsküdar. A posição estratégica de Bizâncio atraiu Constantino, que a rebatizou de Nova Roma. Em 330 tornou-se oficialmente a nova capital do Império Romano, quando também já era conhecida pelo nome que haveria de perdurar — Constantinopla.[23] Segundo uma lenda, Constantino começou por ordenar que a nova capital fosse edificada em Calcedónia, mas um bando de águias levou as ferramentas dos pedreiros para o outro lado do mar de Mármara, o que levou Constantino a decidir que a sua capital ficasse na margem ocidental.[26][a]
A refundação da cidade ficaria na História como um dos feitos mais duradouros de Constantino. O seu novo estatuto de capital imperial deslocou o poder romano para oriente e a cidade tornou-se o centro da cultura grega e do Cristianismo durante os séculos seguintes.[15][31]
Em 395 o império tornou a ser dividido e Constantinopla passou a ser a capital do Império Romano do Oriente, que ficaria conhecido como Império Bizantino.[15][31] Foram construídas numerosas igrejas em toda a cidade, incluindo aquela que durante praticamente mil anos foi a maior catedral do mundo, a Basílica de Santa Sofia.[32]
O Patriarcado Ecuménico de Constantinopla desenvolveu-se na cidade e ainda hoje o seu líder é uma das figuras mais destacadas na Igreja Ortodoxa Grega. A localização de Constantinopla ajudou a assegurar que a sua existência resistiria ao teste do tempo — durante muitos séculos as suas lendárias muralhas protegeram a Europa de invasores vindos de leste e do avanço do Islão.[15]
O estatuto de capital imperial e a posição estratégica, na encruzilhada entre a Europa e a Ásia e entre o Mediterrâneo e o mar Negro, contribuíram para tornar a cidade  um importante centro de comércio, cultura e diplomacia. Enquanto o Império Romano do Ocidente mergulhava numa crise económica, política e demográfica, Constantinopla continuou a prosperar e durante a maior parte da Idade Média e nos últimos séculos do Império Bizantino foi a maior e mais próspera cidade do continente europeu, sendo em alguns períodos a maior metrópole do mundo.[33][a]
Um dos períodos de maior esplendor de Constantinopla foi o reinado de Justiniano I, que mandou ampliar Santa Sofia no século VI. Tendo conhecido algum declínio nos séculos VII e VIII, o império retomaria o seu esplendor nos séculos IX e X.[33]
O grande declínio do império foi marcado com a derrota na Batalha de Manziquerta (1071) frente aos turcos do Império Seljúcida, na sequência da qual grande parte da Anatólia deixou de estar sob o domínio bizantino para passar a constituir o Sultanato de Rum.[34]
A decadência da capital chegou um século depois com a Quarta Cruzada, durante a qual foi saqueada, pilhada e ocupada, ironicamente por forças cristãs. Constantinopla passou então a ser a capital do Império Latino, um estado criado pelos cruzados católico que duraria 55 anos.[35] Em 1261, Miguel VIII Paleólogo reconquistou a cidade e restaurou o Império Bizantino. Constantinopla encontrava-se então em grande decadência, com muitos dos edifícios, serviços básicos e defesas em ruínas,[36] tendo a sua população diminuído de cerca de meio milhão no século IX para 40 000.[37][38]
Apesar de todas as turbulências e da constante ameaça turca, que perdurou para além do fim dos estados cruzados, a cidade manteve a sua importância como centro cultural e comercial do Mediterrâneo, onde a maior parte das potências comerciais dessa parte do mundo mantiveram consulados e colónias de mercadores.[36]
No século XIV várias das reformas económicas e militares de  Andrónico II, como a redução das forças militares, enfraqueceram o império e deixaram-no mais vulnerável a ataques.[39] Em meados desse século, os otomanos, sucessores dos seljúcidas, começaram a por em prática a estratégia de tomar cidades mais pequenas, cortando as rotas de abastecimento de Constantinopla e estrangulando-a lentamente.[40] Finalmente, depois de oito semanas de cerco durante o qual é morto o último imperador bizantino, Constantino XI Paleólogo, o sultão Maomé II, que ficará conhecido como "o Conquistador" (Fatih), conquistou a cidade a 29 de maio de 1453 e declarou-a imediatamente a nova capital do seu Império Otomano.[35][41] Horas depois de entrar na cidade, Maomé foi a Santa Sofia e convocou um imã para proclamar a fé islâmica, convertendo um dos símbolos maiores do cristianismo numa mesquita imperial.[42][a] A Queda de Constantinopla, nome pelo qual é conhecida a conquista da cidade pelos otomanos, é frequentemente apontada como uma das datas que marca do fim da Idade Média.[43]
Depois de conquistar a cidade, Maomé II empenhou-se em revitalizá-la, nomeadamente convidando e forçando a nela se fixarem muitos muçulmanos, judeus e cristãos de outras partes da Anatólia, criando uma sociedade cosmopolita que perdurou praticamente durante todo o período otomano.[44] No fim do século XV a população tinha crescido para 200 000, fazendo de Istambul a segunda maior cidade da Europa.[38] Maomé mandou reparar as infraestruturas danificadas, iniciou a construção do Grande Bazar e  sobre as ruínas da antiga acrópole construiu o Palácio de Topkapı, que serviu de residência imperial oficial durante 400 anos. Ordenou também a construção da primeira mesquita imperial de construção otomana, a Mesquita de Fatih, para o que foi demolida a Igreja dos Santos Apóstolos, a maior da cidade a seguir a Santa Sofia.[44]
Os otomanos transformaram rapidamente Constantinopla, até então um bastião do cristianismo, num símbolo da cultura islâmica. Foram criadas fundações religiosas para financiar a construção de grandes mesquitas imperiais, as quais, além de serem um local de oração, tinham estruturas de apoio social anexas, como escolas, hospitais e balneários públicos.[44] O reinado de Solimão, o Magnífico (1494-1566) foi um período de feitos artísticos e arquitetónicos especialmente grandiosos. São deste período as mesquitas da autoria do arquiteto principal da corte, Mimar Sinan, cuja genialidade o fez ser conhecido no Ocidente como "o Miguel Ângelo otomano". A maior mesquita de Istambul (não considerando Santa Sofia) é a mesquita imperial de Solimão, uma das inúmeras obras de Sinan. Outras formas de arte florescentes foram a cerâmica, a caligrafia e a miniatura.[45] No final do século XVIII residiam na cidade 570 000 pessoas.[38]
Uma série de rebeliões no início do século XIX levou à subida ao poder do sultão progressista Mamude II e ao chamado período reformista Tanzimat (reorganização em turco otomano), que alinhou o império com alguns dos padrões europeus ocidentais em termos económicos, políticos e culturais.[46][47] Durante este período foram construídas pontes sobre o Corno de Ouro[48] e Istambul foi ligada à rede ferroviária europeia na década de 1880.[49] O Tünel, uma das linhas ferroviárias urbanas subterrâneas mais antigas do mundo, foi inaugurada em 1875 na encosta sul de Gálata.[50] Ao longo das décadas seguintes foram também gradualmente criadas outras infraestruturas modernas, como uma rede estável de distribuição de água, eletricidade, telefones e elétricos, embora com algum atraso em relação a outras capitais europeias.[51]
Os esforços de modernização não foram suficientes para evitar o declínio do regime imperial. No início do século XX assistiu-se à Revolução dos Jovens Turcos, herdeiros políticos dos "Jovens Otomanos" de meados do século anterior. Os "Jovens Turcos" depuseram  o sultão Abdulamide II em 1909 e estalaram várias guerras que fustigaram a decadente capital imperial.[52] A última destas guerras foi a Primeira Guerra Mundial, que se saldou numa derrota e subsequente ocupação de Istambul por tropas britânicas, francesas e italianas. O último sultão otomano, Mehmed VI foi deposto e exilado em novembro de 1922 e a família imperial foi expulsa a 3 de março de 1924.[53][54] Em 1923, terminou a ocupação de Istambul com a assinatura do Tratado de Lausana, no qual também era reconhecida a República da Turquia, a qual foi oficialmente declarada em 29 de outubro de 1923.[54]
Os revolucionários nacionalistas liderados por Atatürk tinham estabelecido a sede do seu governo em Ancara, que foi declarada a capital da nova república. Nos primeiros anos do regime republicano, Istambul foi algo descurada a favor da nova capital, mas a partir dos anos 1940 e início dos anos 1950, a cidade sofreu grandes mudanças estruturais, nomeadamente urbanísticas. Foram construídas novas praças (como a Praça Taksim, o centro da cidade moderna), avenidas e edifícios, por vezes derrubando construções históricas.[55] Em 1955 ocorreu o Pogrom de Istambul, uma série de motins dirigidos principalmente à então ainda numerosa comunidade grega da cidade, mas que também afetou outras minorias, como os arménios, judeus e inclusivamente muitos muçulmanos, e acelerou a fuga para a Grécia da população etnicamente grega de Istambul.[56][57][58] O crescimento da população de Istambul começou a acelerar rapidamente nos anos 1970, com o afluxo de pessoas da Anatólia para trabalharem nas muitas novas fábricas que foram construídas nos subúrbios da metrópole em expansão. Este súbito aumento populacional provocou uma grande procura de habitação e muitas das aldeias e florestas que rodeavam a cidade foram absorvidas pela grande área metropolitana de Istambul.[59]
Istambul está situada no noroeste da Turquia, na Região de Mármara (Marmara Bölgesi) e é a capital da província (ill) de Istambul.[60] Em 2004, a área metropolitana administrada pela Autoridade Municipal de Istambul (İstanbul Büyükşehir Belediyesi) ocupava uma área de 5 343, 3 479 km² (65%) no lado europeu e 1 864 km² (35%) no lado asiático.[61]
O Bósforo (em turco: Boğaziçi) é um estreito que divide em duas partes a cidade de Istambul e separa fisicamente a Rumélia, na Europa, da Anatólia, na Ásia, ligando o mar Negro, a norte, com o mar de Mármara, a sul, que por sua vez está ligado ao mar Mediterrâneo pelo estreito de Dardanelos. A cidade apresenta assim a característica peculiar de se estender por dois continentes. Além da divisão leste-oeste, a parte histórica do lado europeu é dividida no sentido leste-oeste pelo Corno de Ouro, um porto natural e estuário de uma ribeira orientada no sentido noroeste-sudeste, situado a norte da península histórica onde foi fundada Bizâncio (o Cabo do Serralho ou Sarayburnu, atual distrito urbano de Fatih).[60][62]
A cidade tem dois centros principais, ambos no lado europeu:  a Praça Sultão Ahmet (Sultanahmet Meydanı) a sul do Corno de Ouro e a Praça Taksim, a norte. A primeira situa-se no local onde foi o hipódromo de Constantinopla, centro da cidade romana e bizantina, e é ladeada por dois dos mais populares monumentos de Istambul, a Mesquita Azul (Sultanahmet, do sultão Amade I), construída em 1616, e a Basílica de Santa Sofia (Ayasofya), construída em 537. A Praça Taksim pode considerar-se o centro da cidade moderna; situa-se no extremo norte daquilo que foi o bairro cultural e etnicamente mais ocidentalizado da capital bizantina e otomana, no atual distrito de Beyoğlu, cujo núcleo histórico inclui os antigos bairros de Pera e Gálata, onde viviam as comunidades imigrantes ocidentais e se situavam as principais embaixadas. Algumas dessas comunidades estrangeiras, nomeadamente a francesa e a italiana, estabeleceram-se na área ainda no período bizantino. Além dos ocidentais, em Beyoğlu concentravam-se também muitos judeus (principalmente sefarditas, fugidos da Península Ibérica no final do século XV, mas também asquenazes), gregos e arménios, entre outros. No lado asiático destacam-se dois centros: o de Kadıköy, a antiga cidade de Calcedónia, e Üsküdar, a Scutari e Crisópolis na Antiguidade.[60]
A confluência do mar de Mármara, do Bósforo e do Corno de Ouro, que hoje constitui o centro geográfico de Istambul, contribuiu para suster forças atacantes durante milhares de anos e ainda é uma das características mais proeminentes da paisagem da cidade.[60] Até ao século XIX, quando foi construído um cais ao longo da embocadura do Corno de Ouro em Gálata, no que é hoje o bairro de Karaköy, existia aí uma longa praia de areia.[62]
É comum dizer-se que a península histórica tem sete colinas, cada uma delas encimada com uma mesquita imperial (construída por e em homenagem a um sultão). É rodeada pelas lendárias Muralhas de Constantinopla, com 22 km de comprimento, construídas principalmente no século IV (Muralhas de Constantino) e século V (Muralhas de Teodósio). O alto da colina mais alta é ocupado pelo Palácio de Topkapı, a residência dos sultões durante quatro séculos. No lado oposto do Corno de Ouro ergue-se outra colina de forma cónica, onde se situa o distrito de Beyoğlu. Devido à topografia, as construções de Beyoğlu foram construídas sobre terraços suportados por muros, alguns deles ainda visíveis, e muitas das ruas tinham ou ainda têm degraus.[63][64]
No outro lado do Bósforo, na Anatólia, Üsküdar exibe as mesmas características topográficas, com colinas que se estendem até à beira-mar, de forma mais abrupta nos bairros de Şemsipaşa e Ayazma, na ponta mais ocidental.[63] É usual referir-se que o ponto mais alto de Istambul é a colina de Çamlıca (Çamlıca Tepesi) (267 m), situada a 4 km do centro de Üsküdar,[65] pois esta situa-se numa área urbanizada e relativamente perto dos principais centros da cidade. No entanto, o ponto mais alto da área metropolitana é o Monte Aydos (Aydos Dağı), no distrito de Kartal, na zona sudeste da parte asiática.[64]
As margens do Bósforo são constituídas por colinas, por vezes com declives bastante acentuados. Apesar da construção ser densa em algumas áreas, em grande parte dos troços a norte da Ponte Fatih Sultão Mehmet predomina o verde das árvores[66] e inclusivamente em áreas mais urbanizadas, como em Beşiktaş e Eyüp (este na parte superior do Corno de Ouro) há muitas manchas de verde, apesar da área de parques e jardins por habitante ser inferior às da maior parte das cidades europeias.[60] Considerando toda a área metropolitana, só 36% do território se considera urbanizado; 17% é ocupado por terrenos agrícolas e 47% por florestas.[67]
As ilhas dos Príncipes (Prens Adaları), situadas no mar de Mármara, a sudeste do centro da cidade, conservam muita da atmosfera de há mais de cem anos, com as suas casas típicas de madeira e as encostas verdejantes. São um destino de recreio popular entre os istambulitas, principalmente no verão. Têm a particularidade do uso de veículos motorizados particulares ser proibido e inclusivamente os táxis são charretes.[66][68]
Istambul está situada próximo da Falha Setentrional da Anatólia, uma falha geologicamente ativa responsável por vários grandes sismos na história da cidade, tanto no passado como em tempos mais recentes. Entre os sismos mais devastadores destaca-se o de 1509, que causou um tsunami  que galgou as muralhas, destruiu mais de cem  mesquitas e causou mais de dez mil mortos. Mais recentemente, em 1999, o sismo de İzmit provocou mais de 18 000 mortes, das quais cerca de mil em Istambul. Os estudos demonstram que há um risco elevado de que nas próximas décadas se produza um terremoto devastador na região de Istambul.[69][70][d]
É provável que devido às dificuldades para estabelecer e impor normas convenientes de segurança na construção dos edifícios, se repita o que aconteceu durante o sismo de agosto de 1999, isto é, que haja um número enorme de desmoronamentos, especialmente nas habitações de menor custo de alvenaria dos gecekondular (favelas) dos subúrbios.[69][71]
O elevado e rápido crescimento urbano, a grande densidade industrial em certas zonas e o tráfego intenso causam problemas ambientais significativos. Nos últimos anos a qualidade do ar tem melhorado devido à utilização de gás natural e um melhor tratamento de resíduos diminuiu os problemas relacionados com lixo. No entanto, a poluição atmosférica e aquática devida às numerosas fábricas, veículos e habitações continua a ser problemática, o mesmo se passando em relação à poluição sonora. Os problemas tendem a ser mais agudos nos bairros mais pobres e suas vizinhanças.[72][e]
O sistema de esgotos funciona mal em algumas zonas, sendo frequentes os entupimentos provocados por lixo, o que por vezes provoca grandes poças quando não inundações, que aumentam o risco de doenças infecciosas. A principal causa desses problemas reside no facto da infraestrutura não estar adaptada ao tremendo crescimento urbano das últimas décadas.[73][e]
Segundo a classificação climática de Köppen-Geiger, Istambul tem um clima de transição entre o úmido subtropical (Cfa, segundo Köppen) e o mediterrânico (Csa, segundo Köppen), visto que há um decréscimo nas precipitações durante o verão, mas não tão forte quanto em outros locais de clima mediterrânico típico,[74][75] e com influências oceânicas (Cfb, segundo Köppen) acentuadas, principalmente a norte.[76] O clima de Istambul caracteriza-se por verões longos, quentes e húmidos, e invernos frios, chuvosos, ocasionalmente com neve que pode ser abundante, embora geralmente não dure mais do que alguns dias, pois não é comum que as temperaturas negativas se mantenham durante muito tempo.[77]
As temperaturas médias durante os meses de inverno variam entre 3 aos 8 graus Celsius, e podem baixar aos 5 °C abaixo de zero. Os meses de junho a setembro têm temperaturas diurnas máximas médias de 28 °C. A temperatura mais alta registada foi 40,5 °C em 12 de julho de 2000; a temperatura mais baixa foi -16,1 °C em 8 de fevereiro de 1927.[78]  Apesar do verão ser a temporada mais seca, a chuva é comum e por vezes ocorrem chuvas intensas semelhantes a monções nesta época do ano. O outono e primavera são temperados e frequentemente húmidos, mas de meteorologia muito imprevisível, havendo dias quentes e dias frios, sendo as noites são quase sempre frias.[78][79]
Em média regista-se precipitação assinalável em 152 dias por ano, que geram 844 mm de chuva.[80] A humidade é muito alta ao longo de todo o ano e pode exacerbar a sensação de calor das temperaturas não muito elevadas de verão, e a sensação de frio em temperaturas não muito baixas de inverno. A humidade é especialmente perceptível de manhã, quando é frequente atingir 80% e o nevoeiro é muito comum, apesar de normalmente se dissipar a meio do dia. Em média há 228 dias de nevoeiro por ano, concentrados sobretudo no inverno.[2]
Istambul tende a ser ventosa, sendo a velocidade média do vento de 18 km/h.[2] Devido à grande dimensão da cidade, à sua topografia e às influências marítimas, Istambul tem diversos microclimas, podendo acontecer estar a nevar em algumas partes e fazer sol noutras, apesar de fazer frio em toda a cidade.[81]
Em 2018, a área metropolitana de Istambul tinha 15 067 724 habitantes.[1] Em 2009 tinha 13 120 596 habitantes, 13 120 596 (50,2%) homens e 6 533 800 (49,8%) mulheres, o que representa 17,8% do total da população da Turquia (73 722 988). A densidade populacional total era 8 229,6 hab./km², variando por distrito desde os 43 831,4 hab./km² de Güngören, a pouca distância do centro histórico do lado europeu, até aos 30,6 hab./km² de Çatalca, o distrito mais a noroeste. Estes distritos são, respetivamente, o menor e o maior em área. As áreas com maior densidade concentram-se sobretudo a oeste, sudoeste e noroeste do centro, estando muito próximos deste. Os distritos históricos de Beyoğlu e Fatih têm, respetivamente, 27 168 e 26 115 hab./km², ocupando a 7ª e 8ª posições entre os distritos com maior densidade. No lado asiático, o distrito com maior densidade é Üsküdar, com 14 760 hab./km², um valor não muito distante dos de Ataşehir e Kadıköy que, como Üsküdar, se situam no extremo sudoeste da parte asiática de Istambul.[85]
Nos últimos 50 anos, imigraram para Istambul mais de dez milhões de pessoas, provenientes de todas as províncias da Turquia.[86] Estima-se que apenas 14% da população atual da cidade tenha raízes locais.[87] O número de residentes em Istambul originários das províncias de Sivas, Sinop, Bayburt, Ardahan, Erzincan, Giresun e Castamonu é superior ao das populações dessas províncias. A maior comunidade de imigrantes internos de Istambul, proveniente da província de Sivas, contava com cerca de 680 000 membros em 2007, enquanto que a população da província era de cerca de 630 000 habitantes. A imigração proveniente de Castamonu ainda é mais drástica: em 2007 viviam em Istambul cerca de 516 000 pessoas provenientes dessa província cuja população não chega aos 360 000 habitantes.[86] Segundo as estatísticas oficias, em 2007 residiam em Istambul 42 228 estrangeiros.[88]
Apesar de, ao longo da sua história, Istambul sempre ter sido uma das maiores cidades da Europa e do mundo, a população triplicou nas últimas três décadas e é atualmente 12 vezes maior do que o que era em 1945, quando viviam na cidade 1 078 000 pessoas. Em 1955 a população já tinha ultrapassado o milhão e meio; em 1990 era 7 309 000 e em 2000 atingiu os dez milhões. Nos últimos anos, todos os distritos e bairros da cidade viram a sua população aumentar exceto Eminönü, no centro histórico, e Şile, um distrito à beira do mar Negro 70 km a nordeste do centro.[86]
A população de Constantinopla / Istambul variou bastante ao longo da sua história. Em 330, no tempo de Constantino, tinha 40 000 habitantes. No fim desse século já tinha 400 000. De 530 a 715 a população decresceu de 550 000 para 300 000; em 950 já tinha crescido para 400 000. Em 1 200 residiam na cidade apenas 150 000 pessoas e após a conquista dos otomanos a população não ultrapassava os 36 000, duplicando nos 20 anos seguintes e atingindo os 600 000 em 1566. O censo de 1817 registava 500 000. A população cresceu durante o século XIX, ultrapassando o milhão de habitantes em 1897, mas voltou a diminuir progressivamente até à década de 1920 (680 587 em 1927), para o que contribui a perda de estatuto de capital e a fuga de gregos e arménios. Desde a década de 1930 que a população não pára de crescer, embora isso também se deva, em parte, à integração administrativa de distritos limítrofes.[87][89][90][91]
Apesar das flutuações de população, a cidade esteve até muito recentemente quase sempre entre a três ou cinco maiores cidades do mundo, desde que Constantino a tornou a sua capital no século IV, inclusivamente nos períodos menos prósperos, entre os séculos XI a XIII. Até essa altura foi a maior cidade da Europa e inclusive do mundo até ao século VIII, quando foi ultrapassada, primeiro por Bagdade e depois por uma ou outra cidade do Extremo Oriente. Após estar ausente do "top" das maiores cidades do mundo nos séculos XIV e XV, em 1500 voltaria a ser a maior cidade da Europa, situação que manteria até cerca de 1750, quando foi ultrapassada por Londres. Em 1600 rivalizava com Pequim como a urbe mais populosa do mundo. Só no último quartel do século XIX deixaria de ser a maior metrópole da Europa. Atualmente é de novo a cidade mais populosa da Europa.[92][93]
Istambul (ou Constantinopla) foi uma cidade muito cosmopolita ao longo da sua história, onde sempre conviveram muitas comunidades estrangeiras e diversas religiões, apesar da sua situação de grande capital, primeiro do Cristianismo e depois do Islão. Constantinopla rivalizou durante mais de um milénio com Roma como capital da Cristandade e mesmo depois da conquista muçulmana continuou a ser a sede da Igreja Ortodoxa. Um dos líderes ortodoxos mais reverenciados do mundo ainda é o Patriarca Ecuménico de Constantinopla, que reside em Istambul. Principalmente após Selim I ter conquistado Meca no início do século XVI, Istambul tornou-se a capital do mundo islâmico e o sultões otomanos eram simultaneamente califas, uma situação que se manteve até aos primeiros meses da república turca.[94]
Atualmente é complicado saber ao certo qual é a "etnia" da maior parte da população da Turquia em geral e de Istambul em particular. É frequente considerar-se que a maior parte da população da Turquia é de "etnia turca" (entre 70[95] e 88%,[96] conforme as fontes). No entanto, atendendo à grande diversidade de povos que habitaram o que é hoje a Turquia desde há milhares de anos, à inevitável miscigenação, ao carácter multiétnico e multicultural do Império Otomano e ao facto de oficialmente todos os cidadãos nacionais serem turcos, não há certezas quanto à verdadeira origem étnica da maioria da população dita "turca", sendo que é provável que uma parte considerável não seja diretamente aparentada com os povos turcomanos asiáticos aos quais se associa historicamente o termo "turco".[96][97]
No período otomano, todos os muçulmanos sunitas eram considerados turcos, mesmo que não falassem turco, enquanto que os não muçulmanos ou muçulmanos não sunitas eram considerados não turcos mesmo que falassem turco.[96] Segundo um estudo levado a cabo pela consultora Konda em 2006, 81,33% da população identificava-se como turca, mas é interessante notar que no mesmo estudo 4,45% identificavam-se como "cidadãos da República da Turquia", apenas 8,61% como curdos (apesar de muitas estimativas apontarem para mais o dobro), e 0,18% yörük (quando as estimativas apontam para cerca de 1%). Muitos membros, senão mesmo a maior parte, das minorias étnicas da Turquia assimilaram a cultura dominante, num processo que em alguns casos foi iniciado no período medieval seljúcida.[97]
A maior parte das minorias religiosas presentes na Turquia, quer muçulmanas, quer de outros credos, principalmente as menos numerosas, concentra-se sobretudo em Istambul. A nível nacional, 70 a 85% da população é muçulmana sunita, principalmente hanafistas, mas também xafiistas (9%).[97] O grupo religioso mais numeroso a seguir aos sunitas são os alevitas,[f]
que constituem entre 15% e 30% da população. Seguem-se os xiitas duodecimanos (em turco: caferilik) não alevitas (entre 0,6% e 4%), mas também há iarsanistas e iazidis. As comunidades sufistas, embora menos importantes que no passado, ainda têm muitos membros[101] e nos últimos anos tem-se assistido a um interesse crescente no misticismo sufista.[99][102] Em 2007 existiam 2 944 mesquitas em Istambul.[103]
A maior minoria étnica de Istambul é a dos curdos, originários das regiões leste e sudeste da Anatólia — algumas fontes estimam que vivam três milhões de curdos em Istambul, ou seja, cerca de 25% da população total da cidade. Istambul é a cidade com mais curdos em todo o mundo e rivaliza em número de curdos com a população das províncias turcas tradicionalmente consideradas "mais curdas", as quais juntas têm pouco mais do que cinco milhões e meio de habitantes.[104] Embora a presença curda na cidade remonte ao início do período otomano[105] e no início do século XX muitos dos porteiros de Istambul serem curdos, o grande afluxo de curdos à cidade acelerou imenso desde o início da guerrilha pela independência do Partido dos Trabalhadores do Curdistão (PKK).[106] A maior parte dos curdos são muçulmanos sunitas, mas há alguns que são iazidis e iarsanistas.[99]
Ainda no período bizantino, instalaram-se na cidade, no que é hoje Beyoğlu, muitos italianos, especialmente das repúblicas marítimas de Génova e Veneza,[107][108][109][110] mas também francos, que estão na origem da comunidade de "levantinos" francófonos, que teve bastante importância na cidade até ao século XX, mas atualmente está praticamente desaparecida. Estas comunidades, como os poucos descentes que delas restam, eram católicas.[109][111]
Outra comunidade importante, tanto em número como em importância económica, que remonta ao período bizantino, é a arménia, que só declinou no século XX, mas ainda perdura e nos últimos anos tem vindo a ser reforçada com imigrantes provenientes da Arménia.[112] Segundo estatísticas oficiais de 2008, viviam em Istambul 45 000 arménios.[113] A esmagadora maioria dos arménios são cristãos ortodoxos e seguidores da Igreja Apostólica Arménia, havendo uma minúscula minoria de seguidores da Igreja Católica Arménia.[99][114]
Os judeus são outra das comunidades presentes na cidade desde tempos históricos muito recuados. Os otomanos foram geralmente tolerantes para com os judeus, uma tradição que remonta aos primeiros beys (príncipes) do século XIII. Uma parte considerável dos sefarditas expulsos de Portugal e de Espanha no fim do século XV foram acolhidos pelos sultões otomanos. O sultão Bajazeto II (1447-1512) foi ao ponto de enviar o seu almirante Quemal Reis a Cádis para recolher judeus espanhóis e levá-los para o império otomano. Mais de 150 000 judeus expulsos de Espanha procuraram refúgio no Império Otomano e a estes juntaram-se alguns anos depois muitos judeus expulsos de Portugal. Além dos sefarditas ibéricos, ao longo dos séculos seguintes, também imigraram para o império, sobretudo para Istambul e outras cidades maiores, dezenas de milhares de asquenazes e caraítas provenientes da Europa de leste.
Antes destas vagas de imigração judaica já havia judeus em Istambul, alguns aí instalados desde o período bizantino. No entanto, a maior e mais influente comunidade judaica da cidade foi e continua a ser a sefardita, a qual representa 96% dos total de judeus turcos.[115] Apesar das gerações mais novas terem como primeira língua o turco, o ladino (judeu-espanhol) ainda é muito usado e é a língua em que os mais velhos são mais fluentes.[116] Ao longo da primeira metade do século XX a população judia da Turquia, a maioria dela de Istambul, manteve-se estável em  cerca de 90 000 pessoas. Após a fundação de Israel iniciou-se uma emigração contínua de judeus turcos para esse novo país. Dependendo das fontes, o número total de judeus na Turquia deverá ser cerca de 20 000.[113][117][g]
À parte da comunidade judia de Esmirna, que se estima ter 2 500 a 3 500 pessoas, quase todos os judeus da Turquia vivem em Istambul.[99][116]
Os gregos, que desde a fundação da cidade foram a comunidade mais importante, perderam  importância após a conquista turca. Apesar disso, só com a república perderam protagonismo na vida da cidade, inclusivamente a nível político — um exemplo que reflete isso é facto do Patriarca de Constantinopla ter sido um membro destacado do sistema político otomano desde o reinado de Maomé II, o Conquistador.[109][110] Em 1924, quando da troca de populações entre a Grécia e a Turquia que se seguiu à guerra de independência turca e ao estabelecimento da república, da qual ficaram excluídos os gregos de Istambul, viviam na cidade 200 000 gregos étnicos. O número foi diminuindo, principalmente após o pogrom de 1955, e em 1995 estimava-se que não restassem mais do que 25 000. Apesar disso, continuavam a ser uma das comunidades mais prósperas.[118] Segundo dados de 2008, estimavam-se entre 3 a 4 mil o número de gregos na Turquia, quase todos a residir em Istambul. Em contrapartida, no mesmo ano havia 60 000 gregos de origem turca residentes na Grécia que ainda mantinham a cidadania turca.[113] À parte de uma minúscula minoria de católicos bizantinos gregos,[119] a esmagadora maioria dos gregos de Istambul são cristãos ortodoxos, à semelhança do que acontecia no passado.[99]
Os limites e divisões administrativas da área metropolitana de Istambul coincidem com as da província homónima desde 2004.[120] Existem 39 distritos (municípios; em turco: ilçeler),[85] os quais, como a generalidade dos distritos turcos, são administrados por um prefeito (belediye başkanı) eleito democraticamente a cada cinco anos e por um governador (kaymakam) nomeado pelo governo central turco.[121] Existe ainda um governador provincial (vali), também nomeado pelo governo central de Ancara. O atual (2011) governador de Istambul é Hüseyın Avnı Mutlu, em funções desde 31 de maio de 2010.[122]
Acima dos municípios "menores", a área metropolitana tem uma administração municipal, a İstanbul Büyükşehir Belediyesi ("Municipalidade Metropolitana de Istambul"), que coordena a gestão dos municípios no sentido de haver coerência nas respetivas políticas e solidariedade entre os municípios.[123] Por exemplo: os orçamentos aprovados por cada distrito podem ser emendados pelo conselho municipal metropolitano. Além disso, a "municipalidade metropolitana" tem a seu cargo a gestão de serviços comuns, como transportes, saneamento, obras de maior envergadura e regulamentação de alguns aspetos da vida da cidade, nomeadamente taxas.[124]
Além do prefeito, os dois organismos superiores da Municipalidade Metropolitana são o Belediye Meclisi (Conselho ou Assembleia Municipal) e o Encümen (Comité Executivo; lit: Conselho da Cidade). O primeiro é um órgão colegial presidido pelo prefeito, onde um quinto dos seus membros é composto por representantes dos municípios distritais e os restantes (onde se inclui o prefeito) são eleitos de cinco em cinco anos. O Encümen é composto por pessoas escolhidas pelo presidente eleito da Municipalidade Metropolitana, presidido por este ou por alguém por ele designado; é um órgão eminentemente executivo, embora tenha algum poder de decisão e também atue como conselho consultivo da município.[124]
O atual (2011) prefeito de Istambul é Kadir Topbaş, militante do CHP.[125] A sede do município metropolitano de Istambul ocupa um edifício construído entre 1953 e 1960 no bairro de Saraçhane, no distrito de Fatih. Em 2001 foi lançado um concurso de arquitetura para a construção de uma nova sede em Caglayan, no distrito de Kâğıthane, o qual foi ganho pela empresa do arquiteto istambulita Emre Arolat.[126]
Pode dizer-se que Istambul tem três centros devido à divisão criada pelo Bósforo e pelo Corno de Ouro: um na península a sul do Corno de Ouro onde foi fundada Bizâncio, cujo principal marco é Sultanahmet, outro na outra margem do Corno de Ouro, cujo principal marco é a Praça Taksim, e outro em Üsküdar e Kadıköy, no lado oriental do Bósforo. Os distritos dos centros históricos são principalmente: Fatih, que grosso modo ocupa o que foi a capital bizantina, Beyoğlu, Üsküdar e Kadıköy. Zeytinburnu, a extremidade sul de Eyüp, Beşiktaş, Şişli e os distritos envolventes, todos na parte europeia, também têm grande concentração de bairros antigos.[127]
Alguns distritos mais distantes dos centros são de carácter mais rural do que propriamente urbano, como se pode inferir da sua baixa densidade populacional. Exemplos disso são principalmente Çatalca e Silivri na extremidade ocidental, Arnavutköy a noroeste e Şile na extremidade nordeste, e Adalar, uma arquipélago na parte leste do mar de Mármara, mas também Çekmeköy, na parte asiática. Beykoz só é densamente urbanizado na extremidade sul junto ao Bósforo. Eyüp e Sarıyer, a norte do centro europeu, têm zonas densamente urbanizadas (o último nem tanto, concentrando-se sobretudo nas margens do Bósforo e uma outra zona junto ao mar Negro) e grandes áreas de terreno não urbanizado, nomeadamente de floresta. Algo semelhante se passa com Pendik e Tuzla, cujas áreas mais a norte são pouco povoadas, ao contrário das áreas mais a sul.[85][128] Além dos distritos, há centenas de bairros em Istambul, alguns deles conhecidos internacionalmente e com algum tipo de personalidade administrativa.[127]
Istambul tem acordos de geminação e/ou de cooperação com mais de cinquenta cidades de todos os continentes. A primeira cidade gémea de Istambul foi o Rio de Janeiro, cujo protocolo respetivo data de 1965.[129]
Além de ser a maior cidade e a antiga capital do país, Istambul foi sempre o centro da vida económica da Turquia, para o que contribui em grande medida a sua localização numa encruzilhada de rotas comerciais internacionais terrestres e marítimas. A metrópole é igualmente o maior centro industrial e financeiro da Turquia, onde estão cerca de 20% dos empregos na indústria e 38% da área industrial do país (dados de 2000). Em 2000 Istambul era responsável por 55% do comércio, 45% do comércio grossista, 21% do PIB e 27,5% do PNB da Turquia. No mesmo ano, 40% das receitas de impostos cobrados a nível nacional provinham de Istambul.[135] Em 2006, a cidade era responsável por 27,5% do consumo nacional e nela estavam sediados 35% dos depósitos bancários; 20% das dependências bancárias turcas encontram-se em Istambul.[136]
Em 2006 o PIB de Istambul foi de 133 mil milhões * de dólares US, mais do triplo do que Ancara e mais elevado do que o de grandes cidades mundiais como Berlim, Pequim ou Singapura. A cidade foi considerada a 34ª mais rica do mundo pela consultora PricewaterhouseCoopers.[137] Em 2005 as empresas baseadas em Istambul exportaram bens no valor de 41,397 mil milhões *US$ e importaram 69 883 mil milhões US$, o que correspondeu, respetivamente, a 56,6% e 60,2% do total da Turquia nesse ano.[138] A distribuição do rendimento entre a população é muito pouco uniforme. Em 1994, 20% dos mais ricos usavam 64% dos recursos, enquanto que os 20% mais pobres usavam apenas 4%.[136] De acordo com a revista Forbes, Istambul tinha 35 bilionários em março de 2008, o que a colocava como quarta cidade do mundo com mais bilionários.[139] Em 2007 havia 43 centros comerciais em Istambul e estavam em construção mais 35, além de mais 30 em fase de projeto.[140]
Entre os setores industriais mais relevantes podem destacar-se os de processamento de alimentos, bebidas alcoólicas, têxtil, química petroquímica, borracha, metalurgia, curtumes, indústria farmacêutica, eletrónica, vidro, maquinaria, indústria automobilística e de veículos de transporte, papel e produtos de papel. As principais produções agrícolas da província são algodão, fruta, azeite, seda e tabaco.[135]
O turismo é uma atividade económica de grande importância na cidade. Há centenas de hotéis (em 2006 estavam registados 338 estabelecimentos hoteleiros) e milhares de empresas ligadas ao turismo.[141][142] Em 2000 a cidade foi visitada por cerca de 1 750 000 turistas[143] e em 2006 cerca de cinco milhões de estrangeiros entraram na Turquia pelos dois principais aeroportos de Istambul — o Aeroporto Atatürk (substituído em 2019 pelo novo Aeroporto de Istambul)[144] e o Aeroporto Sabiha Gökçen.[145] Em 2007 estavam em construção ou em projeto 30 hotéis de cinco estrelas e apesar da abundante oferta hoteleira já então existente, estimava-se que os novos hotéis não fossem suficientes para satisfazer a procura nos anos seguintes.[140]
A única bolsa de valores da Turquia, a İstanbul Menkul Kıymetler Borsası (İMKB; em inglês: ISE), tem a sua sede em Istambul. Embora a sua origem remonte a 1866, quando foi criado o Dersaadet Tahvilat Borsası[146] e desde essa altura sempre tenha havido uma bolsa na cidade, a importância passou a ser residual depois da depressão de 1929. Nos anos 1980 o governo turco decidiu reavivar o mercado de valores mobiliários[146] e em 1985 foi criada a instituição atual. Desde 1995 que a sede é no bairro de İstinye, situado a norte da Ponte Fatih Mehmet.[147]
Durante o século XIX e o início do XX, o centro financeiro do Império Otomano foi a Bankalar Caddesi (Rua dos Bancos, também conhecida por Rua Voyvoda), em Gálata. Aí se situavam as sedes das principais instituições financeiras, nomeadamente a bolsa e o banco central otomano, fundado em 1856 com o nome de Bank-ı Osmanî (Banco Otomano) e posteriormente reorganizado e rebatizado de Bank-ı Osmanî-i Şahane (Banco Imperial Otomano) em 1863.[107][108][148] A Bankalar Caddesi continuou a ser o centro financeiro de Istambul até à década de 1990, quando a maior parte dos bancos turcos transferiram as suas sedes para os bairros modernos de Levent e Maslak.[107]
A beleza e situação estratégica do local onde a cidade se encontra, a sua rica e extensa história, patente nos inúmeros monumentos, a mistura única de Ocidente e Oriente, decorrente não só da sua localização entre dois continentes, mas também da cultura dos seus habitantes do presente e do passado, a vida intensa nas suas ruas e praças, o contraste e harmonia entre o facto da cidade ser simultaneamente uma metrópole europeia e asiática, onde se encontram locais onde se tem a sensação de que não devem ter mudado muito desde há centenas de anos até às zonas mais modernas que não destoariam em qualquer grande capital europeia ocidental, desde aquilo que foi a maior igreja da Cristandade durante mil anos até aos arranha-céus e centros comerciais mais modernos, passando por muitas dezenas de monumentos otomanos que durante séculos inspiraram os artistas ocidentais pelo seu exotismo e luxo, tudo contribui para fazer de Istambul um destino turístico de eleição para milhões de turistas que visitam anualmente a cidade.[60][149]
Uma parte considerável da chamada península histórica de Istambul, situada no distrito de Fatih e mais ou menos centrada em Sultanahmet, está classificada como Património Mundial pela UNESCO desde 1985.[149] A cidade foi também Capital Europeia da Cultura em 2010.[150]
Dada a enorme riqueza monumental, cultural e paisagística de Istambul, é muito complicado encontrar critérios minimamente imparciais e objetivos para enumerar os monumentos e locais turísticos que se podem considerar "mais importantes". A lista que se segue é assumidamente e inevitavelmente incompleta e foi baseada principalmente nos guias Lonely Planet de Istambul (edição de 2002)[127] e Rough Guide da Turquia (edição de 2003).[151]
Um ponto obrigatório de qualquer visita turística em Istambul é a área de Sultanahmet, onde se situava o centro da Bizâncio romana e da Constantinopla bizantina e otomana. A praça tem o nome do sultão Amade I (1590-1617), que ali construiu a grande mesquita que também tem o seu nome (mais conhecida como Mesquita Azul), ocupa o lugar do hipódromo romano. Na mesma área situava-se uma das alas do grandioso Grande Palácio dos imperadores bizantinos, do qual apenas restam algumas ruínas postas a descoberto por escavações e alguns mosaicos de grandes dimensões em exposição no Museu dos Mosaicos do Grande Palácio, situado numa das ruas em volta da praça. A praça é dominada pela Mesquita Azul no lado sudoeste e pela Basílica de Santa Sofia a nordeste. Ao lado da mesquita situa-se o que era o hipódromo, onde se encontra o Museu de Arte Turca e Islâmica e se erguem dois obeliscos, um deles, o chamado Obelisco de Teodósio, trazido do Templo de Carnaque, no Egito. Entre a mesquita e Santa Sofia encontram-se diversos monumentos importantes, como por exemplo, o türbe (mausoléu) de Amade I, onde também se encontram sepultados outros membros da família imperial otomana, e os Banhos de Roxelana (século XVI). Numa das extremidades encontra-se a mais impressionantes das muitas cisternas bizantinas da cidade, a Cisterna da Basílica, cujo nome em turco denuncia a sua grandeza: Yerebatan Sarayı (Palácio Subterrâneo).[152]
A Basílica de Santa Sofia, construída no século VI, foi a maior igreja do mundo até 1453, quando foi transformada em mesquita. Se tivesse continuado a ser igreja, continuaria a ser a maior do mundo até 1590, quando foi terminada a cúpula da Basílica de São Pedro, em Roma, a qual é pouco maior que Santa Sofia. A basílica não impressiona apenas pela sua dimensão, mas por toda a sua arquitetura, iluminação, mosaicos bizantinos, as adições otomanas enquanto foi mesquita e ao facto de ter resistido aos inúmeros sismos que assolaram Istambul e que destruíram várias construções muito posteriores.[153]
Atrás de Santa Sofia e separado pela pitoresca Soğukçeşme Sokağı (Rua da Fonte Fria) encontra-se o vasto Palácio de Topkapı, a sede do poder otomano durante quase quatro séculos. Além da arquitetura, o palácio tem diversas exposições museológicas de grande valor, jardins e vistas privilegiadas sobre toda cidade, o Corno de Ouro, o Bósforo e o mar de Mármara. O palácio é parcialmente rodeado pelo Parque Gülhane, onde também se encontram os Museus Arqueológicos de Istambul, que além de de incluir um dos maiores museus arqueológicos do mundo, tem também dois museus de antiguidades orientais e arte islâmica.[154]
No emaranhado de ruas próximas de Sultanahmet, são de referir duas obras-primas da arquitetura de Istambul: a Mesquita de Sokollu Mehmet Paşa, construída em 1572 pelo grande arquiteto otomano Sinan, autor de algumas da mesquitas mais famosas da cidade, e a Igreja de São Sérgio e São Baco. Esta é mais conhecida como Pequena Santa Sofia (em turco: Küçuk Ayasofya) e foi construída alguns anos antes da Basílica de Santa Sofia e foi um modelo para ela. No início do século XVI foi transformada em mesquita, função que ainda mantém.[155][156]
No distrito de Fatih encontram-se uma série de grandes mesquitas imperiais além da Mesquita Azul. Seguindo pela antiga Estrada Imperial, atual Divan Yolu, passa-se ao lado do türbe de Mamude II e encontra-se a Coluna de Constantino, na mesma praça da Mesquita de Nuruosmaniye e de uma das entradas do Grande Bazar (Kapalıçarşı), um dos maiores e mais antigos mercados cobertos do mundo. Continuando na mesma rua, encontra-se o türbe de Sinan e a grande praça de Beyazıt, onde se encontra a mesquita homónima (1506) e a Universidade de Istambul. Um pouco mais acima, atrás da universidade, situa-se a maior mesquita da cidade (desde que Santa Sofia foi convertida em museu), a Mesquita Süleymaniye (1557), outra obra de Sinan para Solimão, o Magnífico.[157]
Um pouco mais longe, encontra-se o troço mais imponente das lendárias Muralhas de Constantinopla, o trecho mandado construir por Teodósio II entre a costa do mar de Mármara e o Corno de Ouro, que fechava a cidade romana e bizantina pelo lado ocidental; os outros lados estavam rodeados por mar e eram protegidos pela chamada muralha marítima, da qual ainda resistem muitos troços ao lado da Avenida Kennedy, ao longo da margem do mar de Mármara. A maior fortificação das muralhas atualmente ainda existente é a Fortaleza de Yedikule (Castelo das Sete Torres), situada na antiga Porta Aurea junto ao mar de Mármara.[158]
Além dos monumentos bizantinos já citados, destacam-se a Igreja de São Salvador em Chora (Mesquita Kariye)[159] e a Igreja de Pammakaristos (Mesquita de Fethiye), notáveis pelos seus frescos e mosaicos, os mais espetaculares a seguir aos de Santa Sofia. Igualmente digno de nota é o antigo Mosteiro de Cristo Pantocrator (Mesquita de Zeyrek).[160]
Entre as mesquitas otomanas famosas contam-se todas as construídas por Sinan, nomeadamente a de Mihrimah Sultan em Üsküdar, no lado asiático, a sua homónima em Edirnekapı e a de Rüstem Paşa, em Eminönü, junto a outra das atrações turísticas da cidade, o Bazar das Especiarias (ou Egípcio) e de outra grande mesquita, a Yeni(1665). De referir ainda a Fatih (1463; reconstruída em 1771), a de Laleli (1783) e a de Ortaköy (1856). Esta última situa-se num dos locais mais populares para passeios à beira-mar junto à Ponte do Bósforo.[161]
Os diversos palácios imperiais à beira do Bósforo no século XIX são outro dos atrativos arquitetónicos da cidade. O maior é o de Dolmabahçe, mas a grandeza e beleza de, por exemplo, os de Çırağan (este transformado num hotel de luxo), Yıldız, Küçüksu e Beilerbei, não é muito inferior.[161]
Em 2007 existiam em Istambul 4 350 escolas não superiores (comuns e profissionais), cerca de metade delas primárias. O número de professores nessas escolas ascendia a 90 784 e o de estudantes a 2 991 320 (c. 25% da população). No total havia 59 238 salas de aula. O número médio de alunos por escola era de 688, o de alunos por professor 33 e de alunos por sala de aula 50. Nos últimos anos houve um grande reforço das infraestruturas de educação — por exemplo: de 2000 para 2007 quase duplicaram o número de professores e de salas de aula e o número de alunos aumentou mais de 60%.[162]
Istambul tem um grande número de universidades, oito estatais e mais de vinte privadas, a maior partes destas últimas criadas nos últimos anos. Entre elas encontram-se algumas das mais prestigiadas na Turquia, como a Universidade de Istambul (Istambul Üniversitesi) e a Universidade Técnica de Istambul (İstanbul Teknik Üniversitesi), entre outras. Além das universidades, há pelo menos sete escolas superiores profissionais importantes[163] e duas academias militares.
A Universidade de Istambul foi criada oficialmente em 1933, mas é sucessora da Darülfünun-i Osmani ("casa das múltiplas ciências"), fundada em 1846[164] e de uma série de instituições de cariz universitárias que se lhe seguiram, considerando alguns historiadores que se pode também considerar herdeira de uma longa tradição de escolas islâmicas (madraçais) de cariz universitário que remontam ao século XIV.[165] A Universidade Técnica de Istambul foi oficialmente fundada em 1944, mas é a sucessora de um dos mais antigo estabelecimentos de ensino superior do mundo dedicado ao ensino da engenharia, a Mühendishane-i Bahr-i Humayun (Escola de Engenheiros Navais), fundada em 1773.[166][167]
Entre outras universidades com muito prestígio e com história mais longa podem citar-se a Universidade do Bósforo (Boğaziçi Üniversitesi), que existe como universidade desde 1971 mas teve origem no Robert College, uma escola americana fundada em 1863;[168] a Universidade de Belas Artes Mimar Sinan (Mımar Sınan Güzel Sanatlar Üniversitesi), fundada em 1881;[169] a Universidade de Mármara, fundada em 1883;[170] e a Universidade Técnica de Yıldız (Yildiz Teknık Üniversitesi), criada em 1911.[171] A Universidade Galatasaray foi fundada em 1992, mas a existência da escola que lhe deu origem, o Liceu Galatasaray, remonta a 1481, quando o sultão Bajazeto II fundou um enderun, ou seja, uma escola para formação de funcionários da corte imperial.[172]
Em Istambul estão também sediadas a Academia Naval Turca (Deniz Harp Okulu) e a Academia da Força Aérea Turca (Hava Harp Okulu). As primeira partilha as suas origens com a Universidade Técnica de Istambul, isto é com a escola de engenharia naval fundada em 1773.[173] A Academia da Força Aérea foi criada em Istambul em 1912 como "Escola Aérea do Exército" e depois de entre 1926 e 1967 ter estado sediada em Esquiceir, voltou a Istambul.[174]
A cidade tem inúmeras unidades de saúde, tanto estatais como privadas, entre hospitais, clínicas, laboratórios e unidades de investigação médica. Muitas destas unidades dispõem de equipamento de alta tecnologia, o que tem contribuído para um recente crescimento do chamado turismo médico,[175] com origem sobretudo em países da Europa Ocidental, onde os serviços de assistência médica governamentais como os do Reino Unido e Alemanha enviam pacientes com menores rendimentos para serem tratados em Istambul devido ao custo mais baixo dos tratamento que envolvem alta tecnologia.[176] Entre as áreas mais procuradas encontram-se a cirurgia oftalmológica a laser e cirurgia plástica.[175][a]
Há problemas de saúde relacionados com a poluição, especialmente no inverno, devido ao uso de combustíveis para aquecimento. O número crescente de automóveis e a lentidão do desenvolvimento dos transportes públicos é causa de ocorrências frequentes de smog. O uso obrigatório de combustíveis sem chumbo só entrou em vigor em 2006.[95][a]
Os primeiros sistemas de abastecimento de água datam da fundação da cidade no século VII a.C. No tempo dos romanos existiam dois aquedutos principais: o de Mazulkemer e o de Valente (este último ainda é um dos grandes monumentos da Antiguidade de Istambul). Estes aquedutos levavam água recolhida na área de Halkalı, no distrito de Küçükçekmece, a oeste do centro da cidade antiga, até à zona do Fórum de Teodósio (ou Taurino; atualmente a Praça Beyazıt); a água era depois recolhida em numerosas cisternas, algumas delas transformadas em atrações turísticas atualmente, como a da Basílica (Yerebatan Sarayı), a de Teodósio (Şerefiye Sarnıcı) ou a de Filoxeno (Binbirdirek Sarnıcı).[a]
No século XVI, o arquiteto e engenheiro da corte Mimar Sinan foi encarregado pelo sultão Solimão, o Magnífico de melhorar o sistema de abastecimento de água da cidade, para o que construiu os sistema de abastecimento de água de Kırkçeşme em 1555. Ao longo dos anos, a água de várias fontes, nomeadamente da Floresta de Belgrado, onde foram construídas várias barragens desde o tempo de Solimão, foi canalizada para fontes da cidade.[177][178][a] Atualmente Istambul dispõe de uma rede de tratamento de água potável e de esgotos gerida pela agência İSKİ da municipalidade metropolitana, a qual emprega cerca de 6 800 funcionários. Também há diversas empresas privadas que distribuem água potável.[179][a]
A eletricidade é distribuída pela empresa estatal Türkiye Elektrik İletim (TEİAŞ). A primeira central termoelétrica da Turquia, a Silahtarağa Santral foi construída em Istambul em Eyüp, no cimo do Corno de Ouro em 1914 e funcionou até 1983; atualmente é o complexo cultural e universitário de Santralistanbul.[180]
O primeiro serviço moderno de correios e telecomunicações do Império Otomano, o Ministério dos Correios e Telégrafos (Posta ve Telgraf Bakanlığı),  foi criado em Istambul em 1840.[181] A primeira patente do telégrafo de Samuel Morse foi emitida pelo sultão Abdul Mejide I, que testou pessoalmente o aparelho no antigo Palácio de Beilerbei em 1847.[182] Em 9 de agosto de 1847 foi instalada a primeira linha de telégrafo entre Istambul e Edirne.[183]
A primeira estação de correios foi a Postahane-i Amire, junto à Mesquita Yeni. Em 1876 foi criada a primeira rede de correios entre Istambul e o resto do vasto Império Otomano e o estrangeiro. Em 1881 foi inaugurada a primeira linha de telefone entre aquela a Postahane-i Amire e a sede do Ministério dos Correios e Telégrafos, em Soğukçeşme Sokağı, perto de Sultanahmet. Em 1901 foram efetuadas as primeiras transferências de dinheiro pelos correios. No mesmo ano ficaram operacionais os primeiros serviços de encomendas. Em 1909 foi inaugurado o serviço telefónico público, com a instalação de 50 linhas na estação de correios de Büyük Postane, em Sirkeci.[181][183]
Os transportes públicos de Istambul são geridos pela empresa municipal İstanbul Elektrik Tramvay ve Tünel (IETT, "Serviços de Bondes e do Tünel"),[184] sucessora da primeira empresa de transportes públicos da cidade, a Dersaatet Tramvay Şirketi, fundada em 1869.[185]
Em 2010 a IETT tinha uma frota de 2 768 autocarros, os quais percorriam diariamente cerca de 448 000 km em 468 linhas e 7 889 paragens.[186] Há também algumas empresas privadas de autocarros, cuja atuação é controlada pela IETT.[184][187] Além dos serviços convencionais de autocarros, a cidade Istambul dispõe de um serviço de Bus Rapid Transit, o Metrobüs,[188] duas linhas de metropolitano de superfície,[189] e uma linha de metropolitano,[190] e duas linhas modernas de elétricos rápidos,[191][192] duas linhas ferroviárias suburbanas
Outros serviços menores de transporte público são duas linhas de elétricos ditos "nostálgicos", que usam veículos antigos, uma na Avenida İstiklal, e outra em Kadıköy; os teleféricos de Maçka (Beşiktaş) e "Pierre Loti" (em Eyüp, na parte superior do Corno de Ouro); e os funiculares de Taksim-Kabataş e do Tünel.[193] Este último é uma das linhas ferroviárias urbanas mais antigas do mundo, tendo sido inaugurado em 1875 na encosta de Beyoğlu entre o fundo da Avenida İstiklal e as proximidades da Ponte de Gálata.[194]
Tanto a Estação de Sirkeci como a de Haydarpaşa, construções monumentais do século XIX, são igualmente importantes terminais de comboios de longo curso domésticos e internacionais. Ver também: Transporte ferroviário na Turquia.
Em 29 de outubro de 2013, 90º aniversário da proclamação da República Turca, foi parcialmente inaugurada a primeira ligação ferroviária subterrânea entre dois continentes, ligando a Ásia e a Europa, parte do projeto Marmaray, uma linha de 14 km de extensão, grande parte dela debaixo do fundo do estreito do Bósforo e do mar de Mármara.[195]
Os transportes marítimos são uma componente importante da infraestrutura de transportes públicos da cidade. Há inúmeras linhas de ferryboats que cruzam o Bósforo, o mar de Mármara e o Corno de Ouro, as quais servem tanto os locais como os turistas (os passeios no Bósforo são muito populares). A maior parte dos ferries, os chamados vapur, são convencionais, mas há também serviços de catamarãs rápidos, os deniz otobüsü (autocarros marítimos),[196] e os chamados "táxis marítimos" (deniz taksi).[197]
Istambul tem dois aeroportos internacionais de grandes dimensões, que recebem milhões de turistas todos os anos. O Aeroporto de Istambul (IATA: IST, ICAO: LTFM), inaugurado em 2018,[198] substituiu o Aeroporto Atatürk (ISL, LTBA) em 2019.[144] Este último teve um movimento de quase 68 milhões de passageiros em 2018.[199]
O novo aeroporto, situado no distrito de Arnavutköy, no lado europeu, cerca de 40 km a noroeste do centro histórico (Sultanahmet), tem capacidade para 150 milhões de passageiros por ano, tendo sido projetado para ser o maior aeroporto do mundo, podendo servir 200 milhões de passageiros em alguns anos.[144]
O Aeroporto Sabiha Gökçen (SAW, LTFJ), situado em Pendik, na parte oriental do lado asiático, a 35 km do centro da cidade, foi inaugurado em 2001. Grande parte do tráfico doméstico e de companhias de baixo custo de Istambul passa atualmente por este aeroporto. O novo terminal de Sabiha Gökçen, construído em 2003, era o maior edifício com construção anti-sísmica do mundo na data em que foi concluído.[200]
Istambul é considerada a capital cultural da Turquia e também a cidade turca onde a presença da cultura ocidental é mais forte. Para a riqueza cultural da cidade contribui decisivamente a tradição de encontro e fusão entre Ocidente e Oriente, presente desde praticamente a fundação da cidade — por exemplo, um dos mercados de livros mais antigos do mundo é o Sahaflar Çarşısı, o qual existe no mesmo local desde o período bizantino. Este mercado encontra-se na zona de Beyazıt, perto do local onde se erguia o Fórum de Teodósio e ali se encontram muitos livros históricos e raros.[201] Todos estes fatores estiveram na base da escolha de Istambul como Capital Europeia da Cultura em 2010, juntamente com Pécs, na Hungria, e Essen, na Alemanha.[202][a]
A cidade acolhe com frequência estrelas pop internacionais, as quais chegam a encher estádios, e há espetáculos de ópera, jazz, ballet e teatro, tanto de produções nacionais como estrangeiras, muitas vezes com salas repletas. Há diversos festivais culturais sazonais com relevância internacional,[203] como por exemplo o Festival Internacional de Cinema de Istambul e a Bienal de Istambul, uma exposição de arte contemporânea, ambos realizados pela İstanbul Kültür Sanat Vakfı (İKSV), Fundação Para as Artes e Cultura de Istambul, a qual organiza também festivais internacionais de design, teatro, música, ballet, dança contemporânea, ópera, jazz, música tradicional e pop, etc..[204][a]
O principal centro cultural estatal é Centro Cultural Atatürk, sediado num dos edifícios que domina a Praça Taksim, que além de dispor de duas salas de concerto onde têm lugar espetáculos de música, ópera e ballet, nele estão instalados duas orquestras, uma sinfónica e outra de música popular, e um coro e orquestra de música clássica turca. A cidade conta com pelo menos 15 centros culturais importantes, 11 salas de concerto de grande qualidade e centenas de galerias de arte.[205]
Um dos centros culturais mais modernos é o Santralistanbul, o qual inclui um museu de arte moderna, um museu de energia, um anfiteatro, uma sala de concertos e uma biblioteca pública. O complexo está instalado naquilo que foi a primeira central eléctrica da Turquia e está integrado no campus da Universidade Istanbul Bilgi.[206]
A sala principal de ópera é a do Centro Cultural Atatürk, mas em Kadıköy existe desde 1927 a Ópera Süreyya, a qual voltou a reabrir como teatro de ópera em 2007, após ter sido restaurada e antes ter ter sido usada durante décadas como sala de cinema. Aí está sediada a secção de Istambul da Ópera e Ballet do Estado da Turquia (Devlet Opera ve Balesi).[207][208]
Uma das melhores salas de concerto de Istambul é a Cemal Reşid Rey Konser Salonu, inaugurada em 1989 e batizada em homenagem a um dos maiores compositores turcos do século XX, Cemal Reşid Rey (1904-1985).[209] É frequente a organização de concertos e outros espectáculos ao vivo em espaços culturais, como a Igreja de Santa Irene (Hagia Irene), onde no verão decorrem festivais de jazz e música clássica, nos pátios do Palácio de Topkapı, no Parque Gülhane e nos castelos de Rumelihisarı e Yedikule.[210] Um dos objetivos anunciados para o programa de "Istambul, Capital Europeia da Cultura 2010" era o início da construção de uma ópera da autoria do arquiteto americano Frank Gehry.[211]
Há duas orquestras sinfónicas sediadas em Istambul. A Orquestra Sinfónica Estatal de istambul (İstanbul Devlet Senfoni Orkestrası) foi fundada em 1945, mas reclama ser herdeira da orquestra fundada em 1827 pelo músico e compositor italiano  Giuseppe Donizetti, irmão de Gaetano, que foi o "instrutor geral" de música na corte imperial otomana ao serviço do sultão Mamude II desde 1828 até à sua morte em 1856.[212] A Orquestra Filarmónica Borusan Istambul (Borusan İstanbul Filarmoni Orkestrası) foi fundada em 1999 a partir da já existente Orquestra de Câmara Borusan; é um dos diversos organismos da Borusan Kültür Sanat (Borusan Cultura e Arte), uma fundação do grupo económico Borusan que tem em Istambul outras estruturas culturais, como orquestras, uma sala de concertos, uma editora, uma biblioteca, etc..[213]
Istambul tem inúmeros museus e aqui apenas se mencionam alguns dos mais notórios. O İstanbul Modern é um museu de arte contemporânea dedicado sobretudo a obras de artistas turcos, mas que também organiza exposições de artistas estrangeiros. O  Museu de Pera é famoso pelas suas coleções de azulejos e de arte orientalista. O Museu Sakıp Sabancı tem vastas coleções de porcelanas chinesa e europeia, mobiliário, caligrafia islâmica e pintura, sobretudo de obras orientalistas de pintores otomanos e europeus que viveram no Império Otomano. O Museu Doğançay é dedicado principalmente à obra do seu fundador, o pintor impressionista turco Burhan Doğançay, e do seu pai Adil Doğançay. O Museu Rahmi M. Koç é um museu industrial que tem em exposição diverso equipamento industrial, desde automóveis e locomotivas do século XIX e início do século XX até barcos, submarinos, aviões e outras máquinas antigas.
Os Museus Arqueológicos de Istambul, uma instituição fundada em 1881 tem no seu acervo mais de um milhão de peças arqueológicas da Bacia do Mediterrâneo, Balcãs, Médio Oriente, África do Norte e Ásia Central. Além do museu arqueológico propriamente dito, um dos maiores do mundo no seu género, o complexo inclui o Museu de Antiguidades Orientais e o Museu do Quiosque Esmaltado, este último alojado num pavilhão mandado construir por Maomé II, o Conquistador em 1473, que tem em exposição obras de arte islâmica, sobretudo seljúcida e otomana.
O Museu dos Mosaicos do Grande Palácio abriga mosaicos do Grande Palácio de Constantinopla, a sede imperial do Império Bizantino. O Museu de Arte Turca e Islâmica tem uma coleção de mais de 40 000 peças de arte islâmica que vai desde o Califado Omíada até à atualidade, e que inclui obras de caligrafia, uma importante coleção de tapetes e um conjunto de peças etnográficas de povos turcos. O Museu Sadberk Hanım tem duas secções, uma dedicada à arqueologia, com peças das civilizações anatólias, jónias, helénicas, romana e bizantina, e outra de história da arte, com peças de arte islâmica turca e persa, cerâmica e porcelana chinesa e turca, vidro, sedas, bordados, vestuário e outros artefactos etnográficos.
Os diversos ex-palácios imperiais estão quase todos transformados em grandes museus. Quiçá os mais grandiosos e famosos sejam o Topkapı, o centro do poder otomano durante quatro séculos, e o Dolmabahçe, construído no século XIX, mas os de Beilerbei, Küçüksu, Yıldız e Çırağan (este transformado num hotel de luxo), entre outros, são também impressionantes pela sua beleza e pela localização — todos se encontram à beira do Bósforo à exceção do de Yıldız.
A maior parte dos media e da indústria editorial estão baseados em Istambul e a generalidade dos que não estão têm delegações, edições ou emissões na cidade.[214][215] Os jornais mais populares da Turquia em 2008 (Hürriyet, Milliyet, Posta, Sabah, Yeni Asir e Zaman) estão todos sediados em Istambul, o mesmo acontecendo com os dois jornais considerados mais prestigiados, o Milliyet e o Cumhuriyet.[216]
Os dois jornais de maior circulação no início de 2011 foram o Zaman e o Posta, com tiragens de, respetivamente, 826 000 e 485 024.[217] São também publicados diversos jornais e revistas em inglês e outras línguas. Entre os jornais em língua estrangeira com maior difusão encontram-se o Today's Zaman e o Hürriyet Daily News, ambos em inglês.[216]
O maior grupo de média da Turquia, o Doğan tem a sua sede em Istambul. É o quinto maior grupo económico turco e detém três estações de televisão com cobertura nacional e dois jornais de grande circulação que em 2008 se estimava que constituíssem cerca de 50% do mercado.[216]
Os desportos de massas em Istambul têm uma tradição que remonta pelo menos ao período romano. Durante esses tempos e os do Império Bizantino que se lhe seguiu, as corridas de quadrigas que se realizavam no Hipódromo de Constantinopla chegavam a ter mais de 100 000 espetadores[218] e a competição era tão renhida que as equipas tinham um peso político considerável e não raro davam origem a motins, como a Revolta de Nika, em 532, durante a qual uma parte da cidade e a recém ampliada Basílica de Santa Sofia foi destruída.[219]
Como na generalidade da Turquia, atualmente o desporto mais popular em Istambul é o futebol. Os jogos das principais equipas são motivos de grandes festas de rua, que não raro começam logo na véspera e provocam grandes enchentes de multidões cantando, abraçando-se e buzinando, que resultam em grandes congestionamentos de tráfego, sobretudo na área de Beyoğlu em geral e na Praça Taksim em particular.[220] Na cidade estão sediados pelo menos cinco dos mais importantes clubes de futebol turcos: o Galatasaray S.K, o Beşiktaş J.K, o Fenerbahçe S.K, o Kasımpaşa S.K e o İstanbul Başakşehir F.K (chamado İstanbul Büyükşehir até 2014). Os primeiros três quase invariavelmente disputam os lugares cimeiros de todas as competições nacionais e é comum estarem presentes em competições internacionais.[221][222]
Outros desportos relevantes em Istambul são basquetebol, voleibol. Além das equipas dessas modalidades dos grandes clubes de futebol, são de destacar, por exemplo, o Anadolu Efes S.K, detentor de vários títulos europeus em basquetebol[223] e, em voleibol, o Eczacıbaşı[224] e o VakıfBank Güneş Sigorta.[225]
O autódromo Istanbul Park acolhe diversos eventos internacionais de desporto automóvel, como o Grande Prémio de Fórmula 1 da Turquia,[226] provas dos campeonatos mundiais de motociclismo da FIM,[227] carros de turismo da FIA,[228] e GP2.[229]
A importância do desporto a nível internacional e na vida da cidade é evidenciada, por exemplo, pela escolha de Istambul para Capital Europeia dos Desportos.[221][222] e pela candidatura da cidade à organização dos Jogos Olímpicos de 2020.[230]
Bombaim[1][2] ou, oficialmente, Mumbai (em marata: मुंबई ; romaniz.:  Mumbaī; em inglês: Mumbai ou Bombay) é a maior e mais importante cidade da Índia. Conta com uma população estimada em 12 478 447 habitantes (2011)[3] residindo apenas em seu núcleo urbano, ou 20 748 395, se considerarmos sua região metropolitana, conhecida como Grande Bombaim, a segunda maior do país[4] — atrás apenas de Grande Deli — e a 4.ª mais populosa do mundo. Capital e maior cidade do estado de Maarastra, situa-se nas margens do oceano Índico.
As sete ilhas que vieram a constituir Bombaim foram habitadas por nómadas que tinham a pesca como a sua principal fonte de sobrevivência. Durante séculos, as ilhas estiveram sob o controle de sucessivos impérios indianos, antes de serem cedidas aos portugueses que depois as cederam aos britânicos, quando passou a ser controlada pela Companhia Britânica das Índias Orientais. Em meados do século XVIII, a urbanização de Bombaim foi reformulada pelos britânicos, com grandes projetos de engenharia civil, fazendo surgir uma cidade comercial e cosmopolita. O desenvolvimento econômico e educacional caracterizou a cidade durante o século XIX, tornando-a uma forte base para o movimento de independência da Índia, no início do século XX. Quando o país se tornou independente em 1947, a cidade foi incorporada ao estado de Bombaim. Em 1960, após o movimento Maharashtra Samyukta, o novo estado de Maarastra foi criado, com Bombaim como capital, como se mantém até hoje.
Classificada como uma "cidade global alfa", Bombaim é o maior centro econômico e comercial da Índia, abrigando instituições financeiras importantes, como o Reserve Bank of India (o banco central indiano), a Bolsa de Valores de Bombaim e a Bolsa de Valores da Índia, bem como a sede de diversas empresas importantes. Todos esses atributos fazem com que a cidade seja considerada a mais rica do país, com um produto interno bruto estimado em 209 bilhões * USD em 2008,[5] segundo cálculos da PricewaterhouseCoopers, e correspondendo a cerca de 5,5% do produto interno bruto do país. A cidade ainda é responsável por quase 70% de todas as transações comerciais e financeiras da Índia.
Com um dos maiores e mais movimentados portos do mundo, a cidade de Bombaim é responsável por cerca de 70% de toda a atividade portuária do país — devido, principalmente à sua modernidade e a sua posição estratégica dentro do continente asiático. Bombaim atrai imigrantes de todo o país e de vários países vizinhos, devido às grandes oportunidades comerciais e ao nível de vida relativamente alto. Tornou-se, com isto, um núcleo cosmopolita de várias comunidades e culturas. É em Bombaim que se situa Bollywood, o principal centro da indústria indiana de cinema e televisão.
Segundo a versão oficial, o nome do local deriva da deusa adorada no templo de Mumbadevi, um dos mais antigos da ilha de Bombaim construído pela população nativa, encontrado pelos portugueses à sua chegada, que o deixaram intacto.[6]
Acredita-se que a primeira referência europeia à ilha de Bombaim foi feita por Duarte Barbosa em 1516, designando-a Benamajambu; [a] [7] Alguns autores pretendem que majambu poderá ser uma alusão a Mumbadevi.[8] No entanto, enquanto que Bena refere-se provavelmente a Tana, majambu parece referir-se somente à ilha vizinha de Mahim, ambos os locais atualmente subúrbios de Bombaim.[6] O primeiro autor português a se referir ao local como Bombaim foi Gaspar Correia, que chegou à Índia em 1512, nas suas “Lendas da Índia”, cuja escrita começou por essa altura, sendo secretário de Afonso de Albuquerque.[6] Naquele mesmo século, a grafia parece haver evoluído para Mombayn (1525)[9] e depois Mombaim (1563)[10]
José Pedro Machado refuta uma explicação alternativa para o nome, sem bases científicas, segundo a qual Bombaim seria uma corruptela do português "Bom Bahia" (sic) ou "Boa Bahia". Esta confusão teria levado ingleses pouco sabedores de português a suporem a presença dessa palavra no topónimo, pelo que o português Bombaim teria se transformado no inglês Bombay.[11]
Em 1995, o nome da cidade foi alterado pelo partido Shiv Shena, de cariz chauvinista marata e hindu, para o seu equivalente marata मुंबई, transcrito como Mumbai, imediatamente após a sua subida ao poder no estado de Maarastra. Não obstante, a forma Bombay continuou largamente em uso entre a população indiana, incluindo os realizadores cinematográficos de Bollywood.[12] A mudança do nome estimulou um debate em torno do chauvinismo nacionalista que teria motivado a decisão e sobre a própria identidade da cidade.[13] Por outro lado, foram registadas represálias contra quem se recusou a adoptar a nova designação.[12]
Bombaim era originalmente um arquipélago de sete ilhas. Os artefatos encontrados perto de Kandivali, no norte da cidade, indicam que as ilhas eram habitadas desde a Idade da Pedra. A presença humana encontra-se documentada desde 250 a.C., quando, segundo Ptolomeu, o local era conhecido como Heptanésia. Durante o século III a.C., as ilhas integraram o Império Máuria, governado pelo imperador budista Açoca. Posteriormente, os soberanos hindus da dinastia Silhara governaram-nas até 1343, quando foram anexadas pelo Sultanato de Guzarate.
A primeira visita de Portugueses a Bombaim registrou-se em 1509. Posteriormente, em 1534, as ilhas foram entregues aos portugueses pelo sultão Bádur Xá de Guzarate. Em 1661, foram incluídas no dote de Catarina de Bragança, quando esta casou com Carlos II de Inglaterra. Em 1668, a coroa inglesa arrendou as ilhas à Companhia Britânica das Índias Orientais. A partir daí a população cresceu rapidamente de 10 000 habitantes em 1661 para 60 000 em 1675. Em 1687, a companhia transferiu a sua sede de Surate para Bombaim.
A partir de 1817, a cidade foi reformulada, com grandes projetos de engenharia civil destinados a fundir todas as ilhas do arquipélago em um único terreno. A primeira ferrovia indiana, aberta em 1853, ligava Bombaim a Tana. Durante a Guerra Civil Americana (1861–1865), a cidade tornou-se o principal mercado de algodão do mundo, o que estimulou consideravelmente a economia local. A abertura do Canal de Suez em 1869 transformou Bombaim em um dos maiores portos marítimos do mar Arábico.
Nos trinta anos seguintes, a cidade tornou-se um grande centro urbano, com melhorias na infraestrutura e o surgimento de muitas das instituições municipais. A população atingiu um milhão de habitantes em 1906, o que a fez a segunda maior cidade da Índia depois de Calcutá.
Foi uma base importante para o movimento de independência indiano, de que é exemplo o movimento "Deixem a Índia" (em inglês: Quit India), convocado por Gandhi em 1942. Após a independência da Índia em 1947, Bombaim passou a ser a capital do estado de Bombaim. Em 1950, a cidade expandiu-se até os seus limites atuais, ao incorporar partes da ilha de Salsete, ao norte.
Em 1960, quando o estado de Bombaim foi dividido por critérios linguísticos entre os novos estados de Maarastra e Guzarate, a cidade tornou-se a capital do primeiro. O final dos anos 1970 assistiu-se a um surto de construção e a um influxo considerável de imigrantes, que fizeram Bombaim ultrapassar Calcutá em termos populacionais. A presença de forasteiros começou a preocupar a etnia local, marata, o que levou ao surgimento do partido político de direita Shiv Sena, em 1966. A cidade viveu episódios violentos, como o tumulto sectário de 1992 e os atentados bombistas de 1993 e de 2006, este último, como o de 2008 ligado a terroristas islâmicos. Em 2008 ocorreu um ataque terrorista que matou mais de cem pessoas e deixou outras 300 feridas.
Em 1995, o governo do estado de Maarastra, controlado pelo Shiv Sena, repudiou o nome tradicional Bombay em favor da versão marata Mumbai. Em 2004, a cidade sediou o Fórum Social Mundial.
Bombaim situa-se na desembocadura do rio Ulhas, na costa ocidental da Índia, numa região litorânea chamada Concão. A maior parte da cidade está ao nível do mar; a elevação média vai de 10 a 15 m. Bombaim soma 468 km² de área.
O litoral da cidade possui diversas baías e canais; a costa oriental da ilha de Salsete é coberta por grandes manguezais de grande biodiversidade.
A região onde se encontra Bombaim é sismicamente ativa e portanto sujeita a sismos, devido a três falhas geológicas na vizinhança.
Bombaim é formada por duas áreas distintas: a cidade e os subúrbios, que se constituem em distritos separados, pertencentes ao estado de Maarastra.
O clima da cidade, localizada na zona tropical e defronte do mar Arábico, apresenta duas estações definidas — a úmida e a seca. Na estação úmida, entre março e outubro, a temperatura ultrapassa 30 °C, com alta umidade. As monções são caraterísticas desta época do ano, entre junho e setembro, causando uma precipitação anual de 2 200 mm.
A estação seca, entre Novembro e Fevereiro, apresenta níveis mais baixos de umidade e temperaturas moderadas. Os ventos setentrionais baixam a temperatura em Janeiro e Fevereiro. As temperaturas médias anuais variam entre 38 e 11 °C.
A taxa de alfabetização de Bombaim ultrapassa 86%, significativamente maior que a média nacional, que não chega a 60%. As principais religiões presentes na cidade são o hinduísmo (68% da população), o islamismo (17%), o cristianismo e o budismo (4% cada). Há ainda parses, jainistas, siques, judeus e ateus.
A língua mais ouvida nas ruas da cidade é uma variante coloquial de hindi chamada Bambaiya, que mistura hindi, marata, inglês e algumas palavras próprias. O marata é amplamente falado e é também o idioma oficial do estado de Maarastra. O inglês é amplamente falado e é a principal língua nas empresas e escritórios locais. Outras línguas indianas são faladas: tâmil, gujarati, telugo, canarês, concani e urdu.
Bombaim sofre com os mesmos problemas de urbanização que afligem outras cidades de crescimento rápido em países em desenvolvimento: pobreza generalizada e padrões precários de saúde, emprego e educação. Cerca de 55% da população vive em favelas, as quais cobrem apenas 6% das terras da cidade[17]
A Grande Bombaim tem uma área de 603 km2,[18] que consiste nos distritos da cidade de Mumbai em si e de seus subúrbios, como Colaba, Mulund, Dahisar e Mankhurd. Sua população, de acordo com o censo de 2011, era de 12 478 447 de habitantes.[19]
A cidade é administrada pela Corporação Municipal da Grande Bombaim (CMGB) (por vezes referida como Corporação Municipal Brihanmumbai), anteriormente conhecida como Corporação Municipal de Bombaim (CMB). A CMGB está a cargo das necessidades de civis e de infraestrutura de metrópole.[20] O prefeito é escolhido através de eleição indireta pelos conselheiros, para um mandato de dois anos e meio.[21]
Bombaim é geminada com as seguintes cidades:
 Berlim, Alemanha
 Londres, Reino Unido
 Iocoama, Japão
 São Petersburgo, Rússia
 Estugarda, Alemanha
 Los Angeles, Estados Unidos
 São Paulo, Brasil
 Honolulu, Havaí
Bombaim é a maior cidade da Índia, e o principal e mais desenvolvido centro financeiro e comercial do país, sendo responsável por cerca de 6% do produto interno bruto indiano em 2010.[22][23] A cidade e sua região metropolitana servem ainda como grande centro industrial e comercial, contribuindo ao empregar 10% da mão-de-obra fabril, e fornecer 25% do rendimento industrial bruto do país e 33% dos impostos recolhidos pelo governo através da realização de atividades financeiras, além de gerar 20% dos impostos extraídos do setor de serviços na Índia.[24]
Em 2008, o produto interno bruto nominal da região metropolitana de Bombaim foi de 919 bilhões * (cerca de 183 bilhões USD, enquanto sua paridade do poder de compra ficou em pouco mais de um trilhão de rupias (o que equivale a aproximadamente 208 bilhões USD).[25] Já em 2009, o produto interno bruto per capita da Grande Bombaim foi de 48 000 de rupias (cerca de 9 700 USD),[26][27] três vezes superior ao produto interno bruto per capita nacional.[28]
Muitos dos aglomerados empresariais indianos, como Larsen & Toubro, Reliance Industries (RIL), Grupo Tata, Life Corp (LIC), etc., além de cinco das 500 maiores empresas do mundo, segunda a revista Forbes, têm sede em Bombaim.[29] Além disso, a maioria dos bancos e instituições financeiras da Índia estão sediadas na cidade ou em sua região metropolitana.[23]
Desde os anos 1970, Bombaim vem registrando um grande crescimento na atividade portuária e no setor industrial, considerados fundamentais para a economia da cidade desde a independência do Reino Unido. Entretanto, as atividades financeiras e o setor de comércio e seviços vâm ganhando cada vez mais espaço na economia da cidade, principalmente desde o final dos anos 1990.[24][26][30] Em 2008, a Globalization and World Cities Study Group classificou Bombaim como uma cidade global alfa, o mais alto índice para se classificar uma cidade global. Tal alta classificação é fruto do fato da cidade ser a 3ª mais cara do mundo para atividades empresariais, e de ter sido considerada a cidade com mais rápido crescimento econômico da Ásia em 2007.[31]
Assim como na maioria do país, o esporte mais popular de Bombaim é o críquete, o principal time de críquete é o Mumbai Indians da Indian Premier League. Junto com Londres, Bombaim é a única cidade do mundo que recebeu duas finais da Copa do Mundo de Críquete.
A cidade também é a casa do Maratha Warriors da Premier Hockey League (hóquei em campo) e três times de futebol da I-League: Mumbai FC, Mahindra United e Air-India FC.
Tóquio (em japonês: 東京; romaniz.: Tōkyō, pronunciado: [to̞ːkʲo̞ː] (escutar?·info), literalmente "capital do Leste"), oficialmente Metrópole de Tóquio (東京都, Tōkyō-to?), é a capital do Japão e uma das 47 prefeituras do país. Situa-se em Honshu, a maior ilha do arquipélago. Em 2015, Tóquio possuía mais de 13,4 milhões de habitantes, cerca de 11% da população do país, e a Região Metropolitana de Tóquio possui mais de 37 milhões de habitantes, o que torna a aglomeração de Tóquio, independentemente de como se define, como a área urbana mais populosa do mundo. Um de seus monumentos mais famosos é a Torre de Tóquio. Foi fundada em 1457, com o nome de Edo ou Yedo. Tornou-se a capital do Império em 1868 com a atual designação. Sofreu grande destruição duas vezes; uma em 1923, quando foi atingida por um terremoto; e outra em 1944 e 1945, quando bombardeios americanos destruíram grande parte da cidade, sendo que no total foi destruída 51% de sua área[2] e mortas mais de 80 mil pessoas.
Embora seja considerada um dos maiores centros financeiros do mundo (ao lado de Nova Iorque e Londres[3]), e uma "Cidade Global Alfa+", ela não é, tecnicamente, uma cidade. Não há no Japão uma cidade chamada "Tóquio". Na verdade, Tóquio é designada como uma metrópole (都, to?), similar a uma prefeitura do Japão (県, ken?), e é constituída por 23 bairros (区, ku?), 26 cidades primárias (市, shi?), cinco cidades secundárias (町, cho ou machi?) e oito vilas diferentes (村, son ou mura?). Cada uma delas possui um governo que opera no nível regional. Também fazem parte de seu território pequenas ilhas no Oceano Pacífico, localizadas a cerca de mil quilômetros ao sul.
Mais de nove milhões de pessoas vivem dentro dos 23 distritos autônomos que constituem a parte central de Tóquio. Estes 23 distritos definem a "Cidade de Tóquio", possuindo 9,24 milhões de habitantes. Sua população aumenta em 2,4 milhões ao longo do dia, devido aos estudantes e trabalhadores de prefeituras vizinhas, que vão à Tóquio para estudar e trabalhar.[4] A população total dos bairros de Chiyoda, Chuo e Minato, que compõem a região central, e onde está localizado o principal centro financeiro do país, é de 375 mil habitantes; porém, mais de dois milhões de pessoas trabalham na região.[4]
Tóquio é o principal centro político, financeiro, comercial, educacional e cultural do Japão. Assim sendo, possui a maior concentração de sedes de empresas comerciais, instituições de ensino superior, teatros e outros estabelecimentos comerciais e culturais do país. Também possui um sistema de transporte público altamente desenvolvido, com numerosas linhas de trens, metrô e de ônibus, bem como o Aeroporto Internacional de Tóquio.
Era originalmente conhecida como Edo, que significa "estuário".[5] Seu nome foi mudado para Tóquio (Tóquio: Tō (leste) + quio (capital)) quando se tornou a capital imperial em 1868, em linha com a tradição da Ásia Oriental de incluir a palavra capital ('京', capital?) no nome da cidade da capital.[5][6]
Durante o início do período Meiji, a cidade também era chamada de "Tōkei", uma pronúncia alternativa para os mesmos caracteres chineses que representam "Tóquio".[7] Alguns documentos oficiais sobreviventes em inglês usaram a ortografia "Tokei".[7] Entretanto, agora essa pronunciação é considerada obsoleta.[8]
Apesar que desde tempos antigos existiam pequenas populações e templos nas colinas cercando a Baía de Tóquio (東京湾, Tōkyō-wan?), considera-se que a fundação formal de Tóquio foi em 1457,[9] quando um vassalo do clã Uesugi (上杉氏, Uesugi-shi?), Dōkan Ōta (太田  道灌, Oota Doukan?) construiu o Castelo de Edo (江戸城, Edo-jō?), assim a área que rodeava o castelo começou a se chamar Edo (江戸, literalmente "estuário"?).[10][11] Shogunato Tokugawa (徳川幕府, Tokugawa bakufu?), que havia tomado o castelo em 1590 e que tinha o controle quase absoluto do Japão, estabeleceu seu governo em Edo, em 1603, isso deu início ao Período Edo (江戸時代, Edo-jidai?), na história japonesa.[9][12] Durante esse período, a cidade usufruiu de um prolongado período de paz conhecido como Pax Tokugawa.[13] Além disso, Edo adotou uma política rigorosa de isolamento, o que ajudou a evitar ameaças militares sérias a cidade durante bastante tempo.[13] Edo cresceu e por volta do século XVIII se tornou uma das cidades mais populosas do mundo com mais de um milhão de habitantes.[14] A nobreza, junto com o Imperador do Japão, permaneceram em Quioto, que seguiu sendo a capital oficial, porém apenas de maneira protocolar.[15]
Edo sofreu inumeráveis desastres, entre os que se encontram centenas de incêndios, destacando-se o Grande Incêndio de Edo (Edo Taika) de 1657,[16] onde estima-se que morreram 108 mil pessoas; nesse mesmo acontecimento, também conhecido como incêndio de Furisode, a maior parte da cidade foi destruída, incluindo o Castelo de Edo e seus arredores.[17] Outros desastres que sofreu Edo foram à erupção do Monte Fuji em 1707,[18] o Terremoto do Grande Edo em 1855 e outros terremotos menores em 1703, 1782 e 1812.[19][20]
Em 1853, o comandante americano Matthew Perry desembarcou na Baía de Tóquio, à frente de uma frota de quatro navios de guerra, como um enviado do governo americano, com a missão de instituir relações diplomáticas e comerciais entre o Japão e os Estados Unidos.[21] Perry voltou em 1854, à frente de uma frota maior do que a anterior, e assinou um tratado diplomático entre os líderes de governo do Japão.[21] A chegada de Perry ao Japão iniciou um período de abertura na história do país iniciado com a abertura dos portos de Shimoda e Hakodate.[21] [22] Essa situação levou a um aumento na demanda por novos produtos estrangeiros, o que aumentou consideravelmente a inflação.[22] A inquietação social que foi originada pelo aumento dos preços culminou em rebeliões e manifestações, especialmente através da "demolição" de estabelecimentos que comercializavam arroz.[23]
Em novembro de 1867, ocorreu a destituição do último xogum, Tokugawa Yoshinobu, e o fim do Xogunato em todo o Japão.[24] Assim, em 1868 deu-se o início da Restauração Meiji, em que o Imperador se mudou ao Castelo Edo, convertendo-o no Palácio Imperial do Japão e estabeleceu a mesma alteração de nome de Edo para Tóquio, "a capital do leste".[6][9] No entanto, o Imperador não deixou estabelecido de maneira legal que Tóquio era a nova capital do Japão.[9] Algumas pessoas creem que, devido a esse fator, Kioto seria a capital oficial ou co-capital do país.[9] Em 1871 aboliram-se os han ou feudos, e formalmente criaram-se as prefeituras, entre elas a Prefeitura de Tóquio; e ao ano seguinte a prefeitura expandiu-se à área ocupada pelos 23 Bairros Especiais que atualmente possui.[25]
A partir de 1872, começou a construir-se a primeira linha de metropolitano ligando Tóquio com Yokohama[26] e em 1925 construiu-se a Linha Yamanote, linha de metropolitano urbano que é uma das mais importantes de Tóquio na atualidade.[27] Em 1889 estabeleceu-se a Cidade de Tóquio (東京市, Tōkyō-shi?) com 15 bairros, logo em 1893 os distritos de Tama que uniram-se a prefeitura.[28]
Em 1914 inaugurou-se a Estação de Tóquio e, em 1927, inaugurou-se a primeira linha de metrô subterrânea, que ligava Asakusa e Ueno.[28] O Grande terremoto de Kanto (関東大震災, Kantō daishinsai?) golpeou Tóquio em 1923, com um saldo de aproximadamente 140 mil pessoas mortas e desaparecidas e trezentas mil residências destruídas.[28]
Depois da tragédia iniciou-se um plano de reconstrução que não pôde ser completado devido a seu alto custo.[9][28] No dia 8 de janeiro de 1932, ocorreu em Tóquio o Incidente Sakuradamon, uma tentativa de assassinato contra o Imperador Hirohito por um ativista da independência da Coreia, então ocupada pelo Japão.[29] Em 1936 inaugurou-se o edifício da Kokkai (Dieta do Japão);[30] também nesse mesmo ano ocorreu o Incidente de 26 de fevereiro (二・二六事件,  Ni-niroku jiken?), no que 1 400 oficiais do exército japonês ocuparam o edifício da Kokkai, o Kantei (Residência do primeiro-ministro) e outros lugares de Tóquio numa tentativa de golpe de Estado, que foi sufocada três dias depois.[31] Ainda em 1936, Tóquio foi eleita a sede dos Jogos Olímpicos de 1940; porém, devido ao fato do Japão ter invadido a China durante a Segunda Guerra Sino-Japonesa, esta protestou contra a realização das olimpíadas na cidade.[32] Em 16 de julho de 1938, o governo japonês anunciou sua renúncia em sediar as olimpíadas.[32][33] A cidade sede foi transferida para Helsinque, porém com o início da Segunda Guerra Mundial, os Jogos de 1940 terminaram não acontecendo.[33][34]
Durante a Segunda Guerra Mundial, na década de 1940, o governo japonês decidiu instituir a Província Metropolitana de Tóquio.[28] A partir desta iniciativa extinguiu-se aquilo que era conhecido como Cidade de Tóquio (東京市, Tōkyō-shi?).[28] Durante a mesma guerra, foi intensamente bombardeada a partir de 1942 até 1945.[2] Por causa disto, em 1945 a população de Tóquio era a metade que em 1940.[9] Os bombardeios mais pesados a atingiram em 1944 e 1945, destruindo aproximadamente um terço da cidade, e matando mais de 80 mil pessoas.[2][nota 1] Tóquio tinha cerca de 7,3 milhões de habitantes em 1940; no final da guerra, a população havia caído pela metade, para cerca de 3,5 milhões.[28] [35] Ao terminar a guerra, em setembro de 1945, foi ocupada militarmente e passou a ser governada pelas Forças Aliadas.[9] O general Douglas MacArthur estabeleceu os quartéis da ocupação no que atualmente é o edifício DN Tower 21 (anteriormente conhecido como Dai-Ichi Seimei), em frente ao Palácio Imperial.[9] Na segunda metade do século XX, Estados Unidos aproveitou Tóquio como um centro importante de logística durante a guerra da Coreia.[9] Na atualidade, onde permanecem sob controle estadunidense a Base aérea de Yokota e algumas poucas instalações militares menores.[9]
Tóquio foi novamente reconstruída após o fim da guerra em um programa de reconstrução feito para todo o Japão em 1945, que teve como um de seus objetivos reduzir a densidade populacional das áreas urbanas.[36] O programa procurou reconstruir cinquenta mil hectares de 115 cidades do país, porém contemplou somente 102 cidades e uma área de 28 mil hectares.[37] Em 1947, foi reestruturada para ter a estrutura de 23 bairros que possui hoje em dia.[28] Tóquio experimentou o chamado "milagre econômico" durante as décadas de 1950 e 1960.[9] Em 1954 criou-se a segunda linha de metrô com a Linha Marunouchi e em 1961 com a Linha Hibiya.[38] Em um espaço de dezessete anos, a sua população chegou a alcançar a marca de dez milhões de pessoas em 1962.[28] Em 1958 construiu-se a Torre de Tóquio[39] e em 1964 inaugurou-se a primeira linha de Shinkansen (Tokaido Shinkansen[40]), coincidindo com a celebração dos Jogos Olímpicos de Tóquio.[41] Esta prosperidade transformou um país devastado pela guerra na segunda economia do mundo em menos de 20 anos.[9] Durante este período, o governo japonês deu prioridade para a Infra-estrutura e indústrias de manufatura.[9] Como resultado, Japão dominou um amplo ranking de indústrias como a do aço, a automobilística, de semicondutores e eletrodomésticos.[9]
Em 1966, um plano diretor, conhecido como Tama New Town, foi instituído em Tóquio com o objetivo de minimizar a escassez de moradias devido ao alto crescimento populacional.[42][43] Em 1969, foi promulgado o Regulamento de Controle da Poluição, algo que fazia parte de um conjunto de medidas, feitas pelo governo de Tóquio, que tinham como objetivo diminuir a intensidade da poluição atmosférica na região, então ocasionada por fábricas e indústrias químicas.[28][44] Foram devolvidas ao Japão as ilhas Ogasawara em 1968 e a Base Aérea de Tachikawa em 1977.[28][45]
Durante a década de 1970 houve uma migração maciça das áreas rurais para as cidades, Tóquio em especial.[9] Em 1978 inaugurou-se o Aeroporto Internacional de Narita, que serviu principalmente para voos internacionais; enquanto que o Aeroporto Internacional de Tóquio, inaugurado em 1931, serviria principalmente para voos nacionais.[46][47]
A grande população em Tóquio derivou de um crescimento econômico que terminou na década de 1990, mais precisamente de 1991 até 2000, causando uma recessão durante toda essa década, chamada também A década perdida (失われた10年, Ushinawareta Jūnen?).[48] Em 20 de março de 1995, a cidade concentrou a atenção dos meios internacionais sobre o atentado terrorista do culto Aum Shinrikyo no sistema de trens subterrâneos de Tóquio.[49] Nele morreram doze pessoas e milhares foram afetadas pelo gás nervoso Sarín.[49]
Apesar disso, continuou crescendo; em 1990 construiu-se o Tōchō ou Tóquio City Hall e em 1993 inaugurou-se a Rainbow Bridge sobre a Baía de Tóquio.[28][50] Isto, contudo fez que Tóquio fosse uma das cidades mais dinâmicas do planeta com um amplo ganho de atividades sociais e econômicas.[51]
O Japão é um país que tem executado projetos para ganhar terras ao mar e, desde a década de 1980, a criação de ilhas artificiais vem sendo bastante utilizada.[52] Entre estas ilhas se sobressai Odaiba (お台場, Odaiba?), construída durante o período Edo para proteger sua região de invasões marítimas, hoje possui muitos shoppings e áreas de lazer e é um dos pontos turísticos mais populares do Japão.[53] Outros projetos urbanos recentes incluem o Jardim de Ebisu, a ilha Tennozu, o Shiodome, Roppongi Hills e Shinagawa.[54]
O Sismo e Tsunami de Tohoku de 2011 que devastou grande parte da costa nordeste da ilha de Honshu foi sentido em Tóquio.[55] Porém, devido a infraestrutura possuir maior resistência a terremotos, os danos à Tóquio foram menores se comparados a áreas diretamente atingidas pelo tsunami;[56] ainda assim, os edifícios de Tóquio foram sacudidos com força e algumas atividades foram suspensas, entre elas, o funcionamento das linhas telefônicas e do sistema de metrô.[55][57] A subsequente crise nuclear causada pelo tsunami não afetou Tóquio significativamente, apesar de terem ocorrido alguns aumentos no seu nível de radiação.[58][59]
Em 22 de maio de 2012, foi inaugurada em 2012 a Tokyo Skytree, a torre mais alta do mundo com 634 metros.[60] A torre custou 806 milhões de dólares e, segundo engenheiros, pode suportar um terremoto de oito graus na escala Richter sem sofrer dano algum.[60] Em 7 de setembro de 2013, o Comitê Olímpico Internacional (COI) selecionou Tóquio para sediar as Olimpíadas de 2020.[61] Tóquio irá sediar os Jogos Olímpicos pela segunda vez.[61] Está localizada na margem noroeste da Baía de Tóquio. Limita-se com a prefeitura de Chiba a leste, Yamanashi a oeste, Kanagawa ao sul e Saitama ao norte.[62] Fazem também parte de Tóquio ilhas que estão espalhadas no Oceano Pacífico, localizadas a cerca de mil quilômetros ao sul.[63][64] A mais distante delas, Okinotorishima, está a dois mil quilômetros da sua costa.[64]
Tóquio fica próxima de uma junção tripla localizada na Península de Boso, o que faz com que terremotos de menor intensidade ocorram com frequência na sua região; porém, abalos sísmicos com o epicentro na área continental (excluindo-se as ilhas sob sua jurisdição) são algo raro de acontecer.[65]
Tóquio possui muitas ilhas periféricas, que se estendem por uma distância de até dois mil quilômetros de Tóquio.[64] Por causa da distância entre as ilhas do centro administrativo do governo metropolitano de Tóquio, subprefeituras locais administram as ilhas, sendo que estas possuem duas cidades e sete vilas.[62] Além disso, as ilhas compondo as terras mais ao sul e ao leste do Japão ficam sob o controle do distrito administrativo de Ogasawara; essas são, respectivamente: as ilhas de Okinotorishima e Minamitorishima.[62] Sobre Okinotorishima, o Japão alega possuir uma zona econômica exclusiva (ZEE) na sua área, sendo que essa afirmação é contestada pela China pois, de acordo com ela, a região seria desabitada e, sendo assim, não poderia ser ZEE.[66]
As Ilhas Izu são um grupo de ilhas vulcânicas próximo à Península de Izu e que faz parte do Parque Nacional Fuji-Hakone-Izu.[67][68] As Ilhas Izu são divididas em três subprefeituras: Izu Ōshima e Hachijojima possuem cidades.[69] As ilhas restantes são seis vilas, com Nii-jima e Shikine-jima formando uma vila.[69] As ilhas que pertencem ao arquipélago de Izu são as seguintes: Izu Ōshima; To-shima; Nii-jima; Shikine-jima; Miyake-jima; Aogashima;[69] Mikura-jima; Hachijojima; Aogashima; Tori-shima e Udone-shima.[70]
As ilhas Ogasawara ficam a uma distância de, aproximadamente, mil quilômetros de Tóquio e englobam mais de trinta ilhas divididas em quatro grupos e em uma ilha que não faz parte desses grupos (Nishinoshima).[71][72] Apenas as ilhas de Chichi-jima e Haha-jima são habitadas.[71] As ilhas são consideradas patrimônio material pela UNESCO.[71] Os principais grupos de ilhas do arquipélago Ogasawara são: Chichi-jima; Haha-jima; Muko-jima; Nishinoshima e Kazan-retto.[71]
As formas de relevo de Tóquio fazem com que seus rios fluam do oeste para o leste, assim, a desembocadura deles termina sendo na Baía de Tóquio.[73] Existe um total de 107 rios na metrópole, com 92 rios classe A e quinze classe B tendo 857 quilômetros de extensão.[73] Os classe A são aqueles que são observados pelo Ministério da Terra, Infraestrutura, Transporte e Turismo, os classe B são observados pelo governo de Tóquio.[73]
O Governo Metropolitano de Tóquio administra 710 quilômetros de 105 rios, enquanto que o restante do total é administrado pelo ministério.[73] Os 46 rios localizados nos bairros especiais são supervisionados pelos bairros.[73] Costuma-se contar separadamente alguns riachos, chamados de rios secundários, que acabam sendo observados e administrados diretamente pelos governos das cidades e vilas em que se localizam.[73] Tóquio se localiza em uma região com cinco bacias hidrográficas que cobrem uma área total de 22,6 mil quilômetros quadrados, essas bacias pertencem aos rios Tsurumi, Tama, Arakawa, Sagami e Tone.[74] No Japão, se utiliza para rios o sufixo (側, gawa?).[75]
A utilização e o gerenciamento da água tiveram importância histórica para a região de Tóquio.[76] Durante o período Edo, os rios eram utilizados para carregar pessoas e bens comerciais e a cidade possuía muitos canais e, por causa disso, chegou a ser comparada com Veneza.[77] Apesar de ainda ser possível verificar a existência de alguns canais, a importância econômica dos rios para a cidade diminuiu consideravelmente do Período Edo para os dias atuais, principalmente devido ao desenvolvimento urbano e tecnológico de Tóquio durante o século XX.[77]
O Rio Sumida possui 23,5 quilômetros de extensão ficando em uma área densamente povoada do centro de Tóquio, com cerca de três milhões de pessoas vivendo próximas a sua bacia; é um dos rios mais importantes e famosos de Tóquio.[78] Historicamente teve papel de destaque no sistema transporte aquático da cidade e atualmente possui um festival de fogos de artifício conhecido como Sumidagawa Fireworks Festival, que possui esse nome desde 1978, apesar da prática ter sido feita perto do rio desde o século XVIII.[78][79]
Em 1.º de abril de 2014, 36% da área territorial de Tóquio era composta por parques naturais, tendo assim a segunda maior área de parques do país (atrás apenas de Shiga, que possui 37%).[80] Entre os parques nacionais de Tóquio se destacam o Parque Nacional Chichibu-Tama-Kai e o Parque Nacional Ogasawara.[81] O primeiro pelo fato de não possuir áreas com vulcões, algo incomum no Japão, apesar de se situar em uma região a mais de dois mil metros de altitude acima do nível do mar, e o segundo por ser registrado pela UNESCO como Patrimônio Mundial.[81][82] Além desses parques, a metrópole também abriga os seguintes parques: Meiji no Mori Takao, Akikawa Hills, Hamura Kusabana Hills, Sayama, Takao Jinba, Takiyama, e Tama Hills; todos esses parques são diretamente administrados pelo governo de Tóquio, apesar do primeiro ser um tipo de parque especial que possui um maior envolvimento do governo nacional do que os outros.[83]
Tóquio aprovou uma medida para reduzir gases de efeito estufa, o governador Shintaro Ishihara criou o primeiro sistema de limite de emissões do Japão, com o objetivo de reduzir as emissões de gases de efeito estufa em um total de 25% até 2020 em relação ao nível de 2000.[84] Tóquio é um exemplo de uma ilha urbana de calor, e o fenômeno é especialmente sério em suas cidades especiais.[85][86] De acordo com o governo metropolitano de Tóquio, a temperatura média anual aumentou cerca de 3 °C nos últimos cem anos.[87] Tóquio foi citada como um "exemplo convincente da relação entre crescimento urbano e clima".[85]
Em 2007, Tóquio promulgou o "Projeto de Dez Anos para Tóquio Verde" a ser realizado até 2017.[88] Estabeleceu uma meta de aumentar as árvores nos acostamentos das estrada em Tóquio para um milhão (de 480 mil), e adicionar mil hectares (dez quilômetros quadrados) de espaço verde, sendo que parte deles serão um novo parque chamado "Umi no Mori" ("floresta marinha")[89] que estará em uma ilha recuperada na Baía de Tóquio e que costumava ser um aterro sanitário.[88][90] No plano de ação da cidade de 2012, foi estabelecido que as árvores de acostamento em Tóquio aumentariam para 950 mil e mais trezentos hectares de espaço verde seriam adicionados entre 2012 e 2014.[91]
O lixo da metrópole é separado em cinco categorias: incineráveis, não incineráveis, recicláveis, garrafas PET e itens amplos (para entrar nessa categoria o item deve ter as medidas 30 x 30 x 30 em centímetros).[92] Em Tóquio não existem lixeiras nas ruas, isso ocorre porque estas foram retiradas após o ataque terrorista com gás sarin no metrô de Tóquio, em 1995.[93] As bombas estavam nos cestos de lixo das estações.[93] Desde então, o único lugar onde os japoneses encontram lixeiras é ao redor das lojas de conveniência e de forma limitada.[93] Devido a esse fato o lixo na cidade deve ser levado para casa e preparado para a coleta seletiva.[93]
A indústria da reciclagem possui importância significativa na metrópole.[94] O governo diz que promove a estratégia dos três erres (reciclar, reduzir e reutilizar).[94] Toda a semana, milhares de caixotes de plástico são colocados nas ruas de Tóquio para coletar material reciclável.[94] Nos escritórios, supermercados, estações de trem e outras instalações os resíduos são separados meticulosamente e colocados nos receptáculos apropriados.[94] Tóquio possui usinas de reciclagem em alguns de seus 23 distritos especiais, a maioria administrado por empresas privadas, a principal delas é que se localiza em Minato.[94]
A maioria da sua parte continental possui clima subtropical úmido, com verões quentes e úmidos e invernos geralmente frios.[95] Na região, como em grande parte do Japão, o mês mais quente é agosto, com média de 26,4 °C e o mês mais frio janeiro, com média de 5,2 °C.[96]
A temperatura mínima absoluta é de -9,2 °C em 13 de janeiro de 1876, enquanto a máxima absoluta é de 39,5 °C em 20 de julho de 2004.[96] Tóquio é um exemplo de uma ilha de calor urbano, a população da cidade é um contribuição significativa para o clima.[85][97] A queda de neve é esporádica, mas ocorre quase anualmente.[98] Tóquio também costuma ver tufões todos os anos, embora poucos sejam fortes.[85] O mês mais chuvoso desde que os registros começaram em 1876 foi em outubro de 2004, com 780 milímetros de chuva;[96] nos últimos quatro meses registrados para observe que nenhuma precipitação é em dezembro de 1995.[96] A precipitação anual variou de 2 229,6 mm em 1938 até 879,5 mm em 1984.[96]
Tóquio é o centro da maior região metropolitana do mundo, conhecida como Região Metropolitana de Tóquio-Yokohama.[110] Esta região metropolitana inclui as prefeituras japonesas de Chiba, Kanagawa e Saitama.[62] Cerca de 30% de toda a população do Japão vive na região metropolitana de Tóquio.[62] A população desta é de 37 milhões de habitantes.[111]
A megalópole assumiu em 1966 a liderança do ranking mundial de população da Organização das Nações Unidas (ONU).[112] Na atualidade, é considerada uma megacidade, por possuir mais de dez milhões de habitantes.[113]
A prefeitura de Tóquio, de forma isolada, possuía em 2015 mais de 13,4 milhões de habitantes, cerca de 11% da população do Japão; sendo, portanto, a prefeitura mais populosa do país.[4] A população da prefeitura se divide da seguinte forma: cerca de 9,24 milhões de seus habitantes vivem na área das cidades especiais, 4,2 milhões na área Tama e 26 mil nas ilhas sob sua jurisdição.[4]
A densidade populacional de Tóquio é alta, de 6.158 pessoas por quilômetro quadrado, sendo a província mais densamente povoada do Japão.[4] Mais de 90% da imensa população da prefeitura é descendentes de japoneses.[114]
Diferente dos Estados Unidos e de países da Europa, o Japão não é um país com um número significativo de imigrantes.[115][116] O número de nascidos estrangeiros que vivem no país é de aproximadamente 2.73 milhões de pessoas, um valor que chega a apenas 2,1% da população total do país (125 402 911).[115][117] Porém, devido ao envelhecimento da população, que resulta na diminuição da população economicamente ativa, medidas foram adotadas recentemente pelo governo para aumentar a imigração para o país.[115][118] Uma dessas medidas foi aprovar, no ano de 2018, uma lei de imigração que foi feita com o objetivo de atrair 345 mil novos trabalhadores estrangeiros em cinco anos.[118]
Tóquio é a região do Japão com a maior quantidade de imigrantes.[115] Um em cada oito dos jovens que moravam em Tóquio em 2018 chegando à faixa dos vinte anos de idade não havia nascido no Japão.[115] Em 2018, a população de estrangeiros em Tóquio era de aproximadamente 521 mil pessoas, algo próximo a 4% da população total.[114][119] Os dois maiores grupos étnicos minoritários de Tóquio são chineses e coreanos, cada um responsável por menos de 1% da população da prefeitura.[114] Há também pessoas de outras nacionalidades: filipinos, americanos, indianos, vietnamitas, entre outros.[114] Cerca de 84% dos residentes estrangeiros de Tóquio vivem na região dos 23 distritos especiais.[119] Shinjuku é a região com maior número de imigrantes (43 068) e com maior porcentagem de imigrantes dentro da população total (12,4%).[119] A cidade de Minato tem uma porcentagem alta de cidadãos estrangeiros (7,81%) devido ao fato de ser a localidade de muitas embaixadas e corporações do exterior.[119]
No Japão de forma geral, as principais religiões são o Xintoísmo e o Budismo.[120] Enquanto o Xintoísmo é uma religião nativa do Japão e nasceu ao mesmo tempo que a cultura japonesa, o budismo foi criado no século VI a.C e importado ao Japão por influência, principalmente, da China e da Coreia.[120][121] As duas religiões co-existem pacificamente no país e, por isso, muitos de seus cidadãos se consideram de ambas as religiões.[120] O principal santuário xintoísta em Tóquio é o Santuário Meiji, criado em homenagem póstuma ao Imperador Meiji e a Imperatriz Shōken, sua esposa, em 1920.[122][123] É um dos estabelecimentos religiosos mais populares do Japão.[122] Nos primeiros dias do ano novo, recebe regularmente mais de três milhões de visitantes que realizam as primeiras orações do ano, conhecidas como hatsumode.[122] O principal templo budista de Tóquio é o Sensoji que se destaca por ser o tempo mais antigo de Tóquio e por abrigar o pagode Gojunoto (ou das Cinco Histórias), construído em 1648 por Tokugawa Iemitsu.[124] Além dessas duas religiões, também possuem certa abrangência em Tóquio o Cristianismo e o Confucionismo; os principais lugares de prática dessas doutrinas em Tóquio são a Igreja de São Nicolau e o santuário Yushima Seido respectivamente.[125][126] A presença do Islamismo não é significativa no Japão em comparação com as religiões supracitadas; porém, a presença de muçulmanos cresceu significativamente durante a década de 1980.[127] A maior mesquita do Japão é a Mesquita de Tóquio (ou Tokyo Camii), inaugurada em 2000.[128]
De acordo com o Anuário Estatístico de Religião do Japão de 2018, Tóquio possui 2 868 templos (寺院, Jiin?), 1 463 santuários (神社, Jinja?) e 2 371 igrejas (教会, Kyōkai?).[129] De acordo com o mesmo anuário, possui um total de 36 226 763 budistas (仏教系, Bukkyō-kei?), 6 356 842 de xintoístas (神道系, Shintō-kei?), 952 017 de cristãos (キリスト教系, Kirisutokyō-kei?) e de 3 092 599 outros vários ensinamentos (諸教, Sho kyō?).[129][130] Além disso, o anuário coloca que o número de crentes (信者, Shinja?) de Tóquio pode se situar entre 45 882 866 e 46 628 221; ambos os números chegam a aproximadamente 25% do total do país, que é 181 164 731.[129][130]
Em 2019, Tóquio foi eleita como a cidade mais segura do mundo em um ranking feito pela revista The Economist, que ocorre a cada dois anos e avalia sessenta cidades.[131] O ranking, conhecido como Safe Cities Index, é realizado desde 2015 e não só faz uma avaliação geral no aspecto da segurança como também analisa este isoladamente em quatro categorias: segurança pessoal, digital, de saúde e estrutural; destas quatro, apenas as duas primeiras poderiam ser relacionadas à questão da criminalidade.[131] Na categoria segurança pessoal, Tóquio ficou em quarto lugar entre as cidades avaliadas perdendo para Hong Kong, Copenhague e Singapura.[131] No quesito segurança digital, Tóquio ficou em primeiro lugar.[131]
De acordo com uma estatística feita pelo Departamento Metropolitano de Polícia de Tóquio em 2019, a quantidade de incidentes criminosos na metrópole por ano é menor do que quarenta mil.[132] De acordo com a estatística, a área onde acontece mais crimes na cidade é em Shinjuku (7 940 crimes) e onde acontece menos é Bunkyo (1 730).[132] As áreas com maior e menor taxa de criminalidade são, respectivamente, Taito (1,26%) e Chuo (0,41%).[132] Com relação aos crimes violentos, as regiões com maior e menor ocorrências desse tipo foram Shinjuku (757 incidentes) e Minato (catorze).[nota 4][132]
Tóquio possui grande presença da Yakuza, a máfia japonesa e organização criminosa mais poderosa do Japão.[133][134] A principal área de atuação e onde ocorre a maior parte dos negócios desta na metrópole é Kabukicho.[133] Apesar do fato de a Yakuza ser considerada uma organização legitima no Japão e, em algumas situações, a polícia e ela trabalharem em conjunto, o governo de Tóquio aprovou leis com o objetivo de enfraquecê-la.[133][135]
Tóquio não é tecnicamente uma cidade, mas sim, uma das 47 províncias do Japão.[6] Ela está dividida em 23 bairros e 39 cidades e vilas diferentes.[136] Cada uma delas possuem poderes municipais, com seus próprios prefeitos e assembleias municipais.[137] Porém, o governo provincial cria leis que valem para todos os distritos e cidades de Tóquio e atua limitando os poderes destes governos locais.[138] O governo provincial também é responsável pelo fornecimento de serviços de esgoto e abastecimento de água, embora outros serviços públicos sejam de responsabilidade regional, tais como moradia e educação.[136]
Os habitantes elegem um governador para mandatos de quatro anos de duração.[139] Leis provinciais são discutidas e aprovadas por uma Assembleia Metropolitana.[139] Seus 127 membros são eleitos pela população da província para mandatos de quatro anos de duração.[139][140] Cada distrito, cidade ou vila que faz parte da província possui ao menos um representante na assembleia.[140]
Um dos métodos de arrecadação de fundos dá-se através de impostos de propriedade.[136] Cada distrito, cidade ou vila pode criar impostos locais para a manutenção dos serviços realizados por tais subdivisões provinciais.[141] Os impostos são coletados pelo governo de Tóquio e parte destes são repassados para as subdivisões.[136]
O Governo Metropolitano possui alguns símbolos provinciais, os quais são: Flor de cerejeira Yoshino, ginkgo (ginkgo biloba), guincho (Larus ridibundus), além do símbolo metropolitano e o brasão que são usados em facilidades que são de propriedade (ou operadas pelo) do Governo Metropolitano de Tóquio.[142]
A polícia é administrada pelo Departamento Metropolitano de Polícia de Tóquio, o qual se encarrega de manter a ordem cidadã dentro de toda a metrópole, resguardando a segurança das pessoas.[143] Cabe à instituição a tarefa de zelar pela paz e manter a ordem dentro da cidade, além de atuar preventivamente em caso de desastres naturais, como tufões e terremotos, os quais são muito frequentes no Japão.[144][145] Em toda a área comercial e residencial de Tóquio, as forças policiais possuem 102 estações de polícia e 826 postos (koban) repartidos pelos 23 bairros, contando com uma força uniformizada de mais de 43 mil pessoas.[146]
O Departamento Metropolitano de Polícia de Tóquio administra a Academia Metropolitana de Polícia, que foi fundada no ano de Meiji doze (1879) com o objetivo de fornecer fornecer a formação dos policiais locais.[147] Era localizada próximo ao palácio imperial do Japão, porém foi transferida para a cidade de Fuchu em 6 de agosto de 2001.[147] O departamento também tem sob sua jurisdição o Museu da Polícia, que abriga exibições de objetos da história sobre as polícias tanto de Tóquio quanto do Japão.[148] O Museu já chegou a ser criticado por, Chelsea Szendi Schieder, professora adjunta do departamento de ciências políticas e econômicas da Universidade Meiji por, segundo ela, apresentar um ponto de vista da polícia em relação a eventos da história japonesa e por retratar protestos como uma "ameaça para a ordem política".[149]
Tóquio é uma cidade-irmã com as seguintes cidades e estados:[150]
As cidades especiais (特別区 -ku) diferem de outras cidades por terem uma relação administrativa única com o governo da prefeitura.[136] Certas funções municipais, como abastecimento de água, coleta de esgoto e bombeiros são gerenciadas pelo governo metropolitano de Tóquio.[136] Para pagar os custos administrativos adicionais, o município cobra impostos municipais, que normalmente seriam cobrados pela cidade.[136][151] A área das 23 cidades especiais de Tóquio corresponde à antiga fronteira da cidade e é parte de uma unidade geopolítica única chamada de Tóquio-to (都 -to-) traduzido para "metrópole".[63][152] As 23 cidades especiais de Tóquio são:[153]
As vinte e seis cidades (市 -shi) de Tóquio que ficam na parte ocidental são:[153]
O governo metropolitano de Tóquio designou Hachiōji, Tachikawa, Machida, Ōme e Tama New Town como centros regionais da área de Tama, como parte de seus planos de dispersar as funções urbanas para longe do centro de Tóquio.[154]
Os cinco centros (町 -chō or machi) de Tóquio são:[153]
Tóquio, em 2008, era a cidade com maior produto interno bruto (PIB) (medido pelo seu poder de compra) do mundo, valor que na época foi de 1,4 trilhão de dólares;[155] Em 2015, de acordo com o governo metropolitano, o valor do PIB era de 868,6 bilhões de dólares.[156][nota 5]
Atualmente, como um centro financeiro de alcance global,[159] Tóquio é o principal centro de negócios e possui o maior mercado consumidor da Ásia.[160] Foi descrita por Saskia Sassen como um dos três "centros de comando" para a economia mundial, juntamente com a Nova Iorque e Londres.[161] Esta cidade é considerada um cidade global alfa +, listada pelo inventário da GaWC de 2018.[162] No índice Global Financial Centres Index de 2019, Tóquio foi considerada o sexto centro financeiro mais competitivo do mundo e o quarto da Ásia (atrás de Singapura, Hong Kong e Xangai).[163]
A Bolsa de Valores de Tóquio é uma das dez principais do mundo, sendo a principal fora dos Estados Unidos, com uma capitalização de mercado de 5,6 trilhões de dólares e seu principal índice é o Nikkei 225.[164] As maiores empresas com ações na bolsa de Tóquio são: Toyota, Softbank, Nippon Telegraph and Telephone, Keyence e NTT Docomo.[164]
A maioria das instituições financeiras do país, e também multinacionais, tem sua sede em Tóquio, tanto é, que em 2008 verificou-se que das empresas cotadas na Global 500, 47 são baseados em Tóquio, quase duas vezes maior do que o segundo colocado que é a cidade de Paris.[165] Durante o crescimento centralizado da economia japonesa depois da Segunda Guerra Mundial, muitas companhias moveram seus escritórios centrais de cidades como Osaka, que é a capital histórica do comércio, para Tóquio.[166] Na verdade, essa tendência se iniciou durante a Era Meiji, devido ao enfraquecimento do poder comercial de Osaka.[166] Entre as companhias que possuem a sua sede em Tóquio se destacam: Honda, Sony, Mitsubishi e Hitachi.[167]
Durante catorze anos seguidos, foi eleita pela Economist Intelligence Unit como a cidade mais cara (ou de custo de vida mais alto) no mundo.[168] No resultado de 2010, Tóquio ficou em segundo lugar em estudo divulgado pela empresa de consultoria "Mercer", ficando atrás apenas de Luanda, Angola.[169] No ranking de 2018 da Mercer, Tóquio também ficou em segundo lugar, ficando atrás de Hong Kong.[170]
O turismo é uma das suas principais fontes de renda.[172] Em 2006, o turismo representou 5,7% da receita total da cidade de Tóquio.[172] Milhões de turistas, boa parte deles estrangeiros, visitam Tóquio anualmente.[173] Além de suas muitas atrações turísticas, a cidade também sedia alguns grandes eventos anuais, como a parada dos bombeiros de Tóquio em 6 de janeiro.[174] Os festivais mais importantes da metrópole são: o Festival de Sanja, o de Kanda e o de Sanno; o primeiro ocorre na terceira semana de maio, o segundo também no mês de maio em anos ímpares e o terceiro no de junho em anos pares.[175][176][177]
Por ser um dos principais pontos históricos e culturais do Japão, a prefeitura de Tóquio é a região do país que mais recebe turistas.[173] Em 2017, recebeu quase a metade dos turistas internacionais que chegam ao país (cerca de 46,2%), essa porcentagem ultrapassou 50% em todos os anos entre 2011 e 2015 exceto 2013.[173] Em 2017, Tóquio era a oitava cidade mais visitada do mundo de acordo com o Mastercard Global Destination Cities Index.[178] De acordo com o mesmo levantamento, a quantidade de turistas internacionais que visitou a cidade era cerca de 11,93 milhões.[178] Entre os visitantes que chegaram à cidade em 2017: 59,6% eram provenientes de países asiáticos, 12.9% da América do Norte, 9,4% da Europa e 16,2% de outros países.[179][nota 6] O governo metropolitano de Tóquio criou um site que serve como um guia turístico oficial da metrópole conhecido como Go Tokyo.[180]
Entre os hotéis que têm a disposição, os turistas podem optar por ficar naqueles construídos e mobiliados em estilo ocidental, ou em Ryokans, construídos e mobiliados ao estilo japonês.[181][182] Estes hotéis possuem, por exemplo, portas que se deslocam em um sentido lateral, chamadas de shoji, e de um tapete, chamado de tatame.[182][183] A maior parte dos apartamentos da cidade de Tóquio são pequenos, sendo que os mais baratos chegam a ter no máximo 30 m².[184]
Os pontos turísticos mais conhecidos são: a Torre de Tóquio: uma torre de 333 metros de altura,[185] localizada ao sul do Palácio Imperial; o Palácio Imperial do Japão, a residência oficial do imperador;[186] os vários templos budistas também são muito conhecidos, estima-se que haja mais de 77 mil por todo o Japão, entre os principais estão o templo Zozo-ji em Minato e o templo Sensoji em Asakusa;[124] e os jardins e parques, como o Parque Yoyogi[187] e o Parque Ueno, este último famoso por possuir muitos museus, como o Museu Nacional de Tóquio, e mais de mil cerejeiras que florescem entre o final de março e começo de abril, o que atrai muitos observadores, que costumam ser chamados de Hanami.[188][189] O santuário Meiji é o principal santuário xintoísta de Tóquio e é muito visitado durante a época de ano novo.[122]
Segundo o relatório anual do Governo Metropolitano de Tóquio de 2006, o número de passageiros do sistema, que inclui ônibus, metrô, trens de superfície e bondes, chega a 43 milhões por dia - ele supera o da população total porque as pessoas fazem mais de uma viagem diariamente.[112] Os transportes públicos dentro de Tóquio são dominados por uma extensa rede considerada limpa, pontual e eficiente.[190]
O investimento em transporte de massa foi a saída encontrada pelas autoridades para suportar o grande crescimento populacional da metrópole durante o século XX.[112] O transporte público de Tóquio é administrado pelo Departamento de Transportes, também conhecido como Toei (都営, Toei?).[191]
Tóquio, como o centro da Região Metropolitana de Tóquio, é o maior eixo de transporte ferroviário do Japão.[192] O metrô conta com 283 estações e 292 quilômetros de linhas, cinco vezes a extensão do metrô de São Paulo.[112] A Estação de Shinjuku é a estação de trem mais movimentada do mundo em volume de passageiros.[193] Este sistema ferroviário fica tão lotado nas horas de pico que várias estações empregam funcionários, denominados oshiyas, especialmente designados para empurrar e compactar pessoas dentro dos trens, no momento em que as portas dos trens estão fechando.[112]
O sistema de metrôs de Tóquio é administrado por duas empresas: a Toei Subway e a Tokyo Metro; o primeiro é administrado pelo Toei e o segundo foi criado em 30 de dezembro de 1927 e, mesmo que todas as suas ações sejam de propriedade dos governos do Japão (53,4%) de Tóquio (46,6%), é considerado desde 2004 uma empresa privada.[194][195] Essas duas empresas administram um total de treze linhas de metrô, com a Toei sendo responsável por quatro e a Tokyo Metro por nove.[194][196]
A construção do Tōkaidō Shinkansen entre Tóquio e Osaka iniciou-se em abril de 1959 e terminou em 1964, faltando menos de uma semana e meia para as primeiros Jogos Olímpicos de Tóquio.[197] Antes do trem-bala uma viagem entre as duas cidades levaria seis horas e meia.[198] Hoje bastam 2 horas e meia para fazer o percurso de cerca de quinhentos quilômetros.[198]
É servida pelo Tokyo International Airport (東京国際空港, Tōkyō Kokusai Kūkō?) (ou Aeroporto de Haneda (羽田空港, Haneda Kūkō?)), o quinto aeroporto mais movimentado do mundo, e que atende principalmente a voos domésticos.[199][200] Passaram pelo aeroporto 66 935 990 passageiros em um período de doze meses de fevereiro de 2007 até fevereiro de 2008.[201]
Outro aeroporto que se destaca, o Aeroporto Internacional de Narita,[201] está localizado na cidade de Narita, província de Chiba, e movimenta principalmente os voos internacionais que servem Tóquio.[46] Movimentou mais de 40,5 milhões de passageiros em 2017.[46]
Com relação ao uso de ônibus, em Tóquio este não é tão intenso quanto a utilização do sistema de metrô e, geralmente, os ônibus demoram mais para se locomover de um lugar para o outro.[202][203] O principal sistema de ônibus de Tóquio é o Toei Bus, que foi inicialmente constituído em 1924 como uma medida de emergência para a metrópole, pois o sistema de bondes havia sido danificado pelo Grande Terremoto de Kanto no ano anterior.[191] O Toei Bus opera nas 23 cidades especiais e em parte da região de Tama com cerca de 1 400 ônibus, muitos deles com facilidades de acesso para passageiros idosos e, desde 1991, o sistema realiza medidas para fazer com que seus ônibus sejam mais ecológicos.[204] Além do sistema Toei também existem vários outros, tanto públicos quanto privados.[202]
Devido à alta quantidade de canais que possui, o transporte de barcos pelos rios é bastante utilizado.[76] O Ônibus de Água (水上バス, Suijō Basu?) é o serviço de transporte marítimo mais notável de Tóquio.[205] Ele é mais utilizado para acessar as ilhas artificias da metrópole, principalmente Odaiba, e também é usado para viajar pelo Rio Sumida.[205] A maior parte dos ônibus aquáticos é operada pela companhia Tokyo Cruise Ship, uma empresa privada;[206] porém, alguns são operados pela Associação Metropolitana de Parques de Tóquio.[205] A principal rota de balsas que serve como transporte interno é a Tokyo-Wan, entre Yokosuka, Kanagawa e Futtsu, Chiba, cruzando a baía de Tóquio.[207] O portos marítimos de Tóquio e Yokohama são os principais e mais movimentados do Japão.[208][209]
O Governo Metropolitano de Tóquio é responsável pela administração, através do Departamento Metropolitano de Educação de Tóquio, e pelo fornecimento de verbas de mais de duas mil escolas, sendo que algumas delas são para pessoas com necessidades especiais.[210][211][212] A instituição também é responsável pela administração das propriedades de valor cultural da metrópole.[211]
O sistema de bibliotecas públicas é conhecido como Tokyo Metropolitan Library, e na verdade consiste em duas bibliotecas: a Biblioteca central e a Biblioteca Tama.[213] Antigamente existia nesse sistema uma terceira biblioteca, a Biblioteca de Hibiya, porém esta terminou sendo fechada em 1º de abril de 2009.[214] A Biblioteca central, ou Biblioteca Nacional da Dieta, está aberta ao público em geral, mas sua função principal é ajudar os membros do parlamento japonês em pesquisas.[215]
As seis instituições de ensino superior mais proeminentes são conhecidas como As Seis Universidades de Tóquio (东京六大学, roku Tokyo daigaku?): Universidade de Keio, Universidade de Tóquio (a mais conhecida, também chamada de "Tōdai"),[216] Universidade de Waseda, Universidade Hosei, Universidade de Meiji e Universidade Rikkyo.[217] As Seis formam a liga de beisebol mais antiga do Japão: a Tokyo Big6 Baseball League.[218] A Tōdai é a mais prestigiada, e em 2019 classificou-se na posição 99 entre as duzentas maiores universidades do mundo e na posição oito quando são consideradas as universidades da Ásia, sendo estas as melhores colocações entre as universidades japonesas.[219][220] Estão localizadas em Tóquio as universidades Keio e Waseda, as principais instituições de ensino superior privado do Japão.[221][222]
O Japão é um país bastante conhecido pelo seu alto grau de desenvolvimento na área de ciência e tecnologia, principalmente nos ramos de robótica, eletrônicos e medicina.[223][224][225] O governo do país possui o objetivo de fazê-lo "ser o primeiro país a provar que que é possível crescer através da inovação até quando a sua população diminui" e também de transformá-lo em uma "Sociedade 5.0".[225]
Tóquio, como capital do Japão, pode ser considerada um dos centros da inovação tecnológica.[226] É considerada uma cidade futurista devido a produtos como o trem-bala e porque seu governo é muito comprometido a abranger novas tecnologias oferecendo um ambiente favorável para a atuação de companhias de tecnologia e startups.[226] Por causa disso, ficou em primeiro lugar na edição de 2018 do ranking Innovation Cities Index, feito pela provedora de dados comercial 2thinknow, que destacou que a metrópole abarcou bastante a robótica e a fabricação em 3D, que foram identificadas como "tendências que abalam o mundo".[226]
O Governo Metropolitano administra a saúde pública de Tóquio através de três órgãos: o Escritório de Bem-Estar Social e Saúde Pública e o Office of Metropolitan Hospital Management e Tokyo Metropolitan Health and Medical Treatment Corporation.[139] O primeiro lida com a criação de políticas de saúde pública e amparo social, como construir sistemas que facilitem o acesso a cuidados de saúde e fornecer assistência a moradores de rua.[139] O segundo e terceiro órgãos ficam responsáveis por administrar os catorze hospitais públicos de Tóquio sob a tutela das autoridades da metrópole.[139][227] Em 2019, a revista Newsweek, em parceria com uma companhia de análise de dados chamada Statista Inc., realizou uma classificação dos melhores hospitais do mundo.[228] O Hospital da Universidade de Tóquio, conquistando a melhor colocação entre os hospitais japoneses, ficou entre os dez melhores do mundo, atingindo a oitava posição.[228] Dos 105 hospitais japoneses que foram analisados, 21 se localizavam na cidade de Tóquio.[228] Na edição de 2019 do Safe Cities Index, Tóquio ficou na segunda posição dentro da categoria "Segurança de Saúde", perdendo apenas para Osaka, isso significa que o acesso a cuidados de saúde e a qualidade dos serviços desse tipo em Tóquio são considerados muito bons.[131]
Em 2015, a expectativa de vida em Tóquio para pessoas do sexo masculino e do feminino era de, respectivamente, 81,07 anos e 87,26 anos.[229][230] Ao se analisar a diferença entre as pirâmides etárias de Tóquio de 1970 e 2010, percebe-se que houve um estreitamento na base da pirâmide e um alargamento no topo, indicando que houve um aumento da  população idosa.[231] De acordo com o Escritório de Bem-Estar Social e Saúde Pública, a população da metrópole começará a diminuir em 2025 e, dez anos depois, um em cada quatro cidadãos de Tóquio terá mais de 65 anos.[232]
A imensa população de Tóquio cria uma altíssima demanda por residências.[233] Em Tama, o governo provincial criou um projeto de residenciamento barato, para famílias de baixa renda.[42][43] Porém, estas residências estão localizadas muito longe dos principais centros comerciais e industriais, e por isso muitos destes trabalhadores de baixa renda são obrigados a passar por vezes várias horas no caminho de casa para o trabalho.[234][235] De acordo com uma classificação de 2007 feito pelo grupo imobiliário Knight Frank e do Citi Private Bank, subsidiária do Citigroup, Tóquio é a quinta cidade mais cara do mundo quanto ao preço dos imóveis residenciais de luxo: 17,6 mil euros por metro quadrado.[236][237]
Sua tecnologia aliada aos recursos naturais deram ao Japão acesso à água potável e tratamento de esgoto em quase todo o território nacional.[238] Devido à rápida urbanização de suas grandes cidades, ocorreu a degradação ambiental que causou enchentes, aridez e piora da qualidade da água.[238] Para atenuar os danos causados por esses problemas, foram implantadas medidas para melhorar os mecanismos de coordenação sobre o uso da água e prevenir a sua contaminação.[238] Como resultado, o Japão obteve drásticas melhorias em seus recursos hídricos e de higiene e abastecimento de água potável em seu território.[238] Cidades como Tóquio e Quioto foram as grandes beneficiadas dos projetos.[238] A Seção do Sistema de Distribuição de Água do Governo Metropolitano de Tóquio gerencia a uma rede de abastecimento de água de 26 mil quilômetros.[238] A taxa de vazamento da rede de água de Tóquio é de aproximadamente 3,6%, enquanto a taxa de vazamento de água das grandes cidades do mundo é de cerca de 30%, em média.[238] O modelo de abastecimento de Tóquio é seguido em vários países no mundo sendo que enviaram especialistas nessa área para países em desenvolvimento para fornecer assistência técnica nessa área.[238]
O Japão é um país bastante conhecido pelo seu alto número de terremotos.[239] Devido a esse fato, tanto o governo nacional quanto o de Tóquio investem pesadamente em infraestrutura para minimizar danos que poderiam ser causados por um abalo sísmico.[239][240] As leis nacionais que regulam os critérios para a construção de edifícios diz que estes devem ter "danos mínimos" em um terremoto de médio porte, e que "um prédio não deve ser suscetível a desabar em um terremoto de grande porte".[240] De acordo com um estudo da Universidade de Tóquio, 87% dos prédios da metrópole obedecem a essas exigências.[239]
Os arranha-céus mais novos apresentam uma variedade de dispositivos anti-sísmicos, incluindo grandes "amortecedores" que agem como pêndulos e reagem às ondas feitas pelos terremotos, semelhante a absorvedores de choques.[240] Além disso, o governo de Tóquio realiza diversas medidas contra outros desastres naturais como tempestades e enchentes, a principal medida feita nesse sentido foi a construção do maior sistema de drenagem construído pelo homem, o Canal Subterrâneo de Drenagem Externa da Área Metropolitana (também conhecido como G-cans).[240][241] O G-cans possui mais de seis quilômetros de extensão e uma série de cinco silos com 65 metros de altura para coletar o excesso de água que chegar para a cidade, impedindo inundações.[240] Devido a toda a infraestrutura de Tóquio para prevenir danos de desastres naturais, Tóquio ficou na quarta posição dentro da categoria "Segurança Estrutural" na edição de 2019 do Safe Cities Index, perdendo para Barcelona, Osaka e Singapura.[131]
A maioria dos cidadãos de Tóquio usam vestimentas ocidentais, no dia a dia.[242] Algumas pessoas mais velhas - especialmente mulheres - porém, ainda usam o quimono, uma roupa típica japonesa.[242][243] Roupas tradicionais japonesas são usadas, geralmente, apenas em dias ou eventos especiais como o hatsumode (a primeira visita a santuários ou templos no Ano Novo), o seijinshiki (cerimônia que celebra a maioridade entre os jovens), casamentos e cerimônias de graduação.[242][243] Harajuku, um bairro de Shibuya, é conhecido internacionalmente por seu estilo e da moda jovem.[244]
Tóquio é uma metrópole bastante conhecida por sua importância no cenário artístico da Ásia.[245][246] Os dois principais eventos de arte da cidade são o Art Fair Tokyo, que acontece em março ou abril, e o Tokyo Art Beat.[245][247] Um tipo de gravura popular no Japão, conhecido como Ukiyo-e foi criado em Tóquio quando esta ainda se chamava Edo.[248] Dois museus em Tóquio apresentam exibições sobre o Ukiyo-e: o Museu Nacional de Tóquio e o Museu Sumida Hokusai.[248]
Tóquio tem dezenas de museus da arte, história, ciência e tecnologia. O museu mais importante do Japão é o Museu Nacional de Tóquio, dentro das dependências do Parque Ueno.[249][250] O museu é administrado pelo governo do país, através de uma instituição de administração independente conhecida como National Institute for Cultural Heritage.[251][252] O museu possui uma coleção de antiguidades e obras de arte tanto do Japão quanto de outros países asiáticos.[253]
O Museu Metropolitano de Arte (東京都美術館, Tōkyōto Bijutsukan?), fundado em 1926, está dividido em uma galeria que expõe os trabalhos de artistas nacionais contemporâneos; e uma com exposições especiais organizadas com a cooperação de jornais e companhias de televisão.[254] O museu Shitamachi, localizado na esquina sudeste do parque Ueno,[188] está dedicado a preservar a cultura de Tóquio durante a era Edo.[255][256] O Mingeikan é um museu fundado por Yanagi Muneyoshi em 1936, consagrado para o artesanato popular de todo o país.[257] O museu Goto mostra a coleção privada de arte japonesa que era propriedade de Goto Keita, fundador do museu e da Tokyu Corporation.[258][259] Neste museu se encontra um trabalho designado como um tesouro nacional no Japão, conhecido como The Tale of Genji.[258] No Museu da Espada Japonesa, ou Tōken hakubutsukan (刀剣博物館, Tōken hakubutsukan?), regido pela Associação para a Conservação de Arte da Espada Japonesa e tem como objetivo preservar e divulgar espadas japonesas e espalhar a cultura japonesa com relação a espadas.[260] O Museu Metropolitano de Fotografia de Tóquio (東京都写真美術館, Tōkyō-to Shashin Bijutsukan?), localizado em Ebisu,[261] foi inaugurado em 1995 e é o primeiro museu abrangente sobre fotografia e vídeo no país e possui como objetivo aprimorar e desenvolver a cultura de fotografia e vídeo no Japão.[262] Onde atualmente o museu possui uma coleção de aproximadamente 20 mil fotografias, cerca de 30 mil livros e 720 títulos de periódicos em fotografia e imagens visuais.[263] Entre os museus de ciência e tecnologia mais destacados há dois na ilha artificial de Odaiba: o Museu de Ciências Marítimas, e o Museu Nacional de Ciência Emergente e Inovação.[53]
O Japão possui três formas de teatro tradicionais no país e que hoje são consideradas Património Cultural Imaterial da Humanidade pela UNESCO, que são: Noh, Kabuki, e Bunraku.[264] A primeira surgiu no século XIV e as duas últimas durante o Período Edo.[264][265][266] Em Tóquio, o Teatro Kabuki-za é um local de destaque na realização do Kabuki,[267] e o Teatro Nacional de Noh para a arte que lhe dá nome, apesar dela ocorrer muito em apresentações ao ar livre;[264][268] o Bunraku em Tóquio é mais realizado no Teatro Nacional.[266]
A cidade de Tóquio possui diversos teatros em alguns de seus distritos; os que se sobressaem nesse sentido são Shimokitazawa, Hatsudai, Ikebukuro e Sangenjaya.[269] O Teatro Nacional de Tóquio, administrado pelo Conselho de Artes do Japão e considerado patrimônio imaterial pela UNESCO, foi fundado em 1966 com o objetivo de preservar as artes cênicas tradicionais do Japão, realizando apresentações não somente das que já foram mencionadas, como também da Nihon buyo e da Gagaku.[270] Porém, este teatro também realiza apresentações de artes ocidentais como ópera e balé.[271] O teatro administrado pelo governo de Tóquio, o Teatro Metropolitano de Tóquio, se destaca por ser um dos principais locais de apresentação da Orquestra Sinfônica Yomiuri Nippon, considerada uma das mais renomadas do Japão.[272]
Possui um elevado número de restaurantes, enquanto em comparação grandes cidades como Paris e Nova Iorque possuem cerca de 20 mil restaurantes a região metropolitana de Tóquio possui mais de 160 mil.[273][274] Em novembro de 2007, foi lançado no guia Michelin lançou seu guia de restaurantes finos, sendo que Tóquio, ganhou 191 estrelas no total, ou aproximadamente o dobro do seu concorrente mais próximo, no caso Paris.[274] Oito estabelecimentos foram agraciados com o máximo de três estrelas, 25 receberam duas estrelas e 117 ganhou uma estrela.[275] Dos oito melhores restaurantes avaliados, três oferecem jantares finos tradicional japonês, duas casas de sushi e três servem culinária francesa.[275]
Os pratos representativos são sobá (荞麦, sobá?), o macarrão frio, considerados como os melhores em Tóquio,[276] tempurá (てんぷら天麸罗, tempurá?),[277] oden (御田, おでん, oden?)[276] e sushi (寿司, 鮨, 鮓, sushi?). Edo era conhecida pela pressa de seus moradores, e no século XIX Yohei Hanaya criou uma forma fácil de fazer sushis.[278] O Chankonabe (ちゃんこ鍋, Chankonabe?) é o alimento comido por lutadores de sumô.[279] Por causa do vínculo indissolúvel a cidade com esta arte marcial, o Chankonabe tornou-se um alimento popular, então há uma abundância de restaurantes especializados em Chankonabe.[279]
A arquitetura de Tóquio foi bastante influenciada pela história da metrópole, pois esta foi deixada em ruínas duas vezes durante o século XX: a primeira vez foi em 1923 com o Grande Terremoto de Kanto e a segunda devido aos bombardeios na Segunda Guerra Mundial.[2][28] Por causa disto e de outros fatores, predominam na paisagem urbana de Tóquio a sua arquitetura mais moderna, com o número de prédios mais antigos sendo considerado escasso.[280]
Entre as estruturas de Tóquio as que mais se destacam são: a Tokyo Skytree, o Tokyo Midtown e o Shōfuku-ji.[281] A primeira por ser a torre mais alta do mundo, com 634 metros;[60] a segunda por ser o segundo edifício mais alto do Japão;[281] e a terceira por ser o edifício mais antigo de Tóquio, foi construído em 1407.[281] Além desses três, também são importantes os seguintes: A Estação de Tóquio, a Catedral de Santa Maria e a Nakagin Capsule Tower.[281][282]
Tóquio é considerada um lugar eclético quando se trata dos estilos musicais que existem na sua região.[283] A música pop é o estilo que mais se destaca na metrópole, porém também são notáveis nela o jazz, o rock, a música feita por Djs e, claro, a música tradicional japonesa.[284][285] Entre os artistas que nasceram em Tóquio estão: Kyary Pamyu Pamyu, Takahiro Morita, Nujabes, Sho Sakurai e Satoshi Ohno;[carece de fontes?] os grupos musicais AKB48, Arashi, Yellow Magic Orchestra e Puffy (além de vários outros) foram formados em Tóquio.[286]
Os festivais de música que mais se destacam são o Summer Sonic e o Ultra Japan.[283] O primeiro ocorre anualmente durante o verão do hemisfério norte e acontece de maneira simultânea em Osaka e Tóquio (mais especificamente em Chiba), atraindo milhões de pessoas do mundo todo.[283] O Summer Sonic já realizou tanto apresentações de cantores e bandas do Japão, como Kyary Pamyu Pamyu, quanto do exterior, como o grupo Metallica.[283] O Ultra Japan é um evento de música eletrônica internacional que acontece anualmente em Odaiba, durante três dias, e que atrai mais de quatrocentas mil pessoas.[283]
Esportes como o judô e o sumô, que fazem parte da cultura da cidade por séculos ainda são muito apreciados pela população de Tóquio.[287][288] Porém, esportes ocidentais, como futebol, basquetebol, tênis e especialmente o beisebol estão ficando cada vez mais populares entre a população da cidade, especialmente entre os jovens.[289]
Os Jogos Olímpicos de Verão de 1964 foram realizados em Tóquio, e ocasionaram um grande impacto no aspecto urbano da cidade, pois foram construídas grandes obras de complexos desportivos de infraestrutura da cidade e de transporte que custaram cerca de três bilhões de dólares, com parte desse valor paga pelos Estados Unidos.[290][291] Entre instalações utilizadas em 1964, encontram-se o antigo Estádio Olímpico de Tóquio, que sediou o evento, o Nippon Budokan (arena de artes marciais), e o Ginásio Nacional Yoyogi.[291][292][293] A primeira delas foi demolida em maio de 2015 para dar lugar ao Novo Estádio Olímpico.[294][295] Foi candidata para os Jogos Olímpicos de Verão de 2016, mas acabou perdendo para o Rio de Janeiro.[296][297] Quatro anos mais tarde, foi eleita sede dos Jogos Olímpicos de Verão de 2020.[61]
Igual ao resto do país, o sumô (相撲, sumō, às vezes 大相撲, ōzumō) tem um lugar destacado entre os esportes em Tóquio.[288] No Estádio Nacional de Sumô, localizado em Ryogoku, ocorrem os torneios de janeiro, maio e setembro, que atraem muitos espectadores.[288][298] Os treinamentos de sumô são sempre em estábulos, ou baias, e muitos permitem a entrada de espectadores.[288] Aliás no Japão o sumô também deve o impulso de o Imperador Meiji ter sido um praticante do esporte.[299] Na cidade se praticam de maneira profissional outras artes marciais, especialmente o judô, incluído em 1964 como esporte olímpico,[300] além do kendo, do caratê, do kyudo e do aikido.[287]
Atualmente, o esporte mais popular em Tóquio é o beisebol, esporte ocidental mais popular da província.[301] O estádio Tokyo Dome (東京ドーム, Tōkyō Dōmu?) é sede de uma das equipes mais populares de beisebol do país, sendo também a mais antiga delas[302], os Yomiuri Giants (読売ジャイアンツ, Yomiuri Jaiantsu?).[303][304] A cidade também sedia a equipe Tokyo Yakult Swallows (東京ヤクルトスワローズ, Tōkyō Yakuruto Suwarōzu?), que joga no estádio Meiji Jingu Stadium.[303]
A Liga profissional japonesa de futebol, conhecida como J. League (Jリーグ, J Rīgu?), fundada em 1993,[305] tem em Tóquio duas equipes: o F.C. Tokyo (FC東京, Efushī Tōkyō?) e o Tokyo Verdy 1969 (東京ヴェルデ1969, Tokyo Verdy 1969?).[303] Ambas jogam e possuem como sede o Estádio de Tóquio, que possui capacidade para quase 50 mil pessoas.[306] Entre os anos 1980 e 2004 a cidade foi sede da Copa Intercontinental de Clubes, chamada na época de Copa Toyota, que enfrentava os ganhadores da Copa da Europa (atual Liga dos Campeões da UEFA) e a Copa Libertadores da América.[307] A partir de 2005 o campeonato internacional passou ao formato de Campeonato Mundial de Clubes da FIFA, cujos clubes de todas as confederações do mundo se enfrentam no mês de dezembro.[308] Tóquio e Yokohama foram sedes da competição nos intervalos de 2005-2008, 2011-2012 e 2015-2016.[309]
Em Tóquio existe apenas um feriado municipal: o Dia dos Cidadãos (Tomin no Hi), que ocorre no dia 1º de outubro.[310][311] Em 1º de outubro de 1898, durante a Era Meiji, foi o dia em que Tóquio se tornou uma cidade tendo o seu próprio prefeito.[312] A data comemorativa foi instituída em 1922 como "Dia do Auto-governo", tendo o seu nome alterado para o atual em 1952.[312]
Coordenadas: 22° 05' N 90° 50' EO Rio Ganges (em hindi e na maior parte das línguas indianas: गंगा; romaniz.: Gaṅgā; ? गंगा; em bengali: গঙ্গা; romaniz.: Gōnga) é um dos principais rios do subcontinente Indiano, e um dos vinte maiores do mundo em caudal.[1] Suas águas se deslocam rumo ao leste através da planície do Ganges do norte da Índia até ao Bangladesh. Com 2510 km de extensão, nasce no Himalaia ocidental, no estado indiano de Uttarakhand, e deságua no Delta do Ganges, no golfo de Bengala. Desde muito tempo é considerado um rio sagrado para os hindus, que o veneram na forma da deusa Ganga, e também possui um grande valor histórico: diversas capitais de províncias ou impérios, como Patliputra, Kannauj, Kara, Prayagraj, Murshidabad e Calcutá, localizam-se em suas margens. O Ganges e seus afluentes abrangem uma bacia hidrográfica fértil de cerca de um milhão de quilômetros quadrados, que é a mais densamente povoada do planeta, com mais de 400 milhões de pessoas e uma densidade populacional de cerca de 390 hab/km2.[2] A profundidade média do rio é de 16 metros, e a máxima é de 30 metros.
O ex-primeiro-ministro da Índia Jawaharlal Nehru, em seu livro Descoberta da Índia, atribui ao rio diversos significados simbólicos:
Embora diversos cursos de água formem a nascente do Ganges, os seis riachos e suas cinco confluências recebem diferentes ênfases geográficas e culturais (ver mapa). O rio Alaknanda se encontra com o Dhauliganga em Vishnuprayag, o rio Nandakini em Nandprayag, o rio Pindar em Karnaprayag, o rio Mandakini em Rudraprayag e, finalmente, o rio Bhagirathi em Devprayag, formando o curso principal, o Ganges. O Bhagirathi é o principal destes rios, e nasce ao pé do Glaciar Gangotri, em Gaumukh, a uma altitude de 3.892 metros. A nascente do Alaknanda é formada pela água derretida das neves de picos como o Nanda Devi, o Trisul e o Kamet.
Após percorrer 200 quilômetros através de um estreito vale em meio ao Himalaia, o Ganges passa por um desfiladeiro e chega na planície Gangética, na cidade de Haridwar, centro de peregrinação; lá, uma represa desvia parte de suas águas até o Canal do Ganges, que irriga a região de Doab, em Uttar Pradesh. O percurso do Ganges, que até então tinha uma direção sudoeste, passa então a se dirigir ao sudeste, através das planícies do norte da Índia.
O rio segue então um curso curvo, de 800 km, que passa pela cidade de Kanpur antes de receber, do sudoeste, pelo Yamuna, em Prayagraj. Este ponto é conhecido como o Sangam em Allahabad; Sangam é um local sagrado do hinduísmo e, de acordo com textos hindus antigos, certa vez um terceiro rio, o Sarasvati, encontrava os outros dois neste ponto.
Após diversas confluências, com rios como o Kosi, o Son, o Gandaki e o Ghaghra, o Ganges forma uma correnteza formidável no trecho entre Prayagraj e Malda, na Bengala Ocidental. Durante o percurso passa pelas cidades de Mirzapur, Buxar, Varanasi, Patna e Bhagalpur. Nesta, o rio contorna os Montes Rajmahal, e começa a se dirigir rumo ao sul. Em Pakur o rio começa a perder força, com a ramificação do primeiro de seus afluentes, o Bhāgirathi-Hooghly, que forma em seguida o rio Hooghly. Nas proximidades da fronteira com Bangladesh, a Barragem de Farakka, construída em 1974, controla o fluxo do Ganges, desviando parte de suas águas para um canal ligado ao Hooghly, de modo a mantê-lo relativamente livre de silte.
Após entrar no Bangladesh, o Ganges passa a ser conhecido como Padma, até receber as águas do Jamuna, o maior afluente do Brahmaputra. Mais adiante, o Ganges recebe as águas do rio Meghna, o segundo maior afluente do Brahmaputra, e passa a ser chamado de Meghna ao entrar no estuário do Meghna. Ao chegar no delta do Ganges, com 350 km de largura, ele finalmente desagua na baía de Bengala. Apenas dois outros rios no mundo, o Amazonas e o Congo, possuem um volume de água maior que o total combinado do Ganges, do Brahmaputra e do sistema de rios Surma-Meghna.
Situada às margens do rio Ganges, Varanasi é considerada por muitos fiéis a cidade mais sagrada do hinduísmo. O Ganga é mencionado no Rigveda, a mais antiga das escrituras hindus. Consta do Nadistuti sukta (10.75), onde estão listados os rios de leste a oeste. É um costume local espalhar no rio as cinzas de entes queridos que foram cremados.
De acordo com a religião hindu um rei muito famoso, Bhagiratha, praticou por muitos anos, a tapasya, para trazer à Terra Ganga de sua residência nos Céus, para que encontrasse a salvação de seus ancestrais, amaldiçoados por um profeta. Ganga se convence e, através de uma trança de cabelo (Jata) do deus Shiva, desce à Terra para lavar os pecados dos humanos e torná-la pia e fértil. Para os hindus da Índia, o Ganges não é apenas um rio, mas também uma divindade materna, um conjunto de tradições, e muito mais.
Alguns hindus acreditam que uma vida não é completa sem um mergulho no Ganges pelo menos uma vez na vida. Muitas famílias hindus conservam um frasco com água do rio em suas casas, hábito que é considerado prestigioso, para que pessoas à beira da morte possam beber de sua água; muitos hindus acreditam que o Ganges pode limpar uma pessoa de todos os seus pecados, e poderia até mesmo curar a doença. As escrituras antigas mencionam que a água do Ganges porta as bênçãos dos pés do Senhor Vishnu; assim, a Mãe Ganges também é conhecida como Vishnupadi, que significa "pés de Vishnu".
Algumas das congregações religiosas e festivais hindus mais importantes acontecem em torno do rio. Estes eventos, como o Kumbh Mela, realizado a cada doze anos em Prayagraj, são realizados às margens do rio. Varanasi tem centenas de templos situados à beira do Ganges, que frequentemente são alagados durante as estações chuvosas. A cidade, além de ponto importante de peregrinação para hindus de todos os locais, também é tradicionalmente associada à prática da cremação.
A poluição do Ganges tem afetado as 400 milhões de pessoas que vivem próximas  de suas águas.[3]
Desde a última década de 90 e especialmente nos últimos anos, as condições da água do rio e afluentes têm ficado abaixo das consideradas aceitáveis pela OMS, já que o despejo irregular de esgoto tem aumentado, inclusive a partir de um hospital que atende tuberculosos.
O Ganges foi classificado entre os cinco rios mais poluídos do mundo em 2007,[4] com níveis de coliformes fecais próximo a Varanasi mais de mil vezes superior ao limite oficial do governo indiano.[5] A poluição ameaça não somente os seres humanos, mas também as mais de 140 espécies de peixes, 90 de anfíbios e o golfinho-do-ganges, todos ameaçados de extinção.[4] O Plano de Ação Ganga, uma iniciativa ambiental para limpar o rio, tem sido um grande fracasso até agora,[6][7][8] devido à corrupção e à falta de conhecimentos técnicos,[9] falta de um bom planejamento ambiental,[10] crenças e tradições indianas[11] e falta de apoio das autoridades religiosas.[12]
Outro problema é o ritual da cremação dos mortos em suas margens. Dependendo da casta e da situação econômica da família, muitas vezes os corpos não são cremados corretamente e/ou jogados inteiros no rio, contaminando-o. O afluente Yamuna, cuja vida aquática desapareceu, teria recebido US$ 500 milhões em ações de despoluição nos últimos dez anos, segundo o Conselho de Controle de Poluição de Nova Délhi.[13]
Entretanto, o rio oferece aos moradores de sua região suprimento de comida e água fresca. Muitas criaturas nativas, incluindo o crocodilo gavial, vivem às suas margens.
Durante o início das período védico, os rios Indo e Sarasvati eram os principais rios da região; os três Vedas posteriores, no entanto, parecem dar mais importância ao Ganges, gradualmente mais citado nas obras.
Talvez o primeiro ocidental a mencionar o Ganges tenha sido Megástenes, por diversas vezes no decorrer de sua obra, Indika: "A Índia (...) possui muitos rios grandes e navegáveis que, depois de nascerem nas montanhas que se estendem ao longo da fronteira setentrional, atravessam a terra plana; muitos destes, após unirem-se uns aos outros, acabam desaguando no rio chamado Ganges. Este rio, que na sua nascente tem 30 estádios de largura, flui no sentido norte-sul, e deságua no oceano que forma a fronteira leste do Gangaridai, uma nação que possui uma tropa numerosa dos mais enormes elefantes."[14]
Uma representação ocidental do rio pode ser vista na Piazza Navona, de Roma, onde uma escultura célebre — a Fontana dei Quattro Fiumi, "Fonte dos Quatro Rios", da autoria de Gian Lorenzo Bernini, em 1651, — simboliza os quatro grandes rios do mundo (o Ganges, o Nilo, o Danúbio e o Rio da Prata).
A bacia do Ganges, com seu solo fértil, é crucial para as economias agriculturais da Índia e do Bangladesh. Tanto o Ganges quanto seus afluentes fornecem uma fonte perene de irrigação para uma região extensa; entre as principais culturas da área estão o arroz, a cana-de-açúcar, lentilhas, batatas, trigo e sementes usadas na fabricação de óleos.
Os chars são ilhas temporárias formadas pela deposição de sedimentos que sofreram erosão a partir das margens do rio, especialmente no estado de Bengala Ocidental. Cada chat é utilizado como moradia para até 20 000 pessoas; seu solo é extremamente fértil, e portanto pode sustentar plantações ou servir como pasto para o gado — porém pode desaparecer em questão de algumas horas, em consequência de alguma movimentação excessiva na correnteza, especialmente durante a estação das monções. Os habitantes dos chars são refugiados de Bangladesh ou bengaleses, porém o governo de Bengala Ocidental não reconhece sua existência nem lhes concede documentos de identidade para que possam emigrar ou obter empregos em outros locais. O saneamento nestas ilhas é extremamente precário, e os seus moradores não possuem assistência sanitária; a escolaridade também não lhes é acessível, e o analfabetismo é crônico. O governo, no entanto, exige dos moradores das ilhas o pagamento de impostos.[15]
Coordenadas: 6° N, 157° WLocalização do oceano Pacífico* Os valores do perímetro, área e volume podem ser imprecisos devido às estimativas envolvidas, podendo não estar normalizadas.editar - editar código-fonte - editar WikidataO Oceano Pacífico é o maior oceano da Terra, situado entre a América, a leste, a Ásia e a Austrália, a oeste, e a Antártida, ao sul. Com 180 milhões de km² de área superficial, o Pacífico cobre quase um terço da superfície do planeta e corresponde a quase metade da superfície e do volume dos oceanos. Movendo-se um globo terrestre de forma adequada é possível visualizar-se um hemisfério inteiro do planeta coberto apenas por água, ficando todos os continentes no hemisfério oposto, ocultos à visão em tal posição.[1] Em sua essência - excluída pequena área associada ao oceano Antártico - trata-se basicamente do oceano Pacífico, cujas águas ainda avançam sobre o hemisfério não visível. Em vista da teoria das placas tectônicas e da deriva continental, sua origem remonta ao oceano único que cercava a Pangeia em tempos primitivos, o Pantalassa.
Tem 707,5 km de fossas, e 87,8% de sua área apresenta profundidades superiores a 3 000 m; é o oceano com maior profundidade média (4 282 m) e onde se localizam as maiores fossas submarinas (fossa das Marianas, com 11 034 m).
Sua forma grosseiramente circular é delimitada por margens continentais activas (que correspondem ao círculo de fogo do Pacífico) sob as quais se afunda uma crusta oceânica em rápida expansão. Em suas águas foi registrada a maior temperatura em um oceano: 40,4 °C, a uma profundidade de 2 mil metros, a cerca de 480 km ao oeste da costa estadunidense.[2]
Descoberto pelos europeus em 1513 (Vasco Núñez de Balboa), embora desde 1511 que os portugueses navegassem regularmente no mar meridional da China, o qual pertence ao oceano Pacífico, chegando à Tailândia em 1511 e à China em junho de 1513, com Jorge Alvares, portanto antes de Balboa avistar aquele oceano. Transposto pela primeira vez em 1520 (Fernão de Magalhães), o Pacífico tem assistido a um crescimento de sua importância como via de ligação entre algumas das regiões de maior dinamismo econômico da atualidade (Extremo Oriente) e costa ocidental da América do Norte).
É um fenômeno oceânico-atmosférico caracterizado por um aquecimento anormal das águas superficiais no oceano Pacífico Tropical. Altera o clima regional e global, mudando os padrões de vento a nível mundial, afetando assim, os regimes de chuva em regiões tropicais e de latitudes médias.[necessário esclarecer]
La Niña (“a menina” em espanhol) é um fenômeno oceânico-atmosférico que ocorre nas águas do oceano Pacífico (equatorial, central e oriental). A principal característica deste fenômeno é o resfriamento (em média de 2 a 3 °C) fora do normal das águas superficiais nestas regiões do oceano Pacífico.
O fenômeno La Niña não ocorre todos os anos e nem da mesma forma. Sua frequência é de 2 a 7 anos, com duração aproximada de 9 a 12 meses (há casos que pode durar até 2 anos). O La Niña afeta o comportamento climático no continente americano e outras regiões do planeta.
O acompanhamento científico deste fenômeno climático é feito pela Organização Meteorológica Mundial. É feito o monitoramento do oceano Pacífico tropical, através de boias amarradas, marégrafos (instrumentos que registram o fluxo das marés) e satélites. As informações são captadas e analisadas com o objetivo de fazer a previsão do comportamento futuro do La Niña.
Fernão de Magalhães batizou este oceano com o nome de Pacífico por acreditar que ele era mais calmo que o tempestuoso oceano Atlântico. Esta comparação foi feita quando Fernão de Magalhães e os seus companheiros de navegação transpuseram o estreito de Magalhães, uma passagem entre os dois oceanos já citados.[4]
Flanqueado por cadeias montanhosas recentes, com intensa atividade vulcânica, o Pacífico é percorrido por um vasto sistema de dorsais.
A dorsal Sudeste-Pacífica constitui um prolongamento, através da dorsal Pacífico-Antártica, das dorsais do oceano Índico (dorsal Antártico-Australiana). Em sua porção setentrional atinge as latitudes do litoral mexicano, desaparecendo ao penetrar no golfo da Califórnia. Trata-se de uma dorsal em rápida expansão (entre 8,8 e 16,1 cm por ano), sem fossa axial. As zonas de fraturas que a segmentam são numerosas, com deslocamento pronunciado. Essa dorsal emerge na latitude da ilha de Páscoa, unindo-se à dorsal do Chile, que se liga à costa meridional da América, e na latitude das ilhas Galápagos, unindo-se à dorsal de Cocos ou das Galápagos. Essas dorsais dividem o Pacífico em três conjuntos.Os fundos oceânicos situados a leste da dorsal Sudeste-Pacífica pertencem a placa litosférica da Antártida (que corresponde à bacia Pacífico-Antártica e à planície abissal de Bellingshausen), à placa de Nazca (bacias Peruana e Chilena, separadas pela dorsal de Nazca) e à placa de Cocos (limitada pela dorsal de Cocos).
Todo o imenso conjunto de fundos oceânicos situados a oeste da dorsal Sudeste-Pacífica é sustentado pela placa litosférica Pacífica, que a oeste América do Norte apresenta grandes zonas de fraturas, com relevos monumentais, alinhados por milhares de quilômetros ao longo de antigas falhas de transformação.
Mais a oeste, o centro do oceano Pacífico é entrecortado por cadeias submarinas e grandes edifícios vulcânicos, ora emergindo em forma de ilhas (Havaí, Marquesas, Marshall, Carolinas), frequentemente coroadas por formações coralíneas (atóis). As bacias oceânicas que as rodeiam (Médio-Pacífica, Melanésia, Nordeste, Noroeste) apresentam uma delgada cobertura sedimentar sobre a crosta basáltica.
A presença das fossas oceânicas periféricas, ao longo dos arcos insulares (Aleutas, Kurilas, Japão, Marianas, Filipinas, Salomão, Tonga, Kermadec) e da costa ocidental da América (Chile, Peru, América Central) explica-se por corresponderem a zonas de subducção da crosta oceânica, em que esta mergulha sob as placas litosféricas Americana, a leste, e Eurasiática e Indo-Australiana, a oeste. São áreas de intensa atividade sísmica e vulcânica, sujeitas à ocorrência de maremotos.
O oceano Pacífico tem um número considerável de atóis, a maior concentração de todos os oceanos na Terra.
Atol de Bokak
Atol de Wake
Atol Moruroa
Atol de Bikini
Atol de Cosmoledo
Atol de Astove
Atol de Nukuoro
Praia de Ladrilleros em Colombia na costa da região natural de Chocó
ilhota de Tahuna maru , Polinésia Francesa
Los Molinos em Chile da costa de Zona Sur
Existem várias correntes oceânicas, por exemplo: Norte Pacífica, Califórnia, Norte ameno, Sul equatorial, Sul frio, Sul ameno  Norte equatorial, Kuroshio, Aleutas, Sul Equatorial, Humboldt. As quatro primeiras limitam uma área de calmaria chamada Giro Pacífico Norte. Esta área foi descrita principalmente pelo pesquisador Charles Moore, desde 1997 e recebe nomes como "sopa gigante de lixo", "mancha de lixo" ou "ilha de lixo". Sua extensão é incerta, sendo descrita como do tamanho dos Estados Unidos, embora careça de fontes precisas. Foi descrita em fevereiro de 2008 no site da BBC e no jornal britânico The Independent. É composta principalmente de plástico.[5]
Importantes migrações humanas ocorreram no Pacífico em épocas pré-históricas, nomeadamente as dos polinésios a partir da margem asiática do oceano para o Taiti e depois para o Havaí, a Nova Zelândia e a Ilha da Páscoa.
O oceano foi avistado pelos europeus no início do século XVI, inicialmente pelo explorador espanhol Vasco Núñez de Balboa, que cruzou o istmo do Panamá em 1513 e nomeou-o como Mar del Sur (Mar do Sul), e depois pelo explorador português Fernão de Magalhães, que navegou o Pacífico durante a sua circum-navegação entre 1519 e 1522. Contudo, os portugueses já navegavam no Mar da China Meridional, que integra este oceano, desde 1511 e também no Mar de Banda desde 1512.
A poluição marinha é um termo genérico para a entrada nociva no mar de produtos químicos ou partículas. Os maiores culpados são as pessoas que usam os rios para a eliminação de seus resíduos. Os rios em seguida deságuam no oceano e com eles seguem muitos produtos químicos usados como fertilizantes na agricultura. O excesso de oxigênio que se esvai nos produtos químicos na água leva à hipóxia (baixa concentração de oxigênio) e à criação de uma zona morta.[6]
Detritos marinhos, também conhecidos como lixo marinho, é um termo usado para descrever dejetos produzidos pelo homem que se encontram flutuando em um lago, mar, oceano ou outro curso d'água. Detritos oceânicos tendem a se acumular no centro de correntes oceânicas e no litoral, frequentemente restos encalhados onde são conhecidos como lixo da praia.
O Deserto do Saara (português brasileiro) ou Deserto do Sara ou Deserto do Sáara[1] (português europeu) (em árabe: الصحراء الكبرى; romaniz.: aṣ-ṣaḥrā al-koubra) é conhecido por ser o maior deserto quente do mundo. Oficialmente, é o terceiro maior deserto da Terra, logo após a Antártida e o Ártico, pois estas duas também são consideradas desertos.[2] Localizado no Norte da África, tem uma área total de 9 065 000 km², sendo sua área equiparável à da Europa (10 400 000 km²) e à área dos Estados Unidos, e maior que a área de muitos países continentais tais como Brasil, Austrália e Índia. O nome Saara é uma transliteração da palavra árabe صحراء, que por sua vez é a tradução da palavra tuaregue tenere (deserto). O deserto do Saara compreende parte dos seguintes países e territórios: Argélia, Chade, Egito, Líbia, Mali, Mauritânia, Marrocos, Níger, Saara Ocidental, Sudão e Tunísia. Atualmente vivem cerca de 2,5 milhões de pessoas na região do Saara.[3]
Os seres humanos vivem na extremidade do deserto há milhares de anos. Durante a última glaciação, o deserto do Saara foi mais úmido (como o Leste africano) do que é agora, e já possuiu densas florestas tropicais. Seu clima era tão diferente que recentes estudos revelaram que o Rio Nilo corria antigamente para o Oceano Atlântico em vez de desaguar no mar Mediterrâneo. Uma mudança de poucos graus[4][5] no eixo de rotação terrestre causou, há cerca de 10 mil anos, uma grande transformação climática gerando o Saara. Essa alteração, segundo alguns cientistas, gerou as condições necessárias à formação da civilização egípcia quando obrigou pessoas que já haviam desenvolvido formas de vida sedentárias (agricultura e pastoreio) e tradições históricas (civilização) a se deslocarem para o leito atual do Rio Nilo.[carece de fontes?]
O deserto é rico em história, e diversos fósseis de dinossauros e outros animais bem como resquícios de diversas civilizações já foram encontrados ali. O Saara moderno geralmente é isento de vegetação, exceto no vale do Nilo, em poucos oásis, e em algumas montanhas nele dispersas.[carece de fontes?]
O deserto do Saara, no qual se distinguem dois trechos, um dominado por dunas arenosas e denominado Erg, e outro bastante pedregoso denominado Hamadas, compreende parte dos seguintes países e territórios: Argélia, Chade, Egito, Líbia, Mali, Mauritânia, Marrocos, Níger, Saara Ocidental, Sudão e Tunísia. Atualmente, em 2020, vivem cerca de 2,5 milhões de pessoas na região do Saara.[6]
A área do deserto também inclui parte da bacia do Rio Nilo, as montanhas Aïr, Hoggar, Atlas, Tibesti e Adrar dos Ifogas, e as subregiões do deserto da Líbia, do deserto da Núbia, do Ténéré e do deserto Oriental Africano. No interior do Saara, existem alguns poucos e dispersos oásis formados devido ao afloramento de aquíferos subterrâneos, estando entre eles os oásis de Baria, Gardaia, Timimoun, Cufra e Siuá. As fronteiras do Saara são o Oceano Atlântico a oeste, a cordilheira do Atlas e o mar Mediterrâneo a norte, o mar Vermelho a leste e o Sahel a sul. O Saara divide o continente africano em duas partes, o Norte da África e a África Sub-Saariana. A fronteira saariana ao sul é marcada por uma faixa semiárida de savana chamada Sahel.[carece de fontes?]
Os limites do Saara podem também ser definidos por critérios botânicos, definidos por Frank White, que correspondem a zonas climáticas (por exemplo, definidas por Robert Capot-Rey). O limite norte coincide com a região em que se cultiva a tamareira (nos oásis) e com o limite sul do esparto, uma poácea típica do clima mediterrânico; este limite corresponde igualmente à isoieta (linha de igual precipitação anual) dos 100 mm. A sul, o Saara limita com o Sahel, uma cintura de savana seca com um verão chuvoso, que se estende através de toda a África. Aí o limite é definido pela Cornulaca monacantha, uma quenopodiácea tolerante à seca, ou pelo limite norte do Cenchrus biflorus, uma grama característica do Sahel, o que corresponde à isoieta de 150 mm. Este valor é a média de muitos anos, uma vez que a precipitação varia muito de um ano a outro.[7][8][9]
O clima da região que compreende hoje o Saara sofreu enormes variações, indo várias vezes do seco ao húmido durante os últimos cem mil anos chegando a mais de 50 graus e muito seco a 1 hora da tarde, quando a temperatura chega a 53 graus celsius.[10] Durante a última Era do Gelo, o Saara era maior do que é hoje, estendendo para o sul além de seus limites atuais.[11] O fim da idade de gelo trouxe épocas melhores ao Saara no período compreendido entre aproximadamente 8 000 a.C. a 6 000 a.C., isto devido a área de baixa pressão que acompanhou o desmoronar do manto de gelo ao norte.[12]
Quando a Era do Gelo se foi, a parte norte do Saara secou. Entretanto, não muito tempo depois, monções trouxeram chuva ao Saara, neutralizando a tendência de desertificação do Saara na parte sul.[carece de fontes?]
Sabe-se que ar sobre o planeta Terra move-se por convecção de forma a redistribuir a energia pelo planeta, e ascensões de ar, puxando ar úmido do oceano, causam geralmente chuvas em determinadas regiões. Paradoxalmente, o Saara estava mais úmido quando recebeu mais insolação no verão. Por sua vez, todas as mudanças na insolação são causadas por mudanças na geofísica da Terra.[13]
Ao redor de 2 500 a.C., as monções recuaram para o sul onde está hoje,[14] que conduziram a desertificação do Saara. O deserto está atualmente árido na forma que o conhecemos hoje há aproximadamente 13 000 anos. Estas circunstâncias são responsáveis para o que foi chamado de Teoria da Bomba do Saara.[carece de fontes?]
O Saara é conhecido por ter um dos climas mais áridos do mundo. O vento que vem do nordeste prevalece, e pode por várias vezes fazer com que a areia dê forma a "furacões". As precipitações, muito raras mas não desconhecidas, acontecem ocasionalmente nas zonas de beira-mar ao norte e ao sul, e o deserto recebe aproximadamente 25 mm de chuva em um ano. As chuvas acontecem muito raramente, geralmente torrenciais após os longos períodos secos, que podem durar anos.[carece de fontes?]
Em 18 de fevereiro de 1979, nevou em vários lugares no sul da Argélia, incluindo uma tempestade de neve de 30 minutos que parou o tráfego em Gardaia, e foi relatado como sendo "pela primeira vez na memória viva".[15] A neve desapareceu em horas.[16] No entanto, várias cadeias montanhosas recebem neve regularmente. Um exemplo são as montanhas Tibesti, que recebem neve nos picos de mais de 2 500 metros uma vez a cada sete anos, em média.[17][18]
Em 18 de janeiro de 2012 nevou em vários lugares no oeste da Argélia. Ventos fortes sopraram a neve em estradas e edifícios na província de Béchar.[19]
Dromedários e cabras são os animais predominantes no Saara. Por causa das suas habilidades de sobrevivência, da resistência e da velocidade, o dromedário é o animal favorito dos nômades. O Leiurus quinquestriatus (escorpião-amarelo-da-palestina) é um tipo de escorpião do Saara que pode alcançar 10 cm. Ele produz agitoxina e cilatoxina, que são venenos tóxicos. O varano (família varanidae) é um tipo de lagarto que se encontra facilmente. Cerastes é um tipo de cobra que tem em média 50 cm no comprimento que tem proeminências que lembram um par de chifres. Muito ativa à noite, encontra-se geralmente enterrada na areia com somente seus olhos visíveis. As mordidas destas cobras são dolorosas, mas raramente fatais. Há também o feneco, um onívoro. Há o Dassie, cujo primeiro fóssil encontrado remonta a 40 milhões de anos atrás. O adax é um grande antílope branco, e é hoje uma espécie ameaçada. Muito adaptado ao deserto, pode sobreviver por até um ano sem água. A chita do Saara vive no Níger, no Mali e no Chade.[20]
